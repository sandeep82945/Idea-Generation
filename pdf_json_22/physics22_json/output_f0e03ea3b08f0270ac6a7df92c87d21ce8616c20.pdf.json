{
    "abstractText": "A quantum thermal machine is an open quantum system that enables the conversion between heat and work at the micro or nano-scale. Optimally controlling such out-of-equilibrium systems is a crucial yet challenging task with applications to quantum technologies and devices. We introduce a general model-free framework based on Reinforcement Learning to identify out-of-equilibrium thermodynamic cycles that are Pareto optimal trade-offs between power and efficiency for quantum heat engines and refrigerators. The method does not require any knowledge of the quantum thermal machine, nor of the system model, nor of the quantum state. Instead, it only observes the heat fluxes, so it is both applicable to simulations and experimental devices. We test our method on a model of an experimentally realistic refrigerator based on a superconducting qubit, and on a heat engine based on a quantum harmonic oscillator. In both cases, we identify the Pareto-front representing optimal power-efficiency tradeoffs, and the corresponding cycles. Such solutions outperform previous proposals made in the literature, such as optimized Otto cycles, reducing quantum friction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Paolo A. Erdman"
        },
        {
            "affiliations": [],
            "name": "Frank No\u00e9"
        }
    ],
    "id": "SP:56f403f288d458bc7eea8f2eaf232553867fb1e4",
    "references": [
        {
            "authors": [
                "F. Giazotto",
                "T.T. Heikkil\u00e4",
                "A. Luukanen",
                "A.M. Savin",
                "J.P. Pekola"
            ],
            "title": "Opportunities for mesoscopics in thermometry and refrigeration: Physics and applications",
            "venue": "Rev. Mod. Phys. 78, 217 ",
            "year": 2006
        },
        {
            "authors": [
                "J.P. Pekola"
            ],
            "title": "Towards quantum thermodynamics in electronic circuits",
            "venue": "Nat. Phys. 11, 118 ",
            "year": 2015
        },
        {
            "authors": [
                "S. Vinjanampathy",
                "J. Anders"
            ],
            "title": "Quantum thermodynamics",
            "venue": "Contemp. Phys. 57, 545 ",
            "year": 2016
        },
        {
            "authors": [
                "G. Benenti",
                "G. Casati",
                "K. Saito"
            ],
            "title": "and R",
            "venue": "S.Whitney, Fundamental aspects of steady-state conversion of heat to work at the nanoscale, Phys. Rep. 694, 1 ",
            "year": 2017
        },
        {
            "authors": [
                "F. Binder",
                "L. Correa",
                "C. Gogolin",
                "J. Anders",
                "G. Adesso"
            ],
            "title": "Thermodynamics in the Quantum Regime: Fundamental Aspects and New Directions",
            "venue": "Fundamental Theories of Physics ",
            "year": 2019
        },
        {
            "authors": [
                "A. Ronzani",
                "B. Karimi",
                "J. Senior",
                "Y.-C. Chang",
                "J.T. Peltonen",
                "C.-D. Chen",
                "J.P. Pekola"
            ],
            "title": "Tunable photonic heat transport in a quantum heat valve",
            "venue": "Nat. Phys. 14, 991 ",
            "year": 2018
        },
        {
            "authors": [
                "B. Dutta",
                "D. Majidi",
                "A.G. Corral",
                "P.A. Erdman",
                "S. Florens",
                "T.A. Costi",
                "H. Courtois",
                "C.B. Winkelmann"
            ],
            "title": "Direct probe of the seebeck coefficient in a kondocorrelated single-quantum-dot transistor",
            "venue": "Nano Lett. 19, 506 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Senior",
                "A. Gubaydullin",
                "B. Karimi",
                "J.T. Peltonen",
                "J. Ankerhold",
                "J.P. Pekola"
            ],
            "title": "Heat rectification via a superconducting artificial atom",
            "venue": "Commun. Phys. 3, 40 ",
            "year": 2020
        },
        {
            "authors": [
                "O. Maillet",
                "D. Subero",
                "J.T. Peltonen",
                "D.S. Golubev",
                "J.P. Pekola"
            ],
            "title": "Electric field control of radiative heat transfer in a superconducting circuit",
            "venue": "Nat. Commun. 11, 4326 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Ro\u00dfnagel",
                "S.T. Dawkins",
                "K.N. Tolazzi",
                "O. Abah",
                "E. Lutz",
                "F. Schmidt-Kaler",
                "K. Singer"
            ],
            "title": "A single-atom heat engine",
            "venue": "Science 352, 325 ",
            "year": 2016
        },
        {
            "authors": [
                "M. Josefsson",
                "A. Svilans",
                "A.M. Burke",
                "E.A. Hoffmann",
                "S. Fahlvik",
                "C. Thelander",
                "M. Leijnse",
                "H. Linke"
            ],
            "title": "A quantum-dot heat engine operating close to the thermodynamic efficiency limits",
            "venue": "Nat. Nanotechnol. 13, 920 ",
            "year": 2018
        },
        {
            "authors": [
                "J. Klatzow",
                "J.N. Becker",
                "P.M. Ledingham",
                "C. Weinzetl",
                "K.T. Kaczmarek",
                "D.J. Saunders",
                "J. Nunn",
                "I.A. Walmsley",
                "R. Uzdin",
                "E. Poem"
            ],
            "title": "Experimental demonstration of quantum effects in the operation of microscopic heat engines",
            "venue": "Phys. Rev. Lett. 122, 110601 ",
            "year": 2019
        },
        {
            "authors": [
                "D. von Lindenfels",
                "O. Gr\u00e4b",
                "C.T. Schmiegelow",
                "V. Kaushal",
                "J. Schulz",
                "M.T. Mitchison",
                "J. Goold",
                "F. Schmidt-Kaler",
                "U. G"
            ],
            "title": "Poschinger, Spin heat engine coupled to a harmonic-oscillator flywheel",
            "venue": "Phys. Rev. Lett. 123,",
            "year": 2019
        },
        {
            "authors": [
                "G. Maslennikov",
                "S. Ding",
                "R. Habl\u00e4tzel",
                "J. Gan",
                "A. Roulet",
                "S. Nimmrichter",
                "J. Dai",
                "V. Scarani",
                "D. Matsukevich"
            ],
            "title": "Quantum absorption refrigerator with trapped ions",
            "venue": "Nat. Commun. 10, 202 ",
            "year": 2019
        },
        {
            "authors": [
                "J.P.S. Peterson",
                "T.B. Batalh\u00e3o",
                "M. Herrera",
                "A.M. Souza",
                "R.S. Sarthour",
                "I.S. Oliveira",
                "R.M. Serra"
            ],
            "title": "Experimental characterization of a spin quantum heat engine",
            "venue": "Phys. Rev. Lett. 123, 240601 ",
            "year": 2019
        },
        {
            "authors": [
                "D. Prete",
                "P.A. Erdman",
                "V. Demontis",
                "V. Zannier",
                "D. Ercolani",
                "L. Sorba",
                "F. Beltram",
                "F. Rossella",
                "F. Taddei",
                "S. Roddaro"
            ],
            "title": "Thermoelectric conversion at 30 k in inas/inp nanowire quantum dots",
            "venue": "Nano Lett. 19, 3033 ",
            "year": 2019
        },
        {
            "authors": [
                "N.V. Horne",
                "D. Yum",
                "T. Dutta",
                "P. H\u00e4nggi",
                "J. Gong",
                "D. Poletti",
                "M. Mukherjee"
            ],
            "title": "Single-atom energyconversion device with a quantum load",
            "venue": "NPJ Quantum Inf. 6, 37 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Krantz",
                "M. Kjaergaard",
                "F. Yan",
                "T.P. Orlando",
                "S. Gustavsson",
                "W.D. Oliver"
            ],
            "title": "A quantum engineer\u2019s guide to superconducting qubits",
            "venue": "Appl. Phys. Rev. 6, 021318 ",
            "year": 2019
        },
        {
            "authors": [
                "K. Huang"
            ],
            "title": "Statistical Mechanics",
            "venue": "2nd ed. ",
            "year": 1987
        },
        {
            "authors": [
                "M. Esposito",
                "R. Kawai",
                "K. Lindenberg"
            ],
            "title": "and C",
            "venue": "V. den Broeck, Efficiency at maximum power of low-dissipation carnot engines, Phys. Rev. Lett. 105, 150603 ",
            "year": 2010
        },
        {
            "authors": [
                "J. Wang",
                "J. He",
                "X. He"
            ],
            "title": "Performance analysis of a two-state quantum heat engine working with a singlemode radiation field in a cavity",
            "venue": "Phys. Rev. E 84, 041127 ",
            "year": 2011
        },
        {
            "authors": [
                "J.E. Avron",
                "M. Fraas",
                "G.M. Graf",
                "P. Grech"
            ],
            "title": "Adiabatic theorems for generators of contracting evolutions",
            "venue": "Commun. Math. Phys. 314, 163 ",
            "year": 2012
        },
        {
            "authors": [
                "M.F. Ludovico",
                "F. Battista"
            ],
            "title": "F",
            "venue": "von Oppen, and L. Arrachea, Adiabatic response and quantum thermoelectrics for ac-driven quantum systems, Phys. Rev. B 93, 075136 ",
            "year": 2016
        },
        {
            "authors": [
                "V. Cavina",
                "A. Mari",
                "V. Giovannetti"
            ],
            "title": "Slow dynamics and thermodynamics of open quantum systems",
            "venue": "Phys. Rev. Lett. 119, 050601 ",
            "year": 2017
        },
        {
            "authors": [
                "P. Abiuso",
                "V. Giovannetti"
            ],
            "title": "Non-markov enhancement of maximum power for quantum thermal machines",
            "venue": "Phys. Rev. A 99, 052106 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Scandi",
                "M. Perarnau-Llobet"
            ],
            "title": "Thermodynamic length in open quantum systems",
            "venue": "Quantum 3, 197 ",
            "year": 2019
        },
        {
            "authors": [
                "B. Bhandari",
                "P.T. Alonso",
                "F. Taddei"
            ],
            "title": "F",
            "venue": "von Oppen, R. Fazio, and L. Arrachea, Geometric properties of adiabatic quantum thermal machines, Phys. Rev. B 102, 155407 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Terr\u00e9n Alonso",
                "P. Abiuso",
                "M. Perarnau-Llobet",
                "L. Arrachea"
            ],
            "title": "Geometric optimization of nonequilibrium adiabatic thermal machines and implementation in a qubit system",
            "venue": "PRX Quantum 3, 010326 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Eglinton",
                "K. Brandner"
            ],
            "title": "Geometric bounds on the power of adiabatic thermal machines",
            "venue": "Phys. Rev. E 105, L052102 ",
            "year": 2022
        },
        {
            "authors": [
                "P. Abiuso",
                "M. Perarnau-Llobet"
            ],
            "title": "Optimal cycles for low-dissipation heat engines",
            "venue": "Phys. Rev. Lett. 124, 110606 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Abiuso",
                "H.J.D. Miller",
                "M. Perarnau-Llobet",
                "M. Scandi"
            ],
            "title": "Geometric optimisation of quantum thermodynamic processes",
            "venue": "Entropy 22 ",
            "year": 2020
        },
        {
            "authors": [
                "V. Cavina",
                "P.A. Erdman",
                "P. Abiuso",
                "L. Tolomeo",
                "V. Giovannetti"
            ],
            "title": "Maximum-power heat engines and refrigerators in the fast-driving regime",
            "venue": "Phys. Rev. A 104, 032226 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Arrachea",
                "M. Moskalets",
                "L. Martin-Moreno"
            ],
            "title": "Heat production and energy balance in nanoscale engines driven by time-dependent fields",
            "venue": "Phys. Rev. B 75, 245420 ",
            "year": 2007
        },
        {
            "authors": [
                "M. Esposito",
                "R. Kawai",
                "K. Lindenberg"
            ],
            "title": "and C",
            "venue": "Van den Broeck, Quantum-dot carnot engine at maximum power, Phys. Rev. E 81, 041106 ",
            "year": 2010
        },
        {
            "authors": [
                "S. Juergens",
                "F. Haupt",
                "M. Moskalets",
                "J. Splettstoesser"
            ],
            "title": "Thermoelectric performance of a driven double quantum dot",
            "venue": "Phys. Rev. B 87, 245423 ",
            "year": 2013
        },
        {
            "authors": [
                "M. Campisi",
                "J. Pekola",
                "R. Fazio"
            ],
            "title": "Nonequilibrium fluctuations in quantum heat engines: theory",
            "venue": "example, and possible solid state experiments, New J. Phys. 17, 035012 ",
            "year": 2015
        },
        {
            "authors": [
                "R. Dann",
                "R. Kosloff"
            ],
            "title": "Qunatum signatures in the quantum carnot cycle",
            "venue": "New J. Phys. 22, 013055 ",
            "year": 2020
        },
        {
            "authors": [
                "O.A.D. Molitor",
                "G.T. Landi"
            ],
            "title": "Stroboscopic twostroke quantum heat engines",
            "venue": "Phys. Rev. A 102, 042217 ",
            "year": 2020
        },
        {
            "authors": [
                "V. Shaghaghi",
                "G.M. Palma",
                "G. Benenti"
            ],
            "title": "Extracting work from random collisions: A model of a quantum heat engine",
            "venue": "Phys. Rev. E 105, 034101 ",
            "year": 2022
        },
        {
            "authors": [
                "F. Cavaliere",
                "M. Carrega",
                "G. De Filippis",
                "V. Cataudella",
                "G. Benenti",
                "M. Sassetti"
            ],
            "title": "Dynamical heat engines with non-markovian reservoirs",
            "venue": "Phys. Rev. Res. 4, 033233 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Feldmann",
                "E. Geva",
                "R. Kosloff",
                "P. Salamon"
            ],
            "title": "Heat engines in finite time governed by master equations",
            "venue": "Am. J. Phys. 64, 485 ",
            "year": 1996
        },
        {
            "authors": [
                "T. Feldmann",
                "R. Kosloff"
            ],
            "title": "Performance of discrete heat engines and heat pumps in finite time",
            "venue": "Phys. Rev. E 61, 4774 ",
            "year": 2000
        },
        {
            "authors": [
                "Y. Rezek",
                "R. Kosloff"
            ],
            "title": "Irreversible performance of a quantum harmonic heat engine",
            "venue": "New J. Phys. 8, 83 ",
            "year": 2006
        },
        {
            "authors": [
                "H. Quan",
                "Y. Liu",
                "C. Sun",
                "F. Nori"
            ],
            "title": "Quantum thermodynamic cycles and quantum heat engines",
            "venue": "Phys. Rev. E 76, 031105 ",
            "year": 2007
        },
        {
            "authors": [
                "O. Abah",
                "J. Ro\u00dfnagel",
                "G. Jacob",
                "S. Deffner",
                "F. Schmidt- Kaler",
                "K. Singer",
                "E. Lutz"
            ],
            "title": "Single-ion heat engine at maximum power",
            "venue": "Phys. Rev. Lett. 109, 203006 ",
            "year": 2012
        },
        {
            "authors": [
                "A.E. Allahverdyan",
                "K.V. Hovhannisyan",
                "A.V. Melkikh",
                "S.G. Gevorkian"
            ],
            "title": "Carnot cycle at finite power: Attainability of maximal efficiency",
            "venue": "Phys. Rev. Lett. 111, 050601 ",
            "year": 2013
        },
        {
            "authors": [
                "K. Zhang",
                "F. Bariani",
                "P. Meystre"
            ],
            "title": "Quantum optomechanical heat engine",
            "venue": "Phys. Rev. Lett. 112, 150602 ",
            "year": 2014
        },
        {
            "authors": [
                "M. Campisi",
                "R. Fazio"
            ],
            "title": "The power of a critical heat engine",
            "venue": "Nat. Commun. 7, 11895 ",
            "year": 2016
        },
        {
            "authors": [
                "B. Karimi",
                "J.P. Pekola"
            ],
            "title": "Otto refrigerator based on a superconducting qubit: Classical and quantum performance",
            "venue": "Phys. Rev. B 94, 184503 ",
            "year": 2016
        },
        {
            "authors": [
                "R. Kosloff",
                "Y. Rezek"
            ],
            "title": "The quantum harmonic otto cycle",
            "venue": "Entropy 19, 136 ",
            "year": 2017
        },
        {
            "authors": [
                "G. Watanabe",
                "B.P. Venkatesh",
                "P. Talkner"
            ],
            "title": "and A",
            "venue": "del Campo, Quantum performance of thermal machines over many cycles, Phys. Rev. Lett. 118, 050601 ",
            "year": 2017
        },
        {
            "authors": [
                "S. Deffner"
            ],
            "title": "Efficiency of harmonic quantum otto engines at maximal power",
            "venue": "Entropy 20, 875 ",
            "year": 2018
        },
        {
            "authors": [
                "D. Gelbwaser-Klimovsky",
                "A. Bylinskii",
                "D. Gangloff",
                "R. Islam",
                "A. Aspuru-Guzik",
                "V. Vuletic"
            ],
            "title": "Single-atom heat machines enabled by energy quantization",
            "venue": "Phys. Rev. Lett. 120, 170601 ",
            "year": 2018
        },
        {
            "authors": [
                "J. Chen",
                "C. Sun",
                "H. Dong"
            ],
            "title": "Boosting the performance of quantum otto heat engines",
            "venue": "Phys. Rev. E 100, 032144 ",
            "year": 2019
        },
        {
            "authors": [
                "J.P. Pekola",
                "B. Karimi",
                "G. Thomas",
                "D.V. Averin"
            ],
            "title": "Supremacy of incoherent sudden cycles",
            "venue": "Phys. Rev. B 100, 085405 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Das",
                "V. Mukherjee"
            ],
            "title": "Quantum-enhanced finitetime otto cycle",
            "venue": "Phys. Rev. B 2, 033083 ",
            "year": 2020
        },
        {
            "authors": [
                "M.V. Berry"
            ],
            "title": "Transitionless quantum driving",
            "venue": "J. Phys. A: Math. Theor. 42, 365303 ",
            "year": 2009
        },
        {
            "authors": [
                "J.Q.-h. Deng",
                "Wang",
                "Z. Liu",
                "P. H\u00e4nggi",
                "G. J"
            ],
            "title": "Boosting work characteristics and overall heat-engine performance via shortcuts to adiabaticity: Quantum and classical systems",
            "venue": "Phys. Rev. E 88,",
            "year": 2013
        },
        {
            "authors": [
                "E. Torrontegui",
                "S. Ib\u00e1\u00f1ez"
            ],
            "title": "S",
            "venue": "Mart\u0301\u0131nez-Garaot, M. Modugno, A. del Campo, D. Gu\u00e9ry-Odelin, A. Ruschhaupt, X. Chen, and J. G. Muga, Shortcuts to adiabaticity, Adv. At., Mol., Opt. Phys. 62, 117 ",
            "year": 2013
        },
        {
            "authors": [
                "B. \u00c7akmak",
                "O.E. M\u00fcstecapl\u0131o\u011flu"
            ],
            "title": "Spin quantum heat engines with shortcuts to adiabaticity",
            "venue": "Phys. Rev. E 99, 032108 ",
            "year": 2019
        },
        {
            "authors": [
                "S. Deng",
                "A. Chenu"
            ],
            "title": "P",
            "venue": "Diao1, F. Li, S. Yu, I. Coulamy, A. del Campo, and H. Wu, Superadiabatic quantum friction suppression in finite-time thermodynamics, Sci. Adv. 18, eaar5909 ",
            "year": 2018
        },
        {
            "authors": [
                "K. Funo",
                "N. Lambert",
                "B. Karimi",
                "J.P. Pekola",
                "Y. Masuyama",
                "F. Nori"
            ],
            "title": "Speeding up a quantum refrigerator via counterdiabatic driving",
            "venue": "Phys. Rev. B 100, 035407 ",
            "year": 2019
        },
        {
            "authors": [
                "T. Villazon",
                "A. Polkovnikov",
                "A. Chandran"
            ],
            "title": "Swift heat transfer by fast-forward driving in open quantum systems",
            "venue": "Phys. Rev. A 100, 012126 ",
            "year": 2019
        },
        {
            "authors": [
                "I. Khait",
                "J. Carrasquilla",
                "D. Segal"
            ],
            "title": "Optimal control of quantum thermal machines using machine learning",
            "venue": "Phys. Rev. Res. 4, L012029 ",
            "year": 2022
        },
        {
            "authors": [
                "V. Cavina",
                "A. Mari",
                "A. Carlini",
                "V. Giovannetti"
            ],
            "title": "Optimal thermodynamic control in open quantum systems",
            "venue": "Phys. Rev. A 98, 012139 ",
            "year": 2018
        },
        {
            "authors": [
                "N. Suri",
                "F.C. Binder",
                "S. Muralidharan"
            ],
            "title": "B",
            "venue": "amd Vinjanampathy, Speeding up thermalisation via open quantum system variational optimisation, Eur. Phys. J. Spec. Top. 227, 203 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Menczel",
                "T. Pyh\u00e4ranta",
                "C. Flindt",
                "K. Brandner"
            ],
            "title": "Two-stroke optimization scheme for mesoscopic refrigerators",
            "venue": "Phys. Rev. B 99, 224306 ",
            "year": 2019
        },
        {
            "authors": [
                "P.A. Erdman",
                "F. No\u00e9"
            ],
            "title": "Identifying optimal cycles in quantum thermal machines with reinforcementlearning",
            "venue": "NPJ Quantum Inf. 8, 1 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "International Conference on Machine Learning , Vol. 80 ",
            "year": 2018
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "K. Hartikainen",
                "G. Tucker",
                "S. Ha",
                "J. Tan",
                "V. Kumar",
                "H. Zhu",
                "A. Gupta"
            ],
            "title": "P",
            "venue": "Abbeel, et al., Soft actor-critic algorithms and applications, arXiv:1812.05905 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Christodoulou"
            ],
            "title": "Soft actor-critic for discrete action settings",
            "venue": "arXiv:1910.07207 ",
            "year": 2019
        },
        {
            "authors": [
                "O. Delalleau",
                "M. Peter",
                "E. Alonso",
                "A. Logut"
            ],
            "title": "Discrete and continuous action representation for practical rl in video games",
            "venue": "arXiv:1912.11077 ",
            "year": 2019
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller"
            ],
            "title": "A",
            "venue": "K. 19 Fidjeland, G. Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518, 529 ",
            "year": 2015
        },
        {
            "authors": [
                "D. Silver",
                "J. Schrittwieser",
                "K. Simonyan",
                "I. Antonoglou",
                "A. Huang",
                "A. Guez",
                "T. Hubert",
                "L. Baker",
                "M. Lai"
            ],
            "title": "A",
            "venue": "Bolton, et al., Mastering the game of go without human knowledge, Nature 550, 354 ",
            "year": 2017
        },
        {
            "authors": [
                "O. Vinyals",
                "I. Babuschkin",
                "W.M. Czarnecki",
                "M. Mathieu",
                "A. Dudzik",
                "J. Chung",
                "D.H. Choi",
                "R. Powell",
                "T. Ewalds"
            ],
            "title": "P",
            "venue": "Georgiev, et al., Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575, 350 ",
            "year": 2019
        },
        {
            "authors": [
                "T. Haarnoja",
                "S. Ha",
                "A. Zhou",
                "J. Tan",
                "G. Tucker",
                "S. Levine"
            ],
            "title": "Learning to walk via deep reinforcement learning",
            "venue": "arXiv:1812.11103 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Bukov",
                "A.G.R. Day",
                "D. Sels",
                "P. Weinberg",
                "A. Polkovnikov",
                "P. Mehta"
            ],
            "title": "Reinforcement learning in different phases of quantum control",
            "venue": "Phys. Rev. X 8, 031086 ",
            "year": 2018
        },
        {
            "authors": [
                "Z. An",
                "D. Zhou"
            ],
            "title": "Deep reinforcement learning for quantum gate control",
            "venue": "EPL 126, 60002 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Dalgaard",
                "F. Motzoi",
                "J.J. S\u00f8rensen",
                "J. Sherson"
            ],
            "title": "Global optimization of quantum dynamics with alphazero deep exploration",
            "venue": "NPJ Quantum Inf. 6, 6 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Mackeprang",
                "D.B.R. Dasari",
                "J. Wrachtrup"
            ],
            "title": "A reinforcement learning approach for quantum state engineering",
            "venue": "Quantum Mach. Intell. 2, 5 ",
            "year": 2020
        },
        {
            "authors": [
                "F. Sch\u00e4fer",
                "M. Kloc",
                "C. Bruder",
                "N. L\u00f6rch"
            ],
            "title": "A differentiable programming method for quantum control",
            "venue": "Mach. Learn.: Sci. Technol. 1, 035009 ",
            "year": 2020
        },
        {
            "authors": [
                "F. Sch\u00e4fer",
                "P. Sekatski",
                "M. Koppenh\u00f6fer",
                "C. Bruder",
                "M. Kloc"
            ],
            "title": "Control of stochastic quantum dynamics by differentiable programming",
            "venue": "Mach. Learn.: Sci. Technol. 2, 035004 ",
            "year": 2021
        },
        {
            "authors": [
                "R. Porotti",
                "A. Essig",
                "B. Huard",
                "F. Marquardt"
            ],
            "title": "Deep reinforcement learning for quantum state preparation with weak nonlinear measurements",
            "venue": "Quantum 6, 747 ",
            "year": 2022
        },
        {
            "authors": [
                "F. Marquardt"
            ],
            "title": "Machine learning and quantum devices",
            "venue": "SciPost Phys. Lect. Notes , 29 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Brown",
                "P. Sgroi",
                "L. Giannelli",
                "G.S. Paraoanu",
                "E. Paladino",
                "G. Falci",
                "M. Paternostro",
                "A. Ferraro"
            ],
            "title": "Reinforcement learning-enhanced protocols for coherent population-transfer in three-level quantum systems",
            "venue": "New J. Phys. 23, 093035 ",
            "year": 2021
        },
        {
            "authors": [
                "F. Metz",
                "M. Bukov"
            ],
            "title": "Self-correcting quantum manybody control using reinforcement learning with tensor networks",
            "venue": "Nat. Mach. Intell. 5, 780 ",
            "year": 2023
        },
        {
            "authors": [
                "M.Y. Niu",
                "S. Boixo",
                "V.N. Smelyanskiy",
                "H. Neven"
            ],
            "title": "Universal quantum control through deep reinforcement learning",
            "venue": "NPJ Quantum Inf. 5, 33 ",
            "year": 2019
        },
        {
            "authors": [
                "X.-M. Zhang",
                "Z. Wei",
                "R. Asad",
                "X.-C. Yang",
                "X. Wang"
            ],
            "title": "When does reinforcement learning stand out in quantum control? a comparative study on state preparation",
            "venue": "NPJ Quantum Inf. 5, 85 ",
            "year": 2019
        },
        {
            "authors": [
                "T. F\u00f6sel",
                "P. Tighineanu",
                "T. Weiss",
                "F. Marquardt"
            ],
            "title": "Reinforcement learning with neural networks for quantum feedback",
            "venue": "Phys. Rev. X 8, 031084 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Sweke",
                "M.S. Kesselring"
            ],
            "title": "E",
            "venue": "P. L. van Nieuwenburg, and J. Eisert, Reinforcement learning decoders for fault-tolerant quantum computation, Mach. Learn.: Sci. Technol. 2, 025005 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Sgroi",
                "G.M. Palma",
                "M. Paternostro"
            ],
            "title": "Reinforcement learning approach to nonequilibrium quantum thermodynamics",
            "venue": "Phys. Rev. Lett. 126, 020601 ",
            "year": 2021
        },
        {
            "authors": [
                "R. Kosloff",
                "T. Feldmann"
            ],
            "title": "Discrete four-stroke quantum heat engine exploring the origin of friction",
            "venue": "Phys. Rev. E 65, 055102 ",
            "year": 2002
        },
        {
            "authors": [
                "A. Friedenberger",
                "E. Lutz"
            ],
            "title": "When is a quantum heat engine quantum",
            "venue": "EPL 120, 10002 ",
            "year": 2017
        },
        {
            "authors": [
                "K. Brandner",
                "M. Bauer"
            ],
            "title": "and S",
            "venue": "U., Universal coherenceinduced power losses of quantum heat engines in linear response, Phys. Rev. Lett. 119, 170602 ",
            "year": 2017
        },
        {
            "authors": [
                "J. Lekscha",
                "H. Wilming",
                "J. Eisert",
                "R. Gallego"
            ],
            "title": "Quantum thermodynamics with local control",
            "venue": "Phys. Rev. E 97, 022142 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Strasberg",
                "G. Schaller",
                "N. Lambert",
                "T. Brandes"
            ],
            "title": "Nonequilibrium thermodynamics in the strong coupling and non-markovian regime based on a reaction coordinate mapping",
            "venue": "New J. Phys. 18, 073007 ",
            "year": 2016
        },
        {
            "authors": [
                "L.F. Seoane",
                "R. Sol\u00e9"
            ],
            "title": "Multiobjective optimization and phase transitions",
            "venue": "Proceedings of ECCS 2014 , edited by S. Battiston, F. De Pellegrini, G. Caldarelli, and E. Merelli ",
            "year": 2016
        },
        {
            "authors": [
                "H.J.D. Miller",
                "M. Scandi",
                "J. Anders",
                "M. Perarnau- Llobet"
            ],
            "title": "Work fluctuations in slow processes: Quantum signatures and optimal control",
            "venue": "Phys. Rev. Lett. 123, 230603 ",
            "year": 2019
        },
        {
            "authors": [
                "A.P. Solon",
                "J.M. Horowitz"
            ],
            "title": "Phase transition in protocols minimizing work fluctuations",
            "venue": "Phys. Rev. Lett. 120, 180605 ",
            "year": 2018
        },
        {
            "authors": [
                "A. van den Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "arXiv:1512.03385 ",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv:1412.6980 ",
            "year": 2014
        },
        {
            "authors": [
                "V. Gorini",
                "A. Kossakowski",
                "E.C.G. Sudarshan"
            ],
            "title": "Completely positive dynamical semigroups of N-level systems",
            "venue": "J. Math. Phys. 17, 821 ",
            "year": 1976
        },
        {
            "authors": [
                "G. Lindblad"
            ],
            "title": "On the generators of quantum dynamical semigroups",
            "venue": "Commun. Math. Phys 48, 119 ",
            "year": 1976
        },
        {
            "authors": [
                "M. Yamaguchi",
                "T. Yuge",
                "T. Ogawa"
            ],
            "title": "Markovian quantum master equation beyond adiabatic regime",
            "venue": "Phys. Rev. E 95, 012136 ",
            "year": 2017
        },
        {
            "authors": [
                "A.C. Barato",
                "U. Seifert"
            ],
            "title": "Thermodynamic uncertainty relation for biomolecular processes",
            "venue": "Phys. Rev. Lett. 114, 158101 ",
            "year": 2015
        },
        {
            "authors": [
                "G. Guarnieri",
                "G.T. Landi",
                "S.R. Clark",
                "J. Goold"
            ],
            "title": "Thermodynamics of precision in quantum nonequilibrium steady states",
            "venue": "Phys. Rev. Res. 1, 033021 ",
            "year": 2019
        },
        {
            "authors": [
                "H.J.D. Miller",
                "M.H. Mohammady",
                "M. Perarnau- Llobet",
                "G. Guarnieri"
            ],
            "title": "Thermodynamic uncertainty relation in slowly driven quantum heat engines",
            "venue": "Phys. Rev. Lett. 126, 210603 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Achiam"
            ],
            "title": "Spinning Up in Deep Reinforcement Learning (2018)",
            "year": 2018
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction (MIT press, 2018)",
            "year": 2018
        },
        {
            "authors": [
                "R. Dann",
                "A. Levy",
                "R. Kosloff"
            ],
            "title": "Time-dependent markovian quantum master equation",
            "venue": "Phys. Rev. A 98, 052129 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Alicki"
            ],
            "title": "The quantum open system as a model of the heat engine",
            "venue": "J. Phys. A: Math. Gen. 12, L103 ",
            "year": 1979
        },
        {
            "authors": [
                "T. Baumgratz",
                "M. Cramer",
                "M. Plenio"
            ],
            "title": "Quantifying coherence",
            "venue": "Phys. Rev. Lett. 113, 140401 ",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Model-free optimization of power/efficiency tradeoffs in quantum thermal machines using reinforcement learning\nPaolo A. Erdman1, \u2217 and Frank Noe\u03012, 1, 3, 4, \u2020\n1Freie Universita\u0308t Berlin, Department of Mathematics and Computer Science, Arnimallee 6, 14195 Berlin, Germany 2Microsoft Research AI4Science, Karl-Liebknecht Str. 32, 10178 Berlin, Germany\n3Freie Universita\u0308t Berlin, Department of Physics, Arnimallee 6, 14195 Berlin, Germany 4Rice University, Department of Chemistry, Houston, TX 77005, USA\nA quantum thermal machine is an open quantum system that enables the conversion between heat and work at the micro or nano-scale. Optimally controlling such out-of-equilibrium systems is a crucial yet challenging task with applications to quantum technologies and devices. We introduce a general model-free framework based on Reinforcement Learning to identify out-of-equilibrium thermodynamic cycles that are Pareto optimal trade-offs between power and efficiency for quantum heat engines and refrigerators. The method does not require any knowledge of the quantum thermal machine, nor of the system model, nor of the quantum state. Instead, it only observes the heat fluxes, so it is both applicable to simulations and experimental devices. We test our method on a model of an experimentally realistic refrigerator based on a superconducting qubit, and on a heat engine based on a quantum harmonic oscillator. In both cases, we identify the Pareto-front representing optimal power-efficiency tradeoffs, and the corresponding cycles. Such solutions outperform previous proposals made in the literature, such as optimized Otto cycles, reducing quantum friction."
        },
        {
            "heading": "INTRODUCTION",
            "text": "A driving force of the research field of quantum thermodynamic is the quest of understanding and designing quantum thermal machines (QTMs), i.e. devices that convert between heat and work at the micro or nanoscale exploiting quantum effects [1\u20135]. Such devices could be operated as heat engines, which convert heat into work, or refrigerators, that extract heat from a cold bath. Recent experiments have measured the heat flowing across these devices [6\u20139], and early experimental realizations of QTMs have been reported [10\u201317].\nHowever, the optimal control of such devices, necessary to reveal their maximum performance, is an extremely challenging task that could find application in the control of quantum technologies and devices beyond QTMs. The difficulties include: (i) having to operate in finite time, the state can be driven far from equilibrium, where the thermal properties of the system are model-specific; (ii) the optimization is a search over the space of all possible time-dependent controls, which increases exponentially with the number of time points describing the cycle; (iii) in experimental devices, often subject to undesired effects such as noise and decoherence [18], we could have a limited knowledge of the actual model describing the dynamics of the QTM.\nA further difficulty (iv) arises in QTMs, since the maximization of their performance requires a multi-objective optimization. Indeed, the two main quantities that describe the performance of a heat engine (refrigerator) are the extracted power (cooling power) and the efficiency (coefficient of performance). The optimal strategy to\n\u2217 p.erdman@fu-berlin.de \u2020 frank.noe@fu-berlin.de\nmaximize the efficiency consists of performing reversible transformations [19] which are, however, infinitely slow, and thus deliver vanishing power. Conversely, maximum power is typically reached at the expense of reduced efficiency. Therefore, one must seek optimal trade-offs between the two. The theoretical optimization of QTMs is typically carried out making restrictive assumptions on the cycle. For example, optimal strategies have been derived assuming the driving speed of the control to be slow [20\u201329] or fast [30\u201332] compared to the thermalization time. Other approaches consists of assuming a-priori a specific shape of the cycle structure [33\u201340], such as the Otto cycle [41\u201356]. Shortcuts to adiabaticity [57\u201365] and variational strategies [66\u201368] have also been employed. In general, aside from variational approaches, there is no guarantee that these regimes and cycles are optimal. Recently, reinforcement-learning (RL) has been used to find cycles that maximize the power of QTMs without making assumptions on the cycle structure [69]. However, this approach requires a model of the system and the knowledge of the quantum state of the system, which restricts its practical applicability. This calls for the development of robust and general strategies that overcome all above-mentioned difficulties (i-iv). We propose a RL-based method with the following properties: (i) it finds cycles yielding near Pareto-optimal trade-offs between power and efficiency, i.e. the collection of cycles such that it is not possible to further improve either power or efficiency, without decreasing the other one. (ii) It only requires the heat currents as input, and not the quantum state of the system. (iii) It is completely model-free. (iv) It does not make any assumption on the cycle structure, nor on the driving speed. The RL method is based on the Soft Actor-Critic algorithm [70, 71], introduced in the context of robotics and videogames [72, 73], generalized to combined discrete and conar X iv :2 20 4. 04 78 5v 2 [ qu an tph ]\n6 N\nov 2\n02 3\n2 Quantum Thermal Machine\nComputer Agent\nHot bath Quantum System\n\ud835\udc3dH(\ud835\udc61)\n\ud835\udc43(\ud835\udc61)\n\ud835\udc3dC(\ud835\udc61)\nCold bath\nExtracted power\n\u2026 System controls \ud835\udc62 \ud835\udc61 Heat bath coupling\ncontrol \ud835\udc51(\ud835\udc61)\nFIG. 1. Schematic representation of a quantum thermal machine controlled by a computer agent. A quantum system (gray circle) can be coupled to a hot (cold) bath at inverse temperature \u03b2H (\u03b2C), represented by the red (blue) square, enabling a heat flux JH(t) (JC(t)). The quantum system is controlled by the computer agent through a set of experimental control parameters u\u20d7(t), such as an energy gap or an oscillator frequency, that control the power exchange P (t), and through a discrete control d(t) = {Hot,Cold,None} that determines which bath is coupled to the quantum system.\ntinuous actions and to optimize multiple objectives. RL has received great attention for its success at mastering tasks beyond human-level such as playing games [74\u201376], and for robotic applications [77]. RL has been recently used for quantum control [78\u201387], outperforming previous state-of-the-art methods [88, 89], for fault-tolerant quantum computation [90, 91], and to minimize entropy production in closed quantum systems [92].\nWe prove the validity of our approach finding the full Pareto-front, i.e. the collection of all Pareto-optimal cycles describing optimal power-efficency tradeoffs, in two paradigmatic systems that have been well studied in literature: a refrigerator based on an experimentally realistic superconducting qubit [6, 49], and a heat engine based on a quantum harmonic oscillator [43]. In both cases we find elaborate cycles that outperform previous proposal mitigating quantum friction [43, 49, 55, 66, 93\u201395], i.e. the detrimental effect of the generation of coherence in the instantaneous eigenbasis during the cycle. Remarkably, we can also match the performance of cycles found with the RL method of Ref. [69] that, as opposed to our model-free approach, requires monitoring the full quantum state and only optimizes the power."
        },
        {
            "heading": "Setting: Black-box Quantum Thermal Machine",
            "text": "We describe a QTM by a quantum system, acting as a \u201cworking medium\u201d, that can exchange heat with a hot (H) or cold (C) thermal bath characterized by inverse temperatures \u03b2H < \u03b2C (Fig. 1). Our method can be readily generalized to multiple baths, but we focus the description on two baths here.\nWe can control the evolution of the quantum system and exchange work with it through a set of timedependent continuous control parameters u\u20d7(t) that enter in the Hamiltonian H[u\u20d7(t)] of the quantum system [96], and through a discrete control d(t) = {Hot,Cold,None} that determines which bath is coupled to the system. JH(t) and JC(t) denote the heat flux flowing out respectively from the hot and cold bath at time t. Our method only relies on the following two assumptions:\n1. the RL agent can measure the heat fluxes JC(t) and JH(t) (or their averages over a time period \u2206t);\n2. JC(t) and JH(t) are functions of the control history (u\u20d7(t\u2212 T ), d(t\u2212 T )), ..., (u\u20d7(t), d(t)), where T is the timescale over which the QTM remembers past controls.\nIn contrast to previous work [69], the RL optimization algorithm does not require any knowledge of the microscopic model of the inner workings of the quantum system, nor of its quantum state; it is only provided with the values of the heat fluxes JC(t) and JH(t). These can be either computed from a theoretical simulation of the QTM [69], or measured directly from an experimental device whenever the energy change in the heat bath can be monitored without influencing the energetics of the quantum system (see e.g. experimental demonstrations [6\u20139]). In this sense, our quantum system is treated as a \u201cblack-box\u201d, and our RL method is \u201cmodel-free\u201d. Any theoretical model or experimental device satisfying these requirements can be optimized by our method, including also classical stochastic thermal machines. The timescale T is finite because of energy dissipation and naturally emerges by making the minimal assumption that the coupling of the quantum system to the thermal baths drives the system towards a thermal state within some timescale T . Such a timescale can be rigorously identified e.g. within the weak system-bath coupling regime, and in the reaction coordinate framework that can describe non-Markovian and strong-coupling effects [97]. In a Markovian setting, T is related to the inverse of the characteristic thermalization rate.\nThe thermal machines we consider are the heat engine and the refrigerator. Up to an internal energy contribution that vanishes after each repetition of the cycle, the instantaneous power of a heat engine equals the extracted heat:\nPheat(t) = JC(t) + JH(t), (1)\nand the cooling power of a refrigerator is:\nPcool(t) = JC(t). (2)\nThe entropy production is given by\n\u03a3(t) = \u2212\u03b2CJC(t)\u2212 \u03b2HJH(t), (3) where we neglect the contribution of the quantum system\u2019s entropy since it vanishes after each cycle.\n3"
        },
        {
            "heading": "Machine Learning Problem",
            "text": "Our goal is to identify cycles, i.e. periodic functions u\u20d7(t) and d(t), that maximize a trade-off between power and efficiency on the long run. Since power and efficiency cannot be simultaneously optimized, we use the concept of Pareto-optimality [98, 99]. Pareto-optimal cycles are those where power or efficiency cannot be further increased without sacrificing the other one. The Paretofront, defined as the collection of power-efficiency values delivered by all Pareto-optimal cycles, represents all possible optimal trade-offs. To find the Pareto-front, we define the reward function rc(t) as:\nrc(t) = c P (t) P0 \u2212 (1\u2212 c)\u03a3(t) \u03a30 , (4)\nwhere P (t) is the power of a heat engine (Eq. 1) or cooling power of a refrigerator (Eq. 2), P0, \u03a30 are reference values to normalize the power and entropy production, and c \u2208 [0, 1] is a weight that determines the trade-off between power and efficiency. As in Ref. [69], we are interested in cycles that maximize the long-term performance of QTMs; we thus maximize the return \u27e8rc\u27e9 (t), where \u27e8\u00b7\u27e9(t) indicates the exponential moving average of future values:\n\u27e8rc\u27e9 (t) = \u03ba \u222b \u221e 0 e\u2212\u03ba\u03c4rc(t+ \u03c4) d\u03c4. (5)\nHere \u03ba is the inverse of the averaging timescale, that will in practice be chosen much longer than the cycle period, such that \u27e8rc\u27e9 (t) is approximately independent of t. For c = 1, we are maximizing the average power \u27e8r1\u27e9 = \u27e8P \u27e9 /P0. For c = 0, we are minimizing the average entropy production \u27e8r0\u27e9 = \u2212\u27e8\u03a3\u27e9 /\u03a30, which corresponds to maximizing the efficiency. For intermediate values of c, the maximization of \u27e8rc\u27e9 describes trade-offs between power and efficiency (see \u201cOptimizing the entropy production\u201d in Materials and Methods for details). Interestingly, if convex, it has been shown that the full Pareto-front can be identified repeating the optimization of \u27e8rc\u27e9 for many values of c [98, 100]."
        },
        {
            "heading": "RESULTS",
            "text": "Deep reinforcement learning for black-box quantum thermal machines\nIn RL, a computer agent must learn to master some task by repeated interactions with some environment. Here we develop an RL approach where the agent maximizes the return (5) and the environment is the QTM with its controls (Fig. 2A). To solve the RL problem computationally, we discretize time as ti = i\u2206t. By timediscretizing the return (5), we obtain a discounted return whose discount factor \u03b3 = exp(\u2212\u03ba\u2206t) determines the averaging timescale and expresses how much we are interested in future or immediate rewards (see \u201cReinforcement\nLearning Implementation\u201d in Materials and Methods for details).\nAt each time step ti, the agent employs a policy function \u03c0(a|s) to choose an action ai = {u\u20d7(ti), d(ti)} based on the state si of the environment. Here, the policy function \u03c0(a|s) represents the probability of choosing action a, given that the environment is in state s, u\u20d7(t) are the continuous controls over the quantum system, and d(ti) \u2208 {Hot,Cold,None} is a discrete control that selects the bath the system is coupled to. All controls are considered to be constant during time step of duration \u2206t. The aim of RL is to learn an optimal policy function \u03c0(a|s) that maximizes the return. In order to represent a black-box quantum system whose inner mechanics are unknown, we define the con-\n4 trol history during a time interval of length T as the observable state:\nsi = (ai\u2212N , ai\u2212N+1, . . . , ai\u22121), (6)\nwhere N = T/\u2206t. Therefore, the state of the quantum system is implicitly defined by the sequence of the agent\u2019s N recent actions.\nTo find an optimal policy we employ the soft actorcritic algorithm, that relies on learning also a value function Q(s, a), generalized to a combination of discrete and continuous actions [70\u201373]. The policy function \u03c0(a|s) plays the role of an \u201cactor\u201d that chooses the actions to perform, while a value function Q(s, a) plays the role of a \u201ccritic\u201d that judges the choices made by the actor, thus providing feedback to improve the actor\u2019s behavior. We further optimize the method for a multi-objective setting by introducing a separate critic for each objective, i.e. one value function for the power, and one for the entropy production. This allow us to vary the weight c during training, thus enhancing convergence (see \u201cReinforcement Learning Implementation\u201d in Materials and Methods for details).\nWe learn the functions \u03c0(a|s) and Q(s, a) using a deep NN architecture inspired by WaveNet, an architecture that was developed for processing audio signals [101] (See Figs. 2B-C). We introduce a \u201cconvolution block\u201d to efficiently process the time-series of actions defining the state si. It consists of a 1D convolution with kernel size and stride of 2, such that it halves the length of the input. It is further equipped with a residual connection to improve trainability [102] (see \u201cReinforcement Learning Implementation\u201d in Materials and Methods for details). The policy \u03c0(ai|si) is described by a NN that takes the state si as input, and outputs parameters \u00b5 and \u03c3 describing the probability distribution from which action ai is sampled (Fig. 2B). The value function Q(si, ai) is computed by feeding (si, ai) into a NN, and outputting Q(si, ai) (Fig. 2C). Both \u03c0(ai|si) and Q(si, ai) process the state by feeding it through multiple convolution blocks (upper orange boxes in Figs. 2B and 2C), each one halving the length of the time-series, such that the number of blocks and of parameters in the NN is logarithmic in N . Then a series of fully-connected layers produce the final output.\nThe policy and value functions are determined by minimizing the loss functions in Eqs. (39) and (49) using the ADAM optimization algorithm [103]. The gradient of the loss functions is computed off-policy, over a batch of past experience recorded in a replay buffer, using backpropagation (see \u201cReinforcement Learning Implementation\u201d in Materials and Methods for details).\nPareto-optimal cycles for a superconducting qubit refrigerator\nWe first consider a refrigerator based on an experimentally realistic system: a superconducting qubit coupled\nto two resonant circuits that behave as heat baths [49] (Fig. 3A). Such a system was experimentally studied in the steady-state in Ref. [6]. The system Hamiltonian is given by [49, 55, 63]:\nH\u0302[u(t)] = \u2212E0 [\u2206\u03c3\u0302x + u(t)\u03c3\u0302z] , (7)\nwhere E0 is a fixed energy scale, \u2206 characterizes the minimum gap of the system, and u(t) is our control parameter. In this setup the coupling to the baths, described by the commonly employed Markovian master equation [104\u2013107], is fixed, and cannot be controlled. However, the qubit is resonantly coupled to the baths at different energies. The u-dependent coupling strength to the cold (hot) bath is described by the function \u03b3 (C) u (\u03b3 (H) u ), respectively (Fig. 3F). As in Ref. [63], the coupling strength is, respectively, maximal at u = 0 (u = 1/2), with a resonance width determined by the \u201cquality factor\u201d QC (QH) (see \u201cPhysical model\u201d in Materials and Methods for details). This allows us to choose which bath is coupled to the qubit by tuning u(t).\nIn Fig. 3 we show an example of our training procedure to optimize the return \u27e8rc\u27e9 at c = 0.6 usingN = 128 steps determining the RL state, and varying c during training from 1 to 0.6 (Fig. 3C). In the early stages of the training, the return \u27e8rc\u27e9i, computed as in Eq. (28) but over past rewards, and the running averages of the cooling power \u27e8Pcool\u27e9i and of the negative entropy production \u2212\u27e8\u03a3\u27e9i all start off negative (Fig. 3B), and the corresponding actions are random (left panel of Fig. 3D). Indeed, initially the RL agent has no experience controlling the QTM, so random actions are performed, resulting in heating the cold bath, rather than cooling it, and in a large entropy production. However, with increasing steps, the chosen actions exhibit some structure (Fig. 3D), and the return \u27e8rc\u27e9i increases (Fig. 3B). While both the power and the negative entropy production initially increase together, around step 100k we see that \u2212\u27e8\u03a3\u27e9i begins to decrease. This is a manifestation of the fact that power and entropy production cannot be simultaneously optimized. Indeed, the agent learns that in order to further increase the return, it must \u201csacrifice\u201d some entropy production to produce a positive and larger cooling power. In fact, the only way to achieve positive values of \u27e8rc\u27e9i is to have a positive cooling power, which inevitably requires producing entropy. Eventually all quantities in Fig. 3B reach a maximum value, and the corresponding final deterministic cycle (i.e. the cycle generated by policy switching off stochasticity, see \u201cReinforcement Learning Implementation\u201d in Materials and Methods for details) is shown in Fig. 3E as thick black dots.\nFor the same system, Ref. [63] proposed a smoothed trapezoidal cycle u(t) oscillating between the resonant peaks at u = 0 and u = 1/2 and optimized the cycle time (Fig. 3E, dashed line). While this choice outperformed a sine and a trapezoidal cycle [49], the cycle found by our RL agent produces a larger return (Fig. 3B). The optimal trapezoidal cycle found for c = 0.6 is shown in Fig. 3E\n5 Superconducting Qubit Refrigerator \ud835\udc62(\ud835\udc61) \ud835\udc43cool \ud835\udc00\n\u22121\n0\n1B\n\u3008rc\u3009i \u3008Pcool\u3009i /P0 \u2212\u3008\u03a3\u3009i /\u03a30\n0k 100k 200k 300k 400k 500k step\n0.6\n1.0\nc\nC\n10.94k 11k step\n0.0\n0.5u\nD\n139.94k 140k step 489.94k 490k step\n0 20 40 60 80 100 t[dt]\n0.0\n0.5\nu\nE\n0.0 0.5\nF\n\u03b3 (C) u(t), \u03b3 (H) u(t)\nRL Trapezoidal\n1\nFIG. 3. Training of the superconducting qubit refrigerator model to optimize \u27e8rc\u27e9 at c = 0.6. (A): Schematic representation of the energy levels of the qubit (horizontal black lines) that are controlled by u(t). The gray arrow represents the input power, while the colored arrows represent the heat fluxes. (B): Return \u27e8rc\u27e9i computed over past rewards (black curve), running average of the cooling power \u27e8Pcool\u27e9i /P0 (green curve), and of the negative entropy production \u2212\u27e8\u03a3\u27e9i /\u03a30 (orange curve), as a function of the training step. The dashed line represents the value of the return found optimizing the period of a smoothed trapezoidal cycle. (C): Value of the weight c as a function of the step. It is varied during training from 1 to the final value 0.6 to improve convergence. (D): Actions chosen by the agent, represented by the value of u, as a function of step, zoomed around the three black circles in panel (B). (E): Final deterministic cycle found by the agent (thick black dots) and smoothed trapezoidal cycle (thin dashed line) whose return is given by the dashed line in panel (B), as a function of time. (F): coupling strength \u03b3 (C) u (blue curve) and \u03b3 (H) u (red curve) as a function of u (on the y-axis). The parameters used for training are N = 128, gH = gC = 1, \u03b2H = 10/3, \u03b2C = 2\u03b2H, QH = QC = 4, E0 = 1, \u2206 = 0.12, \u03c9H = 1.028, \u03c9C = 0.24, U = [0, 0.75], \u2206t = 0.98, \u03b3 = 0.997, P0 = 6.62 \u00b7 10\u22124 and \u03a30 = 0.037.\nas a dashed line (see \u201cComparing with other methods\u201d in Materials and Methods for details).\nFig. 4 compares optimal cycles for different trade-offs between cooling power and coefficient of performance \u03b7cool, the latter defined as the ratio between the average cooling power, and the average input power. This is achieved by repeating the optimization for various values\n0.5 1.0 c\n0.0\n0.5\n1.0\n\u3008r c\u3009\nF\nE\nD\nC\n0.7 0.8 0.9 \u03b7cool\nF\nE\nDC\n0.0\n0.5\nu\n0 50 100 t[dt]\n0.0\n0.5\nu\n0 50 100 t[dt]\n0.0 0.1 0.0\n0.5\n1.0\n\u3008P co\no l\u3009/ P\n0 RL\nTrapezoidal\nRef. [69]\nA B\nC D\nE F\nc = 1 c = 0.8\nc = 0.6 c = 0.4\nRL\nTrap.\nFIG. 4. Results for the optimization of the superconducting qubit refrigerator model. (A): final value of the return \u27e8rc\u27e9, as a function of c, found using the RL method (black and blue points), and optimizing the period of a trapezoidal cycle (red dots). The error bars represent the standard deviation of the return computed over 5 independent training runs. (B): corresponding values of the final average cooling power \u27e8Pcool\u27e9 and of the coefficient of performance \u03b7cool found using the RL method (black and blue dots), optimizing the trapezoidal cycle (red dots), and using the RL method of Ref. [69] (purple cross). Results for each of the 5 repetitions are shown as separate points to visualize the variability across multiple trainings. (C-F): final deterministic cycles identified by the RL method (thick black dots), as a function of time, corresponding to the blue points in panels (A) and (B) (respectively for c = 1, 0.8, 0.6, 0.4 choosing the training run with the largest return). The dashed line represents the trapezoidal cycle that maximizes the return for the same value of c [not shown in panel (F) since no cycle yields a positive return]. The parameters used for training are chosen as in Fig. 3.\nof c. To demonstrate the robustness of our method, the optimization of \u27e8rc\u27e9 was repeated 5 times for each choice of c (variability shown with error bars in Fig.4A, and as separate points in Fig.4B). The RL method substantially outperforms the trapezoidal cycle by producing larger final values of the return \u27e8rc\u27e9 at all values of c (Fig. 4A), and by producing a better Pareto front (Fig. 4B). The RL cycles simultaneously yield higher power by more than a factor of 10, and a larger \u03b7cool, for any choice of the power-efficiency trade-off. The model-free RL cycles can also deliver the same power at a substantially higher COP (roughly 10 times larger) as compared to the cycle found with the RL method of Ref. [69], which only optimizes the power. This is remarkable since, as opposed to the current model-free method, the method in Ref. [69] has access to the full quantum state of the system, and not only to the heat currents (see \u201cComparing with other methods\u201d in Materials and Methods for details). This\n6 also shows that a large efficiency improvement can be achieved by sacrificing very little power.\nAs expected, the period of the RL cycles increases as c decreases and the priority shifts from high power to high \u03b7cool (Figs. 4C-F, black dots). However, the period is much shorter than the corresponding optimized trapezoidal cycle (dashed line), and the optimal control sequence is quite unintuitive, even going beyond the resonant point at u = 1/2. As argued in [49, 55, 63], the generation of coherence in the instantaneous eigenbasis of the quantum system, occurring because [H\u0302(u1), H\u0302(u2)] \u0338= 0 for u1 \u0338= u2, causes power losses that increase with the speed of the cycle. We find that we can interpret the power enhancement achieved by our cycle as a mitigation of such detrimental effect: indeed, we find that trapezoidal cycles operated at the same frequency as the RL cycle generate twice as much coherence as the RL cycles (see \u201cGeneration of coherence\u201d in Materials and Methods for details). In either case, cycles with higher power tend to generate more coherence.\nGiven the stochastic nature of RL, we also compared the cycles obtained across the 5 independent training runs, finding that cycles are typically quite robust, displaying only minor changes (see Fig. 8 of Methods for four cycles found in independent training runs corresponding to Figs. 4C-F).\nPareto-optimal cycles for a quantum harmonic oscillator engine\nWe now consider a heat engine based on a collection of non-interacting particles confined in a harmonic potential [43] (Fig. 5A). The Hamiltonian is given by\nH\u0302[u(t)] = 1\n2m p\u03022 +\n1 2 m(u(t)w0) 2q\u03022, (8)\nwhere m is the mass of the system, w0 is a reference frequency and p\u0302 and q\u0302 are the momentum and position operators. The control parameter u(t) allows us to change the frequency of the oscillator. Here, at every time step we let the agent choose which bath (if any) to couple to the oscillator. The coupling to the baths, characterized by the thermalization rates \u0393\u03b1, is modeled using the Lindblad master equation as in Ref. [43] (see \u201cPhysical model\u201d in Materials and Methods for details). In contrast to the superconducting qubit case, c is held constant during training.\nFig. 5 reports the results on the optimal trade-offs between extracted power and efficiency \u03b7heat, the latter defined as the ratio between the extracted power and the input heat, in the same style of Fig. 4. In this setup, we compare our RL-based results to the well-known Otto cycle. The authors of Ref. [43] study this system by optimizing the switching times of an Otto cycle, i.e. the duration of each of the 4 segments, shown as a dashed lines in Figs. 5D-E, composing the cycle (see \u201cComparing with other methods\u201d in Materials and Methods for\nQuantum Harmonic Oscillator Heat Engine\n\ud835\udc43heat\n\ud835\udc62(\ud835\udc61)\n\ud835\udc00\n1\ndetails).\nThe RL method produces cycles with a larger return and with a better power-efficiency Pareto-front with re-\n7 spect to the Otto cycle (Fig. 5B,C). The cycles found by the RL method significantly outperforms the Otto engine in terms of delivered power. For c = 1, a high-power cycle is found (Fig. 5D and corresponding blue dots in Figs. 5B-C) but at the cost of a lower efficiency than the Otto cycles. However, at c = 0.5, the RL method finds a cycle that matches the maximum efficiency of the Otto cycles, while delivering a \u223c 30% higher power (Fig. 5E and corresponding blue dots in Figs. 5B-C) Remarkably, our model-free RL method also finds cycles with nearly the same power as the RL method of Ref. [69], but at almost twice the efficiency (see \u201cComparing with other methods\u201d in Materials and Methods for details). As in Fig. 4, we see that a very small decrease in power can lead to a large efficiency increase.\nInterestingly, as shown in Figs. 5D-E, the cycles found by the RL agent share many similarities with the Otto cycle: both alternate between the hot and cold bath (orange and blue portions) with a similar period. However, there are some differences: at c = 1, the RL cycle ramps the value of u while in contact with the bath, eliminating the unitary stroke (Fig. 5D). Instead, at c = 0.5, the RL agent employs a unitary stroke that is quite different respect to a linear ramping of u (Fig. 5E, green dots). As in the superconducting qubit case, the enhanced performance of the RL cycle may be interpreted as a mitigation of quantum friction [43, 93].\nAlso in this setup, we verified that the discovered cycles are quite robust across the 5 independent training runs, displaying only minor changes (see Fig. 9 of Methods for two cycles found in independent training runs corresponding to Figs. 5D-E)."
        },
        {
            "heading": "DISCUSSION",
            "text": "We introduced a model-free framework, based on Reinforcement Learning, to discover Pareto-optimal thermodynamic cycles that describe the best possible tradeoff between power and efficiency of out-of-equilibrium quantum thermal machines (heat engines and refrigerators). Our algorithm only requires monitoring the heat fluxes of the QTM, making it a model-free approach. It can therefore be used both for the theoretical optimization of known systems, and potentially for the direct optimization of experimental devices for which no model is known, and in the absence of any measurement performed on the quantum system. Using state-of-the-art machine learning techniques, we demonstrate the validity of our method applying it to two different prototypical setups. Our black-box method discovered elaborate cycles that outperform previously proposed cycles and are on par with a previous RL method that observes the full quantum state [69]. Up to minor details, the cycles found by our method are reproducible across independent training runs. Physically we find that Otto cycles, commonly studied in literature, are not generally optimal, and that optimal cycles balance a fast operation of the cycle, with\nthe mitigation of quantum friction. Our method paves the way for a systematic use of RL in the field of quantum thermodynamics. Future directions include investing larger systems to uncover the impact of quantum many-body effects on the performance of QTMs, optimizing systems in the presence of noise, and optimizing trade-offs that include power fluctuations [99, 108\u2013110]."
        },
        {
            "heading": "METHODS",
            "text": "In this section we provide details on the optimization of the entropy production, on the reinforcement learning implementation, on the physical model used to describe the quantum thermal machines, on the training details, on the convergence of the method, on the comparison with other methods, and on the computation of the generation of coherence during the cycles. We also provide access to the full code that was used to generate the results presented in the manuscript, and the corresponding data."
        },
        {
            "heading": "Optimizing the entropy production",
            "text": "Here we discuss the relation between optimizing the power and the entropy production, or the power and the efficiency. We start by noticing that we can express the efficiency of a heat engine \u03b7heat and the coefficient of performance of a refrigerator \u03b7cool in terms of the averaged power and entropy production, i.e.\n\u03b7\u03bd = \u03b7 (c) \u03bd [1 + \u27e8\u03a3\u27e9 /(\u03b2\u03bd \u27e8P\u03bd\u27e9)]\u22121, (9)\nwhere \u03bd = heat, cool, \u03b7 (c) heat \u2261 1 \u2212 \u03b2H/\u03b2C is the Carnot efficiency, \u03b7 (c) cool \u2261 \u03b2H/(\u03b2C \u2212 \u03b2H) is the Carnot coefficient of performance, and where we defined \u03b2heat \u2261 \u03b2C and \u03b2cool \u2261 \u03b2C \u2212 \u03b2H. We now show that, thanks to this dependence of \u03b7\u03bd on \u27e8P\u03bd\u27e9 and \u27e8\u03a3\u27e9, if a cycle is a Paretooptimal trade-off between high power and high efficiency, then it is also a Pareto-optimal trade-off between high power and low entropy-production up to a change of c. This means that if we find all optimal trade-offs between high power and low entropy-production (as we do with our method if the Pareto-front is convex), we will have necessarily also found all Pareto-optimal trade-offs between high power and high efficiency. Mathematically, we want to prove that the cycles that maximize\n\u27e8G\u03bd(c)\u27e9 \u2261 c \u27e8P\u03bd\u27e9+ (1\u2212 c)\u03b7\u03bd (10)\nfor some value of c \u2208 [0, 1], also maximize the return in Eq. (5) for some (possibly different) value of c \u2208 [0, 1]. To simplify the proof and the notation, we consider the following two functions\nF (a, b, \u03b8) = aP (\u03b8)\u2212 b\u03a3(P (\u03b8), \u03b7(\u03b8)), G(a, b, \u03b8) = aP (\u03b8) + b\u03b7(\u03b8),\n(11)\n8 where P (\u03b8) and \u03b7(\u03b8) represent the power and efficiency of a cycle parameterized by a set of parameters \u03b8, a > 0 and b > 0 are two scalar quantities, and\n\u03a3(P, \u03b7) = \u03b7 (c) \u03bd \u2212 \u03b7 \u03b7 \u03b2\u03bdP (12)\nis obtained by inverting Eq. (9). We wish to prove the following. Given some weights a1 > 0 and b1 > 0, let \u03b81 be the value of \u03b8 that locally maximizes G(a1, b1, \u03b8). Then, it is always possible to identify positive weights a2 > 0, b2 > 0 such that the same parameters \u03b81 (i.e. the same cycle) is a local maximum for F (a2, b2, \u03b8). In the following, we will use that\n\u2202P\u03a3 \u2265 0 \u2202\u03b7\u03a3 < 0, (13)\nand that the Hessian H(\u03a3) of \u03a3(P, \u03b7) is given by\nH(\u03a3) =  0 \u2212\u03b2\u03bd \u03b7(c)\u03bd\u03b72 \u2212\u03b2\u03bd \u03b7 (c) \u03bd \u03b72 2\u03b2\u03bdP \u03b7(c)\u03bd \u03b73  . (14) Proof: by assumption, \u03b81 is a local maximum for G(a1, b1, \u03b8). Denoting with \u2202i the partial derivative in (\u03b8)i, we thus have\n0 = \u2202iG(a1, b1, \u03b81) = a1\u2202iP (\u03b81) + b1\u2202i\u03b7(\u03b81). (15)\nNow, let us compute the derivative in \u03b8 of F (a2, b2, \u03b81), where a2 > 0 and b2 > 0 are two arbitrary positive coefficients. We have\n\u2202iF (a2, b2, \u03b81) = (a2 \u2212 b2\u2202P\u03a3)\u2202iP (\u03b81)\u2212 (b2\u2202\u03b7\u03a3)\u2202i\u03b7(\u03b81). (16) Therefore, if we choose a2 and b2 such that( a1 b1 ) = ( 1 \u2212\u2202P\u03a3 0 \u2212\u2202\u03b7\u03a3 )( a2 b2 ) , (17)\nthanks to Eq. (15) we have that\n0 = \u2202iF (a2, b2, \u03b81), (18)\nmeaning that the same parameters \u03b81 that nullifies the gradient ofG, nullifies also the gradient of F at a different choice of the weights, given by Eq. (17). The invertibility of Eq. (17) (i.e. a non-null determinant of the matrix) is guaranteed by Eq. (13). We also have to make sure that if a1 > 0 and b1 > 0, then also a2 > 0 and b2 > 0. To do this, we invert Eq. (17), finding(\na2 b2\n) = ( 1 \u2212\u2202P\u03a3/(\u2202\u03b7\u03a3) 0 \u22121/(\u2202\u03b7\u03a3) )( a1 b1 ) . (19)\nIt is now easy to see that also the weights a2 and b2 are positive using Eq. (13).\nTo conclude the proof, we show that \u03b81 is a local maximum for F (a2, b2, \u03b8) by showing that its Hessian is negative semi-definite. Since, by hypothesis, \u03b81 is a local\nmaximum for G(a1, b1, \u03b8), we have that the Hessian matrix\nH (G) ij \u2261 \u2202ijG(a1, b1, \u03b81) = a1\u2202ijP + b1\u2202ij\u03b7 (20)\nis negative semi-definite. We now compute the Hessian H(F ) of F (a2, b2, \u03b8) in \u03b8 = \u03b81:\nH (F ) ij = a2\u2202ijP \u2212 b2 [\u2202P\u03a3 \u2202ijP + \u2202\u03b7\u03a3 \u2202ij\u03b7 +Qij ] , (21)\nwhere\nQij = ( \u2202iP \u2202i\u03b7 ) H(\u03a3) ( \u2202jP \u2202j\u03b7 ) , (22)\nand H(\u03a3) is the Hessian of \u03a3(P, \u03b7) computed in P (\u03b81) and \u03b7(\u03b81). Since we are interested in studying the Hessian of F (a2, b2, \u03b81) in the special point (a2, b2) previously identified, we substitute Eq. (19) into Eq. (21), yielding\nH (F ) ij = H (G) ij + b1 \u2202\u03b7\u03a3 Qij . (23)\nWe now prove that H (F ) ij is negative semi-definite since it is the sum of negative semi-definite matrices. By hypothesis H (G) ij is negative semi-definite. Recalling Eq. (13) and that b1 > 0, we now need to show that Qij is positive semi-definite. Plugging Eq. (14) into Eq. (22) yields\nQij = \u03b2[\u03bd] \u03b7 (c) [\u03bd]\n\u03b72 \u2202i\u03b7 \u2202j\u03b7 Rij , (24)\nwhere\nRij \u2261 2 P\n\u03b7 + Sij + S\nT ij , Sij = \u2212\n\u2202iP \u2202i\u03b7 . (25)\nWe now show that if Rij is positive semi-definite, then also Qij is positive semi-definite. By definition, Qij is positive semidefinite if, for any set of coefficient ai, we have that \u2211 ij aiQijaj \u2265 0. Assuming Rij to be positive semi-definite, and using that \u03b2[\u03bd], \u03b7 (c) [\u03bd] , \u03b7 > 0, we have\n\u2211 ij aiQijaj = \u03b2[\u03bd] \u03b7 (c) [\u03bd] \u03b72 \u2211 ij xiRijxj \u2265 0, (26)\nwhere we define xi \u2261 \u2202i\u03b7 ai. We thus have to prove the positivity of Rij . We prove this showing that it is the sum of 3 positive semi-definite matrices. Indeed, the first term in Eq. (25), 2P/\u03b7, is proportional to a matrix with 1 in all entries. Trivially, this matrix has 1 positive eigenvalue, and all other ones are null, so it is positive semi-definite. At last, Sij and its transpose have the same positivity, so we focus only on Sij . Sij is a matrix with all equal columns. This means that it has all null eigenvalues, except for a single one that we denote with \u03bb. Since the trace of a matrix is equal to the sum of the eigenvalues, we have \u03bb = Tr[S] = \u2211 i Sii. Using the\n9 optimality condition in Eq. (15), we see that each entry of S is positive, i.e. Sij > 0. Therefore \u03bb > 0, thus S is positive semi-definite, concluding the proof that H (F ) ij is negative semi-definite.\nTo conclude, we notice that we can always renormalize a2 and b2, preserving the same exact optimization problem. This way, a value of c \u2208 [0, 1] can be identified."
        },
        {
            "heading": "Reinforcement Learning Implementation",
            "text": "As discussed in the main text, our goal is to maximize the return \u27e8rc\u27e9 (t) defined in Eq. (5). To solve the problem within the RL framework, we discretize time as ti = i\u2206t. At every time-step ti, the aim of the agent is to learn an optimal policy that maximizes, in expectation, the time-discretized return \u27e8rc\u27e9i. The time-discrete reward and return functions are given by:\nri+1 = \u2206t \u22121 \u222b ti+\u2206t ti rc(t)dt, (27)\n\u27e8rc\u27e9i = (1\u2212 \u03b3) \u221e\u2211 j=0 \u03b3jri+1+j . (28)\nEq. (28) is the time-discrete version of Eq. (5), where the discount factor \u03b3 = exp(\u2212\u03ba\u2206t) determines the averaging timescale and expresses how much we are interested in future or immediate rewards.\nTo be precise, plugging Eq. (27) into Eq. (28) gives \u27e8rc\u27e9(t) (up to an irrelevant constant prefactor) only in the limit of \u2206t \u2192 0. However, also for finite \u2206t, both quantities are time-averages of the reward, so they are equally valid definitions to describe a long-term trade-off maximization.\nAs in Ref. [69], we use a generalization of the soft-actor critic (SAC) method, first developed for continuous actions [70, 71], to handle a combination of discrete and continuous actions [72, 73]. We further tune the method to stabilize the convergence in a multi-objective scenario. We here present an overview of our implementation of SAC putting special emphasis on the differences with respect to the standard implementation. However, we refer to [70\u201373] for additional details. Our method, implemented with PyTorch, is based on modifications and generalizations of the SAC implementation provided by Spinning Up from OpenAI [111]. All code and data to reproduce the experiments is available online (see Data Availability and Code Availability sections).\nThe SAC algorithm is based on policy iteration, i.e. it consists of iterating multiple times over two steps: a policy evaluation step, and a policy improvement step. In the policy evaluation step, the value function of the current policy is (partially) learned, whereas in the policy improvement step a better policy is learned by making use of the value function. We now describe these steps more in detail.\nIn typical RL problems, the optimal policy \u03c0\u2217(s|a) is defined as the policy that maximizes the expected return\ndefined in Eq. (28), i.e.:\n\u03c0\u2217 = argmax \u03c0 E\u03c0 s\u223c\u00b5\u03c0 [ \u221e\u2211 k=0 \u03b3k rk+1 \u2223\u2223\u2223s0 = s], (29) where E\u03c0 denotes the expectation value choosing actions according to the policy \u03c0. The initial state s0 = s is sampled from \u00b5\u03c0, i.e. the steady-state distribution of states that are visited by \u03c0. In the SAC method, balance between exploration and exploitation [112] is achieved by introducing an Entropy-Regularized maximization objective. In this setting, the optimal policy \u03c0\u2217 is given by\n\u03c0\u2217 = argmax \u03c0 E\u03c0 s\u223cB [ \u221e\u2211 k=0 \u03b3k ( rk+1 + \u03b1H[\u03c0(\u00b7|sk)] )\u2223\u2223\u2223s0 = s], (30) where \u03b1 \u2265 0 is known as the \u201ctemperature\u201d parameter that balances the trade-off between exploration and exploitation, and\nH[P ] = E x\u223cP\n[\u2212 logP (x)] (31)\nis the entropy of the probability distribution P . Notice that we replaced the unknown state distribution \u00b5\u03c0 with B, which is a replay buffer populated during training by storing the observed one-step transitions (sk, ak, rk+1, sk+1). Developing on Ref. [69], we generalize such approach to a combination of discrete and continuous actions in the following way. Let us write an arbitrary action a as a = (u, d), where u is the continuous action and d is the discrete action (for simplicity, we describe the case of a single continuous action, though the generalization to multiple variables is straightforward). From now on, all functions of a are also to be considered as functions of u, d. We decompose the joint probability distribution of the policy as\n\u03c0(u, d|s) = \u03c0D(d|s) \u00b7 \u03c0C(u|d, s), (32) where \u03c0D(d|s) is the marginal probability of taking discrete action d, and \u03c0C(u|d, s) is the conditional probability density of choosing action u, given action d (D stands for \u201cdiscrete\u201d, and C for \u201ccontinuous\u201d). Notice that this decomposition is an exact identity, thus allowing us to describe correlations between the discrete and the continuous action. With this decomposition, we can write the entropy of a policy as\nH[\u03c0(\u00b7|s)] = H\u03c0D(s) +H\u03c0C(s), (33) where\nH\u03c0D(s) = H[\u03c0D(\u00b7|s)], H\u03c0C(s) = \u2211 d \u03c0D(d|s)H[\u03c0C(\u00b7|d, s)],\n(34)\ncorrespond respectively to the entropy contribution of the discrete (D) and continuous (C) part. These two entropies take on values in different ranges: while the entropy of a discrete distribution with |D| discrete actions\n10\nis non-negative and upper bounded by log(|D|), the (differential) entropy of a continuous distribution can take on any value, including negative values (especially for peaked distributions). Therefore, we introduce a separate temperature for the discrete and continuous contributions replacing the definition of the optimal policy in Eq. (30) with\n\u03c0\u2217 = argmax \u03c0 E\u03c0 s\u223cB [ \u221e\u2211 k=0 \u03b3k ( rk+1 + \u03b1DH \u03c0 D(sk)\n+ \u03b1CH \u03c0 C(sk) )\u2223\u2223\u2223s0 = s], (35) where \u03b1C \u2265 0 and \u03b1D \u2265 0 are two distinct \u201ctemperature\u201d parameters. This is one of the differences with respect to Refs. [69\u201371]. Equation (35) defines our optimization objective. Accordingly, we define the value function Q\u03c0(s, a) of a given policy \u03c0 as\nQ\u03c0(s, a) = E\u03c0 [ r1 + \u221e\u2211 k=1 \u03b3k ( rk+1 + \u03b1DH \u03c0 D(sk)\n+ \u03b1CH \u03c0 C(sk) )\u2223\u2223\u2223s0 = s, a0 = a]. (36) Its recursive Bellman equation therefore reads\nQ\u03c0(s, a) = E s1\na1\u223c\u03c0(\u00b7|s1)\n[ r1 + \u03b3 ( Q\u03c0(s1, a1) + \u03b1DH \u03c0 D(s1)\n+ \u03b1CH \u03c0 C(s1) )\u2223\u2223\u2223s0 = s, a0 = a]. (37) As in Ref. [70, 71], we parameterize \u03c0C(u|d, s) as a squashed Gaussian policy, i.e. as the distribution of the variable u\u0303(\u03be|d, s) = ua + ub \u2212 ua\n2 [1 + tanh (\u00b5(d, s) + \u03c3(d, s) \u00b7 \u03be))],\n\u03be \u223c N (0, 1), (38)\nwhere \u00b5(d, s) and \u03c3(d, s) represent respectively the mean and standard deviation of the Gaussian distribution, N (0, 1) is the normal distribution with zero mean and unit variance, and where we assume that U = [ua, ub]. This is the so-called reparameterization trick.\nWe now describe the policy evaluation step. In the SAC algorithm, we learn two value functions Q\u03d5i(s, a) described by the learnable parameters \u03d5i, for i = 1, 2. Q\u03d5(s, a) is a function approximator, e.g. a neural network. Since Q\u03d5i(s, a) should satisfy the Bellman Eq. (37), we define the loss function for Q\u03d5i(s, a) as the mean square difference between the left and right hand side of Eq. (37), i.e.\nLQ(\u03d5i) = E (s,a,r,s\u2032)\u223cB\n[ (Q\u03d5i(s, a)\u2212 y(r, s\u2032))2 ] , (39)\nwhere\ny(r, s\u2032) = r+\u03b3 E a\u2032\u223c\u03c0(\u00b7|s\u2032) [ min j=1,2 Q\u03d5targ,j (s \u2032, a\u2032)+\u03b1DHD(s \u2032)\n+ \u03b1CHC(s \u2032) ] . (40)\nNotice that in Eq. (40) we replaced Q\u03c0 with minj=1,2 Q\u03d5targ,j , where \u03d5targ,j , for j = 1, 2, are target parameters which are not updated when minimizing the loss function; instead, they are held fixed during backpropagation, and then they are updated according to Polyak averaging, i.e.\n\u03d5targ,i \u2190 \u03c1polyak\u03d5targ,i + (1\u2212 \u03c1polyak)\u03d5i, (41)\nwhere \u03c1polyak is a hyperparameter. This change was shown to improve learning [70, 71]. In order to evaluate the expectation value in Eq. (40), we use the decomposition in Eq. (32) to write\nE a\u2032\u223c\u03c0(\u00b7|s\u2032) [\u00b7] = \u2211 d\u2032 \u03c0D(d \u2032|s\u2032) E u\u2032\u223c\u03c0C(\u00b7|d\u2032,s\u2032) [\u00b7], (42)\nwhere we denote a\u2032 = (u\u2032, d\u2032). Plugging Eq. (42) into Eq. (40) and writing the entropies explicitly as expectation values yields\ny(r, s\u2032) = r + \u03b3 \u2211 d\u2032 \u03c0D(d \u2032|s\u2032) \u00b7 ( E\nu\u2032\u223c\u03c0C(\u00b7|d\u2032,s\u2032)\n[ min j=1,2 Q\u03d5targ,j (s \u2032, d\u2032, u\u2032)\u2212 \u03b1C log \u03c0C(u\u2032|d\u2032, s\u2032) ] \u2212 \u03b1D log \u03c0D(d\u2032|s\u2032) ) . (43)\nWe then replace the expectation value over u\u2032 in Eq. (43) with a single sampling u\u2032 \u223c \u03c0C(\u00b7|d\u2032, s\u2032) (therefore one sampling for each discrete action) performed using Eq. (38). This corresponds to performing a full average over the discrete action, and a single sampling of the continuous action. We now turn to the policy improvement step. Since we introduced two separate temperatures, we cannot use the loss function introduced in Refs. [70, 71]. Therefore, we proceed in two steps. Let us define the following function\nZ\u03c0(s) = \u2212 E a\u223c\u03c0(\u00b7|s)\n[ Q\u03c0 old (s, a) ] \u2212 \u03b1DH\u03c0D(s)\u2212 \u03b1CH\u03c0C(s),\n(44)\nwhere Q\u03c0 old\n(s, a) is the value function of some given \u201cold policy\u201d \u03c0old, and \u03c0 is an arbitrary policy. First, we prove that if a policy \u03c0new satisfies\nZ\u03c0new(s) \u2264 Z\u03c0old(s) (45)\nfor all values of s, then \u03c0new is a better policy than \u03c0old as defined in Eq. (35). Next, we will use this property to define a loss function that implements the policy improvement step. Equation (45) implies that\nE a\u223c\u03c0old(\u00b7|s)\n[ Q\u03c0 old (s, a) ] + \u03b1DH \u03c0old D (s) + \u03b1CH \u03c0old C (s) \u2264\nE a\u223c\u03c0new(\u00b7|s)\n[ Q\u03c0 old (s, a) ] + \u03b1DH \u03c0new D (s) + \u03b1CH \u03c0new C (s).\n(46)\n11\nQ\u03c0 old\n(s, a) = E s1\na1\u223c\u03c0old(\u00b7|s1)\n[ r1 + \u03b3 ( Q\u03c0 old (s1, a1) + \u03b1DH \u03c0old D (s1) + \u03b1CH \u03c0old C (s1) )\u2223\u2223\u2223s0 = s, a0 = a] \u2264\nE s1\na1\u223c\u03c0new(\u00b7|s1)\n[ r1 + \u03b3 ( Q\u03c0 old (s1, a1) + \u03b1DH \u03c0new D (s1) + \u03b1CH \u03c0new C (s1) )\u2223\u2223\u2223s0 = s, a0 = a] =\nE s1\na1\u223c\u03c0new(\u00b7|s1)\n[ r1 + \u03b3 ( \u03b1DH \u03c0new D (s1) + \u03b1CH \u03c0new C (s1) )\u2223\u2223\u2223s0 = s, a0 = a]+ \u03b3 E\ns1 a1\u223c\u03c0new(\u00b7|s1)\n[ Q\u03c0 old (s1, a1) \u2223\u2223\u2223s0 = s, a0 = a]\n\u2264 \u00b7 \u00b7 \u00b7 \u2264 Q\u03c0 new (s, a). (47)\nWe now use this inequality to show that \u03c0new is a better policy. Starting from the Bellmann equation (37) for Q\u03c0 old\n, we have Eq. (47). Using a strategy similar to that described in Refs. [70, 112], in Eq. (47) we make a repeated use of inequality (46) and of the Bellmann equation for Q\u03c0 old\n(s, a) to prove that the value function of \u03c0new is better or equal to the value function of \u03c0old. Let \u03c0\u03b8(a|s) be a parameterization of the policy function that depends on a set of learnable parameters \u03b8. We define the following loss function\nL\u03c0(\u03b8) = E s\u223cB\na\u223c\u03c0\u03b8(\u00b7|s)\n[ \u2212Q\u03c0old(s, a)\u2212 \u03b1DH\u03c0\u03b8D (s)\u2212 \u03b1CH\u03c0\u03b8C (s) ] .\n(48) Thanks to Eqs. (44) and (45), this choice guarantees us to find a better policy by minimizing L\u03c0(\u03b8) with respect to \u03b8. In order to evaluate the expectation value in Eq. (48), as before we explicitly average over the discrete action and perform a single sample of the continuous action, and we replace Q\u03c0 old\nwith minj Q\u03d5j . Recalling the parameterization in Eq. (38), this yields\nL\u03c0(\u03b8) = E s\u223cB [\u2211 d \u03c0D,\u03b8(d|s) ( \u03b1D log \u03c0D,\u03b8(d|s)+\n\u03b1C log \u03c0C,\u03b8(u\u0303\u03b8(\u03be|d, s)|d, s)\u2212 min j=1,2\nQ\u03d5j (s, u\u0303\u03b8(\u03be|d, s), d) )] ,\n\u03be \u223c N (0, 1). (49) We have defined and shown how to evaluate the loss functions LQ(\u03d5) and L\u03c0(\u03b8) that allow us to determine the value function and the policy [see Eqs. (39), (43) and (49)]. Now, we discuss how to automatically tune the temperature hyperparameters \u03b1D and \u03b1C. Ref. [71] shows that constraining the average entropy of the policy to a certain value leads to the same exact SAC algorithm with the addition of an update rule to determine the temperatures. Let H\u0304D and H\u0304C be respectively the fixed average values of the entropy of the discrete and continuous part of the policy. We can then determine the corresponding temperatures \u03b1D and \u03b1C minimizing the following two loss functions\nLD(\u03b1D) = \u03b1D E s\u223cB\n[ H\u03c0D(s)\u2212 H\u0304D ] ,\nLC(\u03b1C) = \u03b1C E s\u223cB\n[ H\u03c0C(s)\u2212 H\u0304C ] .\n(50)\nAs usual, we evaluate the entropies by explicitly taking the average over the discrete actions, and taking a single sample of the continuous action. To be more specific, we evaluate LD by computing\nLD(\u03b1D) = \u03b1D E s\u223cB [ \u2212 \u2211 d \u03c0D(d|s) log \u03c0D(d|s)\u2212 H\u0304D ] ,\n(51) and LC by computing\nLC(\u03b1C) = \u03b1C\n\u00b7 E s\u223cB [ \u2212 \u2211 d \u03c0D(d|s) E u\u223c\u03c0C(\u00b7|d,s) [log \u03c0C(u|d, s)]\u2212 H\u0304C ] (52)\nand replacing the expectation value over u with a single sample.\nTo summarize, the SAC algorithm consists of repeating over and over a policy evaluation step, a policy improvement step, and a step where the temperatures are updated. The policy evaluation step consists of a single optimization step to minimize the loss functions LQ(\u03d5i) (for i = 1, 2), given in Eq. (39), where y(r, s\u2032) is computed using Eq. (43). The policy improvement step consists of a single optimization step to minimize the loss function L\u03c0(\u03b8) given in Eq. (49). The temperatures are then updated performing a single optimization step to minimize LD(\u03b1D) and LC(\u03b1C) given respectively in Eqs. (51) and (52). In all loss functions, the expectation value over the states is approximated with a batch of experience sampled randomly from the replay buffer B.\nWe now detail how we parameterize \u03c0(a|s) and Q(s, a). The idea is to develop an efficient way to process the state that can potentially be a long time-series of actions. To this aim, we introduce a \u201cconvolution block\u201d as a building element for our NN architecture. The convolution block, detailed in Fig. 6, takes an input of size (Cin, Lin), where Cin is the number of channels (i.e. the number of parameters determining an action at every time-step) and Lin is the length of the time-series, and produces an output of size (Cout, Lout = Lin/2), thus halving the length of the time-series. Notice that we include a skip connection (right branch in Fig. 6) to improve trainability [102].\nUsing the decomposition in Eq. (32) and the parameterization in Eq. (38), the quantities that need to\n12\nConvolution Block\nbe parameterized are the discrete probabilities \u03c0D(d|s), the averages \u00b5(d, s) and the variances \u03c3(d, s), for d = 1, . . . , |D|, |D| = 3 being the number of discrete actions. The architecture of the neural network that we use for the policy function is shown in Fig. 7A. The state, composed of the time-series si = (ai\u2212N , . . . , ai\u22121) which has shape (Cin, Lin = N), is fed through a series of ln2(N) convolutional blocks, which produce an output of length (Cout, L = 1). The number of input channels Cin is determined by stacking the components of u\u20d7 (which, for simplicity, is a single real number u in this appendix) and by using a one-hot encoding of the discrete actions. We then feed this output, together with the last action which has a privileged position, to a series of fully connected NNs\nwith ReLU activations. Finally, a linear network outputs W (d|s), \u00b5(d, s) and log(\u03c3(d, s)), for all d = 1, . . . , |D|. The probabilities \u03c0D(d|s) are then produced applying the softmax operation to W (d|s). We parameterize the value function Q\u03d5(s, u, d) as in Fig. 7B. As for the policy function, the state s is fed through ln2(N) stacked convolution blocks which reduce the length of the input to (Cout, L = 1). This output, together with the action u, is fed into a series of fullyconnected layers with ReLU activations. We then add a linear layer that produces |D| outputs, corresponding to the value of Q(s, u, d) for each d = 1, . . . , |D|. At last, we discuss a further change to the current method that we implemented in the superconducting qubit refrigerator case to improve the converge. This idea is the following. The return \u27e8rc\u27e9 is a convex combination of the power and of the negative entropy production. The first term is positive when the system is delivering the desired power, while the second term is strictly negative. Therefore, for c close to 1, the optimal value of the return is some positive quantity. Instead, as c decreases, the optimal value of the return decreases, getting closer to zero (this can be seen explicitly in Figs. 4A and 5B). However, a null return can also be achieved by a trivial cycle that consists of doing nothing, i.e. of keeping the control constant in time. Indeed, this yields both zero power, and zero entropy production. Therefore, as c decreases, it becomes harder and harder for the RL agent to distinguish good cycles from these trivial solutions. We thus modify our method to allow us to smoothly change the value of c during training from 1 to the desired final value, which allows to tackle an optimization problem by \u201cstarting from an easier problem\u201d (c = 1), and gradually increasing its difficulty. This required the following modifications to the previously described method. We introduce two separate value functions, one for each objective (P for the power, and \u03a3 for the entropy production)\nQ\u03c0P(s, a) = E\u03c0 [ r (P) 1 + \u221e\u2211 k=1 \u03b3k ( r (P) k+1 + \u03b1DH \u03c0 D(sk)\n+ \u03b1CH \u03c0 C(sk) )\u2223\u2223\u2223s0 = s, a0 = a], Q\u03c0\u03a3(s, a) = E\u03c0 [ r (\u03a3) 1 +\n\u221e\u2211 k=1 \u03b3k ( r (\u03a3) k+1 + \u03b1DH \u03c0 D(sk)\n+ \u03b1CH \u03c0 C(sk) )\u2223\u2223\u2223s0 = s, a0 = a], (53)\nwhere\nr (P) i+1 \u2261\n1\n\u2206t ti+\u2206t\u222b ti P (\u03c4) P0 d\u03c4, r (\u03a3) i+1 \u2261 1 \u2206t ti+\u2206t\u222b ti \u03a3(\u03c4) \u03a30 d\u03c4,\n(54) represent respectively the normalized average power and average entropy production during each time-step. Since the value functions in Eq. (53) are identical to Eq. (36) up to a change of the reward, they separately satisfy the\n13\nsame Bellmann equation as in Eq. (37), with r1 replaced respectively with r (P) 1 and r (\u03a3) 1 . Therefore, we learn each value functions minimizing the same loss function LQ given in Eq. (39), with ri replaced with r (P) 1 or r (\u03a3) 1 . Both value functions are parameterized using the same architecture, but separate and independent parameters. We now turn to the determination of the policy. Comparing the definition of ri given in the main text with Eq. (54), we see that ri+1 = cr (P) i+1 \u2212 (1\u2212 c)r (\u03a3) i+1. Using this property, and comparing Eq. (36) with Eq. (53), we see that\nQ\u03c0(s, a) = cQ\u03c0P(s, a)\u2212 (1\u2212 c)Q\u03c0\u03a3(s, a). (55)\nTherefore, we learn the policy minimizing the same loss function as in Eq. (49), using Eq. (55) to compute the value function. To summarize, this method allows us to vary c dynamically during training. This requires learning two value functions, one for each objective, and storing in the replay buffer the two separate rewards r (P) i and r (\u03a3) i . At last, when we refer to \u201cfinal deterministic cycle\u201d, we are sampling from the policy function \u201cswitching off the stochasticity\u201d, i.e. choosing continuous actions u setting \u03be = 0 in Eq. (38), and choosing deterministically the discrete action with the highest probability."
        },
        {
            "heading": "Physical model",
            "text": "As discussed in the main text, we describe the dynamics of the two analyzed QTMs employing the Lindblad master equation that can be derived also for nonadiabatic drivings [107], in the weak system-bath coupling regime performing the usual Born-Markov and secular approximation [104\u2013106] and neglecting the Lambshift contribution. This approach describes the timeevolution of the reduced density matrix of the quantum system, \u03c1\u0302(t), under the assumption of weak system-bath interaction. Setting \u210f = 1, the master equation reads\n\u2202 \u2202t \u03c1\u0302(t) = \u2212i\n[ H\u0302[u\u20d7(t)], \u03c1\u0302(t) ] + \u2211\n\u03b1 D(\u03b1)u\u20d7(t),d(t)[\u03c1\u0302(t)], (56)\nwhere H\u0302[u\u20d7(t)] is the Hamiltonian of the quantum system that depends explicitly on time via the control parameters u\u20d7(t), [\u00b7, \u00b7] denotes the commutator, and D(\u03b1)u\u20d7(t),d(t)[\u00b7], known as the dissipator, describes the effect of the coupling between the quantum system and bath \u03b1 = H,C. We notice that since the RL agent produces piece-wise constant protocols, we are not impacted by possible inaccuracies of the master equation subject to fast parameter driving [113], provided that \u2206t is not smaller than the bath timescale. Without loss of generality, the dissipators can be expressed as [105, 106]\nD(\u03b1)u\u20d7(t),d(t) = \u03bb\u03b1[d(t)] \u2211 k \u03b3 (\u03b1) k,u\u20d7(t) ( A\u0302 (\u03b1) k,u\u20d7(t)\u03c1\u0302A\u0302 (\u03b1)\u2020 k,u\u20d7(t)\n\u2212 1 2 A\u0302 (\u03b1)\u2020 k,u\u20d7(t)A\u0302 (\u03b1) k,u\u20d7(t)\u03c1\u0302\u2212 1 2 \u03c1\u0302A\u0302 (\u03b1)\u2020 k,u\u20d7(t)A\u0302 (\u03b1) k,u\u20d7(t)\n) , (57)\nwhere \u03bb\u03b1[d(t)] \u2208 {0, 1} are functions that determine which bath is coupled the quantum system, A\u0302\n(\u03b1) k,u\u20d7(t) are\nthe Lindblad operators, and \u03b3 (\u03b1) k,u\u20d7(t) are the corresponding rates. In particular, \u03bbH(Hot) = 1, \u03bbC(Hot) = 0, while \u03bbH(Cold) = 0, \u03bbC(Cold) = 1, and \u03bbH(None) = \u03bbC(None) = 0. Notice that both the Lindblad operators and the rates can depend on time through the value of the control u\u20d7(t). Their explicit form depends on the details of the system, i.e. on the Hamiltonian describing the dynamics of the overall system including the bath and the system-bath interaction. Below, we provide the explicit form of A\u0302 (\u03b1) k,u\u20d7(t) and \u03b3 (\u03b1) k,u\u20d7(t) used to model the two setups considered in the manuscript. We adopt the standard approach to compute the instantaneous power and heat currents [114]\nP (t) \u2261 \u2212Tr [ \u03c1\u0302(t) \u2202\n\u2202t H\u0302[u\u20d7(t)]\n] ,\nJ\u03b1(t) \u2261 Tr [ H\u0302[u\u20d7(t)]D(\u03b1)u\u20d7(t),d(t) ] ,\n(58)\nthat guarantees the validity of the first law of thermodynamics \u2202U(t)/(\u2202t) = \u2212P (t) + \u2211\u03b1 J\u03b1(t), the internal energy being defined as U = Tr[\u03c1\u0302(t)H\u0302[u\u20d7(t)]]. In the superconducting qubit refrigerator, we employ the model first put forward in Ref. [49], and further studied in Refs. [55, 63]. In particular, we consider the following Lindblad operators and corresponding rates (identifying k = \u00b1):\nA\u0302 (\u03b1) +,u(t) = \u2212i |eu(t)\u27e9 \u27e8gu(t)| , A\u0302 (\u03b1) \u2212,u(t) = +i |gu(t)\u27e9 \u27e8eu(t)| , (59) where |gu(t)\u27e9 and |eu(t)\u27e9 are, respectively, the instantaneous ground state and excited state of Eq. (7). The corresponding rates are given by \u03b3 (\u03b1) \u00b1,u(t) = S\u03b1[\u00b1\u2206\u03f5u(t)], where \u2206\u03f5u(t) is the instantaneous energy gap of the system, and\nS\u03b1(\u2206\u03f5) = g\u03b1 2\n1 1 +Q2\u03b1(\u2206\u03f5/\u03c9\u03b1 \u2212 \u03c9\u03b1/\u2206\u03f5)2 \u2206\u03f5\ne\u03b2\u03b1\u2206\u03f5 \u2212 1 (60)\nis the noise power spectrum of bath \u03b1. Here \u03c9\u03b1, Q\u03b1 and g\u03b1 are the base resonance frequency, quality factor and coupling strength of the resonant circuit acting as bath \u03b1 = H,C (see Refs. [49, 63] for details). As in Ref. [63],\nwe choose \u03c9C = 2E0\u2206 and \u03c9H = 2E0 \u221a\n\u22062 + 1/4, such that the C (H) bath is in resonance with the qubit when u = 0 (u = 1/2). The width of the resonance is governed by Q\u03b1. The total coupling strength to bath \u03b1, plotted in Fig. 3F, is quantified by\n\u03b3 (\u03b1) u(t) \u2261 \u03b3 (\u03b1) +,u(t) + \u03b3 (\u03b1) \u2212,u(t). (61)\nIn the quantum harmonic oscillator based heat engine, following Ref. [43], we describe the coupling to the baths through the Lindblad operators A\u0302 (\u03b1) +,u(t) = a\u0302\u2020u(t), A\u0302 (\u03b1) \u2212,u(t) = a\u0302u(t) and corresponding rates \u03b3 (\u03b1) +,u(t) =\n14\n\u0393\u03b1 n(\u03b2\u03b1u(t)\u03c90) and \u03b3 (\u03b1) \u2212,u(t) = \u0393\u03b1[1+n(\u03b2\u03b1u(t)\u03c90)], where we identify k = \u00b1. a\u0302u(t) = (1/ \u221a 2) \u221a m\u03c90u(t) q\u0302 +\ni/ \u221a m\u03c90u(t) p\u0302 and a\u0302 \u2020 u(t) are respectively the (control dependent) lowering and raising operators, \u0393\u03b1 is a constant rate setting the thermalization timescale of the system coupled to bath \u03b1, and n(x) = [exp(x) \u2212 1]\u22121 is the Bose-Einstein distribution."
        },
        {
            "heading": "Training details",
            "text": "We now provide additional practical details and the hyper parameters used to produce the results of this manuscript.\nIn order to enforce sufficient exploration in the early stage of training, we do the following. As in Ref. [111], for a fixed number of initial steps, we choose random actions sampling them uniformly withing their range. Furthermore, for another fixed number of initial steps, we do not update the parameters to allow the replay buffer to have enough transitions. B is a first-in-first-out buffer, of fixed dimension, from which batches of transitions (sk, ak, rk+1, sk+1, ak+1) are randomly sampled to update the NN parameters. After this initial phase, we repeat a policy evaluation, a policy improvement step and a temperature update step nupdates times every nupdates steps. This way, the overall number of updates coincides with the number of actions performed on the QTM. The optimization steps for the value function and the policy are performed using the ADAM optimizer with the standard values of \u03b21 and \u03b22. The temperature parameters \u03b1D and \u03b1C instead are determined using stochastic gradient descent with learning rate 0.001. To favor an exploratory behavior early in the training, and at the same time to end up with a policy that is approximately deterministic, we schedule the target entropies H\u0304C and H\u0304D. In particular, we vary them exponentially during each step according to\nH\u0304a(nsteps) = H\u0304a,end\n+ (H\u0304a,start \u2212 H\u0304a,end) exp(\u2212nsteps/H\u0304a,decay), (62)\nwhere a = C,D, nsteps is the current step number, and H\u0304a,start, H\u0304a,end and H\u0304a,decay are hyperparameters. In the superconducting qubit refrigerator case, we schedule the parameter c according to a Fermi distribution, i.e. c(nstep) = cend+(cstart\u2212cend) [ 1 + exp ( nstep \u2212 cmean\ncdecay\n)]\u22121 .\n(63) In the harmonic oscillator engine case, to improve stability while training for lower values of c, we do not vary c during training, as we do in the superconducting qubit refrigerator case. Instead, we discourage the agent from never utilizing one of the two thermal baths by adding a negative reward if, withing the last N = 128 actions describing the state, less than 25 describe a coupling to either bath. In particular, if the number of actions N\u03b1 where d = \u03b1, with \u03b1 = Hot,Cold is less than 25 in\nthe state time-series, we sum to the reward the following penalty\nrpenalty = \u22121.4 25\u2212N\u03b1\n25 . (64)\nThis penalty has no impact on the final cycles where N\u03b1 is much larger than 25. All hyperparameters used to produce the results of the superconducting qubit refrigerator and of the harmonic oscillator heat engine are provided respectively in Tables I and II, where c refers to the weight at which we are optimizing the return.\nConvergence of the RL approach The training process presents some degree of stochasticity, such as the initial random steps, the stochastic sampling of actions from the policy function, and the random sampling of a batch of experience from the replay buffer to compute an approximate gradient of the loss functions. We thus need to evaluate the reliability of our approach. As shown in the main text, specifically in Figs. 4 and 5, we ran the full optimization 5 times. Out of 65 trainings in the superconducting qubit refrigerator case, only 4 failed, and out of the 55 in the harmonic oscillator engine, only 2 failed, where by failed we mean that the final return was negative. In such cases, we ran the training an additional time. Figs. 4A and 5B display an error bar corresponding to the standard deviation, at each value of c, computed over the 5 repetitions. Instead, in Figs. 4B and 5C we display one black dot for each individual training. As we can see, the overall performance is quite stable and reliable. At last, we discuss the variability of the discovered cycles. The cycles shown in Figs. 4C-F and 5D-E were\n15\nchosen by selecting the largest return among the 5 repetitions. In Figs. 8 and 9 we display cycles discovered in the last of the 5 repetition, i.e. chosen without any post-selection. They correspond to the same setups and parameters displayed in Figs. 4C-F and 5D-E. As we can see, 5 out of the 6 displayed cycles are very similar to the ones displayed in Figs. 4C-F and 5D-E, with a very slight variability. The only exception is Fig. 8B, where the cycle has a visibly shorter period and amplitude than the one shown in Fig. 4D. Despite this visible difference in the cycle shape, the return of the cycle shown in Fig. 8B is 0.382 compared to 0.385 of the cycle shown in Fig. 4B. We therefore conclude that, up to minor changes, the cycles are generally quite stable across multiple trainings.\nComparing with other methods In Figs. 4 and 5 we compare the performance of our method respectively against optimized trapezoidal cycles, and optimized Otto cycles. In both cases, we also maximize the power using the RL method of Ref. [69]. We now detail how we perform such comparison. In the refrigerator based on a superconducting qubit, we consider the trapezoidal cycle proposed in Ref. [49, 63], i.e. we fix\nu(t) = 1\n4\n( 1 + tanh(a cos\u2126t)\ntanh(a)\n) (65)\nwith a = 2, and we optimize \u27e8rc\u27e9 with respect to frequency \u2126. In the heat engine case based on a quantum harmonic oscillator, we fix an Otto cycle as described in Ref. [43], i.e. a trapezoidal cycle consisting of the 4 strokes shown in Figs. 5D-E as a dashed line, and we optimize over the duration of each of the 4 strokes. In particular, we first performed a grid search in the space of these four durations for c = 1. After identifying the largest power, we ran the Newton algorithm to further maximize the return. We then ran the Newton algorithm for all other values of c. The comparison with Ref. [69] was done using the source code provided in Ref. [69], and using the same exact hyperparameters that were used in Ref. [69]. In particular, in the case of the refrigerator based on a superconducting qubit, we re-ran the code using the hyperparameters reported in Table 1, column \u201cFigs. 3, 4\u201d, of the Methods section of Ref. [69], and we trained for the same number of steps (500k). We then evaluated its power and coefficient of performance evaluating the deterministic policy (which typically has a better performance). In the heat engine case based on a quantum harmonic oscillator, we evaluated the performance of the cycle reported in Fig. 5a,c of Ref. [69], whose training hyperparameters are reported in Table 1, column \u201cFig. 5a\u201d, of the Methods section of Ref. [69]."
        },
        {
            "heading": "Generation of coherence",
            "text": "In order to quantify the coherence generated in the instantaneous eigenbasis of the Hamiltonian in the refrigerator based on a superconducting qubit, we evaluated\n16\nthe time average of relative entropy of coherence [115], defined as\nC(\u03c1\u0302(t)) = S(\u03c1\u0302diag.(t))\u2212 S(\u03c1\u0302(t)), (66)\nwhere S(\u03c1\u0302) = \u2212Tr[\u03c1\u0302 ln \u03c1\u0302] is the Von Neumann entropy, and\n\u03c1\u0302diag.(t) = \u27e8gu(t)|\u03c1\u0302(t)|gu(t)\u27e9 \u00b7 |gu(t)\u27e9\u27e8gu(t)| + \u27e8eu(t)|\u03c1\u0302(t)|eu(t)\u27e9 \u00b7 |eu(t)\u27e9\u27e8eu(t)| (67)\nis the density matrix, in the instantaneous eigenbasis |gu(t)\u27e9 and |eu(t)\u27e9, with the off-diagonal terms canceled out.\nWe compute the time-average of the relative entropy of coherence generated by the final deterministic cycle found by the RL agent, and compare it to the coherence generated by a trapezoidal cycle operated at the same speed, i.e. with the same period. As we can see in Table III, the trapezoidal cycles generate twice as much coherence as the RL cycles shown in Figs. 4C-F, i.e. corresponding to c = 1, 0.8, 0.6, 0.4."
        },
        {
            "heading": "CODE AND DATA AVAILABILITY",
            "text": "The code used to generate all results is available on GitHub (https://github.com/PaoloAE/paper_rl_ blackbox_thermal_machines). All raw data that was generated with the accompanying code and that was used to produce the results in the manuscript is available on Figshare (https://doi.org/10.6084/m9.figshare. 19180907)."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We are greatly thankful to Mart\u0301\u0131 Perarnau-Llobet, Paolo Abiuso and Alberto Rolandi for useful discussions and for suggesting to include the entropy production in the return. We gratefully acknowledge funding by the BMBF (Berlin Institute for the Foundations of Learning and Data \u2013 BIFOLD), the European Research Commission (ERC CoG 772230) and the Berlin Mathematics Center MATH+ (AA1-6, AA2-8, AA2-18).\nCOMPETING INTERESTS\nThe authors declare no competing interests. P.A.E. and F.N. are authors of a patent application containing aspects of this work (Application to the European Patent Office, file number: 21 191 966.7)."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "P.A.E. and F.N. designed the research and method. P.A.E. wrote the computer code and carried out the numerical calculations. P.A.E. and F.N. analysed the data and wrote the manuscript.\n17\n[1] F. Giazotto, T. T. Heikkila\u0308, A. Luukanen, A. M. Savin, and J. P. Pekola, Opportunities for mesoscopics in thermometry and refrigeration: Physics and applications, Rev. Mod. Phys. 78, 217 (2006). [2] J. P. Pekola, Towards quantum thermodynamics in electronic circuits, Nat. Phys. 11, 118 (2015). [3] S. Vinjanampathy and J. Anders, Quantum thermodynamics, Contemp. Phys. 57, 545 (2016). [4] G. Benenti, G. Casati, K. Saito, and R. S.Whitney, Fundamental aspects of steady-state conversion of heat to work at the nanoscale, Phys. Rep. 694, 1 (2017). [5] F. Binder, L. Correa, C. Gogolin, J. Anders, and G. Adesso, Thermodynamics in the Quantum Regime: Fundamental Aspects and New Directions, Fundamental Theories of Physics (Springer International Publishing, 2019). [6] A. Ronzani, B. Karimi, J. Senior, Y.-C. Chang, J. T. Peltonen, C.-D. Chen, and J. P. Pekola, Tunable photonic heat transport in a quantum heat valve, Nat. Phys. 14, 991 (2018). [7] B. Dutta, D. Majidi, A. G. Corral, P. A. Erdman, S. Florens, T. A. Costi, H. Courtois, and C. B. Winkelmann, Direct probe of the seebeck coefficient in a kondocorrelated single-quantum-dot transistor, Nano Lett. 19, 506 (2019). [8] J. Senior, A. Gubaydullin, B. Karimi, J. T. Peltonen, J. Ankerhold, and J. P. Pekola, Heat rectification via a superconducting artificial atom, Commun. Phys. 3, 40 (2020). [9] O. Maillet, D. Subero, J. T. Peltonen, D. S. Golubev, and J. P. Pekola, Electric field control of radiative heat transfer in a superconducting circuit, Nat. Commun. 11, 4326 (2020). [10] J. Ro\u00dfnagel, S. T. Dawkins, K. N. Tolazzi, O. Abah, E. Lutz, F. Schmidt-Kaler, and K. Singer, A single-atom heat engine, Science 352, 325 (2016). [11] M. Josefsson, A. Svilans, A. M. Burke, E. A. Hoffmann, S. Fahlvik, C. Thelander, M. Leijnse, and H. Linke, A quantum-dot heat engine operating close to the thermodynamic efficiency limits, Nat. Nanotechnol. 13, 920 (2018). [12] J. Klatzow, J. N. Becker, P. M. Ledingham, C. Weinzetl, K. T. Kaczmarek, D. J. Saunders, J. Nunn, I. A. Walmsley, R. Uzdin, and E. Poem, Experimental demonstration of quantum effects in the operation of microscopic heat engines, Phys. Rev. Lett. 122, 110601 (2019). [13] D. von Lindenfels, O. Gra\u0308b, C. T. Schmiegelow, V. Kaushal, J. Schulz, M. T. Mitchison, J. Goold, F. Schmidt-Kaler, and U. G. Poschinger, Spin heat engine coupled to a harmonic-oscillator flywheel, Phys. Rev. Lett. 123, 080602 (2019). [14] G. Maslennikov, S. Ding, R. Habla\u0308tzel, J. Gan, A. Roulet, S. Nimmrichter, J. Dai, V. Scarani, and D. Matsukevich, Quantum absorption refrigerator with trapped ions, Nat. Commun. 10, 202 (2019). [15] J. P. S. Peterson, T. B. Batalha\u0303o, M. Herrera, A. M. Souza, R. S. Sarthour, I. S. Oliveira, and R. M. Serra, Experimental characterization of a spin quantum heat engine, Phys. Rev. Lett. 123, 240601 (2019). [16] D. Prete, P. A. Erdman, V. Demontis, V. Zannier, D. Ercolani, L. Sorba, F. Beltram, F. Rossella, F. Tad-\ndei, and S. Roddaro, Thermoelectric conversion at 30 k in inas/inp nanowire quantum dots, Nano Lett. 19, 3033 (2019). [17] N. V. Horne, D. Yum, T. Dutta, P. Ha\u0308nggi, J. Gong, D. Poletti, and M. Mukherjee, Single-atom energyconversion device with a quantum load, NPJ Quantum Inf. 6, 37 (2020). [18] P. Krantz, M. Kjaergaard, F. Yan, T. P. Orlando, S. Gustavsson, and W. D. Oliver, A quantum engineer\u2019s guide to superconducting qubits, Appl. Phys. Rev. 6, 021318 (2019). [19] K. Huang, Statistical Mechanics, 2nd ed. (Wiley, 1987). [20] M. Esposito, R. Kawai, K. Lindenberg, and C. V. den\nBroeck, Efficiency at maximum power of low-dissipation carnot engines, Phys. Rev. Lett. 105, 150603 (2010). [21] J. Wang, J. He, and X. He, Performance analysis of a two-state quantum heat engine working with a singlemode radiation field in a cavity, Phys. Rev. E 84, 041127 (2011). [22] J. E. Avron, M. Fraas, G. M. Graf, and P. Grech, Adiabatic theorems for generators of contracting evolutions, Commun. Math. Phys. 314, 163 (2012). [23] M. F. Ludovico, F. Battista, F. von Oppen, and L. Arrachea, Adiabatic response and quantum thermoelectrics for ac-driven quantum systems, Phys. Rev. B 93, 075136 (2016). [24] V. Cavina, A. Mari, and V. Giovannetti, Slow dynamics and thermodynamics of open quantum systems, Phys. Rev. Lett. 119, 050601 (2017). [25] P. Abiuso and V. Giovannetti, Non-markov enhancement of maximum power for quantum thermal machines, Phys. Rev. A 99, 052106 (2019). [26] M. Scandi and M. Perarnau-Llobet, Thermodynamic length in open quantum systems, Quantum 3, 197 (2019). [27] B. Bhandari, P. T. Alonso, F. Taddei, F. von Oppen, R. Fazio, and L. Arrachea, Geometric properties of adiabatic quantum thermal machines, Phys. Rev. B 102, 155407 (2020). [28] P. Terre\u0301n Alonso, P. Abiuso, M. Perarnau-Llobet, and L. Arrachea, Geometric optimization of nonequilibrium adiabatic thermal machines and implementation in a qubit system, PRX Quantum 3, 010326 (2022). [29] J. Eglinton and K. Brandner, Geometric bounds on the power of adiabatic thermal machines, Phys. Rev. E 105, L052102 (2022). [30] P. Abiuso and M. Perarnau-Llobet, Optimal cycles for low-dissipation heat engines, Phys. Rev. Lett. 124, 110606 (2020). [31] P. Abiuso, H. J. D. Miller, M. Perarnau-Llobet, and M. Scandi, Geometric optimisation of quantum thermodynamic processes, Entropy 22 (2020). [32] V. Cavina, P. A. Erdman, P. Abiuso, L. Tolomeo, and V. Giovannetti, Maximum-power heat engines and refrigerators in the fast-driving regime, Phys. Rev. A 104, 032226 (2021). [33] L. Arrachea, M. Moskalets, and L. Martin-Moreno, Heat production and energy balance in nanoscale engines driven by time-dependent fields, Phys. Rev. B 75, 245420 (2007).\n18\n[34] M. Esposito, R. Kawai, K. Lindenberg, and C. Van den Broeck, Quantum-dot carnot engine at maximum power, Phys. Rev. E 81, 041106 (2010). [35] S. Juergens, F. Haupt, M. Moskalets, and J. Splettstoesser, Thermoelectric performance of a driven double quantum dot, Phys. Rev. B 87, 245423 (2013). [36] M. Campisi, J. Pekola, and R. Fazio, Nonequilibrium fluctuations in quantum heat engines: theory, example, and possible solid state experiments, New J. Phys. 17, 035012 (2015). [37] R. Dann and R. Kosloff, Qunatum signatures in the quantum carnot cycle, New J. Phys. 22, 013055 (2020). [38] O. A. D. Molitor and G. T. Landi, Stroboscopic twostroke quantum heat engines, Phys. Rev. A 102, 042217 (2020). [39] V. Shaghaghi, G. M. Palma, and G. Benenti, Extracting work from random collisions: A model of a quantum heat engine, Phys. Rev. E 105, 034101 (2022). [40] F. Cavaliere, M. Carrega, G. De Filippis, V. Cataudella, G. Benenti, and M. Sassetti, Dynamical heat engines with non-markovian reservoirs, Phys. Rev. Res. 4, 033233 (2022). [41] T. Feldmann, E. Geva, R. Kosloff, and P. Salamon, Heat engines in finite time governed by master equations, Am. J. Phys. 64, 485 (1996). [42] T. Feldmann and R. Kosloff, Performance of discrete heat engines and heat pumps in finite time, Phys. Rev. E 61, 4774 (2000). [43] Y. Rezek and R. Kosloff, Irreversible performance of a quantum harmonic heat engine, New J. Phys. 8, 83 (2006). [44] H. Quan, Y. Liu, C. Sun, and F. Nori, Quantum thermodynamic cycles and quantum heat engines, Phys. Rev. E 76, 031105 (2007). [45] O. Abah, J. Ro\u00dfnagel, G. Jacob, S. Deffner, F. SchmidtKaler, K. Singer, and E. Lutz, Single-ion heat engine at maximum power, Phys. Rev. Lett. 109, 203006 (2012). [46] A. E. Allahverdyan, K. V. Hovhannisyan, A. V. Melkikh, and S. G. Gevorkian, Carnot cycle at finite power: Attainability of maximal efficiency, Phys. Rev. Lett. 111, 050601 (2013). [47] K. Zhang, F. Bariani, and P. Meystre, Quantum optomechanical heat engine, Phys. Rev. Lett. 112, 150602 (2014). [48] M. Campisi and R. Fazio, The power of a critical heat engine, Nat. Commun. 7, 11895 (2016). [49] B. Karimi and J. P. Pekola, Otto refrigerator based on a superconducting qubit: Classical and quantum performance, Phys. Rev. B 94, 184503 (2016). [50] R. Kosloff and Y. Rezek, The quantum harmonic otto cycle, Entropy 19, 136 (2017). [51] G. Watanabe, B. P. Venkatesh, P. Talkner, and A. del Campo, Quantum performance of thermal machines over many cycles, Phys. Rev. Lett. 118, 050601 (2017). [52] S. Deffner, Efficiency of harmonic quantum otto engines at maximal power, Entropy 20, 875 (2018). [53] D. Gelbwaser-Klimovsky, A. Bylinskii, D. Gangloff, R. Islam, A. Aspuru-Guzik, and V. Vuletic, Single-atom heat machines enabled by energy quantization, Phys. Rev. Lett. 120, 170601 (2018). [54] J. Chen, C. Sun, and H. Dong, Boosting the performance of quantum otto heat engines, Phys. Rev. E 100, 032144 (2019).\n[55] J. P. Pekola, B. Karimi, G. Thomas, and D. V. Averin, Supremacy of incoherent sudden cycles, Phys. Rev. B 100, 085405 (2019). [56] A. Das and V. Mukherjee, Quantum-enhanced finitetime otto cycle, Phys. Rev. B 2, 033083 (2020). [57] M. V. Berry, Transitionless quantum driving, J. Phys. A: Math. Theor. 42, 365303 (2009). [58] Q.-h. Deng, J.and Wang, Z. Liu, P. Ha\u0308nggi, and G. J., Boosting work characteristics and overall heat-engine performance via shortcuts to adiabaticity: Quantum and classical systems, Phys. Rev. E 88, 062122 (2013). [59] E. Torrontegui, S. Iba\u0301n\u0303ez, S. Mart\u0301\u0131nez-Garaot, M. Modugno, A. del Campo, D. Gue\u0301ry-Odelin, A. Ruschhaupt, X. Chen, and J. G. Muga, Shortcuts to adiabaticity, Adv. At., Mol., Opt. Phys. 62, 117 (2013). [60] A. del Campo, J. Goold, and M. Paternostro, More bang for your buck: Super-adiabatic quantum engines, Sci. Rep. 4, 6208 (2014). [61] B. C\u0327akmak and O. E. Mu\u0308stecapl\u0131og\u0306lu, Spin quantum heat engines with shortcuts to adiabaticity, Phys. Rev. E 99, 032108 (2019). [62] S. Deng, A. Chenu, P. Diao1, F. Li, S. Yu, I. Coulamy, A. del Campo, and H. Wu, Superadiabatic quantum friction suppression in finite-time thermodynamics, Sci. Adv. 18, eaar5909 (2018). [63] K. Funo, N. Lambert, B. Karimi, J. P. Pekola, Y. Masuyama, and F. Nori, Speeding up a quantum refrigerator via counterdiabatic driving, Phys. Rev. B 100, 035407 (2019). [64] T. Villazon, A. Polkovnikov, and A. Chandran, Swift heat transfer by fast-forward driving in open quantum systems, Phys. Rev. A 100, 012126 (2019). [65] I. Khait, J. Carrasquilla, and D. Segal, Optimal control of quantum thermal machines using machine learning, Phys. Rev. Res. 4, L012029 (2022). [66] V. Cavina, A. Mari, A. Carlini, and V. Giovannetti, Optimal thermodynamic control in open quantum systems, Phys. Rev. A 98, 012139 (2018). [67] N. Suri, F. C. Binder, and S. Muralidharan, B. amd Vinjanampathy, Speeding up thermalisation via open quantum system variational optimisation, Eur. Phys. J. Spec. Top. 227, 203 (2018). [68] P. Menczel, T. Pyha\u0308ranta, C. Flindt, and K. Brandner, Two-stroke optimization scheme for mesoscopic refrigerators, Phys. Rev. B 99, 224306 (2019). [69] P. A. Erdman and F. Noe\u0301, Identifying optimal cycles in quantum thermal machines with reinforcementlearning, NPJ Quantum Inf. 8, 1 (2022). [70] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, in International Conference on Machine Learning , Vol. 80 (PMLR, 2018) p. 1861. [71] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, et al., Soft actor-critic algorithms and applications, arXiv:1812.05905 (2018). [72] P. Christodoulou, Soft actor-critic for discrete action settings, arXiv:1910.07207 (2019). [73] O. Delalleau, M. Peter, E. Alonso, and A. Logut, Discrete and continuous action representation for practical rl in video games, arXiv:1912.11077 (2019). [74] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\n19\nFidjeland, G. Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518, 529 (2015). [75] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., Mastering the game of go without human knowledge, Nature 550, 354 (2017). [76] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575, 350 (2019). [77] T. Haarnoja, S. Ha, A. Zhou, J. Tan, G. Tucker, and S. Levine, Learning to walk via deep reinforcement learning, arXiv:1812.11103 (2018). [78] M. Bukov, A. G. R. Day, D. Sels, P. Weinberg, A. Polkovnikov, and P. Mehta, Reinforcement learning in different phases of quantum control, Phys. Rev. X 8, 031086 (2018). [79] Z. An and D. Zhou, Deep reinforcement learning for quantum gate control, EPL 126, 60002 (2019). [80] M. Dalgaard, F. Motzoi, J. J. S\u00f8rensen, and J. Sherson, Global optimization of quantum dynamics with alphazero deep exploration, NPJ Quantum Inf. 6, 6 (2020). [81] J. Mackeprang, D. B. R. Dasari, and J. Wrachtrup, A reinforcement learning approach for quantum state engineering, Quantum Mach. Intell. 2, 5 (2020). [82] F. Scha\u0308fer, M. Kloc, C. Bruder, and N. Lo\u0308rch, A differentiable programming method for quantum control, Mach. Learn.: Sci. Technol. 1, 035009 (2020). [83] F. Scha\u0308fer, P. Sekatski, M. Koppenho\u0308fer, C. Bruder, and M. Kloc, Control of stochastic quantum dynamics by differentiable programming, Mach. Learn.: Sci. Technol. 2, 035004 (2021). [84] R. Porotti, A. Essig, B. Huard, and F. Marquardt, Deep reinforcement learning for quantum state preparation with weak nonlinear measurements, Quantum 6, 747 (2022). [85] F. Marquardt, Machine learning and quantum devices, SciPost Phys. Lect. Notes , 29 (2021). [86] J. Brown, P. Sgroi, L. Giannelli, G. S. Paraoanu, E. Paladino, G. Falci, M. Paternostro, and A. Ferraro, Reinforcement learning-enhanced protocols for coherent population-transfer in three-level quantum systems, New J. Phys. 23, 093035 (2021). [87] F. Metz and M. Bukov, Self-correcting quantum manybody control using reinforcement learning with tensor networks, Nat. Mach. Intell. 5, 780 (2023). [88] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven, Universal quantum control through deep reinforcement learning, NPJ Quantum Inf. 5, 33 (2019). [89] X.-M. Zhang, Z. Wei, R. Asad, X.-C. Yang, and X. Wang, When does reinforcement learning stand out in quantum control? a comparative study on state preparation, NPJ Quantum Inf. 5, 85 (2019). [90] T. Fo\u0308sel, P. Tighineanu, T. Weiss, and F. Marquardt, Reinforcement learning with neural networks for quantum feedback, Phys. Rev. X 8, 031084 (2018). [91] R. Sweke, M. S. Kesselring, E. P. L. van Nieuwenburg, and J. Eisert, Reinforcement learning decoders for fault-tolerant quantum computation, Mach. Learn.: Sci. Technol. 2, 025005 (2020). [92] P. Sgroi, G. M. Palma, and M. Paternostro, Reinforcement learning approach to nonequilibrium quantum\nthermodynamics, Phys. Rev. Lett. 126, 020601 (2021). [93] R. Kosloff and T. Feldmann, Discrete four-stroke quan-\ntum heat engine exploring the origin of friction, Phys. Rev. E 65, 055102 (2002). [94] A. Friedenberger and E. Lutz, When is a quantum heat engine quantum?, EPL 120, 10002 (2017). [95] K. Brandner, M. Bauer, and S. U., Universal coherenceinduced power losses of quantum heat engines in linear response, Phys. Rev. Lett. 119, 170602 (2017). [96] J. Lekscha, H. Wilming, J. Eisert, and R. Gallego, Quantum thermodynamics with local control, Phys. Rev. E 97, 022142 (2018). [97] P. Strasberg, G. Schaller, N. Lambert, and T. Brandes, Nonequilibrium thermodynamics in the strong coupling and non-markovian regime based on a reaction coordinate mapping, New J. Phys. 18, 073007 (2016). [98] L. F. Seoane and R. Sole\u0301, Multiobjective optimization and phase transitions, in Proceedings of ECCS 2014 , edited by S. Battiston, F. De Pellegrini, G. Caldarelli, and E. Merelli (Springer International Publishing, Cham, 2016) pp. 259\u2013270. [99] H. J. D. Miller, M. Scandi, J. Anders, and M. PerarnauLlobet, Work fluctuations in slow processes: Quantum signatures and optimal control, Phys. Rev. Lett. 123, 230603 (2019). [100] A. P. Solon and J. M. Horowitz, Phase transition in protocols minimizing work fluctuations, Phys. Rev. Lett. 120, 180605 (2018). [101] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, Wavenet: A generative model for raw audio, arXiv:1609.03499 (2016). [102] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, arXiv:1512.03385 (2015). [103] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv:1412.6980 (2014). [104] V. Gorini, A. Kossakowski, and E. C. G. Sudarshan, Completely positive dynamical semigroups of N-level systems, J. Math. Phys. 17, 821 (1976). [105] G. Lindblad, On the generators of quantum dynamical semigroups, Commun. Math. Phys 48, 119 (1976). [106] H. Breuer and F. Petruccione, The theory of open quantum systems (Oxford University Press, 2002). [107] M. Yamaguchi, T. Yuge, and T. Ogawa, Markovian quantum master equation beyond adiabatic regime, Phys. Rev. E 95, 012136 (2017). [108] A. C. Barato and U. Seifert, Thermodynamic uncertainty relation for biomolecular processes, Phys. Rev. Lett. 114, 158101 (2015). [109] G. Guarnieri, G. T. Landi, S. R. Clark, and J. Goold, Thermodynamics of precision in quantum nonequilibrium steady states, Phys. Rev. Res. 1, 033021 (2019). [110] H. J. D. Miller, M. H. Mohammady, M. PerarnauLlobet, and G. Guarnieri, Thermodynamic uncertainty relation in slowly driven quantum heat engines, Phys. Rev. Lett. 126, 210603 (2021). [111] J. Achiam, Spinning Up in Deep Reinforcement Learning (2018). [112] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (MIT press, 2018). [113] R. Dann, A. Levy, and R. Kosloff, Time-dependent markovian quantum master equation, Phys. Rev. A 98, 052129 (2018).\n20\n[114] R. Alicki, The quantum open system as a model of the heat engine, J. Phys. A: Math. Gen. 12, L103 (1979). [115] T. Baumgratz, M. Cramer, and M. Plenio, Quantifying coherence, Phys. Rev. Lett. 113, 140401 (2014)."
        }
    ],
    "title": "Model-free optimization of power/efficiency tradeoffs in quantum thermal machines using reinforcement learning",
    "year": 2023
}