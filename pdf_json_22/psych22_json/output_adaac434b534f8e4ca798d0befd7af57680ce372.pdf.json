{
    "abstractText": "Research demonstrates that students generally find digitally recorded assessment feedback comments to be more satisfying than text-based feedback comments. However, positive perceptions of digitally recorded feedback may be impacted by the confidence and experience of the educator who is providing the comments. As such, this paper reports on an exploratory study in which we compare students\u2019 perceptions of the text-based and digitally recorded feedback created by five tutors in the same subject. Survey data were collected from 81 students, of which 58 received text-based and 23 received digitally recorded feedback comments. Students who received digitally recorded feedback comments provided consistently higher ratings for feedback clarity, usefulness, and satisfaction than students who received textbased feedback comments. It is proposed that the media enables these effects, but the structure of the feedback design is also important.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tracii Ryan"
        }
    ],
    "id": "SP:bb203de79566e3f084d2dc7b39521800b50a6d43",
    "references": [
        {
            "authors": [
                "S. Brown",
                "P. Knight"
            ],
            "title": "Assessing Learners in Higher Education",
            "year": 1994
        },
        {
            "authors": [
                "J. Borup",
                "C. Graham",
                "A. Velasquez"
            ],
            "title": "The Use of Asynchronous Video Communication to Improve Instructor Immediacy and Social Presence in a Blended Learning Environment",
            "venue": "In A. Kitchenham (Ed) Blended Learning Across Disciplines: Models for Implementation,",
            "year": 2011
        },
        {
            "authors": [
                "D. Boud",
                "E. Molloy"
            ],
            "title": "What is the problem with feedback",
            "year": 2013
        },
        {
            "authors": [
                "D. Carless",
                "D. Boud"
            ],
            "title": "The development of student feedback literacy: enabling uptake of feedback",
            "venue": "Assessment and Evaluation in Higher Education,",
            "year": 2018
        },
        {
            "authors": [
                "N. Chang",
                "A.B. Watson",
                "M.A. Bakerson",
                "E.E. Williams",
                "F.X. McGoron",
                "B. Spitzer"
            ],
            "title": "Electronic feedback or handwritten feedback: What do undergraduate students prefer and why",
            "venue": "Journal of Teaching and Learning with Technology,",
            "year": 2012
        },
        {
            "authors": [
                "J. Costello",
                "D. Crane"
            ],
            "title": "Providing Learner-centered Feedback Using a Variety of Technologies",
            "venue": "Paper presented at the International Conference on the Liberal Arts, St. Thomas University,",
            "year": 2010
        },
        {
            "authors": [
                "B. Crisp"
            ],
            "title": "Is it worth the effort? How feedback influences students' subsequent submission of assessable work",
            "venue": "Assessment and Evaluation in Higher Education,",
            "year": 2007
        },
        {
            "authors": [
                "R.L. Daft",
                "R.H. Lengel"
            ],
            "title": "Organizational information requirements, media richness and structural design",
            "venue": "Management Science,",
            "year": 1986
        },
        {
            "authors": [
                "N. Duncan"
            ],
            "title": "Feed-forward': Improving students' use of tutors' comments. Assessment and Evaluation in Higher Education",
            "year": 2007
        },
        {
            "authors": [
                "A. Field"
            ],
            "title": "Discovering Statistics using SPSS (3rd Edition)",
            "year": 2009
        },
        {
            "authors": [
                "J. Hattie"
            ],
            "title": "Visible learning: A synthesis of 800+ meta-analyses on achievement",
            "year": 2009
        },
        {
            "authors": [
                "J. Hattie",
                "H. Timperley"
            ],
            "title": "The power of feedback",
            "venue": "Review of Educational Research,",
            "year": 2007
        },
        {
            "authors": [
                "M. Henderson",
                "M. Phillips"
            ],
            "title": "Technology enhanced feedback on assessment",
            "venue": "Paper presented at the Australian Computers in Education Conference",
            "year": 2014
        },
        {
            "authors": [
                "M. Henderson",
                "M. Phillips"
            ],
            "title": "Video-based feedback on student assessment: scarily personal",
            "venue": "Australasian Journal of Educational Technology,",
            "year": 2015
        },
        {
            "authors": [
                "R. Higgins",
                "P. Hartley",
                "A. Skelton"
            ],
            "title": "Getting the message across: The problem of communicating assessment feedback",
            "venue": "Teaching in Higher Education,",
            "year": 2001
        },
        {
            "authors": [
                "H. Knauf"
            ],
            "title": "Reading, listening and feeling: audio feedback as a component of an inclusive learning culture at universities",
            "venue": "Assessment & Evaluation in Higher Education,",
            "year": 2016
        },
        {
            "authors": [
                "A. Lizzio",
                "K. Wilson"
            ],
            "title": "Feedback on assessment: Students' perceptions of quality and effectiveness",
            "venue": "Assessment & Evaluation in Higher Education,",
            "year": 2008
        },
        {
            "authors": [
                "N. Luongo"
            ],
            "title": "Missing the chalkboard: Using screencasting in the online classroom",
            "venue": "Computers in the Schools,",
            "year": 2015
        },
        {
            "authors": [
                "P. Mahoney",
                "S. Macfarlane",
                "R. Ajjawi"
            ],
            "title": "A qualitative synthesis of video feedback in higher education",
            "venue": "Teaching in Higher Education,",
            "year": 2019
        },
        {
            "authors": [
                "J. McCarthy"
            ],
            "title": "Evaluating written, audio and video feedback in higher education summative assessment",
            "venue": "tasks. Issues in Educational Research,",
            "year": 2015
        },
        {
            "authors": [
                "D. McConnell"
            ],
            "title": "Assessing learning in e-groups and communities",
            "year": 2006
        },
        {
            "authors": [
                "E. Molloy",
                "F. Borrell-Carrio",
                "R. Epstein"
            ],
            "title": "The Impact of Emotions in Feedback",
            "year": 2013
        },
        {
            "authors": [
                "C. Morris",
                "G. Chikwa"
            ],
            "title": "Audio versus written feedback: Exploring learners' preference and the impact of feedback format on students' academic performance",
            "venue": "Active Learning in Higher Education,",
            "year": 2016
        },
        {
            "authors": [
                "M. Phillips",
                "T. Ryan",
                "M. Henderson"
            ],
            "title": "A cross-disciplinary evaluation of digitally recorded feedback in higher education",
            "venue": "Proceedings ASCILITE2017: 34th International Conference on Innovation, Practice and Research in the Use of Educational Technologies in Tertiary Education (pp. 364-361)",
            "year": 2017
        },
        {
            "authors": [
                "M. Phillips",
                "M. Henderson",
                "T. Ryan"
            ],
            "title": "Multimodal feedback is not always clearer, more useful or satisfying",
            "venue": "Show Me The Learning. Proceedings ASCILITE 2016 Adelaide (pp. 514-522)",
            "year": 2016
        },
        {
            "authors": [
                "E. Pitt",
                "L. Norton"
            ],
            "title": "Now That's the Feedback I Want!' students' Reactions to Feedback on Graded Work and What They Do with It",
            "venue": "Assessment & Evaluation in Higher Education,",
            "year": 2017
        },
        {
            "authors": [
                "T Ryan",
                "M Henderson",
                "M Phillips"
            ],
            "title": "Feedback modes matter: comparing perceptions of digital and non- digital feedback modes in higher education",
            "venue": "British Journal of Educational Technology",
            "year": 2019
        },
        {
            "authors": [
                "R. Thompson",
                "M.J. Lee"
            ],
            "title": "Talking with students through screencasting: Experimentations with video feedback to improve student learning",
            "venue": "Journal of Interactive Technology and Pedagogy,",
            "year": 2012
        },
        {
            "authors": [
                "J. West",
                "W. Turner"
            ],
            "title": "Enhancing the assessment experience: Improving student perceptions, engagement and understanding using online video feedback. Innovations in Education and Teaching International",
            "year": 2016
        },
        {
            "authors": [
                "N.E. Winstone",
                "R. Nash",
                "M. Parker",
                "J. Rowntree"
            ],
            "title": "Supporting Learners' Agentic Engagement with Feedback: A Systematic Review and a Taxonomy of Recipience Processes",
            "venue": "Educational Psychologist",
            "year": 2017
        },
        {
            "authors": [
                "N. Winstone",
                "D. Carless"
            ],
            "title": "Designing Effective Feedback Processes in Higher Education: A Learning- Focused Approach",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "ASCILITE 2019 Singapore University of Social Sciences 264\nFrom text to audiovisual feedback: enhancing clarity, usefulness and satisfaction\nTracii Ryan Melbourne Centre for the Study of Higher Education University of Melbourne Australia\nFaculty of Education Monash University Australia\nMichael Phillips Faculty of Education Monash University Australia\nMichael Henderson Faculty of Education Monash University Australia\nResearch demonstrates that students generally find digitally recorded assessment feedback comments to be more satisfying than text-based feedback comments. However, positive perceptions of digitally recorded feedback may be impacted by the confidence and experience of the educator who is providing the comments. As such, this paper reports on an exploratory study in which we compare students\u2019 perceptions of the text-based and digitally recorded feedback created by five tutors in the same subject. Survey data were collected from 81 students, of which 58 received text-based and 23 received digitally recorded feedback comments. Students who received digitally recorded feedback comments provided consistently higher ratings for feedback clarity, usefulness, and satisfaction than students who received textbased feedback comments. It is proposed that the media enables these effects, but the structure of the feedback design is also important.\nKeywords: digitally recorded feedback, text-based feedback, higher education, assessment feedback"
        },
        {
            "heading": "Introduction",
            "text": "Assessment feedback is an important component of the learning process (Carless & Boud, 2018; Winstone & Carless, 2019). Effective feedback comments from educators on both formative and summative assessment can have a powerful influence on student achievement (Brown & Knight, 1994; Hattie & Timperley, 2007). However, Hattie and Clarke (2018) note that despite it being one of the most powerful single influences on student learning, it is also one of the most variable. Indeed, in an earlier work Hattie (2009) had conducted a meta-analysis that revealed a third of feedback studies that found to have a detrimental effect on learning attainment. The reasons for this variability continue to be a focus of feedback research, including the investigation of the modality of the feedback itself. In higher education, educators commonly create and deliver feedback comments through the use of text, such as handwriting or electronic annotations (Chang et al., 2012). However, the potential impact of textbased feedback comments is often undermined by ambiguity and lack of detail (Thompson & Lee, 2012). In contrast, face-to-face feedback, while often rich in detail, can be hampered by performance anxiety and is dependent on student memory (Henderson & Phillips, 2015).\nA growing body of literature suggests that digitally recorded feedback comments, including audio, video, or screencasts, can be used by educators to provide performance information that students find to be clear, detailed, satisfying, and personalised (Ryan, Henderson & Phillips, 2019; Knauf, 2016; Luongo, 2015; Mahoney, Macfarlane & Ajjawi, 2019; Morris & Chikwa, 2016). However, findings also suggest that these perceptions may differ as a result of the educators\u2019 confidence, experience, and demeanour (Phillips, Henderson & Ryan, 2016; Phillips, Ryan & Henderson, 2017). To control for these potential educator differences across modalities, this study compares student perceptions of text-based and digitally recorded feedback comments created by the same group of five tutors."
        },
        {
            "heading": "Background",
            "text": "Feedback plays a critical role in orienting students to learning (McConnell, 2006). High quality feedback enhances student experience, improves motivation, facilitates development, and strengthens future performance (Costello & Crane, 2010; Duncan, 2007; Higgins, Hartley, & Skelton, 2001; Lizzio & Wilson, 2008). Although there is a vast body of research relating to feedback, there is surprisingly little consensus about the recommended design of feedback comments. Consequently, Henderson and Phillips (2014) synthesized a broad range of literature, and\nASCILITE 2019 Singapore University of Social Sciences 265\nreported on a guiding set of eight principles relating to the design of educator-created feedback artefacts on summative assessment. These principles include being timely, clear (unambiguous), educative (and not just evaluative), sensitive to the individual, proportionate to criteria/goals, locating student performance, emphasizing task performance, and presenting the feedback as an ongoing dialogue rather than an end point (a more detailed review of the literature and explanation of the synthesis of design principles can be found in Henderson and Phillips, 2014). Combined, the principles above require not only a quick process (i.e., for a timely completion), but also a means by which considerable individualised detail can be conveyed in a way that is sensitive to each student\u2019s context and needs. Unsurprisingly this balance is difficult achieve with written comments, especially if limited to the margins of essays or with rubrics. A growing body of literature is now revealing the benefits of video, audio, and screencast technologies for feedback in relation to assessment tasks (Ryan, Henderson & Phillips, 2019; Knauf, 2016; Morris & Chikwa, 2016; West & Turner, 2016). Students\u2019 preference for audiovisual feedback has been well established across a number of tertiary studies (Luongo, 2015; McCarthy, 2015). In particular, this mode has been reported to be more detailed, clear, individualised, and supportive (Ryan et al., 2019). The advantages of audiovisual recordings can be explained using media richness theory (Daft & Lengel, 1986), which states that interactions involving complex issues are best conveyed through richer media. This point is highly relevant to assessment feedback, as educators often need to explain difficult concepts in ways that students can understand. However, emerging research suggests that digital recordings are not a silicon bullet, as students\u2019 perceptions and level of satisfaction may differ according to the experience of the educator, and the quality of the comments that they provide. For example, in a previous study (Phillips et al., 2016), we found that students who received digitally recorded feedback comments from one particular tutor rated the comments as less clear than students who received digital recordings from other tutors. This was attributed to that particular tutor\u2019s failure to follow the recommended comment structure, as well as issues with expression of language. Similarly, results from a subsequent study showed that students who received digitally recorded feedback from a tutor who was teaching in a subject for the first time appeared to be more dissatisfied with the quality of the comments than students who received recordings from more experienced tutors (Phillips et al., 2017). Therefore, despite the affordances of digitally recorded feedback, it appears that students\u2019 perceptions may differ as a result of the educators\u2019 confidence, experience, and demeanour (Phillips et al., 2016; Phillips et al, 2017). This paper presents the results of an exploratory study which looked at students\u2019 perceptions of text-based and digitally recorded feedback comments created by the same five tutors. The purpose of this study was to identify useful future lines of enquiry for digitally recorded feedback research."
        },
        {
            "heading": "Method",
            "text": "This paper is based on a subset of data that were originally derived from a larger mixed methods study aimed with assessing the impact and design of digitally recorded feedback comments on assessment tasks across disciplines at a large Australian university. The subset of data examined in this study originates from a Masters level Education subject, which focused on models for learning, and cultural and socio-economic learning contexts. The subject was held in the first semester of the calendar year, and classes ran for nine weeks. The feedback comments under investigation in this paper were provided on the first assessment task of the unit; an essay in which students were asked to compare and contrast learning theories. Ethics approval was received from the university human research ethics committee prior to data collection. It should be noted that, although all student respondents included in this paper were enrolled in the same subject, data collection for the two student groups (i.e. text recipients and digital recording recipients) occurred in separate years. Data from students who received text-based feedback comments were collected during a 2016 iteration of the study. In that iteration, four of 14 tutors teaching into the one subject created digitally recorded comments on assessment tasks. The remaining nine tutors used their usual method of text-based comments (see Phillips et al., 2016 for more details). In the 2017 iteration, 13 out of 15 tutors teaching into the same subject provided digitally recorded feedback to students. The data used in this paper are taken from students of five tutors who provided text comments in 2016 and digitally recorded comments in 2017.\nASCILITE 2019 Singapore University of Social Sciences 266"
        },
        {
            "heading": "Participants",
            "text": "Participants were 81 Masters level Education students, of which 89% were women and 53% were non-native speakers of English. Seventy-two per cent of the sample completed the subject in 2016 and received text-based feedback comments, and 28% completed the subject in 2017 and received digitally recorded comments. Among those who received digitally recorded comments, 57% received video recordings, 39% received audio recordings, and 4% received screencasts. With regard to the frequency of respondents who received feedback comments from each tutor, 35% received comments from Tutor 1, 20% from Tutor 2, 15% from Tutor 3, 18% from Tutor 4, and 12% from Tutor 5. The majority of students completing this subject had been out of the higher education system for some time and, as such, the feedback comments they received on this assessment task were likely to have been the first they had received in a higher education context in several years."
        },
        {
            "heading": "Materials",
            "text": "For the scope of this paper, data from seven survey items - referred to collectively as the Feedback Attitudes Survey (see Appendix) - are included. There are three items related to clarity of the comments, three items related to the usefulness of the comments for future work, and one item measuring satisfaction with the comments. The latter item was rated using a 5-point satisfaction scale (1 = Extremely dissatisfied, 5 = Extremely satisfied), while the remaining six items were rated using 5-point agreement scales (1 = Strongly disagree, 5 = Strongly agree). There was one negatively worded item in the survey, \u2018The feedback was confusing\u2019 and this was reverse-coded and reworded to read \u2018The feedback was not confusing\u2019 for the purposes of reporting."
        },
        {
            "heading": "Procedure",
            "text": "In the 2016 iteration of the study, the five tutors providing text-based feedback were free to follow their normal routine for providing comments on assessment tasks. In 2017, with the introduction of digitally recorded feedback, the researchers provided the tutors with an advised structure of feedback content that had been tested in tertiary settings (see Phillips et al., 2017). Key components of the structure included addressing the student by name, recognizing their context and histories, using examples from their work when discussing issues, and placing the greatest amount of emphasis on how the student could improve their performance in future pieces of work. The tutors were then trained in how to use video, audio, or screencast technologies to provide feedback comments to students. Following this, the tutors selected the mode of digital recording they felt most comfortable with to provide feedback comments to students. We acknowledge that allowing tutors to choose which type of recording they used to provide feedback comments means that there is variability in the richness of media received by students in the digitally recorded feedback group, however, we considered it important that the tutors were able to adapt the interventions to suit their needs and preferences."
        },
        {
            "heading": "Results",
            "text": "Table 1 presents descriptive results showing the percentage breakdown of ratings of the clarity and usefulness of feedback comments by text and digitally recorded feedback recipients, and Table 2 presents descriptive results for satisfaction ratings. The highest proportion of responses for text recipients was in the agree/satisfied responses categories, whereas the majority of digital recording recipients provided responses in the strongly agree/extremely satisfied response categories. These results suggest that students who received digitally recorded feedback comments were more likely to find them to be clear, useful, and satisfying than students who received text-based comments from the same tutors.\nTo examine whether there were any significant differences in the mean ratings of each item for text and digital recording recipients, a series of Mann Whitney U tests were performed (see Table 3). Mann Whitney U tests involve comparisons of ranked means rather than raw means, and are considered to be more robust than t-tests when the data are ordinal and sample sizes are unequal (Field, 2009). As the results reveal, there was a significant difference with a medium-to-large effect between the ranked means of text and digital recording recipients for all survey items.\nASCILITE 2019 Singapore University of Social Sciences 268"
        },
        {
            "heading": "Discussion",
            "text": "As the ranked means for digital recording recipients were consistently higher than text recipients, the results of the inferential statistics strongly indicate that digital recordings are more clear, useful, and satisfying than textbased comments. In considering the positive impact of the digital feedback in this study, we argue that there are two, inextricably linked, factors at play: the first is the affordances of the audio and visual media, and the second is the structure of the feedback content. In particular, we propose that affordances of the media influenced the increase in perceived clarity, while the structured feedback content may have largely influenced the positive ratings of usefulness. Together, these two elements are likely to have contributed to the higher satisfaction ratings. The following discussion addresses each of these important factors in turn. Audiovisual media may improve the clarity of feedback Students who received digitally recorded feedback provided higher ratings than students who received text-based feedback on survey items measuring clarity. These results are likely to be due to the fact that audiovisual media are richer in communication cues, such as tone and pace. According to media richness theory (Daft & Lengel, 1986), the addition of these cues is likely to reduce ambiguity and increase clarity of the information being conveyed. For this reason, audiovisual media are thought to be more appropriate than text when the situation involves the transmission of complex, high stakes, or emotional information, as is the case with assessment feedback (see also Borup, Graham and Velasquez, 2011). In addition, audiovisual media afford more effective communication of tutor empathy and personalisation, which is likely to have influenced student receptiveness and understanding of the constructive nature of the commentary. This argument is supported by our previous work (Henderson & Phillips, 2015), which shows digitally recorded feedback can enhance student perception of the supportive nature of the feedback. Moreover, literature reveals that when students experience adverse emotional reactions to feedback comments, their receptiveness, sense making, and motivation can be negatively impacted (Molloy, Borrell-Carrio, & Epstein 2013; Pitt & Norton 2016; Winstone et al. 2017). Further research obviously needs to be conducted to explore these arguments. In the meantime, we propose several design considerations in the use of audiovisual media for providing feedback comments. First, these rich forms of media better allow for the effective communication of complex information, however this also places a degree of effort on the educator who needs to discuss the deeper ideas or complex issues in the assignment, rather than simply noting superficial corrective feedback. This is a deceptively obvious argument. In the authors\u2019 experience, it is far too easy to spend the entire recording saying more of the same things, such as comments about grammatical changes, rather than deeply engaging with key ideas or conceptualisations within students\u2019 work. A second design consideration is that audiovisual media enhances the educator\u2019s ability to explain difficult ideas more clearly than with text, but it can also increase students\u2019 perceptions of personalisation, and therefore their degree of receptiveness to the feedback. To take full advantage of this opportunity, educators need to consider how to express themselves most clearly, such as by articulating distinctly and explaining key points in more than one way. Furthermore, educators may also make efforts to enhance the level of personalisation, by looking at the camera instead of the screen or assessment task, using the student\u2019s name, referring to interactions or process throughout the semester, and authentically revealing empathy and interest. Feedback structure may improve the usefulness of feedback There is a growing body of literature that reinforces the need to focus on how the feedback process could usefully influence future work or strategies (for example, see Boud & Molloy, 2013; Carless & Boud, 2018). Therefore, tutors in this study were given explicit instructions to structure digitally recorded feedback comments in ways that would enhance their usefulness. For example, it was recommended that a significant proportion of the comments were devoted to the intellectual substance (as opposed to textual issues) of the assignment, with an emphasis on feed forward. More specifically, tutors were told to: Engage with the conclusions, arguments, logic, and justification in the assignment. Select two or three issues to discuss in detail that will be of most use to the students as they move forward in this field and in their future studies. Comment on strengths, weaknesses, flaws, gaps, creativity and insights. Importantly, the comments must be phrased to emphasise how students can improve their future work and thinking. This might include examples of alternative arguments, additional literature and different ways to think or approach the topic. (Extended descriptions of the feedback structure can be found at the project website http://der.monash.edu.au/lnm/technology-mediated-assessment-feedback/)\nASCILITE 2019 Singapore University of Social Sciences 269\nThis careful focus on providing feedback comments that were useful and useable is likely to have impacted on students\u2019 perceptions of the digital recordings, especially in comparison to the text-based feedback comments which tutors created according to their usual practice.\nThere are a few noteworthy design considerations here. First, the digitally recorded feedback comments were largely focused on what the student could most usefully change or strengthen to improve their work or thinking. This is a marked departure from the typical content of feedback comments, which often focus on justifying the grade. Indeed, in this study, the tutors were told not to refer to grades, which had already been communicated through the online gradebook. A second design consideration was that the tutors did not try to address the entire assessment task during the process of creating the digitally recorded feedback comments. Instead, they were selective; focusing on just a small number of key issues that they felt would be most useful for that student. This ensured there was sufficient time in the recording to deal with issues in a considered way that was not rushed. The decision to be selective in providing comments was also informed by researchers such as Crisp (2007), who point out that extensive feedback comments may be inefficient because students are only able to process a proportion of the information within. A third design element was that the tutors were told to begin their feedback recording with a personal salutation, and to explicitly explain the purpose and structure of the feedback comments - namely, that they would focus on only a few key ideas for improvement. Together, these design features are likely to have influenced students\u2019 understanding of the purpose of the feedback."
        },
        {
            "heading": "Conclusion",
            "text": "This exploratory study adds to the growing literature that confirms the value of technology enhanced feedback on assessment, particularly in terms of it being perceived as clearer, more useful, and more satisfying. However, this investigation also proposes that the positive perceptions of audiovisual comments were likely to have also been influenced by the increased focus on actionable and personable comments. This is in alignment with Mahoney, Macfarlane and Ajjawi (2019) who note in their literature review that while audiovisual modes appear promising, they need to be coupled with careful feedback design. While the results from this study showed a positive impact, it is in keeping with the exploratory nature of this study to treat such results critically. Further research now needs to be conducted to understand the complex relationship between the affordances of the media, the instructional content and its structure, as well as the ecology of the individual participants including student preference and educator experience."
        },
        {
            "heading": "Brown, S., & Knight, P. (1994). Assessing Learners in Higher Education. London: Kogan Page. Borup, J., Graham, C. & Velasquez, A. (2011). The Use of Asynchronous Video Communication to Improve Instructor Immediacy and Social Presence in a Blended Learning Environment. In A. Kitchenham (Ed) Blended Learning Across",
            "text": "Disciplines: Models for Implementation, p.38-57. Hershey: IGI Global. https://doi.org/10.4018/978-1-60960-479-0.ch003\nBoud, D. & Molloy, E. (2013). What is the problem with feedback? In D. Boud and E. Molloy (Eds.), Feedback in Higher and Professional Education: Understanding it and Doing it Well. New York, NY: Routledge.\nCarless, D. & Boud, D. (2018). The development of student feedback literacy: enabling uptake of feedback. Assessment and Evaluation in Higher Education, 43(8), 1315-1325. https://doi.org/10.1080/02602938.2018.1463354\nChang, N., Watson, A. B., Bakerson, M. A., Williams, E. E., McGoron, F. X., & Spitzer, B. (2012). Electronic feedback or handwritten feedback: What do undergraduate students prefer and why? Journal of Teaching and Learning with Technology, 1(1), 1-23.\nCostello, J., & Crane, D. (2010). Providing Learner-centered Feedback Using a Variety of Technologies. Paper presented at the International Conference on the Liberal Arts, St. Thomas University, Fredericton, New Brunswick. http://w3.stu.ca/stu/ academic/departments/social_work/pdfs/CostelloandCrane.pdf\nCrisp, B. (2007). Is it worth the effort? How feedback influences students' subsequent submission of assessable work. Assessment and Evaluation in Higher Education, 32(5), 571-581. https://doi.org/10.1080/02602930601116912\nDaft, R. L., & Lengel, R. H. (1986). Organizational information requirements, media richness and structural design. Management Science, 32(5), 554-571. http://collablab.northwestern.edu/CollabolabDistro/nucmc/DaftAndLengelOrgInfoReq- MediaRichnessAndStructuralDesign-MngmtSci-1986.pdf https://doi.org/10.1287/mnsc.32.5.554\nDuncan, N. (2007). 'Feed-forward': Improving students' use of tutors' comments. Assessment and Evaluation in Higher Education, 32(3), 271. https://doi.org/10.1080/02602930600896498\nField, A. (2009). Discovering Statistics using SPSS (3rd Edition). London: Sage.\nHattie, J. (2009). Visible learning: A synthesis of 800+ meta-analyses on achievement. Abingdon: Routledge.\nHattie, J., & Clarke, S. (2018). Visible Learning: Feedback. London: Routledge. https://doi.org/10.4324/9780429485480\nASCILITE 2019 Singapore University of Social Sciences 270\nHattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81-112. https:// doi.org/10.3102/003465430298487 Henderson, M., & Phillips, M. (2014). Technology enhanced feedback on assessment. Paper presented at the Australian Computers in Education Conference 2014, Adelaide, SA. http://acec2014.acce.edu.au Henderson, M., & Phillips, M. (2015). Video-based feedback on student assessment: scarily personal. Australasian Journal of Educational Technology, 31(1), 51-66. https://doi.org/10.14742/ajet.1878 Higgins, R., Hartley, P., & Skelton, A. (2001). Getting the message across: The problem of communicating assessment feedback. Teaching in Higher Education, 6(2), 269-274. https://doi.org/10.1080/13562510120045230 Knauf, H. (2016). Reading, listening and feeling: audio feedback as a component of an inclusive learning culture at universities. Assessment & Evaluation in Higher Education, 41(3), 442-449. https:// doi.org/10.1080/02602938.2015.1021664 Lizzio, A., & Wilson, K. (2008). Feedback on assessment: Students' perceptions of quality and effectiveness. Assessment & Evaluation in Higher Education, 33(3), 263-275. https://doi.org/10.1080/02602930701292548 Luongo, N. (2015). Missing the chalkboard: Using screencasting in the online classroom. Computers in the Schools, 32(2), 144-151. https://doi.org/10.1080/07380569.2015.1030968 Mahoney, P., Macfarlane, S. & Ajjawi, R. (2019). A qualitative synthesis of video feedback in higher education. Teaching in Higher Education, 24(2), 157-179, https://doi.org/10.1080/13562517.2018.1471457 McCarthy, J. (2015). Evaluating written, audio and video feedback in higher education summative assessment tasks. Issues in Educational Research, 25(2), 153-169. http://www.iier.org.au/iier25/mccarthy.pdf McConnell, D. (2006). Assessing learning in e-groups and communities. In D. McConnell (Ed.), E-Learning Groups and communities. Maidenhead: Open University Press. Molloy, E., Borrell-Carrio, F., & Epstein, R. (2013). The Impact of Emotions in Feedback. In Feedback in Higher and Professional Education: Understanding It and Doing It Well, edited by D. Boud and E. Molloy, 50-71. London: Routledge. Morris, C., & Chikwa, G. (2016). Audio versus written feedback: Exploring learners' preference and the impact of feedback format on students' academic performance. Active Learning in Higher Education, 17, 125-137. https:// doi.org/10.1177/1469787416637482 Phillips, M., Ryan, T., & Henderson, M. (2017). A cross-disciplinary evaluation of digitally recorded feedback in higher education. In H. Partridge (Ed.), Me, Us, IT! Proceedings ASCILITE2017: 34th International Conference on Innovation, Practice and Research in the Use of Educational Technologies in Tertiary Education (pp. 364-361). Phillips, M., Henderson, M., & Ryan, T. (2016). Multimodal feedback is not always clearer, more useful or satisfying. In S. Barker, S. Dawson, A. Pardo, & C. Colvin (Eds.), Show Me The Learning. Proceedings ASCILITE 2016 Adelaide (pp. 514-522). Pitt, E., & Norton, L. (2017). 'Now That's the Feedback I Want!' students' Reactions to Feedback on Graded Work and What They Do with It. Assessment & Evaluation in Higher Education, 42(4), 499-516. https:// doi.org/10.1080/02602938.2016.1142500 Ryan T, Henderson M and Phillips M (2019) Feedback modes matter: comparing perceptions of digital and non- digital feedback modes in higher education. British Journal of Educational Technology 50(3): 1507-1523. https://doi.org/10.1111/ bjet.12749 Thompson, R., & Lee, M. J. (2012). Talking with students through screencasting: Experimentations with video feedback to improve student learning. Journal of Interactive Technology and Pedagogy, 1(1), 1-16. https://jitp.commons.gc.cuny.edu/ talking-with-students-through-screencasting-experimentations-with-video- feedback-to-improve-student-learning/ West, J., & Turner, W. (2016). Enhancing the assessment experience: Improving student perceptions, engagement and understanding using online video feedback. Innovations in Education and Teaching International, 53(4), 400 - 410. https:// doi.org/10.1080/14703297.2014.1003954 Winstone, N. E., Nash, R., Parker, M., & Rowntree, J. (2017). Supporting Learners' Agentic Engagement with Feedback: A Systematic Review and a Taxonomy of Recipience Processes. Educational Psychologist 52 (1), 17-37. https:// doi.org/10.1080/00461520.2016.1207538 Winstone, N., & Carless, D. (2019). Designing Effective Feedback Processes in Higher Education: A Learning- Focused Approach. London: Routledge. https://doi.org/10.4324/9781351115940\nASCILITE 2019 Singapore University of Social Sciences 271\nAppendix"
        },
        {
            "heading": "The Feedback Attitudes Survey",
            "text": "Thank you for taking part in this survey, which has been designed to investigate the impact of feedback on assessment tasks.\nPlease indicate how much you agree or disagree with the following statements\u2026\n1. The [recorded/text-based] feedback that you received on your most recent assessment task for [insert name of subject]\u2026\nStrongly disagree Disagree\nNeither disagree nor agree Agree Strongly agree\nUsed language that was easy to understand o o o o o Had a clear message o o o o o Was confusing o o o o o Provided constructive comments that you could use to improve your work o o o o o Improved your confidence for completing future assessment tasks o o o o o Was useful o o o o o\n2. How satisfied were you with the [recorded/text-based] feedback you received for your most recent assessment task for [insert name of subject]?\no Extremely dissatisfied o Dissatisfied o Neither dissatisfied nor satisfied o Satisfied o Extremely satisfied\nPlease cite as: Ryan, T., Phillips, M. & Henderson, M. (2019). From text to audiovisual feedback: enhancing clarity, usefulness and satisfaction. In Y. W. Chew, K. M. Chan, and A. Alphonso (Eds.), Personalised Learning. Diverse Goals. One Heart. ASCILITE 2019 Singapore (pp. 264-271)."
        }
    ],
    "title": "From text to audiovisual feedback: enhancing clarity, usefulness and satisfaction",
    "year": 2021
}