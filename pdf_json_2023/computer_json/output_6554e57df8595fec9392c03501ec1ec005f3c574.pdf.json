{
    "abstractText": "We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. We conduct experiments on various sentence-level and span-level tasks. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all spanlevel tasks and baselines that combine recursive networks with Transformers on natural language inference tasks. More interestingly, the hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiang Hu"
        },
        {
            "affiliations": [],
            "name": "Qingyang Zhu"
        },
        {
            "affiliations": [],
            "name": "Kewei Tu"
        },
        {
            "affiliations": [],
            "name": "Wei Wu"
        }
    ],
    "id": "SP:42d4ccff2b0cde52347267649be5242a9a13ab11",
    "references": [
        {
            "authors": [
                "James K. Baker"
            ],
            "title": "Trainable grammars for speech recognition",
            "venue": "Journal of the Acoustical Society of America,",
            "year": 1979
        },
        {
            "authors": [
                "dlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Francisco Casacuberta"
            ],
            "title": "Statistical estimation of stochastic context-free grammars using the insideoutside algorithm and a transformation on grammars",
            "venue": "Grammatical Inference and Applications, Second International Colloquium,",
            "year": 1994
        },
        {
            "authors": [
                "Jihun Choi",
                "Kang Min Yoo",
                "Sang-goo Lee"
            ],
            "title": "Learning to compose task-specific tree structures",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "Modeling hierarchical structures with continuous recursive neural networks",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "Efficient beam tree recursion, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Minnesota",
                "June"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/N19-1116.",
            "year": 2019
        },
        {
            "authors": [
                "California",
                "June"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/N16-1024.",
            "year": 2016
        },
        {
            "authors": [
                "Haitao Mi",
                "Liang Li",
                "Gerard de Melo"
            ],
            "title": "Fast-r2d2: A pretrained recursive neural",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Kong",
                "Kewei Tu"
            ],
            "title": "A multi-grained self-interpretable symbolic-neural model",
            "year": 2022
        },
        {
            "authors": [
                "June"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/N19-1114. URL",
            "year": 2019
        },
        {
            "authors": [
                "Jean Maillard",
                "Stephen Clark",
                "Dani Yogatama"
            ],
            "title": "Jointly learning sentence embeddings and syntax with unsupervised tree-lstms",
            "venue": "CoRR, abs/1705.09189,",
            "year": 2017
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Beatrice Santorini",
                "Mary Ann Marcinkiewicz"
            ],
            "title": "Building a large annotated corpus of English: The Penn Treebank",
            "venue": "Computational Linguistics,",
            "year": 1993
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "H. Barbara"
            ],
            "title": "Partee. Lexical semantics and compositionality",
            "year": 1995
        },
        {
            "authors": [
                "Jordan B. Pollack"
            ],
            "title": "Recursive distributed representations",
            "venue": "Artif. Intell.,",
            "year": 1990
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "Beam tree recursive cells",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Sartran",
                "Samuel Barrett",
                "Adhiguna Kuncoro",
                "Milos Stanojevic",
                "Phil Blunsom",
                "Chris Dyer"
            ],
            "title": "Transformer grammars: Augmenting transformer language models with syntactic inductive biases at scale",
            "venue": "Trans. Assoc. Comput. Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Yikang Shen",
                "Shawn Tan",
                "Alessandro Sordoni",
                "Aaron C. Courville"
            ],
            "title": "Ordered neurons: Integrating tree structures into recurrent neural networks",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yikang Shen",
                "Yi Tay",
                "Che Zheng",
                "Dara Bahri",
                "Donald Metzler",
                "Aaron C. Courville"
            ],
            "title": "Structformer: Joint unsupervised induction of dependency and constituency structure from masked language modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Shubham Toshniwal",
                "Haoyue Shi",
                "Bowen Shi",
                "Lingyu Gao",
                "Karen Livescu",
                "Kevin Gimpel"
            ],
            "title": "A cross-task analysis of text span representations",
            "venue": "In Proceedings of the 5th Workshop on Representation Learning for NLP, pp. 166\u2013176,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Ralph Weischedel",
                "Martha Palmer",
                "Mitchell Marcus",
                "Eduard Hovy",
                "Sameer Pradhan",
                "Lance Ramshaw",
                "Nianwen Xue",
                "Ann Taylor",
                "Jeff Kaufman",
                "Michelle Franchini"
            ],
            "title": "OntoNotes release 5.0 LDC2013T19",
            "venue": "Linguistic Data Consortium,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u2019emi Louf",
                "Morgan Funtowicz",
                "Jamie Brew"
            ],
            "title": "HuggingFace\u2019s Transformers: State-of-the-art",
            "venue": "Natural Language Processing. arXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Songlin Yang",
                "Yanpeng Zhao",
                "Kewei Tu"
            ],
            "title": "Neural bi-lexicalized PCFG induction",
            "venue": "Virtual Event, August",
            "year": 2021
        },
        {
            "authors": [
                "Songlin Yang",
                "Wei Liu",
                "Kewei Tu"
            ],
            "title": "Dynamic programming in rank space: Scaling structured inference with low-rank HMMs and PCFGs",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Dani Yogatama",
                "Phil Blunsom",
                "Chris Dyer",
                "Edward Grefenstette",
                "Wang Ling"
            ],
            "title": "Learning to compose words into sentences with reinforcement learning",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Yu Zhang",
                "Houquan Zhou",
                "Zhenghua Li"
            ],
            "title": "Fast and accurate neural CRF constituency parsing",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent years have witnessed a plethora of breakthroughs in the field of natural language processing (NLP), thanks to the advances of deep neural techniques such as Transformer (Vaswani et al., 2017), BERT (Devlin et al., 2019), and GPTs (Brown et al., 2020; OpenAI, 2023). The basis of the breakthroughs lies in the multi-layer multi-head self-attention mechanism, which is also the key innovation in Transformer models. With the mechanism, Transformer models have exhibited strong performance on a variety of downstream tasks and powerful representation ability over multi-modal data. Despite the success of the architecture, semantics, and structures of language in Transformer models are represented in an implicit and entangled form, which is somewhat divergent from the desiderata of linguistics. From a linguistic point of view, the means to understand natural language should comply with the principle that \u201cthe meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined\u201d (Partee, 1995). Hence, it is preferable to model hierarchical structures of natural language in an explicit fashion. Indeed, explicit structure modeling could enhance interpretability (Hu et al., 2023), and result in better compositional generalization (Sartran et al., 2022). The problem is then how to combine the ideas of Transformers and explicit hierarchical structure modeling, so as to obtain a model that has the best of both worlds.\nIn this work, we attempt to answer the question by proposing a novel Contextual Inside-Outside (CIO) layer as an augmentation to the Transformer architecture and name the model \u201cRecursive\n1Preprint. Code will be released at https://github.com/ant-research/StructuredLM_ RTDT once the paper is officially accepted.\n\u2217Corresponding authors.\nar X\niv :2\n30 9.\n16 31\n9v 1\n[ cs\n.C L\n] 2\n8 Se\np 20\n23\nComposition Augmented Transformer\u201d (ReCAT). Figure 1 illustrates the architecture of ReCAT. In a nutshell, ReCAT stacks several CIO layers between the embedding layer and the deep self-attention layers in Transformer. These layers explicitly emulate the hierarchical composition process of language and provide Transformers with explicit multi-grained representations. Specifically, inspired by DIORA (Drozdov et al., 2019), we propose a variant of the inside-outside algorithm with contextualization ability, which serves as the backbone of the CIO layer. As illustrated in Figure 1(a), the stacked CIO layers learn contextualized representations of spans by encoding an input sequence in a bottom-up and top-down manner layer-by-layer. During the bottom-up pass, representations of high-level constituents are composed of low-level constituents via a dynamic programming approach (Maillard et al., 2017); while during the top-down pass, each span is represented by merging information from itself, its siblings, and its parents. By this means, the stacked CIO layers are able to induce syntactic structures of the input and provide contextualized constituent representations to the subsequent Transformer layers, where constituents at different levels can sufficiently communicate with each other via the self-attention mechanism. ReCAT enjoys several advantages over existing methods: first, unlike Transformer Grammars (Sartran et al., 2022), our model gets rid of the dependence on expensive parse tree annotations from human experts, and is able to recover hierarchical syntactic structures in an unsupervised manner; second, unlike DIORA (Drozdov et al., 2019) and R2D2 (Hu et al., 2021), our model breaks the restriction that information exchange only happens either inside or outside a span and never across the span boundaries, and realizes cross-span communication in a scalable way; third, we reduce the complexity of the inside-outside algorithm proposed in DIORA from cubic to linear. Thus, the CIO layers can go sufficiently deep, and the whole ReCAT model can be pre-trained with vast amounts of data.\nWe evaluate ReCAT on GLUE (Wang et al., 2019) (sentence-level tasks), OntoNotes (Weischedel et al., 2013) (span-level tasks), and PTB (Marcus et al., 1993) (grammar induction). The empirical results indicate that: (1) ReCAT attains superior performance compared to Transformer-only baselines on all span-level tasks with an average improvement over 3%; (2) ReCAT surpasses the hybrid models composed of RvNNs and Transformers by 5% on natural language inference; and (3) ReCAT outperforms previous RvNN based baselines by 8% on grammar induction. Our main contributions are three-fold: 1. We propose a Contextual Inside-Outside (CIO) layer, which can be jointly pre-trained with Transformers efficiently, induce underlying syntactic structures of text, and learn contextualized multi-grained representations. 2. We further propose ReCAT, a novel architecture that achieves direct communication among constituents at different levels by combining CIO layers and Transformers. 3. We conduct comprehensive experiments and verify the effectiveness of the proposed model on various tasks."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "The idea of equipping deep neural architectures with syntactic structures has been explored in many studies. Depending on whether gold parse trees are required, existing work can be categorized into two groups. In the first group, gold trees are part of the hypothesis of the methods. For example, Pollack (1990) proposes encoding the hierarchical structures of text with an RvNN; Socher et al. (2013) verify the effectiveness of RvNNs with gold trees for sentiment analysis; RNNG (Dyer et al., 2016) and Transformer Grammars (Sartran et al., 2022) perform text generation with syntactic knowledge. Though rendering promising results, these works suffer from scaling issues due to the expensive\nand elusive nature of linguistic knowledge from human experts. To overcome the challenge, the other group of work induces the underlying syntactic structures in an unsupervised manner. For instance, ON-LSTM Shen et al. (2019b) and StructFormer (Shen et al., 2021) integrate syntactic structures into LSTMs or Transformers by masking information in differentiable ways; Yogatama et al. (2017) propose jointly training the shift-reduce parser and the sentence embedding components; URNNG (Kim et al., 2019b) applies variational inference over latent trees to perform unsupervised optimization of the RNNG; Chowdhury & Caragea (2023) propose a Gumbel-Tree (Choi et al., 2018) based approach that can induce tree structures and cooperate with Transformers as a flexible module by feeding contextualized terminal nodes into Transformers; Maillard et al. (2017) propose an alternative approach based on a differentiable neural inside algorithm; Drozdov et al. (2019) propose DIORA, an auto-encoder-like pre-trained model based on an inside-outside algorithm (Baker, 1979; Casacuberta, 1994); and R2D2 (Hu et al., 2021) reduces the complexity of the neural inside process by pruning, which facilitates exploitation of complex composition functions and pre-training with large corpus. Our work is inspired by DIORA and R2D2, and the major innovation lies in the introduction of inter-span and intra-span communications that breaks the information access constraint (i.e., information flow is only allowed within (inside) or among (outside) spans). By this means, our model encourages sufficient interactions of constituents at different levels and forms contextualized multi-grained representations beneficial to downstream tasks."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 MODEL",
            "text": "Overall, there are two basic components in ReCAT: CIO layers and Transformer layers. As illustrated in Figure 1, the CIO layers take token embeddings as inputs and output binary trees with node representations. As each node is fully contextualized within the CIO layers, its representation includes both semantic and relative position information, and thus no extra encoding is required for the subsequent Transformer layers. The multi-grained representations are then fed into the Transformer layers where constituents at different levels can interact with each other directly."
        },
        {
            "heading": "3.1.1 LINEAR NEURAL INSIDE ALGORITHM",
            "text": "Hu et al. (2022) propose a pruned neural inside algorithm within linear complexity. The algorithm consists of two stages: pruning and encoding. During the pruning pass, the primary purpose is to decide which cells are unnecessary to encode, build the encoding order B of the rest cells, and record their valid splits K as explained below. As shown in Figure 2, there is a pruning threshold m which is triggered when the height of encoded cells (in grey) in the chart table reaches m. At the beginning, cells beneath m are added bottom-up to B row by row. Once triggered, the pruning process (refer to (a),(b),(c),(d) in Figure 2) works as follows:\n1. Pick two adjacent cells and merge them into one, e.g., s1 and s2 in (b), s5 and s6 in (d). 2. Remove all conflicting cells that would break the now non-splittable span from Step 1, e.g.,\nthe dark cells in (c), and reorganize the chart table much like in the Tetris game as in (d). 3. Append the cells that just descend to height m to B and record their valid splits in K, e.g.,\nthe cell highlighted with stripes in (d) whose valid splits are {2, 3}. Then go back to Step 1 until no cells are left.\nHu et al. (2022) propose a learn-able top-down parser based on LSTM to recursively split a sentence to obtain a binary parse tree by selecting split points in the descending order of their scores v, as\nshown in Figure 2(e). Thus the reverse order of the split points could be used to decide which cells to merge in Step 1. The optimization of the parser is explained in the next section."
        },
        {
            "heading": "3.1.2 CONTEXTUAL INSIDE-OUTSIDE LAYERS",
            "text": "In this part, we explain contextual insideoutside layers, and how to extend linear neural inside algorithm to our contextual insideoutside.\nOur contextual inside-outside algorithm borrows the neural inside-outside idea from DIORA (Drozdov et al., 2019), but differs from it in the following ways. First, we enable the model to mix information inside and outside a span and learn real contextualized representa-\ntions. Moreover, we transform the non-repeatable inside-outside process into a repeatable one, which allows us to stack multiple CIO layers to refine span representations layer by layer. As a result of introducing information leakage, the original pre-training objective doesn\u2019t work anymore. We discuss how to address the issue in Section 3.2. Denotations. Given a sentence S = {s1, s2, ..., sn} with n tokens, Figure 3 shows multi-layered chart tables. We denote the chart table at the l-th layer as T l, where each cell T li,j covers span (i, j) corresponding to sub-string si:j . For span (i, j), its set of inside pair consists of span pairs {(i, k), (k + 1, j)} for k = i, ..., j \u2212 1. For span (i, j), its set of outside pair consists of span pairs {(i, k), (j+1, k)} for k = j+1, ..., n and {(k, j), (k, i\u22121)} for k = 1, ..., i\u22121. For example, given a sentence {s1, s2, s3, s4}, the inside pairs of span (1, 3) are {(1, 1), (2, 3)} and {(1, 2), (3, 3)}, and the outside pairs of span (2, 3) are {(1, 3), (1, 1)} and {(2, 4), (4, 4)}. We denote the representations computed during the inside pass and the outside pass as e\u0302 and e\u030c respectively. Pruned Inside Pass. During the inside process at the l-th layer, given an inside pair of a span (i, j) split on k s.t. i \u2264 k < j, we denote its composition representation and compatibility score as e\u0302li,j [k] and a l i,j [k] respectively. The compatibility score means how likely an inside pair is to be merged. e\u0302li,j [k] is computed by applying the composition function to its immediate sub-span representations e\u0302li,k and e\u0302 l k+1,j . Meanwhile, the composition function additionally takes the outside representation e\u030cl\u22121i,j from the previous layer to refine the representation, as illustrated in Figure 3. ali,j [k] is computed by recursively summing up the scores of the single step compatibility score a\u0304li,j [k] and the corresponding immediate sub-spans, i.e., a l i,k and a l k+1,j . The inside score a l i,j and representation e\u0302li,j of each cell Ti,j are computed using a soft weighting over all possible inside pairs, i.e., ali,j [k] and e\u0302 l i,j [k] with k \u2208 Ki,j , where K is the set of valid splits for each span generated during the pruning stage. For the l-th layer, the inside representation and score of a cell are computed as follows:\ne\u0302li,j [k] = Compose l \u03b1(e\u030c l\u22121 i,j , e\u0302 l i,k, e\u0302 l k+1,j) , a\u0304 l i,j [k] = \u03d5\u03b1(e\u0302 l i,k, e\u0302 l k+1,j) , a l i,j [k] = a\u0304 l i,j [k] + a l i,k + a l k+1,j ,\nw\u0302li,j [k] = exp(ai,j [k])\u2211\nk\u2032\u2208Ki,j exp(ai,j [k \u2032])\n, e\u0302li,j = \u2211\nk\u2208Ki,j\nw\u0302li,j [k]e\u0302 l i,j [k] , a l i,j = \u2211 k\u2208Ki,j w\u0302li,j [k]a l i,j [k]\n(1) We elaborate on details of the Compose function and the compatibility function \u03d5 afterwards. For a bottom cell at the first chart table T 1i,i, we initialize e\u03021i,i as the embedding of si and a1i,i as zero. For T 0, we assign a shared learn-able tensor to e\u030c0i,j . We encode the cells in the order of B so that when computing the representations and scores of a span, its inside pairs needed for the computation are already computed. Contextual Outside Pass. A key difference in our outside pass is that we mix information from both inside and outside of a span. Meanwhile, to reduce the complexity of the outside to linear, we only consider cells and splits used in the inside pass. Analogous to inside, we denote the outside representation and score of a given span as e\u030cli,j [k] and b l i,j [k] respectively, whose parent span is (i, k) or (k, j) for k > j or k < i. Then, e\u030cli,j [k] and b l i,j [k] can be computed by:\ne\u030c l i,j [k] =\n{ Composel\u03b2(e\u030c l i,k, e\u0302 l i,j , e\u0302 l j+1,k)\nComposel\u03b2(e\u030c l k,j , e\u0302 l k,i\u22121, e\u0302 l i,j)\n, b\u0304 l i,j [k] =\n{ \u03d5\u03b2(e\u030c l i,k, e\u0302 l j+1,k)\n\u03d5\u03b2(e\u030c l k,j , e\u0302 l k,i\u22121)\n, b l i,j [k] =\n{ alj+1,k + b\u0304 l i,j [k] + b l i,k\nalk,i\u22121 + b\u0304 l i,j [k] + b l k,j\n, for k > j for k < i (2)\nAccording to K that records all valid splits for each span during the pruned inside pass, we can obtain a mapping from a span to its immediate sub-spans. By reversing such mapping, we get a mapping from a span to its valid immediate parent spans denoted as P , which records the nonoverlapping endpoint k in the parent span (i, k) or (k, j) for a given span (i, j). Thus, we have:\nw\u030cli,j [k] = exp(bli,j [k])\u2211\nk\u2032\u2208Pi,j exp(b l i,j [k\n\u2032]) , e\u030cli,j = \u2211 k\u2208Pi,j w\u030cli,j [k]e\u030c l i,j [k] , b l i,j = \u2211 k\u2208Pi,j w\u030cli,j [k]b l i,j [k]. (3)\nBy following the reverse order of B, we can ensure that when computing e\u030ci,j [k], the outside representation of parent span (i, j) needed for the computation are already computed.2\nComposition and Compatibility functions. We borrow the idea from R2D2 (Hu et al., 2021) to use a single-layered Transformer as the composition function. As shown in Figure 4, there are three special tokens indicating the role of each input. During the inside pass, we feed e\u030cl\u22121i,j , e\u0302 l i,k, e\u0302 l k+1,j summed up with their role embeddings into the Transformer and take the output of e\u030cl\u22121i,j as e\u0302 l i,j [k] as shown in Figure 4(a). Analogously, we can get the outside representation as shown in Figure 4(b). To\ncompute the compatibility score, we feed e\u0302li,k and e\u0302 l k+1,j through two MLPs separately and calculate their scaled inner product:\nu\u0302li,k = MLP L \u03b1 (e\u0302 l i,k) , u\u0302 l k+1,j = MLP R \u03b1 (e\u0302 l k+1,j) , \u03d5\u03b1(e\u0302 l i,k, e\u0302 l k+1,j) = (u\u0302 l i,k) \u22bau\u0302lk+1,j/ \u221a d, (4)\nwhere u \u2208 Rd. \u03d5\u03b2(e\u030cli,k, e\u0302lj+1,k) and \u03d5\u03b2(e\u030clk,j , e\u0302lk,i\u22121) can be computed in a similar way. For efficiency considerations, \u03d5\u03b1 and \u03d5\u03b2 used in different layers share the same parameters.\nTree induction. For a given span (i, j) at the l-th CIO layer, the best split point is k with the highest ali,j [k], k \u2208 Ki,j . Thus it is straightforward to recover the best parsing tree by recursively selecting the best split point starting from the root node T L1,n in a top-down manner, where L is the total number of CIO layers.\nMulti-grained self-attention. Once we recover the best parsing tree as described above (c.f., Figure 1(c)), we feed outside representations of nodes from the last CIO layer into subsequent Transformers, to enable constituents at different semantic levels to communicate directly and output multigrained representations fully contextualized with other spans."
        },
        {
            "heading": "3.2 PRE-TRAINING",
            "text": "As our contextual outside pass brings information leakage, the original training objective designed for DIORA does not work anymore. Instead, here we use the MLM objective. For simplicity, we use the vanilla MLM objective instead of its more advanced variants such as SpanBERT (Joshi et al., 2020). Since each masked token corresponds to a terminal node in the parsing tree, we just take the Transformer outputs corresponding to these nodes to predict the masked tokens. As masking tokens may bring extra difficulty to the top-down parser, we make all tokens visible to the parser during the pruning stage and only mask tokens during the encoding stage.\nParser feedback. We use a hard-EM approach (Liang et al., 2017) to optimize the parser used for the pruning. Specifically, we select the best binary parsing tree z estimated by the last CIO layer. As shown in Figure 2(e), at t step, the span corresponding to a given split at is determined, which is denoted as (it, jt). Thus we can maximize the log-likelihood of the parser as follows:\np(at|S) = exp(vat)\u2211jt\u22121 k=it exp(vk) , log p(z|S) = n\u22121\u2211 t=1 log p(at|S). (5)\n2Detailed parallel implementation could be found in the Appendix A.1."
        },
        {
            "heading": "3.3 FAST ENCODING",
            "text": "Though we reduce the complexity of inside-outside to linear, the computational load is still many times higher than that of the vanilla Transformer model. Specifically, the complexity of a single CIO layer is O(m2n), where n is the sentence length and m is the pruning threshold which is set to 2 by default. To further speed up the model during the finetuning and evaluation stage, we follow the force encoding proposed in Fast-R2D2. Specifically, we directly use the pretrained top-down parser to predict a parse tree for a given sentence and apply the trained Compose functions to compute contextualized representations of nodes following the tree and feed them into the Transformer.\n4 EXPERIMENTS System # params Fast-R2D2 52M Fast-R2D2+Transformer 67M DIORA*+Transformer 77M Parser+Transformer 142M Transformer\u00d73 46M Transformer\u00d76 67M Transformer\u00d79 88M ReCATnoshare[1, 1, 3] 63M ReCATshare[1, 3, 3] 70M ReCATshare[3, 1, 3] 70M ReCATnoshare[3, 1, 3] 88M\nTable 1: Parameter sizes.\nWe compare ReCAT with vanilla Transformers and recursive models through extensive experiments on a variety tasks.\nData for Pre-training. For English, we pre-train our model and baselines on WikiText103 (Merity et al., 2017). WikiText103 is split at the sentence level, and sentences longer than 200 after tokenization are discarded (about 0.04\u2030 of the original data). The total number of tokens left is 110M.\nReCAT setups. To conduct comprehensive experiments on ReCAT, we prepared different configurations for ReCAT. We use ReCATshare/noshare to indicate whether the Compose functions\nused in the inside and outside passes share the same parameters or not. Meanwhile, we use ReCAT[i, j, k] to denote ReCAT made up of i stacked CIO layers, a j-layer Compose function, and a k-layer Transformer.\nHyperparameters. For all baselines, we use the same vocabulary and hyperparameters but different layer numbers. We follow the setting in Devlin et al. (2019), using 768-dimensional embeddings, a vocabulary size of 30522, 3072-dimensional hidden layer representations, and 12 attention heads as the default setting for the Transformer. The top-down parser of our model uses a 4-layer bidirectional LSTM with 128-dimensional embeddings and a 256-dimensional hidden layer. The pruning threshold m is set to 2. Training is conducted using Adam optimization with weight decay using a learning rate of 5 \u00d7 10\u22125 for the tree encoder and 1 \u00d7 10\u22123 for the top-down parser. Input tokens are batched by length, which is set to 10240. We pre-train all models on 8 A100 GPUs.\nBaselines For fair comparisons, we select baselines whose parameter sizes are close to ReCAT with different configurations. We list the parameter sizes of different models in Table 1. Details of baseline models are explained in the following sections."
        },
        {
            "heading": "4.1 SPAN-LEVEL TASKS",
            "text": "Dataset. We conduct experiments to evaluate contextualized span representations produced by our model on four classification tasks, namely Named entity labeling (NEL), Semantic role labeling (SRL), Constituent labeling (CTL), and Coreference resolution (COREF), using the annotated OntoNotes 5.0 corpus (Weischedel et al., 2013). For all the four tasks, the model is given an input sentence and the position of a span or span pair, and the goal is to classify the target span(s) into the correct category. For NEL and CTL, models are given a single span to label, while for SRL and COREF, models are given a span pair.\nBaselines. We use Transformer and Fast-R2D2 as our baselines. Fast-R2D2 could be regarded as a model only with the inside pass and using a 4-layer Transformer as the Compose function. For reference, we also include BERT as a baseline which is pre-trained on a much larger corpus than our model. For BERT, we use the HuggingFace (Wolf et al., 2019) implementation and select the cased version of BERT-base. Given a span, R2D2-based models (our model and Fast-R2D2) can encode the representation directly, while for Transformer-based models, we try both mean pooling and max pooling in the construction of the span representation from token representations.\nFine-tuning details. Our model and the Transformers are all pre-trained on Wiki-103 for 30 epochs and fine-tuned on the four tasks respectively for 20 epochs. We feed span representations through a two-layer MLP using the same setting as in Toshniwal et al. (2020). For the downstream\nclassification loss, we use the cross entropy loss for single-label classification and binary cross entropy loss for multi-label classification. In particular, during the first 10 epochs of fine-tuning, inputs are also masked by 15% and the final loss is the summation of the downstream task loss, the parser feedback loss, and the MLM loss. For the last 10 epochs, we switch to the fast encoding mode as inputs are not masked anymore and the top-down parser is frozen. We use a learning rate of 5e\u22124 to tune the classification MLP, a learning rate of 5e\u22125 to tune the backbone span encoders, and a learning rate of 1e\u22123 to tune the LSTM top-down parser.\nResults and discussion. Table 2 shows the evaluation results. We can see that ReCAT outperforms the vanilla Transformers across all four tasks, with absolute improvements of around 4 F1 scores on average. Furthermore, though BERT has twice as many layers as our model and is pretrained on a corpus 20x larger than ours, ReCAT is still comparable with BERT (even better than BERT on the CTL task). The results verify our claim that explicitly modeling hierarchical syntactic structures can benefit tasks that require multi-granularity, with span-level tasks being one of them. Since Transformers only directly provide token-level representations, all span-level representations reside in hidden states in an entangled form, and this increases the difficulty of capturing intra-span and inter-span features. On the other hand, our model applies multi-layered self-attention layers on the disentangled contextualized span representations. This enables the construction of higher-order relationships among spans, which is crucial for tasks such as relation extraction and coreference resolution. In addition, we also observe that although Fast-R2D2 works well on NEL, its performance in other tasks is far less satisfactory. Fast-R2D2 performs particularly poorly in the SRL task, where our model improves the most. This is because Fast-R2D2 solely relies on local inside information, resulting in a lack of spatial and contextual information from the outside which is crucial for SRL. On the other hand, our model compensates for this deficiency by contextualizing the representations in an iterative up-and-down manner."
        },
        {
            "heading": "4.2 SENTENCE-LEVEL TASKS",
            "text": "Dataset. We use the GLUE (Wang et al., 2019) benchmark to evaluate the performance of our model on sentence-level language understanding tasks. The General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks.\nBaselines. We select Fast-R2D2, a DIORA variant, and vanilla Transformers as baselines which are all pre-trained on Wiki-103 with the same vocabulary. The Transformer baselines include Transformers with 3,6, and 9 layers corresponding to different parameter sizes of ReCAT with different configurations. The settings of the Fast-R2D2 and the Transformers are the same as in Section 4.1. The DIORA variant uses the linear inside-outside and a Transformer-based Compose function just as our model but still with the information access constraint. We also include an RvNN variant with a trained parser Zhang et al. (2020). The parser provides constituency trees and we use the encoder of Fast-R2D2 to compose constituent representations following the tree structure. The parser is trained with gold trees from Penn Treebank(PTB) (Marcus et al., 1993). Except for the RvNN+Parser baseline, all other models have no access to gold trees. For fair comparisons, we further combine Fast-R2D2, DIORA, and RvNN+Parser with Transformer by taking their node representations as the input of the Transformer and name the models Fast-R2D2+Transformer, DIORA+Transformer, and Parser+Transformer respectively. To study the gain brought by representations of non-terminal\nnodes, we include a baseline in which representations of non-terminal nodes in ReCAT are not fed into the subsequent Transformer, denoted as ReCATw/o NT . In addition, we list results of other RvNN variants such as GumbelTree Choi et al. (2018), OrderedMemory (Shen et al., 2019a), and CRvNN (Chowdhury & Caragea, 2021) for reference.\nFine-tuning details. Our model and the Transformer models are all pre-trained on Wiki-103 for 30 epochs and fine-tuned for 20 epochs. Regarding to the training scheme, we use the same masked language model warmup as in 4.1 for models pre-trained via MLM. The batch size for all models is 128. As the test dataset of GLUE is not published, we fine-tune all models on the training set and report the best performance (acc. by default) on the validation set.\nResults and discussion. Table 3 reports evaluation results on GLUE benchmark. First, we observe an interesting phenomenon that ReCAT significantly outperforms RvNN-based baselines on NLI tasks, underperforms the models on single-sentence tasks, and achieves comparable performance with them on sentence similarity tasks. Our conjecture is that the phenomenon arises from the contextualization and information integration abilities of the self-attention mechanism that enable the MLPs in pre-trained Transformers to retrieve knowledge residing in parameters in a better way. Therefore, for tasks requiring inference (i.e., knowledge beyond the literal text), ReCAT can outperform RvNN-based baselines. RvNN-based baselines perform well on tasks when the literal text is already helpful enough (e.g., single-sentence tasks and sentence similarity tasks), since they can sufficiently capture the semantics within local spans through explicit hierarchical composition modeling and deep bottom-up encoding. ReCAT, on the other hand, sacrifices such a feature in exchange for the contextualization ability that enables joint pre-training with Transformers, and thus suffers from degradation on single-sentence tasks. Second, even with only one CIO layer, ReCATnoshare[1, 1, 3] significantly outperforms Transformer\u00d73 on the MNLI task. When the number of CIO layers increases to 3, ReCATnoshare[3, 1, 3] even outperforms Transformer \u00d76. The results verify the effectiveness of explicit span representations introduced by the CIO layers. An explanation is that multi-layer self-attention over explicit span representations is capable of modeling intra-span and inter-span relationships, which are critical to NLI tasks but less beneficial to single-sentence and sentence similarity tasks. Third, stacking multiple CIO layers are helpful, as ReCATnoshare[3, 1, 3] significantly outperforms ReCATshare[1, 3, 3] and ReCATnoshare[1, 1, 3], even though ReCATnoshare[3, 1, 3] and ReCATshare[1, 3, 3] have almost the same number of parameters. We speculate that multiple CIO layers could enhance the robustness of the masked language models because the masked tokens may break the semantic information of inputs and thus affect the induction of syntactic trees, while multi-layer CIO could alleviate the issue by contextualization during multiple rounds of bottom-up and top-down operations. Fourth, representations of non-terminal nodes are useful. When we remove non-terminal node representations, the performance of ReCATw/o NTshare [3, 1, 3] drops on most tasks. Finally, we notice that ReACTnoshare[3, 1, 3] has a parameter size close to Transformer\u00d79 but underperforms Transformer\u00d79. This may stem\nmem. PTB Model cplx F1(\u00b5) Fast-R2D2m=4 O(n) 57.22 oursm=4/share O(n) 56.07 oursm=2/share O(n) 55.11 oursm=4/noshare O(n) 65.00 oursm=2/noshare O(n) 64.06 For Reference C-PCFG O(n3) 55.2\u2020 NBL-PCFG O(n3) 60.4\u2020 TN-PCFG O(n3) 64.1\u2020 ON-LSTM O(n) 47.7\u2021 S-DIORA O(n3) 57.6\u2020 StructFormer O(n2) 54.0\u2021\nModel NNP VP NP ADJP Fast-R2D2 83.44 63.80 70.56 68.47 oursm=2/share 77.22 67.05 66.53 69.44 oursm=4/share 85.41 66.14 68.43 72.92 oursm=2/noshare 80.36 64.38 80.93 80.96 oursm=4/noshare 81.71 70.55 82.94 78.28\nTable 4: Left table: F1 score of unsupervised parsing on PTB dataset. Values with \u2020 and \u2021 are taken from Yang et al. (2022) and Shen et al. (2021) respectively. Upper table: Recall of constituents. Word-level: NNP (proper noun). Phrase-level: VP (Verb Phrase), NP (Noun Phrase), ADJP (Adjective Phrase). Samples of deduced trees can be found in Appendix A.2.\nfrom the difference in the number of layers (i.e., 6 v.s. 9). The results imply that the increase in parameters of the independent Compose functions in the inside and outside pass cannot reduce the gap brought by the number of layers. It is noteworthy that Parser+Transformer is worse than FastR2D2+Transformer, especially on CoLA, indicating that syntactic information learned from a few human annotations is less useful than that obtained by learning from vast amounts of data."
        },
        {
            "heading": "4.3 STRUCTURE ANALYSIS",
            "text": "We further evaluate our model on unsupervised grammar induction and analyze the accuracy of syntactic trees induced by our model.\nBaselines & Datasets. For fair comparison, we select Fast-R2D2 pre-trained on Wiki-103 as the baseline. In both models, only the top-down parser after training is utilized during test. Some works have cubic complexity, and thus not feasible to pre-train over a large corpus. Therefore, we only report their performance on PTB (Marcus et al., 1993) for reference. These works include approaches specially designed for grammar induction based on PCFGs such as C-PCFG (Kim et al., 2019a), NBL-PCFG (Yang et al., 2021), and TN-PCFG (Yang et al., 2022), and approaches based on language modeling tasks to extract structures from the encoder such as S-DIORA (Drozdov et al., 2020), ON-LSTM (Shen et al., 2019b), and StructFormer (Shen et al., 2021).\nTraining details. We pre-train our model on Wiki-103 for 60 epochs, and then continue to train it on the training set of PTB for another 10 epochs. We randomly pick 3 seeds during the continuetrain stage, then select the checkpoints with the best performance on the validation set, and report their mean f1 on the test set. As our model takes word pieces as inputs, to align with other models with word-level inputs, we provide our model with word-piece boundaries as non-splittable spans.\nResults and discussions. Table 4 reports the evaluation results. ReCAT significantly outperforms most of the baseline models in terms of F1 score, and achieves comparable performance with TNPCFG which is specially designed for the task. Moreover, from the cases shown in Appendix A.2, one can see strong consistency between the structures induced by ReCAT and the gold trees obtained from human annotations. The results indicate that the CIO layers can accurately recover the syntactic structures of language, even though the model is learned through a fully unsupervised approach. The induced structures also greatly enhance the interpretability of Transformers, since each tensor participating in self-attention corresponds to an explicit constituent. We also notice a gap between ReCATshare and ReCATnoshare. To further understand the difference, we compute the recall of different constituents, as shown in the right table in Table 4. We find that both models work well on low-level constituents such as NNP, but ReCATnoshare is more performant on highlevel constituents such as ADJP and NP. Considering the intrinsic difference in the meaning of the Compose function for the inside and outside passes, non-shared Compose functions may cope with such discrepancy in a better way. Despite of this, when referring to Table 3, we do not see a significant difference between ReCATshare and ReCATnoshare on downstream tasks. A plausible reason might be that the self-attention mechanism mitigates the gap owing to the structural deficiencies in its expressive power. The pruning threshold m only has a limited impact on the performance of the model on grammar induction, which means that we can largely reduce the computational cost by decreasing m without significant loss on the learned structures."
        },
        {
            "heading": "5 CONCLUSION & LIMITATION",
            "text": "We propose ReCAT, a recursive composition augmented Transformer that combines Transformers and explicit recursive syntactic compositions. It enables constituents at different levels to communicate directly, and results in significant improvements on span-level tasks and the grammar induction task.\nThe application of ReCAT could be limited by its computational load during training. A straightforward method is to reduce the parameter size of CIO layers. Meanwhile, the limitation can be mitigated to a significant extent under the fast encoding mode during the fine-tuning and the inference stage, whose total cost is around 2\u00d7 as the cost of Transformers."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 PARALLELED IMPLEMENTATION FOR OUTSIDE\nUnlike the inside pass in which the count of the inside pairs for a given span is m at most, the count of the outside pairs for a given span is not determined. Thus this brings extra difficulty to compute weighted outside representations. To compute outside representations efficiently, we propose a cumulative update algorithm. We traverse span batches in the reverse order of B and then compute the outside representations for their immediate sub-spans and update their weighted outside representations by cumulative softmax as follows:w\u0304ti,k\nwti,k\n = Softmax( bcuml,t\u22121i,k\nbl,ti,k[ \u2322 p ] ) , bcuml,ti,k = log(exp(bcuml,t\u22121i,k ) + exp(b\u0304l,ti,k[\u2322p ])) c\u030cl,ti,k = w\u0304 t i,k c\u030c l,t\u22121 i,k + w t i,k c\u030c l i,k[ \u2322 p ] , bl,ti,k = w\u0304 t i,kb l,t\u22121 i,k + w t i,kbi,k[ \u2322 p ]\n(6)\nAs a sub-span may be updated many times, we denote the outside score at t-th time as b\u0304l,ti,k[ \u2322 p ]. We initialize bcuml,0i,k as \u2212\u221e. Thus once all parents are traversed, the final result of c\u030c l,t i,k will be equivalent to the result computed according to Equation 3.\nA.2 SAMPLES OF LEARNED CONSTITUENCY TREES"
        }
    ],
    "title": "AUGMENTING TRANSFORMERS WITH RECURSIVELY COMPOSED MULTI-GRAINED REPRESENTATIONS",
    "year": 2023
}