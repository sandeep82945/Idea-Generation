{
    "abstractText": "Computational imaging has been revolutionized by compressed sensing algorithms, which offer guaranteed uniqueness, convergence, and stability properties. Model-based deep learning methods that combine imaging physics with learned regularization priors have emerged as more powerful alternatives for image recovery. The main focus of this paper is to introduce a memory efficient model-based algorithm with similar theoretical guarantees as CS methods. The proposed iterative algorithm alternates between a gradient descent involving the score function and a conjugate gradient algorithm to encourage data consistency. The score function is modeled as a monotone convolutional neural network. Our analysis shows that the monotone constraint is necessary and sufficient to enforce the uniqueness of the fixed point in arbitrary inverse problems. In addition, it also guarantees the convergence to a fixed point, which is robust to input perturbations. We introduce two implementations of the proposed MOL framework, which differ in the way the monotone property is imposed. The first approach enforces a strict monotone constraint, while the second one relies on an approximation. The guarantees are not valid for the second approach in the strict sense. However, our empirical studies show that the convergence and robustness of both approaches are comparable, while the less constrained approximate implementation offers better performance. The proposed deep equilibrium formulation is significantly more memory efficient than unrolled methods, which allows us to apply it to 3D or 2D+time problems that current unrolled algorithms cannot handle.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aniket Pramanik"
        }
    ],
    "id": "SP:bccacf80f32fc9ee21eb4ae3cc162ffac27be6ea",
    "references": [
        {
            "authors": [
                "D. Gilton",
                "G. Ongie",
                "R. Willett"
            ],
            "title": "Deep equilibrium architectures for inverse problems in imaging",
            "venue": "IEEE Transactions on Computational Imaging, vol. 7, pp. 1123\u20131133, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Bai",
                "J.Z. Kolter",
                "V. Koltun"
            ],
            "title": "Deep equilibrium models",
            "venue": "Advances in Neural Information Processing Systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Martin",
                "C. Fowlkes",
                "D. Tal",
                "J. Malik"
            ],
            "title": "A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics",
            "venue": "Proc. 8th Int\u2019l Conf. Computer Vision, vol. 2, July 2001, pp. 416\u2013423.",
            "year": 2001
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Model-based deep learning, Monotone operator learning, Deep equilibrium models.\nI. INTRODUCTION\nThe recovery of images from a few noisy measurements is a common problem in several imaging modalities, including MRI [1], CT [2], PET [3], and microscopy [4]. In the undersampled setting, multiple images can give a similar fit to the measured data, making the recovery ill-posed. Compressive sensing (CS) algorithms pose the recovery as a convex optimization problem, where a strongly convex prior is added to the data-consistency term to regularize the recovery [5]. The main benefit of convex priors is in the uniqueness of the solutions. In particular, the strong convexity of the priors guarantees that the overall cost function in (3) is strongly convex, even when A operator has a large null space. Another desirable property of convex priors in CS is the robustness of the solution to input perturbations.\nAniket Pramanik and Mathews Jacob are from the Department of Electrical and Computer Engineering at the University of Iowa, Iowa City, IA, 52242, USA (e-mail: aniket-pramanik@uiowa.edu; mathewsjacob@uiowa.edu). M. Bridget Zimmerman is from the Department of Biostatistics at the University of Iowa, Iowa City, IA, 52242, USA (e-mail: bridget-zimmerman@uiowa.edu). This work is supported by grants NIH R01 AG067078 and R01 EB031169.\nIn recent years, several flavors of model-based deep learning algorithms, which combine imaging physics with learned priors, were introduced to significantly improve the performance compared to CS algorithms. For example, plug and play (PnP) methods use denoiser modules to replace the proximal mapping steps in CS algorithms [6]\u2013[11], and the algorithms are run until convergence. While earlier approaches chose off-theshelf denoisers such as BM3D [12], recent methods use pretrained convolutional neural network (CNN) modules [9], [10]. The pre-trained CNN modules that learn the image prior are agnostic to the forward model, which enables their use in arbitrary inverse problems. These methods come with convergence and uniqueness guarantees when the forward model is fullrank or the data term is strongly convex [13]. When the data term is not strongly convex, weaker convergence guarantees are available [9], but uniqueness is not guaranteed. Another category of approaches relies on unrolled optimization; these algorithms unroll finite number of iterative optimization steps in CS algorithms to obtain a deep network, which is composed of CNN blocks and optimization blocks to enforce data consistency; the resulting deep network is trained in an end-to-end fashion [14]\u2013[17]. A key difference between unrolled methods and PnP methods is that the CNN block is trained end-toend, assuming a specific forward model; such model-based methods typically offer better performance than PnP methods that are agnostic to the forward model [14]\u2013[20]. Unlike PnP approaches that run the algorithm until convergence, the number of iterations in unrolled methods are restricted by the memory of the GPU devices during training; this often limits the applicability of unrolled algorithms to large-scale multidimensional problems. Several strategies were introduced to overcome the memory limitations of unrolled methods. For an unrolled network with N iterations and shared CNN modules across iterations, the computational complexity and memory demand of backpropagation areO(N) andO(N), respectively. The forward steps can be recomputed during backpropagation, which reduces the memory demand to O(1), while the computational complexity increases to O(N2). Forward checkpointing [21] saves the variables for every K layers during forward propagation, which reduces the computational demand to O(NK), while the memory demand is O(N/K). Reverse recalculation has been proposed to reduce the memory demand to O(1) and computational complexity to O(N) [22]. However, the approach in [22] requires multiple iterations to invert each CNN block, resulting in high computational complexity in practical applications.\nGilton et al. recently extended the deep equilibrium (DEQ) model [23] to significantly improve the memory demand\nar X\niv :2\n20 6.\n04 79\n7v 4\n[ cs\n.C V\n] 2\n8 Fe\nb 20\n23\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\n[24] of unrolled methods. Unlike unrolled methods, DEQ schemes run the iterations until convergence, similar to PnP algorithms. This property allows one to perform forward and backward propagation using fixed-point iteration involving a single physical layer, which reduces the memory demand to O(1), while the computational complexity is O(N); this offers better tradeoffs than the alternatives discussed above [21], [22]. The runtime of DEQ methods that are iterated until convergence are variable compared to unrolled methods, which use a finite number of iterations. In addition, the convergence of the iterative algorithm is crucial for the accuracy of backpropagation steps in DEQ, unlike in unrolled methods. Convergence guarantees were introduced in [10], [24] for the alternating direction method of multipliers (ADMM), proximal gradient (PG), and forward-backward DEQ algorithms. The convergence guarantees rely on restrictive conditions on the CNN denoising blocks, which are dependent on the forward models. Unfortunately, when the minimum singular value of the forward operator is small (e.g., highly accelerated parallel MRI) or zero (e.g., super-resolution), the CNN denoiser needs to be close to an identity operator for the iterations to converge. Another challenge associated with DEQ methods is the way the non-expansive constraints on the network are imposed. Most methods [10], [24] use spectral normalization of each layer of the network. Our experiments in Fig. 3 show that spectral normalization often translates to networks with lower performance. Another theoretical problem associated with current DEQ methods is the potential non-uniqueness of the fixed point, which can also affect the stability/robustness of the algorithm in the presence of input perturbations. We note that the stability of deep image reconstruction networks is a debated topic. While deep networks are reported to be more fragile to input perturbations than are conventional algorithms [25], some of the recent works have presented a more optimistic view [26], [27].\nThe main goal of this work is to introduce a modelbased DEQ algorithm that shares the desirable properties of convex CS algorithms, including guaranteed uniqueness of the fixed point solutions, convergence, and robustness to input perturbations. By enabling the training of the CNN modules in an end-to-end fashion, the proposed algorithm can match the performance of unrolled approaches while being significantly more memory efficient. The main contributions of this paper are:\n\u2022 We introduce a forward-backward DEQ algorithm (14) involving a learned network F . Existing algorithms [8], [16], [28] such as MoDL and RED are special cases of this algorithm when the damping parameter \u03b1 = 1. \u2022 We show that constraining the CNN module as an m > 0 monotone operator is necessary and sufficient to guarantee the uniqueness of the fixed point of the algorithm. Because the monotone constraint is central to our approach, we term the proposed scheme as the monotone operator learning (MOL) algorithm. \u2022 We show that an m-monotone operator F can be realized as a residual CNN: F = I \u2212 H\u03b8, where the Lipschitz constant of the denoiser module H\u03b8 is L[H\u03b8] = 1 \u2212m.\nWe also determine the range of values of \u03b1 and L[H\u03b8] for which the algorithm converges; the analysis and the experiments in Fig. 3 show that the direct application of the MoDL and RED (\u03b1 = 1) algorithms to the DEQ setting will diverge unless a highly constrained CNN (L[H\u03b8] < 0.24) is used, which restricts performance. By contrast, the use of a smaller \u03b1 translates to higher L[H\u03b8] and hence improved performance. \u2022 We theoretically analyze the worst-case sensitivity of the the resulting DEQ scheme. Our analysis shows that the norm of the perturbations in the reconstructed images are linearly proportional to the norm of the measurement perturbations, with the proportionality dependent on 1/m. \u2022 We introduce two implementations of the proposed MOL algorithm. The first approach uses spectral normalization to enforce the monotone constraint in the strict sense. We also introduce an approximate implementation, where we replace L[H\u03b8] by an approximation l[H\u03b8]. While the second approach does not satisfy the monotone constraint in the strict sense, our experiments in Fig. 3 shows that the resulting algorithm converges, while Fig. 6 shows that the robustness of both schemes to adversarial and Gaussian noise are comparable. We note that spectral normalization based estimate for Lipschitz constant is very conservative; our experiments in Fig. 4 show that the second approach offers improved performance over the exact approach. \u2022 We experimentally compare the performance against unrolled algorithms that use similar-sized CNNs in twodimensional MR imaging problems. Our results show that the performance of the MOL scheme is comparable to that of unrolled algorithms. In addition, the MOL scheme is associated with a ten-fold reduction in memory compared to the unrolling algorithms with ten unrolls. The significant gain in memory demand allows us to extend our algorithm to the 3D or 2D+time setting, where it offers improved performance over unrolled 2D approaches. The experimental results in Fig. 6 and 7 show the improved robustness of the proposed scheme compared to existing unrolled algorithms [16], [19] and UNET [29]. The recorded run-times in Table I show that MOL has higher computational complexity (\u2248 2.5 times) compared to unrolling algorithms due to more iterations, when compared with fixed number of unrolls in the latter. Our experiments in Fig. 6 show that the increased computational complexity translate to an improvement in robustness performance over unrolled algorithms, when Lipschitz regularization is applied on the networks."
        },
        {
            "heading": "II. BACKGROUND",
            "text": "We consider recovery of an image x from its noisy undersampled measurements b, specified by\nb = Ax + n, (1)\nwhere A is a linear operator and n \u223c N (0, \u03c32I) is an additive white Gaussian noise. The measurement model provides a conditional probability p(b|x) = N (Ax, \u03c32I). The maximum\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\na posteriori (MAP) estimation of x from the measurements b poses the recovery as\nxMAP = arg max x\nlog p(x|b). (2)\nUsing Bayes\u2019 rule, p(x|b) \u221d p(b|x)p(x) and from the monotonicity of the log function, one obtains\nxMAP = arg min x C(x),\nwhere C(x) = \u03bb\n2 \u2016Ax\u2212 b\u201622\n\ufe38 \ufe37\ufe37 \ufe38 D(x)=\u2212 log p(b|x)\n+ \u03c6(x)\ufe38\ufe37\ufe37\ufe38 \u2212 log p(x) . (3)\nHere, \u03bb = 1\u03c32 . The first term D(x) = \u2212 log p(b|x) is the data-consistency term, while the second term is the log prior. Compressed sensing algorithms use convex prior distributions (e.g., \u03c6(x) = \u2016x\u2016`1 ) to result in a strongly convex cost function with unique minimum.\nWe note that the minimum of (3) satisfies the fixed-point relation:\n\u03bbAH(Ax\u2212 b)\ufe38 \ufe37\ufe37 \ufe38 G(x) +\u2207x \u03c6(x)\ufe38 \ufe37\ufe37 \ufe38 F(x) = 0, (4)\nwhere AH is the Hermitian operator of A. We note that the first term G(x) is the noise in the measurements, translated to the image domain. When the above fixed-point relation holds, F(x) is essentially a noise estimator, often referred to as the score function. F points to the maximum of the prior p(x).\nSeveral algorithms that converge to the fixed point of (4) have been introduced in the CS setting [30]\u2013[32]. For example, forward-backward algorithms rewrite (4) as (I + \u03b1F)(x) = (I \u2212 \u03b1G)(x), \u03b1 > 0, which has the same fixed point as (4). Classical PG algorithms use the iterative rule xn+1 = (I + \u03b1F)\u22121(I \u2212 \u03b1G)(xn) that converges to the fixed point of (4). In the linear measurement setting (1), this translates to\nxn+1 = (I + \u03b1F)\u22121\ufe38 \ufe37\ufe37 \ufe38 prox\u03b1\u03c6 (xn \u2212 \u03b1\u03bbAH(Axn \u2212 b)\ufe38 \ufe37\ufe37 \ufe38 G(xn) ) (5)\nHere, prox\u03b1\u03c6 is the proximal operator of \u03c6."
        },
        {
            "heading": "A. Plug and play methods",
            "text": "The steepest descent update zn = xn \u2212 \u03b1\u03bbAH(Axn \u2212 b) improves the data consistency, while the proximal mapping xn+1 = (prox\u03b1\u03c6)(zn) in (5) can be viewed as denoising the current solution xn \u2212\u03b1\u03bbAH(Axn \u2212b), thus moving the iterate towards the maximum of prior p(x). Plug and play methods replace the proximal mapping with off-the-shelf or CNN denoisers H\u03b8 [6], [13], [17], [33]:\nxn+1 = H\u03b8 ( xn \u2212 \u03b1\u03bbAH(Axn \u2212 b) ) = T (xn, \u03b8) (6)\nHere, \u03b8 denotes the learnable parameters. There are PnP algorithms that use different optimization algorithms (e.g., ADMM, PG) with convergence guarantees to the fixed point x\u2217 = H\u03b8(x\u2217\u2212\u03b1G(x\u2217)) [13]. The solutions obtained by these approaches often do not have a one-to-one correspondence to the MAP setting in (3); they may be better understood from the consensus equilibrium setting [7]. See [13] for a detailed review of the PnP framework and associated convergence guarantees."
        },
        {
            "heading": "B. Unrolled algorithms",
            "text": "Unrolled optimization schemes [14]\u2013[16], [18] aim to learn a CNN denoiser, either as a prior or to replace the proximal mapping as in (6). A key difference with PnP is in the training strategy. The alternation between the physics-based data consistency (DC) update and the CNN update is unrolled for a finite number of iterations and trained end-to-end, while PnP methods alternate between the DC update and the pretrained CNN. Unrolled schemes learn the CNN parameters that offer improved reconstruction for a specific sampling operator A. These schemes obtain a deep network with shared CNN blocks. The parameters \u03b8 of the CNN are optimized to minimize the loss L = \u2211 i \u2016x\u0303\u03b8,i \u2212 xi\u201622, where xi are the ground truth images and x\u0303\u03b8,i are the output of the deep network. The main challenge of this scheme is the high memory demand of the unrolled algorithm, especially in higher-dimensional (e.g., 3D, 2D + time) settings. This is mainly due to memory required for backpropagation updates scaling linearly with the number of unrolls. While memory-efficient techniques [21], [22] have been proposed, these methods come at the cost of increased computational complexity during training. The choice of minimum number of unrolls to offer good performance is usually ten [15], [16], which is feasible for 2D problems. However, this approach is often infeasible for higher-dimensional applications (3D, 4D, 5D)."
        },
        {
            "heading": "C. Deep equilibrium models",
            "text": "To overcome the challenge associated with unrolled schemes, [24] adapted the elegant DEQ approach introduced in [23]. Deep equilibrium models assume that the forward iterations in (6) are run until convergence to the fixed point x\u2217 that satisfies x\u2217 = T (x\u2217, \u03b8). This approach allows one to compute the back-propagation steps using fixed-point iterations [23], [24] with just one physical layer.\nUnlike unrolled methods, the convergence of the forward and backward iterations are key to maintaining the accuracy of backpropagation in DEQ methods. The convergence of such algorithms are analyzed in [10], [24]. In the general case with a full-rank A considered in [10], convergence of the forward-backward splitting (FBS) algorithm in (6) is guaranteed when the Lipschitz constant of H\u03b8 satisfies L[H\u03b8] \u2264 2\u00b5min/(\u00b5max\u2212\u00b5min), where \u00b5max and \u00b5min are the maximum and minimum eigenvalues of AHA [10]. In many inverse problems (e.g., parallel MRI with high acceleration), A is ill conditioned \u00b5max >> \u00b5min. In these cases L[H\u03b8] should be close to zero to ensure convergence. Similar issues exist with PnP-ADMM and PnP-PROX, as discussed in [24]."
        },
        {
            "heading": "III. MONOTONE OPERATOR LEARNING",
            "text": "The main goal of this work is to introduce DEQ algorithms that share the desirable properties of CS algorithms, including uniqueness of the solution, robustness to input perturbations, and guaranteed fast convergence. We constrain F(x) to be an m-monotone (m > 0) CNN network to achieve these goals.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4"
        },
        {
            "heading": "A. Monotone Operators",
            "text": "We constrain the CNN module F to be m-monotone: Assumption: The operator F : CM \u2192 CM is m-monotone\nif:\n< (\u2329 x\u2212 y,F(x)\u2212F(y) \u232a) \u2265 m\u2016x\u2212 y\u201622, m > 0, (7)\nfor all x,y \u2208 CM . Here, <(\u00b7) denotes the real part. Monotone operators enjoy several desirable properties and have been carefully studied in the context of convex optimization algorithms [34], mainly due to its following relation with convex priors.\nLemma III.1. [9], [11] Let \u03c6 : CM \u2192 R+ be a proper, continuously differentiable, strongly convex function with m > 0. Then F = \u2207\u03c6 is an m-monotone operator.\nWhile derivatives of convex priors are monotone, the converse is not true in general. Our results show that the parameter m plays an important role in the convergence, uniqueness, and robustness of the algorithm to perturbations. In many CS applications, A often has a large null space, and hence the dataconsistency term is not strictly convex. The following result shows that constraining F to be m-monotone is necessary and sufficient to ensure the uniqueness of the fixed point of (4).\nProposition III.2. The fixed point of (4) is unique for a specific b, iff F is m-monotone with m > 0.\nThe proof is provided in the Appendix. The following result shows that monotone operators can be represented efficiently as a residual neural network.\nProposition III.3. If H\u03b8 : CM \u2192 CM is a (1\u2212m) Lipschitz function: L[H\u03b8] = (1\u2212m); 0 < m < 1. (8) Then, the residual function F : CM \u2192 CM\nF = I \u2212H\u03b8, (9) is m-monotone:\n< (\u3008F(x)\u2212F(y),x\u2212 y\u3009) \u2265 m\u2016x\u2212 y\u201622. (10)"
        },
        {
            "heading": "In addition, the Lipschitz constant of F is 2\u2212m:",
            "text": "\u2016F(x)\u2212F(y)\u20162 \u2264 (2\u2212m) \u2016x\u2212 y\u20162. (11) This result allows us to construct a monotone operator as a residual CNN. Because F is a score network that predicts the noise, H\u03b8 can be viewed as a denoiser."
        },
        {
            "heading": "B. Proposed iterative algorithm",
            "text": "We now introduce a novel forward-backward algorithm using m-monotone CNN F . To obtain an algorithm with guaranteed convergence even when A is low-rank, we swap F and G in (5):\nxn+1 = (I + \u03b1G)\u22121\ufe38 \ufe37\ufe37 \ufe38 prox\u03b1D (I \u2212 \u03b1F)(xn). (12)\nTherefore, this approach involves a gradient descent to improve the prior, followed by a proximal map of the data\nterm. A similar swapping approach was introduced in [28] to explain MoDL [16]. When F is an m-monotone operator, the Lipschitz constant of the gradient descent step (I \u2212 \u03b1F)(x) can be made lower than one as shown in Lemma IV.1, while that of prox\u03b1D is upper-bounded by one. This ensures that the resulting algorithm converges.\n<latexit sha1_base64=\"L5BQ1Uj+8FduLhePuqY7Dkt1ONU=\">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahbkoioi6Lbly2YB/QhHAzmbZDJw9mJkIJBTf+ihsXirj1J9z5N07aLLT1wIXDOfdy7z1+wplUlvVtlFZW19Y3ypuVre2d3T1z/6Aj41QQ2iYxj0XPB0k5i2hbMcVpLxEUQp/Trj++zf3uAxWSxdG9miTUDWEYsQEjoLTkmUdOCGpEgOOWlznAkxFMaw4JYnXmmVWrbs2Al4ldkCoq0PTMLyeISRrSSBEOUvZtK1FuBkIxwum04qSSJkDGMKR9TSMIqXSz2Q9TfKqVAA9ioStSeKb+nsgglHIS+rozv1guern4n9dP1eDazViUpIpGZL5okHKsYpwHggMmKFF8ogkQwfStmIxAAFE6tooOwV58eZl0zuv2Zd1uXVQbN0UcZXSMTlAN2egKNdAdaqI2IugRPaNX9GY8GS/Gu/Exby0Zxcwh+gPj8wd9+pds</latexit>Q\u21b5(\u00b7) <latexit sha1_base64=\"3L6ZGYXJzZcT0pDcJ1EWcQFoHAw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhN7JbDJmdmaZmRVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqeDG+v63t7K6tr6xWdgqbu/s7u2XDg4bRmWasjpVQulWhIYJLlndcitYK9UMk0iwZjS8nfrNJ6YNV/LBjlIWJtiXPOYUrZMaHRTpALulsl/xZyDLJMhJGXLUuqWvTk/RLGHSUoHGtAM/teEYteVUsEmxkxmWIh1in7UdlZgwE45n107IqVN6JFbalbRkpv6eGGNizCiJXGeCdmAWvan4n9fObHwdjrlMM8sknS+KM0GsItPXSY9rRq0YOYJUc3croQPUSK0LqOhCCBZfXiaN80pwWQnuL8rVmzyOAhzDCZxBAFdQhTuoQR0oPMIzvMKbp7wX7937mLeuePnMEfyB9/kDjXmPHg==</latexit>\u21b5\n<latexit sha1_base64=\"mfUmzCvfklPy/awVtadA17703HQ=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBiyURUY9FLx4r2A9oQ5lsN+3SzSbuboQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dgorq2vrG8XN0tb2zu5eef+gqeNUUdagsYhVO0DNBJesYbgRrJ0ohlEgWCsY3U791hNTmsfywYwT5kc4kDzkFI2V2t5ZF0UyxF654lbdGcgy8XJSgRz1Xvmr249pGjFpqECtO56bGD9DZTgVbFLqppolSEc4YB1LJUZM+9ns3gk5sUqfhLGyJQ2Zqb8nMoy0HkeB7YzQDPWiNxX/8zqpCa/9jMskNUzS+aIwFcTEZPo86XPFqBFjS5Aqbm8ldIgKqbERlWwI3uLLy6R5XvUuq979RaV2k8dRhCM4hlPw4ApqcAd1aAAFAc/wCm/Oo/PivDsf89aCk88cwh84nz9oQ4+Q</latexit>\n1 \u21b5<latexit sha1_base64=\"jwupRAfu/2d3ZTOTU7v5fhYDZbM=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkRdVl047KCfUA7lEx6pw3NZMYkUyxDv8ONC0Xc+jHu/BvTdhbaeiBwOOde7skJEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dJwqhnUWi1i1AqpRcIl1w43AVqKQRoHAZjC8nfrNESrNY/lgxgn6Ee1LHnJGjZX8TkTNIAjJUzeTk26p7FbcGcgy8XJShhy1bumr04tZGqE0TFCt256bGD+jynAmcFLspBoTyoa0j21LJY1Q+9ks9IScWqVHwljZJw2Zqb83MhppPY4COzkNqRe9qfif105NeO1nXCapQcnmh8JUEBOTaQOkxxUyI8aWUKa4zUrYgCrKjO2paEvwFr+8TBrnFe+y4t1flKs3eR0FOIYTOAMPrqAKd1CDOjB4hGd4hTdn5Lw4787HfHTFyXeO4A+czx/haJIt</latexit>xn <latexit sha1_base64=\"S9MYzV0MvceZrTGjg4+aPAZFdhs=\">AAAB9HicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLoxmUF+4AmlJvJpB06eTgzKZTQ73DjQhG3fow7/8Zpm4W2Hhg4nHMP987xU8GVtu1vq7S2vrG5Vd6u7Ozu7R9UD4/aKskkZS2aiER2fVRM8Ji1NNeCdVPJMPIF6/iju5nfGTOpeBI/6knKvAgHMQ85RW0kz0WRDtEVJhBgv1qz6/YcZJU4BalBgWa/+uUGCc0iFmsqUKmeY6fay1FqTgWbVtxMsRTpCAesZ2iMEVNePj96Ss6MEpAwkebFmszV34kcI6UmkW8mI9RDtezNxP+8XqbDGy/ncZppFtPFojATRCdk1gAJuGRUi4khSCU3txI6RIlUm54qpgRn+curpH1Rd67qzsNlrXFb1FGGEziFc3DgGhpwD01oAYUneIZXeLPG1ov1bn0sRktWkTmGP7A+fwDPyZIh</latexit> \u21b5\n<latexit sha1_base64=\"5lSUjfYjxLW5v/qoJXlNPaE8Plo=\">AAAB+nicbVDLSgMxFM3UV62vqS7dBItQN2VGRF0WBXFZwT6gM5RMJtOGZpIhyShl7Ke4caGIW7/EnX9jpp2Fth4IHM65l3tygoRRpR3n2yqtrK6tb5Q3K1vbO7t7dnW/o0QqMWljwYTsBUgRRjlpa6oZ6SWSoDhgpBuMr3O/+0CkooLf60lC/BgNOY0oRtpIA7vqxUiPMGLwpu7hUOiTgV1zGs4McJm4BamBAq2B/eWFAqcx4RozpFTfdRLtZ0hqihmZVrxUkQThMRqSvqEcxUT52Sz6FB4bJYSRkOZxDWfq740MxUpN4sBM5kHVopeL/3n9VEeXfkZ5kmrC8fxQlDKoBcx7gCGVBGs2MQRhSU1WiEdIIqxNWxVTgrv45WXSOW245w337qzWvCrqKINDcATqwAUXoAluQQu0AQaP4Bm8gjfryXqx3q2P+WjJKnYOwB9Ynz8wMJNO</latexit>F(\u00b7) <latexit sha1_base64=\"/KAcK4GWAd+ULrSoJeyHWMTDu6Q=\">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1gEQSiJiLosunFZwT6gDWEynbRDJ5MwMxFr6Je4caGIWz/FnX/jpM1CWw8MHM65l3vmBAlnSjvOt1VaWV1b3yhvVra2d3ar9t5+W8WpJLRFYh7LboAV5UzQlmaa024iKY4CTjvB+Cb3Ow9UKhaLez1JqBfhoWAhI1gbyber/QjrURCiRz8Tp+7Ut2tO3ZkBLRO3IDUo0PTtr/4gJmlEhSYcK9VznUR7GZaaEU6nlX6qaILJGA9pz1CBI6q8bBZ8io6NMkBhLM0TGs3U3xsZjpSaRIGZzGOqRS8X//N6qQ6vvIyJJNVUkPmhMOVIxyhvAQ2YpETziSGYSGayIjLCEhNtuqqYEtzFLy+T9lndvai7d+e1xnVRRxkO4QhOwIVLaMAtNKEFBFJ4hld4s56sF+vd+piPlqxi5wD+wPr8ATiIks4=</latexit>xn+1\n<latexit sha1_base64=\"Cn8SI8qgiPhT2BMPwVbu9cXwAbs=\">AAAB/nicbVDLSsNAFL2pr1pfUXHlZrAIrkoioi6rbrqsYB/QxjKZTtqhk0mYmQglFPwVNy4Ucet3uPNvnLQRtPXAwOGce7lnjh9zprTjfFmFpeWV1bXiemljc2t7x97da6ookYQ2SMQj2faxopwJ2tBMc9qOJcWhz2nLH91kfuuBSsUicafHMfVCPBAsYARrI/Xsg26I9dAP0NV9Df1wv2eXnYozBVokbk7KkKPesz+7/YgkIRWacKxUx3Vi7aVYakY4nZS6iaIxJiM8oB1DBQ6p8tJp/Ak6NkofBZE0T2g0VX9vpDhUahz6ZjILqOa9TPzP6yQ6uPRSJuJEU0Fmh4KEIx2hrAvUZ5ISzceGYCKZyYrIEEtMtGmsZEpw57+8SJqnFfe84t6elavXeR1FOIQjOAEXLqAKNahDAwik8AQv8Go9Ws/Wm/U+Gy1Y+c4+/IH18Q3dzpTI</latexit> AHb\n<latexit sha1_base64=\"1FvSpsZDzRk/aU+luYbrSurljxY=\">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahbkoioi6LbrqsYB/QhDCZTNuhkwczN0IJBTf+ihsXirj1J9z5N07aLLT1wIXDOfdy7z1+IrgCy/o2Siura+sb5c3K1vbO7p65f9BRcSopa9NYxLLnE8UEj1gbOAjWSyQjoS9Y1x/f5n73gUnF4+geJglzQzKM+IBTAlryzCMnJDCiROCmlzkwYkCmNYcGMZx5ZtWqWzPgZWIXpIoKtDzzywlimoYsAiqIUn3bSsDNiAROBZtWnFSxhNAxGbK+phEJmXKz2Q9TfKqVAA9iqSsCPFN/T2QkVGoS+rozv1gtern4n9dPYXDtZjxKUmARnS8apAJDjPNAcMAloyAmmhAqub4V0xGRhIKOraJDsBdfXiad87p9WbfvLqqNmyKOMjpGJ6iGbHSFGqiJWqiNKHpEz+gVvRlPxovxbnzMW0tGMXOI/sD4/AGIvJdz</latexit>H\u2713(\u00b7)\n(a) Iterative Rule\n<latexit sha1_base64=\"WE0t3ZZSsuEAGWK5cMROI5e+bi4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9ltQT0WvXisYD+gXUo2zbax2WRJsmJZ+h+8eFDEq//Hm//GdLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW8tEEdoikkvVDbCmnAnaMsxw2o0VxVHAaSeY3Mz9ziNVmklxb6Yx9SM8EixkBBsrtevoCdXRoFxxq24GtEq8nFQgR3NQ/uoPJUkiKgzhWOue58bGT7EyjHA6K/UTTWNMJnhEe5YKHFHtp9m1M3RmlSEKpbIlDMrU3xMpjrSeRoHtjLAZ62VvLv7n9RITXvkpE3FiqCCLRWHCkZFo/joaMkWJ4VNLMFHM3orIGCtMjA2oZEPwll9eJe1a1buoene1SuM6j6MIJ3AK5+DBJTTgFprQAgIP8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwDPqY34</latexit> 3 x 3 <latexit sha1_base64=\"bkbSifDDi7xv87RW90jqtkkn2zY=\">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKezmoB6DuXiMYB6QLGF2MpsMmccyMxsIIb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vcLW9s7uXnG/dHB4dHxSPj1rG5VqQltEcaW7ETaUM0lblllOu4mmWEScdqJJI/M7U6oNU/LJzhIaCjySLGYE20xqKDkdlCt+1V8CbZIgJxXI0RyUv/pDRVJBpSUcG9ML/MSGc6wtI5wuSv3U0ASTCR7RnqMSC2rC+fLWBbpyyhDFSruSFi3V3xNzLIyZich1CmzHZt3LxP+8Xmrju3DOZJJaKslqUZxyZBXKHkdDpimxfOYIJpq5WxEZY42JdfGUXAjB+subpF2rBjfV4LFWqd/ncRThAi7hGgK4hTo8QBNaQGAMz/AKb57wXrx372PVWvDymXP4A+/zBwn9jjw=</latexit> Conv\n<latexit sha1_base64=\"AWqAFNb1FhuaUM451nTe28Vw5ZI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBZBEErSg3osevHYgq2FNpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewtr6xuVXcLu3s7u0flA+P2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvp35D0+oNI/lvZkk6Ed0KHnIGTVWal70yxW36s5BVomXkwrkaPTLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5odOyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhtZ9xmaQGJVssClNBTExmX5MBV8iMmFhCmeL2VsJGVFFmbDYlG4K3/PIqadeq3mXVa9Yq9Zs8jiKcwCmcgwdXUIc7aEALGCA8wyu8OY/Oi/PufCxaC04+cwx/4Hz+AHLdjLM=</latexit>\n+ <latexit sha1_base64=\"O8tev5oQ4VYjq0gBTTp7aCqkYSo=\">AAAB63icbVA9T8MwEHXKVylfBUYWiwqJqUo6AGMFCwNDQaSt1EaV415aq7YT2Q5SFfUvsDCAECt/iI1/g9NmgJYnnfT03p3u7oUJZ9q47rdTWlvf2Nwqb1d2dvf2D6qHR20dp4qCT2Meq25INHAmwTfMcOgmCogIOXTCyU3ud55AaRbLRzNNIBBkJFnEKDG59AB3/qBac+vuHHiVeAWpoQKtQfWrP4xpKkAayonWPc9NTJARZRjlMKv0Uw0JoRMygp6lkgjQQTa/dYbPrDLEUaxsSYPn6u+JjAitpyK0nYKYsV72cvE/r5ea6CrImExSA5IuFkUpxybG+eN4yBRQw6eWEKqYvRXTMVGEGhtPxYbgLb+8StqNundR9+4bteZ1EUcZnaBTdI48dIma6Ba1kI8oGqNn9IreHOG8OO/Ox6K15BQzx+gPnM8fq+2N/g==</latexit> ReLU\n<latexit sha1_base64=\"pFiz00bVitqVA/Z9TqWphHYRlFU=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0mKVI9FLx4r2A9IQ9lsJ+3SzSbsToRS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmObR4IhPdDZkBKRS0UKCEbqqBxaGETji+m/udJ9BGJOoRJykEMRsqEQnO0Ep+/YpGwDDTYPrlilt1F6DrxMtJheRo9stfvUHCsxgUcsmM8T03xWDKNAouYVbqZQZSxsdsCL6lisVgguni5Bm9sMqARom2pZAu1N8TUxYbM4lD2xkzHJlVby7+5/kZRjfBVKg0Q1B8uSjKJMWEzv+nA6GBo5xYwrgW9lbKR0wzjjalkg3BW315nbRrVa9e9R5qlcZtHkeRnJFzckk8ck0a5J40SYtwkpBn8kreHHRenHfnY9lacPKZU/IHzucPoPWQ1Q==</latexit> 64 features\n<latexit sha1_base64=\"WE0t3ZZSsuEAGWK5cMROI5e+bi4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9ltQT0WvXisYD+gXUo2zbax2WRJsmJZ+h+8eFDEq//Hm//GdLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW8tEEdoikkvVDbCmnAnaMsxw2o0VxVHAaSeY3Mz9ziNVmklxb6Yx9SM8EixkBBsrtevoCdXRoFxxq24GtEq8nFQgR3NQ/uoPJUkiKgzhWOue58bGT7EyjHA6K/UTTWNMJnhEe5YKHFHtp9m1M3RmlSEKpbIlDMrU3xMpjrSeRoHtjLAZ62VvLv7n9RITXvkpE3FiqCCLRWHCkZFo/joaMkWJ4VNLMFHM3orIGCtMjA2oZEPwll9eJe1a1buoene1SuM6j6MIJ3AK5+DBJTTgFprQAgIP8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwDPqY34</latexit> 3 x 3 <latexit sha1_base64=\"bkbSifDDi7xv87RW90jqtkkn2zY=\">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKezmoB6DuXiMYB6QLGF2MpsMmccyMxsIIb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vcLW9s7uXnG/dHB4dHxSPj1rG5VqQltEcaW7ETaUM0lblllOu4mmWEScdqJJI/M7U6oNU/LJzhIaCjySLGYE20xqKDkdlCt+1V8CbZIgJxXI0RyUv/pDRVJBpSUcG9ML/MSGc6wtI5wuSv3U0ASTCR7RnqMSC2rC+fLWBbpyyhDFSruSFi3V3xNzLIyZich1CmzHZt3LxP+8Xmrju3DOZJJaKslqUZxyZBXKHkdDpimxfOYIJpq5WxEZY42JdfGUXAjB+subpF2rBjfV4LFWqd/ncRThAi7hGgK4hTo8QBNaQGAMz/AKb57wXrx372PVWvDymXP4A+/zBwn9jjw=</latexit> Conv\n<latexit sha1_base64=\"AWqAFNb1FhuaUM451nTe28Vw5ZI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBZBEErSg3osevHYgq2FNpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewtr6xuVXcLu3s7u0flA+P2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvp35D0+oNI/lvZkk6Ed0KHnIGTVWal70yxW36s5BVomXkwrkaPTLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5odOyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhtZ9xmaQGJVssClNBTExmX5MBV8iMmFhCmeL2VsJGVFFmbDYlG4K3/PIqadeq3mXVa9Yq9Zs8jiKcwCmcgwdXUIc7aEALGCA8wyu8OY/Oi/PufCxaC04+cwx/4Hz+AHLdjLM=</latexit>\n+ <latexit sha1_base64=\"O8tev5oQ4VYjq0gBTTp7aCqkYSo=\">AAAB63icbVA9T8MwEHXKVylfBUYWiwqJqUo6AGMFCwNDQaSt1EaV415aq7YT2Q5SFfUvsDCAECt/iI1/g9NmgJYnnfT03p3u7oUJZ9q47rdTWlvf2Nwqb1d2dvf2D6qHR20dp4qCT2Meq25INHAmwTfMcOgmCogIOXTCyU3ud55AaRbLRzNNIBBkJFnEKDG59AB3/qBac+vuHHiVeAWpoQKtQfWrP4xpKkAayonWPc9NTJARZRjlMKv0Uw0JoRMygp6lkgjQQTa/dYbPrDLEUaxsSYPn6u+JjAitpyK0nYKYsV72cvE/r5ea6CrImExSA5IuFkUpxybG+eN4yBRQw6eWEKqYvRXTMVGEGhtPxYbgLb+8StqNundR9+4bteZ1EUcZnaBTdI48dIma6Ba1kI8oGqNn9IreHOG8OO/Ox6K15BQzx+gPnM8fq+2N/g==</latexit> ReLU\n<latexit sha1_base64=\"pFiz00bVitqVA/Z9TqWphHYRlFU=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0mKVI9FLx4r2A9IQ9lsJ+3SzSbsToRS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmObR4IhPdDZkBKRS0UKCEbqqBxaGETji+m/udJ9BGJOoRJykEMRsqEQnO0Ep+/YpGwDDTYPrlilt1F6DrxMtJheRo9stfvUHCsxgUcsmM8T03xWDKNAouYVbqZQZSxsdsCL6lisVgguni5Bm9sMqARom2pZAu1N8TUxYbM4lD2xkzHJlVby7+5/kZRjfBVKg0Q1B8uSjKJMWEzv+nA6GBo5xYwrgW9lbKR0wzjjalkg3BW315nbRrVa9e9R5qlcZtHkeRnJFzckk8ck0a5J40SYtwkpBn8kreHHRenHfnY9lacPKZU/IHzucPoPWQ1Q==</latexit> 64 features\n<latexit sha1_base64=\"WE0t3ZZSsuEAGWK5cMROI5e+bi4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9ltQT0WvXisYD+gXUo2zbax2WRJsmJZ+h+8eFDEq//Hm//GdLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW8tEEdoikkvVDbCmnAnaMsxw2o0VxVHAaSeY3Mz9ziNVmklxb6Yx9SM8EixkBBsrtevoCdXRoFxxq24GtEq8nFQgR3NQ/uoPJUkiKgzhWOue58bGT7EyjHA6K/UTTWNMJnhEe5YKHFHtp9m1M3RmlSEKpbIlDMrU3xMpjrSeRoHtjLAZ62VvLv7n9RITXvkpE3FiqCCLRWHCkZFo/joaMkWJ4VNLMFHM3orIGCtMjA2oZEPwll9eJe1a1buoene1SuM6j6MIJ3AK5+DBJTTgFprQAgIP8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwDPqY34</latexit> 3 x 3 <latexit sha1_base64=\"bkbSifDDi7xv87RW90jqtkkn2zY=\">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKezmoB6DuXiMYB6QLGF2MpsMmccyMxsIIb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vcLW9s7uXnG/dHB4dHxSPj1rG5VqQltEcaW7ETaUM0lblllOu4mmWEScdqJJI/M7U6oNU/LJzhIaCjySLGYE20xqKDkdlCt+1V8CbZIgJxXI0RyUv/pDRVJBpSUcG9ML/MSGc6wtI5wuSv3U0ASTCR7RnqMSC2rC+fLWBbpyyhDFSruSFi3V3xNzLIyZich1CmzHZt3LxP+8Xmrju3DOZJJaKslqUZxyZBXKHkdDpimxfOYIJpq5WxEZY42JdfGUXAjB+subpF2rBjfV4LFWqd/ncRThAi7hGgK4hTo8QBNaQGAMz/AKb57wXrx372PVWvDymXP4A+/zBwn9jjw=</latexit> Conv\n<latexit sha1_base64=\"AWqAFNb1FhuaUM451nTe28Vw5ZI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBZBEErSg3osevHYgq2FNpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewtr6xuVXcLu3s7u0flA+P2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvp35D0+oNI/lvZkk6Ed0KHnIGTVWal70yxW36s5BVomXkwrkaPTLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5odOyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhtZ9xmaQGJVssClNBTExmX5MBV8iMmFhCmeL2VsJGVFFmbDYlG4K3/PIqadeq3mXVa9Yq9Zs8jiKcwCmcgwdXUIc7aEALGCA8wyu8OY/Oi/PufCxaC04+cwx/4Hz+AHLdjLM=</latexit>\n+ <latexit sha1_base64=\"O8tev5oQ4VYjq0gBTTp7aCqkYSo=\">AAAB63icbVA9T8MwEHXKVylfBUYWiwqJqUo6AGMFCwNDQaSt1EaV415aq7YT2Q5SFfUvsDCAECt/iI1/g9NmgJYnnfT03p3u7oUJZ9q47rdTWlvf2Nwqb1d2dvf2D6qHR20dp4qCT2Meq25INHAmwTfMcOgmCogIOXTCyU3ud55AaRbLRzNNIBBkJFnEKDG59AB3/qBac+vuHHiVeAWpoQKtQfWrP4xpKkAayonWPc9NTJARZRjlMKv0Uw0JoRMygp6lkgjQQTa/dYbPrDLEUaxsSYPn6u+JjAitpyK0nYKYsV72cvE/r5ea6CrImExSA5IuFkUpxybG+eN4yBRQw6eWEKqYvRXTMVGEGhtPxYbgLb+8StqNundR9+4bteZ1EUcZnaBTdI48dIma6Ba1kI8oGqNn9IreHOG8OO/Ox6K15BQzx+gPnM8fq+2N/g==</latexit> ReLU\n<latexit sha1_base64=\"pFiz00bVitqVA/Z9TqWphHYRlFU=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0mKVI9FLx4r2A9IQ9lsJ+3SzSbsToRS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmObR4IhPdDZkBKRS0UKCEbqqBxaGETji+m/udJ9BGJOoRJykEMRsqEQnO0Ep+/YpGwDDTYPrlilt1F6DrxMtJheRo9stfvUHCsxgUcsmM8T03xWDKNAouYVbqZQZSxsdsCL6lisVgguni5Bm9sMqARom2pZAu1N8TUxYbM4lD2xkzHJlVby7+5/kZRjfBVKg0Q1B8uSjKJMWEzv+nA6GBo5xYwrgW9lbKR0wzjjalkg3BW315nbRrVa9e9R5qlcZtHkeRnJFzckk8ck0a5J40SYtwkpBn8kreHHRenHfnY9lacPKZU/IHzucPoPWQ1Q==</latexit> 64 features\n<latexit sha1_base64=\"WE0t3ZZSsuEAGWK5cMROI5e+bi4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9ltQT0WvXisYD+gXUo2zbax2WRJsmJZ+h+8eFDEq//Hm//GdLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW8tEEdoikkvVDbCmnAnaMsxw2o0VxVHAaSeY3Mz9ziNVmklxb6Yx9SM8EixkBBsrtevoCdXRoFxxq24GtEq8nFQgR3NQ/uoPJUkiKgzhWOue58bGT7EyjHA6K/UTTWNMJnhEe5YKHFHtp9m1M3RmlSEKpbIlDMrU3xMpjrSeRoHtjLAZ62VvLv7n9RITXvkpE3FiqCCLRWHCkZFo/joaMkWJ4VNLMFHM3orIGCtMjA2oZEPwll9eJe1a1buoene1SuM6j6MIJ3AK5+DBJTTgFprQAgIP8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwDPqY34</latexit> 3 x 3 <latexit sha1_base64=\"bkbSifDDi7xv87RW90jqtkkn2zY=\">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKezmoB6DuXiMYB6QLGF2MpsMmccyMxsIIb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vcLW9s7uXnG/dHB4dHxSPj1rG5VqQltEcaW7ETaUM0lblllOu4mmWEScdqJJI/M7U6oNU/LJzhIaCjySLGYE20xqKDkdlCt+1V8CbZIgJxXI0RyUv/pDRVJBpSUcG9ML/MSGc6wtI5wuSv3U0ASTCR7RnqMSC2rC+fLWBbpyyhDFSruSFi3V3xNzLIyZich1CmzHZt3LxP+8Xmrju3DOZJJaKslqUZxyZBXKHkdDpimxfOYIJpq5WxEZY42JdfGUXAjB+subpF2rBjfV4LFWqd/ncRThAi7hGgK4hTo8QBNaQGAMz/AKb57wXrx372PVWvDymXP4A+/zBwn9jjw=</latexit> Conv\n<latexit sha1_base64=\"AWqAFNb1FhuaUM451nTe28Vw5ZI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBZBEErSg3osevHYgq2FNpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewtr6xuVXcLu3s7u0flA+P2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvp35D0+oNI/lvZkk6Ed0KHnIGTVWal70yxW36s5BVomXkwrkaPTLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5odOyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhtZ9xmaQGJVssClNBTExmX5MBV8iMmFhCmeL2VsJGVFFmbDYlG4K3/PIqadeq3mXVa9Yq9Zs8jiKcwCmcgwdXUIc7aEALGCA8wyu8OY/Oi/PufCxaC04+cwx/4Hz+AHLdjLM=</latexit>\n+ <latexit sha1_base64=\"O8tev5oQ4VYjq0gBTTp7aCqkYSo=\">AAAB63icbVA9T8MwEHXKVylfBUYWiwqJqUo6AGMFCwNDQaSt1EaV415aq7YT2Q5SFfUvsDCAECt/iI1/g9NmgJYnnfT03p3u7oUJZ9q47rdTWlvf2Nwqb1d2dvf2D6qHR20dp4qCT2Meq25INHAmwTfMcOgmCogIOXTCyU3ud55AaRbLRzNNIBBkJFnEKDG59AB3/qBac+vuHHiVeAWpoQKtQfWrP4xpKkAayonWPc9NTJARZRjlMKv0Uw0JoRMygp6lkgjQQTa/dYbPrDLEUaxsSYPn6u+JjAitpyK0nYKYsV72cvE/r5ea6CrImExSA5IuFkUpxybG+eN4yBRQw6eWEKqYvRXTMVGEGhtPxYbgLb+8StqNundR9+4bteZ1EUcZnaBTdI48dIma6Ba1kI8oGqNn9IreHOG8OO/Ox6K15BQzx+gPnM8fq+2N/g==</latexit> ReLU\n<latexit sha1_base64=\"pFiz00bVitqVA/Z9TqWphHYRlFU=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0mKVI9FLx4r2A9IQ9lsJ+3SzSbsToRS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmObR4IhPdDZkBKRS0UKCEbqqBxaGETji+m/udJ9BGJOoRJykEMRsqEQnO0Ep+/YpGwDDTYPrlilt1F6DrxMtJheRo9stfvUHCsxgUcsmM8T03xWDKNAouYVbqZQZSxsdsCL6lisVgguni5Bm9sMqARom2pZAu1N8TUxYbM4lD2xkzHJlVby7+5/kZRjfBVKg0Q1B8uSjKJMWEzv+nA6GBo5xYwrgW9lbKR0wzjjalkg3BW315nbRrVa9e9R5qlcZtHkeRnJFzckk8ck0a5J40SYtwkpBn8kreHHRenHfnY9lacPKZU/IHzucPoPWQ1Q==</latexit> 64 features\n<latexit sha1_base64=\"WE0t3ZZSsuEAGWK5cMROI5e+bi4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9ltQT0WvXisYD+gXUo2zbax2WRJsmJZ+h+8eFDEq//Hm//GdLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW8tEEdoikkvVDbCmnAnaMsxw2o0VxVHAaSeY3Mz9ziNVmklxb6Yx9SM8EixkBBsrtevoCdXRoFxxq24GtEq8nFQgR3NQ/uoPJUkiKgzhWOue58bGT7EyjHA6K/UTTWNMJnhEe5YKHFHtp9m1M3RmlSEKpbIlDMrU3xMpjrSeRoHtjLAZ62VvLv7n9RITXvkpE3FiqCCLRWHCkZFo/joaMkWJ4VNLMFHM3orIGCtMjA2oZEPwll9eJe1a1buoene1SuM6j6MIJ3AK5+DBJTTgFprQAgIP8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwDPqY34</latexit> 3 x 3 <latexit sha1_base64=\"bkbSifDDi7xv87RW90jqtkkn2zY=\">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKezmoB6DuXiMYB6QLGF2MpsMmccyMxsIIb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vcLW9s7uXnG/dHB4dHxSPj1rG5VqQltEcaW7ETaUM0lblllOu4mmWEScdqJJI/M7U6oNU/LJzhIaCjySLGYE20xqKDkdlCt+1V8CbZIgJxXI0RyUv/pDRVJBpSUcG9ML/MSGc6wtI5wuSv3U0ASTCR7RnqMSC2rC+fLWBbpyyhDFSruSFi3V3xNzLIyZich1CmzHZt3LxP+8Xmrju3DOZJJaKslqUZxyZBXKHkdDpimxfOYIJpq5WxEZY42JdfGUXAjB+subpF2rBjfV4LFWqd/ncRThAi7hGgK4hTo8QBNaQGAMz/AKb57wXrx372PVWvDymXP4A+/zBwn9jjw=</latexit>\nConv <latexit sha1_base64=\"D08xPdD7WENdSA9jYqXtidkkIN8=\">AAAB8XicbVDLSgNBEOz1GeMr6tHLYBA8hd0c1GPQi8cI5oFJCLOT3mTI7Owy0yuEkL/w4kERr/6NN//GyeOgiQUNRVU33V1hqqQl3//21tY3Nre2czv53b39g8PC0XHdJpkRWBOJSkwz5BaV1FgjSQqbqUEehwob4fB26jee0FiZ6AcapdiJeV/LSApOTnosswg5ZQZtt1D0S/4MbJUEC1KEBardwle7l4gsRk1CcWtbgZ9SZ8wNSaFwkm9nFlMuhryPLUc1j9F2xrOLJ+zcKT0WJcaVJjZTf0+MeWztKA5dZ8xpYJe9qfif18oouu6MpU4zQi3mi6JMMUrY9H3WkwYFqZEjXBjpbmViwA0X5ELKuxCC5ZdXSb1cCi5LwX25WLlZxJGDUziDCwjgCipwB1WogQANz/AKb571Xrx372PeuuYtZk7gD7zPHyTxkJM=</latexit> 2 features\n<latexit sha1_base64=\"1FvSpsZDzRk/aU+luYbrSurljxY=\">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahbkoioi6LbrqsYB/QhDCZTNuhkwczN0IJBTf+ihsXirj1J9z5N07aLLT1wIXDOfdy7z1+IrgCy/o2Siura+sb5c3K1vbO7p65f9BRcSopa9NYxLLnE8UEj1gbOAjWSyQjoS9Y1x/f5n73gUnF4+geJglzQzKM+IBTAlryzCMnJDCiROCmlzkwYkCmNYcGMZx5ZtWqWzPgZWIXpIoKtDzzywlimoYsAiqIUn3bSsDNiAROBZtWnFSxhNAxGbK+phEJmXKz2Q9TfKqVAA9iqSsCPFN/T2QkVGoS+rozv1gtern4n9dPYXDtZjxKUmARnS8apAJDjPNAcMAloyAmmhAqub4V0xGRhIKOraJDsBdfXiad87p9WbfvLqqNmyKOMjpGJ6iGbHSFGqiJWqiNKHpEz+gVvRlPxovxbnzMW0tGMXOI/sD4/AGIvJdz</latexit>H\u2713(\u00b7)\n(b) CNN architecture Fig. 1. Figure (a) shows fixed-point iterative rule of the proposed MOL algorithm from (14) and (b) shows the architecture of the five-layer CNN H\u03b8(\u00b7) used in the experiments. When \u03b1 = 1, the approach reduces to the MoDL [16], which was originally introduced for unrolled optimization, or RED [8], which was introduced for PnP models. Our analysis shows that using \u03b1 = 1 in the DEQ setting requires highly constrained networks for convergence, which translates to poor performance.\nThe fixed points of the above relation are equal to the fixed points of (3) for all \u03b1 > 0. In the linear setting considered in (3), suppose we have p = (prox\u03b1D)(u) = (I + \u03b1G)\u22121u, which is the solution of p + \u03b1 (\u03bbAH(A(p)\u2212 b))\ufe38 \ufe37\ufe37 \ufe38\nG(p)\n= u, or\n(prox\u03b1D)(u) = ( I + \u03b1\u03bbAHA )\u22121 \ufe38 \ufe37\ufe37 \ufe38\nQ\u03b1\n( u + \u03b1\u03bbAHb ) (13)\nCombining with (9) and (12), we obtain the proposed MOL algorithm:\nxn+1 = (I + \u03b1\u03bbA HA)\u22121 ( (1\u2212 \u03b1)xn + \u03b1H\u03b8(xn) )\n\ufe38 \ufe37\ufe37 \ufe38 TMOL(xn)\n+\n(I + \u03b1\u03bbAHA)\u22121 ( \u03b1\u03bbAHb ) \ufe38 \ufe37\ufe37 \ufe38 z\n(14)\n= TMOL(xn) + z (15)\nWe show below that this iterative rule will converge to a unique fixed point x\u2217(b) specified by TMOL(x\u2217(b)) + z = x\u2217(b), which is identical to (4).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5"
        },
        {
            "heading": "C. Relation to existing algorithms",
            "text": "We now consider a special case, which has been introduced by other researchers. When \u03b1 = 1, the fixed-point algorithm in the iterative rule in (14) can be rewritten as\nxn+1 = ( I + \u03bbAHA )\u22121 (H\u03b8(xn) + \u03bbAHb ) . (16)\nWe note that the above update rule is used by multiple algorithms [16], [28], [8, equation (37)]. This update rule has been used in fixed-point RED algorithm (see [8, equation (37)]). Model-based deep learning (MoDL) [16] has trained H\u03b8 by unrolling the iterative algorithm with a fixed number of iterations; it did not require the iterative rule to converge. Our analysis shows that the corresponding DEQ algorithms will converge only if the Lipschitz constant of H\u03b8 is very low, which would translate to poor performance. The update rule in (14) can be viewed as a damped version of MoDL or the fixed-point RED algorithm. As will be seen in our analysis later, the use of the damping factor \u03b1 < 1 enables us to relax the constraints on the CNN network H\u03b8 that are needed for convergence, which will translate to improved performance. For both MoDL and the proposed fixed-point algorithms, the scalar \u03bb is kept trainable."
        },
        {
            "heading": "IV. THEORETICAL ANALYSIS",
            "text": "The monotone nature of F allows us to characterize the fixed point of the iterative algorithm xn+1 = TMOL (xn) + z in (14). In particular, we will now analyze the convergence and the robustness of the solution to input perturbations."
        },
        {
            "heading": "A. Convergence of the algorithm to a fixed point",
            "text": "The algorithm specified by (14) converges if the Lipschitz constant of\nTMOL(x) = Q\u03b1  x\u2212 \u03b1 (I \u2212H\u03b8)\ufe38 \ufe37\ufe37 \ufe38\nF\nx   = Q\u03b1 \u25e6 (I \u2212 \u03b1F)\ufe38 \ufe37\ufe37 \ufe38\nR\nx\n(17) is less than one. We will now focus on the composition Q\u03b1 \u25e6 R. When AHA is full-rank, Q\u03b1 = ( I + \u03b1\u03bbAHA\n)\u22121 is a contraction. In many inverse problems including superresolution and compressed sensing, the Lipschitz constant of Q\u03b1 is 1. Assuming that F is m-monotone, we have the following result for L[R]. Lemma IV.1. Let F : CM \u2192 CM be an m-monotone operator. Then, the operator R = (I \u2212 \u03b1F) has a Lipschitz constant of\nL[R] \u2264 \u221a\n1\u2212 2\u03b1m+ \u03b12(2\u2212m)2. (18) From the above relation, we note that R is a contraction\n(i.e., L[R] < 1) when the damping factor \u03b1 satisfies\n\u03b1 < 2m\n(2\u2212m)2 = \u03b1max. (19)\nProposition IV.2. Consider the algorithm specified by (14), where F is an m-monotone operator. Assume that (14) has a fixed point specified by x\u2217(b). Then,\n\u2016xn \u2212 x\u2217(b)\u20162 \u2264 (L[TMOL])n \u2016x0 \u2212 x\u2217(b)\u20162, (20)\nwhere L[TMOL] = 1(1+\u03bb\u00b5min) L[R]. Here, \u00b5min is the minimum eigenvalue of AHA and L[R] is specified by (18).\nWe note that TMOL being a contraction translates to geometric convergence with a factor of L[TMOL]; this is faster than the sublinear convergence rates [9], [11] available for ISTA [31] and ADMM [30] in the CS setting ( \u00b5min = 0).\nB. Benefit of damping parameter \u03b1 in the MOL algorithm (14)\nWe note from Section III-C that the algorithms [8], [16], [28] correspond to the special case of \u03b1 = 1. Setting \u03b1 = 1 in (19), we see from Lemma IV.1 and (19) that the DEQ algorithm will converge if\nm \u2265 3\u2212 \u221a 5 = 0.76 (21)\nor L[H\u03b8] < 0.24. As discussed previously, the denoising ability of a network is dependent on its Lipschitz bound; a smaller L[H\u03b8] bound translates to poor performance of the resulting MOL algorithm. The use of the damping factor \u03b1 < 1 allows us to use denoising networks H\u03b8 with larger Lipschitz bounds and hence improved denoising performance. For instance, if we choose m = 0.1;L[H\u03b8] = 0.9, from (19), the algorithm will converge if \u03b1 < 0.055."
        },
        {
            "heading": "C. Robustness of the solution to input perturbation",
            "text": "The following result shows that the robustness of the proposed algorithm is dependent on m or, equivalently, L[H\u03b8]. We note that the link between the Lipschitz bound on the network and sensitivity to perturbations is straightforward in a direct inversion scheme (e.g., UNET). By contrast, such relations are not available in the context of DEQ-based image recovery algorithms, to the best of our knowledge.\nProposition IV.3. Consider z1 and z2 to be measurements with \u03b4 = z2 \u2212 z1 as the perturbation. Let the corresponding outputs of the MOL algorithm be x\u2217(z1) and x\u2217(z2), respectively, with \u2206 = x\u2217(z2)\u2212 x\u2217(z1) as the output perturbation,\n\u2016\u2206\u20162 \u2264 \u03b1\u03bb/(1 + \u03bb\u00b5min)\n1\u2212 \u221a 1\u2212 2\u03b1m+ \u03b12(2\u2212m)2 \u2016\u03b4\u20162.\n(22)\nThe above result shows that the norm of the perturbation in the reconstructed images is linearly dependent on the norm of the input perturbations. The constant factor is a function of the monotonicity parameter m and the step size \u03b1 in Fig. 2, where we plot the constant term without the \u03bb parameter for different values of m. The plots show that the constant decreases with m roughly at 1/m rate. We note that the algorithm will converge to the same fixed point as long as the damping parameter \u03b1 satisfies the condition (19). Hence, we consider the case with small damping parameter \u03b1 \u2192 0 and set \u00b5min = 0 corresponding to the CS and super-resolution settings, when we obtain a simpler expression:\nlim \u03b1\u21920 \u2016\u2206\u20162 \u2264\n\u03bb m \u2016\u03b4\u20162. (23)\nThe above results show that the robustness of the algorithm is fundamentally related to m; a higher value of m translates to a\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nFig. 2. Plot of \u03b1 1\u2212 \u221a 1\u22122\u03b1m+\u03b12(2\u2212m)2 in (IV.3) vs m. Here, we choose \u03b1 = factor\u00d7\u03b1max, where \u03b1max is specified by (19) in the paper. We note that all the curves roughly decay with m with a 1/m decay rate. As \u03b1\u2192 0 or equivalently low values of factor, the curves approach 1/m.\nmore robust algorithm. However, note that the Lipschitz constant of the denoiserH\u03b8 is specified by L[H\u03b8] = 1\u2212m;m > 0. We need to choose a denoising network with a lower Lipschitz constant, which translates to lower performance, to make the resulting algorithm more robust to perturbations. There is a trade-off between robustness and performance of the algorithm, controlled by either the parameter m or the Lipschitz constant L[H\u03b8].\nV. IMPLEMENTATION DETAILS"
        },
        {
            "heading": "A. DEQ: forward and backward propagation",
            "text": "During training and inference, we use the forward iteration rule, xn+1 = TMOL(xn) + z. We terminate the algorithm at the nth iteration xn if it satisfies,\nen = \u2016xn \u2212 xn\u22121\u20162 \u2016xn\u22121\u20162 \u2264 \u03ba. (24)\nWe set \u03ba = 1 \u00d7 10\u22124 for the experiments. Please see the pseudo-code for forward propagation in Algorithm 1 of the supplementary material. We denote the fixed point of the algorithm as x\u2217(b), such that x\u2217(b) \u2248 TMOL (x\u2217(b)) + z. DEQ schemes [24] rely on fixed-point iterations for backpropagating the gradients. The details are shown in Algorithm 1 and 2 in the supplementary material. The iterations are evaluated until convergence, using similar termination conditions as in (24).\nB. Implementation of the monotone CNN operator\nWe note from (III.3) that a monotone F = I \u2212 H\u03b8 can be learned by constraining the Lipschitz constant of the denoiser network H\u03b8. We propose two different implementations of the MOL algorithm, which differs in the way the Lipschitz constraint is implemented.\n1) Spectral normalization: Similar to [35], we use normalization of the spectral norm of the CNN layers to constrain L[H\u03b8]. In particular, we bound the spectral norm of each layer to s \u221a L[H\u03b8], where s is the number of layers. We term this\nversion of MOL as MOL-SN (MOL-spectral normalization). This approach can guaranteeH\u03b8 to be a contraction, and hence the guarantees are satisfied exactly. However, the product of the spectral norms of the individual layers is a conservative estimate of the Lipschitz constant of the network. As shown by our experiments, the use of spectral normalization in our setting (MOL-SN) translates to lower performance. Another challenge with the spectral normalization approach is that it restricts the type of networks that can be used; architectures with skipped connections cannot be used with this strategy. We note that spectral normalization is indeed a conservative bound for the Lipschitz constant and hence may over constrain the network, translating to lower performance.\n2) Approximating monotone constraint using a Lipschitz penalty: Motivated by [36], we propose to train the MOL algorithm using a training loss which minimizes a constrained optimization problem. In [37], authors use Jacobian regularization instead to learn a contractive network. The estimation of the Lipschitz constant of H\u03b8 is posed as a maximization problem [36]:\nl [H\u03b8] = max x\u2208S P (x)\ufe37 \ufe38\ufe38 \ufe37 sup \u03b7\n\u2016H\u03b8(x + \u03b7)\u2212H\u03b8(x)\u201622 \u2016\u03b7\u201622\ufe38 \ufe37\ufe37 \ufe38 p(x,\u03b7)\n(25)\nWe denote the estimated Lipschitz constant as l [H\u03b8] to differentiate it from the true Lipschitz constant L [H\u03b8]. Here, \u03b7 is a perturbation, and S is the set of training data samples. We note that this estimate is less conservative than the one using spectral normalization. However, this is only an approximation of the true Lipschitz constant, even though our experiments show that the use of this estimate can indeed result in algorithms with convergence and robustness as predicted by the theory. We note that several researchers have recently introduced tighter estimates for the Lipschitz constant [38], [39], and they could be used to replace the above estimate. The theoretical results derived in the earlier sections will still hold, irrespective of the specific choice of the Lipschitz estimation strategy. We initialize \u03b7 by a small random vector, which is then updated using steepest ascent. It is solved using a logbarrier approach which constrains the estimated Lipschitz of the CNN below a threshold value. The total training loss is a linear combination of the log-barrier term and the supervised mean squared error (MSE) loss. We call this method as MOLLR (MOL-Lipschitz Regularized)."
        },
        {
            "heading": "C. Training the MOL-LR algorithm",
            "text": "In the supervised learning setting, we propose to minimize\nC(\u03b8) = Nt\u2211\ni=0 \u2016x\u2217i \u2212 xi\u201622 \ufe38 \ufe37\ufe37 \ufe38\nC\nsuch that\nP (x\u2217i )\ufe38 \ufe37\ufe37 \ufe38 Local Lipschitz estimate \u2264 T ; i = 0, .., Nt (26)\nHere, the threshold is selected as T = 1 \u2212 m and P (x) is indicated in (25). The above loss function is minimized\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nwith respect to parameters \u03b8 of the CNN H\u03b8. x\u2217i is a fixed point of (4) described in Section V-A, which is dependent on the CNN parameters \u03b8. xi; i = 0, .., Nt and bi, i = 0, .., Nt are the ground truth images in the training dataset and the corresponding under-sampled measurements, respectively. We solve the above constrained optimization scheme by using a log-barrier approach:\n\u03b8\u2217 = arg min \u03b8\nNt\u2211\ni=0\n( \u2016x\u2217i \u2212 xi\u201622 \u2212 \u03b2 log (T \u2212 P (x\u2217i )) )\n\ufe38 \ufe37\ufe37 \ufe38 Ci\n.\n(27) Here \u03b2 is a parameter that decays with training epochs similar to conventional log-barrier methods. This optimization strategy ensures that the estimate does not exceed T . For implementation purposes, we evaluate the worst-case perturbations \u03b7i for each x \u2217 i by maximizing (25) at each epoch. These perturbations are then assumed to be fixed to evaluate the above loss function, which is used to optimize \u03b8. The training algorithm is illustrated in the pseudo-code shown in Algorithm 1, which is illustrated for a batch size of a single image and gradient descent for simplicity.\nAlgorithm 1 : Training: input=training data xi; i = 1, .., Nt 1: for ep = 1, 2, . . . do 2: for i = 1, 2, . . . , Nt do 3: Determine x\u2217i using DEQ forward iterations 4: Determine \u03b7\u2217i = arg max\u03b7 p(x \u2217 i ,\u03b7)\n5: Ci = \u2016xi \u2212 x\u2217i \u20162 \u2212 1m log(T \u2212 p(H\u03b8(x\u2217i ),\u03b7\u2217i )) 6: Determine \u2207\u03b8Ci using DEQ backward iterations 7: \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8Ci, where Ci is the loss in (27) 8: end for 9: end for"
        },
        {
            "heading": "D. Unrolled algorithms used for comparison",
            "text": "We compare the proposed MOL algorithm against SENSE [40], MoDL [16], ADMM-Net [19], DE-GRAD [24], and UNET [29]. SENSE is a CS-based approach that uses a forward model consisting of coil sensitivity weighting and undersampled Fourier transform. MoDL and ADMM-Net are unrolled deep learning algorithms, which alternate between the DC step and the CNN-based denoising step. Both approaches are trained in an end-to-end fashion for 10 iterations. DEGRAD is a deep equilibrium network, where we use spectral normalization as described in [24]. UNET is a direct inversion approach, which uses a CNN without any DC steps. We choose five-layer CNNs for all the unrolled deep-learning and DEQ based algorithms used for comparisons. The CNN architecture is shown in Fig. 1 (b).\nWe consider two versions of MOL: MOL-SN, which relies on spectral normalization during training to constrain the Lipschitz constant of the overall network, and MOLLR, which consists of an additional loss term computing the Lipschitz constant of the CNN. We also consider Lipschitz regularized versions of UNET, ADMM-Net, and MoDL for\nrobustness experiments, and those are denoted by UNET-LR, ADMM-Net-LR, and MoDL-LR, respectively. The Lipschitz of the CNNs in these methods is regularized by the proposed training strategy in (27). For 2D+time experiments on cardiac data, we compare a 2D+time version of MOL-LR (with 3D convolutions) against the 2D MoDL (with 2D convolutions)."
        },
        {
            "heading": "E. Architecture of the CNNs and training details",
            "text": "The MOL architecture is shown in Fig. 1. In our 2D experiments, the CNN H\u03b8 consists of five 2D convolution layers, each followed by rectified linear unit (ReLU) non-linearity, except for the last layer. The convolution layers consist of 64 filters with 3x3 kernels. The parameter \u03bb in (14), weighing the DC term, is kept trainable. A SENSE reconstruction with \u03bb0 = 100 is performed initially on the undersampled image AHb to initialize the MOL network as x0. A similar approach is used for the other deep learning networks (MoDL, ADMMNet, UNET) to ensure fair comparisons. We share the CNN weights across the iterations for all the unrolled deep learning algorithms (MoDL, ADMM-Net). We use a full-size UNET, consisting of four layers of pooling and unpooling. Note that the number of trainable parameters in the chosen UNET is at least twice the number of parameters in five-layer CNNs. In 2D+time experiments, a five-layer CNN is chosen; it is similar to the 2D case, with the exception of 3D convolution layers instead of 2D. We set \u03b1 = 0.055, which corresponds to m = 0.1 or L[H\u03b8] = 0.9 for all MOL algorithms. We keep this variable fixed and non-trainable because this parameter is chosen based on (19), which depends on the bound T = 1\u2212m. We show in Fig. 3 that the MOL algorithm with \u03b1 = 1, which is the DEQ extension of MoDL [16] and RED [8], diverges.\nAll the trainings are performed on a 16 GB NVIDIA P100 GPU. The CNN weights are Xavier initialized and trained using an Adam optimizer for 200 epochs. The learning rates for updating CNN weights and \u03bb are chosen empirically as 10\u22124 and 1.0, respectively. MoDL and ADMM-Net are unrolled for 10 iterations; MOL, on the other hand, consumes memory equivalent to a single iteration. All the methods are implemented in PyTorch. During inference, the reconstruction results are quantitatively evaluated in terms of the Structural Similarity Index (SSIM) [41] and the Peak Signal-to-Noise Ratio (PSNR)."
        },
        {
            "heading": "F. Computing worst-case (adversarial) perturbations",
            "text": "We determine the robustness of the networks to Gaussian as well as worst-case perturbations. We determine the worst-case perturbation \u03b3 by solving the following optimization problem:\n\u03b3\u2217 = max \u03b3; \u2016\u03b3\u20162< \u00b7\u2016b\u20162 \u2016x\u2217 (b + \u03b3)\u2212 x\u2217(b)\u201622\ufe38 \ufe37\ufe37 \ufe38 U(\u03b3)\n(28)\nHere, \u03b3 is solved using a projected gradient algorithm; we alternate between gradient ascent steps and renormalization of \u03b3 to satisfy the constraint \u2016\u03b3\u20162 < \u00b7 \u2016b\u20162. For MOL, we use the fixed-point iterations described in Section V-A to compute the gradient \u2207\u03b3U(\u03b3). We note that the fixed-point iterations for back-propagation are accurate as long as the forward and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nbackward iterations converge. We track the maximum number of iterations and the termination criterion (24) to ensure that the iterations converge. In this work we relied on an `2 norm on the perturbations, while `\u221e constraints have also been used in the literature [25].\nG. 2D Brain and knee datasets\nWe used the 2D multi-coil brain data from the publicly available Calgary-Campinas Public (CCP) Dataset [42]. The dataset consists of twelve-coil T1-weighted brain data from 117 healthy subjects, collected on a 3.0 Tesla MRI scanner. The scan parameters are: TR (repetition time)/TE (echo time)/TI (inversion time) = 6.3 ms/2.6ms/650 ms or TR/TE/TI = 7.4ms/3.1ms/400ms. Matrix sizes are 256x208x170/180 with 256, 208, and 170/180 being the readout, phase encoding, and slice encoding directions, respectively. For the experiments, we choose subjects with fully sampled data (67 out of 117) and split them into training (45), validation (2), and testing (20) sets. The k-space measurements are retrospectively undersampled along the phase and slice encoding directions using a four-fold 2D non-uniform variable-density mask.\nWe also perform experiments on the multi-channel knee MRI datasets from the fastMRI challenge [43]. It consists of 15-coil coronal proton-density weighted knee data with or without fat suppression. The sequence parameters were: matrix size 320 x 320, in-plane resolution 0.5mm x 0.5mm, slice thickness 3mm, and repetition time (TR) varying from 2200 to 3000 ms, and echo time (TE) between 27 and 34 ms. We use the k-space measurements from 50 subjects for training, 5 for validation, and 20 for testing, respectively. The data is retrospectively undersampled along the phase-encoding direction, for four-fold, using a 1D non-uniform variable density mask. In another set of experiments, we consider fourfold undersampling using 1D uniform mask."
        },
        {
            "heading": "H. Cardiac MRI datasets",
            "text": "Compressed sensing and low-rank methods have been extensively used to reduce the breath-hold duration in cardiac cine [44], [45]. Several authors have introduced unrolled algorithms for cardiac cine MRI acceleration. For instance, one of the initial works considered the independent recovery of 2D images using unrolled methods [18], together with data sharing. More recent works [46], [47] rely on a 15- iteration unrolled scheme, where they used separable (2+1)- D spatio-temporal convolutions to keep the memory demand manageable.\nIn this work, we show the preliminary utility of the proposed MOL approach with 3D CNN to accelerate cardiac cine MRI. We used the multi-coil cardiac data from the open-source OCMR Dataset [48]. We note that the high memory demand often restricts the training of unrolled algorithms such as MoDL [16] in the 2D+time setting. We chose data from 1.5 Tesla scans, which consists of a series of short-axis, long-axis, or four-chamber views. We use fifteen subjects for training, two for validation, and eight for testing. Each dataset consists of 20-25 time-frames per slice, with 1-3 slices per subject. We retrospectively undersample the k-t space data\nusing a 1D non-uniform variable-density mask along the phase encode direction. In these experiments, we compare the MOL algorithm using a 3D network against the MoDL algorithm [16] using a 2D network. The MOL training is performed on 2D+time datasets. On the other hand, the MoDL algorithm [16] processes each of the time-frames independently, and hence is not capable of exploiting inter-frame dependencies."
        },
        {
            "heading": "VI. EXPERIMENTS AND RESULTS",
            "text": "The proposed method can handle wide range of inverse problems. We showcase its application in parallel MRI recovery from non-uniform and uniform undersampled acquisition settings in the subsequent sections. In addition, we also demonstrate it on image super-resolution problems. The superresolution experiments and results are discussed at Section III in the supplementary material."
        },
        {
            "heading": "A. Characterization of the models",
            "text": "In Fig. 3, we show the characteristics of the different models during training. Fig. 3.(a) shows plots of Lipschitz constant against epochs for different methods. We note that the computed Lipschitz constant l[H\u03b8] of the MOL-SN and DE-GRAD schemes that use spectral normalization stays around 0.7 and 0.4, respectively, which translates to lower performance seen from the testing error curves in (c). By contrast, the unrolled MoDL and ADMM-Net, and UNET, have no Lipschitz constraints and, therefore, have more flexibility in CNN weight updates. It is observed that the estimated Lipschitz constants of these networks often exceed 1. The proposed MOL-LR algorithm maintains the Lipschitz constant less than 0.9. The slightly lower performance of MOL-LR compared to that of other unrolled methods can be attributed to its lower Lipschitz constant.\nThe plot in Fig. 3.(b) shows the number of iterations of (14) (equivalently, the number of CNN function evaluations, denoted by nFE) needed to converge to the fixed point with a precision of \u03ba = 1 \u00d7 10\u22124. As expected, MOL-SN and DE-GRAD, which have lower Lipschitz constants and hence higher m, converge more rapidly than MOL-LR, which has a lower value of m. We note that the MOL-LR for \u03b1 = 1 algorithm is essentially the MoDL algorithm [16] used in the DEQ setting. As predicted by theory, we note that this algorithm fails to converge, evidenced by the number of iterations (nFE) increasing rapidly around 100 epochs. Similarly, the MOL algorithm without Lipschitz constraints also diverges around 10 epochs. The proposed MOL-LR with \u03b1 = 0.055 converges at approximately nFE = 28."
        },
        {
            "heading": "B. Performance comparison in the parallel MRI setting",
            "text": "The comparison of performance of the algorithms on fourfold accelerated knee data is shown in Fig. 4 and Table I, respectively. Cartesian 1D non-uniform variable density mask has been used for undersampling the data. Table I reports the quantitative performance in terms of mean PSNR and SSIM on 20 subjects. We observe that the performance of MOL-LR is only marginally lower than ten-iteration MoDL and ADMMNet. The marginally lower performance of MOL-LR can be\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\n1\nattributed to the stricter Lipschitz constraint on the CNN blocks, compared to MoDL and ADMM-Net. This is also confirmed by our experiments on Calgary brain data in Fig. 6, where the performance of MoDL and ADMM-Net decreases even more with the addition of the Lipschitz constraint (see MoDL-LR and ADMM-Net-LR reconstructions with no input noise). We also note that the reduction in performance is higher for the MOL-SN and DE-GRAD, both of which use spectral normalization. The performance of the UNET is lower than that of the unrolled algorithms MoDL, ADMMNet, and MOL-LR. The SENSE reconstruction performance is the lowest among all. The comparison of the average runtimes of the algorithms show that the MOL-LR scheme with around 25 DEQ iterations is around 2.5 times higher than MoDL and ADMM-Net with 10 unrolling steps. The qualitative comparisons are shown in Fig. 4. The error images show higher errors for MOL-SN, DE-GRAD, SENSE and UNET, while the error images from MOL-LR, MoDL, and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\n1\nADMM-Net are comparable. We performed statistical tests to compare MOL-LR against the other methods in terms of PSNR and SSIM reported in Table I. This was done for PSNR using linear mixed model analysis with Dunnett\u2019s test for\npairwise comparison of means. For SSIM values which did not meet normality assumptions, Friedman\u2019s test was used, with pairwise comparisons tested using Wilcoxon signed-rank test with Bonferroni adjustment applied to the p-values to account for multiplicity. The PSNR comparisons showed statistically significant differences between MOL-LR and each of the other methods. MoDL and ADMM-Net performed only slightly better than MOL-LR, with difference in mean PSNR of +0.40 (95% confidence interval (CI): 0.16, 0.64; p < 0.0001) and +0.29 (95% CI: 0.05, 0.53; p < 0.009), respectively. All the other methods underperformed compared to MOL-LR with a much larger mean difference in PSNR, from -1.81 (95% CI: -1.05, -2.05) for MoDL-LR to -5.30 (95% CI: -5.03, - 5.54) for SENSE. Comparison of SSIM showed no significant difference between MOL-LR and MoDL (median difference 0.000; 95% CI: -0.001, +0.001; p = 1.00) and ADMMNet (median difference 0.000; 95% CI: 0.000, +0.001; p = 1.00). All the other methods had significantly smaller SSIM compared to MOL-LR (p < 0.0001), from -0.003 (95% CI: -0.002, -0.004) for MoDL-LR and ADMM-Net-LR to -0.007 (95% CI: -0.006, -0.008) for SENSE and UNET-LR.\nWe also perform experiments to compare the proposed MOL-LR against unrolled method MoDL and the CS approach SENSE in uniform undersampling setting. The multi-channel knee data is four-fold undersampled using a uniform mask as shown in Fig. 5. Table II reports the quantitative results in terms of PSNR and SSIM on 20 subjects. The quality of MOL-LR is slightly lower than MoDL with no regularization, which is consistent. Both the methods significantly outperform SENSE which has visible aliasing in the reconstructed image as evident from Fig. 5. It is found through statistical analysis that MoDL is only slightly better than MOL-LR in PSNR (mean difference of +0.59; 95% CI: 0.43, 0.74; p < 0.0001), but with no significant difference in SSIM (median difference of 0.000; p = 1.00). SENSE underperformed compared to MOL-LR with a mean difference of -5.27 (95% CI: -5.11, - 5.43; p < 0.0001) for PSNR, and median difference of -0.007 (95% CI: -0.006, -0.008; p < 0.0001) for SSIM (See Table II). The statistical methods used in this case are the same as the ones used for non-uniform undersampling case, mentioned above.\nQuantitative comparisons of the methods on four-fold accelerated Calagary brain data is reported by Table I in the supplementary material. For this case, cartesian 2D nonuniform variable density mask has been used. A similar trend is observed here in terms of performance metrics PSNR, SSIM and run-time of the algorithms. The reconstructed brain images for qualitative comparisons are shown in Fig. 6 in the main paper and also in Fig. 1 from the supplementary material. The first column of Fig. 6 shows reconstructions from different methods when no noise has been added. MOLLR performs comparable to the unrolled algorithms MoDL and ADMM-Net. The Lipschitz constrained MoDL (MoDLLR) and ADMM-Net (ADMM-Net-LR) show relatively lower performance due to constrained CNNs. Both UNET and UNET-LR show reduction in performance as compared to the unrolled algorithms while MOL-SN and DE-GRAD also show reduced performance due to Lipschitz constraint enforced\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\n1 1\n1\nthrough spectral normalization. Similar trends are also seen in a different slice in Fig. 1 of the supplementary material."
        },
        {
            "heading": "C. Robustness to input perturbations",
            "text": "We compare the robustness of the networks on four-fold accelerated brain data in Fig. 6 and Fig. 7 respectively. Specifically, we study the change in output with respect to the perturbations to input to determine the stability of the models.\nThe first column in Fig. 6 shows the reconstructions given by different methods when there is no additional noise in\nthe measured k-space data (no-noise). The second and third column shows reconstructions when the measurements are corrupted by worst-case perturbations (adversarial) with energy as 10% and 15% of the energy of the measurements respectively. The fourth column shows reconstructed images when the measurements are corrupted by Gaussian noise with energy as 15% of the energy of the measurements. Here, MoDL, ADMM-Net and UNET are traditional methods with no Lipschitz constraint. By contrast, MOL-SN and DE-GRAD use spectral normalization, while MOL-LR uses the proposed\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\n1\nLipschitz constraint. MoDL-LR, ADMM-Net-LR, and UNETLR correspond to the above traditional methods with the proposed Lipschitz constraints added to the CNN blocks.\nThe performance of MOL-LR is comparable to that of MoDL and ADMM-Net in the setting with no additional noise, which is also consistent with the findings in Table I and the PSNR plots in Fig. 7. The improved performance of these unrolled methods over UNET is well established. The MOLSN and DE-GRAD schemes that use spectral normalization are associated with lower performance. We notice from the last column that the performance of all the Lipschitz-constrained methods only decrease by around 4 \u2212 5 dB with the addition of 15% Gaussian noise. However, the performance of ADMM-Net, MoDL, and UNET drops by around 10 dB with adversarial perturbations of the same amount of norm. By contrast, the performance drop of the Lipschitz-constrained models are largely consistent between the Gaussian and the worst-case setting, indicating that the proposed constraint can stabilize unrolled methods as well. However, we note that ADMM-Net-LR, MoDL-LR, and UNET-LR are associated with a decreased performance in the case with no additional noise. The decrease in performance can be attributed to the more constrained CNN block. The MOL-LR scheme offers performance comparable to the competing methods in the case without additional noise, while it is also more robust to worst-case perturbations. The better performance of MOL-LR compared to other LR methods in the absence of additional noise perturbations can be attributed to the higher number of iterations (nFE=28) compared to the 10 unrolling steps used in those methods. These trends can also be appreciated from the plot of the PSNRs in Fig. 7. (a). The models without Lipschitz constraints (MoDL, ADMM-Net, UNET) exhibit a drastic drop with > 0.1, whereas SENSE, MOL-LR, and MOL-SN have an approximately linear drop. We note that\nour theory predicts a linear drop in performance with MOL methods. MOL-SN and DE-GRAD (blue & red curves) are the flattest, which can be explained with the smaller Lipschitz constant. Although quantitative analysis of the reconstructions clearly show the proposed MOL-LR to be superior, a rigorous qualitative analysis by radiologists is needed to determine if the MOL-LR reconstructions are fit for clinical purposes.\nD. Illustration in high-dimensional settings The proposed MOL training approach only requires one physical layer to evaluate the forward and backward propagation; the memory demand of MOL-LR is ten times smaller than that of the unrolled networks with ten iterations. Our 2D experiments show that MOL-LR achieves performance similar to unrolled algorithms with a much lower memory footprint, which makes it an attractive option for large-scale problems. In this section, we illustrate the preliminary applicability of the MOL framework to large-scale problems. The joint recovery of 2D+time data using different undersampling patters for each time point can capitalize on the strong redundancy between the time-frames. However, it is challenging to use conventional unrolled optimization algorithms because of the high memory demand. We compare the performance of a 2D+time version of MOL-LR against a 2D MoDL (ten iterations) for recovery of time series of cardiac CINE MRI.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\n1\n1\nThe reconstruction results for six-fold and four-fold accelerated CINE MRI recovery are shown in Fig. 8.(a) and Fig. 8.(b), respectively. The top two rows correspond to the diastole and systole phases, respectively. The third row corresponds to the time series. It is observed from the top two rows that the 2D+time MOL-LR approach is able to minimize spatial blurring when compared to the 2D MoDL approach. The frame-to-frame changes in aliasing artifacts can also be appreciated from the time plots. Table III displays the mean PSNR and SSIM over eight test subjects. MOL-LR 3D outperforms MoDL 2D in terms of PSNR (by \u2248 2 dB) and SSIM at both the acceleration factors. The masks are shown in the bottom row. The four-fold and six-fold Poisson density sampling masks used in these experiments have eight lines in the center. While the preliminary experiments in this context are encouraging, more experiments are needed to compare the MOL in this setting to state-of-the-art dynamic MRI methods. We plan to pursue this in the future."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "We introduced a deep monotone operator learning framework for model-based deep learning to solve inverse problems in imaging. The proposed approach learns a monotone CNN in a deep equilibrium algorithm. The DEQ formulation enables forward and backward propagation using a single physical layer, thus significantly reducing the memory demand. The monotone constraint on the CNN allows us to introduce guarantees on the uniqueness of the solution, rapid convergence, and stability of the solution to input perturbations. We introduced two implementations that differ in the way the monotone constraint is imposed. The first approach relies on an exact\nspectral normalization strategy, while the second method relies on an approximate regularization approach. Our experiments show that both approaches result in convergent algorithms that are more robust to input perturbations than other deep learning approaches. However, the less conservative regularizationbased implementation offers improved performance compared to the more constrained spectral normalization approach. The validations in the context of parallel MRI show that the proposed MOL framework provides performance similar to the unrolled MoDL algorithms, but with significantly reduced memory demand and improved robustness to worst-case input perturbations. The memory efficiency of the proposed scheme enabled us to demonstrate the preliminary utility of this scheme in a larger-scale (2D+time) problem. The preliminary comparisons in the super-resolution setting also show that the proposed method is broadly applicable to other linear inverse problems."
        },
        {
            "heading": "VIII. APPENDIX",
            "text": ""
        },
        {
            "heading": "A. Proof of Proposition III.2",
            "text": "Proof. Assume that there exist two fixed points x 6= y for a specific b:\n\u03bbAH(Ax\u2212 b) + F(x) = 0 (29) \u03bbAH(Ay \u2212 b) + F(y) = 0 (30)\nwhich gives\nz = \u03bbAH(A(x\u2212 y)) + F(x)\u2212F(y) = 0. (31)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nSetting, v = x\u2212 y, we consider\n< (\u3008z,v\u3009) = < (\u2329 \u03bbAHAv,v \u232a)\n\ufe38 \ufe37\ufe37 \ufe38 \u2265\u03bb\u00b5min\u2016v\u201622\n+\n< (\u2329 F(x)\u2212F(y),v \u232a) \ufe38 \ufe37\ufe37 \ufe38 \u2265m\u2016v\u201622\n\u2265 (\u03bb\u00b5min +m) \u2016v\u201622\nwhere \u00b5min \u2265 0 is the minimum eigenvalue of AHA operator and F is m-monotone. The above relation is true only if v = 0 or z 6= 0. The first condition is true if x = y, while the second condition implies that (31) is not true for v 6= 0 or x 6= y.\nWe will now present a counter-example to show that the constraint is necessary. Suppose F is a linear non-monotone operator, denoted by a symmetric matrix F that has a nullspace N (F) which overlaps with null-space N (A) of A. Since F is not monotone, it will not satisfy < (\u3008Fv,v\u3009) > 0, which implies that F is not positive definite. If the null-spaces of A and F overlap, we can choose a v \u2208 N (A)\u22c2N (F) such that \u3008z,v\u3009 = 0. This counter-example shows that there exist non-monotone operators such that the fixed points are not unique."
        },
        {
            "heading": "B. Proof of Proposition III.3",
            "text": "Proof. Let the Lipschitz constant of H\u03b8 is 1\u2212m:\n\u2016H\u03b8(x)\u2212H\u03b8(y)\u20162 \u2264 (1\u2212m)\ufe38 \ufe37\ufe37 \ufe38 \u2016x\u2212 y\u20162, > 0. (32)\nUsing Cauchy Schwartz, \u2212\u2016a\u20162 \u00b7 \u2016b\u20162 \u2264 < (\u3008a,b\u3009) and (32), we have\n\u2212 (1\u2212m)\u2016x\u2212 y\u201622 \u2264 < (\u3008H\u03b8(x)\u2212H\u03b8(y),x\u2212 y\u3009) (33)\nWe consider the inner product,\ns = < (\u3008F(x)\u2212F(y),x\u2212 y\u3009) s = < (\u3008(I \u2212H\u03b8)(x)\u2212 (I \u2212H\u03b8)(y),x\u2212 y\u3009)\n= \u2016x\u2212 y\u201622 \u2212< (\u2329 H\u03b8(x)\u2212H\u03b8(y),x\u2212 y \u232a) .\n\u2265 m\u2016x\u2212 y\u201622 (34)\nIn the second step, we used (33). The relation (34) shows that F is m-monotone. The second relation specified by (11) can be derived using the triangle equality,\n\u2016F(x)\u2212F(y)\u20162 = \u2016x\u2212H\u03b8(x)\u2212 y +H\u03b8(y)\u20162 \u2264 \u2016x\u2212 y\u20162 + \u2016H\u03b8(x)\u2212H\u03b8(y)\u20162 \u2264 (2\u2212m)\u2016x\u2212 y\u20162."
        },
        {
            "heading": "C. Proof of Lemma IV.1",
            "text": "Proof. Using R = (I \u2212 \u03b1F), \u2016R(x)\u2212R(y)\u201622 = \u2016x\u2212 y \u2212 \u03b1F(x) + \u03b1F(y)\u201622\n= \u2016x\u2212 y\u201622 + \u03b12 \u2016F(x)\u2212F(y)\u201622\ufe38 \ufe37\ufe37 \ufe38 <(2\u2212m)2\u2016x\u2212y\u201622 +\n\u22122\u03b1< (\u2329 x\u2212 y,F(x)\u2212F(y) \u232a) \ufe38 \ufe37\ufe37 \ufe38 <\u22122\u03b1m\u2016x\u2212y\u201622 \u3009.\n(35) \u2264 \u2016x\u2212 y\u201622 \u221a 1 + \u03b12(2\u2212m)2 \u2212 2\u03b1m,\nwhich shows that L[R] = \u221a\n1 + \u03b12(2\u2212m)2 \u2212 2\u03b1m. The first inequality in (35) follows from the Lipschitz bound for F in (11), while the second one is from the m-monotonicity (7) condition on F ."
        },
        {
            "heading": "D. Proof of Proposition IV.2",
            "text": "Proof. We first show that the operator TMOL in the iterative relation (14): xn+1 = TMOL(xn) + z (36) is a contraction. In particular, the Lipschitz constant of Q\u03b1 in (13) is given by L[Q\u03b1] = 1(1+\u03bb\u00b5min) , where \u00b5min \u2265 0 is the minimum eigenvalue of AHA.\nUnder the conditions of the theorem, the Lipschitz constant L[I \u2212 \u03b1F ] is less than one. Combining the two, we have L[TMOL] < 1. If x\u2217 is a fixed point, we have x\u2217 = TMOL(x\u2217) + z. The result follows by the straightforward application of the Banach fixed-point theorem."
        },
        {
            "heading": "E. Proof of Proposition IV.3",
            "text": "Proof. Consider the iterative rule in (14),\nxn = Q\u03b1(I \u2212 \u03b1F)\ufe38 \ufe37\ufe37 \ufe38 T (xn\u22121) + \u03b1\u03bbQ\u03b1(AHb\ufe38 \ufe37\ufe37 \ufe38 w )\n= T 2(xn\u22122) + (T + I) \u03b1\u03bbQ\u03b1(w) = T n(x0) + (T n\u22121 + T + . . .+ I) \u03b1\u03bbQ\u03b1(w)\n(37)\nThe Lipschitz bound of Q\u03b1 is,\nLQ\u03b1 = 1\n1 + \u03bb\u00b5min , (38)\nL[T ] = LQ\u03b1 \u221a\n1 + \u03b12(2\u2212m)2 \u2212 2\u03b1m. (39)\nConsider z1 and z2 as two input measurements with \u03b4 = z2\u2212z1 as the perturbation in the input. Let the corresponding outputs be x1,n and x2,n, respectively, with \u2206n = x2,n\u2212x1,n as the perturbation in the output. Thus, the perturbation in the output can be written as\n\u2016\u2206n\u20162 = \u2016T (x1,n\u22121)\u2212 T (x2,n\u22121) + \u03b1\u03bbQ\u03b1(\u03b4)\u20162 \u2264 \u2016T (x1,n\u22121)\u2212 T (x2,n\u22121)\u20162 + \u03b1\u03bb\u2016Q\u03b1(\u03b4)\u20162 \u2264 L[T ]\u2016\u2206n\u22121\u20162 + \u03b1\u03bbLQ\u03b1\u2016\u03b4\u20162\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nUsing (37), we can expand the above relation as\n\u2016\u2206n\u20162 \u2264 (L[T ])n\u2016\u22060\u20162 + \u03b1\u03bb((L[T ])n\u22121 + (L[T ])n\u22122 + . . . 1)LQ\u03b1\u2016\u03b4\u20162.\nWhen L[T ] < 1, the first term vanishes, and we have\nlim n\u2192\u221e\n\u2016\u2206n\u20162 \u2264 \u03b1\u03bbLQ\u03b1\n1\u2212 L[T ] \u2016\u03b4\u20162.\nWe thus have\nlim n\u2192\u221e\n\u2016\u2206n\u20162 = \u2016\u2206\u20162 \u2264 \u03b1\u03bb/(1 + \u03bb\u00b5min)\n1\u2212 \u221a 1 + \u03b12(2\u2212m)2 \u2212 2\u03b1m \u2016\u03b4\u20162. (40)"
        },
        {
            "heading": "I. DEQ: FORWARD AND BACKWARD PROPAGATION",
            "text": "This section provides more details on the implementation of the DEQ based MOL algorithm, briefly described in V.A. During inference and training, we use the forward iterations xn+1 = TMOL(xn) + z until convergence as shown in the pseudo-code in Algorithm 1. The expanded equation is,\nxn+1 = (I+ \u03b1\u03bbA HA)\u22121 ( (1\u2212 \u03b1)xn + \u03b1H\u03b8(xn) )\n\ufe38 \ufe37\ufe37 \ufe38 TMOL(xn)\n+\n(I+ \u03b1\u03bbAHA)\u22121 ( \u03b1\u03bbAHb ) \ufe38 \ufe37\ufe37 \ufe38 z\n(1)\n= TMOL(xn) + z (2) from equation (14) in the main paper. We terminate the algorithm at the nth iteration, when xn satisfies,\nen = \u2016xn \u2212 xn\u22121\u20162 \u2016xn\u22121\u20162 < \u03ba. (3)\nWe set \u03ba = 1 \u00d7 10\u22124 for the experiments. We denote the fixed point of the algorithm as x\u2217(b), such that x\u2217(b) \u2248 TMOL (x\u2217(b)) + z.\nDEQ schemes [1] rely on fixed point iterations for backpropagating the gradients. Using chain rule, the gradient of the loss C with respect to the CNN parameters \u03b8 is computed as \u2207\u03b8C = (\u2207\u03b8x\u2217)T (\u2207x\u2217C). Using fixed point relation x\u2217 = TMOL(x\u2217) + z, one obtains \u2207\u03b8x\u2217 = (I \u2212 \u2207xTMOL(x)|x=x\u2217)\u22121 (\u2207\u03b8TMOL(x\u2217)) which translates to \u2207\u03b8C = (\u2207\u03b8TMOL(x\u2217))\u1d40 (I \u2212\u2207xTMOL(x)|x=x\u2217)\u2212\u1d40 (\u2207x\u2217C)\ufe38 \ufe37\ufe37 \ufe38\nq\n(4) Here, q is solved using fixed point iterations [1], [2] as\nq = (\u2207xTMOL(x)|x=x\u2217)\u1d40 q+\u2207x\u2217C (5) with initialization as q0 = 0. This iteration is evaluated until convergence, using the similar termination conditions as in (3). The computed q is substituted in (4) to obtain the gradient \u2207\u03b8C = (\u2207\u03b8TMOL(x\u2217))\u1d40 q. The pseudo-code for the backpropagation steps are shown in Algorithm 2.\nAniket Pramanik and Mathews Jacob are from the Department of Electrical and Computer Engineering at the University of Iowa, Iowa City, IA, 52242, USA (e-mail: aniket-pramanik@uiowa.edu; mathewsjacob@uiowa.edu). M. Bridget Zimmerman is from the Department of Biostatistics at the University of Iowa, Iowa City, IA, 52242, USA (e-mail: bridget-zimmerman@uiowa.edu). This work is supported by grants NIH R01 AG067078 and R01 EB031169.\nAlgorithm 1 : Forward propagation in MOL: input b 1: x0 = \u03bb0(I+ \u03bb0A HA)\u22121AHb\n2: z = (I+ \u03b1\u03bbAHA)\u22121 ( \u03b1\u03bbAHb ) 3: e =\u221e 4: while e > \u03ba do 5: xold = x 6: x = TMOL(x) + z from (1) 7: e = \u2016xold \u2212 x\u201622/\u2016xold\u201622 8: end while 9: Return x\nAlgorithm 2 : Backpropagation in MOL: input \u2207x\u2217C 1: q0 = 0 2: e =\u221e 3: while e > \u03ba do 4: qold = q 5: q = (\u2207xTMOL(x)|x=x\u2217)\u1d40 q+\u2207x\u2217C in (5) 6: e = \u2016qold \u2212 q\u201622/\u2016qold\u201622 7: end while 8: Return \u2207\u03b8C = (\u2207\u03b8TMOL(x\u2217))\u1d40 q"
        },
        {
            "heading": "II. PERFORMANCE COMPARISON ON PARALLEL MRI FROM CALGARY BRAIN DATA",
            "text": "Qualitative analysis of the reconstructions from different algorithms on four-fold accelerated brain MRI data is shown in Fig. 1. Proposed MOL-LR performs at par with ten-iterations of the unrolled MoDL and ADMM-Net. A slight drop in PSNR for MOL-LR is due to Lipschitz constraint on the CNN block. MOL-SN uses spectral normalization of each layer of the CNN which gives stricter bounds on its Lipschitz and thus provides lower performance. The drop in performance of DEGRAD is also for similar reasons as in MOL-SN. UNET has a lower performance compared to unrolled algorithms MoDL and ADMM-Net. The error images show high amplitude of errors for SENSE, MOL-SN, DE-GRAD and UNET. Proposed MOL-LR and unrolled MoDL, ADMM-Net show fewer errors in the error images and that too with much lower amplitude. These comparisons show that the Lipschitz regularization strategy offers better performance than spectral normalization. Similar trends are also visible in the quantitative analysis of these methods over 20 subjects as reported in Table I. ar X\niv :2\n20 6.\n04 79\n7v 4\n[ cs\n.C V\n] 2\n8 Fe\nb 20\n23\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\n1\nIII. ILLUSTRATION IN IMAGE SUPER-RESOLUTION SETTING\nThe proposed theory and algorithms are broadly applicable to general linear inverse problems. We now show the preliminary feasibility of the proposed MOL-LR approach in three-fold super-resolution image reconstruction in Fig. 2. MOL-LR is compared against MoDL and Tikhonov regularized reconstructions, respectively. Similar to the parallel MRI setting, MoDL and MOL-LR offer similar performance while outperforming the Tikhonov regularized reconstruction\nin terms of PSNR. There is visible aliasing and blurring in Tikhonov, which is reduced to an appreciable extent in MoDL and MOL-LR. All the models have been trained and tested on the Berkeley Segmentation Dataset and Benchmark [3]. The dataset consists of 300 images, out of which 200 are split for training and the remaining ones for testing. While the preliminary experiments in this context are encouraging, more experiments are needed to compare MOL to state-ofthe-art super-resolution methods. We plan to pursue this in the future.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3"
        }
    ],
    "title": "Memory-efficient model-based deep learning with convergence and robustness guarantees",
    "year": 2023
}