{
    "abstractText": "Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the stateof-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 \u00d7 4 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s twolevel algorithm for the first time, to our knowledge, since its discovery 50 years ago. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alhussein Fawzi"
        },
        {
            "affiliations": [],
            "name": "Matej Balog"
        },
        {
            "affiliations": [],
            "name": "Aja Huang"
        },
        {
            "affiliations": [],
            "name": "Thomas Hubert"
        },
        {
            "affiliations": [],
            "name": "Bernardino Romera-Paredes"
        },
        {
            "affiliations": [],
            "name": "Mohammadamin Barekatain"
        },
        {
            "affiliations": [],
            "name": "Alexander Novikov"
        },
        {
            "affiliations": [],
            "name": "Francisco J. R. Ruiz"
        },
        {
            "affiliations": [],
            "name": "Julian Schrittwieser"
        },
        {
            "affiliations": [],
            "name": "Grzegorz Swirszcz"
        },
        {
            "affiliations": [],
            "name": "David Silver"
        },
        {
            "affiliations": [],
            "name": "Demis Hassabis"
        },
        {
            "affiliations": [],
            "name": "Pushmeet Kohli"
        }
    ],
    "id": "SP:ea2fa4a68173c67140edabb471735e2d9cd668b6",
    "references": [
        {
            "authors": [
                "D Silver"
            ],
            "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
            "venue": "Science 362,",
            "year": 2018
        },
        {
            "authors": [
                "V. Strassen"
            ],
            "title": "Gaussian elimination is not optimal",
            "venue": "Numer. Math. 13,",
            "year": 1969
        },
        {
            "authors": [
                "M. Bl\u00e4ser"
            ],
            "title": "Fast matrix multiplication",
            "venue": "Theory Comput",
            "year": 2013
        },
        {
            "authors": [
                "J.M. Landsberg"
            ],
            "title": "Geometry and Complexity Theory 169",
            "year": 2017
        },
        {
            "authors": [
                "V.Y. Pan"
            ],
            "title": "Fast feasible and unfeasible matrix multiplication",
            "venue": "Preprint at https://arxiv.org/",
            "year": 2018
        },
        {
            "authors": [
                "Lim",
                "L.-H"
            ],
            "title": "Tensors in computations",
            "venue": "Acta Numer. 30,",
            "year": 2021
        },
        {
            "authors": [
                "A. Sch\u00f6nhage"
            ],
            "title": "Partial and total matrix multiplication",
            "venue": "SIAM J. Comput. 10,",
            "year": 1981
        },
        {
            "authors": [
                "D. Coppersmith",
                "S. Winograd"
            ],
            "title": "Matrix multiplication via arithmetic progressions",
            "venue": "In ACM Symposium on Theory of Computing 1\u20136 (ACM,",
            "year": 1987
        },
        {
            "authors": [
                "V. Strassen"
            ],
            "title": "The asymptotic spectrum of tensors and the exponent of matrix multiplication",
            "venue": "In 27th Annual Symposium on Foundations of Computer Science",
            "year": 1986
        },
        {
            "authors": [
                "F. Le Gall"
            ],
            "title": "Powers of tensors and fast matrix multiplication",
            "venue": "In International Symposium on Symbolic and Algebraic Computation",
            "year": 2014
        },
        {
            "authors": [
                "J. Alman",
                "V.V. Williams"
            ],
            "title": "A refined laser method and faster matrix multiplication",
            "venue": "In ACM-SIAM Symposium on Discrete Algorithms",
            "year": 2021
        },
        {
            "authors": [
                "C.J. Hillar",
                "Lim",
                "L.-H"
            ],
            "title": "Most tensor problems are NP-hard",
            "venue": "J. ACM 60,",
            "year": 2013
        },
        {
            "authors": [
                "J.D. Laderman"
            ],
            "title": "A noncommutative algorithm for multiplying 3 \u00d7 3 matrices using 23 multiplications",
            "venue": "Bull. Am. Math. Soc",
            "year": 1976
        },
        {
            "authors": [
                "J.E. Hopcroft",
                "L.R. Kerr"
            ],
            "title": "On minimizing the number of multiplications necessary for matrix multiplication",
            "venue": "SIAM J. Appl. Math. 20,",
            "year": 1971
        },
        {
            "authors": [
                "A.V. Smirnov"
            ],
            "title": "The bilinear complexity and practical algorithms for matrix multiplication",
            "venue": "Comput. Math. Math. Phys. 53,",
            "year": 2013
        },
        {
            "authors": [
                "A. Sedoglavic",
                "A.V. Smirnov"
            ],
            "title": "The tensor rank of 5x5 matrices multiplication is bounded by 98 and its border rank by 89",
            "venue": "In Proc. 2021 on International Symposium on Symbolic and Algebraic Computation",
            "year": 2021
        },
        {
            "authors": [
                "M.J. Heule",
                "M. Kauers",
                "M. Seidl"
            ],
            "title": "New ways to multiply 3 \u00d7 3-matrices",
            "venue": "J. Symb. Comput. 104,",
            "year": 2021
        },
        {
            "authors": [
                "T Hubert"
            ],
            "title": "Learning and planning in complex action spaces",
            "venue": "In International Conference on Machine Learning 4476\u20134486",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhang",
                "T.G. Dietterich"
            ],
            "title": "A reinforcement learning approach to job-shop scheduling",
            "venue": "In International Joint Conferences on Artificial Intelligence",
            "year": 1995
        },
        {
            "authors": [
                "A. Vaswani"
            ],
            "title": "Attention is all you need",
            "venue": "In International Conference on Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "J. Ho",
                "N. Kalchbrenner",
                "D. Weissenborn",
                "T. Salimans"
            ],
            "title": "Axial attention in multidimensional transformers",
            "venue": "Preprint at https://arxiv.org/abs/1912.12180",
            "year": 2019
        },
        {
            "authors": [
                "Drevet",
                "C.-\u00c9",
                "M.N. Islam",
                "\u00c9. Schost"
            ],
            "title": "Optimization techniques for small matrix multiplication",
            "venue": "Theor. Comput. Sci. 412,",
            "year": 2011
        },
        {
            "authors": [
                "A. Sedoglavic"
            ],
            "title": "A non-commutative algorithm for multiplying (7 \u00d7 7) matrices using 250 multiplications",
            "year": 2017
        },
        {
            "authors": [
                "Battaglia",
                "P. W"
            ],
            "title": "Relational inductive biases, deep learning, and graph networks. Preprint at https://arxiv.org/abs/1806.01261 (2018)",
            "year": 2018
        },
        {
            "authors": [
                "M. Balog",
                "B. van Merri\u00ebnboer",
                "S. Moitra",
                "Y. Li",
                "D. Tarlow"
            ],
            "title": "Fast training of sparse graph neural networks on dense hardware",
            "year": 1906
        },
        {
            "authors": [
                "K. Ye",
                "Lim",
                "L.-H. Fast structured matrix computations"
            ],
            "title": "tensor rank and Cohn\u2013Umans method",
            "venue": "Found. Comput. Math. 18, 45\u201395",
            "year": 2018
        },
        {
            "authors": [
                "Bradbury",
                "J. et al. JAX"
            ],
            "title": "composable transformations of Python+NumPy programs",
            "venue": "GitHub http://github.com/google/jax",
            "year": 2018
        },
        {
            "authors": [
                "A.R. Benson",
                "G. Ballard"
            ],
            "title": "A framework for practical parallel fast matrix multiplication",
            "venue": "ACM SIGPLAN Not",
            "year": 2015
        },
        {
            "authors": [
                "J. Huang",
                "T.M. Smith",
                "G.M. Henry",
                "R.A. Van De Geijn"
            ],
            "title": "Strassen\u2019s algorithm reloaded",
            "venue": "In International Conference for High Performance Computing, Networking, Storage and Analysis",
            "year": 2016
        },
        {
            "authors": [
                "Abadi",
                "M. et al. Tensorflow"
            ],
            "title": "a system for large-scale machine learning",
            "venue": "In USENIX Symposium On Operating Systems Design And Implementation 265\u2013283",
            "year": 2016
        },
        {
            "authors": [
                "W. Dabney",
                "M. Rowland",
                "M. Bellemare",
                "R. Munos"
            ],
            "title": "Distributional reinforcement learning with quantile regression",
            "venue": "In AAAI Conference on Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma",
                "Ba",
                "J. Adam"
            ],
            "title": "a method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2015
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "D Silver"
            ],
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "venue": "Nature 529,",
            "year": 2016
        },
        {
            "authors": [
                "A. Sedoglavic"
            ],
            "title": "A non-commutative algorithm for multiplying 5x5 matrices using 99 multiplications",
            "year": 2017
        },
        {
            "authors": [
                "H.F. de Groote"
            ],
            "title": "On varieties of optimal algorithms for the computation of bilinear mappings II. optimal algorithms for 2 \u00d7 2-matrix multiplication",
            "venue": "Theor. Comput. Sci",
            "year": 1978
        },
        {
            "authors": [
                "V.P. Burichenko"
            ],
            "title": "On symmetries of the Strassen algorithm",
            "venue": "Preprint at https://arxiv.org/",
            "year": 2014
        },
        {
            "authors": [
                "L. Chiantini",
                "C. Ikenmeyer",
                "J.M. Landsberg",
                "Ottaviani",
                "G. The geometry of rank decompositions of matrix multiplication I"
            ],
            "title": "2 \u00d7 2 matrices",
            "venue": "Exp. Math. 28, 322\u2013327",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Grochow",
                "C. Moore"
            ],
            "title": "Designing Strassen\u2019s algorithm",
            "venue": "Preprint at https://arxiv.org/",
            "year": 2017
        },
        {
            "authors": [
                "G. Ballard",
                "C. Ikenmeyer",
                "J.M. Landsberg",
                "Ryder",
                "N. The geometry of rank decompositions of matrix multiplication II"
            ],
            "title": "3 \u00d7 3 matrices",
            "venue": "J. Pure Appl. Algebra 223, 3205\u20133224",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Grochow",
                "C. Moore"
            ],
            "title": "Matrix multiplication algorithms from group orbits",
            "year": 2016
        },
        {
            "authors": [
                "T.G. Kolda",
                "B.W. Bader"
            ],
            "title": "Tensor decompositions and applications",
            "venue": "SIAM Rev",
            "year": 2009
        },
        {
            "authors": [
                "A. Bernardi",
                "J. Brachat",
                "P. Comon",
                "B. Mourrain"
            ],
            "title": "General tensor decomposition, moment matrices and applications",
            "venue": "J. Symb. Comput",
            "year": 2013
        },
        {
            "authors": [
                "V. Elser"
            ],
            "title": "A network that learns Strassen multiplication",
            "venue": "J. Mach. Learn. Res",
            "year": 2016
        },
        {
            "authors": [
                "M. Tschannen",
                "A. Khanna",
                "A Anandkumar",
                "StrassenNets"
            ],
            "title": "deep learning with a multiplication budget",
            "venue": "In International Conference on Machine Learning 4985\u20134994",
            "year": 2018
        },
        {
            "authors": [
                "J. Huang",
                "C.D. Yu",
                "Geijn",
                "R.A.V. D"
            ],
            "title": "Strassen\u2019s algorithm reloaded on GPUs",
            "venue": "ACM Trans. Math. Softw. 46,",
            "year": 2020
        },
        {
            "authors": [
                "A Mirhoseini"
            ],
            "title": "A graph placement methodology for fast chip design",
            "venue": "Nature 594,",
            "year": 2021
        },
        {
            "authors": [
                "R. Bunel",
                "A. Desmaison",
                "P. Kohli",
                "P.H. Torr",
                "M.P. Kumar"
            ],
            "title": "Learning to superoptimize programs",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2017
        },
        {
            "authors": [
                "Y. Li",
                "F. Gimeno",
                "P. Kohli",
                "O. Vinyals"
            ],
            "title": "Strong generalization and efficiency in neural programs",
            "venue": "Preprint at https://arxiv.org/abs/2007.03629",
            "year": 2020
        },
        {
            "authors": [
                "Lagoudakis",
                "M. G"
            ],
            "title": "Algorithm selection using reinforcement learning",
            "venue": "In International Conference on Machine Learning",
            "year": 2000
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "Evolutionary Principles in Self-Referential Learning. On Learning now to Learn: The Meta-Meta-Meta...-Hook",
            "venue": "Diploma thesis, Technische Univ. Munchen",
            "year": 1987
        },
        {
            "authors": [
                "C. Kaliszyk",
                "J. Urban",
                "H. Michalewski",
                "M. Ol\u0161\u00e1k"
            ],
            "title": "Reinforcement learning of theorem proving",
            "venue": "In International Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "B. Piotrowski",
                "Urban",
                "J. ATPboost"
            ],
            "title": "learning premise selection in binary setting with ATP feedback",
            "venue": "In International Joint Conference on Automated Reasoning 566\u2013574",
            "year": 2018
        },
        {
            "authors": [
                "K. Bansal",
                "S. Loos",
                "M. Rabe",
                "C. Szegedy",
                "Wilcox",
                "S. HOList"
            ],
            "title": "an environment for machine learning of higher order logic theorem proving",
            "venue": "In International Conference on Machine Learning 454\u2013463",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zombori",
                "J. Urban",
                "C.E. Brown"
            ],
            "title": "Prolog technology reinforcement learning prover",
            "venue": "In International Joint Conference on Automated Reasoning 489\u2013507",
            "year": 2020
        },
        {
            "authors": [
                "A.Z. Wagner"
            ],
            "title": "Constructions in combinatorics via neural networks",
            "venue": "Preprint at https://",
            "year": 2021
        },
        {
            "authors": [
                "M. Popova",
                "O. Isayev",
                "A. Tropsha"
            ],
            "title": "Deep reinforcement learning for de novo drug",
            "venue": "design. Sci. Adv. 4,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhou",
                "S. Kearnes",
                "L. Li",
                "R.N. Zare",
                "P. Riley"
            ],
            "title": "Optimization of molecules via deep reinforcement learning",
            "venue": "Sci. Rep. 9,",
            "year": 2019
        },
        {
            "authors": [
                "M.H. Segler",
                "M. Preuss",
                "M.P. Waller"
            ],
            "title": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "venue": "Nature 555,",
            "year": 2018
        },
        {
            "authors": [
                "M. Dalgaard",
                "F. Motzoi",
                "J.J. S\u00f8rensen",
                "J. Sherson"
            ],
            "title": "Global optimization of quantum dynamics with AlphaZero deep exploration",
            "venue": "npj Quantum Inf. 6,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Nature | Vol 610 | 6 October 2022 | 47"
        },
        {
            "heading": "Article",
            "text": "Discovering faster matrix multiplication algorithms with reinforcement learning\nAlhussein Fawzi1,2 \u2709, Matej Balog1,2, Aja Huang1,2, Thomas Hubert1,2, Bernardino Romera-Paredes1,2, Mohammadamin Barekatain1, Alexander Novikov1, Francisco J. R. Ruiz1, Julian Schrittwieser1, Grzegorz Swirszcz1, David Silver1, Demis Hassabis1 & Pushmeet Kohli1\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the stateof-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 \u00d7 4 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s twolevel algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\nWe focus on the fundamental task of matrix multiplication, and use deep reinforcement learning (DRL) to search for provably correct and efficient matrix multiplication algorithms. This algorithm discovery process is particularly amenable to automation because a rich space of matrix multiplication algorithms can be formalized as low-rank decompositions of a specific three-dimensional (3D) tensor2, called the matrix multiplication tensor3\u20137. This space of algorithms contains the standard matrix multiplication algorithm and recursive algorithms such as Strassen\u2019s2, as well as the (unknown) asymptotically optimal algorithm. Although an important body of work aims at characterizing the complexity of the asymptotically optimal algorithm8\u201312, this does not yield practical algorithms5. We focus here on practical matrix multiplication algorithms, which correspond to explicit low-rank decompositions of the matrix multiplication tensor. In contrast to two-dimensional matrices, for which efficient polynomial-time algorithms computing the rank have existed for over two centuries13, finding low-rank decompositions of 3D tensors (and beyond) is NP-hard14 and is also hard in practice. In fact, the search space is so large that even the optimal algorithm for multiplying two 3 \u00d7 3 matrices is still unknown. Nevertheless, in a longstanding research effort, matrix multiplication algorithms have been discovered by attacking this tensor decomposition problem using human search2,15,16, continuous optimization17\u201319 and combinatorial search20. These approaches often rely on human-designed heuristics, which are probably suboptimal. We instead use DRL to learn to recognize and generalize over patterns in tensors, and use the learned agent to predict efficient decompositions. We formulate the matrix multiplication algorithm discovery procedure (that is, the tensor decomposition problem) as a single-player game, called TensorGame. At each step of TensorGame, the player selects how to combine different entries of the matrices to multiply. A score is assigned based on the number of selected operations required to reach the correct multiplication result. This is a challenging game with an enormous action space (more than 1012 actions for most interesting cases) that is much larger than that of traditional board games such as chess and Go (hundreds of actions). To solve TensorGame and find efficient matrix multiplication algorithms, we develop a DRL agent, AlphaTensor. AlphaTensor is built on AlphaZero1,21, where a neural network is trained to guide a planning procedure searching for efficient matrix multiplication algorithms. Our framework uses a single agent to decompose matrix multiplication tensors of various sizes, yielding\nhttps://doi.org/10.1038/s41586-022-05172-4\nReceived: 2 October 2021\nAccepted: 2 August 2022"
        },
        {
            "heading": "Published online: 5 October 2022",
            "text": ""
        },
        {
            "heading": "Open access",
            "text": "Check for updates\n1DeepMind, London, UK. 2These authors contributed equally: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert and Bernardino Romera-Paredes. \u2709e-mail: afawzi@deepmind.com\n48 | Nature | Vol 610 | 6 October 2022"
        },
        {
            "heading": "Article",
            "text": "transfer of learned decomposition techniques across various tensors. To address the challenging nature of the game, AlphaTensor uses a specialized neural network architecture, exploits symmetries of the problem and makes use of synthetic training games.\nAlphaTensor scales to a substantially\u00a0larger algorithm space than what is within reach for either human or combinatorial search. In fact, AlphaTensor discovers from scratch many provably correct matrix multiplication algorithms that improve over existing algorithms in terms of number of scalar multiplications. We also adapt the algorithm discovery procedure to finite fields, and improve over Strassen\u2019s two-level algorithm for multiplying 4 \u00d7 4 matrices for the first time, to our knowledge, since its inception in 1969. AlphaTensor also discovers a diverse set of algorithms\u2014up to thousands for each size\u2014showing that the space of matrix multiplication algorithms is richer than previously thought. We also exploit the diversity of discovered factorizations to improve state-of-the-art results for large matrix multiplication sizes. Through different use-cases, we highlight AlphaTensor\u2019s flexibility and wide applicability: AlphaTensor discovers efficient algorithms for structured matrix multiplication improving over known results, and finds efficient matrix multiplication algorithms tailored to specific hardware, by optimizing for actual runtime. These algorithms multiply large matrices faster than human-designed algorithms on the same hardware."
        },
        {
            "heading": "Algorithms as tensor decomposition",
            "text": "As matrix multiplication (A, B) \u21a6 AB is bilinear (that is, linear in both arguments), it can be fully represented by a 3D tensor: see Fig.\u00a01a for how to represent the 2 \u00d7 2 matrix multiplication operation as a 3D tensor of size 4 \u00d7 4 \u00d7 4, and refs. 3,5,7 for more details. We write nT for the tensor describing n \u00d7 n matrix multiplication. The tensor Tn is fixed (that is, it is independent of the matrices to be multiplied), has entries in {0, 1}, and is of size n2 \u00d7 n2 \u00d7 n2. More generally, we use n m p, ,T to describe the rectangular matrix multiplication operation of size n \u00d7 m with m \u00d7 p (note that =n n n n, ,T T ). By a decomposition of Tn into R rank-one terms, we mean\nT \u2211= \u2297 \u2297 , (1)n r\nR r r r\n=1\n( ) ( ) ( )u v w\nwhere \u2297 denotes the outer (tensor) product, and u(r), v(r) and w(r) are all vectors. If a tensor T can be decomposed into R rank-one terms, we say the rank of T is at most R, or RRank ( ) \u2264T . This is a natural extension from the matrix rank, where a matrix is decomposed into \u2211 \u2297r R r r =1 ( ) ( )u v .\nA decomposition of Tn into R rank-one terms provides an algorithm for multiplying arbitrary n \u00d7 n matrices using R scalar multiplications (see Algorithm 1). We refer to Fig.\u00a01b,c for an example algorithm multiplying 2 \u00d7 2 matrices with R = 7 (Strassen\u2019s algorithm).\nCrucially, Algorithm 1 can be used to multiply block matrices. By using this algorithm\u00a0recursively, one can multiply matrices of arbitrary size, with the rank R controlling the asymptotic complexity of the algorithm. In particular, N \u00d7 N matrices can be multiplied with asymptotic complexity O N( )Rlog ( )n ; see ref. 5 for more details."
        },
        {
            "heading": "DRL for algorithm discovery",
            "text": "We cast the problem of finding efficient matrix multiplication algorithms as a reinforcement learning problem, modelling the environment as a single-player game, TensorGame. The game state after step t is described by a tensor St, which is initially set to the target tensor we wish to decompose: S T= n0 . In each step t of the game, the player selects a triplet (u(t), v(t), w(t)), and the tensor St is updated by subtracting the resulting rank-one tensor: \u2190 \u2212 \u2297 \u2297t t t t t \u22121\n( ) ( ) ( )S S u v w . The goal of the player is to reach the zero tensor 0=tS by applying the smallest number of moves. When the player reaches the zero tensor, the sequence of selected factors satisfies u v wT = \u2211 \u2297 \u2297n t R t t t =1\n( ) ( ) ( ) (where R denotes the number of moves), which guarantees the correctness of the resulting matrix multiplication algorithm. To avoid playing unnecessarily long games, we limit the number of steps to a maximum value, Rlimit.\nFor every step taken, we provide a reward of \u22121 to encourage finding the shortest path to the zero tensor. If the game terminates with a non-zero tensor (after Rlimit steps), the agent receives an additional terminal reward equal to \u03b3\u2212 ( )R limitS , where \u03b3( )R limitS is an upper bound on the rank of the terminal tensor. Although this reward optimizes for rank (and\u00a0hence for\u00a0the complexity of the resulting algorithm), other reward schemes can be used to optimize other properties, such as practical runtime (see \u2018Algorithm discovery\u00a0results\u2019). Besides, as our aim is to find exact matrix multiplication algorithms, we constrain {u(t), v(t), w(t)} to have entries in a user-specified discrete set of coefficients F (for example, F = {\u22122, \u22121, 0, 1, 2}). Such discretization is common practice to avoid issues with the finite precision of floating points15,18,20.\nTo play TensorGame, we propose AlphaTensor (Fig.\u00a02), an agent based on AlphaZero1, which achieved tabula rasa superhuman performance in the classical board games of Go, chess and shogi, and on its extension to handle large action spaces Sampled AlphaZero21. Similarly to AlphaZero, AlphaTensor uses a deep neural network to guide a Monte\nb, Strassen's algorithm2 for multiplying 2 \u00d7 2 matrices using 7 multiplications. c, Strassen's algorithm in tensor factor representation. The stacked factors U, V and W (green, purple and yellow, respectively)\u00a0provide a rank-7 decomposition of 2T (equation\u00a0(1)).\u00a0The correspondence between arithmetic operations (b) and\u00a0factors (c) is shown by using the aforementioned colours.\nNature | Vol 610 | 6 October 2022 | 49\nCarlo tree search (MCTS) planning procedure. The network takes as input a state (that is, a tensor tS to decompose), and outputs a policy and a value. The policy provides a distribution over potential actions. As the set of potential actions (u(t), v(t), w(t)) in each step is enormous, we rely on sampling actions rather than enumerating them21,22. The value provides an estimate of the distribution z of returns (cumulative reward) starting from the current state tS . With the above reward scheme, the distribution z models the agent\u2019s belief about the rank of the tensor St. To play a game, AlphaTensor starts from the target tensor ( nT ) and uses the MCTS planner at each step to choose the next action. Finished games are used as feedback to the network to improve the network parameters.\nOvercoming the challenges posed by TensorGame\u2014namely, an enormous action space, and game states described by large 3D tensors representing an abstract mathematical operation\u2014requires multiple advances. All these components, described briefly below, \u00a0substantially\nimprove the overall performance over a plain AlphaZero agent (see Methods and Supplementary Information for details)."
        },
        {
            "heading": "Neural network architecture",
            "text": "We propose a transformer-based23 architecture that incorporates inductive biases for tensor inputs. We first project the S \u00d7 S \u00d7 S input tensor into three S \u00d7 S grids of feature vectors by using linear layers applied to the\u00a0three cyclic\u00a0transpositions of the tensor. The main part of the model comprises a sequence of attention operations, each applied to a set of features belonging to a pair of grids (Extended Data Figs.\u00a03 and 4). This generalizes axial attention24 to multiple grids, and is both more efficient and yields better results than naive self-attention. The proposed architecture, which disregards the order of rows and columns in the grids, is inspired by the invariance of the tensor rank to slice reordering. The final feature representation of the three matrices is passed both to the policy head (an autoregressive model) and the value head (a multilayer perceptron)."
        },
        {
            "heading": "Synthetic demonstrations",
            "text": "Although tensor decomposition is NP-hard, the inverse task of constructing the tensor from its rank-one factors is elementary. Hence, we generate a large dataset of tensor-factorization pairs (synthetic demonstrations)\u00a0by first sampling factors u v w{( , , )}r r r r R( ) ( ) ( ) =1 at random, and then constructing the tensor = \u2211 \u2297 \u2297r R r r r =1 ( ) ( ) ( )D u v w . We train the network on a mixture of supervised loss (that is, to imitate synthetic demonstrations) and standard reinforcement learning loss (that is, learning to decompose a target tensor nT ) (Fig.\u00a02). This mixed training strategy\u2014training on the target tensor and random tensors\u2014\u00a0substantially outperforms each training strategy separately. This is despite randomly generated tensors having different properties from the target tensors."
        },
        {
            "heading": "Change of basis",
            "text": "nT (Fig.\u00a01a) is the tensor representing the matrix multiplication bilinear operation in the canonical basis. The same bilinear operation can be expressed in other bases, resulting in other tensors. These different\nAlgorithm 1 A meta-algorithm parameterized by =u v w{ , , } r r r( ) ( ) ( ) r R\n1 for computing the matrix product C = AB. It is noted that R controls the number of multiplications between input matrix entries.\nParameters: =u v w{ , , } r r r( ) ( ) ( ) r R 1: length-n 2 vectors such that\nTn r r r( ) ( ) ( ) r R\n1= \u2211 \u2297 \u2297= u v w Input: A, B: matrices of size n \u00d7 n Output: C = AB (1) for r = 1, \u2026, R do (2)\u00a0 \u00a0\u00a0 \u2190 + + + + m u a u a v b v b( ) ( )r n n1 1 r n r r n r 1 ( ) ( ) 1 ( ) ( ) 2 22 2 (3) for i = 1, \u2026, n2 do (4)\u00a0 \u00a0\u00a0 c w m w mi R1i i\nR(1) ( )\u2190 + + return C\n50 | Nature | Vol 610 | 6 October 2022"
        },
        {
            "heading": "Article",
            "text": "tensors are equivalent: they have the same rank, and decompositions obtained in a custom basis can be mapped to the canonical basis, hence obtaining a practical algorithm of the form in Algorithm 1. We leverage this observation by sampling a random change of basis at the beginning of every game, applying it to Tn, and letting AlphaTensor play the game in that basis (Fig.\u00a02). This crucial step injects diversity into the games played by the agent."
        },
        {
            "heading": "Data augmentation",
            "text": "From every played game, we can extract additional tensor-factorization pairs for training the network. Specifically, as factorizations are order invariant (owing to summation), we build an additional tensor-factorization training pair by swapping a random action with the last action from each finished game."
        },
        {
            "heading": "Algorithm discovery\u00a0results",
            "text": "Discovery of matrix multiplication algorithms We train a single AlphaTensor agent to find matrix multiplication algorithms for matrix sizes n \u00d7 m with m \u00d7 p, where n, m, p \u2264 5. At the beginning of each game, we sample uniformly a triplet (n, m, p) and train AlphaTensor to decompose the\u00a0 tensor Tn m p, , . Although we consider tensors of fixed size (Tn m p, , has size nm \u00d7 mp \u00d7 pn), the discovered algorithms can be applied recursively to multiply matrices of arbitrary size. We use AlphaTensor to find matrix multiplication algorithms over different arithmetics\u2014namely, modular arithmetic (that is, multiplying matrices in the quotient ring 2Z ), and standard arithmetic (that is, multiplying matrices in R).\nFigure\u00a03 (left) shows the complexity (that is, rank) of the algorithms discovered by AlphaTensor. AlphaTensor re-discovers the best algorithms known for multiplying matrices (for example,\nStrassen\u2019s2 and Laderman\u2019s15 algorithms). More importantly, AlphaTensor improves over the best algorithms known for several matrix sizes. In particular, AlphaTensor finds an algorithm for multiplying 4 \u00d7 4 matrices using 47 multiplications in 2Z , thereby outperforming Strassen\u2019s two-level algorithm2, which involves 72 = 49 multiplications. By applying this algorithm recursively, one obtains a practical matrix multiplication algorithm in Z2 with complexity O N( )\n2.778 . Moreover, AlphaTensor discovers efficient\u00a0algorithms for multiplying matrices in standard arithmetic; for example, AlphaTensor finds a rank-76 decomposition of 4,5,5T , improving over the previous state-of-the-art complexity of 80 multiplications. See Extended Data Figs.\u00a01 and 2 for examples.\nAlphaTensor generates a large database of matrix multiplication algorithms\u2014up to thousands of algorithms for each size. We exploit this rich space of algorithms by combining them recursively, with the aim of decomposing larger matrix multiplication tensors. We refer to refs. 25,26 and Appendix H in Supplementary Information for more details. Using this approach, we improve over the state-of-the-art results for more than 70 matrix multiplication tensors (with n, m, p \u2264 12). See Fig.\u00a03 (right) and Extended Data Table\u00a01 for the results.\nA crucial aspect of AlphaTensor is its ability to learn to transfer knowledge between targets (despite providing no prior knowledge on their relationship). By training one agent to decompose various tensors, AlphaTensor shares learned strategies among these, thereby improving the overall performance (see Supplementary Information for analysis). Finally, it is noted that AlphaTensor scales beyond current computational approaches for decomposing tensors. For example, to our knowledge, no previous approach was able to handle T4, which has an action space 1010 times larger than T3. Our agent goes beyond this limit, discovering decompositions matching or surpassing state-of-the-art for large tensors such as 5T .\nIn all cases, AlphaTensor discovers algorithms that match or improve over known state of the art (improvements are shown in red). See Extended Data Figs.\u00a01 and 2 for examples of algorithms found with AlphaTensor. Right: results (for arithmetic in R) of applying AlphaTensor-discovered algorithms on larger tensors. Each red dot represents a tensor size, with a subset of them labelled. See Extended Data Table\u00a01 for the results in table form. State-of-the-art results are obtained from the list in ref. 64.\nNature | Vol 610 | 6 October 2022 | 51\nAnalysing the symmetries of matrix multiplication algorithms From a mathematical standpoint, the diverse algorithms discovered by AlphaTensor show that the space is richer than previously known. For example, while\u00a0the only known rank-49 factorization decomposing T T T= \u22974 2 2 before this paper conforms to the product structure (that is, it uses the factorization of T2 twice, which we refer to as Strassensquare2), AlphaTensor finds more than 14,000 non-equivalent factorizations (with standard arithmetic) that depart from this scheme, and have different properties (such as matrix ranks and sparsity\u2014see Supplementary Information). By non-equivalent, we mean that it is not possible to obtain one from another by applying a symmetry transformation (such as permuting the factors). Such properties of matrix multiplication tensors are of great interest, as these tensors represent fundamental objects in algebraic complexity theory3,5,7. The study of matrix multiplication symmetries can also provide insight into the asymptotic complexity of matrix multiplication5. By exploring this rich space of algorithms, we believe that AlphaTensor will be useful for generating results and guiding mathematical research. See Supplementary Information for proofs and details on the symmetries of factorizations.\nBeyond standard matrix multiplication Tensors can represent any bilinear operation, such as structured matrix multiplication, polynomial multiplication or more custom bilinear operations used in machine learning27,28. We demonstrate here a use-case where AlphaTensor finds a state-of-the-art algorithm for multiplying an n x n skew-symmetric matrix \u00a0with a vector\u00a0of length n. Figure\u00a04a shows the obtained decompositions for small instance sizes n. We observe a pattern that we generalize to arbitrary n, and prove that this yields a general algorithm for the skew-symmetric matrix-vector product (Fig.\u00a04b). This algorithm, which uses n n n( \u2212 1)( + 2)/2 ~ 12\n2 multiplications\u00a0(where\u00a0\u223c\u00a0indicates asymptotic similarity), outperforms the previously known algorithms using \u00a0asymptotically n2 multiplications29, and is asymptotically optimal. See Supplementary Information\nfor a proof, and for another use-case showing AlphaTensor\u2019s ability to re-discover the Fourier basis (see also Extended Data Table\u00a02). This shows that AlphaTensor can be applied to custom bilinear operations, and yield efficient algorithms leveraging the problem structure.\nRapid tailored algorithm discovery We show a use-case where AlphaTensor finds practically efficient matrix multiplication algorithms, tailored to specific hardware, with zero prior hardware knowledge. To do so, we modify the reward of AlphaTensor: we provide an additional reward at the terminal state (after the agent found a correct algorithm) equal to the negative of the runtime of the algorithm when benchmarked on the target hardware. That is, we set r r \u03bbb\u2032 = +t t t, where rt is the reward scheme described in \u2018DRL for algorithm discovery\u2019, bt is the benchmarking reward (non-zero only at the terminal state) and \u03bb is a user-specified coefficient. Aside from the different reward, the exact same formulation of TensorGame is used.\nWe train AlphaTensor to search for efficient algorithms to multiply 4 \u00d7 4 block matrices, and focus on square matrix multiplication of size 8,192 (each block is hence of size 2,048) to define the benchmarking reward. AlphaTensor searches for the optimal way of combining the 16 square blocks of the input matrices on the considered hardware. We do not apply the 4 \u00d7 4 algorithm recursively, to leverage the efficient implementation of matrix multiplication on moderate-size matrices (2,048 \u00d7 2,048 in this case). We study two hardware devices\u00a0commonly used in machine learning and scientific computing: an Nvidia V100 graphics processing unit (GPU) and a Google tensor processing unit (TPU) v2. The factorization obtained by AlphaTensor is transformed into JAX30 code, which is compiled ( just in time) before benchmarking.\nFigure\u00a05a,b shows the efficiency of the AlphaTensor-discovered algorithms on the GPU and the TPU, respectively. AlphaTensor discovers algorithms that outperform the\u00a0Strassen-square algorithm, which is a fast algorithm for large square matrices31,32. Although the discovered algorithm has the same theoretical complexity as Strassen-square, it outperforms it in practice, as it is optimized for the considered hardware. Interestingly, AlphaTensor finds algorithms\n52 | Nature | Vol 610 | 6 October 2022"
        },
        {
            "heading": "Article",
            "text": "with a larger number of additions compared with Strassen-square (or equivalently, denser decompositions), but the discovered algorithms generate individual operations that can be efficiently fused by the specific XLA33 grouping procedure and thus are more tailored towards the compiler stack we use. The algorithms found by AlphaTensor also provide gains on matrix sizes larger than what they were optimized for. Finally, Fig.\u00a05c shows the importance of tailoring to particular hardware, as algorithms optimized for one hardware do not perform as well on other hardware."
        },
        {
            "heading": "Discussion",
            "text": "Trained from scratch, AlphaTensor discovers matrix multiplication algorithms that are more efficient than existing human and computer-designed algorithms. Despite improving over known algorithms, we note that a limitation of AlphaTensor is the need to pre-define a set of potential factor entries F, which discretizes the search space but can possibly lead to missing out on efficient algorithms. An interesting direction for future research is to adapt AlphaTensor to search for F. One important strength of AlphaTensor is its flexibility to support complex stochastic and non-differentiable rewards (from the tensor rank to practical efficiency on specific hardware), in addition to finding algorithms for custom operations in a wide variety of spaces (such as finite fields). We believe this will spur applications of AlphaTensor towards designing algorithms that optimize metrics that we did not consider here, such as numerical stability or energy usage.\nThe discovery of matrix multiplication algorithms has far-reaching implications, as matrix multiplication sits at the core of many computational tasks, such as matrix inversion, computing the\u00a0determinant and solving linear systems, to name a few7. We also note that our methodology can be extended to tackle related primitive mathematical problems, such as computing other notions of rank (for example, border rank\u2014see Supplementary Information), and NP-hard matrix factorization problems (for example, non-negative factorization). By tackling a core NP-hard computational problem in mathematics using DRL\u2014the computation of tensor ranks\u2014AlphaTensor demonstrates the viability of DRL in addressing difficult mathematical problems, and potentially assisting mathematicians in discoveries."
        },
        {
            "heading": "Online content",
            "text": "Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-022-05172-4.\n1. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140\u20131144 (2018). 2. Strassen, V. Gaussian elimination is not optimal. Numer. Math. 13, 354\u2013356 (1969). 3. B\u00fcrgisser, P., Clausen, M. & Shokrollahi, A. Algebraic Complexity Theory Vol. 315 (Springer\nScience & Business Media, 2013). 4. Bl\u00e4ser, M. Fast matrix multiplication. Theory Comput. 5, 1\u201360 (2013). 5. Landsberg, J. M. Geometry and Complexity Theory 169 (Cambridge Univ. Press, 2017). 6. Pan, V. Y. Fast feasible and unfeasible matrix multiplication. Preprint at https://arxiv.org/\nabs/1804.04102 (2018). 7. Lim, L.-H. Tensors in computations. Acta Numer. 30, 555\u2013764 (2021). 8. Sch\u00f6nhage, A. Partial and total matrix multiplication. SIAM J. Comput. 10, 434\u2013455 (1981). 9. Coppersmith, D. & Winograd, S. Matrix multiplication via arithmetic progressions. In ACM\nSymposium on Theory of Computing 1\u20136 (ACM, 1987). 10. Strassen, V. The asymptotic spectrum of tensors and the exponent of matrix\nmultiplication. In 27th Annual Symposium on Foundations of Computer Science 49\u201354 (IEEE, 1986).\n11. Le Gall, F. Powers of tensors and fast matrix multiplication. In International Symposium on Symbolic and Algebraic Computation 296\u2013303 (ACM, 2014). 12. Alman, J. & Williams, V. V. A refined laser method and faster matrix multiplication. In ACM-SIAM Symposium on Discrete Algorithms 522\u2013539 (SIAM, 2021). 13. Gauss, C. F. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solum Ambientium (Perthes and Besser, 1809). 14. Hillar, C. J. & Lim, L.-H. Most tensor problems are NP-hard. J. ACM 60, 1\u201339 (2013). 15. Laderman, J. D. A noncommutative algorithm for multiplying 3 \u00d7 3 matrices using 23\nmultiplications. Bull. Am. Math. Soc. 82, 126\u2013128 (1976). 16. Hopcroft, J. E. & Kerr, L. R. On minimizing the number of multiplications necessary for\nmatrix multiplication. SIAM J. Appl. Math. 20, 30\u201336 (1971). 17. Vervliet, N., Debals, O., Sorber, L., Van Barel, M. & De Lathauwer, L. Tensorlab 3.0 (2016);\nhttps://www.tensorlab.net/ 18. Smirnov, A. V. The bilinear complexity and practical algorithms for matrix multiplication.\nComput. Math. Math. Phys. 53, 1781\u20131795 (2013). 19. Sedoglavic, A. & Smirnov, A. V. The tensor rank of 5x5 matrices multiplication is bounded\nby 98 and its border rank by 89. In Proc. 2021 on International Symposium on Symbolic and Algebraic Computation 345\u2013351 (ACM, 2021).\n20. Heule, M. J., Kauers, M. & Seidl, M. New ways to multiply 3 \u00d7 3-matrices. J. Symb. Comput. 104, 899\u2013916 (2021). 21. Hubert, T. et\u00a0al. Learning and planning in complex action spaces. In International Conference on Machine Learning 4476\u20134486 (PMLR, 2021). 22. Zhang, W. & Dietterich, T. G. A reinforcement learning approach to job-shop scheduling. In International Joint Conferences on Artificial Intelligence Vol. 95, 1114\u20131120 (Morgan Kaufmann Publishers, 1995).\n8,192\n10,240\n12,288\n14,336\n16,384\n18,432\n20,480\nM at\nrix s\niz e\n4.3%\n8.5%\n6.8%\n10.7%\n10.1%\n13.3%\n16.1%\n19.6%\n13.8% 16.6%\n15.3%\n17.9%\n21.3% 23.9%\nAlphaTensor\n8,192\n10,240\n12,288\n14,336\n16,384\n18,432\n20,480\nM at\nrix s\niz e\n6.6% 10.3%\n9.0% 12.4%\n8.9% 13.9%\n9.2% 13.4%\n7.2% 11.2%\n6.9% 12.3%\n8.4% 13.9%\nTPU\nGPU\nB en\nch m\nar k\nd ev\nic e\n10.3%\n2.8%\n4.4%\n8.5%\nOptimized for TPU Optimized for GPU\nSpeed-up on Nvidia V100 GPU Speed-up on TPU v2 Speed-up of tailored agorithms on both devices\na b c\nStrassen-square AlphaTensor Strassen-square\nFig. 5 | Speed-ups of the AlphaTensor-discovered algorithm. a,b, Speed-ups (%) of the AlphaTensor-discovered algorithms tailored for a GPU (a) and a TPU (b), optimized for a matrix multiplication of size 8,192 \u00d7 8,192. Speed-ups are measured relative to standard (for example, cuBLAS for the GPU) matrix multiplication on the same hardware. Speed-ups are reported for various\nmatrix sizes (despite optimizing the algorithm only on one matrix size). We also report the speed-up of the\u00a0Strassen-square\u00a0 algorithm. The median speed-up is reported over 200 runs. The standard deviation over runs is <0.4 percentage points (see Supplementary Information for more details). c, Speed-up of both algorithms (tailored to a GPU and a TPU) benchmarked on both devices.\nNature | Vol 610 | 6 October 2022 | 53\n23. Vaswani, A. Attention is all you need. In International Conference on Neural Information Processing Systems Vol 30, 5998\u20136008 (Curran Associates, 2017). 24. Ho, J., Kalchbrenner, N., Weissenborn, D. & Salimans, T. Axial attention in multidimensional transformers. Preprint at https://arxiv.org/abs/1912.12180 (2019). 25. Drevet, C.-\u00c9., Islam, M. N. & Schost, \u00c9. Optimization techniques for small matrix multiplication. Theor. Comput. Sci. 412, 2219\u20132236 (2011). 26. Sedoglavic, A. A non-commutative algorithm for multiplying (7 \u00d7 7) matrices using 250 multiplications. Preprint at https://arxiv.org/abs/1712.07935 (2017). 27. Battaglia, P. W. et\u00a0al. Relational inductive biases, deep learning, and graph networks. Preprint at https://arxiv.org/abs/1806.01261 (2018). 28. Balog, M., van Merri\u00ebnboer, B., Moitra, S., Li, Y. & Tarlow, D. Fast training of sparse graph neural networks on dense hardware. Preprint at https://arxiv.org/abs/1906.11786 (2019). 29. Ye, K. & Lim, L.-H. Fast structured matrix computations: tensor rank and Cohn\u2013Umans method. Found. Comput. Math. 18, 45\u201395 (2018). 30. Bradbury, J. et\u00a0al. JAX: composable transformations of Python+NumPy programs. GitHub http://github.com/google/jax (2018). 31. Benson, A. R. & Ballard, G. A framework for practical parallel fast matrix multiplication. ACM SIGPLAN Not. 50, 42\u201353 (2015). 32. Huang, J., Smith, T. M., Henry, G. M. & Van De Geijn, R. A. Strassen\u2019s algorithm reloaded. In International Conference for High Performance Computing, Networking, Storage and Analysis 690\u2013701 (IEEE, 2016). 33. Abadi, M. et\u00a0al. Tensorflow: a system for large-scale machine learning. In USENIX Symposium On Operating Systems Design And Implementation 265\u2013283 (USENIX, 2016). Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. \u00a9 The Author(s) 2022"
        },
        {
            "heading": "Article",
            "text": ""
        },
        {
            "heading": "Methods",
            "text": ""
        },
        {
            "heading": "TensorGame",
            "text": "TensorGame is played as follows. The start position S0 of the game corresponds to the tensor T representing the bilinear operation of interest, expressed in some basis. In each step t of the game, the player writes down three vectors (u(t), v(t), w(t)), which specify the rank-1 tensor u(t) \u2297 v(t) \u2297 w(t), and the state of the game is updated by subtracting the newly written down factor:\nS S u v w\u2190 \u2212 \u2297 \u2297 . (2)t t t t t \u22121 ( ) ( ) ( )\nThe game ends when the state reaches the zero tensor, 0=RS . This means that the factors written down throughout the game form a factorization of the start tensor S0, that is, = \u2211 \u2297 \u2297t R t t t 0 =1\n( ) ( ) ( )S u v w . This factorization is then scored. For example, when optimizing for asymptotic time complexity the score is \u2212R, and when optimizing for practical runtime the algorithm corresponding to the factorization {( , , )}t t t t R( ) ( ) ( ) =1u v w is constructed (see Algorithm 1) and then benchmarked on the fly (see\u00a0Supplementary Information). In practice, we also impose a limit Rlimit on the maximum number of moves in the game, so that a weak player is not stuck in unnecessarily (or even infinitely) long games. When a game ends \u00a0because it has run out of moves, a penalty score is given so that it is never advantageous to deliberately exhaust the move limit. For example, when optimizing for asymptotic time complexity, this penalty is derived from an upper bound on the tensor rank of the final residual tensor R limitS . This upper bound on the tensor rank is obtained by summing the matrix ranks of the slices of the tensor.\nTensorGame over rings. We say that the decomposition of nT in equation\u00a0(1) is in a ring E (defining the arithmetic operations) if each of the factors u(t), v(t) and w(t) has entries belonging to the set E, and additions and multiplications are interpreted according to E. The tensor rank depends, in general, on the ring. At each step of TensorGame, the additions and multiplications in equation\u00a0(2) are interpreted in E . For example, when working in 2Z , (in this case, the factors u(t), v(t) and w(t) live in F = {0, 1}), a modulo 2 operation is applied after each state update (equation\u00a0(2)).\nWe note that integer-valued decompositions u(t), v(t) and w(t) lead to decompositions in arbitrary rings E. Hence, provided F only contains integers, algorithms we find in standard arithmetic apply more generally to any ring."
        },
        {
            "heading": "AlphaTensor",
            "text": "AlphaTensor builds on AlphaZero1 and its extension Sampled AlphaZero21, combining a deep neural network with a sample-based MCTS search algorithm.\nThe deep neural network, f\u03b8(s) = (\u03c0, z) parameterized by \u03b8, takes as input the current state s of the game and outputs a probability distribution \u03c0(\u22c5\u2223s) over actions and z(\u22c5\u2223s) over returns (sum of future rewards) G. The parameters \u03b8 of the deep neural network are trained by reinforcement learning from self-play games and synthetic demonstrations. Self-play games are played by actors, running a sample-based MCTS search at every state st encountered in the game. The MCTS search returns an improved probability distribution over moves from which an action at is selected and applied to the environment. The sub-tree under at is reused for the subsequent search at st+1. At the end of the game, a return G is obtained and the trajectory is sent to the learner to update the neural network parameters \u03b8. The distribution over returns z(\u22c5\u2223st) is learned through distributional reinforcement learning using the quantile regression distributional loss34, and the network policy \u03c0(\u22c5\u2223st) is updated using a Kullback\u2013Leibler divergence loss, to maximize its similarity to the search policy for self-play games or to the next action for synthetic demonstrations. We use the Adam optimizer35\nwith decoupled weight decay36 to optimize the parameters \u03b8 of the neural network.\nSample-based MCTS search. The sample-based MCTS search is very similar to the one described in Sampled AlphaZero. Specifically, the search consists of a series of simulated trajectories of TensorGame that are aggregated in a tree. The search tree therefore\u00a0consists of nodes representing states and edges representing actions. Each state-action pair (s, a) stores a set of statistics N s a Q s a \u03c0 s a( , ), ( , ), \u02c6( , ), where N(s, a) is the visit count, Q(s, a) is the action value and \u03c0 s a\u02c6( , ) is the empirical policy probability. Each simulation traverses the tree from the root state s0 until a leaf state sL is reached by recursively selecting in each state s an action a that has not been frequently explored, has high empirical policy probability and high value. Concretely, actions within the tree are selected by maximizing over the probabilistic upper confidence tree bound21,37\n\u2211 Q s a c s \u03c0 s a N s b\nN s a argmax ( , ) + ( ) \u22c5 \u02c6( , )\n( , )\n1 + ( , ) ,\na\nb\nwhere c(s) is an exploration factor controlling the influence of the empirical policy \u03c0 s a\u02c6( , ) relative to the values Q(s, a) as nodes are visited more often. In addition, a transposition table is used to recombine different action sequences if they reach the exact same tensor. This can happen particularly often in TensorGame as actions are commutative. Finally, when a leaf state sL is reached, it is evaluated by the neural network, which returns K actions {ai} sampled from \u03c0(a\u2223sL), alongside the empirical distribution \u2211\u03c0 a s \u03b4\u02c6( ) = K i a aL 1 , i\nand a value v(sL) constructed from z(\u22c5\u2223sL). Differently from AlphaZero and Sampled AlphaZero, we chose v not to be the mean of the distribution of returns z(\u22c5\u2223sL) as is usual in most reinforcement learning agents, but instead to be a risk-seeking value, leveraging the facts that TensorGame is a deterministic environment and that we are primarily interested in finding the best trajectory possible. The visit counts and values on the simulated trajectory are then updated in a backward pass as in Sampled AlphaZero.\nPolicy improvement. After simulating N(s) trajectories from state s using MCTS, the normalized visit counts of the actions at the root of the search tree N(s, a)/N(s) form a sample-based improved policy. Differently from AlphaZero and Sampled AlphaZero, we use an adaptive temperature scheme to smooth the normalized visit counts distribution as some states can accumulate an order of magnitude more visits than others because of sub-tree reuse and transposition table. Concretely, we define the improved policy as \u03c0 s a N s a N s b\u02c6( , ) = ( , )/ \u2211 ( , )\u03c4 s b\n\u03c4 s1/ ( ) 1/ ( )I where \u03c4 s N s N N N( ) = log ( )/log if > and 1 otherwise, with N being a hyperparameter. For training, we use I\u03c0\u0302 directly as a target for the network policy \u03c0. For acting, we additionally discard all actions that have a value lower than the value of the most visited action, and sample proportionally to \u03c0\u0302I among those remaining high-value actions.\nLearning one agent for multiple target tensors. We train a single agent to decompose the different tensors n m p, ,T in a given arithmetic (standard or modular). As the network works with fixed-size inputs, we pad all tensors (with zeros) to the size of the largest tensor we consider ( 5T , of size 25 \u00d7 25 \u00d7 25). At the beginning of each game, we sample uniformly at random a target n m p, ,T , and play TensorGame. Training a single agent on different targets leads to better results thanks to the\u00a0transfer between targets. All our results reported in Fig.\u00a03 are obtained using multiple runs of this multi-target setting. We also train a single agent to decompose tensors in both arithmetics. Owing to learned transfer between the two arithmetics, this agent discovers a different distribution of algorithms (of the same ranks) in standard arithmetic than the agent trained on standard arithmetic only, thereby increasing the overall diversity of discovered algorithms.\nSynthetic demonstrations. The synthetic demonstrations buffer contains tensor-factorization pairs, where the factorizations u v w{( , , )}r r r r R( ) ( ) ( )\n=1 are first generated at random, after which the tensor u v w= \u2211 \u2297 \u2297r R r r r =1\n( ) ( ) ( )D is formed. We create a dataset containing 5 million such tensor-factorization pairs. Each element in the factors is sampled independently and identically distributed (i.i.d.) from a given categorical distribution over F (all possible values that can be taken). We discarded instances whose decompositions were clearly suboptimal (contained a factor with u = 0, v = 0, or w = 0).\nIn addition to these synthetic demonstrations, we further add to the demonstration buffer previous games that have achieved large scores to reinforce the good moves made by the agent in these games.\nChange of basis. The rank of a bilinear operation does not depend on the basis in which the tensor representing it is expressed, and for any invertible matrices A, B and C we have Rank ( ) = Rank ( )( , , )T T A B C , where ( , , )A B CT is the tensor after change of basis given by\n\u2211 \u2211 \u2211= . (3)ijk a\nS\nb\nS\nc\nS\nia jb kc abc ( , , ) =1 =1 =1 T TA B CA B C\nHence, exhibiting a rank-R decomposition of the matrix multiplication tensor nT expressed in any basis proves that the product of two n \u00d7 n matrices can be computed using R scalar multiplications. Moreover, it is straightforward to convert such a rank-R decomposition into a rank-R decomposition in the canonical basis, thus yielding a practical algorithm of the form shown in Algorithm 1. We leverage this observation by expressing the matrix multiplication tensor nT in a large number of randomly generated bases (typically 100,000) in addition to the canonical basis, and letting AlphaTensor play games in all bases in parallel.\nThis approach has three appealing properties: (1) it provides a natural exploration mechanism as playing games in different bases automatically injects diversity into the games played by the agent; (2) it exploits properties of the problem as the agent need not succeed in all bases\u2014it is sufficient to find a low-rank decomposition in any of the bases; (3) it enlarges coverage of the algorithm space because a decomposition with entries in a finite set F = {\u22122, \u22121, 0, 1, 2} found in a different basis need not have entries in the same set when converted back into the canonical basis.\nIn full generality, a basis change for a 3D tensor of size S \u00d7 S \u00d7 S is specified by three invertible S \u00d7 S matrices A, B and C. However, in our procedure, we sample bases at random and impose two restrictions: (1) A = B = C, as this performed better in early experiments, and (2) unimodularity ( Adet \u2208 {\u22121, + 1}), which ensures that after converting an integral factorization into the canonical basis it still contains integer entries only (this is for representational convenience and numerical stability of the resulting algorithm). See Supplementary Information for the exact algorithm.\nSigned permutations. In addition to playing (and training on) games in different bases, we also utilize a data augmentation mechanism whenever the neural network is queried in a new MCTS node. At acting time, when the network is queried, we transform the input tensor by applying a change of basis\u2014where the change of basis matrix is set to a random signed permutation. We then query the network on this transformed input tensor, and finally invert the transformation in the network\u2019s policy predictions. Although this data augmentation procedure can be applied with any generic change of basis matrix (that is, it is not restricted to signed permutation matrices), we use signed permutations mainly for computational efficiency. At training time, whenever the neural network is trained on an (input, policy targets, value target) triplet (Fig.\u00a02), we apply a randomly chosen signed permutation to both the input and the policy targets, and train the network on this\ntransformed triplet. In practice, we sample 100 signed permutations at the beginning of an experiment, and use them thereafter.\nAction canonicalization. For any \u03bb1, \u03bb2, \u03bb3 \u2208 {\u22121, +1} such that \u03bb1\u03bb2\u03bb3 = 1, the actions (\u03bb1u, \u03bb2v, \u03bb3w) and (u, v, w) are equivalent because they lead to the same rank-one tensor (\u03bb1u) \u2297 (\u03bb2v) \u2297 (\u03bb3w) = u \u2297 v \u2297 w. To prevent the network from wasting capacity on predicting multiple equivalent actions, during training we always present targets (u, v, w) for the policy head in a canonical form, defined as having the first non-zero element of u and the first non-zero element of v strictly positive. This is well defined because u or v cannot be all zeros (if they are to be part of a minimal rank decomposition), and for any (u, v, w) there are unique \u03bb1, \u03bb2, \u03bb3 \u2208 {\u22121, +1} (with \u03bb1\u03bb2\u03bb3 = 1) that transform it into canonical form. In case the network predicts multiple equivalent actions anyway, we merge them together (summing their empirical policy probabilities) before inserting them into the MCTS tree.\nTraining regime. We train AlphaTensor on a TPU v3, with a total batch size of 2,048. We use 64 TPU cores, and train for 600,000 iterations. On the actor side, the games are played on standalone TPU v4, and we use 1,600 actors. In practice, the procedure \u00a0takes a week to converge."
        },
        {
            "heading": "Neural network",
            "text": "The architecture is composed of a torso, followed by a policy head that predicts a distribution over actions, and a value head that predicts a distribution of the returns from the current state (see Extended Data Fig.\u00a03).\nInput. The input to the network contains all the relevant information of the current state and is composed of a list of tensors and a list of scalars. The most important piece of information is the current 3D tensor St of size S \u00d7 S \u00d7 S. (For simplicity, in the description here we assume that all the three dimensions of the tensor are equal in size. The generalization to different sizes is straightforward.) In addition, the model is given access to the last h actions (h being a hyperparameter usually set to 7), represented as h rank-1 tensors that are concatenated to the input. The list of scalars includes the time index t of the current action (where 0 \u2264 t < Rlimit).\nTorso. The torso of the network is in charge of mapping both scalars and tensors from the input to a representation that is useful to both policy and value heads. Its architecture is based on a modification of transformers23, and its main signature is that it operates over three S \u00d7 S grids projected from the S \u00d7 S \u00d7 S input tensors. Each grid represents two out of the three modes of the tensor. Defining the modes of the tensor as , ,U V W, the rows and columns of the first grid are associated to U and V, respectively, the rows and columns of the second grid are associated to W and U, and the rows and columns of the third grid are associated to V and W. Each element of each grid is a feature vector, and its initial value is given by the elements of the input tensors along the grid\u2019s missing mode. These feature vectors are enriched by concatenating an S \u00d7 S \u00d7 1 linear projection from the scalars. This is followed by a linear layer projecting these feature vectors into a 512-dimensional space.\nThe rest of the torso is a sequence of attention-based blocks with the objective of propagating information between the three grids. Each of those blocks has three stages, one for every pair of grids. In each stage, the grids involved are concatenated, and axial attention24 is performed over the columns. It is noted that in each stage we perform in parallel S self-attention operations of 2S elements in each. The representation sent to the policy head corresponds to the 3S2\u00a0512-dimensional feature vectors produced by the last layer of the torso. A detailed description of the structure of the torso is specified in Extended Data Fig.\u00a04 (top) and Appendix A.1.1 in Supplementary Information.\nPolicy head. The policy head uses the transformer architecture23 to model an autoregressive policy. Factors are decomposed into k"
        },
        {
            "heading": "Article",
            "text": "tokens of dimensionality d such that k \u00d7 d = 3S. The transformer conditions on the tokens already generated and cross-attends to the features produced by the torso. At training time, we use teacher-forcing, that is, the ground truth actions are decomposed into tokens and taken as inputs into the causal transformer in such a way that the prediction of a token depends only on the previous tokens. At inference time, K actions are sampled from the head. The feature representation before the last linear layer of the initial step (that is, the only step that is not conditioned on the ground truth) is used as an input to the value head, described below. Details of the architecture are presented in Extended Data Fig.\u00a04 (centre) and Appendix A.1.2 in Supplementary Information.\nValue head. The value head is composed of a four-layer multilayer perceptron whose last layer produces q outputs corresponding to the\n, , \u2026q q q q 1 2 3 2 2 \u2212 1 2 quantiles. In this way, the value head predicts the distribution of returns from this state in the form of values predicted for the aforementioned quantiles34. At inference time, we encourage the agent to be risk-seeking by using the average of the predicted values for quantiles over 75%. A detailed description of the value head is presented in Extended Data Fig.\u00a04 (bottom) and Appendix A.1.3 in Supplementary Information."
        },
        {
            "heading": "Related work",
            "text": "The quest for efficient matrix multiplication algorithms started with Strassen\u2019s breakthrough in ref. 2, which showed that one can multiply 2 \u00d7 2 matrices using 7 scalar multiplications, leading to an algorithm of complexity O n( )2.81 . This led to the development of a very active field of mathematics attracting worldwide interest, which studies the asymptotic complexity of matrix multiplication (see refs. 3\u20136). So far, the best known complexity for matrix multiplication is n( )2.37286O (ref. 12), which improves over ref. 11, and builds on top of fundamental results in the field8\u201310. However, this does not yield practical algorithms, as such approaches become advantageous only for astronomical matrix sizes. Hence, a significant body of work aims at exhibiting explicit factorizations of matrix multiplication tensors, as these factorizations provide practical algorithms. After Strassen\u2019s breakthrough showing that Trank ( ) \u2264 72 , efficient algorithms for larger matrix sizes were found15,16,18,26,38. Most notably, Laderman showed in ref. 15 that 3 \u00d7 3 matrix multiplications can be performed with 23 scalar multiplications. In addition to providing individual low-rank factorizations, an important research direction aims at understanding the space of matrix multiplication algorithms\u2014as opposed to exhibiting individual low-rank factorizations\u2014by studying the symmetry groups and diversity of factorizations (see ref. 5 and references therein). For example, the symmetries of 2 \u00d7 2 matrix multiplication were studied in refs. 39\u201342, where Strassen\u2019s algorithm was shown to be essentially unique. The case of 3 \u00d7 3 was studied in ref. 43, whereas a symmetric factorization for all n is provided in ref. 44.\nOn the computational front, continuous optimization has been the main workhorse for decomposing tensors17,45,46, and in particular matrix multiplication tensors. Such continuous optimization procedures (for example, alternating least squares), however, yield approximate solutions, which correspond to inexact matrix multiplication algorithms with floating point operations. To circumvent this issue, regularization procedures have been proposed, such as ref. 18, to extract exact decompositions. Unfortunately, such approaches often require \u00a0substantial human intervention and expertise to decompose large tensors. A different line of attack was explored in refs. 47,48, based on learning the continuous weights of a two-layer network that mimics the structure of the matrix multiplication operation. This method, which is trained through supervised learning of matrix multiplication examples, finds approximate solutions to 2 \u00d7 2 and 3 \u00d7 3 matrix multiplications. In ref. 48, a quantization procedure is further used to obtain an exact decomposition for 2 \u00d7 2. Unlike continuous optimization-based\napproaches, AlphaTensor directly produces algorithms from the desired set of valid algorithms, and is flexible in that it allows us to optimize a wide range of (even non-differentiable) objectives. This unlocks tackling broader settings (for example, optimization in finite fields, optimization of runtime), as well as larger problems (for example, T4 and T5) than those previously considered. Different from continuous optimization, a boolean satisfiability (SAT)\u00a0based\u00a0 formulation of the problem of decomposing 3 \u00d7 3 matrix multiplication was recently proposed in ref. 20, which adds thousands of new decompositions of rank 23 to the list of known 3 \u00d7 3 factorizations. The approach relies on a state-of-the-art SAT solving procedure, where several assumptions and simplifications are made on the factorizations to reduce the search space. As is, this approach is, however, unlikely to scale to larger tensors, as the search space grows very quickly with the size.\nOn the practical implementation front, ref. 31 proposed several ideas to speed up implementation of fast matrix multiplication algorithms on central processing units\u00a0(CPUs). Different fast algorithms are then compared and benchmarked, and the potential speed-up of such algorithms is shown against standard multiplication. Other works focused on getting the maximal performance out of a particular fast matrix multiplication algorithm (Strassen\u2019s algorithm with one or two levels of recursion) on a CPU32 or a GPU49. These works show that, despite popular belief, such algorithms are of practical value. We see writing a custom low-level implementation of a given algorithm to be distinct from the focus of this paper\u2014developing new efficient algorithms\u2014and we believe that the algorithms we discovered can further benefit from a more efficient implementation by experts.\nBeyond matrix multiplication and bilinear operations, a growing amount of research studies the use of optimization and machine learning to improve the efficiency of computational operations. There are three levels of abstractions at which this can be done: (1) in the hardware design, for example, chip floor planning50, (2) at the hardware\u2013software interface, for example, program super-optimization of a reference implementation for specific hardware51, and (3) on the algorithmic level, for example, program induction52, algorithm selection53 or meta-learning54. Our work focuses on the algorithmic level of abstraction, although AlphaTensor is also flexible to discover efficient algorithms for specific hardware. Different from previous works, we focus on discovering matrix multiplication algorithms that are provably correct, without requiring initial reference implementations. We conclude by relating our work broadly to existing reinforcement learning methods for scientific discovery. Within mathematics, reinforcement learning was applied, for example, to theorem proving55\u201358, and to finding counterexamples refuting conjectures in combinatorics and graph theory59. Reinforcement learning was further shown to be useful in many areas in science, such as molecular design60,61 and synthesis62 and optimizing quantum dynamics63."
        },
        {
            "heading": "Data availability",
            "text": "The data used to train the system were generated synthetically according to the procedures explained in the paper. The algorithms discovered by AlphaTensor are available for download at https://github.com/ deepmind/alphatensor."
        },
        {
            "heading": "Code availability",
            "text": "An interactive notebook with code to check the non-equivalence of algorithms is provided. Moreover, the fast algorithms from the \u2018Algorithm discovery\u00a0results\u2019 section\u00a0on a GPU and a TPU are provided. These are available for download at https://github.com/deepmind/alphatensor. A full description of the AlphaZero algorithm that this work is based on is available in ref. 1, and the specific neural network architecture we use is described using pseudocode in the Supplementary Information.\n34. Dabney, W., Rowland, M., Bellemare, M. & Munos, R. Distributional reinforcement learning with quantile regression. In AAAI Conference on Artificial Intelligence Vol. 32, 2892\u20132901 (AAAI Press, 2018). 35. Kingma, D. P., & Ba, J. Adam: a method for stochastic optimization. In International Conference on Learning Representations (ICLR) (2015). 36. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR) (2019). 37. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484\u2013489 (2016). 38. Sedoglavic, A. A non-commutative algorithm for multiplying 5x5 matrices using 99 multiplications. Preprint at https://arxiv.org/abs/1707.06860 (2017). 39. de Groote, H. F. On varieties of optimal algorithms for the computation of bilinear mappings II. optimal algorithms for 2 \u00d7 2-matrix multiplication. Theor. Comput. Sci. 7, 127\u2013148 (1978). 40. Burichenko, V. P. On symmetries of the Strassen algorithm. Preprint at https://arxiv.org/ abs/1408.6273 (2014). 41. Chiantini, L., Ikenmeyer, C., Landsberg, J. M. & Ottaviani, G. The geometry of rank decompositions of matrix multiplication I: 2 \u00d7 2 matrices. Exp. Math. 28, 322\u2013327 (2019). 42. Grochow, J. A. & Moore, C. Designing Strassen\u2019s algorithm. Preprint at https://arxiv.org/ abs/1708.09398 (2017). 43. Ballard, G., Ikenmeyer, C., Landsberg, J. M. & Ryder, N. The geometry of rank decompositions of matrix multiplication II: 3 \u00d7 3 matrices. J. Pure Appl. Algebra 223, 3205\u20133224 (2019). 44. Grochow, J. A. & Moore, C. Matrix multiplication algorithms from group orbits. Preprint at https://arxiv.org/abs/1612.01527 (2016). 45. Kolda, T. G. & Bader, B. W. Tensor decompositions and applications. SIAM Rev. 51, 455\u2013500 (2009). 46. Bernardi, A., Brachat, J., Comon, P. & Mourrain, B. General tensor decomposition, moment matrices and applications. J. Symb. Comput. 52, 51\u201371 (2013). 47. Elser, V. A network that learns Strassen multiplication. J. Mach. Learn. Res. 17, 3964\u20133976 (2016). 48. Tschannen, M., Khanna, A. & Anandkumar, A, StrassenNets: deep learning with a multiplication budget. In International Conference on Machine Learning 4985\u20134994 (PMLR, 2018). 49. Huang, J., Yu, C. D. & Geijn, R. A. V. D. Strassen\u2019s algorithm reloaded on GPUs. ACM Trans. Math. Softw. 46, 1\u201322 (2020). 50. Mirhoseini, A. et\u00a0al. A graph placement methodology for fast chip design. Nature 594, 207\u2013212 (2021). 51. Bunel, R., Desmaison, A., Kohli, P., Torr, P. H. & Kumar, M. P. Learning to superoptimize programs. In International Conference on Learning Representations (ICLR) (2017). 52. Li, Y., Gimeno, F., Kohli, P. & Vinyals, O. Strong generalization and efficiency in neural programs. Preprint at https://arxiv.org/abs/2007.03629 (2020). 53. Lagoudakis, M. G. et\u00a0al. Algorithm selection using reinforcement learning. In International Conference on Machine Learning 511\u2013518 (Morgan Kaufmann Publishers, 2000). 54. Schmidhuber, J. Evolutionary Principles in Self-Referential Learning. On Learning now to Learn: The Meta-Meta-Meta...-Hook. Diploma thesis, Technische Univ. Munchen (1987). 55. Kaliszyk, C., Urban, J., Michalewski, H. & Ol\u0161\u00e1k, M. Reinforcement learning of theorem proving. In International Conference on Neural Information Processing Systems 8836\u20138847 (Curran Associates, 2018). 56. Piotrowski, B. & Urban, J. ATPboost: learning premise selection in binary setting with ATP feedback. In International Joint Conference on Automated Reasoning 566\u2013574 (Springer, 2018). 57. Bansal, K., Loos, S., Rabe, M., Szegedy, C. & Wilcox, S. HOList: an environment for machine learning of higher order logic theorem proving. In International Conference on Machine Learning 454\u2013463 (PMLR, 2019). 58. Zombori, Z., Urban, J. & Brown, C. E. Prolog technology reinforcement learning prover. In International Joint Conference on Automated Reasoning 489\u2013507 (Springer, 2020). 59. Wagner, A. Z. Constructions in combinatorics via neural networks. Preprint at https:// arxiv.org/abs/2104.14516 (2021). 60. Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning for de\u00a0novo drug design. Sci. Adv. 4, eaap7885 (2018). 61. Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley, P. Optimization of molecules via deep reinforcement learning. Sci. Rep. 9, 10752 (2019). 62. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural networks and symbolic AI. Nature 555, 604\u2013610 (2018). 63. Dalgaard, M., Motzoi, F., S\u00f8rensen, J. J. & Sherson, J. Global optimization of quantum dynamics with AlphaZero deep exploration. npj Quantum Inf. 6, 6 (2020). 64. Fast matrix multiplication algorithms catalogue. Universit\u00e9 de Lille https://fmm.univlille.fr/ (2021). Acknowledgements We thank O. Fawzi, H. Fawzi, C. Ikenmeyer, J. Ellenberg, C. Umans and A. Wigderson for the inspiring\u00a0discussions on the use of machine learning for maths; A. Davies, A. Gaunt, P. Mudigonda, R. Bunel and O. Ronneberger for their advice on early drafts of the paper; A. Ruderman, M. Bauer, R. Leblond, R. Kabra and B. Winckler for participating in a hackathon at the early stages of the project; D. Visentin, R. Tanburn and S. Noury for sharing their expertise on TPUs; P.\u00a0Wang and R.\u00a0Zhao for their help on benchmarking algorithms;\u00a0 G. Holland, A. Pierce, N. Lambert and C. Meyer for assistance coordinating the research; and our colleagues at DeepMind for encouragement and support. Author contributions A.F. conceived the project, with support from B.R.-P. and P.K.; T.H., A.H. and J.S. developed the initial AlphaZero codebase, and B.R.-P., M. Balog, A.F., A.N., F.J.R.R. and G.S. developed an early supervised network prototype. A.H., T.H., B.R.-P., M. Barekatain and J.S. designed the network architecture used in the paper. T.H., J.S., A.H., M. Barekatain, A.F., M. Balog and F.J.R.R. developed the tensor decomposition environment and data generation pipeline, and A.H., T.H., M. Barekatain, M. Balog, B.R.-P., F.J.R.R. and A.N. analysed the experimental results and algorithms discovered by AlphaTensor. A.N., A.F. and T.H. developed the benchmarking pipeline and experiments, and B.R.-P., F.J.R.R. and A.N. extended the approach to structured tensors. A.F., B.R.-P., G.S. and A.N. proved the results in the paper. D.S., D.H. and P.K. contributed technical advice and ideas. A.F., M. Balog, B.R.-P., F.J.R.R., A.N. and T.H. wrote the paper. These authors contributed equally, and are listed alphabetically by last name after the corresponding author: A.F., M. Balog, A.H., T.H., B.R.-P.\u00a0These authors contributed equally, and are listed alphabetically by last name: M. Barekatain, A.N., F.J.R.R., J.S. and G.S. Competing interests The authors of the paper are planning to file a patent application relating to subject matter contained in this paper in the name of DeepMind Technologies Limited. Additional information Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41586-022-05172-4. Correspondence and requests for materials should be addressed to Alhussein Fawzi. Peer review information Nature thanks Grey Ballard, Jordan Ellenberg, Lek-Heng Lim, Michael Littman and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints."
        },
        {
            "heading": "Article",
            "text": "Extended Data Fig. 1 | Algorithm for multiplying 4 \u00d7 4 matrices in modular arithmetic ( 2Z ) with 47 multiplications. This outperforms the two-level Strassen\u2019s algorithm, which involves 72 = 49 multiplications.\nExtended Data Fig. 2 | Algorithm for multiplying 4 \u00d7 5 by 5 \u00d7 5 matrices in standard arithmetic with 76 multiplications. This outperforms the previously best known algorithm, which involves 80 multiplications."
        },
        {
            "heading": "Article",
            "text": "Extended Data Fig. 3 | AlphaTensor\u2019s network architecture. The network takes as input the list of tensors containing the current state and previous history of actions, and a list of scalars, such as the time index of the current action. It produces two kinds of outputs: one representing the value, and the\nother inducing a distribution over the action space from which we can sample from. The architecture of the network is accordingly designed to have a common torso, and two heads, the value and the policy heads. c is set to 512 in all experiments.\nExtended Data Fig. 4 | Detailed view of AlphaTensor\u2019s architecture, included torso, policy and value head. We refer to Algorithms A.1-A.11 in Supplementary Information for the details of each component."
        },
        {
            "heading": "Article",
            "text": "Extended Data Table 1 | Rank results obtained by combining decompositions\u00a0(in standard arithmetic)\nThe table shows the cases where we were able to obtain an improvement over state-of-the-art, for tensors Tn m p, , (with n, m, p\u226412). The recipe column indicates the low-level matrix multiplication algorithms used to build the corresponding factorization. \u2329n, m, p\u232a denotes the best known bound on the rank of Tn m p, , ; see Appendix H in Supplementary Information for more details. For tensors that were directly decomposed by AlphaTensor, the recipe shows a star mark, e.g. \u23293, 4, 5\u232a*. All the factorizations are made available.\nExtended Data Table 2 | Result of applying AlphaTensor to the tensor representing the cyclic convolution operation\nAlphaTensor finds the discrete Fourier matrix (DFT) and the inverse DFT matrix in finite fields. The figure shows the decompositions found by AlphaTensor of the n \u00d7 n \u00d7 n tensor representing the cyclic convolution of two vectors, for three different values of n in the finite field of order 17. The action space, characterized by the number of possible factor triplets {u(r), v(r), w(r)}, is thus 173n, which is of the order of 1029 for n = 8. Despite the huge action space, AlphaTensor finds the optimal rank-n decompositions for the three values of n. The factors in the figure are stacked vertically, i.e., U = [u(1), \u2026, u(n)]. For ease of visualization, the factor entries have been expressed in terms of powers of an n-th primitive root of unity in the finite field. Within each column, each colour uniquely represents one element of the field (e.g., for the column n = 4, we have depicted in grey 40 = 44 = 4\u22124 = 1). By inspecting the patterns in the decompositions, one could extrapolate the results for other values of n and other fields. Indeed, the factors u(r) and v(r) correspond to the DFT coefficients, since = =u v zkrk r k r( ) ( ) , whereas the factors w(r) correspond to the inverse DFT, since w z n/krk r( ) = \u2212 for 0\u2264k, r < n, where z is an n-th primitive root of unity (i.e., zn = 1 and zj \u2260 1 for any 1\u2264j < n)."
        }
    ],
    "title": "Discovering faster matrix multiplication algorithms with reinforcement learning",
    "year": 2022
}