{
    "abstractText": "The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.",
    "authors": [
        {
            "affiliations": [],
            "name": "TSUYOSHI YONEDA"
        }
    ],
    "id": "SP:91fbe858aa1308595b4b744631f1949298052ee1",
    "references": [
        {
            "authors": [
                "Z. Allen-Zhu",
                "Y. Li",
                "Z. Song"
            ],
            "title": "A convergence theory for deep learning via over-parameterization",
            "venue": "Proc. 36th Intern. Conf. Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S. Amari"
            ],
            "title": "Any target function exists in a neighborhood of any sufficiently wide random network: a geometrical perspective",
            "venue": "Neural Computation,",
            "year": 2020
        },
        {
            "authors": [
                "J. Bourgain",
                "D. Li"
            ],
            "title": "Strong ill-posedness of the incompressible Euler equations in borderline",
            "venue": "Sobolev spaces. Invent. math.,",
            "year": 2015
        },
        {
            "authors": [
                "J. Bourgain",
                "D. Li"
            ],
            "title": "Strong illposedness of the incompressible Euler equation in integer Cm spaces",
            "venue": "Geom. funct. anal.,",
            "year": 2015
        },
        {
            "authors": [
                "D. Chae"
            ],
            "title": "Local existence and blow-up criterion for the Euler equations in the Besov spaces",
            "venue": "Asymptot. Anal.,",
            "year": 2004
        },
        {
            "authors": [
                "P. Cheridito",
                "A. Jentzen",
                "F. Rossmannek"
            ],
            "title": "Non-convergence of stochastic gradient descent in the training of deep neural networks",
            "venue": "J. Complexity,",
            "year": 2021
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal Function",
            "venue": "Math. Control Signals Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Z. Ding",
                "S. Chen",
                "Q. Li",
                "S.J. Wright"
            ],
            "title": "Overparameterization of deep ResNet: zero loss and mean-field analysis",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "T. Elgindi",
                "Jeong",
                "I.-J"
            ],
            "title": "Ill-posedness for the incompressible Euler equations in critical",
            "venue": "Sobolev spaces. Ann. PDE,",
            "year": 2017
        },
        {
            "authors": [
                "T. Elgindi",
                "N. Masmoudi"
            ],
            "title": "L\u221e ill-posedness for a class of equations arising in hydrodynamics",
            "venue": "Arch. Ration. Mech. Anal.,",
            "year": 2020
        },
        {
            "authors": [
                "B. Fehrman",
                "B. Gess",
                "A. Jentzen"
            ],
            "title": "Convergence rates for the stochastic gradient descent method for non-convex objective functions",
            "venue": "J. Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "S. Hayakawa",
                "T. Suzuki"
            ],
            "title": "On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces",
            "venue": "Neural Networks,",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "G.E. Hinton",
                "S. Osindero",
                "Teh",
                "T.-W"
            ],
            "title": "A fast learning algorithm for deep belief nets",
            "venue": "Neural computation,",
            "year": 2006
        },
        {
            "authors": [
                "M. Imaizumi",
                "K. Fukumizu"
            ],
            "title": "Deep Neural Networks Learn Non-Smooth Functions Effectively",
            "venue": "Proc. Twenty-Second Inter. Conf. Artificial Intel. Stat., PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "A. Jentzen",
                "A. Riekert"
            ],
            "title": "On the existence of global minima and convergence analyses for gradient descent methods in the training of deep neural networks",
            "venue": "J. Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "A. Jentzen",
                "A. Riekert"
            ],
            "title": "A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions",
            "venue": "J. Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "T. Kato"
            ],
            "title": "Nonstationary flows of viscous and ideal fluids in R3",
            "venue": "J. Func. Anal.,",
            "year": 1972
        },
        {
            "authors": [
                "Kato T",
                "G. Ponce"
            ],
            "title": "Commutator estimates and the Euler and Navier-Stokes equations",
            "venue": "Comm. Pure Appl. Math.,",
            "year": 1988
        },
        {
            "authors": [
                "K. Kawaguchi",
                "J. Huang",
                "L.P. Kaelbling"
            ],
            "title": "Effect of depth and width on local minima in deep learning",
            "venue": "Neural Computation,",
            "year": 2019
        },
        {
            "authors": [
                "S. Kuratsubo"
            ],
            "title": "On pointwise convergence of Fourier series of the indicator function of D dimensional ball",
            "venue": "J. Fourier Anal. Appl.,",
            "year": 2010
        },
        {
            "authors": [
                "S. Kuratsubo",
                "Nakai E",
                "K. Ootsubo"
            ],
            "title": "Generalized Hardy identity and relations to Gibbs-Wilbraham and Pinsky phenomena",
            "venue": "J. Funct. Anal.,",
            "year": 2010
        },
        {
            "authors": [
                "S. Kuratsubo",
                "E. Nakai"
            ],
            "title": "Multiple Fourier series and lattice point problems",
            "venue": "J. Funct. Anal.,",
            "year": 2022
        },
        {
            "authors": [
                "G. Misio lek",
                "T. Yoneda"
            ],
            "title": "Continuity of the solution map of the Euler equations in H\u00f6lder spaces and weak norm inflation in Besov spaces",
            "venue": "Trans. Amer. Math. Soc.,",
            "year": 2018
        },
        {
            "authors": [
                "H.C. Pak",
                "Y.J. Park"
            ],
            "title": "Existence of solution for the Euler equations in a critical Besov space B1 \u221e,1(R)",
            "venue": "Comm. Part. Differ. Equ.,",
            "year": 2004
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "Deep learning in neural net works: An overview",
            "venue": "Neural networks,",
            "year": 2015
        },
        {
            "authors": [
                "J. Schmidt-Hieber"
            ],
            "title": "Nonparametric regression using deep neural networks with ReLU activation function",
            "venue": "The Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "D. Soudry",
                "E. Hoffer",
                "M.S. Nacson",
                "S. Gunasekar",
                "N. Srebro"
            ],
            "title": "The implicit bias of gradient descent on separable data",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Sun",
                "Q. Song",
                "F. Liang"
            ],
            "title": "Learning sparse deep neural networks with a spikeand-slab prior",
            "venue": "Stat. Prob. Letters,",
            "year": 2022
        },
        {
            "authors": [
                "T. Suzuki"
            ],
            "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality",
            "venue": "The 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "K. Tsuji",
                "T. Suzuki"
            ],
            "title": "Estimation error analysis of deep learning on the regression problem on the variable exponent Besov space",
            "venue": "Electron. J. Stat.,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Tsukamoto"
            ],
            "title": "On certain Riemannian manifolds of positive curvature",
            "venue": "Tohoku Math. J.,",
            "year": 1966
        },
        {
            "authors": [
                "Y. Tsukamoto"
            ],
            "title": "Closed geodesics on certain Riemannian manifolds of positive curvature",
            "venue": "Tohoku Math. J.,",
            "year": 1966
        },
        {
            "authors": [
                "M. Vishik"
            ],
            "title": "Hydrodynamics in Besov spaces",
            "venue": "Arch. Rat. Mech. Anal.,",
            "year": 1998
        },
        {
            "authors": [
                "D. Yarotsky"
            ],
            "title": "Error bounds for approximations with deep ReLU networks",
            "venue": "Neural Networks,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations."
        },
        {
            "heading": "1. Introduction",
            "text": "Recently, deep learning has been the successful tool for various tasks of data analysis (see [14, 15, 26, 29] for example). Also, the theoretical structure of deep neural network (DNN) has been clarified gradually. In particular, Amari [2] gave a simple observation showing that any target function is in a sufficiently small neighborhood of any randomly connected DNN, with sufficiently large number of neurons in a layer (see also Kawaguchi-Huang-Kaelbling [22] and references therein). In ResNet with a continuous limit argument, Ding-Chen-Li-Wright [8] partially clarified that certain DNNs allow gradient descent to find the minimizer (see also [17]). They employed a mean-field limit argument and gradient flow analysis. These arguments translate gradient descent for parameter training into a partial differential equation (PDE), where the PDE analysis is useful for tracing the convergence to the minimizer. ResNet is the limiting regime when both the number of layers and the numbers of weights per layer approach infinity. They showed that gradient descent can be translated to a gradient flow of the limiting probability distribution. Furthermore, under some conditions, they proved that the limiting gradient flow captures the zero-loss as its global minimum. Another result focusing on the converging theory of DNN is Allen-Zhu et.al. [1]. They proved that stochastic gradient descent can find global minima on the training objective of DNNs in polynomial time, with the separable assumption on the training data, starting from randomly initialized weights. Here, it should be noted that [1, 2, 8] are in the over-parametrized regime, and in this regime, we essentially assume that there are a lot of trainable network parameters than input-output data pairs. However, this assumption is typically\nDate: July 13, 2023. 2020 Mathematics Subject Classification. Primary 68T27; Secondary 68T07, Tertiary 41A29 . Key words and phrases. deep neural network, ReLU function, gradient descent, pointwise\nconvergence.\n1\nar X\niv :2\n30 4.\n08 17\n2v 2\n[ cs\n.L G\n] 1\n2 Ju\nl 2 02\n3\nnot at all fulfilled in practical DNNs, since it is computationally expensive, memory intensive and mis-calibrated. Under the Bayesian framework, Sun-Song-Liang [32] indicated that sparsity is essential for improving the prediction accuracy and calibration of the DNN.\nIn the last, we mention another important results which are not in the overparametrized regime at all. Jentzen-Riekert [18] proved convergence of gradient descent type of optimization in both time-continuous and time-discrete DNNs, under the assumptions that the probability distribution of the input data and the target functions are both piecewise polynomials, while the number of trainable network parameters is finite (see also [19]). The piecewise polynomial assumption on the target function is needed for employing the Kurdyka- Lojasiewicz inequality. To the contrary, there are counterexamples of non-convergence to objective functions (see [6, 12]). To review the recent optimization error studies, see [11] and references therein.\nKeeping these celebrated results in mind, our next work would be clarifying the precise convergence structure of DNNs even if initial data are already close to the target function. Imaizumi-Fukumizu [16] examined learning of non-smooth functions, which was not covered by the previous theory, and clarified that, compare the DNN with the previous theories (such as the kernel methods), the convergence rates are almost optimal for non-smooth functions, while some of the popular models do not attain this optimal rate. Suzuki [33] (also references therein) clarified that the learning ability of ReLU-DNN is superior to the linear method when the target function is in the supercritical Besov spaces Bsp,q with p < 2 and s < d/2 (d is the dimension, note that the case s = d/2 is called \u201ccritical\u201d), which indicates the spatial inhomogeneity of the shape of the target function including the non-smooth functions. Thus, with the aid of these results, we can conclude that ReLU-DNN is suitable for recognizing jump discontinuity of the non-smooth functions.\nWe now briefly explain the key idea of [33]. To show the approximation error theorems, he first applied the wavelet expansion to the target functions and then approximated each wavelet bases (composed of spline functions) by ReLU-DNN (see [38]). More specifically, let g : [0, 1] \u2192 [0, 1] be a tent function such that\ng(x) =\n{ 2x (x < 1/2),\n2(1 \u2212 x) (x \u2265 1/2)\nand let gs be a s times composite function, and fm be a function approximating to the second order polynomial such that\ngs(x) = g \u25e6 g \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 g\ufe38 \ufe37\ufe37 \ufe38 s (x) and fm(x) = x\u2212 m\u2211 s=1 gs(x) 22s .(1)\nNote that fm(x) \u2192 x2 (m \u2192 \u221e) uniformly. For deriving the multi-dimensional polynomials, it suffices to apply the following formula:\nxy = 1\n2 ((x + y)2 \u2212 x2 \u2212 y2),\nand then we can easily approximate multi-dimensional spline functions by ReLUDNN. The other idea (for variance) is applying the statistical argument in [30] combined with a covering number evaluation.\nFor the mathematical analysis on the gradient descent when the target functions are non-smooth, in particular, indicator functions, we need to mention the result on Soudry et.al [31, Corollary 8]. For a set of input data {xn}Nn=1 \u2282 Rd, the corresponding dataset {xn, yn}Nn=1 which is accompanied by binary labels yn \u2208 {\u22121, 1} was considered. More precisely, they mathematically investigated the gradient descent problem with the following error function:\nE(wt) = N\u2211 n=1 \u2113 ( yn(w t \u00b7 xn) ) , wt+1 = wt \u2212 \u03f5\u2207E(wt),\nwhere \u201c \u00b7 \u201d is the usual inner product, \u03f5 is a learning coefficient, wt \u2208 Rd (t = 0, 1, 2, \u00b7 \u00b7 \u00b7 ) are the weight vectors, \u2113(\u00b7) : R \u2192 R>0 is differentiable, monotonically decreasing to zero. Imposing reasonable assumptions, they proved the convergence: limt\u2192\u221e E(w\nt) = 0. The key idea of the proof is applying a contradiction argument, see the proof of Lemma 1 in [31]. This convergence result was extended to DNNs with the optimization in a single weight layer.\nHowever, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument) even if the initial data are already close to the target function, and this attempt seems to be closer to the practical DNNs. In contrast to the previous works, this paper discusses convergence with respect to iterations of learning algorithms i.e., learning dynamics itself.\nBefore going any further, we point out that manipulating the structure of function spaces may not be enough to capture the discontinuity structure of the target functions. This means that, to make any further progress, we might need to directly analyze each DNN dynamics. The flavor of this insight seems similar to the recent mathematical studies on the incompressible inviscid flows. See [3, 4, 9, 10, 27] for example. More precisely, these have been directly looking into the dynamics of inviscid fluids in the critical function spaces (to show ill-posedness), and the argument seems quite different from the classical studies focusing on well-posedness in subcritical type of function spaces. See [5, 20, 21, 28, 37] for example. To show the well-posedness, the structure of function spaces, more precisely, commutator estimates are crucially indispensable.\nThis paper is organized as follows: In the next section, we construct target functions and the corresponding estimators. In Section 3, we investigate the pointwise convergence of gradient descent in terms of ReLU-DNN if initial data are already close to the target function."
        },
        {
            "heading": "2. Target functions and the corresponding estimators",
            "text": "In this section, we define a set of target functions and the corresponding estimators, which is one of the typical function set assuring pointwise convergence. For pairs of points and unit vectors: (yj , \u03c4j) \u2208 [\u22121, 1)d \u00d7 Sd\u22121 (j = 1, 2, \u00b7 \u00b7 \u00b7 ), let us define half spaces H\u25e6 and H\u25e6\u03f5 as follows:\nH\u25e6(yj , \u03c4j) := {x \u2208 [\u22121, 1)d : (x\u2212 yj) \u00b7 \u03c4j < 0}, H\u25e6\u03f5 (yj , \u03c4j) := {x \u2208 [\u22121, 1)d : (x\u2212 yj) \u00b7 \u03c4j < \u2212\u03f5}.\nWe employ a set of indicator functions {\u03c7\u2126,\u2126 \u2208 M} as the set of target functions, where M is a set of convex smooth manifolds (with internal filling) as follows:\nM := { \u2126 \u2282 [\u22121, 1)d : \u2202\u2126 is smooth, and the following three conditions hold. }\n\u2022 There exists {(yj , \u03c4j)}\u221ej=1 \u2282 \u2202\u2126 \u00d7 Sd\u22121 such that \u22c2\u221e j=1 H \u25e6(yj , \u03c4j) = \u2126. \u2022 For each j and any N \u2208 N, min\nj\u2032 \u0338=j, 1\u2264j\u2032\u2264N dist\n( yj \u2212N\u2212 2 d\u22121 \u03c4j , \u2202H\n\u25e6 N \u2212 2 d\u22121\n(yj\u2032 , \u03c4j\u2032) ) \u2272 N\u2212 1 d\u22121 .\n\u2022 For each j, there is a set of points {cji}di=1 \u2282 (\u2126 \u2229 \u2202H\u25e6\nN \u2212 2 d\u22121 (yj , \u03c4j)) which are linearly independent.\nThe first condition is nothing more than expressing the convexity. The second one represents a uniformly covering condition, which is needed for the estimate of the difference between the target function and the corresponding estimator (see Lemma\n1). Note that, we choose \u22c2N\nj\u2032=1 \u2202H \u25e6 N \u2212 2 d\u22121 (yj\u2032 , \u03c4j\u2032) as a regular polytope, and by the\ndimensional analysis, the powers \u2212 1d\u22121 and \u2212 2 d\u22121 naturally appear. The points {cji}di=1 in the third one is needed for specifying training samples (see the next section). Note that, the typical example of \u2126 \u2208 M is a sphere: \u2126 = {x \u2208 [\u22121, 1)d : |x| < 1/2}. Remark 1. An interesting question naturally arises: for \u2126 \u2208 M, whether or not \u2202\u2126 is a manifold isometric to the sphere. We leave it as an open question (c.f. Tsukamoto [35, 36]).\nDefinition 1. (Definition of estimator.) For a target function f\u25e6 = \u03c7\u2126 (\u2126 \u2208 M), we define the corresponding estimator f\u25e6N as follows:\nf\u25e6N := \u03c7\u2126\u25e6N , \u2126 \u25e6 N := N\u22c2 j=1 H\u25e6 N \u2212 2 d\u22121 (yj , \u03c4j).\nLemma 1. We have (2) f\u25e6N (x) \u2192 f\u25e6(x) (N \u2192 \u221e) for any x \u2208 [\u22121, 1)d. Moreover we have the following convergence rate:\n(3) \u2225f\u25e6N \u2212 f\u25e6\u2225rLr \u2272 N\u2212 2\nd\u22121 for 1 \u2264 r < \u221e. Proof. To show (2) and (3), we employ a standard local coordinate system. Let us choose a set of unit vectors {\u03c4\u22a5ji} d\u22121 i=1 \u2282 Sd\u22121 satisfying \u03c4\u22a5ji \u00b7\u03c4j = 0 (i = 1, 2, \u00b7 \u00b7 \u00b7 , d\u22121) and \u03c4\u22a5ji \u00b7 \u03c4\u22a5ji\u2032 = 0 (i \u0338= i\u2032). Then we have{ yj \u2212N\u2212 2 d\u22121 \u03c4j +\nd\u22121\u2211 i=1 si\u03c4 \u22a5 ji : si \u2208 R, |s| \u2272 N\u2212 1 d\u22121\n} \u2282 \u2202H\u25e6\nN \u2212 2 d\u22121 (yj , \u03c4j)\nand { yj +\nd\u22121\u2211 i=1 si\u03c4 \u22a5 ji + g(s)\u03c4j : si \u2208 R, |s| \u2272 N\u2212 1 d\u22121\n} \u2282 \u2202\u2126\u25e6\nfor some g(s) = c1s 2 1 + \u00b7 \u00b7 \u00b7 + cd\u22121s2d\u22121 + O(|s|3) with positive constants ci > 0 (independent of N). Thus, combining the second condition (uniformly covering condition), we have the following distance property:\ndist (\u2202\u2126, x) \u2272 N\u2212 2 d\u22121 + c1s 2 1 + \u00b7 \u00b7 \u00b7 cd\u22121s2d\u22121 \u2272 N\u2212 2 d\u22122 for any x \u2208 \u2202\u2126\u25e6N ,\nwhich guarantees pointwise convergence (2), and also we deduce\n|\u2126\u25e6N \\ \u2126\u25e6| + |\u2126\u25e6 \\ \u2126\u25e6N | \u2272 |\u2202\u2126\u25e6| sup x\u2208\u2202\u2126\u25e6N dist (\u2202\u2126, x) \u2272 N\u2212 2 d\u22121 .\nTherefore\n\u2225f\u25e6N \u2212 f\u25e6\u2225rLr \u2272 N\u2212 2\nd\u22121 for 1 \u2264 r < \u221e. \u25a1"
        },
        {
            "heading": "3. Pointwise convergence of gradient descent",
            "text": "In what follows we mathematically investigate the pointwise convergence of gradient descent in terms of ReLU-DNN. Let f\u25e6 be a target function and fN be a prescribed neural network with N -nodes. Also let {W t}\u221et=0 be sets of weights and biases, which are generated by the following gradient descent:\nE(W t) := 1\n2 \u222b D |fN (W t, x) \u2212 f\u25e6(x)|2dx,\n(4) W t+1 = W t \u2212 \u03f5 1 |D| \u2207W tE(W t),\nwhere \u03f5 \u2208 R>0 is a learning coefficient and D \u2282 [\u22121, 1)d is a set of training samples. However, it cannot be expected to have the key efficiency (14) for this original gradient descent (4). Thus, to state the main theorem, we employ a suitable change of variables, and employ the corresponding gradient descent (9) instead of (4). Note that, since the gradient is normalized by 1/|D|, D can be replaced by a non-zero measure set or a set of lines. Let {f\u25e6N}N be a sequence of estimators such that\nf\u25e6N (x) := lim t\u2192\u221e fN (W t, x).\nOur specific purpose is to find neural networks fN , suitable D and \u03f5 assuring pointwise convergence to the corresponding estimator f\u25e6N , which is already given in the last section.\nRemark 2. This problem setting clarifies the crucial difference between the shallow and deep neural networks, as follows: Since sin and cos functions are continuous, we can recover them from linear combination of activation functions (see [7] for example). Thus mathematical analysis of shallow neural network can be replaced by a linear combination of sin and cos functions, which is nothing more than the Fourier series. For x \u2208 [\u22121, 1)d, we set the target function f\u25e6 as the indicator function of the d dimensional ball such that\nf\u25e6(x) = { 1, |x| \u2264 1/2, 0, |x| > 1/2,\nand let fN be a Fourier series with spherical partial sum:\nfN (W t, x) := \u2211 |k|<N ctke ik\u00b7x \u2208 R, W t := {ctk}k\u2208Zd \u2282 C, ct\u2212k = c\u0304tk, k \u2208 Zd,\nwhere c\u0304k is the complex conjugate of ck. Note that, by c\u0304k = c\u2212k, this fN (W t, x) is always real-valued function. Let D = [\u22121, 1)d, and by the Parseval\u2019s identity, we\nimmediately have the following estimator (of course, different from the one given in the last section):\nf\u25e6N (x) = \u2211\n|k|<N\nc\u2217ke ik\u00b7x for c\u2217k = \u222b [\u22121,1)d f\u25e6(x)e\u2212ik\u00b7xdx with c\u0304\u2217k = c \u2217 \u2212k.\nThen we obtain the following counterexample, which clarifies the crucial difference between the shallow and deep neural networks.\nCounterexample. Let d \u2265 5 and D = [\u22121, 1)d. Then, for any x \u2208 Qd \u2229 [\u22121, 1)d, f\u25e6N (x) \u2212 f\u25e6(x) diverges as N \u2192 \u221e.\nThe proof is just direct consequence of Kuratsubo [23] (see also [24, 25]). Note that, for the Fourier series of the indicator functions of such several dimensional balls, the Gibbs and Pinsky phenomena have already been well-known. The assertion of this counterexample is that Kuratsubo [23] discoverd the third phenomenon (preventing pointwise convergence), other than the previous two phenomena. To understand this third phenomenon intuitivelly, see numerical computations in Section 7 in [24].\nIn contrast with the Fourier series case (shallow neural network), we will show pointwise convergence to f\u25e6N which is already given in the last section. The DNN has a sparse and special shape, with certain variable transformations. Let N = 2n (n \u2208 N) and let us now construct a deep neural network fN . For the initial layer, we define\nz1 := h(w1x + b1) :=  h(w 1 1 \u00b7 x + b11)\n... h(w12n \u00b7 x + b12n)  for x \u2208 [\u22121, 1)d, w1 := {w1j}2 n j=1 := {w1ji}ji \u2208 R2 n\u00d7d, b1, z1 \u2208 R2n . Recall that w is the weight and b is the bias. For the 2k-th layer, we set\nz2k := h(w2kz2k\u22121 + b2k)\nfor w2k \u2208 R3\u00b72n\u2212k\u00d72n\u2212k+1 , b2k, z2k \u2208 R3\u00b72n\u2212k . Moreover, we impose the following sparsity condition (w2k be constants): for J = 1, 2, \u00b7 \u00b7 \u00b7 , 2n\u2212k and 1 \u2264 k \u2264 n,\nz2k3J\u22122 = h\n( 1\n2 z2k\u221212J\u22121 +\n1 2 z2k\u221212J\n) ,\nz2k3J\u22121 = h\n( 1\n2 z2k\u221212J\u22121 \u2212\n1 2 z2k\u221212J\n) ,\nz2k3J = h\n( \u22121\n2 z2k\u221212J\u22121 +\n1 2 z2k\u221212J\n) , (5)\nwhere z2k = {z2kj }j . For the 2k + 1 layer, we set\nz2k+1 = w2k+1z2k,\nw2k+1 \u2208 R2n\u2212k\u00d73\u00b72n\u2212k , z2k+1 \u2208 R2n\u2212k . In this layer, we impose the following restriction (w2k+1 be constants): for J = 1, 2, \u00b7 \u00b7 \u00b7 2n\u2212k,\n(6) z2k+1J = z 2k 3J\u22122 \u2212 z2k3J\u22121 \u2212 z2k3J .\nSee Figure 1. We crucially employed the sparse architecture for capturing discontinuity of indicator functions. The key idea of designing such sparse DNN is\nthat, combined (5) and (6) induce cancellations of certain weights and biases, and then these cancellations suppress the generation of concave shape of discontinuities. Consequently, the convex shape of discontinuity remains.\nConjecture. Therefore our conjecture is that highly mixed (i.e. highly oscillating) concave and convex shapes of discontinuities of indicator functions (target functions) cannot be captured by DNNs themselves. If this conjecture is correct, then we might be able to insist that \u201coverfitting\u201d is naturally suppressed by the DNNs themselves.\nRemark 3. It is doable to apply the same argument to a broader class of functions (an element of the Sobolev space, for example) as the target function. In this case, it must be good strategy to combine the ideas of (1), piecewise polynomials in [18, 19], (5) and (6) to capture various continuous slopes in target functions. However, it is nontrivial how to design DNN architecture so far, and thus we leave it as an open question.\nRemark 4. In this paper we have employed ReLU function as the activate function, for simplicity. Of course, employing the sigmoid function case is also attractive problem.\nThen we see that, in the 2n+ 1 layer, z2n+1 becomes a real number. In the final layer, we apply the following clipping:\nfN := z 2n+2 = max{z2n+1, 1}\n= 1 \u2212 h(1 \u2212 z2n+1). To state the mathematical rigorous theorem, we need to assume that the initial function is already close to the target function, and need to give suitable change of variables (W t 7\u2192 (\u03b1t, \u03b2t)):\n(7) \u03b1tj := w 1,t j \u00b7 \u03c4j and \u03b2 t ji := w 1,t j \u00b7 cji + b t j .\nFor the initial weights and biases, we assume the following:\n(8)\n \u03b1t=0j = |w 1,t=0 j |, in other words, w 1,t=0 j \u22a5 \u2202H \u25e6 N \u2212 2 d\u22121 (yj , \u03c4j), there exists \u03b3 > 0 such that \u03b1t=0j > \u03b3 (we determine this \u03b3 later),\n0 < \u03b2t=0ji < 1.\nNote that, if \u03b1tj = |w 1,t j |, then w 1,t ji \u00b7 cji is independent of i, and this means that \u03b2tji is also independent of i. In this paper we employ the following gradient descent\ninstead of (4):\n(\u03b1t+1, \u03b2t+1) = (\u03b1t, \u03b2t) \u2212 \u03f5 |D| \u2207(\u03b1t,\u03b2t)E(\u03b1t, \u03b2t),(9)\nwhere \u03b1t := {\u03b1tj}2 n j=1 and \u03b2 t := {\u03b2tj}2 n j=1. We emphasize that the linear map W t 7\u2192 (\u03b1t, \u03b2t) is not invertible (degenerate). Thus we cannot express \u2207(\u03b1t,\u03b2t)E by using \u2207W tE. This suggests us that larger number of trainable network parameters might not always refine the convergence structure in the sparse DNN regime.\nRemark 5. It is an open question whether or not the gradient descent of the original coefficient W t case (4) is also converging to the same estimator f\u25e6N .\nThen the main theorem is as follows:\nTheorem 2. Assume W t=0 satisfies (8), and let the learning coefficient \u03f5 be such that \u03f5 < 2n+1. Then, by choosing D appropriately, fN (\u03b1t, \u03b2t) converges to f\u25e6N pointwisely (as t \u2192 \u221e) by the gradient descent process (9). Moreover we have the following exact convergence rate:\n\u2225fN (\u03b1t, \u03b2t) \u2212 f\u25e6N\u2225rLr \u2248 t\u22121/3 for 1 \u2264 r < \u221e,\nwhere a \u2248 b means C\u22121a \u2264 b \u2264 Ca for some universal constant C > 1.\nRemark 6. By applying the dimensional analysis, we can expect that the momentum methods such as heavy-ball and Nesterov accelerated methods indeed improve the convergence rate t\u22121/3. More precisely, for some learning coefficients \u03f5, \u03f5\u0303 > 0, the heavy-ball and Nesterov accelerated methods can be expressed as follows:\n(heavy-ball) \u03b1t+1 = \u03b1t \u2212 \u03f5\u2202\u03b1tE(\u03b1t) + \u03f5\u0303(\u03b1t \u2212 \u03b1t\u22121), (Nasterov) \u03b1t+1 = \u03b1t \u2212 \u03f5\u2202\u03b1tE(\u03b1t) + \u03f5\u0303(\u03b1t \u2212 \u03b1t\u22121)\n\u2212 \u03f5\u0303(\u2202\u03b1tE(\u03b1t) \u2212 \u2202\u03b1t\u22121E(\u03b1t\u22121)).\nLet us set \u03b1t+1 \u223c t\u03c3 (this \u03c3 means power, not index), and our specific purpose here is to figure out the appropriate power \u03c3 > 0. First, by (15), we can estimate \u2202\u03b1tE(\u03b1\nt) \u223c t\u22122\u03c3. Heuristically, let \u03f5\u0303 = 1, and then we can regard \u03b1t+1 \u2212 2\u03b1t + \u03b1t\u22121 \u223c t\u03c3\u22122 as the second derivative in t, and \u2202\u03b1tE(\u03b1t) \u2212 \u2202\u03b1t\u22121E(\u03b1t\u22121) \u223c t\u22122\u03c3\u22121 as the first derivative in t. Then we obtain the following dimension estimate:\n(heavy-ball) t\u03c3\u22122 \u223c \u03f5t\u22122\u03c3, (Nasterov) t\u03c3\u22122 \u223c \u03f5t\u22122\u03c3 + t\u22122\u03c3\u22121 \u223c \u03f5t\u22122\u03c3 (for sufficiently large t).\nThis suggests the appropriate power \u03c3 = 2/3 (in both cases), which indeed improves the convergence rate. For the rigorous mathematical argument, we leave it as an open question.\nProof. First we consider a pair of 2k\u22121, 2k and 2k+1 layers. Taking a derivative, we have\n\u2202z2k\u221212J\u22121 z2k+1J =\n1 2 \u2202h\n( 1\n2 z2k\u221212J\u22121 +\n1 2 z2k\u221212J ) \u2212 1\n2 \u2202h\n( 1\n2 z2k\u221212J\u22121 \u2212\n1 2 z2k\u221212J\n) + 1\n2 \u2202h\n( \u22121\n2 z2k\u221212J\u22121 +\n1 2 z2k\u221212J\n) .\nDue to the cancellation of Heaviside functions in the following domain,\nD0k,J :=\n{ x : 1\n2 z2k\u221212J <\n1 2 z2k\u221212J\u22121\n} ,\nwe have\n\u2202z2k\u221212J\u22121 z2k+1J = 0 for x \u2208 D 0 k,J .\nNote that, rigorously saying, z2k\u22121 := z2k\u22121 \u25e6 z2k \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 z1. To the contrary, there is no cancellation of Heaviside functions in the following domain:\nD1k,J :=\n{ x : 1\n2 z2k\u221212J >\n1 2 z2k\u221212J\u22121\n} .\nIn other words,\n\u2202z2k\u221212J\u22121 z2k+1J = 1 for x \u2208 D 1 k,J .\nThe same argument goes through also in the case \u2202z2k\u221212J z2k+1J (omit its detail). In this case, we have\n\u2202z2k\u221212J z2k+1J = 1 for x \u2208 D 0 k,J , \u2202z2k\u221212J z2k+1J = 0 for x \u2208 D 1 k,J .\nWe apply these properties inductively in the reverse direction (as the back propergation), and we divide the non-zero region {x : fN (W t, x) > 0} into several parts appropriately. To do that, we suitably rewrite the natural number j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 2n} as follows:\nj = \u03b4j1 + 2\u03b4 j 2 + 2 2\u03b4j3 + \u00b7 \u00b7 \u00b7 + 2n\u22121\u03b4jn, where \u03b4jk \u2208 {0, 1}. Let\nDj := n\u22c2 k=1 D \u03b4jk k,Jjk for Jjk := n\u2211 \u2113=k 2\u2113\u2212k\u03b4j\u2113 .\nBy using this Dj , the derivative formula becomes much simpler:\n(10) \u2202xz 2n+1(x) = w1j for x \u2208 Dj .\nBy the construction of Dj , we observe that\n{x : fN = 0} \u2229 \u2202Dj \u2282 {x : w1j \u00b7 x + b1j = 0},\nthen, by the fundamental theorem of calculus, we have\nz2n+1(x) = 2n\u2211 j=1 ( h(w1j \u00b7 x + b1j )\u03c7Dj (x) ) .\nTherefore we obtain the following explicit formula:\n(11) fN (x) = max  2n\u2211 j=1 ( h(w1j \u00b7 x + b1j )\u03c7Dj (x) ) , 1  . Next we choose a set of training samples D. In order to do that, we define a geometric a-priori region. Since {cji}di=1 are linearly independent, by the Gram\u2013Schmidt process, there is a unique \u03c4\u0303j \u2208 Sd\u22121 such that\n(cji(r) \u2212 cji\u2032(r)) \u00b7 \u03c4\u0303j = 0 (i \u0338= i\u2032),\nand then we define the hyper planes {hj}2 n j=1 as follows:\nhj := {x : (x\u2212 cj1) \u00b7 \u03c4\u0303j = 0}.\nNote that, in our setting, \u03c4\u0303j coincides with \u03c4j , and hj = \u2202H \u25e6 N \u2212 2 d\u22121 (yj , \u03c4j). By using this hj , we can define the a-priori region Lj as follows:\nLj := { \u03c4jr + x, r \u2208 (\u2212\u03b3\u22121, \u03b3\u22121), x \u2208 hj } for some \u03b3 > 0.\nTo specify a set of training samples D, we need the following a-priori estimate (from this estimate, we can figure our the appropriate \u03b3).\nProposition 3. There exists a \u03b3 > 2 such that\n(12) \u2113ji(s) := \u03c4js + cji \u2208 Lj \\ (\u222aj \u0338=j\u2032Lj\u2032) for s \u2208 [\u2212\u03b3\u22121, \u03b3\u22121].\nWe need this property (12) for completely separating each linear slopes (generated by the composite ReLU functions) along each lines \u2113ji.\nProof. The case \u03b3 = \u221e automatically satisfies (12) and then we just apply the continuity argument. \u25a1\nNow we specify the set of training samples D, which has 2n \u00b7d elements of straight lines \u2113ji passing through {cji}ji, as follows:\nD := d\u22c3\ni=1 2n\u22c3 j=1 \u03b3\u22121\u22c3 s=\u2212\u03b3\u22121 \u2113ji(s).\nWe plug the lines D into (11), apply change of variables (7):\nw1j \u00b7 \u2113ji(s) + b1j = w1j \u00b7 \u03c4js + (w1j \u00b7 cji + b1j ) = \u03b1js + \u03b2j ,(13)\nand then we explicitly write down the error function E as follows:\nE(\u03b1, \u03b2) := 1\n2 2n\u2211 j=1 d\u2211 i=1 Ej(\u03b1j , \u03b2ji)\n:= 1\n2 2n\u2211 j=1 d\u2211 i=1 (\u222b 0 \u2212\u03b3\u22121 (fN (\u03b1js + \u03b2ji)) 2 ds\n+ \u222b \u03b3\u22121 0 (fN (1 \u2212 (\u03b1js + \u03b2ji)))2 ds ) .\nNote that, due to Proposition 3, we can completely separate each lines \u2113ji, and thus we obtain the above explicit formula. If \u03b1j > \u03b3 and \u03b2ji \u2208 (0, 1), then\n= 1\n2 2n\u2211 j=1 d\u2211 i=1 \u222b 0 \u2212 \u03b2j \u03b1j (\u03b1js + \u03b2ji) 2ds + \u222b 1\u2212\u03b2ji \u03b1j 0 (1 \u2212 (\u03b1js + \u03b2ji))2ds  . The gradient descent (9) provides the following key efficiency:\n(14) if w1,tj \u22a5 hj , then \u03b2 t ji is independent of i, and thus we have w 1,t+1 j \u22a5 hj .\nTherefore, if we show \u03b1t+1 > \u03b1t, \u03b1t \u2192 \u221e (t \u2192 \u221e) and 0 < \u03b2tj < 1, then we can conclude f(\u03b1t, \u03b2t) \u2192 f\u25e6N (t \u2192 \u221e) pointwisely. Now we calculate the gradient descent in \u03b1j and \u03b2j precisely. A direct calculation yields\nEj(\u03b1j , \u03b2j) = 1 \u2212 3\u03b2j + 3\u03b22j\n3\u03b1j .\nThen we have\n(15) \u2202\u03b1jEj = \u2212 1\n3\u03b12j\n( 1 \u2212 3\u03b2j + 3\u03b22j ) and \u2202\u03b2jEj = \u2212 1\n\u03b1j (1 \u2212 2\u03b2j).\nFirst we show \u03b1t+1j > \u03b1 t j . Since 1\u2212 3\u03b2j + 3\u03b22j \u2265 1/4 > 0 for any \u03b2j \u2208 R, we always have \u2202\u03b1jE < 0 and\n\u03b1t+1j = \u03b1 t j \u2212\n\u03f5d\n2|D| \u2202\u03b1jE \u2265 \u03b1tj +\n\u03f5d\n24|D|(\u03b1tj)2 .\nThus \u03b1t+1 > \u03b1t. By directly solving the ODE: ddtg(t) = 1/g(t) 2, applying the meanvalue theorem and the comparison principle, we have \u03b1tj \u2273 t 1/3 (c.f. dimensional analysis in Remark 6). Next we show 0 < \u03b2t+1j < 1 if 0 < \u03b2 t j < 1. We recall\n\u03b2t+1j = \u03b2 t j \u2212\n\u03f5d\n2|D| \u2202\u03b2jEj(\u03b2\nt),\nand easily see that\n\u2212 \u03f5d 2|D| \u2202\u03b2jEj(\u03b2) < 0 for \u03b2 \u2208 (1/2, 1), \u2212 \u03f5d 2|D| \u2202\u03b2jEj(\u03b2) > 0 for \u03b2 \u2208 (0, 1/2).\nBy the assumption \u03f5 < 2n+1 and |D| = d2n+1/\u03b3, we see \u03f5d2|D|\u03b1t < 1 2 . Thus \u03b2 t+1 does not exceed 1 when \u03b2t \u2208 (0, 1/2). To the contrary, \u03b2t+1 does not exceed 0 when \u03b2t \u2208 (1/2, 1). Thus \u03b2tj \u2208 (0, 1) for any t. Then go back to (15). Since sup\n0<\u03b2<1 (1 \u2212 3\u03b2 + 3\u03b22) = 1, we have\n\u03b1t+1j \u2264 \u03b1 t j +\n\u03f5d\n6|D|(\u03b1tj)2 .\nApplying the ODE argument (with the mean-value theorem and the comparison principle) again, we have \u03b1tj \u2248 t1/3. Therefore we finally have the following desired estimate:\n\u2225fN (\u03b1t, \u03b2t) \u2212 f\u25e6N\u2225rLr \u2248 t\u2212 1 3 |\u2202\u2126\u25e6N | \u2248 t\u22121/3."
        },
        {
            "heading": "4. Conclusion",
            "text": "We found a ReLU DNN architecture which is suitable for capturing convex shape of discontinuity on indicator functions (target functions), accompanied by pointwise convergence. Our next question would be, what kind of ReLU-DNN architectures really attain pointwise convergence (or not) for mixed concave and convex discontinuities, and this is our future work. Also this theoretical result requires experimental verification. We will attempt numerical computation on the usual mini-batch gradient decent for ReLU-DNN, and try to find the sparse DNN architecture like (5) and (6) (or something similar) in this paper. This is also our future work.\nAcknowledgments. The author sincerely thank the anonymous referees for very helpful comments regarding the manuscript, which have been incorporated in the current paper. Also the author especially thank Professor Arnulf Jentzen for letting me know various articles related to this manuscript and fruitful discussions, in particular, on the over-parametrized regime. The author is grateful to Professors Masaharu Nagayama, Eiichi Nakai, Kengo Nakai, Yoshitaka Saiki and Yuzuru Sato for valuable comments. Research of TY was partly supported by the JSPS Grantsin-Aid for Scientific Research 20H01819 and 21K03304. This paper was a part of the lecture note on the class: Mathematical Analysis I (spring semester 2023) for undergrad/graduate courses in Hitotsubashi University.\nConflict of Interest. The authors have no conflicts to disclose."
        }
    ],
    "title": "POINTWISE CONVERGENCE THEOREM OF GRADIENT DESCENT IN SPARSE DEEP NEURAL NETWORK",
    "year": 2023
}