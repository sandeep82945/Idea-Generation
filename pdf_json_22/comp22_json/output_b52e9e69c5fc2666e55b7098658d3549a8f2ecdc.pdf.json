{
    "abstractText": "Having access to accurate game state information is of utmost importance for any artificial intelligence task including game-playing, testing, player modeling, and procedural content generation. SelfSupervised Learning (SSL) techniques have shown to be capable of inferring accurate game state information from the highdimensional pixel input of game footage into compressed latent representations. Contrastive Learning is a popular SSL paradigm where the visual understanding of the game\u2019s images comes from contrasting dissimilar and similar game states defined by simple image augmentation methods. In this study, we introduce a new game scene augmentation technique\u2014named GameCLR\u2014that takes advantage of the game-engine to define and synthesize specific, highlycontrolled renderings of different game states, thereby, boosting contrastive learning performance. We test our GameCLR technique on images of the CARLA driving simulator environment and compare it against the popular SimCLR baseline SSLmethod. Our results suggest that GameCLR can infer the game\u2019s state information from game footage more accurately compared to the baseline. Our proposed approach allows us to conduct game artificial intelligence research by directly utilizing screen pixels as input.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chintan Trivedi"
        },
        {
            "affiliations": [],
            "name": "Konstantinos Makantasis"
        },
        {
            "affiliations": [],
            "name": "Antonios Liapis"
        },
        {
            "affiliations": [],
            "name": "Georgios N. Yannakakis"
        }
    ],
    "id": "SP:32444b1d1161e115d0f8cb13fb94256714335bbf",
    "references": [
        {
            "authors": [
                "Ankesh Anand",
                "Evan Racah",
                "Sherjil Ozair",
                "Yoshua Bengio",
                "Marc-Alexandre C\u00f4t\u00e9",
                "R Devon Hjelm"
            ],
            "title": "Unsupervised state representation learning in Atari",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Barthet",
                "Antonios Liapis",
                "Georgios N Yannakakis"
            ],
            "title": "Go-Blend behavior and affect",
            "venue": "In Proc. of the IEEE Intl. Conf. on Affective Computing and Intelligent Interaction Workshops and Demos",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Berner",
                "Greg Brockman",
                "Brooke Chan",
                "Vicki Cheung",
                "Przemysaw Debiak",
                "Christy Dennison",
                "David Farhi",
                "Quirin Fischer",
                "Shariq Hashme",
                "Chris Hesse"
            ],
            "title": "DotA 2 with large scale deep reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proc. of the Intl. Conf. on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Enrico Fini",
                "Moin Nabi",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "title": "Solo-learn: A library of self-supervised methods for visual representation learning",
            "venue": "Journal of Machine Learning Research 23,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "German Ros",
                "Felipe Codevilla",
                "Antonio Lopez",
                "Vladlen Koltun"
            ],
            "title": "CARLA: An open urban driving simulator",
            "venue": "In Proc. of the Conf. on Robot Learning. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Jaiswal",
                "Ashwin Ramesh Babu",
                "Mohammad Zaki Zadeh",
                "Debapriya Banerjee",
                "Fillia Makedon"
            ],
            "title": "A Survey on Contrastive Self-Supervised Learning",
            "venue": "Technologies 9,",
            "year": 2021
        },
        {
            "authors": [
                "Yannis Kalantidis",
                "Mert Bulent Sariyildiz",
                "Noe Pion",
                "Philippe Weinzaepfel",
                "Diane Larlus"
            ],
            "title": "Hard negative mixing for contrastive learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Mark J Nelson"
            ],
            "title": "Estimates for the branching factors of Atari games",
            "venue": "In Proc. of the IEEE Conf. on Games",
            "year": 2021
        },
        {
            "authors": [
                "Connor Shorten",
                "Taghi M Khoshgoftaar"
            ],
            "title": "A survey on image data augmentation for deep learning",
            "venue": "Big Data 6,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Stooke",
                "Kimin Lee",
                "Pieter Abbeel",
                "Michael Laskin"
            ],
            "title": "Decoupling representation learning from reinforcement learning",
            "venue": "In Proc. of the Intl. Conf. on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Chintan Trivedi",
                "Konstantinos Makantasis",
                "Antonios Liapis",
                "Georgios N. Yannakakis"
            ],
            "title": "Learning Task-Independent Game State Representations from Unlabeled Images",
            "venue": "In Proc. of the IEEE Conf. on Games",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Computing methodologies\u2192 Learning latent representations; Visual content-based indexing and retrieval; \u2022 Applied computing \u2192 Computer games.\nKEYWORDS computer vision, contrastive learning, self-supervised learning, representation learning, game state representations\nACM Reference Format: Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, and Georgios N. Yannakakis. 2022. Game State Learning via Game Scene Augmentation. In FDG \u201922: Proceedings of the 17th International Conference on the Foundations of Digital Games (FDG \u201922), September 5\u20138, 2022, Athens, Greece. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3555858.3555902\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. FDG \u201922, September 5\u20138, 2022, Athens, Greece \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9795-7/22/09. . . $15.00 https://doi.org/10.1145/3555858.3555902"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Extensive work [2, 3, 13] in dissimilar domains of AI and games such as player experience modeling, general gameplaying or content generation make use of the internal state of the game [1, 9] obtained from the game engine. Using computer vision to obtain such state information from on-screen game footage, instead of directly from the game engine, remains challenging [11]. Recent computer vision advancements with contrastive learning [7], however, show promise in tackling these challenges.\nContrastive learning belongs to the family of self-supervised representation learning (SSL) methods in computer vision that use a \u201cpairwise-comparison\u201d approach which operates by contrasting semantically similar and dissimilar images. The pairwise mechanism helps the vision model to identify critical visual features that define the semantics of these images. Recent work [12] has applied SSL to learn state representations in games from pixel input. Such methods, however, rely on simple image augmentation techniques (e.g. image flipping, rotation, and brightness change) to create semantically similar pairs of images. In this work, we investigate whether having access to a game engine can help us synthesize highly-controlled image augmentations that are better suited for learning such vision models. In particular, we use the game engine to construct better similar and dissimilar pairings via the proposed game scene augmentation technique, named GameCLR, for the task of game state representation learning.\nGameCLR synthesizes images that represent similar game states that are highly dissimilar in the pixel-space (synthetic positives) and images that represent different game states but are very similar in the pixel-space (synthetic negatives). Within the context of a car racing game, Fig. 1 visualizes an example of such a representation space containing images that act as synthetic positives and negatives to a reference image called the anchor. Our hypothesis is that by including such images in the contrastive learning process, our model will be better equipped to learn the important visual features that define any particular game state. Moreover, as we are defining the positive and negative pairings generated by the game engine, we also have a level of control over the learning process by guiding the model to learn what distinguishing factors are of importance to us (i.e., traffic information) and which ones can be considered as invariant for learning (e.g. rainy weather, color of the car). We test how training a vision model using GameCLR compares against a baseline SSL method SimCLR, on the CARLA driving simulator [6]. Our findings suggest that synthesizing specific images from the game engine can boost the performance of contrastive learning methods for learning critical game state features from images."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": "In this study, we use the CARLA [6] urban driving simulator (see Fig. 1) which provides access to its Unreal engine via a Python API. A scene \ud835\udc46 in CARLA is defined as the current game state of its Unreal engine, which, when put through the game\u2019s graphic renderer \ud835\udc5f , yields the pixel output \ud835\udc4b shown to the user on the screen (i.e., \ud835\udc4b = \ud835\udc5f (\ud835\udc46)). We take advantage of this game engine to generate a dataset for testing two contrastive learning methods: (1) a baseline SSL method SimCLR [4] which uses simple image augmentations\ud835\udc54(\ud835\udc4b ); and (2) our proposedGameCLRmethod which uses the CARLA game engine to first apply game scene augmentation \ud835\udc46 \u2032 = \ud835\udc52 (\ud835\udc46) before going through rendering \ud835\udc4b \u2032 = \ud835\udc5f (\ud835\udc46 \u2032) and then applying regular image augmentations \ud835\udc54(\ud835\udc4b \u2032). All the augmentation techniques used across both methods are described in Table 1."
        },
        {
            "heading": "2.1 SimCLR",
            "text": "In 2020, Chen et al. [4] proposed SimCLR, a simple framework for contrastive learning of visual representations. A contrastive approach between similar and dissimilar images is used to learn image representations based on the content present in the images. Its pipeline has four major components: an image augmentation function \ud835\udc54 using simple image augmentations, a convolutional neural network encoder function \ud835\udc53 , a small fully-connected network called\nthe projection head \u210e that maps representations to an embedding space, and a contrastive loss L that is applied on these embeddings. Under this framework, simple image augmentations [10] (e.g. rotation, brightness, addition of noise, etc.) are used to create two different views of the same image that are semantically similar, referred to as a positive pair. Similarly, any two views coming from distinct images are defined as negative pairs due to semantic dissimilarity. For a given embedding \ud835\udc4e of a reference image called the anchor, and its positive pair\u2019s embedding \ud835\udc5d as well as multiple negative pairs\u2019 embeddings in set \ud835\udc41 , the contrastive probability can be calculated as per Eq. (1).\nP(\ud835\udc4e, \ud835\udc5d) = exp(\ud835\udc4e \ud835\udc47 \ud835\udc5d/\ud835\udf0f) exp(\ud835\udc4e\ud835\udc47 \ud835\udc5d/\ud835\udf0f) +\u2211\ud835\udc5b\u2208\ud835\udc41 exp(\ud835\udc4e\ud835\udc47\ud835\udc5b/\ud835\udf0f) (1) where \ud835\udf0f is the temperature hyper-parameter and \ud835\udc4e\ud835\udc47 is the transposed \ud835\udc4e vector. Thus, the contrastive loss in SimCLR with respect to the anchor \ud835\udc4e and all its associated positive pairs in a set \ud835\udc43 can be defined as per Eq. (2):\nL(\ud835\udc4e, \ud835\udc43, \ud835\udc41 ) = \u2212 \u2211\ufe01 \ud835\udc5d\u2208\ud835\udc43 logP(\ud835\udc4e, \ud835\udc5d) (2)\nWe implement this SimCLR training method on our CARLA game dataset using the solo-learn framework [5]. We spawn the ego-vehicle at random locations and place the camera behind it. We also randomize the time of day, weather, color of the ego-vehicle, and traffic around it through the \ud835\udc52\ud835\udc4e and \ud835\udc52\ud835\udc5d functions described in Table 1. Through this process, we collect 50,000 anchor images to train a ResNet18 encoder [5] over 20 epochs."
        },
        {
            "heading": "2.2 GameCLR (Our Approach)",
            "text": "Our work follows the literature [8] regarding synthesizing hard negatives which can provide more information to the SimCLR loss compared to regular negatives occurring through image augmentation. Our approach, however, exploits access to a game engine and thereby our ability to generate relevant images for learning meaningful representations. Our assumption in this paper is that\nwe can accurately describe the traffic around the ego-vehicle without concern of changes in game aesthetics\u2014such as car color\u2014and lighting conditions arising from changes in weather and day time.\nTowards this end, we first render an anchor image by spawning the ego-vehicle at a random location. Then, we change the weather, time of the day conditions, or the color of the ego-vehicle while the ego-vehicle remains at the same state, using the Game Scenepreserving Augmentations \ud835\udc52\ud835\udc5d (\ud835\udc46) listed in Table 1. We define all such images as \ud835\udc43\ud835\udc60\ud835\udc66\ud835\udc5b indicating the set of synthetic positives with respect to the anchor image. Similarly, we synthesize negatives (\ud835\udc41\ud835\udc60\ud835\udc66\ud835\udc5b) by spawning random vehicles around our ego-vehicle. This is done by performing Game Scene-altering augmentations \ud835\udc52\ud835\udc4e (\ud835\udc46) in addition to \ud835\udc52\ud835\udc5d (\ud835\udc46). Figure 1 provides a few examples of the synthetic and regular images for a given anchor image. Note that all these images in GameCLR also undergo simple image augmentations during training, similar to SimCLR. Thus, we can now compute the GameCLR loss as L(\ud835\udc4e, \ud835\udc43\ud835\udc60\ud835\udc66\ud835\udc5b, \ud835\udc41 \u222a \ud835\udc41\ud835\udc60\ud835\udc66\ud835\udc5b) following the loss formulation of SimCLR in Eq. (2). This framework is showcased in Fig. 1 and we name it GameCLR. Our experiments for GameCLR follow similar choice of training hyper-parameters used in SimCLR as described in Section 2.1."
        },
        {
            "heading": "3 RESULTS",
            "text": "We present the results of our experiments with both augmentation approaches as a two-part assessment. First, we analyze in Section 3.1 how the representations of game states change throughout the learning process, especially focusing on the behavior of the synthetic images used in GameCLR. Next, in Section 3.2 we focus on highlighting the benefits of using such image representations for applications to game research that require extracting game state information from the game\u2019s images."
        },
        {
            "heading": "3.1 Analyzing the Training Process",
            "text": "To investigate the role of different images encountered in a training batch during the contrastive learning process in SimCLR, we start by measuring the average cosine similarities between the positive and negative pairs of image embeddings with respect to the anchor images in a given training batch. Figure 2 (left) showcases those measurements during the training process across 20 epochs. At the beginning of training, both sets of positive and negative images have similar cosine similarity to the anchor images. This implies that\nthe employed model before training cannot discriminate images that are semantically similar to an anchor image from images that are semantically different. As the training goes on, however, we notice that the images that belong to positive sets are more closely embedded to the anchor images compared to the images that belong to the negative sets. This behavior indicates that the model learns semantic similarities between images and embeds those semantics into the produced high-level image representations.\nFor GameCLR, to evaluate the degree to which game scene augmentation impacts the learning process, we measure the changes in cosine similarities between synthetic positives, synthetic negatives and regular negatives with respect to the anchor throughout training (see Fig. 2). We observe that all sets of images start at a similar level of cosine similarity with the anchor, but as training advances, synthetic negatives prove harder to contrast than regular negatives. Since both negatives are included in the contrastive loss of Eq. (1), the higher and more granular loss provides a more informative learning signal to themodel. Interestingly, by the time the algorithm converges, the model learns to distinguish synthetic negatives at a similar level as that of regular negatives. This indicates that after convergence the model is easily able to distinguish between the distinct game states including the synthetic hard negatives.\nThis analysis shows the superior learning capability afforded by the synthetic images obtained by directly modifying the pixels of the image with the help of the game engine. In order to quantify this benefit in terms of applicability to games research, we compare the models trained by these two approaches based on post-training evaluation, described in the following section."
        },
        {
            "heading": "3.2 Post-Training Evaluation",
            "text": "As proposed by Anand et al. [1], we evaluate how well the learned representations have captured information relevant to the game state through linear probing. Linear probing includes freezing the weights of the ResNet encoder after the self-supervised training is over (i.e. the contrastive loss has converged). Then, we train linear regression models with the learned representations acting as the predictor variables (input) and certain variables describing the game state acting as the response variables (output). We measure the performance of these regression models with the \ud835\udc452 correlation metric, where higher correlation values suggest that the model has better learned to identify the game state variables from the images.\nWe aim to test whether the derived representations of our models can describe the traffic around the ego-vehicle irrespective of weather and lighting conditions. Therefore, we prepare an evaluation dataset in CARLA by spawning an ego-vehicle at a random location with a camera and collecting RGB images, and at the same time collecting information about the coordinates and motion direction of the vehicles surrounding this ego-vehicle, similar to [12]. We refer to these as traffic variables, as they can describe the state of traffic around the ego-vehicle. For each frame in our dataset, we collect a total of 6 synchronized traffic variables: Distance (left vehicle), Direction (left vehicle), Distance (front vehicle), Direction (front vehicle), Distance (right vehicle), Direction (right vehicle). Note that we are able to find this ground truth of the traffic variables due to direct access to the game engine of CARLA. Let us stress that the traffic variables are not used during the training of our contrastive models; they are only used as desired output for linear probing after training is completed.\nTable 2 presents the average correlation values observed for each traffic variable in our evaluation dataset. We observe that both methods\u2014SimCLR and GameCLR\u2014improve upon the baseline of a randomly initialized ResNet18 model, verifying that contrastive learning is an effective solution for learning to differentiate between distinct game states. Across the six in-game variables, SimCLR provides a 157% improvement over the untrained baseline, on average, whereas GameCLR provides an improvement of 174% on average.\nSince contrastive learning is guided by engine-specific hard negatives in GameCLR, the representations obtained by this method outperform SimCLR by 11% on average on the linear probing task while using the same amount of images and training steps. This suggests that the ResNet18 encoder trained using the GameCLR approach extracts more meaningful representations that better capture traffic information in the game image compared to SimCLR. All \ud835\udc452 values for the different in-game variables in GameCLR are significantly higher than SimCLR (\ud835\udc5d < 0.05), with the highest improvement achieved for the distance to left vehicle (20% improvement over SimCLR), and\u2014surprisingly\u2014the least improved \ud835\udc452 was for the direction to the left vehicle (5.5% improvement over SimCLR)."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this paper we introduced GameCLR, a contrastive learning technique for learning game state representations. The main contribution of this technique is the introduction of game engines for synthesizing training images and enriching data augmentation in\nthis fashion. We notice that by synthesizing hard positives and negatives for each associated anchor image, we can better guide the contrastive learning process. Our results in the driving simulator CARLA suggest a 11% average improvement (in terms of \ud835\udc452) when extracting critical traffic-related game state features from images of this game with our GameCLR approach over another comparable approach (SimCLR). Our proposed method enables the user to control which visual features of a game the SSL method learns from the input RGB images by specifyingwhich engine variables (that impact rendering) produce synthetic positives andwhich produce synthetic negatives. Moreover, it shows the performance improvement over the standard contrastive learning approach SimCLR which uses simple image-based augmentation methods and does not exploit the game engine, as traditionally done when such computer vision methods are applied to games. Our proposed method enables the use of the game\u2019s images as input instead of explicit state information, with downstream applications in AI and games research like deep reinforcement learning for game-playing, pixel-level procedural content generation or correlating affect with game-play footage in player modeling."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Chintan Trivedi, Antonios Liapis and Georgios N. Yannakakis were supported by the European Union\u2019s H2020 research and innovation programme (Grant Agreement No. 951911). Konstantinos Makantasis was supported by the European Union\u2019s H2020 research and innovation programme (Grant Agreement No. 101003397)."
        }
    ],
    "title": "Game State Learning via Game Scene Augmentation",
    "year": 2022
}