{
    "abstractText": "Autonomous driving has now made great strides thanks to artificial intelligence, and numerous advanced methods have been proposed for vehicle end target detection, including single sensor or multi-sensor detection methods. However, the complexity and diversity of real traffic situations necessitate an examination of how to use these methods in real road conditions. In this paper, we propose RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving. We use a ROS-based virtual environment to simulate real-world conditions, in particular the physical and functional construction of the sensors. Then we implement muti-type sensor detection and multi-group sensors fusion in this environment, including camera-radar and camera-lidar detection based on result-level fusion. We produce local datasets and real sand table field, and conduct various experiments. Furthermore, we link a multi-agent collaborative scheduling system to the fusion detection system. Hence, the whole roadside detection system is formed by roadside perception, fusion detection, and scheduling planning. Through the experiments, it can be seen that RMMDet system we built plays an important role in vehicle-road collaboration and its optimization. The code and supplementary materials can be found at: https://github.com/OrangeSodahub/RMMDet",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiuyu Yang"
        },
        {
            "affiliations": [],
            "name": "Zhuangyan Zhang"
        },
        {
            "affiliations": [],
            "name": "Haikuo Du"
        },
        {
            "affiliations": [],
            "name": "Sui Yang"
        },
        {
            "affiliations": [],
            "name": "Fengping Sun"
        },
        {
            "affiliations": [],
            "name": "Yanbo Liu"
        },
        {
            "affiliations": [],
            "name": "Ling Pei"
        },
        {
            "affiliations": [],
            "name": "Wenchao Xu"
        },
        {
            "affiliations": [],
            "name": "Weiqi Sun"
        },
        {
            "affiliations": [],
            "name": "Zhengyu Li"
        }
    ],
    "id": "SP:a8fd4bf26260124ed7a5835837ba793e5e17cf44",
    "references": [
        {
            "authors": [
                "Z. Li",
                "M. Yan",
                "W. Jiang",
                "P. Xu"
            ],
            "title": "Vehicle Object Detection Based on RGB-Camera and Radar Sensor Fusion,",
            "venue": "International Joint Conference on Information, Media and Engineering (IJCIME),",
            "year": 2019
        },
        {
            "authors": [
                "R. Nabati",
                "H. Qi"
            ],
            "title": "RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles,",
            "venue": "IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan,",
            "year": 2019
        },
        {
            "authors": [
                "R. Nabati",
                "H. Qi"
            ],
            "title": "CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection,",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2021
        },
        {
            "authors": [
                "S. Shi",
                "X. Wang",
                "H. Li"
            ],
            "title": "PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "S. Shi"
            ],
            "title": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "X. Weng",
                "K. Kitani"
            ],
            "title": "Monocular 3D Object Detection with Pseudo- LiDAR Point Cloud,",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), Seoul, Korea (South),",
            "year": 2019
        },
        {
            "authors": [
                "C. Reading",
                "A. Harakeh",
                "J. Chae",
                "S.L. Waslander"
            ],
            "title": "Categorical Depth Distribution Network for Monocular 3D Object Detection,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "D. Xu",
                "D. Anguelov",
                "A. Jain"
            ],
            "title": "PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Wang",
                "K. Jia"
            ],
            "title": "Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection,",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zeng",
                "C. Ma",
                "M. Zhu",
                "Z. Fan",
                "X. Yang"
            ],
            "title": "Cross-Modal 3D Object Detection and Tracking for Auto-Driving,",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Prague, Czech Republic,",
            "year": 2021
        },
        {
            "authors": [
                "C. Wang",
                "C. Ma",
                "M. Zhu",
                "X. Yang",
                "\u201dPointAugmenting: Cross"
            ],
            "title": "Modal Augmentation for 3D Object Detection,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "N. Hendy"
            ],
            "title": "Fishingnet: Future inference of semantic heatmaps in grids",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "R. Ravindran",
                "M.J. Santora",
                "M.M. Jamali"
            ],
            "title": "Camera lidar and radar sensor fusion based on bayesian neural network (clr-bnn)",
            "venue": "IEEE Sensors Journal,",
            "year": 2022
        },
        {
            "authors": [
                "L. Wang"
            ],
            "title": "InterFusion: Interaction-based 4D Radar and LiDAR Fusion for 3D Object Detection,",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Kyoto, Japan,",
            "year": 2022
        },
        {
            "authors": [
                "L. Wang",
                "Z. Zhang",
                "X. Di",
                "J. Tian"
            ],
            "title": "A Roadside Camera-Radar Sensing Fusion System for Intelligent Transportation,",
            "venue": "European Radar Conference (EuRAD), Utrecht,",
            "year": 2021
        },
        {
            "authors": [
                "G.T. Ho",
                "Y.P. Tsang",
                "C. Wu",
                "W.H. Wong",
                "K.L. Choy"
            ],
            "title": "A Computer Vision-Based Roadside Occupation Surveillance System for Intelligent Transport in Smart Cities",
            "venue": "Sensors (Basel, Switzerland),",
            "year": 2019
        },
        {
            "authors": [
                "A. Kr\u00e4mmer",
                "C. Sch\u00f6ller",
                "D. Gulati",
                "V. Lakshminarasimhan",
                "F. Kurz",
                "D. Rosenbaum",
                "C. Lenz",
                "A. Knoll"
            ],
            "title": "Providentia \u2013 A Large-Scale Sensor System for the Assistance of Autonomous Vehicles and Its Evaluation",
            "venue": "Field Robotics",
            "year": 2019
        },
        {
            "authors": [
                "S. Pang",
                "D. Morris",
                "H. Radha"
            ],
            "title": "CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection,",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhao",
                "H. Zhou",
                "X. Zhu",
                "X. Song",
                "H. Li",
                "W. Tao"
            ],
            "title": "LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR",
            "venue": "Semantic Segmentation. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Quigley",
                "Morgan"
            ],
            "title": "ROS: an open-source Robot Operating System.",
            "venue": "IEEE International Conference on Robotics and Automation",
            "year": 2009
        },
        {
            "authors": [
                "T. Moore",
                "D. Stouch"
            ],
            "title": "A Generalized Extended Kalman Filter Implementation for the Robot Operating System",
            "venue": "Intelligent Autonomous Systems 13. Advances in Intelligent Systems and Computing,",
            "year": 2016
        },
        {
            "authors": [
                "J. Cao",
                "S. Leng",
                "K. Zhang"
            ],
            "title": "Multi-Agent Learning Empowered Collaborative Decision for Autonomous Driving Vehicles,",
            "venue": "International Conference on UK-China Emerging Technologies (UCET),",
            "year": 2020
        },
        {
            "authors": [
                "TY Lin"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2014
        },
        {
            "authors": [
                "M. Himmelsbach",
                "F. v. Hundelshausen",
                "H. . -J. Wuensche"
            ],
            "title": "Fast segmentation of 3D point clouds for ground vehicles,",
            "venue": "IEEE Intelligent Vehicles Symposium, La Jolla, CA,",
            "year": 2010
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "R. Urtasun"
            ],
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Sager",
                "Christoph",
                "Zschech",
                "Patrick",
                "K\u00fchl",
                "Niklas"
            ],
            "title": "label- Cloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "S. Liu",
                "X. Shen",
                "J. Jia"
            ],
            "title": "Fast Point R-CNN,",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South),",
            "year": 2019
        },
        {
            "authors": [
                "C. Li",
                "D. Parker",
                "Q. Hao"
            ],
            "title": "A Value-based Dynamic Learning Approach for Vehicle Dispatch in Ride-Sharing,",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Kyoto, Japan,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nNow for autonomous driving, the self-control and decision are obtained based on the collection of massive traffic states and intensive information processing. Note that the spatial-temporal characteristics of the traffic states and the constrained perception range of an individual vehicle seriously undermine the effectiveness of the state collection, and sensors have their weakness. However, roadside sensing provides extended coverage and more information compared\nThis work was supported in part by Cooperative education program of the Ministry of Education 202101001039 and in part by the National Natural Science Foundation of China under Grant Nos. 61873163. *These authors contributed equally to this work.\n1Xiuyu Yang and Fengping Sun are with the Department of Electronic Engineering of SEIEE, Shanghai Jiao Tong University. {gzzyyxy, sunfengping}@sjtu.edu.cn\n2Zhuangyan Zhang and Sui Yang are with the Department of Electrical Engineering of SEIEE, Shanghai Jiao Tong University. {zhangzhuangyan, suiyang}@sjtu.edu.cn\n3Haikuo Du is with the Department of Automation of SEIEE, Shanghai Jiao Tong University. duhaikuo1013@sjtu.edu.cn\n\u2020 is the corresponding author. 4Yanbo Liu and Ling Pei are with Teaching development and Student Innovation Center of SEIEE, Shanghai Jiao Tong University. {liuyanbo1205, ling.pei}@sjtu.edu.cn\n6Wenchao Xu is with Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University. wchxu@ce.ecnu.edu.cn\n7Weiqi Sun is with Xin Dong Interactive Entertainment Company Ltd, Shanghai. sunweiqi7777@gmail.com\n7Zhengyu Li is with Summer Son Smart Technology Company Ltd, Shanghai. zhengyuli@gmail.com\nwith independent vehicle sensors, and now many multisensor fusion strategies are proposed to achieve complementary advantages. In [1], the author proposed to combine color image information and point cloud geometry information by using MobelNet V3 as the backbone and migrated RPN layer, this work basically introduces two modal data fusion to us. As for the camera-radar information use, RRPN [2] mapped radar detection results to the image plane for anchor frames generation, so it also brings large computation reduction. CenterFusion [3] proposed the frustum association mechanism to realize the radar and image detection fusion. In contrast, more work has been done on lidar since its strong characteristics and ability to carry more severe and complex detection tasks. Generally it could be divided into: lidar-only methods [4]-[6], image-only methods [7] [8] and lidar-RGB image methods [9]-[12]. While less work focused on camera-radar-lidar fusion detection [13] [14] or radar-lidar fusion [15]. Although these fusion work continued the SOTA performance on detecting or tracking objects, unfortunately, could suffer from the spatial-temporal misalignment inherent in point fusion [12], or inaccurate correspondence among dense or sparse feature vectors for feature-level fusion [11], as CLOC [19] and LIF-Seg [20] proved. For roadside sensing and detection, previous work [16] [17] [18] built a system more useful to intelligence transport relying on camera-radar or vision only, despite the demonstrated success, there exists limitations on its single application scenario, such as the inner city straight road [17] for traffic activities surveillance ar X iv :2 30 3. 05 20\n3v 3\n[ cs\n.R O\n] 1\n0 Ju\nn 20\n23\nor the suburban highway [18]. As for an real roadside detection system for auto-driving, we identify the key points to (i) Efficiency (i.e. high speed and low memory)\u2014undertake the task of real time data acquisition, transmission and inference, and even scheduling. (ii) Robustness\u2014ensure that system will not fail under abnormal conditions. Considering the interdependence and computation in data-level fusion and feature-level fusion detection algorithms [9]-[15], we chose to establish a resultlevel fusion mechanism. (iii) Modularity\u2014use any pair of 2D and 3D detectors as modules, which will be easily for system renovation and experiments. And (iv) Virtual counterpart\u2014use a simple yet functional simulation environment with strong correlation with the real conditions for the maximum plasticity and convenience. In this paper, we present RMMDet, a roadside multi-type and multi-group sensor detection system for autonomous driving. We use ROS [21] to model all the elements, including the environment, vehicles and sensors. We adopt PointPillars [5] and YOLOv4 [22] as our independent 3D and 2D detection algorithms. We use the method adapted from EKF [23] and RRPN [2] to execute the radar-side prediction and radar-camera detection, and NMS-like mechanism to help with the lidarcamera detection.\nAdditionally, we add a multi-agent collaborative scheduling system, so as to expand our work to a more comprehensive level. Some researches [24] on vehicle-road cooperation introduced multi-mode information fusion mechanism, which expands the range of traffic information exchange. Since all the work mentioned above have carried out hierarchical research from various aspects, here we try to consider them as a whole and perform experiments.\nThis paper contributes:\n1) A comprehensive simulation environment based on ROS integrating multi-type sensors physically and functionally which reaches mentioned efficiency, robustness and modularity. 2) An end-to-end multi-type and multi-group sensor detection system initially running on the general algorithms for autonomous vehicles, which supports the interface between algorithmic reasoning and data streams and any other algorithm tests. 3) Virtual and actual experiments on this detection system based on downstream module (i.e. multi-agent collaborative scheduling system) to evaluate the whole real-time work and it validates our idea of integrating perception, detection and decision."
        },
        {
            "heading": "II. SIMULATION ENVIRONMENT",
            "text": "ROS [21] is used to construct a 1:18 scale simulation model in this paper. We first build models in SolidWorks based on accurate scaling and component connection and import them into ROS through URDF files. Completing the sensor function configuration, finally, it can achieve the information acquisition and transmission of various sensors."
        },
        {
            "heading": "A. Traffic Environment",
            "text": "We design a rectangular field of 8m \u00d7 10m (width and length) which takes the actual situation into full consideration, including viaduct, ring road, intersection, straight road, turning road, and other common elements.\nAs shown in Figure 2, among them, the detection system is deployed in the circular section, intersection, and viaduct ramps. Considering the traffic flow around the ring road and the intersection is relatively complex and heavy, we set four monocular cameras positioned 90\u00b0 apart in four directions, and one 32-line lidar centrally suspended for the circular section, two 16-line lidars diagonally located for the intersection, where every four cameras could cover 360\u00b0 with additional overlaps.\nAt the viaduct sections, vehicles are usually aligned with long lines, not particularly clumpy or clustered distribution, and the directions are not irregularly staggered at multiple angles, where millimeter wave radar plays a greater role in measuring the speed, position, and other information of the target accurately. Therefore, we set the camera-radar detection system, which is also suitable for others with similar characteristics such as the highways [18].\nIn addition, we also set up the driving environment of the real vehicle (See Experiments Sec.VI) to validate our scheme in real world."
        },
        {
            "heading": "B. Sensors: camera, radar and lidar",
            "text": "Bodies of sensors are modeled according to their lifesizes and the scale. Then they are functionally configured to capture and transmit massive data, specifically using\nthe camera plug-in in ROS, Robosense lidar plug-in1 and millimeter wave radar plug-in2 for the image acquisition, point cloud generation and radar points acquisition. Table I shows the attributes of sensors and their data stream. \u201dFPS\u201d and \u201dShape\u201d are both in the simulation environment running on CPU intel i7 and GPU RTX3080."
        },
        {
            "heading": "III. OVERALL FRAMEWORK",
            "text": "Figure 1 illustrates the main framework of the whole system with three parts: sensor layer, fusion perception layer, and decision planning layer. The sensor layer contains physical devices of three types of sensors which have been constructed in Sensors (Sec. II-B).\nThe fusion detection layer contains multi-type sensor target detection and multi-group sensor fusion detection. After preprocessing the raw data obtained from the sensors, feed them into the separate branches of image detection based on cameras, 2D point cloud detection based on the radars, and 3D point cloud detection based on lidars. This operation could be performed in the respective mobile edge computing platform (MEC). Then different fusion strategies enhance the detection and output final results, including cameraradar group and camera-lidar group. Additionally, the unique information participated in fusion, such as target velocity detected by millimeter wave radar, etc, will also be hold in outputs.\nMulti-agent Collaborative system, a lightweight module for downstream tasks, is realized in the Decision planning layer.Based on different decision-making agents divided according to the traffic function or spatial-level characteristics, each body detects within its area respectively and carries out the information transmission among the vehicles and bodies."
        },
        {
            "heading": "IV. RMMDET: ROAD-SIDE MULTITYPE AND MULTIGROUP SENSOR DETECTION SYSTEM",
            "text": ""
        },
        {
            "heading": "A. Coordinate System",
            "text": "Four coordinate systems are depicted in Fig.3: camera(pixel), radar, lidar, world coordinates.The subsequent steps involve the transformations of these four coordinate systems to ensure the unity of spatial representation of different data, which can be summarized as following equations:\nChw = [ R T 0 1 ] Chc,r,l (1)\nZc \u00b7Chu,v = PK \u00b7Chc = PKE \u00b7Chw (2)\nIn eq.1, three types of subscripts of homogeneous coordinates Ch{\u00b7} denote the pixel frame (i.e. C h u,v), world frame (i.e. Chw) and reference frame (i.e. C h c,r,l) respectively, where the reference represents the sensors. Other forms of this formula could project the target from any frame to the other one. And Eq.2 enables the projection from world or camera coordinates to the 2-dimensional pixel coordinates, where K and E represents the intrinsic and extrinsic camera parameter matrix. Matrix P transforms the unit from meter to pixel and produces the pixel coordinates Chu,v."
        },
        {
            "heading": "B. Multi-type Sensor Detection",
            "text": "As mentioned before, an ideal road-side detection system must be efficient and robust to perform high-load real-time detection tasks. Most of detectors [9]-[15] uses the features after fusion as the input of prediction head. Unfortunately, this results in strong correlation and interdependence among different modal data. Once any of them is contaminated or fails, others associated will not contribute to the detection system. Thus we turn to result-level fusion mechanism and build independent multi-type sensor detection in advance.\nRadar. Inspired by RRPN [2] that generates anchors based on projected radar points on images, and CenterFusion [3] that expands the radar points to pillars to eliminate the variance of data in z-dimension, after the projection from radar coordinates to camera coordinates and producing the points of interests (POIs) on image, we expand the discrete points to the region of interests (ROIs) based on the known\n1Robosense lidar plug-in: https://github.com/RoboSense-LiDAR/rslidarsdk\n2Millimeter wave radar plug-in: https://github.com/OrangeSodahub/DelphiESR-SSR-gazebo-plugin\ntarget sizes and the distance compensation. We apply the scaling factor to the predefined sizes to complete the distance compensation:\nSi = \u03b1 di +\u03b2 (3)\nIn eq.3, Si is the scaling factor, di is the perpendicular distance along the normal detection plane to the target, \u03b1 and \u03b2 are two parameters determined by the size of the vehicle. The process of radar ROIs generation is shown in Figure 4.\nCamera and Lidar. YOLOv4 [22] and PointPillars [5] are our independent 2D detection and 3D point cloud detection tools. According to the report of PointPillars [5], it has excellent detection accuracy especially low latency which enables the real-time applications. And YOLOv4 achieved 43.5% AP (65.7% AP50) for the MS COCO dataset [25] at a real-time speed of \u223c65 FPS on Tesla V100. Other advanced algorithms could also be added to the system.\nPoint cloud data from different lidars are merged first to reach the unified spatial representations and enhance the perception information, that is, apply the coordinates transformation based on eq.1 and concatenation. And the whole size of points data in our simulation environment is up to 6000 \u223c 7000, of which invalid points account for a large proportion. To greatly improve the efficiency, following M. Himmelsbach et al. [26], we assign the Gaussian filter and RANSAC to filter the noise, outliers and ground points, which brings reduction by around 40%.\nData synchronization. Though multiple detectors work independently, different data stream should be synchronized to ensure the valid results. Specifically, Coordinate system (Sec. IV-A) already achieves the spatial synchronization. And we align the timestamps of data steams released by ROS to get the temporal synchronization."
        },
        {
            "heading": "C. Multi-group Sensor Fusion Detection",
            "text": "Millimeter-wave Radar and Camera Fusion. The fusion layer takes in the ROIs from the radars and the cameras and returns the world coordinates of the detected objects. It includes two steps, IOU matching and target tracking, as shown in Fig.5.\nThe IOU matching step identifies potential objects by the ROIs from radars and cameras, whose correlation is evaluated by the intersection over union (IOU). As shown in Algorithm 1, the program traverses the ROIs, calculates the IOU matrix, separates the raw detection results into pairs (potential objects) by the K-M algorithm, sorts them by IOU and discards those have low correlation (IOU < 0.6). The pixel coordinates of each pair are transformed to the world coordinates and then returned.\nAlgorithm 1: Radar-Camera IOU Matching Input: Radar ROIs and Camera ROIs Output: Concatenated Pixel Coordinates of Potential\nObjects 1 for each radar ROI and camera ROI do 2 calculate the IOU of radar ROI and camera ROI; 3 end 4 matched pairs = K-M algorithm (IOU matrix); 5 for each pair of matched pairs do 6 if IOU < 0.6 then 7 remove the pair from matched pairs; 8 else 9 concatenate the pixel coordinates;\n10 end 11 end 12 return concatenated pixel coordinates of potential\nobjects;\nThe target tracking step checks the potential objects and denoises. As shown in Fig.6, the program calculates the distance between previous detected objects and the potential objects and then categorizes the potential objects as new, lost, and existing ones. The new objects are not returned for stability, while the lost ones will be discarded if not being detected for over 3 consecutive frames. Two nearing objects in two frames are regarded as an existing object, whose coordinates are filtered by Kalman Filter and then returned.\nThe time update equations calculate the prior state estimate\nbased on the results of the last frame.{ x\u0303\u2212k = Ax\u0303k\u22121 P\u2212k = APk\u22121A T +Q (4)\nThe measurement update equations calculate the posterior state estimate based on the prior state estimate and the measurement. The Kalman gain K is the ratio of predicted error to measurement error.\nK = P\u2212k H T(HP\u2212k HT +R)\u22121 (5){\nx\u0303k = x\u0303\u2212k +K ( zk \u2212Hx\u0303\u2212k ) Pk = (I\u2212KH)P\u2212k\n(6)\nAs shown in Eq.4-6, xk is the real world coordinate of an existing object, zk is the observed one. Pk is the state covariance matrix, which we try to minimize. The transition matrix A and the observation matrix H are identity matrices in this article. The transition covariance Q and the observation covariance R are manually set.\nAfter that, the world coordinates of new and existing objects are stored in the storage and wait to be loaded in the next frame.\nLidar and Camera Fusion. The fusion strategy is shown in Fig.5. The data obtained by the lidar and the camera are processed and then fed into separate detection branches.\nYOLOv4 [22] and PointPillars [5], two conventional and popular algorithms, are used for different model object detection. We get two types of predicted boxes at the ends of the branches, DC \u2208 RN\u00d78 and DL \u2208 RL\u00d7M\u00d74. To get the last single result, as shown in the Algorithm 2, boxes are concatenated and matched. Specifically, for each 3D detection box, there exists its multiple 2D detection boxes lie on different image frame, due to the overlap of range of cameras, and those with larger areas on the image plane are relatively more accurate, in this regard, we sort them based on the distances from the 3D prediction to the cameras. This operation only depends on the soft physical associations among sensors, which nearly has no influence on the accuracy. We mark the matched box of the first camera (geographically nearest) in the sorted camera list, and mask the matched boxes of others. By traversing the camera list and all of 2D boxes for each 3D box, we calculate the IOU to determine whether each 2D box is marked, masked or discarded, except the marked or masked ones in the former steps. When N 3D detection boxes in the scene are traversed, all the 2D detection boxes\nare traversed N times, hence we discard those who are still not matched.\nAlgorithm 2: Lidar-Camera Box Matching Input: Raw detection boxes DC and DL Output: Merged detection boxes\n1 matched boxes: D\u0307C, D\u0307L = [], [ ]; 2 for dli, i=1,2,3... in D\nL do 3 calculate the distances array A = {||dli \u2212 c j||}; 4 generate the ordered list Ci and DCi sorted by A; 5 for ci j, j=1,2,3... in Ci do 6 map dli to the image plane of ci j and get d\u0303 l i ; 7 for dci jk, k=1,2,3... in D C i j do 8 skip if dci jh exist in D\u0307 C; 9 calculate IOU of dci jk and d\u0303 l i and form the\narray Vi j; 10 end 11 get the max IOU from the list; 12 if max\nj Vi j \u2a7e 0.7 then\n13 get the corresponding 2d box dci jh; 14 if dli not in D\u0307L then 15 add dli to D\u0307L, and add d c i jh to D\u0307\nC;"
        },
        {
            "heading": "16 else",
            "text": "17 delete dci jh from D\nC;"
        },
        {
            "heading": "18 end",
            "text": ""
        },
        {
            "heading": "19 end",
            "text": "20 end 21 end 22 Discard unmatched 2d and 3d boxes;\nMerging is after matching. By projecting the center of image box to 3D world given the height (z-axis) of the target, we get finalize the box center based on weighted coordinates. And adjust the azimuth angle of coordinates-fixed 3d box by adding or subtracting angle increments until the aspect ratio of the lidar-based 2d box on the image plane is approximately equal to the weighted mean ratio. Generally, this step does not require much computation.\nFigure 7 shows the detection results of one frame, where the green boxes represent the results after successful matching and merging, while the white boxes represent the failures ones. Note the only one car in the far right corner, with\nthe misdetected result successfully removed through multisensor detection."
        },
        {
            "heading": "V. MULTI-AGENT COLLABORATIVE SCHEDULING AND DECISION",
            "text": ""
        },
        {
            "heading": "A. Agent Division",
            "text": "In the study of [23], the author proposed to switch between V2I mode and V2V mode as the traffic flows change to find the lowest delay and the most reliable decisions. This paper further extends its V2I mode, dividing the complex traffic scene into different subjects, each of which contains a roadside unit (RSU) consists of a sensor detection system and an edge computing platform.\nThe bases for dividing the main body are the traffic function, spatial hierarchy, or the distance. So we introduce the agent division to handle the long range of distance and functional differences. See in Fig.9, the agents consist of three parts: agent1\u2014the intersection, agent2\u2014the overpass ramp, and agent3\u2014the ring road. The three subjects complete the perception, fusion, and scheduling within their respective areas. In the decision stage, the information is exchanged and merged on the cloud platform to make a collaborative decision. The overall information transmission nodes and topic framework of the system are also shown in Fig.9."
        },
        {
            "heading": "B. Cooperative Scheduling",
            "text": "Given the agents covering the whole field, we link it to the perception and fusion system. For simplicity, we number and label all the roads and areas where vehicles may occur the traffic jam. From the information offered by perception fusion system, agents could directly build a vehicle density and speed distribution map, hence then transmit the messages that point out where to avoid the peak traffic or else. For our follow-up experiments, we make the vehicles led by agents to optimize the route at any time."
        },
        {
            "heading": "VI. EXPERIMENTS",
            "text": "A. Implementation Details\nDatasets. We build our image detection datasets through blender and gazebo. The collections consist of 13985 images all of which are annotated as VOC format. Train, test and validate split account for 6300, 4285 and 3400 respectively. These images are given by various perspectives and distances, untruncated and partially truncated. And it covers almost the states of the vehicles driving under all the roads.\nFor 3D point cloud detection, we shot the frames of them as they run in the environment similar to image shots and annotated as KITTI [27] format using LabelCloud toolbox [28]. Finally we used 1750 frames of the simulation scene and 1250, 550 of them for training and test. respectively. And data augmentations [29] are applied to improve the model generalization performance.\nModel. We adapted the configurations from PointPillars [5] to train our 3D detector. For our own datasets, we set the detection range to [\u221212m,12m] for X and Y axes, and [\u22121m,4m] for Z axis. We use (0.015m,0.015m) as the pillar size for experiments. For other default settings, the readers could refer to the OpenPCDet toolbox [30] used in this work.\nSim-to-Real. To valid the comprehensive function of our system, we performed successful sim-to-real transfer and built a real 8m \u00d7 10m sand table consistent with the simulation. We use PC as the MEC and communicate with AWS Deepracers via AIoT technology."
        },
        {
            "heading": "B. Muti-type and Muti-group Sensor Detection",
            "text": "In the simulation, gaussian noises are added to evaluate the KF. The performance of KF is shown in Fig.8 (top), where points in different color: green points\u2014observation of single sensor, cyan points\u2014observation of multiple sensors and red points\u2014output of the Kalman filter, showing the KF denoises effectively. In this case, the fusion can balance the noise influence and filter the false detection that may occur irregularly due to the hardware. Once any of detectors fails, the other can ensure the overall detection effectiveness.\nFig.8 (bottom) shows the performance of lidar-camera detection of 2.9\u00d7104 consecutive frames. Fig.8 (top) has the bird view of the overall scene, which acts as a geographical reference. The thermal map on the left shows the average pose error represented by Euclidean distance. We take the odometry information released by the vehicle itself as the ground truth. It can be seen that the error of most areas is within 0.045m.The map in the middle shows the average orientation similarity (AOS) of detections, using the cosine distance of the azimuth angle difference (Eq.7), and most parts excess 0.9. Similarly, the BEV IOUs of boxes are mostly greater than 0.7.\nAOS = 1 |D| \u03a3s\u2208D 1+ cos(2\u2206\u03b8s ) 2 (7)"
        },
        {
            "heading": "C. Scheduling System",
            "text": "To apply the detection system to the actual traffic flow and perform the downstream tasks, we attached a lightweight scheduling system with three agents covering the whole scene. We use AIoT to associate real vehicles with simulated environments and conduct experiments simultaneously, thus through the real vehicles we can feel more intuitively and preliminarily verify the feasibility of our scheme in real world. Fig.10 (Left) introduces one scenario, with 6 vehicles heading the same intersection.\nWe focus on the action of one vehicle, located on the right of frame 1. The critical points are the frame 2 and 4,\nwhere the vehicle faced with the possibility of joining the traffic jam. With the perception system offering the global information and the scheduling system offering route plans, most vehicles could always get what they want\u2014to avoid congestion while reach the least time cost. Here we only conduct a simple one\u2014to showcase the functionality and work process of our proposed system, under the limited conditions.\nIn the simulation, we conducted automatic driving of 10 vehicles randomly and schedulingly. Fig.10 (Right) shows the mean speed of all vehicles over 2.9\u00d7104 frames, where a great reduction on traffic load can be seen.\nAs depicted in Fig.11, the other proof is the flows of different road intersections over time. Under the help of global detection and scheduling system, the utilization rate of different roads is optimized, with the increased total flow (624 for random and 668 for scheduling, see Table II).\nFinally, we achieve a success of the whole system, get FPS \u223c 10\u2014the biggest loss lies in time synchronization among the three sensors, yet we reach the real-time run on RMMDet system."
        },
        {
            "heading": "VII. CONCLUSIONS AND DISCUSSIONS",
            "text": "In this paper, we present RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving, for more intelligent vehicle-road collaboration. RMMDet uses a virtual simulation counterpart based on ROS, implementing three types sensors detection and lidar-camera, radar-camera fusion detection in the complex traffic scene. Real-time multimodal data streams are captured, processed, fed into model and generate the desired results to construct an end-to-end efficient roadside system. For the detection algorithms, we initially choose YOLOv4 and pointpillars, which could be replaced by any other methods and it\u2019s convenient for further research and experiments. And we link a lightweight scheduling system to showcase the functionality.\nIn general, RMMDet presents an early attempt to build a comprehensive roadside system virtually and actually for autonomous driving. For further improvements, we plan to take more advanced algorithms into account and transfer the result-level fusion method to more learnable methods. And conduct more complex experiments under real conditions with more resources. This is a long-line work and we will keep on it."
        }
    ],
    "title": "RMMDet: Road-Side Multitype and Multigroup Sensor Detection System for Autonomous Driving",
    "year": 2023
}