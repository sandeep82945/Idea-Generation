{
    "abstractText": "Customer care is an essential pillar of the e-commerce shopping experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pretrained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Crossdomain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value. In this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Saurabh Kumara"
        },
        {
            "affiliations": [],
            "name": "Sourav Bansalb"
        },
        {
            "affiliations": [],
            "name": "Neeraj Agrawala"
        },
        {
            "affiliations": [],
            "name": "Priyanka Bhatta"
        }
    ],
    "id": "SP:e14a9142aac0bac7aa76ae7cb671e0d98748158f",
    "references": [
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding\u2019, in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
            "year": 2019
        },
        {
            "authors": [
                "Ruiying Geng",
                "Binhua Li",
                "Yongbin Li",
                "Xiaodan Zhu",
                "Ping Jian",
                "Jian Sun"
            ],
            "title": "Induction networks for few-shot text classification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Jing Gu",
                "Qingyang Wu",
                "Chongruo Wu",
                "Weiyan Shi",
                "Zhou Yu"
            ],
            "title": "PRAL: A tailored pre-training model for task-oriented dialog generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith"
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Henderson",
                "Ivan Vuli\u0107",
                "Daniela Gerz",
                "I\u00f1igo Casanueva",
                "Pawe\u0142 Budzianowski",
                "Sam Coope",
                "Georgios Spithourakis",
                "Tsung-Hsien Wen",
                "Nikola Mrk\u0161i\u0107",
                "Pei-Hao Su"
            ],
            "title": "Training neural response selection for task-oriented dialogue systems",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Hoang Nguyen",
                "Chenwei Zhang",
                "Congying Xia",
                "Philip Yu"
            ],
            "title": "Dynamic semantic matching and aggregation network for few-shot intent detection\u2019, in Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Baolin Peng",
                "Chenguang Zhu",
                "Chunyuan Li",
                "Xiujun Li",
                "Jinchao Li",
                "Michael Zeng",
                "Jianfeng Gao"
            ],
            "title": "Few-shot natural language generation for task-oriented dialog\u2019, in Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "CoRR, abs/2004.09813,",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Manex Serras",
                "Naiara Perez",
                "M. Torres",
                "Arantza Pozo",
                "Raquel Justo"
            ],
            "title": "Topic classifier for customer service dialog systems",
            "year": 2015
        },
        {
            "authors": [
                "Jingyi Xu",
                "Hieu Le"
            ],
            "title": "Generating representative samples for few-shot classification, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Yan",
                "Nan Duan",
                "Peng Chen",
                "Ming Zhou",
                "Jianshe Zhou",
                "Zhoujun Li"
            ],
            "title": "Building task-oriented dialogue systems for online shopping",
            "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Haode Zhang",
                "Haowen Liang",
                "Yuwei Zhang",
                "Liming Zhan",
                "Xiao-Ming Wu",
                "Xiaolei Lu",
                "Albert Y.S. Lam"
            ],
            "title": "Fine-tuning pre-trained language models for few-shot intent detection: Supervised pre-training and isotropization, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Haode Zhang",
                "Yuwei Zhang",
                "Li-Ming Zhan",
                "Jiaxin Chen",
                "Guangyuan Shi",
                "Xiao-Ming Wu",
                "Albert Y.S. Lam"
            ],
            "title": "Effectiveness of pre-training for few-shot intent classification\u2019, in Findings of the Association for Computational Linguistics",
            "venue": "EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Jianguo Zhang",
                "Kazuma Hashimoto",
                "Wenhao Liu",
                "Chien-Sheng Wu",
                "Yao Wan",
                "Philip Yu",
                "Richard Socher",
                "Caiming Xiong"
            ],
            "title": "Discriminative nearest neighbor few-shot intent detection by transferring natural language inference",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pretrained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Crossdomain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value.\nIn this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models."
        },
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue systems have been widely deployed for a variety of sectors ranging from Shopping [15] to Customer Care [13], to provide an interactive experience. Intent classification is a core module that drives the working of task-oriented dialogue systems. Thus, building accurate classifiers is important for the development of such systems.\nTraditional methods have fine-tuned pre-trained language models for various NLP (natural language processing) applications and have achieved state-of-the-art performance. While fine-tuning certainly generates improvements, a significant challenge is the availability of sufficient annotated data as the performance is highly de-\n\u2217 Corresponding Author. Email: saurabh.kumar1@walmart.com \u2217\u2217 Work was done as an intern at Walmart Global Tech, Bangalore, India\npendent on it. However, large annotated datasets are usually available only for selected languages and tasks. This poses a challenge in the development of models which could serve users in multiple domains (defined by channel, geography, and language) when only a few labeled samples are available. Hence, building a generalized few-shot cross-domain classifier that leverages information across domains is an important problem.\nRecently, the problem has received considerable attention. Some recent works have used generation-based methods [14], induction network [4] and metric learning [9] for building few-shot classifiers. The focus of these works has been designing new algorithms which consequently come with complicated models. With rising interest and credibility of large-scale pre-trained language models, many works have fine-tuned these models [18, 17] which have yielded promising results in various downstream tasks. However, the construction of large annotated datasets is an expensive and laborious task. Consequently, the focus of many recent works has shifted to unsupervised pre-training of language models [6, 5] with specifically designed objectives to suit downstream tasks. Such methods have generated good results but utilize large-scale corpora of unsupervised data with closely related semantics.\nAn important prerequisite for models to work across multiple domains and languages is the alignment of vector spaces across domains. This enables the model to produce language-agnostic sentence representations which can capture rich semantic information, highly useful for downstream classification tasks. In this paper, we present a method to train a generalized sentence embedding model useful for cross-domain classification tasks. The solution provides the capability to expose domain-specific state-of-the-art models to new domains with just a few labelled utterances.\nThe proposed method utilizes a knowledge distillation strategy [11] in order to extend the intelligence of the existing domainspecific model, called the teacher model to a cross-domain multilingual model, called the student model. The teacher model is a finetuned large language model, which is used to generate sentence embeddings of the utterances for the source domain. The student model is trained to mimic the teacher model in a multilingual setup, that is, it maps utterances with similar meaning in other languages close to the original utterance. The trained student model can now be deployed in new domains by training a classifier with just a few examples.\nThe proposed method also adopts isotropic regularizers for im-\nproving sentence representations generated by the models. [16] mentions that fine-tuned PLMs (Pre-Trained Language models) may suffer from anisotropy, which could be the reason for the sub-optimal performance of PLMs on downstream tasks. We utilize a correlation matrix-based regularizer to regularize the supervised training of the teacher model. This improves the embeddings generated by the teacher model, which in turn leads to a more accurate student model.\nExtensive experiments were conducted to validate the performance of the model. Experiments on Canada e-commerce Customer Care domain English dataset with few-shot intent detection have shown an increase in accuracy by 23% against the existing state-ofthe-art pre-trained models. Experiments across Mexico e-commerce Customer Care domain Spanish Dataset have also shown a boost in accuracy of 20% against the existing state-of-the-art models.\nThe main contributions of our paper are summarised as follows:\n\u2022 We propose an embedder-cum-classifier model architecture that extends domain-specific monolingual models to other domains with only a few labelled utterances. \u2022 We fine-tune pre-trained language models using labelled data, introduce isotropic regularizers to improve sentence representations in source domains, and transfer the intelligence to a student model using a knowledge distillation process in a multilingual setup, which allows it to function in multiple domains. \u2022 Experiments demonstrate that the proposed model achieves an improvement of 23% and 20% in Canada and Mexico Customer Care Dataset respectively against the existing state-of-the-art pretrained models.\nThe rest of the paper is organized as follows. Section 2 presents the methodology used for training. Section 3 presents the results of experiments used for evaluating the model.\n2 Methodology\nThis section describes the proposed model in detail. Figures 1 and 3 show the overall architecture of the model.\n2.1 Text Encoder\nMultilingual BERT [2], one of the state-of-the-art encoders for multilingual texts, has been used as the text encoder. BERT uses WordPiece algorithm to tokenize the input text. Given an input text x, it is tokenized into sequence of tokens x1, x2, ...xn\u22122. BERT adds [CLS] and [SEP], two special tokens indicating the beginning and the end of the sequence. Therefore, the final sequence of tokens of length n is represented as:\nx = {[CLS], x1, x2, ..., xn\u22122, [SEP]} BERT encodes input tokens and outputs encodings corresponding to each token. We use encoding corresponding to [CLS] token as the representation of the sentence fed.\nh = BERT(x)\nWhere, h \u2208 Rd, d is size of the sentence embedding generated.\n2.2 Supervised Pre-training\nRecent works have shown that further pre-training off-the-shelf PLMs using dialogue corpora [7, 10] is beneficial for task-oriented downstream tasks such as intent detection.\nSimilar to [17, 16], our pre-training method relies on the existence of a labeled dataset Dlabeledsource = {(xi, yi)}, where yi is the label for utterance xi.\nGiven Dlabeledsource = {(xi, yi)} with N different classes, we employ a simple method to fine-tune BERT. Specifically, a linear layer is attached on top of BERT as the classifier, i.e.,\np (y | hi) = softmax (Whi + b) \u2208 RN where hi \u2208 Rd is the feature representation of xi given by the [CLS] token, W \u2208 RN\u00d7d and b \u2208 RN are parameters of the linear layer. The model parameters \u03b8 = {\u03c6,W,b}, with \u03c6 being the parameters of BERT, are trained on Dlabeledsource with a cross-entropy loss:\n\u03b8\u2217 = \u03b8argminLce ( Dlabledsource; \u03b8 )"
        },
        {
            "heading": "2.2.1 Fine-tuning leads to Anisotropy",
            "text": "Recent work from [16] mentions that further pre-training off-theshelf PLMs may lead to anisotropy, which has been identified as the reason for the sub-optimal performance of PLMs on downstream tasks. Hence, we utilize isotropic regularizers as mentioned in the next section."
        },
        {
            "heading": "2.3 Regularizing Supervised Pre-training with Isotropization",
            "text": "Isotropization techniques can be applied to adjust the embedding space and yield significant performance improvement in many tasks. Figure 2 illustrates the effect of supervised pre-training and regularised supervised pre-training on isotropy. To mitigate the anisotropy of the PLM fine-tuned by supervised pre-training, as proposed by [16], we use a joint training objective by adding a regularization term Lreg for isotropization:\nL = Lce (Dsource; \u03b8) + \u03bbLreg (Dsource; \u03b8) where \u03bb is a weight parameter. The aim is to learn intent detection skills while maintaining an appropriate degree of isotropy. We use a Correlation-matrix-based regularizer [16]\nLreg = \u2016\u03a3\u2212 I\u2016 where \u2016 \u00b7 \u2016 denotes Frobenius norm, I \u2208 Rd\u00d7d is the identity matrix, \u03a3 \u2208 Rd\u00d7d is the correlation matrix with \u03a3ij being the Pearson correlation coefficient between the ith dimension and the jth dimension. \u03a3 is estimated with utterances in the current batch. By pushing the correlation matrix towards the identity matrix during training, we can learn a more isotropic feature space."
        },
        {
            "heading": "2.4 Knowledge distillation",
            "text": "With regularized supervised pre-training as explained in section 2.3, we already have a model which can generate accurate embeddings for a domain. We use this model as the teacher model (M ) and transfer the intelligence to the student model (M\u0302 ). The multilingual knowledge distillation process has been explained in Figure 3. The teacher model maps the sentences in the source domain to a highdimensional vector space.\nFor the multilingual distillation process, we utilize an unsupervised dataset of parallel translated sentences, denoted as D = {((s1, t1), . . . , (sn, tn))}, where si is the sentence in the source domain language and ti is the sentence in the target domain language. The training process for the student model minimizes the mean-squared loss between embeddings generated by the teacher and student model. The mean-squared loss is taken between the embeddings of the teacher model in source language and the embeddings\nof the student model in source language as well as the embeddings of the teacher model in source language and embeddings of the student model in target language. The exact objective for a batch \u03b2 is mentioned in the equation below.\n1 |\u03b2| \u2211\nj\u2208\u03b2 [(M(sj)\u2212 M\u0302(sj))2 + (M(sj)\u2212 M\u0302(tj))2]\nThe loss function tries to remove the language bias, that is, sentences with similar meanings but in different languages are mapped closer than sentences in the same language with different meanings. In our experiments, we use the multilingual BERT as the teacher model M and multilingual DistilBERT as the student model M\u0302 ."
        },
        {
            "heading": "2.5 Few-shot Intent Classification",
            "text": "The student model (M\u0302 ) generated by Knowledge Distillation can be immediately used as a feature extractor for novel few-shot crossdomain multilingual intent classification tasks when used along with a classifier on top of it. The classifier can be a parametric one such as Support Vector Machine (SVM) or a non-parametric one such as nearest neighbor. A parametric classifier will be trained with the few labeled examples provided in a task and make predictions on the unlabeled queries. As evaluated in the experiments presented in the next section, a simple linear classifier suffices to achieve very good performance, owing to the effective utterance representations produced by our Knowledge Distilled Student Model.\n3 Experiments\nIn order to assess the effectiveness of the proposed method, we run several experiments across different datasets as mentioned below.\n3.1 Experiment Setup\nDatasets and Evaluation Metrics To perform supervised pretraining of the teacher model, we use a labeled US Customer Care English Chat dataset which consists of manually labeled issue summary texts written by users interacting with the e-commerce chatbot. For multilingual knowledge-distilled training of the student model, we use an unsupervised subset of the same dataset and its Spanish translation. For evaluation of cross-domain performance, we use Canada E-commerce Customer Care domain dataset which consists of utterances in the English language, Mexico E-commerce Customer Care Domain Chat dataset which consists of utterances in the Spanish language and US Customer Care IVR dataset which consists of utterances in the English language. Dataset statistics are summarized in Table 1.\nImplementation Details We conduct experiments on two of the most popular PLMs for multilingual texts, BERT [2] Multilingual version and DistilBERT [12] Multilingual version. BERT Multilingual has been used as the teacher model and DistilBERT Multilingual as the student model. Both models are trained using the process outlined in the previous section. As explained in section 2.1, for few-shot intent classification tasks, the embedding of [CLS] from the student model is used as the utterance representation. We employ Support Vector Machine (SVM) as the classifier.\nBaseline We compare our model to the following strong baselines. Firstly, for pre-trained large language models, we present a comparison against DistilBERT multilingual [2], BERT multilingual [12], LaBSE [3] and use-cmlm-multilingual [1]. LaBSE is a language-agnostic BERT embedder supporting 109 languages and use-cmlm-multilingual is model trained using Conditional Masked Language Modelling for improved sentence embeddings. Further, we compare our proposed method against four multilingual knowledgedistilled models pre-trained by authors of [11] and made available in the Sentence Transformers library. distiluse-base-multilingualcased-v1 and distiluse-base-multilingual-cased-v2 are multilingual distilBERT models with teacher model as USE, supporting 15 and 50 languages respectively. paraphrase-multilingual-MiniLML12-v2 is trained with the teacher model as paraphrase-MiniLML12-v2 and the student model as Multilingual-MiniLM-L12-H384 while paraphrase-multilingual-mpnet-base-v2 is trained with teacher model as paraphrase-mpnet-base-v2 and the student model as xlmroberta-base.\nTraining details The complete code has been written using Python and PyTorch library. We use Hugging Face implementation of bert-base-multilingual-cased and distilbert-base-multilingual-cased. Adam [8], with a learning rate of 2e\u22125 has been used as the optimizer. The model is trained with Nvidia V100 GPU."
        },
        {
            "heading": "3.2 Experiment Results",
            "text": "Table 2 compares the performance of the proposed approach against the baselines. The following observations can be made. All our models consistently outperform all the baselines by a significant margin, indicating the robustness of the proposed methods against cross-domain multilingual data. For the 5-shot classification task, our Student Model (M\u0302 ) outperforms the strongest baseline paraphrasemultilingual-mpnet-base-v2 by an absolute margin of 20% on Mexico e-commerce Customer Care Domain Spanish Language Dataset and 14% on Canada e-commerce Customer Care Domain English Language Dataset. The gain is attributed to supervised isotropic finetuning and knowledge-distillation strategy. In the IVR Domain, the student model shows a marginal improvement of 1%. This could be attributed to the very different nature of IVR Data which has been converted to text using Automatic Speech Recognition (ASR) and\nusually comes with smaller and broken phrases. On the Spanish language dataset of Mexico e-commerce Customer Care Domain, both student models outperform the corresponding teacher models by a large margin of around 10%. This is because the teacher models are fine-tuned on the English language dataset of US e-commerce Customer Care Domain, hence, unable to learn embeddings in the multilingual space. While for the English language dataset of Canada e-commerce Customer Care domain and IVR Domain, the student models and the corresponding teacher models yield similar performance since the utterances are in the same language as the US Customer Care Domain dataset (used for training). This also explains why the model achieves maximum performance on 5- shot training for Canada e-commerce Customer Care domain, as the model seems to overfit on adding more samples."
        },
        {
            "heading": "3.3 Analysis and Ablation Study",
            "text": ""
        },
        {
            "heading": "3.3.1 Effect of Isotropy",
            "text": "Table 2 shows that introducing a Correlation Matrix based regularizer for Isotropization leads to around 1% improvement in the accuracy score. We adjust the weight parameter \u03bb of Cor-Reg to analyze the relationship between the isotropy of the feature space and the performance of few-shot intent detection. Similar to [16], we observed that moderate isotropy is helpful. The performance of few-shot intent detection task increases with an increase in isotropy only up to a certain extent and decreases after that. This could be attributed to the fact that high isotropization may reduce the essence of supervised fine-tuning. Therefore, it is crucial to find a good balance between learning intent detection skills and learning anisotropic feature space."
        },
        {
            "heading": "3.3.2 Effect of Knowledge Distillation",
            "text": "We use t-SNE for visualization of high-dimensional embeddings in two-dimensional space as shown in Figure 4. Figure 4(a) shows embeddings generated by the pretrained multilingual BERT which exhibit very weak alignment among English and Spanish Language sentence representations. Though the graph shows some clustering for utterances of different intents, the clusters for English and Spanish languages are far apart. Figure 4(b) depicts embeddings for BERT fine-tuned on English labeled dataset (TEACHERv2). The clusters\nfor English sentence embeddings representing different intents have improved, however, representations for Spanish have degraded and are clustered in the center far apart from their English counterparts. Figure 4(c) shows the representation generated by the student model (STUDENTv2). It shows that the representation of sentences from English and Spanish Language are close. Further, there are no clusters which offer a generalized embedder that can be used for different classification tasks. Clearly, knowledge distillation helps produce language-agnostic representations, that is, representations are neutral with respect to the language."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this paper, we proposed a training method that utilizes a multilingual knowledge distillation strategy in order to extend a domainspecific state-of-the-art model to a cross-domain multilingual model. The paper also discusses the use of isotropic regularizers based on correlation matrix in order to improve sentence representations generated by the models. Combination of these two approaches leads to significantly improved performance on cross-domain multilingual few-shot intent detection task. Experiments confirm the feasibility and practicality of developing a few-shot intent classifier that could be deployed in multiple domains even with different languages, saving the cost involved in data annotation. Detailed ablation studies prove the effectiveness of the different components."
        }
    ],
    "title": "Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care",
    "year": 2023
}