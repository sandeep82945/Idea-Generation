{
    "abstractText": "We propose an information-theoretical measure, the relative cluster entropy DC[P\u2016Q], to discriminate among cluster partitions characterised by probability distribution functions P and Q. The measure is illustrated with the clusters generated by pairs of fractional Brownian motions with Hurst exponents H1 and H2 respectively. For subdiffusive, normal and superdiffusive sequences, the relative entropy sensibly depends on the difference between H1 and H2. By using the minimum relative entropy principle, cluster sequences characterized by different correlation degrees are distinguished and the optimal Hurst exponent is selected. As a case study, real-world cluster partitions of market price series are compared to those obtained from fully uncorrelated sequences (simple Browniam motions) assumed as a model. The minimum relative cluster entropy yields optimal Hurst exponents H1 = 0.55, H1 = 0.57, and H1 = 0.63 respectively for the prices of DJIA, S&P500, NASDAQ: a clear indication of non-markovianity. Finally, we derive the analytical expression of the relative cluster entropy and the outcomes are discussed for arbitrary pairs of power-laws probability distribution functions of continuous random variables. Copyright A. Carbone and L. Ponta. This work is licensed under the Creative Commons Attribution 4.0 International License. Published by the SciPost Foundation. Received 09-06-2022 Accepted 18-08-2022 Published 30-09-2022 Check for updates doi:10.21468/SciPostPhys.13.3.076",
    "authors": [
        {
            "affiliations": [],
            "name": "Anna Carbone"
        },
        {
            "affiliations": [],
            "name": "Linda Ponta"
        }
    ],
    "id": "SP:5e239be922af5d793c6bb5ff83f2925fdad9a89b",
    "references": [
        {
            "authors": [
                "C. Cafaro",
                "S. Alan Ali",
                "A. Giffin"
            ],
            "title": "Thermodynamic aspects of information transfer in complex dynamical systems",
            "venue": "Phys. Rev. E 93, 022114 (2016), doi:10.1103/PhysRevE.93.022114. 9 SciPost Phys. 13, 076 ",
            "year": 2022
        },
        {
            "authors": [
                "J.M.R. Parrondo",
                "J.M. Horowitz",
                "T. Sagawa"
            ],
            "title": "Thermodynamics of information",
            "venue": "Nat. Phys. 11, 131 ",
            "year": 2015
        },
        {
            "authors": [
                "R. Kawai"
            ],
            "title": "J",
            "venue": "M. R. Parrondo and C. Van den Broeck, Dissipation: The phase-space perspective, Phys. Rev. Lett. 98, 080602 ",
            "year": 2007
        },
        {
            "authors": [
                "J.M. Horowitz",
                "M. Esposito"
            ],
            "title": "Thermodynamics with continuous information flow",
            "venue": "Phys. Rev. X 4, 031015 ",
            "year": 2014
        },
        {
            "authors": [
                "S. Still",
                "D.A. Sivak",
                "A.J. Bell",
                "G.E. Crooks"
            ],
            "title": "Thermodynamics of prediction",
            "venue": "Phys. Rev. Lett. 109, 120604 ",
            "year": 2012
        },
        {
            "authors": [
                "P.A. Ortega",
                "D.A. Braun"
            ],
            "title": "Thermodynamics as a theory of decision-making with information-processing costs",
            "venue": "Proc. R. Soc. A. 469, 20120683 ",
            "year": 2013
        },
        {
            "authors": [
                "X. San Liang",
                "R. Kleeman"
            ],
            "title": "Information transfer between dynamical system components",
            "venue": "Phys. Rev. Lett. 95, 244101 ",
            "year": 2005
        },
        {
            "authors": [
                "J. Chen",
                "J. Feng",
                "W. Lu"
            ],
            "title": "A Wiener causality defined by divergence",
            "venue": "Neural Proc. Lett. 53, 1773 ",
            "year": 2020
        },
        {
            "authors": [
                "V. Vedral"
            ],
            "title": "The role of relative entropy in quantum information theory",
            "venue": "Rev. Mod. Phys. 74, 197 ",
            "year": 2002
        },
        {
            "authors": [
                "R. Kleeman"
            ],
            "title": "Measuring dynamical prediction utility using relative entropy",
            "venue": "J. Atmos. Sci. 59, 2057 (2002), doi:10.1175/1520-0469",
            "year": 2002
        },
        {
            "authors": [
                "C. Granero-Belinch\u00f3n",
                "S.G. Roux",
                "N.B. Garnier"
            ],
            "title": "Kullback-Leibler divergence measure of intermittency: Application to turbulence",
            "venue": "Phys. Rev. E 97, 013107 ",
            "year": 2018
        },
        {
            "authors": [
                "D. Backus",
                "M. Chernov",
                "S. Zin"
            ],
            "title": "Sources of entropy in representative agent models",
            "venue": "J. Finance 69, 51 ",
            "year": 2014
        },
        {
            "authors": [
                "A. Tozzi",
                "J.F. Peters"
            ],
            "title": "Information-devoid routes for scale-free neurodynamics",
            "venue": "Synthese 199, 2491 ",
            "year": 2020
        },
        {
            "authors": [
                "T. Ullmann",
                "C. Hennig",
                "A. Boulesteix"
            ],
            "title": "Validation of cluster analysis results on validation data: A systematic framework",
            "venue": "WIREs Data Min. & Knowl. 12, e1444 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Meil\u0103"
            ],
            "title": "Comparing clusterings\u2014an information based distance",
            "venue": "J. Multivar. Anal. 98, 873 ",
            "year": 2007
        },
        {
            "authors": [
                "T. Warren Liao"
            ],
            "title": "Clustering of time series data\u2014a survey",
            "venue": "Pattern Recognit. 38, 1857 ",
            "year": 2005
        },
        {
            "authors": [
                "A. Carbone",
                "G. Castelli",
                "H.E. Stanley"
            ],
            "title": "Analysis of clusters formed by the moving average of a long-range correlated time series",
            "venue": "Phys. Rev. E 69, 026105 ",
            "year": 2004
        },
        {
            "authors": [
                "A. Carbone",
                "H. Eugene Stanley"
            ],
            "title": "Scaling properties and entropy of long-range correlated time series",
            "venue": "Phys. A: Stat. Mech. Appl. 384, 21 ",
            "year": 2007
        },
        {
            "authors": [
                "A. Carbone"
            ],
            "title": "Information measure for long-range correlated sequences: The case of the 24 human chromosomes",
            "venue": "Sci. Rep. 3, 2721 (2013), doi:10.1038/srep02721. 10 SciPost Phys. 13, 076 ",
            "year": 2022
        },
        {
            "authors": [
                "L. Ponta",
                "P. Murialdo",
                "A. Carbone"
            ],
            "title": "Information measure for long-range correlated time series: Quantifying horizon dependence in financial markets",
            "venue": "Phys. A: Stat. Mech. Appl. 570, 125777 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Clauset",
                "C. Rohilla Shalizi",
                "M.E.J. Newman"
            ],
            "title": "Power-law distributions in empirical data",
            "venue": "SIAM Rev. 51, 661 ",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "We propose an information-theoretical measure, the relative cluster entropy DC[P\u2016Q], to discriminate among cluster partitions characterised by probability distribution functions P and Q. The measure is illustrated with the clusters generated by pairs of fractional Brownian motions with Hurst exponents H1 and H2 respectively. For subdiffusive, normal and superdiffusive sequences, the relative entropy sensibly depends on the difference between H1 and H2. By using the minimum relative entropy principle, cluster sequences characterized by different correlation degrees are distinguished and the optimal Hurst exponent is selected. As a case study, real-world cluster partitions of market price series are compared to those obtained from fully uncorrelated sequences (simple Browniam motions) assumed as a model. The minimum relative cluster entropy yields optimal Hurst exponents H1 = 0.55, H1 = 0.57, and H1 = 0.63 respectively for the prices of DJIA, S&P500, NASDAQ: a clear indication of non-markovianity. Finally, we derive the analytical expression of the relative cluster entropy and the outcomes are discussed for arbitrary pairs of power-laws probability distribution functions of continuous random variables.\nCopyright A. Carbone and L. Ponta. This work is licensed under the Creative Commons Attribution 4.0 International License. Published by the SciPost Foundation.\nReceived 09-06-2022 Accepted 18-08-2022 Published 30-09-2022\nCheck for updates\ndoi:10.21468/SciPostPhys.13.3.076\nContents"
        },
        {
            "heading": "1 Introduction 1",
            "text": ""
        },
        {
            "heading": "2 Methods and Results 3",
            "text": ""
        },
        {
            "heading": "3 Discussion and Conclusion 8",
            "text": "References 9"
        },
        {
            "heading": "1 Introduction",
            "text": "Flow of information in complex systems with interacting components can be quantified via entropy measures [1\u20137]. In this context, discriminating between empirical data and models in\nterms of information content is interesting from several viewpoints. Consider an experiment with the outcomes obeying the probability distribution P whereas the distribution Q is a model for the same experiment. Quantifying the error of the wrong assumption of the model compared to the empirical information content is relevant to a broad class of phenomena [8, 9]. Such information-theoretical concepts bring also together the thermodynamic implications intrisically related to the evolution of the system under investigation. The dynamic of the information transferred along subsequent transformative states of a complex system can be described in terms of divergence of the probability distributions P at time t and P \u2032 at a subsequent time t \u2032. Hence, information-theoretical tools find applications in fields as diverse as climate, turbulence, neurology, biology and economics [10\u201313] and are increasingly adopted in unsupervised learning of unlabelled data where similarity/dissimilarity measures are concerned with dynamic rather than static features of the clustered data [14\u201316].\nA recently proposed information measure, with the ability to quantify heterogeneity and dynamics of long-range correlated processes in a broad range of application areas, is the cluster entropy SC[P] [17\u201320]. The measure has been defined as a Shannon functional with P the power-law probability distribution of the clusters formed in a long-range correlated data sets. If P is a distribution concentrated on a single cluster value, SC[P] = 0 corresponds to the minimum uncertainty on the outcome of the cluster size, the random variable of interest. If P is a fully developed power-law distribution, SC[P] = ln N corresponds to the maximum uncertainty obtained as the power-law distribution spreads over a broad range of cluster values. Thus, according to the Shannon interpretation, SC[P] can be understood as a measure of uncertainty of all the possible cluster outcomes. By extending the definition to continuous variables, the differential cluster entropy SC[P] added clues to the approach by clarifying the interplay of the different terms entering the cluster entropy and thus the origin of the excess randomness.\nIn this work, we go beyond the simple measure of uncertainty of the random variable outcomes provided by SC[P]. An inference method for hypothesis testing of a general class of models underlying a relevant stochastic process is developed. We put forward the relative cluster entropy or cluster divergence DC[P\u2016Q] with the first argument P the empirical distribution and the second argument Q a model within a broad class of probability distributions. DC[P\u2016Q] is a metric on the space of probability distributions, interpreted as a divergence rather than as a distance since it does not obey symmetry and triangle inequality. The asymmetry of the relative entropy reflects the asymmetry between data and models, hence it can be used for inference purposes on the model underlying a given distribution. If DC[P\u2016Q] >> 0, the hypothesis likelihood is very low and, unless the quality of the empirical data should be questioned, the model distribution Q must be rejected. The higher DC[P\u2016Q], the lower the likelihood of the hypothesis. If the hypothesis on the model were true, P should fluctuate around its expected value Q, with fluctuations of limited amplitude and occurrence probability greater than the significance level, resulting in the acceptance of the model Q.\nThe cluster entropy SC[P] and the relative cluster entropy DC[P\u2016Q] can be interpreted as information measures over partitions generated by a coarse-grained mapping of the twodimensional phase-space spanned by a particle, e.g. a simple Brownian path described by the random variable {x t}. According to Gibbs\u2019 original idea at the core of the information entropy concept, a coarse grained description is defined by smoothing out fine details and increasing the observer\u2019s ignorance about the exact microstate of the system. As the structure description becomes blurrier, randomness and entropy increase. A coarse-grained description is obtained by performing a local average over the phase-space cells with increasing size. In the information clustering approach adopted here, the coarse grained description of the particle path {x t} is obtained by a local average {ex t,n} over the phase-space cells with the parameter n defining the cell sizes. The regression x t = ex t,n + \u03b5t,n yields the errors \u03b5t,n = x t \u2212 ex t,n which\nultimately generate a finite partition {C}= Cn,1, Cn,2, . . . , Cn, j for each n. The partition process generates regions, named as clusters, bounded between the values of t when \u03b5t,n = 0, which correspond to complete information with minimum entropy. The probability distribution functions of the random variables defined by \u03b5t,n univoquely quantify the loss of structure and information of the coarse grained representation. As already noted, the cluster entropy SC[P] is bounded, involves integrating over cell components, ranges from the minimum to the maximum value as the description ranges from the finest-grained (corresponding to the smallest clusters) to the coarsest-grained partition (corresponding to the largest clusters). The relative cluster entropy DC[P\u2016Q] is also bounded, involves integration over cells, ranges between a maximum value, depending on the two distributions, and the minimum value 0 for P =Q.\nThe ability of the cluster divergence DC[P\u2016Q] to select an optimal distribution could be relevant in several contexts. In particular, complex phenomena obeying power-law distributions are still raising concerns regarding accuracy and veracity of the estimation of the power law exponent [21]. To illustrate how the relative cluster entropy operates, synthetic and realworld data featuring power-law distribution behavior are considered. First, the approach is implemented on pairs of synthetic fractional Brownian motions (fBms) with given Hurst exponent. A systematic dependence of DC[P\u2016Q] on the Hurst exponents of the pair is found. The minimum relative entropy principle is then implemented as a selection criterion to extract the optimal correlation exponent of the sequence. Second, as a real-world case, we study the divergence DC[P\u2016Q] of financial price series. The probability distribution P is obtained by ranking the clusters generated in each price time series and compared to the distribution Q drawn from synthetic fBms data adopted as model. The minimum relative entropy principle yields the best estimate of the correlation exponents of the financial series and quantifies the deviation of the price series from the assumed model.\nThe manuscript is organized as follows. In Section 2 the main computational steps of the relative cluster entropy method are described for discrete variables. The approach is illustrated for synthetic (fractional Brownian motions) and real-world (market price series) data. In Section 3 the relative cluster entropy is extended to continuous random variables, conclusions and suggestions for further developments are drawn."
        },
        {
            "heading": "2 Methods and Results",
            "text": "In this section, the main steps of the relative cluster entropy approach are described. The interest is towards the development of a divergence measure able to evaluate the situation where a model probability distribution Q is defined in parallel to the true probability distribution function P of the cluster partition. Before illustrating how the proposed cluster divergence works, a few definitions are recalled.\nConsider the time series {x t} of length N and the local average ex t,n = 1 n \u2211n\u22121 n\u2032=0 x(t \u2212 n \u2032) of length N \u2212 n with n \u2208 (1, N). For each n, a partition {C} of non-overlapping clusters is generated between consecutive intersections of {x t} and {ex t,n} defined by the time instances which make the error \u03b5t,n = x t \u2212 ex t,n equal to zero. Hence, each cluster j is characterized by the random variable \u03c4 j \u2261 \u2016t j \u2212 t j\u22121\u2016, with the instances t j\u22121 and t j referring to subsequent intersection pairs. The random variable \u03c4 j is named as the cluster duration. The empirical distribution of the cluster duration frequencies P(\u03c4 j , n) can be obtained by ranking the number of clusters N (\u03c41, n),N (\u03c42, n), ...,N (\u03c4 j , n) according to their duration \u03c41,\u03c42, ...,\u03c4 j for each n as:\nP(\u03c4 j , n) = N (\u03c4 j , n) NC(n) , (1)\nwith NC(n) = \u2211k(n)\nj=1 N (\u03c4 j , n) the number of clusters generated by the partition for each n, k =\n\u2211N n=1 NC(n) the total number of clusters for all the possible values of n, and the normal-\nization condition holding as usual:\nN \u2211\nn=1\nNC (n) \u2211\nj=1\nP(\u03c4 j , n) = 1 . (2)\nThe cluster entropy is defined as:\nSC[P] = \u2212 \u2211\nj,n\nP(\u03c4 j , n) log P(\u03c4 j , n) , (3)\nwhich is obtained by introducing the cluster frequency P(\u03c4 j , n) in the Shannon functional.\nIn this work, the relative cluster entropy or cluster divergence DC[P\u2016Q] is proposed to quantify the wrong information yield when a model probability distribution Q is assumed in place of the empirical probability distribution P. A measure of distinguishability between two probability distributions P and Q is the Kullback-Leibler divergence, defined for discrete variables as DKL[P\u2016Q] = \u2211 j Pj log Pj/Q j\n, with the conditions supp(P) \u2286 supp(Q) and DKL[P\u2016Q] \u2265 0, with DKL[P\u2016Q] = 0 for P = Q . Then, the minimum relative entropy principle can be adopted as optimization criterion for model selection and statistical inference.\nThe quantity D j,n[P||Q] is defined for each macrostate in terms of the cluster durations \u03c4 j as follows:\nD j,n[P||Q] = P(\u03c4 j , n) log P(\u03c4 j , n)\nQ(\u03c4 j , n) , (4)\nwhere the index j refers to the set of clusters with duration \u03c4 j generated by the partition for a given n. The cluster frequencies P(\u03c4 j , n) and Q(\u03c4 j , n) satisfy the condition supp(P) \u2286 supp(Q). By using Eq. (4) and summing D j,n[P||Q] over all the accessible cell states , the relative cluster entropy is written as:\nDC[P||Q] = N \u2211\nn=1\nNC (n) \u2211\nj=1\nP(\u03c4 j , n) log P(\u03c4 j , n)\nQ(\u03c4 j , n) , (5)\nwhere the index j runs over the clusters obtained by each partition with size n, which in turn runs over the allowed set of time window values, n \u2208 (1, N).\nTo exemplify how the relative cluster entropy could be applied in practice, pairs of artificially generated fractional Brownian motions (fBms) are analysed in terms of the relative cluster entropy defined by Eqs. (4-5). Fractional Brownian motions (fBms) xHt with t \u00be 0 are powerlaw correlated stochastic processes, defined by a centered Gaussian process with stationary increments and covariance given by < xHs x H t >= 1 2 t2H + s2H \u2212 |t \u2212 s|2H\nwith H \u2208 (0, 1) the Hurst exponent. Power-law behaviour of the correlation function implies slow memory decay and non-Markovianity. Synthetic fBm sequences have been generated with assigned Hurst exponent H and length N by using the FRACLAB code [22]. The cluster frequencies P(\u03c4 j , n) and Q(\u03c4 j , n) have been estimated by counting the number of clusters with duration \u03c4 j and window n for each f Bm.\nFig. 1 shows a few examples of plots of the quantity D j,n, defined by Eq. (4). D j,n is estimated for cluster frequency P, obtained from clusters generated in f Bms with H1 varying from 0.20 (top-left) to 0.80 (bottom-right) with step 0.05, and model distribution Q obtained from uncorrelated Brownian paths, i.e. f Bms with H2 = 0.50. The values of the Hurst exponents correspond respectively to correlation exponents \u03b11 = 2\u2212H1 ranging from 1.80 to 1.20, whereas \u03b12 = 2\u2212 H2 is kept constant and equal to 1.50. The quantity D j,n shows characteristic deviations with respect to the null hypothesis corresponding to a fully random process with H2 = 0.5. In particular, at small values of the cluster duration \u03c4 j , the quantity D j,n takes positive and negative values respectively for fBms with 0.5< H1 < 1 and 0< H1 < 0.5. As the cluster duration \u03c4 j increases, D j,n tends to reach the horizontal axis implying that the divergence between the distributions become negligible for very large clusters. Note in particular the three panels of the middle row in Fig. 1 showing the results obtained for fractional Brownian motions with H1 = 0.45, H1 = 0.50 and H1 = 0.55 with respect to the simple Brownian path, i.e. the f Bm with H2 = 0.50, taken as the model. Thus, fBm pairs with close values of H1 and H2 correspond to more realistic experimental conditions. Inference problems with data sequences featuring correlation exponents statistically close to each other and small deviations from the model distribution should be reasonably expected in the cases of practical interest.\nTo further illustrate how the proposed method operates with real-world data, price series {pt} of Dow Jones Industrial Average (DJIA), Standard and Poor 500 (S&P500), National Association of Securities Dealers Automated Quotations Composite (NASDAQ), are considered. Data include tick-by-tick prices from January to December 2018. Details (Ticker; Extended name; Country; Currency; Members; Length) provided by Bloomberg [23]. Raw data prices {pt} have different lengths (NDIJA = 5749145, NS&P500 = 6142443, NNASDAQ = 6982017). To perform the relative cluster entropy analysis over comparable data sets, raw data prices are\nsampled to yield equally spaced data sequences with equal length N . The cluster frequency P(\u03c4 j , n) is estimated by counting the clusters generated in the market price series. Q(\u03c4 j , n) is estimated by counting the clusters generated in synthetic stochastic processes assumed as a model. In this analysis, the divergence between each price series, with unknown correlation exponent, and artificially generated samples of fractional Brownian motions fBms with assigned Hurst exponent H2, is considered. Results of the analysis are plotted in Fig. 2, showing the relative cluster entropy for the three markets. Several samples of the divergence obtained for different values of the parameter n, shown in Fig. 2, have been summed over the parameter n, with same interval of cluster duration \u03c4 j . Fig. 3 shows the relative cluster entropy DC[P||Q] for the data shown in Fig. 2.\nTo infer the optimal probability distribution P, the minimum relative entropy principle is implemented non-parametrically on the values plotted in Fig. 3. To this purpose, the variance \u03c32DC\nof DC[P||Q] around the value DC[Q||Q] (the null hypothesis for P = Q) is written as follows:\n\u03c32DC \u2261 1\nk\u2212 1\nk \u2211\nj=1\n[DC[P||Q]\u2212DC[Q||Q]]2 , (6)\nwhere the sum runs over the total number of clusters obtained by the partition process. By using the value DC[Q||Q] = 0, Eq. (6) writes:\n\u03c32DC = 1\nk\u2212 1\nk \u2211\nj=1\n[DC[P||Q]]2 . (7)\nThe quantity \u03c32DC corresponds to the mean square value of the area of the region between the curve DC[P||Q] and the horizontal axis (DC[Q||Q] = 0). Given the linearity of the relative cluster entropy operator, \u03c32DC exhibits a quadratic behaviour with the typical asymmetry of the Kullback-Leibler entropy. The quadratic functional can be easily used to estimate the\nminimum. The minimization criterion provided by Eq. (6) has been applied to the data shown in Fig. 3 to yield the best estimate of the correlation degree of the market prices. The value of the Hurst exponent for the series of the prices {pt} has been deduced from the value of H2 for which \u03c32DC takes its minimum, implying H1 = H2. By using this rule, H1 = H2 = 0.55, H1 = H2 = 0.57, and H1 = H2 = 0.63 have been found respectively for DJIA, S&P500 and NASDAQ. The minimization outcomes are plotted in Fig. 4 for the markets shown in Fig. 3."
        },
        {
            "heading": "3 Discussion and Conclusion",
            "text": "In this Section, the relative cluster entropy is extended to continuous random variables. For NC(n) \u2192 \u221e, the characteristic size of generated clusters C behaves as continuous random variables \u03c4 \u2208 [1,\u221e] with probability distribution function P(\u03c4) varying as a power-law [17, 18]. By taking the limits P(\u03c4 j) \u2192 P(\u03c4)d\u03c4 and Q(\u03c4 j) \u2192 Q(\u03c4)d\u03c4, Eq. (5) can be written for continuous random variables in the form of an integral:\nDC[P(\u03c4)||Q(\u03c4)] = \u222b P(\u03c4) log P (\u03c4) Q (\u03c4) d\u03c4 , (8)\nwith \u03c4 \u2208 [1,\u221e]. We are interested in the situations where the probability distributions are power-law functions, i.e. for P(\u03c4) and Q(\u03c4) respectively in the form:\nP(\u03c4) = (\u03b11 \u2212 1)\u03c4\u2212\u03b11 Q(\u03c4) = (\u03b12 \u2212 1)\u03c4\u2212\u03b12 , (9)\nwhere \u03b11 and \u03b12 are the correlation exponents, \u03b11 \u2212 1 and \u03b12 \u2212 1 are the normalization constants for \u03c4 \u2208 [1,\u221e]. By using Eqs. (9), Eq. (8) writes:\nDC[P(\u03c4)||Q(\u03c4)] = \u222b (\u03b11 \u2212 1)\u03c4\u2212\u03b11 log (\u03b11 \u2212 1)\u03c4\u2212\u03b11\n(\u03b12 \u2212 1)\u03c4\u2212\u03b12 d\u03c4 , (10)\nthat after integration becomes:\nDC[P(\u03c4)||Q(\u03c4)] = \u03c41\u2212\u03b11 log \u03b11 \u2212 1 \u03b12 \u2212 1 + log\u03c4(\u03b11\u2212\u03b12) + \u03b11 \u2212\u03b12 1\u2212\u03b11\n+ constant , (11)\nwhere the integration constant is equal to zero by setting DC[P||P] = 0. By estimating the definite integral over the interval [1,\u221e], one obtains:\nDC[P||Q] = log \u03b11 \u2212 1 \u03b12 \u2212 1 + \u03b11 \u2212\u03b12 1\u2212\u03b11 , (12)\nthat for \u03b11 = \u03b12 , i.e. for the distribution P coincident with the model distribution Q, provides DC[P||Q] = 0.\nDC[P||Q] quantifies the divergence between P(\u03c4) and Q(\u03c4), respectively true and model distribution, as a function of the cluster lifetime \u03c4 in terms of the pair of correlation exponents \u03b11 and \u03b12. Eq. (11) is plotted as a function of \u03c4 for different values of the exponents \u03b11 and \u03b12 in Fig. 5. At small values of the cluster duration (\u03c4\u2192 1), DC[P||Q] is strongly dependent on the difference of the power-law exponent \u03b11 with respect to the exponent \u03b12 of the model distribution. Conversely, as the cluster duration increases (\u03c4 >> 1), DC[P||Q] becomes negligible. The decay can be understood by considering that as \u03c4 increases the cluster becomes disordered as a consequence of the spread of the distribution and the onset of finite-size effect. The correlation vanishes as the process becomes almost fully uncorrelated. The behaviour of the cluster distribution divergence obtained by using continuous variables is consistent with the empirical tests performed on discrete data sets. In particular, the behaviour shown by the fractional Brownian motions with different correlation exponents discussed in the Section II is reproduced by the curves shown in Fig. 5, ensuring that the approach is sound and robust.\nThe relative cluster entropy can be therefore exploited to estimate the deviation of the power law exponent corresponding respectively to experimental and model probability distributions.\nLong-range correlated processes obeying power-law distributions occur frequently in complex system data related to several natural and man-made phenomena. Due to their ubiquity, the extent of long-range correlation and the scaling exponents are relevant to many disciplines, though several difficulties are met for their estimation which require suitable computational procedures to be carefully implemented [21]. A random variable x obeys a power law if it is drawn from a probability distribution p(x)\u221d x\u2212\u03b1 with \u03b1 > 1 the correlation exponent. Empirical real-world data barely follow a power-law for all the values of x . Due to normalization requirements and finite-size effects, ideal power-law behaviour usually holds at values greater than some minimum xmin up to a maximum xmax. An exponential cut-off is often artificially introduced to account for the deviation from the ideal power-law behaviour x\u2212\u03b1e\u2212\u03bbx .\nThe non-parametric minimization of the relative entropy has some advantages compared to the parametric approaches, whose implementation requires normality of the random variables and knowledge of the first two moments of the distribution for the calculation of the Lagrange multipliers. The proposed relative cluster entropy approach yields the optimal value of the correlation exponent \u03b1 without relying on the estimate of the slope in a log-log plot and thus is robust against computational biases which usually affect least-squares estimates."
        }
    ],
    "title": "Relative cluster entropy for power-law correlated sequences",
    "year": 2022
}