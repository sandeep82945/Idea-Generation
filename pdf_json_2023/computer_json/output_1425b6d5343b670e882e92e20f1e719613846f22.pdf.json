{
    "abstractText": "In recent years, learning-based approaches have revolutionized motion planning. The data generation process for these methods involves caching a large number of high quality paths for different queries (start, goal pairs) in various environments. Conventionally, a uniform random strategy is used for sampling these queries. However, this leads to inclusion of \u201ctrivial paths\u201d in the dataset (e.g.,, straight line paths in case of length-optimal planning), which can be solved efficiently if the planner has access to a steering function. This work proposes a \u201cnon-trivial\u201d query sampling procedure to add more complex paths in the dataset. Numerical experiments show that a higher success rate can be attained for neural planners trained on such a non-trivial dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sagar Suhas Joshi"
        },
        {
            "affiliations": [],
            "name": "Panagiotis Tsiotras"
        }
    ],
    "id": "SP:8e8c3243561f04eb3a83bd4b4a97803cf3384ebf",
    "references": [
        {
            "authors": [
                "P.E. Hart",
                "N.J. Nilsson",
                "B. Raphael"
            ],
            "title": "A formal basis for the heuristic determination of minimum cost paths",
            "venue": "IEEE Transactions on Systems Science and Cybernetics, vol. 4, no. 2, pp. 100\u2013107, 7 1968.",
            "year": 1968
        },
        {
            "authors": [
                "S. Koenig",
                "M. Likhachev",
                "D. Furcy"
            ],
            "title": "Lifelong planning astar",
            "venue": "Artificial Intelligence, vol. 155, no. 1-2, pp. 93\u2013146, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "S.M. LaValle",
                "J.J. Kuffner Jr"
            ],
            "title": "Randomized kinodynamic planning",
            "venue": "The International Journal of Robotics Research, vol. 20, no. 5, pp. 378\u2013400, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "S. Karaman",
                "E. Frazzoli"
            ],
            "title": "Sampling-based algorithms for optimal motion planning",
            "venue": "The International Journal of Robotics Research, vol. 30, no. 7, pp. 846\u2013894, June 2011.",
            "year": 2011
        },
        {
            "authors": [
                "O. Arslan",
                "P. Tsiotras"
            ],
            "title": "Use of relaxation methods in samplingbased algorithms for optimal motion planning",
            "venue": "IEEE International Conference on Robotics and Automation, Karlsruhe, Germany, May 6\u201310 2013, pp. 2421\u20132428.",
            "year": 2013
        },
        {
            "authors": [
                "J.D. Gammell",
                "S.S. Srinivasa",
                "T.D. Barfoot"
            ],
            "title": "Batch informed trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs",
            "venue": "IEEE International Conference on Robotics and Automation, Seattle, WA, May, 25\u201330 2015, pp. 3067\u20133074.",
            "year": 2015
        },
        {
            "authors": [
                "L. Janson",
                "E. Schmerling",
                "A. Clark",
                "M. Pavone"
            ],
            "title": "Fast marching tree: A fast marching sampling-based method for optimal motion planning in many dimensions",
            "venue": "The International Journal of Robotics Research, vol. 34, no. 7, pp. 883\u2013921, May 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Phillips",
                "N. Bedrossian",
                "L.E. Kavraki"
            ],
            "title": "Guided expansive spaces trees: a search strategy for motion-and cost-constrained state spaces",
            "venue": "IEEE International Conference on Robotics and Automation, New Orleans, LA, April 26\u201330 2004, pp. 3968\u20133973.",
            "year": 2004
        },
        {
            "authors": [
                "S.M. Persson",
                "I. Sharf"
            ],
            "title": "Sampling-based A* algorithm for robot path-planning",
            "venue": "The International Journal of Robotics Research, vol. 33, no. 13, pp. 1683\u20131708, 10 2014.",
            "year": 2014
        },
        {
            "authors": [
                "B. Akgun",
                "M. Stilman"
            ],
            "title": "Sampling heuristics for optimal motion planning in high dimensions",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, San Francisco, CA, Sept. 25\u201330 2011, pp. 2640\u20132645.",
            "year": 2011
        },
        {
            "authors": [
                "S. Rodriguez",
                "X. Tang",
                "J.-M. Lien",
                "N.M. Amato"
            ],
            "title": "An obstaclebased rapidly-exploring random tree",
            "venue": "IEEE International Conference on Robotics and Automation, Orlando, FL, May 15\u201319 2006, pp. 895\u2013900.",
            "year": 2006
        },
        {
            "authors": [
                "C. Urmson",
                "R. Simmons"
            ],
            "title": "Approaches for heuristically biasing RRT growth",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems., vol. 2, Las Vegas, NV, Oct. 27\u201331 2003, pp. 1178\u20131183.",
            "year": 2003
        },
        {
            "authors": [
                "J.D. Gammell",
                "T.D. Barfoot",
                "S.S. Srinivasa"
            ],
            "title": "Informed sampling for asymptotically optimal path planning",
            "venue": "IEEE Transactions on Robotics, vol. 34, no. 4, pp. 966\u2013984, Aug. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S.S. Joshi",
                "S. Hutchinson",
                "P. Tsiotras"
            ],
            "title": "Time-informed exploration for robot motion planning",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Mandalika",
                "R. Scalise",
                "B. Hou",
                "S. Choudhury",
                "S.S. Srinivasa"
            ],
            "title": "Guided incremental local densification for accelerated samplingbased motion planning",
            "venue": "arXiv preprint arXiv:2104.05037, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.S. Joshi",
                "P. Tsiotras"
            ],
            "title": "Relevant region exploration on general cost-maps for sampling-based motion planning",
            "venue": "International Conference on Intelligent Robots and Systems (IROS). Las Vegas, NV: IEEE/RSJ, Oct. 25\u201329 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Arslan",
                "P. Tsiotras"
            ],
            "title": "Dynamic programming guided exploration for sampling-based motion planning algorithms",
            "venue": "IEEE International Conference on Robotics and Automation, Seattle, WA, May 26\u201330 2015, pp. 4819\u20134826.",
            "year": 2015
        },
        {
            "authors": [
                "B. Chen",
                "B. Dai",
                "Q. Lin",
                "G. Ye",
                "H. Liu",
                "L. Song"
            ],
            "title": "Learning to plan in high dimensions via neural exploration-exploitation trees",
            "venue": "International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=rJgJDAVKvB",
            "year": 2020
        },
        {
            "authors": [
                "A.H. Qureshi",
                "Y. Miao",
                "A. Simeonov",
                "M.C. Yip"
            ],
            "title": "Motion planning networks: Bridging the gap between learning-based and classical motion planners",
            "venue": "IEEE Transactions on Robotics, vol. 37, no. 1, pp. 48\u201366, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Ichter",
                "J. Harrison",
                "M. Pavone"
            ],
            "title": "Learning sampling distributions for robot motion planning",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA). Brisbane, Australia: IEEE, May, 21\u201325 2018, pp. 7087\u20137094.",
            "year": 2018
        },
        {
            "authors": [
                "M. Zucker",
                "J. Kuffner",
                "J.A. Bagnell"
            ],
            "title": "Adaptive workspace biasing for sampling-based planners",
            "venue": "2008 IEEE International Conference on Robotics and Automation. IEEE, 2008, pp. 3757\u20133762.",
            "year": 2008
        },
        {
            "authors": [
                "M. Bhardwaj",
                "S. Choudhury",
                "S. Scherer"
            ],
            "title": "Learning heuristic search via imitation",
            "venue": "Conference on Robot Learning. PMLR, 2017, pp. 271\u2013280.",
            "year": 2017
        },
        {
            "authors": [
                "S. Choudhury",
                "M. Bhardwaj",
                "S. Arora",
                "A. Kapoor",
                "G. Ranade",
                "S. Scherer",
                "D. Dey"
            ],
            "title": "Data-driven planning via imitation learning",
            "venue": "The International Journal of Robotics Research, vol. 37, no. 13-14, pp. 1632\u20131672, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Yonetani",
                "T. Taniai",
                "M. Barekatain",
                "M. Nishimura",
                "A. Kanezaki"
            ],
            "title": "Path planning using neural a* search",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 12 029\u201312 039.",
            "year": 2021
        },
        {
            "authors": [
                "J. Huh",
                "G. Xing",
                "Z. Wang",
                "V. Isler",
                "D.D. Lee"
            ],
            "title": "Learning to generate cost-to-go functions for efficient motion planning",
            "venue": "arXiv preprint arXiv:2010.14597, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "C. Chamzas",
                "A. Shrivastava",
                "L.E. Kavraki"
            ],
            "title": "Using local experiences for global motion planning",
            "venue": "International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 8606\u20138612.",
            "year": 2019
        },
        {
            "authors": [
                "T. Lai",
                "P. Morere",
                "F. Ramos",
                "G. Francis"
            ],
            "title": "Bayesian local samplingbased planning",
            "venue": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1954\u20131961, 2020.",
            "year": 1954
        },
        {
            "authors": [
                "C. Zhang",
                "J. Huh",
                "D.D. Lee"
            ],
            "title": "Learning implicit sampling distributions for motion planning",
            "venue": "International Conference on Intelligent Robots and Systems (IROS). IEEE/RSJ, 2018, pp. 3654\u2013 3661.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-L. Kuo",
                "A. Barbu",
                "B. Katz"
            ],
            "title": "Deep sequential models for sampling-based planning",
            "venue": "International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 6490\u20136497.",
            "year": 2018
        },
        {
            "authors": [
                "B. Ichter",
                "E. Schmerling",
                "T.-W.E. Lee",
                "A. Faust"
            ],
            "title": "Learned critical probabilistic roadmaps for robotic motion planning",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 9535\u20139541.",
            "year": 2020
        },
        {
            "authors": [
                "R. Kumar",
                "A. Mandalika",
                "S. Choudhury",
                "S. Srinivasa"
            ],
            "title": "Lego: Leveraging experience in roadmap generation for sampling-based planning",
            "venue": "International Conference on Intelligent Robots and Systems (IROS), pp. 1488\u20131495, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Huh",
                "D.D. Lee",
                "V. Isler"
            ],
            "title": "Learning continuous cost-to-go functions for non-holonomic systems",
            "venue": "arXiv preprint arXiv:2103.11168, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.J. Bency",
                "A.H. Qureshi",
                "M.C. Yip"
            ],
            "title": "Neural path planning: Fixed time, near-optimal path generation via oracle imitation",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019, pp. 3965\u20133972.",
            "year": 2019
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627\u2013635.",
            "year": 2011
        },
        {
            "authors": [
                "D. Lopez-Paz",
                "M. Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems, vol. 30, pp. 6467\u20136476, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.J. Johnson",
                "L. Li",
                "F. Liu",
                "A.H. Qureshi",
                "M.C. Yip"
            ],
            "title": "Dynamically constrained motion planning networks for non-holonomic robots",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 6937\u20136943.",
            "year": 2020
        },
        {
            "authors": [
                "I.A. Sucan",
                "M. Moll",
                "L.E. Kavraki"
            ],
            "title": "The open motion planning library",
            "venue": "IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 72\u201382, Dec. 2012.",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION Motion planning, a core problem in artificial intelligence and robotics, is one of finding a collision free, low cost path connecting a start and goal state in a search-space. Popular discrete-space planners such as A* [1] and LPA* [2] conduct a prioritized search using heuristics and guarantee resolution-optimal paths. On the other hand, single-query sampling-based motion planning (SBMP) algorithms, such as RRT [3], solve this problem by constructing a connectivity graph online using a set of probing samples. The RRT algorithm is probabilistically complete, while it\u2019s asymptotically optimal variants such as RRT* [4], RRT# [5], BIT* [6], FMT* [7] converge to the optimal solution almost surely, as the number to samples tends to infinity. However, these algorithms may suffer from a slow convergence rate, especially in higher dimensional settings. In order to address this issue, several techniques that leverage heuristics and collision information have been suggested to improve the performance of these planners. These include [8], [9], [10], [11], [12] among others. The recently proposed Informed Set [13], [14], [15] and Relevant Region [16], [17] family of algorithms utilize current solution information and heuristics to focus the search onto a subset of the search-space, while still maintaining the theoretical guarantees of asymptotic optimality.\nAlthough the above methods can improve the performance of motion planning algorithms, they require handcrafted heuristics for efficacy. Also, these methods do not leverage prior experience or data gathered from expert demonstrations. Deep learning based approaches for motion planning address these two limitations by generating a dataset of high quality paths in various environments. In the offline phase, this dataset is used to train a deep neural network (DNN) model to predict quantities of interest, such as costto-go [18], next point along the optimal path [19], or a\n1,2 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, USA. Email: sagarsjoshi94@gmail.com\nsampling distribution [20]. The DNN model can then be used online to focus search and dramatically increase the efficiency of planning algorithms.\nThe data generation process for DNN-based motion planning methods involves sampling and solving a set of queries (start, goal pairs) in a given environment. For many applications, the map/environment in which the robot needs to operate may be fixed or given apriori. However, the query sampling distribution can be modified in order to extract more informative paths beneficial to the learning process. Conventionally, uniform random sampling is used to generate the start and goal states. Many queries in this uniformly sampled dataset can be solved by greedily connecting the start and goal state using a steering function (if available). Such a steering function provides the optimal path between any two states after relaxing the collision constraint. This work proposes adding more \u201cnon-trivial\u201d queries to the dataset, which cannot be solved by a simple greedy connection. The efficiency of the neural planners can be boosted by training deep models on this dataset comprising of relatively more complex paths. This is demonstrated by creating datasets with different degrees of non-triviality and benchmarking the performance of the neural planner on various robotic planning tasks."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Exciting progress has been made at the intersection of learning and motion planning in recent years. To improve the performance of discrete space planners, methods that leverage reinforcement learning [21] and imitation learning [22], [23] have been proposed. Neural A* [24] presents a data-driven approach that reformulates conventional A* as a differentiable module. Huh et al [25] present a higher-order function network that can predict cost-to-go values to guide the A* search. For sampling-based planning, techniques\nar X\niv :2\n30 3.\n06 73\n7v 1\n[ cs\n.R O\n] 1\n2 M\nar 2\n02 3\nAlgorithm 1: Neural Planner 1 NeuralPlanner ( xs, xg,Xobs): 2 \u03c0 \u2190 {xs}; 3 for i = 1 : Nplan do 4 if steerTo(\u03c0end, xg,Xobs) then 5 \u03c0 \u2190 \u03c0 \u222a {xg}; 6 break; 7 else 8 xnew \u2190 PNet(\u03c0end, xg); 9 \u03c0 \u2190 \u03c0 \u222a {xnew};\n10 if xg /\u2208 \u03c0 then 11 return \u2205 ; 12 if Feasible(\u03c0) then 13 return \u03c0; 14 else 15 return Replan(\u03c0);\nsuch as [26] and [27] learn a local sampling strategy for global planning. Zhang et al [28] present a deep learning based rejection sampling scheme for SBMP. Ichter et al [20] train a conditional variational autoencoder (CVAE) model to learn sampling distributions and generate samples along the solution paths. NEXT [18] learns a local sampling policy and cost-to-go function using \u201cmeta self-improving learning\u201d. Kuo et al [29] input the planner state and local environment information into a deep sequential model to guide search during planning. Approaches such as [30], [31] focus on identifying critical or bottleneck states in an environment to generate a sparse graph while planning. While the above techniques may differ in terms of their model architectures or outputs, they do not tackle the problem of improving the data-generation process via query sampling for increasing the efficacy of neural planners.\nIn [32], Huh et al extend their previous work to present a query sampling technique for non-holonomic robots. However, this technique is purely based on the dynamics of the robot, does not utilize obstacle information and is only applicable for car-like robots. The recently proposed OracleNet [33] and Motion Planning Networks (MPNet) algorithm [19] learn a deep model to recursively predict the next state along the solution path, given a query. The authors in [19] use \u201cActive Continual Learning (ACL)\u201d to improve the dataefficiency of the training process. Similar to the DAGGER algorithm [34] the ACL process involves training a MPNet deep model on a set of expert demonstrations for a Nc > 0 number of initial iterations. The ACL algorithm then finds the cases where the MPNet planner fails and invokes the expert planner only to solve them. The solutions generated by the expert planner are stored in a replay buffer to be used during training. ACL also leverages the \u201cGradient Episodic Memory (GEM)\u201d technique [35] to alleviate the problem of catastrophic forgetting during the learning process. While more data-efficient, the ACL process can be tricky to implement and computationally more expensive, as it involves interleaving the training process with running the neural\nplanner multiple times. The performance of ACL models can also be worse than that of batch-offline models in many cases [19]. In contrast, this work proposes a modified query sampling procedure for data-generation and trains all models in a batch-offline manner."
        },
        {
            "heading": "III. PROBLEM DEFINITION",
            "text": ""
        },
        {
            "heading": "A. Path Planning Problem",
            "text": "Let X \u2282 Rd denote the search-space for the planning problem with dimension d \u2265 2. Let Xobs \u2282 X denote the obstacle space and Xfree = X \\ Xobs denote the free space. Let c\u03c0(xs, xg) denote the cost of moving from a point xs \u2208 X to xg \u2208 X along a path \u03c0. This path \u03c0 can be represented as an ordered list of length L \u2265 2, \u03c0 = {\u03c01, \u03c02 . . . \u03c0L}. Let \u03c0end denote the last point on the path \u03c0. Then, the optimal path planning problem is one of finding the minimum cost, feasible path \u03c0\u2217 connecting a start xs and goal xg state.\n\u03c0\u2217 = argmin \u03c0\u2208\u03a0 c\u03c0(xs, xg),\nsubject to: \u03c01 = xs, \u03c0end = xg \u03c0i \u2208 Xfree, i = 1, 2 . . . L.\n(1)\nClassical planners discretize Xfree or build a connectivity graph and then perform a search over this graph to solve the above planning problem (1)."
        },
        {
            "heading": "B. Supervised Learning for Planning",
            "text": "Learning-based methods use the data gathered from successful plans to train models in the offline phase. In the online phase, this learned model can be used to solve (1) or assist the classical planners. The data generation process involves creating an environment and sampling a set of Ktrain > 0 queries or (start, goal) pairs in Xfree. Let Q = Xfree \u00d7 Xfree denote the \u201cquery-space\u201d. A classical planner is then used to solve the path planning problem (1) for each of the Ktrain queries and obtain a good quality solution. A quantity of interest to be learned as output (such as costto-go, next state on the optimal path etc) is extracted from these solution paths. The objective of the training process is to learn a function f\u03b8 (usually a deep neural network), on the dataset D by minimizing an empirical loss with respect to weight parameters \u03b8"
        },
        {
            "heading": "C. Neural Planner",
            "text": "The MPNet procedure involves learning a planning network PNet, that predicts the next state on the optimal path, given a current state, a goal state and environment information as the input. A typical neural planning algorithm, in line with the one described in [19], is illustrated in Algorithm 1. Given a query xs, xg, the path \u03c0 to be returned is initialized with the start-state. At each iteration, the neural planner attempts a greedy connection to the goal using the steerTo function. For length-optimal or geometric planning case, the steerTo connects any two points using a straight line. For car-like robots, this steering function can use the Dubins curves for dynamically feasible connections [36]. To probe the feasibility of this greedy connection, the steerTo\nAlgorithm 2: Data Generation Algorithm 1 D \u2190 \u2205; 2 X ,Xobs \u2190 createEnvironment(); 3 for j = 1 : Ktrain do 4 urand \u223c U [0, 1]; 5 if urand < pnt then 6 xs, xg \u2190 nonTrivialQuerySampling(Xfree); 7 else 8 xs, xg \u2190 uniformSampling(Xfree); 9 \u03c0 \u2190 solveQuery(xs, xg,Xobs);\n10 D \u2190 includeData(D, \u03c0); 11 return D\nprocedure discretizes the path and collision-checks the points on it. If the greedy connection \u03c0end to xg is valid, the goal-state is appended to the path and the planning loop terminates. Else, the learned planning network PNet, is used to predict the next state on the optimal path. If the path \u03c0 is infeasible, a neural replanning procedure is performed on this coarse path in an attempt to repair it. Please see [19] for more details about these procedures."
        },
        {
            "heading": "IV. NON-TRIVIAL QUERY SAMPLING",
            "text": ""
        },
        {
            "heading": "A. Non-trivial Queries",
            "text": "Conventionally, uniform random sampling is used in the data generation process to obtain a query xs, xg \u2208 Q. However, this may result in the inclusion of \u201ctrivial\u201d queries in the dataset, for which steerTo(xs, xg) = True. In case of such trivial queries, the neural planning Algorithm 1 terminates in the first iteration after processing lines 4-6. Thus, a key observation is that steerTo procedure in Algorithm 1, line 4 performs an implicit classification of queries, so that only \u201cnon-trivial\u201d queries are passed over to the PNet. Concretely, the set of non-trivial queries can be defined as\nQnt = {xs, xg \u2208 Q | steerTo(xs, xg) = False}. (2)\nThis motivates the proposed data generation Algorithm 2, which aims to increase the number of non-trivial data samples in D. After an environment X ,Xobs is created, data is generated by solving a total of Ktrain queries. With probability pnt, the proposed nonTrivialQuerySampling procedure is used to obtain xs, xg \u2208 Qnt. Else, conventional uniformSampling returns a query in Q. A classical planner such as A* or BIT* then solves this query and outputs a good quality solution path \u03c0. Samples from this path \u03c0 are appended to the dataset D with the proposed data inclusion procedure includeData."
        },
        {
            "heading": "B. Non-trivial Query Sampling",
            "text": "A rejection sampling algorithm to generate new queries in Qnt is given in Algorithm 3. For Nnt number of attempts, uniform sampling is used to first generate a valid query xs, xg \u2208 Q. The steerTo module then validates the connection between start and goal state. If found invalid, the corresponding non-trivial query is returned. Thus, this procedure intends to filter out trivial paths while maintaining the\nAlgorithm 3: Non-trivial Query Sampling 1 nonTrivialQuerySampling ( Xobs): 2 for i = 1 : Nmax do 3 (xs, xg)\u2190 uniformSampling(Xfree); 4 valid\u2190 steerTo(xs, xg); 5 if not valid then 6 return (xs, xg);\n7 return (xs, xg);\nAlgorithm 4: Data Inclusion Procedure 1 includeData ( D, \u03c0): 2 for i = 1 : L\u2212 1 do 3 if pruneData then 4 valid\u2190 steerTo(\u03c0i, \u03c0end); 5 if valid then 6 continue;\n7 D \u2190 D \u222a {(\u03c0i, \u03c0end), \u03c0i+1}\n8 return D;\nexploratory/coverage property of uniform sampling. Please see Fig. 2 and Fig. 4 for a visualization of queries generated using the proposed non-trivial sampling procedure."
        },
        {
            "heading": "C. Data Inclusion",
            "text": "The data inclusion Algorithm 4 iterates over the segments of path \u03c0 and logs the current state, goal state (\u03c0i, \u03c0end) as the input and the next state (\u03c0i+1) as the output/label. However, if the flag pruneData = True, the algorithm skips including the data-sample {(\u03c0i, \u03c0end), \u03c0i+1} if \u03c0i, \u03c0end 6\u2208 Qnt. Thus, pruneData = True ensures that only the nontrival segments of \u03c0 are incorporated in D. Please see Fig. 1 for an illustration of this step.\nDepending on the topology of Xobs, the neural planner may find it relatively harder to predict feasible paths in certain environments. The notion of non-trivial queries can be used to define a metric that captures this level of difficulty. Consider a \u201cnon-triviality ratio\u201d, which can be defined as,\n\u03b3nt = # Non-trivial queries\n# Uniformly sampled queries . (3)\nThus, \u03b3nt is the ratio of number of non-trivial queries found in a (large enough) set of uniformly sampled queries. This ratio will be high for complex, cluttered and narrow-passage type environments and low for relatively simpler, singleobstacle type environments."
        },
        {
            "heading": "V. NUMERICAL EXPERIMENTS",
            "text": "In order to benchmark the proposed data generation algorithm, the following procedure was implemented for each planning environment. First, four datasets with different parameter settings were created. These were as follows: D0 (pnt = 0, pruneData = False), D1 (pnt = 0.5, pruneData = False), D2 (pnt = 1.0, pruneData =\nFalse), D3 (pnt = 1.0, pruneData = True). Thus, D0 represents the dataset generated using the conventional uniform query sampling, whereas D3 is created using the proposed non-trivial query sampling and data pruning procedure. Four deep models, PNet0,PNet1,PNet2,PNet3 were then trained on their respective datasets. The neural network shape, size and the training parameters were held constant while learning all four models. Performance of the neural planner 1 using these four models was evaluated on 1) Ktest number of new uniform queries and 2) Ktest number of new nontrivial queries. Two performance metrics, namely, success ratio and cost ratio were considered. Success ratio gives the number of times (out of Ktest in total) the neural planner was successful in finding a feasible (collision-free) solution. Cost ratio denotes the ratio of the neural planner\u2019s solution cost to that of classical planner, averaged over Ktest trials. Model training and evaluation was performed using the Python PyTorch API on a 64 bit, 16 GB RAM laptop with Intel i7 processor and a NVIDIA GeForce RTX 2060 GPU. A description of robotic planning tasks along with a discussion of results is given below.\nPoint Robot: Four different 20\u00d720 environments, illustrated in Fig. 2, were considered for the case of point robot planning. Four datasets {Di}3i=0, as described above, were generated for each environment. A total of Ktrain = 3000 number of queries were sampled for each dataset. An A* planner, followed by post-processing/smoothening, was used to solve these queries and obtain length-optimal paths. A small padding of 0.8 units around the obstacles was propagated during the data generation step, and was relaxed during the final performance evaluation step. This was found to greatly boost the success ratio of the neural planner, while making slight compromise in the cost ratio metric. The performance metrics were logged by solving Ktest = 500 number of unseen uniform and non-trivial queries with the\nfour learned PNet models. Rigid Body Planning: Fig. 3 shows the instance of planning for a rigid robot in four 10 \u00d7 10 environments. A total of Ktrain = 5000 queries were considered to create each of the four datasets {Di}3i=0. All the queries were solved in the SE(2) space using OMPL\u2019s [37] implementation of the BIT* planner. An obstacle-padding of 0.4 units was propagated during the data-generation phase. The learned PNet models predicted a three dimensional [x, y, \u03b8] vector representing the robot\u2019s pose. These models were evaluated on Ktest = 500 unseen uniform and non-trivial queries. The BIT* planner was allowed a run-time of 3 seconds during the data-generation phase and 2 seconds during the evaluation phase. n-link Manipulator Planning: To observe performance of the neural planner in higher dimensions, a planning problem for 2, 4 and 6-link manipulator robot was considered. Please see Fig. 4. The joint angles were constrained to lie between \u2212\u03c0 and \u03c0. Four datasets {Di}3i=0 were created for each of the 2, 4 and 6-link case by considering a total of 3000, 4000 and 5000 queries respectively. These queries were solved using OMPL\u2019s BIT* planner with an padding of 0.8 units around the workspace obstacles. The final performance evaluation was done by solving Ktest = 500 new queries with the neural planner. The BIT* planner was run for 2, 4, 6 seconds during the data-generation stage and 1, 2, 4 seconds during the evaluation stage for the 2, 4 and 6-link planning respectively.\nGeneral trends seen from the results in Table I, II, IV are as follows. In most cases, PNet2 and PNet3 outperform PNet0 in terms of success ratio, where as the performance of PNet1 is more sporadic. The cost ratio is relatively lower for the rigid body and 6-link case compared to others, as the BIT* planner may not find a good quality solution in the given planning time for these challenging cases. The success ratio for all PNet models is naturally higher over uniform test queries rather than non-trivial test queries. The success ratio also generally has an inverse relation with \u03b3nt, as seen\nstrongly in the case of rigid body and n-link manipulator planning. For the case of point robot planning, all models perform well with a success rate of over 90% (see Table I). However, slight performance gains due to the proposed method can be seen for Environments 0, 2, 3. These gains are much more noticeable for the rigid body planning case (see Table II). The PNet3 model has the highest success ratio in all cases except one (Environment 3, Non-trivial Query), where its performance is comparable to PNet1. The gradation in performance due to dimensionality and \u03b3nt can be seen clearly in the n-link planning case (see Table IV). The success ratio over uniform queries is in the range of 0.9, 0.8 and 0.7 for the case of 2, 4 and 6-link planning\ncase respectively. For the relatively simpler 2-link planning case with \u03b3nt = 0.225, only small gains in the success ratio over non-trivial queries can be seen. However, the improvement in performance is much more evident for the higher dimensional 4 and 6-link cases. The PNet3 model shows about a 25% increase in the success ratio over PNet0 for the 6-link (non-trivial queries) case.\nThe neural planning Algorithm 1 and the corresponding results discussed above assume the availability of a steering function. While this is readily available for cases such as geometric or non-holonomic (car-like) planning [36], it may not be computationally tractable for others. To analyze the performance of PNet models without the steerTo function,\nsimulations were performed by only executing the lines 8 and 9 of the neural planner 1 for maximum Nplan iterations. Instead of lines 4-6 in Algorithm 1, the following termination condition was implemented, \u2016\u03c0end\u2212 xg\u20162 \u2264 \u03b4, with a small \u03b4 > 0. As illustrated in Fig. 5, the PNet3 model, which has no trivial sample in its training dataset, naturally cannot solve a trivial query. Numerical results for the rigid body planning without the steerTo function and \u03b4 = 1.0 are tabulated in Table III. The success ratio of all PNet models is adversely affected in this case. The PNet0 model performs best on uniform queries in all environments, whereas the performance of PNet3 is the worst. However, PNet1 or PNet2 show better performance over non-trivial queries in some cases. Thus, without a steering function, a uniformly sampled training dataset might be the best choice if the test queries are uniformly distributed too. However, a model trained over a dataset with an appropriate value of pnt may perform better over non-trivial test queries. This makes a case for an ensemble model."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "Many of the previous techniques in the literature have focused on exploring different deep architectures for planning, while using a uniformly sampled dataset for training. This work, on the other hand, investigates the problem of improving the data-generation process while holding the model architecture and planning algorithm constant. The proposed query sampling and data pruning procedures add more complicated paths in the dataset. Numerical experi-\nments show that the success rate of the neural planner can be boosted using the deep models trained on such non-trivial datasets.\nThis work presents many opportunities for future research. An ensemble model can be constructed by combining predictions from different models trained on datasets with varying degrees of non-triviality. Instead of a Boolean pruneData flag, calling the pruning procedure with a probability of \u03b3nt can be explored. This can prevent excessive pruning and result in a drastic reduction in the size of the dataset for relatively less cluttered environments. Acknowledgements: Authors would like to thank Prof. Le Song, Binghong Chen and Ethan Wang for insightful discussions on this topic."
        }
    ],
    "title": "Non-Trivial Query Sampling For Efficient Learning To Plan",
    "year": 2023
}