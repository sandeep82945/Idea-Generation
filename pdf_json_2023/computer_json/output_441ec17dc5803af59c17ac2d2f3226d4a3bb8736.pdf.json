{
    "abstractText": "Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an ecommerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly repurposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler approach called copy-paste staging using retrieval assisted GANs. In copy paste staging, we first retrieve (from the catalog) staged products similar to the un-staged input product, and then copy-paste the background of the retrieved product in the input image. A GAN based in-painting model is used to fill the holes left after this copy-paste operation. We show the efficacy of our copypaste staging method via offline metrics, and human evaluation. In addition, we show how our staging approach can enable animations of moving products leading to a video ad from a product image. ACM Reference Format: Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, and Paloma de Juan. 2023. Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation. In Proceedings of (AdKDD 2023). ACM, New York, NY, USA, 6 pages. https://doi.org/XXXXX",
    "authors": [
        {
            "affiliations": [],
            "name": "Yueh-Ning Ku"
        },
        {
            "affiliations": [],
            "name": "Mikhail Kuznetsov"
        },
        {
            "affiliations": [],
            "name": "Shaunak Mishra"
        },
        {
            "affiliations": [],
            "name": "Paloma de Juan"
        }
    ],
    "id": "SP:ffb287c34c65f548d787c62b39f1a8428b7ea01b",
    "references": [
        {
            "authors": [
                "n. d"
            ],
            "title": "New Study by Verizon Media, Magna, & IPG Media Lab Finds Interactive Ad Formats Engage Hard-to-Convince Audiences",
            "venue": "https://www.verizonmedia",
            "year": 2021
        },
        {
            "authors": [
                "Narayan Bhamidipati",
                "Ravi Kant",
                "Shaunak Mishra"
            ],
            "title": "A Large Scale Prediction Engine for App Install Clicks and Conversions",
            "venue": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM",
            "year": 2017
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets. Advances in neural information processing systems",
            "year": 2014
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Zaeem Hussain",
                "Mingda Zhang",
                "Xiaozhong Zhang",
                "Keren Ye",
                "Christopher Thomas",
                "Zuha Agha",
                "Nathan Ong",
                "Adriana Kovashka"
            ],
            "title": "Automatic Understanding of Image and Video Advertisements",
            "year": 2017
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-toimage translation with conditional adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2017
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784",
            "year": 2014
        },
        {
            "authors": [
                "Shaunak Mishra",
                "Mikhail Kuznetsov",
                "Gaurav Srivastava",
                "Maxim Sviridenko"
            ],
            "title": "VisualTextRank: Unsupervised Graph-Based Content Extraction for Automating Ad Text to Image Search (KDD \u201921)",
            "year": 2021
        },
        {
            "authors": [
                "Shaunak Mishra",
                "Manisha Verma",
                "Jelena Gligorijevic"
            ],
            "title": "Guiding Creative Design in Online Advertising",
            "venue": "In Proceedings of the 13th ACM Conference on Recommender Systems",
            "year": 2019
        },
        {
            "authors": [
                "Shaunak Mishra",
                "Manisha Verma",
                "Yichao Zhou",
                "Kapil Thadani",
                "Wei Wang"
            ],
            "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad Creative Refinement",
            "venue": "In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM",
            "year": 2020
        },
        {
            "authors": [
                "Kamyar Nazeri",
                "Eric Ng",
                "Tony Joseph",
                "Faisal Qureshi",
                "Mehran Ebrahimi"
            ],
            "title": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning",
            "year": 2019
        },
        {
            "authors": [
                "Xuebin Qin",
                "Zichen Zhang",
                "Chenyang Huang",
                "Masood Dehghan",
                "Osmar R Zaiane",
                "Martin Jagersand"
            ],
            "title": "U2-Net: Going deeper with nested U-structure for salient object detection",
            "venue": "Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever. [n. d"
            ],
            "title": "Zero-Shot Text-to-Image Generation (ICML 2021)",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computer-assisted intervention. Springer,",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In IEEE CVPR",
            "year": 2016
        },
        {
            "authors": [
                "Manisha Verma",
                "Shaunak Mishra"
            ],
            "title": "Recommendation Systems for Ad Creation: A View from the Trenches (RecSys \u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Tengfei Wang",
                "Hao Ouyang",
                "Qifeng Chen"
            ],
            "title": "Image Inpainting with External-internal Learning and Monochromic Bottleneck",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In IEEE CVPR",
            "year": 2018
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zhe Lin",
                "Jimei Yang",
                "Xiaohui Shen",
                "Xin Lu",
                "Thomas S Huang"
            ],
            "title": "Free-form image inpainting with gated convolution",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2019
        },
        {
            "authors": [
                "Yichao Zhou",
                "Shaunak Mishra",
                "Jelena Gligorijevic",
                "Tarun Bhatia",
                "Narayan Bhamidipati"
            ],
            "title": "Understanding Consumer Journey Using Attention Based Recurrent Neural Networks",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD",
            "year": 2019
        },
        {
            "authors": [
                "Yichao Zhou",
                "Shaunak Mishra",
                "Manisha Verma",
                "Narayan Bhamidipati",
                "Wei Wang"
            ],
            "title": "Recommending Themes for Ad Creative Design via Visual-Linguistic Representations",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ACM Reference Format: Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, and Paloma de Juan. 2023. Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation. In Proceedings of (AdKDD 2023). ACM, New York, NY, USA, 6 pages. https://doi.org/XXXXX"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The choice of image for an online ad can have a significant impact on the online user exposed to the ad. If the ad image is enticing enough, it can not only create brand awareness among online users but also\n*Work done while at Yahoo Research. \u2020Work done while at Yahoo Research. \u2021Work done while at Yahoo Research.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. AdKDD 2023, Long Beach, CA \u00a9 2023 Association for Computing Machinery. https://doi.org/XXXXX\ndrive them to click the ad and make subsequent purchases (conversions) [2, 20]. However, if the ad image is not properly designed to capture the user\u2019s attention, it would lead to poor user interactions and adversely affect the advertising platform (by lowering revenue) and the advertiser (by lowering conversion rate). In this context, a common observation [10] is that ad images with products in a natural or real world setting (lifestyle images) tend to have better online performance. For example, an ad selling a chair is expected to perform better if the image shows a chair in a living room versus a chair against a solid (synthetic) background (as shown in Figure 1). However, such staging of products may be expensive and time consuming, specially when a vendor is selling multiple products at the same time. In DPA offerings from ad platforms (e.g., Yahoo), the catalog images from an e-commerce vendor (e.g., Walmart, Amazon) are typically used directly as ad images. As described later in our data analysis (based on data from an ad platform), a major fraction of such images are not staged, and hence there is a scope to enhance such images (e.g., by generating a suitable background for the product).\nImage generation has been an actively studied topic for the past few years. Diffusion models [13] and GANs [3, 6] have been powering the state-of-the-art results in this area. Image in-painting [11] is a slightly easier version of the problem where only parts of the image need to be generated as opposed to the whole image. To the best of our knowledge, we are the first to study GAN based image generation approaches for enhancing product images to serve as ad images. Our main contributions can be summarized as follows.\n(1) We study three tasks (as outlined below): (i) vanilla staging, (ii) copy-paste staging, and (iii) image-to-parallax animation.\nar X\niv :2\n30 7.\n15 32\n6v 1\n[ cs\n.C V\n] 2\n8 Ju\nl 2 02\n3\nAdKDD 2023, Long Beach, CA Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, and Paloma de Juan\n(2) In task 1, we aim to generate the entire background for a product. We use pix2pix [6] to train a GAN model with pairs of images (input: segmented out product image, output: staged product image with ground truth background). (3) In task 2, our goal is to retrieve a similar product image (with staging) and copy-paste the background while filling in gaps (holes) created in the process of swapping products. We leverage GAN-based in-painting to fill in the gaps mentioned above. We also introduce a weighted boundary loss for inpainting to focus on the image generation quality at product boundaries. Through Frechet inception distance (FID) score, and human evaluations we show that copy-paste staging is significantly better than the vanilla staging baseline. (4) In task 3, we use GAN-based in-painting to create a sequence of images simulating the main product\u2019s movement against the staged background as in a parallax animation. The foreground and background both move, but at different speeds, creating the illusion of depth. This is to show how our approach can lead to video ads from product images.\nOur retrieval based approach (second task above) shares the intuition common in text generation: retrieval augmented generation (RAG) has better context understanding and generation quality. The remainder of this paper is organized as follows: related work in Section 2, problem formulation in Section 3, relevant data in Section 4, and our proposed approaches in Section 5. We go over our experimental results in Section 6, and end with a discussion in Section 7."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Online advertising: In online advertising, the ad creative (text and image) plays an important role in influencing online users towards brand awareness, clicks and purchases [8, 16, 20]. Studying ad images and text using state-of-the-art deep learning models in computer vision and natural language processing (NLP) is an emerging area of research. In [5], ad image content was studied using computer vision models, and their dataset had manual annotations for: ad category, reasons to buy products advertised in the ad, and expected user response given the ad. Using this dataset, [9, 21] used ranking models to recommend themes for ad creative design using a brand\u2019s Wikipedia page. In [10], object tag recommendations for improving an ad image was studied using data from A/B tests. Although related to ads, the above methods are not applicable in our setup since none of them are image generation methods.\nGANs for image generation. Generative Adversarial Networks (GANs) are a popular approach for image generation. While vanilla GANs [3] generate images from random noise, Conditional GANs [7] also allow to use extra information in a generative process. Recently developed pix2pix method [6] and its successors [18] use Conditional GANs for image-to-image generation optimizing GAN objective together with distance to a target image. While we focus on GANs in this paper, our copy-paste staging approach can be generalized to more recent diffusion models [13] (discussed in Section 7).\nSaliency detection. Saliency detection in product images is needed to understand which parts of an image correspond to the main product being advertised (as opposed to the background). Salient object detection (SOD) aims to detect the most visually attractive objects\nFigure 2: Tasks 1 and 2 explained with examples. In task 2, we need to retrieve a related product image with staging and then copy paste its background onto the input image, whereas in task 1 the goal is to generate the entire background.\nwith precise boundaries in images (i.e., it returns a boundary map which can be used to segment out objects from the image). With the introduction of convolutional neural networks (CNNs) in computer vision, SOD accuracy has witnessed remarkable improvements. Recently, U2-Net [12] achieved state-of-the-art results for saliency detection by using a nested version of U-Net [14]-like architecture to capture richer local and global information. In our proposed approaches, we use saliency detection via U2-Net as a building block."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "We study three tasks in this paper as outlined below.\nTask 1: vanilla staging. In task 1, the input is a product image (without staging), and the desired output is a product image with a model generated relevant background (stage). The model should generate the entire background as shown in Figure 2.\nTask 2: retrieval assisted copy-paste staging. Task 2 is a simpler version of task 1. Here, we are given a pool of existing product images P, and we need to retrieve a similar product image with staging such that we can copy-paste the staged background from the retrieved image onto the input image as shown in Figure 2. Image generation (in-painting) is needed to fill in the gaps after copy-pasting (since the input product and the product in the retrieved image are not identical, gaps will be created when we swap products).\nTask 3: image-to-parallax animation. In this task, the goal is to take an input image (as shown in Figure 2 for task 2), and create an animation (i.e., sequence of images), where the object in the input image (as in Figure 2) appears to be moving against a stationary but staged background. Such animations are expected to lead to higher user engagement [1]."
        },
        {
            "heading": "4 DATA",
            "text": "For our experiments, we sampled data from Yahoo Gemini DPA (spanning November-December 2020). With an impression threshold of 10, 000, we collected a sample of \u223c 18, 899 ads (each corresponding to a product), from which 11, 927 had solid backgrounds, and the rest 6, 972 had staged backgrounds. Each product in the sample\nStaging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation AdKDD 2023, Long Beach, CA\nbelongs to a certain hierarchical category, e.g. \"Kids > Kids Nightstands\", \"Furniture > Bedroom > Headboards > Queen\", \"Shoe > Heel > Oxford Heel\". For our experiments, we considered only furniture images with staged backgrounds, which left us with 2071 images. Top-frequent subcategories with instance counts for the \u201cFurniture\" category (as in our sample) are shown in Figure 3."
        },
        {
            "heading": "5 PRODUCT STAGING USING GANS",
            "text": "In this section, we first explain the salient object detection model which is common to our proposed approaches for all the tasks outlined in Section 3. Next, we cover our proposed approaches for tasks 1, 2, and 3 (tasks explained in Section 3) in Sections 5.2, 5.3, and 5.4 respectively. Our major modeling contributions are in the approaches for tasks 2 and 3; for task 1, although we define the task, we use existing approaches (pix2pix [6]) to solve it. Task 2 is an easier version of task 1, and our proposed approach leads to more realistic staged product images compared to pix2pix for task 1."
        },
        {
            "heading": "5.1 Product segmentation via saliency",
            "text": "For tasks 1, 2 and 3, we use U2-Net [12] as our saliency object detector. The saliency object detector plays a crucial role in the first step of our approaches since it separates the main product(s) that will be replaced or copy-pasted versus the background in a product image. Once saliency probability maps are obtained from U2-Net, we set the threshold at 0.5 to generate binary masks and separate foreground pixels from the background pixels. For each task, we use product segmentation in a different manner as explained below."
        },
        {
            "heading": "5.2 Vanilla staging",
            "text": "For vanilla staging, we use the pix2pix method to generate an image background for a product which is segmented by a saliency mask. The algorithm is a conditional GAN optimizing the loss combining 1) regular GAN objective, and 2) \u21131 distance between original and restored images. We use product segmentation to prepare pairs of images to train pix2pix for stage (background generation). In particular, given an image with a staged product, we remove the background via product segmentation and use this as the input image to pix2pix. Figure 4 shows an example for this approach (original image in the middle, segmented product on the left, and the version with generated staging on the right)."
        },
        {
            "heading": "5.3 Retrieval assisted copy-paste staging",
            "text": "For copy paste staging, we bypass the problem of generating the entire image background by using backgrounds from other relevant images. Our method consists of the following steps:\n(1) For a given segmented product (Figure 1, left), retrieve top-\ud835\udc58 similar products from a training collection (Fig. 6a). Similarity measure is a cosine distance between embeddings of corresponding product images provided by Inception-V3 [15]. (2) For the top-\ud835\udc58 similar images, segment out the original products (Fig. 6b) and fill in the holes by inpainting using EdgeConnect [11] (GAN based model) and a new loss function that we introduce in Section 5.3.1. (3) Copy-paste the original product image to the inpainted top\ud835\udc58 similar images, aligning shape and center mass for the corresponding product masks (Fig. 6c).\nThe above algorithm is illustrated with examples in Figure 5. We provide additional examples in Figure 6. After completing steps 1-3, we generate \ud835\udc58 product images with various backgrounds, only small parts of which (holes around the product before/after) are generated by GANs, which makes the images look more real if comparing with vanilla staging. For better background generation we introduce a new loss function as described below.\n5.3.1 Weighted Boundary Loss. Recent works [11] and [17] explore coarse-to-fine inpainting approaches, since the structures\nAdKDD 2023, Long Beach, CA Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, and Paloma de Juan\nof objects are complex and diverse, adding an intermediate step, like edge maps or monochromic images, can help models to learn progressively and eventually generate better final inpainted outputs. We propose a weighted boundary loss (WBL) to not only simplify the learning process (since the model needs to focus on lesser area), but also mimic the end application use case. Following prior work [11], our total generator loss consists of a conventional adversarial loss \ud835\udc3f\ud835\udc34\ud835\udc37\ud835\udc49 and a feature-matching loss \ud835\udc3f\ud835\udc39\ud835\udc40 . In addition to these two losses, since our goal is to make the model learn better at the boundary of the masked area, we add weighted boundary loss \ud835\udc3f\ud835\udc4a\ud835\udc35\ud835\udc3f to amplify the loss penalty at the boundary area pixels. WBL is:\n\ud835\udc3f\ud835\udc4a\ud835\udc35\ud835\udc3f =\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc5d \u2217 \ud835\udc3f\u21131\u2212\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a (\ud835\udc38\ud835\udc3a\ud835\udc47 , \ud835\udc38\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 ), (1)\nwhere \ud835\udc38\ud835\udc3a\ud835\udc47 is ground truth edge map of input images, \ud835\udc38\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 is predicted edge map generated by the generator. The\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc5d is a pixelwise weighted map and has the same size as input masked images and ground truth. To be more specific, the\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc5d has \ud835\udf06\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 for pixels around the boundary between masked area and unmasked area, and \ud835\udf06\ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 for pixels away from the boundary, the pixel-wise \ud835\udc3f\ud835\udc591\u2212\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a will multiply the corresponding \ud835\udf06 as we calculate \ud835\udc3f\ud835\udc4a\ud835\udc35\ud835\udc3f . As Figure 7 illustrates, for each training sample, we create free-form dense masks by the method proposed by [19]. Then, we find the boundary area of the free-form mask and assign \ud835\udf06\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 (white area in Figure 7c) and \ud835\udf06\ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 (gray area in Figure 7c). For experiments, we fixed \ud835\udf06\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 = 0.9 and \ud835\udf06\ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 = 0.1.\n(a) (b) (c)\nFigure 7: Illustration of weighted boundary loss. 7a: Groundtruth training data. 7b: Free-form masked training data. 7c: Weighted mask.\n(a) (b) (c)\nFigure 8: Illustration of image parallax effect. 8a: original image 8b: image after shifting all the salient pixels. 8c: gap in-painted."
        },
        {
            "heading": "5.4 Image-to-animation",
            "text": "Parallax effect happens when the background pixels move slower than foreground objects in an animation, thereby creating an illusion of depth in a two-dimensional image. Generally, parallax effect requires independent foreground images and background images, and proper technique to make transparent backgrounds. In our proposed approach, by leveraging the power of salient object detection and in-painting, a parallax effect animation can be generated from a 2D image. Practically, we run salient object detection to define foreground pixels, then gradually move the foreground object around creating empty gaps between the current position of the object and original position. To fill the empty gap, we then use image in-painting model to in-paint those pixels and create serial realistic images. We illustrate the sample results of the above approach in Figure 8."
        },
        {
            "heading": "6 RESULTS",
            "text": "We first go over some sample results for tasks 1-3 followed by offline metrics (retrieval performance, generation quality) and human perceptual study results."
        },
        {
            "heading": "6.1 Sample results for tasks 1, 2 and 3",
            "text": "Task 1 (vanilla staging): sample results for task 1 (obtained via pix2pix) are shown in Figures 9 (b) and 10 (b). Compared to the original image with the background, the generated image has a lot of artifacts, and does not look so realistic. As we discuss below, the copy-paste staging results look more realistic.\nTask 2 (copy-paste staging): Figures 9 (e) and 10 (e) show sample results for task 2 using the proposed copy-paste staging approach.\nStaging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation AdKDD 2023, Long Beach, CA\n(a) Original image from DPA catalogs (left) and the input product image for staging (right).\nOverall, the copy-paste staging results look much more realistic compared to pix2pix results.\nTask 3 (parallax animation). A sample for the generated image-toparallax animation: https://www.dropbox.com/s/9at5gz24ukhf2gi/ product_staging_image_to_parallax_demo.mp4?dl=0.\n(a) Original image from DPA catalogs (left) and the input product image for staging (right).\nAdKDD 2023, Long Beach, CA Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, and Paloma de Juan"
        },
        {
            "heading": "6.2 Offline metrics",
            "text": "Similar product image retrieval performance: a retrieved image is considered similar, if it belongs to the same subcategory as the input product. For example, if the input is a queen bed of subcategory \"Furniture > Bedroom > Headboards > Queen\", the retrieved product should fall into the same subcategory. Table 1 shows precision-recall results for similar products retrieval. For evaluation, we considered furniture subcategories (42 in total) with number of images \u2265 20.\nGeneration quality: we measure the performance of our copypaste staging results by evaluating Frechet inception distance (FID) [4]. FID is a popular metric for evaluating the quality of images created by GANs. The Wasserstein-2 distance in FID is calculated by comparing the features distribution of in-painted images with the distribution of real images, where the features are generated by a pre-trained InceptionV3 model. The comparison results are shown in Table 2. Since the copy-paste method in-paints only small regions of image around an object, it achieves much better FID score than vanilla staging. WBL further improves FID score in both methods."
        },
        {
            "heading": "6.3 Human evaluation",
            "text": "For a human perceptual study, we performed pairwise comparison tests, where experts were given two images at once, and the task was to determine which image appears more realistic (natural). For each comparison, we used three independent expert judgements, and decided the winner by majority voting. We performed three such tests: 1) 100 comparisons of vanilla staging images vs. ground truth images; 2) 100 copy-paste staging (with WBL) vs. ground truth; 3) 100 copy-paste (with WBL) vs. vanilla staging. The study showed:\n\u2022 0% of pix2pix images were better than ground truth; \u2022 3% of copy-paste images were better than ground truth; \u2022 76% of copy-paste images were better than pix2pix.\nThe above results clearly demonstrate the superiority of copy-paste staging and are in line with offline FID scores."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "Our proposed approach provides low budget advertisers a way to stage products digitally without having to spend on the physical resources needed for staging. Staging a room can easily cost up to few hundred dollars for an advertiser, and with image generation methods like the ones we have proposed, this would be basically free of cost (except for the legalities around copying backgrounds\nfrom other images). Leveraging the recent progress in prompt based image generation models, our approach can be further improved along the following lines: backgrounds from similar images could be used to generate prompts which then generate the background of the original product image. In addition, staged ads and parallax animations are expected to drive user engagement, and validating such hypothesis via an A/B test is one of our next steps."
        }
    ],
    "title": "Staging E-Commerce Products for Online Advertising  using Retrieval Assisted Image Generation",
    "year": 2023
}