{
    "abstractText": "The performance of modern digital cameras approaches physical limits and enables highprecision measurements in optical metrology and in computer vision. All camera-assisted geometrical measurements are fundamentally limited by the quality of camera calibration. Unfortunately, this procedure is often effectively considered a nuisance: calibration data are collected in a non-systematic way and lack quality specifications; imaging models are selected in an ad hoc fashion without proper justification; and calibration results are evaluated, interpreted, and reported inconsistently. We outline an (arguably more) systematic and metrologically sound approach to calibrating cameras and characterizing the calibration outcomes that is inspired by typical machine learning workflows and practical requirements of camera-based measurements. Combining standard calibration tools and the technique of active targets with phase-shifted cosine patterns, we demonstrate that the imaging geometry of a typical industrial camera can be characterized with sub-mm uncertainty up to distances of a few meters even with simple parametric models, while the quality of data and resulting parameters can be known and controlled at all stages.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexey Pak"
        },
        {
            "affiliations": [],
            "name": "Steffen Reichel"
        },
        {
            "affiliations": [],
            "name": "Jan Burke"
        }
    ],
    "id": "SP:58eef524b498f5557eb5bcdb87d66cf7913f7755",
    "references": [
        {
            "authors": [
                "E. Betzig",
                "G.H. Patterson",
                "R. Sougrat",
                "O.W. Lindwasser",
                "S. Olenych",
                "J.S. Bonifacino",
                "M.W. Davidson",
                "J. Lippincott-Schwartz",
                "H.F. Hess"
            ],
            "title": "Imaging Intracellular Fluorescent Proteins at Nanometer Resolution",
            "venue": "Science",
            "year": 2006
        },
        {
            "authors": [
                "Z. Zhang"
            ],
            "title": "A Flexible New Technique for Camera Calibration",
            "venue": "Pattern Anal. Mach. Intell. IEEE Trans",
            "year": 2000
        },
        {
            "authors": [
                "G. Bradski"
            ],
            "title": "The OpenCV Library",
            "year": 2000
        },
        {
            "authors": [
                "R. Hartley",
                "A. Zisserman"
            ],
            "title": "Multiple View Geometry in Computer Vision, 2nd ed.; Cambrige",
            "year": 2004
        },
        {
            "authors": [
                "T. Hanning"
            ],
            "title": "High Precision Camera Calibration: Habilitation Thesis; Springer Fachmedien",
            "venue": "Vieweg+Teubner Verlag: Wiesbaden, Germany,",
            "year": 2011
        },
        {
            "authors": [
                "H. Schilling",
                "M. Diebold",
                "M. Gutsche",
                "B. J\u00e4hne"
            ],
            "title": "On the design of a fractal calibration pattern for improved camera calibration. Tm-Tech",
            "year": 2017
        },
        {
            "authors": [
                "T. Sch\u00f6ps",
                "V. Larsson",
                "M. Pollefeys",
                "T. Sattler"
            ],
            "title": "Why Having 10,000 Parameters in Your Camera Model is Better Than Twelve",
            "venue": "Proc. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "M. Hannemose",
                "J. Wilm",
                "J.R. Frisvad"
            ],
            "title": "Superaccurate camera calibration via inverse rendering",
            "venue": "In Proceedings of the SPIE 11057, Modeling Aspects in Optical Metrology VII, Munich, Germany,",
            "year": 2019
        },
        {
            "authors": [
                "L. Huang",
                "Q. Zhang",
                "A. Asundi"
            ],
            "title": "Camera calibration with active phase target: Improvement on feature detection and optimization",
            "year": 2013
        },
        {
            "authors": [
                "C. Schmalz",
                "F. Forster",
                "E. Angelopoulou"
            ],
            "title": "Camera calibration: Active versus passive targets",
            "venue": "Opt. Eng. 2011,",
            "year": 2011
        },
        {
            "authors": [
                "M. Fischer",
                "M. Petz",
                "R. Tutsch"
            ],
            "title": "Vorhersage des Phasenrauschens in optischen Messsystemen mit strukturierter Beleuchtung",
            "venue": "Tm-Tech. Mess",
            "year": 2012
        },
        {
            "authors": [
                "M.D. Grossberg",
                "S.K. Nayar"
            ],
            "title": "A general imaging model and a method for finding its parameters",
            "venue": "In Proceedings of the 8th IEEE International Conference on Computer Vision (ICCV),",
            "year": 2001
        },
        {
            "authors": [
                "S. Ramalingam",
                "P.F. Sturm",
                "S.K. Lodha"
            ],
            "title": "Towards Complete Generic Camera Calibration",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), San Diego, CA, USA,",
            "year": 2005
        },
        {
            "authors": [
                "T. Bothe",
                "A. Gesierich",
                "W. Li",
                "M. Schulte"
            ],
            "title": "Verfahren und Vorrichtung zur Kalibrierung einer optischen Einrichtung",
            "venue": "German Patent DE102005061931A1,",
            "year": 2007
        },
        {
            "authors": [
                "T. Bothe",
                "W. Li",
                "M. Schulte",
                "C. von Kopylow",
                "R.B. Bergmann"
            ],
            "title": "Jueptner, W.P.O. Vision ray calibration for the quantitative geometric description of general imaging and projection optics in metrology",
            "venue": "Appl. Opt",
            "year": 2010
        },
        {
            "authors": [
                "T. Reh",
                "W. Li",
                "J. Burke",
                "R.B. Bergmann"
            ],
            "title": "Improving the generic camera calibration technique by an extended model of calibration display",
            "venue": "J. Europ. Opt. Soc. Rap. Public",
            "year": 2014
        },
        {
            "authors": [
                "H. Dierke",
                "M. Petz",
                "R. Tutsch"
            ],
            "title": "Photogrammetric determination of the refractive properties of liquid crystal displays",
            "venue": "In Forum Bildverarbeitung",
            "year": 2018
        },
        {
            "authors": [
                "J. Salvi",
                "X. Armangue",
                "J. Batlle"
            ],
            "title": "A comparative review of camera calibrating methods with accuracy evaluation",
            "venue": "Pattern Recognit",
            "year": 2002
        },
        {
            "authors": [
                "O. Bogdan",
                "V. Eckstein",
                "F. Rameau",
                "J.C. Bazin"
            ],
            "title": "DeepCalib: A deep learning approach for automatic intrinsic calibration of wide field-of-view cameras",
            "venue": "In Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production (CVMP\u201918),",
            "year": 2018
        },
        {
            "authors": [
                "H. Brunken",
                "C. G\u00fchmann"
            ],
            "title": "Deep learning self-calibration from planes",
            "venue": "In Proceedings of the 12th International Conference on Machine Vision (ICMV\u201919), Amsterdam, The Netherlands,",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "B. Luo",
                "Z. Xiang",
                "Q. Zhang",
                "Y. Wang",
                "X. Su",
                "J. Liu",
                "L. Li",
                "W. Wang"
            ],
            "title": "Deep-learning-based adaptive camera calibration for various defocusing degrees",
            "year": 2021
        },
        {
            "authors": [
                "J. Kannala",
                "S. Brandt"
            ],
            "title": "A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2006
        },
        {
            "authors": [
                "V. Popescu",
                "J. Dauble",
                "C. Mei",
                "E. Sacks"
            ],
            "title": "An Efficient Error-Bounded General Camera Model. In Proceedings of the 3DPVT\u201906: Proceedings of the Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT\u201906)",
            "venue": "Chapel Hill, NC, USA,",
            "year": 2006
        },
        {
            "authors": [
                "A.K. Dunne",
                "J. Mallon",
                "P.F. Whelan"
            ],
            "title": "Efficient generic calibration method for general cameras with single centre of projection",
            "venue": "Comput. Vis. Image Underst",
            "year": 2010
        },
        {
            "authors": [
                "Q. Sun",
                "Y. Hou",
                "Q. Tan"
            ],
            "title": "A new method of camera calibration based on the segmentation model",
            "venue": "Optik",
            "year": 2013
        },
        {
            "authors": [
                "P. Miraldo",
                "H. Araujo"
            ],
            "title": "Calibration of smooth camera models",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2013
        },
        {
            "authors": [
                "Z. Xiang",
                "X. Dai",
                "X. Gong"
            ],
            "title": "Noncentral catadioptric camera calibration using a generalized unified model",
            "venue": "Opt. Lett",
            "year": 2013
        },
        {
            "authors": [
                "A. Pak"
            ],
            "title": "The concept of smooth generic camera calibration for optical metrology",
            "venue": "Tm-Tech. Mess",
            "year": 2015
        },
        {
            "authors": [
                "OpenCV Documentation"
            ],
            "title": "Camera Calibration and 3D Reconstruction Section",
            "venue": "2021. Available online: https://docs.opencv.org/4. 5.2/d9/d0c/group__calib3d.html",
            "year": 2022
        },
        {
            "authors": [
                "J. Nocedal",
                "S.J. Wright"
            ],
            "title": "Numerical Optimization; Springer Series in Operations Research",
            "venue": "Sensors 2022,",
            "year": 1999
        },
        {
            "authors": [
                "T. Hastie",
                "R. Tibshirani",
                "J. Friedman"
            ],
            "title": "The Elements of Statistical Learning\u2014Data Mining, Inference, and Prediction",
            "year": 2017
        },
        {
            "authors": [
                "S.B. Werling",
                "M. Mai",
                "M. Heizmann",
                "J. Beyerer"
            ],
            "title": "Inspection of specular and partially specular surfaces",
            "venue": "Metrol. Meas. Syst. 2009,",
            "year": 2009
        },
        {
            "authors": [
                "E. Jones",
                "T. Oliphant",
                "P. SciPy Peterson"
            ],
            "title": "Open Source Scientific Tools for Python",
            "venue": "2022. Available online: http://www.scipy.org",
            "year": 2022
        },
        {
            "authors": [
                "D. Maclaurin",
                "D. Duvenaud",
                "M. Johnson",
                "J. Townsend"
            ],
            "title": "AutoGrad Package for Python. 2022. Available online: https://github.com/ HIPS/autograd (accessed on 1 August 2022)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Citation: Pak, A.; Reichel, S.; Burke, J.\nMachine-Learning-Inspired\nWorkflow for Camera Calibration.\nSensors 2022, 22, 6804. https://\ndoi.org/10.3390/s22186804\nAcademic Editor: Alessandro\nArtusi\nReceived: 2 August 2022\nAccepted: 31 August 2022\nPublished: 8 September 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional affil-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: camera calibration; machine learning; active targets; phase shifting; 3D localization"
        },
        {
            "heading": "1. Introduction",
            "text": "Digital cameras are relatively inexpensive, fast, and precise instruments capable of measuring 2D and 3D shapes and distances. Equipped with O(106 \u2212 108) pixels, each sensing O(102 \u2212 103) intensity levels in several color channels, and a perspective lens commanding a field of view of O(101 \u2212 102) degrees, a typical digital camera may technically resolve features as small as 10\u22123 to 10\u22124 rad. Moreover, by exploiting the prior knowledge of the scene, one may transcend the physical sensor resolution limits and locate extended objects or prominent features to within O(10\u22121 \u2212 10\u22122) pixels. This impressive sensitivity powers metrological methods, such as laser triangulation and deflectometry, as well as various shape-from-X techniques in the closely related field of computer vision. In some state-of-the-art camera-based measurements, the residual uncertainty reaches tens of nanometers, even with non-coherent illumination [1]. Depending on the employed optical scheme, translation of images to metric statements may be a non-trivial task. In order to take full advantage of data, one needs a sufficiently accurate mathematical model of the imaging process and its uncertainties as well as an adequate calibration procedure to fix all relevant parameters based on dedicated measurements (calibration data). In metrology, once an instrument is calibrated, one naturally should quantify the following properties of the model:\n(a) Consistency: How well does the calibrated model agree with the calibration data? (b) Reproducibility: Which outcomes should be expected if the calibration were repeated with data collected independently under similar conditions? (c) Reliability: Which uncertainties should one expect from a measurement made in a\ngiven application-specific setup that uses the calibrated camera model?\nSensors 2022, 22, 6804. https://doi.org/10.3390/s22186804 https://www.mdpi.com/journal/sensors\nIn Bayesian terms, calibration aims to estimate the posterior probability distribution function (PDF) over the model parameters given evidence (calibration data) and prior knowledge (e.g., physical bounds on parameter values). In case such a posterior PDF can be formulated, it may be analyzed using entropy, Kullback\u2013Leibler divergence, and other information-theoretical tools in order to optimally address the points (a\u2013c) above. However, finding true closed-form PDFs for complex systems with multiple parametric inter-dependencies such as a camera may be difficult and/or impractical. Nevertheless, Bayesian methods may be used to analyze, e.g., Gaussian approximations to true PDFs. A complementary approach adopted in the field of Machine Learning and advocated for in this paper treats camera models as trainable black or gray boxes whose adequacy to the actual devices can be estimated statistically using sufficiently large datasets, interpreted as samples drawn from implicit stationary PDFs. The above quality characteristics (a\u2013c) then may be naturally related to empirical metrics such as loss function values on training/testing/validation datasets, dispersion of outcomes in cross-validation tests, various generalizability tests, etc. In what follows, we provide a simple example of how such techniques may deliver more useful descriptions than the commonly accepted practices while remaining relatively inexpensive to implement. The wider acceptance of similar methods may therefore lead to better utilization of hardware and benefit numerous existing applications. Ultimately, unambiguous and reproducible quality indicators for geometrical camera calibration may even facilitate the introduction of new industrial standards similar to those that exist for photometric calibration [2]. Throughout this paper, we assume the most typical use-case: a compact rigid imaging unit\u2014a camera consisting of optics and a sensor\u2014is placed at various positions in space and acquires data at rest with respect to the observed scene. Light propagates in a transparent homogeneous medium (air, vacuum, or non-turbulent fluid). For simplicity, we exclude \u201cdegenerate\u201d imaging geometries with non-trivial caustics of view rays (e.g., telecentric or complex catadioptric systems): such devices usually serve specific purposes, and there exist dedicated methods and separate bodies of literature devoted to their calibration. We further only consider the constellation parameters consistent with geometrical optics, i.e., diffraction and interference effects are assumed negligible. The internal state of the camera is supposed to be fixed throughout all measurements. Finally, we assume that the calibration is based on data collected in a dedicated session with a controlled scene. The end user/application then is ultimately interested in the geometry of uncontrolled scenes recorded under similar conditions, e.g., sizes and shapes of objects and/or positions and orientations of the camera itself. The purpose of this paper is to provide some background and motivate and experimentally illustrate a practical ML-inspired calibration approach intended for applications in precision optical metrology and advanced computer vision tasks. This procedure and the quality assurance tools it is based on represent our main contribution. In order to keep the focus on the workflow rather than technical details, we deliberately employ a very simple experimental setup (described in Appendix B) and use the standard calibration algorithm by Zhang [3] implemented in the popular OpenCV library [4]. The structure of this paper is as follows. Section 2 briefly mentions a few camera models and calibration techniques often used in practice and outlines typical quality characterization methods. Section 3 introduces the generic notation for camera models and Section 4\u2014for calibration data. After that, Section 5 discusses the quantification of model-to-data discrepancies and schematically explains how calibration algorithms work. Section 6 provides some basic methods of the quantitative calibration quality assessment; Section 7 adjusts these recipes accounting for the specifics of camera calibration requirements. Our proposed workflow is then summarized in Section 8 and subsequently illustrated with an experiment in Section 9. It is thus Sections 7 and 8 that contain novel concepts and algorithms. We discuss the implications of our approach in Section 10 and conclude in Section 11."
        },
        {
            "heading": "2. Typical Calibration Methods, Data Types, and Quality Indicators",
            "text": "The basics of projective geometry may be found, e.g., in [5]. The common theory and methods of high-precision camera calibration are thoroughly discussed in [6], including the pinhole model that virtually all advanced techniques use as a starting point. In what follows, we many times mention the popular Zhang algorithm [3] that minimizes re-projection errors (RPEs) in image space and estimates the parameters of a pinhole model extended by a few lower-order polynomial distortion terms. In its canonical version, the Zhang algorithm needs a sparse set of point-like features with known relative 3D positions that can also be reliably identified in the 2D images. In practice, one often uses cell corners in a flat checkerboard pattern printed on a rigid flat surface. More sophisticated calibration patterns may include fractals [7] or complex star-shaped features [8] in order to reduce biases and enable more robust feature detection. Advanced detection algorithms may be too complex to allow an easy estimation of the residual localization errors. In order to reduce the influence of these (unknown) errors, one may employ sophisticated procedures such as the minimization of discrepancies between the recorded and inversely rendered pattern images [9]. However, all these improvements leave the sparse nature of datasets intact."
        },
        {
            "heading": "2.1. Calibration with Dense Datasets Produced by Active Target Techniques",
            "text": "As a fundamentally better type of input data, calibration may employ active target techniques (ATTs) [10,11] that use high-resolution flat screens as targets. The hardware setup here is slightly more complex than that with static patterns: a flat screen displays a sequence of special coded patterns (10 s to 100 s of patterns depending on the model and the expected quality) while the camera synchronously records frames. From these images, one then independently \u201cdecodes\u201d screen coordinates corresponding to each modulated camera pixel. Note that the decoding does not rely on pattern recognition and that the resulting datasets (3D to 2D point correspondences) are dense. Furthermore, from the same data, one may easily estimate decoding uncertainties in each pixel [12]. For generic non-central projection, Grossberg and Nayar [13] propose a general framework where a camera induces an abstract mapping between the image space and the space of 3D view rays; this is the base of many advanced camera models. As a rule, the calibration of such models needs significantly better quality data than the Zhang algorithm, and ATTs have for a long time been a method of choice, in particular, for the calibration of non-parametric models in optical metrology [10,14\u201317]. However, even for simpler models, ATTs are long known to outperform the static pattern-based techniques [11]. For the best accuracy, one may relatively easily include corrections for the refraction in the cover glass of the display [18] and other minor effects [17]. In our experiments, ATTs routinely deliver uncorrelated positional decoding uncertainties of order 0.1 mm (less than half of the typical screen pixel size), while the camera-to-screen distances and screen sizes are of order 1 m. We claim that there are no reasons to not use ATTs as a method of choice also in computer vision (except, perhaps, some exotic cases where this is technically impossible). Even if an inherently sparse algorithm such as Zhang\u2019s cannot use full decoded datasets without sub-sampling, one may easily select more abundant and better quality feature points from dense maps at each pose than a static pattern would allow. In practice, subsampled ATT data considerably improve the stability of results and reduce biases."
        },
        {
            "heading": "2.2. Quality Assessment of Calibration Outcomes",
            "text": "Regardless of the nature of calibration data, in most papers, the eventual quality assessment utilizes the same datasets as the calibration. This approach addresses the consistency of the outcomes but not their reproducibility and reliability. Very rarely, the calibration is validated with dedicated scenes and objects of known geometry [19]. As an alternative example, consider a prominent recent work [8] from the field of computer vision. Its authors propose novel static calibration patterns along with a dedi-\ncated detection scheme and calibrate several multi-parametric models capable of fitting higher-frequency features. The calibration quality is evaluated (and the models are compared) based on dedicated testing datasets, i.e., data not used during the calibration. This is a serious improvement over the usual practice. However, following the established tradition, the authors report the results in terms of RPEs on the sensor measured in pixels. Without knowing (in this case, tens of thousands) the calibrated camera parameters, it is impossible to translate these maps and their aggregate values into actual metric statements about the 3D geometry of the camera view rays. As we demonstrate below, such translation may be quite helpful; the comparison of model-related to data-related uncertainties is a powerful instrument of quality control. Finally, in the absence of dense datasets, the authors of [8] have to interpolate between the available data points in order to visualize artifacts in dense error maps; this step may potentially introduce complex correlations and biases into integral (compounded) quality indicators. Dense datasets generated by an ATT could have been useful to resolve these issues. Even better, estimates of uncertainties in decoded 3D point positions\u2014a by-product in advanced ATTs\u2014could potentially enable an even more metrologically sound characterization of residual model-to-data discrepancies."
        },
        {
            "heading": "2.3. Note on Deep-Learning-Based Camera Calibration Methods",
            "text": "\u201cTraditional\u201d calibration algorithms such as Zhang\u2019s rely on the explicit numerical optimization of some loss function (cf. Section 5.2). As an alternative, some recent works demonstrate that multi-layer neural networks can be trained to infer some camera parameters using, e.g., images of unknown scenes or otherwise inferior-quality data [20\u201322]. While we acknowledge these developments, we point out that so far they mostly address issues that are irrelevant in the context of this paper. Nevertheless, if at some point a \u201cblack-box\u201d solution for ML-based high-quality calibration appears, it may also be trivially integrated into and benefit from our proposed workflow of Section 7."
        },
        {
            "heading": "3. Camera Models and Parameters",
            "text": "The approach presented in this paper is model-agnostic and can be easily applied to arbitrarily flexible discrete and smooth generic parameterizations [6,8,14,23\u201329]. However, in our experiments and illustrations, we employ the venerable Zhang model, and more specifically, its implementation in the open-source library OpenCV [4,30] (version 4.2.0 as of writing). We also adopt the OpenCV-style notation for the model parameters. The simplest part of a camera model is its pose, or embedding in the 3D world, which includes the 3D position and the orientation of the camera. At a given pose, the transformation between the world and the camera\u2019s coordinate systems is described by six extrinsic parameters: a 3D translation vector~t and three rotation angles that we represent as a 3D vector ~u. The relevant coordinate systems are sketched in Figure 1. According to this picture, a 3D point P with the world coordinates ~p W = ( xW , yW , zW\n)T has coordinates ~p C = ( xC, yC, zC\n)T in the camera\u2019s frame that are related by ~p C = R(~u) ~p W +~t. (1)\nThe 3 \u00d7 3 rotation matrix R(~u) may be parameterized in many ways that may be more or less convenient at a given scenario; the recipe adopted in OpenCV is shown in Equation (A1). The direct projection model ~\u03c0 = ~\u03a0(~p C | ~\u03b8) then describes the mapping of 3D point coordinates ~p C onto 2D pixel coordinates ~\u03c0 = ( xI , yI\n)T of the projection of point P on the sensor. The number of intrinsic parameters ~\u03b8 may be as low as four in the pinhole model in Figure 1 or reach thousands in high-quality metrological models. The basic model ~\u03a0(CV)(\u00b7|\u00b7) implemented in OpenCV has 18 parameters as described in Equation (A2).\nIn addition to a direct mapping, it is often useful to also introduce an inverse projection model~r = ~R(~\u03c0 | ~\u03b8) with the same parameterization that returns a direction vector~r for a view ray that corresponds to the pixel ~\u03c0. By definition, the two projection functions must satisfy ~\u03c0 = ~\u03a0(\u03b1~R(~\u03c0 | ~\u03b8) | ~\u03b8) for any pixel ~\u03c0 and any scaling factor \u03b1 > 0. Although no closed-form inversion is known for the OpenCV model, Equation (A3) presents a practical way to define the respective inverse projection ~R(CV)(\u00b7|\u00b7). Note that the choice of the direct or the inverse projection function to define a camera model for non-degenerate optical schemes is arbitrary and is mostly a matter of convenience. For example, the OpenCV model is often employed for rendering where a succinct formulation of ~\u03a0(\u00b7|\u00b7) is a plus, but many advanced metrological models are formulated in terms of the inverse mapping ~R(\u00b7|\u00b7). (For non-central projection schemes, the inverse model should also define view ray origins~o = ~O(~\u03c0 | ~\u03b8) in addition to their directions~r at each pixel, but this is a relatively straightforward extension [29].) Generally, powerful and highly parallel modern hardware in practice eliminates any real difference between the two formulations for any camera model. Each component of ~\u03b8 in the OpenCV model has a well-defined physical meaning (cf. the description in Appendix A): the model of Equation (A2) was clearly constructed to be \u201cinterpretable\u201d. A direct relation between intrinsic parameters and simple lens properties is obviously useful when we, e.g., design an optical scheme to solve a given inspection task. However, when our goal is to fit the imaging geometry of a real camera to an increasingly higher accuracy, we necessarily need more and more parameters to describe its \u201chigh-frequency features\u201d [8]\u2014minor local deviations from an ideal smooth scheme. At some point, the convenient behavior of the model in numerical optimization becomes more important than its superficial interpretability. Alternative\u2014less transparent, or even \u201cblack-box\u201d\u2014models then may end up being more practical. We may control the \u201cflexibility\u201d of the OpenCV model by fixing some intrinsic parameters to their default values. For illustration purposes, we devise two \u201ctoy models\u201d as follows. The simpler \u201cmodel A\u201d uses only fx, fy, cx, cy, k1, k2, k3, p1, and p2 (cf. Equation (A2)), while keeping the remaining nine parameters fixed to zeros. The full model with all 18 parameters enabled is referred to as \u201cmodel B\u201d. In our tests, the latter has proven to be less stable: in optimization, it often becomes caught in local minima and generally converges more slowly than the \u201cmodel A\u201d, even with high-quality calibration data."
        },
        {
            "heading": "4. Calibration Data Acquisition and Pre-Processing",
            "text": "Cameras are typically calibrated based on a collection of points in 3D and their respective sensor images. Let us denote a pair (~p W , ~\u03c0)\u2014the world coordinates and the respective projection onto the sensor for some point-like object\u2014a record, a collection\nA = {( ~p Wj , ~\u03c0j )}M\nj=1 (dense or sparse) of M records obtained at a fixed camera pose\u2014a\ndata frame, and a collection Q = {Ai}Ni=1 of N data frames for various camera poses but identical intrinsic parameters\u2014a dataset. Most calibration algorithms expect such a dataset as input; in particular, the central function calibrateCamera() in OpenCV [30] receives a dataset Q collected at N > 3 distinct poses and fits extrinsic parameters (~ti,~ui) for each pose i and intrinsic parameters~\u03b8 that are assumed to be common to all poses. ATTs collect data in a setup where a camera looks directly at a flat screen. The screen displays a sequence of modulated patterns while the camera captures respective frames. Finally, these images are decoded, and for each camera pixel, we obtain corresponding screen pixel coordinates. The procedure is outlined, e.g., in [29]; our experimental setup is described in Appendix B and is shown with Display 1 in operation in Figure 2.\nIn order to convert screen coordinates to 3D world vectors, we prescribe them a zero z-component, so that each world point is ~p W = ( xS, yS, 0 )T , where xS and yS are the decoded screen coordinates. At each pose, we then obtain a dense data frame of M records, where M may be as large as the total number of camera pixels. As a by-product, at each pixel, the decoding algorithm may also estimate \u2206~p W\u2014the uncertainty in ~p W originating from the observed noise level in the recorded camera pixel values. Unfortunately, the calibrateCamera() function cannot use dense data nor take advantage of estimated uncertainties. We therefore have to extract a sparse subset of valid records from each data frame. To that end, we apply a Gaussian filter with the size of 3 pixels to the decoded coordinate maps and then pick valid pixels in the nodes of a uniform grid in the image space. We found that a 100 \u00d7 100 grid (providing at most 10,000 valid records per frame) ensures rather stable and robust convergence of calibration, while its runtimes remain under ten minutes on a modern CPU. Collecting equivalent-quality data with static (checkerboard) patterns would be very challenging. A complete calibration dataset then includes several (normally 10\u201330) such subsampled sparse data frames recorded at different camera poses. Our experimental datasets span 3 to 4 different distances between the camera and screen and include typically 4 or more poses at each distance; for details, see Appendix B. An example of a dense decoded data frame before filtering and sub-sampling is shown in Figure 3. Non-modulated pixels are identified during the decoding and are displayed as the white space in the maps. The panel (a) shows the decoded values xS, the panel (c)\u2014estimated uncertainties \u2206xS. As should be expected, the high-frequency \u201cjitter\u201d in the coordinates that is visible, e.g., in the cutout 1D profile (b), has a typical amplitude that is close to the respective estimates in (d). In this particular case, the mean decoding uncertainty is about 0.09 mm, which is roughly the same as the screen pixel size of our\nDisplay 2 (that was used to collect these data). Note that the decoding works even with a partial view of the target; by contrast, many simple checkerboard detectors require a full unobstructed view of the entire pattern. This may lead to systematic undersampling and reduce the reliability of calibrated models near the frame boundaries [7]."
        },
        {
            "heading": "5. Data-to-Model Consistency Metrics",
            "text": "Consider a camera with intrinsic parameters~\u03b8 and extrinsic parameters~t, ~u. The consistency of a model ~\u03a0(~p C | ~\u03b8) to a record (~p W , ~\u03c0) may be evaluated in the image space using point-wise vector- and scalar-valued re-projection errors (RPEs):\n~DRPE ( ~p W , ~\u03c0 | ~\u03b8,~t,~u ) = ~\u03a0 ( R(~u) ~p W +~t | ~\u03b8 ) \u2212 ~\u03c0, (2)\nDRPE ( ~p W , ~\u03c0 | ~\u03b8,~t,~u ) = \u2225\u2225\u2225~DRPE(~p C, ~\u03c0 | ~\u03b8,~t,~u)\u2225\u2225\u2225.\nIn order to aggregate point-wise RPEs over a data frame A = {( ~p Wj , ~\u03c0j )}M\nj=1 we may\ndefine the respective \u201croot mean squared\u201d (RMS) value:\nD2DF RMS RPE ( A | ~\u03b8,~t,~u ) =\n1 M\nM\n\u2211 j=1\nD2RPE ( ~p Wj , ~\u03c0j | ~\u03b8,~t,~u ) . (3)\nSimilarly, for a dataset Q = {Ai}Ni=1 we may define\nD2DS RMS RPE ( Q | ~\u03b8, { ~ti,~ui }N i=1 ) =\n1 \u2211i Mi\nN\n\u2211 i=1\nMi \u2211 j=1\nD2RPE ( ~p Wij , ~\u03c0ij | ~\u03b8,~ti,~ui ) , (4)\nwhere Mi is the number of records in the frame Ai, and the normalization is chosen in order to match the convention adopted in the calibrateCamera() function. Figure 4 shows residual RPEs for the pose 13 of the dataset 0 upon the calibration of our \u201cmodel A\u201d."
        },
        {
            "heading": "5.1. Forward Projection Errors",
            "text": "However useful, RPEs have limitations. Most importantly, they are defined in terms of pixels. Effective pixel sizes can often be re-defined by software or camera settings and do not trivially correspond to any measurable quantities in the 3D world. Pixel-based discrepancies are justified when the uncertainties in data are also naturally represented in pixels\u2014for example, when the detections are based on pattern recognition, as is the case with checkerboards and cell corners. Decoding errors in ATTs, however, are defined in length units (e.g., meters) in the 3D space and cannot be directly related to RPEs. Therefore, it appears useful in addition to RPEs to also define forward projection errors (FPEs). Using the same notation as in Equation (2), point-wise FPEs can be defined as\n~DFPE ( ~p W , ~\u03c0 | ~\u03b8,~t,~u ) = ~p E(~\u03c0 | ~\u03b8,~t,~u)\u2212 ~p W and (5)\nDFPE ( ~p W , ~\u03c0 | ~\u03b8,~t,~u ) = \u2225\u2225\u2225~DFPE(~p C, ~\u03c0 | ~\u03b8,~t,~u)\u2225\u2225\u2225, where\n~p E(~\u03c0 | ~\u03b8,~t,~u) = (xE, yE, zE)T =~o W + \u03b1~r W is the expected hit point on the target, ~o W = \u2212R(~u)T~t is the projection center in world coordinates, and ~r W = R(~u)T ~R(~\u03c0 | ~\u03b8) is the view ray direction in world coordinates.\nThe scaling factor \u03b1 here is found as the solution of the linear equation zE = 0, which encodes our assumption that the (flat) active target is located in the plane zW = 0.\nIn plain words, Equation (5) may be interpreted as follows: We emit a view ray from~o W along the direction~r W that corresponds to the pixel ~\u03c0 according to our inverse projective mapping ~R(\u00b7|\u00b7) and find its intersection ~p E with the canonical screen plane zW = 0. The discrepancy on the screen between ~p E and the actual decoded point ~p W then determines a 3D (effectively a 2D) vector ~DFPE. FPEs are defined in physical length units and may be directly compared with the estimated decoding uncertainties \u2206~p W when the latter are available. By analogy to Equations (3) and (4), we may also trivially define aggregate values DDF RMS FPE(. . . ) and DDS RMS FPE(. . . ) over data frames and datasets. Figure 5 shows FPE maps for the same model and data frame as Figure 4. Qualitatively, FPE distributions in this case look similar to those of RPEs, but their values can now be interpreted without any reference to sensor parameters and model details.\nOne possible reason why FPEs have been less popular than RPEs in practice so far is their dependence on the inverse camera mapping ~R(\u00b7|\u00b7), whose evaluation may be more computationally expensive than the direct projection. However, as mentioned above, modern hardware increasingly tends to obsolesce this argument. If the assumption of a flat target is inconvenient or inapplicable in a given setup, one may easily modify the definition Equation (5) as necessary. Furthermore, it is possible to combine FPEs and estimated decoding uncertainties into dimensionless \u201cweighted\u201d error metrics. The latter then optimally exploit the available information and represent the best minimization objective for metrological calibration algorithms [29]."
        },
        {
            "heading": "5.2. Calibration Algorithms",
            "text": "The work principle of most calibration algorithms is to find model parameters that minimize some model-to-data consistency metric. In particular, assuming the definitions of Equations (2) and (4), the Zhang algorithm may be formulated as the following optimization problem. Given a dataset Q = {Ai}Ni=1, it finds\n~\u0398\u2217 \u2261 ( ~\u03b8 \u2217, { ~t \u2217i ,~u \u2217 i }N\ni=1 ) = argmin(~\u03b8,{~ti ,~ui}Ni=1)D2DS RMS RPE ( Q | ~\u03b8, { ~ti,~ui }N i=1 ) , (6)\nwhere the camera model ~\u03a0(\u00b7|\u00b7) = ~\u03a0(CV)(\u00b7|\u00b7) in the definition of DDS RMS RPE is the OpenCV model Equation (A2) and~\u03b8 represents the selected subset of intrinsic parameters affected by the optimization (i.e., those that are not fixed to their default values). Note that Equation (6) treats all the records in the dataset Q equally. This is equivalent to implicitly prescribing the same isotropic uncertainty \u2206~\u03c0 to the projected sensor coordinates at all calibration points. In terms of imaging geometry, this means that detections of similar quality (metric uncertainty) that originate at 3D points further away from the camera constrain the model more strongly than those that are located near the camera. This effect may introduce a non-trivial bias into calibrated models. Alternatively, it is possible to use Equation (5) or similar definitions in order to design calibration algorithms that minimize FPEs instead of RPEs [14]. Such approach is in fact preferable for metrological applications, but we refrain from discussing it here."
        },
        {
            "heading": "6. Characterization of Calibration Quality",
            "text": "Let us imagine that a calibration algorithm such as Equation (6) returns not only the most likely parameters ~\u0398\u2217 but also the complete posterior PDF p(~\u0398 | Q). For a sufficiently well-behaved model and high-quality data, such a (in general case, intractable) PDF may be expected to have a high \u201cpeak\u201d at ~\u0398\u2217, and therefore we may reasonably well approximate it with a Gaussian N (~\u0398 | ~\u00b5\u0398, \u03a3\u0398) that has some central point ~\u00b5\u0398 = ~\u0398\u2217 and a covariance matrix \u03a3\u0398. If we further assume that the true ~\u00b5\u0398 and \u03a3\u0398 are known, the calibration quality aspects (a\u2013c) formulated in Section 1 could be addressed as follows."
        },
        {
            "heading": "6.1. Consistency",
            "text": "It is a common practice to inspect residual RMS values and per-pose maps of RPEs and FPEs computed for ~\u0398\u2217 such as in Figures 4 and 5. In the best case, typical FPEs should match the level of uncertainties \u2206~p W if these are available. RPEs, in turn, may be compared with the diffraction-related blurring scale for the optics measured in pixels. For example, in our experiments, the size of the diffractive spot on the sensor is about 4 \u00b5m, which corresponds to 1\u20132 pixels. A significantly higher level of RPEs/FPEs and the presence of prominent large-scale non-uniformities in per-pose error maps may indicate data collection issues (cf. Moir\u00e9 structures in Figures 4 and 5), convergence problems in optimization, or an excessively \u201cstiff\u201d model (the situation known as \u201cunderfitting\u201d)."
        },
        {
            "heading": "6.2. Reproducibility",
            "text": "Let us assume the following splitting of the calibration state vector ~\u0398 and the parameters of the respective Gaussian posterior PDF:\n~\u0398 = (\n~\u03b8 ~\u03b3\n) , ~\u00b5\u0398 = ( ~\u00b5\u03b8 ~\u00b5\u03b3 ) , and \u03a3\u0398 = ( \u03a3\u03b8 \u03a3\u03b8\u03b3 \u03a3T\u03b8\u03b3 \u03a3\u03b3 ) , (7)\nwhere~\u03b8 represents intrinsic camera parameters, ~\u03b3 collectively denotes all per-pose extrinsic parameters, and \u03a3\u03b8 and \u03a3\u03b3 are some symmetric positive-definite matrices. The off-diagonal block \u03a3\u03b8\u03b3 captures the correlations between~\u03b8 and ~\u03b3. As discussed below, these correlations are typically hard to estimate reliably, and we ignore them. Only~\u03b8 may be compared between independent calibration attempts since poses are chosen each time anew. We propose to use a Gaussian distribution N (~\u03b8 | ~\u00b5\u03b8 , \u03a3\u03b8) as the induced posterior PDF over the expected calibration outcomes. This form is equivalent to the full PDF marginalized over ~\u03b3 and is consistent with the absence of any additional information that could constrain the model parameters. In case such information does exist (in the form of, e.g., independent measurements of camera positions by an external sensor), one should modify this rule and, e.g., implement some form of conditioning. The consistency of some set of intrinsic parameters~\u03b8 \u2032 with the current calibration results then may be estimated with the help of Mahalanobis distance DM(~\u03b8 \u2032 | ~\u00b5\u03b8 , \u03a3\u03b8) defined in Equation (A4) and the respective plausibility level PM(~\u03b8 \u2032 | ~\u00b5\u03b8 , \u03a3\u03b8) of Equation (A6)."
        },
        {
            "heading": "6.3. Reliability",
            "text": "If one intends to use the calibrated camera model in geometric measurements, its crucial physical characteristic is the expected 3D uncertainty of the view rays induced by the uncertainty in the model parameters. Let us assume that the inverse camera model ~R(\u00b7|\u00b7) returns a vector~r = (rx, ry, 1)T whose third component is fixed at unity similarly to our definition of the inverse Zhang model in Equation (A3). Then, the function\n\u03c1(~\u03c0 | ~\u03b8, \u03a3\u03b8) = \u221a Tr ( \u03a3r(~\u03c0 | ~\u03b8) ) , where (8)\n\u03a3r(~\u03c0 | ~\u03b8, \u03a3\u03b8) = J(~\u03c0 | ~\u03b8) \u03a3\u03b8 J(~\u03c0 | ~\u03b8)T and J(~\u03c0 | ~\u03b8) = \u2202~R(~\u03c0 | ~\u03b8)\n\u2202~\u03b8\ndescribes the scalar projected uncertainty of the view rays at their intersection with the plane zC = 1. In other words, \u03c1(~\u03c0 |~\u03b8, \u03a3\u03b8) defines a map of \u201cexpected FPE gains\u201d (EFPEGs). These may be interpreted as expected FPEs (EFPEs) evaluated over a virtual screen orthogonal to the camera\u2019s axis and displaced by 1 m from the projection center. Obviously, for any central model, the uncertainty of view ray positions is linearly proportional to the distance from the camera. That is, an EFPE for a view ray corresponding to some pixel ~\u03c0 on the plane zC = z in the camera\u2019s frame is\nDEFPE(~\u03c0 | ~\u03b8, \u03a3\u03b8 , z) = z \u03c1(~\u03c0 | ~\u03b8, \u03a3\u03b8). (9)\nThus, a single map of EFPEGs over the sensor is sufficient to derive calibration uncertainty-related errors for any scene of interest. In particular, we can derive EFPEs for the actual calibration targets and compare them with the respective residual FPEs. If EFPEs significantly exceed residual errors, the model may be \u201cunder-constrained\u201d, i.e., the calibration dataset is too small or the camera poses in it are insufficiently diverse."
        },
        {
            "heading": "7. Calibration Workflow and Data Management",
            "text": "The approach of Section 6 allows us, in principle, to fully characterize the quality of calibration. This section focuses on obtaining the best estimates of ~\u00b5\u03b8 and \u03a3\u03b8 in practice. Given a dataset Q with N data frames and a camera model, one may simply calibrate the latter using all N poses in order to obtain the best-fit parameters ~\u0398(ini). We denote this step as initial calibration. In addition to ~\u0398(ini), many calibration tools in fact also estimate their stability in some form. For example, the extended version of the calibrateCamera() function in OpenCV (overloaded under the same name in the C++ API, calibrateCameraExtended() in the Python API [30]) returns the estimated variances \u03b42~\u0398(ini) for the individual components of ~\u0398(ini). From these, one may recover a covariance matrix \u03a3(ini)\u0398 = diag(\u03b4\n2~\u0398(ini)). This diagonal form is a conservative estimate of the true variability of outcomes that assumes no available knowledge about possible correlations between the errors in different components of ~\u0398(ini). Covariances of parameters are hard to estimate reliably since they in practice need many more data frames than what is typically available and are not returned by OpenCV. The values \u03b42~\u0398(ini) are found (at high added computational cost) from the approximate Hessian of the objective function in Equation (6) and the residual RPEs at the optimum, which is the standard approach for non-linear least square problems [31]. Unfortunately, in practice, this method may wildly under- or overestimate the variability of parameters depending on the lens properties, constellation geometries, and input data quality. For example, Figure 6 shows EFPEs computed according to Equation (9) for the same calibration target as Figure 5 based on the output from calibrateCamera(). We see that this projection seriously underestimates the actual deviations, and \u03a3(ini)\u0398 is thus unreliable."
        },
        {
            "heading": "7.1. Rejection of Outlier Data Frames",
            "text": "The data collection process is not trivial, and it is possible for some data records to have a significantly poorer quality than others. This may happen due to, e.g., a bad choice of calibration poses, illumination changes or vibration during the data acquisition, etc. In the context of ATTs, such quality degradation typically affects entire data frames\u2014adjacent pixels in a frame are not likely to feature significantly different error levels. If the decoded dataset contains per-record uncertainty estimates as in Figure 3, the latter will reflect any decoding problems, and an uncertainty-aware calibration tool may then automatically detect bad data and ignore them in the fits. Unfortunately, the Zhang method in Equation (6) cannot detect faulty frames and the latter, if present, may randomly affect the optimization outcomes. We therefore must detect and remove such \u201coutlier frames\u201d manually before we produce the final results and conclusions.\nTo that end, we analyze the residual per-pose RPEs after the initial calibration:\nE(ini)i = DDF RMS RPE(Ai | ~\u03b8 (ini),~t (ini)i ,~u (ini) i ) (10)\nfor the data frames Ai and the respective camera parameters ~\u03b8(ini), ~t (ini) i , and ~u (ini)\ni (i = 1, ..., N) extracted from ~\u0398(ini). We expect that the RPEs for the \u201cgood\u201d frames will \u201ccluster\u201d together, while the \u201cbad\u201d frames will demonstrate significantly different residual errors. For example, Figure 7 shows residual per-pose RPEs for all 29 poses in our dataset 0. Indeed, the values appear to group together except for a few points. In this particular case, the \u201cproblematic\u201d frames correspond to the camera being placed too close to the screen (at about 10 cm); the finite entrance aperture of the lens and the mismatching focus then lead to a high dispersion of the decoded point coordinates.\nThe most well-known formal outlier detection techniques are based on Z-scores, modified Z-scores, and interquartile ranges. In our code, we use the modified Z-score\nmethod with the threshold of 2.0 since it is more stable for small samples and allows an easy interpretation. The shaded region in Figure 7 shows the acceptance bounds dictated by this method, according to which the frames 25 and 26 must be declared outliers. However, one is free to use any reasonable alternative methods and thresholds. In what follows, we assume that our dataset Q is free from such outliers."
        },
        {
            "heading": "7.2. Empirical Evaluation of Calibration Quality",
            "text": "As discussed in Section 6.1, residual per-dataset and per-pose errors as in Figure 7, as well as maps such as in Figure 4, characterize the consistency of the model with the data that it has \u201cseen\u201d during the calibration. However, they tell us nothing about its performance on \u201cunseen\u201d data. In machine learning (ML), this problem is solved by randomly splitting the available data into \u201ctraining\u201d and \u201ctesting\u201d sets Qtrain and Qtest. One then calibrates the model over Qtrain and evaluates its performance on Qtest. Usually, one allocates 70% of data to Qtrain; in our experiments, this proportion also seems to work well. This approach addresses the so-called \u201cgeneralizability\u201d of the model. A well-trained model must demonstrate similar metrics on Qtrain and Qtest. If, e.g., RPEs on Qtest significantly exceed the residual values in training, this may indicate \u201coverfitting\u201d: an excessively flexible model that learns random fluctuations rather than physical features inherent in the data. Such a model cannot be considered reliable, nor its parameters reproducible. As a remedy, one could use more data or switch to a \u201cstiffer\u201d model. Furthermore, most ML models have some \u201chyperparameters\u201d such as discretization granularity, numbers of layers in a neural network, their types, etc. Generally, these affect the flexibility of the model as well as the number of trainable parameters. Sometimes, one may choose hyperparameters a priori based on physical considerations. Often, however, they may only be found from data along with the model\u2019s parameters. In this case, one uses the third separate \u201cvalidation\u201d dataset Qvalid [32]. Again, the basic rule here is that we should make the final evaluation on data that the model has not \u201cseen\u201d during its training nor during the fine-tuning of hyperparameters. We can easily adapt this procedure to camera calibration. Given a dataset Q, we first split it into Qtrain, Qvalid, and Qtest. Let us assume that we wish to calibrate either our \u201cmodel A\u201d or \u201cmodel B\u201d. This choice is our hyperparameter: model B can fit more complex imaging geometries but is more prone to overfitting. We calibrate both models according to Equation (6) using Qtrain and determine respective intrinsic parameters ~\u03b8 ( f in A) and ~\u03b8 ( f in B). After that, we evaluate DDF RMS RPE(Qvalid | ...) for both models. Depending on the outcomes, we pick the model that better fits the data and demonstrates neither overnor underfitting. Finally, once the model is fixed, we find E( f in) = DDS RMS RPE(Qtest | ...) and report it as a measure of the final calibration quality. In what follows, we in fact assume a simpler procedure (we call it final calibration) which uses only Qtrain and Qtest; we do not optimize hyperparameters and do not choose the model based on data. A similar strategy has been used in [8]. The recipe above warrants a few remarks. First, all records in a frame depend on the same camera pose. We thus can only split datasets at the level of frames. Second, in order to compute RPEs/FPEs on a new data frame for some fixed intrinsic parameters~\u03b8, we need to first find a best-fit camera pose via the so-called bundle adjustment:(\n~t \u2217,~u \u2217 ) = argmin(~t,~u)D 2 DF RMS RPE ( A | ~\u03b8,~t,~u ) . (11)\nIn OpenCV, this optimization is implemented in the solvePnP() function [30]. Note that formally the \u201cfinal calibration\u201d uses a smaller dataset and may thus produce a model inferior to the \u201cinitial calibration\u201d. This is the price of the added quality guarantees; we believe that the benefits are almost always worth it."
        },
        {
            "heading": "7.3. K-Fold Cross-Validation",
            "text": "The \u201cfinal calibration\u201d above provides us the final set of intrinsic parameters ~\u00b5\u03b8 = ~\u03b8 ( f in). In order to estimate their variability, we employ yet another ML-inspired empirical technique\u2014the so-called K-fold cross-validation [32]. In essence, we repeat the same steps as in the \u201cfinal calibration\u201d K times (we use K = 10), each time making a new random splitting of data into Qtrain and Qtest. The collection of the resulting intrinsic parameters ~\u03b8 (k) and the residual RMS RPE values E(k)train and E(k)test for k = 1, . . . , K is retained; the remaining calibrated parameters and evaluation results are discarded. From that, we derive the following two indicators:\n\u03a3(KF f ull)\u03b8 = cov ({ ~\u03b8 (k) }K\nk=1\n) , (12)\n\u03b42E(KF) = var ({ E(k)train }K\nk=1\n) + var ({ E(k)test }K k=1 ) .\nThe value \u03b4E(KF) estimates the stability of the residual RPEs and quantifies any claims of \u201csignificantly higher/lower\u201d RPE levels when, e.g., detecting overfitting (Section 6.1). The matrix \u03a3(KF f ull)\u03b8 can in principle be used as an estimate for \u03a3\u03b8 of Sections 6.2 and 6.3. However, in practice, K is usually significantly smaller than the number of independent components in \u03a3(KF f ull)\u03b8 . The latter then ends up being rank-deficient, and even if it does have a full rank, its inverses are often unstable due to insufficient statistics. As a pragmatic fix to this problem, we define a \u201crobustified\u201d matrix \u03a3(KF)\u03b8 that has the same diagonal as \u03a3(KF f ull)\u03b8 and zero non-diagonal elements. As discussed above, such construction does not introduce new biases and improves stability. With \u03a3\u03b8 = \u03a3 (KF) \u03b8 , we then may complete the characterization of the model\u2019s quality. Note that our recipe is different from the typical descriptions of K-fold cross-validation in the literature. In ML, one typically assumes relatively large datasets that may be arbitrarily sub-divided into parts; by contrast, in camera calibration, we usually deal with at most a few dozen camera poses, hence our pragmatical modification.\nFigure 8 shows EFPEs obtained with~\u03b8 = ~\u03b8( f in) and \u03a3\u03b8 = \u03a3 (KF) \u03b8 produced as discussed\nabove. The typical values in Figure 8d are more consistent with Figure 5d than those in Figure 6; we thus believe that \u03a3(KF)\u03b8 in this case better characterizes the model than the internal estimation in the calibrateCamera() function. One may argue whether EFPEG plots such as Figure 8a should use the units of mm/m or radians. Indeed, scalar EFPEs for central models directly correspond to angular uncertainties of the view rays emitted from the projection center. One reason to prefer our notation is that Equation (8) can easily accommodate anisotropic errors (separate estimates for \u2206xW and \u2206yW); respective angular quantities may be tricky to define. An even more complex picture arises for non-central camera models. In this case, the uncertainty profile for a view ray in 3D is described by a \u201cGaussian beam\u201d: a complicated object that induces a 2D Gaussian PDF over any intersecting plane. Depending on the calibration camera poses and the data quality, the \u201cwaist\u201d of such beam will not necessarily be located at the camera\u2019s origin. A practical way to characterize the expected model errors in this case could include a series of plots such as Figure 8a but evaluated at different distances from the camera (selected as dictated by applications). (We thank the members of the audience at the 2022 Annual Meeting of the German Society for Applied Optics (DGaO) who have drawn our attention to this issue.)"
        },
        {
            "heading": "8. Proposed Workflow and Reporting of Outcomes",
            "text": "Summarizing the discussion in the previous section, here, we present the list of essential steps needed to ensure a controllable calibration session. The method receives a dataset Q with N data frames as an input and produces intrinsic parameters~\u03b8\u2217 accompanied by sufficient quality specifications.\n1. Initial calibration. Calibrate the model with all N data frames. Output N per-pose\nresidual RPE values E(ini)i (Section 7.1).\n2. Outlier rejection. Based on values E(ini)i , remove \u201cbad\u201d data frames (Section 7.1). 3. Final calibration. Randomly sub-divide Q into Qtrain and Qtest. Calibrate the model\nover Qtrain. Output the resulting intrinsic parameters~\u03b8( f in) and the residual RPEs/FPEs as per-pose/per-dataset values and point-wise maps (Section 7.2).\n4. K-fold cross-validation. Repeat K times the splitting of Q into training and test sets;\ncalibrate sub-models. Output stability indicators \u03b4E(KF) and \u03a3 (KF) \u03b8 (Section 7.3).\n5. Generalizability check. Perform bundle adjustment for ~\u03b8( f in), evaluate RPEs and FPEs over Qtest. Output respective per-pose, per-dataset values, and point-wise maps. 6. Reliability metrics. Map expected FPEs based on~\u03b8( f in) and \u03a3(KF)\u03b8 . Output the map of EFPEGs and per-pose maps of EFPEs for the poses in Q (Section 6.3).\nThe outcomes of this procedure include the intrinsic parameters ~\u03b8 \u2217 \u2261 ~\u03b8( f in), the covariance matrix \u03a3\u2217\u03b8 \u2261 \u03a3 (KF) \u03b8 , and the map of EFPEGs over the sensor\u2014as discussed above, these objects may be useful for the downstream applications of the calibrated model. In addition, one may report the residual RPEs and FPEs over Qtrain and Qtest and the\nestimated variability \u03b4E(KF) of the residual RPEs. The user then may decide whether the model is sufficiently flexible and the size of the calibration dataset adequate."
        },
        {
            "heading": "9. Experimental Illustration",
            "text": "We conducted our experiments and collected five datasets as described in Appendix B. For each dataset, we calibrated three models: \u201cmodel A\u201d, \u201cmodel B\u201d, and the additional \u201cmodel AB\u201d. The latter was introduced in order to overcome instabilities of the \u201cmodel B\u201d. It uses the same parameterization, but the program first calibrates the \u201cmodel A\u201d and then uses it as the starting point for the subsequent calibration of the full \u201cmodel B\u201d. Table 1 summarizes our results in terms of the following metrics:\n\u2022 Nposes is the number of data frames in the dataset before the rejection of outliers; \u2022 E(ini) is the residual RMS RPE for the entire dataset after the \u201cinitial calibration\u201d; \u2022 Noutliers is the number of detected (and rejected) \u201cbad\u201d data frames; \u2022 E( f in)train is the residual RMS RPE after the \u201cfinal calibration\u201d evaluated over Qtrain; \u2022 E( f in)test is the residual RMS RPE after the \u201cfinal calibration\u201d evaluated over Qtest; \u2022 \u03b4E(KF) is the empirical variability scale of the residual per-dataset RMS RPEs; \u2022 D(KF)RMS EFPEG is the RMS \u201cexpected FPE gain\u201d (EFPEG) estimated from \u03a3 (KF) \u03b8 ; \u2022 \u201cSample pose\u201d is used as an example to illustrate the subsequent per-pose values; \u2022 D( f in)DF RMS RPE is a sample per-pose RMS RPE upon the \u201cfinal calibration\u201d; \u2022 D( f in)DF RMS FPE is a sample per-pose RMS FPE upon the \u201cfinal calibration\u201d; \u2022 D(KF)DF RMS EFPE is a sample per-pose RMS EFPE based on K-fold cross-validation.\nFigure 9 illustrates the calibration outcomes of the more nuanced \u201cmodel AB\u201d over the dataset 3. The EFPEG map and the EFPEs for pose 10 promise here sub-mm uncertainties.\nDiscussion of Experimental Results\nBy inspecting Table 1, we may derive a number of non-trivial conclusions:\n1. Data collected with an 8K monitor (datasets 3, 4) enable a significantly better calibration that those obtained with an HD screen (datasets 0, 1, 2) as can be seen, e.g., in the residual error levels E( f in)test . Apart from larger screen pixels, this may be due to the aforementioned Moir\u00e9 effect (more pronounced with an HD screen). 2. The commonly used aggregate error indicators are very weakly sensitive to the detailed calibration quality aspects defined in Section 1: models with wildly different reliability and reproducibility metrics demonstrate very similar values of E(ini). 3. The full OpenCV camera model (\u201cmodel B\u201d) is much less stable than the reduced \u201cmodel A\u201d, as follows from the values DRMS EFPEG. The two-step optimization (\u201cmodel AB\u201d) significantly improves the situation; one possible explanation is that the \u201cmodel B\u201d too easily becomes trapped in some shallow local minima, and the informed initialization helps it select a better local minimum. Therefore, in practice, one should always prefer the \u201cmodel AB\u201d to the \u201cmodel B\u201d. 4. Our larger dataset 0 has relatively high decoding errors, and the performance metrics of the \u201cmodel A\u201d and the \u201cmodel AB\u201d are almost identical. Therefore, in this case, one should prefer the simpler \u201cmodel A\u201d. As a benefit, the latter promises slightly lower EFPEGs (DRMS EFPEG) due to its higher \u201cstiffness\u201d. (For a better justification one should use the \u201cvalidation\u201d logic as discussed in Section 7.2.) 5. The datasets 1 and 2 are apparently too small to constrain our models, as follows from the increased \u03b4E(KF) and DRMS EFPEG values compared with the larger datasets. This may be also a reason for the test metrics E( f in)test to be lower that the training accuracy E( f in)train (although the difference is still of order \u03b4E\n(KF)). The higher flexibility of the \u201cmodel AB\u201d then translates to higher expected errors DRMS EFPEG.\n6. With the better quality datasets 3 and 4, the \u201cmodel AB\u201d finally demonstrates an advantage and promises slightly better metric uncertainties DRMS EFPEG. With the dataset 3, we see sub-mm RMS uncertainty at the distance of 1 m; in the center of the frame, the errors are even lower (Figure 9). Note that the residual FPEs D( f in)DF RMS FPE for the dataset 4/pose 17 nicely agree with the estimated decoding errors in Figure 3."
        },
        {
            "heading": "10. General Discussion",
            "text": "Our results suggest that it is possible to relatively easily calibrate typical industrial cameras to the accuracy of order 1 mm when camera-assisted measurements happen at the distances of order 1 m and at the same time produce some \u201ccertificates\u201d of calibration quality. The main enabling factor here is the superior quality of calibration data delivered by ATTs. We further observe that modern higher-resolution screens (4K and 8K) offer a significant advantage due to smaller pixels and suppressed Moir\u00e9 effects. As such screens become widely available, we see no reason to calibrate cameras with static patterns, even for less demanding applications in computer vision. We also observe that the decoding errors for the affordable 1920 \u00d7 1080 screens are relatively high and do not justify the use of advanced models such as our \u201cmodel AB\u201d. A simple pinhole model with low-order distortions is sufficient here to obtain quite consistent, reliable, and reproducible results. Note, however, that this effect may be also due to the relatively high quality of lenses used in our experiments: simpler or wider-angle optics may cause higher distortions and necessitate a more flexible model. The ML-inspired procedure outlined in Section 7 appears to consistently deliver adequate variability estimates for the parameters and the performance metrics of camera models. By contrast, the internal estimates built into the OpenCV functions may be significantly off, sometimes by a factor of 10 or more. (For example, typical error scales in Figure 6 and in Figure 8c,d differ by about a factor of 6; we have witnessed significantly more extreme examples in our complete experimental database.) The numerical behavior of the complete OpenCV model also appears to be unstable, and one may need to apply some ad hoc fixes such as our \u201cmodel AB\u201d in order to obtain useful calibration results.\nFurther Improvements and Future Work\nIf order to further push the boundaries in terms of data quality and the resulting model uncertainties, one has to account for various minor effects that are present in any setup similar to ours. One such issue is the refraction in the screen\u2019s cover glass that introduces systematic deviations into decoded coordinates xS and yS of order 0.1 mm when the view angles are sufficiently far away from the normal incidence [18]. Respective optical properties of the screen as well as its deformations due to gravity or thermal expansion [17] then must be modeled and calibrated separately. Furthermore, one may switch to high-dynamic-range screens and separately calibrate the pixel response functions (gamma curves). Respective corrections may improve the decoding accuracy or, alternatively, reduce the necessary data acquisition times. As shown above, Moir\u00e9 structures may significantly impact data quality. To the best of our knowledge, these effects have not been studied systematically. In practice, one usually adjusts calibration poses and screen parameters until such structures become less visible. For ultimate-quality measurements, one may require more rigorous procedures. Another less-studied effect is blurred (non-sharp) projection. There is a general assumption that blurring does not affect coordinates decoded with phase-shifted cosine patterns [33]. However, this statement is true only for Gaussian blurring kernels whose scale is significantly smaller than the spatial modulation wavelengths of patterns. The decoding is also noticeably biased near screen boundaries if they appear in the frame. Note that already our achieved uncertainty levels may approach the breaking point for the assumption of the ideal central projection. Depending on the quality of optics, at some point one then has to switch to non-central models. There is no simple and clear rule to\ndetermine this point; also, the calibration of such models is significantly more challenging in practice, and their theory still has some open questions [29]. At the system level, it may be interesting to extend the above approach and address the online calibration problem. In particular, once a camera is studied in a laboratory, one could use simpler procedures and less \u201cexpensive\u201d indicators (that ideally do not interfere with its main task) in order to detect and quantify parametric drifts due to environmental factors along with variations in the quality metrics (such as EFPEGs). All comparisons and quality evaluations in this paper are based on high-quality datasets collected with ATTs. Each such dataset may contain millions of detected 3D points, and therefore provides a very stringent test for a calibrated camera model. Nevertheless, it may be quite instructive to demonstrate the advantages of the proposed workflow in the context of demanding practical applications such as fringe projection-based or deflectometric measurements. We leave such illustrations to future work."
        },
        {
            "heading": "11. Conclusions",
            "text": "In this paper, we discuss the quality of camera calibration and provide practical recommendations for obtaining consistent, reproducible, and reliable outcomes. We introduce several quality indicators that, if accepted in common practice, may potentially lead to the better characterization of calibration outcomes in multiple applications and use-cases both in camera-assisted optical metrology and computer vision fields. Our approach is empirical in nature and is inspired by the techniques used in Machine Learning. As an illustration of the proposed method, we conduct a series of experiments with typical industrial cameras and commercially available displays. Our procedure to characterize the quality of outputs is shown to be superior to the previously known methods based on fragile assumptions and approximations. The metric uncertainty of the calibrated models in our experiments corresponds to forward projection errors of order 0.1\u20131.0 mm at the distances of order 1 m and is consistent with the observed levels of residual projection errors. We hope that the workflows and tools demonstrated in this paper may appear attractive to other practitioners of camera-based measurements. In order to lower the threshold for the adoption of these methods, we published the datasets and the Python code that were used to obtain our results; respective links are provided below.\nAuthor Contributions: Conceptualization, all authors; methodology, A.P. and S.R.; software, A.P. and S.R.; experiments, all authors; resources, J.B.; data curation, A.P.; writing\u2014original draft preparation, A.P. and S.R.; writing\u2014review and editing, all authors. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nData Availability Statement: The Python code and the original datasets for this paper are available upon request from the authors.\nAcknowledgments: Most of this work was performed during S.R.\u2019s research sabbatical in the winter 2020/21 at or with Fraunhofer IOSB in Karlsruhe. The authors would like therefore to acknowledge Fraunhofer IOSB in Karlsruhe and Pforzheim University for supporting this work. We are also thankful to the anonymous reviewers whose comments and suggestions have contributed to a better presentation of this work.\nConflicts of Interest: The authors declare no conflict of interest."
        },
        {
            "heading": "Appendix A. Camera Model Implemented in the OpenCV Library",
            "text": "The camera embedding into the 3D world is given by Equation (1) according to the extrinsic parameters~t, ~u. The parameterization of rotation matrices R(~u) in OpenCV is as follows:\nR(~u) = I cos \u03b8 + (1\u2212 cos \u03b8) ~n ~nT + N(~u) sin \u03b8, where I is a 3 \u00d7 3 unit matrix, (A1)\nN(~u) =  0 \u2212nz nynz 0 \u2212nx \u2212ny nx 0 , \u03b8 = |~u|, and ~n = (nx, ny, nz)T = ~u \u03b8 .\nThe intrinsic camera model adopted in OpenCV is formulated in terms of the direct projection relation for a 3D point onto the sensor plane. Given the coordinates ~p C = ( xC, yC, zC\n)T of some 3D point P in the camera\u2019s intrinsic frame, the respective coordinates ~\u03c0 = ( xI , yI )T = ~\u03a0(CV)(~p C | ~\u03b8) of its projection on the sensor are\n~\u03a0(CV)(~p C | ~\u03b8) = ( fxx\u2032\u2032\u2032 + cx, fyy\u2032\u2032\u2032 + cy )T , where (A2)\nx\u2032\u2032\u2032 = X\u2032\u2032\u2032\nZ\u2032\u2032\u2032 , y\u2032\u2032\u2032 =\nY\u2032\u2032\u2032 X\u2032\u2032\u2032 ,  X\u2032\u2032\u2032Y\u2032\u2032\u2032 Z\u2032\u2032\u2032  = T1T2  x\u2032\u2032y\u2032\u2032 1 , T1 =\n cos \u03c4y cos \u03c4x 0 sin \u03c4y cos \u03c4x0 cos \u03c4y cos \u03c4x \u2212 sin \u03c4x 0 0 1 , T2 =\n cos \u03c4y sin \u03c4y sin \u03c4x \u2212 sin \u03c4y cos \u03c4x0 cos \u03c4x sin \u03c4x sin \u03c4y \u2212 cos \u03c4y sin \u03c4x cos \u03c4y cos \u03c4x , x\u2032\u2032 = f x\u2032 + 2p1x\u2032y\u2032 + p2(r2 + 2x\u20322) + s1r2 + s2r4, y\u2032\u2032 = f y\u2032 + p1(r2 + 2y\u20322) + 2p2x\u2032y\u2032 + s3r2 + s4r4,\nf = 1 + k1r2 + k2r4 + k3r6\n1 + k4r2 + k5r4 + k6r6 , r2 = x\u20322 + y\u20322, x\u2032 =\nxC zC , and y\u2032 = yC zC .\nThe 18 intrinsic parameters~\u03b8 = ( fx, fy, cx, cy, k1, k2, k3, p1, p2, k4, k5, k6, s1, s2, s3, s4, \u03c4x, \u03c4y) can be interpreted as follows. The values fx, fy, cx, cy define the sensor\u2019s size, resolution, and placement in the image plane according to the basic pinhole camera model (skewed projection is not allowed). The remaining terms describe various types of distortions: k1, k2, k3, k4, k5, k6\u2014radial, p1, p2\u2014tangential, and s1, s2, s3, s4\u2014thin-prism. Finally, \u03c4x and \u03c4y correspond to the possible tilting of the image plane with respect to the lens (related via Scheimpflug principle to the tilt of the focus plane). When the parameters k1, . . . , \u03c4y vanish, Equation (A2) reduces to the undistorted pinhole model. In order to derive an inverse projection relation for the model Equation (A2), one would need to invert these formulas\u2014a task that is intractable in closed form. Instead, in practice, we find the direction~r = ~R(CV)(~\u03c0 | ~\u03b8) of a view ray corresponding to a pixel ~\u03c0 numerically as a solution of a small optimization problem:\n~R(CV)(~\u03c0 | ~\u03b8) = (r\u2217x, r\u2217y , 1)T , where (A3)\n(r\u2217x, r \u2217 y) = argmin(rx ,ry) \u2225\u2225\u2225~\u03c0 \u2212 ~\u03a0(CV)((rx, ry, 1)T | ~\u03b8)\u2225\u2225\u22252. In non-pathological cases, Equation (A3) can be solved by some Newton\u2019s scheme or another\niterative method. In our code used in the experiments, we employ the Levenberg\u2013Marquardt method [31] implemented in the SciPy library [34] and observe solid convergence. Finally, the analysis of the calibrated model requires the derivatives \u2202~\u03a0/\u2202~p C, \u2202~\u03a0/\u2202~\u03b8, \u2202~R/\u2202~\u03c0, and \u2202~R/\u2202~\u03b8 (cf. Section 6.3 and Equation (8)). The differentiation of Equation (A2) and the conversion of formulas to code is trivial but tedious. Instead, we apply an automated differentiation tool implemented in the AutoGrad package [35] to the code for ~\u03a0(CV)(\u00b7|\u00b7). The derivatives of ~R(CV)(\u00b7|\u00b7) are then obtained by trivial matrix manipulations."
        },
        {
            "heading": "Appendix B. Hardware Setup and Collected Datasets",
            "text": "Our experimental setup uses two identical industrial cameras (Camera 1 and Camera 2) by Allied Vision, model Mako G-507 B. Their 2/3 sensor (11.1 mm in diagonal) has 2464 \u00d7 2056 monochromatic pixels (horizontal \u00d7 vertical) working at 12-bit resolution and arranged at the pitch of 3.45 \u00b5m \u00d7 3.45 \u00b5m. Each camera is coupled with a Fujinon HF8XA-5M lens chosen to provide adequate optical resolution. In all experiments, the focus distance was fixed at about 900 mm and the F-number was fixed at 4.0 when collecting all datasets, except for the dataset 3, where it was set to 2.0. The lens has a focal length of 8.0 mm, the field of view with a 2/3 sensor is about 56\u25e6 \u00d7 48\u25e6.\nFigure A1. Visualization of camera poses for the dataset 0. The rectangle indicates the coding screen and each pyramid denotes a calibration camera pose. The projection center corresponds to the tip of each pyramid.\nAs active targets, we have used two different computer displays. We assume them to be sufficiently flat and make no modification to ensure planarity other than placing them vertically without tilting. The first one (Display 1) is a 55\u2032\u2032 Philips monitor (model 55BDL5057P) with 1920 \u00d7 1080 pixels and the active region of 1209.6 mm \u00d7 680.4 mm (the pixel pitch is 0.630 mm \u00d7 0.630 mm). The second (Display 2) is a 32\u2032\u2032 8K Dell monitor (UP3218K) with 7680 \u00d7 4320 pixels in the active region of 697.0 mm \u00d7 392.0 mm (pitch 0.091 mm \u00d7 0.091 mm). Each recorded calibration dataset includes data frames captured at different camera poses. In order to efficiently exploit the screen and the sensor resolution, and avoid Moir\u00e9 effects as well as excessive blurring, the camera positions are chosen at about 100 mm, 250 mm, 1000 mm, and 2500 mm from the 55\u2032\u2032 Philips monitor and at about 300 mm, 800 mm, and 1700 mm from the 8K Dell monitor. At each distance, we rotated the camera so as to cover most of the frame area with overlapping observations. Figure 2 shows the experimental setup where the camera is placed at a distance of about 1000 mm from the Philips monitor and Figure A1 visualizes all camera positions for the dataset 0. The spatial frequencies and the numbers of phase shifts of the coding patterns were selected to enable robust decoding both at the shortest and at the furthest camera poses; the number of displayed patterns/camera images at each pose was 16 for the datasets collected with the Phillips monitor and 45 with the Dell monitor.\nThe collected datasets have the following characteristics:\nDataset Camera F-Number Display Number of Poses Uncertainties\n0 1 4 1 29 no 1 2 4 1 18 no 2 1 4 1 17 no 3 2 2 2 17 yes 4 2 4 2 19 yes"
        },
        {
            "heading": "Appendix C. Mahalanobis Distance and the Plausibility of Outcomes",
            "text": "Given an n-dimensional Gaussian PDF P = N (~\u00b5; \u03a3) with a central value ~\u00b5 and covariance matrix \u03a3, how can we evaluate the consistency of an arbitrary vector~\u03b8 with P? In other words, how plausible is the assumption that~\u03b8 has been sampled from P?\nIn the one-dimensional case (n = 1), we could find the difference between~\u03b8 and ~\u00b5 in the units of standard deviation (\u201csigma\u201d) and apply the well-known \u201cempirical rule\u201d: the probability of encountering a deviation of one \u201csigma\u201d or less is 68.2%, a deviation of two \u201csigmas\u201d or less\u201495.4%, etc. The generalization of this rule to higher dimensions n > 1 uses the so-called Mahalanobis distance:\nDM(~\u03b8 | ~\u00b5, \u03a3) = \u221a (~\u03b8 \u2212~\u00b5)T\u03a3\u22121(~\u03b8 \u2212~\u00b5). (A4)\nFigure A2. Profiles of the accumulated Gaussian probability mass q(n, d) of Equation (A5) for several values of space dimensionality n and Mahalanobis distance d.\nIn order to interpret the values of DM, one needs a generalized \u201cempirical rule\u201d that represents the probability for a random sample drawn fromN (~\u00b5; \u03a3) to land within a given Mahalanobis distance d from ~\u00b5. This probability can be evaluated in closed form:\nProb [ DM(~\u03b8 | ~\u00b5, \u03a3) < d for~\u03b8 \u223c N (~\u00b5; \u03a3) ] = q(n, d) = 1\u2212 \u0393(n/2, d\n2/2) \u0393(n/2) , (A5)\nwhere \u0393(z) = \u222b \u221e\n0 x z\u22121e\u2212xdx and \u0393(a, z) = \u222b \u221e z x\na\u22121e\u2212xdx are the regular and the incomplete gamma-functions (cf. \u03c72-distribution and its cumulative distribution function). Some values of the function q(n, d) are shown in Figure A2. In particular, the n = 1 case reproduces the above-mentioned one-dimensional \u201cempirical rule\u201d. In higher dimensions, the bulk of probability mass moves further away from the center. For instance, in n = 9 dimensions, only 0.056% of the accumulated probability remains inside the \u201cone-sigma\u201d (d = 1) hyperellipsoid, and there is only a 56.27% chance for a random sample from a Gaussian distribution to land within three \u201csigmas\u201d away from its central point. These definitions allow us to define a convenient \u201cplausibility\u201d metric:\nPM(~\u03b8 | ~\u00b5, \u03a3) = q(n, DM(~\u03b8 | ~\u00b5, \u03a3)). (A6)\nUsing Equation (A6) and~\u03b8 \u2217, \u03a3\u03b8\u2014the outputs from the workflow of Section 8\u2014one can, e.g., compare the outcomes of several calibrations and decide if they are compatible.\nReferences 1. Betzig, E.; Patterson, G.H.; Sougrat, R.; Lindwasser, O.W.; Olenych, S.; Bonifacino, J.S.; Davidson, M.W.; Lippincott-Schwartz, J.; Hess, H.F. Imaging Intracellular Fluorescent Proteins at Nanometer Resolution. Science 2006, 313, 1642\u20131645. [CrossRef] [PubMed] 2. EMVA. Standard for Characterization and Presentation of Specification Data for Image Sensors and Cameras; EMVA\u2014European Machine\nVision Association: Barcelona, Spain, 2021.\n3. Zhang, Z. A Flexible New Technique for Camera Calibration. Pattern Anal. Mach. Intell. IEEE Trans. 2000, 22, 1330\u20131334. [CrossRef] 4. Bradski, G. The OpenCV Library. 2000. Available online: https://www.drdobbs.com/open-source/the-opencv-library/184404319 (accessed on 1 August 2022). 5. Hartley, R.; Zisserman, A. Multiple View Geometry in Computer Vision, 2nd ed.; Cambrige University Press: Cambrige, UK, 2004. 6. Hanning, T. High Precision Camera Calibration: Habilitation Thesis; Springer Fachmedien, Vieweg+Teubner Verlag: Wiesbaden, Germany, 2011. 7. Schilling, H.; Diebold, M.; Gutsche, M.; J\u00e4hne, B. On the design of a fractal calibration pattern for improved camera calibration. Tm-Tech. Mess. 2017, 84, 440\u2013451. [CrossRef] 8. Sch\u00f6ps, T.; Larsson, V.; Pollefeys, M.; Sattler, T. Why Having 10,000 Parameters in Your Camera Model is Better Than Twelve.\nProc. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 13\u201319 June 2020; pp. 2535\u20132544.\n9. Hannemose, M.; Wilm, J.; Frisvad, J.R. Superaccurate camera calibration via inverse rendering. In Proceedings of the SPIE 11057, Modeling Aspects in Optical Metrology VII, Munich, Germany, 21 June 2019; p. 1105717. [CrossRef] 10. Huang, L.; Zhang, Q.; Asundi, A. Camera calibration with active phase target: Improvement on feature detection and optimization. Opt. Lett. 2013, 38, 1446\u20131448. [CrossRef] 11. Schmalz, C.; Forster, F.; Angelopoulou, E. Camera calibration: Active versus passive targets. Opt. Eng. 2011, 50, 113601. 12. Fischer, M.; Petz, M.; Tutsch, R. Vorhersage des Phasenrauschens in optischen Messsystemen mit strukturierter Beleuchtung. Tm-Tech. Mess. 2012, 79, 451\u2013458. [CrossRef] 13. Grossberg, M.D.; Nayar, S.K. A general imaging model and a method for finding its parameters. In Proceedings of the 8th IEEE International Conference on Computer Vision (ICCV), Vancouver, BC, Canada, 7\u201314 July 2001; pp. 108\u2013115. 14. Ramalingam, S.; Sturm, P.F.; Lodha, S.K. Towards Complete Generic Camera Calibration. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), San Diego, CA, USA, 20\u201325 June 2005; IEEE Computer Society: Washington, DC, USA, 2005; pp. 1093\u20131098. [CrossRef]\n15. Bothe, T.; Gesierich, A.; Li, W.; Schulte, M. Verfahren und Vorrichtung zur Kalibrierung einer optischen Einrichtung. German Patent DE102005061931A1, 28 June 2007. 16. Bothe, T.; Li, W.; Schulte, M.; von Kopylow, C.; Bergmann, R.B.; Jueptner, W.P.O. Vision ray calibration for the quantitative geometric description of general imaging and projection optics in metrology. Appl. Opt. 2010, 49, 5851\u20135860. [CrossRef] 17. Reh, T.; Li, W.; Burke, J.; Bergmann, R.B. Improving the generic camera calibration technique by an extended model of calibration display. J. Europ. Opt. Soc. Rap. Public. 2014, 9, 14044. [CrossRef] 18. Dierke, H.; Petz, M.; Tutsch, R. Photogrammetric determination of the refractive properties of liquid crystal displays. In Forum Bildverarbeitung 2018; L\u00e4ngle, T., Puente Le\u00f3n, F., Heizmann, M., Eds.; KIT Scientific Publishing: Karlsruhe, Germany, 2018; pp. 13\u201324. 19. Salvi, J.; Armangue, X.; Batlle, J. A comparative review of camera calibrating methods with accuracy evaluation. Pattern Recognit. 2002, 35, 1617\u20131635. [CrossRef] 20. Bogdan, O.; Eckstein, V.; Rameau, F.; Bazin, J.C. DeepCalib: A deep learning approach for automatic intrinsic calibration of wide field-of-view cameras. In Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production (CVMP\u201918), London, UK, 13\u201314 December 2018; pp. 1\u201310. [CrossRef] 21. Brunken, H.; G\u00fchmann, C. Deep learning self-calibration from planes. In Proceedings of the 12th International Conference on Machine Vision (ICMV\u201919), Amsterdam, The Netherlands, 31 January 2020; Volume 11433, p. 114333L. [CrossRef] 22. Zhang, J.; Luo, B.; Xiang, Z.; Zhang, Q.; Wang, Y.; Su, X.; Liu, J.; Li, L.; Wang, W. Deep-learning-based adaptive camera calibration for various defocusing degrees. Opt. Lett. 2021, 46, 5537\u20135540. [CrossRef] 23. Kannala, J.; Brandt, S. A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses. IEEE Trans. Pattern Anal. Mach. Intell. 2006, 28, 1335\u20131340. [CrossRef] [PubMed] 24. Popescu, V.; Dauble, J.; Mei, C.; Sacks, E. An Efficient Error-Bounded General Camera Model. In Proceedings of the 3DPVT\u201906: Proceedings of the Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT\u201906), Chapel Hill, NC, USA, 14\u201316 June 2006; IEEE Computer Society: Washington, DC, USA, 2006; pp. 121\u2013128. [CrossRef] 25. Dunne, A.K.; Mallon, J.; Whelan, P.F. Efficient generic calibration method for general cameras with single centre of projection. Comput. Vis. Image Underst. 2010, 114, 220\u2013233. [CrossRef] 26. Sun, Q.; Hou, Y.; Tan, Q. A new method of camera calibration based on the segmentation model. Optik 2013, 124, 6991\u20136995. [CrossRef] 27. Miraldo, P.; Araujo, H. Calibration of smooth camera models. IEEE Trans. Pattern Anal. Mach. Intell. 2013, 35, 2091\u20132103. [CrossRef] 28. Xiang, Z.; Dai, X.; Gong, X. Noncentral catadioptric camera calibration using a generalized unified model. Opt. Lett. 2013, 38, 1367\u20131369. [CrossRef] [PubMed] 29. Pak, A. The concept of smooth generic camera calibration for optical metrology. Tm-Tech. Mess. 2015, 83, 25\u201335. [CrossRef] 30. OpenCV Documentation: Camera Calibration and 3D Reconstruction Section. 2021. Available online: https://docs.opencv.org/4. 5.2/d9/d0c/group__calib3d.html (accessed on 1 August 2022). 31. Nocedal, J.; Wright, S.J. Numerical Optimization; Springer Series in Operations Research; Springer: New York, NY, USA, 1999.\n32. Hastie, T.; Tibshirani, R.; Friedman, J. The Elements of Statistical Learning\u2014Data Mining, Inference, and Prediction, 2nd ed.; Springer: New York, NY, USA, 2017. 33. Werling, S.B.; Mai, M.; Heizmann, M.; Beyerer, J. Inspection of specular and partially specular surfaces. Metrol. Meas. Syst. 2009, 16, 415\u2013431. 34. Jones, E.; Oliphant, T.; Peterson, P. SciPy: Open Source Scientific Tools for Python. 2022. Available online: http://www.scipy.org (accessed on 1 August 2022). 35. Maclaurin, D.; Duvenaud, D.; Johnson, M.; Townsend, J. AutoGrad Package for Python. 2022. Available online: https://github.com/ HIPS/autograd (accessed on 1 August 2022)."
        }
    ],
    "title": "Machine-Learning-Inspired Workflow for Camera Calibration",
    "year": 2022
}