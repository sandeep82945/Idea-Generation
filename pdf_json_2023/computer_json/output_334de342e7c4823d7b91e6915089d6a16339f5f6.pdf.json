{
    "abstractText": "Spatio-temporal graph neural networks (STGNN) have become the most popular solution to traffic forecasting. While successful, they rely on the message passing scheme of GNNs to establish spatial dependencies between nodes, and thus inevitably inherit GNNs\u2019 notorious inefficiency. Given these facts, in this paper, we propose an embarrassingly simple yet remarkably effective spatio-temporal learning approach, entitled SimST. Specifically, SimST approximates the efficacies of GNNs by two spatial learning techniques, which respectively model local and global spatial correlations. Moreover, SimST can be used alongside various temporal models and involves a tailored training strategy. We conduct experiments on five traffic benchmarks to assess the capability of SimST in terms of efficiency and effectiveness. Empirical results show that SimST improves the prediction throughput by up to 39 times compared to more sophisticated STGNNs while attaining comparable performance, which indicates that GNNs are not the only option for spatial modeling in traffic forecasting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xu Liu"
        },
        {
            "affiliations": [],
            "name": "Yuxuan Liang"
        },
        {
            "affiliations": [],
            "name": "Chao Huang"
        },
        {
            "affiliations": [],
            "name": "Hengchang Hu"
        },
        {
            "affiliations": [],
            "name": "Yushi Cao"
        },
        {
            "affiliations": [],
            "name": "Bryan Hooi"
        },
        {
            "affiliations": [],
            "name": "Roger Zimmermann"
        }
    ],
    "id": "SP:2590128c86ccf9ec7ce0eea6920a982d4c9472a4",
    "references": [
        {
            "authors": [
                "U. Alon",
                "E. Yahav"
            ],
            "title": "On the bottleneck of graph neural networks and its practical implications",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "J.T. Ash",
                "C. Zhang",
                "A. Krishnamurthy",
                "J. Langford",
                "A. Agarwal"
            ],
            "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "L. Bai",
                "L. Yao",
                "C. Li",
                "X. Wang",
                "C. Wang"
            ],
            "title": "Adaptive graph convolutional recurrent network for traffic forecasting",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "D. Cao",
                "Y. Wang",
                "J. Duan",
                "C. Zhang",
                "X. Zhu",
                "C. Huang",
                "Y. Tong",
                "B. Xu",
                "J. Bai",
                "J Tong"
            ],
            "title": "Spectral temporal graph neural network for multivariate time-series forecasting",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "T. Ma",
                "C. Xiao"
            ],
            "title": "Fastgcn: fast learning with graph convolutional networks via importance sampling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "Y. Sui",
                "X. Chen",
                "A. Zhang",
                "Z. Wang"
            ],
            "title": "A unified lottery ticket hypothesis for graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "I. Segovia",
                "Y.R. Gel"
            ],
            "title": "Z-gcnets: time zigzags at graph convolutional networks for time series forecasting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "I. Segovia-Dominguez",
                "B. Coskunuzer",
                "Y. Gel"
            ],
            "title": "Tamp-s2gcnets: coupling time-aware multipersistence knowledge representation with spatio-supra graph convolutional networks for time-series forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Chiang",
                "W.-L",
                "X. Liu",
                "S. Si",
                "Y. Li",
                "S. Bengio",
                "Hsieh",
                "C.-J"
            ],
            "title": "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
            "venue": "In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "J. Choi",
                "H. Choi",
                "J. Hwang",
                "N. Park"
            ],
            "title": "Graph neural controlled differential equations for traffic forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chung",
                "C. Gulcehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555,",
            "year": 2014
        },
        {
            "authors": [
                "M. Defferrard",
                "X. Bresson",
                "P. Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Fang",
                "Q. Long",
                "G. Song",
                "K. Xie"
            ],
            "title": "Spatial-temporal graph ode networks for traffic flow forecasting",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "S. Guo",
                "Y. Lin",
                "N. Feng",
                "C. Song",
                "H. Wan"
            ],
            "title": "Attention based spatial-temporal graph convolutional networks for traffic flow forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "L. Han",
                "B. Du",
                "L. Sun",
                "Y. Fu",
                "Y. Lv",
                "H. Xiong"
            ],
            "title": "Dynamic and multi-faceted spatio-temporal deep learning for traffic speed forecasting",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "X. He",
                "L. Liao",
                "H. Zhang",
                "L. Nie",
                "X. Hu",
                "Chua",
                "T.-S"
            ],
            "title": "Neural collaborative filtering",
            "venue": "In Proceedings of the 26th international conference on world wide web,",
            "year": 2017
        },
        {
            "authors": [
                "N.S. Keskar",
                "D. Mudigere",
                "J. Nocedal",
                "M. Smelyanskiy",
                "P.T.P. Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "S. Lan",
                "Y. Ma",
                "W. Huang",
                "W. Wang",
                "H. Yang",
                "P. Li"
            ],
            "title": "Dstagnn: Dynamic spatial-temporal aware graph neural network for traffic flow forecasting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "M. Li",
                "Z. Zhu"
            ],
            "title": "Spatial-temporal fusion graph neural networks for traffic flow forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Li",
                "R. Yu",
                "C. Shahabi",
                "Y. Liu"
            ],
            "title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "X. Liu",
                "Y. Liang",
                "C. Huang",
                "Y. Zheng",
                "B. Hooi",
                "R. Zimmermann"
            ],
            "title": "When do contrastive learning signals help spatio-temporal graph forecasting",
            "venue": "In Proceedings of the 30th International Conference on Advances in Geographic Information Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Oord",
                "A. v. d",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "In The 9th ISCA Speech Synthesis Workshop,",
            "year": 2016
        },
        {
            "authors": [
                "B. Pan",
                "U. Demiryurek",
                "C. Shahabi"
            ],
            "title": "Utilizing realworld transportation data for accurate traffic prediction",
            "venue": "In 12th IEEE International Conference on Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "Z. Pan",
                "Y. Liang",
                "W. Wang",
                "Y. Yu",
                "Y. Zheng",
                "J. Zhang"
            ],
            "title": "Urban traffic prediction from spatio-temporal data using deep meta learning",
            "venue": "In Proceedings of the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "D.I. Shuman",
                "S.K. Narang",
                "P. Frossard",
                "A. Ortega",
                "P. Vandergheynst"
            ],
            "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2013
        },
        {
            "authors": [
                "C. Song",
                "Y. Lin",
                "S. Guo",
                "H. Wan"
            ],
            "title": "Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "C. Zhang",
                "Z. Guo",
                "X. Zhang",
                "N.V. Chawla"
            ],
            "title": "Nosmog: Learning noise-robust and structure-aware mlps on graphs",
            "venue": "arXiv preprint arXiv:2208.10010,",
            "year": 2022
        },
        {
            "authors": [
                "H. Toda"
            ],
            "title": "Vector autoregression and causality",
            "venue": "Yale University,",
            "year": 1991
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wu",
                "S. Pan",
                "G. Long",
                "J. Jiang",
                "C. Zhang"
            ],
            "title": "Graph wavenet for deep spatial-temporal graph modeling",
            "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "B. Yu",
                "H. Yin",
                "Z. Zhu"
            ],
            "title": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
            "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "H. Zeng",
                "H. Zhou",
                "A. Srivastava",
                "R. Kannan",
                "V. Prasanna"
            ],
            "title": "Graphsaint: Graph sampling based inductive learning method",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhang",
                "L. Yao",
                "A. Sun",
                "Y. Tay"
            ],
            "title": "Deep learning based recommender system: A survey and new perspectives",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhang",
                "Y. Liu",
                "Y. Sun",
                "N. Shah"
            ],
            "title": "Graph-less neural networks: Teaching old mlps new tricks via distillation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "C. Zheng",
                "X. Fan",
                "C. Wang",
                "J. Qi"
            ],
            "title": "Gman: A graph multi-attention network for traffic prediction",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "C. Zheng",
                "B. Zong",
                "W. Cheng",
                "D. Song",
                "J. Ni",
                "W. Yu",
                "H. Chen",
                "W. Wang"
            ],
            "title": "Robust graph representation learning via neural sparsification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zheng",
                "L. Capra",
                "O. Wolfson",
                "H. Yang"
            ],
            "title": "Urban computing: concepts, methodologies, and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In recent years, urban traffic forecasting has emerged as one of the most important components of Intelligent Transportation Systems. Given historical traffic observations (e.g., traffic speed, flow) collected from sensors on road networks, the task focuses on predicting future traffic trends for each sensor, which provides insights for improving urban planning and traffic management (Zheng et al., 2014). Spatio-Temporal Graph Neural Networks (STGNNs) have become the de facto most popular tool for traffic forecasting, in which sequential models such as Temporal Convolution Networks (TCNs) or Recurrent Neural Networks (RNNs)\n1National University of Singapore 2University of Hong Kong 3Nanyang Technological University. Correspondence to: Xu Liu <liuxu@comp.nus.edu.sg>, Yuxuan Liang <yuxliang@outlook.com>.\nPreprint. Under review.\nare applied for modeling temporal dependencies (Yu et al., 2018; Pan et al., 2019; Wu et al., 2020; Lan et al., 2022), and Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Defferrard et al., 2016) are utilized to capture spatial correlations among different locations.\nAfter revisiting the STGNNs proposed in recent years, we find that they mostly focus on enhancing the spatial learning modules (i.e., GNNs) with either complex aggregation rules or sophisticated layers to improve predictive performance. While successful, we note that they rely heavily on GNNs for performing the message passing step and thus inevitably inherit GNNs\u2019 notorious inefficiencies, especially when the graphs are large or with dense connections (Chen et al., 2021a; Zhang et al., 2022). As a concrete example, the commonly used adaptive adjacency matrix is built by matrix multiplication of two node embedding tables, producing a fully-connected graph (Wu et al., 2019; Bai et al., 2020; Han et al., 2021). During feature aggregation, such a fully-connected structure leads to quadratic computational complexity w.r.t. the number of sensors. Consequently, the scalability challenges of GNNs hinder the deployment of STGNNs in large-scale and real-time traffic forecasting systems that are latency-bound and require fast inference.\nAlthough there are plenty of efforts in the graph domain to improve the efficiency of GNNs, such as via graph simplification (Hamilton et al., 2017; Chen et al., 2018; Chiang et al., 2019; Zeng et al., 2020; Zheng et al., 2020b), the related studies in STGNNs are still scarce to the best of our knowledge, as such simplification usually leads to information loss and performance degradation (Wu et al., 2020). Given these facts and inspired by recent progress in eliminating GNNs for node classification (Zhang et al., 2022; Tian et al., 2022), we may ask: can we remove GNNs to trim down the explosive complexity while still remaining competitive in traffic forecasting accuracy?\nPresent Work To answer this question, we first demonstrate the functionalities of GNNs as follows. The superior performance of GNNs stems from its structure-aware exploitation of graphs: (1) for each node in the graph, a single GNN layer is used to first aggregate features from nodes\u2019 neighbors and then transform the aggregated representations via a feed-forward network, and (2) by stacking multiple layers, the hidden representations of nodes receive messages from long-distance neighbors.\nar X\niv :2\n30 1.\n12 60\n3v 1\n[ cs\n.L G\n] 3\n0 Ja\nn 20\n23\nIn this work, we propose two spatial learning modules to approximate the above efficacies of GNNs without requiring message passing, reducing the time complexity to linear. (1) Local Proximity Modeling. We take a local view and fragment the traffic network to build an ego-graph for each node, which is constructed by incorporating historical observations of the node\u2019s neighbors. Then an MLP is applied to transform the observations to the hidden states at each time step. (2) Global Correlation Learning. Inspired by recommender systems that learn user embeddings to reflect user behavioral similarities (He et al., 2017; Zhang et al., 2019), we propose to use sensor embeddings to represent sensors\u2019 inherent properties and collaboratively capture spatial relationships between arbitrary sensor pairs in a data-driven manner. Compared with stacking a number of GNN layers to enlarge the receptive field and build long-range spatial dependencies, our module sets up direct connections between nodes and thereby learns the global correlations.\nIn practice, GNNs can be used alongside various temporal models, such as GRU (Chung et al., 2014), WaveNet (Oord et al., 2016), and Transformer (Vaswani et al., 2017), for spatio-temporal learning. Analogously, we empirically find that our proposed spatial modules are agnostic to the temporal encoders and work in a plug-and-play manner. Moreover, we devise a new training strategy for SimST to further boost performance, which increases sample diversity during batch formation and enhances generalization.\nIntegrating the above components, we propose a Simple yet effective Spatio-Temporal learning approach termed SimST, which trims the quadratic cost and performs on par with STGNNs in empirical studies. Our contributions are summarized as follows. \u2022 We for the first time demonstrate that GNNs are not the\nonly choice for spatial modeling in traffic forecasting, by presenting a simple yet admirable method called SimST.\n\u2022 Despite its simplicity, SimST achieves remarkable empirical results in terms of throughput and accuracy against sophisticated state-of-the-art STGNNs: SimST is significantly more efficient than baselines, with up to 39\u00d7 higher inference throughput, while performing on par with them.\n\u2022 We conduct ablation and case studies to promote a better understanding of our method and motivate future research to rethink the importance of GNNs in traffic forecasting."
        },
        {
            "heading": "2. Related Work",
            "text": "Traffic forecasting is a crucial application in smart city efforts. In recent years, STGNNs have become the most widely used tools for predicting traffic (Cao et al., 2020; Liu et al., 2022b; Chen et al., 2021b; 2022). Generally, they integrate GNNs with either RNNs or TCNs to capture the spatial and temporal dependencies in traffic data. For\nexample, DCRNN (Li et al., 2018) considered traffic flow as a diffusion process and combined a novel diffusion convolution with GRU. Other efforts (Pan et al., 2019; Fang et al., 2021; Liu et al., 2022a) were also based on RNNs. To improve training speed and enjoy parallel computation, plenty of works (Wu et al., 2019; 2020; Li & Zhu, 2021) replaced RNNs with dilated causal convolution.\nA fundamental problem for using GNNs in traffic modeling is how to establish a graph structure. The mainstream approach to define such structures is using either a predefined and sparse matrix constructed from the road network distances between sensors (Yu et al., 2018; Li et al., 2018; Song et al., 2020), or an adaptive and dense matrix that records the pairwise relationships between nodes (Wu et al., 2019; Bai et al., 2020; Han et al., 2021; Choi et al., 2022). However, researchers have noted that the predefined matrix is heuristic and does not reflect the genuine dependencies between nodes, which degrades model performance (Wu et al., 2019; Bai et al., 2020). Models applying the adaptive matrix generally achieve superior performance. Though successful, the adaptive matrix discards the sparsity of the graph and incurs a quadratic computational cost, making it hard to deploy for latency-constrained or large-scale traffic applications. Other methods that use attention mechanisms for spatial learning (Zheng et al., 2020a) also suffer from quadratic time complexity. In this study, we propose SimST to alleviate the inefficiency issue by eliminating the inefficient GNN component in the model."
        },
        {
            "heading": "3. Preliminary",
            "text": "Traffic Forecasting Let G = (V, E) represent a directed sensor graph with |V| nodes and |E| edges, and XT \u2208 R|V|\u00d7T\u00d7F denote the features of all nodes from time step 1 to T , where F is the feature dimension. The features usually consist of a target attribute (e.g., traffic speed) and other auxiliary information, such as time of day (Wu et al., 2019). Following common settings (Li et al., 2018; Wu et al., 2019), a weighted adjacency matrix A \u2208 R|V|\u00d7|V| is applied to describe the graph topology, where Aij = exp(\u2212dist(vi,vj) 2\ns2 ) if dist(vi, vj) 6 r else Aij = 0, dist(vi, vj) denotes the road network distance between sensors vi and vj , s is the standard deviation of distances, and r is a threshold for sparsity (Shuman et al., 2013). The non-zero entries in A form the set of edges in E .\nIn traffic forecasting, we aim to learn a neural network \u0398 to predict the target attribute in future Tf steps based on Th historical observations over the sensor graph:\nG,XTh \u0398\u2212\u2192 Y\u0302 Tf (1)\nwhere XTh \u2208 R|V|\u00d7Th\u00d7F indicates the observations and Y\u0302 Tf \u2208 R|V|\u00d7Tf\u00d71 is the predictions. A prediction loss, e.g.,\nmean absolute error, is utilized to train the neural network:\nL(\u0398) = 1 |V| \u2211 vi\u2208V |Y\u0302 Tf vi \u2212 Y Tf vi | (2)\nGraph Neural Networks Although various forms of GNNs exist, our work refers to the conventional message passing scheme (Kipf & Welling, 2017) that is widely adopted in traffic forecasting. Specifically, message passing functions in two ways: aggregating one-hop neighbors to model local correlations, and stacking layers to build longrange relationships. Formally, the node representations H l at the l-th layer of GNNs are learned by first aggregating messages from node neighborsN (v), and then transforming the representations via feed-forward propagation:\nH l = \u03c3(A\u0303H l\u22121W l) (3)\nwhere A\u0303 denotes the normalized adjacency matrix, W l denotes the learnable weights at the l-th layer, and \u03c3 is the activation function (e.g., ReLU)."
        },
        {
            "heading": "4. Methodology",
            "text": "As a typical spatio-temporal data mining task, traffic forecasting requires modeling both spatial and temporal aspects. In this section, we start by describing how spatial correlations are set up by the proposed spatial learning modules in Section 4.1. We then introduce three temporal encoders to capture the temporal dependencies and a predictor for generating the future in Section 4.2. Finally, we present a node-based batch sampling method for training SimST in Section 4.3. Figure 1 depicts the architecture of SimST."
        },
        {
            "heading": "4.1. Spatial Learning Modules",
            "text": "The inefficiency of GNNs has been extensively discussed within the graph domain (Chiang et al., 2019; Chen et al., 2021a; Zhang et al., 2022). Relying on GNNs to set up spatial correlations via message passing, STGNNs also suffer from flagrant inefficiencies. In this work, we propose two simple yet effective alternatives to approximate the function-\nalities of GNNs and thus allow each node to exploit spatial information without message passing.\nLocal Proximity Modeling A single GNN layer is capable of aggregating neighborhood features in a non-Euclidean structure. To consider such a locality characteristic without using a GNN layer, we propose to first perform graph fragmentation to generate an ego-graph for each node. The features of each ego-graph are the Th steps historical time series of the center node and its one-hop neighbors. Multihop neighbors can also be incorporated if needed, but we found that incorporating more neighbors may suffer from overfitting in experiments (see Section 5.4.3). We then transform the raw features at each time step to their hidden representations via a multi-layer perceptron (MLP).\nSpecifically, for each node v, we consider its top-k one-hop neighbors based on the weights in the normalized matrix A\u0303, where A\u0303 = D\u0303 \u2212 12 (A + I)D\u0303\n\u2212 12 and D\u0303 is the degree matrix of A + I . We also consider the neighbors in the reversed direction, i.e., the entries in AT , so that the model can perceive the impact from forward and backward neighbors. Let N 1f (v) and N 1b (v) denote the selected one-hop neighbors in two directions. We construct the feature matrix XThGv \u2208 R Th\u00d7(2k+3) of an ego-graph Gv during period Th:\nXThGv = COMBINE(X Th v , {X Th vf : vf \u2208 N 1f (v)},\n{XThvb : vb \u2208 N 1 b (v)},X Th avgf ,XThavgb) (4)\nwhere the last two terms are the average histories of all onehop neighbors in two directions, allowing the center node to learn more about its local context. Note that the execution of building ego-graphs can be completed in data preprocessing, so it has no influence on model training and inference. Then, we apply an MLP to transform the raw features in XThGv and generate the representations H Th Gv \u2208 RTh\u00d7Dm , where Dm is the model hidden dimension. Compared with a GNN layer, our module replaces message aggregation (i.e., weighted sum) operation with ego-graph construction that can be finished in the data preprocessing stage.\nGlobal Correlation Learning Representing users with unique embeddings and learning with these embeddings to capture similarities in behavior among users, is a fundamental technique in recommender systems (He et al., 2017; Zhang et al., 2019). Along a similar line, we argue that the traffic situation of a sensor is largely influenced by its specific position on a road, e.g., on the mainline or on a ramp. This is an intrinsic, time-invariant property of the sensor: hence, we propose to represent it using static sensor location embeddings. During end-to-end training, the embedding table collaboratively learns spatial relationships between arbitrary node pairs in a data-driven manner. Apart from capturing global correlations, this module can serve as a complement to the local proximity modeling function, which is particularly useful when the connections in A are noisy (Wu et al., 2019; Bai et al., 2020). A detailed case study is conducted in Section 5.5, where we show that the learned sensor embeddings contain significant information for sensor relevance reasoning.\nCompared with stacking multiple GNN layers to enlarge the receptive field and establish long-range correlations, our proposed module uses embedding similarity to reflect the relationships between arbitrary sensor pairs, and thus further considers global knowledge. Moreover, the connections between sensors are built in a direct way, which keeps our module from over-squashing (Alon & Yahav, 2021). As for implementation, we randomly initialize a low-dimensional embedding for each sensor, leading to an embedding table E \u2208 R|V|\u00d7Dn , where Dn is the node embedding size. To fuse the embeddings with model hidden representations, we apply an MLP to map E to H \u2208 R|V|\u00d7Dm .\nComplexity Comparison We provide a complexity comparison between the spatial learning modules here. As GCN (Kipf & Welling, 2017) is the widely adopted form of GNNs in spatio-temporal models, we take it as a baseline. Suppose a L-layer GCN with fixed hidden features Dm is applied. For STGNNs applying the predefined adjacency matrix, the time complexity isO(L|E|Dm+L|V|D2m). When applying the adaptive adjacency matrix that boosts performance, the complexity becomes O(L|V|2Dm + L|V|D2m). In SimST, the complexity of proximity modeling is O(|V|R), where R is the average degree of nodes. The complexity of correlation learning is O(|V|DnDm). It is obvious that SimST has lower complexity and is capable of supporting forecasting applications on large-scale sensor networks."
        },
        {
            "heading": "4.2. Temporal Encoder & Predictor",
            "text": "There are several viable options for building the temporal dependencies of a node\u2019s history. To demonstrate that SimST can be generalized to various temporal models, we incorporate three basic and popular backbones in this study: GRU (Chung et al., 2014) (RNN-based), WaveNet (Oord\net al., 2016) (TCN-based), and Transformer (Vaswani et al., 2017). The representations HThGv \u2208 R\nTh\u00d7Dm are the input of the temporal encoder. For GRU and WaveNet, they compress the temporal dimension to 1, generating the output hGv \u2208 RDm , while for Transformer, the resulting representation still has the same shape as HThGv . We only take the last time step as the final output.\nMoreover, since the standard Transformer is aware of all input time steps during self-attention and the traffic status at the current step is in fact not conditioned on its future, we follow WaveNet to introduce causality into the attention mechanism, leading to the design of the Causal Transformer. This ensures that the model cannot violate the temporal order of inputs and such causality can be easily implemented by masking the specific entries in the attention map. Finally, we concatenate the spatio-temporal summary hGv \u2208 RDm of node v with its location embedding Hv \u2208 RDm , which is then fed into the predictor (implemented as an MLP) for generating the forecasting results of node v: Y\u0302 Tf v = MLP(hGv ||Hv)."
        },
        {
            "heading": "4.3. Node-based Batch Sampling for Training SimST",
            "text": "When training an STGNN, the input data are usually a fourdimensional tensor XTh \u2208 RB\u00d7|V|\u00d7Th\u00d7F , where B denotes the set batch size. That said, nodes are strictly bound together and the batch sampling unit is the entire graph. However, directly applying this graph-based sampling to SimST leads to two issues. First, since SimST is a singlenode architecture, the actual batch size B\u2217 for SimST is B \u00d7 |V|. For example, there are 883 nodes in the PeMSD7 dataset (Song et al., 2020) and usually B is set to 64, then B\u2217 = 56,512. Such a large number reduces the noise in the gradient estimation and leads to a degenerated generalization (Keskar et al., 2017). Also, when |V| is large, it can easily cause GPU out-of-memory problems. Second, we argue that the graph-based sampling method significantly diminishes sample diversity when forming mini-batches. For example, the traffic flow of all nodes from 8 to 9 a.m. on January 26 will always appear in the same batch.\nTo tackle these issues, we propose to remove the restriction on nodes and change the batch sampling unit from a graph to a single node. Note that the same node at different time periods is viewed as different instances. Consequently, the input to SimST becomes XTh \u2208 RB\u2217\u00d7Th\u00d7F , where B\u2217 B \u00d7 |V|. The benefits are two-fold. First, our solution provides flexibility to choose B\u2217, which can be disproportionate to |V| and thus allows for smaller values like 256. This property enhances generalization (Keskar et al., 2017), and allows SimST to have low GPU memory costs and to easily scale to large graphs. Second, our approach provides more sample diversity when forming mini-batches, which yields a generalization improvement (Ash et al., 2020)."
        },
        {
            "heading": "5. Experiments",
            "text": ""
        },
        {
            "heading": "5.1. Experimental Setup",
            "text": "Datasets We conduct experiments on commonly used traffic benchmarks. The details are presented in Table 2. All traffic readings are aggregated into 5-minute windows, resulting in 288 data points per day. Note that the values in traffic flow datasets are generally much larger than those in speed-related datasets. Following (Li et al., 2018; Song et al., 2020; Choi et al., 2022), we use the 12-step historical data to predict the next 12 steps. Z-score normalization is applied to the input data for fast training. We build the adjacency matrix of each dataset by using road network distances with a thresholded Gaussian kernel (Shuman et al., 2013). The threshold r is set to 0 for PeMSD4, PeMSD7, PeMSD8, and 0.1 for LA and BAY. More information is provided in Appendix A.\nBaselines We compare SimST with the following baselines. Historical Average (HA) (Pan et al., 2012) and Vector Autoregression (VAR) (Toda, 1991) are traditional methods. DCRNN (Li et al., 2018), STGCN (Yu et al., 2018) , ASTGCN (Guo et al., 2019), GWNET (Wu et al., 2019), STSGCN (Song et al., 2020), and AGCRN (Bai et al., 2020) are well-known STGNNs. We also incorporate methods that are published recently: STGODE (Fang et al., 2021), STGNCDE (Choi et al., 2022), GMSDR (Liu et al., 2022a).\nImplementation Details SimST is implemented with PyTorch 1.12. There are three variants of SimST based on the applied temporal encoders. We describe the specific configurations for different variants in Appendix B. The following settings are the same for all variants. We train our method via the Adam optimizer with an initial learning rate of 0.001 and a weight decay of 0.0001. We set the maximum epochs to 150 and use an early stop strategy with a patience\nof 20. The batch size is 1,024. The embedding size Dn is 20. The top-k nearest neighbors are set to 3 for LA and BAY, and 0 for PeMSD4, PeMSD7, and PeMSD8 (due to limited available edges). For baselines, we run their codes based on the recommended configurations if their accuracy is not known for a dataset. If known, we use their officially reported accuracy. Experiments are repeated five times with different seeds on an NVIDIA RTX A6000 GPU.\nEvaluation Metrics We adopt three common metrics in forecasting tasks to evaluate the model performance, including mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE). For efficiency measurement, the commonly used floating point operations per second (FLOPS) cannot reflect the \u201creal\u201d running speed of methods because it ignores the effects of parallelization. Hence, we apply the metric of throughput per second (TPS) in this study, which indicates the average number of samples the network can process in one second. We provide a wall-clock time comparison in Appendix D."
        },
        {
            "heading": "5.2. Performance Comparison",
            "text": "In this section, we conduct a model comparison between SimST variants and state-of-the-art STGNNs on five realworld traffic benchmarks. The three variants of SimST are SimST-GRU, SimST-WaveNet (WN), and SimST-Causal Transformer (CT). According to the empirical results in\nTable 1, all variants of SimST achieve competitive performance on the three evaluation metrics of all datasets, which validates the effectiveness of SimST and indicates SimST can generalize to various architectures of temporal models. For comparison among SimST variants: note that for fairness we ensure that the variants have a similar number of parameters (around 150k). SimST-GRU generally achieves comparable performance to state-of-the-art baselines, i.e., GWNET and STGNCDE. SimST-CT also attains competitive accuracy, with little difference from SimST-GRU and GWNET. We notice that while Transformers are generally considered a more powerful model, our results show that a simple GRU works well for traffic prediction. In summary, our results provide strong support for our claim that GNNs are not the only option for spatial modeling in traffic forecasting. Moreover, considering the very different design of SimST compared to other state-of-the-art methods, we hope that this will inspire follow-on studies in graph-less designs.\nAdditionally, we find that: (1) STGNNs approaches surpass HA and VAR by a large margin due to their greater learning capacity. (2) The capability of methods that apply the adaptive matrix, such as GWNET and AGCRN, has been overlooked. They have achieved similar performance to the recently proposed approaches."
        },
        {
            "heading": "5.3. Efficiency Comparison",
            "text": "In this part, we select the top-performing approaches and compare their efficiency with SimST variants. It can be seen from Table 3 that all SimST variants are significantly more efficient than baselines, thanks to their simple model architecture and linear time complexity w.r.t. |V|. Specifically, comparing TCN-based models, SimST-WN has around 45% fewer parameters than GWNET but achieves 3.3\u00d7 \u2013 5.6\u00d7 higher TPS. For RNN-based methods, SimST-GRU is 3.7\u00d7 \u2013 6.1\u00d7 and 19\u00d7 \u2013 26\u00d7 faster than AGCRN and GMSDR, respectively. STGODE and STGNCDE are the recently published methods that are based on neural ordinary differential equations. Though achieving competitive performance, they suffer from significant inefficiency problems. For example,\nSimST-WN has comparable performance to them but is 11\u00d7 \u2013 17\u00d7 and 30\u00d7 \u2013 39\u00d7 faster. Among the variants, SimSTWN achieves the highest TPS due to WaveNet\u2019s superior parallelism capability. SimST-GRU also achieves good TPS results, which we attribute to an optimized implementation in the PyTorch library. See Appendix D for more results on efficiency comparison."
        },
        {
            "heading": "5.4. Ablation Study",
            "text": "We have demonstrated the effectiveness and efficiency of SimST. Next, we select SimST-GRU and SimST-CT as the representatives (due to their preferable performance) and conduct a series of ablation and case studies on one flowbased dataset PeMSD4, and one speed-related dataset LA."
        },
        {
            "heading": "5.4.1. EFFECTS OF SPATIAL LEARNING MODULES",
            "text": "To study the effects of two spatial modules, we consider the following three settings for comparisons: (1) w/o Correlation Learning (CL): we turn off the spatial correlation learning module. (2) w/o Proximity Modeling (PM): for each node, we only input the histories of itself. (3) CT/GRU: we do not perform spatial learning. The results\nare shown in Figure 2. First, we find that removing CL leads to significant degradation of MAE for both SimST-GRU and SimST-CT on both datasets, revealing the great importance of building global correlations under the SimST framework. Second, the benefit of PM is marginal on PeMSD4. This is because the average degree of nodes in PeMSD4 is only 1.1, so the available neighbor information is very limited. In contrast, the average degree of nodes in the LA dataset is 7.3, where PM affects performance more. Third, we notice that removing the CL module has a greater impact on the PeMSD4 dataset than on the LA dataset. The reason is that when neighbor information is scarce, the CL module takes a greater responsibility to supplement neighbor knowledge. Conversely, when there are more neighbors, the influence of CL on performance is less significant."
        },
        {
            "heading": "5.4.2. EFFECTS OF NODE-BASED BATCH SAMPLING",
            "text": "We examine the effects of the node-based sampling method from two aspects. First, we show the influence of the batch size number in the first row of Figure 3. Generally, both SimST-GRU and SimST-CT reach their best performance when setting B\u2217 to 1,024 on the two datasets, revealing the necessity of applying a small B\u2217. The negative influence of\nusing a large batch size such as 19,648, is more significant on the PeMSD4 dataset. Besides, we find that SimST-CT only occupies around 2GB of memory on our GPU across all datasets. In contrast, the memory cost of STGNNs usually grows as |V| increases. For example, on the dataset with the lowest |V| = 170 and the highest |V| = 883, GWNET requires around 3GB and 11GB of memory, respectively.\nSecond, we present the effects of different batch-forming approaches, i.e., graph-based sampling and node-based sampling by drawing the convergence curves in the second and third rows of Figure 3. Note that we ensure fair comparisons by setting the same B\u2217 in experiments. In Figure 3, it can be seen that node-based batch sampling surpasses the counterpart. Concretely, node-based sampling is only slightly better than the graph-based method when a large B\u2217 is applied (e.g., 19,648/13,248). This is because the sample diversity of graph-based sampling is rich enough at this time. As B\u2217 decreases to small values (e.g., 1,228/828), which is necessary to improve performance, the impact of sample diversity becomes significant, i.e., generalization performance is greatly improved, especially on PeMSD4. Also, we find that the curves of node-based sampling are generally more stable than the graph-based method."
        },
        {
            "heading": "5.4.3. HYPERPARAMETER STUDY",
            "text": "In Figure 4, first row, we show the effects of the number of top-k one-hop neighbors. We replace PeMSD4 with the BAY dataset because the average degree of nodes is only 1.1 in PeMSD4. From the figure, we observe that the performance gradually improves when k increases from 0 to 3. But the performance worsens when we set k to 4, indicating a potential overfitting issue. Next, we assess the effects of the sensor embedding dimension size in the second row of Figure 4. We find that both SimST-GRU and SimST-\nCT are not sensitive to the changes in the dimension on both datasets. The results also suggest that a small dimension such as 16, is sufficient to learn correlations between nodes."
        },
        {
            "heading": "5.5. Case Study",
            "text": "In this section, we explore further to understand what exactly the sensor embeddings have learned through a case study that makes connections between embedding similarities and real-world sensor locations. We apply SimSTCT on LA and BAY since only these two datasets provide sensors\u2019 coordinates. Concretely, we first compute pairwise cosine similarities between sensor embeddings, i.e., for vi, vj \u2208 V, similarity =\nEvi \u00b7Evj \u2016Evi\u20162\u2016Evj\u20162 , and pairwise\ngeodesic distances between sensors based on their coordinates. Then we calculate the average cosine similarities within the ranges of 0\u20131, 1\u20135, 5\u201310, 10\u201320, and 20\u201335 kilometers of all sensors, leading to a statistical overview shown in Figure 5 (left part). It can be seen that cosine similarity becomes smaller when the geodesic distance gets larger. This result obeys Tobler\u2019s First Law of Geography, i.e., near things are more related than distant things.\nNext, we look at an example by selecting sensor 62 in LA as the anchor node, which is located at the intersection of two highways. We also visualize its top similar neighbors based on cosine similarity in Figure 5 (upper right). We find that (1) sensors 13 and 58 are the nearby neighbors, which are also included in the adjacency matrix. (2) sensors 201 and 178 are distant nodes. They are considered similar sensors because they are also located at intersections, and thus share similar traffic patterns to the anchor. The situation in the\nBAY dataset is similar to LA (shown in the lower right of Figure 5), so we do not elaborate further."
        },
        {
            "heading": "5.6. Discussion & Future Work",
            "text": "We find that SimST-GRU and SimST-CT perform worse than the state-of-the-art STGNNs (i.e., STGNCDE and GWNET) when we turn off the node-based batch sampling strategy. This result indicates that the proposed two spatial learning modules can only approximate the efficacies of GNNs, and the training strategy of SimST is an indispensable component to obtain comparable performance to the state-of-the-art. In addition, we note that when the neighbor information is relatively rich, such as with LA, SimST-CT is slightly below the state-of-the-art. Therefore, we plan to address this performance gap by utilizing pretrained models and knowledge distillation (Zhang et al., 2022). Lastly, our findings in this work are currently limited to traffic forecasting, while we would like to expand SimST to other spatio-temporal applications such as air quality prediction."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we propose SimST, a simple, effective, and efficient spatio-temporal learning method for traffic forecasting. It approximates GNNs with two proposed spatial learning modules, involves a node-based batch sampling strategy, and is temporal-model-agnostic. Our study suggests that message passing is not the only effective way of modeling spatial relations in traffic forecasting; hence, we hope to spur the development of new model designs with both high efficiency and effectiveness in the community."
        },
        {
            "heading": "A. Datasets",
            "text": "We conduct experiments on the traffic datasets of PeMSD41, PeMSD71, PeMSD81, LA2 and BAY2.\n\u2022 PEMS-04 (Song et al., 2020): The dataset refers to the traffic flow in San Francisco Bay Area. There are 307 sensors and the period ranges from Jan. 1 - Feb. 28, 2018.\n\u2022 PEMS-07 (Song et al., 2020): This dataset involves traffic flow readings collected from 883 sensors, and the time range is from May 1 - Aug. 6, 2017.\n\u2022 PEMS-08 (Song et al., 2020): The dataset contains traffic flow information collected from 170 sensors in the San Bernardino area from Jul. 1 - Aug. 31, 2016.\n\u2022 LA (Li et al., 2018): The dataset records the traffic speed collected from 207 loop detectors in the highway of Los Angeles County, ranging from Mar. 1 - Jun. 27, 2012.\n\u2022 BAY (Li et al., 2018): This dataset contains traffic speed information from 325 sensors in the Bay Area. It has 6 months of data ranging from Jan. 1 - Jun. 30, 2017.\nThe datasets are split into three parts for training, validation, and testing with a ratio of 6:2:2 on PeMSD4, PeMSD7, and PeMSD8, and 7:1:2 on LA and BAY. The statistics of data partition are in Table 4."
        },
        {
            "heading": "B. Configuration of SimST Variants",
            "text": ""
        },
        {
            "heading": "C. Additional Performance Comparison",
            "text": "In Figure 6, we provide a visual comparison of throughput, accuracy, and parameter size between top-performing STGNNs and SimST. It can be seen that SimST variants are substantially more efficient and use much fewer parameters than state-of-the-art STGNNs, while achieving comparable performance. Additionally, we notice that adding an auxiliary feature \u2013 day of week can significantly improve the predictive accuracy, as shown in Table 6. The reason is that there is a significant difference in traffic patterns between weekdays and weekends. Therefore, this feature is a powerful prior knowledge for the model, and future research may consider incorporating this feature to enhance performance."
        },
        {
            "heading": "D. Wall-Clock Time Comparison",
            "text": "In Table 7, we provide the training and inference time per epoch of top-performing baselines and SimST variants, to enable straightforward efficiency comparison. We observe that SimST variants have significantly shorter inference time, thanks to their linear complexity w.r.t the number of nodes and small parameter size. As for training efficiency, although SimST-GRU applies a small actual batch size B\u2217 = 1, 024, which is necessary to achieve better predictive performance, its training time is still comparable to the fastest two baselines GWNET and AGCRN. Furthermore, we notice that methods based on ordinary differential equations (STGODE and STGNCDE) require significant training and inference time, which largely affects their practical applications in the real world.\nMoreover, since we need to perform ego-graph construction for the proximity modeling module, we provide details of the time spent here based on our current implementation: 41.9 seconds on PeMSD4, 206.5 seconds on PeMSD7, 28.8 seconds on PeMSD8, 284.1 seconds on LA, and 716.1 seconds on BAY. Note that this construction process only need to run once, because such process is deterministic and we can store the processed data and load them during training and inference."
        },
        {
            "heading": "E. Learning Curve Comparison",
            "text": "In this section, we first scale up the parameter size of SimSTGRU from 130k to 340k (denoted as SimST-GRU-L), to ensure our method has comparable parameters to the baselines. Then in Figure 7, we compare the learning curves between top-performing baselines and SimST-GRU-L. It can be seen that SimST-GRU-L exhibits very fast convergence rate: it drops to a small validation MAE within a few epochs, and almost converges in around 20 epochs. In addition, we observe that the standard deviation of SimST is generally smaller than those in baselines, indicating the stability provided by our approach."
        }
    ],
    "title": "Do We Really Need Graph Neural Networks for Traffic Forecasting?",
    "year": 2023
}