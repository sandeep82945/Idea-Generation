{
    "abstractText": "Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, kNearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs\u2019 classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and fully-supervised settings, respectively, across eight diverse end-tasks. We hope our exploration will encourage the community to revisit the power of classical methods for efficient NLP1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Li"
        },
        {
            "affiliations": [],
            "name": "Jing Chen"
        },
        {
            "affiliations": [],
            "name": "Botzhong Tian"
        },
        {
            "affiliations": [],
            "name": "Ningyu Zhang"
        }
    ],
    "id": "SP:cd39ffd3c3d116137735d5525f86a5836bc221e2",
    "references": [
        {
            "authors": [
                "Uri Alon",
                "Frank F. Xu",
                "Junxian He",
                "Sudipta Sengupta",
                "Dan Roth",
                "Graham Neubig"
            ],
            "title": "Neuro-symbolic language modeling with automaton-augmented retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Alt",
                "Aleksandra Gabryszak",
                "Leonhard Hennig."
            ],
            "title": "TACRED revisited: A thorough evaluation of the TACRED relation extraction task",
            "venue": "Proceedings of ACL 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Oren Boiman",
                "Eli Shechtman",
                "Michal Irani."
            ],
            "title": "In defense of nearest-neighbor based image classification",
            "venue": "pages 1\u20138. IEEE.",
            "year": 2008
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel M. Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Xin Xie",
                "Shumin Deng",
                "Yunzhi Yao",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction",
            "venue": "CoRR, abs/2104.07650.",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "Proceedings of Sinn und Bedeutung.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume",
            "year": 2019
        },
        {
            "authors": [
                "Jerome H Friedman."
            ],
            "title": "The elements of statistical learning: Data mining, inference, and prediction",
            "venue": "springer open.",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Search engine guided neural machine translation",
            "venue": "Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5133\u20135140. AAAI Press.",
            "year": 2018
        },
        {
            "authors": [
                "Junxian He",
                "Graham Neubig",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Efficient nearest neighbor language models",
            "venue": "Proc. of EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Iris Hendrickx",
                "Su Nam Kim",
                "Zornitsa Kozareva",
                "Preslav Nakov",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Sebastian Pad\u00f3",
                "Marco Pennacchiotti",
                "Lorenza Romano",
                "Stan Szpakowicz."
            ],
            "title": "SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
            "venue": "Proceedings of SemEval, pages 33\u201338.",
            "year": 2010
        },
        {
            "authors": [
                "Nora Kassner",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Bert-knn: Adding a knn search component to pretrained language models for better QA",
            "venue": "Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Angela Fan",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Nearest neighbor machine translation",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Kamran Kowsari",
                "Kiana Jafari Meimandi",
                "Mojtaba Heidarysafa",
                "Sanjana Mendu",
                "Laura Barnes",
                "Donald Brown."
            ],
            "title": "Text classification algorithms: A survey",
            "venue": "Information, 10(4):150.",
            "year": 2019
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Ruotian Ma",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "KNN-BERT: fine-tuning pretrained models with KNN classifier",
            "venue": "CoRR, abs/2110.02523.",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Yuxian Meng",
                "Shi Zong",
                "Xiaoya Li",
                "Xiaofei Sun",
                "Tianwei Zhang",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "GNN-LM: language modeling based on global contexts via GNN",
            "venue": "CoRR, abs/2110.08743.",
            "year": 2021
        },
        {
            "authors": [
                "Guoshun Nan",
                "Zhijiang Guo",
                "Ivan Sekuli\u0107",
                "Wei Lu."
            ],
            "title": "Reasoning with latent structure refinement for document-level relation extraction",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Emin Orhan."
            ],
            "title": "A simple cache model for image recognition",
            "venue": "31:10107\u201310116.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever."
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "OpenAI.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100, 000+ questions for machine comprehension of text",
            "venue": "Jian Su, Xavier Carreras, and Kevin Duh, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2383\u20132392. The Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Weijia Shi",
                "Julian Michael",
                "Suchin Gururangan",
                "Luke Zettlemoyer."
            ],
            "title": "Nearest neighbor zero-shot inference",
            "venue": "CoRR, abs/2205.13792.",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1631\u20131642. ACL.",
            "year": 2013
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "the 23rd annual international ACM SIGIR conference on Research and development in information retrieval.",
            "year": 2000
        },
        {
            "authors": [
                "Yan Wang",
                "Wei-Lun Chao",
                "Kilian Q Weinberger",
                "Laurens van der Maaten."
            ],
            "title": "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning",
            "venue": "arXiv preprint arXiv:1911.04623.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1112\u20131122. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Xin Xie",
                "Xiang Chen",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Xu Cheng",
                "Huajun Chen."
            ],
            "title": "Reasoning through memorization: Nearest neighbor knowledge graph embeddings",
            "venue": "CoRR, abs/2201.05575.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\neager learners\nlazy learners\nData\nTraining Predicting\nOutput\nOutput\nPredicting -NN\nFigure 1: Revisiting how does a lazy learner (k-NN) help the eager learner (PLM).\nPre-trained Language Models (PLMs) (Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020) have shown superior performance across a wide range of language-related downstream tasks (Kowsari et al., 2019; Nan et al., 2020). Afterward, the conventional paradigm fine-tuning, which extends extra task-specific classifiers on the top of PLMs, has been proposed to apply PLMs for downstream tasks. Recently, a new paradigm called prompt-tuning, which originated from GPT-3 (Brown et al., 2020), has been introduced and has shown better results for PLMs on few-shot and zero-shot tasks. Finetuning has proved to be effective on supervised tasks and is widely used as the standard method for natural language processing (NLP). Despite the effectiveness of adapting PLMs, parametric-based eager learners (Friedman, 2017), like PLMs with neural networks, require estimating the model parameters\n\u2217 Corresponding Author. 1Code and datasets are available in https://github.com/zjunlp/Revisit-KNN.\nar X\niv :2\n30 4.\n09 05\n8v 2\n[ cs\n.C L\n] 1\n8 Ju\nn 20\n23\nwith an intensive learning stage. Besides, Training a large PLM model can require significant computing resources and energy, which have negative environmental consequences. As a result, there has been a growing interest in developing more efficient and sustainable methods for training and deploying PLMs.\nA stark contrast to PLMs is the k-NN classifier: a simplest machine learning algorithm that does not have a training phase but simply predicts labels based on the nearest training examples instead. NLP researchers (Khandelwal et al., 2020; He et al., 2021) have found that k-NN enable excellent unconditional language modeling (Khandelwal et al., 2020; He et al., 2021) during test phrase. According the definition in (Friedman, 2017), k-NN is actually a lazy learner that can avoid over-fitting of parameters (Boiman et al., 2008) and effectively smooths out the impact of isolated noisy training data (Orhan, 2018). Though k-NN has the above advantages, previous works only leverage k-NN for testing, and there is no systematic examination of the full utilization of k-NN for PLMs.\nTo this end, we have conducted a comprehensive and in-depth empirical study of the k-NN classifier for natural language understanding (NLU). Our approach involves leveraging the predictive results of a k-NN classifier and augmenting conventional parametric PLM classifiers in two steps: (1) We explore the role of k-NN as prior knowledge for calibrating training by using k-NN results as an indicator of easy vs. hard examples in the training set; (2) During inference, we linearly interpolate probability distributions with the PLM\u2019s predicted distributions to make the final prediction; (3) We conduct extensive experiments with fine-tuning in fully-supervised, few-shot and zero-shot settings, aiming to reveal the different scenarios where k-NN is applicable. We hope this work can open up new avenues for improving NLU of PLMs via k-NN and inspire future research to reconsider the role of \u201dold-school\u201c methods."
        },
        {
            "heading": "2 Related Work",
            "text": "k-NN in the era of PLMs. The k-Nearest Neighbor (kNN) classifier is a classic non-parametric algorithm that predicts based on representation similarities. While kNN has lost some visibility compared to current deep learning approaches in recent years, it has not fallen off the radar completely. In fact, kNN has been used to enhance pre-trained language models (PLMs) in various tasks, such as unconditional language modeling (Khandelwal et al., 2020; He et al., 2021), machine translation (Khandelwal et al., 2021; Gu et al., 2018), and question answering (Kassner and Sch\u00fctze, 2020). Most recently, (Alon et al., 2022; Meng et al., 2021) further respectively propose automaton-augmented and GNN-augmented retrieval to alleviate the computationally costly datastore search for language modeling. However, previous researchers (He et al., 2021; Khandelwal et al., 2021; Kassner and Sch\u00fctze, 2020; Li et al., 2021; Meng et al., 2021; Alon et al., 2022; Zhang et al., 2022) mainly focus on generative tasks or adopt simple interpolation strategies to combine k-NN PLMs only at test time. (Shi et al., 2022) propose to leverage k-NN for zero-shot inference.\nRevisiting k-NN for PLMs. Unlike them, we focus on empirically demonstrating that incorporating k-NN improves PLMs across a wide range of NLP tasks in fine-tuning and prompt-tuning paradigms on various settings, including the fully-supervised, few-shot and zero-shot settings. Note that our work is the first to comprehensively explore k-NN during both the training and inference process further for fruitful pairings: in addition to the approaches mentioned above, we propose to regard the distribution predicted by k-NN as the prior knowledge for calibrating training, so that the PLM will attend more to the examples misclassified by k-NN."
        },
        {
            "heading": "3 Methodology",
            "text": "The overall framework is presented in Figure 2. We regard the PLM as the feature extractor that transforms the input textual sequence x into an instance representation x with dimensions D. We revisit k-NN in \u00a73.1 and then introduce our method to integrate k-NN with tuning paradigms in \u00a73.2."
        },
        {
            "heading": "3.1 Nearest Neighbors Revisited",
            "text": "Given the training set of n labeled sentences {x1, . . . , xn} and a set of target labels {y1, . . . , yn}, y \u2208 [1, C], the k-NN classifier can be illustrated in the next three parts:\nFeature Representations For k-NN, we firstly have to collect the corresponding set of features D = {x1, . . . ,xn} from the training set. Concretely, we assign x with the embedding of the [CLS] token of the last layer of the PLM for the fine-tuning procedure. More specifically, we define the feature representations as follows:\nx = h[CLS], (1)\nThe feature representation q of a query example xq also follows the above equation.\nRetrieve k Neighbors Following the commonly practiced in k-NN (Friedman, 2017; Wang et al., 2019), we pre-process both q and features in the training set D with l2-normalization. We then compute the similarity between the query q and each example in D with Euclidean distance as : d(q,x), \u2200x \u2208 D, where d(\u00b7, \u00b7) is the Euclidean distance calculation function. According to the similarity, we select the top-k representations from D, which are the closest in the distance to q in the embedding space.\nSimilarity-based Aggregation Let N donate the set of retrieved top-k neighbors, and Ny be the subset of N where the whole examples have the same class y. Then the k-NN algorithm converts the top-k neighbors to q and the corresponding targets into a distribution over C labels. The probability distribution of q being predicted as c is:\npkNN(c|q) = \u2211\nx\u2208Ny exp (\u2212d(q,x)/\u03c4)\u2211 y\u2208C \u2211 x\u2208Ny exp (\u2212d(q,x)/\u03c4) , (2)\nwhere \u03c4 is the hyper-parameter of temperature.\n3.2 Comprehensive Exploiting of k-NN In this section, we propose to comprehensively leverage the k-NN, the representative of lazy learning, to augment the PLM-based classifier.\nRole of k-NN as Prior Knowledge for Calibrating Training. As k-NN can easily make predictions for each query instance encountered without any training, it is intuitive to regard its predictions as priors to guide the network in focusing on hard examples during the training process of language models. We distinguish between easy and hard examples based on the results of k-NN. Given the probability distribution pkNN of q being predicted as true label y, we propose to adjust the relative loss for the\ncorrectly-classified or misclassified instances identified by k-NN, in order to reweight the cross-entropy loss LCE . Specifically, we define the calibrated training loss LJ as:\nLU = (1 + f(pkNN))LCE , (3)\nwhere f(pkNN) donates the modulating factor 1 for calibration. We are inspired by Focal-loss (Lin et al., 2018) to employ the modulating factor, while our focus is on exploring the application of k-NN in the fine-tuning of PLMs.\nIntergrating k-NN into Inference Let PM denote the class distribution predicted by the PLM, and PkNN be the class distribution predicted by a k-NN classifier. Then, the PM is reformulates by interpolating the non-parametric k nearest neighbor distribution PkNN using parameter \u03bb (Khandelwal et al., 2020) to calculate the final probability PU of the label as:\nPU = \u03bbPkNN + (1\u2212 \u03bb)PM, (4)\nwhere \u03bb \u2208 [0, 1] is an adjustable hyper-parameter."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We choose a variety of NLP tasks to evaluate our proposed methods, including sentiment analysis task (SST-5 (Socher et al., 2013)), question classification task (TREC (Voorhees and Tice, 2000)), NLI tasks (MNLI (Williams et al., 2018) and QNLI (Rajpurkar et al., 2016)), sentence-pair classification task (BoolQ (Clark et al., 2019) and CB (De Marneffe et al., 2019) ), and information extraction tasks (SemEval (Hendrickx et al., 2010) and TACREV (Alt et al., 2020)). We also list a detailed introduction of datasets in Table 1."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "Compared Baseline Methods. We adopt RoBERTalarge (Liu et al., 2019) as the underline PLM and conduct comprehensive experiments to integrate k-NN into PLMs. We choose the baseline approaches and the variant of our proposed method as follows: (1) k-NN: the method described in \u00a73.1, which performs classification directly through nearest neighbor retrieval of instance features without relying on any pre-trained language models (PLMs). (2) FT: which denotes vanilla fine-tuning with PLMs. (3) FT_Scratch: which denotes vanilla PLMs in zero-shot setting. (4) PT: which denotes prompt-tuning with PLMs, similar to (Gao et al., 2021). (5) UNION-INF: a variant of our method, which simply linear interpolate k-NN and paradigms of PLMs during the test time. (6) UNION-ALL: the completeness of our approach, which involves applying k-NN as prior knowledge for calibrating training and also integrating k-NN into inference.\n1We specify the f(pkNN) = (1\u2212 pkNN)\u03b3 , and other factors are also alternative.\nSettings. We test the above methods in full-supervised, few-shot and zero-shot experiments, we assign different settings, respectively: (1) Full-supervised setting: We use full trainsets to train the PLMs and as neighbors to retrieve. (2) Few-shot setting: We follow LM-BFF (Gao et al., 2021) to conduct 16-shot experiment and test the average performance with a fixed set of seeds Sseed, across three different sampled Dtrain for each task. In this setting, we use the few-shot training set as k-NN neighbors to retrieve. (3) Zero-shot setting: We directly evaluate the vanilla FT and UNION-INF on the test set without training. As for UNION-ALL, we take the prompt tuning (Gao et al., 2021) to tag the pseudo labels on unlabeled trainsets and apply untrained k-NN in the training and inference."
        },
        {
            "heading": "4.3 Hyper-parameter Settings",
            "text": "We report the hyper-parameters in Table 3. For the GLUE and SuperGLUE datasets, we follow LMBFF2 to construct templates and verbalizer for prompt-tuning. While for RE datastes, we follow KnowPrompt (Chen et al., 2021) to construct templates and verbalizer. We utilize Pytorch to conduct experiments with 1 Nvidia 3090 GPUs. We used the AdamW optimizer for all optimizations, with a linear warmup of the learning ratefollowed by a linear decay over the remainder of the training. The hyperparameter settings used in our experiments are listed below.\n2https://github.com/princeton-nlp/LM-BFF"
        },
        {
            "heading": "4.4 Main Results",
            "text": "k-NN features result in performance gains. We compare the specific results with baseline models and provide comprehensive insights of k-NN on different paradigms and different settings. The results as shown in Table 1. Leverage k-NN features results in performance gains in both few-shot and fullysupervised settings. In the zero-shot setting, PT-based methods outperform FT-based and k-NN features further enhance the performance of PT-based methods, which demonstrates that it is flexible and general to integrate k-NN for PLMs.\nCalibrating training vs. Incorporating into inference. It is necessary to study the different application scenarios of incorporating k-NN during the training and testing phases. From Table 2, we observe the following: (1) Leveraging k-NN during the test phrase is especially helpful for the zero-shot setting. While UNION-ALL performs worse due to the noise brought from the pseudo-labels on unsupervised data. (2) UNION-INF is not doing as well in the fully-supervised and few-shot setting. In contrast, UNION-ALL outperforms UNION-INF in these settings, especially in the few-shot setting. These findings reveal to us the applicable scenarios of incorporating k-NN and inspire further studies to utilize k-NN classifier more practically for efficient NLP."
        },
        {
            "heading": "4.5 Analysis",
            "text": "Q1: How does the lazy learner benefit eager learner? To further understand how does the lazy learner (k-NN) benefit the eager learner (PLM), we manually check cases in which k-NN, PT, UNIONINF and UNION-ALL produce different results. As shown in the example of the upper row of Figure 3, k-NN and UNION-ALL predict correctly when PT fails. This result is because UNION-ALL produces a more confident probability for the correct class via calibrating the attention on the easy vs. hard examples identified by the k-NN classifier. Note that the bottom row shows that UNION-ALL predicts correctly even when k-NN predicts wrongly, possibly due to the robustness of k-NN calibration.\nQ2: Does the similarity metric matter? In the above experiments, we mainly utilize negative L2 distance to measure the similarity between the query q and the instance representation of the data store. It is intuitive to estimate the impact of different similarity metrics, such as cosine similarity. Thus, we present the performance of UNION-ALL using both metrics with the same hyperparameters as below.\nSimilarity Metric L2 cos\n16-shot SST-5 (%) 43.7 42.8 16-shot TREC (%) 90.0 89.4 16-shot QNLI (%) 58.1 57.2\nWe can find that UNION-ALL with cosine distance achieves nearly the same performance as those trained with L2, revealing that our UNION-ALL is robust to the similarity metric.\nQ3: How dose the modulating factor f(pkNN) works? Since we adopt focal loss (Focal) as the modulating factor for main experiments, we further explore other functions as modulating factors, such as negative log-likelihood (NLL). As shown in Figure 4, we visualize two modulating factors with different settings of \u03b1 and \u03b3, where \u03b1 donates a scalar that represent the proportion of the term of NLL, and \u03b3 is the exponential coefficient for Focal. We can find that NLL and Focal produce large weights for the misclassified examples, demonstrating the diversity of modulating factor selection."
        },
        {
            "heading": "5 Limitations",
            "text": "We only explore leveraging the training data for k-NN search, while various external domain data are also suitable for k-nearest neighbor retrieval. Moreover, incorporating k-NN also faces the following limitations: (1) the requirement of a large memory for retrieval; (2) hyper-parameters (such as \u03bb and \u03b1) used for retrieval have an impact on the performance of model training; (3) if the number of nearest neighbors k is too large, it will also affect the efficiency."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this paper, we propose a novel method to enhance PLM-based classifiers using k-NN. Specifically, we introduce a calibration process and linear interpolation of inference phrases to effectively integrate k-NN into the training pipeline. To evaluate the effectiveness of our approach, we conduct a comprehensive and in-depth analysis of the role of k-NN in various NLU tasks and tuning paradigms. Our results demonstrate that the integration of k-NN is flexible and can significantly enhance the performance of large models. Future work should explore the combination of k-NN and LLMs such as (1) Inject external knowledge into the LLMs with k-NN. Specifically, k-NN can be used to retrieve relevant knowledge from an external database during the reasoning process, which can help correct errors and reduce the prevalence of gibberish output and factual errors that are common in LLMs. (2) Retrieve contextual information to enhance LLMs. k-NN algorithms can automatically retrieve relevant information based on the input sentence, such as instructions or other relevant context. (3) Augment the training data for LLMs. k-NN is a powerful tool for identifying similar instances in a large dataset, which can help overcome the limitations of data scarcity and improve the performance LLMs."
        }
    ],
    "title": "Revisiting k-NN for Fine-tuning Pre-trained Language Models",
    "year": 2023
}