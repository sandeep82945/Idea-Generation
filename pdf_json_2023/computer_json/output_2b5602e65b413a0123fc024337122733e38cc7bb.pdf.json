{
    "abstractText": "Decentralized learning (DL) has gained prominence for its potential benefits in terms of scalability, privacy, and fault tolerance. It consists of many nodes that coordinate without a central server and exchange millions of parameters in the inherently iterative process of machine learning (ML) training. In addition, these nodes are connected in complex and potentially dynamic topologies. Assessing the intricate dynamics of such networks is clearly not an easy task. Often in literature, researchers resort to simulated environments that do not scale and fail to capture practical and crucial behaviors, including the ones associated to parallelism, data transfer, network delays, and wall-clock time. In this paper, we propose decentralizepy, a distributed framework for decentralizedML, which allows for the emulation of large-scale learning networks in arbitrary topologies. We demonstrate the capabilities of decentralizepy by deploying techniques such as sparsification and secure aggregation on top of several topologies, including dynamic networks with more than one thousand nodes. CCS Concepts: \u2022 Networks \u2192 Programming interfaces; \u2022 Computing methodologies \u2192 Distributed algorithms; Machine learning algorithms; \u2022 Computer systems organization \u2192 Peer-to-peer architectures.",
    "authors": [
        {
            "affiliations": [],
            "name": "Akash Dhasade"
        },
        {
            "affiliations": [],
            "name": "Anne-Marie Kermarrec"
        },
        {
            "affiliations": [],
            "name": "Rafael Pires"
        },
        {
            "affiliations": [],
            "name": "Rishi Sharma"
        },
        {
            "affiliations": [],
            "name": "Milos Vujasinovic"
        }
    ],
    "id": "SP:4b695f234c4f1fa74cd57557eb03eec62edbc839",
    "references": [
        {
            "authors": [
                "Mart\u00edn Abadi",
                "Paul Barham",
                "Jianmin Chen",
                "Zhifeng Chen",
                "Andy Davis",
                "Jeffrey Dean",
                "Matthieu Devin",
                "Sanjay Ghemawat",
                "Geoffrey Irving",
                "Michael Isard",
                "Manjunath Kudlur",
                "Josh Levenberg",
                "Rajat Monga",
                "Sherry Moore",
                "Derek G. Murray",
                "Benoit Steiner",
                "Paul Tucker",
                "Vijay Vasudevan",
                "Pete Warden",
                "Martin Wicke",
                "Yuan Yu",
                "Xiaoqiang Zheng"
            ],
            "title": "TensorFlow: a system for Large-Scale machine learning (OSDI\u201916)",
            "year": 2016
        },
        {
            "authors": [
                "Dan Alistarh",
                "Demjan Grubic",
                "Jerry Z. Li",
                "Ryota Tomioka",
                "Milan Vojnovic"
            ],
            "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding (NIPS\u201917)",
            "year": 2017
        },
        {
            "authors": [
                "Dan Alistarh",
                "Torsten Hoefler",
                "Mikael Johansson",
                "Sarit Khirirat",
                "Nikola Konstantinov",
                "C\u00e9dric Renggli"
            ],
            "title": "The Convergence of Sparsified Gradient Methods (NIPS\u201918)",
            "venue": "https://proceedings.neurips.cc/paper_",
            "year": 2018
        },
        {
            "authors": [
                "Batiste Le Bars",
                "Aur\u00e9lien Bellet",
                "Marc Tommasi",
                "Erick Lavoie",
                "Anne-Marie Kermarrec"
            ],
            "title": "Refined Convergence and Topology Learning for Decentralized Optimization with Heterogeneous Data (AISTATS\u201923)",
            "year": 2023
        },
        {
            "authors": [
                "Aur\u00e9lien Bellet",
                "Anne-Marie Kermarrec",
                "Erick Lavoie"
            ],
            "title": "2022. D- Cliques: Compensating for Data Heterogeneity with Topology in Decentralized Federated Learning (SRDS\u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Juan Benet"
            ],
            "title": "IPFS - Content Addressed, Versioned, P2P File System",
            "venue": "CoRR",
            "year": 2014
        },
        {
            "authors": [
                "Daniel J Beutel",
                "Taner Topal",
                "Akhil Mathur",
                "Xinchi Qiu",
                "Titouan Parcollet",
                "Nicholas D Lane"
            ],
            "title": "Flower: A Friendly Federated Learning Research Framework",
            "year": 2020
        },
        {
            "authors": [
                "Luca Boccassi"
            ],
            "title": "ZeroMQ: An open-source universal messaging",
            "year": 2023
        },
        {
            "authors": [
                "Keith Bonawitz",
                "Hubert Eichner",
                "Wolfgang Grieskamp"
            ],
            "title": "Towards Federated Learning at Scale: System Design (ML- Sys\u201919)",
            "year": 2019
        },
        {
            "authors": [
                "Keith Bonawitz",
                "Vladimir Ivanov",
                "Ben Kreuter",
                "Antonio Marcedone",
                "H. Brendan McMahan",
                "Sarvar Patel",
                "Daniel Ramage",
                "Aaron Segal",
                "Karn Seth"
            ],
            "title": "Practical Secure Aggregation for Privacy-Preserving Machine Learning (CCS \u201917)",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Caldas",
                "PeterWu",
                "Tian Li",
                "Jakub Kone\u010dn\u00fd",
                "H. BrendanMcMahan",
                "Virginia Smith",
                "Ameet Talwalkar"
            ],
            "title": "Leaf: A benchmark for federated settings. In 2nd Intl. Workshop on Federated Learning for Data Privacy and Confidentiality (FL-NeurIPS\u201919)",
            "year": 2019
        },
        {
            "authors": [
                "Akash Dhasade",
                "Nevena Dresevic",
                "Anne-Marie Kermarrec",
                "Rafael Pires"
            ],
            "title": "TEE-based decentralized recommender systems: The raw data sharing redemption (IPDPS\u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Paulo Gouveia",
                "Jo\u00e3o Neves",
                "Carlos Segarra",
                "Luca Liechti",
                "Shady Issa",
                "Valerio Schiavoni",
                "Miguel Matos"
            ],
            "title": "Kollaps: Decentralized and Dynamic Topology Emulation (EuroSys \u201920)",
            "year": 2020
        },
        {
            "authors": [
                "Chaoyang He",
                "Songze Li",
                "Jinhyun So",
                "Mi Zhang",
                "Hongyi Wang",
                "Xiaoyang Wang",
                "Praneeth Vepakomma",
                "Abhishek Singh",
                "Hang Qiu",
                "Li Shen",
                "Peilin Zhao",
                "Yan Kang",
                "Yang Liu",
                "Ramesh Raskar",
                "Qiang Yang",
                "Murali Annavaram",
                "Salman Avestimehr"
            ],
            "title": "FedML: A research library and benchmark for federated machine learning",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Hsieh",
                "Amar Phanishayee",
                "Onur Mutlu",
                "Phillip B. Gibbons"
            ],
            "title": "The Non-IID Data Quagmire of Decentralized Machine Learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "M\u00e1rk Jelasity",
                "Spyros Voulgaris",
                "Rachid Guerraoui",
                "Anne-Marie Kermarrec",
                "Maarten Van Steen"
            ],
            "title": "Gossip-based peer sampling",
            "venue": "ACM Transactions on Computer Systems (TOCS) 25,",
            "year": 2007
        },
        {
            "authors": [
                "Peter Kairouz",
                "H. Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "ArjunNitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings",
                "Rafael G.L. D\u2019Oliveira",
                "Hubert Eichner",
                "Salim El Rouayheb",
                "David Evans",
                "Josh Gardner",
                "Zachary Garrett",
                "Adri\u00e0 Gasc\u00f3n",
                "Badih Ghazi",
                "Phillip B. Gibbons",
                "Marco Gruteser",
                "Zaid Harchaoui",
                "Chaoyang He",
                "Lie He",
                "Zhouyuan Huo",
                "Ben Hutchinson",
                "Justin Hsu",
                "Martin Jaggi",
                "Tara Javidi",
                "Gauri Joshi",
                "Mikhail Khodak",
                "Jakub Konecn\u00fd",
                "Aleksandra Korolova",
                "Farinaz Koushanfar",
                "Sanmi Koyejo",
                "Tancr\u00e8de Lepoint",
                "Yang Liu",
                "Prateek Mittal",
                "Mehryar Mohri",
                "Richard Nock",
                "Ayfer \u00d6zg\u00fcr",
                "Rasmus Pagh",
                "Hang Qi",
                "Daniel Ramage",
                "Ramesh Raskar",
                "Mariana Raykova",
                "Dawn Song",
                "Weikang Song",
                "Sebastian U. Stich",
                "Ziteng Sun",
                "Ananda Theertha Suresh",
                "Florian Tram\u00e8r",
                "Praneeth Vepakomma",
                "Jianyu Wang",
                "Li Xiong",
                "Zheng Xu",
                "Qiang Yang",
                "Felix X. Yu",
                "Han Yu",
                "Sen Zhao"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends in Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Tao Lin",
                "Sebastian U Stich",
                "andMartin Jaggi"
            ],
            "title": "Decentralized Deep Learning with Arbitrary Communication Compression (ICLR\u201920)",
            "year": 2020
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Nicolas Loizou",
                "Sadra Boreiri",
                "Martin Jaggi",
                "Sebastian Stich"
            ],
            "title": "A Unified Theory of Decentralized SGD with Changing Topology and Local Updates (ICML\u201920)",
            "venue": "https://proceedings. mlr.press/v119/koloskova20a.html EuroMLSys",
            "year": 2020
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Sebastian Stich",
                "Martin Jaggi"
            ],
            "title": "Decentralized stochastic optimization and gossip algorithms with compressed communication (ICML\u201919)",
            "year": 2019
        },
        {
            "authors": [
                "Fan Lai",
                "Yinwei Dai",
                "Sanjay Singapuram"
            ],
            "title": "FedScale: Benchmarking Model and System Performance of Federated Learning at Scale (ICML\u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Xiangru Lian",
                "Ce Zhang",
                "Huan Zhang",
                "Cho-Jui Hsieh",
                "Wei Zhang",
                "Ji Liu"
            ],
            "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent (NIPS\u201917)",
            "venue": "https://proceedings.neurips.cc/paper_files/",
            "year": 2017
        },
        {
            "authors": [
                "Yujun Lin",
                "Song Han",
                "Huizi Mao",
                "Yu Wang",
                "Bill Dally"
            ],
            "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training (ICLR\u201918)",
            "year": 2018
        },
        {
            "authors": [
                "Yang Liu",
                "Tao Fan",
                "Tianjian Chen",
                "Qian Xu",
                "Qiang Yang"
            ],
            "title": "FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection",
            "venue": "J. Mach. Learn. Res. 22,",
            "year": 2021
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data (AISTATS\u201917). https://proceedings",
            "year": 2017
        },
        {
            "authors": [
                "Christodoulos Pappas",
                "Dimitris Chatzopoulos",
                "Spyros Lalis",
                "Manolis Vavalis"
            ],
            "title": "IPLS: A Framework for Decentralized Federated Learning (IFIP Networking\u201921)",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "In NeurIPS\u201919. https://proceedings.neurips.cc/paper_files/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf",
            "year": 2019
        },
        {
            "authors": [
                "Holger R Roth",
                "Yan Cheng",
                "Yuhong Wen",
                "Isaac Yang",
                "Ziyue Xu",
                "YuanTing Hsieh",
                "Kristopher Kersten",
                "Ahmed Harouni",
                "Can Zhao",
                "Kevin Lu",
                "Zhihong Zhang",
                "Wenqi Li",
                "Andriy Myronenko",
                "Dong Yang",
                "Sean Yang",
                "Nicola Rieke",
                "Abood Quraini",
                "Chester Chen",
                "Daguang Xu",
                "Nic Ma",
                "Prerna Dogra",
                "Mona G Flores",
                "Andrew Feng"
            ],
            "title": "2022. NVIDIA FLARE: Federated Learning from Simulation to Real-World",
            "venue": "In Workshop on Federated Learning: Recent Advances and New Challenges",
            "year": 2022
        },
        {
            "authors": [
                "Rishi Sharma"
            ],
            "title": "decentralizepy: An open-source decentralized learning research framework",
            "year": 2022
        },
        {
            "authors": [
                "Nikko Strom"
            ],
            "title": "Scalable distributed DNN training using commodity GPU cloud computing",
            "venue": "In 16th Annual Conference of the International Speech Communication Association (INTER- SPEECH\u201915)",
            "year": 2015
        },
        {
            "authors": [
                "Thijs Vogels",
                "Hadrien Hendrikx",
                "Martin Jaggi"
            ],
            "title": "Beyond spectral gap: the role of the topology in decentralized learning (NeurIPS\u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Thijs Vogels",
                "Sai Praneeth Karimireddy",
                "Martin Jaggi"
            ],
            "title": "Practical Low-Rank Communication Compression in Decentralized Deep Learning (NeurIPS\u201920)",
            "venue": "https://proceedings.neurips.cc/paper_files/",
            "year": 2020
        },
        {
            "authors": [
                "Milos Vujasinovic"
            ],
            "title": "Secure Aggregation on Sparse Models in Decentralized Learning Systems",
            "venue": "Master\u2019s thesis. EPFL. https://www.epfl.ch/labs/sacs/wp-content/uploads/2023/02/Secure_ Aggregation_on_Sparse_Models_in_Decentralized_Learning_ Systems___Milos_Vujasinovic.pdf",
            "year": 2023
        },
        {
            "authors": [
                "Lin Xiao",
                "Stephen Boyd",
                "Seung-Jean Kim"
            ],
            "title": "Distributed average consensus with least-mean-square deviation",
            "venue": "J. Parallel and Distrib. Comput",
            "year": 2007
        },
        {
            "authors": [
                "Timothy Yang",
                "Galen Andrew",
                "Hubert Eichner",
                "Haicheng Sun",
                "Wei Li",
                "Nicholas Kong",
                "Daniel Ramage",
                "Fran\u00e7oise Beaufays"
            ],
            "title": "Applied federated learning: Improving google keyboard query suggestions",
            "year": 2018
        },
        {
            "authors": [
                "Tongtian Zhu",
                "Fengxiang He",
                "Lan Zhang",
                "Zhengyang Niu",
                "Mingli Song",
                "Dacheng Tao"
            ],
            "title": "Topology-aware generalization of decentralized SGD (ICML\u201922)",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Ziller",
                "Andrew Trask",
                "Antonio Lopardo"
            ],
            "title": "PySyft: A library for easy federated learning",
            "venue": "In Federated Learning Systems",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "potential benefits in terms of scalability, privacy, and fault tolerance. It consists of many nodes that coordinate without a central server and exchange millions of parameters in the inherently iterative process of machine learning (ML) training. In addition, these nodes are connected in complex and potentially dynamic topologies. Assessing the intricate dynamics of such networks is clearly not an easy task. Often in literature, researchers resort to simulated environments that do not scale and fail to capture practical and crucial behaviors, including the ones associated to parallelism, data transfer, network delays, and wall-clock time. In this paper, we propose decentralizepy, a distributed framework for decentralizedML, which allows for the emulation of large-scale learning networks in arbitrary topologies. We demonstrate the capabilities of decentralizepy by deploying techniques such as sparsification and secure aggregation on top of several topologies, including dynamic networks with more than one thousand nodes.\nCCS Concepts: \u2022 Networks \u2192 Programming interfaces; \u2022 Computing methodologies \u2192 Distributed algorithms; Machine learning algorithms; \u2022 Computer systems organization \u2192 Peer-to-peer architectures.\nKeywords: decentralized learning, middleware, machine learning, distributed systems, peer-to-peer, network topology ACM Reference Format: Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Rishi Sharma, and Milos Vujasinovic. 2023. Decentralized Learning Made Easy with DecentralizePy. In 3rd Workshop on Machine Learning and Systems (EuroMLSys \u201923), May 8, 2023, Rome, Italy. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3578356.3592587 \u2217Corresponding author: first.last@epfl.ch\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. EuroMLSys \u201923, May 8, 2023, Rome, Italy \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0084-2/23/05. . . $15.00 https://doi.org/10.1145/3578356.3592587"
        },
        {
            "heading": "1 Introduction",
            "text": "There has been a shift in machine learning (ML) training, which is now taking place at the location where data is generated, instead of the earlier method of first moving the data to be processed in data centers. Federated learning (FL) [26] and decentralized learning (DL) [23] have become prominent collaborative training methods that prioritize privacy, by solely exchanging updates of the model being trained. In FL, training is orchestrated by a central server where the server broadcasts the global model to participating nodes and later aggregates the updates received back from them. Conversely, in DL, nodes are connected in a fully decentralized communication topology. Rather than relying on a server, nodes in DL train on their local datasets and share the resulting models with neighboring nodes. The aggregated models obtained from these exchanges lead to a converged global model at the end of the DL training process. While FL has already received more attention from academia and industry [9, 11, 17, 37], DL is also recently gaining a lot of traction [18\u201320, 23]. The popularity of FL has been partially enabled by the availability of several frameworks for simulation or emulation [1, 7, 14, 22, 25, 30, 39], which allow researchers to quickly implement and assess novel FL algorithms. Support for DL configurations, on the other hand, is rare and limited in these existing frameworks [14]. FL frameworks typically offer abstractions for client-server interactions which are not suitable for DL, since nodes in DL communicate only with their immediate neighbors. These limitations call for frameworks that provide abstractions involving peer-to-peer communication channels and topology manipulation for collaborative ML training.\nDL research has often focused on exploring the impact of topology on learning performance [5, 33], which has led to simulations using various static and dynamic topologies during training [19]. Another important area of DL research targets to improve communication efficiency through compression, which reduces the number of model parameters exchanged between DL nodes [3, 24, 32]. This includes techniques such as sparsification [3] and quantization [2], which are either not readily available in FL frameworks, or incompatible with DL. The same happens in secure aggregation [10], which imposes additional challenges when deployed in DL training. decentralizepy offers these capabilities, empowering researchers to explore and innovate on\nDL research without being constrained by the limitations of existing FL frameworks. Although we are not exhaustive in implementing all aspects of DL systems, we provide, along with reference implementations, a modular and extensible framework that aims at easing the prototyping and deployment of such systems. We demonstrate this flexibility with various topologies, sparsification, and secure aggregation techniques in DL training. Our results provide insights to encourage more research on these areas in order to improve the practicality of DL.\nContributions \u2022 We present decentralizepy, a novel decentralized learning framework, which allows researchers and practitioners to experiment with the components of DL systems and deploy them in real-world settings.\n\u2022 We showcase how decentralizepy can be leveraged to assess the effects of several topologies, dynamic networks, sparsification techniques, and secure aggregation in DL systems. \u2022 decentralizepy is modular, easily extendable, and open-source [31] under MIT License.\nThe remainder of this paper is organized as follows: Section 2 describes the framework internals and Section 3 showcases its usage in several scenarios. We survey related work in Section 4 and conclude in Section 5."
        },
        {
            "heading": "2 DecentralizePy",
            "text": ""
        },
        {
            "heading": "2.1 Design overview",
            "text": "Works in DL research are mostly evaluated in simulated environments. These simulations are either done in scenarios where a single machine simulates all nodes (i.e., not scalable), or in cluster settings over MPI (i.e., constrained to the local area network (LAN)) [5, 33, 34, 38]. The design of decentralizepy takes into account two key needs: the ability to quickly develop research prototypes, as well as actually deploying large-scale DL systems. We elaborate next on these design goals.\nModularity. DL systems consist of multiple components, and conducting research in this field requires adjusting and testing them. For generality, it is often necessary to evaluate different datasets and models when building learning systems. In decentralized training, the overlay topology, i.e., the way nodes are connected, is especially critical for achieving satisfactory model convergence [4]. Finally, the protocol itself, i.e., who to communicatewith, what is themessage content, and how to aggregate the received parameters, varies among systems. To facilitate the implementation of new DL systems and experimentation with various ML aspects, including datasets, models, and topologies, decentralizepy incorporates loosely coupled modules with an object-oriented design. Section 2.2 provides a more detailed explanation of these modules.\nOne-node one-process. In order to achieve rapid prototyping, researchers commonly favor simple implementations built from scratch. In DL systems, this often means simulations on a single machine or in a cluster environment [5, 33, 34, 38]. Real-world deployment of such systems would however imply numerous nodes running on geographically-distant machines. Unfortunately, DL systems specifically designed for cluster environments are difficult to configure and scale beyond the LAN. decentralizepy has a one-node as one-process design principle, so that we are able to scale and measure realistic system metrics at the granularity of a node (or process). In other words, one DL node is represented as a single process. These nodes communicate over network sockets and do not distinguish processes on the same or different machines. This means that the same testbed can run in a cluster environment or on real-world machines over wide area networks (WANs) by just configuring the IP address information. This approach simplifies the configuration of system parameters like the number of processes per node (according to, for instance, the number of CPU cores), network bandwidth, latency, and packet drop, enabling practitioners to study their systems in detail. In addition, measurements of system metrics such as data transferred, memory usage, and CPU time become easier to collect on each node. Furthermore, the emulation and deployment can reveal behaviors that would not show\nup in simulations. These benefits streamline the assessment of the performance and scalability of DL systems."
        },
        {
            "heading": "2.2 Architecture",
            "text": "In this section, we describe decentralizepy modules and how they can be customized to develop new DL systems. Figure 1 shows the architecture of the framework and how different modules are integrated.\nNode. The node module acts as the skeleton code that performs the DL task by instantiating and calling the methods of the rest of the modules. An object of a sub-class of the node module is instantiated when a DL process starts. A node can be designed to perform a variety of tasks from being a DL client to a FL server or a centralized peer sampler. The node module provides complete flexibility in creating and decoding messages to be exchanged with peers. To get practitioners started with decentralizepy, the framework is already equipped with the implementations of DL clients, a centralized peer sampler, a parameter server, a FL server, and secure aggregation clients as nodes.\nGraph. The graph module manages the overlay network, which constrains the communication of nodes to only immediate neighbors. This overlay graph can be modified at run time by the node, hence supporting both static and dynamic topologies. The topology can be read from a graph file having edges or an adjacency list. With this, we support swift switching of topologies, which could be generated by external libraries, during experimentation. A graph file specifying the topology is shown as the topology specification in Figure 1.\nModel. This is a lightweight module inheriting the model class of the underlying ML framework. The main purpose of having a model module is to store additional states. For instance, this module allows the developer to store past gradients or how much the learning parameters changed in the last iteration. This is especially convenient for some sparsification algorithms. Since the models are quite specific to the datasets, these are implemented as part of the dataset modules.\nDataset. ML frameworks must support a diverse range of datasets and models. The dataset module of our framework provides this for a variety of learning tasks over multiple datasets and models, seamlessly integrating with the other modules. Among the tasks it performs, we highlight: (i) reading the train and test sets; (ii) partitioning the datasets among nodes; (iii) evaluating the performance of a trained model on the test set; and (iv) specific model implementations. decentralizepy includes six datasets from the LEAF FL benchmark [11] and CIFAR-10 [21] with both independent and identically distributed (IID) and non independent and identically distributed (non-IID) data partitioning.\nTraining. The training module performs local training steps of the model on the given training set. The optimizer and loss function for the learning tasks are provided as part of the training specifications shown in Figure 1. Having access to the instances of both model and dataset modules, the training module can modify the additional state variables in these modules for various purposes, e.g., prioritizing certain weights during model compression.\nSharing and Communication In DL, the nodes interact by exchanging messages. The sharing module decides the contents of these messages and the aggregation procedure. For model sharing, the messages would contain serialized parameters and the aggregation scheme will average the received models. In the presence of sparsification (model compression), the messages would contain a subset of model parameters, and the aggregation scheme needs to account for missing parameters. Implementations of sparsification schemes such as random sampling, TopK [3], and Choco-SGD [20] are available as sharing modules in decentralizepy. For data-sharing architectures [12], in turn, the sharingmodule would include raw data in the messages, and the aggregation procedure would append the received data to the local dataset. These messages are passed on to communication by the node to send them to the correct recipients. decentralizepy also has an existing implementation of communication using ZeroMQ [8] over TCP.\nMapping, Compression, and Utils There are some auxiliary modules to support the framework: (1) Mapping, (2) compression, and (3) utils. To support both cluster environments and real-world deployment, mapping associates"
        },
        {
            "heading": "12 dataset.test()",
            "text": ""
        },
        {
            "heading": "11 sharing.average(rcv)",
            "text": ""
        },
        {
            "heading": "10 rcv = communication.receive_from_all()",
            "text": "nodes with particular machines. Compression module packages general-purpose compression algorithms for floatingpoint and integer lists. Utils contains commonly used functions across the framework like Python dictionary manipulations and command-line argument parsing. decentralizepy allows the developer to instantiate a node and provide the implementations of the other modules separately, as shown in Figure 1. Each module provides a base class that defines the interface for the module. New implementations of modules can extend the base class and legacy implementations may be ported by wrapping them in the module interface. These implementations are dynamically loaded to streamline the process of substituting the implementations for rapid prototyping. Furthermore, since decentralizepy is fully-decentralized, the nodes can have heterogenous datasets, different sharing strategies, optimizers, and learning rates for full flexibility. Figure 2 shows how to write a simplified decentralized learning node using decentralizepy. The functions of the other modules (e.g., sharing) invoked here can be overloaded to customize the system. Given the decentralized nature of the nodes, each of them locally writes logs and results in JSON files. To compute aggregate statistics of the system, we collect and process the results in a single machine at the end."
        },
        {
            "heading": "2.3 Implementation",
            "text": "Written in Python v3.8, the modules span a total of over 10 000 lines of code. Additionally, the framework inherits core ML functionalities from PyTorch v1.10.2 [29]. decentralizepy contains several implementations of modules to facilitate building DL systems as described earlier. It is important to note that these are just reference implementations for a quick start in developing new DL systems with the framework. We call for the support of the scientific community to improve decentralizepy by adding more implementations of modules derived from recent and upcoming research."
        },
        {
            "heading": "3 Evaluation",
            "text": "In this section, we demonstrate the power of decentralizepy by implementing changing topologies, state-of-the-art sparsification algorithms, secure aggregation in DL, and a scalability study."
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "The experiments are run on 16 hyperthreading-enabled machines equipped with 2 Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz having 8 cores. Based on the experiment, we run 48, 256, or 1024 DL nodes (processes). Each process is constrained to one logical CPU core in the experiments. The nodes are oblivious about being on the same or different machines and communicate via TCP sockets using ZeroMQ [8]. We tuned the learning-rate for the basic stochastic gradient descent (SGD) optimizer without momentum, and use it for training along with cross-entropy loss. The DL clients used decentralized parallel stochastic gradient descent (D-PSGD) [23] over Metropolis-Hastings weights [36] for aggregation. We use CIFAR-10 [21] dataset with a 2-sharding non-IID data partitioning [26] which limits the number of classes per node to 4. In addition to CIFAR10, we also use CelebA [11] for secure-aggregation experiments. The evaluation is done on the test-set of the respective datasets. We run each experiment 5 times with different random seeds and present the average metrics with a 95% confidence interval."
        },
        {
            "heading": "3.2 Topologies and Dynamicity",
            "text": "The performance of DL is heavily influenced by the communication topology [18]. We now demonstrate how decentralizepy can flexibly simulate different topologies. We start by running decentralizepy with 256 learning nodes on a ring, d-regular with degree 5, and fully-connected static topologies. The framework allows us to effortlessly change topologies for the experiments by only replacing the graph\nfile described in Section 2.2. To simulate dynamic topologies, decentralizepy uses a centralized peer sampler which instantiates new topologies every round using the graph module. Any dynamic graph can be realized within the peer sampler which then notifies each node of its neighbors. We demonstrate a sample case where the peer sampler creates a random 5-regular topology every round.\nFigure 3 shows the convergence plots with respect to the communication rounds, wall-clock time, and the cumulative bytes sent per node when run for the same number of communication rounds. As one would expect, fully connected has the highest accuracy and the ring has the lowest accuracy at the end (Figure 3 (a)). decentralizepy also reveals that experiments with fully-connected topologies take nearly 3\u00d7 more time when compared to the rest for the same number of communication rounds (Figure 3 (b)). This detail is often not evaluated in literature due to limited framework support. We observe that d-regular topologies represent a favorable tradeoff between the extreme topologies (Figure 3 (b) and (c)). Dynamic topologies perform much better than their static counterparts. Moreover, dynamic 5-regular topology achieves almost identical accuracies to fully-connected given the same time deadline while having 51\u00d7 less communication cost. Therefore, research should focus more on dynamic topologies to study their tradeoffs in more detail."
        },
        {
            "heading": "3.3 Sparsification",
            "text": "DL systems use sparsification algorithms to reduce the number of bytes exchanged in the network [3, 24]. In sparsification, a communication budget specifies the percentage of model parameters shared by nodes with respect to full sharing, i.e., sharing all parameters. We now showcase how decentralizepy supports commonly-used sparsification algorithms as part of the sharing module. In full sharing, the basic sharing module generates a serialized parameter vector to be sent to neighboring nodes. For sparsification, in contrast, we modify basic sharing to generate serialized tuples of the indices and values of the parameters chosen to be shared. For the experiments, we consider a setup with a 5-regular topology of 256 learning nodes with a communication budget of 10%.\nFigure 4 shows the test accuracy vs. communication cost of two sparsification algorithms: (1) random sampling, and (2) hyperparameter-tuned state of the art Choco-SGD [20] against the baseline of full sharing DL. The random sampling algorithm picks 10% random parameters every round for sharing. Choco-SGD uses sophisticated parameter-ranking and error-correction schemes to limit the loss of information due to sparsification. We observe that sparsification loses too much information and performs significantly worse than full sharing. This can be attributed to the complexity of the learning task. The convergence of sparsification algorithms drastically slows down in non-IID settings with a large number of nodes. To reach the same accuracy as both\nsparsification algorithms, full sharing uses significantly less communication. Most works studying sparsification algorithms are often evaluated in IID settings with a limited number of nodes, resulting in overly optimistic estimations of the performance [15]. The use of decentralizepy will enable researchers to study sparsification algorithms that are robust to difficult data distributions at scale."
        },
        {
            "heading": "3.4 Secure Aggregation",
            "text": "The well-established technique of secure aggregation [10], commonly used in centralized settings, ensures that participating nodes only have access to the aggregated model parameters, while keeping private the individual models of other training nodes. This is achieved through the masking of parameters where pairs of nodes add cancellable masks to their respective models before sharing. The receiver node upon aggregation gets the same aggregated model as one without secure aggregation. However, the masks prevent access to individual models. Through a non-trivial implementation [35], we show that decentralizepy can also be leveraged to perform such secure aggregation in the DL setting.We design the core procedure as part of the nodemodule of decentralizepy and conduct experiments with 48 nodes on CIFAR-10 and CelebA datasets for 10 000 communication rounds. Figure 5 shows the test accuracy vs. cumulative communication cost of DL with and without secure aggregation. Secure aggregation incurs approximately 3% more communication due to metadata (shared seeds for pseudo-random number generation and masks) in addition to the parameters. Furthermore, because masks and parameters are floating point numbers, there is a loss of precision which leads to 3% loss in accuracy."
        },
        {
            "heading": "3.5 Scalability",
            "text": "Finally, we perform a scalability study of DL by increasing the number of nodes from 256 to 1024. In this study, we\nparticularly analyze the effect of the number of data samples per node and the degree of the node on the performance of DL. Since the underlying dataset size remains the same when the nodes are scaled, each node receives 4\u00d7 fewer data samples in the setup with 1024 nodes compared to the setup with 256 nodes.\nWe report the evolution of test accuracy for 256-node 5- regular, 1024-node 5-regular, and 1024-node 9-regular topologies in Figure 6. It is worth noting that the final performance of the 5-regular topology with 1024 and 256 nodes remain almost the same, even with non-IID data and 4\u00d7 fewer data samples in the 1024-node setting. Increasing the degree from\n5 to 9 boosts the performance by 5.8 accuracy points on average. This suggests that the number of neighbors has a higher impact on learning than the number of data samples per node. Such novel insights are possible through advanced scalability studies, thanks to decentralizepy. We highlight that decentralizepy can run a much higher number of nodes relative to the relevant literature in decentralized learning systems [5, 20, 33, 34, 38]."
        },
        {
            "heading": "4 Related work",
            "text": "Flower [7] is a framework for FL emulation. Like decentralizepy, Flower scales to one thousand nodes training in parallel. It is however limited to the FL setup, i.e., it relies on a central server that orchestrates the ML training by collecting and aggregating client models on every iteration. In addition, it does not offer seamless support for arbitrary communication topologies.\nFedScale [22] is both a collection of realistic datasets (with natural non-IID and unbalanced data) and a runtime platform that supports cluster and mobile back-ends. In simulation mode, FedScale orchestrates execution requests over available resources (either single GPU/CPU or distributed) while keeping a client virtual clock. This way, it achieves shorter evaluation times in comparison to the simulated scenario and reports up to 10 000 training nodes in parallel on top of a cluster of 10 GPU nodes. In addition, FedScale incorporates system speeds and availability traces of mobile devices to achieve realistic heterogeneity simulations. Unlike decentralizepy, it has no support for DL.\nIPLS [28] proposes ML training that uses a decentralized filesystem as the communication channel. To reduce network traffic while ensuring some fault tolerance, they also propose model partitioning and replication of partitions across nodes. Each node only shares a particular set of layers (rather than the whole model) and each layer is potentially shared by several nodes. Its feasibility is yet to be confirmed, as authors show a very small deployment (50 nodes) with a simple learning task (MNIST) on top of IID data. Moreover, IPLS depends on the underlying IPFS [6] implementation and is therefore not flexible in terms of topology manipulation. Kollaps [13] is a decentralized network emulator that allows for the manipulation of end-to-end communication properties between nodes. By shaping the network latency, bandwidth, packet loss, and jitter, they achieve very similar results of bare-metal deployments in geo-distributed setups. Kollaps is orthogonal and complementary to decentralizepy, as it operates at the level of containers (e.g., Docker) and orchestrators (e.g., Kubernetes).\nCloser to our proposal, FedML [14] is a Federated Learning framework that provides several baseline implementations of models and datasets for reproducible research and benchmarking. FedML supports distinct topologies, including decentralized ones. Since it uses MPI [27] for communication,\nit is primarily targeted to cluster environments. decentralizepy, in contrast, can be smoothly deployed in WAN and is more flexible than FedML in terms of dynamic topologies."
        },
        {
            "heading": "5 Conclusion",
            "text": "Research in DL systems requires efficient scaling to many nodes, ease of deployment, and a quick development cycle. In this paper, we present decentralizepy, a DL framework designed to support these requirements. Its modular design facilitates the replacement of components with customized ones, allowing rapid prototyping and deployment. We demonstrate the power, flexibility, and scalability of decentralizepy through a series of experiments across different topologies, sparsification algorithms, and privacypreserving secure aggregation. Using this framework, we were able to derive new insights concerning the effectiveness of dynamic topologies and the performance degradation of state-of-the-art sparsification algorithms on non-IID learning tasks at scale.\ndecentralizepy has been used by over a dozen students for projects in DL. Their feedback has been valuable in improving the usability and efficiency of the framework. We expect the scientific community to also try and contribute to it. As future work, we plan to extend decentralizepy with real-world traces of network traffic over Kollaps [13], client availability from FedScale [22], and decentralized peer sampling [16]. We also plan to integrate additional state-ofthe-art DL algorithms, including quantization and topology construction. Finally, using the framework as a platform, the community can build communication-efficient and privacypreserving DL systems that are robust to non-IID data distributions at scale."
        }
    ],
    "title": "Decentralized Learning Made Easy with DecentralizePy",
    "year": 2023
}