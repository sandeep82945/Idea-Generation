{
    "abstractText": "In this work, we consider and analyze the sample complexity of model-free reinforcement learning with a generative model. Particularly, we analyze mirror descent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al. (2020a), which uses the Kullback-Leibler divergence and entropy regularization in its value and policy updates. Our analysis shows that it is nearly minimax-optimal for finding an \u03b5-optimal policy when \u03b5 is sufficiently small. This is the first theoretical result that demonstrates that a simple model-free algorithm without variance-reduction can be nearly minimax-optimal under the considered setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tadashi Kozuno"
        },
        {
            "affiliations": [],
            "name": "Wenhao Yang"
        },
        {
            "affiliations": [],
            "name": "Nino Vieillard"
        },
        {
            "affiliations": [],
            "name": "Toshinori Kitamura"
        },
        {
            "affiliations": [],
            "name": "Yunhao Tang"
        },
        {
            "affiliations": [],
            "name": "Jincheng Mei"
        },
        {
            "affiliations": [],
            "name": "Pierre M\u00e9nard"
        },
        {
            "affiliations": [],
            "name": "Mohammad Gheshlaghi Azar"
        },
        {
            "affiliations": [],
            "name": "Michal Valko"
        },
        {
            "affiliations": [],
            "name": "R\u00e9mi Munos"
        },
        {
            "affiliations": [],
            "name": "Olivier Pietquin"
        },
        {
            "affiliations": [],
            "name": "Matthieu Geist"
        },
        {
            "affiliations": [],
            "name": "Csaba Szepesv\u00e1ri"
        }
    ],
    "id": "SP:6166d964d7fe354e5b3c101dbd1b842a6ef900dc",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Sham Kakade",
                "Lin F. Yang"
            ],
            "title": "Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "TW Archibald",
                "KIM McKinnon",
                "LC Thomas"
            ],
            "title": "On the Generation of Markov Decision Processes",
            "venue": "Journal of the Operational Research Society,",
            "year": 1995
        },
        {
            "authors": [
                "Mohammad Azar",
                "Mohammad Ghavamzadeh",
                "Hilbert Kappen",
                "R\u00e9mi Munos"
            ],
            "title": "Speedy Q-Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "Mohammad Azar",
                "R\u00e9mi Munos",
                "Hilbert J. Kappen"
            ],
            "title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model",
            "venue": "Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Mohammad Gheshlaghi Azar",
                "Ian Osband",
                "R\u00e9mi Munos"
            ],
            "title": "Minimax Regret Bounds for Reinforcement Learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Kazuoki Azuma"
            ],
            "title": "Weighted sums of certain dependent random variables",
            "venue": "Tohoku Mathematical Journal,",
            "year": 1967
        },
        {
            "authors": [
                "Sergei Natanovich Bernstein"
            ],
            "title": "The Theory of Probabilities",
            "venue": "Gastehizdat Publishing House,",
            "year": 1946
        },
        {
            "authors": [
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi",
                "Pascal Massart"
            ],
            "title": "Concentration Inequalities - A Nonasymptotic Theory of Independence",
            "year": 2013
        },
        {
            "authors": [
                "Shicong Cen",
                "Chen Cheng",
                "Yuxin Chen",
                "Yuting Wei",
                "Yuejie Chi"
            ],
            "title": "Fast global convergence of natural policy gradient methods with entropy regularization",
            "venue": "Operations Research,",
            "year": 2021
        },
        {
            "authors": [
                "Eyal Even-Dar",
                "Yishay Mansour",
                "Peter Bartlett"
            ],
            "title": "Learning Rates for Q-learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2003
        },
        {
            "authors": [
                "Roy Fox",
                "Ari Pakman",
                "Naftali Tishby"
            ],
            "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Matthieu Geist",
                "Bruno Scherrer",
                "Olivier Pietquin"
            ],
            "title": "A Theory of Regularized Markov Decision Processes",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Wassily Hoeffding"
            ],
            "title": "Probability Inequalities for Sums of Bounded Random Variables",
            "venue": "Journal of the American Statistical Association,",
            "year": 1963
        },
        {
            "authors": [
                "Chi Jin",
                "Zeyuan Allen-Zhu",
                "Sebastien Bubeck",
                "Michael I. Jordan"
            ],
            "title": "Is Q-Learning Provably Efficient",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Koulik Khamaru",
                "Eric Xia",
                "Martin J Wainwright",
                "Michael I Jordan"
            ],
            "title": "Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning",
            "venue": "arXiv preprint arXiv:2106.14352,",
            "year": 2021
        },
        {
            "authors": [
                "Tadashi Kozuno",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Theoretical Analysis of Efficiency and Robustness of Softmax and Gap-Increasing Operators in Reinforcement Learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Guanghui Lan"
            ],
            "title": "Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes",
            "venue": "Mathematical programming,",
            "year": 2022
        },
        {
            "authors": [
                "Tor Lattimore",
                "Marcus Hutter"
            ],
            "title": "PAC Bounds for Discounted MDPs",
            "venue": "In International Conference on Algorithmic Learning Theory,",
            "year": 2012
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba"
            ],
            "title": "Szepesvari. Bandit Algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Kyungjae Lee",
                "Sungjoon Choi",
                "Songhwai Oh"
            ],
            "title": "Sparse Markov decision processes with causal sparse tsallis entropy regularization for reinforcement learning",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Gen Li",
                "Yuting Wei",
                "Yuejie Chi",
                "Yuantao Gu",
                "Yuxin Chen"
            ],
            "title": "Breaking the Sample Size Barrier in ModelBased Reinforcement Learning with a Generative Model",
            "venue": "In Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gen Li",
                "Changxiao Cai",
                "Yuxin Chen",
                "Yuantao Gu",
                "Yuting Wei",
                "Yuejie Chi"
            ],
            "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis",
            "venue": "arXiv preprint arXiv:2102.06548,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Li",
                "Wenhao Yang",
                "Zhihua Zhang",
                "Michael I Jordan"
            ],
            "title": "Polyak-Ruppert Averaged Q-Leaning is Statistically Efficient",
            "venue": "arXiv preprint arXiv:2112.14582,",
            "year": 2021
        },
        {
            "authors": [
                "Jincheng Mei",
                "Chenjun Xiao",
                "Csaba Szepesvari",
                "Dale Schuurmans"
            ],
            "title": "On the Global Convergence Rates of Softmax Policy Gradient Methods",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Gergely Neu",
                "Anders Jonsson",
                "Vicen\u00e7 G\u00f3mez"
            ],
            "title": "A unified view of entropy-regularized Markov decision processes",
            "venue": "arXiv preprint arXiv:1705.07798,",
            "year": 2017
        },
        {
            "authors": [
                "Bruno Scherrer",
                "Boris Lesner"
            ],
            "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Aaron Sidford",
                "Mengdi Wang",
                "Xian Wu",
                "Lin Yang",
                "Yinyu Ye"
            ],
            "title": "Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Vamplew",
                "Richard Dazeley",
                "Cameron Foale"
            ],
            "title": "Softmax exploration strategies for multiobjective reinforcement",
            "venue": "learning. Neurocomputing,",
            "year": 2017
        },
        {
            "authors": [
                "Nino Vieillard",
                "Tadashi Kozuno",
                "Bruno Scherrer",
                "Olivier Pietquin",
                "Remi Munos",
                "Matthieu Geist"
            ],
            "title": "Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nino Vieillard",
                "Olivier Pietquin",
                "Matthieu Geist"
            ],
            "title": "Munchausen Reinforcement Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Martin J Wainwright"
            ],
            "title": "Variance-reduced Q-learning is minimax optimal",
            "venue": "arXiv preprint arXiv:1906.04697,",
            "year": 2019
        },
        {
            "authors": [
                "Wenhao Yang",
                "Xiang Li",
                "Zhihua Zhang"
            ],
            "title": "A regularized approach to sparse optimal policy in reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dongruo Zhou",
                "Quanquan Gu",
                "Csaba Szepesvari"
            ],
            "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the generative model setting, the agent has access to a simulator of a Markov decision process (MDP), to which the agent can query next states of arbitrary state-action pairs (Azar et al., 2013). The agent seeks a near-optimal policy using as small number of queries as possible.\nWhile the generative model setting is simpler than the online reinforcement learning (RL) setting, proof techniques developed under this setting often generalize to more complex settings. For example, the totalvariance technique developed by Azar et al. (2013) and Lattimore & Hutter (2012) is now an indispensable tool for a sharp analysis of RL algorithms in the online RL setting for tabular MDP (Azar et al., 2017; Jin et al., 2018) and linear function approximation (Zhou et al., 2021).\nIn this paper, we consider a model-free approach for the generative model setting with tabular MDP. Particularly, we analyze mirror descent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al. (2020a), which uses Kullback-Leibler (KL) divergence and entropy regularization in its value and policy updates. We prove its near minimax-optimal sample complexity for finding an \u03b5-optimal policy when \u03b5 is sufficiently small. Our result and analysis have the following consequences.\nFirst, we demonstrate the effectiveness of KL and entropy regularization. There are some previous works that argue the benefit of regularization from a theoretical perspective in value-iteration-like algorithms (Kozuno et al., 2019; Vieillard et al., 2020a,b) and policy optimizaiton (Mei et al., 2020; Cen et al., 2021; Lan, 2022). Compared to those works, we show that simply combining value iteration with regularization achieves the near minimax-optimal sample complexity.\nSecond, as discussed by Vieillard et al. (2020a), MDVI encompasses various algorithms as special cases or equivalent forms. While we do not analyze each algorithm, most of them are minimax-optimal too in the generative model setting with tabular MDP.\nLastly and most importantly, MDVI uses no variance-reduction technique, in contrast to previous modelfree approaches (Sidford et al., 2018; Wainwright, 2019; Khamaru et al., 2021). Consequently, our analysis is straightforward, and it would be easy to extend it to more complex settings, such as the online RL and linear function approximation. Furthermore, previous approaches need pessimism to obtain a near-optimal\n\u2217Correspondence: tadashi.kozuno@gmail.com. 1 University of Alberta, 2 Peking University, 3 Google Research, Brain team, 4 Universit\u00e9 de Lorraine, CNRS, INRIA, IECL, 5 University of Tokyo, 6 DeepMind, 7 Otto von Guericke University Magdeburg.\nar X\niv :2\n20 5.\n14 21\n1v 1\n[ cs\n.L G\n] 2\n7 M\nay 2\npolicy, which prevents them from being extended to the online RL setting, where the optimism plays an important role for an efficient exploration (Azar et al., 2017; Jin et al., 2018). On the other hand, MDVI is compatible with optimism. Our analysis paves the way for the combination of online exploration techniques with minimax model-free algorithms."
        },
        {
            "heading": "2 Related work",
            "text": "Write \u03b3, H, X, and A for the discount factor, effective horizon 11\u2212\u03b3 , and number of states and actions.\nLearning with a generative model In the generative model setting, there are two problem settings: finding (i) an \u03b5-optimal Q-value function with probability at least 1\u2212 \u03b4, and (ii) an \u03b5-optimal policy with probability at least 1\u2212 \u03b4, where \u03b4 \u2208 (0, 1), and \u03b5 > 0. Both problems are known to have sample complexity lower bounds of \u2126(XAH3/\u03b52) (Azar et al., 2013; Sidford et al., 2018). Note that even if an \u03b5-optimal Q-value function is obtained, additional data and computation are necessary to find an \u03b5-optimal policy (Sidford et al., 2018). In this paper, we consider the learning of an \u03b5-optimal policy.\nThere exist minimax-optimal model-based algorithms for learning a near-optimal value function (Azar et al., 2013) and policy (Agarwal et al., 2020; Li et al., 2020). Also, there exist minimax-optimal model-free algorithms for learning a near-optimal value function (Wainwright, 2019; Khamaru et al., 2021; Li et al., 2021b) and policy (Sidford et al., 2018). While model-based algorithms are conceptually simple, they have a higher computational complexity than that of model-free algorithms. The algorithm (MDVI) we analyze in this paper is a model-free algorithm for finding a near-optimal policy, and has a low computational complexity.\nArguably, Q-learning is one of the simplest model-free algorithms (Watkins & Dayan, 1992; Even-Dar et al., 2003). Unfortunately, Li et al. (2021a) provide a tight analysis of Q-learning and show that its sample complexity is O\u0303(XAH4/\u03b52) for finding an \u03b5-optimal Q-value function, 1 which is one H factor away from the lower bound. To remove the extra H factor, some works (Sidford et al., 2018; Wainwright, 2019; Khamaru et al., 2021) leverage variance reduction techniques. While elegant, variance reduction techniques lead to multi-epoch algorithms with involved analyses. In contrast, MDVI requires no variance reduction and is significantly simpler.\nMDVI\u2019s underlying idea that enables such simplicity is, while implicit, the averaging of value function estimates. Li et al. (2021b) shows that averaging Q-functions computed in Q-learning can find a near-optimal Q-function with a minimax-optimal sample complexity. Azar et al. (2011) also provides a simple algorithm called Speedy Q-learning (SQL), which performs the averaging of value function estimates. In fact, as argued in (Vieillard et al., 2020a), SQL is equivalent to a special case of MDVI with only KL regularization. While previous works on MDVI (Vieillard et al., 2020a) and an equivalent algorithm called CVI (Kozuno et al., 2019) provide error propagation analyses, they do not provide sample complexity. 2 This paper proves the first nearly minimax-optimal sample complexity bound for MDVI-type algorithm. We tighten previous results by (i) using the entropy regularization, which speeds up the convergence rate, (ii) improved error propagation analyses (Lemmas 1 and 9), and (iii) careful application of the total variance technique (Azar et al., 2013).\nIn addition to the averaging, Theorem 1 is based on the idea of using a non-stationary policy (Scherrer & Lesner, 2012). While the last policy of MDVI is near-optimal when \u03b5 is small, a non-stationary policy constructed from policies outputted by MDVI is near-optimal for a wider range of \u03b5.\nRange of Valid \u03b5 Although there are multiple minimax-optimal algorithms for the generative model setting, there ranges of valid \u03b5 differ. The model-based algorithm by Azar et al. (2013) is nearly minimax-optimal for \u03b5 \u2264 \u221a H/X, which is later improved to \u221a H by Agarwal et al. (2020), and to H by Li et al. (2020). As for model-free approaches, the algorithm by Sidford et al. (2018) is nearly minimax-optimal for \u03b5 \u2264 1. MDVI is nearly minimax-optimal for \u03b5 \u2264 1/ \u221a H (non-stationary policy case, Theorem 1) and \u03b5 \u2264 1/H (last policy\n1O\u0303 hides terms poly-logarithmic in H, X, A, 1/\u03b5, and 1/\u03b4. 2Vieillard et al. (2020a) note SQL\u2019s sample complexity of O\u0303(XAH4/\u03b52) for finding a near-optimal policy as a corollary of\ntheir result without proof.\ncase, Theorem 2). Therefore, it has one of the narrowest range of valid \u03b5 (second worst) compared to other algorithms. It is unclear if this is an artifact of our analysis or the real limitation of MDVI-type algorithm. We leave this topic as a future work.\nRegularization in MDPs Sometimes, regularization is added to the reward to encourage exploration in MDPs (Fox et al., 2016; Vamplew et al., 2017). In recent years, Neu et al. (2017); Geist et al. (2019); Lee et al. (2018); Yang et al. (2019) have provided a unified framework for regularized MDPs. Specifically, Geist et al. (2019) propose the Regularized Modified Policy Iteration algorithm and Mirror Descent Modified Policy Iteration to solve regularized MDPs. In the meantime, Vieillard et al. (2020a) provide theoretical guarantees of KL-regularized value iteration in the approximate setting. Particularly, they show that KL regularization results in the averaging of Q-value functions and show that the averaging leads to an improved error propagation result. We extend their improved error propagation result to a KL- and entropy- regularization case. Our results provide theoretical underpinnings to many regularized RL algorithms in Vieillard et al. (2020a, Table 1) and a high-performing deep RL algorithm called Munchausen DQN (Vieillard et al., 2020b)."
        },
        {
            "heading": "3 Preliminaries",
            "text": "For a set S, we denote its complement as Sc. For a positive integer N , we let [N ] := {1, . . . , N}. Without loss of generality, every finite set is assumed to be a subset of integers. For a finite set, say S, the set of probability distributions over S is denoted by \u2206(S). For a vector v \u2208 RM , its m-th element is denoted by vm or v(m). 3 We let 1 := (1, . . . , 1)> and 0 := (0, . . . , 0)>, whose dimension will be clear from the context. For a matrix A \u2208 RN\u00d7M , we denote its n-th row and m-th value of the n-th row by An and Amn , respectively. The expectation and variance of a random variable X are denoted as E[X] and V[X], respectively. The empty sum is defined to be 0, e.g., \u2211k i=j ai = 0 if j > k.\nWe consider a Markov Decision Process (MDP) defined by (X,A, \u03b3, r, P ), where X is the state space of size X, A the action space of size A, \u03b3 \u2208 [0, 1) the discount factor, r \u2208 [\u22121, 1]XA the reward vector with rx,a denoting the reward when taking an action a at a state x, and P \u2208 RXA\u00d7X state transition probability matrix with P yx,a denoting the state transition probability to a new state y from a state x when taking an action a. We let H be the (effective) time horizon 1/(1\u2212 \u03b3).\nNote that (Pv)(x, a) = E [v(X1)|X0 = x,A0 = a] for any v \u2208 RX . Any policy \u03c0 is identified as a matrix \u03c0 \u2208 RX\u00d7XA such that (\u03c0q)(x) := \u2211 a\u2208A \u03c0(a|x)q (x, a) for any q \u2208 RXA. For convenience, we adopt a shorthand notation, P\u03c0 := P\u03c0. With these notations, the Bellman operator T\u03c0 for a policy \u03c0 is defined as an operator such that T\u03c0q := r+ \u03b3P\u03c0q. The Q-value function q\u03c0 for a policy \u03c0 is its unique fixed point. The state-value function v\u03c0 is defined as \u03c0q\u03c0. An optimal policy \u03c0\u2217 is a policy such that v\u2217 := v\u03c0\u2217 \u2265 v\u03c0 for any policy \u03c0, where the inequality is point-wise."
        },
        {
            "heading": "4 Mirror Descent Value Iteration and Main Results",
            "text": "For any policies \u03c0 and \u00b5, let log \u03c0 and log \u03c0\u00b5 be the functions x, a 7\u2192 log \u03c0(a|x) and x, a 7\u2192 log \u03c0(a|x) \u00b5(a|x) over X\u00d7A. We analyze (approximate) MDVI whose update is the following (Vieillard et al., 2020a):\nqk+1 = r + \u03b3Pvk + \u03b5k , (1)\nwhere vk := \u03c0k ( qk \u2212 \u03c4 log\n\u03c0k \u03c0k\u22121\n\u2212 \u03ba log \u03c0k ) ,\n\u03c0k (\u00b7|x) = arg max p\u2208\u2206(A) \u2211 a\u2208A p(a) ( qk(s, a)\u2212 \u03c4 log\np(a)\n\u03c0k\u22121 (a|x) \u2212 \u03ba log p(a)\n) (2)\n3Unless noted otherwise, all vectors are column vectors.\nAlgorithm 1: MDVI(\u03b1,K,M) Input: \u03b1 \u2208 [0, 1), number of iterations K, and number of next-state samples per iteration M . Let s0 = 0 \u2208 RXA and w0 = w\u22121 = 0 \u2208 RX ; for k from 0 to K \u2212 1 do\nLet vk = wk \u2212 \u03b1wk\u22121; for each state-action pair (x, a) \u2208 X\u00d7A do\nSample (yk,m,x,a)Mm=1 from the generative model P (\u00b7|x, a); Let qk+1(x, a) = r(x, a) + \u03b3M\u22121 \u2211M m=1 vk(yk,m,x,a);\nend Let sk+1 = qk+1 + \u03b1sk and wk+1(x) = maxa\u2208A sk+1(x, a) for each x \u2208 X;\nend return (\u03c0k)Kk=0 , where \u03c0k is greedy policy with respect to sk;\nfor all x \u2208 X, and \u03b5k : X\u00d7A\u2192 R is an \u201cerror\u201d function, which abstractly represents the deviation of qk+1 from the update target r+\u03b3Pvk. In other words, MDVI is value iteration with KL and entropy regularization.\nLet sk := qk + \u03b1sk\u22121 = \u2211k\u22121 j=0 \u03b1\njqk\u2212j . The policy (2) can be rewritten as a Boltzmann policy of sk, i.e., \u03c0k(a|x) \u221d exp (\u03b2sk(x, a)), where \u03b1 := \u03c4/(\u03c4 +\u03ba), and \u03b2 := 1/(\u03c4 +\u03ba), see Appendix B for details. Substituting \u03c0k\u22121 and \u03c0k in vk with this expression of the policy, we deduce that\nvk(x) = 1 \u03b2 log \u2211 a\u2208A exp (\u03b2sk(x, a))\u2212 \u03b1 \u03b2 log \u2211 a\u2208A exp (\u03b2sk\u22121(x, a)) .\nThus, letting wk be the function x 7\u2192 \u03b2\u22121 log \u2211 a\u2208A exp (\u03b2sk(x, a)) over X, MDVI\u2019s update rules can be equivalently written as\nqk+1 = r + \u03b3P (wk \u2212 \u03b1wk\u22121) + \u03b5k and \u03c0k(a|x) \u221d exp (\u03b2sk(x, a)) for all (x, a) \u2208 X\u00d7A .\nA sample-approximate version of MDVI shown in Algorithm 1 (MDVI) uses this equivalent form of MDVI. Furthermore, for simplicity of the analysis, we consider the limit of \u03c4, \u03ba\u2192 0 while keeping \u03b1 = \u03c4/(\u03c4 + \u03ba) to a constant value (which corresponds to letting \u03b2 \u2192\u221e).\nRemark 1. Even if \u03b2 is finite, MDVI is nearly minimax-optimal as long as \u03b2 is large enough. Indeed, \u03b2\u22121 log \u2211 a\u2208A exp(q(x, a)) satisfies (Kozuno et al., 2019, Lemma 7) that\nmax a\u2208A q(x, a) \u2264 \u03b2\u22121 log \u2211 a\u2208A exp(q(x, a)) \u2264 max a\u2208A q(x, a) + \u03b2\u22121 logA .\nThus, while \u03b2 appears in the proofs of Theorems 1 and 2 if it is finite, it always appear as \u03b2\u22121 logA multiplied by H-dependent constant. Therefore, MDVI is nearly minimax-optimal as long as \u03b2 is large enough.\nWhy KL Regularization? The weight \u03b1 used in sk updates monotonically increases as the coefficient of the KL regularization \u03c4 increases. As we see later, error terms appear in upper bounds of \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e as (1\u2212 \u03b1) \u2211k j=1 \u03b1 k\u2212j\u03b5j . Applying Azuma-Hoeffiding inequality, it is approximately bounded by H \u221a\n1\u2212 \u03b1. Therefore, MDVI becomes more robust to sampling error as \u03b1 increases. The KL regularization confers this benefit to the algorithm.\nWhy Entropy Regularization? When there is no entropy regularization (\u03b1 = 1), the convergence rate of MDVI becomes 1/K while it is \u03b1K for \u03b3 \u2264 \u03b1 < 1 (Vieillard et al., 2020a). In the former case, we need to set K \u2248 H2/\u03b5, whereas in the latter case, K \u2248 1/(1\u2212 \u03b1) suffices. Since we will set \u03b1 to either \u03b3 or 1\u2212 (1\u2212 \u03b3)2, K \u2248 H or H2. Thus, we can use more samples per one value update (i.e., larger M). A larger M leads to a smaller value estimation variance (\u03c3(vk) in Lemma 6), which is important to improve the range of \u03b5. Even when \u03b1 = 1, MDVI is nearly minimax-optimal (proof omitted). However, \u03b5 must be less than or equal to 1/H2.\nMain Theoretical Results The following theorems show the near minimax-optimality of MDVI. For a sequence of policies (\u03c0k)Kk=0 outputted by MDVI, we let \u03c0 \u2032 k be the non-stationary policy that follows \u03c0k\u2212t at the t-th time step until t = k, after which \u03c00 is followed. 4 Note that the value function of such a non-stationary policy is given by v\u03c0 \u2032 k = \u03c0kT \u03c0k\u22121 \u00b7 \u00b7 \u00b7T\u03c01q\u03c00 . Theorem 1. Assume that \u03b5 \u2208 (0, 1/ \u221a H]. Then, there exist positive constants c1, c2 \u2265 1 independent of H, X, A, \u03b5, and \u03b4 such that when MDVI is run with the settings\n\u03b1 = \u03b3 ,K =\n\u2308 3\n1\u2212 \u03b1 log\nc1H\n\u03b5 + 2\n\u2309 , and M = \u2308 c2H 2\n\u03b52 log\n16KXA\n\u03b4\n\u2309 ,\nit outputs a sequence of policies (\u03c0k)Kk=0 such that \u2016v\u2217 \u2212 v\u03c0 \u2032 K\u2016\u221e \u2264 \u03b5 with probability at least 1\u2212 3\u03b4/4, using O\u0303 ( H3XA/\u03b52 ) samples from the generative model.\nStoring all policies requires the memory space of KXA and can be prohibitive in some cases. The next theorem shows that the last policy outputted by MDVI is near-optimal when \u03b5 \u2264 1/H.\nTheorem 2. Assume that \u03b5 \u2208 (0, 1/H]. Then, there exist positive constants c3, c4 \u2265 1 independent of H, X, A, \u03b5, and \u03b4 such that when MDVI is run with the settings\n\u03b1 = 1\u2212 (1\u2212 \u03b3)2 ,K = \u2308 5\n1\u2212 \u03b1 log\nc3H\n\u03b5 + 2\n\u2309 , and M = \u2308 c4H\n\u03b52 log\n16KXA\n\u03b4\n\u2309 ,\nit outputs a sequence of policies (\u03c0k)Kk=0 such that \u2016v\u2217 \u2212 v\u03c0K\u2016\u221e \u2264 \u03b5 with probability at least 1 \u2212 \u03b4, using O\u0303 ( H3XA/\u03b52 ) samples from the generative model."
        },
        {
            "heading": "5 Proofs of the Main Results",
            "text": "Before the proof, we introduce some notations. A table of notations is provided in Appendix A.\nNotation. denotes an indefinite constant that changes throughout the proof and is independent of H, X, A, \u03b5, and \u03b4. We let A\u03b3,k := \u2211k\u22121 j=0 \u03b3 k\u2212j\u03b1j and Ak := \u2211k\u22121 j=0 \u03b1\nj for any non-negative integer k with A\u221e := 1/(1 \u2212 \u03b1). Fk,m denotes the \u03c3-algebra generated by random variables {yj,n,x,a|(j, n, x, a) \u2208 [k \u2212 2]\u00d7 [M ]\u00d7X\u00d7A} \u222a {yj,n,x,a|(j, n, x, a) \u2208 {k \u2212 1} \u00d7 [m\u2212 1]\u00d7X\u00d7A}. For any k \u2208 {0} \u222a [K \u2212 1] and v \u2208 RX , Var(v) and P\u0302kv denote the functions\nVar(v) : (x, a) 7\u2192 (Pv2)(x, a)\u2212 (Pv)2(x, a) and P\u0302kv : (x, a) 7\u2192 \u2211M m=1 v(yk,m,x,a)/M ,\nrespectively. We often write \u221a Var(v) as \u03c3(v). Furthermore, \u03b5k and Ek denote \u201cerror\u201d functions\n\u03b5k : (x, a) 7\u2192 \u03b3P\u0302k\u22121vk\u22121(x, a)\u2212 \u03b3Pvk\u22121(x, a) and Ek : (x, a) 7\u2192 \u2211k j=1 \u03b1 k\u2212j\u03b5j(x, a) ,\nrespectively. (Note that \u03b51 = E1 = 0 since v0 = 0.) For a sequence of policies (\u03c0k)k\u2208Z, we let T ij := T\u03c0iT\u03c0i\u22121 \u00b7 \u00b7 \u00b7T\u03c0j+1T\u03c0j for i \u2265 j, and T ij := I otherwise. We also let P ij := P\u03c0iP\u03c0i\u22121 \u00b7 \u00b7 \u00b7P\u03c0j+1P\u03c0j for i \u2265 j, and P ij := I otherwise. As a special case with \u03c0k = \u03c0\u2217 for all k, we let P i\u2217 := (P\u03c0\u2217)i. Finally, throughout the proof, \u03b91 and \u03b92 denotes log(8KXA/\u03b4) and log(16KXA/\u03b4), repspectively.\n4The time step index t starts from 0."
        },
        {
            "heading": "5.1 Proof of Theorem 1 (Near-optimality of the Non-stationary Policy)",
            "text": "The first step of the proof is the error propagation analysis of MDVI given below. It differs from the one of Vieillard et al. (2020a) since ours upper-bounds v\u2217 \u2212 v\u03c0\u2032k . It is proven in Appendix F.1\nLemma 1. For any k \u2208 [K], 0 \u2264 v\u2217 \u2212 v\u03c0\u2032k \u2264 \u0393k, where\n\u0393k := 1\nA\u221e k\u22121\u2211 j=0 \u03b3j ( \u03c0kP k\u22121 k\u2212j \u2212 \u03c0\u2217P j \u2217 ) Ek\u2212j + 2H ( \u03b1k + A\u03b3,k A\u221e ) 1 .\nFrom this result, it can be seen that an upper bound for each Ek is necessary. The following lemma provides an upper bound, which readily lead to Lemma 3 when combined with Lemma 1. These lemmas are proven in Appendix F.2.\nLemma 2. Let E1 be the event that \u2016Ek\u2016\u221e < 3H \u221a A\u221e\u03b91/M for all k \u2208 [K]. Then, P (Ec1) \u2264 \u03b4/4.\nLemma 3. Assume that \u03b5 \u2208 (0, 1]. When MDVI is run with the settings \u03b1, K, and M in Theorem 1, under the event E1, its output policies (\u03c0k)Kk=0 satisfy that \u2016v\u2217 \u2212 v\u03c0 \u2032 k\u2016\u221e \u2264 2(k +H)\u03b3k + \u03b5 \u221a H/c2 for all k \u2208 [K].\nFurthermore, \u2016v\u2217 \u2212 v\u03c0\u2032K\u2016\u221e \u2264 \u221a H\u03b5 for some c1, c2 \u2265 1.\nUnfortunately, Lemma 3 is insufficient to show the minimax optimality of MDVI since it only holds that \u2016v\u2217 \u2212 v\u03c0\u2032K\u2016\u221e \u2264 \u221a H\u03b5 while P(E1) \u2265 1\u2212 \u03b4. Any other setting of \u03b1, \u03b2, K, and M does not seem to lead to \u2016v\u2217 \u2212 v\u03c0\u2032K\u2016\u221e \u2264 \u03b5. Nonetheless, Lemma 3 turns out to be useful later to obtain a refined result. To show the minimax optimality, we need to remove the extra \u221a H factor. The standard tools for this purpose are a Bernstein-type inequality and the total variance (TV) technique (Azar et al., 2013), which leverages the fact that \u2016(I \u2212 \u03b3P\u03c0)\u22121\u03c3(v\u03c0)\u2016\u221e \u2264 \u221a 2H3 for any policy \u03c0. In our case, the TV technique for a non-stationary policy is required due to \u03c0kP k\u22121k\u2212j , though. Recall the definition of \u03b5k and note that its standard deviation consists of \u03c3(vk\u22121). As we use a Bernstein inequality for martingale because of Ek, we derive an upper bound for the sum of \u03c3(vj\u22121)2 over j \u2208 [k] (V in Lemma 19) using the fact that \u03c3(vj\u22121) \u2248 \u03c3(v\u2217) when vj\u22121 \u2248 v\u2217. To this end, the following lemma, proven in Appendix F.3, is useful.\nLemma 4. For any k \u2208 [K],\n\u22122\u03b3kH1\u2212 k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22121 k\u2212j \u03b5k\u2212j \u2264 v \u2217 \u2212 vk \u2264 \u0393k\u22121 + 2H\u03b3k1\u2212 k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22122 k\u22121\u2212j\u03b5k\u2212j .\nCombining this lemma with Lemma 2 and the following one, we can obtain an upper-bound for \u03c3(vk\u22121). The proofs of both results are given in Appendix F.4.\nLemma 5. Let E2 be the event that \u2016\u03b5k\u2016\u221e < 3H \u221a \u03b91/M for all k \u2208 [K]. Then, P (Ec2) \u2264 \u03b4/4.\nLemma 6. Conditioned on the event E1 \u2229 E2, it holds for any k \u2208 [K] that\n\u03c3(vk) \u2264 2H min {\n1, 2 max{\u03b1, \u03b3}k\u22121 + A\u03b3,k\u22121 A\u221e + 6H \u221a \u03b91 M } 1 + \u03c3(v\u2217) . (3)"
        },
        {
            "heading": "Furthermore, \u03c3(v0) = 0.",
            "text": "Using Lemma 6, we can prove refined bounds for Ek and \u03b5k, as in Appendix F.5.\nLemma 7. Let E3 be the event that\n|Ek|(x, a) < 4H\u03b92 3M\n+ \u221a 2Vk(x, a)\u03b92 for all (x, a, k) \u2208 X\u00d7A\u00d7 [K] ,\nwhere Vk := 4 \u2211k j=1 \u03b1 2(k\u2212j)Varj/M with\nVarj := Var(v \u2217) + 4H2 ( 4 max{\u03b1, \u03b3}2j\u22122 +\nA2\u03b3,j\u22122 A2\u221e + 36H2\u03b91 M\n) 1\nfor k \u2265 2 and Var1 := 0. Then, P (Ec3 |E1 \u2229 E2) \u2264 \u03b4/4.\nLemma 8. Let E4 be the event that\n|\u03b5k|(x, a) < 4H\u03b92 3M\n+ \u221a 2Wk(x, a)\u03b92 for all (x, a, k) \u2208 X\u00d7A\u00d7 [K]\nwhere Wk := 4Vark/M . Then, P (Ec4 |E1 \u2229 E2) \u2264 \u03b4/4.\nWith these lemmas, we are ready to prove Theorem 1.\nProof of Theorem 1. We condition the proof by E1 \u2229 E2 \u2229 E3. As for any events A and B, P(A \u2229 B) = P((A \u222aBc) \u2229B) \u2265 1\u2212 P(Ac \u2229B)\u2212 P(Bc) , and P(Ac \u2229B) = P(Ac|B)P(B) \u2264 P(Ac|B),\nP(E1 \u2229 E2 \u2229 E3) \u2265 1\u2212 P(Ec3 |E1 \u2229 E2)\u2212 P((E1 \u2229 E2)c) \u2265 1\u2212 P(Ec3 |E1 \u2229 E2)\u2212 P(Ec1)\u2212 P(Ec2) .\nTherefore, from Lemmas 2, 5, and 7, we conclude that P(E1 \u2229 E2 \u2229 E3) \u2265 1\u2212 3\u03b4/4 . Accordingly, any claim proven under E1 \u2229 E2 \u2229 E3 holds with probability at least 1\u2212 3\u03b4/4.\nFrom Lemma 1, the setting that \u03b1 = \u03b3, and the monotonicity of stochastic matrices,\nv\u2217 \u2212 v\u03c0 \u2032 K \u2264 1\nH K\u22121\u2211 k=0\n\u03b3k\u03c0\u2217P k \u2217 |EK\u2212k|\ufe38 \ufe37\ufe37 \ufe38 \u2665\n+ 1\nH K\u22121\u2211 k=0\n\u03b3k\u03c0KP K\u22121 K\u2212k |EK\u2212k|\ufe38 \ufe37\ufe37 \ufe38 \u2663 +2 (H +K) \u03b3K1 .\nAs the last term is less than \u03b5/c1 from Lemma 15, it remains to upper-bound \u2665 and \u2663. We note that A\u221e = H and A\u03b3,k = k\u03b3k under the considered setting of \u03b1.\nFrom the settings of \u03b1 and M ,\n2Vk\u03b92 \u2264 Var(v\u2217)\u03b52\nc2H +\n\u03b52\nc2\n( k\u03b32(k\u22122) + \u03b32(k\u22122)\nH2 k\u2211 j=2\n(j \u2212 2)2\ufe38 \ufe37\ufe37 \ufe38 \u2264 k3 from (a)\n+ \u03b52\nc2 k\u2211 j=2\n\u03b32(k\u2212j)\ufe38 \ufe37\ufe37 \ufe38 \u2264H\n) 1 ,\nwhere (a) follows from Lemma 16. From this result and Lemma 11, it follows that\n\u2665 \u2264 \u03b5 2\nc2 1 + \u03b5\u221a c2H K\u22121\u2211 k=0 \u03b3k\u03c0\u2217P k \u2217 \u03c3(v\n\u2217)\ufe38 \ufe37\ufe37 \ufe38 \u2264 \u221a 2H31from Lemma 22 + \u03b5 \u221a c2\n( \u03b3K\u22122 K\u2211 k=1 ( \u221a k + k \u221a k H ) \ufe38 \ufe37\ufe37 \ufe38 \u2264 (K2.5/H) from (a) +H \u221a H\u03b5 ) 1 ,\nwhere (a) follows from Lemma 16 and that H \u2264 K. From Lemma 15, K2.5\u03b3K\u22122/H \u2264 \u03b5/c1. Therefore, using the inequality \u03b5 \u2264 1/ \u221a H \u2264 1, H\u22121\u2665 \u2264 ( c\u221212 + c \u22120.5 2 ) \u03b51.\nAlthough an upper bound for \u2663 can be similarly derived, a care must be taken when upper-bounding \u2666 := \u2211K\u22121 k=0 \u03b3 k\u03c0KP K\u22121 K\u2212k\u03c3(v \u2217). From Lemma 21, for any k \u2208 [K],\n\u03c3(v\u2217) \u2264 \u03c3(v\u2217 \u2212 v\u03c0 \u2032 k) + \u03c3(v\u03c0 \u2032 k) \u2264 2(k +H)\u03b3k1 + \u221a H/c2\u03b51 + \u03c3(v \u03c0\u2032k) ,\nwhere the second inequality follows from Lemmas 3 and 20. Accordingly,\n\u2666 \u2264 2\u03b3K K\u22121\u2211 k=0\n(k +H)\ufe38 \ufe37\ufe37 \ufe38 \u2264 K2 from (a)\n1 + H \u221a H/c2\u03b51 + K\u22121\u2211 k=0 \u03b3k\u03c0KP K\u22121 K\u2212k\u03c3(v\n\u03c0\u2032K\u2212k)\ufe38 \ufe37\ufe37 \ufe38 \u2264 \u221a 2H31 from Lemma 22\n\u2264 H \u221a H ,\nwhere (a) follows from Lemma 16 and that H \u2264 K, and the second inequality follows since \u03b5 \u2264 1/ \u221a H \u2264 1 and K2\u03b3K \u2264 \u03b5/c1 from Lemma 15. Thus, H\u22121\u2663 \u2264 (c\u221212 + c \u22120.5 2 )\u03b51.\nCombining these results, we conclude that there are constants c1 and c2 that satisfy the claim."
        },
        {
            "heading": "5.2 Proof of Theorem 2 (Near-optimality of the Last Policy)",
            "text": "We need the following error propagation result. Its proof is given in Appendix G.1.\nLemma 9 (Error Propagation of MDVI). For any k \u2208 [K],\n0 \u2264 v\u2217 \u2212 v\u03c0k \u2264 2H ( \u03b1k +\nA\u03b3,k A\u221e\n) 1 + 1\nA\u221e (N \u03c0k\u03c0k \u2212N \u03c0\u2217\u03c0\u2217)Ek\n+ 1\nA\u221e k\u2211 j=1 \u03b3j ( N \u03c0\u2217\u03c0\u2217P kk+1\u2212j \u2212N \u03c0k\u03c0kP k\u22121k\u2212j ) E\u2032k+1\u2212j ,\nwhere N \u03c0 := \u2211\u221e t=0(\u03b3\u03c0P ) t for any policy \u03c0, and E\u2032k+1\u2212j := \u03b5k+1\u2212j \u2212 (1\u2212 \u03b1)Ek\u2212j.\nThe following lemma is an analogue of Lemma 3. It is proven in Appendix G.2.\nLemma 10. Assume that \u03b5 \u2208 (0, 1]. When MDVI is run with the settings \u03b1, K, and M in Theorem 2, under the event E1 \u2229 E2, its output policies (\u03c0k)Kk=0 satisfy that \u2016v\u2217 \u2212 v\u03c0 \u2032 k\u2016\u221e \u2264 H\u03b1k + \u03b5 \u221a H/c4 and\n\u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 H\u03b1k + \u03b5 \u221a H/c4 for all k \u2208 [K].\nNow, we are ready to prove Theorem 2.\nProof of Theorem 2. We condition the proof by E1 \u2229 E2 \u2229 E3 \u2229 E4. Since for any events A and B, P(A\u2229B) = P((A \u222aBc) \u2229B) \u2265 1\u2212 P(Ac \u2229B)\u2212 P(Bc), and P(Ac \u2229B) = P(Ac|B)P(B) \u2264 P(Ac|B),\nP(E1 \u2229 E2 \u2229 E3 \u2229 E4) \u2265 1\u2212 P((E3 \u2229 E4)c|E1 \u2229 E2)\u2212 P((E1 \u2229 E2)c) \u2265 1\u2212 P(Ec3 \u222a Ec4 |E1 \u2229 E2)\u2212 P(Ec1)\u2212 P(Ec2) \u2265 1\u2212 P(Ec3 |E1 \u2229 E2)\u2212 P(Ec4 |E1 \u2229 E2)\u2212 P(Ec1)\u2212 P(Ec2) .\nTherefore, from Lemmas 2, 5, 7, and 8, we conclude that P(E1 \u2229 E2 \u2229 E3 \u2229 E4) \u2265 1\u2212 \u03b4 . Accordingly, any claim proven under E1 \u2229 E2 \u2229 E3 \u2229 E4 holds with probability at least 1\u2212 \u03b4.\nFrom Lemma 9, the setting that \u03b1 = 1\u2212 (1\u2212 \u03b3)2, and the monotonicity of stochastic matrices,\nv\u2217 \u2212 v\u03c0K \u2264 2H ( \u03b1K +\n2A\u03b3,K H\n) 1 + 1\nH2 (N \u03c0K\u03c0K +N \u03c0\u2217\u03c0\u2217) |EK |\ufe38 \ufe37\ufe37 \ufe38\n:=\u2665\n+ 1\nH2 K\u2211 k=1 \u03b3k ( N \u03c0\u2217\u03c0\u2217PKK+1\u2212k +N \u03c0K\u03c0KPK\u22121K\u2212k )( |\u03b5K+1\u2212k|+ 1 H2 |EK\u2212k| ) \ufe38 \ufe37\ufe37 \ufe38\n:=\u2663\n,\nwhere E0 := 0. The first term can be bounded by \u03b1KH \u2264 \u03b5/c3 from Lemmas 13 and 15. In the sequel, we derive upper bounds for \u2665 and \u2663. We note that A\u221e = H2 and A\u03b3,k \u2264 \u03b1kH.\nNext, we derive an upper bound for \u2665. From the settings of \u03b1(\u2265 \u03b3) and M ,\n2Vk\u03b92 \u2264 HVar(v\u2217)\u03b52\nc4 +\nH\u03b52\nc4\n( k\u03b12(k\u22122) + H3\u03b52\nc4\n) 1 .\nFrom this result and Lemma 11, it follows that\n\u2665 H2 \u2264 \u03b5 2 c4H +\n\u03b5\nH \u221a c4H (N \u03c0K\u03c0K +N \u03c0\u2217\u03c0\u2217)\u03c3(v\u2217) + \u03b5 \u221a c4 (\u221a K/H\u03b1K\u22122\ufe38 \ufe37\ufe37 \ufe38\n\u2264\u03b5/c3 from (a)\n+ H\u03b5/ \u221a c4\ufe38 \ufe37\ufe37 \ufe38\n\u22641/\u221ac4 from (b)\n) 1 ,\nwhere (a) follows from Lemma 15, and (b) follows by the assumption that \u03b5 \u2264 1/H. By Lemma 22, N \u03c0\u2217\u03c0\u2217\u03c3(v\u2217) \u2264 \u221a H3. Furthermore, from Lemmas 10 and 20,\nN \u03c0K\u03c0K\u03c3(v\u2217) \u2264 H2\u03b1K\ufe38 \ufe37\ufe37 \ufe38 \u2264 H \u221a H from (a)\n+ \u03b5H \u221a H/c4 + N \u03c0K\u03c0K\u03c3(v\u03c0K )\ufe38 \ufe37\ufe37 \ufe38\nH \u221a H from Lemma 22\n\u2264 H \u221a H1 ,\nwhere (a) follows from Lemma 15, and the last inequality follows since \u03b5 \u2264 1. Consequently, H\u22122\u2665 \u2264 ( 1/c4 + 1/ \u221a c4 ) \u03b51.\nAs for an upper bound for \u2663, we derive upper bounds for the following two components:\n\u2666 := 1 H2 K\u22121\u2211 k=1 \u03b3kN \u03c0\u2217\u03c0\u2217PKK+1\u2212k|EK\u2212k| and \u2660 := K\u2211 k=1 \u03b3kN \u03c0\u2217\u03c0\u2217PKK+1\u2212k|\u03b5K+1\u2212k| .\nUpper bounds for H\u22122 \u2211K k=1 \u03b3 kN \u03c0K\u03c0KPK\u22121K\u2212k |EK\u2212k| and \u2211K k=1 \u03b3\nkN \u03c0K\u03c0KPK\u22121K\u2212k |\u03b5K+1\u2212k| can be similarly derived.\nFrom Lemma 2, \u2666 \u2264 maxk\u2208[K]\u2016Ej\u2016\u221e1 \u2264 \u03b5 \u221a H3/c4, and thus, H\u22122\u2666 \u2264 \u03b5/ \u221a c4. On the other hand,\nfrom the assumption that \u03b3 \u2264 \u03b1,\n2Wk\u03b92 \u2264 \u03b52\nc4H Var(v\u2217) +\nH\u03b52\nc4\n( \u03b12(k\u22122) + \u03b52H\nc4\n) 1\nfor k > 1. Using Lemmas 8 and 11 as well as \u03b3 \u2264 \u03b1,\n\u2660 \u2264 \u03b5N \u03c0\u2217\u03c0\u2217 K\u2211 k=1 \u03b3kPKK+1\u2212k\n( \u03b5\nc4 1 + \u03c3(v\u2217)\u221a c4H +\n\u221a H\nc4\n( \u03b1K\u2212k\u22122 + \u03b5 \u221a H\nc4\n) 1 )\n\u2264 \u03b5\n( H2\u03b5\nc4 1 +N \u03c0\u2217\u03c0\u2217 K\u2211 k=1 \u03b3kPKK+1\u2212k \u03c3(v\u2217)\u221a c4H +\n\u221a H3\nc4\n( K\u03b1K\u22122\ufe38 \ufe37\ufe37 \ufe38\n\u2264 \u03b5/c3 from (a)\n+H\u03b5\n\u221a H\nc4\n) 1 )\n\u2264 \u03b5\n( H2\u03b5\nc4 1 +\n\u221a H3\nc4\n( \u03b5\nc3 +H\u03b5\n\u221a H\nc4 ) 1\ufe38 \ufe37\ufe37 \ufe38\n\u2264H2/\u221ac4 as \u03b5\u22641/H\n) +\n\u03b5\u221a c4H N \u03c0\u2217\u03c0\u2217 K\u2211 k=1 \u03b3kPKK+1\u2212k\u03c3(v \u2217) .\nNow, it remains to upper-bound \u2211K k=1 \u03b3 kPKK+1\u2212k\u03c3(v \u2217). From Lemma 21,\n\u03c3(v\u2217) \u2264 \u03c3(v\u2217 \u2212 v\u03c0 \u2032 k) + \u03c3(v\u03c0 \u2032 k) \u2264 \u03b1kH1 + \u03b5 \u221a H/c41 + \u03c3(v \u03c0\u2032k)\nfor any k \u2208 [K], where Lemmas 10 and 20 are used. Consequently,\nK\u2211 k=1 \u03b3kPKK+1\u2212k\u03c3(v \u2217) \u2264 K\u2211 k=1 \u03b3kPKK+1\u2212k ( H\u03b1K+1\u2212k1 + \u03b5 \u221a H/c41 + \u03c3(v \u03c0\u2032K+1\u2212k) )\n\u2264 ( HK\u03b1K+1\ufe38 \ufe37\ufe37 \ufe38 \u2264\u03b5/c3 1 + \u03b5 \u221a H3/c41 + K\u2211 k=1 \u03b3kPKK+1\u2212k\u03c3(v \u03c0\u2032K+1\u2212k)\ufe38 \ufe37\ufe37 \ufe38\n\u2264 \u221a H31\n) ,\nwhere the second inequality follows since \u03b3 \u2264 \u03b1. Consequently, H\u22122\u2660 \u2264 \u03b5/\u221ac4. Combining these inequalities, we deduce that v\u2217 \u2212 v\u03c0K \u2264 \u03b5 ( c\u221213 + c \u22120.5 4 ) 1.\n6 Empirical illustration\n10 3 10 2 10 1 100\n102\n103\n104\n105\nNu m\nbe r o\nf s am\npl es\nSample complexity as a function of the error MDVI( ) Q-LEARNING\nFigure 1: Sample complexities of MDVI with \u03b1 = 1 and Q-LEARNING (synchronous version of Q-learning) on Garnets. MDVI is run in the stationary policy setting. Both algorithms use M = 1. As noted in Section 5, MDVI with \u03b1 = 1 is also nearly minimaxoptimal.\nWe compare MDVI to a synchronous version of Qlearning (e.g., Even-Dar et al. (2003)) in a simple setting on a class of random MDPs called Garnets (Archibald et al., 1995), with \u03b3 = 0.9. Figure 1 shows the sample complexity of MDVI as a function of \u03b5. We run MDVI on 100 random MDPs, and, given \u03b5, we report the number of samples KM MDVI uses to find \u03b5-optimal policy. We compare this empirical sample complexity with the one of Q-LEARNING, which has a tight quadratic dependency to the horizon (Li et al., 2021a) \u2013 compared to the cubic one of MDVI (Theorem 2). Figure 1 shows the difference in sample complexity between the two methods: especially for low \u03b5, MDVI reaches an \u03b5-optimal policy with much fewer samples, up to H = 10 times less samples for \u03b5 = 10\u22123. Complete details, pseudocodes, and results with other \u03b1 are provided in Appendix H."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we considered and analyzed the sample complexity of a model-free algorithm called MDVI (Geist et al., 2019; Vieillard et al., 2020a) under the generative model setting. We showed that it is nearly minimax-optimal for finding an \u03b5-optimal policy despite its simplicity compared to previous model-free algorithms (Sidford et al., 2018; Wainwright, 2019; Khamaru et al., 2021). We believe that our results are significant for the following three reasons.\nFirst, we demonstrate the effectiveness of KL and entropy regularization. Second, as discussed by Vieillard et al. (2020a), MDVI encompasses various algorithms as special cases or equivalent forms, and our results provide theoretical guarantees for most of them at once. Third, MDVI uses no variance-reduction technique, which leads to multi-epoch algorithms and involved analyses (Sidford et al., 2018; Wainwright, 2019; Khamaru et al., 2021). As such, our analysis is straightforward, and it would be easy to extend it to more complex settings.\nA disadvantage of MDVI is that its range of valid \u03b5 is limited compared to previous algorithms (Sidford et al., 2018; Agarwal et al., 2020; Li et al., 2020). It is unclear if this is an artifact of our analysis or the real limitation of MDVI-type algorithms. We leave this topic as a future work."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": "A Notations 14\nB Equivalence of MDVI Update Rules 14\nC Auxiliary Lemmas 15\nD Tools from Probability Theory 16\nE Total Variance Technique 17\nF Proof of Lemmas for Theorem 1 (Bound for a Non-Stationary Policy) 19 F.1 Proof of Lemma 1 (Error Propagation Analysis) . . . . . . . . . . . . . . . . . . . . . . . 20 F.2 Proof of Lemmas 2 and 3 (Coarse State-Value Bound) . . . . . . . . . . . . . . . . . . . . 21 F.3 Proof of Lemma 4 (Value Estimation Error Bound) . . . . . . . . . . . . . . . . . . . . . 22 F.4 Proof of Lemmas 5 and 6 (Value Estimation Variance Bound) . . . . . . . . . . . . . . . 23 F.5 Proof of Lemmas 7 and 8 (Error Bounds with Bernstein\u2019s Inequality) . . . . . . . . . . . 24\nG Proof of Lemmas for Theorem 2 (Bound for a Stationary Policy) 25 G.1 Proof of Lemma 9 (Error Propagation Analysis) . . . . . . . . . . . . . . . . . . . . . . . 25 G.2 Proof of Lemma 10 (Coarse State-Value Bounds) . . . . . . . . . . . . . . . . . . . . . . . 26\nH Details on empirical illustrations 27 H.1 Detailed setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 H.2 Additional numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nA Notations"
        },
        {
            "heading": "B Equivalence of MDVI Update Rules",
            "text": "The policy update (2) can be rewritten as follows (e.g., Equation (5) of Kozuno et al. (2019)):\n\u03c0k(a|x) = \u03c0k\u22121(a|x)\u03b1 exp (\u03b2qk(x, a))\u2211 b\u2208A \u03c0k\u22121(b|x)\u03b1 exp (\u03b2qk(x, b)) ,\nwhere \u03b1 := \u03c4/(\u03c4 + \u03ba), and \u03b2 := 1/(\u03c4 + \u03ba). It can be further rewritten as, defining sk = qk + \u03b1sk\u22121\n\u03c0k(a|x) = exp (\u03b2sk(x, a))\u2211 b\u2208A exp (\u03b2sk(x, b)) .\nPlugging in this policy expression to vk, we deduce that\nvk(x) = 1 \u03b2 log \u2211 a\u2208A exp (\u03b2qk(x, a) + \u03b1 log \u03c0k\u22121(a|x))\n= 1 \u03b2 log \u2211 a\u2208A exp (\u03b2sk(x, a))\u2212 \u03b1 \u03b2 log \u2211 a\u2208A exp (\u03b2sk\u22121(x, a)) .\nKozuno et al. (2019, Appendix B) show that when \u03b2 \u2192\u221e, vk(x) = wk(x)\u2212 \u03b1wk\u22121(x) . Furthermore, the Boltzmann policy becomes a greedy policy. Accordingly, the update rules used in MDVI is a limit case of the original MDVI updates."
        },
        {
            "heading": "C Auxiliary Lemmas",
            "text": "In this appendix, we prove some auxiliary lemmas used in the proof. Lemma 11. For any positive real values a and b, \u221a a+ b \u2264 \u221a a+ \u221a b. Proof. Indeed, a+ b \u2264 a+ 2 \u221a ab+ b = ( \u221a a+ \u221a b)2.\nLemma 12. For any real values (an)Nn=1, ( \u2211N n=1 an) 2 \u2264 N \u2211N n=1 a 2 n.\nProof. Indeed, from the Cauchy\u2013Schwarz inequality,( N\u2211 n=1 an \u00b7 1 )2 \u2264 ( N\u2211 n=1 1 )( N\u2211 n=1 a2n ) = N N\u2211 n=1 a2n ,\nwhich is the desired result.\nLemma 13. For any k \u2208 [K],\nA\u03b3,k = \u03b3 \u03b1k \u2212 \u03b3k \u03b1\u2212 \u03b3 if \u03b1 6= \u03b3\nk\u03b3k otherwise .\nProof. Indeed, if \u03b1 6= \u03b3\nA\u03b3,k = k\u22121\u2211 j=0 \u03b1j\u03b3k\u2212j = \u03b3k (\u03b1/\u03b3)k \u2212 1 (\u03b1/\u03b3)\u2212 1 = \u03b3 \u03b1k \u2212 \u03b3k \u03b1\u2212 \u03b3 .\nIf \u03b1 = \u03b3, A\u03b3,k = k\u03b3k by definition.\nLemma 14. For any real value x \u2208 (0, 1], 1\u2212 x \u2264 log(1/x).\nProof. Since log(1/x) is convex and differentiable, log(1/x) \u2265 log(1/y) \u2212 (x \u2212 y)/y. Choosing y = 1, we concludes the proof.\nLemma 15. Suppose \u03b1, \u03b3 \u2208 [0, 1), \u03b5 \u2208 (0, 1], c \u2208 [1,\u221e), m \u2208 N, and n \u2208 [0,\u221e). Let K := m 1\u2212 \u03b1 log cH \u03b5 . Then,\nKn\u03b1K \u2264 ( mn\n(1\u2212 \u03b1)e )n ( \u03b5 cH )m\u22121 .\nProof. Using Lemma 14,\nK = m\n1\u2212 \u03b1 log\ncH\n\u03b5 \u2265 log\u03b1 ( \u03b5 cH )m .\nTherefore,\nKn\u03b1K \u2264 ( m\n1\u2212 \u03b1 log\ncH\n\u03b5 )n ( \u03b5 cH )m =\nmn (1\u2212 \u03b1)n ( \u03b5 cH )m( log cH \u03b5 )n .\nSince x ( log 1\nx )n \u2264 (n e )n for any x \u2208 (0, 1] as shown later,\nKn\u03b1K \u2264 ( mn\n(1\u2212 \u03b1)e )n ( \u03b5 cH )m\u22121 .\nNow it remains to show f(x) := x ( log 1\nx )n \u2264 (n e )n for x < 1. We have that\nf \u2032(x) = (\u2212 log x)n \u2212 n(\u2212 log x)n\u22121 =\u21d2 f \u2032(x) = 0 at x = e\u2212n.\nTherefore, f takes its maximum (n e )n at e\u2212n when x \u2208 (0, 1).\nThe following lemma is a special case of a well-known inequality that for any increasing function f K\u2211 k=1 f(k) \u2264 \u222b K+1 1 f(x)dx .\nLemma 16. For any K \u2208 N and n \u2208 [0,\u221e), K\u2211 k=1 kn \u2264 1 n+ 1 (K + 1)n+1."
        },
        {
            "heading": "D Tools from Probability Theory",
            "text": "We extensively use the following two concentration inequalities. The first one is Azuma-Hoeffding inequality (Azuma, 1967; Hoeffding, 1963; Boucheron et al., 2013), and the second one is Bernstein\u2019s inequality (Bernstein, 1946; Boucheron et al., 2013) for a martingale (Lattimore & Szepesvari, 2020, Excercises 5.14 (f)). For a real-valued stochastic process (Xn)Nn=1 adapted to a filtration (Fn)Nn=1, we let En[Xn] := E[Xn|Fn\u22121] for n \u2265 1, and E1[X1] := E[X1].\nLemma 17 (Azuma-Hoeffding Inequality). Consider a real-valued stochastic process (Xn)Nn=1 adapted to a filtration (Fn)Nn=1. Assume that Xn \u2208 [ln, un] and En[Xn] = 0 almost surely, for all n. Then,\nP  N\u2211 n=1 Xn \u2265 \u221a\u221a\u221a\u221a N\u2211 n=1 (un \u2212 ln)2 2 log 1 \u03b4  \u2264 \u03b4 for any \u03b4 \u2208 (0, 1).\nLemma 18 (Bernstein\u2019s Inequality). Consider a real-valued stochastic process (Xn)Nn=1 adapted to a filtration (Fn)Nn=1. Suppose that Xn \u2264 U and En[Xn] = 0 almost surely, for all n. Then, letting V \u2032 := \u2211N n=1 En[X2n],\nP ( N\u2211 n=1 Xn \u2265 2U 3 log 1 \u03b4 + \u221a 2V log 1 \u03b4 and V \u2032 \u2264 V ) \u2264 \u03b4\nfor any V \u2208 [0,\u221e) and \u03b4 \u2208 (0, 1).\nIn our analysis, we use the following corollary of this Bernstein\u2019s inequality.\nLemma 19 (Conditional Bernstein\u2019s Inequality). Consider the same notations and assumptions in Lemma 18. Furthermore, let E be an event that implies V \u2032 \u2264 V for some V \u2208 [0,\u221e) with P(E) \u2265 1\u2212\u03b4\u2032 for some \u03b4\u2032 \u2208 (0, 1). Then,\nP ( N\u2211 n=1 Xn \u2265 2U 3 log\n1\n\u03b4(1\u2212 \u03b4\u2032) +\n\u221a 2V log\n1\n\u03b4(1\u2212 \u03b4\u2032) \u2223\u2223\u2223\u2223\u2223E ) \u2264 \u03b4\nfor any \u03b4 \u2208 (0, 1).\nProof. Let A and B denote the events of\nN\u2211 n=1 Xn \u2265 2U 3 log\n1\n\u03b4(1\u2212 \u03b4\u2032) +\n\u221a 2V log\n1\n\u03b4(1\u2212 \u03b4\u2032)\nand V \u2032 \u2264 V , respectively. Since E \u2282 B, it follows that A\u2229E \u2282 A\u2229B, and P(A\u2229E) \u2264 P(A\u2229B). Accordingly,\nP(A|E) = P(A \u2229 E) P(E) \u2264 P(A \u2229B) P(E) (a) \u2264 \u03b4(1\u2212 \u03b4 \u2032) P(E) (b) \u2264 \u03b4 ,\nwhere (a) follows from Lemma 18, and (b) follows from P(E) \u2265 1\u2212 \u03b4\u2032.\nLemma 20 (Popoviciu\u2019s Inequality for Variances). The variance of any random variable bounded by x is bounded by x2."
        },
        {
            "heading": "E Total Variance Technique",
            "text": "The following lemma is due to Azar et al. (2013).\nLemma 21. Suppose two real-valued random variables X,Y whose variances, VX and VY , exist and are finite. Then, \u221a VX \u2264 \u221a V [X \u2212 Y ] + \u221a VY .\nFor completeness, we prove Lemma 21.\nProof. Indeed, from Cauchy-Schwartz inequality,\nVX = V[X \u2212 Y + Y ] = V[X \u2212 Y ] + VY + 2E [(X \u2212 Y \u2212 E[X \u2212 Y ])(Y \u2212 EY )]\n\u2264 V[X \u2212 Y ] + VY + 2 \u221a V[X \u2212 Y ]VY = (\u221a V [X \u2212 Y ] + \u221a VY )2 .\nThis is the desired result.\nThe following lemma is an extension of Lemma 7 by Azar et al. (2013) and its refined version by Agarwal et al. (2020).\nLemma 22. Suppose a sequence of deterministic policies (\u03c0k)Kk=0 and let\nq\u03c0 \u2032 k :=\n{ r + \u03b3Pv\u03c0 \u2032 k\u22121 for k \u2208 [K]\nq\u03c00 for k = 0 .\nFurthermore, let \u03c32k and \u03a3 2 k be non-negative functions over X\u00d7A defined by\n\u03c32k(x, a) :=\n{ P (v\u03c0 \u2032 k\u22121) 2 (x, a)\u2212 (Pv\u03c0 \u2032 k\u22121) 2 (x, a) for k \u2208 [K]\nP (v\u03c00) 2 (x, a)\u2212 (Pv\u03c00)2(x, a) for k = 0\nand\n\u03a32k(x, a) := Ek ( \u221e\u2211 t=0 \u03b3tr(Xt, At)\u2212 q\u03c0 \u2032 k(X0, A0) )2\u2223\u2223\u2223\u2223\u2223\u2223X0 = x,A0 = a \nfor k \u2208 {0} \u222a [K], where Ek is the expectation over (Xt, At)\u221et=0 wherein At \u223c \u03c0k\u2212t(\u00b7|Xt) until t = k, and At \u223c \u03c00(\u00b7|Xt) thereafter. Then,\nk\u22121\u2211 j=0 \u03b3j+1P k\u22121k\u2212j \u03c3k\u2212j \u2264 \u221a 2H3\nfor any k \u2208 [K]. For its proof, we need the following lemma.\nLemma 23. Suppose a sequence of deterministic policies (\u03c0k)Kk=0 and notations in Lemma 22. Then, for any k \u2208 [K], we have that\n\u03a32k = \u03b3 2\u03c32k + \u03b3 2P\u03c0k\u22121\u03a32k\u22121 . Proof. Let Rus := \u2211u t=s \u03b3 t\u2212sr(Xt, At) and Ek [\u00b7|x, a] := Ek [\u00b7|X0 = x,A0 = a]. We have that\n\u03a32k(x, a) = Ek [( R\u221e0 \u2212 q\u03c0 \u2032 k(X0, A0) )2\u2223\u2223\u2223\u2223x, a] := Ek [(I1 + \u03b3I2)2\u2223\u2223\u2223x, a] , where I1 := r(X0, A0) + \u03b3q\u03c0 \u2032 k\u22121(X1, A1)\u2212 q\u03c0 \u2032 k(X0, A0), and I2 := R\u221e1 \u2212 q\u03c0 \u2032 k\u22121(X1, A1). With these notations, we see that\n\u03a32k(x, a) = Ek [ I21 + \u03b3 2I22 + 2\u03b3I1I2 \u2223\u2223x, a]\n= Ek [ I21 + \u03b3 2I22 + 2\u03b3I1Ek\u22121 [I2|X1, A1] \u2223\u2223x, a]\n= Ek [ I21 \u2223\u2223x, a]+ \u03b32Ek[I22 \u2223\u2223x, a]\n= Ek [ I21 \u2223\u2223x, a]+ \u03b32P\u03c0k\u22121\u03a32k\u22121(x, a) ,\nwhere the second line follows from the law of total expectation, and the third line follows since Ek\u22121 [I2|X1, A1] = 0 due to the Markov property. The first term in the last line is \u03b32\u03c32k(x, a) because\nEk [ I21 \u2223\u2223x, a] (a)= \u03b32Ek[( q\u03c0\u2032k\u22121(X1, A1)\ufe38 \ufe37\ufe37 \ufe38\nv \u03c0\u2032 k\u22121 (X1) from (b)\n\u2212(Pv\u03c0 \u2032 k\u22121)(X0, A0) )2\u2223\u2223\u2223\u2223\u2223x, a ]\n= \u03b32 ( P ( v\u03c0 \u2032 k\u22121 )2) (x, a) + \u03b32(Pv\u03c0 \u2032 k\u22121)2(x, a)\u2212 2(Pv\u03c0 \u2032 k\u22121)2(x, a)\n= \u03b32 ( P ( v\u03c0 \u2032 k\u22121 )2) (x, a)\u2212 \u03b32(Pv\u03c0 \u2032 k\u22121)2(x, a) ,\nwhere (a) follows from the definition that q\u03c0 \u2032 k = r+\u03b3Pv\u03c0 \u2032 k\u22121 , and (b) follows since the policies are deterministic. From this argument, it is clear that \u03a32k = \u03b3 2\u03c32k + \u03b3 2P\u03c0k\u22121\u03a32k\u22121 , which is the desired result.\nNow, we are ready to prove Lemma 22. Proof of Lemma 22. Let Hk := \u2211k\u22121 j=0 \u03b3 j . Using Jensen\u2019s inequality twice,\nk\u22121\u2211 j=0 \u03b3j+1P k\u22121k\u2212j \u03c3k\u2212j \u2264 k\u22121\u2211 j=0 \u03b3j+1 \u221a P k\u22121k\u2212j \u03c3 2 k\u2212j\n\u2264 \u03b3Hk k\u22121\u2211 j=0 \u03b3j+1 Hk \u221a P k\u22121k\u2212j \u03c3 2 k\u2212j\n\u2264 \u221a\u221a\u221a\u221aHk k\u22121\u2211 j=0 \u03b3j+2P k\u22121k\u2212j \u03c3 2 k\u2212j \u2264 \u221a\u221a\u221a\u221aH k\u22121\u2211 j=0 \u03b3j+2P k\u22121k\u2212j \u03c3 2 k\u2212j .\nFrom Lemma 23, we have that\nk\u22121\u2211 j=0 \u03b3j+2P k\u22121k\u2212j \u03c3 2 k\u2212j\n= k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j ( \u03a32k\u2212j \u2212 \u03b32P\u03c0k\u22121\u2212j\u03a32k\u22121\u2212j ) =\nk\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j ( \u03a32k\u2212j \u2212 \u03b3P\u03c0k\u22121\u2212j\u03a32k\u22121\u2212j + \u03b3(1\u2212 \u03b3)P\u03c0k\u22121\u2212j\u03a32k\u22121\u2212j ) =\nk\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j \u03a3 2 k\u2212j \u2212 k\u2211 j=1 \u03b3jP k\u22121k\u2212j \u03a3 2 k\u2212j + \u03b3(1\u2212 \u03b3) k\u22121\u2211 j=0 \u03b3jP k\u22121k\u22121\u2212j\u03a3 2 k\u22121\u2212j .\nThe final line is equal to \u03a32k \u2212 \u03b3kP k\u22121 0 \u03a3 2 0 + \u03b3(1\u2212 \u03b3) \u2211k\u22121 j=0 \u03b3 jP k\u22121k\u22121\u2212j\u03a3 2 k\u22121\u2212j . Finally, from the monotonicity of stochastic matrices and that 0 \u2264 \u03a32j \u2264 H21 for any j,\nk\u22121\u2211 j=0 \u03b3j+1P k\u22121k\u2212j \u03c3k\u2212j \u2264 \u221a 2H3 .\nThis concludes the proof."
        },
        {
            "heading": "F Proof of Lemmas for Theorem 1 (Bound for a Non-Stationary Policy)",
            "text": "Before starting the proof, we introduce some notations and facts frequently used in the proof.\nFrequently Used Facts. We frequently use the following fact, which follows from definitions:\nsk = Akr + \u03b3Pwk\u22121 + Ek for any k \u2208 [K] . (4)\nIndeed, sk = \u2211k j=1 \u03b1\nk\u2212j(r + \u03b3P (wj\u22121 \u2212 \u03b1wj\u22122) + \u03b5j) = Akr + \u03b3Pwk\u22121 +Ek. In addition, we often mention the \u201cmonotonicity\u201d of stochastic matrices: any stochastic matrix \u03c1 satisfies that \u03c1v \u2265 \u03c1u for any vectors v, u such that v \u2265 u. Examples of stochastic matrices in the proof are P , \u03c0, P\u03c0, and \u03c0P . The monotonicity property is so frequently used that we do not always mention it."
        },
        {
            "heading": "F.1 Proof of Lemma 1 (Error Propagation Analysis)",
            "text": "Proof. Note that\n0 \u2264 v\u2217 \u2212 v\u03c0 \u2032 k = Ak A\u221e\n( v\u2217 \u2212 v\u03c0 \u2032 k ) + \u03b1k ( v\u2217 \u2212 v\u03c0 \u2032 k ) \u2264 Ak A\u221e ( v\u2217 \u2212 v\u03c0 \u2032 k ) + 2H\u03b1k1\nsince v\u2217 \u2212 v\u03c0\u2032k \u2264 2H1. Therefore, we need an upper bound for Ak(v\u2217 \u2212 v\u03c0 \u2032 k). We decompose Ak(v\u2217 \u2212 v\u03c0 \u2032 k) to Akv\u2217 \u2212 wk and wk \u2212 Akv\u03c0 \u2032 k . Then, we derive upper bounds for each of them (inequalities (5) and (6), respectively). The desired result is obtained by summing up those bounds.\nUpper bound for Akv\u2217 \u2212 wk. We prove by induction that for any k \u2208 [K],\nAkv \u2217 \u2212 wk \u2264 HA\u03b3,k1\u2212 k\u22121\u2211 j=0 \u03b3j\u03c0\u2217P j \u2217Ek\u2212j . (5)\nWe have that\nAkv \u2217 \u2212 wk\n(a) \u2264 \u03c0\u2217(Akq\u2217 \u2212 sk) (b) = \u03c0\u2217 (Akq\n\u2217 \u2212Akr \u2212 \u03b3Pwk\u22121 \u2212 Ek) (c) = \u03c0\u2217 (\u03b3P (Akv\n\u2217 \u2212 wk\u22121)\u2212 Ek) (d) \u2264 \u03c0\u2217 ( \u03b3P (Ak\u22121v \u2217 \u2212 wk\u22121) + \u03b1k\u22121\u03b3H1\u2212 Ek ) ,\nwhere (a) is due to the greediness of \u03c0k, (b) is due to the equation (4), (c) is due to the Bellman equation for q\u2217, and (d) is due to the fact that (Ak \u2212Ak\u22121)v\u2217 = \u03b1k\u22121v\u2217 \u2264 \u03b1k\u22121H1. From this result and the fact that w0 = 0, A1v\u2217 \u2212 w1 \u2264 \u03b3H1\u2212 \u03c0\u2217E1 . Therefore, the inequality (5) holds for k = 1. From the step (d) above and induction, it is straightforward to verify that the inequality (5) holds for other k.\nUpper bound for wk \u2212Akv\u03c0 \u2032 k . We prove by induction that for any k \u2208 [K],\nwk \u2212Akv\u03c0 \u2032 k \u2264 HA\u03b3,k1 + k\u22121\u2211 j=0 \u03b3j\u03c0kP k\u22121 k\u2212j Ek\u2212j . (6)\nRecalling that v\u03c0 \u2032 k = \u03c0kT k\u22121 0 q \u03c00 , we deduce that\nwk \u2212Akv\u03c0 \u2032 k (a) = \u03c0k ( sk \u2212AkT k\u221210 q\u03c00 ) (b) = \u03c0k ( Akr + \u03b3Pwk\u22121 \u2212AkT k\u221211 q\u03c00 + Ek\n) (c) = \u03c0k ( \u03b3P ( wk\u22121 \u2212Akv\u03c0 \u2032 k\u22121 ) + Ek\n) (d)\n\u2264 \u03c0k ( \u03b3P (wk\u22121 \u2212Ak\u22121v\u03c0 \u2032 k\u22121) + \u03b1k\u22121\u03b3H1 + Ek ) ,\nwhere (a) follows from the definition of wk, (b) is due to the equation (4), (c) follows from the definition of the Bellman operator, and (d) is due to the fact that (Ak \u2212 Ak\u22121)v\u03c0 \u2032 k\u22121 = \u03b1k\u22121v\u03c0 \u2032 k\u22121 \u2265 \u2212\u03b1k\u22121H1. From this result and the fact that w0 = 0,\nw1 \u2212A1v\u03c0 \u2032 1 \u2264 \u03c01 (\u03b3Pw0 + \u03b3H1 + E1) \u2264 \u03b3H1 + \u03c01E1 .\nTherefore, the inequality (6) holds for k = 1. From the step (d) above and induction, it is straightforward to verify that the inequality (6) holds for other k."
        },
        {
            "heading": "F.2 Proof of Lemmas 2 and 3 (Coarse State-Value Bound)",
            "text": "The next lemma is necessary to bound Ek by using the Azuma-Hoeffding inequality (Lemma 17).\nLemma 24. For any k \u2208 [K], vk\u22121 is bounded by H.\nProof. We prove the claim by induction. The claim holds for k = 1 since v0 = 0 by definition. Assume that vk\u22121 is bounded by H for some k \u2265 1. Then, from the greediness of the policies \u03c0k and \u03c0k\u22121,\n\u03c0k\u22121qk = \u03c0k\u22121(sk \u2212 \u03b1sk\u22121) \u2264 vk \u2264 \u03c0k(sk \u2212 \u03b1sk\u22121) = \u03c0kqk\nSince qk = r + \u03b3P\u0302k\u22121vk\u22121 is bounded by H due to the induction hypothesis, the claim holds.\nProof of Lemma 2. Consider a fixed k \u2208 [K] and (x, a) \u2208 X\u00d7A. Since\nEk(x, a) = \u03b3\nM k\u2211 j=1 \u03b1k\u2212j M\u2211 m=1 (vj\u22121(yj\u22121,m,x,a)\u2212 Pvj\u22121(x, a))\ufe38 \ufe37\ufe37 \ufe38 bounded by 2H from Lemma 24 ,\nEk(x, a) is a sum of bounded martingale differences with respect to the filtraion (Fj,m) k,M j=1,m=1. Therefore, using the Azuma-Hoeffding inequality (Lemma 17),\nP ( |Ek|(x, a) \u2265 3H \u221a A\u221e\u03b91 M ) \u2264 \u03b4 4KXA ,\nwhere the bound in P(\u00b7) is simplified by 2 \u221a 2\u03b3 \u2264 3 and \u2211k j=1 \u03b1 2(k\u2212j) \u2264 \u2211k j=1 \u03b1\nk\u2212j = A\u221e. Taking the union bound over (x, a, k) \u2208 X\u00d7A\u00d7 [K],\nP (E1) \u2265 1\u2212 \u2211\n(x,a)\u2208X\u00d7A K\u2211 k=1 P\n( |Ek|(x, a) \u2265 3H \u221a A\u221e\u03b91 M ) \u2265 1\u2212 \u03b4 4 ,\nand thus P (Ec1) \u2264 \u03b4/4, which is the desired result.\nProof of Lemma 3. We condition the proof by the event E1. This event occurs with probability at least 1\u2212 \u03b4/4. Note that under the current setting of \u03b1, A\u221e = H. From Lemma 2 and the settings of \u03b1 and M ,\nk\u22121\u2211 j=0 \u03b3j ( \u03c0kP k\u22121 k\u2212j \u2212 \u03c0\u2217P j \u2217 ) Ek\u2212j \u2264 2 k\u22121\u2211 j=0 \u03b3j\u2016Ek\u2212j\u2016\u221e \u2264 H \u221a H\u03b5 \u221a c2 .\nThus, from Lemma 1, v\u2217 \u2212 v\u03c0\u2032k \u2264 \u221a H/c2\u03b5+ 2(H + k)\u03b3 k1. Finally, using Lemma 15,\n2(H +K)\u03b3K \u2264 \u03b5 c1 ,\nand thus,\n\u2016v\u2217 \u2212 v\u03c0 \u2032 K\u2016\u221e \u2264 \u03b5\n\u221a H\nc2 + \u03b5 c1 \u2264\n( 1\nc1 + 1 \u221a c2\n)\u221a H\u03b5 .\nTherefore, for some c1 and c2, the claim holds."
        },
        {
            "heading": "F.3 Proof of Lemma 4 (Value Estimation Error Bound)",
            "text": "We first prove an intermediate result.\nLemma 25. For any k \u2208 [K],\nv\u03c0 \u2032 k\u22121 + k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22122 k\u22121\u2212j\u03b5k\u2212j \u2212 \u03b3 kH1 \u2264 vk \u2264 v\u03c0 \u2032 k + k\u22121\u2211 j=0 \u03b3j\u03c0kP k\u22121 k\u2212j \u03b5k\u2212j + \u03b3 kH1 .\nProof. From the greediness of \u03c0k\u22121, vk = wk\u2212\u03b1wk\u22121 \u2264 \u03c0k(sk\u2212\u03b1sk\u22121) = \u03c0k(r+\u03b3Pvk\u22121 +\u03b5k). By induction on k, therefore,\nvk \u2264 k\u22121\u2211 j=0 \u03b3j\u03c0kP k\u22121 k\u2212j (r + \u03b5k\u2212j) + \u03b3 k\u03c0kP k\u22121 0 v0\ufe38 \ufe37\ufe37 \ufe38\n=0\n= k\u22121\u2211 j=0 \u03b3j\u03c0kP k\u22121 k\u2212j (r + \u03b5k\u2212j) ,\nNote that\nT k\u221210 q \u03c00 = k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j r + \u03b3 k P k\u221210 q \u03c00\ufe38 \ufe37\ufe37 \ufe38 \u2265\u2212H1 =\u21d2 k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j r \u2264 T k\u22121 0 q \u03c00 + \u03b3kH .\nAccordingly, vk \u2264 \u03c0kT k\u221210 q\u03c00 + \u2211k\u22121 j=0 \u03b3 j\u03c0kP k\u22121 k\u2212j \u03b5k\u2212j + \u03b3\nkH1 . Similarly, from the greediness of \u03c0k, vk = wk \u2212 \u03b1wk\u22121 \u2265 \u03c0k\u22121(sk \u2212 \u03b1sk\u22121) \u2265 \u03c0k\u22121(r + \u03b3Pvk\u22121 + \u03b5k). By\ninduction on k, therefore,\nvk \u2265 k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22122 k\u22121\u2212j (r + \u03b5k\u2212j) + \u03b3 k\u22121\u03c0k\u22121P k\u22122 0 Pv0\ufe38 \ufe37\ufe37 \ufe38\n=0\n.\nNote that T k\u221220 q \u03c00 = T k\u221220 (r + \u03b3Pv \u03c00), and\nT k\u221220 q \u03c00 = k\u22121\u2211 j=0 \u03b3jP k\u22122k\u22121\u2212jr + \u03b3 k P k\u221220 Pv \u03c00\ufe38 \ufe37\ufe37 \ufe38 \u2264H1 =\u21d2 k\u22121\u2211 j=0 \u03b3jP k\u22122k\u22121\u2212jr \u2265 T k\u22122 0 q \u03c00 \u2212 \u03b3kH .\nAccordingly, vk \u2265 \u03c0k\u22121T k\u221220 q\u03c00 + \u2211k\u22121 j=0 \u03b3 j\u03c0k\u22121P k\u22122 k\u22121\u2212j\u03b5k\u2212j \u2212 \u03b3kH1 .\nProof of Lemma 4. From Lemma 25 and \u03c0kT\u03c0k\u22121:1q\u03c00 = v\u03c0 \u2032 k \u2264 v\u2217, we have that\nv\u03c0 \u2032 k\u22121 + k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22122 k\u22121\u2212j\u03b5k\u2212j \u2212 2\u03b3 kH1 \u2264 vk \u2264 v\u2217 + k\u22121\u2211 j=0 \u03b3j\u03c0kP k\u22121 k\u2212j \u03b5k\u2212j + 2\u03b3 kH1 ,\nwhere we loosened the bound by multiplying \u03b3kH by 2. By simple algebra, the lower bound for v\u2217 \u2212 vk is obtained. On the other hand, from Lemma 1,\nv\u03c0 \u2032 k\u22121 \u2265 v\u2217 \u2212 1\nA\u221e k\u22122\u2211 j=0 \u03b3j ( \u03c0k\u22121P k\u22122 k\u22121\u2212j \u2212 \u03c0\u2217P j \u2217 ) Ek\u22121\u2212j \u2212 2H ( \u03b1k\u22121 + A\u03b3,k\u22121 A\u221e ) 1\nfor any k \u2208 {2, . . . ,K}. Therefore, we have that v\u2217 \u2212 vk \u2264 2H ( \u03b1k\u22121 + \u03b3k +\nA\u03b3,k\u22121 A\u221e\n) 1\n+ 1\nA\u221e k\u22122\u2211 j=0 \u03b3j ( \u03c0k\u22121P k\u22122 k\u22121\u2212j \u2212 \u03c0\u2217P j \u2217 ) Ek\u22121\u2212j \u2212 k\u22121\u2211 j=0 \u03b3j\u03c0k\u22121P k\u22122 k\u22121\u2212j\u03b5k\u2212j\nfor any k \u2208 {2, . . . ,K}. Finally, for k = 1, since v1 = \u03c01q1 = \u03c01r,\n\u2212\u03b3H1 \u2264 \u03c0\u2217 (q\u2217 \u2212 r) \u2264 v\u2217 \u2212 v1 \u2264 \u03b3\u03c0\u2217Pv\u2217 \u2264 \u03b3H1 .\nAs \u03931 \u2265 0, the claim holds for k = 1 too."
        },
        {
            "heading": "F.4 Proof of Lemmas 5 and 6 (Value Estimation Variance Bound)",
            "text": "Proof of Lemma 5. Consider a fixed k \u2208 [K] and (x, a) \u2208 X\u00d7A. Since\n\u03b5k(x, a) = \u03b3\nM M\u2211 m=1 (vk\u22121(yk\u22121,m,x,a)\u2212 Pvk\u22121(x, a))\ufe38 \ufe37\ufe37 \ufe38 bounded by 2H from Lemma 24 ,\n\u03b5k(x, a) is a sum of martingale differences with respect to the filtraion (Fk,m)Mm=1 and bounded by 2\u03b3H/M . Therefore, using the Azuma-Hoeffding inequality (Lemma 17),\nP ( |\u03b5k|(x, a) \u2265 3H \u221a \u03b91 M ) \u2264 \u03b4 4KXA ,\nwhere the bound in P(\u00b7) is simplified by 2 \u221a 2 \u2264 3. Taking the union bound over (x, a, k) \u2208 X\u00d7A\u00d7 [K],\nP (E2) \u2265 1\u2212 \u2211\n(x,a)\u2208X\u00d7A K\u2211 k=1 P ( |\u03b5k|(x, a) \u2265 3H \u221a \u03b91 M ) \u2265 1\u2212 \u03b4 4 ,\nand thus P (Ec2) \u2264 \u03b4/4, which is the desired result.\nNext, we prove a uniform bound on v\u2217 \u2212 vk.\nLemma 26. Conditioned on E1 \u2229 E2,\n\u2016v\u2217 \u2212 vk\u2016\u221e < 2H min {\n1, \u03b3k + \u03b1k\u22121 + A\u03b3,k\u22121 A\u221e + 6H \u221a \u03b91 M } for all k \u2208 [K], where 1/0 :=\u221e.\nProof. Let ek := \u03b3kH +H max j\u2208[k] \u2016\u03b5j\u2016\u221e. From Lemma 4, v\u2217 \u2212 vk \u2265 \u22122ek1 for any k \u2208 [K], and\nv\u2217 \u2212 vk \u2264 2H ( \u03b1k\u22121 +\nA\u03b3,k\u22121 A\u221e + 1 A\u221e max j\u2208[k\u22121]\n\u2016Ej\u2016\u221e ) 1 + 2ek1\nfor any k \u2208 {2, . . . ,K}. Note that \u2016v\u2217 \u2212 vk\u2016\u221e \u2264 2H from Lemma 24 for any k. Combining these results with Lemmas 2 and 5,\n\u2016v\u2217 \u2212 vk\u2016\u221e < 2H min {\n1, \u03b3k + \u03b1k\u22121 + A\u03b3,k\u22121 A\u221e + 3H \u221a \u03b91 M ( 1 + \u221a 1 A\u221e )} \u2264 2H min { 1, \u03b3k + \u03b1k\u22121 +\nA\u03b3,k\u22121 A\u221e + 6H \u221a \u03b91 M } for all k \u2208 [K], where we used the fact that 1 \u2264 A\u221e. This concludes the proof.\nNow, we are ready to prove Lemma 6.\nProof of Lemma 6. Clearly \u03c3(v0) = 0 since v0 = 0. From Lemma 21, \u03c3(vk) \u2264 \u03c3 (vk \u2212 v\u2217) + \u03c3(v\u2217) . Using Popoviciu\u2019s inequality on variances (Lemma 20) together with Lemma 26,\n\u03c3 (vk \u2212 v\u2217) \u2264 2H min {\n1, \u03b3k + \u03b1k\u22121 + A\u03b3,k\u22121 A\u221e + 6H \u221a \u03b91 M } ,\nwhere we used a simple formula, min{a, b}2 = min{a2, b2} for any scalars a, b \u2265 0. Finally, loosening the bound by replacing \u03b3k + \u03b1k\u22121 by 2 max{\u03b1, \u03b3}k\u22121, the claim holds."
        },
        {
            "heading": "F.5 Proof of Lemmas 7 and 8 (Error Bounds with Bernstein\u2019s Inequality)",
            "text": "Proof of Lemma 7. Consider a fixed k \u2208 [K] and (x, a) \u2208 X\u00d7A. Since\nEk(x, a) = \u03b3\nM k\u2211 j=1 \u03b1k\u2212j M\u2211 m=1 (vj\u22121(yj\u22121,m,x,a)\u2212 Pvj\u22121(x, a))\ufe38 \ufe37\ufe37 \ufe38 bounded by 2H from Lemma 24 ,\nEk(x, a) is a sum of bounded martingale differences with respect to the filtraion (Fj,m) k,M j=1,m=1. From the facts that v0 = 0, and \u03b3 \u2264 1,\nV \u2032 = \u03b32\nM k\u2211 j=1 \u03b12(k\u2212j)Var (vj\u22121) (x, a) \u2264 1 M k\u2211 j=2\n\u03b12(k\u2212j)Var (vj\u22121) (x, a)\ufe38 \ufe37\ufe37 \ufe38 :=\u2665 ,\nSince we are conditioned with the event E1 \u2229 E2, the inequality (3) in Lemma 6 holds and implies that the predictable quadratic variation V \u2032 satisfies the following inequality:\n\u2665 \u2264 k\u2211 j=2 \u03b12(k\u2212j) ( \u03c3(v\u2217)(x, a) + 2H min { 1, 2 max{\u03b1, \u03b3}j\u22122 + A\u03b3,j\u22122 A\u221e + 6H \u221a \u03b91 M })2\n\u2264 k\u2211 j=2 \u03b12(k\u2212j) ( \u03c3(v\u2217)(x, a) + 2H ( 2 max{\u03b1, \u03b3}j\u22122 + A\u03b3,j\u22122 A\u221e + 6H \u221a \u03b91 M ))2 \u2264 4 k\u2211 j=2 \u03b12(k\u2212j) ( Var(v\u2217)(x, a) + 4H2 ( 4 max{\u03b1, \u03b3}2(j\u22122) + A2\u03b3,j\u22122 A2\u221e + 36H2\u03b91 M )) ,\nwhere the last line follows from Lemma 12. Consequently, V \u2032 is bounded by\nV \u2032 \u2264 4 M k\u2211 j=2 \u03b12(k\u2212j)\n( Var(v\u2217)(x, a) + 4H2 ( 4 max{\u03b1, \u03b3}2(j\u22122) +\nA2\u03b3,j\u22122 A2\u221e + 36H2\u03b91 M\n)) ,\nwhich is equal to Vk(x, a). Using Lemma 19 and taking the union bound over (x, a, k) \u2208 X\u00d7A\u00d7 [K], P ( \u2203(x, a, k) \u2208 X\u00d7A\u00d7 [K] s.t. |EK |(x, a) \u2265\n4H\u03b92 3M\n+ \u221a\n2Vk(x, a)\u03b92 \u2223\u2223\u2223\u2223E1 \u2229 E2) \u2264 \u03b44 . (Recall that P(E1 \u2229 E2) \u2265 1\u2212 \u03b4\n2 \u2265 1 2 , and hence, we need to use \u03b92.) Thus, P (Ec3 |E1 \u2229 E2) \u2264 \u03b4 4 .\nProof of Lemma 8. Consider a fixed k \u2208 [K] and (x, a) \u2208 X\u00d7A. Since\n\u03b5k(x, a) = \u03b3\nM M\u2211 m=1 (vk\u22121(yk\u22121,m,x,a)\u2212 Pvk\u22121(x, a))\ufe38 \ufe37\ufe37 \ufe38 bounded by 2H from Lemma 24 ,\n\u03b5k(x, a) is a sum of bounded martingale differences with respect to Fk,m. Since we are conditioned with the event E1 \u2229 E2, the inequality (3) in Lemma 6 holds and implies that the predictable quadratic variation V \u2032 can be shown to satisfy the following inequality as in the proof of Lemma 7:\nV \u2032 = \u03b32\nM Var (vk\u22121) (x, a) \u2264\n4\nM Vark ,\nwhere the last line is equal to Wk(x, a). (Note that v0 = 0.) Using Lemma 19 and taking the union bound over (x, a, k) \u2208 X\u00d7A\u00d7 [K],\nP ( \u2203(x, a, k) \u2208 X\u00d7A\u00d7 [K] s.t. |\u03b5k|(x, a) \u2265\n4H\u03b92 3M\n+ \u221a\n2Wk(x, a)\u03b92 \u2223\u2223\u2223\u2223E1 \u2229 E2) \u2264 \u03b44 . (Recall that P(E1 \u2229 E2) \u2265 1\u2212 \u03b4\n2 \u2265 1 2 , and hence, we need to use \u03b92.) Thus, P (Ec4 |E1 \u2229 E2) \u2264 \u03b4 4 ."
        },
        {
            "heading": "G Proof of Lemmas for Theorem 2 (Bound for a Stationary Policy)",
            "text": "We use the same notations as those used in Appendix F."
        },
        {
            "heading": "G.1 Proof of Lemma 9 (Error Propagation Analysis)",
            "text": "To prove Lemma 9, we need the following lemma.\nLemma 27. For any k \u2208 [K], let \u2206k := wk \u2212 wk\u22121. Then, for any k \u2208 [K],\n\u03c0k\u22121 k\u22121\u2211 j=0 \u03b3jP k\u22122k\u22121\u2212jE \u2032 k\u2212j \u2212A\u03b3,k1 \u2264 \u2206k \u2264 \u03c0k k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j E \u2032 k\u2212j +A\u03b3,k1 .\nProof. We prove only the upper bound by induction as the proof for a lower bound is similar. We have that \u2206k = \u03c0ksk \u2212 \u03c0k\u22121sk\u22121 \u2264 \u03c0k(sk \u2212 sk\u22121), where the inequality follows from the greediness of \u03c0k\u22121. Let \u2665k := sk\u2212 sk\u22121. Since s0 = 0, \u26651 = r+E\u20321 \u2264 1+E\u20321. From the monotonicity of \u03c01, the claim holds for k = 1. Assume that for some k \u2212 1 \u2265 1, the claim holds. Then, from the equation (4), the induction hypothesis, and the monotonicity of P ,\n\u2665k = (Ak \u2212Ak\u22121) r + \u03b3P\u2206k\u22121 + E\u2032k\n\u2264 k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j E \u2032 k\u2212j + (\u03b1 k\u22121 + \u03b3A\u03b3,k\u22121)1 = k\u22121\u2211 j=0 \u03b3jP k\u22121k\u2212j E \u2032 k\u2212j +A\u03b3,k1 .\nThe claimed upper bound follows from the monotonicity of \u03c0k.\nNow, we are ready to prove Lemma 9.\nProof of Lemma 9. Note that\n0 \u2264 v\u2217 \u2212 v\u03c0k = Ak A\u221e (v\u2217 \u2212 v\u03c0k) + \u03b1k (v\u2217 \u2212 v\u03c0k) \u2264 Ak A\u221e (v\u2217 \u2212 v\u03c0k) + 2H\u03b1k1\nsince v\u2217 \u2212 v\u03c0k \u2264 2H1. Therefore, we need an upper bound for Ak(v\u2217 \u2212 v\u03c0k). We decompose Ak(v\u2217 \u2212 v\u03c0k) to Akv\n\u2217 \u2212 wk and wk \u2212Akv\u03c0k . Then, we derive upper bounds for each of them. The desired result is obtained by summing up those bounds.\nUpper bound for Akv\u2217 \u2212 wk. Note that\nAkv \u2217 \u2212 wk (a) = N \u03c0\u2217 (\u03c0\u2217 (Akr + \u03b3Pwk)\u2212 wk) (b)\n\u2264 N \u03c0\u2217\u03c0\u2217 (Akr + \u03b3Pwk \u2212 sk) (c) = N \u03c0\u2217\u03c0\u2217 (\u03b3P (wk \u2212 wk\u22121)\u2212 Ek)\n(d) \u2264 N \u03c0\u2217\u03c0\u2217  k\u2211 j=1 \u03b3jP kk+1\u2212jE \u2032 k+1\u2212j \u2212 Ek +HA\u03b3,k1 , where (a) is due to the fact that I = N \u03c0(I \u2212 \u03b3\u03c0P ) and v\u03c0 = N \u03c0\u03c0r for any policy \u03c0, (b) is due to the greediness of \u03c0k, (c) follows from the equation (4), and (d) follows from Lemma 27.\nUpper bound for wk \u2212Akv\u03c0k . We have that\nwk \u2212Akv\u03c0k (a) = N \u03c0k (wk \u2212 \u03c0k (Akr + \u03b3Pwk)) (b) = N \u03c0k\u03c0k (wk \u2212Akr \u2212 \u03b3Pwk) (c) = N \u03c0k\u03c0k (\u2212\u03b3P (wk \u2212 wk\u22121) + Ek)\n(d) \u2264 N \u03c0k\u03c0k Ek \u2212 k\u2211 j=1 \u03b3jP k\u22121k\u2212j E \u2032 k+1\u2212j +HA\u03b3,k1 , where (a) is due to the fact that I = N \u03c0(I \u2212 \u03b3\u03c0P ) and v\u03c0 = N \u03c0\u03c0r for any policy \u03c0, (b) is due to the definition of wk, (c) follows from the equation (4), and (d) follows from Lemma 27."
        },
        {
            "heading": "G.2 Proof of Lemma 10 (Coarse State-Value Bounds)",
            "text": "Before starting the proof, we note that A\u221e = H2 under the current setting.\nProof of Lemma 10. From Lemma 2, \u2016Ek\u2016\u221e \u2264 3H \u221a A\u221e\u03b91/M \u2264 3\u03b5 \u221a H3/c4 for any k \u2208 [K]. On the other\nhand, from Lemma 5, \u2016\u03b5k\u2016\u221e \u2264 3H \u221a \u03b91/M \u2264 3\u03b5 \u221a H/c4 for any k \u2208 [K]. Combining these bounds with Lemma 1,\nv\u2217 \u2212 v\u03c0 \u2032 k \u2264\nH max j\u2208[k] \u2016Ej\u2016\u221e1 + H\u03b1k1 \u2264\n( \u03b5 \u221a H\nc4 +H\u03b1k\n) 1\nfor any k \u2208 [K], where we used the fact that A\u03b3,k/A\u221e \u2264 \u03b1k/H \u2264 \u03b1k, which follows from Lemma 13, is used. Furthermore, combining previous upper bounds for errors with Lemma 9,\nv\u2217 \u2212 v\u03c0k \u2264 2H ( \u03b1k +\nA\u03b3,k A\u221e ) \ufe38 \ufe37\ufe37 \ufe38 \u22642\u03b1k from (a) 1 + 1 A\u221e (N \u03c0k\u03c0k \u2212N \u03c0\u2217\u03c0\u2217)Ek\ufe38 \ufe37\ufe37 \ufe38 \u22642H\u2016Ek\u2016\u221e1 from (b)\n+ 1\nA\u221e k\u2211 j=1 \u03b3j ( N \u03c0\u2217\u03c0\u2217P kk+1\u2212j \u2212N \u03c0k\u03c0kP k\u22121k\u2212j ) E\u2032k+1\u2212j\ufe38 \ufe37\ufe37 \ufe38\n\u22642H(\u2016\u03b5k+1\u2212j\u2016\u221e+(1\u2212\u03b1)\u2016Ek\u2212j\u2016\u221e)1 from (c)\n(d) \u2264 4H\u03b1k1 + 2 H \u2016Ek\u2016\u221e + 2 max j\u2208[k]\n( \u2016\u03b5j\u2016\u221e + 1\nH2 \u2016Ej\u2016\u221e ) \u2264 4H\u03b1k1 + 6\u03b5 \u221a H\nc4 + 6\u03b5 \u221a c4\n(\u221a H +\n1\u221a H\n) 1 = ( \u03b5 \u221a H\nc4 +H\u03b1k\n) 1\nfor any k \u2208 [K], where (a) follows as A\u03b3,k/A\u221e \u2264 \u03b1k/H \u2264 \u03b1k from Lemma 13, (b) is due to the monotonicity of stochastic matrices, and \u2212\u2016Ek\u2016\u221e1 \u2264 Ek \u2264 \u2016Ek\u2016\u221e1 for any k \u2208 [K], (c) is due to the monotonicity of stochastic matrices, and \u2212(\u2016\u03b5k\u2016\u221e+ (1\u2212\u03b1)\u2016Ek\u22121\u2016\u221e)1 \u2264 E\u2032k \u2264 (\u2016\u03b5k\u2016\u221e+ (1\u2212\u03b1)\u2016Ek\u22121\u2016\u221e)1 for any k \u2208 [K], and (d) follows by taking the maximum over j."
        },
        {
            "heading": "H Details on empirical illustrations",
            "text": "This appendix details the settings used for the illustrations of Section 6. It provides\n\u2022 a precise definition of the Garnet setting and pseudo-code for Q-LEARNING in Appendix H.1;\n\u2022 additional numerical experiments illustrating the effects of \u03b1 and M on the algorithm in Appendix H.2."
        },
        {
            "heading": "H.1 Detailed setting",
            "text": "Garnets. We use the Garnets (Archibald et al., 1995) class of random MDPs. A Garnet is characterized by three integer parameters, X, A, and B, that are respectively the number of states, the number of actions, and the branching factor \u2013 the maximum number of accessible new states in each state. For each (x, a) \u2208 X\u00d7A, we draw B states (y1, . . . , yB) from X uniformly without replacement. Then, we draw B \u2212 1 numbers uniformly in (0, 1), denoting them sorted as (p1, . . . , pB\u22121). We set the transition probability P ykx,a = pk\u2212pk\u22121 for each 1 \u2264 k \u2264 B, with p0 = 0 and pB = 1. Finally, the reward function, depending only on the states, is drawn uniformly in (\u22121, 1) for each state. In our examples, we used X = 8, A = 2, and B = 2. We compute our experiments with \u03b3 = 0.9.\nQ-learning. For illustrative purposes, we compare the performance of MDVI to the one of a sampled version of Q-LEARNING, that we know is not minimax-optimal. For completeness, the pseudo-code for this method is given in Algorithm 2. It shares the time complexity of MDVI, but has a lower memory complexity, since it does not need to store an additional XA table.\nAlgorithm 2: Q-LEARNING(K,M,w) Input: number of iterations K, number of samples per iteration M , w \u2208 [0.5, 1] a learning rate\nparameter. Let q0 = 0 \u2208 RXA; for k from 0 to K \u2212 1 do\nfor each state-action pair (x, a) \u2208 X\u00d7A do Sample (yk,m,x,a)Mm=1 from the generative model P (\u00b7|x, a); Let mk+1(x, a) = r(x, a) + \u03b3M\u22121 \u2211M m=1 maxa\u2032 qk(yk,m,x,a, a\n\u2032); end Let \u03b7k = (k + 1)\u2212w; Let qk+1 = (1\u2212 \u03b7k)qk + \u03b7kmk+1;\nend return \u03c0K , a greedy policy with respect to qK ;"
        },
        {
            "heading": "H.2 Additional numerical illustrations",
            "text": "Additional experiment for sample complexity. In Figure 1, we plot the sample complexity of a standard version of Q-LEARNING using w = 1 (i.e performing an exact average of q-values). However, we know (Even-Dar et al., 2003) that we can reach a better sample complexity by choosing a more appropriate w in (0.5, 1). In Figure 2, we provide the sample complexity for MDVI, and Q-LEARNING with w = 1 and w = 0.7.\nThe version with w = 0.7 catches up with MDVI at high errors, but the difference is still quite large at higher precision. Note that we add additional data points for \u03b5 < 10\u22123. Both versions of Q-LEARNING do not have sample complexity plotted for these errors, because they did not reach these \u03b5 in the number of iterations we ran them (up to 105 iterations).\nInfluence of \u03b1. We showcase the impact of \u03b1 whenM = 1 in Figure 3. With \u03b1 = 1, MDVI will asymptotically converge to \u03c0\u2217. With a \u03b1 < 1, MDVI will reach an \u03b5-optimal policy, but will not actually converge to the optimal policy of the MDP (although this \u03b5 can be controlled by choosing a large enough value for \u03b1, or a larger value of M). Indeed, in the latter case, the distance to the optimal policy depends on a moving average of the errors (by a factor \u03b1). The moving average reduces the variance, but does not bring it zero, contrarily to the exact average implicitly performed when \u03b1 = 1. This behaviour is illustrated in Figure 3. We observe there that, with M = 1, one has to choose a large enough value of \u03b1 to reach a policy close enough to the optimal one.\nInfluence of M . Choosing the right M is not that obvious from the theory (it notably depends on an unknown constant c2). We illustrate in Figure 4 the influence it has on the speed of convergence of MDVI. We run MDVI with \u03b1 = 0.99 (for a setting where \u03b3 = 0.9), and for different values of M . With a fixed \u03b1, a larger M allows MDVI to reach a lower asymptotic error, but slows down the learning in early iterations. M cannot however be chosen as large as possible: at one point it start to be useless to increase its value. For instance, moving from M = 5 to M = 10 does not allow for a noticeable lower error, but slows the learning. We compare this to the setting where \u03b1 = 1 for completeness."
        }
    ],
    "title": "KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal",
    "year": 2022
}