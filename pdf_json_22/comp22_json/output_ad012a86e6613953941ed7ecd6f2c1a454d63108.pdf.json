{
    "abstractText": "Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space. The approximate class-specific kernel spectral regression (ACS-KSR) method is a powerful tool for face verification. This method consists of two steps: an eigenanalysis step and a kernel regression step, however, it may suffer from heavily computational overhead in practice, especially for large-sample data sets. In this paper, we propose two randomized algorithms based on the ACS-KSR method. The main contribution of our work is four-fold. First, we point out that the formula utilized in the eigenanalysis step of the ACS-KSR method is mathematically incomplete, and we give a correction to it. Moreover, we consider how to efficiently solve the ratio-trace problem and the trace-ratio problem involved in this method. Second, it is well known that kernel matrix is approximately low-rank, however, to the best of our knowledge, there are few theoretical results that can provide simple and feasible strategies to determine the numerical rank of a kernel matrix without forming it explicitly. To fill-in this gap, we focus on the commonly used Gaussian kernel and provide a practical strategy for determining numerical rank of the kernel matrix. Third, based on numerically low-rank property of the kernel matrix, we propose a modified Nystr\u00f6m method with fixed-rank for the kernel regression step, and establish a probabilistic error bound on the approximation. Fourth, although the proposed Nystr\u00f6m method can reduce the computational cost of the original method, it is required to form and store the reduced kernel matrix explicitly. This is unfavorable to extremely large-sample data sets. To settle this problem, we propose a randomized block Kaczmarz method for kernel regression problem with multiple right-hand sides, in which there is no need to compute and store the reduced kernel matrix explicitly. The convergence of this method is established. Comprehensive numerical experiments on real-world data sets are performed to show the effectiveness of our theoretical results and the efficiency of the proposed methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ke Li1 \u00b7 Gang Wu"
        }
    ],
    "id": "SP:724dc66bd73cc73aaf02c5267f12377867f092c9",
    "references": [
        {
            "authors": [
                "F. Anaraki",
                "S. Becker"
            ],
            "title": "Improved fixed-rank Nystr\u00f6m approximation via QR decomposition: Practical and theoretical aspects",
            "year": 2019
        },
        {
            "authors": [
                "S. Arashloo",
                "J. Kittler"
            ],
            "title": "Class-specific kernel fusion of multiple descriptors for face verification using multiscale binarised statistical image features",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2014
        },
        {
            "authors": [
                "F. Bach"
            ],
            "title": "Sharp analysis of low-rank kernel matrix approximations",
            "venue": "JMLR: Workshop and Conference Proceedings,",
            "year": 2013
        },
        {
            "authors": [
                "P. Barr",
                "J. Noble",
                "R. Biddle"
            ],
            "title": "Video game values: Human-computer interaction and games",
            "venue": "Interacting with Computers,",
            "year": 2007
        },
        {
            "authors": [
                "G. Baudat",
                "F. Anouar"
            ],
            "title": "Generalized discriminant analysis using a kernel approach",
            "venue": "Neural Computation,",
            "year": 2000
        },
        {
            "authors": [
                "S. Bucak",
                "R. Jin",
                "A. Jain"
            ],
            "title": "Multiple kernel learning for visual object recognition: a review",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "L. Cambier",
                "E. Darve"
            ],
            "title": "Fast low-rank kernel matrix factorization using skeletonized interpolation",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2019
        },
        {
            "authors": [
                "G. Cao",
                "A. Iosifidis",
                "M. Gabbouj"
            ],
            "title": "Neural class-specific regression for face verification",
            "venue": "IET Biometrics,",
            "year": 2018
        },
        {
            "authors": [
                "C. Cortes",
                "M. Mohri",
                "A. Talwalkar"
            ],
            "title": "On the impact of kernel approximation on learning",
            "year": 2010
        },
        {
            "authors": [
                "P. Drineas",
                "M. Mahoney"
            ],
            "title": "On the Nystr\u00f6m method for approximating a gram matrix for improved",
            "venue": "racy, Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "P. Hart",
                "D. Stork"
            ],
            "title": "An introduction to ROC analysis",
            "venue": "kernel-based learning. Journal of Machine Learning Research,",
            "year": 2000
        },
        {
            "authors": [
                "A. Gittens"
            ],
            "title": "The spectral norm error of the naive Nystr\u00f6m extension, arXiv: 1110",
            "year": 2011
        },
        {
            "authors": [
                "G.H. Golub",
                "C.F. Van Loan"
            ],
            "title": "Matrix Computations (4th ed.)",
            "year": 2014
        },
        {
            "authors": [
                "M. Press. G\u00f6nen",
                "E. Alpayin"
            ],
            "title": "Multiple kernel learning algorithms",
            "venue": "Journal of Machine Learning",
            "year": 2011
        },
        {
            "authors": [
                "G. 2211\u20132268. Goudelis",
                "S. Zafeiriou",
                "A. Tefas",
                "I. Pitas"
            ],
            "title": "Class-specific kernel discriminant analysis for face",
            "year": 2007
        },
        {
            "authors": [
                "P. Press. Grother"
            ],
            "title": "NIST special database 19 handprinted forms and characters database",
            "year": 1995
        },
        {
            "authors": [
                "N. Tech. Rep. Halko",
                "P. Martinsson",
                "J. Troop"
            ],
            "title": "Finding structure with randomness: probabilistic algorithms",
            "year": 2011
        },
        {
            "authors": [
                "N.J. Higham",
                "T. Mary"
            ],
            "title": "A new preconditioner that exploits low-rank approximations to factoriza",
            "year": 2019
        },
        {
            "authors": [
                "A. Iosifidis",
                "M. Gabbouj"
            ],
            "title": "Hierarchical class-specific kernel discriminant analysis for face verifi",
            "year": 2016
        },
        {
            "authors": [
                "Y. Kittler",
                "J. Li",
                "J. Matas"
            ],
            "title": "Face verification using client specific Fisher faces, The Statistics",
            "year": 2000
        },
        {
            "authors": [
                "D. Needell",
                "J.A. Tropp"
            ],
            "title": "Paved with good intentions: Analysis of a randomized block Kaczmarz",
            "year": 2014
        },
        {
            "authors": [
                "S. Winkler"
            ],
            "title": "2014).A data-driven approach to cleaning large face",
            "venue": "Linear Algebra and its Applications,",
            "year": 2014
        },
        {
            "authors": [
                "T. Ngo",
                "M. Bellalij",
                "Y. Saad"
            ],
            "title": "The trace-ratio optimization problem",
            "venue": "SIAM Review,",
            "year": 2012
        },
        {
            "authors": [
                "C. Park",
                "H. Park"
            ],
            "title": "A comparision of generalized linear discriminant analysis",
            "year": 2008
        },
        {
            "authors": [
                "W. Shi",
                "G. Wu"
            ],
            "title": "New algorithms for trace-ratio problem with application to high-dimension",
            "year": 2021
        },
        {
            "authors": [
                "S. Baker",
                "M. Bsat"
            ],
            "title": "The CMU pose, illumination, and expression database",
            "venue": "Feature Engineering, Article,",
            "year": 2003
        },
        {
            "authors": [
                "T. 3\u201310. Tran",
                "A. Douzal-Chouakria",
                "S Yazdi"
            ],
            "title": "Interpretable time series kernel analytics by preim",
            "year": 2020
        },
        {
            "authors": [
                "A. Wathen",
                "S. Zhu"
            ],
            "title": "On spectral distribution of kernel matrices related to radial basis functions",
            "year": 2015
        },
        {
            "authors": [
                "M. Seeger"
            ],
            "title": "Using the Nystr\u00f6m method to speed up kernel machines",
            "venue": "Numerical Algorithms,",
            "year": 2001
        },
        {
            "authors": [
                "T. Hassner",
                "I. Maoz"
            ],
            "title": "Face recognition in unconstrained videos",
            "venue": "Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "V. Kumar",
                "J Quinlan"
            ],
            "title": "Top 10 algorithms in data mining",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2008
        },
        {
            "authors": [
                "J. Xu",
                "J. Wang",
                "Y. Li",
                "W. Li",
                "Y. Guo"
            ],
            "title": "Identity authentication on mobile devices",
            "venue": "tion Systems,",
            "year": 2019
        },
        {
            "authors": [
                "S. Zafeiriou",
                "G. Tzimiropoulos",
                "M. Petrou",
                "T. Stathaki"
            ],
            "title": "Regularized kernel discriminant analy",
            "venue": "Matrix Analysis and Applications,",
            "year": 2012
        },
        {
            "authors": [
                "X. Li",
                "Y. Feng",
                "Z. Liu"
            ],
            "title": "The use of ROC and AUC in the validation of objective image",
            "venue": "Learning Systems,",
            "year": 2015
        },
        {
            "authors": [
                "R. Chan",
                "P. Tang",
                "T. Chow",
                "S. Wong"
            ],
            "title": "Trace-ratio linear discriminant analysis",
            "venue": "fusione valuation metrics. Signal Processing,",
            "year": 2013
        },
        {
            "authors": [
                "C. Zhou",
                "L. Wang",
                "Q. Zhang",
                "X. Wei"
            ],
            "title": "Face recognition based on PCA image reconstruction",
            "year": 2013
        },
        {
            "authors": [
                "V. Zoric"
            ],
            "title": "Mathematical Analysis I",
            "venue": "LDA. Optik,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\n1 3\nKeywords Face verification\u00a0\u00b7 Approximate class-specific kernel spectral regression (ACSKSR)\u00a0\u00b7 Kernel matrix\u00a0\u00b7 Nystr\u00f6m method\u00a0\u00b7 Block Kaczmarz method\nEditor: Paolo Frasconi.\n* Gang Wu gangwu@cumt.edu.cn; gangwu76@126.com\nExtended author information available on the last page of the article\n1 3"
        },
        {
            "heading": "1 Introduction",
            "text": "The face verification problem has attracted great attention for more than two decades due to its application demands including security, human\u2013computer interaction, human behaviour analysis for assisted living, and so on (Barr et\u00a0 al., 2007; Lei et\u00a0 al., 2012; Li et\u00a0 al., 2011; Tefas & Pitas, 2011). In essence, face verification is conceptually different from the famous face recognition problem (Cao et\u00a0 al., 2018; Iosifidis & Gabbouj, 2016a, 2016b, 2017; Iosifidis et\u00a0al., 2015). On one hand, face recognition is a multi-class problem, which concentrates on recognizing the identity of a person from a pool of some known person identities. On the other hand, face verification is a binary problem where it focuses on verifying whether a facial image depicts a person of interest (Iosifidis & Gabbouj, 2016b; Wu et\u00a0al., 2019).\nSubspace learning method is done by projecting images into a lower dimensional space, and after that recognition is performed by measuring the distances between known images and the image to be recognized. Two representative subspace learning methods are principal component analysis (PCA) (Duda et\u00a0al., 2000) and linear discriminant analysis (LDA) (Li et\u00a0 al., 2011; Lei et\u00a0 al., 2012; Duda et\u00a0al., 2000). PCA is an unsupervised method in which the discriminative information encoded in the labels of training data is not exploited. Hence, its discrimination power is often limited (Zhou et\u00a0 al., 2013). On the other hand, the maximal dimensionality of the learnt subspace in LDA is restricted by the number of classes s, which limits the application of LDA in face verification problem. Indeed, since the rank of the between-class scatter matrix is at most s \u2212 1 , the subspace learned by the LDA method has only one dimension in binary (two-class) problems, which might not be the optimal choice for discrimination problems (Iosifidis & Gabbouj, 2016; Zhou et\u00a0 al., 2013).\nTo remedy these limitations, class-specific approaches are investigated in Zafeiriou et\u00a0al. (2012), Kittler et\u00a0al. (2000), Goudelis et\u00a0al. (2007), Iosifidis et\u00a0al. (2015), Arashloo and Kittler (2014). In class-specific subspace learning techniques, an optimal subspace that highlights the discrimination of one class (noted as client class hereafter) from all other possibilities (i.e. data not belonging to the client class, forming the so-called impostor class) is determined (Iosifidis and Gabbouj, 2016). Meanwhile, to achieve nonlinear data projections which have been found to outperform linear ones with a large extend in face recognition and verification problems, class-specific subspace learning methods can be extended to their nonlinear counterparts by exploiting the well-known kernel trick (Baudat & Anouar, 2000; Hofmann et\u00a0al., 2008; Lu et\u00a0al., 2003; M\u00fcller et\u00a0al., 2001).\nKernel methods are well known to be effective in dealing with nonlinear machine learning problems in general, and are often required for machine learning tasks on complex data sets (Hofmann et\u00a0 al., 2008; M\u00fcller et\u00a0 al., 2001). The main idea behind kernel machines is to map the data from the input space to a higher dimension feature space via a nonlinear map, where the mapped data can be then analysed by linear models. Kernel learning techniques aim at constructing kernel matrices whose structure is well aligned with the learning target, which improves the generalization performance of kernel methods (Lan et\u00a0al., 2017; Tran et\u00a0al., 2020). However, most kernel learning approaches are computationally very expensive. It is well known that kernel versions of subspace learning techniques require to compute the kernel matrix K \u2208 \u211dn\u00d7n explicitly, where n is the training set cardinality. However, as the computational complexities and the storage requirements are O(n3) and O(n2) (Iosifidis et\u00a0al., 2015; Tavernier et\u00a0al., 2019), respectively, both approaches above will become computationally intractable as n is large.\n1 3\nClass-specific kernel discriminant analysis (CS-KDA) (Goudelis et\u00a0al., 2007; Iosifidis et\u00a0al., 2015) and class-specific kernel spectral regression (CS-KSR) (Arashloo & Kittler, 2014; Iosifidis et\u00a0al., 2015) are commonly used class-specific kernel approaches. Recently, Iosifidis and Gabbouj (2016) put forward an approximate class-specific kernel spectral regression (ACS-KSR) method that employs the reduced kernel matrix \ufffdK \u2208 \u211dr\u00d7n(r < n) to take place of the kernel matrix, which speedups the computation. Roughly speaking, this method is composed of two steps: the eigenanalysis step for computing the eigenvector matrix T, and a kernel regression step for the reconstruction weights matrix A. Unfortunately, we find that the widely used eigenanalysis step in the ACS-KSR method (Iosifidis & Gabbouj, 2016) and other class-specific kernel discriminant analysis methods (Cao et\u00a0 al., 2018; Iosifidis & Gabbouj, , 2017; Iosifidis et\u00a0 al., 2015) is incomplete. Moreover, the explicit computation of the cross-product matrix K\u0303K\u0303T in the kernel regression step is computationally impracticable, in addition to some useful information may be lost (Golub & Van Loan, 2014). Therefore, it is necessary to revisit the ACS-KSR method and improve the numerical performance of this type of methods.\nWith the development of science and technology, the ability to generate data at the scale of millions and even billions has increased rapidly, posing great computational challenges to scientific computation problems involving large-scale kernel matrices (Hofmann et\u00a0al., 2008). Low-rank approximations are popular techniques to reduce the highly computational cost of algorithms involving large-scale kernel matrices (Halko et\u00a0 al., 2011; Hofmann et\u00a0al., 2008; Wang et\u00a0al., 2018; Wathen & Zhu, 2015). Indeed, the success of these low-rank approximation algorithms hinges on a large spectrum gap or a fast decay of the spectrum of the kernel matrix (Halko et\u00a0 al., 2011; Pan et\u00a0 al., 2011). This motivates the analysis on the numerical rank of kernel matrix. In recent years, the low-rank property and low-rank approximation of kernel matrix have attracted great attention (Cambier & Darve, 2019; Iske et\u00a0 al., 2017; Wang et\u00a0 al., 2018; Wathen & Zhu, 2015; Xing & Chow, 2020). For example, the low-rank property of kernel matrix is investigated in Wang et\u00a0al. (2018), Wathen and Zhu (2015), an interpolation method is used to construct the approximation of kernel matrix (Cambier & Darve, 2019; Xing & Chow, 2020), and a low-rank approximation is constructed in Iske et\u00a0al. (2017), with the help of hierarchical low-rank property of kernel matrix. Although there has been a lot of research on the low-rank approximation of the kernel matrix, the estimation of the numerical rank of the kernel matrix is still in the theoretical stage. To the best of our knowledge, most existing results usually require the key information of kernel matrices, and there are few theoretical results could provide simple and feasible strategies for determining the numerical rank of a kernel matrix without forming the matrix explicitly. Moreover, estimations to the upper bound of the numerical rank are often too large to be used in practice.\nTo fill in this gap and to tackle the computational challenges mentioned above, we aim to improve the approximate class-specific kernel spectral regression method in this work. The main contribution is four-fold. First, we give a correction to the eigenanalysis step used in the ACS-KSR method, and consider how to solve the ratio-trace problem and the trace-ratio problem by exploiting the structure of the intra-class and out-of-class scatter matrices. Second, we consider low-rank property of the Gaussian kernel matrix, and provide a practical strategy for determining numerical rank of kernel matrix without forming it beforehand. Third, based on the numerically low-rank property of Gaussian kernel matrix, we provide a modified Nystr\u00f6m method with fixed-rank for the kernel regression step, and establish a probabilistic error bound on the approximation. Although the proposed Nystr\u00f6m method can reduce the computational cost of the original method, it is required to form and store the reduced kernel matrix K\u0303 explicitly. In the era of big data, however,\n1 3\nthe reduced kernel matrix may be so huge that it can not be stored in main memory, and the proposed Nystr\u00f6m method can still be time-consuming. To deal with this problem, the fourth contribution of this paper is to propose a randomized block Kaczmarz method for kernel regression problem with multiple right-hand sides. The convergence of this method is established.\nThe structure of this paper is as follows. In Sect.\u00a02, we briefly introduce the face verification problem, the class-specific kernel discriminant analysis method and its two variations. The eigenanalysis step involved in the CS-KSR and ACS-KSR methods is corrected in Sect.\u00a0 3, moreover, we consider how to solve the trace-ratio or the ratio-trace problems involved in this step. In Sect.\u00a04, we focus on the numerically low-rank property of the Gaussian kernel matrix, and propose a modified and fixed-rank Nystr\u00f6m method. To further reduce the computational overhead, in Sect.\u00a05, we propose a randomized block Kaczmarz method for regression with multiple right-hand sides. In Sect.\u00a0 6, we perform numerical experiments on some real-world data sets, to show the numerical behavior of the proposed algorithms as well as the effectiveness of our theoretical results. Some conclusions are drawn in Sect.\u00a0 7. MATLAB notations are utilized in our algorithms whenever necessary, and some notations used in this paper are listed in Table\u00a01."
        },
        {
            "heading": "2 Class\u2011specific kernel discriminant analysis and\u00a0its variants",
            "text": "Denote by U a training set consists of n facial images. Assume that all facial images in U have been preprocessed and represented by facial vectors i \u2208 \u211dm, i = 1, 2,\u2026 , n, which are followed by binary labels i \u2208 {+1,\u22121} denoting whether the facial vector\nm,\u00a0n The data dimension and the number of training samples n1, n2 The number of client class, and the number of impostor class, with n = n1 + n2 d,\u00a0r Discriminant space dimensionality, and reference vector set cardinality , [n] The binary label vector, and the set {1, 2,\u2026 , n} k,\u00a0l The target rank and the number of sampling, with l > k [n] \u29f5 [r] The set whose elements belong to [n] but not [r], i.e., the set subtraction , I\ni Zero matrix or vector, and identity matrix of dimension i\ndim(W) Dimension of the subspace W span{A} Subspace spanned by the columns of a matrix A A T ,A\u2020 Transpose and Moore\u2013Penrose inverse of a matrix A\nA i ,A ii The i-th column and the i-th diagonal element of A rank(A), tr(A) Rank and trace of a matrix A [[A]]\nk The best rank-k approximation to a matrix A, with k \u2264 rank(A)\nN(A),R(A) Null space and range of a matrix A \u2016 \u22c5 \u20162, \u2016 \u22c5 \u2016F 2-norm and Frobenius norm of a vector or matrix N(A) \u29f5N(B) The subspace in N(A) but not in N(B) k (A), k (A) The k-th largest singular value and eigenvalue of A \ud835\udd3c(\u22c5),\u2119(\u22c5) Expectation and probability\n1 3\ni belongs to the client (+1) or the impostor (\u22121) class. Suppose that there are n1 facial images belong to the client class (i.e., the person of interest), while the remaining n2 facial images belong to the impostor class.\nThe kernel approaches map the input space \u211dm to the kernel space F by using a nonlinear function (\u22c5) , and then determine a linear projection W in the kernel space F , such that\nwhere W \u2208 \u211d|F|\u00d7d . However, the data representation ( i) in F cannot be computed directly in practice, and the kernel trick is used instead (Lu et\u00a0al., 2003; M\u00fcller et\u00a0al., 2001; Zheng et\u00a0al., 2013). Indeed, the multiplication in (1) is inherently computed by using dot products in F . More precisely, one exploits the kernel function (\u22c5, \u22c5) to express dot products ( i, j) = ( i)T ( j) between training data in the kernel space F . The dot products between all the training vectors in the kernel space F are stored in the kernel matrix K \u2208 \u211dn\u00d7n whose i-th column is\nDenote by \u03a6 = [ ( 1), ( 2),\u2026 , ( n)] \u2208 \u211d|F|\u00d7n , the kernel matrix K can be written as K = \u03a6T\u03a6 , and the projection matrix W can be represented as\nwhere A \u2208 \u211dn\u00d7d is a reconstruction weights matrix. A combination of (1) and (2) yields\nIn Class-Specific Kernel Discriminant Analysis (CS-KDA) (Goudelis et\u00a0 al., 2007), we denote by = 1\nn1\n\u2211 i, i=1\ni the client class mean vector, and define DI ,DC the distances of the impostor vectors and the client vectors from the client class mean vector , respectively. From (1), we have = WT , where = 1\nn1\n\u2211 i, i=1\n( i) is the client class mean expressed in F . Hence,\nThe objective of CS-KDA is to determine data representations i \u2208 \u211dd in a feature space, such that the client class is as compact as possible, while the impostor class is spread far away as much as possible from the client class. Mathematically, we aim to seek a matrix W\u2217 , such that\n(1) i = WT ( i),\ni = [ ( i) T ( 1), ( i) T ( 2),\u2026 , ( i) T ( n)] T , i = 1, 2,\u2026 , n.\nW =\nn\u2211\ni=1\n( i) T i = \u03a6A,\n(3) i = AT\u03a6T ( i) = AT i.\nDI = \ufffd\ni, i=\u22121\n\u2016WT ( i) \u2212W T \u2016 2 2 and DC = \ufffd\ni, i=1\n\u2016WT ( i) \u2212W T \u2016 2 2 .\n(4) W\u2217 = argmax\nW \u2208 span{\u03a6}\nW \u2208 \u211d|F|\u00d7d\nDI DC .\n1 3\nThat is,\nwhere tr(\u22c5) is the trace of a matrix, and\nand\nare the out-of-class and the in-class scatter matrices in F , respectively. However, solving (5) directly is impractical since |F| is very large or even infinite in practice. Fortunately, by substituting (2) in (6) and (7), the Eq. (5) can be equivalently expressed as the following trace-ratio problem on A (Iosifidis et\u00a0al., 2015)\nwhere\nand\nand I \u2208 \u211dn2 and C \u2208 \u211dn1 are vectors of all ones, KI \u2208 \u211dn\u00d7n2 and KC \u2208 \u211dn\u00d7n1 are matrices formed by the columns of K corresponding to the impostor and client class data, respectively. However, the trace-ratio problem (8) is difficult to solve (Jia et\u00a0al., 2009; Wang et\u00a0al., 2007), and one often solves the following ratio-trace problem instead\nwhich reduces to a generalized eigenproblem MI = MC . By (9) and (10), the rank of MI and MC are at most n2 + 3 and n1 \u2212 1 , respectively, and both of the two matrices are rank-deficient. Thus, the generalized eigenvalue problem can be non-regular (Golub & Van Loan, 2014). Notice that (8) and (11) are not mathematically equivalent in general (Shi & Wu, 2021).\nRecently, a spectral regression-based method for (5) was proposed in Arashloo and Kittler (2014), Iosifidis et\u00a0al. (2015). Let ( , ) be an eigenpair satisfying SI = SC . From (2), we have = \u03a6 , moreover, if we set K = , where K = \u03a6T\u03a6 is the kernel matrix, then this eigenproblem reduces to Arashloo and Kittler (2014), Iosifidis et\u00a0al. (2015)\n(5) W\u2217 = argmax\nW \u2208 span{\u03a6}\nW \u2208 \u211d|F|\u00d7d\ntr(WTSIW) tr(WTSCW) ,\n(6)SI = \u2211\ni, i=\u22121\n( ( i) \u2212 )( ( i) \u2212 )T \u2208 \u211d|F|\u00d7|F|\n(7)SC = \u2211\ni, i=1\n( ( i) \u2212 )( ( i) \u2212 )T \u2208 \u211d|F|\u00d7|F|\n(8)T(A) = max A\u2208\u211dn\u00d7d\ntr(ATMIA) tr(ATMCA) ,\n(9)MI = KIK T I \u2212\n1 n1 KI I T C KT C \u2212 1 n1 KC C T I KT I +\nn2\nn2 1\nKC C T C KT C \u2208 \u211dn\u00d7n,\n(10)MC = KCKTC \u2212 1\nn1 KC C\nT C KT C \u2208 \u211dn\u00d7n,\n(11)T\u0302(A) = max A\u2208\u211dn\u00d7d tr\n(( A T M C A )\u22121( A T M I A )) ,\n1 3\nwhere\nand\nand C \u2208 \u211dn is a vector whose elements C,i = 1 if i = 1 and C,i = 0 if i = \u22121 , moreover, I \u2208 \u211d\nn is a vector whose elements I,i = 1 if i = \u22121 and I,i = 0 if i = 1 . However, both PI and PC are singular, and (12) is non-regular. Usually, some regularization techniques are used, and the following eigenproblem is solved instead\nwhere is a regularization parameter. In the Class-Specific Kernel Spectral Regression (CS-KSR) method, the reconstruction weights matrix A is computed as follows\n\u2022 Eigenanalysis Step: Compute T = [ 1, 2,\u2026 , d] from solving the large-scale eigenproblem (15), where i is the eigenvector corresponding to the i-th largest eigenvalue and d is the dimension of the discriminant space. \u2022 Kernel Regression Step: Solving K i = i, i = 1,\u2026 , d , for the reconstruction weights matrix A = [ 1, 2,\u2026 , d].\nIn the aforementioned CS-KDA and CS-KSR approaches, we have to calculate and store the kernel matrix K \u2208 \u211dn\u00d7n , whose computational cost is O(n3) flops, and the space complexity is O(n2) . Here n is the number of training samples. Therefore, forming and storing the full kernel matrix K explicitly are very time consuming, especially for large classification problems (Tavernier et\u00a0al., 2019).\nTo partially overcome this difficulty and speedup the kernel regression step, an approximate class-specific kernel spectral regression (ACS-KSR) method is proposed (Iosifidis & Gabbouj, 2016), in which an approximate kernel space is exploited. More precisely, in terms of the structure of the intra-class and out-of-class scatter matrices, Iosifidis and Gabbouj (2016) show that the eigenanalysis step can be solved by applying a much simpler and faster process involving only the class labels of the training data; see Algorithm\u00a01. Furthermore, recall that CS-KSR resorts to the following kernel regression problem\nand the matrix A is obtained from expressing W as a linear combination of all training data representations in the kernel space, i.e., W = \u03a6A.\nThe key idea of Iosifidis and Gabbouj (2016) is that the matrix W is expressed as a linear combination of r reference vectors, i.e., W = \u03a8A , where \u03a8 \u2208 \u211d|F|\u00d7r with r < n . In this case, the kernel regression problem (16) can be written as\n(12)PI = PC ,\n(13)PI = I T I \u2212\n1 n1 I T C \u2212 1 n1 C T I + 1\nn2 1\nC T C ,\n(14)PC = (1 \u2212 2\nn1 +\n1\nn2 1\n) C T C ,\n(15)(PI + In) = PC ,\n(16) W\u2217 = argmin W \u2208 span{\u03a6}\nW \u2208 \u211d\ufffdF\ufffd\u00d7d\n\u2016WT\u03a6 \u2212 TT\u20162 F ,\n1 3\nwhere K\u0303 = \u03a8T\u03a6 \u2208 \u211dr\u00d7n is a reduced kernel matrix expressing the training data representations in a kernel space defined on the reference data \u03a8 . As a result, we have from (17) that Golub and Van Loan (2014)\nThe ACS-KSR method is presented in Algorithm\u00a02.\nRemark 1 Some remarks are in order. The adoption of such approximate kernel regression scheme leads to an important reduction on memory requirements, which allows one to apply ACS-KSR method to large-scale verification problems. Unfortunately, we find that the formulas (13) and (14) for computing T, which are widely used in Iosifidis et\u00a0al. (2015), Iosifidis and Gabbouj (2016), Iosifidis and Gabbouj (2016), Iosifidis and Gabbouj (2017), Cao et\u00a0al. (2018), are incomplete. On the other hand, an explicit computation of K\u0303K\u0303T will cost us O(nr2) flops, and some useful information in K\u0303 may be lost when forming the crossproduct matrix (Golub & Van Loan, 2014). Furthermore, both the CS-KDA method and the ACS-KSR method focus on ratio-trace problems, rather than the original trace-ratio\n(17)A \u2217 = argmin A\u2208\u211dr\u00d7d \u2016AT\u03a8T\u03a6 \u2212 TT\u20162 F = argmin A\u2208\u211dr\u00d7d \u2016K\u0303TA \u2212 T\u20162 F ,\n(18)A\u2217 = (K\u0303K\u0303T )\u22121K\u0303T .\n1 3\nproblem (5). Thus, it is necessary to give new insight into the ACS-KSR method, and improve this method substantially."
        },
        {
            "heading": "3 On the\u00a0ratio\u2011trace and\u00a0the\u00a0trace\u2011ratio problems for\u00a0the\u00a0eigenanalysis step",
            "text": "In this section, we first show that (13) and (14) are incomplete for solving the ratio-trace problem (11) corresponding to (8). Some corrections to the two matrices PI and PC are given. Second, we consider how to solve the trace-ratio problem (8) and the corresponding ratio-trace problem (11) efficiently.\nTheorem\u00a0 3.1 Let EI \u2208 \u211dn\u00d7n2 and EC \u2208 \u211dn\u00d7n1 be matrices constituted by some columns of the identity matrix In \u2208 \u211dn\u00d7n , corresponding to the impostor and client class index, respectively. Then under the above notations, (8) is equivalent to the following trace-ratio problem\nwhere\nand\nwhere C \u2208 \u211dn is a vector with elements C,i = 1 if i = 1 , and C,i = 0 if i = \u22121 , and I \u2208 \u211d n is a vector with elements I,i = 1 if i = \u22121 and I,i = 0 if i = 1.\nProof Notice that KEI = KI , KEC = KC , and EI I = I , EC C = C . It follows from (9) that\nwhere PI = EIETI \u2212 1\nn1 I\nT C \u2212\n1 n1 C T I + n2 n2 1 C T C .\nSimilarly, we obtain from (10) that\n(19)max T\u2208\u211dn\u00d7d\ntr(TTPIT) tr(TTPCT) ,\n(20)PI = EIE T I \u2212\n1 n1 I T C \u2212 1 n1 C T I +\nn2\nn2 1\nC T C ,\n(21)PC = ECETC \u2212 1\nn1 C\nT C ,\n(22)\nMI =KIK T I \u2212\n1 n1 KI I T C KT C \u2212 1 n1 KC C T I KT I +\nn2\nn2 1\nKC C T C KT C\n=K (\nEIE T I \u2212\n1 n1 EI I T C ET C \u2212 1 n1 EC C T I ET I +\nn2\nn2 1\nEC C T C ET C\n)\nK\n=K (\nEIE T I \u2212\n1 n1 I T C \u2212 1 n1 C T I +\nn2\nn2 1\nC T C\n)\nK\n\u2261KPIK,\n1 3\nwhere PC = ECETC \u2212 1\nn1 C\nT C . Substitute (22) and (23) into (8), and note that KA = T , the\ntrace-ratio problem (8) can be equivalently rewritten as (19). \u25a1\nRemark 2 Theorem\u00a0 3.1 indicates that (13) and (14) are mathematically incomplete, and (20), (21) give some corrections to them. Unlike (13) and (14), it is seen that the two new matrices are not rank-2 and rank-1 matrices any more.\nWith (20) and (21) at hand, we consider how to solve the optimization problem (19) efficiently. So as to get structured intra-class and out-of-class scatter matrices, we first reorder the elements in the binary label vector . More precisely, suppose that the training binary label vector is permuted to the binary label \u0303 , in which all the client ( +1) classes in \u0303 are sorted before all the impostor ( \u22121 ) classes. Mathematically speaking, there exists a permutation matrix P \u2208 \u211dn\u00d7n such that P = \u0303 . Corresponding to I , C , EI and EC , we define the four variables \u0303I , \u0303C , E\u0303I and E\u0303C with respect to the new binary label \u0303 . Moreover, we have that P I = \u0303I , P C = \u0303C , PEI = E\u0303I and PEC = E\u0303C , and\nDenote by P\u0303I = PPIPT , then it follows from (20) that\nSimilarly, denote by P\u0303C = PPCPT , we have from (21) that\nTherefore, combining (25) and (26), the Eq. (19) can be rewritten as\nwhere T\u0303 = PT , and we make use of the property PTP = In , as P is a permutation matrix. In summary, we solve the target matrix T in the following two steps:\n\u2022 Solving the following trace-ratio problem\n(23)\nMC =KCK T C \u2212\n1 n1 KC C T C KT C\n=K ( ECE T C \u2212 1\nn1 EC C\nT C ET C\n) K\n=K ( ECE T C \u2212 1\nn1 C\nT C\n) K\n\u2261KPCK,\n(24)E\u0303TI E\u0303C = , E\u0303C C = \u0303C, \u0303 T I E\u0303C = .\n(25)\nP\u0303I =PPIP T = P\n(\nEIE T I \u2212\n1 n1 I T C \u2212 1 n1 C T I +\nn2\nn2 1\nC T C\n)\nPT\n=E\u0303I E\u0303 T I \u2212\n1 n1 \u0303I \u0303 T C \u2212 1 n1 \u0303C \u0303 T I +\nn2\nn2 1\n\u0303C \u0303 T C .\n(26)P\u0303C = PPCPT = P ( ECE T C \u2212 1\nn1 C\nT C\n)\nPT = E\u0303CE\u0303 T C \u2212\n1 n1 \u0303C \u0303 T C .\ntr(TTPIT) tr(TTPCT) =\ntr ( TTPT (PPIP T )PT ) tr ( TTPT (PPCP T )PT ) \u2261 tr(T\u0303T P\u0303I T\u0303) tr(T\u0303T P\u0303CT\u0303) ,\n1 3\nor the ratio-trace problem\nfor the matrix T\u0302 , where P\u0303I and P\u0303C are defined in (25) and (26), respectively. \u2022 Let T = PTT\u0302.\nRemark 3 An advantage of the problem (27) over the original one (19) is that, one can take full advantage of the special structure of matrices P\u0303I and P\u0303C . Keep in mind that there is no need to form and store the perturbation matrix P explicitly in the two methods.\nNext, we propose two methods for solving the ratio-trace problem (28) and the trace-ratio problem (27), respectively."
        },
        {
            "heading": "3.1 Solution of\u00a0the\u00a0ratio\u2011trace problem for\u00a0the\u00a0eigenanalysis step",
            "text": "It is well known that the trace-ratio problem (27) is difficult to solve (Jia et\u00a0al., 2009; Wang et\u00a0al., 2007). As an alternative, one often solves the relatively easier ratio-trace problem (28). Note that it is different from the one given in (12) which is widely used in Iosifidis et\u00a0 al., (2015), Iosifidis and Gabbouj (2016), Iosifidis and Gabbouj (2016), Iosifidis and Gabbouj (2017), Cao et\u00a0al., (2018). First, we show that both P\u0303I and P\u0303C are positive semidefinite matrices. Recall that we have obtained four structured variables \u0303I , \u0303C , E\u0303I and E\u0303C with respect to the new binary label \u0303 . In fact, due to the characteristics of the new binary label \u0303 , we see that E\u0303C and E\u0303I are the first n1 columns and the last n2 columns of the identity matrix In \u2208 \u211dn\u00d7n , respectively. In addition, the first n1 elements of \u0303C are all 1 and the rest are all 0, and the last n2 elements of \u0303I are all 1 and the rest are all 0. Thus, by (25) and (26), the two matrices P\u0303I and P\u0303C are block matrices of the following form, i.e.,\nand\nOn one hand, since the eigenvalues of In1 \u2212 1\nn1 C\nT C are either 1 or 0, P\u0303C is positive sem-\nidefinite. On the other hand, we consider the following positive semidefinite matrix\n(27)T\u0302tr = argmax T\u0303\u2208\u211dn\u00d7d\ntr(T\u0303T P\u0303I T\u0303) tr(T\u0303T P\u0303CT\u0303) ,\n(28)T\u0302rt = argmax T\u0303\u2208\u211dn\u00d7d\ntr [\n(T\u0303T P\u0303CT\u0303) \u22121(T\u0303T P\u0303I T\u0303)\n]\n(29)P\u0303I =\n( n2 n2 1 C T C \u2212 1 n1 C T I\n\u2212 1\nn1 I\nT C\nIn2\n)\n,\n(30)P\u0303C =\n( In1 \u2212 1\nn1 C\nT C\n)\n.\nB = \u239b \u239c \u239c \u239d\n\u221a n2\nn1 C\n\u2212 1\n\u221a n2 I \u239e \u239f \u239f \u23a0 \ufffd \u221a n2 n1 T C \u2212 1 \u221a n2 T I \ufffd =\n\ufffd n2 n2 1 C T C \u2212 1 n1 C T I\n\u2212 1\nn1 I\nT C\n1 n2 I T I\n\ufffd\n.\n1 3\nNotice that\nis a positive semidefinite matrix. Thus, P\u0303I is also positive semidefinite. Indeed, the solution of the ratio-trace problem (28) can be reduced to the following generalized eigenvalue problem (Duda et\u00a0al., 2000)\nHowever, both P\u0303I and P\u0303C may be singular, and this generalized eigenvalue problem can be non-regular in practice (Golub and Van Loan, 2014). One remedy is to use the regularized technique\nwhere \ud835\udefc > 0 is a user-described regularization parameter. Denote by P\u0303 = (P\u0303I + In)\u22121P\u0303C , we are interested in the eigenvectors corresponding to the smallest d eigenvalues of the matrix P\u0303 . As\nit follows from the Sherman\u2013Morrison\u2013Woodbury formula (Golub & Van Loan, 2014) that\nwhere\nThus,\nP\u0303I \u2212 B =\n(\nIn2 \u2212 1\nn2 I\nT I\n)\nP\u0303I \u0303 = \u0303P\u0303C \u0303.\n(31)(P\u0303I + In )\u0302 = \u0302P\u0303C \u0302,\nP\u0303I = E\u0303I E\u0303 T I \u2212\n1 n1 \u0303I \u0303 T C \u2212 1 n1 \u0303C \u0303 T I +\nn2\nn2 1\n\u0303C \u0303 T C\n= \ufffd E\u0303I \u0303I \u0303C \ufffd \u239b \u239c \u239c \u239c \u239d In2\n\u2212 1\nn1\n\u2212 1\nn1\nn2 n2 1 \u239e \u239f \u239f \u239f \u23a0 \u239b \u239c \u239c \u239d E\u0303T I \u0303T I \u0303T C \u239e \u239f \u239f \u23a0 \u2208 \u211dn\u00d7n,\n(P\u0303I + In) \u22121 = \u23a1 \u23a2 \u23a2 \u23a2 \u23a3 In + \ufffd E\u0303I \u0303I \u0303C \ufffd \u239b \u239c \u239c \u239c \u239d In2\n\u2212 1\nn1\n\u2212 1\nn1\nn2 n2 1 \u239e \u239f \u239f \u239f \u23a0 \u239b \u239c \u239c \u239d E\u0303T I \u0303T I \u0303T C \u239e \u239f \u239f \u23a0 \u23a4 \u23a5 \u23a5 \u23a5 \u23a6\n\u22121\n= 1\nIn \u2212\n1\n2\n\ufffd E\u0303I \u0303I \u0303C \ufffd \u0398\u22121 \u239b \u239c \u239c \u239c \u239d In2\n\u2212 1\nn1\n\u2212 1\nn1\nn2 n2 1 \u239e \u239f \u239f \u239f \u23a0 \u239b \u239c \u239c \u239d E\u0303T I \u0303T I \u0303T C \u239e \u239f \u239f \u23a0 ,\n\u0398 = In2+2 + 1 \u239b \u239c \u239c \u239c \u239d In2\n\u2212 1\nn1\n\u2212 1\nn1\nn2 n2 1 \u239e \u239f \u239f \u239f \u23a0 \u239b \u239c \u239c \u239d E\u0303T I \u0303T I \u0303T C \u239e \u239f \u239f \u23a0 \ufffd E\u0303I \u0303I \u0303C \ufffd \u2208 \u211d(n2+2)\u00d7(n2+2).\n1 3\nNext, we will prove that\nOn one hand, we obtain from (26) that\nFrom E\u0303C C = \u0303C , we have span{\ufffdPC} \u2286 span{\ufffdEC} . A combination of the above equation with (24) yields\nOn the other hand, we have from (26) that\nSo we get (33) from combining (34) and (35). In conclusion, we have from (30), (32) and (33) that\nThus, P\u0303 has n2 + 1 eigenvalues 0 and n1 \u2212 1 eigenvalues which equal to 1 . In practice, we often have d \u2264 n1 \u2212 1 and n2 \u2265 n1 (Iosifidis and Gabbouj 2016), and (28) can be reduced to the problem of finding d vectors in the null space of P\u0303.\nHence, it is only necessary to consider the null space of P\u0303 . Assume that \u2208 N(P\u0303) , and let = [ T\n1 , T 2 ]T \u2208 \u211dn , with 1 \u2208 \u211dn1 and 2 \u2208 \u211dn2 . Then we have from (36) that\nwhich can be equivalently rewritten as\nAs a result, the solution of (28) has the following form\n(32)\nP\u0303 = (P\u0303I + In) \u22121P\u0303C\n= 1\nP\u0303C \u2212\n1\n2\n\ufffd E\u0303I \u0303I \u0303C \ufffd \u0398\u22121 \u239b \u239c \u239c \u239c \u239d In2\n\u2212 1\nn1\n\u2212 1\nn1\nn2 n2 1 \u239e \u239f \u239f \u239f \u23a0 \u239b \u239c \u239c \u239d E\u0303T I \u0303T I \u0303T C \u239e \u239f \u239f \u23a0 P\u0303C.\n(33) \u239b \u239c \u239c \u239d E\u0303T I \u0303T I\n\u0303T C\n\u239e \u239f \u239f \u23a0 P\u0303C = \u239b \u239c \u239c \u239d\nE\u0303T I P\u0303C \u0303T I P\u0303C \u0303T C P\u0303C \u239e \u239f \u239f \u23a0 = .\nP\u0303C = E\u0303CE\u0303 T C \u2212\n1 n1 \u0303C \u0303 T C = ( E\u0303C \u0303C\n) (\nIn1\n\u2212 1\nn1\n)( E\u0303T C\n\u0303T C\n)\n.\n(34)E\u0303TI P\u0303C = and \u0303 T I P\u0303C = .\n(35)\u0303TCP\u0303C = \u0303 T C (E\u0303CE\u0303 T C \u2212\n1 n1 \u0303C \u0303 T C ) = T C E\u0303T C \u2212\nn1 n1 \u0303T C =\u0303T C \u2212 \u0303T C = .\n(36)P\u0303 = 1 P\u0303C = 1\n( In1 \u2212 1\nn1 C\nT C\n)\n.\n( In1 \u2212 1\nn1 C\nT C\n)( 1 2 ) = ( ) ,\n(37) \u23a7 \u23aa \u23a8 \u23aa \u23a9 (In1 \u2212 1 n1 C T C ) 1 = , \u2200 2 \u2208 \u211d n2 .\n1 3\nwhere i \u2208 \u211dn2 , i = 1, 2,\u2026 , d, are arbitrary such that the columns of T\u0302rt are linear independent. In summary, we have Algorithm\u00a03 for the eigenanalysis step."
        },
        {
            "heading": "3.2 Solution of\u00a0the\u00a0trace\u2011ratio problem for\u00a0the\u00a0eigenanalysis step",
            "text": "In the previous subsection, we solve the ratio-trace problem (28) for the eigenanalysis step. However, the ratio-trace model and the trace-ratio model (27) are not mathematically equivalent (Park and Park, 2008; Shi and Wu, 2021). The trace-ratio problem has regained great concerns in recent years. The reason is that the trace-ratio model can yield markedly improved recognition results compared with the ratio-trace model (Jia et\u00a0al., 2009; Ngo et\u00a0al., 2012; Shi & Wu, 2021; Wang et\u00a0al., 2007).\nIn this subsection, we focus on the trace-ratio problem (3.9). It has been long believed that there is no closed-form solution for the trace-ratio problem, and some commonly used techniques are inner-outer iterative methods (Jia et\u00a0 al., 2009; Ngo et\u00a0al., 2012; Wang et\u00a0al., 2007; Zhao et\u00a0al., 2013). Recently, Shi and Wu point out that the trace-ratio problem has a close-form solution when the dimension of data points is greater than or equal to the number of training samples (Shi & Wu, 2021), as the following theorem indicates.\nTheorem\u00a03.2 Shi and Wu (2021) Let P\u0303T = P\u0303I + P\u0303C , then the subspace N(P\u0303C) \u29f5N(P\u0303T ) , i.e., the subspace in N(P\u0303C) but not in N(P\u0303T ) , is the solution space of the trace-ratio problem (27). Let d be the reducing dimension, if dim(N(P\u0303C) \u29f5N(P\u0303T )) \u2265 d , then any orthonormal basis of a d-dimensional subspace of N(P\u0303C) \u29f5N(P\u0303T ) , is a solution to (27).\nBased on Theorem\u00a03.2 and the structure of the three matrices P\u0303I , P\u0303C and P\u0303T , we consider how to solve trace-ratio problem (27) efficiently. First, we obtain from (36) that\nSecond, it follows from (29) and (30) that\n(38)T\u0302rt = ( C C \u2026 C 1 2 \u2026 d ) \u2208 \u211dn\u00d7d,\n(39)N(P\u0303C) = N(P\u0303) = span{T\u0302rt}.\n1 3\nSuppose that \u2208 N(P\u0303T ) , and let = [ T1 , T 2 ]T \u2208 \u211dn , where 1 \u2208 \u211dn1 and 2 \u2208 \u211dn2 , we have\nwhich is equivalent to\nTherefore, the solution is = [ T C T I ]T = n \u2208 \u211d n . So we obtain from Theorem\u00a03.2 that\nis a solution to trace-ratio problem (27). We present Algorithm\u00a04 for solving the eigenanalysis step. It is seen that the solutions to the trace-ratio problem (27) and the ratio-trace problem (28) are related, but are different from each other in essence.\nP\u0303T =P\u0303I + P\u0303C\n=\n( In1 + n2\u2212n1\nn2 1\nC T C \u2212 1\nn1 C\nT I\n\u2212 1\nn1 I\nT C\nIn2\n)\n.\n( In1 + n2\u2212n1\nn2 1\nC T C \u2212 1\nn1 C\nT I\n\u2212 1\nn1 I\nT C\nIn2\n)( 1 2 ) = ( ) ,\n(40) \u23a7 \u23aa \u23a8 \u23aa \u23a9\n(In1 \u2212 1\nn1 C\nT C ) 1 = ,\n1 n1 I T C 1 = 2.\n(41)T\u0302tr = (In \u2212 1\nn n\nT n ) \u22c5 T\u0302rt\n1 3"
        },
        {
            "heading": "4 A modified Nystr\u00f6m method based on\u00a0low\u2011rank approximation for\u00a0the\u00a0kernel regression step",
            "text": "In this section, we focus on the kernel regression step. In conventional methods, one has to compute the kernel matrix K \u2208 \u211dn\u00d7n in this step, and the computational complexities and the storage requirements are O(n3) and O(n2) , respectively. This will be very time-consuming and even be infeasible when n is extremely large.\nFortunately, kernel matrix is often approximately low-rank, based on the observation that the spectrum of the Gaussian kernel decays rapidly (Hofmann et\u00a0al., 2008; Pan et\u00a0al., 2011; Wathen & Zhu, 2015; Wang et\u00a0al., 2018). Hence, devising scalable algorithms for kernel methods has long been an active research topic, and the key is to construct low-rank approximations to the kernel matrix (Iosifidis et\u00a0al., 2015; Wang et\u00a0al., 2018; Wathen & Zhu, 2015). For example, an interpolation method was used to construct the approximation of kernel matrix (Cambier & Darve, 2019; Xing & Chow, 2020), and a low-rank approximation was constructed in Iske et\u00a0al., (2017) with the help of hierarchical low-rank property of kernel matrix. However, to the best of our knowledge, most of the existing results are purely theoretical and are difficult to use in practice.\nIn this section, we first show the numerically low-rank property of the popular used Gaussian kernel matrix from a theoretical point of view. Based on the proposed results, we shed light on how to determine an appropriate target rank for randomized algorithms. We then provide a modified Nystr\u00f6m method with fixed-rank, and establish a probabilistic error bound on the low-rank approximation."
        },
        {
            "heading": "4.1 On the\u00a0approximately low\u2011rank property of\u00a0kernel matrix",
            "text": "Low-rank approximations are popular techniques to reduce the high computational cost of algorithms for large-scale kernel matrices (Halko et\u00a0al., 2011; Hofmann et\u00a0al., 2008; Wang et\u00a0al., 2018; Wathen & Zhu, 2015). In essence, the success of these low-rank algorithms hinges on a large spectrum gap or a fast decay of the spectrum of the kernel matrix (Halko et\u00a0al., 2011; Hofmann et\u00a0al., 2008; Wang et\u00a0al., 2018; Wathen & Zhu, 2015). This motivates the analysis on the numerical rank of kernel matrix; see Bach (2013), Wathen and Zhu (2015), Wang et\u00a0al. (2018) and the references therein. However, it seems there are few theoretical results that can provide both simple and feasible strategies for the target rank used in randomized algorithms hitherto.\nTo fill in this gap, we investigate the numerical rank of the kernel matrix, and provide a suitable target rank for practical use. The popular used Radial Basis Function (RBF) or Gaussian kernel function is considered\nwhere the value of the Gaussian scale is set to be the mean Euclidean distance between the training vectors, corresponding to the natural scaling value of each data set (Iosifidis & Gabbouj, 2016). We need the following definition for numerical rank of a matrix.\nDefinition 4.1 Higham and Mary (2019) Let A \u2208 \u211dn\u00d7n be nonzero. For k \u2264 n , the rank-k accuracy of A is\n(42) ( i, j) = exp \ufffd \u2212 \u2016 i \u2212 j\u2016 2 2\n2 2\n\ufffd\n,\n1 3\nWe call Wk an optimal rank-k approximation to A if Wk achieves the minimum in (43). The numerical rank of A at accuracy , denoted by k (A) , is\nThe matrix A is of low numerical rank if \ud835\udf00k(A) \u226a 1 for some k \u226a n.\nLet U\u03a3VT be the singular value decomposition (SVD) of A, with singular values 1 \u2265 2 \u2265 \u22ef \u2265 n . Denote by Uj,Vj be the matrices composed of the first j columns of U,\u00a0V, respectively, and by \u03a3j the j-by-j principle submatrix of \u03a3 . In terms of Definition 4.1, Wj = Uj\u03a3jVTj is an optimal rank-j approximation to A, and if\nthen the matrix A is of low numerical rank, where j+1(A) is the (j + 1)-th largest singular value of A.\nThe main aim of this subsection is to show that kernel matrix K has low numerical rank which depends on the number of clusters s. Let X = [ 1, 2,\u2026 , n] \u2208 \u211dm\u00d7n be the set of training samples, with i \u2208 \u211dm and \u2016 i\u20162 = 1, i = 1,\u2026 , n . Assume that the data matrix is partitioned into s classes as X = [X1,X2,\u2026 ,Xs] , where Xj is the j-th set with nj being the number of samples. In supervised methods, the number of s is known, otherwise, one can use, say, the K-means method (Wu et\u00a0al., 2008) for choosing an appropriate s in advance. Additionally, denote by j the centroid vector of Xj and by \u0394j = Xj \u2212 j Tnj , j = 1, 2,\u2026 , s , then\nwhere nj is the vector of all ones with dimension nj , and\nThanks to the structure of the kernel matrix K, we can decompose it into s blocks corresponding to the classification indexes in Xj , i.e.,\nwhere \u0302j is the centroid vector of Kj, j = 1, 2,\u2026 , s , and\nWe are ready to present the main theorem of this subsection on numerically low-rank property of Gaussian kernel matrix.\nTheorem\u00a04.2 Under the above notations, we have\n(43) k(A) = min Wk\u2208\u211d n\u00d7n\n\ufffd \u2016A \u2212Wk\u20162\n\u2016A\u20162 \u2236 rank(Wk) \u2264 k\n\ufffd\n.\nk (A) = min{k \u2236 k(A) \u2264 }.\n(44)\ud835\udf00j(A) = \ud835\udf0ej+1(A)\n\ud835\udf0e1(A) \u226a 1,\n(45)X = [X1,X2,\u2026 ,Xs] = [ 1 Tn1 , 2 T n2 ,\u2026 , s T ns ] + [\u03941,\u03942,\u2026 ,\u0394s] = X\u0302 + \u0394,\n(46)\u0394 = X \u2212 [ 1 Tn1 , 2 T n2 ,\u2026 , s T ns ] = [\u03941,\u03942,\u2026 ,\u0394s].\n(47)K = [K1,K2,\u2026 ,Ks] = [\u03021 Tn1 , \u03022 T n2 ,\u2026 , \u0302s T ns ] + [\u0394\u03021, \u0394\u03022,\u2026 , \u0394\u0302s] =K\u0302 + \u0394\u0302,\n(48)K\u0302 = [\u03021 Tn1 , \u03022 T n2 ,\u2026 , \u0302s T ns ], \u0394\u0302 = [\u0394\u03021, \u0394\u03022,\u2026 , \u0394\u0302s].\n1 3\nwhere is Gaussian scale value in the radial basis function (RBF), and \u2212 2 \ud835\udf0e2 < \ud835\udf01 < 0.\nProof Let i and j be in the q-th class, 1 \u2264 i, j \u2264 n, 1 \u2264 q \u2264 s . First, we establish the relationship between the i-th column i and the j-th column j of the RBF kernel matrix defined in (42). Notice that\nDenote by\nand without loss of generality, we suppose that tz,i < tz,j . Since the exponential function is continuous and derivable in the interval [tz,i, tz,j] , it follows from the Lagrange mean value theorem (Zoric, 2008) that there exists a point i,j,z \u2208 (tz,i, tz,j) , such that\nwhere we have \u2212 2 \ud835\udf0e2 < \ud835\udf01i,j,z < 0 , as \u2016 i\u20162 = \u2016 j\u20162 = \u2016 z\u2016 = 1 . Moreover, we obtain from (50) that\nA combination of (51) and (52) yields\nwhere = max i,j,z i,j,z and \u2212 2 \ud835\udf0e2 < \ud835\udf01 < 0 . As a result,\nSecond, we consider the relation between \u2016\u0394q\u20162 and \u2016\u0394\u0302q\u2016F , 1 \u2264 q \u2264 s . It follows from (54) that\n(49) s+1(K)\n1(K) \u2264 4\n\ufffd nmax 1\u2264i\u2264s ni e 2 \u22c5 \u2016\u0394\u2016F\n\u2016K\u20162 ,\n(50) \u2016 i \u2212 j\u20162 = \u2016 i \u2212 q + q \u2212 j\u20162 \u2264 \u2016 i \u2212 q\u20162 + \u2016 j \u2212 q\u20162 \u2264 2\u2016Xq \u2212 q T nq \u20162 = 2\u2016\u0394q\u20162.\ntz,i = \u2212 \u2016 z \u2212 i\u2016\n2 2\n2 2 and tz,j = \u2212\n\u2016 z \u2212 j\u2016 2 2\n2 2 , 1 \u2264 z \u2264 n,\n(51)\n\ufffdetz,j \u2212 etz,i \ufffd =e i,j,z \u22c5 \ufffd(tz,j \u2212 tz,i)\ufffd\n=e i,j,z \u22c5\n\ufffd \ufffd \ufffd \u2016 z \u2212 i\u2016 2 2 \u2212 \u2016 z \u2212 j\u2016 2 2 \ufffd \ufffd \ufffd\n2 2 , 1 \u2264 z \u2264 n,\n(52)\n\ufffd \ufffd \ufffd \u2016 z \u2212 i\u2016 2 2 \u2212 \u2016 z \u2212 j\u2016 2 2 \ufffd \ufffd \ufffd = \ufffd \ufffd \ufffd (\u2016 z \u2212 i\u20162 + \u2016 z \u2212 j\u20162) \u22c5 (\u2016 z \u2212 i\u20162 \u2212 \u2016 z \u2212 j\u20162) \ufffd \ufffd \ufffd\n\u22644 \ufffd \ufffd \ufffd \u2016 z \u2212 i\u20162 \u2212 \u2016 z \u2212 j\u20162 \ufffd \ufffd \ufffd\n\u22644\u2016 j \u2212 i\u20162 \u2264 8\u2016\u0394q\u20162.\n(53)\ufffdetz,j \u2212 etz,i \ufffd \u2264 e i,j,z \u22c5 4\u2016\u0394q\u20162\n2 \u2264 e \u22c5\n4\u2016\u0394q\u20162\n2 , 1 \u2264 z \u2264 n,\n(54)\u2016 i \u2212 j\u20162 =\n\ufffd n\ufffd\nz=1\n\ufffdetz,j \u2212 etz,i \ufffd2\n\ufffd 1 2\n\u2264 4 \u221a ne \u22c5 \u2016\u0394q\u20162\n2 .\n1 3\nwhere q,t , t = 1, 2,\u2026 , nq , are the t-th column of the matrix Kq. Third, we focus on the relationship between \u2016\u0394\u0302\u2016F and \u2016\u0394\u2016F . We have from (55) that\nFinally, it follows from (48) that rank(K\u0302) \u2264 s , and s+1(K\u0302) = 0 . Thus, we have from the perturbation theory of singular values (Golub & Van Loan, 2014),\u00a0Corollary 8.6.2 and (56) that\nwhich completes the proof. \u25fb\nRemark 4 We show that the kernel matrix K has low numerical rank that depends on the number of clusters s. Let \u2016\u0394\u20162 = \u2211s i=1 \u2016\u0394i\u20162\ns , which reflects the clustering effect of the origi-\nnal data X. Then Theorem\u00a04.2 indicates that\nIn other words, if \u2016\u0394\u20162 \u2016K\u20162 is sufficiently small, then the kernel matrix K is numerically lowrank, and the number of clusters s can be viewed as a numerical rank of K. This provides a target rank for solving the kernel regression problem, with applications to some randomized algorithms; see Sect.\u00a04.2. Moreover, the proof also applies to other kernel functions such as the Laplacian kernel (Hofmann et\u00a0al., 2008)."
        },
        {
            "heading": "4.2 A modified Nystr\u00f6m method with\u00a0fixed\u2011rank",
            "text": "In this subsection, we consider how to solve the kernel regression problem (17) efficiently. As was mentioned in Remark 1, an explicit computation of the matrix K\u0303K\u0303T can be prohibitive, and an alternative is to use some low-rank approximations to K\u0303K\u0303T without forming it explicitly. We have from Sect.\u00a04.1 that the kernel matrix K is numerically low-rank and the number of clusters s can be used as a numerical rank of K. Hence, s can also be used as\n(55)\n\u2016\u0394\u0302q\u2016F =\u2016Kq \u2212 \u0302q T nq \u2016F\n=\n\ufffd \ufffd \ufffd \ufffd nq\ufffd\nh=1\n\u2016 q,h \u2212 \u0302q\u2016 2 2\n\u2264\n\ufffd \ufffd \ufffd \ufffd nq\ufffd\nh=1\n( \u2211nq\nt=1 \u2016 q,h \u2212 q,t\u20162)\n2\nn2 q\n\u22644 \u221a n \u22c5 nqe \u22c5\n\u2016\u0394q\u20162\n2 ,\n(56)\n\u2016\u0394\u0302\u2016F =\n\ufffd \ufffd \ufffd \ufffd s\ufffd\ni=1\n\u2016\u0394\u0302i\u2016 2 F \u2264\n\ufffd \ufffd \ufffd \ufffd s\ufffd\ni=1\n\ufffd\n4 \u221a n \u22c5 nie \u22c5 \u2016\u0394i\u20162\n2\n\ufffd2 \u2264 4 \ufffd n \u22c5 max\n1\u2264i\u2264s ni e 2 \u2016\u0394\u2016F .\ns+1(K)\n1(K) =\n\ufffd s+1(K) \u2212 s+1(K\u0302)\ufffd\n1(K) \u2264\n\u2016\u0394\u0302\u2016F \u2016K\u20162 \u2264 4 \ufffd n \u22c5 max 1\u2264i\u2264s ni e 2 \u2016\u0394\u2016F \u2016K\u20162 ,\n(57) s+1(K)\n1(K) = O\n\ufffd \u2016\u0394\u20162\n\u2016K\u20162\n\ufffd\n.\n1 3\na target rank of the reduced kernel matrix K\u0303 \u2208 \u211dr\u00d7n which is a sub-matrix of the original kernel matrix K \u2208 \u211dn\u00d7n . Thus, the idea is to choose s as the numerical rank of K\u0303K\u0303T , and compute\ninstead of (17) for the kernel regression step. Given the target rank k, the standard Nystr\u00f6m method (Williams & Seeger, 2001; Drineas & Mahoney, 2005) constructs a rank-k approximation to an arbitrary symmetric positive semidefinite (SPSD) kernel matrix H \u2208 \u211dr\u00d7r by using only a few columns (or rows) of the matrix. More precisely, let l be the number of sampling, we denote by C \u2208 \u211dr\u00d7l(r > l > k) the matrix consists of l columns sampled from the kernel matrix H, and by W \u2208 \u211dl\u00d7l the intersection matrix formed by the intersection of these l columns and the corresponding l rows. The rank-l and rank-k Nystr\u00f6m approximation are\nrespectively, where [[W]]k represents the best rank-k approximation to W. Although this method can avoid accessing the entire kernel matrix, and thus greatly reduces the amount of calculation cost and storage requirement, it may suffer from losing of accuracy. Indeed, it was shown that no matter what sampling technique is employed, the incurred error in the Nystr\u00f6m approximation must grow with the matrix size r at least linearly (Wang & Zhang, 2013). As a result, the approximation obtained from the standard Nystr\u00f6m method may be unsatisfactory when r is large, unless a considerable number of columns are selected.\nIn Cortes et\u00a0 al. (2010), Cortes et\u00a0 al., pointed out that a tighter kernel approximation may lead to a better learning accuracy, so it is necessary to find kernel approximation models with better accuracies than the standard Nystr\u00f6m method. For instance, a modified Nystr\u00f6m method (Wang & Zhang, 2013; Sun et\u00a0al., 2015) was proposed by borrowing the techniques in CUR matrix decomposition. With the selected columns C \u2208 \u211dr\u00d7l at hand, the rank-l modified Nystr\u00f6m approximation uses\nas an approximation to the kernel matrix H, where\nIt is seen from (60) and (59) that the modified Nystr\u00f6m approximation H\u0303mod is no worse than the standard rank-l Nystr\u00f6m approximation H\u0303nys\nl .\nAlthough Nystr\u00f6m method aims to compute a rank-k approximation, it is often preferred to choose l > k landmark points and then restrict the resultant approximation to have rank at most k. Recently, a new alternative called the fixed-rank Nystr\u00f6m approximation was proposed (Anaraki & Becker, 2019), in which\nis utilized as an approximation to H. Theoretical analysis and numerical experiments show that the fixed-rank Nystr\u00f6m approximation H\u0303opt is superior to the standard rank-k Nystr\u00f6m method H\u0303nys\nk with respect to the nuclear norm (Anaraki & Becker, 2019).\n(58)A\u0303 = (K\u0303K\u0303T )\u2020K\u0303T ,\n(59)H\u0303nysl = CW \u2020CT and H\u0303 nys k = C[[W]]\u2020 k CT ,\n(60)H\u0303mod = C(C\u2020H(C\u2020)T )CT = CUmodCT\nUmod = argmin U\u2208\u211dl\u00d7l \u2016H \u2212 CUCT\u2016F = C \u2020H(C\u2020)T .\n(61)H\u0303opt = [[CW\u2020CT ]]k\n1 3\nInspired by the fixed-rank Nystr\u00f6m method (Anaraki & Becker, 2019) and the modified Nystr\u00f6m method (Wang & Zhang, 2013), we knit the two methods together and propose a modified Nystr\u00f6m method with fixed-rank. More precisely, we first perform the economized QR decomposition C = QR , then the rank-l approximation (60) can be rewritten as\nAfterward, we make use of\ni.e., the best rank-k approximation to QQTHQQT , as approximation to the kernel matrix H. In summary, we present in Algorithm\u00a05 our modified Nystr\u00f6m method with fixed-rank for the computation of the reconstruction weights matrix A arising in (58).\nRemark 5 Compared with the fixed-rank Nystr\u00f6m approximation (61), for the same selected columns C \u2208 \u211dr\u00d7l , the intersection matrix in our method reaches the solution of the optimization problem as in (60), and our approximation (63) is more accurate than the one obtained from the fixed-rank Nystr\u00f6m method. On the other hand, unlike many Nystr\u00f6m methods (Sun et\u00a0al., 2015), an advantage of (63) is that it is free of computing the Moore\u2013Penrose inverse C\u2020 . Finally, for clarity, we list the time and space complexities of\n(62)H\u0303mod = C(C\u2020H(C\u2020)T )CT = CC\u2020HCC\u2020 = QQTHQQT .\n(63)H\u0303modopt = [[QQ THQQT ]]k,\nments\n1 3\nthe fixed-rank Nystr\u00f6m method, the modified Nystr\u00f6m method, as well as Algorithm\u00a05; see Table\u00a02.\nNext, we will establish a probabilistic error bound for our modified Nystr\u00f6m method with fixed-rank. We first need the following Lemma.\nLemma 4.3 Tropp (2012) Given independent random p \u00d7 p symmetric positive semidefinite (SPSD) matrix G1,G2,\u2026 ,G , with the property\nwhere 1(Gi) is the largest eigenvalue of Gi and \ud835\udefe > 0 is a uniform upper bound of 1(Gi), i = 1, 2,\u2026 , . Defining Y = \u2211 i=1 Gi and min = min( (Y)) , then for any \u2208 (0, 1] , the following probability inequality holds\nwhere (Y) denotes expectation with respect to the random matrix Y.\nNotice that K\u0303 is an r \u00d7 n matrix, let K\u0303 = U\u0303\u03a3\u0303V\u0303T be the economized singular value decomposition of K\u0303 , where U\u0303 \u2208 \u211dr\u00d7n , \u03a3\u0303 \u2208 \u211dn\u00d7n and V\u0303 \u2208 \u211dn\u00d7n . Then K\u0303K\u0303T = U\u0303\u03a3\u03032U\u0303T , and we rewrite the singular value decomposition as\nwhere \u03a3\u03032 1 \u2208 \u211dk\u00d7k and U\u03031 \u2208 \u211dr\u00d7k . Let S \u2208 \u211dr\u00d7l be a random matrix that has only one entry equals to one and the rest are zero in each column, and at most one nonzero element in each row. Denote by\nAs k < l , one can assume that S1 is of full row rank. Now we are ready to present the following theorem for the probabilistic error bound on the low-rank approximation from Algorithm\u00a05.\nTheorem\u00a04.4 Let 1 \u2265 2 \u2265 3 \u2265 \u22ef \u2265 r \u2265 0 be the singular values of \ufffdK \u2208 \u211dr\u00d7n(r < n) , and let UkDkUTk be the low-rank approximation from Algorithm\u00a05. If S1 is of full row rank, then we have that\nwith probability at least 1 \u2212 2 , where\nand\n1(Gi) \u2264 , i = 1, 2,\u2026 , ,\n\u2119 { min(Y) \u2264 min } \u2264 p \u22c5\n( e \u22121 ) min\n,\n(64)K\u0303K\u0303T = U\u0303\u03a3\u03032U\u0303T = U\u0303 ( \u03a3\u03032 1\n\u03a3\u03032 2\n)( U\u0303T\n1 U\u0303T 2\n)\n,\n(65)S1 = U\u0303T1 S \u2208 \u211d k\u00d7l and S2 = U\u0303 T 2 S \u2208 \u211d(n\u2212k)\u00d7l.\n\u2016K\u0303K\u0303T \u2212 UkDkU T k \u20162\n\u2016K\u0303K\u0303T\u20162\n\u2264 2\n\ufffd\n3 2 + 1 \u221a \u22c5\n\ufffd\n1 +\n\ufffd r \u2212 1\nr +\n\ufffd (r \u2212 1)(n \u2212 k)\nrl\n\ufffd\ufffd 2 k+1 (K\u0303)\n2 1 (K\u0303)\n\ud835\udeff = k\n( e\ud835\udf03 \u2212 1\n\ud835\udf03\ud835\udf03\n) l k\ud835\udf070\n, 0 < \ud835\udf03 \u2264 1,\n1 3\nis the matrix coherence of U\u03031 , with (U\u0303T1 )i and (U\u03031U\u0303 T 1 )ii being the i-th column of U\u0303T1 and the i-th diagonal element of U\u03031U\u0303T1 , respectively.\nProof We have from Algorithm\u00a05 that\nwhere [[UDUT ]]k denotes the best rank-k approximation of the matrix UDUT . By using the notations in Algorithm\u00a05, we have\nand\nFirst, we analyze the probabilistic error bound on (66). Notice that\nBased on (67) and the singular value interlacing theorem (Golub & Van Loan, 2014,\u00a0p. 443), we have from (66) that\nwhere k+1(QQTK\u0303K\u0303TQQT ) and k+1(K\u0303K\u0303T ) are the (k + 1)-th largest singular value of the matrices QQTK\u0303K\u0303TQQT and K\u0303K\u0303T , respectively.\nSecond, we consider the term \u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162 . As S1 is of full row rank, we obtain from (Halko et\u00a0al., 2011),\u00a0Theorem\u00a09.1) that\nand thus\nThird, we consider the upper bound of \u2016S\u2020 1 \u20162 , whose proof is along the line of (Gittens, 2011),\u00a0Lemma 1. Notice that\n0 = r\nk max 1\u2264i\u2264r\n\u2016(U\u0303T 1 )i\u2016 2 2 =\nr k max 1\u2264i\u2264r (U\u03031U\u0303 T 1 )ii\n\u2016K\u0303K\u0303T \u2212 UkDkU T k \u20162 = \u2016K\u0303K\u0303 T \u2212 [[UDUT ]]k\u20162,\n[[UDUT ]]k = [[QVDV TQT ]]k = [[QWW TQT ]]k = [[QQ TK\u0303K\u0303TQQT ]]k,\n(66)\u2016K\u0303K\u0303T \u2212 UkDkUTk \u20162 = \u2016K\u0303K\u0303 T \u2212 [[QQTK\u0303K\u0303TQQT ]]k\u20162.\n(67) \u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303TQQT\u20162 \u2264\u2016K\u0303K\u0303 T \u2212 QQTK\u0303K\u0303T\u20162 + \u2016QQ T (K\u0303K\u0303T \u2212 K\u0303K\u0303TQQT )\u20162\n\u22642\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162.\n(68)\n\u2016K\u0303K\u0303T \u2212 [[QQTK\u0303K\u0303TQQT ]]k\u20162\n\u2264 \u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303TQQT\u20162 + \u2016QQ TK\u0303K\u0303TQQT \u2212 [[QQTK\u0303K\u0303TQQT ]]k\u20162\n\u2264 2\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162 + k+1(QQ TK\u0303K\u0303TQQT )\n\u2264 2\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162 + k+1(K\u0303K\u0303 T ),\n(69)\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u201622 \u2264 \u2016\u03a3\u0303 2 2 \u20162 2 + \u2016\u03a3\u03032 2 S2S\n\u2020 1 \u20162 2 ,\n(70)\n\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162\n\u2264 (\u2016\u03a3\u03032 2 \u20162 2 + \u2016\u03a3\u03032 2 S2S\n\u2020 1 \u20162 2 ) 1 2\n\u2264 \u2016\u03a3\u03032 2 \u20162 + \u2016\u03a3\u0303 2 2 S2S\n\u2020 1 \u20162\n\u2264 \u2016\u03a3\u03032 2 \u20162 + \u2016\u03a3\u0303 2 2 S2\u20162 \u22c5 \u2016S\n\u2020 1 \u20162.\n1 3\nwhere k(U\u0303T1 SS TU\u03031) is the k-th largest eigenvalue of U\u0303T1 SS TU\u03031 . Denote by (U\u0303T1 )i the i-th column of U\u0303T\n1 , then we have that U\u0303T 1 U\u03031 = \u2211r i=1 (U\u0303T 1 )i \u22c5 [(U\u0303 T 1 )i]\nT . Thanks to the property of S, let Gi \u2208 \u211d k\u00d7k, i = 1, 2,\u2026 , l , be matrices chosen randomly from the set {(U\u0303T 1 )i \u22c5 [(U\u0303 T 1 )i] T}r i=1\n, then we have from (71) that\nDefine = max 1\u2264i\u2264l 1(Gi) , then\nwhere 0 = r\nk max 1\u2264i\u2264r\n\u2016(U\u0303T 1 )i\u2016 2 2 =\nr k max 1\u2264i\u2264r (U\u03031U\u0303 T 1 )ii is the matrix coherence of U\u03031 (Gittens,\n2011), and (U\u03031U\u0303T1 )ii stands for the i-th diagonal element of U\u03031U\u0303 T 1 .\nDenote by min = min( ( \u2211l i=1 Gi)) , then\nwhere we use the orthogonality of the matrix U\u03031 . From Lemma 4.3, we obtain\nwhere \u2208 (0, 1] . Thus, a combination of (72) and (73) yields\nwhere = k ( e \u22121 ) l k 0.\nFourth, we establish a probabilistic bound on \u2016\u03a3\u03032 2 S2\u20162 in (70). Recall that S is a Gaussian matrix whose entries are independent normal variables with mean and variance 2 , i.e., S \u223c N( , 2) . Denote by \u03a9 = S\u2212 r T l\n, where r is the vector of all ones with dimension\nr, then \u03a9 is a standard Gaussian matrix and S = \u03a9 + r Tl . It follows from (65) that\nwhere we use \u2016 r Tl \u20162 = \u221a rl in the last inequality. Taking expectation with respect to (75) gives\n(71)\u2016S \u2020 1 \u20162 2 = \u2016(U\u0303T 1 S)\u2020\u20162 2 =\n1\n2 k (U\u0303T 1 S)\n= 1\nk(U\u0303 T 1 SSTU\u03031)\n,\n(72)\u2016S \u2020 1 \u20162 2 =\n1\nk(U\u0303 T 1 SSTU\u03031)\n= 1\nk( \u2211l i=1 Gi)\n.\n= max 1\u2264i\u2264l 1(Gi) = max 1\u2264i\u2264r\n\u2016(U\u0303T 1 )i\u2016 2 2 =\nk r 0,\nmin = min\n(\n(\nl\u2211\ni=1\nGi )) = l min ( 1\nr U\u0303T 1 U\u03031\n)\n= l\nr ,\n(73)\u2119\n{\nmin(\nl\u2211\ni=1\nGi) \u2264 l\nr\n}\n\u2264 k\n( e \u22121 ) l k 0\n,\n(74)\u2119 \ufffd \u2016S \u2020\n1 \u20162 \u2265\n\ufffd r\nl\n\ufffd\n\u2264 ,\n(75)\n\u2016\u03a3\u03032 2 S2\u20162 =\u2016\u03a3\u0303 2 2 U\u0303T 2 ( \u03a9 + r T l )\u20162\n\u2264 \u2016\u03a3\u03032 2 U\u0303T 2 \u03a9\u20162 + \u2016\u03a3\u0303 2 2 U\u0303T 2 r T l \u20162\n\u2264\n\ufffd\n\u2016U\u0303T 2 \u03a9\u20162 + \u2016U\u0303 T 2 r T l \u20162\n\ufffd\n\u2016\u03a3\u03032 2 \u20162\n\u2264\n\ufffd\n\u2016U\u0303T 2 \u03a9\u20162 +\n\u221a rl \ufffd\n\u2016\u03a3\u03032 2 \u20162,\n1 3\nNotice that U\u03032 is a column orthogonal matrix and the distribution of the standard Gaussian matrix \u03a9 is rotationally invariant, and U\u0303T\n2 \u03a9 \u2208 \u211d(n\u2212k)\u00d7l is also a standard Gaussian matrix.\nSo we have (Halko et\u00a0al., 2011)\nThus, a combination of (76) and (77) gives\nIn light of the Markov\u2019s inequality (Grimmett & Stirzaker, 2001), we get\nCombining (74) and (78), and applying the union bound, we have\nHence, we have the following probabilistic error bound for \u2016\u03a3\u03032 2 S2\u20162 \u22c5 \u2016S\n\u2020 1 \u20162 in (70), i.e.,\nwith probability at least 1 \u2212 2 , where\nand\nis the matrix coherence of U\u03031. Finally, based on (68), (70) and (80), a probabilistic error bound on the low-rank approximation [[QQTK\u0303K\u0303TQQT ]]k to K\u0303K\u0303T is given as follows.\n(76) (\u2016\u03a3\u030322S2\u20162) \u2264 \u2016\u03a3\u0303 2 2 \u20162 \u22c5\n\ufffd\n(\u2016U\u0303T 2 \u03a9\u20162) +\n\u221a rl \ufffd .\n(77) (\u2016U\u0303T 2 \u03a9\u20162) \u2264\n\u221a l + \u221a n \u2212 k.\n(\u2016\u03a3\u03032 2 S2\u20162) \u2264 \u2016\u03a3\u0303 2 2 \u20162 \u22c5\n\ufffd\n( \u221a l + \u221a n \u2212 k) + \u221a rl \ufffd .\n(78)\n\u2119\n\ufffd\n\u2016\u03a3\u03032 2 S2\u20162 \u2265\n( \u221a l + \u221a n \u2212 k) + \u221a rl\n\u2016\u03a3\u03032\n2 \u20162\n\ufffd\n\u2264 \ud835\udd3c(\u2016\u03a3\u03032 2 S2\u20162)\n( \u221a l+ \u221a n\u2212k)+ \u221a rl\n\u2016\u03a3\u03032\n2 \u20162\n\u2264 .\n(79)\u2119\n\ufffd\n\u2016S \u2020\n1 \u20162 \u2265\n\ufffd r\nl or \u2016\u03a3\u03032 2 S2\u20162 \u2265\n( \u221a l + \u221a n \u2212 k) + \u221a rl\n\u2016\u03a3\u03032\n2 \u20162\n\ufffd\n\u2264 2 .\n(80)\u2016\u03a3\u03032 2 S2\u20162 \u22c5 \u2016S\n\u2020 1 \u20162 \u2264\n\ufffd r\nl \u22c5\n( \u221a l + \u221a n \u2212 k) + \u221a rl\n\u2016\u03a3\u03032\n2 \u20162,\n\ud835\udeff = k\n( e\ud835\udf03 \u2212 1\n\ud835\udf03\ud835\udf03\n) l k\ud835\udf070\n, 0 < \ud835\udf03 \u2264 1,\n0 = r\nk max 1\u2264i\u2264r\n\u2016(U\u0303T 1 )i\u2016 2 2 =\nr k max 1\u2264i\u2264r (U\u03031U\u0303 T 1 )ii\n\u2016K\u0303K\u0303T \u2212 [[QQTK\u0303K\u0303TQQT ]]k\u20162\n\u2264 2\u2016K\u0303K\u0303T \u2212 QQTK\u0303K\u0303T\u20162 + k+1(K\u0303K\u0303 T ) \u2264 2 \ufffd\n\u2016\u03a3\u03032 2 \u20162 + \u2016\u03a3\u0303 2 2 S2\u20162 \u22c5 \u2016S\n\u2020 1 \u20162\n\ufffd\n+ k+1(K\u0303K\u0303 T )\n\u2264 2\n\ufffd\n1 +\n\ufffd r\nl \u22c5\n( \u221a l + \u221a n \u2212 k) + \u221a rl \ufffd\n\u2016\u03a3\u03032 2 \u20162 + k+1(K\u0303K\u0303 T )\n1 3\nholds with probability at least 1 \u2212 2 . Recall that S \u2208 \u211dr\u00d7l is a Gaussian distribution matrix with mean and variance 2 , and it is easy to check that = 1\nr and 2 = r\u22121 r2 . So we have\nholds with probability at least 1 \u2212 2 . In summary, we have from (66) and (81) that\nwith probability at least 1 \u2212 2 . \u25fb\nRemark 6 Theorem\u00a04.4 gives a relative error bound on approximating the matrix K\u0303K\u0303T . As was mentioned in Remark 4, if the clustering result of the original data X is ideal, then the reduced kernel matrix K\u0303 will have low numerical rank that is the number of clusters s. In other words, one can choose s as the target rank in randomized algorithms to solve the kernel regression step. Thus, from the probabilistic error bound established in Theorem\u00a04.4, if we adopt the number of clusters s as the target rank k, then 2\nk+1 (K\u0303)\u2215 2 1 (K\u0303) is sufficiently\nsmall, and our proposed algorithm will be effective."
        },
        {
            "heading": "5 A randomized block Kaczmarz method for\u00a0kernel regression problem with\u00a0multiple right\u2011hand sides",
            "text": "In Sect.\u00a04, we propose a low-rank approximation to K\u0303K\u0303T for solving (17). Although the proposed Nystr\u00f6m method is not required to form the matrix K\u0303K\u0303T explicitly, this method needs to form and store the reduced kernel matrix K\u0303 explicitly. In the era of big data, the reduced kernel matrix may be so huge that it can not be stored in main memory. To deal with this problem, in this section, we propose a randomized block Kaczmarz method for kernel regression problem. For notation simplicity, we write (17) as\nAs T \u2208 \u211dn\u00d7d with d > 1 , we call it a kernel regression problem with multiple right-hand sides.\n(81)\n\u2016K\u0303K\u0303T \u2212 [[QQTK\u0303K\u0303TQQT ]]k\u20162\n\u2264 2\n\ufffd\n1 +\n\ufffd r\nl \u22c5\n1\n\u22c5\n\u221a r \u2212 1( \u221a l + \u221a n \u2212 k) + \u221a rl\nr\n\ufffd\nk+1(K\u0303K\u0303 T ) + k+1(K\u0303K\u0303 T )\n= 2 \u239b \u239c \u239c \u239c \u239c \u239d 3 2 + 1 \u221a \u22c5\n\u221a r \u2212 1\n\ufffd\n1 + \ufffd n\u2212k\nl\n\ufffd\n+ \u221a r\n\u221a r\n\u239e \u239f \u239f \u239f \u239f \u23a0 k+1(K\u0303K\u0303 T )\n= 2\n\ufffd\n3 2 + 1 \u221a \u22c5\n\ufffd\n1 +\n\ufffd r \u2212 1\nr +\n\ufffd (r \u2212 1)(n \u2212 k)\nrl\n\ufffd\ufffd\n2 k+1 (K\u0303),\n\u2016K\u0303K\u0303T \u2212 UkDkU T k \u20162\n\u2016K\u0303K\u0303T\u20162\n\u2264 2\n\ufffd\n3 2 + 1 \u221a \u22c5\n\ufffd\n1 +\n\ufffd r \u2212 1\nr +\n\ufffd (r \u2212 1)(n \u2212 k)\nrl\n\ufffd\ufffd 2 k+1 (K\u0303)\n2 1 (K\u0303)\n,\n(82)X \u2217 = argmin X\u2208\u211dr\u00d7d \u2016BX \u2212 T\u20162 F , where B \u2261 K\u0303T \u2208 \u211dn\u00d7r.\n1 3\nThe randomized Kaczmarz method is a popular solver for large-scale and dense linear systems (Strohmer & Vershynin, 2009). An advantage of this type of method is that there is no need to access the full data matrix into main memory, and only a small portion of the data matrix is utilized to update the solution in each iteration. In Zouzias and Freris (2013), Zouzias and Freris introduced a randomized extended Kaczmarz (REK) method for solving the least squares problem min \u2208\u211dr \u2016B \u2212 \u201622 . In essence, it is a specific combination of the randomized orthogonal projection method together with the randomized Kaczmarz method. It was shown that, the solution of the randomized extended Kaczmarz method approaches the l2 norm least squares solution up to an additive error that depends on the distance between the right-hand side vector and the column space of the matrix B (Zouzias & Freris, 2013). More precisely, the randomized extended Kaczmarz method exploits the randomized orthogonal projection method to efficiently reduce the norm of the \u201cnoisy\u201d part R(B)\u22a5 of , where R(B)\u22a5 = (In \u2212 BB\u2020) . As the least squares solution is\nthe randomized Kaczmarz method is applied to a new linear system whose right-hand side is R(B) = BB\u2020 .\nThe block Kaczmarz method is generally considered to be more efficient than the classical Kaczmarz method, because of subtle computational issues involved in data transfer and basic linear algebra subroutines (Necoara, 2019; Needell & Tropp, 2014; Needell et\u00a0al., 2015). In Needell et\u00a0al. (2015), Needell et\u00a0al., put forward a randomized double block Kaczmarz method that is an extension to the block Kaczmarz method and the randomized extended Kaczmarz (REK) method. The randomized double block Kaczmarz method exploits column partition for the projection step and row partition for the Kaczmarz step. Consequently, the computational cost will be very high for large-scale data. As an alternative, a randomized block coordinate descent (RBCD) method was also proposed in Needell et\u00a0al. (2015), which utilizes only column paving for the projection step and Kaczmarz step for the resulting linear system. To reduce the amount of calculation, this method acquires a suitable partition to the columns based on random selections; for more details, refer to Needell et\u00a0al. (2015).\nLS = B \u2020 = B\u2020BB\u2020 \u2261 B\u2020 R(B),\n1 3\nTo the best of our knowledge, however, Kaczmarz-type methods have not yet been used to solve linear systems and least squares problems with multiple right-hand sides. Hence, on the basis of the randomized block coordinate descent method proposed in Needell et\u00a0al. (2015), we present in Algorithm\u00a07 a randomized block Kaczmarz method for regression problem with multiple right-hand sides.\nRemark 7 Some remarks are given to Algorithm\u00a0 7. First, compared with some existing methods for the optimization problem (17), an advantage of Algorithm\u00a07 is that there is no need to explicitly form and store all the elements of the reduced kernel matrix K\u0303 . Second, the randomized block Kaczmarz methods proposed in Necoara (2019), Needell and Tropp (2014), Needell et\u00a0al., (2015) are just for solving least squares problems or linear systems with only one right-hand side. As a comparison, Algorithm\u00a0 7 can solve the d problems once for all, where d is the discriminant space dimensionality. So the proposed method can accelerate some existing randomized block Kaczmarz methods significantly, especially when d is large. Third, we propose a new sampling scheme which is different from the standard block Kaczmarz method, see Step 5 in Algorithm\u00a07. More precisely, we first perform a random partition to the column index set with p blocks, and select one block arbitrarily from the partition. Then, we choose half of the columns corresponding to the largest norm from the selected block.\n1 3\nThe stopping criteria utilized in the existing randomized block Kaczmarz methods often relies on the full coefficient matrix B more or less (Necoara, 2019; Needell & Tropp, 2014; Needell et\u00a0al., 2015), which contradicts the purpose of not storing the entire coefficient matrix in main memory. To circumvent this difficulty, in Algorithm\u00a07, we propose a practical stopping criterion (83) in which there is no need to access the full coefficient matrix. Indeed, compared with the ( \u2212 1)-th iterative solution X( \u22121) , the -th iterative solution X( ) only updates those rows corresponding to the index set , while the other rows remain unchanged. More precisely, in Step 10 of Algorithm\u00a07, we make use of\nas a stopping criterion, and there is no need to access the entire coefficient matrix B. Now we show the rationality of this scheme. Denote by XLS = B\u2020T the least square solution with least F-norm of the optimization problem (82). It is easy to see that\nThus, if the iterative sequence {X( )}\u221e =0 converges to XLS , then \u2016X( ) \u2212 X( \u22121)\u2016F\u2215\u2016T\u2016F converges to 0, and (83) can be used as a stopping criterion for solving (82).\nDenote by B(\u2236, \u0303 ) = B\u0303 , in Algorithm\u00a07, we notice that the conditioning of the blocks B \u0303\nplays a crucial role in the behavior of the block Kaczmarz methods. In Needell and Tropp (2014), Needell et\u00a0al. (2015), Needell and Tropp give a definition of a \u201cpaving\" for the matrix B.\nDefinition 5.1 Needell and Tropp (2014) A column paving (p, , ) of an n \u00d7 r matrix B is a partition T\u0303 = {\u03031, \u03032,\u2026 , \u0303p} of the column indices such that\nwhere B \u0303 i = B(\u2236, \u0303 i) is composed of the columns in matrix B corresponding to the index \u0303 i.\nInspired by Needell et\u00a0al. (2015),\u00a0Theorem\u00a07), we give the following theoretical analysis on the convergence of the iterative sequence {X( )}\u221e\n=0 generated by Algorithm\u00a07.\nTheorem\u00a0 5.2 Denote by {X( )}\u221e =0 the iterative sequence generated by Algorithm\u00a0 7, and by (p, , ) a column paving of B. Assume that B is of full column rank, then there exists a scalar \u2208 (0, 1) such that\nwhere nz min (B) is the smallest non-zero singular value of B, and (B) = max(B) nz min (B) is the 2-norm condition number of B.\nProof We note that\n(83)err = \u2016X( ) \u2212 X( \u22121)\u2016F\n\u2016T\u2016F =\n\u2016X( )( , \u2236) \u2212 X( \u22121)(\n, \u2236)\u2016F\n\u2016T\u2016F =\n\u2016W \u2016F\n\u2016T\u2016F\n\ufffd \ufffd\u2016X ( ) \u2212 XLS\u2016F \u2212 \u2016X ( \u22121) \u2212 XLS\u2016F \ufffd \ufffd\n\u2016T\u2016F \u2264\n\u2016X( ) \u2212 X( \u22121)\u2016F\n\u2016T\u2016F \u2264\n\u2016X( ) \u2212 XLS\u2016F + \u2016X ( \u22121) \u2212 XLS\u2016F\n\u2016T\u2016F .\n\u2264 min(B\u0303iB T\n\u0303 i\n) and max(B\u0303 iB T \u0303i ) \u2264 , i = 1, 2,\u2026 , p,\n\ufffd \u2016XLS \u2212 X (\ud835\udcc1)\u20162 F\n\u2016XLS\u2016 2 F\n\ufffd\n\u2264\n\ufffd\n1 \u2212 \u22c5 ( nz min (B))2\np\n\ufffd \ud835\udcc1\n2(B),\n(84) B(XLS \u2212 X\n( )) = BB\u2020T \u2212 BX( ) = T \u2212 BX( ) \u2212 (In \u2212 BB \u2020)T = T \u2212 BX( ) \u2212 TR(B)\u22a5 ,\n1 3\nwhere XLS = B\u2020T and TR(B)\u22a5 = (In \u2212 BB\u2020)T . First, we prove that Z = T \u2212 BX( ) by induction on . For = 0 , we have from Algorithm\u00a0 7 that Z0 = T \u2212 BX(0) . Denote by B(\u2236, ) = B\n, we note that X(1)( 1, \u2236) = W1 and X(1)([r] \u29f5 1, \u2236) = , where [r] is the set {1, 2,\u2026 , r} , \u201c \u29f5 \" is the set subtraction operation, and [r] \u29f5 1 is the set whose elements belong to [r] but not to 1 . Thus, we have Z1 = Z0 \u2212 B 1W1 = T \u2212 BX (1) . Now we assume that Z \u22121 = T \u2212 BX ( \u22121) with \u2265 1 , then\nIt follows from Algorithm\u00a07 that BX( ) = BX( \u22121) + B W . For the sake of simplicity, we denote by X( )( , \u2236) = X( ) . From X( ) = X( \u22121) +W and X( )\n[r]\ufffd\n= X ( \u22121)\n[r]\ufffd\n, we obtain\nCombining (85) and (86), we arrive at\nAs a result, it follows from (84) and (87) that\nSecond, let F = Z \u2212 TR(B)\u22a5 , we take conditional expectation on F over , then\nwhere the second equality follows from the definition of Z , and the third one is from the definition of W and the fact that the two subspaces span{B B\u2020\n} and span{TR(B)\u22a5} are orthogonal.\nNotice that\nNext we consider the lower bound on \u2016B B\u2020\nF \u22121\u2016 2 F . Let B = U \u03a3 VT be the economized SVD decomposition of B , where U ,V are orthonormal and \u03a3\nis a diagonal matrix containing the non-zero singular values of B . Therefore,\nand we have from Definition 5.1 that\n(85)Z = Z \u22121 \u2212 B\nW = T \u2212 BX( \u22121) \u2212 B W .\n(86)\nBX( ) =B X( )\n+ B[r]\ufffd\nX ( )\n[r]\ufffd\n=B X( \u22121) + B\nW + B[r]\ufffd\nX ( )\n[r]\ufffd\n=B X( \u22121) + B\nW + B[r]\ufffd\nX ( \u22121)\n[r]\ufffd\n=BX( \u22121) + B W .\n(87)Z = T \u2212 (BX( ) \u2212 B W ) \u2212 B W = T \u2212 BX( ).\n(88)B(XLS \u2212 X( )) = Z \u2212 TR(B)\u22a5 .\n(89)\n\u2016F \u20162 F = \u2016Z \u2212 TR(B)\u22a5\u2016 2 F\n= \u2016Z \u22121 \u2212 B\ud835\udf0f W \u2212 TR(B)\u22a5\u2016 2 F\n= \u2016Z \u22121 \u2212 B\ud835\udf0f B\u2020 \ud835\udf0f Z \u22121 \u2212 (In \u2212 B\ud835\udf0f B\u2020 \ud835\udf0f )TR(B)\u22a5\u2016 2 F\n= \u2016(In \u2212 B\ud835\udf0f B\u2020 \ud835\udf0f )(Z \u22121 \u2212 TR(B)\u22a5 )\u2016 2 F\n= \u2016(In \u2212 B\ud835\udf0f B\u2020 \ud835\udf0f )F \u22121\u2016 2 F ,\n(90)\u2016(In \u2212 B\nB\u2020\n)F \u22121\u2016 2 F = \u2016F \u22121\u2016 2 F \u2212 \u2016B B\u2020 F \u22121\u2016 2 F .\n\u2016B B\u2020\nF \u22121\u2016 2 F = \u2016U UT F \u22121\u2016 2 F = \u2016\u03a3\u22121 VT BT F \u22121\u2016 2 F ,\n1 3\nFurther, we have from (88) that span{F \u22121} \u2286 R(B) , so it is known from the Courant-Fischer Theorem (Golub & Van Loan, 2014,\u00a0 p. 441) that \u2016BTF \u22121\u2016 2 F \u2265 ( nz min (B))2\u2016F \u22121\u2016 2 F , where nz min\n(B) is the smallest non-zero singular value of B. Thus, it follows from Step 5 in Algorithm\u00a07 that there exists a scalar \u2208 (0, 1) , such that\nHence, combining (89), (90) and (58), we get\nNotice that \u2016F0\u20162F = \u2016Z0 \u2212 TR(B)\u22a5\u2016 2 F = \u2016BB\u2020T\u20162 F , and a combination of (88) and (93) yields\nFinally, when B is of full column rank, we obtain\nwhere (B) = max(B) nz min (B) denotes condition number of matrix B, and the proof is completed.\n\u25fb\n(91)\n\u2016B \ud835\udcc1 B\u2020 \ud835\udcc1 F \ud835\udcc1\u22121\u2016 2 F\n\u2265\n\ufffd\n2 min (\u03a3\u22121 \ud835\udcc1 VT \ud835\udcc1 ) \u22c5 \u2016BT \ud835\udcc1 F \ud835\udcc1\u22121\u2016 2 F\n\ufffd\n=\n\ufffd \u2016BT\n\ud835\udcc1\nF \ud835\udcc1\u22121\u2016 2 F\n2 max (B \ud835\udcc1 )\n\ufffd\n\u2265 1 \u22c5 \u2016BT\n\ud835\udcc1\nF \ud835\udcc1\u22121\u2016 2 F .\n(92)\n\u2016B \ud835\udcc1 B\u2020 \ud835\udcc1 F \ud835\udcc1\u22121\u2016 2 F\n\u2265 1 \u2016BT\n\ud835\udcc1\nF \ud835\udcc1\u22121\u2016 2 F\n= 1\n\ufffd\n\u2208T\n\u2016BT F \ud835\udcc1\u22121\u2016 2 F \u22c5\n1\np\n= p \u2016BTF \ud835\udcc1\u22121\u2016 2 F\n\u2265 \u22c5 ( nz min (B))2\np \u2016F\n\ud835\udcc1\u22121\u2016 2 F .\n(93)\n\u2016F \ud835\udcc1 \u20162 F =\u2016F \ud835\udcc1\u22121\u2016 2 F \u2212 \u2016B\n\ud835\udcc1 B\u2020 \ud835\udcc1 F \ud835\udcc1\u22121\u2016 2 F\n\u2264\n\ufffd\n1 \u2212 \u22c5 ( nz min (B))2\np\n\ufffd\n\u2016F \ud835\udcc1\u22121\u2016 2 F\n\u2264\n\ufffd\n1 \u2212 \u22c5 ( nz min (B))2\np\n\ufffd \ud835\udcc1\n\u2016F0\u2016 2 F .\n(94) \u2016B(XLS \u2212 X(\ud835\udcc1))\u20162F \u2264\n\ufffd\n1 \u2212 \u22c5 ( nz min (B))2\np\n\ufffd \ud835\udcc1\n\u2016BB\u2020T\u20162 F .\n\ufffd \u2016XLS \u2212 X (\ud835\udcc1)\u20162 F\n\u2016XLS\u2016 2 F\n\ufffd\n\u2264\n\ufffd\n1 \u2212 \u22c5 ( nz min (B))2\np\n\ufffd \ud835\udcc1\n2(B),\n1 3"
        },
        {
            "heading": "6 Numerical experiments",
            "text": "In this section, we perform numerical experiments on some real-world databases to illustrate the numerical behavior of our proposed algorithms. In all the experiments, the data vectors are normalized so that \u2016 i\u20162 = 1 , i = 1, 2,\u2026 , n . Moreover, we consider three popular used kernels (Hofmann et\u00a0al., 2008), including the Gaussian kernel function\nthe Laplacian kernel function\nand the Polynomial kernel function\nwhere = 2 , and the value of the Gaussian scale \ud835\udf0e > 0 is set to be the mean Euclidean distance between the training vectors i (Iosifidis & Gabbouj, 2017). All the numerical experiments are carried on a Hp workstation with 16 cores double Intel(R)Xeon(R) E5-2620 v4 processors, and with CPU 2.10 GHz and RAM 64 GB. The operation system is 64-bit Windows 10. The numerical results are obtained from running the MATLAB R2016b software.\nThere are seven databases used in our experiments, including five facial image databases AR, CMU-PIE, Extended YaleB, Facescrub, and YouTube Faces, a handwritten digits database MNIST, and a tiny images database CIFAR-100. Table\u00a0 3 lists the details of these databases.\n\u2022 The AR database1 consists of over 4000 facial images (70 male and 56 female) having a frontal facial pose, exhibiting several facial expressions (e.g. anger, smiling and screaming), in different illumination source directions (left and/or right) and with some occlusions (e.g. sun glasses and scarf). A subset of s = 100 persons (50 males and 50 females) with 26 images of per people, i.e., 2600 images are utilized in our experiments. We re-scaled the original facial images to 40 \u00d7 30-pixel images, which are subsequently vectorized to m = 1200 dimensional facial vectors. \u2022 The CMU-PIE2 (Sim et\u00a0al., 2003) database consists of more than 40,000 images for s = 68 subjects with more than 500 images in each class. These face images are captured by 13 synchronized cameras and 21 flashes under varying pose, illumination, expression and lights. In our experiments, we choose 170 images under different illuminations, lights, expressions and poses for each subject. Thus, the total number of images chosen from CMU-PIE database is 11,\u00a0 560. We crop the images to 32 \u00d7 32 pixels and get m = 1024 dimensional facial vector representations. \u2022 The Extended YaleB3 database contains 5760 single light source images of 10 subjects, each seen under 576 viewing conditions (9 different poses and 64 illumination\n( i, j) = exp \ufffd \u2212 \u2016 i \u2212 j\u2016 2 2\n2 2\n\ufffd\n,\n( i, j) = exp \ufffd \u2212 \u2016 i \u2212 j\u20162 \ufffd ,\n( i, j) = ( T i j) ,\n1 http:// rvl1. ecn. purdue. edu/ ~aleix/ aleix_ face_ DB. html. 2 http:// www. cs. cmu. edu/ afs/ cs/ proje ct/ PIE/ web/. 3 http:// cvc. yale. edu/ proje cts/ yalef acesB/ yalef acesB. html.\n1 3\nconditions of each person). The images have normal, sleepy, sad and surprising expressions. In this experiment, we make use of a subset of s = 38 persons with 64 images to per people, i.e., 2432 images, which are cropped and scaled to 64 \u00d7 64 pixels. \u2022 The Facescrub4 (Ng and Winkler, 2014) database contains 106,863 photos of 530 celebrities (265 male and 265 female). The initial images that make up this dataset are procured using Google Image Search. Subsequently, they are processed using the Haarcascade-based face detector from OpenCV 2.4.7 on the images to obtain a set of faces for each celebrity name, with the requirement that a face must be at least 96 \u00d7 96 pixels. In our experiment, we use 22631 photos from 256 male, and scale the images to 9216 pixels. \u2022 The Youtube Faces5 (Wolf et\u00a0al., 2011) consists of 621126 facial images depicting 1595 persons. In our experiments, we choose the facial images of people with at least 500 images, resulting to a dataset of 370319 images and s = 340 classes. Subsequently, each facial image is vectorized to a facial image representation of m = 1024 dimensions. \u2022 The MNIST6 database of handwritten digits has 70,000 examples. It was derived from a much larger dataset known as the NIST Special Database 19 (Grother, 1995) which contains digits, uppercase and lowercase handwritten letters. Moreover, the MNIST database contains a total of s = 10 numbers from 0 to 9. For simplicity, each digit image is flattened and converted into a one-dimensional array of m = 28 \u00d7 28 = 784 features. \u2022 The dataset CIFAR-1007 (Krizhevsky, 2009), named after the Canadian Institute for Advanced Research, is labeled subset of the 80 million tiny images dataset. Furthermore, it comes in 20 superclasses of five classes each. For example, the superclass reptile consists of the five classes crocodile, dinosaur, lizard, turtle and snake. The idea is that classes within the same superclass are similar. Each image comes with a \u201cfine\u201d label (the class to which it belongs) and a \u201ccoarse\u201d label (the superclass to which it belongs). In our experiment, the \u201cfine\u201d labels are utilized, which results in 600 examples of each of s = 100 non-overlapping classes. In other words, in our experiment, 60,000 images with each m = 3072-pixel are used.\nOne refers to Figs.\u00a01, 2 and 3 for some samples of the five face databases, the handwritten digits database MNIST and the tiny images dataset CIFAR-100, respectively. In all the experiments, we randomly split each ID class into two sets, 70 percent for training and 30 percent for testing. To measure the effectiveness of the compared algorithms, we calculate the value of Area Under the Receiver Operating Characteristic Curve (AUC) (Fawcett, 2006; Zhang et\u00a0 al., 2015; Ling et\u00a0 al., 2003) and the value of the Equal Error Rate (EER) (Goudelis et\u00a0al., 2007; Friedman et\u00a0al., yyy) for each face verification problem. More precisely, to calculate the AUC and EER metrics, we project the test samples to the corresponding discriminant subspace and depict the similarity between the reduced vector i and the client class mean vector , by using si = \u2016 i \u2212 \u2016\u221212 . The similarity values of all test samples are then sorted in a descending order, and the AUC and EER metrics are calculated. Notice that the smaller the EER values, the larger the AUC values, and the less\n4 http:// vinta ge. winkl erbros. net/ faces crub. html. 5 https:// www. cs. tau. ac. il/ ~wolf/ ytfac es/. 6 http:// yann. lecun. com/ exdb/ mnist/. 7 http:// www. cs. toron to. edu/ ~kriz/ cifar. html.\n1 3\nthe CPU time, the better an algorithm. In order to eliminate randomness caused by the training-test partition, we apply the above process five times and list the mean values of AUC, EER, the mean CPU time in seconds, as well as the mean standard deviation (StdDev) in the tables below.\nIn this section, the target rank used in all the Nystr\u00f6m-type methods including the standard Nystr\u00f6m, the modified Nystr\u00f6m, the fixed-rank Nystr\u00f6m methods and Algorithm\u00a05, is based on our proposed strategy. That is, the number of clusters s is used as the target rank unless otherwise stated.\nExample 1 In this example, we show the efficiency of our proposed trace-ratio model (27) and ratio-trace model (28) in the eigenanalysis step for the calculation of T. To this aim, we compare Algorithm\u00a03 and Algorithm\u00a04 with Algorithm\u00a01 proposed in Iosifidis and Gabbouj (2016a). Three test sets including AR, CMU-PIE and Extended YaleB are used in this example. We choose the reference vector set cardinality r = 1000 and the discriminant space dimensionality d = 10, 20, 30 for the AR data set, r = 3000 and the discriminant\n1 3\nspace dimensionality d = 50, 60, 70 for the CMU-PIE data set, and r = 1000 and the discriminant space dimensionality d = 10, 20, 30 for the Extended YaleB data set. Tables\u00a04, 5 and 6 list the experimental results, where we explore the Cholesky factorization for solving (18) in the kernel regression step.\nIt is obvious to see from Tables\u00a04, 5 and 6 that the AUC values obtained and the CPU time used are comparable for the three algorithms, while the EER values from Algorithm\u00a03 and Algorithm\u00a04 are better than those from Algorithm\u00a01. These show the advantages and illustrate the superiority of the two proposed algorithms over Algorithm\u00a01.\nExample 2 The aim of this example is twofold. First, we try to show the effectiveness of Theorem\u00a04.2. Second, we illustrate the rationality of using the number of clusters s as a target rank for randomized algorithms. In Theorem\u00a0 4.2, it is pointed out that the kernel matrix K is numerically low-rank, and the numerical rank is closely related to the clustering effect of the original data X. Moreover, the better the clustering effect, the closer the numerical rank is to the number of cluster s. We make use of some semi-artificial data based on the two data sets CMU-PIE and Extended YaleB to illustrate this. For the two databases, we first compute the centroid vector j of each class Xj to get X = [ 1\nT n1 , 2 T n2 ,\u2026 , s T ns ] , where ni \u2208 \u211d ni is the vector of all ones. We then use the MATLAB built-in function rand.m to generate a random matrix \u0394\u0303 with uniform distribution, and construct the semi-artificial data X\u0303 = X + \u22c5 \u0394\u0303 with 0 < \ud835\udf07 \u226a 1.\nTable\u00a07 presents the values of s+1(K) 1(K) and \u2016\u0394\u20162\u2215\u2016K\u20162 in (57). Although the theoretical upper bound given in Theorem\u00a04.2 may not be sharp in practice, it is seen that the values of s+1(K)\u2215 1(K) and \u2016\u0394\u20162\u2215\u2016K\u20162 are close to each other. Thus, one can use \u2016\u0394\u20162\u2215\u2016K\u20162 as an estimation to s+1(K)\u2215 1(K) in practice.\nIn Fig.\u00a04, we plot the ratio { i(X\u0303)\u2215 1(X\u0303) }min(m,n) i=1\nof the semi-artificial data X\u0303 , and the ratio { i(K)\u2215 1(K) }n i=1\nof the corresponding kernel matrix K for the CMU-PIE database and the Extended YaleB database with = 5 \u00d7 10\u22123 . Here 1, i are the largest and the i-th largest singular values, respectively, and the values in brackets are s + 1 and the ratio\n1 3\nTa bl\ne 3\nD at\nab as\nes u\nse d\nin th\ne ex\npe rim\nen ts\nD at\nas et\ns D\nim en\nsi on\nal ity\n(m\n) \u266f\nof c\nla ss\nes (s\n) \u266f\nof d\nat a\n\u266f of\ntr ai\nni ng\nd at\na (n\n) \u266f\nof te\nst da\nta B\nac kg\nro un\nd\nA R\n12 00\n10 0\n26 00\n19 00\n70 0\nFa ce\nre co\ngn iti on C M U - P I E 10 24 68 11 56 0 80 92 34 68 Fa ce re co gn iti on E x t e n d e d Y a l e B 40 96 38 24 32 17 10 72 2 Fa ce re co gn iti on F a c e s c r u b 92 16 25 6 22 ,6 31 15 ,9 56 66 75 Fa ce re co gn iti on Y o u T u b e F a c e s 10 24 34 0 37 0, 31 9 25 9, 22 3 11 1, 09 6 Fa ce re co gn iti on M N I S T 78 4 10 70 00 0 49 00 5 20 99 5 H an dw rit te n di gi\nts re\nco gn\niti on\nC I F A R - 1 0 0\n30 72\n10 0\n60 ,0\n00 42\n,0 00\n18 ,0\n00 O\nbj ec\nt r ec\nog ni\ntio n\n1 3\ns+1(X\u0303) 1(X\u0303) or s+1(K) 1(K) . It is seen from Fig.\u00a04 that when the clustering effect is good (i.e., is relatively small), both the singular values of X\u0303 and those of the kernel matrix K decay quickly. More precisely, there is a gap between s\n1 and s+1 1 , which validates our theory. Hence, s can\nbe utilized as a numerical rank to K, provided that the clustering effect to X is satisfactory; refer to (45).\nNext, we further explain the rationality of using the number of clusters s as a target rank for randomized algorithms. Notice that the performance of a randomized algorithm strongly relies on the chosen target rank, which is difficult to determine in advance, if there is no information available a prior. Indeed, if the chosen parameter is too large, the computational cost and storage requirement will be high. However, if it is too small, the recognition results such as the value of EER will be unsatisfactory. Thus, the idea is to strike a balance between CPU time and EER. More precisely, we aim to find a reasonable target rank that makes both the computation time and the value of EER be relatively small in some degree.\nRecall that the kernel spectral regression method is composed of two steps, i.e., the eigenanalysis step and the kernel regression step. Given a range for possible target ranks, we first compute the CPU time (used in seconds) and the value of EER obtained from running \u201cAlgorithm\u00a04 + Algorithm\u00a05\" for each scatter point, and plot the TimeEER curve. Here \u201cAlgorithm\u00a04 + Algorithm\u00a05\" means Algorithm\u00a04 for the eigenanalysis step and Algorithm\u00a05 for the kernel regression step. We then use the MATLAB built-in curve fitting toolbox cftool to get the analytic expression of the fitted curve. Here the \u201cideal\" target rank is corresponding to the point which has the shortest distance to the origin in the fitted curve. More precisely, this process can be divided into the following four steps.\n(1) We first set the target ranks to be R = {r1, r2, r3,\u2026 , rt} = {5, 15, 25,\u2026 , 195} , i.e., from 5 to 200 with an interval of 10. Using ri(i = 1, 2,\u2026 , t) as the target rank in \u201cAlgorithm\u00a04 + Algorithm\u00a05\", we can get the CPU time used in seconds C = {c1, c2, c3,\u2026 , ct} and the values of EER, i.e., F = {f1, f2, f3,\u2026 , ft}.\n1 3\n(2) To make the roles of CPU time and EER be of equal importance, we scale the data C and F to get C\u0303 and F\u0303 , such that they are in about the same order. We then exploit the MATLAB built-in curve fitting toolbox cftool to fit the points in C\u0303 = [\u0303c1, c\u03032, c\u03033,\u2026 , c\u0303t] and F\u0303 = [\u0303f1, f\u03032, f\u03033,\u2026 , f\u0303t] , in which the type of fit is chosen as \u201cCustom Equation\". The analytic expression of the fitted curve is y = a \u22c5 e\u2212bx + c, x \u2208 [xmin, xmax] , where a,\u00a0b,\u00a0c are parameters depending on the data source used.\n1 3\nTa bl\ne 7\nE xa\nm pl\ne 2:\nT he\nv al\nue s\nof\ns + 1 (K\n)\n1 (K\n) a\nnd \u2016 \u0394 \u2016 2 \u2016 K \u2016 2\n(i n\nbr ac\nke ts\n) a pp\nea re\nd in\n(5 7)\nfo r d\niff er\nen t v\nal ue\ns\no n\nth e\nse m\ni-a rti\nfic ia\nl d at\na se\nts X\u0303\nb as\ned o\nn C M U - P I E\na nd\nE x t e n d e d\nY a l e B\nd at\nab as\nes\nD at\nas et\n= 5 \u00d7 1 0 \u2212 3\n= 5 \u00d7 1 0 \u2212 4\n= 5 \u00d7 1 0 \u2212 5\n= 5 \u00d7 1 0 \u2212 6\nX\u0303 ( C M U - P I E )\n5 .1 3 \u00d7 1 0 \u2212 3 (3 .9 2 \u00d7 1 0 \u2212 4 )\n4 .5 7 \u00d7 1 0 \u2212 5 (2 .2 2 \u00d7 1 0 \u2212 5 )\n4 .4 9 \u00d7 1 0 \u2212 7 (2 .1 8 \u00d7 1 0 \u2212 6 )\n4 .4 1 \u00d7 1 0 \u2212 9 (2 .1 8 \u00d7 1 0 \u2212 7 )\nX\u0303 ( E x t e n d e d Y a l e B )\n2 .9 3 \u00d7 1 0 \u2212 2 (3 .5 4 \u00d7 1 0 \u2212 3 )\n2 .6 4 \u00d7 1 0 \u2212 4 (1 .2 3 \u00d7 1 0 \u2212 4 )\n2 .5 1 \u00d7 1 0 \u2212 6 (1 .1 9 \u00d7 1 0 \u2212 5 )\n2 .4 4 \u00d7 1 0 \u2212 8 (1 .1 8 \u00d7 1 0 \u2212 6 )\n1 3\n(3) By using the MATLAB built-in optimization function fmincon, we look for a target point in the fitted curve, which corresponds to the shortest distance from the origin to the curve. Here the target point should be associated with both good recognition effect (small EER value) and less CPU time. In light of the analytic expression of the fitted curve, we define the constraint function as\nThen, the MATLAB built-in optimization function fmincon is exploited to solve the problem\nand (c\u0302, f\u0302 ) is the desired target point. (4) Seeking the smallest domain that contains the point (c\u0302, f\u0302 ) . More precisely, we find the\nindex j (1 \u2264 j \u2264 t \u2212 1) such that (c\u0302, f\u0302 ) \u2208 [\ufffdcj,\ufffdcj+1] \u00d7 [\ufffdfj,\ufffdfj+1] . To judge the validity of our theorem, for the index j, we determine the range [rj, rj+1] (1 \u2264 j \u2264 t \u2212 1) in R, and check whether the target rank s defined in Theorem\u00a04.2 is in this interval or not.\nWe run Algorithm\u00a0 4 + Algorithm\u00a0 5 on the CMU-PIE database with r = 3000 , d = 50 , and the Extended YaleB database with r = 1000 , d = 10 . The numerical results are the mean of five runs. In Fig.\u00a05, we depict the curves, where the root mean squared errors are RMSE = 0.0837 and RMSE = 0.0858 for CMU-PIE and Extended YaleB, respectively. Notice that a smaller RMSE value implies a better fit.\nWe observe from Fig.\u00a0 5 that, for the CMU-PIE and the Extended YaleB databases, the desired balance points p1 and p2 corresponding to the shortest distance from the origin to the fitted curve are (c\u0302, f\u0302 ) = (0.63, 0.89) \u2208 [\ufffdc8,\ufffdc9] \u00d7 [\ufffdf8,\ufffdf9] and (c\u0302, f\u0302 ) = (0.76, 1.14) \u2208 [\ufffdc4,\ufffdc5] \u00d7 [\ufffdf4,\ufffdf5] , respectively. Moreover, for the CMU-PIE database with s = 68 and the Extended YaleB database with s = 38 , the ranges for target ranks corresponding to the balance point p1 and p2 are [r8, r9] = [75, 85] and\n(95)h(x, y) = y \u2212 ( a \u22c5 e\u2212bx + c ) , x \u2208 [xmin, xmax] = , y \u2208 [ymin, ymax] = .\n(96) (c\u0302, f\u0302 ) = argmin x \u2208 , y \u2208\nh(x, y) = 0\n\u221a x2 + y2,\n1(X\u0303)\nor s+1(K)\n1(K)\n1 3\n[r4, r5] = [35, 45] , respectively. These demonstrate that Theorem\u00a04.2 is very effective in practice, and our proposed target rank s can be utilized as a numerical rank to the kernel matrix K.\nExample 3 In this experiment, we show the numerical behavior of our modified Nystr\u00f6m method with fixed-rank for low-rank approximation of matrices. We run Algorithm\u00a05 and three popular Nystr\u00f6m methods including the standard Nystr\u00f6m (Anaraki & Becker, 2019; Drineas & Mahoney, 2005; Wang & Zhang, 2013), the modified Nystr\u00f6m (Wang & Zhang, 2013) and the fixed-rank Nystr\u00f6m (Anaraki & Becker, 2019) on the Facescrub database of size 9216 \u00d7 22631 . As the matrix K\u0303K\u0303T can be very large in practical applications, it is desirable to seek approximations with no need to form it explicitly. Indeed, in the Nystr\u00f6mtype methods, one usually selects a subset of the columns of the matrix in question to build an approximation (Sun et\u00a0al., 2015). We denote by S \u2208 \u211dr\u00d7l(l \u226a r) a permutation matrix, i.e., a random matrix which has only one entry is one and the rest are zero in each column, and at most one nonzero element in each row. Then we sample the matrix K\u0303K\u0303T efficiently by computing K\u0303 \u22c5 (K\u0303TS) \u2208 \u211dr\u00d7l , where the permutation matrix S is stored as a vector, and there is no need to form and store the matrix explicitly. As was stressed in Remark 4, we choose the number of clusters s as the target rank. Moreover, we set the range of the target rank used in the four randomized algorithms from s \u2212 25 to s + 25 with an interval of 5, and the oversampling parameter is set to be 10.\nThe numerical performances of four Nystr\u00f6m algorithms on the Facescrub database are depicted in Fig.\u00a06, with the reference vector set cardinality r = 6000 . Denote by H = K\u0303K\u0303T , we define the relative error as \u2016H \u2212 H\u0303\u2016F\u2215\u2016H\u2016F , where H\u0303 is low-rank approximation obtained from different Nystr\u00f6m methods. All the four algorithms are run for 10 times, and the relative error and CPU time are the mean from the 10 runs.\nWe observe from Fig.\u00a06 that the relative errors of our approximation is much better than those from the standard Nystr\u00f6m and the fixed-rank Nystr\u00f6m methods. In addition, it is a little better than the one from the modified Nystr\u00f6m method. On the other hand, we see that our algorithm is comparable in CPU time to the modified Nystr\u00f6m algorithm, but it\n1 3\nis slightly slower than the standard Nystr\u00f6m and the fixed-rank Nystr\u00f6m methods. More precisely, Algorithm\u00a05 is better than the standard Nystr\u00f6m, the modified Nystr\u00f6m and the fixed-rank Nystr\u00f6m methods according to accuracy, and it is comparable to the modified Nystr\u00f6m method in terms of CPU time. Thus, Algorithm\u00a05 is a good choice for overall consideration, and it is a competitive candidate for providing low-rank approximations to large-scale kernel matrices.\nExample 4 In this example, we show the efficiencies of Algorithms 5 and 7 for large-scale face verification problem. Three popular used kernels, the Gaussian kernel, the Laplacian kernel and Polynomial kernel are utilized. The test sets are the face databases YouTube Face of size 1024 \u00d7 370, 319 and Facescrub of size 9216 \u00d7 22, 631 . In this example, we set the reference vector set cardinality r = 6000 and the discriminant space dimensionality d = 50, 100 for the YouTube Face database, and r = 5000 , d = 10, 50 for the Facescrub database. Recall that the kernel spectral regression methods are composed of the eigenanalysis step and the kernel regression step. For the sake of justification, we choose Algorithm\u00a04 for the eigenanalysis step in all the algorithms.\nIn the kernel regression step, we run six algorithms including the two proposed Algorithms 5 and 7, the modified Nystr\u00f6m method (Wang & Zhang, 2013), the fixed-rank Nystr\u00f6m method (Anaraki & Becker, 2019), the original ACS-KSR method (Algorithm\u00a02), as well as the randomized block coordinate descent (RBCD) method due to Needell et\u00a0al., (Needell et\u00a0al., 2015, Algorithm\u00a02). In view of Theorem\u00a04.2 and Remark 4, the target rank and the oversampling parameter are selected to be the number of clusters s and 10, respectively, in the modified Nystr\u00f6m method, the fixed-rank Nystr\u00f6m method and Algorithm\u00a05. Notice that the kernel regression step in the ACS-KSR-based algorithm is solved \u201cexactly\", while it is solved \u201cinexactly\" in the other algorithms. Tables\u00a0 8 and 9 list the numerical results of six algorithms, including the values of EER, AUC, standard deviations, as well as CPU time in seconds. In these tables, the total CPU time includes two parts: the time for generating the reduced kernel matrix K\u0303 (the common time), and that for the eigenanalysis step and the kernel regression step (the computational time).\nSome remarks are in order. First, regardless of kernel function used, we observe from Tables\u00a08 and 9 that for the YouTube Face dataset, the AUC values of all the algorithms are comparable, while the EER values from ACS-KSR, Needell\u2019s RBCD and Algorithm\u00a07\n1 3\nare better than those from Modified Nystr\u00f6m method, Fixed-rank Nystr\u00f6m method and Algorithm\u00a05. For the Facescrub dataset, we see that the AUC and EER values of Algorithm\u00a07 are better than those from the other five algorithms. Moreover, it is seen from the two tables that Algorithm\u00a05 and Modified Nystr\u00f6m method are comparable in view of EER values, and both of them are better than the Fixed-rank Nystr\u00f6m method. It is important to mention that the selection of the target rank in the standard Nystr\u00f6m, the modified Nystr\u00f6m and the fixed-rank Nystr\u00f6m methods, all relies on our proposed strategy, i.e., the number of clusters s is used as the target rank. Without this strategy, the numerical performances of the standard Nystr\u00f6m, the modified Nystr\u00f6m as well as the fixed-rank Nystr\u00f6m methods, may not be so satisfactory.\nSecond, as is shown in Tables\u00a08 and 9, the computation of the reduced kernel matrix is the main overhead for the first four algorithms. The total CPU timings are about the same for the three randomized algorithms, which are much fewer than those for ACS-KSR. Taking the large-scale dataset YouTube Face as an example, we see from Table\u00a08 that Algorithm\u00a05 is about three times faster than the ACS-KSR method. However, when the number of samples is large, explicitly forming or storing the reduced kernel matrix is very costly, and it is interesting to investigate new algorithms that are free of computing and storing the reduced kernel matrix K\u0303 directly.\nThird, we show the efficiency of our randomized block Kaczmarz method for kernel regression problem with multiple right-hand sides. In Algorithm\u00a0 7, the stopping criterion tol and the maximum number of iteration iter are set to be 10\u22122 and 20, respectively. Moreover, we make use of the MATLAB built-in function svd.m to compute the Moore-Penrose inverse appeared in Step 7. As a comparison, we apply the randomized block coordinate descent (RBCD) method (Needell et\u00a0al., 2015, Algorithm\u00a02) proposed by Needell et\u00a0al., to\u00a0solve (82), in which the stopping criterion (83) is also used in this algorithm.\nFor the large-scale dataset YouTube Face, it is obvious to see from Table\u00a08 that Algorithm\u00a0 7 runs much faster than the other algorithms. Indeed, unlike the methods ACS-KSR, modified Nystr\u00f6m, fixed-rank Nystr\u00f6m methods and Algorithm\u00a05, there is no need to explicitly form and store the full reduced kernel matrix K\u0303 in Algorithm\u00a07. With d = 50 and 100, we see that Algorithm\u00a07 is about 60 and 12 times faster than the ACS-KSR-based algorithm, and it is about 13 and 4 times faster than the modified and fixed-rank Nystr\u00f6m methods, and Algorithm\u00a05. Although there is no need to explicitly form and store the full reduced kernel matrix K\u0303 in Needell\u2019s RBCD algorithm, one has to solve the kernel regression problem with multiple right-hand sides one by one. Consequently, the RBCD-based algorithm often runs much slower than Algorithm\u00a07. This demonstrate the efficiency of our randomized block Kaczmarz method.\nFor the relatively small database Facescrub, it is seen from Table\u00a09 that the CPU time of Algorithm\u00a07 is comparable to the three Nystr\u00f6m-like methods and Algorithm\u00a05. This is because Algorithm\u00a07 is more suitable to solve large-scale systems with tall (i.e., the number of rows is much larger than the columns) coefficient matrices (Zouzias & Freris, 2013). On the other hand, the AUC and EER values obtained from Algorithm\u00a07 are better than those from the Nystr\u00f6m-like methods and Algorithm\u00a05. Considering the AUC, EER values and CPU time as a whole, Algorithm\u00a07 is a competitive algorithm among all the algorithms.\n1 3\nTa bl\ne 8\nE xa\nm pl\ne 4:\nN um\ner ic\nal r\nes ul\nts o\nf th\ne al\ngo rit\nhm s\nus in\ng th\ne G\nau ss\nia n\nke rn\nel , L\nap la\nci an\nk er\nne l a\nnd P\nol yn\nom ia\nl k er\nne l o\nn th\ne fa\nce d\nat ab\nas e Y o u T u b e F a c e\nw ith\nd = 5 0 , 1 0 0 a\nnd r = 6 0 0 0 , w\nhe re\nth e\nei ge\nna na\nly si\ns s te\np is\nso lv\ned b\ny us\nin g\nA lg\nor ith\nm \u00a04\nin a\nll th\ne al\ngo rit\nhm s\nA lg\nor ith\nm s\nFa ce\nd at\nab as\ne: Y o u T u b e F a c e\n(r =\n6 00\n0, d\n= 5\n0) Fa\nce d\nat ab\nas e:\nY o u T u b e F a c e\n(r =\n6 00\n0, d\n= 1\n00 )\n(M et\nho ds\n) EE\nR \u00b1\nS td\n-D ev\n% A\nU C\n\u00b1 S\ntd -D\nev C\nPU ti\nm e\n(s )\nEE R\n\u00b1 S\ntd -D\nev %\nA U\nC \u00b1\nS td\n-D ev\nC PU\nti m\ne (s\n)\nG au\nss ia\nn K\ner ne\nl G\nau ss\nia n\nK er\nne l\nA C\nSK\nSR 0.\n16 \u00b1\n0 .0\n21 %\n0. 99\n\u00b1 0\n.0 00\n16 18\n5. 18\n(2 8.\n26 +\n1 56\n.9 2)\n0. 17\n\u00b1 0\n.0 22\n% 0.\n99 \u00b1\n0 .0\n00 16\n14 9.\n14 (2\n8. 93\n+ 1\n20 .2 1) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 4. 66 \u00b1 0 .0 63 % 0. 97 \u00b1 0 .0 00 87 40 .8 0 (2 8. 26 + 1 2. 54 ) 4. 56 \u00b1 0 .1 6% 0. 98 \u00b1 0 .0 01 6 50 .2 6( 28 .9 3 + 2 1. 33 ) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 5. 37 \u00b1 0 .2 6% 0. 97 \u00b1 0 .0 02 2 36 .5 4 (2 8. 26 + 8 .2 8) 5. 33 \u00b1 0 .0 51 % 0. 97 \u00b1 0 .0 00 71 46 .9 7( 28 .9 3 + 1 8. 04 ) A lg or ith m \u00a05 4. 59 \u00b1 0 .0 92 % 0. 98 \u00b1 0 .0 00 52 40 .3 4 (2 8. 26 + 1 2. 08 ) 4. 53 \u00b1 0 .0 35 % 0. 98 \u00b1 0 .0 00 74 50 .8 6( 28 .9 3 + 2 1. 93 ) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 0. 71 \u00b1 0 .3 8% 0. 99 \u00b1 0 .0 00 21 72 .7 1 (0 + 7 2. 71 ) 0. 79 \u00b1 0 .3 7% 0. 99 \u00b1 0 .0 00 25 15 8. 11 (0 + 1 58 .1 1) A lg or ith m \u00a07 1. 51 \u00b1 0 .7 9% 0. 99 \u00b1 0 .0 01 0 2. 89 (0 + 2 .8 9) 0. 68 \u00b1 0 .3 8% 0. 99 \u00b1 0 .0 00 42 11 .7 9( 0 + 1 1. 79 ) La pl ac ia n K er ne l La pl ac ia n K er ne l A C SK SR 1. 48 \u00b1 0 .0 49 % 0. 98 \u00b1 0 .0 00 53 18 6. 71 (2 8. 52 + 1 58 .1 9) 1. 30 \u00b1 0 .0 65 % 0. 98 \u00b1 0 .0 00 67 14 4. 44 (2 9. 89 + 1 14 .5 5) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 6. 50 \u00b1 0 .1 7% 0. 96 \u00b1 0 .0 01 8 40 .1 0 (2 8. 52 + 1 1. 58 ) 6. 47 \u00b1 0 .2 0% 0. 96 \u00b1 0 .0 01 6 51 .3 2( 29 .8 9 + 2 1. 43 ) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 8. 48 \u00b1 0 .2 9% 0. 95 \u00b1 0 .0 03 0 35 .6 5 (2 8. 52 + 7 .1 3) 8. 37 \u00b1 0 .2 3% 0. 95 \u00b1 0 .0 02 0 47 .7 3( 29 .8 9 + 1 7. 84 ) A lg or ith m \u00a05 6. 70 \u00b1 0 .1 3% 0. 96 \u00b1 0 .0 01 4 39 .3 5 (2 8. 52 + 1 0. 83 ) 6. 67 \u00b1 0 .1 5% 0. 96 \u00b1 0 .0 00 82 51 .6 5( 29 .8 9 + 2 1. 76 ) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 0. 52 \u00b1 0 .3 1% 0. 99 \u00b1 0 .0 03 2 83 .1 1 (0 + 8 3. 11 ) 0. 84 \u00b1 0 .1 4% 0. 99 \u00b1 0 .0 00 20 16 2. 60 (0 + 1 62 .6 0) A lg or ith m \u00a07 1. 52 \u00b1 1 .8 % 0. 99 \u00b1 0 .0 02 7 2. 95 (0 + 2 .9 5) 0. 92 \u00b1 0 .6 0% 0. 99 \u00b1 0 .0 00 57 12 .1 5( 0 + 1 2. 15 ) Po ly no m ia l K er ne l Po ly no m ia l K er ne l A C SK SR 0. 31 \u00b1 0 .0 24 % 0. 99 \u00b1 0 .0 00 29 12 3. 82 (1 7. 59 + 1 06 .2 3) 0. 30 \u00b1 0 .0 17 % 0. 99 \u00b1 0 .0 00 15 13 4. 48 (1 7. 86 + 1 16 .6 2) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 4. 52 \u00b1 0 .0 66 % 0. 98 \u00b1 0 .0 00 37 29 .6 7 (1 7. 59 + 1 2. 08 ) 4. 60 \u00b1 0 .1 1% 0. 98 \u00b1 0 .0 00 83 39 .4 1( 17 .8 6 + 2 1. 55 ) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 5. 00 \u00b1 0 .0 75 % 0. 97 \u00b1 0 .0 00 89 25 .8 5 (1 7. 59 + 8 .2 6) 5. 05 \u00b1 0 .0 53 % 0. 97 \u00b1 0 .0 00 26 35 .5 4( 17 .8 6 + 1 7. 68 ) A lg or ith m \u00a05 4. 52 \u00b1 0 .0 95 % 0. 98 \u00b1 0 .0 01 0 29 .4 8 (1 7. 59 + 1 1. 89 ) 4. 53 \u00b1 0 .0 81 % 0. 98 \u00b1 0 .0 00 66 39 .6 7( 17 .8 6 + 2 1. 81 ) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 1. 25 \u00b1 0 .2 6% 0. 99 \u00b1 0 .0 00 18 46 .0 0 (0 + 4 6. 00 ) 0. 97 \u00b1 0 .3 4% 0. 99 \u00b1 0 .0 00 32 89 .1 5( 0 + 8 9. 15 )\n1 3\nTh e\nC PU\nti m\ne Z (Z\n1 + Z 2 ) m\nea ns\nth e\nto ta\nl C PU\nti m\ne Z\nis c\nom po\nse d\nof Z\n1 ( th\ne co\nm m\non ti\nm e)\nfo r g\nen er\nat in\ng th\ne re\ndu ce\nd ke\nrn el\nm at\nrix K\u0303\n, a nd\nZ 2 (t\nhe c\nom pu\nta tio\nn tim\ne) fo r th e ei ge na na ly si s s te p an d th e ke rn el re gr es si on st ep Ta bl e 8 (c on tin ue d) A lg or ith m s Fa ce d at ab as e: Y o u T u b e F a c e (r = 6 00 0, d = 5 0) Fa ce d at ab as e: Y o u T u b e F a c e (r = 6 00 0, d = 1 00 ) (M et ho ds ) EE R \u00b1 S td -D ev % A U C \u00b1 S td -D ev C PU ti m e (s ) EE R \u00b1 S td -D ev % A U C \u00b1 S td -D ev C PU ti m e (s ) A lg or ith m \u00a07 1. 55 \u00b1 0 .5 7% 0. 99 \u00b1 0 .0 00 46 2. 41 (0 + 2 .4 1) 1. 43 \u00b1 0 .5 9% 0. 99 \u00b1 0 .0 00 65 12 .2 6( 0 + 1 2. 26 )\n1 3\nTa bl\ne 9\nE xa\nm pl\ne 4:\nN um\ner ic\nal r\nes ul\nts o\nf th\ne al\ngo rit\nhm s\nus in\ng th\ne G\nau ss\nia n\nke rn\nel ,\nLa pl\nac ia\nn ke\nrn el\na nd\nP ol\nyn om\nia l\nke rn\nel o\nn th\ne fa\nce d\nat ab\nas e F a c e s c r u b\nw ith\nd = 1 0 , 5 0 a\nnd r = 5 0 0 0 , w\nhe re\nth e\nei ge\nna na\nly si\ns s te\np is\nso lv\ned b\ny us\nin g\nA lg\nor ith\nm \u00a04\nin a\nll th\ne al\ngo rit\nhm s\nTh e\nC PU\nti m\ne Z (Z\n1 + Z 2 ) m\nea ns\nth e\nto ta\nl C PU\nti m\ne Z\nis c\nom po\nse d\nof Z\n1 ( th\ne co\nm m\non ti\nm e)\nfo r g\nen er\nat in\ng th\ne re\ndu ce\nd ke\nrn el\nm at\nrix K\u0303\n, a nd\nZ 2 (t\nhe c\nom pu\nta tio\nn tim\ne) fo r th e ei ge na na ly si s s te p an d th e ke rn el re gr es si on st ep A lg or ith m s Fa ce d at ab as e: F a c e s c r u b (r = 5 00 0, d = 1 0) Fa ce d at ab as e: F a c e s c r u b (r = 5 00 0, d = 5 0) (M et ho ds ) EE R \u00b1 S td -D ev % A U C \u00b1 S td -D ev C PU ti m e (s ) EE R \u00b1 S td -D ev % A U C \u00b1 S td -D ev C PU ti m e (s ) G au ss ia n K er ne l G au ss ia n K er ne l A C SK SR 19 .2 5 \u00b1 0 .2 9% 0. 88 \u00b1 0 .0 01 8 12 .9 5 (7 .0 6 + 5 .8 9) 19 .5 7 \u00b1 0 .2 4% 0. 87 \u00b1 0 .0 01 4 12 .6 8( 6. 99 + 5 .6 9) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 22 .7 6 \u00b1 0 .2 0% 0. 84 \u00b1 0 .0 01 2 7. 72 (7 .0 6 + 0 .6 6) 26 .5 3 \u00b1 0 .1 4% 0. 80 \u00b1 0 .0 02 2 7. 69 (6 .9 9 + 0 .7 0) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 25 .0 1 \u00b1 0 .3 0% 0. 82 \u00b1 0 .0 03 2 7. 49 (7 .0 6 + 0 .4 3) 29 .9 1 \u00b1 0 .3 7% 0. 76 \u00b1 0 .0 03 5 7. 46 (6 .9 9 + 0 .4 7) A lg or ith m \u00a05 22 .6 1 \u00b1 0 .2 7% 0. 85 \u00b1 0 .0 01 7 7. 67 (7 .0 6 + 0 .6 1) 26 .3 9 \u00b1 0 .0 85 % 0. 80 \u00b1 0 .0 01 2 7. 64 (6 .9 9 + 0 .6 5) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 14 .2 5 \u00b1 5 .5 7% 0. 91 \u00b1 0 .0 52 91 .6 7 (0 + 9 1. 67 ) 30 .1 9 \u00b1 9 .5 1% 0. 77 \u00b1 0 .0 97 5 43 1. 82 (0 + 4 31 .8 2) A lg or ith m \u00a07 13 .9 7 \u00b1 5 .4 5% 0. 92 \u00b1 0 .0 45 10 .6 0 (0 + 1 0. 60 ) 16 .4 7 \u00b1 8 .6 6% 0. 89 \u00b1 0 .0 75 4 10 .3 0( 0 + 1 0. 30 ) La pl ac ia n K er ne l La pl ac ia n K er ne l A C SK SR 26 .5 4 \u00b1 0 .1 3% 0. 80 \u00b1 0 .0 01 6 12 .9 9 (7 .1 1 + 5 .8 8) 17 .9 2 \u00b1 0 .2 5% 0. 89 \u00b1 0 .0 00 72 12 .4 7( 7. 15 + 5 .3 2) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 25 .1 8 \u00b1 0 .5 2% 0. 82 \u00b1 0 .0 06 2 7. 78 (7 .1 1 + 0 .6 7) 24 .4 4 \u00b1 0 .2 0% 0. 82 \u00b1 0 .0 02 4 7. 85 (7 .1 5 + 0 .7 0) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 28 .6 5 \u00b1 0 .4 4% 0. 77 \u00b1 0 .0 05 6 7. 54 (7 .1 1 + 0 .4 3) 27 .5 0 \u00b1 0 .2 8% 0. 79 \u00b1 0 .0 02 7 7. 63 (7 .1 5 + 0 .4 8) A lg or ith m \u00a05 24 .8 4 \u00b1 0 .3 7% 0. 82 \u00b1 0 .0 03 1 7. 73 (7 .1 1 + 0 .6 2) 24 .3 7 \u00b1 0 .4 6% 0. 82 \u00b1 0 .0 02 8 7. 80 (7 .1 5 + 0 .6 5) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 15 .7 1 \u00b1 5 .9 0% 0. 90 \u00b1 0 .0 53 91 .7 7 (0 + 9 1. 77 ) 30 .1 6 \u00b1 1 1. 31 % 0. 77 \u00b1 0 .1 8 43 3. 76 (0 + 4 33 .7 6) A lg or ith m \u00a07 15 .2 5 \u00b1 5 .6 6% 0. 94 \u00b1 0 .0 34 10 .6 8 (0 + 1 0. 68 ) 16 .3 6 \u00b1 5 .9 2% 0. 92 \u00b1 0 .0 56 10 .2 7( 0 + 1 0. 27 ) Po ly no m ia l K er ne l Po ly no m ia l K er ne l A C SK SR 22 .9 6 \u00b1 0 .3 2% 0. 83 \u00b1 0 .0 01 7 11 .5 2 (5 .9 9 + 5 .5 3) 27 .9 3 \u00b1 0 .3 8% 0. 78 \u00b1 0 .0 01 7 11 .8 1( 6. 22 + 5 .5 9) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 21 .9 0 \u00b1 0 .2 4% 0. 85 \u00b1 0 .0 01 4 6. 64 (5 .9 9 + 0 .6 5) 31 .2 1 \u00b1 0 .2 7% 0. 74 \u00b1 0 .0 02 8 6. 91 (6 .2 2 + 0 .6 9) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r ( 20 19 ) 23 .8 6 \u00b1 0 .1 6% 0. 82 \u00b1 0 .0 02 1 6. 41 (5 .9 9 + 0 .4 2) 33 .9 5 \u00b1 0 .3 5% 0. 71 \u00b1 0 .0 02 1 6. 68 (6 .2 2 + 0 .4 6) A lg or ith m \u00a05 22 .0 0 \u00b1 0 .1 4% 0. 85 \u00b1 0 .0 00 33 6. 59 (5 .9 9 + 0 .6 0) 30 .9 9 \u00b1 0 .2 7% 0. 74 \u00b1 0 .0 01 6 6. 87 (6 .2 2 + 0 .6 5) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 17 .7 7 \u00b1 4 .3 7% 0. 91 \u00b1 0 .0 38 27 .7 5 (0 + 2 7. 75 ) 25 .5 4 \u00b1 9 .3 7% 0. 80 \u00b1 0 .0 99 14 6. 30 (0 + 1 46 .3 0) A lg or ith m \u00a07 12 .1 2 \u00b1 4 .5 4% 0. 95 \u00b1 0 .0 16 4. 34 (0 + 4 .3 4) 18 .8 0 \u00b1 3 .9 8% 0. 89 \u00b1 0 .0 32 4. 77 (0 + 4 .7 7)\n1 3\nExample 5 In this example, we show that our strategies proposed in Algorithm\u00a0 5 and Algorithm\u00a0 7 also apply to other types of data sets, such as the handwritten digits database and the tiny images database. The Gaussian kernel, the Laplacian kernel and Polynomial kernel are utilized in this example. The tiny images database CIFAR-100 is of size 3072 \u00d7 60, 000 and the handwritten digits database MNIST is of size 784 \u00d7 70, 000 . In this experiment, we set the reference vector set cardinality r = 6000 and the discriminant space dimensionality d = 50, 100 for the CIFAR-100 and the MNIST databases. Similar to Example 4, we apply Algorithm\u00a04 for the eigenanalysis step in all the algorithms. In the kernel regression step, we also run the proposed Algorithm\u00a05 and Algorithm\u00a07, the modified Nystr\u00f6m method (Wang & Zhang, 2013), the fixed-rank Nystr\u00f6m method (Anaraki & Becker, 2019), the original ACS-KSR method (Algorithm\u00a02), as well as the randomized block coordinate descent (RBCD) method due to (Needell et\u00a0al., 2015, Algorithm\u00a02). The target rank and the oversampling parameter are selected to be the number of clusters s and 10, respectively.\nTables\u00a010 and 11 list EER, AUC, standard deviations, as well as CPU time in seconds of the six algorithms. For the handwritten digits database MNIST, we see from Table\u00a010 that the EER values obtained from Algorithm\u00a07 are often smaller than the three Nystr\u00f6m-type methods, and it runs much faster than the other algorithms in many cases. For the CIFAR100 database in Table\u00a011, it is seen that the EER and AUC values obtained from the six algorithms are with ups and downs, and we cannot tell which one is the best. Indeed, the results are closely related to the data sets and the selection of the kernel functions. Moreover, we observe from Table\u00a011 that the EER and AUC values from the first four algorithms are comparable. This illustrates the rationality of using s as the target rank in the randomized algorithms for solving the kernel regression problem.\nAgain, we observe from Tables\u00a010 and 11 that the total CPU timings of the modified Nystr\u00f6m method (Wang & Zhang, 2013), the fixed-rank Nystr\u00f6m method (Anaraki & Becker, 2019) and Algorithm\u00a05 are about the same, and they are much faster than the ACSKSR method. As a comparison, the randomized block coordinate descent (RBCD) method is the slowest one, while the proposed Algorithm\u00a0 7 performs the best in terms of CPU time. More precisely, Algorithm\u00a05 and Algorithm\u00a07 are about two to three times faster than the ACS-KSR method, respectively. Compared with the RBCD-based algorithm which does not require explicitly form and store the reduced kernel matrix K\u0303 , either, Algorithm\u00a07 is nearly 10 times faster. All these illustrate the superiority of Algorithm\u00a05 and Algorithm\u00a07 for high-dimensional and large-sample kernel regression problems."
        },
        {
            "heading": "7 Concluding remarks",
            "text": "Face verification is a crucial problem in many applications such as human-computer interaction and human behaviour analysis for assisted living. The approximate class-specific kernel spectral regression (ACS-KSR) method is an improvement to the class-specific kernel discriminant analysis (CS-KDA) and the class-specific kernel spectral regression (CSKSR) methods, and it is an effective method for face verification problem. However, when the scale of data sets is very large, ACS-KSR may suffer from heavily computational overhead or even be infeasible in practice.\n1 3\nTa bl\ne 10\nEx\nam pl\ne 5:\nN um\ner ic\nal re\nsu lts\no f t\nhe a\nlg or\nith m\ns us\nin g\nth e\nG au\nss ia\nn ke\nrn el\n, L ap\nla ci\nan k\ner ne\nl a nd\nP ol\nyn om\nia l k\ner ne\nl o n\nth e\nha nd\nw rit\nte n\ndi gi\nts d\nat ab\nas e M N I S T\nw ith\nd = 5 0 , 1 0 0 a\nnd r = 6 0 0 0 , w\nhe re\nth e\nei ge\nna na\nly si\ns s te\np is\nso lv\ned b\ny us\nin g\nA lg\nor ith\nm \u00a04\nin a\nll th\ne al\ngo rit\nhm s\nA lg\nor ith\nm s\nH an\ndw rit\nte n\ndi gi\nts d\nat ab\nas e:\nM N I S T\n(r =\n6 00\n0, d\n= 5\n0) H\nan dw\nrit te\nn di\ngi ts\nd at\nab as\ne: M N I S T\n(r =\n6 00\n0, d\n= 1\n00 )\n(M et\nho ds\n) EE\nR \u00b1\nS td\n-D ev\n% A\nU C\n\u00b1 S\ntd -D\nev C\nPU ti\nm e\n(s )\nEE R\n\u00b1 S\ntd -D\nev %\nA U\nC \u00b1\nS td\n-D ev\nC PU\nti m\ne (s\n)\nG au\nss ia\nn K\ner ne\nl G\nau ss\nia n\nK er\nne l\nA C\nSK\nSR 2.\n63 \u00b1\n0 .0\n99 %\n0. 99\n\u00b1 0\n.0 00\n33 26\n.4 2(\n4. 53\n+ 2\n1. 89\n) 6.\n77 \u00b1\n0 .1\n0% 0.\n97 \u00b1\n0 .0\n00 88\n26 .3\n4 (4\n.6 0\n+ 2\n1. 74 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 16 .3 9 \u00b1 0 .8 0% 0. 89 \u00b1 0 .0 07 8 5. 18 (4 .5 3 + 0 .6 5) 15 .9 0 \u00b1 0 .5 6% 0. 90 \u00b1 0 .0 05 7 6. 81 (4 .6 0 + 2 .2 1) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 16 .1 3 \u00b1 0 .2 0% 0. 90 \u00b1 0 .0 01 7 5. 06 (4 .5 3 + 0 .5 3) 15 .9 1 \u00b1 0 .0 90 % 0. 90 \u00b1 0 .0 01 4 6. 63 (4 .6 0 + 2 .0 3) A lg or ith m \u00a05 15 .7 7 \u00b1 0 .1 3% 0. 90 \u00b1 0 .0 01 0 5. 19 (4 .5 3 + 0 .6 6) 15 .6 0 \u00b1 0 .1 2% 0. 90 \u00b1 0 .0 00 90 6. 79 (4 .6 0 + 2 .1 9) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 7. 86 \u00b1 0 .5 5% 0. 96 \u00b1 0 .0 04 8 46 .4 2( 0 + 4 6. 42 ) 8. 69 \u00b1 0 .7 4% 0. 97 \u00b1 0 .0 05 3 92 .6 0 (0 + 9 2. 60 ) A lg or ith m \u00a07 7. 85 \u00b1 0 .7 4% 0. 97 \u00b1 0 .0 04 9 5. 12 (0 + 5 .1 2) 8. 42 \u00b1 0 .4 7% 0. 97 \u00b1 0 .0 02 8 5. 66 (0 + 5 .6 6) La pl ac ia n K er ne l La pl ac ia n K er ne l A C SK SR 1. 21 \u00b1 0 .0 68 % 0. 99 \u00b1 0 .0 00 71 26 .2 0( 4. 96 + 2 1. 24 ) 1. 19 \u00b1 0 .0 00 45 % 0. 99 \u00b1 0 .0 00 17 26 .7 3 (4 .9 5 + 2 1. 78 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 16 .9 4 \u00b1 0 .3 9% 0. 89 \u00b1 0 .0 04 9 5. 62 (4 .9 6 + 0 .6 6) 16 .6 9 \u00b1 0 .3 0% 0. 89 \u00b1 0 .0 01 8 7. 20 (4 .9 5 + 2 .2 5) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 16 .6 4 \u00b1 0 .5 5% 0. 89 \u00b1 0 .0 05 0 5. 48 (4 .9 6 + 0 .5 2) 16 .5 6 \u00b1 0 .5 5% 0. 90 \u00b1 0 .0 04 5 7. 02 (4 .9 5 + 2 .0 7) A lg or ith m \u00a05 16 .0 7 \u00b1 0 .2 0% 0. 90 \u00b1 0 .0 01 6 5. 65 (4 .9 6 + 0 .6 9) 15 .8 9 \u00b1 0 .1 3% 0. 91 \u00b1 0 .0 01 0 7. 17 (4 .9 5 + 2 .2 2) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 5. 61 \u00b1 0 .6 8% 0. 98 \u00b1 0 .0 03 5 55 .9 8( 0 + 5 5. 98 ) 6. 32 \u00b1 0 .8 8% 0. 98 \u00b1 0 .0 06 0 11 8. 05 (0 + 1 18 .0 5) A lg or ith m \u00a07 5. 37 \u00b1 0 .6 4% 0. 98 \u00b1 0 .0 03 2 6. 55 (0 + 6 .5 5) 6. 67 \u00b1 0 .5 1% 0. 98 \u00b1 0 .0 04 1 6. 69 (0 + 6 .6 9) Po ly no m ia l K er ne l Po ly no m ia l K er ne l A C SK SR 5. 56 \u00b1 0 .1 1% 0. 97 \u00b1 0 .0 00 55 24 .6 6( 2. 80 + 2 1. 86 ) 12 .9 6 \u00b1 0 .1 9% 0. 93 \u00b1 0 .0 00 32 24 .2 9 (2 .7 5 + 2 1. 54 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 17 .2 4 \u00b1 0 .9 7% 0. 89 \u00b1 0 .0 08 8 3. 44 (2 .8 0 + 0 .6 4) 16 .2 6 \u00b1 0 .2 4% 0. 90 \u00b1 0 .0 02 7 4. 89 (2 .7 5 + 2 .1 4) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 16 .5 6 \u00b1 0 .3 9% 0. 90 \u00b1 0 .0 03 8 3. 32 (2 .8 0 + 0 .5 2) 16 .4 7 \u00b1 0 .4 3% 0. 90 \u00b1 0 .0 03 8 4. 77 (2 .7 5 + 2 .0 2) A lg or ith m \u00a05 16 .0 0 \u00b1 0 .1 7% 0. 90 \u00b1 0 .0 01 3 3. 46 (2 .8 0 + 0 .6 6) 15 .9 8 \u00b1 0 .1 8% 0. 91 \u00b1 0 .0 01 1 4. 90 (2 .7 5 + 2 .1 5) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 4. 95 \u00b1 0 .3 2% 0. 98 \u00b1 0 .0 01 5 7. 89 (0 + 7 .8 9) 5. 56 \u00b1 1 .2 1% 0. 98 \u00b1 0 .0 04 7 15 .3 5 (0 + 1 5. 35 )\n1 3\nTa bl\ne 10\n( co\nnt in\nue d)\nA lg\nor ith\nm s\nH an\ndw rit\nte n\ndi gi\nts d\nat ab\nas e:\nM N I S T\n(r =\n6 00\n0, d\n= 5\n0) H\nan dw\nrit te\nn di\ngi ts\nd at\nab as\ne: M N I S T\n(r =\n6 00\n0, d\n= 1\n00 )\n(M et\nho ds\n) EE\nR \u00b1\nS td\n-D ev\n% A\nU C\n\u00b1 S\ntd -D\nev C\nPU ti\nm e\n(s )\nEE R\n\u00b1 S\ntd -D\nev %\nA U\nC \u00b1\nS td\n-D ev\nC PU\nti m\ne (s\n)\nA lg\nor ith\nm \u00a07\n10 .9\n2 \u00b1\n1 .6\n1% 0.\n95 \u00b1\n0 .0\n12 0.\n76 (0\n+ 0\n.7 6)\n9. 94\n\u00b1 1\n.1 8%\n0. 96\n\u00b1 0\n.0 09\n7 2.\n02 (0\n+ 2\n.0 2)\nTh e\nC PU\nti m\ne Z (Z\n1 + Z 2 ) m\nea ns\nth e\nto ta\nl C PU\nti m\ne Z\nis c\nom po\nse d\nof Z\n1 ( th\ne co\nm m\non ti\nm e)\nfo r g\nen er\nat in\ng th\ne re\ndu ce\nd ke\nrn el\nm at\nrix K\u0303\n, a nd\nZ 2 (t\nhe c\nom pu\nta tio\nn tim\ne) fo r th e ei ge na na ly si s s te p an d th e ke rn el re gr es si on st ep\n1 3\nTa bl\ne 11\nEx\nam pl\ne 5:\nN um\ner ic\nal re\nsu lts\no f t\nhe a\nlg or\nith m\ns us\nin g\nth e\nG au\nss ia\nn ke\nrn el\n, L ap\nla ci\nan k\ner ne\nl a nd\nP ol\nyn om\nia l k\ner ne\nl o n\nth e\ntin y\nim ag\nes d\nat as\net C I F A R - 1 0 0\nw ith\nd = 5 0 , 1 0 0 a\nnd r = 6 0 0 0 , w\nhe re\nth e\nei ge\nna na\nly si\ns s te\np is\nso lv\ned b\ny us\nin g\nA lg\nor ith\nm \u00a04\nin a\nll th\ne al\ngo rit\nhm s\nA lg\nor ith\nm s\nTi ny\nim ag\nes d\nat as\net : C I F A R - 1 0 0\n(r =\n6 00\n0, d\n= 5\n0) Ti\nny im\nag es\nd at\nas et\n: C I F A R - 1 0 0\n(r =\n6 00\n0, d\n= 1\n00 )\n(M et\nho ds\n) EE\nR \u00b1\nS td\n-D ev\n% A\nU C\n\u00b1 S\ntd -D\nev C\nPU ti\nm e\n(s )\nEE R\n\u00b1 S\ntd -D\nev %\nA U\nC \u00b1\nS td\n-D ev\nC PU\nti m\ne (s\n)\nG au\nss ia\nn K\ner ne\nl G\nau ss\nia n\nK er\nne l\nA C\nSK\nSR 32\n.1 0\n\u00b1 0\n.1 9%\n0. 73\n\u00b1 0\n.0 02\n0 26\n.9 4\n(8 .6\n4 +\n1 8.\n30 )\n36 .8\n5 \u00b1\n0 .0\n56 %\n0. 67\n\u00b1 0\n.0 00\n56 26\n.5 3(\n8. 20\n+ 1\n8. 33 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 33 .8 4 \u00b1 0 .2 4% 0. 70 \u00b1 0 .0 02 0 9. 68 (8 .6 4 + 1 .0 4) 35 .1 2 \u00b1 0 .2 2% 0. 68 \u00b1 0 .0 02 0 10 .4 7( 8. 20 + 2 .2 7) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 36 .5 7 \u00b1 0 .2 2% 0. 67 \u00b1 0 .0 03 2 9. 37 (8 .6 4 + 0 .7 3) 38 .4 3 \u00b1 0 .2 3% 0. 64 \u00b1 0 .0 02 7 10 .1 6( 8. 20 + 1 .9 6) A lg or ith m \u00a05 33 .8 0 \u00b1 0 .2 1% 0. 70 \u00b1 0 .0 02 0 9. 68 (8 .6 4 + 1 .0 4) 35 .1 6 \u00b1 0 .3 0% 0. 68 \u00b1 0 .0 02 7 10 .4 7( 8. 20 + 2 .2 7) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l. (2 01 5) 42 .2 4 \u00b1 3 .2 6% 0. 61 \u00b1 0 .0 39 79 .5 3 (0 + 7 9. 53 ) 44 .2 4 \u00b1 4 .2 9% 0. 57 \u00b1 0 .0 51 14 6. 98 (0 + 1 46 .9 8) A lg or ith m \u00a07 39 .9 9 \u00b1 1 .3 3% 0. 63 \u00b1 0 .0 11 4. 30 (0 + 4 .3 0) 41 .3 4 \u00b1 1 .7 0% 0. 61 \u00b1 0 .0 15 5. 29 (0 + 5 .2 9) La pl ac ia n K er ne l La pl ac ia n K er ne l A C SK SR 22 .6 7 \u00b1 0 .1 7% 0. 84 \u00b1 0 .0 01 8 27 .3 4 (9 .0 9 + 1 8. 25 ) 23 .1 8 \u00b1 0 .1 9% 0. 83 \u00b1 0 .0 01 8 28 .5 5( 8. 78 + 1 9. 77 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 32 .2 9 \u00b1 0 .2 2% 0. 72 \u00b1 0 .0 01 5 10 .1 3 (9 .0 9 + 1 .0 4) 32 .9 3 \u00b1 0 .2 8% 0. 71 \u00b1 0 .0 01 9 11 .0 0( 8. 78 + 2 .2 2) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 37 .0 4 \u00b1 0 .3 4% 0. 67 \u00b1 0 .0 03 5 9. 83 (9 .0 9 + 0 .7 4) 38 .1 9 \u00b1 0 .2 5% 0. 65 \u00b1 0 .0 02 4 10 .7 6( 8. 78 + 1 .9 8) A lg or ith m \u00a05 32 .2 6 \u00b1 0 .1 9% 0. 72 \u00b1 0 .0 01 6 10 .1 0 (9 .0 9 + 1 .0 1) 32 .9 3 \u00b1 0 .1 3% 0. 71 \u00b1 0 .0 01 5 11 .0 4( 8. 78 + 2 .2 6) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 43 .4 3 \u00b1 3 .2 3% 0. 61 \u00b1 0 .0 34 41 4. 43 (0 + 4 14 .4 3) 46 .3 2 \u00b1 1 .1 9% 0. 56 \u00b1 0 .0 17 80 0. 49 (0 + 8 00 .4 9) A lg or ith m \u00a07 36 .4 7 \u00b1 1 .2 6% 0. 68 \u00b1 0 .0 17 11 .8 0 (0 + 1 1. 80 ) 39 .0 9 \u00b1 1 .8 4% 0. 66 \u00b1 0 .0 19 12 .7 0( 0 + 1 2. 70 ) Po ly no m ia l K er ne l Po ly no m ia l K er ne l A C SK SR 42 .1 3 \u00b1 0 .1 4% 0. 61 \u00b1 0 .0 01 7 24 .2 3 (6 .6 8 + 1 7. 55 ) 45 .5 7 \u00b1 0 .1 2% 0. 56 \u00b1 0 .0 00 85 25 .0 8( 6. 52 + 1 8. 56 ) M od ifi ed N ys tr\u00f6 m m et ho d W an g an d Zh an g (2 01 3) 38 .1 4 \u00b1 0 .1 6% 0. 65 \u00b1 0 .0 02 2 7. 70 (6 .6 8 + 1 .0 2) 40 .2 9 \u00b1 0 .1 2% 0. 62 \u00b1 0 .0 01 3 8. 76 (6 .5 2 + 2 .2 4) Fi xe dra nk N ys tr\u00f6 m m et ho d A na ra ki a nd B ec ke r (2 01 9) 39 .8 7 \u00b1 0 .1 9% 0. 62 \u00b1 0 .0 01 8 7. 42 (6 .6 8 + 0 .7 4) 42 .1 4 \u00b1 0 .2 1% 0. 60 \u00b1 0 .0 03 3 8. 50 (6 .5 2 + 1 .9 8) A lg or ith m \u00a05 38 .0 7 \u00b1 0 .0 75 % 0. 65 \u00b1 0 .0 01 2 7. 70 (6 .6 8 + 1 .0 2) 40 .3 0 \u00b1 0 .1 3% 0. 62 \u00b1 0 .0 01 6 8. 78 (6 .5 2 + 2 .2 6) N ee de ll\u2019 s R B C D N ee de ll et \u00a0a l., (2 01 5) 44 .0 7 \u00b1 2 .4 4% 0. 58 \u00b1 0 .0 26 10 8. 81 (0 + 1 08 .8 1) 45 .9 6 \u00b1 0 .6 5% 0. 56 \u00b1 0 .0 15 21 3. 31 (0 + 2 13 .3 1)\n1 3\nTa bl\ne 11\n( co\nnt in\nue d)\nA lg\nor ith\nm s\nTi ny\nim ag\nes d\nat as\net : C I F A R - 1 0 0\n(r =\n6 00\n0, d\n= 5\n0) Ti\nny im\nag es\nd at\nas et\n: C I F A R - 1 0 0\n(r =\n6 00\n0, d\n= 1\n00 )\n(M et\nho ds\n) EE\nR \u00b1\nS td\n-D ev\n% A\nU C\n\u00b1 S\ntd -D\nev C\nPU ti\nm e\n(s )\nEE R\n\u00b1 S\ntd -D\nev %\nA U\nC \u00b1\nS td\n-D ev\nC PU\nti m\ne (s\n)\nA lg\nor ith\nm \u00a07\n44 .6\n5 \u00b1\n1 .2\n5% 0.\n58 \u00b1\n0 .0\n19 5.\n67 (0\n+ 5\n.6 7)\n48 .1\n1 \u00b1\n0 .8\n5% 0.\n54 \u00b1\n0 .0\n08 7\n7. 11\n(0 +\n7 .1\n1)\nTh e\nC PU\nti m\ne Z (Z\n1 + Z 2 ) m\nea ns\nth e\nto ta\nl C PU\nti m\ne Z\nis c\nom po\nse d\nof Z\n1 ( th\ne co\nm m\non ti\nm e)\nfo r g\nen er\nat in\ng th\ne re\ndu ce\nd ke\nrn el\nm at\nrix K\u0303\n, a nd\nZ 2 (t\nhe c\nom pu\nta tio\nn tim\ne) fo r th e ei ge na na ly si s s te p an d th e ke rn el re gr es si on st ep\n1 3\nIn this paper, we propose new algorithms based on ACS-KSR method to speed up the computation of large-scale face verification problem. By exploiting the special structure of the scatter matrices, we give a correction to the eigenanalysis step in the ACS-KSR method. The first main contribution of this work is to show why low-rank matrix approximation works well for the kernel methods from a theoretical point of view, and we propose a practical strategy for determining target rank for the randomized Nystr\u00f6m method. Based on this strategy, a modified Nystr\u00f6m method with fixed-rank for low-rank approximation of matrices is proposed.\nIn the big data era, however, the size of kernel matrix is so huge that it is impractical to form or store the matrix in main memory. Therefore, the second main contribution is to propose a randomized block Kaczmarz algorithm for kernel regression with multiple righthand sides, in which one only needs to compute a very small portion of the reduced kernel matrix. The convergence analysis of the new method is established. Numerical experiments on some real-world data sets demonstrate that the proposed approaches achieve satisfactory performance, especially for huge-scale data problems.\nWe would like to provide more specific information for helping the interested readers to identify pros and cons associated with the two proposed methods. On one hand, if the reduced kernel matrix is not very large and one can store some portions of it, we recommend using Algorithm\u00a05 for its simplicity. On the other hand, if the reduced kernel matrix is so huge that even a small portion is hard to store in main memory, we highly recommend using Algorithm\u00a07 for its high efficiency and low workload. Finally, we would like to stress that the strategies proposed in this paper also apply to many conventional kernel methods (Hofmann et\u00a0al., 2008), the multiple kernel methods (Bucak et\u00a0al., 2014; G\u00f6nen & Alpayin, 2011), and deep learning (Wang et\u00a0al., 2021). These are very interesting topics and deserve further investigation.\nAcknowledgements We would like to express our sincere thanks to our editor and the anonymous referees for insightful comments and suggestions that greatly improved the representation of this paper.\nFunding This work is supported by the Fundamental Research Funds for the Central Universities of China under Grant No. 2019XKQYMS89."
        }
    ],
    "title": "Randomized approximate class\u2010specific kernel spectral regression analysis for large\u2010scale face verification",
    "year": 2022
}