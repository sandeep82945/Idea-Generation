{
    "abstractText": "Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deepsea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-fromMotion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach comM. She, D. Nakath, K. K\u00f6ser Marine Data Science Christian-Albrechts-University of Kiel Neufeldtstra\u00dfe 6, 24118 Kiel, Germany E-mail: {mshe,ysong,dnakath,kkoeser}@geomar.de M. She, D. Nakath, Y. Song, K. K\u00f6ser GEOMAR Helmholtz Centre for Ocean Research Kiel Wischhofstrasse 1-3, 24148 Kiel, Germany Tel.: ++49 431 60",
    "authors": [
        {
            "affiliations": [],
            "name": "Mengkun She"
        },
        {
            "affiliations": [],
            "name": "Yifan Song"
        },
        {
            "affiliations": [],
            "name": "David Nakath"
        },
        {
            "affiliations": [],
            "name": "Kevin K\u00f6ser"
        }
    ],
    "id": "SP:11a7459d2bc076b7512a5b89e43065296f2affd4",
    "references": [
        {
            "authors": [
                "S. Agarwal",
                "Y. Furukawa",
                "N. Snavely",
                "B. Curless",
                "S.M. Seitz",
                "R. Szeliski"
            ],
            "title": "Reconstructing rome",
            "venue": "Computer 43(6), 40\u201347",
            "year": 2010
        },
        {
            "authors": [
                "A. Agrawal",
                "Y. Taguchi",
                "S. Ramalingam"
            ],
            "title": "Analytical forward projection for axial non-central dioptric and catadioptric cameras",
            "venue": "European Conference on Computer Vision, pp. 129\u2013143. Springer",
            "year": 2010
        },
        {
            "authors": [
                "P.N. Andono",
                "E.M. Yuniarno",
                "M. Hariadi",
                "V. Venus"
            ],
            "title": "3d reconstruction of under water coral reef images using low cost multi-view cameras",
            "venue": "2012 International Conference on Multimedia Computing and Systems, pp. 803\u2013808. IEEE",
            "year": 2012
        },
        {
            "authors": [
                "A. Arnaubec",
                "M. Ferrera",
                "J. Escart\u0301\u0131n",
                "M. Matabos",
                "N. Gracias",
                "J. Opderbecke"
            ],
            "title": "Underwater 3d reconstruction from video or still imagery: Matisse and 3dmetrics processing and exploitation software",
            "venue": "Journal of Marine Science and Engineering 11(5), 985",
            "year": 2023
        },
        {
            "authors": [
                "B. Bhowmick",
                "S. Patra",
                "A. Chatterjee",
                "V.M. Govindu",
                "S. Banerjee"
            ],
            "title": "Divide and conquer: Efficient large-scale structure from motion using graph partitioning",
            "venue": "Computer Vision\u2013ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part II 12, pp. 273\u2013287. Springer",
            "year": 2015
        },
        {
            "authors": [
                "A. Bodenmann",
                "B. Thornton",
                "T. Ura"
            ],
            "title": "Generation of high-resolution three-dimensional reconstructions of the seafloor in color using a single camera and structured light",
            "venue": "Journal of Field Robotics 34(5), 833\u2013851",
            "year": 2017
        },
        {
            "authors": [
                "M. Borgetto",
                "V. Rigaud",
                "J.F. Lots"
            ],
            "title": "Lighting correction for underwater mosaicking enhancement",
            "venue": "Proceedings of the 16th international conference on vision interface",
            "year": 2003
        },
        {
            "authors": [
                "M. Bryson",
                "M. Johnson-Roberson",
                "O. Pizarro",
                "S.B. Williams"
            ],
            "title": "True color correction of autonomous underwater vehicle imagery",
            "venue": "Journal of Field Robotics 33(6), 853\u2013874",
            "year": 2016
        },
        {
            "authors": [
                "C. Cadena",
                "L. Carlone",
                "H. Carrillo",
                "Y. Latif",
                "D. Scaramuzza",
                "J. Neira",
                "I. Reid",
                "J.J. Leonard"
            ],
            "title": "Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age",
            "venue": "IEEE Transactions on robotics 32(6), 1309\u20131332",
            "year": 2016
        },
        {
            "authors": [
                "M. Cassidy",
                "J. M\u00e9lou",
                "Y. Qu\u00e9au",
                "F. Lauze",
                "J.D. Durou"
            ],
            "title": "Refractive multi-view stereo",
            "venue": "2020 International Conference on 3D Vision (3DV), pp. 384\u2013393. IEEE",
            "year": 2020
        },
        {
            "authors": [
                "D. Cernea"
            ],
            "title": "OpenMVS: Multi-view stereo reconstruction library (2020)",
            "venue": "URL https://cdcseacave.github.io/ openMVS",
            "year": 2020
        },
        {
            "authors": [
                "F. Chadebecq",
                "F. Vasconcelos",
                "G. Dwyer",
                "R. Lacher",
                "S. Ourselin",
                "T. Vercauteren",
                "D. Stoyanov"
            ],
            "title": "Refractive structure-from-motion through a flat refractive interface",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 5315\u20135323",
            "year": 2017
        },
        {
            "authors": [
                "F. Chadebecq",
                "F. Vasconcelos",
                "R. Lacher",
                "E. Maneas",
                "A. Desjardins",
                "S. Ourselin",
                "T. Vercauteren",
                "D. Stoyanov"
            ],
            "title": "Refractive two-view reconstruction for underwater 3d vision",
            "venue": "International Journal of Computer Vision pp. 1\u2013 17",
            "year": 2019
        },
        {
            "authors": [
                "A. Chatterjee",
                "V.M. Govindu"
            ],
            "title": "Efficient and robust large-scale rotation averaging",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 521\u2013528",
            "year": 2013
        },
        {
            "authors": [
                "Y. Chen",
                "S. Shen",
                "Y. Chen",
                "G. Wang"
            ],
            "title": "Graph-based parallel large scale structure from motion",
            "venue": "Pattern Recognition 107, 107537",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chen",
                "J. Zhao",
                "L. Kneip"
            ],
            "title": "Hybrid rotation averaging: A fast and robust rotation averaging approach",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10358\u201310367",
            "year": 2021
        },
        {
            "authors": [
                "H. Cui",
                "X. Gao",
                "S. Shen",
                "Z. Hu"
            ],
            "title": "Hsfm: Hybrid structure-from-motion",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1212\u20131221",
            "year": 2017
        },
        {
            "authors": [
                "I.S. Dhillon",
                "Y. Guan",
                "B. Kulis"
            ],
            "title": "Weighted graph cuts without eigenvectors a multilevel approach",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 29(11), 1944\u20131957",
            "year": 2007
        },
        {
            "authors": [
                "J. Dong",
                "S. Soatto"
            ],
            "title": "Domain-size pooling in local descriptors: Dsp-sift",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5097\u20135106",
            "year": 2015
        },
        {
            "authors": [
                "P. Drap"
            ],
            "title": "Underwater photogrammetry for archaeology",
            "venue": "D.C. da Silva (ed.) Special Applications of Photogrammetry, chap. 6. IntechOpen, Rijeka",
            "year": 2012
        },
        {
            "authors": [
                "B. Elnashef",
                "S. Filin"
            ],
            "title": "Direct linear and refractioninvariant pose estimation and calibration model for underwater imaging",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing 154, 259\u2013271",
            "year": 2019
        },
        {
            "authors": [
                "B. Elnashef",
                "S. Filin"
            ],
            "title": "Drift reduction in underwater egomotion computation by axial camera modeling",
            "venue": "IEEE Robotics and Automation Letters",
            "year": 2023
        },
        {
            "authors": [
                "J. Engel",
                "T. Sch\u00f6ps",
                "D. Cremers"
            ],
            "title": "Lsd-slam: Large-scale direct monocular slam",
            "venue": "European conference on computer vision, pp. 834\u2013849. Springer",
            "year": 2014
        },
        {
            "authors": [
                "R. Eustice",
                "H. Singh",
                "J. Howland"
            ],
            "title": "Image registration underwater for fluid flow measurements and mosaicking",
            "venue": "OCEANS 2000 MTS/IEEE Conference and Exhibition. Conference Proceedings (Cat. No. 00CH37158), vol. 3, pp. 1529\u20131534. IEEE",
            "year": 2000
        },
        {
            "authors": [
                "W. Figueira",
                "R. Ferrari",
                "E. Weatherby",
                "A. Porter",
                "S. Hawes",
                "M. Byrne"
            ],
            "title": "Accuracy and precision of habitat structural complexity metrics derived from underwater photogrammetry",
            "venue": "Remote Sensing 7(12), 16883\u201316900",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Frahm",
                "P. Fite-Georgel",
                "D. Gallup",
                "T. Johnson",
                "R. Raguram",
                "C. Wu",
                "Y.H. Jen",
                "E. Dunn",
                "B. Clipp",
                "S. Lazebnik",
                "M. Pollefeys"
            ],
            "title": "Building rome on a cloudless day",
            "venue": "K. Daniilidis, P. Maragos, N. Paragios (eds.) Computer Vision \u2013 ECCV 2010, pp. 368\u2013381. Springer Berlin Heidelberg, Berlin, Heidelberg",
            "year": 2010
        },
        {
            "authors": [
                "I.Z. Gazis",
                "T. Schoening",
                "E. Alevizos",
                "J. Greinert"
            ],
            "title": "Quantitative mapping and predictive modeling of mn nodules\u2019 distribution from hydroacoustic and optical auv data linked by random forests machine learning",
            "venue": "Biogeosciences 15(23), 7347\u20137377",
            "year": 2018
        },
        {
            "authors": [
                "A.S. Glassner"
            ],
            "title": "An introduction to ray tracing",
            "venue": "Elsevier",
            "year": 1989
        },
        {
            "authors": [
                "M. Grimaldi",
                "D. Nakath",
                "M. She",
                "K. K\u00f6ser"
            ],
            "title": "Investigation of the challenges of underwater-visual-monocularslam",
            "venue": "arXiv preprint arXiv:2306.08738",
            "year": 2023
        },
        {
            "authors": [
                "M.D. Grossberg",
                "S.K. Nayar"
            ],
            "title": "The raxel imaging model and ray-based calibration",
            "venue": "International Journal of Computer Vision 61(2), 119\u2013137",
            "year": 2005
        },
        {
            "authors": [
                "J. Henderson",
                "O. Pizarro",
                "M. Johnson-Roberson",
                "I. Mahon"
            ],
            "title": "Mapping submerged archaeological sites using stereo-vision photogrammetry",
            "venue": "International Journal of Nautical Archaeology 42(2), 243\u2013256",
            "year": 2013
        },
        {
            "authors": [
                "K. Hissmann",
                "M. Rothenbeck",
                "E. Wenzlaff",
                "T. Wei\u00df",
                "P. Leibold"
            ],
            "title": "Rv alkor fahrtbericht/cruise report al533mutual field trials of the manned submersible jago and the hover-auvs anton and luise off the aeolian islands, mediterranean sea, catania (italy)\u2013la seyne-surmer (france) 05.02.\u201318.02",
            "year": 2020
        },
        {
            "authors": [
                "A. Hogue",
                "A. German",
                "J. Zacher",
                "M. Jenkin"
            ],
            "title": "Underwater 3D mapping: Experiences and lessons learned",
            "venue": "Computer and Robot Vision, 2006. The 3rd Canadian Conference on",
            "year": 2006
        },
        {
            "authors": [
                "S. Jiang",
                "C. Jiang",
                "W. Jiang"
            ],
            "title": "Efficient structure from motion for large-scale uav images: A review and a comparison of sfm tools",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing 167, 230\u2013251",
            "year": 2020
        },
        {
            "authors": [
                "M. Johnson-Roberson",
                "M. Bryson",
                "A. Friedman",
                "O. Pizarro",
                "G. Troni",
                "P. Ozog",
                "J.C. Henderson"
            ],
            "title": "Highresolution underwater robotic vision-based mapping and three-dimensional reconstruction for archaeology",
            "venue": "Journal of Field Robotics 34(4), 625\u2013643",
            "year": 2017
        },
        {
            "authors": [
                "M. Johnson-Roberson",
                "O. Pizarro",
                "S.B. Williams",
                "I.J. Mahon"
            ],
            "title": "Generation and visualization of largescale three-dimensional reconstructions from underwater robotic surveys",
            "venue": "Journal of Field Robotics 27",
            "year": 2010
        },
        {
            "authors": [
                "A. Jordt"
            ],
            "title": "Underwater 3d reconstruction based on physical models for refraction and underwater light propagation",
            "venue": "Ph.D. thesis, Christian-Albrechts-Universt\u00e4t zu Kiel, Germany",
            "year": 2014
        },
        {
            "authors": [
                "A. Jordt",
                "K. K\u00f6ser",
                "R. Koch"
            ],
            "title": "Refractive 3d reconstruction on underwater images",
            "venue": "Methods in Oceanography 15-16, 90\u2013113",
            "year": 2016
        },
        {
            "authors": [
                "A. Jordt-Sedlazeck",
                "D. Jung",
                "R. Koch"
            ],
            "title": "Refractive plane sweep for underwater images",
            "venue": "J. Weickert, M. Hein, B. Schiele (eds.) Pattern Recognition, Lecture Notes in Computer Science, vol. 8142, pp. 333\u2013 342. Springer Berlin Heidelberg",
            "year": 2013
        },
        {
            "authors": [
                "A. Jordt-Sedlazeck",
                "R. Koch"
            ],
            "title": "Refractive calibration of underwater cameras",
            "venue": "A. Fitzgibbon, S. Lazebnik, P. Pietro, Y. Sato, C. Schmid (eds.) Computer Vision - ECCV 2012, Lecture Notes in Computer Science, vol. 7576, pp. 846\u2013859. Springer Berlin Heidelberg",
            "year": 2012
        },
        {
            "authors": [
                "A. Jordt-Sedlazeck",
                "R. Koch"
            ],
            "title": "Refractive structure-frommotion on underwater images",
            "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 57\u2013 64",
            "year": 2013
        },
        {
            "authors": [
                "B. Joshi",
                "H. Damron",
                "S. Rahman",
                "I. Rekleitis"
            ],
            "title": "Sm/vio: Robust underwater state estimation switching between model-based and visual inertial odometry",
            "venue": "arXiv preprint arXiv:2304.01988",
            "year": 2023
        },
        {
            "authors": [
                "B. Joshi",
                "S. Rahman",
                "M. Kalaitzakis",
                "B. Cain",
                "J. Johnson",
                "M. Xanthidis",
                "N. Karapetyan",
                "A. Hernandez",
                "A.Q. Li",
                "N Vitzilaios"
            ],
            "title": "Experimental comparison of open source visual-inertial-based state estimation algorithms in the underwater domain",
            "venue": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7227\u20137233. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "B. Joshi",
                "M. Xanthidis",
                "S. Rahman",
                "I. Rekleitis"
            ],
            "title": "High definition, inexpensive, underwater mapping",
            "venue": "2022 International Conference on Robotics and Automation (ICRA), pp. 1113\u20131121. IEEE",
            "year": 2022
        },
        {
            "authors": [
                "K. K\u00f6ser",
                "U. Frese"
            ],
            "title": "Challenges in underwater visual navigation and slam",
            "venue": "AI Technology for Underwater Robots, pp. 125\u2013135. Springer",
            "year": 2020
        },
        {
            "authors": [
                "K. K\u00f6ser",
                "Y. Song",
                "L. Petersen",
                "E. Wenzlaff",
                "F. Woelk"
            ],
            "title": "Robustly removing deep sea lighting effects for visual mapping of abyssal plains",
            "venue": "arXiv preprint arXiv:2110.00480",
            "year": 2021
        },
        {
            "authors": [
                "C. Kunz",
                "H. Singh"
            ],
            "title": "Hemispherical refraction and camera calibration in underwater vision",
            "venue": "OCEANS 2008, pp. 1\u20137. IEEE",
            "year": 2008
        },
        {
            "authors": [
                "T. Kwasnitschka",
                "K. K\u00f6ser",
                "J. Sticklus",
                "M. Rothenbeck",
                "T. Wei\u00df",
                "E. Wenzlaff",
                "T. Schoening",
                "L. Triebe",
                "A. Steinf\u00fchrer",
                "C Devey"
            ],
            "title": "Deepsurveycam\u2014a deep ocean optical mapping system",
            "venue": "Sensors 16(2), 164",
            "year": 2016
        },
        {
            "authors": [
                "J.J. Leonard",
                "A. Bahr"
            ],
            "title": "Autonomous underwater vehicle navigation",
            "venue": "Springer Handbook of Ocean Engineering pp. 341\u2013358",
            "year": 2016
        },
        {
            "authors": [
                "S. Leutenegger",
                "S. Lynen",
                "M. Bosse",
                "R. Siegwart",
                "P. Furgale"
            ],
            "title": "Keyframe-based visual\u2013inertial odometry using nonlinear optimization",
            "venue": "The International Journal of Robotics Research 34(3), 314\u2013334",
            "year": 2015
        },
        {
            "authors": [
                "T. Luczynski",
                "M. Pfingsthorn",
                "A. Birk"
            ],
            "title": "The pinaxmodel for accurate and efficient refraction correction of underwater cameras in flat-pane housings",
            "venue": "Ocean Engineering 133, 9 \u2013 22",
            "year": 2017
        },
        {
            "authors": [
                "H.G. Maas"
            ],
            "title": "New developments in multimedia photogrammetry",
            "venue": "Optical 3-D Measurement Techniques III. Wichmann Verlag, Karlsruhe",
            "year": 1995
        },
        {
            "authors": [
                "I. Mahon",
                "S.B. Williams",
                "O. Pizarro",
                "M. JohnsonRoberson"
            ],
            "title": "Efficient view-based slam using visual loop closures",
            "venue": "IEEE Transactions on Robotics 24(5), 1002\u2013 1014",
            "year": 2008
        },
        {
            "authors": [
                "D. Martinec",
                "T. Pajdla"
            ],
            "title": "Robust rotation and translation estimation in multiview reconstruction",
            "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138. IEEE",
            "year": 2007
        },
        {
            "authors": [
                "F. Menna",
                "E. Nocerino",
                "S. Malek",
                "F. Remondino",
                "S. Schiaparelli"
            ],
            "title": "A combined approach for long-term monitoring of benthos in antarctica with underwater photogrammetry and image understanding",
            "venue": "International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences",
            "year": 2022
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.D. Tard\u00f3s"
            ],
            "title": "Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras",
            "venue": "IEEE transactions on robotics 33(5), 1255\u20131262",
            "year": 2017
        },
        {
            "authors": [
                "D. Nakath",
                "M. She",
                "Y. Song",
                "K. K\u00f6ser"
            ],
            "title": "In-situ joint light and medium estimation for underwater color 26 Mengkun She et al",
            "venue": "restoration. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3731\u20133740",
            "year": 2021
        },
        {
            "authors": [
                "E. Nocerino",
                "F. Menna",
                "A. Gruen",
                "M. Troyer",
                "A. Capra",
                "C. Castagnetti",
                "P. Rossi",
                "A.J. Brooks",
                "R.J. Schmitt",
                "S.J. Holbrook"
            ],
            "title": "Coral reef monitoring by scuba divers using underwater photogrammetry and geodetic surveying",
            "venue": "Remote Sensing 12(18), 3036",
            "year": 2020
        },
        {
            "authors": [
                "O. Ozyesil",
                "A. Singer"
            ],
            "title": "Robust camera location estimation by convex programming",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2674\u20132683",
            "year": 2015
        },
        {
            "authors": [
                "A. Peukert",
                "T. Schoening",
                "E. Alevizos",
                "K. K\u00f6ser",
                "T. Kwasnitschka",
                "J. Greinert"
            ],
            "title": "Understanding mn-nodule distribution and evaluation of related deep-sea mining impacts using auv-based hydroacoustic and optical data",
            "venue": "Biogeosciences 15(8), 2525\u20132549",
            "year": 2018
        },
        {
            "authors": [
                "O. Pizarro",
                "H. Singh"
            ],
            "title": "Toward large-area mosaicing for underwater scientific applications",
            "venue": "IEEE journal of oceanic engineering 28(4), 651\u2013672",
            "year": 2003
        },
        {
            "authors": [
                "M. Pollefeys",
                "D. Nist\u00e9r",
                "J.M. Frahm",
                "A. Akbarzadeh",
                "P. Mordohai",
                "B. Clipp",
                "C. Engels",
                "D. Gallup",
                "S.J. Kim",
                "P Merrell"
            ],
            "title": "Detailed real-time urban 3d reconstruction from video",
            "venue": "International Journal of Computer Vision 78, 143\u2013167",
            "year": 2008
        },
        {
            "authors": [
                "D. Ribas",
                "N. Palomeras",
                "P. Ridao",
                "M. Carreras",
                "A. Mallios"
            ],
            "title": "Girona 500 auv: From survey to intervention",
            "venue": "IEEE/ASME Transactions on mechatronics 17(1), 46\u201353",
            "year": 2011
        },
        {
            "authors": [
                "P. Ridao",
                "M. Carreras",
                "D. Ribas",
                "R. Garcia"
            ],
            "title": "Visual inspection of hydroelectric dams using an autonomous underwater vehicle",
            "venue": "Journal of Field Robotics 27(6), 759\u2013 778",
            "year": 2010
        },
        {
            "authors": [
                "H.S. Sawhney",
                "S. Hsu",
                "R. Kumar"
            ],
            "title": "Robust video mosaicing through topology inference and local to global alignment",
            "venue": "Computer Vision\u2014ECCV\u201998: 5th European Conference on Computer Vision Freiburg, Germany, June 2\u20136, 1998 Proceedings, Volume II 5, pp. 103\u2013119. Springer",
            "year": 1998
        },
        {
            "authors": [
                "H.S. Sawhney",
                "R. Kumar"
            ],
            "title": "True multi-image alignment and its application to mosaicing and lens distortion correction",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 21(3), 235\u2013243",
            "year": 1999
        },
        {
            "authors": [
                "T. Schoening",
                "T. Kuhn",
                "D.O. Jones",
                "E. Simon-Lledo",
                "T.W. Nattkemper"
            ],
            "title": "Fully automated image segmentation for benthic resource assessment of poly-metallic nodules",
            "venue": "Methods in Oceanography 15, 78\u201389",
            "year": 2016
        },
        {
            "authors": [
                "J.L. Schonberger",
                "J.M. Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4104\u2013 4113",
            "year": 2016
        },
        {
            "authors": [
                "P. Sch\u00f6ntag",
                "D. Nakath",
                "S. R\u00f6hrl",
                "K. K\u00f6ser"
            ],
            "title": "Towards cross domain transfer learning for underwater correspondence search",
            "venue": "S. Sclaroff, C. Distante, M. Leo, G.M. Farinella, F. Tombari (eds.) Image Analysis and Processing \u2013 ICIAP 2022, pp. 461\u2013472. Springer International Publishing, Cham",
            "year": 2022
        },
        {
            "authors": [
                "T. Schops",
                "V. Larsson",
                "M. Pollefeys",
                "T. Sattler"
            ],
            "title": "Why having 10,000 parameters in your camera model is better than twelve",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2535\u20132544",
            "year": 2020
        },
        {
            "authors": [
                "A. Sedlazeck",
                "K. K\u00f6ser",
                "R. Koch"
            ],
            "title": "3D reconstruction based on underwater video from ROV kiel 6000 considering underwater imaging conditions",
            "venue": "Proc. OCEANS \u201909. OCEANS 2009-EUROPE, pp. 1\u201310",
            "year": 2009
        },
        {
            "authors": [
                "M. Seidel",
                "T. Frey",
                "J. Greinert"
            ],
            "title": "Underwater uxo detection using magnetometry on hovering auvs",
            "venue": "Journal of Field Robotics",
            "year": 2023
        },
        {
            "authors": [
                "M. She",
                "D. Nakath",
                "Y. Song",
                "K. K\u00f6ser"
            ],
            "title": "Refractive geometry for underwater domes",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing 183, 525\u2013540",
            "year": 2022
        },
        {
            "authors": [
                "M. She",
                "Y. Song",
                "J. Mohrmann",
                "K. K\u00f6ser"
            ],
            "title": "Adjustment and calibration of dome port camera systems for underwater vision",
            "venue": "German Conference on Pattern Recognition, pp. 79\u201392. Springer",
            "year": 2019
        },
        {
            "authors": [
                "M. She",
                "T. Wei\u00df",
                "Y. Song",
                "P. Urban",
                "J. Greinert",
                "K. K\u00f6ser"
            ],
            "title": "Marine bubble flow quantification using wide-baseline stereo photogrammetry",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing 190, 322\u2013341",
            "year": 2022
        },
        {
            "authors": [
                "M. Shortis"
            ],
            "title": "Calibration techniques for accurate measurements by underwater camera systems",
            "venue": "Sensors 15(12), 30810\u201330826",
            "year": 2015
        },
        {
            "authors": [
                "K.A. Skinner",
                "E. Iscar",
                "M. Johnson-Roberson"
            ],
            "title": "Automatic color correction for 3d reconstruction of underwater scenes",
            "venue": "2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 5140\u20135147. IEEE",
            "year": 2017
        },
        {
            "authors": [
                "N. Snavely",
                "S. Seitz",
                "R. Szeliski"
            ],
            "title": "Modeling the world from internet photo collections",
            "venue": "Int. J. Comput. Vision 80(2), 189\u2013210",
            "year": 2008
        },
        {
            "authors": [
                "N. Snavely",
                "S.M. Seitz",
                "R. Szeliski"
            ],
            "title": "Photo tourism: exploring photo collections in 3d",
            "venue": "ACM siggraph 2006 papers, pp. 835\u2013846",
            "year": 2006
        },
        {
            "authors": [
                "Y. Song",
                "D. Nakath",
                "M. She",
                "K. K\u00f6ser"
            ],
            "title": "Optical imaging and image restoration techniques for deep ocean mapping: A comprehensive survey",
            "venue": "PFG\u2013Journal of Photogrammetry, Remote Sensing and Geoinformation Science 90(3), 243\u2013267",
            "year": 2022
        },
        {
            "authors": [
                "C. Sweeney",
                "T. Sattler",
                "T. Hollerer",
                "M. Turk",
                "M. Pollefeys"
            ],
            "title": "Optimizing the viewing graph for structurefrom-motion",
            "venue": "Proceedings of the IEEE international conference on computer vision, pp. 801\u2013809",
            "year": 2015
        },
        {
            "authors": [
                "G. Telem",
                "S. Filin"
            ],
            "title": "Photogrammetric modeling of underwater environments",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing 65(5), 433\u2013444",
            "year": 2010
        },
        {
            "authors": [
                "T. Treibitz",
                "Y.Y. Schechner",
                "H. Singh"
            ],
            "title": "Flat refractive geometry",
            "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition CVPR 2008, pp. 1\u20138",
            "year": 2008
        },
        {
            "authors": [
                "P. Urban",
                "K. K\u00f6ser",
                "J. Greinert"
            ],
            "title": "Processing of multibeam water column image data for automated bubble/seep detection and repeated mapping",
            "venue": "Limnology and Oceanography: Methods 15(1), 1\u201321",
            "year": 2017
        },
        {
            "authors": [
                "F. Verbiest",
                "M. Proesmans",
                "L. Van Gool"
            ],
            "title": "Modeling the effects of windshield refraction for camera calibration",
            "venue": "A. Vedaldi, H. Bischof, T. Brox, J.M. Frahm (eds.) Computer Vision \u2013 ECCV 2020, pp. 397\u2013412. Springer International Publishing, Cham",
            "year": 2020
        },
        {
            "authors": [
                "K. Wilson",
                "N. Snavely"
            ],
            "title": "Robust global translations with 1dsfm",
            "venue": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III 13, pp. 61\u201375. Springer",
            "year": 2014
        },
        {
            "authors": [
                "W. Yang",
                "X. Zhang",
                "H. Ma",
                "G. Zhang",
                "G. Yang"
            ],
            "title": "Noncentral refractive camera calibration using co-planarity constraints for a photogrammetric system with an optical sphere cover",
            "venue": "Optics and Lasers in Engineering 139, 106487",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang"
            ],
            "title": "A flexible new technique for camera calibration",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence 22",
            "year": 2000
        },
        {
            "authors": [
                "S. Zhu",
                "T. Shen",
                "L. Zhou",
                "R. Zhang",
                "J. Wang",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Parallel structure from motion from local increment to global averaging",
            "venue": "arXiv preprint arXiv:1702.08601",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deepsea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-fromMotion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach com-\nM. She, D. Nakath, K. Ko\u0308ser Marine Data Science Christian-Albrechts-University of Kiel Neufeldtstra\u00dfe 6, 24118 Kiel, Germany E-mail: {mshe,ysong,dnakath,kkoeser}@geomar.de\nM. She, D. Nakath, Y. Song, K. Ko\u0308ser GEOMAR Helmholtz Centre for Ocean Research Kiel Wischhofstrasse 1-3, 24148 Kiel, Germany Tel.: ++49 431 600 2272,\nbining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.\nKeywords Seafloor mapping \u00b7 Underwater imaging \u00b7 Structure-from-motion \u00b7 Camera calibration \u00b7 Robotic mapping"
        },
        {
            "heading": "1 Introduction",
            "text": "More than half of Earth\u2019s surface is covered by the deep ocean with at least one kilometer of water depth and there is a growing interest in exploring this uncharted terrain. This is typically carried out by robotic platforms with various sensing technologies such as cabled and remotely operated vehicles (ROVs) [71] or those that navigate autonomously (AUVs) [49,36]. In particular, optical images have become increasingly attractive due to their high-resolution and suitability for human interpretation, making them a complementary survey technology to ship-based acoustic methods such as side scan sonar or multi-beam echo sounders [48]. Over the past decades, numerous visual mapping algorithms on land have achieved remarkable outcomes, leading to the perception that visual mapping is considered a solved problem under certain circumstances [9]. State-of-the-art Structure-from-Motion (SfM) algorithms, like COLMAP [68], demonstrate high robustness in reconstruction of on-land scenarios. However, in contrast to reconstructions from highly redundant photo collections [78,1,26], robotic photogrammetric surveys of the deep sea have to minimize redundancy for ar X iv :2 30 8.\n06 14\n7v 1\n[ cs\n.C V\n] 1\n1 A\nug 2\ncost reasons and must use the available dive time as efficient as possible. Consequently, where photo-collection approaches simply discard difficult images or drop connections for efficiency, deep sea robotic surveys must try to register as many images as possible and at the same achieve the best reconstruction even under challenging situations. Unfortunately, in the underwater domain, when applying state-of-the-art Simultaneous Localization and Mapping (SLAM) approaches [56,23,50], significant positional errors, divergence or tracking loss, or even failure have been observed [43,45,29]. As a result, directly transferring these technologies to the deep-sea remains challenging due to the harsh and unique environmental conditions encountered in this setting.\nFirstly, artificial illumination systems on deepdiving robots are essential to provide sufficient lighting for image capture in deeper waters. However, the presence of co-moving light sources creates challenges for image-based reconstruction. Specifically, the assumption of photo-consistency, which states that the appearance of the same 3D point in different images is consistent, will be violated if the lighting conditions change significantly between images[69]. This can result in reduced robustness of feature correspondences, which are the key to accurately aligning images during reconstruction. Additionally, the effective field-of-view of the cameras may be limited by the illumination cone, which can lead to a loss of image content, rendering it impossible to extract any meaningful information in areas with poor illumination (see Fig. 1, left and center). As shown in Fig. 2, a simple comparison can be made between a typical drone survey on land and a deep-sea AUV survey. In the case of an on-land survey, features are uniformly distributed throughout the image. However, in a deep-sea AUV survey, features are mostly concentrated within the area illuminated by the light cone. As a consequence, there is a reduction in image overlap (especially side overlap) when photographing the seafloor, which can result in inconsistent reconstructions due to the insufficient amount of\nfeature matches between adjacent tracks. Secondly, radiometric degradation caused by light scattering and attenuation can introduce a layer of haze in the image, reducing its contrast, and affect its color. This is illustrated in Fig. 1, where the degradation is clearly visible in the images. To facilitate deep sea / artificial light mapping and to create an artefact-free mosaic of the target scene, effective image restoration techniques are required, but are largely missing [80], except for simple scenarios (e.g. [46]). Moreover, from a geometrical perspective, the refraction of incident light rays at the water-pressure-housing interface alters their directions, which complicates the image formation model [40,73]. Therefore, special calibration methods should be employed to account for refraction effects or incorporate them into the visual mapping pipeline to correct for geometrical distortion [41,39,38,12,22].\nAll these challenges are specific to the underwater scenario, but there is another crucial aspect that poses a challenge in general visual mapping scenarios: scalability and efficiency. Deep-sea surveys are not only costly but also require significant human resources. Therefore, it is desirable for each robot deployment to have the capability to map a large area of seafloor, which often leads to a substantial amount of image data. While many current state-of-the-art SfM [68] approaches are incremental and have demonstrated accuracy and robustness against outliers, they suffer from inefficiency when dealing with large-scale datasets. The computational complexity of these incremental methods increases about cubically with the number of images, making them impractical for large-scale mapping tasks. Additionally, they are prone to drift due to error accumulation over time. To address scalability in visual mapping scenarios, researchers have proposed Global SfM methods. These methods are designed to handle large-scale scenes by leveraging the concept of averaging rotational and translational inconsistencies of all images over the entire view graph [86,14,59]. By doing so, they reduce the drift and avoid the need for repeated bun-\ndle adjustment when adding images to the reconstruction. Furthermore, the divide-and-conquer approach has been employed to address even larger-scale scenes by partitioning the view graph into smaller clusters and performing separate reconstructions in parallel. Subsequently, these sub-reconstructions are merged to create a global and consistent reconstruction [5,89,15]. However, global approaches come with their own limitations. One limitation is a lack of effective outlier filtering and an intrinsic degeneracy that relates to the missing scale in visual two-view relations (relative rotation and translation direction): For scenarios, where robots move predominantly forward in a fixed direction like in lawnmower patterns, absolute camera positions become ambiguous at the global SfM level.\nDespite the aforementioned challenges, there is still a strong motivation to advance large-scale deep-sea visual mapping for various applications in marine science and industry, such as resource estimation and deep-\nsea mining [67,27], monitoring of deposited munitions [72], underwater archaeology [31,20], habitat mapping [3,33], or gas seepage monitoring [75,84], among others. Therefore in this work, we focus on the practical challenges and leverage the recent developments in both underwater imaging and visual mapping and present an automated approach that can map large areas of the seafloor (hectares) efficiently while avoiding to discard difficult imagery.\nOur contributions are mainly three-fold: Firstly, we carefully consider refraction to avoid reconstruction biases [74,73], which has not received much attention in earlier seminal underwater SLAM/SfM works [36,6,25, 77,64,4,44]. Secondly, we propose a hybrid mapping approach that combines the benefits of SLAM to overcome the limitation of global SfM and the accuracy of smallto-medium scale incremental SfM to achieve a robust and efficient reconstruction of large underwater scenes. Specifically, we partition the large scene into smaller\nclusters and perform local SfM reconstructions concurrently. Subsequently, a global pose graph optimization is performed to obtain a consistent camera trajectory using the upgraded view graph from the local reconstructions. Thirdly, to improve the quality of the pose graph optimization, we propose to identify weakly reconstructed areas during the local reconstruction phase and revisit them to reduce inconsistencies in the resulting camera trajectory. We conduct detailed and extensive evaluations on several real-world datasets with various visual characteristics, demonstrating that the proposed approach achieves much more physically consistent results, i.e., the vehicle exhibits expected motions without gaps, jumps, than vanilla COLMAP on difficult datasets while reducing processing time by an order of magnitude. Our ablation study further confirms the effectiveness and necessity of the proposed approach."
        },
        {
            "heading": "2 Related Work",
            "text": "2.1 Handling of Refraction Effects\nTo protect cameras from water and high pressure in the ocean, they are enclosed in a pressure housing and observe the surroundings through a glass window. This window typically has a planar or spherical shape, as can be seen in Fig. 3. As the pressure housing is filled with air, the media changes at the glass interfaces (waterglass-air) due to the different optical densities. Light rays that come from the underwater scene change their direction when they travel through the interfaces in a non-orthogonal manner, a phenomenon known as multimedia photogrammetry [52].\nIn earlier times, refraction effects were considered by approximating the overall system, including the glass port, as a perspective camera [76], and calibrating it by submerging the entire system, including calibration targets, underwater. Refraction effects could then be\npartially mitigated by intrinsic parameters such as focal length, principal points, and distortion parameters. This approach was considered acceptable for a predefined fixed working distance since the refraction effect is distance-dependent but becomes less suitable the more scene distances can vary. The advantage of ignoring refraction is that this allows to use traditional multi-view geometry, making it elegant since no extra effort is required to model the refraction effects in the SfM and MVS pipeline. Alternatively, refraction effects can also be handled explicitly. For instance, the overall imaging system can be represented by the a generic raybased camera model that describes how rays from the scene map to the image [30,70,51]. It has been demonstrated that this type of modeling outperforms traditional parametric models, even in on-land applications [70]. Another approach to handle refraction effects is by physically modeling the camera housing with geometric primitives such as planes and spheres [83,2,40,37, 82,21]. In this method, light rays propagate through the interfaces and interact with the scene in a ray-tracing manner [28]. This approach is considered state-of-art for calibrating a multi-media camera system and is beneficial not only for underwater applications but also for on-land applications, such as cameras behind the windshield of the car [85] or behind the optical cover of a coal mining vehicle [87]. Exact refraction modeling, while providing high accuracy, invalidates traditional pinhole multi-view geometry techniques such as epipolar geometry, pose estimation, triangulation, bundle adjustment, and dense depth estimation. Even when the camera system is properly calibrated, applying the calibration parameters in the later visual mapping pipeline remains challenging. Recent works have addressed these challenges by presenting solutions for refractive two-view geometry [13], refractive SfM [41,12], and refractive dense MVS [39,10]. However, achieving large-scale 3D reconstruction using a refractive camera requires solving the refractive two-view geometry problem for a large number of image pairs. To exclude outliers, this expensive solver is embedded in a RANSAC (Random Sample Consensus) framework, further increasing the computational cost. Additionally, projecting a 3D point into the image requires either an iterative back-projection strategy [47] or a high-degree polynomial solution [2,73]. Hence, robustness, accuracy, numerical stability, and scalability remain the major concerns when mapping a large seafloor area. Alternative approaches exist that preserve traditional multi-view geometry while modeling refraction effects [82,21]. These methods use axial camera modeling, and apply a correction to the projection center of every ray that enters the camera, creating a virtual camera that appears to look at the scene\npoint as if there is no refraction. These techniques have not yet been employed in large reconstructions so far, but have been demonstrated in direct pose estimation [21] and visual odometry [22] for short reconstruction tracks.\nRecent studies have shown that refraction effects can be effectively minimized in dome-port camera systems when the projection center of the camera is precisely aligned with the center of the spherical dome port housing [73]. Small forward and backward decentering of the lens inside the dome can be largely compensated by radial distortion parameters when perspective underwater calibration is performed. Therefore, in this work, we have upgraded our camera system from a flat port to a dome port and carefully mechanically adjusted it [74] to avoid explicit handling of refraction effects.\n2.2 Visual Mapping\nVisual mapping is a fundamental process in robotics that involves recovering 3D structure and camera poses of images captured from different viewpoints [68,34, 42]. It has been extensively studied in the past few decades and has numerous applications, such as autonomous driving, aerial and underwater robotics, and augmented and virtual reality. Depending on the purpose of the application, visual mapping can be categorized into two main categories: those that focus on the map quality and those that focus on online operation ability. The former case mostly refers to approaches that rely on the SfM (Structure-from-Motion) algorithm, while the latter case is mainly referred to as SLAM (Simultaneous Localization and Mapping), which aims to incrementally build a map of the environment while simultaneously estimating the robot\u2019s pose with respect to the map in real-time [9]. Earlier deepsea/underwater visual mapping systems mostly focused on 2D image mosaicking [65,66]. Ridao et al. [64] improved the photomosaicking results by including a loopdetection module and considering non-consecutive images. They use this approach for automatically inspecting underwater dams. Moving beyond 2D information, Johnson-Roberson et al. [36] presented a complete, robust and automated system for large-scale 3D reconstruction and visualization of the seafloor using images fused with vehicle\u2019s navigation data. The underlying SLAM system [53] used by this work was based on an information filter. However, the two systems presented above depend on depth information obtained from a stereo camera system while we work on purely monocular image data. Although the ability for online operation is a future pursuit, it is currently not practical due to the high cost of losing an expensive deep-sea robot.\nFurthermore, researchers have identified the challenge of finding good features on visually degraded images as a major reason for the failure of live-SLAM systems [43,45,29]. Instead, SfM has greater flexibility in selecting the images and the order in which they are reconstructed. This also provides the opportunity to choose more robust and high-quality features for the reconstruction process. Therefore, in this work, our primary focus is on generating high-quality maps in an offline mode.\nSfM has shown remarkable performance in various applications, including urban scenes [62] and unordered internet photo collections [78,79,1]. A state of the art SfM approach nowadays is contained in the general purpose reconstruction library COLMAP [68], and it is commonly used for photogrammetry surveying, not only in on-land environments but also in underwater settings [55,20,58]. A recent comprehensive review on the SfM approaches for large-scale UAV images can be found in [34]. However, it is important to note that these achievements primarily focus on incremental SfM, which is also known to suffer from drift accumulation and inefficiency when dealing with large-scale datasets, as mentioned earlier in Sect. 1.\nGlobal SfM approaches on the other hand, consider the entire reconstruction problem at once, typically involving three major steps: 1) view graph construction by feature extraction and matching, 2) estimation of global poses using the view graph and 3) triangulation of scene structure and bundle adjustment. Since camera poses are solved non-incrementally using relative constraints derived from the view graph, every image is treated equally and all sequential and loop-closure constrains are included in the optimization, helping to mitigate drift. However, there are two main challenges in global SfM. First, the epipolar geometries derived from the view graph can be noisy, even with one layer of outlier rejection by RANSAC during geometric verification. Second, the epipolar geometry only provides a relative rotation and a translation direction, resulting in a 5-DOFs relative pose estimate, making it difficult to accurately determine absolute camera positions. To address these issues, many works have focused on robustly averaging rotations from noisy inputs, which has matured in recent years [14,54,81,16]. As for the second challenge, 1DSfM [86] addresses this issue by filtering out unreliable measurements through the projection of translation directions to one of the axes, ensuring that the order constraints are maintained. They also introduce a cost function to minimize translation directions, although they do not address the issue of scale ambiguities. O\u0308zyesil and Singer [59] utilized parallel rigidity theory to identify images where the absolute camera\npositions can be determined uniquely. However these works mainly focus on the reconstruction of unordered internet photos, where removing some bad images is acceptable. On the contrary, in the deep-sea AUV mapping scenario, the AUVs typically move forward and capture images at a constant interval. As a result, the view graph formed by these images does not exhibit parallel rigidity, and scale ambiguity becomes a significant issue. Moreover, for completeness and cost reasons, we seek to reconstruct all images if possible even with challenging conditions because each AUV dive is expensive and the target area may only be visited a limited number of times.\nTo tackle the above challenges, we perform a full incremental SfM on a small local region of images. This is because small-to-medium-scale incremental SfM has been proven to be reliable. By doing so, we obtain an upgraded view graph that contains the full 6-DOFs relative motions, and the reconstructed 3D points in this local region indicate the true inliers. Using the upgraded view graph, we can then construct a global pose graph optimizer to perform motion averaging and solve for all camera poses. Although the idea of combining incremental SfM with global SfM has been proposed before by Cui [17], we are inherently different since they first perform global rotation averaging to obtain rotations for all cameras, and then incrementally reconstruct all images with fixed rotations. The most similar work to us is [15], where they propose a novel algorithm for scene partitioning and a framework for performing parallel/distributed incremental SfM on partitioned clusters, followed by a merging step to form a global reconstruction. However, incremental SfM on each cluster produces an arbitrarily scaled subreconstruction, requiring the estimation of similarity transformations based on overlapping images for merging, which makes the partitioning of the scene crucial. In contrast, our approach addresses a different scenario. Modern AUVs come with their own navigation systems for self-navigating in the water. Although we have no strict requirement on the accuracy of the navigation system, we incorporate the navigation data into incremental SfM to produce a fixed-scale reconstruction, thus relaxing the need for scene partitioning."
        },
        {
            "heading": "3 System Design",
            "text": "3.1 The AUVs and the CoraMo Camera System\nThe GEOMAR Helmholtz Centre for Ocean Research Kiel, Germany (GEOMAR) owns two Girona 500 AUVs\nnamed \u201dAnton\u201d and \u201dLuise\u201d1 (shown in Fig. 4, left), developed by the Underwater Robotics Laboratory of the University of Girona, Spain [63]. The Girona 500 AUVs are specifically designed for slower speeds with hovering capabilities, enabling them to operate in a close distance to the seafloor and to achieving higher resolutions. Furthermore, they are designed to be reconfigurable for custom tasks and variety of sensor suites. More comprehensive informations regarding the design and specifications of the Girona 500 AUVs can be found in [63].\nOn top of that basis, the GEOMAR\u2019s AUV Team has developed the CoraMo Mk II camera system 2 [32], as shown in Fig. 4, right. The CoraMo Mk II camera system incorporates a high-performance machine vision camera (iDS UI-3000SE-C-HQ) featuring a Sony IMX253 1.1 CMOS sensor. The camera has a resolution of 4104 x 3006 pixels, and is equipped with a TAMRON fisheye lens to achieve a very wide field of view. The camera system is then enclosed in a pressure housing with a 10cm diameter spherical optical port, capable of withstanding water depths of up to 6000 meters. Additionally, a 1m diameter LED ring with eight LEDs is mounted around the camera system to provide adequate illumination."
        },
        {
            "heading": "4 Visual Seafloor Mapping",
            "text": "The visual seafloor mapping pipeline consists of several key steps, as shown in Fig. 5. First, an underwater camera calibration is conducted to calibrate the domeport camera system prior to the mission. After the mission, the navigation data, obtained from a sensor fusion module, is extracted and used for geo-referencing the captured images. Next, a color normalization approach, such as the one described in [46], is applied to the image sequence, removing deep-sea lighting effects. Afterwards, a navigation-aided hybrid SfM is performed to create a globally consistent sparse representation of the scene, followed by a chunk-based Multi-View Stereo (MVS) step to produce a densified 3D point cloud. Finally, it is of interest for marine data science application to reconstruct a textured surface mesh and an orthophoto for geographic interpretation.\n1 https://www.geomar.de/tlz/auv-autonomeunterwasserfahrzeuge/autonome-unterwasserfahrzeuge 2 https://www.geomar.de/tlz/auv-autonomeunterwasserfahrzeuge/programme-und-projekte/coramo\n4.1 Color Normalization\nWhen propagating in water, light undergoes absorption and scattering, limiting the penetration of sunlight from the water surface into the ocean to just a few hundred meters. For deep-sea AUV mapping tasks, which are often conducted at depths of hundreds to thousands of meters, artificial light sources are required to provide adequate illumination in the otherwise absolute darkness. However, introducing artificial lighting leads to different light and water effects in images compared to shallow water conditions [80], including pronounced light patterns and non-isotropic scattering light cones. To obtain a 3D model which is free of illumination effects, various methods have been developed to remove artificial light patterns. For instance, Pizarro et al. [61] estimates an illumination image in log space by averaging frames to remove the illumination pattern. Borgetto et al. [7] uses natural halo images to approximate the lighting pattern, while Johnson-Roberson et al. [35] employs the gray-world algorithm to estimate gain and offset images based on mean and variance over clustered image sets. Additionally, Eustice et al. [24] applies contrast limited adaptive histogram equalization to homogenize the illumination in images. Some physical model-based approaches not only remove light patterns but also restore true colors. For example, Bryson et al. [8] modifies the Jaffe-McGlamery image formation model and estimates its parameters from image correspondences to simultaneously remove light patterns and correct the colors of underwater images. Nakath et al.\n[57] uses a Monte-Carlo based differentiable ray-tracing approach to estimate the light and water parameters by minimizing differences between simulated and real images, followed by estimating object colors under a similar rendering pipeline using the previously estimated parameters. However, these approaches either require prior knowledge of the lighting system or are computationally intensive and suitable only for small-scale applications. In this work, we employ a rather more practical approach presented in [46], which utilizes the assumption that the seafloor has a constant dominant color (>50% of pixels), and perform a statistics-based estimation of additive and multiplicative nuisances that avoid explicit parameters for light, camera, and the water. The normalization procedure involves subtracting the previously estimated additive scatter component and dividing by the multiplicative factor image. The approach is implemented in CUDA and it runs in (near) real-time, therefore, it is more suitable for large-scale AUV mapping tasks.\n4.2 Underwater Camera Calibration\nWe employ a two-staged calibration approach for the underwater camera systems. In the first stage, we follow the procedure outlined in [74] to align the projection center of the camera with the center of the dome port. To achieve this, we remove the camera system from the AUV and submerge it halfway underwater in a water tank, ensuring that the camera observes the\nwater surface in a parallel manner. Subsequently, a calibration target (in our case, a checkerboard) is positioned in front of the camera, also submerged halfway underwater, as depicted in Fig. 6. By visually examining the continuity of the vertical edges of the checkerboard at the water boundary, we can determine if the camera is properly centered. The underlying principle is that if the camera is offset by a certain amount, refraction occurs, causing light rays from the submerged portion of the checkerboard to be refracted, while light rays from the above-water portion remain unaffected, resulting in a discontinuity in the vertical edges at the water surface. Since visual examination is subjective and relies on human observers, multiple photos of the fully submerged checkerboard in water are captured to to estimate the decentering offset parameters. This estimation can be done using the approach presented in [73]. This process is repeated, and mechanical adjustments are made until the decentering offset parameters fall within the acceptable range of toleration. Empirically, the values are typically set to be much less than 1mm in the sideward direction and in the range of a millimeter in the forward-backward direction.\nIn the second stage of the calibration process, the centered dome port camera is treated as a normal perspective camera, and we perform a standard checkerboard-based calibration procedure as described in [88], with the calibration target and the camera system submerged underwater. Example calibration images can be seen in Fig. 7. At this stage, the remaining decentering offset can be compensated by adjusting the camera intrinsics. Due to the large field of view of the AUV camera, we use the fisheye camera model for the calibration.\n4.3 Visual Seafloor Reconstruction\nNotations. We denote the absolute camera poses as cTiw = [ cRiw | ctiw ] , where cRiw and ctiw are the rotation and the translation of the i-th image. And cTiw transforms a 3D point in the world frame to the local camera frame. Then, we introduce the prior coordinate frame p to represent the vehicle body of the i-th image\nwhich is denoted as pTiw = [ pRiw | ptiw ] . In addition, we introduce pTc as the relative transformation from the camera frame to the prior frame. A simple illustration of the coordinate systems and their relations is shown in Fig. 8.\nHaving obtained the color normalized images, georeferenced navigation data, and calibrated camera intrinsics, we now introduce our proposed navigationaided hybrid SfM approach for achieving efficient and large-scale visual seafloor reconstruction. The overview of the approach is illustrated in Fig. 9.\nSimilar to previous SfM work from COLMAP [68], we first extract SIFT features and their descriptors across the entire image collection. Then, we perform spatial matching for each image within localized regions. This is possible by leveraging the absolute positions of each image from the prior poses. Afterwards, a geometric verification step is performed to compute the two-view geometries for all possible image pairs and a view graph that encodes the spatial distribution of the features is constructed. Note that in this process, outliers which do not agree with the estimated twoview geometries are eliminated by RANSAC. In order to effectively handle large-scale datasets and to mitigate the accumulation drift, we employ a divide-andconquer strategy similar to [15,89,5]. Using the view graph computed from the previous step, where images are represented as graph nodes and edges denote the number of inliers between image pairs, we apply a normalized graph-cut algorithm [18] to partition the scene into multiple overlapping clusters.\nLocal SfM. Subsequently, we perform (incremental) SfM on each local cluster concurrently. Traditional SfM yields reconstructions with arbitrary scales, necessitating the estimation of a similarity transformation between sub-reconstructions that share overlapped images. However, relying on a few connected images to\nestimate the similarity transformation is not robust, especially when the view graph is sparse. This limitation frequently results in inconsistencies and gaps between sub-reconstructions. To overcome this, we incorporate prior poses into the local SfM process, ensuring that the resulting reconstruction adheres to the same coordinate system and scale. When reconstructing the initial image pair, we begin by estimating the two-view geometry using the inlier matches, and obtain the relative rotation and translation between the image pair, with the translation vector normalized to unit length. We then set the first camera pose directly as its prior pose. Next, we scale the translation vector by the length obtained from the prior poses. Subsequently, we concatenate the relative pose with the first camera pose, yielding the second camera pose. By doing this, the resulting reconstruction is under the same coordinate system as the prior coordinate system. It is important to note that, the navigation data is provided in the prior coordinate frame pTw. However, to use this information in the camera frame, we must transform it by c\u2032Tw = ( pTc) \u22121 \u00b7 pTw. In our specific setup, the camera is positioned underneath the robot, and is rotated by 90\u25e6 around the Z-axis, as depicted in Fig. 4 and Fig. 8. Then, the offset between the camera body and the vehicle body is determined based on measurements\ntaken from the CAD drawing. We acknowledge that the actual values may deviate from the initial assumptions due to real-world variations. However, these values are used solely as an initialization and will be optimized during the subsequent bundle adjustment process.\nOnce the initial reconstruction is established, we proceed with bundle adjustment to refine and optimize the registered images along with the corresponding 3D structures. In this process, we supervise the optimization to mitigate drift by adding a pose prior term that penalizes the absolute difference between the optimized camera poses and their prior poses. In essence, the cost function being optimized is as follows: C = \u2211 i \u2211 j \u2225xji\u2212\u03c0(Xi, K, cTjw)\u22252+\u2225d ( cTjw, c\u2032Tjw ) \u22252\n(1)\nwhere xji means that the jth image observes the ith 3D point. K denotes the intrinsic parameters of the camera. We define the 6-dimensional residual vector d (Ti,Tj) as the absolute difference between two transformations:\nd (Ti,Tj) =\n[ \u03c1r \u00b7 2 \u00b7 vec(jqi)\n\u03c1t \u00b7j ti\n] (2)\nwhere vec(jqi) returns the real part of the quaternion representation of the rotation from frame i to frame j. \u03c1r \u2208 R3\u00d73 and \u03c1t \u2208 R3\u00d73 are the weighting factors, which can be obtained by taking the inverse of the covariance matrix associated with the prior poses. We follow the procedure in [68] to incrementally reconstruct the remaining images within the cluster.\nWeak-area Revisit. In the previous clustered local SfM reconstruction process, there may be images that are not reconstructed within any of the clusters, or image pairs that exhibit a significant number of inlier feature matches but lack shared reconstructed 3D points (see Fig. 10, top). These situation can arise due to the separation of images into different clusters during the scene clustering process or the inherent difficulty in reconstructing certain images. Some of the image pairs are even potential loop-closing candidates however not connected and aligned.\nTo address these issues, previous works have employed a graph-expansion step during the scene clustering phase to increase the number of overlapping images [15,89]. However, this technique may not be effective for images that are too challenging to be reconstructed. In contrast, we propose an alternative approach. Firstly, we identify the weakly reconstructed areas, which consist of images that remain unreconstructed within any of the clusters and weakly reconstructed image pairs.\nNext, we construct new clusters centered around these areas, including the target area and its surrounding images. Afterwards, we can perform parallel local SfM on each of them. Since the new clusters are smaller in size, re-performing local SfM on these limited sets of images does not significantly increase the overall computation time. However, it helps increasing the connectivity between sub-reconstructions and allows for multiple attempts to reconstruct challenging images, which is beneficial for the subsequent global pose graph optimization step. To identify the weak image pairs, we maintain an upgraded view-graph where we store the total number of common visible 3D points Np for each image pair. We then compare this information with the original view-graph obtained from feature matching, where we store the number of feature matches (Nm) for each image pair. Image pairs that satisfy Np < 0.2 \u00b7Nm and Nm > \u00b5 are considered as weak image pairs and are revisited for further reconstruction. Here, \u00b5 is a hyperparameter that determines whether an image pair contains a sufficient number of feature matches to be considered for reconstruction. It can be seen in Fig. 10,\nbottom, that the weak-area is revisited and these challenging images can also be well reconstructed.\nGlobal Pose Graph Optimization. We align different sub-reconstructions by optimizing a joint global pose graph, where vertices represent camera poses and edges represent relative transformations between two cameras. The pose graph contains three types of constraints: relative pose constraints, absolute pose constraints, and smooth motion constraints.\nTo establish the relative pose constraints, we collect all available image pairs within each sub-reconstruction that share common 3D points. We compute their relative poses and utilize them as edge measurements within the pose graph. This includes both sequential edges and loop-closing edges. For each vertex in the graph, we impose a weak absolute pose constraint using prior pose measurements. This constraint penalizes vertices if they deviate significantly from the prior poses, helping to maintain consistency with the initial pose estimates. Furthermore, for completeness reasons, we introduce a third local smooth motion constraint, which ensures that isolated vertices adhere to a plausible and coherent motion pattern within the reconstructed scene. This enable us to handle vertices that lack any relative constraints, such as images not reconstructed within any clusters, even after revisiting them.\nThe entire graph of camera poses is then optimized\nby minimizing the following cost function: C = \u03c1rel \u2211\n(i,j)\u2208R\n\u2225d ( jTi, jT\u0302i ) \u22252+\u03c1abs \u2211 j\u2208V \u2225d ( cTjw, c\u2032Tjw ) \u22252\n+ \u03c1smooth \u2211 i\u2208S \u2225d ( iTi\u22121, i+1Ti ) \u22252 (3)\nwhere R is the set of all relative edges, V is the set of all vertices and S is the set of vertices that have no edges connected to them. Again, the distance function d(\u00b7) measures the dissimilarity between the two transformations. The smooth motion term penalizes the dissimilarity between the relative transformation from vertex i\u2212 1 to i and the relative transformation from vertex i to i+1, effectively encouraging constant velocity. \u03c1abs, \u03c1rel and \u03c1smooth are the scalar multipliers to balance the influence of different constraints in the optimization process.\nTriangulation and Bundle Adjustment. We gather all feature tracks available from each subreconstruction and merge them based on their feature matching graph. The local SfM process incorporates robust outlier rejection mechanisms, such as RANSAC, in several reconstruction steps. Therefore, we can confidently consider these feature tracks as inliers. Next, we conduct re-triangulation on the combined feature\ntracks using the globally optimized camera poses obtained from the previous step to create a comprehensive scene structure. To further refine the reconstruction, we perform a global bundle adjustment, similar to the approach described in [68] to minimize the global reprojection error, ensuring that overall reconstruction is of high quality and accuracy.\nMulti-View Stereo. Once a sparse reconstruction is obtained, the registered images are processed using the open-source package OpenMVS [11] to generate a dense representation of the scene through Multi-View Stereo (MVS). Due to the computational limitations of processing a large number of images, we again employ a divide-and-conquer strategy. We divide the input reconstruction into several smaller subsets or \u201dchunks\u201d and each chunk is then processed independently using the MVS algorithm. Note that in this step, we substitute the original images by the color normalized images to compensate for the varying lighting effect in the final output mesh."
        },
        {
            "heading": "5 Evaluation",
            "text": "In this section, we evaluate the performance of our proposed pipeline using various datasets gathered by GEOMAR\u2019s AUVs under different sea conditions. To implement our approach, we utilize the state-of-art SfM software package COLMAP [68] as the underlying framework. We first integrate the navigation data into the incremental SfM process of COLMAP, and then we build upon it to implement our hybrid SfM approach. For the implementation details, we set the number of times to revisit the weak-area to 2. For the global pose graph optimization, we set \u03c1rel = 1.0, \u03c1abs = 0.001 and \u03c1smooth = 2.0.\nDatasets. To analyze the performance of our pipeline, we empirically categorize our datasets into different levels of difficulty based on factors such as the imaging conditions, image quality, visibility, water scattering, light variations, and the connectivity of the view graph. In each dataset, the AUV follows a classical lawn mower pattern, maintaining a stable flying altitude. Additionally, the AUV is planned to have sufficient sidetrack image overlap, and subsequently, a cross-track is planned to facilitate loop-closures. Figure 11 provides an overview of the datasets, showcasing four example images for each dataset. Additionally, the rightmost figure depicts the AUV trajectory during data collection. The camera trajectory is visualized in red, and the pink lines represent the connections between image pairs that share common visible points. The density of the pink lines indicates the strength of the view graph connectivity.\nBefore evaluating the datasets, we observe distinct characteristics between different datasets. In the Easy dataset (44m \u00d7 35m), the input images exhibit clear visibility and are well illuminated, resulting in a dense view graph connection, allowing for strong feature matching and robust reconstruction. In the Medium1 (45m \u00d7 42m), the water scattering is mild, but the image overlap is less dense compared to the Easy dataset in certain areas. The Medium2 (51m \u00d7 18m) dataset\nconsists of clear images, however, the target scene, which consists of a sea-grass area, lacks distinctive features. As a result, feature matches are limited, especially at the image border due to the varying light cone. This leads to a weaker side-track connectivity in the view graph. The Hard1 (8m \u00d7 21m) dataset presents challenges due to strong scattering in the images. In certain areas, the water conditions are extremely murky, making it difficult to find any features for reconstruc-\ntion. Consequently, these regions cannot be effectively reconstructed, despite the view graph being sufficiently dense. The Hard2 (442m \u00d7 133m) dataset represents an exceptionally challenging scenario where the AUV mapped a Maganness nodule field in water depths of more than 4000m to monitor the deep-sea impacts [60]. The target scene lacks distinctive features, and the presence of strong scattering and varying illumination further hinders feature matching. Despite a high ratio of side-track image overlap, the side-ward view graph connections remain poor.\n5.1 Results\nWe present the evaluation results of our approach on the given datasets and compare it to default COLMAP [68] as a baseline work. We refer to our implementation of incremental SfM supervised by the navigation data as INC-NAV, while HYBRID represents our proposed hybrid SfM approach. We therefore refer to the default, non-expert setting of COLMAP as COLMAPDEF. However, it is known that COLMAP was originally designed for reconstructing internet photo collections, their default settings might not be suitable for robotic mapping scenarios. We make certain modifications in the settings of COLMAP to make it better suited for underwater robotic missions, which is referred as COLMAP-PRO. Specifically, we set the minimum number of inliers for pose estimation to be 6, allowing for the reconstruction of images with a limited number of features, ensuring completeness in the reconstruction. Moreover, we set the maximum number of trials for re-triangulation to be 5, which allows us to make multiple attempts to re-triangulate image pairs and this helps better to close potential loops. Since COLMAP produces arbitrarily scaled reconstructions, we employ robust similarity transformation estimation to align the reconstruction with the given navigation trajectory for metric evaluations.\nA visual comparison of the evaluation results on the datasets can be seen in Fig. 12 and Fig. 13. In addition, quantitative evaluation results are given in Table 1. The table includes the number of reconstructed images Nc and the number of images N available in the dataset. L represents the average track length of a single 3D point. To evaluate the accuracy of the reconstruction, we report the Reprojection Error (RE) in pixels and the RMSE of the Absolute Translational Error (ATE) between the reconstructed camera poses and the given navigation data. Regarding the running time, we record the duration of the SfM process in minutes, excluding the feature extraction and matching steps. Note however that, neither the Reprojection Error nor the Ab-\nsolute Translational Error can serve as definitive indicators of the reconstruction quality. The reprojection error is an internal estimate that the bundle adjustment optimization is based on, and the ATE measures the similarity between the reconstructed camera poses and the given navigation data, which we use to guide our pose estimates during reconstruction. As a result, it is expected that the reported ATE values in our approach would be relatively low. Although the navigation data does not represent the ground truth of the vehicle trajectory, our ablation study reveals that it exhibits global reasonability but local inaccuracies. As a result, although it cannot be used directly for the reconstruction process, it can serve as a reference trajectory.\nAs can be seen from the figures that our approach consistently produces visually more appealing results across almost all datasets. In terms of running time, our approach demonstrates faster reconstruction times compared to the other methods shown in the table, except for the Hard1 dataset. The reason behind this discrepancy is that the Hard1 dataset contains a relatively small number of images, around 1000, which limits the advantage of the divide-and-conquer strategy in terms of efficiency. In terms of statistics, enabling the PROsetting leads to an increased number of reconstructed images. Our approach achieves the highest number of reconstructed images with slightly increased reprojection error, which could be attributed to the fact that global SfM approaches are less susceptible to error accumulation as compared to incremental approaches. Regarding the increased reprojection error, an explanation is that COLMAP solely optimizes for the reprojection error in bundle adjustment, whereas we incorporate additional information by penalizing the disparity between the reconstructed camera poses and their prior poses.\nIn terms of quality, the original COLMAP performs already very well in the Easy dataset even with a dataset size of nearly 5000 images. In scenarios where the imaging conditions are favorable and the view graph is dense, standard incremental SfM can still produce satisfactory and reliable reconstructions. Our approach achieves similar reconstruction quality but with a reduced processing time. However, as the imaging conditions underwater worsen, especially in the deep-sea environments, the level of difficulty increases, which leads to a decrease of the performance of standard incremental SfM, which finally results in inconsistent reconstructions. For instance, in the Medium1 and Medium2 dataset (as depicted in Fig. 12), several inconsistencies in the camera trajectory can be observed, indicated by the green boxes. These inconsistencies can mostly be explained by the accumulation of drift during\nthe incremental reconstruction process. In theMedium2 dataset, COLMAP produces a curved reconstruction of the seafloor, while in the INC-NAV case, the reconstruction appears to be flatter due to the supervision\nby the navigation data. However, even with navigation data supervision, the accumulated error can still be significant, which in turn may prevent loop-closure. For the Hard1 dataset, we specifically employed Domain-\nsize Pooling (DSP-SIFT) [19] due to the poor performance of the default SIFT feature. The images in this dataset suffer from low illumination and strong scattering, which adversely affects feature extraction and\nmatching. Despite neither approach achieving a complete reconstruction in this challenging dataset, our approach still demonstrates superior reconstruction quality. In the case of the Hard2 dataset which contains\nmore than 10000 images and challenging imaging conditions, the default setting of COLMAP fails to generate a valid output, with only 8 images being reconstructed. By enabling the PRO-setting, COLMAP is able to reconstruct a larger number of images, achieving a low reprojection error of 0.54 pixels, which is the lowest among all approaches. However, despite the low reprojection error, the visual appearance of the reconstruction in Fig. 13 clearly indicates significant errors, including positional and scale drift. INC-NAV, on the other hand, is able to address the scale drift issue but produces a poor camera trajectory. In contrast, our approach achieves both global consistency and local accuracy in the reconstruction. This conclusion can be supported by observing the reconstructed model of the seafloor, which shows the traces left behind by the Manganese nodule mining vehicles during the mining operation (see also the textured mesh in Fig. 21).\n5.2 Ablation Study"
        },
        {
            "heading": "On the Accuracy of the Navigation and the Pose",
            "text": "Graph. To assess the accuracy of the navigation data obtained from a deep-sea AUV, we conduct an experiment to directly triangulate (DT) 3D points based on the given poses. The underlying premise is that if the navigation data is sufficiently accurate, it should enable direct 3D reconstruction without using Structure-fromMotion. Instead of presenting the uncertainty measures of the navigation data, this approach allows us to evaluate the consistency between the navigation data and the actual visual measurements.\nIn addition to the direct triangulation using the prior poses, we also perform direct triangulation of 3D points using camera poses obtained from the global pose graph optimization (PGO) in our proposed hybrid SfM approach. This allows us to evaluate the accuracy of the camera poses after the global motion averaging step. Since we perform local SfM on clusters, we are able to collect inlier 3D points in this step. Therefore, We additionally report the results of directly triangulating the true inlier feature matches instead of using all available feature matches, which is referred to as PGO(inlier). It is important to note that the final global bundle adjustment is not performed in this evaluation.\nThe evaluation results are shown in Table 2, where RE stands for the Reprojection Error in pixels and L is the average track length. From the table, it can be observed that directly triangulating 3D points from the navigation data alone was not very successful, resulting in an average reprojection error of 29.01 pixels. However, when applying the same procedure to camera\nposes obtained from the global pose graph optimization, an average reprojection error of 1.82 pixels was achieved. Furthermore, by excluding the outliers, an average reprojection error of 1.20 pixels can be achieved, which is only possible in our proposed approach. These results indicate a significant improvement in the accuracy of the computed camera poses and serve as a good starting point for the final bundle adjustment.\nOn Weak-area Revisit. The experiment conducted to analyze the effectiveness of the proposed weak-area revisiting is presented in Fig. 14. The topleft figure shows the reconstruction results of the Hard2 dataset after the global pose graph optimization step (PGO) without weak-area revisiting, while the bottomleft figure shows the results with weak-area revisiting. The green boxes in the figures indicate the detected weak-areas where the images receive a limited number of relative pose constraints. However, after revisiting these weak-areas, the challenging images can also be well-reconstructed, as evident from the improved reconstruction quality in the bottom-left figure. The right part of the figure presents a histogram of the number of relative pose constraints for each image in the pose graph optimization. It can be observed that weak-area revisiting effectively eliminates the weak areas by 100%, ensuring a more robust and consistent pose graph optimization.\nOn Color Normalization. A recent study by Grimaldi et al. [29] demonstrated that pre-processing approaches resulted in improvements in SLAM performance in visually challenging underwater environments. We additionally conducted an experiment to perform reconstruction using the color normalized images and compare it against reconstruction using the original color images. As can be seen from Fig. 15, the reconstruction using color normalized images exhibits improved connectivity in the resulting view graph, especially between side-tracks. This can be attributed to the ability of color normalization to effectively reduce nonuniform illumination in the images, thereby enhancing feature matching (see also Fig. 16). Moreover, the use of more sophisticated deep-learning-based feature matching methods, specifically designed for underwater conditions, has the potential to further enhance the quality of visual mapping in deep-sea environments. However, we leave this for future investigation.\n5.3 Meshing, Texturing and Orthophoto\nFig. 17 to Fig. 21 display the final results of the last step of the visual mapping pipeline on the given datasets. We apply chunk-based dense Multi-View Stereo (MVS) estimation and meshing using the OpenMVS [11] package\nTable 1 Quantitative evaluation results on the self-gathered AUV datasets. Nc denotes the number of reconstructed images and N denotes the number of images available in the dataset. L represents the average track length of a 3D point and T records the running time of the SfM process in minutes. To evaluate the reconstruction accuracy, we report the average reprojection error in pixels and the RMSE of the Absolute Translational Error (ATE) between the reconstructed camera poses and the given navigation.\nDatasets N COLMAP-DEF[68] COLMAP-PRO INC-NAV HYBRID\nNc L RE ATE T Nc L RE ATE T Nc L RE ATE T Nc L RE ATE T\nEasy 4746 4710 5.09 0.68 0.407 4123.53 4710 5.09 0.68 0.405 4581.42 4710 5.09 0.68 0.165 5251.75 4710 5.12 0.70 0.183 1021.692 Medium1 5752 5635 4.28 0.56 0.526 1284.55 5740 4.25 0.56 0.515 1300.11 5740 4.28 0.53 0.568 1551.92 5740 4.31 0.58 0.325 344.25 Medium2 2977 2872 3.69 0.51 0.513 294.16 2881 3.68 0.52 0.531 519.61 2881 3.68 0.55 0.114 511.50 2881 3.71 0.54 0.183 123.20 Hard1 1065 808 4.25 0.75 1.182 26.51 847 4.32 0.75 1.348 36.68 847 4.47 0.52 0.095 68.11 848 4.51 0.53 0.095 60.41 Hard2 10715 8 - - - - 10479 3.24 0.54 101.08 206.70 10025 3.15 0.69 0.476 458.23 10713 3.23 0.62 0.487 126.41\nPGO without Weak-area Revisit\nPGO with Weak-area Revisit Weak-area\nFig. 14 A comparison of the pose graph optimization results without weak-area revisiting (top-left) and with weak-area revisiting (bottom-left). The green boxes indicate the weak-areas before and after revisiting. The blue lines represent the connected image pairs in the view graph. Right part of the figure shows a histogram of the number of pose graph constraints for each image in the pose graph optimization. A higher number of constraints for an image indicates higher robustness and accuracy.\non both the original color images and on the normalized color images. It is evident that the lighting effects are largely compensated in the final output mesh, resulting in a clearer and more visually appealing scene structure. The zoomed views indicate that we are able to achieve\na high-resolution and detailed 3D reconstruction of the seafloor."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we presented a fully automated, and comprehensive workflow for mapping large areas of the seafloor, leveraging the recent developments in both underwater imaging and visual mapping techniques. Our main focus was on achieving a geometrically consistent and accurate reconstruction of the seafloor. To this end, we carefully consider refraction to avoid reconstruction biases and proposed a hybrid mapping approach that combines the benefits of SLAM and the accuracy of Structure-from-Motion. Through a thorough evaluation on multiple datasets with varying characteristics,\nwe demonstrated the effectiveness and accuracy of our system. The proposed work will serve as a baseline for future developments to enable more robust and accurate deep-sea-specific visual mapping.\nAcknowledgements This publication has been funded by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) Projektnummer 396311425, through the Emmy Noether Programme. We are also grateful for support from the Chinese Scholarship Council (CSC) for M. She (202006050015) and Y. Song (201608080215). We would also like to thank CSSF, Schmidt Ocean Institute, GEOMAR AUV and JAGO Team for providing the underwater image materials."
        }
    ],
    "title": "Efficient Large-scale AUV-based Visual Seafloor Mapping",
    "year": 2023
}