{
    "abstractText": "In recent years, significant advances have been made in novel modelling tasks centred on aspectual sentiment analysis. This task shows relatively good results by extracting aspect-category-opinion-sentiment (ACOS) quaternions. To further improve the performance of the model in ACOS quadruplet extraction, this study proposes an integrated approach combining encoder-decoder networks, multilevel perceptrons (MLP), supervised contrast loss (SCL) and representation structure (ES) metrics. The integration of these techniques aims to improve the generation of more informative features by propagating aggregated representations from the encoder to the multilevel perceptron. In addition, contrast learning methods are used to encourage models to generate discriminable input representations on key attributes, thereby facilitating distinguishable input representations that facilitate the task of second-level prediction. In addition, an expression structure output target was introduced which was combined into an autoregressive encoder-decoder model to generate quartets, and empirical results confirmed the excellent performance of the model on three ACOS datasets. The experiment effectively established the feasibility of the task and highlighted its strengths in generating aspects for sentiment analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanchao CHEN"
        },
        {
            "affiliations": [],
            "name": "Cuiju LUAN"
        }
    ],
    "id": "SP:d0732197caab78547df8b35d484d905343edfac5",
    "references": [
        {
            "authors": [
                "H JIANG",
                "P HE",
                "W CHEN"
            ],
            "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization[J/OL",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "M ALHARBI A S",
                "E. DE DONCKER"
            ],
            "title": "Twitter sentiment analysis with a deep neural network: An enhanced approach using user behavioral information[J/OL",
            "venue": "Cognitive Systems Research,",
            "year": 2019
        },
        {
            "authors": [
                "Z CHEN",
                "T. QIAN"
            ],
            "title": "Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis[C/OL]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "venue": "Online: Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "M HU",
                "Y PENG",
                "Z HUANG"
            ],
            "title": "Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification[J/OL",
            "year": 2019
        },
        {
            "authors": [
                "H DAI",
                "Y. SONG"
            ],
            "title": "Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision[J/OL",
            "year": 2019
        },
        {
            "authors": [
                "H CAI",
                "R XIA",
                "J. YU"
            ],
            "title": "Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions[C/OL]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
            "year": 2021
        },
        {
            "authors": [
                "W ZHANG",
                "Y DENG",
                "X LI"
            ],
            "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation[J/OL",
            "year": 2021
        },
        {
            "authors": [
                "D HOANG C",
                "V DINH Q",
                "H. TRAN N"
            ],
            "title": "Aspect-Category-Opinion-Sentiment Extraction Using Generative Transformer Model[C/OL]//2022 RIVF International Conference on Computing and Communication Technologies (RIVF)",
            "year": 2022
        },
        {
            "authors": [
                "Z LI",
                "Y ZOU",
                "C ZHANG"
            ],
            "title": "Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training[J/OL",
            "year": 2021
        },
        {
            "authors": [
                "H SEDGHAMIZ",
                "S RAVAL",
                "E SANTUS"
            ],
            "title": "SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations[J/OL",
            "year": 2021
        },
        {
            "authors": [
                "Y MAO",
                "Y SHEN",
                "J YANG"
            ],
            "title": "Seq2Path: Generating Sentiment Tuples as Paths of a Tree[C/OL]//Findings of the Association for Computational Linguistics: ACL 2022",
            "venue": "Dublin, Ireland: Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "R MENG",
                "X YUAN",
                "T WANG"
            ],
            "title": "An Empirical Study on Neural Keyphrase Generation[J/OL",
            "year": 2021
        },
        {
            "authors": [
                "L XU",
                "H LI",
                "W LU"
            ],
            "title": "Position-Aware Tagging for Aspect Sentiment Triplet Extraction[J/OL",
            "year": 2021
        },
        {
            "authors": [
                "H WAN",
                "Y YANG",
                "J DU"
            ],
            "title": "Target-Aspect-Sentiment Joint Detection for Aspect-Based Sentiment",
            "venue": "Analysis[C/OL]//Proceedings of the AAAI Conference on Artificial Intelligence: Vol",
            "year": 2020
        },
        {
            "authors": [
                "J PEPER J",
                "L. WANG"
            ],
            "title": "Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure[J/OL",
            "venue": "Quadruple Extraction of Aspect-Category-Emotion-Opinion",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "results by extracting aspect-category-opinion-sentiment (ACOS) quaternions. To further improve the performance of the model in ACOS quadruplet extraction, this study proposes an integrated approach combining encoder-decoder networks, multilevel perceptrons (MLP), supervised contrast loss (SCL) and representation structure (ES) metrics. The integration of these techniques aims to improve the generation of more informative features by propagating aggregated representations from the encoder to the multilevel perceptron. In addition, contrast learning methods are used to encourage models to generate discriminable input representations on key attributes, thereby facilitating distinguishable input representations that facilitate the task of second-level prediction. In addition, an expression structure output target was introduced which was combined into an autoregressive encoder-decoder model to generate quartets, and empirical results confirmed the excellent performance of the model on three ACOS datasets. The experiment effectively established the feasibility of the task and highlighted its strengths in generating aspects for sentiment analysis.\nKeywords. Multilevel perceptron, Supervised contrast loss, Features, Sentiment prediction"
        },
        {
            "heading": "1. Introduction",
            "text": "Aspect-based sentiment analysis (ABSA) is a specialized task that aims to extract subtle sentiment information from text data. Its main goal is to identify and analyze the sentiment orientations associated with different aspects mentioned in the text. Compared to traditional sentiment analysis methods, ABSA enables a more accurate understanding of sentiment information by distinguishing sentiment orientations attributed to various aspects and examining the text in greater detail. Initially, ABSA revolved around the classification of sentences to determine their positive or negative nature [1]. Subsequently, it evolved to include the extraction of aspects and opinions expressed in sentences [2]. This development process gave rise to several subtasks, ranging from the extraction of independent aspects and opinions to the extraction in combination with categories [3]. In addition, there is a task to extract multiple opinion targets directly from the sentence, using span to categorize the corresponding polarity [4]. Finally, aspect,\n1 Corresponding author: Yuanchao CHEN, Shanghai Maritime University; e-mail: cy16637617870@163.com\nopinion and sentiment are extracted as a coherent triad, which is a means to extract more substantive and meaningful information simultaneously [5]. However, some studies nowadays consider only explicit levels of views and opinions, which leads to the neglect of implicit views and aspects; in fact, product reviews contain a large number of implicit aspects and opinions [6].\nIn recent years, a pioneering subtask has been introduced in the area of aspect-based sentiment analysis (ABSA), the aspect-category-sentiment-opinion (ACOS) quadruple extraction. To advance the task of ACOS quaternion extraction, I introduced new improvements by leveraging cutting-edge T5-based techniques. These improvements enable us to generate structured and interpretable ACOS quadruplet predictions for a given text [7]. In this study, I propose MLP-SCL-ES, which is an improved variant of the method that incorporates two novel modifications. By using our proposed model for extensive testing, we performed a comparative analysis with the previous model. By conducting experiments on the RESTaurant, LAPTOP and LAPTOP_L1 datasets. The results show noteworthy improvements, with the F1 scores of ACOS improving by 0.24, 1.10, and 1.11 percentage points on the three data sets, respectively. Our method has made substantial progress on these specific tasks."
        },
        {
            "heading": "2. Related Work",
            "text": "To extract information from these review sentences, the ACOS task incorporates categories for aspect classification based on the sentence's context. Categories not only aid in identifying explicit aspects but also demonstrate notable effectiveness in detecting implicit aspects within the sentence (refer to Figure 1). Regarding opinions, the sentiment expressed in the sentence can be considered as the opinion's category (either negative or neutral) [8]. Illustrated in Figure 1, within the two review sentences \"the food was great, the margaritas too but the waitress was too busy being nice to her other larger party than to take better care of my friend and me. go to open sesame!!!,\" the aspect is \" margaritas,\" its corresponding category is \"Food,\" the expressed opinion is \"great,\" and the associated sentiment is \" Positive.\" Additionally, the figure depicts the extraction of three quadruples: margaritas-Food-great-Positive, waitress-Service-busy-Negative, and sesame-Restaurant-NULL-Positive (incorporating implicit opinions). Numerous similar sentences exist within the reviews."
        },
        {
            "heading": "3. Method",
            "text": "In this section, I present the changes made about the model and get better results about its construction (1) MLP-SCL, a joint multilayer perceptron (MLP) specific supervised comparative learning (SCL) target. (2) The output is based on a specific target structure format, Figure 2 shows the ACOS task and the four components.\nThere are two reasons for using the model for this task: The model uses a pre-trained T5 model that retains most of the architecture of the\noriginal Transformer, so that fine-tuning it can solve the subtasks well.\nThe model is more flexible. It can solve different subtasks, and does not require any modification of the core algorithm or additional models when solving different subtasks."
        },
        {
            "heading": "3.1. MLP-SCL Supervised Contrastive Loss",
            "text": "This model is improved by the introduction of MLP-SCL, which projects the final hidden state of the encoder obtained by summation and the state after dropout to a 1024- dimensional feature representation respectively through a non-linear transformation of the multi-layer perceptron to improve modelling of the input data, allowing better capture of complex relationships and non-linear features in the input data compared to a singlelayer linear model. Improving the model's ability to learn tasks with paradigm-level aspects, sentiment and opinion features. In addition, the overfitting problem is mitigated by adding a dropout layer to improve the generalisation of the model. Figure 2 shows the aspect, sentiment and opinion labels."
        },
        {
            "heading": "3.2. Representation Generation Process",
            "text": "The process of generating representations involves passing the aggregated encoder representation, Encode(yi), through a dedicated fully connected layer for each feature c\n(Aspect, Sentiment, Opinion), resulting in the generation of the representation wci [9]. In the experimental setup, an initial reduction in dimensionality is achieved by employing a fully connected layer. Subsequently, the processed data undergoes further transformations through a function before being subjected to another fully connected layer, thereby enabling the model to acquire more comprehensive and intricate feature representations."
        },
        {
            "heading": "3.3. SCL Applications",
            "text": "Supervised comparative learning encourages models to maximize the representational similarity between identically labeled exemplars and minimize the representational similarity between differently labeled exemplars. We follow the general SCL formulation [10]. where for feature c and training example yi, the loss when training a small batch of data N is:\nsin( , )/\nsin( , )/ p ( )\n( )\n-1 = log\n| ( ) |\nci cp\nci cq\nw w\nw w P i q Q\nc i\ni\nL e p i e\n(1)\nTo ensure that each example yi N has at least one identically labeled example for comparison, we first extend M using one replacement view per mini-batch element with replacement probability p = 0.1. This becomes N*. B (i) N*\\yi is all other examples in the extended mini-batch, and P (i) {p Q(i) : kcp=kci} is the set of examples with matching labels subset. where is the temperature value, = 0.25."
        },
        {
            "heading": "3.4. Final Training",
            "text": "By adding to the existing decoder cross-entropy loss L the loss of aspect, sentiment and opinion features generated through contrast learning:\n[0] 1 aspect_SCL 2 sentiment_SCL 3 opinion_SCLall outputsL L L L L (2)\nwhere is the loss weighting factor, which determines the counterexample of severe\npunishment severity. For the REST and LAPTOP dev sets, adjust 1 = 2 = 3 = =0.05, and for the LAPTOP-L1 dev set, adjust 1 = 2 = 3 = =0.005."
        },
        {
            "heading": "3.5. Output Target Based on Expression Structure",
            "text": "In the previous experiments done, by reading others' papers They highlight the main emotional elements in the target sentence during the paraphrase. Based on this motivation, thewe linearized the sentiment quadruple q = (c, a, o, p) into a natural sentence, as follows: {c, a, o, p}, which maps the sentiment elements from the original format to the natural language form. By employing a suitable projection function, a structured sentiment cube q can be converted into an equivalent natural language sentence. For input sentences with multiple sentiment cubes quaternions, each quaternion q is first linearised into a natural sentence, and then these sentences are joined together with a special notation [SSEP]. to form the final target sequence, which contains all sentiment quadruples of sentences. [7].\nPc (ci) is Ps(si) because Pa(ai) is Po(oi) (3)"
        },
        {
            "heading": "3.6. My Target Generation Format:",
            "text": "By modifying the previous structured generation format for the interpretation, the following target generation format was obtained:\nPc(ci)| the Pa(ai) is Po(oi) |Ps(si) (4)\nAbove I redefine Pc(ci) as a mapping from the original category ci (e.g., \"laptop function\") to the natural category description (e.g., \"laptop function\"), which for aspects and opinions is described as the pa(ai) is Po(oi). The final sentiment was simply linearized using [\"positive\", \"neutral\", \"negative\"], because for some sentence comments, the sentiment is not positive, but the negative sentiment is implied (e.g., \"The color of this dish is okay, right?\"), which only indicates that the color is okay, but implies that the taste is not. The quadruplet was output in a top-down order, and the format of the quadruplet (c,a,o,s) was changed [11]. In order to obtain the predicted sentiment polarity, the prediction needs to be based on the previous aspects of sentiment and the output of opinion (a,o), and the quadruplet structure is divided by \"|\" to reduce ambiguity and facilitate the mapping of predictions. Some sentences prediction results have multiple outputs, when sorting is needed [12]."
        },
        {
            "heading": "4. Experiment",
            "text": "In this section, I detail the experimental procedure on the target task, including the dataset in Table 1, a comparison with several other models on the ACOS task, and an experimental evaluation. The sentences, which contain implicit aspects and opinions, have been labelled, and LAPTOP-L1 differs from LAPTOP in that it uses only the 21 top-level categories from the two-level category hierarchy in LAPTOP for its category labels. Comparing my technique with the remaining four techniques, the performance is evaluated in relation to their respective performances JET-ACOS One of the most advanced aspect-emotion triple extraction methods,\nwhich introduces an end-to-end framework for this task. [13]. TAS-BERT-ACOS model incorporates the TASBERT design two-step pipeline approach to extracting triplets [14]. Extract-Classify-ACOS The model performs aspect-opinion co-extraction and predicts category-sentiment based on the extracted aspect-opinion pairs. Finally the category and sentiment connections are executed. Seq2Path This model generates ACOS quartets as paths through the tree, supports multiple quartets by multibeam search, and filters candidates by learned discriminator tokens [11]. GEN-NAT-SCL Generative aspect sentiment analysis by supervised contrast loss metrics [15]. MLP-SCL-ES Finally, I propose the altered scheme with three components, MLP, SCL and ES. For our backbone model we used the 770M T5-large as a pre-trained raw encoder\ndecoder model."
        },
        {
            "heading": "5. Main Results",
            "text": "I use the F1 score as an evaluation metric for the quadruple extraction task (ACOS), where correct extraction requires that all tuples are correct."
        },
        {
            "heading": "5.1. Overall Performance on ACOS",
            "text": "Table 2 reports the extraction performance of six different systems on the ACOS task, the model performs well on this task. and it can be seen that JET-ACOS and TAS-BERTACOS achieve comparable F1 performance:the former performs better on the RESTaurant dataset, and the latter performs better on the LAPTOP dataset.ExtractClassify-ACOS achieves comparable F1 performance compared to the first two baseline systems, the performance is improved significantly. On Restaurant-ACOS, it outperformed JET-ACOS by 5.60 percentage points; on Laptop-ACOS, it outperformed TAS-BERT-ACOS by 8.49 percentage points. Again, implementing the ACOS task on the last three methods clearly outperforms the first three systems, where Seq2Path performs slightly worse, probably due to the limitations of its beam search candidate pruning method, and the lower GEN-NAT-SCL results are probably due to the relatively poor handling of the implicit aspects, in contrast, my method MLP-SCL-ES performs relatively well can be used in a sequence to output all quaternions."
        },
        {
            "heading": "6. Mlp-Scl-Es Ablation Study",
            "text": "I ablated the MLP-SCL-ES model by retaining the various components of the technique, including each ES target output format enhancement, SCL loss, and MLP enhancement. Table 3 reports my ablation results."
        },
        {
            "heading": "6.1. MLP-SCL Ablation",
            "text": "By ablating the MLP-SCL component of the component, where ablating the MLP resulted in an average decrease of 0.90 percentage points, ablating the Aspect SCL resulted in an average decrease of 0.92 percentage points, ablating the Opinion SCL resulted in an average decrease of 1.26 percentage points, and ablating the Sentiment SCL resulted in an average decrease of 0.36 percentage points, it can be seen that the opinion loss had the largest and most influential average decrease, while aspect loss and sentiment loss were also reduced respectively due to the consideration of implied language, in addition I eliminated all SCL losses and all results were significantly reduced after the removal of these components."
        },
        {
            "heading": "6.2. ES Ablation",
            "text": "In ablation experiments on ES (Expression structure), removing all expression structures, including the sequential output quadruplets mentioned in the text, revealed an average decrease of 1.77 percentage points in results, showing that the correct target structure is also important."
        },
        {
            "heading": "7. Conclusion",
            "text": "This paper presents MLP-SCL-ES, an innovative approach that integrates Multi-Layer Perceptron (MLP), Super-vised Contrastive Learning (SCL), and an expressive target output format (ES) tailored for the Aspect Quadruple Extraction (ACOS) task. The MLP architecture, comprising stacked perceptron layers, enables the model to capture intricate\nrelationships and nonlinear features within the input data, facilitating comprehensive feature extraction and representation. This empowers the model to effectively learn complex data patterns. SCL, a variant of super-vised contrastive learning, is utilized to enhance the representation of examples, leading to improved performance in downstream tasks. The ES format establishes a specific structure for ACOS targets, which, when combined with the aforementioned techniques, achieves state-of-the-art results. The primary focus of this work lies in introducing a new task, employing relatively simple baseline systems that provide ample opportunities for further enhancements. It is anticipated that future research will propose more robust advancements in this domain."
        }
    ],
    "title": "Quadruple Extraction of Aspect-Category- Emotion-Opinion Through Comparative Learning",
    "year": 2023
}