{
    "abstractText": "Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains, in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a per-topic and inter-topic basis. These make multi-domain stance detection a challenging task, requiring standardization and domain adaptation. To overcome this challenge, we propose Topic Efficient StancE Detection (TESTED), consisting of a topic-guided diversity sampling technique and a contrastive objective that is used for fine-tuning a stance classifier. We evaluate the method on an existing benchmark of 16 datasets with in-domain, i.e. all topics seen and out-of-domain, i.e. unseen topics, experiments. The results show that our method outperforms the state-of-the-art with an average of 3.5 F1 points increase in-domain, and is more generalizable with an averaged increase of 10.2 F1 on out-of-domain evaluation while using \u2264 10% of the training data. We show that our sampling technique mitigates both interand per-topic class imbalances. Finally, our analysis demonstrates that the contrastive learning objective allows the model a more pronounced segmentation of samples with varying labels.",
    "authors": [
        {
            "affiliations": [],
            "name": "Erik Arakelyan"
        },
        {
            "affiliations": [],
            "name": "Arnav Arora"
        },
        {
            "affiliations": [],
            "name": "Isabelle Augenstein"
        }
    ],
    "id": "SP:49b90cf9aada09ecf3f44d073a956015949614da",
    "references": [
        {
            "authors": [
                "Ehud Aharoni",
                "Anatoly Polnarov",
                "Tamar Lavee",
                "Daniel Hershcovich",
                "Ran Levy",
                "Ruty Rinott",
                "Dan Gutfreund",
                "Noam Slonim."
            ],
            "title": "A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics",
            "venue": "Proceedings of",
            "year": 2014
        },
        {
            "authors": [
                "Abeer Aldayel",
                "Walid Magdy."
            ],
            "title": "Your Stance is Exposed! Analysing Possible Factors for Stance Detection on Social Media",
            "venue": "Proc. ACM Hum.-Comput. Interact., 3(CSCW).",
            "year": 2019
        },
        {
            "authors": [
                "Emily Allaway",
                "Kathleen McKeown."
            ],
            "title": "ZeroShot Stance Detection: A Dataset and Model using Generalized Topic Representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8913\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Dimo Angelov."
            ],
            "title": "Top2vec: Distributed representations of topics",
            "venue": "ArXiv preprint, abs/2008.09470.",
            "year": 2020
        },
        {
            "authors": [
                "Isabelle Augenstein",
                "Tim Rockt\u00e4schel",
                "Andreas Vlachos",
                "Kalina Bontcheva."
            ],
            "title": "Stance detection with bidirectional conditional encoding",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 876\u2013885,",
            "year": 2016
        },
        {
            "authors": [
                "Roy Bar-Haim",
                "Indrajit Bhattacharya",
                "Francesco Dinuzzo",
                "Amrita Saha",
                "Noam Slonim."
            ],
            "title": "Stance classification of context-dependent claims",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2017
        },
        {
            "authors": [
                "Bjorn Barz",
                "Joachim Denzler."
            ],
            "title": "Deep learning on small datasets without pre-training using cosine loss",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1371\u20131380.",
            "year": 2020
        },
        {
            "authors": [
                "Alina Beygelzimer",
                "Sanjoy Dasgupta",
                "John Langford."
            ],
            "title": "Importance weighted active learning",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of",
            "year": 2009
        },
        {
            "authors": [
                "David M. Blei",
                "Andrew Y. Ng",
                "Michael I. Jordan."
            ],
            "title": "Latent Dirichlet Allocation",
            "venue": "Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver,",
            "year": 2001
        },
        {
            "authors": [
                "Filip Boltu\u017ei\u0107",
                "Jan \u0160najder."
            ],
            "title": "Back up your Stance: Recognizing Arguments in Online Discussions",
            "venue": "Proceedings of the First Workshop on Argumentation Mining, pages 49\u201358, Baltimore, Maryland. Association for Computational Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Buchert",
                "Nassir Navab",
                "Seong Tae Kim."
            ],
            "title": "Exploiting Diversity of Unlabeled Data for LabelEfficient Semi-Supervised Active Learning",
            "venue": "2022 26th International Conference on Pattern Recognition (ICPR), pages 2063\u20132069. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Sihao Chen",
                "Daniel Khashabi",
                "Wenpeng Yin",
                "Chris Callison-Burch",
                "Dan Roth."
            ],
            "title": "Seeing things from a different angle: Discovering diverse perspectives about claims",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Costanza Conforti",
                "Jakob Berndt",
                "Mohammad Taher Pilehvar",
                "Chryssi Giannitsarou",
                "Flavio Toxvaerd",
                "Nigel Collier."
            ],
            "title": "Will-they-won\u2019t-they: A very large dataset for stance detection on Twitter",
            "venue": "Proceedings of the 58th Annual Meeting of the Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Bo Dai",
                "Dahua Lin."
            ],
            "title": "Contrastive Learning for Image Captioning",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 898\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Rajshekhar Das",
                "Yu-Xiong Wang",
                "Jos\u00e9 MF Moura."
            ],
            "title": "On the importance of distractors for few-shot classification",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9030\u20139040.",
            "year": 2021
        },
        {
            "authors": [
                "Leon Derczynski",
                "Kalina Bontcheva",
                "Maria Liakata",
                "Rob Procter",
                "Geraldine Wong Sak Hoi",
                "Arkaitz Zubiaga."
            ],
            "title": "SemEval-2017 task 8: RumourEval: Determining rumour veracity and support for rumours",
            "venue": "Proceedings of the 11th International",
            "year": 2017
        },
        {
            "authors": [
                "William Ferreira",
                "Andreas Vlachos."
            ],
            "title": "Emergent: a novel data-set for stance classification",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "John Giorgi",
                "Osvald Nitski",
                "Bo Wang",
                "Gary Bader."
            ],
            "title": "DeCLUTR: Deep contrastive learning for unsupervised textual representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Kyle Glandt",
                "Sarthak Khanal",
                "Yingjie Li",
                "Doina Caragea",
                "Cornelia Caragea."
            ],
            "title": "Stance detection in COVID-19 tweets",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Ivan Habernal",
                "Henning Wachsmuth",
                "Iryna Gurevych",
                "Benno Stein."
            ],
            "title": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Karen Hambardzumyan",
                "Hrant Khachatrian",
                "Jonathan May."
            ],
            "title": "WARP: Word-level Adversarial ReProgramming",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Hanselowski",
                "Avinesh PVS",
                "Benjamin Schiller",
                "Felix Caspelherr",
                "Debanjan Chaudhuri",
                "Christian M. Meyer",
                "Iryna Gurevych."
            ],
            "title": "A Retrospective Analysis of the Fake News Challenge StanceDetection Task",
            "venue": "Proceedings of the 27th Inter-",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Hanselowski",
                "Christian Stab",
                "Claudia Schulz",
                "Zile Li",
                "Iryna Gurevych."
            ],
            "title": "A richly annotated corpus for different tasks in automated factchecking",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),",
            "year": 2019
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "Cross-domain labeladaptive stance detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9011\u20139028, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "A Survey on Stance Detection for Mis- and Disinformation Identification",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1259\u20131277, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10729\u201310737.",
            "year": 2022
        },
        {
            "authors": [
                "Kazi Saidul Hasan",
                "Vincent Ng."
            ],
            "title": "Stance classification of ideological debates: Data, models, features, and constraints",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1348\u20131356, Nagoya, Japan. Asian",
            "year": 2013
        },
        {
            "authors": [
                "Kazi Saidul Hasan",
                "Vincent Ng."
            ],
            "title": "Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751\u2013762,",
            "year": 2014
        },
        {
            "authors": [
                "Hideitsu Hino."
            ],
            "title": "Active learning: Problem settings and recent developments",
            "venue": "ArXiv preprint, abs/2012.04225.",
            "year": 2020
        },
        {
            "authors": [
                "Tamanna Hossain",
                "Robert L. Logan IV",
                "Arjuna Ugarte",
                "Yoshitomo Matsubara",
                "Sean Young",
                "Sameer Singh."
            ],
            "title": "COVIDLies: Detecting COVID-19 misinformation on social media",
            "venue": "Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2)",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Jaiswal",
                "Ashwin Ramesh Babu",
                "Mohammad Zaki Zadeh",
                "Debapriya Banerjee",
                "Fillia Makedon."
            ],
            "title": "A survey on contrastive selfsupervised learning",
            "venue": "Technologies, 9(1):2.",
            "year": 2020
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Ksenia Konyushkova",
                "Raphael Sznitman",
                "Pascal Fua."
            ],
            "title": "Learning Active Learning from Data",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long",
            "year": 2017
        },
        {
            "authors": [
                "Dilek K\u00fc\u00e7\u00fck",
                "Fazli Can."
            ],
            "title": "Stance detection: A survey",
            "venue": "ACM Computing Surveys (CSUR), 53(1):1\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Fanjin Zhang",
                "Zhenyu Hou",
                "Li Mian",
                "Zhaoyu Wang",
                "Jing Zhang",
                "Jie Tang."
            ],
            "title": "Selfsupervised learning: Generative or contrastive",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "ArXiv preprint, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Frank J. Massey."
            ],
            "title": "The Kolmogorov-Smirnov Test for Goodness of Fit",
            "venue": "Journal of the American Statistical Association, 46(253):68\u201378.",
            "year": 1951
        },
        {
            "authors": [
                "Saif Mohammad",
                "Svetlana Kiritchenko",
                "Parinaz Sobhani",
                "Xiaodan Zhu",
                "Colin Cherry."
            ],
            "title": "SemEval-2016 task 6: Detecting stance in tweets",
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Saif M Mohammad",
                "Parinaz Sobhani",
                "Svetlana Kiritchenko."
            ],
            "title": "Stance and sentiment in tweets",
            "venue": "ACM Transactions on Internet Technology (TOIT), 17(3):1\u201323.",
            "year": 2017
        },
        {
            "authors": [
                "Marius Mosbach",
                "Maksym Andriushchenko",
                "Dietrich Klakow."
            ],
            "title": "On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Aus-",
            "year": 2021
        },
        {
            "authors": [
                "Moritz Osnabr\u00fcgge",
                "Elliott Ash",
                "Massimo Morelli."
            ],
            "title": "Cross-domain topic classification for political texts",
            "venue": "Political Analysis, 31(1):59\u201380.",
            "year": 2023
        },
        {
            "authors": [
                "Malte Ostendorff",
                "Nils Rethmeier",
                "Isabelle Augenstein",
                "Bela Gipp",
                "Georg Rehm."
            ],
            "title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in Neural Information Processing Systems, 34:11054\u201311070.",
            "year": 2021
        },
        {
            "authors": [
                "Dean Pomerleau",
                "Delip Rao."
            ],
            "title": "Fake news challenge stage 1 (FNC-I): Stance detection",
            "venue": "URL www. fakenewschallenge. org.",
            "year": 2017
        },
        {
            "authors": [
                "Vahed Qazvinian",
                "Emily Rosengren",
                "Dragomir R. Radev",
                "Qiaozhu Mei."
            ],
            "title": "Rumor has it: Identifying Misinformation in Microblogs",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589\u20131599, Edin-",
            "year": 2011
        },
        {
            "authors": [
                "Qi Qian",
                "Lei Shang",
                "Baigui Sun",
                "Juhua Hu",
                "Tacoma Tacoma",
                "Hao Li",
                "Rong Jin."
            ],
            "title": "Softtriple loss: Deep metric learning without triplet sampling",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Octo-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Hiranmayi Ranganathan",
                "Hemanth Venkateswara",
                "Shayok Chakraborty",
                "Sethuraman Panchanathan"
            ],
            "title": "Deep active learning for image classification",
            "year": 2017
        },
        {
            "authors": [
                "Pengzhen Ren",
                "Yun Xiao",
                "Xiaojun Chang",
                "Po-Yao Huang",
                "Zhihui Li",
                "Brij B Gupta",
                "Xiaojiang Chen",
                "Xin Wang."
            ],
            "title": "A survey of deep active learning",
            "venue": "ACM computing surveys (CSUR), 54(9):1\u201340.",
            "year": 2021
        },
        {
            "authors": [
                "Nils Rethmeier",
                "Isabelle Augenstein."
            ],
            "title": "A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned, and Perspectives",
            "venue": "ACM Comput. Surv., 55(10).",
            "year": 2023
        },
        {
            "authors": [
                "Myrthe Reuver",
                "Antske Fokkens",
                "Suzan Verberne."
            ],
            "title": "No NLP task should be an island: Multidisciplinarity for diversity in news recommender systems",
            "venue": "Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report",
            "year": 2021
        },
        {
            "authors": [
                "Jingyu Shao",
                "Qing Wang",
                "Fangbing Liu."
            ],
            "title": "Learning to sample: an active learning framework",
            "venue": "2019 IEEE International Conference on Data Mining (ICDM), pages 538\u2013547. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Parinaz Sobhani",
                "Diana Inkpen",
                "Stan Matwin."
            ],
            "title": "From Argumentation Mining to Stance Classification",
            "venue": "Proceedings of the 2nd Workshop on Argumentation Mining, pages 67\u201377, Denver, CO. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Parinaz Sobhani",
                "Diana Inkpen",
                "Xiaodan Zhu."
            ],
            "title": "A dataset for multi-target stance detection",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 551\u2013557,",
            "year": 2017
        },
        {
            "authors": [
                "Swapna Somasundaran",
                "Janyce Wiebe."
            ],
            "title": "Recognizing Stances in Ideological On-Line Debates",
            "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116\u2013124, Los Angeles,",
            "year": 2010
        },
        {
            "authors": [
                "Christian Stab",
                "Tristan Miller",
                "Benjamin Schiller",
                "Pranav Rai",
                "Iryna Gurevych."
            ],
            "title": "Cross-topic argument mining from heterogeneous sources",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3664\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Ali Raza Syed",
                "Andrew Rosenberg",
                "Ellen Kislal."
            ],
            "title": "Supervised and unsupervised active learning for automatic speech recognition of low-resource languages",
            "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP",
            "year": 2016
        },
        {
            "authors": [
                "Matt Thomas",
                "Bo Pang",
                "Lillian Lee"
            ],
            "title": "Get out the vote: Determining support or opposition",
            "year": 2006
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Marilyn Walker",
                "Jean Fox Tree",
                "Pranav Anand",
                "Rob Abbott",
                "Joseph King."
            ],
            "title": "A Corpus for Research on Deliberation and Debate",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 812\u2013",
            "year": 2012
        },
        {
            "authors": [
                "Rui Wang",
                "Deyu Zhou",
                "Mingmin Jiang",
                "Jiasheng Si",
                "Yang Yang."
            ],
            "title": "A survey on opinion mining: From stance to product aspect",
            "venue": "IEEE Access, 7:41101\u2013 41124.",
            "year": 2019
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Penghui Wei",
                "Junjie Lin",
                "Wenji Mao."
            ],
            "title": "MultiTarget Stance Detection via a Dynamic MemoryAugmented Network",
            "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI,",
            "year": 2018
        },
        {
            "authors": [
                "Penghui Wei",
                "Wenji Mao."
            ],
            "title": "Modeling Transferable Topics for Cross-Target Stance Detection",
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July",
            "year": 2019
        },
        {
            "authors": [
                "Xuyang Yan",
                "Shabnam Nazmi",
                "Biniam Gebru",
                "Mohd Anwar",
                "Abdollah Homaifar",
                "Mrinmoy Sarkar",
                "Kishor Datta Gupta."
            ],
            "title": "Mitigating shortage of labeled data using clustering-based active learning with diversity exploration",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Yuanmeng Yan",
                "Rumei Li",
                "Sirui Wang",
                "Fuzheng Zhang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "ConSERT: A contrastive framework for self-supervised sentence representation transfer",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Yi Yang",
                "Zhigang Ma",
                "Feiping Nie",
                "Xiaojun Chang",
                "Alexander G Hauptmann."
            ],
            "title": "Multi-class active learning by uncertainty sampling with diversity maximization",
            "venue": "International Journal of Computer Vision, 113(2):113\u2013127.",
            "year": 2015
        },
        {
            "authors": [
                "Yi Yang",
                "Shimei Pan",
                "Doug Downey",
                "Kunpeng Zhang."
            ],
            "title": "Active learning with constrained topic model",
            "venue": "Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30\u201333, Baltimore, Maryland, USA. As-",
            "year": 2014
        },
        {
            "authors": [
                "Giulio Zhou",
                "Gerasimos Lampouras."
            ],
            "title": "Informed sampling for diversity in concept-to-text NLG",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2494\u20132509, Punta Cana, Dominican Republic. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jingbo Zhu",
                "Huizhen Wang",
                "Tianshun Yao",
                "Benjamin K Tsou."
            ],
            "title": "Active learning with sampling by uncertainty and density for word sense disambiguation and text classification",
            "venue": "Proceedings of the 22nd International Conference on Computational",
            "year": 2008
        },
        {
            "authors": [
                "Arkaitz Zubiaga",
                "Ahmet Aker",
                "Kalina Bontcheva",
                "Maria Liakata",
                "Rob Procter."
            ],
            "title": "Detection and resolution of rumours in social media: A survey",
            "venue": "ACM Computing Surveys (CSUR), 51(2):1\u201336.",
            "year": 2018
        },
        {
            "authors": [
                "Arkaitz Zubiaga",
                "Elena Kochkina",
                "Maria Liakata",
                "Rob Procter",
                "Michal Lukasik",
                "Kalina Bontcheva",
                "Trevor Cohn",
                "Isabelle Augenstein."
            ],
            "title": "Discourseaware rumour stance classification in social media using sequential classifiers",
            "venue": "Information Processing",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 13448\u201313464\nJuly 9-14, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "The goal of stance detection is to identify the viewpoint expressed by an author within a piece of text towards a designated topic (Mohammad et al., 2016). Such analyses can be used in a variety of domains ranging from identifying claims within political or ideological debates (Somasundaran and Wiebe, 2010; Thomas et al., 2006), identifying mis- and disinformation (Hanselowski et al.,\n2018; Hardalov et al., 2022a), public health policymaking (Glandt et al., 2021; Hossain et al., 2020; Osnabr\u00fcgge et al., 2023), news recommendation (Reuver et al., 2021) to investigating attitudes voiced on social media (Qazvinian et al., 2011; Augenstein et al., 2016; Conforti et al., 2020). However, in most domains, and even more so for crossdomain stance detection, the exact formalisation of the task gets blurry, with varying label sets and their corresponding definitions, data collection protocols and available annotations. Furthermore, this is accompanied by significant changes in the topicspecific vocabulary (Somasundaran and Wiebe, 2010; Wei and Mao, 2019), text style (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) and topics mentioned either explicitly (Qazvinian et al., 2011; Walker et al., 2012) or implicitly (Hasan and Ng, 2013; Derczynski et al., 2017). Recently, a benchmark of 16 datasets (Hardalov et al., 2021) covering a variety of domains and topics has been proposed for testing stance detection models across multiple domains. It must be noted that these datasets are highly imbalanced, with an imbalanced label distribution between the covered topics, i.e. inter-topic and within each topic, i.e. per-topic, as can be seen in Figure 2 and Figure 3. This further complicates the creation of a robust stance detection classifier.\nGiven the inherent skew present within the dataset and variances within each domain, we propose a topic-guided diversity sampling method, which produces a data-efficient representative subset while mitigating label imbalances. These samples are used for fine-tuning a Pre-trained Language Model (PLM), using a contrastive learning objective to create a robust stance detection model. These two components form our Topic Efficient StancE Detection (TESTED) framework, as seen in Figure 1, and are analysed separately to pinpoint the factors impacting model performance and robustness. We test our method on\n13448\nthe multi-domain stance detection benchmark by Hardalov et al. (2021), achieving state-of-the-art results with both in-domain, i.e. all topics seen and out-of-domain, i.e. unseen topics evaluations. Note though that TESTED could be applied to any text classification setting.\nIn summary, our contributions are:\n\u2022 We propose a novel framework (TESTED) for predicting stances across various domains, with data-efficient sampling and contrastive learning objective; \u2022 Our proposed method achieves SOTA results both in-domain and out-of-domain; \u2022 Our analysis shows that our topic-guided sampling method mitigates dataset imbalances while accounting for better performance than other sampling techniques; \u2022 The analysis shows that the contrastive learning objective boosts the ability of the classifier to differentiate varying topics and stances."
        },
        {
            "heading": "2 Related Work",
            "text": "Stance Detection is an NLP task which aims to identify an author\u2019s attitude towards a particular topic or claim. The task has been widely explored in the context of mis- and disinformation detection (Ferreira and Vlachos, 2016; Hanselowski et al., 2018; Zubiaga et al., 2018b; Hardalov et al., 2022a), sentiment analysis (Mohammad et al., 2017; Aldayel and Magdy, 2019) and argument mining (Boltu\u017eic\u0301 and \u0160najder, 2014; Sobhani et al., 2015; Wang et al., 2019). Most papers formally define stance detection as a pairwise sequence classification where stance targets are provided (K\u00fc\u00e7\u00fck and Can, 2020). However, with the emergence of differ-\nent data sources, ranging from debating platforms (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014) to social media (Mohammad et al., 2016; Derczynski et al., 2017), and new applications (Zubiaga et al., 2018a; Hardalov et al., 2022a), this formal definition has been subject to variations w.r.t. the label dictionary inferred for the task.\nPrevious research has predominantly focused on a specific dataset or domain of interest, outside of a few exceptions like multi-target (Sobhani et al., 2017; Wei et al., 2018) and cross-lingual (Hardalov et al., 2022b) stance detection. In contrast, our work focuses on multi-domain stance detection, while evaluating in- and out-of-domain on a 16 dataset benchmark with state-of-the-art baselines (Hardalov et al., 2021).\nTopic Sampling Our line of research is closely associated with diversity (Ren et al., 2021) and importance (Beygelzimer et al., 2009) sampling and their applications in natural language processing (Zhu et al., 2008; Zhou and Lampouras, 2021). Clustering-based sampling approaches have been used for automatic speech recognition (Syed et al., 2016), image classification (Ranganathan et al., 2017; Yan et al., 2022) and semi-supervised active learning (Buchert et al., 2022) with limited use for textual data (Yang et al., 2014) through topic modelling (Blei et al., 2001). This research proposes an importance-weighted topic-guided diversity sampling method that utilises deep topic models, for mitigating inherent imbalances present in the data, while preserving relevant examples.\nContrastive Learning has been used for tasks where the expected feature representations should be able to differentiate between similar and divergent inputs (Liu et al., 2021; Rethmeier and Augenstein, 2023). Such methods have been used for image classification (Khosla et al., 2020), captioning (Dai and Lin, 2017) and textual representations (Giorgi et al., 2021; Jaiswal et al., 2020; Ostendorff et al., 2022). The diversity of topics (Qazvinian et al., 2011; Walker et al., 2012; Hasan and Ng, 2013), vocabulary (Somasundaran and Wiebe, 2010; Wei and Mao, 2019) and expression styles (Pomerleau and Rao, 2017) common for stance detection can be tackled with contrastive objectives, as seen for similar sentence embedding and classification tasks (Gao et al., 2021; Yan et al., 2021)."
        },
        {
            "heading": "3 Datasets",
            "text": "Our study uses an existing multi-domain dataset benchmark (Hardalov et al., 2021), consisting of 16 individual datasets split into four source groups: Debates, News, Social Media, Various. The categories include datasets about debating and political claims including arc (Hanselowski et al., 2018; Habernal et al., 2018), iac1 (Walker et al., 2012), perspectum (Chen et al., 2019), poldeb (Somasundaran and Wiebe, 2010), scd (Hasan and Ng, 2013), news like emergent (Ferreira and Vlachos, 2016), fnc1 (Pomerleau and Rao, 2017), snopes (Hanselowski et al., 2019), social media like mtsd (Sobhani et al., 2017), rumour (Qazvinian et al., 2011), semeval2016t6 (Mohammad et al., 2016), semeval2019t7 (Derczynski et al., 2017), wtwt (Conforti et al., 2020) and datasets that cover a variety of diverse topics like argmin (Stab et al., 2018), ibmcs (Bar-Haim et al., 2017) and vast (Allaway and McKeown, 2020). Overall statistics for all of the datasets can be seen in Appendix C."
        },
        {
            "heading": "3.1 Data Standardisation",
            "text": "As the above-mentioned stance datasets from different domains possess different label inventories, the stance detection benchmark by Hardalov et al. (2021) introduce a mapping strategy to make the class inventory homogeneous. We adopt that same mapping for a fair comparison with prior work, shown in Appendix C."
        },
        {
            "heading": "4 Methods",
            "text": "Our goal is to create a stance detection method that performs strongly on the topics known during training and can generalize to unseen topics. The benchmark by Hardalov et al. (2021) consisting of 16 datasets is highly imbalanced w.r.t the intertopic frequency and per-topic label distribution, as seen in Figure 2.\nThese limitations necessitate a novel experimental pipeline. The first component of the pipeline we propose is an importance-weighted topic-guided diversity sampling method that allows the creation of supervised training sets while mitigating the inherent imbalances in the data. We then create a stance detection model by fine-tuning a Pre-trained Language Model (PLM) using a contrastive objective."
        },
        {
            "heading": "4.1 Topic-Efficient Sampling",
            "text": "We follow the setting in prior work on data-efficient sampling (Buchert et al., 2022; Yan et al., 2022), framing the task as a selection process between multi-domain examples w.r.t the theme discussed within the text and its stance. This means that given a set of datasets D = (D1, . . .Dn) with their designated documents Di = (d1i , . . . dmi ), we wish to select a set of diverse representative examples Dtrain, that are balanced w.r.t the provided topics T = (t1, . . . tq) and stance labels L = (l1, . . . lk).\nDiversity Sampling via Topic Modeling We thus opt for using topic modelling to produce a supervised subset from all multi-domain datasets. Selecting annotated examples during task-specific fine-tuning is a challenging task (Shao et al., 2019), explored extensively within active learning research (Hino, 2020; Konyushkova et al., 2017). Random sampling can lead to poor generalization and knowledge transfer within the novel problem domain (Das et al., 2021; Perez et al., 2021). To mitigate the inconsistency caused by choosing suboptimal examples, we propose using deep unsupervised topic models, which allow us to sample relevant examples for each topic of interest. We further enhance the model with an importance-weighted diverse example selection process (Shao et al., 2019; Yang et al., 2015) within the relevant examples generated by the topic model. The diversity maximisation sampling is modeled similarly to Yang et al. (2015).\nThe topic model we train is based on the technique proposed by Angelov (2020) that tries to find topic vectors while jointly learning document and word semantic embeddings. The topic model is initialized with weights from the all-MiniLM-L6 PLM, which has a strong performance on sentence embedding benchmarks (Wang et al., 2020). It is shown that learning unsupervised topics in this fashion maximizes the total information gained, about all texts D when described by all wordsW .\nI(D,W) = \u2211\nd\u2208D\n\u2211\nw\u2208W P (d,w) log\n( P (d,w)\nP (d)P (w)\n)\nThis characteristic is handy for finding relevant samples across varying topics, allowing us to search within the learned documents di. We train a deep topic modelMtopic using multi-domain data D and obtain topic clusters C = (Ci, . . . Ct),\nAlgorithm 1 Topic Efficient Sampling Require: S \u2265 0 \u25b7 Sampling Threshold Require: Avg \u2208 {moving, exp} Ensure: |C| > 0 Dtrain \u2190 {} I \u2190 { |C1|\u2211\nCi\u2208C Ci . . . |Ct|\u2211 Ci\u2208C Ci } \u25b7 Cluster Importances\nfor Ci \u2208 C do \u25b7 Iterating for each cluster Ei \u2190 {PLM(d1i ) . . . } = {e1i . . . emi } si \u2190 max(1, S \u00b7 Ii) \u25b7 Threshold per cluster j\u2190 0\ncent0 \u2190 \u2211 ei\u2208E ei\n|E| \u25b7 Centroid of the cluster while j \u2264 si do\nsim = \u27e8E,cent\u27e9\u2225E\u2225\u2225cent\u2225 \u25b7 Similarity Ranking sample = arg sort(sim,Ascending)[0]\n\u25b7 Take the sample most diverse from the centroid Dtrain \u2190 Dtrain \u222a sample j\u2190 j + 1 centj \u2190 { \u03b1 \u00b7 esample + (1\u2212 \u03b1) \u00b7 centj\u22121 exp (j\u22121)\nj \u00b7 centj + esample\nj moving \u25b7 Centroid update w.r.t. sampled data\nend while end for return Dtrain\nwhere |C| = t is the number of topic clusters. We obtain the vector representation for \u2200di from the tuned PLM embeddings E = (e1, . . . em) in Mtopic, while iteratively traversing through the clusters Ci \u2208 C.\nOur sampling process selects increasingly more diverse samples after each iteration. This search within the relevant examples is presented in Algorithm 1. This algorithm selects a set of diverse samples from the given multi-domain datasets D, using the clusters from a deep topic modelMtopic and the sentence embeddings E of the sentences as a basis for comparison. The algorithm starts by selecting a random sentence as the first diverse sample and uses this sentence to calculate a \u201ccentroid\u201d embedding. It then iteratively selects the next most dissimilar sentence to the current centroid, until the desired number of diverse samples is obtained."
        },
        {
            "heading": "4.2 Topic-Guided Stance Detection",
            "text": "Task Formalization Given the topic, ti for each document di in the generated set Dtrain we aim to classify the stance expressed within that text towards the topic. For a fair comparison with prior work, we use the label mapping from the\nprevious multi-domain benchmark (Hardalov et al., 2021) and standardise the original labels L into a five-way stance classification setting, S = {Positive, Negative, Discuss, Other, Neutral}. Stance detection can be generalized as pairwise sequence classification, where a model learns a mapping f : (di, ti)\u2192 S. We combine the textual sequences with the stance labels to learn this mapping. The combination is implemented using a simple prompt commonly used for NLI tasks (Lan et al., 2020; Raffel et al., 2020; Hambardzumyan et al., 2021), where the textual sequence becomes the premise and the topic the hypothesis.\n[CLS] premise: premise\nhypothesis: topic [EOS]\nThe result of this process is a supervised dataset for stance prediction Dtrain = ((Prompt(d1, t1), s1) . . . (Prompt(dn, tn), sn)) where \u2200si \u2208 S. This method allows for dataefficient sampling, as we at most sample 10% of the data while preserving the diversity and relevance of the selected samples. The versatility of the method allows TESTED to be applied to any text classification setting.\nTuning with a Contrastive Objective After obtaining the multi-domain supervised training set Dtrain, we decided to leverage the robustness of PLMs, based on a transformer architecture (Vaswani et al., 2017) and fine-tune on Dtrain with a single classification head. This effectively allows us to transfer the knowledge embedded within the PLM onto our problem domain. For standard finetuning of the stance detection modelMstance we use cross-entropy as our initial loss:\nLCE = \u2212 \u2211\ni\u2208S yi log (Mstance(di)) (1)\nHere yi is the ground truth label. However, as we operate in a multi-domain setting, with variations in writing vocabulary, style and covered topics, it is necessary to train a model where similar sentences have a homogeneous representation within the embedding space while keeping contrastive pairs distant. We propose a new contrastive objective based on the cosine distance between the samples to accomplish this. In each training batch B = (d1, . . . db), we create a matrix of contrastive pairs P \u2208 Rb\u00d7b, where \u2200i, j = 1, b, Pij = 1\nif i-th and j-th examples share the same label and \u22121 otherwise. The matrices can be precomputed during dataset creation, thus not adding to the computational complexity of the training process. We formulate our pairwise contrastive objective LCL(xi, xj ,Pij) using matrix P .\nLCL = { e(1\u2212 ecos(xi,xj)\u22121),Pij = 1 emax(0,cos(xi,xj)\u2212\u03b2) \u2212 1,Pij = \u22121\n(2)\nHere xi, xj are the vector representations of examples di, dj . The loss is similar to cosine embedding loss and soft triplet loss (Barz and Denzler, 2020; Qian et al., 2019); however, it penalizes the opposing pairs harsher because of the exponential nature, but does not suffer from computational instability as the values are bounded in the range [0, e\u2212 1e ]. The final loss is:\nL = LCE + LCL (3)\nWe use the fine-tuning method from Mosbach et al. (2021); Liu et al. (2019) to avoid the instability caused by catastrophic forgetting, small-sized fine-tuning datasets or optimization difficulties."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Evaluation",
            "text": "We evaluate our method on the 16 dataset multidomain benchmark and the baselines proposed by Hardalov et al. (2021). To directly compare with prior work, we use the same set of evaluation metrics: macro averaged F1, precision, recall and accuracy."
        },
        {
            "heading": "5.2 Model Details",
            "text": "We explore several PLM transformer architectures within our training and classification pipelines in order to evaluate the stability of the proposed technique. We opt to finetune a pre-trained robertalarge architecture (Liu et al., 2019; Conneau et al., 2020). For fine-tuning, we use the method introduced by Mosbach et al. (2021), by adding a linear warmup on the initial 10% of the iteration raising the learning rate to 2e\u22125 and decreasing it to 0 afterwards. We use a weight decay of \u03bb = 0.01 and train for 3 epochs with global gradient clipping on the stance detection task. We further show that learning for longer epochs does not yield sizeable improvement over the initial fine-tuning. The optimizer used for experimentation is an AdamW\n(Loshchilov and Hutter, 2019) with a bias correction component added to stabilise the experimentation (Mosbach et al., 2021).\nTopic Efficiency Recall that we introduce a topicguided diversity sampling method within TESTED, which allows us to pick relevant samples per topic and class for further fine-tuning. We evaluate its effectiveness by fine-tuning PLMs on the examples it generates and comparing it with training on a random stratified sample of the same size."
        },
        {
            "heading": "6 Results and Analysis",
            "text": "In this section, we discuss and analyze our results, while comparing the performance of the method against the current state-of-the-art (Hardalov et al., 2021) and providing an analysis of the topic efficient sampling and the contrastive objective."
        },
        {
            "heading": "6.1 Stance Detection",
            "text": "In-domain We train on our topic-efficient subset Dtrain and test the method on all datasets D in the multi-domain benchmark. Our method TESTED is compared to MoLE (Hardalov et al., 2021), a strong baseline and the current state-of-the-art on the benchmark. The results, presented in Table 1, show that TESTED has the highest average performance on in-domain experiments with an increase of 3.5 F1 points over MoLE, all while using\u2264 10% of the amount of training data in our subset Dtrain sampled from the whole dataset D. Our method is able to outperform all the baselines on 10 out of 16 datasets. On the remaining 6 datasets the maximum absolute difference between TESTED and MoLE is 1.1 points in F1. We also present ablations for TESTED, by replacing the proposed sampling method with other alternatives, removing the contrastive objective or both simultaneously. Replacing Topic Efficient sampling with either Random or Stratified selections deteriorates the results for all datasets with an average decrease of 8 and 5 F1 points, respectively. We attribute this to the inability of other sampling techniques to maintain inter-topic distribution and per-topic label distributions balanced while selecting diverse samples. We further analyse how our sampling technique tackles these tasks in subsection 6.2. We also see that removing the contrastive loss also results in a deteriorated performance across all the datasets with an average decrease of 3 F1 points. In particular, we see a more significant decrease in datasets with similar topics and textual expressions, i.e. poldeb\nand semeval16, meaning that learning to differentiate between contrastive pairs is essential within this task. We analyse the effect of the contrastive training objective further in subsection 6.4.\nOut-of-domain In the out-of-domain evaluation, we leave one dataset out of the training process for subsequent testing. We present the results of TESTED in Table 2, showing that it is able to overperform over the previous state-of-the-art significantly. The metrics in each column of Table 2 show the results for each dataset held out from training and only evaluated on. Our method records an increased performance on 13 of 16 datasets, with an averaged increase of 10.2 F1 points over MoLE, which is a significantly more pronounced increase than for the in-domain setting, demonstrating that the strength of TESTED lies in better outof-domain generalisation. We can also confirm that replacing the sampling technique or removing the contrastive loss results in lower performance across all datasets, with decreases of 9 and 5 F1 points respectively. This effect is even more pronounced compared to the in-domain experiments, as adapting to unseen domains and topics is facilitated by diverse samples with a balanced label distribution."
        },
        {
            "heading": "6.2 Imbalance Mitigation Through Sampling",
            "text": "Inter-Topic To investigate the inter-topic imbalances, we look at the topic distribution for the top 20 most frequent topics covered in the complete multi-domain dataset D, which accounts for \u2265 40% of the overall data. As we can see in Figure 2, even the most frequent topics greatly vary in their representation frequency, with \u03c3 = 4093.55, where \u03c3 is the standard deviation between represented amounts. For the training dataset Dtrain, by contrast, the standard deviation between the topics is much smaller \u03c3 = 63.59. This can be attributed to the fact that Dtrain constitutes \u2264 10% of D, thus we also show the aggregated data distributions in Figure 2. For a more systematic analysis, we employ the two sample KolmogorovSmirnov (KS) test (Massey, 1951), to compare topic distributions in D and Dtrain for each dataset present in D. The test compares the cumulative distributions (CDF) of the two groups, in terms of their maximum-absolute difference, stat = supx |F1(x)\u2212 F2(x)|.\nThe results in Table 3 show that the topic distribution within the full and sampled data D, Dtrain, cannot be the same for most of the datasets. The results for the maximum-absolute difference also show that with at least 0.4 difference in CDF, the\nsampled dataset Dtrain on average has a more balanced topic distribution. The analysis in Figure 2 and Table 3, show that the sampling technique is able to mitigate the inter-topic imbalances present in D. A more in-depth analysis for each dataset is provided in Appendix A.\nPer-topic For the per-topic imbalance analysis, we complete similar steps to the inter-topic analysis, with the difference that we iterate over the top 20 frequent topics looking at label imbalances within each topic. We examine the label distribution for the top 20 topics for a per-topic comparison. The standard deviation in label distributions averaged across those 20 topics is \u03c3 = 591.05 for the whole dataset D and the sampled set Dtrain \u03c3 = 11.7. This can be attributed to the stratified manner of our sampling technique. This is also\nevident from Figure 3, which portrays the overall label distribution in D and Dtrain.\nTo investigate the difference in label distribution for each of the top 20 topics in D, we use the KS test, presented in Table 4. For most topics, we see that the label samples in D and Dtrain cannot come from the same distribution. This means that the per-topic label distribution in the sampled dataset Dtrain, does not possess the same imbalances present in D.\nWe can also see the normalized standard deviation for the label distribution within Dtrain is lower than inD, as shown in Figure 4. This reinforces the finding that per-topic label distributions in the sampled dataset are more uniform. For complete pertopic results, we refer the reader to Appendix A.\nPerformance Using our topic-efficient sampling method is highly beneficial for in- and out-ofdomain experiments, presented in Table 1 and Table 2. Our sampling method can select diverse and representative examples while outperforming Random and Stratified sampling techniques by 8 and 5 F1 points on average. This performance can be attributed to the mitigated inter- and per-topic\nimbalance in Dtrain."
        },
        {
            "heading": "6.3 Data Efficiency",
            "text": "TESTED allows for sampling topic-efficient, diverse and representative samples while preserving the balance of topics and labels. This enables the training of data-efficient models for stance detection while avoiding redundant or noisy samples. We analyse the data efficiency of our method by training on datasets with sizes [1%, 15%] compared to the overall data size |D|, sampled using our technique. Results for the in-domain setting in terms of averaged F1 scores for each sampled dataset size are shown in Figure 5. One can observe a steady performance increase with the more selected samples, but diminishing returns from the 10% point onwards. This leads us to use 10% as the optimal threshold for our sampling process, reinforcing the data-efficient nature of TESTED."
        },
        {
            "heading": "6.4 Contrastive Objective Analysis",
            "text": "To analyse the effect of the contrastive loss, we sample 200 unseen instances stratified across each dataset and compare the sentence representations before and after training. To compare the representations, we reduce the dimension of the embeddings with t-SNE and cluster them with standard K-means. We see in Figure 6 that using the objective allows for segmenting contrastive examples in a more pronounced way. The cluster purity also massively rises from 0.312 to 0.776 after training with the contrastive loss. This allows the stance detection model to differentiate and reason over the contrastive samples with greater confidence."
        },
        {
            "heading": "7 Conclusions",
            "text": "We proposed TESTED, a novel end-to-end framework for multi-domain stance detection. The method consists of a data-efficient topic-guided sampling module, that mitigates the imbalances inherent in the data while selecting diverse examples, and a stance detection model with a contrastive training objective. TESTED yields significant performance gains compared to strong baselines on indomain experiments, but in particular generalises well on out-of-domain topics, achieving a 10.2 F1 point improvement over the state of the art, all\nwhile using \u2264 10% of the training data. While in this paper, we have evaluated TESTED on stance detection, the method is applicable to text classification more broadly, which we plan to investigate in more depth in future work.\nLimitations\nOur framework currently only supports English, thus not allowing us to complete a cross-lingual study. Future work should focus on extending this study to a multilingual setup. Our method is evaluated on a 16 dataset stance benchmark, where some domains bear similarities. The benchmark should be extended and analyzed further to find independent datasets with varying domains and minimal similarities, allowing for a more granular out-ofdomain evaluation."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is funded by a DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, as well as supported by the Pioneer Centre for AI, DNRF grant number P1."
        },
        {
            "heading": "B Evaluation Metrics",
            "text": "To evaluate our models and have a fair comparison with the introduced benchmarks we use a standard set of metrics for classification tasks such as macroaveraged F1, precision, recall and accuracy.\nAcc = TP + TN\nTP + TN + FP + FN (4)\nPrec = TP\nTP + FP (5)\nRecall = TP\nTP + FN (6)\nF1 = 2 \u2217 Prec \u2217Recall Prec+Recall = 2 \u2217 TP\n2 \u2217 TP + FP + FN (7)"
        },
        {
            "heading": "C Dataset Statistics",
            "text": "We use a stance detection benchmark (Hardalov et al., 2021) whose data statistics are shown in Table 5. The label mapping employed is shown in Table 6."
        },
        {
            "heading": "D TESTED with different backbones",
            "text": "We chose to employ different PLM\u2019s as the backbone for TESTED and report the results in the Table 7. The PLMs are taken from the set of robertabase, roberta-large, xlm-roberta-base, xlm-robertalarge. The differences between models with a similar number of parameters are marginal. We can\nLabel Description\nPositive agree, argument for, for, pro, favor, support, endorse Negative disagree, argument against, against, anti, con, undermine, deny, refute Discuss discuss, observing, question, query, comment Other unrelated, none, comment Neutral neutral\nTable 6: Hard stance label mapping employed in this paper, following the stance detection benchmark by Hardalov et al. (2021).\nsee a degradation of the F1 score between the base and large versions of the models, which can be attributed to the expressiveness the models possess. We also experiment with the distilled version of the model and can confirm that in terms of the final F1 score, it works on par with the larger models. This shows that we can utilise smaller and more computationally efficient models within the task with marginal degradation in overall performance.\nACL 2023 Responsible NLP Checklist"
        },
        {
            "heading": "A For every submission:",
            "text": ""
        },
        {
            "heading": "3 A1. Did you describe the limitations of your work?",
            "text": "Left blank.\nA2. Did you discuss any potential risks of your work? Not applicable. Left blank."
        },
        {
            "heading": "3 A3. Do the abstract and introduction summarize the paper\u2019s main claims?",
            "text": "Left blank.\n7 A4. Have you used AI writing assistants when working on this paper? Left blank.\nB 7 Did you use or create scientific artifacts? Left blank.\nB1. Did you cite the creators of artifacts you used? Not applicable. Left blank.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank."
        },
        {
            "heading": "3 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and",
            "text": "linguistic phenomena, demographic groups represented, etc.? 3\n3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Appendix D\nC 3 Did you run computational experiments? 6\n7 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? We use standard pre-trained language models.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance."
        },
        {
            "heading": "3 C2. Did you discuss the experimental setup, including hyperparameter search and best-found",
            "text": "hyperparameter values? 5"
        },
        {
            "heading": "3 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary",
            "text": "statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 6"
        },
        {
            "heading": "3 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did",
            "text": "you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 5\nD 7 Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants\u2019 demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response."
        }
    ],
    "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
    "year": 2023
}