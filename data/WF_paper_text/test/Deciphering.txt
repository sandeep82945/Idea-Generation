Peer reviews are one of the most important artifacts in scholarly communications. Peer reviews can serve as a rich source of knowledge discovery from texts that are human-generated and also opinionated on the paper under scrutiny. Reviewers comment on several implicit aspects of the paper (Originality, Soundness, Clarity, Appropriateness, etc.) where they sometimes appreciate, sometimes discuss, or sometimes question or criticize the work. Hence, correctly understanding the reviewer’s aspectual perspective on the paper is crucial for chairs/editors to take a stand and also for the authors to respond or revise accordingly. In this paper, we introduce MASEPR, a novel multitask deep neural architecture to jointly discover the aspects and associated sentiments from the peer review texts. Our proposed approach leverages the knowledge sharing between aspect and sentiment lexicons to generate predictions. We outperform the standard baselines by a significant margin. We also make our codes available at https://github.com/cruxieu17/MASEPR. 1 introduction Peer review is the standard process of evaluating the scientific work of researchers submitted to academic journals or conferences.But recently, peer review system has often been criticized as non transparent, arbitrary to generate the predictions. We outperform the standard base-[5] ,biased [49] and inconsistent[27, 46] which has led researchers to argue over its reliability[9] and quality[43]. This, coupled with the rapid increase of paper submissions, has raised the need to study the paper-vetting system and build proposals towards mitigating its problems. If we consider the example of computer science conferences: The Conference on Neural Information Processing Systems (NeurIPS) and the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) which received 9467 and 6656 submissions in 2020, respectively; the numbers are five times the number of submissions they received in 2010. Hence,
∗Both authors contributed equally to the paper †performed this work while at ÚFAL, MFF, Charles University, CZ
to guarantee a minimum number of reviews per paper, these conferences are forced to recruit many reviewers of different expertise, levels, and backgrounds. As a result, the submitted reviews often fail to meet conference conformity standards, such as supporting the claims with sufficient evidence, providing valid arguments (e.g., not self-contradictory), and providing insightful comments.Fortunately, research communities have noted some of these problems and have proposed measures to change reviewing practices. For example, The Association for Computational Linguistics (ACL) recently adopted the Rolling Review system 1 to aid reviewing in a reasonable and timely manner to avoid management issues with a large number of submissions; also conferences like International Conference on Computational Linguistics (COLING) and Conference on Empirical Methods in Natural Language Processing (EMNLP) have adopted the review-rebuttal process which is also an attempt to maintain the integrity of the peer-review process. Conferences in recent years have also organized tutorials[11] for researchers to write good quality reviews [34]. These are commendable efforts to restore faith in the widely accepted method of scholarly communication. However, peer-review suffers from another major problem, i.e., reviewers’ tendency to invest less time in this critical voluntary job [20]. Sometimes the reviewers themselves are unsure of their judgment on the merit of the work, which becomes evident with their reviews [4] [28]. Thus, these reform programs targeted to improve reviewing practices might not be sufficient. They could be frustrating for an enthusiastic prospective author, especially in the double-blind conference model without a rebuttal period. In order to maintain trust in the peer-review process, we believe establishing confidence in peer-review reports is essential. Given this direction’s submission load and urgency, an automatic system would help. As per the rubrics defined in [45], we expect a reviewer to evaluate the work for indicators like novelty, theoretical and empirical soundness of the research methodology, writing, clarity of the work, and impact of the work in a broader academic context. An indicative measure of how exhaustively and critically the reviewer has reviewed the paper could be the answer to the question that how many aspects of the paper has the reviewer touched upon or how thoroughly he has analyzed the paper, under the light of those aspects and in what sense (positive or negative) the reviewer has expressed his views. Hence, in this work, we attempt to automatically extract all the implicit aspect categories and their corresponding sentiments present in the review sentences. This information would
1https://aclrollingreview.org/
35
2575-8152/23/$31.00 ©2023 IEEE DOI 10.1109/JCDL57899.2023.00015
20 23
A CM
/I EE
E Jo
in t C
on fe
re nc
e on
D ig
ita l L
ib ra
rie s (
JC DL
) | 9
79 -8
-3 50
3- 99
31 -8
/2 3/
$3 1. 00 ©
20 23
IE EE
| D
O I:
10 .1
10 9/
JC DL
57 89
9. 20
23 .0
00 15
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. help the chairs to quickly estimate the informativeness of the review, understand the agreements and disagreements among the reviewers on certain aspects, and initiate discussions to arrive at a consensus on the decision. Even authors could benefit from such an automatic aspect and sentiment extractor, which would help them understand their action points and subsequently pursue appropriate steps in rebuttals or revisions. In this work, we propose MASEPR, a Multi Aspect cum Sentiment Extraction model from Peer Reviews, which does a joint learning of representations for extracting the aspects present in a review sentence along with their associated sentiments (POS,NEG). MASEPR explicitly models that the sentiment of a particular aspect is an aggregation of the sentiment of words indicating the aspect category. Specifically, MASEPR first predicts the instance sentiments, finds the key instances for the aspect categories, and finally aggregates the sentiments to make the final prediction. We conduct our experiments on a recent dataset of peer-reviews [56] and include the following aspect-categories2 in our investigation: Motivation/Impact (MOT), Originality (ORI), Soundness/Correctness (SOU), Substance (SUB), Replicability (REP), Meaningful Comparison (CMP), and Clarity (CLA),a description about them can be found in Table 1. To the best of our knowledge, this is probably the first work to explore multi-aspect sentiment learning on peer reviews. We plan to deploy MASEPR on a web-based interface which chairs and editors can use to discover implicit knowledge from the peer reviews. A demo video of how the interface will look like can be found on our GitHub https://anonymous.4open.science/r/MASEPR-266F. 2 relatedwork Peer review is the internationally recognized gold standard for scientific quality control. Reviewers examine the manuscript against various aspects and expertise to determine whether the content and style meet the appropriate journal publication standards. Generally, the process is anonymous, but this is not always the case. These anonymizations are processed to minimize bias toward the author(s) during the peer review process,ensuring that the information published in journals is accurate, trustworthy, and original [14]. During the peer review process, both the reviewer and the author recognize each other in an ‘open’ review. There is growing recognition of the need to alter the traditional peer review process to address the challenges posed by an ever-expanding scientific enterprise, the emergence of new forms of science, and the impact of new technologies. The increasing application of science and science-based technologies in society necessitates the development 2motivated from the ACL review form https://aclrollingreview.org/
of novel quality control mechanisms and the engagement of new stakeholders. [15]. Peer-Review and Meta-Research. There have been bodies of work in the meta-research community (Peer Review Week3, Peer Review Congress4, etc.). As one of the first such attempts, Ragone et al. (2013) [41] studied the need to improve the standards and efficiency in the practice of peer review, a study associated explicitly with the fairness, reliability, and validity of the process. Later, in 2016, Wicherts et al. [53] explained how transparency might be seen as an indicator of the quality of the practice of peer-review and also developed a tool enabling different stakeholders to analyze this factor. Wang et al. (2018) [52] proposes a novel multi-instance learning network with an abstract-based memory mechanism (MILAM) for sentiment analysis on peer review texts. As a result of this increased research in the peer-review domain, newer datasets like PeerRead, introduced in Kang et al. (2018) [23], and ‘CiteTracked’ [39] have also emerged to overcome the persisting shortage of peer-review data. In recent years, various attempts have been made to quantify the qualitative aspects of peer reviews to achieve more comprehensive insights. Gao et al. (2019) [16] assesses a newly introduced corpus of peer reviews and author responses and show how the authors’ rebuttals (especially for borderline papers) influence the paper’s final recommendation scores. While Ghosal et al. (2019) [17] propose a deep neural network to compute the peer-review outcome based on features extracted from the paper and the review text alongside its sentiment. Chakraborty et al. (2020) [8] demonstrates a typical aspect-based sentiment analysis on a peer-review data. Yuan et al. (2021) [56] makes an ambitious attempt to automatically generate peer reviews with aspect, sentiment, and various other features, proposing a dataset consisting of peer-reviews annotated on a phrase-level according to the aspects present in the review text and also show a preclusive result of sentence-level aspect prediction. The study also introduces the ‘ASAP-Review Dataset’, the experimental test bed for our investigations. The underlying objective in these studies might be coherent with that of ours; however, they essentially cover a different scope in the meta-research domain. Aspect Extraction andAnalysis. Earlier attempts at sentiment analysis (Pang and Lee, (2008); Liu, (2012)) [30, 35] attracted a lot of attention from the community. As a result, further research gradually opened the doors for investigations in the aspect-based analysis of various types of textual data like reviews from the product, food, service industries, etc. (Pontiki et al., 2014, 2015, 2016)
3https://peerreviewweek.wordpress.com/ 4https://peerreviewcongress.org/
36
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. [40] introduce and foster the research in Aspect-based Sentiment Analysis (ABSA), a sentiment analysis task including subtasks like Aspect-category Detection (ACD), and Aspect-category Sentiment Analysis (ACSA). Studies like (Tay et al. (2018); Hu et al. (2019)) [19, 48] introduce several attention-based models that allocate the appropriate sentiment words for the given aspect category. While Xue & Li et al. (2018) [54] propose a system that generates aspect category-specific representations with the help of convolutional neural networks and gating mechanisms. Several joint-learning models (Schmitt et al., 2018; Wang et al., 2019; Li et al., 2019) [29, 44] that perform tasks like ACD and ACSA have also emerged. In 2020, Akhtar et al. (2020) [1] propose a multi-task framework in the product reviews domain(Restaurant, Laptop, and like) to weigh and analyze aspects and opinion terms simultaneously. And recently, in 2021, Verma et al. (2021) [51] demonstrated an attention-based deep neural architecture to extract the aspects from the review of a given scholarly document. Multi-InstanceMulti-Label Learning. Zhou and Zhang, (2012) [58] made one of the very first attempts at defining the task of Multi-Instance Multi-Label Learning (MIML). The saliency of the task lies in representing each training example with multiple instances that are further associated with multiple class labels. Earlier studies revolving around the concept of MIML include its application in several downstream tasks like text categorization (Zhang and Zhou, 2008) [57], relation extraction (Surdeanu et al., 2012; Jiang et al., 2016)[22, 47], etc. In recent years, studies, including (Kotzias et al., 2015; Angelidis and Lapata, 2018)[2, 25], attempted to leverage multi-instance learning for sentiment analysis. Pappas and Popescu-Belis (2014) [36] propose a multiple instance regression (MIR) model to assign sentiment scores to specific aspects of products, thus extending the method to the sub-domain of ABSA. To our knowledge, neither MIL (Multi-Instalce Learning) nor MIML have been used in the peer-review domain. Common product and restaurant reviews are semantically easier to learn as it is guided by explicitly used aspect-category lexicons (e.g., ‘battery life’ for gadgets, ‘taste’ of the food, ‘ambiance’ of the place, and like). At the same time, the sentiment is usually identified as the sentiment of the entire text. A peer review text is very different from what we see in previous reviews and exhibits features that contrast highly from the regular, more straightforward reviews. Peer reviews are often far more comprehensive and detailed. The aspect categories present in a text can rarely be identified with the help of obvious lexicons and like. While the presence of a particular aspect in a sentence is primarily implicit, the overall ’sentiment of the sentence’ and the ’sentiment related to a particular aspect being talked about in the same sentence’ might differ. In this study, we attempt a joint extraction of aspects-categories and the sentiments associated with each of them from a given peer-review text. Technically, in the above studies, the tasks mainly address a ‘multi-class singlelabel, OR a ‘multi-class multi-label’ task limited only to the aspectcategory prediction. 3 problem definition Over the years, numerous studies involving multi-instance, multilabel learning, and aspect-sentiment-based analysis formulated and defined the tasks in MIL, MIML and ACD, ABSA, and ACSA. We combine the two problems and formulate the goals for MASEPR in a
multi-instance, multi-label aspect-category cum sentiment-polarity prediction task. Given a review 𝑅 = {𝑆1, 𝑆2, ..., 𝑆𝑚}, consisting of𝑚-number of sentences, where 𝑆𝑖 ∈ 𝑅 denotes the i-th sentence in the review. And, the sentence 𝑆𝑖 = {𝑤1,𝑤2, ...,𝑤𝑛}, comprising a sequence of 𝑛-number of words, where𝑤𝑖 ∈ 𝑆𝑖 denotes a single word. Our goal is to predict:
• the 𝑘-number of corresponding aspect-categories: 𝑎𝑆𝑖 = {𝑎1, 𝑎2, ..., 𝑎𝑘 }, where 𝑎𝑆𝑖 ⊂ 𝐴 associated with the sentence, 𝑆𝑖 , where𝐴 denotes set of aspect categories introduced and described in section 1. • and, the sentiment polarity distributions for all the 𝑘 aspect categories, 𝑝 = {𝑝1, 𝑝2, ...., 𝑝𝑘 }, where 𝑝𝑘 ⊂ 𝑃 , P denotes the set of sentiments-polarities - (Postitive: POS, Negative: NEG). As a result, a given review line may have multiple aspect categories discussed by its reviewer within its text. Furthermore, comments on each such aspect category may contain opinions of mixed polarity (i.e., positive sentiment, negative sentiment). Because of human thinking and the use of heuristics, we keep a reviewer’s ambivalence in mind and train MASEPR to be flexible and open to all possibilities when making a prediction. Our multi-label learning assumes that for every unobserved word-level sentiment distribution, there exists an unknown function 𝑝𝑘 given by:
pk = ĝ k 𝜃𝑆 (s1, s2, ..., sn); (1)
sj = f̂𝜃𝑤 (wj) (2)
where 𝑠 𝑗 is the sentiment distribution for the j-th word𝑤 𝑗 from a given review line. 4 dataset description and statistics For our experimental considerations, we reconstruct the data by creating a sentence-level split across all the reviews using the NLTK library. We assigned each sentence a label based on aspect-sentimentbased phrase-level annotations in the original data. The data thus created is multi-class, having around 200K instances. Table 2 summarizes the different categories of sentences present in the dataset. Each instance has two-dimensional annotations consisting of a sentiment-polarity label corresponding to each aspect-category label. We introduced a new label ‘no_aspect’ in our reconstructed data for denoting sentences that do not discuss any of the aforementioned aspects. We did some preliminary analysis to show the distribution of different aspect and sentiment categories in the overall reconstructed sentence level dataset. Figure 2 and Figure 3 summarizes our results. 5 methodology  5.1 model description MASEPR combines the aspect-category task and the sentimentpolarity prediction task in a unified training. The architecture of MASEPR comprises of a few shared representation layers, followed by the two task-specific heads. In essence, MASEPR has two major components: 1. Aspect-category prediction, and, 2. Sentimentpolarity prediction corresponding to each aspect category. For a given review line, we first obtain the contextual embeddings from
37
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. pretrained transformer-based encoders. Then, we pass these representations to a shared Bi-directional LSTM layer. The first phase in MASEPR’s architecture consists of:
• a) an attention layer added on top of the shared layer • b) an aspect category prediction layer (comprising feedfor-
ward linear layers). While the second phase comprise of:
• a) a word-sentiment prediction layer, and • b) an aspect category sentiment prediction layer which
maps the sentiment associated with each aspect category. 5.1.1 Representation Layer. The input to our model is a review sentence consisting of n words. To represent the words, we use contextual embeddings of these words from various pre-trained transformer models. Alongside being pretrained on large language corpora, these transformer models also leverage a self-attention mechanism to generate contextual representations of input texts. These encoded representations of serve as inputs to the shared BiLSTM layer. 5.1.2 Shared-BiLSTM Layer. This layer takes input from the shared representation layer. In order to assure the validity of the sentence representations and the weights computed by attention mechanism, we use a single Bi-directional LSTM layer [18]. This LSTM layer takes the word embeddings as input, and returns its hidden states 𝐻 = ℎ1, ℎ2, ..., ℎ𝑛 . This allows MASEPR to learn over richer contextual information in every representation vector space. At each time step j, the hidden state ℎ 𝑗 is computed by:
ℎ 𝑓
𝑗 = 𝐿𝑆𝑇𝑀 (ℎ 𝑗−1, ℎ 𝑗 ) (3)
ℎ𝑏𝑗 = 𝐿𝑆𝑇𝑀 (ℎ 𝑗+1, ℎ 𝑗 ) (4)
𝐻 𝑗 = [ℎ𝑓𝑗 ;ℎ 𝑏 𝑗 ] (5)
The size of hidden state of BILSTM is set to𝑑/2. The hidden state𝐻 𝑗 of the Bi-LSTM, refers to the representation of the j-th word. These enhanced representations serve as input to MASEPR’s Attention Layer. 38
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. 5.1.3 Attention Layer. MASEPR consists of a unique attention mechanism that helps it leverage the sentiment information of each review sentence to better comprehend the semantic structure of a review and the style of reviewers’ language. It combines the tasks of aspect-category and sentiment polarity prediction into a unified training in which the knowledge acquired from each task-specific feature assists the other sub-task and improves the performance on both. This layer takes the enhanced representations from the shared BiLSTM as input and produces an attention weight vector for each word with respect to each aspect category. For 𝑗𝑡ℎ word in a review sentence, the attention vector is calculated as:
𝛼 𝑗 = 𝑆𝑜 𝑓 𝑡𝑚𝑎𝑥 ((𝑊𝐻𝑇𝑗 )) (6)
where 𝛼 𝑗 ∈ 𝑅𝑛×1 represents the attention weight vector, 𝑛 is the total number of aspects, and𝑊 ∈ 𝑅𝑛×𝑑 , is a learnable parameter. We utilize these attention weights during the training of the taskspecific heads in subsequent stages. 5.1.4 Aspect-Category Prediction Layer. For the Aspect category prediction layer, we combine the attention weights produced by MASEPR’s Attention Layer with the enhanced representations from the hidden state of the BiLSTM layer. We use this weighted representation vector as the feature representation for each review line. For the 𝑖𝑡ℎ aspect category, the probability of the presence of the
respective aspect is calculated as:
𝑦𝑖 = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑊𝑖𝑟𝑇𝑖 + 𝑏𝑖 ), 𝑖 = 1, 2, 3.....𝑛 (7) 𝑟𝑖 = ∑︁ 𝑗 𝛼𝑖𝑗𝐻 𝑗 ,𝑤ℎ𝑒𝑟𝑒 𝑖 = 1, 2, 3...𝑛 (8)
where 𝑦𝑖 represents the probability of the 𝑖𝑡ℎ aspect being present in a given review line, 𝑟𝑖 is the the attention-infused feature representation vector of the review line, and𝑊𝑗 ∈ 𝑅1×𝑑 and 𝑏 𝑗 is a scalar trainable weight parameter. 5.1.5 Word Sentiment Prediction Layer. In order to predict the sentiment polarity for j-th word in a given review line, we pass the the enhanced representations from the hidden state 𝐻 𝑗 of the BiLSTM layer, through two fully connected layers as shown. 𝑝 𝑗 = (𝑊 (2) )𝑇𝑅𝑒𝐿𝑈 (𝑊 (1)𝐻𝑇𝑗 + 𝑏 (1) ) + 𝑏 (2) (9)
Here, 𝑊 (1) ∈ 𝑅𝑑 ′×𝑑 ,𝑊 (2) ∈ 𝑅𝑑 ′×2, 𝑏 (1) ∈ 𝑅𝑑 ′×1, 𝑏 (2) ∈ 𝑅2×1,and 𝑝 𝑗 represents the sentiment-polarity prediction label
5.1.6 Aspect-Category-wise Sentiment Prediction Layer. Finally, to obtain the aspect-category sentiment predictions (i.e. sentimentpolarity corresponding to each aspect-category) by aggregating the word-sentiment predictions from the Word-Sentiment prediction Layer. This is similar to the Aspect-Category Prediction Layer, except that this time, we combine the attention weights produced by MASEPR’s Attention Layer with the aggregated word-sentiment predictions. Formally, for the i-th aspect category, the corresponding sentiment 𝑠𝑖 is computed as:
𝑠𝑖 = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 ( ∑︁ 𝑗 𝑝 𝑗𝛼 𝑖 𝑗 ) (10)
where 𝑠𝑖 ∈ 𝑅2 , 𝛼𝑖𝑗 represents the attention weight of 𝑗 𝑡ℎ word with respect to 𝑖𝑡ℎ aspect category in the attention weight vector 𝛼 𝑗 obtained from MASEPR’s Attention Layer. 5.2 loss function We divide our loss function into two parts. the first one corresponds to the aspect category detection problem and is denoted by :
𝐿1 (𝜃1) = − 𝑛∑︁ 𝑖=1 (𝑟𝑖 log(𝑟𝑖 ) + (1 − 𝑟𝑖 ) log(1 − 𝑟𝑖 )) (11)
where 𝑟𝑖 ,is the i-th scalar values in the model output, and 𝑟𝑖 is the corresponding target values for aspect respectively. The second part corresponds to the sentiment of the Z aspect categories mentioned in the sentence and is given by
𝐿2 (𝜃2) = − 𝑍∑︁ 𝑐=1 (𝑠𝑖𝑐 log(𝑠𝑖𝑐 ) + (1 − 𝑠𝑖𝑐 ) log(1 − 𝑠𝑖𝑐 )) (12) where 𝑠𝑖 is the i-th scalar values in the model output, and 𝑠𝑖 is the corresponding target value for sentiment corresponding to each aspect category. We jointly train our model for the two tasks and the parameters are updated by minimizing the combined loss function:
𝐿(𝜃 ) = 𝐿1 (𝜃1) + 𝐿2 (𝜃2) (13) where 𝜃1 and 𝜃2 denotes all the model parameters. 39
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. 6 experiments and analysis  6.1 baselines and comparing systems We compare MASEPR with several baseline systems from previous literature. A recent approach proposed in [26] leverages the interdependence by sharing the representations between the aspectcategory and sentiment-category features in an end-to-end deep multitasking model. Although the problem under study was a bit different than ours (a multi class single-label classification one), their experiments show that the two categories help the multitask model in learning each others’ features. We draw a comparison between our method and a few common multi-label classification techniques and have also implemented certain baselines to prove the effectiveness of our proposed architecture. The details of the models are mentioned as follows:
(1) GloVe Embeddings (GloVe): This baseline uses GloVe[38] representations, instead of the transformer-based encoder models. We again provide the same attention mechanism and the outer architecture is also preserved. The performance on this baseline clearly justifies the use of pretrained transformers encoders in MASEPR. (2) CapsNet-BERT and CapsNet-Recurrent [21]: This is a model based on CapsNet and CapsNet-BERT. They combined the strength of BERT and capsule networks by replace the embedding layer and encoding layer of CapsNet with pre-trained BERT. In one setting they feed the sentence and aspect representations into capsule layers and predict the corresponding sentiment polarities while in other they used the bidirectional gated recurrent unit (BiGRU) instead of BERT for their experiments. (3) Shared Feature Extraction (SFE): Introduced in [26], the model uses SciBERT [3] embeddings and feeds the encoded representations directly to a shared layer for feature extraction. This shared layer has two task−specific MLPs (for aspect and sentiment predictions) and a combined loss if computed for training the model. (4) sentence-BERT + attention (sBERT+attn) [50]: This model is designed for review-level aspect prediction and uses s-BERT [42] encoder model fine tuned on STSB (semantic textual similarity) task. Encoded representations of the review text are passed through a BiLSTM layer followed by
40
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. a DL-based attention mechanism which helps distinguish the aspect-category features during training. The performances of baseline models are compared by evaluating aspect-category wise F1 scores, followed by sentiment-category scores corresponding to each aspect. The distinct features in baselines, and respective performances further support the methods adopted in MASEPR. 6.2 implementation details We use Pytorch [37] as the DL framework and pre-trained transformer models from 5hugging face for implementation. For our experimental purposes we split our dataset into 80 % train,5% validation and 15 % test sets. During validation, we experiment with different network configurations and for optimal performance we take the following hyper-parameters :Batch size=32,activation function=ReLU,dropout= 0.5,Learning rate= 1e-3. We used Adam[24] optimizer with a weight_decay=1e-3( for avoiding overfitting) and trained our model for 15 epochs.We run all models for 5 times and report the average results on the test datasets.All our experiments are done on a GPU (GeForce RTX 2080) with 16 GB of memory. 6.3 result and analysis We illustrate the experimental results of our proposed model along with other comparing system in Table 3.We show results for various language models we use in our representation layer namely BERT [12] (trained on Wikipedia and Book Corpus dataset, SCIBERT [3] (trained on a large multi-domain corpus of scientific publications), RoBERTa [31] (trained on CC NEWS dataset), GPT2 [6] (trained on MultiWOZ dataset), XLNET [55] (trained on Wikipedia, Book Corpus, Giga5, ClueWeb 2012-B, and Common Crawl dataset), SPECTRE [10] (trained on SCIDOCS dataset). According to the experimental results, we can come to the following conclusions:
• First, MASEPR outperforms all non-BERT baselines on the Review Advisor dataset, which indicates that MASEPR has better ability to detect different multi-sentiment polarities in one sentence toward different aspect categories. • Second,MASEPR-BERT surpasses all BERT-basedmodels[13], indicating that MASEPR can achieve better performance by using more powerful sentence encoders. 6.4 ablation study Multi-task learning [7] achieves improved performance by exploiting commonalities and differences across tasks. In this section we explore the performance of MASEPR in two settings namely single aspect and joint.The single aspect refers that the model is trained only for aspect category detection while joint refers that model is trained for both aspect category detection and its associated sentiment prediction together. We report the results of the experiment in Table 6,and it is clearly evident that multi task learning performs better than the single task learning.To prove the effectiveness of our model with respect to the real world scenario we tested our model on different data parameters : sentence length and number of aspects in a given sentence.Table 4 refers to the results of the model obtained on different sentence lengths.It clearly show that as the sentence length increases, modeling the information contained in the review sentence becomes complex and hence the model suffers a slight decrease in performance.We also explore the 5https://huggingface.co
effect of number of aspects on model performance and report the results in Table 5. We can see as the number of aspect increases, aspect categories with similar writing styles such as (originality and motivation), (substance and soundness) which are harder to distinguish, confuses our model. 6.5 Qualitative Analysis We analyze the predictions made by our model by estimating the performance of our model in detecting the key instances (KID: words indicating an aspect category) of the given aspect category using SHAP[32]. Additive explanations by Shapley (SHAP)Machine learning models are commonly thought of as ‘BlackBox’ algorithms that provide little insight into input-output relationships. A lack of interpretability and formalisms demonstrate the importance of features in supervised learning of labeled data, both globally and locally. Shapley Additive exPlanations (SHAP) are a new development that allows for quantitative model interpretability estimation [33]. SHAP is a family of explanation algorithms that use Shapley values, a principled approach to allocating credit for a feature. Given a sentence 𝑆𝑖 , we use Shapley values to compute the contribution/importance of each word𝑤 𝑗= v in 𝑠𝑖 using partition explainer. Intuitively, Shapley values compute the marginal contribution for each feature over all possible subsets of features. Computing the exact Shapley value requires exponential time. Hence, the values are computed approximately through sampling. Feature importances computed via Shapley values have several appealing theoretical properties. Figure 5 shows an example of SHAP analysis, formally known as a force diagram for a sentence corresponding to a review. In each force diagram, the base value denotes an aspect’s average predicted probability, while the figure’s f(inputs) denotes the SHAP’s predicted score concerning the model’s prediction. If this score is greater than the base value, it shows that the model has identified that aspect, while if it is less than the base value, the model has not predicted that aspect. Also, if the arrows over the words or phrases shown in the bar area towards the model’s prediction, then they say that these words support the model’s decision and if they are opposite, then they that these words oppose the prediction of that aspect. In this case, the supporting words and phrases are shown in red, while the ones opposing the prediction are shown in blue. Also, the width of the words or phrase in the bar shown first in the diagram denotes the importance of that word or phrase; the more the width, the more the importance of the word. To demonstrate the effectiveness of our model, We would first highlight some of the major roadblocks in the prediction of multiple aspects and their sentiments in a sentence and then use SHAP to demonstrate how our model tackles them.They are as follows :
6.5.1 Predominance of one aspect over another. Often, a major roadblock in identifying multiple aspects of a sentence is the predominance of one aspect over another. There are usually fewer words devoted to one aspect than to others .For example in the sentence shown in Figure 5, the dominant aspect is motivation, while the suppressed aspect is substance.Our model takes advantage of contextualised word representations generated by transformers, computing the importance of each word relative to every aspect, enabling it to predict the output correctly even if a single word merely represents the aspect. We can see from SHAP that our model focuses on phrases such as ’interesting problem’ and ’interesting
41
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. SUBSTANCE NEGATIVE
ideas worth exploring’ and can infer that the sentence talks about motivation positively. Apart from this, as evident from SHAP, our model establishes a relationship between different parts of the sentence even though the reviewer’s language is confusing and can recognize that the sentence is speaking negatively about the aspect substance. 6.5.2 Confusing Aspects. The purpose of this section is to raise a concern about the way of writing with regards to certain aspects such as (clarity, replicability), (motivation, originality), and (soundness, substance). We observed that some lexicons are commonly used to describe multiple aspects, creating a conflation of meaning in the model when it attempts to predict them. 42
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. The sentence in the figure 6 indicates that specific clarity lexicons such as clearly might create ambiguity as to what to predict; however, the force diagram shows that our model focuses more on the phrase ’not clear’ when predicting replicability. The model
recognizes the relationship between the phrase ’not clear’ with the words ’hinges,’ ’rollouts,’ and ’how it works’ and infers that the sentence is discussing replicability in a negative sense. Furthermore, as can be seen from the force map, when predicting clarity, our model gives greater weight to the phrase ’commented lines in algorithm’ than the phrase ’not clear,’ which proves that our model can learn how usage of different words in various settings. 43
Authorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply. Another sentence shown in the figure 7 demonstrates one of the most confusing aspect pairs : ( soundness and substance); one talks about whether the experiments/claims in the paper are convincingly supported or not, while the other talks about whether the paper contains substantial experiments to demonstrate the effectiveness of the proposed methods. The main reason for the confusion is the similarity in the reviewer’s tone while he/she refers to these aspects. The phrase ’rather weak’ is used both to refer to the lack of adequate evidence supporting the paper’s claims and the absence of any substantive experiments in the paper. Our model MASEPR, as illustrated by explanation maps, can comprehend the deeper underlying meaning of the sentence and correlates the phrase "rather weak" with various parts of the sentence while predicting the aspect, soundness, and substance along with their negative sentiment. It completely utilizes complex word representations encoded by transformers for learning such deep relationships. 6.5.3 Unique style of writing. This section intends to talk about those sentences which are too technically written ,with the absence of any sentiment denoting or directly aspect focussed words . One such sentence is shown in Figure 8 ,it talks about the originality of the paper but in a tone and style that is a bit different from the common style of sentence writing used to denote the aspect originality . Our model even struggles to understand such deeply engraved sentences but still is able to weakly identify the underlying aspect present in the sentence by establishing some kind of relationship existing between the words to learn the meaning of the sentence , as evident from the explainability diagram and hence relate the meaning with the aspect originality. 7 error analysis • Implicit semantic-level association of aspect categories:
Most of the time, the aspect category- Substance is implicitly commented on. The essence lies in identifying specific phrases that talk about the amount of information or the overall presentation of the study. These phrases often closely resemble n-grams that may represent other aspect categories. At times, MASEPR confuses the close association for a different aspect category. In one of such cases where a reviewer wrote "authors need to conduct more analyses to show why q-mtl is superior to supervised learning ", the phrase “analyses to show why" (pointing at the aspect category: Substance) resembles common phrases that indicate the presence of aspect category Soundness. Hence, the model has incorrectly predicted the aspect category - Soundness. Another example - ‘also, they fail at emphasizing the technical contribution of the paper see below as well’, Predicted Label: SUBSTANCE-NEGATIVE, SOUNDNESS-NEGATIVE, True Label: SUBSTANCE-NEGATIVE The reason is reviewer often comments about the motivation with respect to to experimental methods used in a paper. • The presence of complex mathematical structures: For example in the sentence "however, the theorems only prove bounds on the expected ∥𝑛𝑎𝑏𝑙𝑎𝑓 𝑥𝑎∥2
where a is uniformly sampled in t and x𝑎 is a random object that depends on s𝑡 sampled from math. ", Our model sometimes fails to determine correct aspect category or sentiment due to difficulty in understanding the context of a sentence having complex mathematical terms or an equation and predicts it as having no aspect while the true aspect is substance.. 8 conclusion Here in this work, we present our novel multitasking approach for joint multi-label aspect and sentiment extraction from peer review texts. Our model advances the current state of the art by predicting multiple aspects with varying polarity.