{
    "abstractText": "Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many imagebased benchmarks. In this work, we question if projectionbased methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate ViTs\u2019 lack of inductive bias by substituting a tailored convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https:// github.com/valeoai/rangevit.",
    "authors": [
        {
            "affiliations": [],
            "name": "Angelika Ando"
        },
        {
            "affiliations": [],
            "name": "Spyros Gidaris"
        },
        {
            "affiliations": [],
            "name": "Andrei Bursuc"
        },
        {
            "affiliations": [],
            "name": "Gilles Puy"
        },
        {
            "affiliations": [],
            "name": "Alexandre Boulch"
        },
        {
            "affiliations": [],
            "name": "Renaud Marlet"
        }
    ],
    "id": "SP:cc7150d136aa55492f8dc695b4a9e9280423f912",
    "references": [
        {
            "authors": [
                "Eren Erdal Aksoy",
                "Saimir Baci",
                "Selcuk Cavdar"
            ],
            "title": "SalsaNet: Fast road and vehicle segmentation in LiDAR point clouds for autonomous driving",
            "venue": "In IV,",
            "year": 2020
        },
        {
            "authors": [
                "Xuyang Bai",
                "Zeyu Hu",
                "Xinge Zhu",
                "Qingqiu Huang",
                "Yilun Chen",
                "Hongbo Fu",
                "Chiew-Lan Tai"
            ],
            "title": "TransFusion: Robust LiDAR-camera fusion for 3D object detection with transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jens Behley",
                "Martin Garbade",
                "Andres Milioto",
                "Jan Quenzel",
                "Sven Behnke",
                "Cyrill Stachniss",
                "Jurgen Gall"
            ],
            "title": "SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Maxim Berman",
                "Amal Rannen Triki",
                "Matthew B Blaschko"
            ],
            "title": "The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Lucas Beyer",
                "Pavel Izmailov",
                "Alexander Kolesnikov",
                "Mathilde Caron",
                "Simon Kornblith",
                "Xiaohua Zhai",
                "Matthias Minderer",
                "Michael Tschannen",
                "Ibrahim Alabdulmohsin",
                "Filip Pavetic"
            ],
            "title": "FlexiViT: One model for all patch sizes",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "In arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuScenes: A multimodal dataset for autonomous driving",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The Cityscapes dataset for semantic urban scene understanding",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Tiago Cortinhal",
                "George Tzelepis",
                "Eren Erdal Aksoy"
            ],
            "title": "SalsaNext: Fast, uncertainty-aware semantic segmentation of LiDAR point clouds",
            "venue": "In ISVC,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In arXiv,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Francis Engelmann",
                "Martin Bokeloh",
                "Alireza Fathi",
                "Bastian Leibe",
                "Matthias Nie\u00dfner"
            ],
            "title": "3D-MPA: Multi-proposal aggregation for 3D semantic instance segmentation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Everingham",
                "SM Eslami",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes challenge: A retrospective",
            "venue": "In IJCV,",
            "year": 2015
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
            "venue": "In CVPR,",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Girardeau-Montaut"
            ],
            "title": "CloudCompare. France: EDF R&D Telecom ParisTech, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Benjamin Graham",
                "Martin Engelcke",
                "Laurens Van Der Maaten"
            ],
            "title": "3D semantic segmentation with submanifold sparse convolutional networks",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Saurabh Gupta",
                "Judy Hoffman",
                "Jitendra Malik"
            ],
            "title": "Cross modal distillation for supervision transfer",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Lei Han",
                "Tian Zheng",
                "Lan Xu",
                "Lu Fang"
            ],
            "title": "OccuSeg: Occupancy-aware 3D instance segmentation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyong Hu",
                "Bo Yang",
                "Linhai Xie",
                "Stefano Rosa",
                "Yulan Guo",
                "Zhihua Wang",
                "Niki Trigoni",
                "Andrew Markham"
            ],
            "title": "RandLA-Net: Efficient semantic segmentation of large-scale point clouds",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Sebastian Borgeaud",
                "Jean-Baptiste Alayrac",
                "Carl Doersch",
                "Catalin Ionescu",
                "David Ding",
                "Skanda Koppula",
                "Daniel Zoran",
                "Andrew Brock",
                "Evan Shelhamer"
            ],
            "title": "PerceiverIO: A general architecture for structured inputs & outputs",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Deyvid Kochanov",
                "Fatemeh Karimi Nejadasl",
                "Olaf Booij"
            ],
            "title": "KPRNet: Improving projection-based LiDAR semantic segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Kr\u00e4henb\u00fchl",
                "Vladlen Koltun"
            ],
            "title": "Efficient inference in fully connected CRFs with gaussian edge potentials",
            "venue": "In NeurIPS,",
            "year": 2011
        },
        {
            "authors": [
                "Loic Landrieu",
                "Martin Simonovsky"
            ],
            "title": "Large-scale point cloud semantic segmentation with superpoint graphs",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yingwei Li",
                "Adams Wei Yu",
                "Tianjian Meng",
                "Ben Caine",
                "Jiquan Ngiam",
                "Daiyi Peng",
                "Junyang Shen",
                "Yifeng Lu",
                "Denny Zhou",
                "Quoc V Le"
            ],
            "title": "DeepFusion: LiDAR-camera deep fusion for multi-modal 3D object detection",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Haotian Liu",
                "Mu Cai",
                "Yong Jae Lee"
            ],
            "title": "Masked discrimination for self-supervised learning on point clouds",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Yueh-Cheng Liu",
                "Yu-Kai Huang",
                "Hung-Yueh Chiang",
                "Hung- Ting Su",
                "Zhe-Yu Liu",
                "Chin-Tang Chen",
                "Ching-Yu Tseng",
                "9 Winston H Hsu"
            ],
            "title": "Learning from 2D: Contrastive pixel-topoint knowledge transfer for 3D pretraining",
            "venue": "In arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Hsien-Yu Meng",
                "Lin Gao",
                "Yu-Kun Lai",
                "Dinesh Manocha"
            ],
            "title": "VV-Net: Voxel VAE net with group convolutions for point cloud segmentation",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Andres Milioto",
                "Ignacio Vizzo",
                "Jens Behley",
                "Cyrill Stachniss"
            ],
            "title": "RangeNet++: Fast and accurate LiDAR semantic segmentation",
            "venue": "In IROS,",
            "year": 2019
        },
        {
            "authors": [
                "Quang-Hieu Pham",
                "Thanh Nguyen",
                "Binh-Son Hua",
                "Gemma Roig",
                "Sai-Kit Yeung"
            ],
            "title": "JSIS3D: Joint semantic-instance segmentation of 3D point clouds with multi-task pointwise networks and multi-value conditional random fields",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "PointNet: Deep learning on point sets for 3D classification and segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "PointNet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Guocheng Qian",
                "Xingdi Zhang",
                "Abdullah Hamdi",
                "Bernard Ghanem"
            ],
            "title": "Pix4Point: Image pretrained transformers for 3D point cloud understanding",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Razani",
                "Ran Cheng",
                "Ehsan Taghavi",
                "Liu Bingbing"
            ],
            "title": "Lite-HDSeg: LiDAR semantic segmentation using lite harmonic dense convolutions",
            "venue": "In ICRA,",
            "year": 2021
        },
        {
            "authors": [
                "Corentin Sautier",
                "Gilles Puy",
                "Spyros Gidaris",
                "Alexandre Boulch",
                "Andrei Bursuc",
                "Renaud Marlet"
            ],
            "title": "Image-to- LiDAR self-supervised distillation for autonomous driving data",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Wenzhe Shi",
                "Jose Caballero",
                "Ferenc Husz\u00e1r",
                "Johannes Totz",
                "Andrew P. Aitken",
                "Rob Bishop",
                "Daniel Rueckert",
                "Zehan Wang"
            ],
            "title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Robin Strudel",
                "Ricardo Garcia",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Segmenter: Transformer for semantic segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Lyne Tchapmi",
                "Christopher Choy",
                "Iro Armeni",
                "JunYoung Gwak",
                "Silvio Savarese"
            ],
            "title": "SEGCloud: Semantic segmentation of 3D point clouds",
            "venue": "In 3DV,",
            "year": 2017
        },
        {
            "authors": [
                "Hugues Thomas",
                "Charles R Qi",
                "Jean-Emmanuel Deschaud",
                "Beatriz Marcotegui",
                "Fran\u00e7ois Goulette",
                "Leonidas J Guibas"
            ],
            "title": "KPConv: Flexible and deformable convolution for point clouds",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alaaeldin El-Nouby",
                "Jakob Verbeek",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Three things everyone should know about vision transformers",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alexandre Sablayrolles",
                "Gabriel Synnaeve",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Going deeper with image transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Larissa T Triess",
                "David Peter",
                "Christoph B Rist",
                "J Marius Z\u00f6llner"
            ],
            "title": "Scan-based semantic segmentation of LiDAR point clouds: An experimental study",
            "venue": "In IV,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Chunwei Wang",
                "Chao Ma",
                "Ming Zhu",
                "Xiaokang Yang"
            ],
            "title": "PointAugmenting: Cross-modal augmentation for 3D object detection",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Lei Wang",
                "Yuchun Huang",
                "Yaolin Hou",
                "Shenman Zhang",
                "Jie Shan"
            ],
            "title": "Graph attention convolution for point cloud semantic segmentation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Wang",
                "Zhiwen Fan",
                "Tianlong Chen",
                "Hehe Fan",
                "Zhangyang Wang"
            ],
            "title": "Can we solve 3D vision tasks starting from a 2D vision transformer",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Wang",
                "Yongbin Sun",
                "Ziwei Liu",
                "Sanjay E Sarma",
                "Michael M Bronstein",
                "Justin M Solomon"
            ],
            "title": "Dynamic graph CNN for learning on point clouds",
            "venue": "In TOG,",
            "year": 2019
        },
        {
            "authors": [
                "Yikai Wang",
                "TengQi Ye",
                "Lele Cao",
                "Wenbing Huang",
                "Fuchun Sun",
                "Fengxiang He",
                "Dacheng Tao"
            ],
            "title": "Bridged Transformer for vision and point cloud 3D object detection",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Wenxuan Wu",
                "Zhongang Qi",
                "Li Fuxin"
            ],
            "title": "PointConv: Deep convolutional networks on 3D point clouds",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Tete Xiao",
                "Mannat Singh",
                "Eric Mintun",
                "Trevor Darrell",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Early convolutions help transformers see better",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Chenfeng Xu",
                "Bichen Wu",
                "Zining Wang",
                "Wei Zhan",
                "Peter Vajda",
                "Kurt Keutzer",
                "Masayoshi Tomizuka"
            ],
            "title": "Squeeze- SegV3: Spatially-adaptive convolution for efficient pointcloud segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Chenfeng Xu",
                "Shijia Yang",
                "Bohan Zhai",
                "Bichen Wu",
                "Xiangyu Yue",
                "Wei Zhan",
                "Peter Vajda",
                "Kurt Keutzer",
                "Masayoshi Tomizuka"
            ],
            "title": "Image2Point: 3D point-cloud understanding with pretrained 2D convnets",
            "venue": "In arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Xu Yan",
                "Chaoda Zheng",
                "Zhen Li",
                "Sheng Wang",
                "Shuguang Cui"
            ],
            "title": "PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Xumin Yu",
                "Lulu Tang",
                "Yongming Rao",
                "Tiejun Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Point-BERT: Pre-training 3D point cloud transformers with masked point modeling",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Feihu Zhang",
                "Jin Fang",
                "Benjamin Wah",
                "Philip Torr"
            ],
            "title": "Deep FusionNet for point cloud semantic segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Yanan Zhang",
                "Jiaxin Chen",
                "Di Huang"
            ],
            "title": "CAT-Det: Contrastively augmented transformer for multi-modal 3D object detection",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhang",
                "Zixiang Zhou",
                "Philip David",
                "Xiangyu Yue",
                "Zerong Xi",
                "Boqing Gong",
                "Hassan Foroosh"
            ],
            "title": "PolarNet: An improved grid representation for online LiDAR point clouds semantic segmentation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Junsheng Zhou",
                "Xin Wen",
                "Yu-Shen Liu",
                "Yi Fang",
                "Zhizhong Han"
            ],
            "title": "Self-supervised point cloud representation learning with occlusion auto-encoder",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Xinge Zhu",
                "Hui Zhou",
                "Tai Wang",
                "Fangzhou Hong",
                "Wei Li",
                "Yuexin Ma",
                "Hongsheng Li",
                "Ruigang Yang",
                "Dahua Lin"
            ],
            "title": "Cylindrical and asymmetrical 3D convolution networks for LiDAR-based perception",
            "venue": "In CVPR,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many imagebased benchmarks. In this work, we question if projectionbased methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate ViTs\u2019 lack of inductive bias by substituting a tailored convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https:// github.com/valeoai/rangevit."
        },
        {
            "heading": "1. Introduction",
            "text": "Semantic segmentation of LiDAR point clouds permits vehicles to perceive their surrounding 3D environment in*This project was done during an internship at Valeo.ai.\ndependently of the lighting condition, providing useful information to build safe and reliable vehicles. A common approach to segment large scale LiDAR point clouds is to project the points on a 2D surface and then to use regular CNNs, originally designed for images, to process the projected point clouds [1, 11, 26, 36, 60, 66]. Recently, Vision Transformers (ViTs) were introduced as an alternative to convolutional neural networks for processing images [14]: images are divided into patches which are linearly embedded into a high-dimensional space to create a sequence of visual tokens; these tokens are then consumed by a pure transformer architecture [51] to output deep visual representations of each token. Despite the absence of almost any domain-specific inductive bias apart from the image tokenization process, ViTs have a strong representation learning capacity [14] and achieve excellent results on various\nar X\niv :2\n30 1.\n10 22\n2v 2\n[ cs\n.C V\n] 2\n5 A\npr 2\nimage perception tasks, such as image classification [14], object detection [8] or semantic segmentation [45].\nInspired by this success of ViTs for image understanding, we propose to implement projection-based LiDAR semantic segmentation with a pure vision transformer architecture at its core. Our goals are threefold in doing so: (1) Exploit the strong representation learning capacity of vision transformer for LiDAR semantic segmentation; (2) Work towards unifying network architectures used for processing LiDAR point clouds or images so that any advance in one domain benefits to both; (3) Show that one can leverage ViTs pre-trained on large-size natural image datasets for LiDAR point cloud segmentation. The last goal is crucial because the downside of having few inductive biases in ViTs is that they underperform when trained from scratch on small or medium-size datasets and that, for now, the only well-performing pre-trained ViTs [9,14,45] publicly available are trained on large collections of images that can be acquired, annotated and stored easier than point clouds.\nIn this context, our main contribution is a ViT-based LiDAR segmentation approach that compensates ViTs\u2019 lack of inductive biases on our data and that achieves state-ofthe-art results among projection-based methods. To the best of our knowledge, although works using ViT architectures on dense indoor point clouds already exists [63, 67], this is the first solution using ViTs for the LiDAR point clouds of autonomous driving datasets, which are significantly sparser and noisier than the dense depth-map-based points clouds found in indoor datasets. Our solution, RangeViT, starts with a classical range projection to obtain a 2D representation of the point cloud [11, 26, 36, 60]. Then, we extract patch-based visual tokens from this 2D map and feed them to a plain ViT encoder [14] to get deep patch representations. These representations are decoded using a lightweight network to obtain pixel-wise label predictions, which are projected back to the 3D point cloud.\nOur finding is that this ViT architecture needs three key ingredients to reach its peak performance. First, we leverage ViT models pre-trained on large natural image datasets for LiDAR segmentation and demonstrate that our method benefits from them despite the fact that natural images display little resemblance with range-projection images. Second, we further compensate for ViTs\u2019 lack of inductive bias by substituting the classical linear embedding layer with a multi-layer convolutional stem. Finally, we refine pixelwise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine lowlevel but fine-grain features of the convolutional stem with the high-level but coarse predictions of the ViT encoder.\nIn summary, our contributions are the following: (1) To the best of our knowledge, we are the first to exploit the strong representation learning capacity of vision transformers architectures for 3D semantic segmentation from\nLiDAR point clouds. By revisiting in the context of our problem the tokenization process of the ViT\u2019s encoder and adding a light-weight convolutional decoder for refining the coarse patch-wise ViT representations, we derive a simple but effective projection-based LiDAR segmentation approach, which we call RangeViT. (2) Furthermore, as shown in Fig. 1, the proposed approach allows someone to harness ViT models pre-trained on the RGB image domain for the LiDAR segmentation problem. Indeed, despite the large gap between the two domains, we empirically demonstrate that using such pre-training strategies improves segmentation performance. (3) Finally, our RangeViT approach, despite its simplicity, achieves state-of-the-art results among projection-based segmentation methods."
        },
        {
            "heading": "2. Related work",
            "text": ""
        },
        {
            "heading": "2.1. CNNs for Point Cloud Segmentation",
            "text": "2D Methods. Several works [3,11,24,26,28,36,42,47,50, 60, 64, 66, 70] project the 3D point cloud into the 2D space with range, perspective or bird\u2019s-eye-view (BEV) projection and process the projected images with 2D CNNs. For instance, PolarNet [66] employs bird\u2019s-eye-view projection to polar coordinates and then processes the bird-eye-view features with a Ring CNN. DarkNetSeg [3], SalsaNext [11], KPRNet [26], RangeNet++ [36], Lite-HDSeg [42] and SqueezeSegV3 [60] use range projection and then process the input images with a U-Net-like architecture. PMF [70], a multi-modal segmentation approach for point clouds and RGB images [2, 29, 53, 65, 70], projects the 3D points onto 2D camera frames and processes them together with the RGB images using a dual-stream CNN network. 3D Methods. Instead of the 2D space, voxel-based approaches [20,22,35,46] process the point clouds in their 3D space by first dividing the 3D space into voxels using cartesian coordinates and then applying 3D convolutions. Cylinder3D [69] show that dividing the 3D space into voxels using cylindrical coordinates instead of cartesian improves the segmentation performance. Although voxel-based methods consider the geometric properties of the 3D space, their drawback is that they are computationally expensive.\nAlso, for indoor-scene point cloud segmentation there are many methods [15, 37, 47, 52, 54, 56, 58, 62] relying on PointNet-inspired architectures [38] that directly process raw 3D points. However, these approaches cannot be easily adapted to outdoor-scene point cloud segmentation, which we consider in this work, as the large quantity of points cause computational difficulties."
        },
        {
            "heading": "2.2. ViTs for Point Cloud Segmentation",
            "text": "The recent rise to prominence of ViT architectures in computer vision has inspired a series of works for indoor point cloud segmentation [31, 57, 63, 67, 68].\nProcessing point clouds with a ViT model can be challenging and computationally expensive due to the large number of points. Point-BERT [63] uses farthest point sampling [39] and the KNN algorithm to define input tokens for the transformer. Also, it proposes a self-supervised pretraining strategy for point data inspired by the masked token modeling approaches in the RGB image [23] and text [13] domains. Point Transformer [67] has a U-Net-like architecture but without convolutional layers and it integrates selfattention mechanism in all of its blocks. Finally, Bridged Transformer [57] jointly processes point clouds and RGB images with a fully transformer-based architecture.\nUsing a transformer-based architecture for the semantic segmentation task on outdoor LiDAR point clouds remains challenging. To the best of our knowledge, there has been no work published yet about outdoor LiDAR point cloud semantic segmentation with a ViT-based architecture."
        },
        {
            "heading": "2.3. Transfer Learning from Images to Point Clouds",
            "text": "ViT models have the capacity to learn powerful representations, but require vast amounts of training data for it. However, large LiDAR point cloud datasets are less common due to the costly and time consuming annotation procedure. Recent works [21,32,40,43,55,61] explore transfer learning of 2D image pre-trained models to 3D point clouds.\nImage2Point [61] converts a 2D CNN into a 3D sparse CNN [61] by inflating 2D convolutions into 3D convolutions, which can be done by repeating the 2D kernel along the third dimension. SLidR [43] is a self-supervised pretraining method on LiDAR data. It uses a student-teacher architecture, where the 2D teacher network pre-trained on images transfers information into the 3D student network. Pix4Point [40] and Simple3D-Former [55] study transfer learning from images to indoor point clouds with fully transformer-based architectures. They adapt the tokenizer and the head layer to be specialized for 3D point cloud data. Pix4Point [40] applies farthest point sampling [39], the KNN algorithm and then a graph convolution on the aggregated neighborhoods to extract input tokens for the ViT encoder. Simple3D-Former [55] memorizes the ImageNet [12] representation of 2D image classification by incorporating a KL divergence term in the loss between its 2D image classification predictions and those obtained from a fixed ImageNet [12] pre-trained ViT network."
        },
        {
            "heading": "3. RangeViT",
            "text": "In this section, we describe our ViT-based LiDAR semantic segmentation approach, for which we provide an overview in Fig. 2."
        },
        {
            "heading": "3.1. General architecture",
            "text": "We represent a LiDAR point cloud of N points with a matrix P \u2208 RN\u00d74. Each point p = (x, y, z, i) \u2208 P has\nCartesian coordinates denoted by (x, y, z) and LiDAR intensity denoted by i, and is annotated with a label \u2113 \u2208 {1, . . . ,K} encoding one of the K semantic classes.\nRange projection. The input of our RangeViT backbone is a 2D representation of the input point cloud. We use the well-known range projection [36]. Each point p \u2208 P with coordinates (x, y, z) is projected on a range image of size H \u00d7W . The projected 2D coordinates satisfies(\nh w\n) = ( 1 2 ( 1\u2212 arctan(y, x)\u03c0\u22121 ) W(\n1\u2212 (arcsin(z, r\u22121) + |fdown|)f\u22121v ) H\n) , (1)\nwhere fv = |fdown| + |fup| is the vertical field-of-view of the LiDAR sensor. We associate C=5 low-level features (r, x, y, z, i) to each projected point, where r =\u221a\nx2 + y2 + z2 is the range of the corresponding point (i.e., its distance from the LiDAR sensor), to create the range image I \u2208 RC\u00d7H\u00d7W . Note that if more than one point is projected onto the same pixel, then only the feature with the smallest range is kept. Pixels with no point projected on them have their features filled with zeros.\nConvolutional stem. In a standard ViT, the image is divided into M patches of size PH \u00d7 PW , which are linearly embedded to provide D-dimensional visual tokens. Yet, our empirical study in Sec. 4.2 shows that this tokenization process of standard ViTs is far from optimal on both our task and datasets of interest. In order to bridge this potential domain gap between range images and standard ViTs, we replace the embedding layer with a non-linear convolutional stem [59]. Non-linear convolutional stems have been shown to increase optimization stability and predictive performance of ViTs [59], whereas we leverage them primarily for steering range images towards ViT-like inputs.\nThe first part of the convolutional stem consists of the first 4 residual blocks of SalsaNext [11], called context module. This context module captures short-range dependencies of the 3D points projected in the range image and produces pixel-wise features with Dh channels at the same resolution of the input range image, hence the tensor of context features tc has size Dh \u00d7 H \u00d7 W 1. Then, in order to produce tokens compatible with the input of a ViT, we use an average pooling layer that reduces the spatial dimensions of the context features tc from H \u00d7 W to (H/PH) \u00d7 (W/PW ), and use a final 1\u00d71 convolutional layer with D output channels. The convolutional stem thus yields M = (HW )/(PHPW ) visual tokens v1, . . . ,vM of dimension D, i.e., matching the input dimension and number of tokens of a traditional ViT.\n1The first 3 residual layers actually have Din = 32 channels, which is typically smaller than Dh.\nViT encoder. The output of the convolutional stem can be fed directly to a ViT [14]. The input t0 to our ViT encoder is prepared by stacking all the visual tokens v1, . . . ,vM and a classification token vclass \u2208 RD, to which we add the positional embeddings Epos \u2208 R(M+1)\u00d7D:\nt0 = [vclass,v1, . . . ,vM ] +Epos. (2)\nThe input tokens are then transformed by the ViT encoder to obtain an updated sequence of tokens tL \u2208 R(M+1)\u00d7D, where L denotes the number of transformer blocks. Then, we remove the classification token from tL to keep only the deep patch representations t\u2032L \u2208 RM\u00d7D.\nDecoder. The representations t\u2032L provided by the ViT encoder are patch representations which are unfortunately too coarse to obtain good point predictions. Therefore, we use a decoder to refine these coarse patch representations. First, t\u2032L is reshaped in form of a 2D feature map of size D \u00d7H/PH \u00d7W/PW . Our convolutional decoder consists of a 1\u00d71 convolution layer with DhPHPW output channels, followed by a Pixel Shuffle layer [44] which yields feature maps with shape Dh \u00d7H \u00d7W , i.e., with the same resolution as the original range image. While the convolutional decoder can still produce coarse features or decoding artifacts, the Pixel Shuffle is particularly effective in recovering fine information from features. Then, we concatenate these features with the context features tc from the convolutional stem and use a series of two convolutional layers with 3\u00d73 and 1\u00d71 kernels respectively, each of them followed by Leaky ReLU and batch normalisation, to obtain the refined feature map tdec \u2208 RDh\u00d7H\u00d7W .\n3D refiner. Ultimately, we need to convert the pixel-wise features from the range image space into point-wise predictions in the 3D space. Most prior range-projection based methods first make pixel-wise class predictions and then unproject them to the 3D space, where at inference time often\nthere is a post-processing step relying, e.g., on K-NN [36] or CRFs [27]. The purpose of the latter post-process step is to fix segmentation mistakes related to the projection and processing of 3D points in a 2D space (e.g., multiple 3D points being projected on the same pixel, or 2D boundary prediction errors for points that are actually far away in the 3D space). Instead, we follow the approach of KPRNet [26] that proposes an end-to-end approach that learns this postprocessing step with a KPConv [47] layer.\nKPConv [47] is a point convolution technique which works directly on the original 3D points. It permits to leverage the underlying geometry of the 3D point clouds to refine features at the point level. So, in our network, we project the 2D feature maps tdec of the 2D decoder back to original 3D points by bilinear upsampling, thus obtaining point-wise features with shape N \u00d7Dh, where N is the number of 3D points. Then, these point features are given to the KPConv layer as input along with the 3D coordinates of the corresponding points, which outputs Dh-dimensional point features. Finally, the logits s \u2208 RN\u00d7K are obtained by applying a BatchNorm, a ReLU and a final point-wise linear layer on these point features."
        },
        {
            "heading": "3.2. Implementation details",
            "text": "Training loss. We use the sum of the multi-class focal loss [30] and the Lova\u0301sz-softmax loss [4]. The focal loss is a scaled version of the cross-entropy loss [19] adapting its penalty to the hardness of the samples, making it suited for datasets with class imbalance, such as semantic segmentation. The Lova\u0301sz-softmax is developed specifically for semantic segmentation and built to optimize the mIoU. Inference. As in [45], we use a sliding-window method during inference. The network actually never sees the entire range images during training but only crops extracted from it. At inference, the range image is divided into overlapping crops of the same size as those used during training. The corresponding 2D features at the output of the decoder are\nthen averaged to reconstruct the entire feature map, which is then processed by our 3D refinement layer."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Setup",
            "text": "Datasets and metrics. We validate our approach for 3D point cloud semantic segmentation on two different commonly used datasets: nuScenes [7] and SemanticKITTI [3]. We conduct most of our ablation studies on nuScenes and compare against previous works on both nuScenes and SemanticKITTI. As evaluation metric, we use the mean Intersection over Union (mIoU) [16].\nNuScenes [7] consists of 1,000 scenes of 20 seconds in Boston and Singapore with various urban scenarios, lighting and weather conditions. The LiDAR sensor has 32 laser beams. Furthermore, there are 16 annotated semantic classes and the dataset is split into 28,130 training and 6,019 validation point cloud scans.\nSemanticKITTI [3] is created from the KITTI Vision Odometry Benchmark [17] and consists of urban scenes collected in Germany. Sequences 00-10 are used for training, except sequence 08 which is used for validation. There are 19,130 training and 4,071 validation scans. Sequences 11-21 are used for test and they contain 20,351 scans. The LiDAR sensor has 64 laser beams and there are 19 annotated semantic classes.\nModel and pre-trained weights. For all experiments, we use the ViT-S/16 model [14] as the encoder. It has L=12 layers, 6 attention heads and D=384 channels, amounting to approximately 21M parameters. Unless otherwise stated, (a) this ViT encoder is initialized with weights pre-trained on ImageNet21k [12] for classification and then fine-tuned on Cityscapes [10] for semantic image segmentation [45]; (b) the stem, decoder and 3D refiner layers, which are always randomly initialized, use Dh = 256 feature channels.\nOptimization. We use the AdamW optimizer [34] with \u03b21 = 0.9, \u03b22 = 0.999 and weight decay 0.01. The batchsize is 32 and 16 for nuScenes and SemanticKITTI, respectively. For the learning rate lr, we use a linear warm-up from 0 to its peak value for 10 epochs and then we decrease it over the remaining training epochs to 0 with a cosine annealing schedule [33]. In SemanticKITTI, we use 60 train-\ning epochs and the peak lr is 4 \u00d7 10\u22124. In nuScenes, we use 150 epochs and the peak lr is 8 \u00d7 10\u22124 when training from Cityscapes, ImageNet21k and random initializations, and 2\u00d7 10\u22124 when training from DINO initialization. Data augmentations. As point cloud augmentations we use: (a) flips over the y axis (the vertical axis on the range image), (b) random translations, and (c) random rotations between \u00b15\u25e6 (using the roll, pitch and yaw angles). All augmentations are applied randomly with probability 0.5. Finally, after range projection, we take a random image crop with a fixed size of 32\u00d7 384 for nuScenes and 64\u00d7 384 for SemanticKITTI. Note that the full size of a range image is 32\u00d7 2048 for nuScenes and 64\u00d7 2048 for SemanticKITTI."
        },
        {
            "heading": "4.2. What makes a ViT architecture for 3D semantic",
            "text": "segmentation?\nOur aim is to adapt to LiDAR point clouds with the fewest possible modifications of the standard ViT architecture [14], which already follows faithfully the original self-attention Transformer design [51]. Differently from ViT that performs classification on images, RangeViT performs semantic segmentation of point cloud-derived range images. We study here the possible choices for the input processing and decoder layers using the nuScenes dataset.\nStem and decoder. In Tab. 1, we study the impact of the convolutional stem and the UpConv decoder. To that end, we report results using a linear patch embedding [14] as a stem and a linear decoder.2 Starting from a model with linear stem and linear decoder, introducing the proposed convolutional stem leads to a significant mIoU boost (65.52 \u2192 69.82). This highlights the importance of having a non-linear network component for producing appropriate input token features for the ViT backbone. In addition, when using image pre-trained ViTs (Sec. 4.3) the convolutional stem can effectively steer the distribution of input range images towards the image-based feature distribution the ViT has been pre-trained on, leading to smoother fine-tuning. Then, also replacing the linear decoder with the proposed UpConv decoder leads to another notable improvement on mIoU (69.8 \u2192 73.83), validating this design choice. Finally, replacing the K-NN refinement [36] with\n2The linear decoder consists of a simple 1\u00d71 conv. layer, which predicts patch-wise classification scores, and a bilinear upsampling to reach the input image resolution.\nthe KPConv layer (in the 3D refiner layer), leads to a small but non-trivial mIoU increase (73.83 \u2192 74.60).\nIn Tab. 2, we study what is the impact, on the mIoU, of changing the number of feature channels Dh on the stem, decoder and 3D refiner layer. On nuScenes, we observe that the results gradually increase when increasing the dimension Dh. In the SemanticKITTI case, Dh = 128 gives the best results and further increasing Dh does not help. Note that the size of the ViT\u2019s embeddings is fixed at D = 384, regardless of the Dh size.\nWhat patch-size for range image \u201ctokenization\u201d? The size of the input patch tokens is an essential factor for controlling the trade-off between speed and accuracy performance without changing the number of parameters in the model. Intuitively, reducing the patch-size results in a finer representation, but also a longer input sequence that takes longer to process. Conversely, larger patches lead to a coarser representation, yet faster inference. The performance impact of the patch-size can sometimes match the one of model size [45]. For simplicity, standard ViT models use square patches of size 32, 16 or 8, as they are typically trained on square images. In contrast, range images have a much different aspect ratio (high width) and different layout of the content in the image (rows of points corresponding to LiDAR beams). With this insight, we revisit the practice of using square patches and look into rectangular patches with different aspect ratio (1:2, 1:4, 1:8) that would better capture the specific local patterns of the range images.\nTab. 3 shows the mIoU scores for different patch sizes for dividing the range image to patch tokens. Note that the convolutional stem first produces pixel-wise features and then, given a patch-size (PH , PW ), reduces the spatial dimensions of these features to (H/PH ,W/PW ) using local average pooling. We observe that smaller patch area is better but not necessarily with a square patch, commonly used for ViT models. The wide range images benefit more from rectangular, patches (2 \u00d7 8 performing best). The smaller patches enable the extraction of more fine-grained information leading to more precise predictions for smaller and thinner objects and points at object boundaries."
        },
        {
            "heading": "4.3. Exploiting image pre-trained ViTs",
            "text": "So far, we have studied the architectural choices necessary for using ViT models for point cloud semantic segmentation. The final RangeViT model preserves the ViT back-\nbone intact allowing us to initialize it with weights from models pre-trained on large datasets. We study now whether using such an initialization helps or not and what fine-tuning strategies would be more suitable in this context.\nIs pre-training on RGB images beneficial? In Tab. 4, we study the effect of transferring to our task ViT models pre-trained on natural RGB images. In particular, we explore initializing RangeViT\u2019s backbone with ViTs pretrained: (a) on supervised ImageNet21k classification [14] (entry IN21k), (b) on supervised image segmentation on Cityscapes with Segmenter [45] (entry CS), which in its turn was pre-trained with IN21k, and (c) with the DINO [9] self-supervised approach on ImageNet1k (entry DINO).\nWe observe that, despite the large domain gap, using ViT models pre-trained on RGB images is always better than training from scratch on LiDAR data (entry Rand). For instance, using the IN21k and CS pre-trained ViTs leads to improving the mIoU scores by 2.4 and 2.8 points, respectively. Additionally, as we see in Fig. 3, which plots the nuScenes validation mIoU as a function of the training epochs, using such pre-trained ViTs leads to faster training convergence of the LiDAR segmentation model. We argue that this is a highly interesting finding. It means that our RangeViT approach, by being able to use off-the-shelf pre-trained ViT models, can directly benefit from current and future advances on the training ViT models with natural RGB images, a very active and rapidly growing re-\nsearch field [23, 41, 48, 49]. Furthermore, from the small difference between the mIoU scores with the IN21k and CS pre-trainings, we infer that the pre-training can lead to consistent performance improvements even if it is not on the strongly-supervised image segmentation task, which requires expensive-to-annotate datasets.\nWhich ViT layers is better to fine-tune? In the image domain, several practices have emerged for fine-tuning a pre-trained convolutional network on a downstream dataset. Taking into consideration the domain gap between the pretraining and downstream data and the amount of labeled data available, the entire network can be fine-tuned or only a part of it. Here the domain gap between RGB images and range images is major and we would expect a full finetuning of the network to be better. To understand how much prior knowledge of the image pre-trained ViT is useful for point clouds, we study different fine-tuning strategies: finetuning all layers, only the attention (ATTN) layers or only the feedforward network (FFN) layers.\nIn Tab. 5, we show results with these different fine-tuning strategies. Interestingly, the best result are not achieved with full fine-tuning (model (a)) but when the attention layers are kept frozen (model (e)). This suggests that the pretrained ATTN layers are already well learnt and ready to generalize to range images. With CS pre-training ATTN layers may capture the layout of the scenes more easily without much fine-tuning, as the LiDAR scans have been also acquired from urban road scenes. Fine-tuning FFN may have more impact due to the different specifics of the LiDAR data compared to RGB images. In addition, FFN layers are in practice easier and more stable to optimize\n(they are essentially fully-connected and normalization layers) than ATTN layers that usually require more careful hyper-parameter selection. These findings align with recent ones from the image domain [48], confirming that the convolutional stems do steer the input LiDAR data to behave like image data once on in the ViT backbone.\nAblating encoder backbones. In Tab. 6, we replace the ViT-S encoder backbone of RangeViT with a ResNet-50 (RN50) encoder or the identity function (i.e., the decoder follows directly the stem). Both ViT-S and RN50 are pretrained on IN21k. We see that switching from ViT-S to RN50 decreases the mIoU from 74.77% to 72.30%. Furthermore, when the backbones remain frozen (ViT-S\u2020 and RN50\u2020 models), we reach 67.88% with ViT-S and 60.48% with RN50, demonstrating that ViT features are more appropriate for transfer learning to LiDAR data than CNNs. Finally, with the Identity backbone, we achieve 53.73%, which is more than 14 points worse than the ViT-S\u2020 model that has the same number of learnable parameters."
        },
        {
            "heading": "4.4. Comparison to the state-of-the-art",
            "text": "Tabs. 7 and 8 report the final comparison on the nuScenes validation set and on the SemanticKITTI test set including class-wise IoU scores. We observe that our model achieves superior mIoU performance compared to prior 2D projection based methods on both datasets, reducing the gap with the strong voxel-based Cylinder3D [69] method.\nClass-wise IoU result analysis. In Tabs. 7 and 8, we can see that RangeViT often achieves the best or the second best class-wise IoU scores. In both datasets, the classes are imbalanced and due to the sparsity and varying density of LiDAR point clouds, some classes (e.g., bicycle, pedestrian) are represented with few, not necessarily structured points per scene, so it is difficult recognize them. This problem could possibly be reduced by jointly processing point clouds and RGB images, since RGB images might provide extra cues about the outline and the shape of these objects."
        },
        {
            "heading": "4.5. Qualitative Results",
            "text": "We visualize the 3D point clouds with the ground truth semantic labels as well as the predictions with CloudCompare [18]. Fig. 4 shows the predictions of RangeViT on three validation point clouds of nuScenes. We notice minor errors such as man-made objects predicted as sidewalk and imperfect borders for the vegetation. We also remark difficulties in recognizing pedestrians and confusion between the driveable surface and the sidewalk for few points."
        },
        {
            "heading": "5. Conclusion",
            "text": "We studied the feasibility of leveraging (pre-trained) ViTs for LiDAR 3D semantic segmentation with projectionbased methods. We discover that in spite of the significant domain gap between RGB images and range images and\ntheir high requirements of training data, ViTs can be successfully used without any changes in the original transformer backbone. We achieve this thanks to an adapted tokenization and pre-processing for the ViT encoder and a simple convolutional decoder. We show that ViTs pre-trained on large image datasets can be effectively repurposed for LiDAR segmentation towards reaching state-of-the-art performance among 2D projection methods. We release the code for our implementation and hope that it could be used as a testbed for evaluating the ability of ViT image \u201cfoundation\u201d models [6] to generalize on different domains. Future work. Although the results are promising, there is still room for improvement. For instance, we identified the tokenization of LiDAR data as a crucial factor for success. As future work, we could further improve this process, e.g., with FlexiViT [5] (random patch sizes) or Perceiver IO [25] (learning to extract tokens), and consider tokenizing raw 3D data instead of the 2D projections.\nAcknowledgements. We would like to express our gratitude to Matthieu Cord and Hugo Touvron for their insightful comments, Robin Strudel et al. [45] for kindly providing ViT-S pre-trained on Cityscapes, and Oriane Sime\u0301oni for her valuable contribution to certain experiments. This work was performed using HPC resources from GENCI-IDRIS (Grants 2022- AD011012884R1, 2022-AD011013413 and 2022-AD011013701). The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grant ANR-21-CE23-0032 (project MultiTrans)."
        },
        {
            "heading": "A. Additional visualizations",
            "text": "Figs. 6 and 7 show visualizations of the segmentation accuracy of RangeViT and Fig. 8 shows instances of correct and incorrect predictions. The visualizations are made on nuScenes validation point clouds. More information is provided in the captions of these figures."
        },
        {
            "heading": "B. Model parameter count analysis",
            "text": "In Tab. 1 of the main paper, we studied the impact of the non-linear convolutional stem and the UpConv decoder. In Tab. 9, we complete the results of Tab. 1 with a model (e) which, like model (a), has a linear stem and a linear decoder but for which the ViT backbone contains L = 14 transformer layers3 instead of L = 12. Hence, this additional model (e) and our full RangeViT solution (model (d)) have a similar number of parameters. This experiment shows that the significant performance improvement of our full solution (d) is not simply due to a higher number of parameters, since model (e) performs much worse than our RangeViT (d). Besides, our full RangeViT solution with Dh = 64 (model (f)) also reaches a significantly better mIoU than models (a), (b) and (e) while having nearly the same number of parameters as model (a). This confirms that the proposed convolutional stem and UpConv decoder play an important role in the performance improvement."
        },
        {
            "heading": "C. Computation cost comparison",
            "text": "In Tab. 10, we compare the number of parameters and the inference time of RangeViT with other LiDAR segmentation methods. RangeViT has 27.1M parameters, which\n3Note that, as pre-trained weights for the additional 2 transformer layers of model (e), which were placed on top of the existing 12 transformer layers of ViT-S, we used the pre-trained weights from the last available transformer layer.\nis four times more than SalsaNext [11] (6.73M), half of Cylinder3D [69] (55.9M) and eight times less than KPRNet [26] (213.2M). The inference time on nuScenes using the same GeForce RTX 2080 GPU is: 25ms for RangeViT, 28ms for SalsaNext with K-NN post-processing (15ms without it), and 49ms for Cylinder3D (using the simplified and faster re-implementation of [43])."
        },
        {
            "heading": "D. Additional ablation analysis",
            "text": "Impact of crop size. During training, we take a fixedsized random crop from the range image, which is the input of the model. This design choice avoids computing selfattention on the whole range image in the ViT encoder, but it lacks the global information carried by the whole image. Nevertheless, our method is still successful for the semantic segmentation task since the H \u00d7 384 window crop covers the whole vertical field-of-view (FOV) and one fifth of the horizontal (azimuthal) FOV (67.5 degrees). This is what a single nuScenes camera captures and where objects are already well identifiable. Moreover, we recall that sliding windows are also successfully used for 2D semantic segmentation with ViTs, e.g., in Segmenter [45].\nIn Tab. 11, we experiment with different crop sizes. As we see, the crop size has a small impact on the performance. It is also likely that tuning the learning rate and the number of epochs for these new crop sizes will reduce these small gaps even further.\nRole of the classification token. The classification token interacts with the patch embeddings in the ViT encoder, but it is removed from the encoder output. As we do not use it directly for the semantic segmentation task, we explored its role by omitting it completely from the pipeline. Thus omitting the class token makes the mIoU drop from 75.21% to 74.64% and from 72.37% to 72.24% for the Cityscapes pre-training and no pre-training (random initialization), respectively. We hypothesize that the larger mIoU drop for Cityscapes pre-training is because, during 2D segmentation pre-training, the class token learned to carry global information that the patch tokens exploit via self-attention in order to extract better features for the segmentation task. In any case, the differences are small and thus feature extraction can still be achieved without adding the class token."
        },
        {
            "heading": "E. Additional implementation details",
            "text": "Convolutional stem and UpConv decoder. Fig. 5 shows the detailed architecture of the convolutional stem and the UpConv decoder. All convolutions that are applied before the average pooling layer in the convolutional stem or after the Pixel Shuffle layer in the decoder do not change the spatial dimensions of the feature map H \u00d7 W , so appropriate paddings were applied where necessary.\nAs described in Sec. 3 of the main paper, the average pooling layer reduces the spatial dimensions from H \u00d7W to (H/PH)\u00d7 (W/PW ). To achieve this, we use kernel size (PH +1)\u00d7 (PW +1), kernel stride PH \u00d7PW and padding (PH/2)\u00d7 (PW /2).\nIn the stem, the first three residual blocks use 32 feature channels and the change of dimensions from C = 5 input channels to 32 channels happens in the first convolutional layer of the first residual block. The fourth residual block in the stem uses Dh feature channels and similarly the change of dimensions from 32 input channels to Dh channels happens again in the first convolutional layer. Finally, the last convolutional layer in the stem (that is after the average pooling layer) uses D output feature channels.\nKPConv layer of the 3D Refiner. The KPConv [47] layer of the 3D Refiner has Dh input and output feature channels. Its kernel size is 15 points and the influence radius of each kernel point is 1.2.\nReplacing ViT-S with ResNet-50. In Tab. 6 of the main paper, we report the results that we achieve with our model when we replace the ViT-S encoder backbone with a ResNet-50 (RN50) backbone. To implement this RN50based model, instead of the ViT-S we use the four residual blocks of RN50, to which we introduced dilations to main-\ntain a constant spatial resolution as in ViTs. The stem and decoder remain the same, except for the number of output channels of the stem (64) and the input channels of the decoder (2048) to make them compatible with RN50. The resulting model has comparable FLOPs (the inference time for both is 25ms) but much more parameters (25.2M vs 35.3M).\nRange-projection for the SemanticKITTI experiments. To generate the 2D range images for the SemanticKITTI experiments, instead of using the spherical projection described by Eq. (1) of the main paper, we follow [26,50] and unfold the LiDAR scans according to the order in which they are captured by the sensor."
        }
    ],
    "title": "RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving",
    "year": 2023
}