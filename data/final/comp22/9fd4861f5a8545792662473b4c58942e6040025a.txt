In critical infrastructure applications, timely and consistent fault detection and diagnosis is an increasingly important operational process, especially in the energy sector where safety is of the utmost importance. To realise this, engineers have to manually analyse data acquired from several assets using predefined diagnostic processes, but this is a time-consuming process requiring significant amounts of specialist expert knowledge. Data-driven approaches to support fault detection and diagnosis, and other similar problems, can produce accurate results comparable to what the engineers can achieve in a fraction of the time. However, the majority of these data-driven techniques are black box techniques and lack explainability which is often necessary for explaining decisions about critical assets in the power generation industry. Knowledge-based systems, such as rule-based expert systems have been shown to provide not only accurate decisions but also the explanation and reasoning behind these decisions in some related applications. However, there is a significant time cost associated with the development of knowledge-based systems, and in particular with the knowledge elicitation process, where the domain expert’s knowledge is formalised and is encoded into the system. This challenge is commonly referred to as the knowledge elicitation bottleneck. In this paper, we present a novel approach to performing the knowledge elicitation using a set of symbolic primitives (rise, fall, fluctuate, and stable) to parameterise typical time-series condition monitoring data. The knowledge is represented by using a common language that can easily be communicated with (and from) the domain experts. This allows for the quick and accurate elicitation of the domain experts knowledge, but also the formalisation and implementation of the knowledge into a rapidly produced diagnostic system. Further to this, due to the parametrisation of the knowledge, it is possible to iteratively improve the knowledge base by updating these parameters based on new unseen data. This approach was applied to the Tennessee Eastman dataset, which is simulated data of a real-world industrial process. It was found that by using this approach it was possible to accurately and quickly capture the knowledge required to detect several faults within the case study dataset, but also provided fully explained reasons why each fault was detected by relating the explanations to the symbolic primitives previously defined. 1. introduction For many critical assets across the power generation sector, accurate and efficient fault detection and diagnosis are extremely important. This is due to both the overriding need to ensure safe continued operation of the asset and the costs associated with downtime of the plant, through reduced or zero power generation but also with potentially expensive and time-consuming repairs or replacements, and also the cost related to the time associated with diagnosing said fault. ∗ Corresponding author. E-mail addresses: andrew.young.101@strath.ac.uk (A. Young), graeme.west@strath.ac.uk (G. West), blair.brown@strath.ac.uk (B. Brown),
ruce.stephen@strath.ac.uk (B. Stephen), a.duncan@imperial.ac.uk (A. Duncan), c.michie@strath.ac.uk (C. Michie), s.mcarthur@strath.ac.uk (S.D.J. McArthur). Evaluation of faults is traditionally a time-consuming, manually intensive process and automation of this process provides the opportunity to introduce efficiency savings. To ensure the adoption of any automated approach similar or improved accuracy must be achieved over what can already be achieved manually by the engineers. Two main approaches to automating fault diagnosis are; data-driven approaches and knowledge-based approaches. With the increases in computing power available over the last two decades, it has been observed in Liu
vailable online 19 July 2022 957-4174/© 2022 The Authors. Published by Elsevier Ltd. This is an open access ar
https://doi.org/10.1016/j.eswa.2022.118169 Received 6 September 2021; Received in revised form 18 January 2022; Accepted
ticle under the CC BY license (http://creativecommons.org/licenses/by/4.0/). 13 July 2022
h t t t t
t t a t o e a t c b t t
b s t t t h t e r q t h c a
r r h d
s a
2
n L
i t c d t
et al. (2018) that there has been a huge swing to more data-driven approaches over the traditional knowledge-based approaches. However, for many critical assets in power generation there is often a requirement to provide supporting evidence when making decisions and before implementing related solutions, due to the costs associated with reduced power generation, or expensive repairs or replacements. Hence, the lack of explainability of many data-driven approaches is currently a problem that is being extensively researched (Chakraborty, Başağaoğlu, and Winterle (2021) and Moradi and Samwald (2021)), owever, this is a problem that is yet to be solved. In addition to his, due to many of these types of assets having conservative design olerances, to compensate for and avoid lost revenue during downtime, here is frequently a lack of failure case studies to train data-driven echniques and ensure their accuracy or effectiveness. Knowledge-based techniques present an opportunity to address both
he lack of: explainability; and, training data by capturing and codifying he domain expert’s knowledge and expertise. Expert knowledge is direct replacement for explainability as diagnostic results can be
raced back to the knowledge from which it was derived. Training f models is not required as the model, in this case, is one of the xpert’s knowledge, which is assumed to contain years of experience nd training. This knowledge in many cases has already been recorded hrough documented diagnostic procedures, however, this is not always omplete enough to be formalised into the rules for a knowledgeased system. The main drawback of this type of approach is the time aken to elicit this required knowledge from the engineers to enable his approach. In Cullen and Bryman (1988) this has been coined the ‘‘knowledge elicitation bottleneck’’. This paper presents a novel approach for tackling elements of the knowledge elicitation bottleneck for fault diagnosis problems and provides a case study of its use with a publicly available engineering data set. The next section of this paper provides more details of the various approaches to automated fault diagnosis, with an overview of the trade-offs between data-driven and knowledge-based approaches. This is important as most data driven approaches, by their nature lack the explainability inherent in knowledge based approaches have. Although, for knowledge-based approaches to become more viable the time taken to develop these systems needs to be addressed. Section three presents the various current approaches to perform knowledge elicitation for the development of knowledge-based systems. A new streamlined approach to performing knowledge elicitation using symbolic primitives is presented in section four, along with a step by step process for how to perform a knowledge elicitation session using this approach. This looks to address the issue of the time taken in the development of these systems, specifically in the knowledge elicitation phase. Section five demonstrates the proposed approach for knowledge elicitation and the development of a prototype expert system using a condition monitoring case study dataset. Conclusions and future work are presented in section six. 2. background  2.1. fault diagnosis Fault diagnosis in many industrial applications involves engineers following a predefined diagnostic procedure. These procedures have typically developed over many years based on physical laws, industry standards and the engineers understanding of the specific asset under analysis and how the individual process variables, e.g. temperature, vibration, pressure, flow, current, voltage, etc., change under different fault conditions. Based on these factors, the engineers will look for features, or trends in the specific process variable to determine if a fault condition has been met, then by cross-referencing this with the current operating conditions of the plant will determine if any actions
are required to be undertaken. Fig. 1 shows an example of an industrial fault detection (or trouleshooting) procedure (Asturias & Gagen, 2007). This process involves plitting the troubleshooting process into two stages, first identifying he issue and secondly performing the actual repair. The seven steps hat must be taken to identify the fault are; gather information, verify he issue, try quick fixes, use appropriate diagnostics, perform a splitalf search, use additional resources to research the issue, and escalate he issue (if necessary). In this example diagnosis process, after gathring the relevant data the system has to be assessed by an expert, as eferenced by ‘‘schedule service call’’, to verify the problem, failing any uick fixes resolving the issue, an extensive diagnostic procedure has o be undertaken involving; running diagnostics, performing a splitalf search, and general research into the problem. Each of these steps an be a very time-consuming process for the experts, therefore, it is dvantageous to automate as much of this process as possible. The main parts in these types of processes that can be automated
elate specifically to the assessment of gathered information or data, unning diagnostics, assessing how the data changes over time, and ow the relationship between various datastreams change all under ifferent fault conditions. The two approaches discussed in this paper to automate these
teps are categorised into data-driven approaches and knowledge-based pproaches. .2. Data-driven approaches
There are several data-driven approaches to automated fault diagosis, however, the current state of the art is predominately Machine earning oriented. A convolutional neural network-based approach to fault diagnosis
s proposed in Wen, Li, Gao, and Zhang (2018), the authors highlight he limitation of the approach being able to classify common fault onditions, i.e. those that have a large amount of labelled training ata available. Therefore, for faults that have not been learned the echnique would be unable to classify them. The authors of Guo, Lei, Xing, Yan, and Li (2019) propose a deep convolutional transfer learning network (DCTLN) method for fault diagnosis of unlabelled data. The results show that the DCTLN trained on labelled data from one machine could accurately classify unlabelled data acquired for a new machine. As before, this approach requires labelled data to exist for a machine similar to the machine currently being analysed. In Ding, Ma, Ma, Wang, and Lu (2019) the authors propose a generative adversarial network-based approach in an attempt to solve the ‘‘small sample size problem’’, by producing an augmented training dataset that was used to accurately classify the given faults. Also in Meng, Guo, Pan, Sun, and Liu (2019), the authors propose a new data augmentation by segmenting the sample data and randomly recombining the segments to artificially produce a larger amount of training data. From the literature, for industrial fault diagnostics for a critical asset where there is extremely limited labelled faulty training data (if any at all) and supporting evidence is required when making diagnostic conclusions; in general, machine learning or data-driven approaches are unable to solve this type of problem. This is mainly due to the large amount of training data required to learn the faults or the lack of explainability due to the black-box nature of these techniques. In addition to this, for these types of industrial problems, there is a significant amount of domain expert knowledge available due to the current engineers having to perform this fault diagnosis process manually. The process of incorporating the domain expert knowledge into data-driven models is currently an active area of research but is far from a trivial
problem (Deng, Ji, Rainey, Zhang, & Lu, 2020). 2
k k d e u e c d
s k L t m r l t a s r a
.3. Knowledge-based approaches
An alternative class of methods to solve this problem involves nowledge-based approaches. Unlike data-driven approaches, nowledge-based approaches do not require a large amount of training ata to be available, but instead, encode a significant amount of xpert knowledge or human expertise into the system that can be sed to perform fault diagnosis. As the domain experts knowledge is ncoded into the system, they are not only able to produce diagnostic onclusions but are also able to explain the reasoning behind why a ecision has been made. In Qian, Li, Jiang, and Wen (2003) the authors present an expert ystem for fault diagnosis of chemical processes, using a pre-existing nowledge base. A Vibration-based expert system is presented in Yang, im, and Tan (2005) and uses a decision tree style approach to perform he analysis. This decision tree is generated from the cause-symptom atrix presented in Jackson et al. (1978). Due to the nature of how the ules are formalised and stored in a rule-based expert system, having ow generality and low expandability (Gao, Cecati, & Ding, 2015) a ask-based diagnostic expert system was proposed in Bo, Zhi-nong, nd Zhong-qing (2012). This allows for the rules to be customised for pecific assets and stored as more general rules, however, this also equires a well-structured logic to exist and applies only to similar ssets. As stated in Angeli (2010), Rafea, Hassen, and Hazman (2003),
and Shadbolt and Smart (2015) the main hurdle to overcome in the
development of knowledge-based systems is the knowledge elicitation, or knowledge acquisition, bottleneck. Various research has been undertaken in an attempt to solve this problem (Chen & Rao, 2008; Wagner, Otto, & Chung, 2002; Xing, Huang, & Shi, 2003; Xiong, Litz, & Ressom, 2002), however, the majority of this research has focused on ways of extending the current knowledge elicitation techniques by the inclusion of more participants or additional data sources. 3. knowledge elicitation Knowledge elicitation is the process of attempting to elicit knowledge from domain experts using various types of methods and techniques. This information can be gathered from a variety of sources, e.g. technical manual, case studies, textbooks, etc., however, this is typically done by direct interaction with the domain expert. Generally, this is because the knowledge is often derived through practical experience. This implies that there is often the requirement for the expert to be involved in the knowledge elicitation process, representing the main challenge for the knowledge engineer. The person conducting the knowledge elicitation session is therefore required to determine an appropriate way for the experts to communicate their knowledge. The approach proposed in this paper aims to build on this premise by developing an approach that is both familiar to the domain expert, easy for them to communicate their understanding, and also easy to codify this knowledge into a knowledge base. Additionally, issues arise during knowledge elicitation, for example, due to the nature of the
d s
E f m b r a k w
4
4
t d n
e r
i T t o p t A F
m o t
knowledge being acquired by the expert over several years and hence becoming part of their routine, it can often, therefore, be difficult for the experts to formalise their knowledge. Also, for larger organisations, the knowledge can be distributed across a number of experts in various geographical locations or with different management oversight. Various approaches can be used to perform this knowledge elicitation. The main techniques have been categorised into five main categories (Shadbolt & Smart, 2015); interviews, protocol analysis, iagramming, sorting and rating, and constrained processing which are hown below:
• Interviews – Structured
∗ Fixed Probe (Shadbolt & Burton, 1990) ∗ Focused Interviews (Hart, 1989) ∗ Forward Scenario Simulation (Grover, 1983) ∗ Critical Decision Method (Hoffman, 1998)
– Semi-Structured
∗ Knowledge Acquisition Grid (LaFrance, 1987) ∗ Teach Back (Johnson & Johnson, 1987)
– Unstructured (Weiss & Kulikowski, 1984)
• Protocol Analysis – Verbal
∗ Online (Johnson, Zualkernan, & Garber, 1987) ∗ Offline (Elstein, Shulman, & Sprafka, 1978) ∗ Shadowing (Clarke, 1987) ∗ Collegial Verbalization (Erlandsson & Jansson, 2007)
– Behavioural (Ericsson & Simon, 1984)
• Diagramming – Laddered Grid (Corbridge, Rugg, Major, Shadbolt, & Burton, 1994)
– Concept Mapping (Novak & Cañas, 2006) – Process Mapping (Milton, 2012)
• Sorting and Rating – Concept Sorting (Gammack, 1987) – Repertory Grid (Shaw & Gaines, 1987) – Pathfinder (Schvaneveldt et al., 1985)
• Constrained Processing – Limited-Information Task (Hoffman, 1987) – 20 Questions (Grover, 1983)
ach of these approaches has different positives and negatives, and a ull discussion of this is beyond the scope of this paper. However, the ain negative for all of these approaches is they are time-consuming for oth the knowledge engineer and the domain expert. They also either equire face to face meetings between both parties which takes time way from the day job of the domain experts, or requires access for the nowledge engineer to observe the domain expert performing the task, hich for certain industries may not be possible. . Methodology
.1. Background
In the literature, it is agreed that the knowledge elicitation botleneck is a major problem to overcome before being able to rapidly evelop and deploy industrial fault diagnostic expert systems. The
ovel approach proposed in this paper alters the typical expert system
architecture by the inclusion of a signal to symbol transformation (SST) Costello, West, McArthur, and Campbell (2012) and Young, West, Brown, Stephen, and McArthur (2019) step. Fig. 2 shows the updated xpert system architecture with the inclusion of an SST between the eal world view of data and the inference engine. SST is used to transform time series data gathered from the asset
nto a series of symbolic primitives that accurately represent the data. his is achieved by segmenting the data into discrete time intervals, hen for each time segment a symbolic primitive is assigned, examples f, these primitives are intentionally rationalised (to minimise comlexity) as: rising, falling, fluctuating or stable, see Fig. 3. However, he symbolic primitives can be customised for the specific application. n example of this applied to two-time series data sources is shown in ig. 4. The asset-specific knowledge that needs to be elicited from the doain experts is the subtle difference for each symbolic primitives. Each f the differences for each primitive has been assigned a parameter, his is shown in Fig. 5. Fig. 3(a) shows an example of how the 𝑥 and 𝑦
parameters are defined for a stable trend. Here the parameters define
i
t o
Table 1 Example format of pressure datastream specific rules. (– is Stable, F is Fluctuating, ↑ is Rising and ↓ is Falling). Name D1 x y z D2 x y z D3 x y z D4 x y z
Fault 1 – 1.5 2.0 N/A ↑ 600 2.0 0.25 ↓ 600 1.5 0.5 F 600 10 N/A
o a n 4 d s i A p 5 s l i T
5
5
n p
Fig. 5. Definition of parameters for subtle difference in symbolic primitives. Where 𝜇 s the mean of the signal. he maximum deviation from the mean of the signal, both in terms f an increase and a decrease. Conversely, Fig. 3(d) gives an example of a fluctuating trend, for the data samples that fall outside of the stable regions, the number of fluctuations (mean crossings) is calculated if this is above the threshold defined by 𝑦 the signal is considered fluctuating. For Fig. 3(b) and Fig. 3(c) the 𝑥, 𝑦 and 𝑧 parameters are defined the same except where 𝑦 defined an increase for a rising trend and a decrease for a falling trend. The 𝑥 parameter defines the time the trend occurs over and the 𝑧 parameter is the expected deviation in the rise or fall of the trend. By eliciting a combination of symbolic primitives for specific datastreams and eliciting for each of the 𝑥, 𝑦, and 𝑧 parameters for each primitive it will be possible to construct a rule that can be directly interpreted by an expert system. 4.2. knowledge elicitation session In the previous subsection, an approach to alter the standard expert system architecture was described to include a SST step that allows for the parametrisation of the expert knowledge required to perform fault diagnosis. In addition to this, four example symbolic primitives were described and how each of these symbols were parametrised. This section describes how framing the problem in this manner allows for more efficient domain knowledge capture from engineers in a knowledge elicitation session. In all of the knowledge elicitation approaches mentioned in Section 3, the knowledge engineer has to understand the process that the expert adopts when analysing their problem. The proposed approach simplifies the explanation of this process into several key questions; What datastreams are relevant to the given fault?, What trends are associated with the relevant datastreams?, What is the expected magnitude of these trends?, and How long will the fault take to manifest? Below, we provide a step by step description of how to perform a knowledge elicitation session using the proposed approach. 0. Fault selection: The fault selection step, numbered step zero in the process as it occurs before the knowledge elicitation session. In this step, the domain experts must collate example case studies for each
T
Algorithm 1: Signal to symbol transformation. Where 𝑥, 𝑦 and 𝑧 are defined in Fig. 5 if 50% of data (< x*mean(data) or > y*mean(data)) then
Result: Stable else if Number of mean crossings > y then
Result: Fluctuating else
Calculate average of first and last 10% of data for x period of time; if First < y*Last ± z*Last then
Result: Rising else if First > y*Last ± z*Last then
Result: Falling
fault they want the system to be able to detect. Ideally, this should be in a digital format (e.g. plots of process variables from a condition monitoring system) to allow for ease of sharing, and accurately defining of parameters in Step 4. 1. Datastream selection: The initial stage in the knowledge elicitation session, is datastream selection. Based on the case study data under investigation and supplied from the domain experts, all datastreams except the specific datastreams they would use to determine the fault are eliminated. In the case of the eliminated datastreams, these ‘‘Don’t care’’ or ‘‘N/A’’ states are ignored and not referenced in the diagnostic process. 2. Time Interval selection: The second stage involves exploration of the case study data, to determine the approximate time frame/scale during which the fault will present or develop. That is the time from steady-state normal operation to a fault occurring, or being fully represented in the datastreams under analysis. Using the SST as described above, this time is used to segment the data. While this time interval does not need to be exact, and it is assumed that no two fault cases will be the same, a relatively accurate time interval is necessary to produce an accurate rule for the expert system. 3. Symbolic Primitive selection: For the next stage, each datastream is required to be assigned a symbolic primitive. Fig. 5 shows an example f four trends that can be used, however, additional primitives can be dded to this list assuming appropriate parameters are defined for each ew primitive. . Parameter Calculation: The final stage before a rule can be prouced involves the setting of the individual parameters. Algorithm 1 hows pseudocode for how the symbolic primitive can be defined for ndividual time segments using the four previously defined primitives. s before this can be updated with the addition of new symbolic rimitives. . Diagnostic Rule Induction: Implementation of the pseudocode hown in Algorithm 1, allows for the symbolic primitives to be calcuated for any input datastream. A diagnostic rule can then be stored n the knowledge base for any number of datastreams, as shown in able 1.
. Case study: Tennessee eastman process simulation data
.1. Background
The Tennessee Eastman process is a hypothetical set of interconected industrial processes based on the physical behaviours of actual lant that was modelled computationally by Downs and Vogel (1993). he process contains five major units: the condenser, compressor,
l t t t d
reactor, vapour/liquid separator and the product stripper, as shown in Fig. 6. This dataset has been used extensively in many areas including fault detection, plant-wide control, and statistical process monitoring. In Lomov, Lyubimov, Makarov, and Zhukov (2021) a temporal deep earning model was proposed and in Krishnannair and Aldrich (2017) he use of nonlinear singular spectrum analysis is proposed both for he purposes of fault detection. The dataset used in this section is he ‘‘TEP_Faulty_Training’’ portion of the Tennessee Eastman Process ataset (Averkiev, 2020). This contains 52 individual monitored process variables, detailed in Table 7, and 8. This portion of the dataset contains 500 simulations of 20 specific fault types each with 500 samples per fault. The sampling rate for the dataset was 3 min, therefore giving approximately 40 years worth of condition monitoring data with complete ground truth. 5.2. knowledge capture Using the process proposed in this paper the knowledge necessary to capture each of the five faults in the dataset was captured. For the application case study, as there were no domain experts involved in the process, all 52 datastreams were assessed visually to determine which datastreams were necessary to detect the given fault. To provide substantially different faults, fault numbers 1, 2, 5, 6, and 7 (as defined in the dataset) were selected for analysis. For Fault 1, this was assessed as xmeas_1, xmeas_4, xmeas_7, xmeas_13, xmeas_16, xmeas_19, xmeas_22, xmeas_34, xmv_3, xmv_4, and xmv_9, the individual time segments relating to fault 1 are shown in Fig. 7. For each of the ten datastreams selected, three randomly selected examples of fault 1 were selected. The signal to symbol transformation was then performed. For the rising and falling trends the average rise or fall was taken as the 𝑦 parameters, and the max variation was taken as the 𝑧 parameter. Fluctuating trends are defined as the number of mean crossing from the segment of data, therefore, the 𝑦 was defined as the max mean crossings out of the three examples. Finally, the stable trends were defined as the range where less than 50% of the data was
outside of the boundary. The extracted knowledge for Fault 1 is shown in Table 2. As there are 500 samples per fault in the dataset the time interval for each piece of knowledge was set to 500. This process was then repeated for the remaining 4 faults, and the extracted knowledge
is shown in Tables 2 to 6.
mentioned in the tables were set as ‘‘don’t care’’ states. This produced a knowledge-based expert system that contained 5 rules that were able to accurately detect Faults 1, 2, 5, 6 and 7 from the Tennessee Eastman Process dataset. 5.3. results Using this approach it was possible to produce a system that can accurately detect specific types of faults in the condition monitoring dataset. Out of the 10,000-time segments analysed only 8 (0.08%) false positives were produced, these were 6 from fault class 1 and 2 from fault class 2. Slightly more false positives were produced, 55 (0.55%) fault class 5 and 7 faults class 7 were detected as no-fault. However, as this system was being designed for fault diagnostics, more false positives and fewer false negatives was preferred. There was also 2 (0.02%) instances of fault class 1 that was detected as fault class 5, however, upon further analysis, it was visually impossible to differentiate these two instances for the general fault 5 class. Fig. 8 presents a confusion matrix for the full system. 6. comparison with state-of-the-art The methods highlighted in Section 3 are general approaches that can be applied to the capture of a wide range of knowledge from a broad range of domains. In this paper, we focus on the capture of a specific class of knowledge, that is associated with the interpretation of time series data. By constraining the type of knowledge being elicited allows additional structure to be placed around the representation of the captured knowledge, which in turn offers the opportunity to streamline the process. Comparing approaches for the capture of human expertise is challenging, not least in that no two subject matter experts will respond to the interview process in the same manner, and attempting to elicit the same knowledge from the same expert, using two different methods in consecutive sessions is going to favour the latter method as the expert will be re-running through the same material. To attempt to address this issue a study was performed which compared the capture of a set of condition assessment rules derived for the health of an industrial asset through the examination of multiple streams of time series data. A subset of the rules pertaining to a specific fault mode were captured using a structured interview process, while a second subset of rules for a different fault mode was captured using the proposed methodology. In the former, 2 three hour knowledge elicitation sessions were held with the domain experts and it was possible to understand the problem, capture the relevant datastreams, and capture the relevant rules, however, within these two sessions it was not possible to parametrise the rules. For the same problem, in 1 two hour knowledge elicitation session it was possible to capture the relevant datastreams, capture the relevant rules, and parametrise the faults detected in four different case study datasets. While the specifics of the rules and parameters captured in each of these approaches are different, it was felt that they were suitably similar to permit valid comparisons in terms of the time required to be drawn. In conclusion, comparing the proposed technique with the standard structured interview, showed in this example a twofold saving in time. 7. conclusion and future work This paper has introduced a new approach to tackle the knowledge elicitation process for a sub-set of problems, namely the codification and capture of knowledge involved in the interpretation of time series data. This type of problem is seen across many engineering domains and is particularly prevalent in the development of expert systems for supporting the health assessment of industrial assets. While knowledgedriven approaches, such as expert systems were prominent in the 1980’s and 1990’s advances in neural networks, and in particular the advent of deep neural networks have resulted in significant advances in many AI applications. However, for many engineering applications such as health monitoring and assessment of industrial assets, there
is still the need for the explainability that is currently unavailable
from purely data drive black-box approaches. As a result, the capture and codification of specialist knowledge relating to the interpretation of condition monitoring data is of importance, and the issue of the knowledge elicitation bottleneck still exists. This proposed approach has been shown to offer benefits in terms of time-saving. Historically, the main drawback to the development of these types of systems was the time involved in the capturing of domain expert knowledge from already time-poor engineers. The reduction in time, specifically in the knowledge elicitation phase, from a business perspective and the development of these types of systems places much less of a time burden on the already busy engineers. This new approach to performing knowledge elicitation for knowledge-based systems allows for the parameterisation of domain expert knowledge and the rapid prototyping of an expert system. This is important as there is an inherent brittleness in traditional expert systems due to the formalisation of the knowledge, and the ease in which the knowledge can be updated. It was demonstrated using a case study that it was possible to produce a knowledge-based system that can detect specific faults quickly and accurately. Also using this
A
A
B
C
C
approach to formulate the knowledge in a symbolic manner results in the explanation of the results in an accessible way that does not require any specialist training. Due to the nature of how an expert system is designed using this approach, updating specific parameters allows for the iterative improvement of the knowledge base and hence the system without the need to redesign the entire system. This paper represents a step towards reducing the knowledge elicitation bottleneck, but there are limitations to the proposed approach. It currently focuses only on the interpretation of time series data and anticipates the captured knowledge will reason about trends in data. Furthermore, temporal relationships between multiple data streams are relatively common in condition monitoring data but have not been fully addressed here. For example, seeing an increase in one parameter, flow rate, for example, may result in a corresponding, but delayed increase in value in another parameter, say temperature. Secondly, the elicitation process is still predominantly a human to human process, which is then transferred to the expert system. This data imbalance, where there are often large volumes of normal operating data, and very few instances of failure data is another common challenge, and capture and codification of this diagnostic knowledge in a manner that they can be shared across multiple instances of expert systems remains an open problem.
See Tables 7 and 8.