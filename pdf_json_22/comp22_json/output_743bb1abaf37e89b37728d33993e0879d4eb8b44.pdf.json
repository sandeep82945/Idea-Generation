{
    "abstractText": "Semantic Web or Knowledge Graphs (KG) emerged to one of the most important information source for intelligent systems requiring access to structured knowledge. One of the major challenges is the extraction and processing of unambiguous information from textual data. Following the human perception, overlapping semantic linkages between two named entities become clear due to our common-sense about the context a relationship lives in which is not the case when we look at it from an automatically driven process of a machine. In this work, we are interested in the problem of Relational Resolution within the scope of KGs, i.e, we are investigating the inherent semantic of relationships between entities within a network. We propose a new adaptive AutoEncoder, called V-Coder, to identify relations inherently connecting entities from different domains. Those relations can be considered as being ambiguous and are candidates for disentanglement. Likewise to the Adaptive Learning Theory (ART), our model learns new patterns from the KG by increasing units in a competitive layer without discarding the previous observed patterns whilst learning the quality of each relation separately. The evaluation on real-world datasets of Freebase, Yago and NELL shows that the V-Coder is not only able to recover links from corrupted input data, but also shows that the semantic disclosure of relations in a KG show the tendency to improve link prediction. A semantic evaluation wraps the evaluation up.",
    "authors": [
        {
            "affiliations": [],
            "name": "Christian M.M. Frey"
        },
        {
            "affiliations": [],
            "name": "Matthias Schubert"
        }
    ],
    "id": "SP:3b0ca6285efca8b1e1b404847b530b26b000d471",
    "references": [
        {
            "authors": [
                "Indrajit Bhattacharya",
                "Lise Getoor"
            ],
            "title": "Collective Entity Resolution in Relational Data",
            "venue": "ACM Trans. Knowl. Discov. Data",
            "year": 2007
        },
        {
            "authors": [
                "Kurt Bollacker",
                "Colin Evans",
                "Praveen Paritosh",
                "Tim Sturge",
                "Jamie Taylor"
            ],
            "title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge",
            "venue": "In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data (Vancouver, Canada) (SIGMOD \u201908)",
            "year": 2008
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garcia-Dur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko"
            ],
            "title": "Translating Embeddings for Modeling Multi-Relational V-Coder: Adaptive AutoEncoder for Semantic Disclosure in Knowledge Graphs Data. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe, Nevada) (NIPS\u201913)",
            "year": 2013
        },
        {
            "authors": [
                "Leonardo Enzo Brito da Silva",
                "Islam Elnabarawy",
                "Donald C. Wunsch"
            ],
            "title": "A Survey of Adaptive Resonance Theory Neural Network Models for Engineering Applications",
            "venue": "Neural Netw. 120, C (Dec",
            "year": 2019
        },
        {
            "authors": [
                "G.A. Carpenter",
                "S. Grossberg"
            ],
            "title": "A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine",
            "venue": "Massachusetts Institute of Technology,",
            "year": 1988
        },
        {
            "authors": [
                "Jinxiu Chen",
                "Donghong Ji",
                "Chew Lim Tan",
                "Zhengyu Niu"
            ],
            "title": "Unsupervised Relation Disambiguation with Order Identification Capabilities",
            "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Sydney,",
            "year": 2006
        },
        {
            "authors": [
                "Aron Culotta",
                "Andrew McCallum"
            ],
            "title": "Joint Deduplication of Multiple Record Types in Relational Data",
            "venue": "In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (Bremen, Germany) (CIKM \u201905)",
            "year": 2005
        },
        {
            "authors": [
                "Tim Dettmers",
                "Minervini Pasquale",
                "Stenetorp Pontus",
                "Sebastian Riedel"
            ],
            "title": "Convolutional 2D Knowledge Graph Embeddings",
            "venue": "In Proceedings of the 32th AAAI Conference on Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research 12,",
            "year": 2011
        },
        {
            "authors": [
                "Lise Getoor",
                "Ashwin Machanavajjhala"
            ],
            "title": "Entity Resolution: Theory, Practice & Open Challenges",
            "venue": "Proc. VLDB Endow. 5,",
            "year": 2012
        },
        {
            "authors": [
                "S. Grossberg"
            ],
            "title": "Adaptive Pattern Classification and Universal Recoding: I. Parallel Development and Coding of Neural Feature Detectors",
            "venue": "Biol. Cybern. 23,",
            "year": 1976
        },
        {
            "authors": [
                "Xu Han",
                "Shulin Cao",
                "Lv Xin",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Juanzi Li"
            ],
            "title": "OpenKE: An Open Toolkit for Knowledge Embedding",
            "venue": "In Proceedings of EMNLP",
            "year": 2018
        },
        {
            "authors": [
                "Seyed Mehran Kazemi",
                "David Poole"
            ],
            "title": "SimplE Embedding for Link Prediction in Knowledge Graphs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations",
            "venue": "San Diego,",
            "year": 2014
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu"
            ],
            "title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion",
            "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (Austin, Texas)",
            "year": 2015
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Yuexin Wu",
                "Yiming Yang"
            ],
            "title": "Analogical Inference for Multi-relational Embeddings",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Farzaneh Mahdisoltani",
                "Joanna Biega",
                "Fabian M. Suchanek"
            ],
            "title": "YAGO3: A Knowledge Base from Multilingual Wikipedias",
            "venue": "In CIDR. www.cidrdb.org",
            "year": 2015
        },
        {
            "authors": [
                "George A. Miller"
            ],
            "title": "WordNet: A Lexical Database for English. COMMUNICA",
            "venue": "TIONS OF THE ACM",
            "year": 1995
        },
        {
            "authors": [
                "Erhard Rahm",
                "Philip A. Bernstein"
            ],
            "title": "A Survey of Approaches to Automatic Schema Matching",
            "venue": "The VLDB Journal 10,",
            "year": 2001
        },
        {
            "authors": [
                "Sebastian Riedel",
                "Limin Yao",
                "Andrew McCallum",
                "Benjamin M. Marlin"
            ],
            "title": "Relation Extraction with Matrix Factorization and Universal Schemas",
            "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,",
            "year": 2013
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N. Kipf",
                "Peter Bloem",
                "Rianne van den Berg",
                "Ivan Titov",
                "Max Welling"
            ],
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "venue": "In Proceedings of the 15th Extended Semantic Web Conference (ESWC)",
            "year": 2018
        },
        {
            "authors": [
                "Parag Singla",
                "Pedro Domingos"
            ],
            "title": "Entity Resolution with Markov Logic",
            "venue": "In Proceedings of the Sixth International Conference on Data Mining (ICDM \u201906). IEEE Computer Society,",
            "year": 2006
        },
        {
            "authors": [
                "Richard Socher",
                "Danqi Chen",
                "Christopher D Manning",
                "Andrew Ng"
            ],
            "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
            "venue": "In Advances in Neural Information Processing Systems 26,",
            "year": 2013
        },
        {
            "authors": [
                "Fabian M. Suchanek",
                "Gjergji Kasneci",
                "Gerhard Weikum"
            ],
            "title": "Yago: A Core of Semantic Knowledge. In Proceedings of the 16th International Conference on World WideWeb (Banff, Alberta, Canada) (WWW \u201907)",
            "venue": "Association for Computing Machinery,",
            "year": 2007
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang"
            ],
            "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
            "venue": "In International Conference on Learning Representations",
            "year": 2019
        },
        {
            "authors": [
                "Sheila Tejada",
                "Craig A. Knoblock",
                "Steven Minton"
            ],
            "title": "Learning Object Identification Rules for Information Integration",
            "venue": "Inf. Syst. 26,",
            "year": 2001
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Danqi Chen",
                "Patrick Pantel",
                "Hoifung Poon",
                "Pallavi Choudhury",
                "Michael Gamon"
            ],
            "title": "Representing Text for Joint Embedding of Text and Knowledge Bases",
            "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
            "year": 2015
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Eric Gaussier",
                "Guillaume Bouchard"
            ],
            "title": "Complex Embeddings for Simple Link Prediction",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Kai Wang",
                "Yu Liu",
                "Xiujuan Xu",
                "Dan Lin"
            ],
            "title": "Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network",
            "year": 2018
        },
        {
            "authors": [
                "Quan Wang",
                "Zhendong Mao",
                "Bin Wang",
                "Li Guo"
            ],
            "title": "Knowledge Graph Embedding: A Survey of Approaches and Applications",
            "venue": "IEEE Trans. Knowl. Data Eng. 29,",
            "year": 2017
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen"
            ],
            "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
            "venue": "In AAAI",
            "year": 2014
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Oksana Yakhnenko",
                "Nicolas Usunier"
            ],
            "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
            "year": 2013
        },
        {
            "authors": [
                "Steven Euijong Whang",
                "Hector Garcia-Molina"
            ],
            "title": "Joint Entity Resolution on Multiple Datasets",
            "venue": "The VLDB Journal 22,",
            "year": 2013
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Thien Hoang",
                "William Yang Wang"
            ],
            "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
            "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Bishan Yang",
                "Scott Wen-tau Yih",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng"
            ],
            "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR) 2015 (proceedings of the international conference on learning representations (iclr) 2015 ed.)",
            "year": 2015
        },
        {
            "authors": [
                "Zhicheng Zheng",
                "Xiance Si",
                "Fangtao Li",
                "Edward Y. Chang",
                "Xiaoyan Zhu"
            ],
            "title": "Entity Disambiguation with Freebase",
            "venue": "IEEE Computer Society,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Information systems \u2192 Clustering; \u2022 Computing methodologies \u2192 Neural networks; Cluster analysis; Topic modeling.\nKEYWORDS Semantic Web, knowledge graph, adaptive resonance theory, adaptive AutoEncoder, Relation Resolution"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In 2010 Google introduced their concept of \u2018Knowledge Graphs\u2019 (KGs) to a broader community and has since then been used as a term in computer science articles, papers and diverse publications. Whereas graphs have always been pervasive as a major research area in Artificial Intelligence (AI), there is nowadays a need to enable machines to understand and giving them the ability to infer new knowledge. Therefore, one of the main intuition behind Knowledge Graphs was to be able to search for \u2018things not strings\u2019 opening the gate of reasoning about entities, attributes and relationships which comes along with this kind of structured data in the form of graphs. For example, in order to enrich search queries over the web, Google developed their so-called knowledge panels in their web\ninterface. This enables to provide additional information in a condensed form to the user where this information is extracted from the underlying knowledge graph. With Google being undoubtedly one of the pioneers in this area, other companies started using the concept of representing knowledge in forms of graph andwe can observe even KG-centric startups arising for all kinds of applications such as in government, economic or non-profit and last but not least the KG ecosystem emerged to one of the major research fields in cognitive automation systems. An interesting problem arises from the given or inherent ontology on which the KG is built upon on. Having a hierarchical semantic tree in mind we can ask ourselves questions about the types of entities, semantic of relationships, level of granularity of resolutions for entities as well as linkages, connections across levels, level of perception we want to provide and so on. Whereas entity resolution is a well-studied problem [10], we want to emphasize the resolution for relationships in this work. Relation Resolution is the problem of devising algorithms solutions for determining linkages between two entities within a knowledge graph. Hence, we focus on the inherent semantics coming along with connections between entities being of valuable importance whenever researchers are faced with linked data, including the communities for knowledge discovery and data mining, databases and semantic web. The most important challenge in automating Relation Resolution is the ambiguity of the extracted information. The relationships within a knowledge graph have an inherent nature of a common-sense which allows by human perception to tell whenever two linkages are the same or not, i.e., to tell that two relationships have a different semantic between entities although the linkages are labeled with the same identifier. This phenomena can also be observed in natural language, where we can infer information just by setting it in a broader context. For example, if ar X iv :2 20 8. 01 73 5v 1\n[ cs\n.A I]\n2 2\nJu l 2\n02 2\nwe just consider a relationship \u2019write\u2019, we cannot infer any broader information from it with a sufficient high level of confidence. If we have the information of the context, e.g., the entities \u2019Voltaire\u2019 and \u2019Candide ou l\u2019Optimisme\u2019, from a human perception point of view, we automatically infer the information of the first entity (subject) being an author having written (predicate \u2019write\u2019) a piece of work called \u2019Candide ou l\u2019Optimisme\u2019 (object). This results from the human common-sense about nature and things we are aware of. Knowledge graphs encode information such that information can also be interpreted and extracted automatically by a machine. Therefore, the information is encoded via semantic triples having the structure (subject, predicate, object). Nevertheless, the predicate \u2019write\u2019 is not solely used to connect authors with their works, but may also connect musicians and their piece of works, directors with their screenplays or in modern terms people write blog entries and so on (c.f. figure 1).\nHence, from a mathematical point of view, the information provided by \u2019write\u2019 tries to connect several subspaces from the subjects with subspaces from the objects, namely the ones in which the various entities (authors, composers, directors, ...) lives in. The crucial part here is that these subspaces might be independent from each other resulting in the worst case in lower performance of link prediction tasks in a knowledge graph. Moreover, the mentioned context is given by entities in the neighborhood of a node which is connected via links carrying a specific semantic information. By that, a machine can use this semantic information and represent it. But it fails in distinguishing the inherent more fine-granular semantics of an identifier (e.g., a string). From the human perception the linkage of e.g. member_of can be interpreted from different angles such as connections of people to sport clubs, parties, organizations, enterprises, clans and so on. Nevertheless, a machine will deal with all entities being connected by this link in the same way even though these entities represent various and probably independent groups. In the following we will deal with this problem of Relation Resolution in more detail and show that we are able to automatically extract ambiguous relations and separate them semantically from each other.\nOutline. In section 2, we give an overview of published works being relevant for our task at hand. In section 3, a summary of important definitions are given being used throughout the presentation of our framework and a short recap of theAdaptive Resonance Theory is presented. We introduce our novel architecture, the VCoder in section 4. The results of the evaluation are presented in section 5. Finally, section 6 concludes this paper."
        },
        {
            "heading": "2 RELATEDWORKS",
            "text": "Starting from the 2010s the research community could observe a rapid growth in knowledge graph (KG) construction and applications. Not only in the number of public available datasets like, Freebase [2], Wordnet [19] or Yago [25] but also technically finesses weighting the inherent semantic meanings which naturally comes along with the structure of a knowledge graph. Embedding Methods In the trend of representing high-dimensional data in a lower dimensional space knowledge graphs are processed by embeddings models for which the interested reader is referred to a thorough\noverview given by [31]. It is worth to mention that the challenge of embeddings is to find lower dimensional manifolds by taking the information given by an entity (and its neighborhood) rather than dealing with the resolution of the entities or of the relationships. A lower-dimensional representation can then further be used for tasks like link-based clustering grouping objects in relational data being similar, hence, exploiting the homophily property.\nTranslational methods. One of the pioneers work was proposed by Bordes et al. in 2013 when introducing TransE [3]. In this work the learned representation of relations served as translations in vector space. Further improvements were introduced subsequently based on this model, e.g., TransH [32] introduced relation-specific hyperplanes with a normal vector, TransR [15] introduced relation-specific spaces where entity representations are projected into the space specific to a relation with a projection matrix from the entity space to the relation space. In 2018, RotatE was introduced in [26] proposing a rotation-based translational method but in complex space.\nSemantic Matching Models. Rather than translating vectors such that they match, semantic matching models measures the plausibility of relations by matching latent semantics of entities and relations. Yang et al. introduced DistMult in [36] where each relation is represented as a diagonal interaction matrix between the head and tail embeddings. SimplE [13] introduced a bilinear approach where head entity serve also as tail entities and vice versa. In [29], the authors introduced ComplEx, extending DistMult by working with embeddings in the complex space C such that asymmetric relations have more expressiveness notion in the model. A model which subsumed all of the mentioned semantic matching models is ANALOGY [16].\nMatching with Neural Networks. A Neural Tensor Network [24] uses projection of entities in the embedding space and combines themwith a relation-specific tensor. Neural AssociationModel works with concatenations of vector embeddings and feds them into a deep neural network such that the output of the last hidden layer and the embedding of the tail entity tells us the matching score. More recently, [8] introduced ConVE, a convolution neural networks, graph convolutional networks were introduced by [22], or deep memory networks in [30].\nNevertheless, even though the above methods represents the relational information in a lower-dimensional space, they are not dealing with the relational resolution in order to uncover the inherent semantics of a relation. Disambiguation Tasks In the community of Natural Language Processing (NLP) we can find more specific tasks taking care of the Named Entity Disambiguation (NED) or Named Entity Linking whose primary goal is to assign unique identifiers to entities being mentioned in a text. For example, Einstein won the Noble Prize in Physics in 1921. The entity mention of \"Einstein\" should be linked to the entity of Albert Einstein. NED can be considered as a follow-up module in a pipeline with Named Entity Recognition (NER) in first place. For example in [37] the authors present an approach on how to solve disentangling ambiguous entities for the Freebase dataset. Another framing of the problem would be Entity Resolution[1, 23, 34] (or Schema Matching [20], De-duplication [7], Object Identification [27]) where the task would be to enrich the inherent ontology for the\nvarious entity types. This is of specific usage when decisions are made for all object which can be referred to the same domain rather than doing them independently for each object pair. On the other hand objects referring to the same type are merged together.\nResearch groups approached the topic of knowledge graphs also from an Information Extraction (IE) point of view which aims at generating structured graph data from text populating a knowledge base. More related to what we are concerned about, the relations, is the sub-task of Relation Extraction (RE)which assigns themost likely type of the relation given by a knowledge base. Such alignments were proposed for example with the combination of embedding model [33] or with the help of latent factors given by a matrix factorization [21].\nOur main task of Relation Resolution (which could also be framed as Named Relation Disambiguation (NRD), Relation De-duplication refers to the problem of disentangling different relations linking named entities which were extracted from a text and therefore form a knowledge base. A more related work is given by [6] presenting an unsupervised approach for relation disambiguation in a text by an elongated k-means approach performing for each increment of \ud835\udc58 a Spectral clustering."
        },
        {
            "heading": "3 FUNDAMENTALS",
            "text": ""
        },
        {
            "heading": "3.1 Adaptive Resonance Theory",
            "text": "3.1.1 Basics of ART. TheAdaptive Resonance theory (ART) [11]was introduced tomimic biologically processes of how a brain learns and adapts to patterns in a constantly changing environment. Its terms \u2019adaptive\u2019 and \u2019resonance\u2019 are used to underline the functionality of the system being able to learn new patterns without discarding the previous observed patterns, resp., the old information. Therefore, the resonance in the system is regulated in the architecture with feedback. The ART networks are known to solve the so-called Stability-plasticity dilemma [5], where stability refers to their nature of memorizing the learning and plasticity refers to the fact that they adapt towards new information. Nevertheless, plasticity can lead to instabilities in the system where new knowledge leads to a loss of previously learned information. This phenomenon is also know es catastrophic forgetting which is addressed by the ART architectures.\nIn general, ART models implement a clustering algorithm where the input is presented to the networks and themodel checkswhether it fits into one of the already learned clusters. If such a cluster cannot be found, a new cluster is formed. Hence, it follows an unsupervised learning approach.\n3.1.2 Basic Architecture of ART models. The essence of the ART is that it is self-organizing as well as competitive. Generally, various models have been introduced for unsupervised learning tasks (e.g. ART1, ART2, etc.) or supervised ones (e.g. ARTMAP). The basic ART model is unsupervised and consists of various layers:\n\ud835\udc391 Layer: this layer denotes the input layer and propagates the input samples to the \ud835\udc392 layer via bottom-up long-term memory units (LTMs) \ud835\udf03\ud835\udc4f\ud835\udc62 . As in the feedback mode, this layer provides the information being compared to the expectation of \ud835\udc392\u2019s output (via a top-down LTM \ud835\udf03\ud835\udc61\ud835\udc51 ), it is also known as comparison layer.\n\ud835\udc392 Layer: this layer yields the network output \ud835\udc66\ud835\udc392 and serves as competitive layer for categories. The LTM associated with a specific category \ud835\udc57 is described with \ud835\udf03 \ud835\udc57 = {\ud835\udf03\ud835\udc4f\ud835\udc62\ud835\udc57 , \ud835\udf03 \ud835\udc61\ud835\udc51 \ud835\udc57 }\nOrienting Subsystem: this module acts as control mechanism by inhibiting or allowing categories to resonate.\nGenerally, a sample \ud835\udc65 is fed to the network and a winner-takesall competition over the categories takes place at \ud835\udc392 w.r.t to some objective function (e.g. similarity metrics). Afterwards, the orienting subsystem determines the adequacy of the selected category. According to a threshold (vigilance parameter \ud835\udf0c), either the system selects the best category and the system enters a resonance state and adapts the assigned LTM units. If the orienting subsystem rejects the category, a new one is created to encode the new sample. Hence, the vigilance parameter helps to incorporate new memories or new information. Higher vigilance produces more detailed memories, lower vigilance produces more general memories. For the interested reader, we would like to refer to the survey about various ART models presented by Silva et al. [4].\nIn the following, we will introduce our architecture which is designed similar to the ART models but where the weights are updated automatically within an AutoEncoder model. Because of the adaptive response of our new AutoEncoder model, we refer to it as the \"Adaptive AutoEncoder\"."
        },
        {
            "heading": "3.2 Terminology of KG",
            "text": "The term \u2019knowledge Graph\u2019 has been emerged as a buzz word when speaking of applications on complex networks, and hence, the terminology blurs the line to other related topics terms like Knowledge Base or Ontology.\nOntologies themselves describe semantic modeling of knowledge and inherits not only the classes and properties but we can also have realizations (instances) of the ontology comprising entities. Due do the linkages defined by the ontology, the system gains semantic information. Even though this might already cover large parts where the term knowledge graph has been used in the literature, a characteristic of the KG is its reasoning capabilities which demarcates it from pure ontologies, resp., knowledge bases. The reasoning module is used to gain new (semantic) information.\nIn our work, we shine a light on the semantic linkage, therefore, we are interested in the underlying ontology of the graph at hand where our reasoning module is defined in chapter 4. Formally, we can describe the knowledge graph by a set of entities E = {\ud835\udc521, . . . , \ud835\udc52\ud835\udc41\ud835\udc52 } and a set of relation types R = {\ud835\udc5f1, . . . , \ud835\udc5f\ud835\udc41\ud835\udc5f }. A triple (\u210e, \ud835\udc5f, \ud835\udc61) models the interaction between the entities \u210e : \u210e\ud835\udc52\ud835\udc4e\ud835\udc51 and \ud835\udc61 : \ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59 by the relation type (predicate) \ud835\udc5f ."
        },
        {
            "heading": "4 ADAPTIVE AUTOENCODER",
            "text": "From cognitive science we know that inhibitory control allows us to control our attention, thoughts and leads to our behavior of how we act. It enables us also to suppress impulses from our environment which might tempt our disposition and lead to old habits and to conditioned responses. Therefore, inhibitory control allows us to adapt to our environment such that we can change our behavior, enables us to think about our habits and gives us last but not least choices. This phenomenon leads us to the terminology of Lateral Inhibition which describes that a neuron\u2019s response to\na stimulus is inhibited by the excitation of a neighboring neuron. In cognitive science, this is further studied in the so-called \u2019stopsignal\u2019 paradigm [17]. In general, an individual performs a \u2019go\u2019 task after an imperative stimulus occurred and a \u2019stop\u2019 signal leads to an inhibition of the individuals\u2019 response.\nIn this spirit we present an architecture suppressing signals to neighboring neuron within the same layer to create a bottleneck where information can pass through. In the next section, we discuss the V-Coder more formally."
        },
        {
            "heading": "4.1 V-Coder",
            "text": "4.1.1 Logic of ARTs in V-Coder. The V-Coder inherits the 2-fold idea from ART in a way such that the F1 layer (accepts and transforms the input data) is described in our model as the Encoder module. The output of our encoder model is then processed in a way that resembles the functionality of the F2 layer. We pass the output to single neurons in the hidden layer between Encoder and Decoder of an AutoEncoder. Whereas in the vanilla AE this layer acts as the description of the input vectors in latent space, we interpret it in the V-Coder as the competitive layer, where only single neurons are activated. This inherits the idea of cognitive control mechanism. Likewise to traditional ART models, where the candidate neuron can now learn the input pattern, in our model, the information can also just flow through these selected neurons. In comparison to ART, where the following vigilance test would decide upon the clustering, we measure the reconstruction loss in order to decide whether the input can be assigned to the selected neuron on the competitive layer or not. The idea is that for similar input data, the output of the encoder is similar, and therefore, a neuron in the competitive layer can be selected which subsume these similar input patterns. The information which is passed from the encoder module to the selected neuron on the competitive layer is used for the reconstruction in the Decoder module. For similar input data, the reconstruction loss is fairly low compared to input data where we have a lot of variance from the encoding step.\n4.1.2 Encoding of input data. In the area of knowledge graphs our input data consists of facts. For the sake of brevity, we will discuss our approach on static knowledge bases where we have triples (\u210e\ud835\udc52\ud835\udc4e\ud835\udc51, \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc52, \ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59). Nevertheless, we would like to mention that if the semantic of a link is not changing over time, the approach could also be applied to non-static knowledge bases, also-called temporal knowledge graph (tKG), which can be represented by a set of quadruples (timedependent facts) \ud835\udc3a = {\ud835\udc60, \ud835\udc5f, \ud835\udc5c, \ud835\udc61 |\ud835\udc60, \ud835\udc5c \u2208 E, \ud835\udc5f \u2208 R \ud835\udc61 \u2208 T }, where E is\nthe set of entities, R is a set of relations and T denotes the temporal domain. In this case the predicate links two entities \ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 and \ud835\udc5c\ud835\udc4f \ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 only at a certain point in time or within a time interval.\nAs described above, the first step of our model is to compute a lower-dimensional representation, which we will refer to as a fingerprint of an entity \ud835\udc65 . The role of an entity in a knowledge graph is described by its incident relationsN(\ud835\udc65) = {\ud835\udc5f |\u2203\ud835\udc66 \u2208 E . (\ud835\udc65, \ud835\udc5f,\ud835\udc66)}. For example, an unknown entity with incident relations \u2019acts\u2019,\u2019nominee\u2019, \u2019awarded_to\u2019 let us infer that the entity is most probably a person working in film industry as an actor.\nWe can construct a binary vector representation 1\ud835\udc65 \u2208 {0, 1} |R | for an entity by its incident relations, which we define as:\n1\ud835\udc65 = { 1 if \ud835\udc5f\ud835\udc58 \u2208 N (\ud835\udc65) 0 otherwise\n(1)\nAfter the relational information of the head and tail entity of a fact (h,r,t) is translated to binary vectors, we concatenate them 1\u210e | |1\ud835\udc61 and use them as input for a deep forward network, an encoder parameterized by \ud835\udf03\ud835\udc52\ud835\udc5b\ud835\udc50 , yielding the fingerprint for the topology, denoted by \u03a6\u210e,\ud835\udc61 . In other words, the function \u03a6 : \ud835\udc48 \u2192 \ud835\udc45\ud835\udc51 describes a non-linear transformation from the space \ud835\udc48 \u2286 1\u210e \u00d7 1\ud835\udc61 to a lower \ud835\udc51-dimensional vector in the real vector space. Hence, the highly sparse representation are transformed to a dense lower dimensional space. The procedure is illustrated in figure 2.\n4.1.3 Lateral Inhibition. Avanilla AutoEncoder is designed such that we impose a bottleneck in the network forcing to compress our original input data into a lower-dimensional space. If the input data inherits some sort of structure, e.g., correlation between input feature, the model is able to learn it. However, if the input features are independent from each other, the compression task and therefore the reconstruction in the decoder is a difficult task.\nIn our architecture, we can think of the layer between the encoder and decoder - which in the AE case computes the latent representation of the input data - as the competitive layer as they have been used in ART architectures. Hence, the activation of the competitive layer is defined by a sigmoid function to squash the receiving data to the range [0,1]. The neuron with the maximal value is the one being considered as the only active one, therefore, transforming the idea of lateral inhibition from the field of cognitive science into our architecture. All other neurons are considered as inactive.\nMore formally the competitive layer with lateral inhibition receives the input data from the previous layer. Recall, that the previous layer is the output of the encoder module, therefore, we are\nreceiving the fingerprint \u03a6. For each neuron in the competitive layer we are calculating the activations in a straightforward way:\n\u210e \ud835\udc57 = \ud835\udf0e ( \u2211\ufe01 \ud835\udc64\ud835\udc56 \ud835\udc57\u03a6\ud835\udc56 ), (2)\nwhere the weights\ud835\udc64\ud835\udc56 \ud835\udc57 denote the weights connecting the encoder\u2019s output with the competitive layer and \ud835\udf0e (\u00b7) defining an activation function. In the same spirit as the F2-layer of ART networks, we define the winner neuron \ud835\udc57 in an unsupervised learning setting as the one with the highest score: \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc650\u2264 \ud835\udc57\u2264\ud835\udc58\ud835\udf0e ( \u2211 \ud835\udc64\ud835\udc56 \ud835\udc57\u03a6\ud835\udc56 ).\nThe winner neuron is now the only neuron the information can pass through, therefore, is responsible for the reconstruction of the received data. All other neurons are inhibited.\nIn order to reconstruct the input data, we now calculate the Hadamard product of the received fingerprint with the weights connecting it to the winner neuron. The score of the winner neuron is used as scaling parameter. Therefore, we define the input to the decoder module - parameterized by \ud835\udf03\ud835\udc51\ud835\udc52\ud835\udc50 - as (cf. figure 4):\n\ud835\udc63 \ud835\udc57 = (\ud835\udc4a:, \ud835\udc57 \u2299 \u03a6)\u210e \ud835\udc57 , (3) Finally, The output layer yields the reconstruction \ud835\udc67 \ud835\udc57 = \ud835\udc37\ud835\udc52\ud835\udc50 (\ud835\udc63 \ud835\udc57 ) where the mean squared error (MSE) defines the reconstruction loss. As the information passed solely one node in the competitive layer, we define it as a conditioned loss:\n\ud835\udc3d (\ud835\udc65, \ud835\udc67 | \ud835\udc57) = \u2225\ud835\udc65 \u2212 \ud835\udc67 \ud835\udc57 \u22252, (4) where \ud835\udc57 denotes the active unit in the competitive layer and \ud835\udc67 \ud835\udc57 defines the reconstruction based upon the information it received according to equation 3.\n4.1.4 Adaptive Step. In our setting of semantic disclosure in a knowledge graph, we integrate a supervision upon the competitive layer. We exploit the fact that there is a limited expressiveness when the information flows through single nodes in the competitive layer. If the input data, i.e., the fingerprints, do not yield any correlation within their latent features, then we conclude that the topologies of the original entities are too diverse. In other words, similar entities have common incident relation types, and therefore, the encoder yields similar fingerprints for those entities. The supervision we integrate in the V-Coder is the relation type connecting two entities. Hence, the number of neurons in the competitive layer are the number of relation types |R | in the KG. Considering the graph in figure 2, the edge connecting the two entities refers to a specific unit in the competitive layer. Using the additional supervision in the application\nof semantic disclosure, we get a quality measure for a fingerprint w.r.t a specific relation by passing the information through the respective unit in the competitive layer. Consequently, we get the information which relation has to capture too many information in the sense that it has to express too diverse fingerprints. A high variance in the reconstruction is therefore an indicator that a relation is too general and needs a further inspection in the underlying ontology. Likewise to traditional ART networks, we extend the size of the competitive layer such that the information results in different neurons (cluster), i.e., we extend the semantic of a relation. Let \ud835\udc36 = {\ud835\udc500, . . . , \ud835\udc50 \ud835\udc57 , . . . , \ud835\udc50\ud835\udc5a} be the units in the competitive layer. Suppose that for the unit \ud835\udc50 \ud835\udc57 we receive the highest variance w.r.t the reconstruction loss. The V-Coder selects this unit and extends the layer by one additional unit \ud835\udc50\ud835\udc5a+1. Then it receives again data whose supervision was formerly conditioned on \ud835\udc50 \ud835\udc57 , we now allow the information to select the best one amongst the set {\ud835\udc50 \ud835\udc57 , \ud835\udc50\ud835\udc5a+1} in the manner described in 4.1.3, i.e., we select as winner neuron the one resulting in the minimal reconstruction loss:\nargmin \ud835\udc50 \ud835\udc57 \u2208\ud835\udc36\n\ud835\udc3d (\ud835\udc65, \ud835\udc37\ud835\udc52\ud835\udc50 (\ud835\udc63 \ud835\udc57 ) |\ud835\udc50 \ud835\udc57 ) = \u2225\ud835\udc65 \u2212 \ud835\udc37\ud835\udc52\ud835\udc50 (\ud835\udc63 \ud835\udc57 )\u22252, (5)\nThe adaption step, i.e., increasing the number of neurons in the competitive layer (plasticity) can be done without touching any other information learned so far within the network. Note that the encoder module (\ud835\udf03\ud835\udc52\ud835\udc5b\ud835\udc50 ) and decoder module (\ud835\udf03\ud835\udc51\ud835\udc52\ud835\udc50 ) are preserved, as well as the weights from the output of the encoder to the neurons in the competitive layer (stability). We extend the weight matrix connecting the encoder with the competitive layer by one additional dimension\ud835\udc4a:,\ud835\udc5a+1 =\ud835\udc4a:, \ud835\udc57 which creates the linkage to the new unit in the competitive layer (c.f. clustering ART models)."
        },
        {
            "heading": "5 EVALUATION",
            "text": "5.1 Settings 5.1.1 Datasets. For our evaluation, we use three benchmark datasets:\n\u2022 FB15k-237 [28] was created from FB15k by removing the inverse of many relations from the training set as well as from\nthe validation and test set which makes the link prediction task more difficult \u2022 YAGO3-10 [18] is a subset of YAGO3 consisting of entities that have a minimum of 10 relations each. It has 123.182 entities and 37 relations. Most of the triples deal with descriptive attributes of people, such as \u2019citizenship\u2019, \u2019gender\u2019, \u2019profession\u2019 or \u2019marital status\u2019\n\u2022 NELL-995 [35] is a subset of the 995-th iteration of NELL The statistics of the datasets are summarized in table 6.\n5.1.2 Implementation Details. All experiments were implemented in Python 3.7.3 with PyTorch 1.2.0. For the computation of the link prediction we use the OpenKE framework [12] which is an open-source package for the computation of knowledge graph embeddings.\n5.1.3 Experimental Setup. In our evaluation we use a single hidden layer for the encoder module of dimension {16, 32} encoding the fingerprint of the input triple. The reconstruction in the decoder module uses also one hidden layer equally sized. The number of neurons in the competitive layer are given by the number of relational types within a dataset.\nWe run the clustering procedure for 20 epochs. After that, the cluster neuron with the highest variance in the reconstruction loss is selected and splitted into two units whilst preserving the learned information of other nodes. For tackling bias in the new cluster nodes, we use an exponential decay function\n\ud835\udc5f = (\ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 + (\ud835\udf16\ud835\udc52\ud835\udc5b\ud835\udc51 \u2212 \ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 )) \u2217 \ud835\udc52\ud835\udc65\ud835\udc5d (\u2212\ud835\udc61 \u00b7 \ud835\udf16\ud835\udc51\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc66), (6)\nwhere \ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 , \ud835\udf16\ud835\udc52\ud835\udc5b\ud835\udc51 and \ud835\udc52\ud835\udc51\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc66 (decay factor) are hyperparameters and \ud835\udc61 is the current iteration number. After a splitting operation, this heuristic initially explores both units to be the best one for the input data and introduces some randomness till it homes in to the one with minimal reconstruction loss. This procedure is beneficial due to the unknown cluster size, resp., semantic rewirings in the knowledge graph. We set the learning rate of the V-Coder from {0.01, 0.001, 0.0001} with a batch size in the range of {16, 32, 64}, a weight decay of 0.001 and the epsilon values of: \ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 = 1.0, \ud835\udf16\ud835\udc52\ud835\udc5b\ud835\udc51 = 0.01, \ud835\udf16\ud835\udc51\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc66 \u2208 {1\ud835\udc52 \u2212 4, 1\ud835\udc52 \u2212 5, 1\ud835\udc52 \u2212 6}. As optimizer we are using ADAM [14]."
        },
        {
            "heading": "5.2 Analytical Evaluation",
            "text": "5.2.1 Effectiveness of V-Coder. The first part of the evaluation are hand-crafted experiments to show evidence that the proposed V-Coder indeed separates links according to their inherently semantic structure within the knowledge graph. For that, we randomly select links within the datasets FB15K-237 and NELL-995 to create a copy where 2 links are merged.\nHence, we loss relational types by mixing up the semantic information. Just according to the incident relations towards entities being linked to the new created links, the task for the V-Coder is to uncover again the original relations and separate them such that they reproduce the linkages of the original dataset. In the competitive layer of the V-Coder we pass therefore additional information through the selected neuron being now in charge for both original semantics. As the merging operation might not result in the highest corruption within the dataset (which might be also of low sample rates within the input), we tell the V-Coder which neuron in the competitive layer to split. After the splitting operation, the V-Coder learns again the assignments of the input data. The results for various recovery tasks are shown in table 3. For example, when merging the two relations with ids 124 (/education/educational_institution/school_type) and 16 (/organization/organization/headquarters./location/mailing_address/country) from FB15K-237 in a new dataset, the V-Coder reproduces the linkages for the id 124 with 96.29% accuracy and for 16 with 85.39%, resulting in an overall reproducibility of 90.84%. Note that the quality of the recovery is dependent on the sample size in the original dataset. The V-Coder recognized different fingerprints which are then passed to different neurons in the competitive layer such that we can split the hand-made relation again into the original ones.\n5.2.2 Error Reduction.\nNext, we discuss the individual error reduction achieved by the VCoder. For our discussion we use the FB15K-237 and the YAGO3-10 datasets. Analysing the individual error terms, we can observe that some fingerprints, and therefore, characteristics of the knowledge graph can be learnt effortless. A low reconstruction loss results from the fact that similar input topologies have similar fingerprints. Apart from that we also observe links where the variance of the reconstruction loss is high which results from topologies where the fingerprints are too diverse to be expressible by one active neuron. This is illustrated in the case of FB15K-237 in figure 5 and for YAGO3-10 in figure 7. The reason for that lies in the underlying dataset, more precisely, in the ambiguous semantic a link carries. In the illustration we see that the relation with the id 15 is highly difficult to reconstruct which is the relation \"/location/location/contains\". The ambiguity results from the fact that \"contains\" is a generalization interlinking entities from various domains. For example, \"contain\" might describe that a city is \"contained\" within a state, a university is \u2019contained\u2019 within a city, or a federal state is \"contained\" within a country. Therefore, this relation can be regarded as a candidate for a semantic disclosure and the question is if the disentangling helps in reducing the reduction error. The answer is illustrated in 6. In our example, the link\n\u2019/music/.../parent_genre\u2019] Figure 6: Reconstruction loss after splitting operation in competitive layer;\nLink 15 splitted in [15, 237] (top), Link 228 splitted in [228, 238] (bottom). Red lines indicate averages of the losses.\nwith id \"15\" is chosen to be splitted as it exceeds its capabilities to encompass all the input data which is directed to it. On top of fig 6, we see that after splitted into the relations with the ids 15,237, the mean loss is reduced leading to a better description of the relations. Also in the case of the relation with id 228 being splitted into the relation 228, 238, we observe a denoising of the reconstruction loss, hence, reducing the variance in the reconstruction.\n5.2.3 Link Prediction Results. In this section, we show the effects on the link prediction task.\nOur evaluation metrics follows the standard on link prediction tasks, i.e, we use mean reciprocal rank (MRR) and Hits@k, with \ud835\udc58 \u2208 {1, 10}. Mean reciprocal rank denotes the average of the inverse of themean rank assigned to the true triple over all candidate triples, whereas Hits@k measures the percentage of times a true triple is ranked within the top-\ud835\udc58 candidate triples.\nWe choose state-of-the-art embedding methods being also suitable for large knowledge graphs like YAGO3-10. For the single embeddings methods we choose the learning rate from {0.0001, 0.001, 0.01, 0.1}, the weight decay from {0.001, 0.001, 0.01, 0.1} and \ud835\udc51 for the dimensionality of the latent space from {100, 150, 200}. We train the models using Adam [14] and Adagrad [9] with a batch size ranging in {64, 128, 256}.\nThe results are summarized in table 4, where we show the results on the original datasets as well as on the datasets after passing the V-Coder, i.e., after a splitting of the relations has been executed. Note that, for the run on the extended datasets, we fix all hyperparameters having been used on the original dataset. The suffixes in the datasets denote the number of splits having been made. Note that we are not feeding the system with an increased number of samples at hand to boost the link prediction dramatically., but we arrange it new w.r.t the new semantics. Nevertheless, as we can\nsee, through the separation of relations we can observe small improvements on the various datasets. These improvements can be explained by the re-arranged features in latent space as well as the additional expressiveness of the gained relations. Hence, disentangling the manifolds in the low-dimensional space helps for the link prediction itself."
        },
        {
            "heading": "5.3 Semantic Evaluation",
            "text": "In figure 6, we show that the V-Coder identifies the relation with id 15:\u2019location/location/contains\u2019 in the dataset FB15K-237 as the one with the highest variance on the reconstruction loss. Not only our common-sense already signals that \u2019contains\u2019 can connect various entities from different domains, but also the V-Coder identifies this relation as being ambiguous in the sense that the fingerprints created by the encoder vary such that leading the information through one neuron on the competitive layer we lose too much information for the reconstruction. Hence, to tackle the ambiguity, the V-Coder creates a new unit which creates a new cluster in the relational domain. In table 5 we show in decreasing order the number of entities which are now captured by two units on the competitive layer. We see that the original relation 15 indeed related entities from different domains such as countries, states, rivers, cities but also entities for colleges/universities (not shown in the table). Hence, the V-Coder enriches the knowledge graph automatically-driven by a new semantic relation splitting the entities up such that similar fingerprints are interlinked by the relations with id 15 and a new relation with id 237 (notice that the ids are zero-based). What we can observe is that the V-Coder now tunnels countries wellseparated through the unit 237, whereas states and counties are captured by the unit 15. In conclusion, the V-Coder clusters similar fingerprints on country-level on one side, and similar fingerprints resulting from state-level on the other side and enriches therefore the knowledge graph by a new semantic relation. This follows our\nintuition of the original relation, where a sub-level in the ontology could be extended by the relations \u2019contains_state\u2019, \u2019contains_city\u2019, \u2019contains_county\u2019, etc."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we introduced a novel adaptive model, called the V-Coder. It inherits the unsupervised learning idea from Adaptive Learning Theory (ART) and wraps it up in an architecture similar to an AutoEncoder. We showed that the V-Coder is able to identify relations which inherently connects entities from various domains. Those relations can be considered as being ambiguous and are candidates for disentanglement. For that, a competitive\nlayer carries the idea of lateral inhibition which suppresses information to flow through neighboring nodes. Upon the variance in the reconstruction loss, the information is inferred which (cluster) neuron is overloaded with information, and hence, yields an indicator to split this (cluster) neuron into two units. Therefore, the V-Coder adaptively changes their size on the competitive layer while learning the clusters information. In our application on semantic disclosure in knowledge graphs, we show a first application of the V-Coder. The evaluation shows that the V-Coder is able to recover semantic links from corrupted input data. By enriching a knowledge graph with new semantic information, we can show the tendency to improve link prediction tasks on the benchmark datasets FB15K-237, YAGO3-10 and NELL-995. This augmentation is justified by semantic reasoning based on the existence of independent semantic clusters for a relation being present in the input knowledge graph. For future works, we would like to extend the idea of V-Coder also to other architectures and applications for dynamic neural nets."
        },
        {
            "heading": "A RELATIONAL DESCRIPTIONS",
            "text": "The semantics of the relations being used throughout the paper are as follows:"
        }
    ],
    "title": "V-Coder: Adaptive AutoEncoder for Semantic Disclosure in Knowledge Graphs",
    "year": 2022
}