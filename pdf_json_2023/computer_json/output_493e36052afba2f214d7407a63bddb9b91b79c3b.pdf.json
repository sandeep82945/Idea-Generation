{
    "abstractText": "Bayesian Physics Informed Neural Networks (B-PINNs) have gained significant attention for inferring physical parameters and learning the forward solutions for problems based on partial differential equations. However, the overparameterized nature of neural networks poses a computational challenge for high-dimensional posterior inference. Existing inference approaches, such as particle-based or variance inference methods, are either computationally expensive for highdimensional posterior inference or provide unsatisfactory uncertainty estimates. In this paper, we present a new efficient inference algorithm for B-PINNs that uses Ensemble Kalman Inversion (EKI) for high-dimensional inference tasks. By reframing the setup of B-PINNs as a traditional Bayesian inverse problem, we can take advantage of EKI\u2019s key features: (1) gradient-free, (2) computational complexity scales linearly with the dimension of the parameter spaces, and (3) rapid convergence with typically O(100) iterations. We demonstrate the applicability and performance of the proposed method through various types of numerical examples. We find that our proposed method can achieve inference results with informative uncertainty estimates comparable to Hamiltonian Monte Carlo (HMC)-based B-PINNs with a much reduced computational cost. These findings suggest that our proposed approach has great potential for uncertainty quantification in physics-informed machine learning for practical applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "ANDREW PENSONEAULT"
        },
        {
            "affiliations": [],
            "name": "XUEYU ZHU"
        }
    ],
    "id": "SP:8336a74e8d849ebb594f54b15197aa67076073f4",
    "references": [
        {
            "authors": [
                "Jeffrey L Anderson"
            ],
            "title": "An adaptive covariance inflation error correction algorithm for ensemble filters",
            "venue": "Tellus A: Dynamic meteorology and oceanography,",
            "year": 2007
        },
        {
            "authors": [
                "Harbir Antil",
                "Howard C Elman",
                "Akwum Onwunta",
                "Deepanshu Verma"
            ],
            "title": "Novel deep neural networks for solving bayesian statistical inverse",
            "venue": "arXiv preprint arXiv:2102.03974,",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Ba",
                "Murat A Erdogdu",
                "Marzyeh Ghassemi",
                "Shengyang Sun",
                "Taiji Suzuki",
                "Denny Wu",
                "Tianzong Zhang"
            ],
            "title": "Understanding the variance collapse of svgd in high dimensions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Nathan Baker",
                "Frank Alexander",
                "Timo Bremer",
                "Aric Hagberg",
                "Yannis Kevrekidis",
                "Habib Najm",
                "Manish Parashar",
                "Abani Patra",
                "James Sethian",
                "Stefan Wild",
                "Karen Willcox",
                "Steven Lee"
            ],
            "title": "Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence",
            "venue": "Technical report, Office of Scientific and Technical Information,",
            "year": 2019
        },
        {
            "authors": [
                "C Basdevant",
                "M Deville",
                "P Haldenwang",
                "J.M Lacroix",
                "J Ouazzani",
                "R Peyret",
                "P Orlandi",
                "A.T Patera"
            ],
            "title": "Spectral and finite difference solutions of the burgers equation",
            "venue": "Computers and Fluids,",
            "year": 1986
        },
        {
            "authors": [
                "Tyrus Berry",
                "Timothy Sauer"
            ],
            "title": "Adaptive ensemble kalman filtering of non-linear systems",
            "venue": "Tellus A: Dynamic Meteorology and Oceanography,",
            "year": 2013
        },
        {
            "authors": [
                "David M. Blei",
                "Alp Kucukelbir",
                "Jon D. McAuliffe"
            ],
            "title": "Variational inference: A review for statisticians",
            "venue": "Journal of the American Statistical Association,",
            "year": 2017
        },
        {
            "authors": [
                "Imke Botha",
                "Matthew P. Adams",
                "Dang Khuong Tran",
                "Frederick R. Bennett",
                "Christopher Drovandi"
            ],
            "title": "Component-wise iterative ensemble kalman inversion for static bayesian models with unknown measurement error covariance, 2022",
            "year": 2022
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Daniele Ceccarelli"
            ],
            "title": "Bayesian physics-informed neural networks for inverse uncertainty quantification problems in cardiac electrophysiology",
            "year": 2021
        },
        {
            "authors": [
                "Neil K Chada",
                "Marco A Iglesias",
                "Lassi Roininen",
                "Andrew M Stuart"
            ],
            "title": "Parameterizations for ensemble kalman inversion",
            "venue": "Inverse Problems,",
            "year": 2018
        },
        {
            "authors": [
                "Neil K. Chada",
                "Andrew M. Stuart",
                "Xin T. Tong"
            ],
            "title": "Tikhonov regularization within ensemble kalman inversion, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Neil K. Chada",
                "Xin T. Tong"
            ],
            "title": "Convergence acceleration of ensemble kalman inversion in nonlinear settings, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Chong Chen",
                "Yixuan Dou",
                "Jie Chen",
                "Yaru Xue"
            ],
            "title": "A novel neural network training framework with data assimilation",
            "venue": "The Journal of Supercomputing,",
            "year": 2022
        },
        {
            "authors": [
                "Yan Chen",
                "Dean S Oliver"
            ],
            "title": "Ensemble randomized maximum likelihood method as an iterative ensemble smoother",
            "venue": "Mathematical Geosciences,",
            "year": 2012
        },
        {
            "authors": [
                "Yuming Chen",
                "Daniel Sanz-Alonso",
                "Rebecca Willett"
            ],
            "title": "Reduced-order autodifferentiable ensemble kalman filters",
            "venue": "arXiv preprint arXiv:2301.11961,",
            "year": 2023
        },
        {
            "authors": [
                "Adam D. Cobb",
                "Brian Jalaian"
            ],
            "title": "Scaling hamiltonian monte carlo inference for bayesian neural networks with symmetric splitting, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Simon Duane",
                "A.D. Kennedy",
                "Brian J. Pendleton",
                "Duncan Roweth"
            ],
            "title": "Hybrid monte carlo",
            "venue": "Physics Letters B,",
            "year": 1987
        },
        {
            "authors": [
                "Samuel Duffield",
                "Sumeetpal S. Singh"
            ],
            "title": "Ensemble kalman inversion for general likelihoods",
            "venue": "Statistics and Probability Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Geir Evensen"
            ],
            "title": "The ensemble kalman filter: Theoretical formulation and practical implementation",
            "venue": "Ocean dynamics,",
            "year": 2003
        },
        {
            "authors": [
                "Geir Evensen",
                "Femke C. Vossepoel",
                "Peter Jan van Leeuwen"
            ],
            "title": "Low-Rank Ensemble Methods. In Data Assimilation Fundamentals: A Unified Formulation of the State and Parameter Estimation Problem, pages 73\u201377,79\u201393",
            "year": 2022
        },
        {
            "authors": [
                "Philipp A. Guth",
                "Claudia Schillings",
                "Simon Weissmann"
            ],
            "title": "Ensemble kalman filter for neural network based one-shot inversion",
            "venue": "ArXiv, abs/2005.02039,",
            "year": 2020
        },
        {
            "authors": [
                "Eldad Haber",
                "Felix Lucka",
                "Lars Ruthotto"
            ],
            "title": "Never look back-a modified enkf method and its application to the training of neural networks without back propagation",
            "venue": "arXiv preprint arXiv:1805.08034,",
            "year": 2018
        },
        {
            "authors": [
                "Martin Hanke"
            ],
            "title": "Regularizing properties of a truncated newton-cg algorithm for nonlinear inverse problems",
            "venue": "Numerical Functional Analysis and Optimization,",
            "year": 1997
        },
        {
            "authors": [
                "Daniel Zhengyu Huang",
                "Jiaoyang Huang",
                "Sebastian Reich",
                "Andrew M. Stuart"
            ],
            "title": "Efficient derivative-free bayesian inference for large-scale inverse problems, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Zhengyu Huang",
                "Tapio Schneider",
                "Andrew M Stuart"
            ],
            "title": "Iterated kalman methodology for inverse problems",
            "venue": "Journal of Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "Marco A Iglesias"
            ],
            "title": "Iterative regularization for ensemble data assimilation in reservoir models",
            "venue": "Computational Geosciences,",
            "year": 2015
        },
        {
            "authors": [
                "Marco A Iglesias"
            ],
            "title": "A regularizing iterative ensemble kalman method for PDE-constrained inverse problems",
            "venue": "Inverse Problems,",
            "year": 2016
        },
        {
            "authors": [
                "Marco A Iglesias",
                "Kody J H Law",
                "Andrew M Stuart"
            ],
            "title": "Ensemble kalman methods for inverse problems",
            "venue": "Inverse Problems,",
            "year": 2013
        },
        {
            "authors": [
                "Xinchao Jiang",
                "Xin Wanga",
                "Ziming Wena",
                "Enying Li",
                "Hu Wang"
            ],
            "title": "An e-pinn assisted practical uncertainty quantification for inverse problems, 2022",
            "year": 2022
        },
        {
            "authors": [
                "George Em Karniadakis",
                "Ioannis G Kevrekidis",
                "Lu Lu",
                "Paris Perdikaris",
                "Sifan Wang",
                "Liu Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Nikola B Kovachki",
                "Andrew M Stuart"
            ],
            "title": "Ensemble kalman inversion: a derivative-free technique for machine learning tasks",
            "venue": "Inverse Problems,",
            "year": 2019
        },
        {
            "authors": [
                "Fran\u00e7ois Le Gland",
                "Val\u00e9rie Monbet",
                "Vu-Duc Tran"
            ],
            "title": "Large sample asymptotics for the ensemble Kalman filter",
            "venue": "Research Report RR-7014,",
            "year": 2009
        },
        {
            "authors": [
                "Hong Li",
                "Eugenia Kalnay",
                "Takemasa Miyoshi"
            ],
            "title": "Simultaneous estimation of covariance inflation and observation errors within an ensemble kalman filter. Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography",
            "year": 2009
        },
        {
            "authors": [
                "Guang Lin",
                "Yating Wang",
                "Zecheng Zhang"
            ],
            "title": "Multi-variance replica exchange sgmcmc for inverse and forward problems via bayesian pinn",
            "venue": "Journal of Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "Ignacio Lopez-Gomez",
                "Costa Christopoulos",
                "Haakon Ludvig Langeland Ervik",
                "Oliver R.A. Dunbar",
                "Yair Cohen",
                "Tapio Schneider"
            ],
            "title": "Training physics-based machine-learning parameterizations with gradient-free ensemble kalman methods",
            "venue": "Journal of Advances in Modeling Earth Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lu Lu",
                "Xuhui Meng",
                "Zhiping Mao",
                "George Em Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Review,",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Alekseevich Morozov"
            ],
            "title": "Methods for solving incorrectly posed problems",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Bayesian learning for neural networks, volume 118",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Theodore Papamarkou",
                "Jacob Hinkle",
                "M Todd Young",
                "David Womble"
            ],
            "title": "Challenges in markov chain monte carlo for bayesian neural networks",
            "venue": "Statistical Science,",
            "year": 2022
        },
        {
            "authors": [
                "Apostolos F Psaros",
                "Xuhui Meng",
                "Zongren Zou",
                "Ling Guo",
                "George Em Karniadakis"
            ],
            "title": "Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons, 2022",
            "year": 2022
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Luning Sun",
                "Jian-Xun Wang"
            ],
            "title": "Physics-constrained bayesian neural network for fluid flow reconstruction with sparse and noisy data",
            "venue": "Theoretical and Applied Mechanics Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Tandeo",
                "Pierre Ailliot",
                "Marc Bocquet",
                "Alberto Carrassi",
                "Takemasa Miyoshi",
                "Manuel Pulido",
                "Yicun Zhen"
            ],
            "title": "Joint Estimation of Model and Observation Error Covariance Matrices in Data Assimilation: a Review",
            "year": 2018
        },
        {
            "authors": [
                "Dilin Wang",
                "Zhe Zeng",
                "Qiang Liu"
            ],
            "title": "Stein variational message passing for continuous graphical models, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jared Willard",
                "Xiaowei Jia",
                "Shaoming Xu",
                "Michael Steinbach",
                "Vipin Kumar"
            ],
            "title": "Integrating scientific knowledge with machine learning for engineering and environmental systems, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Liang Yan",
                "Tao Zhou"
            ],
            "title": "Stein variational gradient descent with local approximations",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Liu Yang",
                "Xuhui Meng",
                "George Em Karniadakis"
            ],
            "title": "B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Xin-Lei Zhang",
                "Heng Xiao",
                "Xiaodong Luo",
                "Guowei He"
            ],
            "title": "Ensemble kalman method for learning turbulence models from indirect observation data",
            "venue": "Journal of Fluid Mechanics,",
            "year": 2022
        },
        {
            "authors": [
                "Zongren Zou",
                "Xuhui Meng",
                "Apostolos F Psaros",
                "George Em Karniadakis"
            ],
            "title": "Neuraluq: A 28 comprehensive library for uncertainty quantification in neural differential equations and operators",
            "venue": "arXiv preprint arXiv:2208.11866,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Key words. Bayesian Physically Informed Neural Networks, Inverse Problems, Ensemble Kalman Inversion, Gradient-free\n1. Introduction. Many applications in science and engineering can be accurately modeled by partial differential equations (PDEs). These equations often contain parameters corresponding to the physical properties of the system. In practice, these properties are often challenging to measure directly. Inverse problems arise when indirect measurements of the system are used to infer these model parameters. These problems are often ill-posed, with the available measurements being limited and noisy. Traditional approaches for solving these problems can be sensitive to noise and data scarcity. In addition, they often require many runs of sophisticated forward numerical solvers [46], which can be computationally expensive. Furthermore, it is not uncommon that the initial or boundary conditions are missing for real-world applications. In such a case, the traditional numerical forward solver might not even be able to run. To address these issues, there is growing interest in developing more efficient, robust, and flexible alternatives.\nRecently, Scientific Machine Learning (Sci-ML) [4, 42, 37, 41], a set of approaches that combine domain-specific scientific and engineering knowledge with powerful machine learning tools, has received much attention. These approaches have proven effective in solving PDE-based inverse problems efficiently. One particularly promising approach is Physics Informed Neural Networks (PINNs) [42, 31]. PINNs construct a neural network approximation of the forward solution to the underlying problem while inferring model parameters simultaneously by minimizing data misfit regularized by the underlying governing PDE residual loss. In addition to obtaining estimates of these quantities, the presence of noise and lack of data make it essential to quantify the impact of uncertainty on the surrogate and parameters, especially in high-stakes\n\u2217Department of Mathematics, University of Iowa, Iowa City, IA 52246, USA. Email: andrewpensoneault@uiowa.edu. \u2020Department of Mathematics, University of Iowa, Iowa City, IA 52246. USA. Email: xueyuzhu@uiowa.edu.\n1\nar X\niv :2\n30 3.\n07 39\n2v 1\n[ st\nat .M\nL ]\n1 3\nM ar\n2 02\n3\n2 applications [50, 41]. However, standard PINN approaches provide only deterministic estimates. Non-Bayesian approaches to uncertainty quantification (UQ) in PINN, such as neural network dropout [48], have been explored. Despite their efficiency, the estimates provided by these methods tend to be less satisfactory, as pointed out in [48, 30].\nAlternatively, several attempts to develop Bayesian PINNs (B-PINNs) have been explored, enabling uncertainty quantification in the neural network approximations and the corresponding physical parameter estimates. These methods utilize Bayesian Neural Networks (BNNs) as surrogates by treating weights of neural networks and physical parameters as random variables. In general, Markov Chain Monte Carlo (MCMC) methods are one of the most popular approaches for Bayesian inference tasks. These methods approximate the posterior distribution of parameters with a finite set of samples. While these approaches provide inference with asymptotic convergence guarantees, the use of standard MCMC methods in deep learning applications is limited by their poor scalability for large neural network architectures and large data sets [40]. In practice, Hamiltonian Monte Carlo (HMC) is the gold standard for inference for Bayesian neural networks [17]. HMC is an MCMC method that constructs and solves a Hamiltonian system from the posterior distribution. This approach enables higher acceptance rates than traditional MCMC methods. In the context of B-PINNs [48], an HMC-based B-PINN has been investigated for inference tasks. Despite this, the computational cost of inference remains high.\nAlternatively, Variational Inference (VI) approaches to B-PINNs [48] posit that the posterior distribution of the physical and neural network parameters lives within a family of parameterized distributions and solves a deterministic optimization problem to find an optimal distribution within that family [7]. In general, VI approaches are more efficient than HMC approaches and scale well for large parameter spaces and data sizes. Variational inference methods, however, do not share the same theoretical guarantees as MCMC approaches and only provide estimates within the function space of the family of parameterized distributions. Additionally, the inference quality depends on the space of parameterized densities chosen. For example, in [48], KL divergence-based B-PINNs tend to provide less satisfactory estimates than the corresponding HMC B-PINNs and are less robust to measurement noise.\nThe particle-based VI approach bridges the gap between VI and MCMC methods, combining the strengths of both approaches to provide more efficient inference than MCMC methods while also providing greater flexibility through non-parametric estimates in contrast with the parametric approximations utilized in VI [47]. In the context of B-PINNs, Stein Variational Gradient Descent (SVGD) has been proposed for inference tasks to reconstruct idealized vascular flows with sparse and noisy velocity data [48]. However, SVGD tends to underestimate the uncertainty of the distribution for high dimensional problems, collapsing to several modes [3, 45].\nRecently, Ensemble Kalman inversion (EKI) [29, 32, 28] has been introduced as a particle-based VI approach that uses the Ensemble Kalman filter algorithm to solve traditional Bayesian inverse problems. EKI methods have many appealing properties: they are gradient-free, easily parallelizable, robust to noise, and computationally scale linearly with ensemble size [36]. Additionally, these methods use low-rank approximate Hessian (and gradient) information, allowing for rapid convergence of the methods [49]. In the case of a Gaussian prior and linear Gaussian likelihood [33], Ensemble Kalman methods have asymptotic convergence to the correct posterior distribution in a Bayesian sense. Ensemble Kalman methods are asymptotically biased\n3 when these assumptions are violated, yet they are computationally efficient compared to asymptotically unbiased methods and empirically provide reasonable estimates [19, 8]. Recently, these methods have also been used to train neural networks efficiently [32, 23, 14, 22, 49]. In [32], EKI was first proposed as a derivative-free approach to train the neural networks but primarily for traditional purely data-driven machine learning problems. Recently, a one-shot variant of EKI [22] has been used to learn maximum a posteriori (MAP) estimates of a NN surrogate and model parameters for several inverse problems involving PDEs; however, this approach still requires traditional numerical solvers or discretizations to enforce the underlying physical laws, which can be computationally expensive for large scale complex applications. Additionally, while EKI has been traditionally used to obtain MAP estimates of unknown parameters, recent works have begun to investigate the use of the EKI methods to efficiently draw approximate samples for Bayesian inference for traditional Bayesian inverse problems [8, 19, 25, 26, 15].\nMotivated by the recent advances in EKI, we present a novel efficient inference method for B-PINNs based on EKI. Specifically, we first recast the setup of B-PINNs as a traditional Bayesian inverse problem so that EKI can be applied to obtain approximate posterior estimates. Furthermore, based on the variant of EKI in [32], we present an efficient sampling-based inference approach to B-PINNs. Because our approach inherits the properties of EKI, it provides efficient gradient-free inference and is well suited for large-scale neural networks thanks to the linear computational complexity of the dimension of unknown parameters [36]. Further, unlike the traditional setting for EKI [29], this approach replaces the expensive numerical forward solver with a NN surrogate trained on the measurements and the underlying physics laws to jointly estimate the physical parameters and forward solution with the corresponding uncertainty estimation. Because the trained neural network surrogate is cheap to evaluate, our proposed approach can draw a large number of samples efficiently and is thus expected to reduce sampling errors significantly. Through various classes of numerical examples, we show that the EKI method can efficiently infer physical parameters and learn valuable uncertainty estimates in the context of B-PINNs. Furthermore, the empirical results show that EKI B-PINNs can deliver comparable inference results to HMC B-PINNs while requiring significantly less computational time. To our best knowledge, this is the first attempt to use EKI for efficient inference under the context of B-PINNs.\nThe rest of the paper is organized as follows: in section 2, we first introduce the setup of the PDE-based inverse problem and briefly review B-PINNs. In section 3, we first reframe the problem in terms of a Bayesian Inverse problem and introduce EKI under the context of Bayesian inverse problems. We then introduce the building blocks of our proposed EKI B-PINNs framework. We demonstrate the performance of this approach via several numerical examples in section 4. Finally, we conclude in section 5.\n2. Problem Setup and Background. We consider the following partial differential equation (PDE):\nNx(u(x);\u03bb) = f(x) x \u2208 \u2126, (2.1) Bx(u(x);\u03bb) = b(x) x \u2208 \u2202\u2126, (2.2)\nwhere Nx and Bx denote the differential and boundary operators, respectively. The spatial domain D \u2286 Rd has boundary \u0393, and \u03bb \u2208 RN\u03bb represents a vector of unknown physical parameters. The forcing function f(x) and boundary function b(x) are given,\n4 and u(x) is the solution of the PDE. For time-dependent problems, we consider time t as a component of x and consider domain \u2126 and boundary \u2202\u2126 to additionally contain the temporal domain and initial boundary, respectively.\nIn this setting, we additionally have access toNu measurementsDu = {(xiu, ui)} Nu i=1\nof the forward solution at various locations. Given the available information, we aim to infer the physical parameters \u03bb with uncertainty estimation.\n2.1. Bayesian Physics Informed Neural Networks (B-PINNs). Over the last several years, SciML approaches for solving inverse problems have received much attention [4, 31]. One of the most promising approaches is the Physics Informed Neural Networks (PINNs), which approximate the forward solution u(x) with a fully connected neural network surrogate u\u0303(x;\u03b8), parameterized by neural network\u2019s weight parameter \u03b8 \u2208 RN\u03b8 . Denote \u03be = [\u03b8,\u03bb] as the concatenation of the neural network and physical parameters. The neural networks are trained by minimizing a weighted sum of the data misfit regularized by initial/boundary data misfit and underlying PDE residual loss (2.1) over a set of discrete points - \u201cresidual points\u201d and \u201cboundary points,\u201d respectively, in the domain as follows:\nDf = {(xif , f(xif ))} Nf i=1 = {(x i f , f i)}Nfi=1 (2.3)\nDb = {(xib, b(xib))} Nb i=1 = {(x i b, b i)}Nbi=1, (2.4)\nwith residual locations xif \u2208 \u2126 and boundary locations xib \u2208 \u2202\u2126. With these notions, the corresponding PINN loss function is defined as follows:\nL(\u03be) = \u03c9uLu(\u03be) + \u03c9fLf (\u03be) + \u03c9bLb(\u03be), (2.5)\nwhere\nLu(\u03be) = 1\nNu Nu\u2211 i=1 |ui \u2212 u\u0303(xiu;\u03b8)|2, (2.6)\nLf (\u03be) = 1\nNf Nf\u2211 i=1 |f i \u2212Nx(u\u0303(xif ;\u03b8);\u03bb)|2, (2.7)\nLb(\u03be) = 1\nNb Nb\u2211 i=1 |bi \u2212 Bx(u\u0303(xib;\u03b8);\u03bb)|2, (2.8)\nand \u03c9u, \u03c9f , and \u03c9b are the weights for each term. In practice, this loss is often minimized with ADAM or L-BFGS optimizers. Standard PINNs typically provide only a deterministic estimate of the target parameters \u03be [42, 37, 31]. These estimates may be inaccurate and unreliable for problems with small or noisy datasets. Therefore, qualifying the uncertainty in the estimate would be desirable.\nTo account for the uncertainty, Bayesian Physics-Informed Neural Networks (BPINNs) (e.g., [43, 48, 35, 2]) have been proposed. B-PINNs are built on Bayesian Neural Networks (BNNs) by treating neural network weights and biases \u03b8 and physical parameters \u03bb as random variables. By Bayes\u2019 theorem, the posterior distribution of the parameters \u03be conditions on the forward measurements Du, the residual points Df , and the boundary points Db can be obtained as follows:\np(\u03be|Du,Df ,Db) \u221d p(\u03be)p(Du,Df ,Db|\u03be). (2.9)\n5 The choice of prior distribution p(\u03be) and likelihood p(Du,Df ,Db|\u03be) will greatly affect the properties of the posterior distribution p(\u03be|Du,Df ,Db). A typical choice for the prior is to assume independence between the physical parameters \u03bb and neural network parameters \u03b8, i.e. p(\u03be) = p(\u03b8)p(\u03bb). Additionally, the neural network parameters \u03b8 = {\u03b8i}N\u03b8i=1 are often assumed to follow independent zero-mean Gaussian distributions, i.e.\np(\u03b8) = N\u03b8\u220f i=1 p(\u03b8i), p(\u03b8i) \u223c N ( 0, \u03c3i\u03b8 ) , (2.10)\nwhere \u03c3i\u03b8 is the standard deviation of the corresponding neural network parameter \u03b8i. For the likelihood, independence between the forward measurements Du, residual points Df , and boundary points Db is often assumed as follows:\np(Du,Df ,Db|\u03be) = p(Du|\u03be)p(Df |\u03be)p(Db|\u03be). (2.11)\nEach term within Du, Df , and Db is often assumed to follow a Gaussian distribution of the form\np(Du|\u03be) = Nu\u220f i=1 p(ui|\u03be), p(Df |\u03be) = Nf\u220f i=1 p(f i|\u03be), p(Db|\u03be) = Nb\u220f i=1 p(bi|\u03be), (2.12)\np(ui|\u03be) = 1\u221a 2\u03c0\u03c32\u03b7u exp\n( \u2212 ( ui \u2212 u\u0303(xiu;\u03b8) )2 2\u03c32\u03b7u ) , (2.13)\np(f i|\u03be) = 1\u221a 2\u03c0\u03c32\u03b7f exp\n\u2212 ( f i \u2212Nx(u\u0303(xif ;\u03b8);\u03bb) )2 2\u03c32\u03b7f  , (2.14) p(bi|\u03be) = 1\u221a\n2\u03c0\u03c32\u03b7b\nexp ( \u2212 ( bi \u2212 Bx(u\u0303(xib;\u03b8);\u03bb) )2 2\u03c32\u03b7b ) . (2.15)\nHere, \u03c3\u03b7u , \u03c3\u03b7f , and \u03c3\u03b7b are the standard deviations for the forward measurement, residual point, and boundary point, respectively. Choice of physical parameter prior p(\u03bb) is often problem-dependent, as this distribution represents domain knowledge of the corresponding physical property. Given these choices of prior and likelihood functions, one can construct the corresponding posterior distribution of the BNN. For most BNNs, the closed-form expressions for the posterior distribution are unavailable, and approximate inference methods must be employed. Due to the overparameterized nature of neural networks, the resulting Bayesian inference problem is often highdimensional for even moderately sized BNNs.\n2.2. Hamiltonian Monte Carlo (HMC). Next, we briefly review Hamiltonian Monte Carlo (HMC), a popular inference algorithm for B-PINNs [48] that serves as a baseline method for our proposed method. Hamiltonian Monte Carlo (HMC) is a powerful method for sampling based posterior inference [18] and has been utilized in the context of inference in Bayesian Neural Networks (BNNs) [39]. HMC employs Hamiltonian dynamics to propose states in parameter space with high acceptance in the Metropolis-Hastings acceptance step. Given the posterior distribution\n6 p(\u03be|Du,Df ,Db) = e\u2212U(\u03be), where U is the negative log-density of the posterior, we define the Hamiltonian dynamics as follows\nH(\u03be, r) = U(\u03be) + 1\n2 rTM\u22121r, (2.16)\nwhere r \u2208 RN\u03be is an auxiliary momentum vector, and M \u2208 RN\u03be\u00d7N\u03be is the corresponding mass matrix, often set to the identity IN\u03be . Starting from an initial sample of \u03be, the HMC generates proposal samples by resampling momentum r \u223c N (0,M) and advancing (\u03be, r) through Hamiltonian dynamics\nd\u03be dt = \u2212Mr, (2.17) dr dt = \u2212\u2207U(\u03be). (2.18)\nThis is often done via Leapfrog integration [18] for L steps given a step size \u03b4t. Following this, a Metropolis-Hastings acceptance step is applied to determine if the given sample will be accepted. The details of the HMC are shown in Algorithm 1. The variant HMC B-PINN used in this paper is based on the version in [50], to which we refer interested readers for more details.\nAlgorithm 1 Hamiltonian Monte Carlo (HMC)\n1: Input: \u03be0 (initial sample), \u03b4t (step size), L (leapfrog steps) 2: for i = 1, ..., J do 3: \u03bei \u2190 \u03bei\u22121 4: Sample ri \u223c N (0,M) 5: \u03be\u0302i \u2190 \u03bei 6: r\u0302i \u2190 ri 7: for j = 1, ..., L do 8: r\u0302i \u2190 r\u0302i \u2212 \u03b4t2 \u2207U(\u03be\u0302i) 9: \u03be\u0302i \u2190 \u03be\u0302i + \u03b4tM\u22121r\u0302i\n10: r\u0302i \u2190 r\u0302i \u2212 \u03b4t2 \u2207U(\u03be\u0302i) 11: end for 12: Sample p \u223c U(0, 1) 13: \u03b1\u2190 min[1, exp(H(\u03be\u0302i, r\u0302i)\u2212H(\u03bei, ri))] 14: if p < \u03b1 then 15: \u03bei \u2190 \u03be\u0302i 16: end if 17: end for 18: Return: \u03be1, ..., \u03beJ (Posterior samples)\nFrom the HMC algorithm, we obtain a set of approximate samples from the BPINNs posterior distribution p(\u03be|Du,Df ,Db). We shall use these samples to obtain uncertainty estimates of the approximate forward solution and physical parameters. Denote \u03bb\u0304 and u\u0304 to be the sample mean of physical parameter \u03bb and forward surrogate u\u0303, respectively. Additionally, we denote the corresponding sample standard deviations s\u03bb and su\u0303. We compute the sample statistics over the J samples {\u03bbj}Jj=1 and\n7 {u\u0303(x;\u03b8j)}Jj=1 obtain from the HMC as follows:\n\u03bb\u0304 = 1\nJ J\u2211 j=1 \u03bbj , u\u0304(x) = 1 J J\u2211 j=1 u\u0303(x;\u03b8j), (2.19)\ns\u03bb =\n\u221a\u2211J j=1 ( \u03bbj \u2212 \u03bb\u0304 )2 J \u2212 1 , su\u0303(x) = \u221a\u2211J j=1 (u\u0303(x;\u03b8j)\u2212 u\u0304(x)) 2 J \u2212 1 . (2.20)\n3. Ensemble Kalman Inversion-based B-PINNs. In this section, we first briefly review Ensemble Kalman Inversion (EKI) as an efficient method for solving Bayesian inverse problems. Following that, we present our proposed method, denoted EKI B-PINNs. Specifically, we first recast the setup of B-PINNs in the traditional Bayesian inverse problem setting and then employ EKI for efficient sampling-based inference.\n3.1. Ensemble Kalman Inversion (EKI). Ensemble Kalman Inversion (EKI) [29, 27, 28, 32] is a popular class of methods that utilize the Ensemble Kalman Filter (EnKF) [20] in the context of traditional inverse problems. These methods are derivative-free, easily parallelizable, and scale well in high-dimension inverse problems with ensemble sizes much smaller than the total number of parameters [32]. Assume that the unknown parameters \u03be \u2208 RN\u03be have a prior distribution p(\u03be) and the observations y \u2208 RNy are related to the parameters through the observation operator G:\ny = G(\u03be) + \u03b7, (3.1)\nwhere \u03b7 \u2208 RNy is a zero-mean Gaussian random vector with observation covariance matrix R \u2208 RNy\u00d7Ny , i.e., \u03b7 \u223c N (0, R). In the Bayesian context, this problem corresponds to the following posterior distribution:\np(\u03be|y) \u221d p(\u03be) exp ( \u2212 \u2225\u2225R\u22121/2(y \u2212 G(\u03be))\u2225\u22252 2\n2\n) . (3.2)\nGiven a prior of the form p(\u03be) \u223c N (\u03be0, C0), the posterior becomes\np(\u03be|y) \u221d exp \u2212 \u2225\u2225\u2225C\u22121/20 (\u03be0 \u2212 \u03be)\u2225\u2225\u22252 2 + \u2225\u2225\u2225R\u22121/2(y \u2212 G(\u03be))\u2225\u2225\u22252 2\n2  , (3.3) For weakly nonlinear systems, approximate samples from (3.3) can be obtained by minimizing an ensemble of loss functions of the form\nf(\u03be|\u03bej ,yj) = 1\n2 \u2225\u2225\u2225C\u22121/20 (\u03bej \u2212 \u03be)\u2225\u2225\u22252 2 + 1 2 \u2225\u2225\u2225R\u22121/2(yj \u2212 G(\u03be))\u2225\u2225\u22252 2 , (3.4)\nwhere \u03bej \u223c N (\u03be0, C0) and yj \u223c N (y, R) [15, 21]. EnKF-based methods such as EKI can be derived as an approximation to the minimizer of an ensemble of loss functions of the form (3.4) [21].\nIn practice, EKI and its variants consider the following artificial dynamics statespace model formulation based on the original Bayesian inverse problem (3.1) so that\n8 the EnKF update equations can be applied:\n\u03bei = \u03bei\u22121 + i, i \u223c N (0, Q), (3.5) yi = G(\u03bei) + \u03b7i, \u03b7i \u223c N (0, R), (3.6)\nwhere i is an artificial parameter noise term with the artificial parameter covariance Q \u2208 RN\u03be\u00d7N\u03be and \u03b7i represents the observation error with the observation covariance R \u2208 RNy\u00d7Ny . Given an initial ensemble of J ensemble members {\u03be(j)0 }Jj=1, the iterative EKI methods iteratively correct the ensemble {\u03be(j)i }Jj=1 based on Kalman update equations similar to [32]:\n\u03be\u0302 (j) i = \u03be (j) i\u22121 + (j) i ,\n(j) i \u223c N (0, Q), (3.7)\ny\u0302 (j) i = G(\u03be\u0302 (j) i ), (3.8)\n\u03be (j) i = \u03be\u0302 (j) i + C \u03be\u0302y\u0302 i (C y\u0302y\u0302 i +R) \u22121(y \u2212 y\u0302(j)i + \u03b7 (j) i ), \u03b7 (j) i \u223c N (0, R), (3.9)\nwhere C y\u0302y\u0302i and C \u03be\u0302y\u0302 i are the sample covariance matrices defined as follows:\nC y\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (y\u0302 (j) i \u2212 y\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T , (3.10)\nC \u03be\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (\u03be\u0302 (j) i \u2212 \u03be\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T . (3.11)\nHere, \u03be\u0304i and y\u0304i are the corresponding sample average of prior ensembles {\u03be\u0302(j)i } and {y\u0302(j)i }. We remark that for many EKI variants, Q = 0, which may lead to the ensemble collapsing [13]. This ensemble collapse is not observed for positive definite observation error covariance Q, and desirable convergence properties with reasonable uncertainty estimates have been shown for traditional Bayesian inverse problems [26, 25]. In the case of Gaussian prior and linear measurement operator, convergence to the correct Bayesian posterior corresponding to the artificial dynamics (3.5)-(3.6) can be shown [33]. However, these assumptions are typically not satisfied, and thus the corresponding estimates will be biased. Nonetheless, empirical evidence suggested that EKI can still provide reasonable posterior estimates even when these assumptions are violated [8].\n3.2. EKI B-PINN. While EKI-based methods have often been applied to estimate physical model parameters (e.g., [29, 11, 12]) under the context of traditional Bayesian inverse problems, these approaches often rely on existing numerical forward solvers or the corresponding discrete operators. As the EKI requires multiple evaluations of the numerical forward solvers over EKI iterations, this can be computationally expensive for large-scale complex applications. In contrast, our approach (EKI B-PINN) learns a neural network surrogate and infers the model parameters simultaneously without the need for a traditional numerical solver. Combining the inexpensive forward surrogate with EKI allows us to efficiently explore the high-dimensional posterior with larger ensemble sizes.\nTo employ EKI under the context of B-PINNs, we first recast the setup of BPINNs in section 2.1 by interpreting the corresponding notation in the context of EKI. Recall the notation introduced in B-PINNs section 2.1: the forward measurement\n9 Algorithm 2 Ensemble Kalman Inversion (EKI)\n1: Input: y (observations), Q (evolution covariance), R (observation covariance) 2: Initialize prior samples for j = 1, ..., J :\n\u03be (j) 0 \u223c p(\u03be).\n3: for i = 1, ..., I do 4: Obtain i-th prior parameter and measurement ensembles for j = 1, ..., J :\n(j) i \u223c N (0, Q), \u03b7 (j) i \u223c N (0, R).\n\u03be\u0302 (j) i = \u03be (j) i + (j) i . y\u0302 (j) i = Gi(\u03be\u0302 (j) i ).\n5: Compute the sample mean and covariance:\n\u03be\u0304i = 1\nJ J\u2211 j=1 \u03be\u0302 (j) i , y\u0304i = 1 J J\u2211 j=1 y\u0302 (j) i .\nC y\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (y\u0302 (j) i \u2212 y\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T .\nC \u03be\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (\u03be\u0302 (j) i \u2212 \u03be\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T .\n6: Update the posterior ensemble for j = 1, ..., J :\n\u03be (j) i = \u03be\u0302 (j) i + C \u03be\u0302y\u0302 i (C y\u0302y\u0302 i +R) \u22121(y \u2212 y\u0302(j)i + \u03b7 (j) i ).\n7: end for 8: Return: \u03be\n(1) I , ..., \u03be (J) I\nvector u = {ui}Nui=1, the residual vector f = {f i} Nf i=1 and boundary vector b = {bi} Nb i=1 from datasets Du = {(xiu, ui)} Nu i=1, Df = {(xif , f i)} Nf i=1 and Db = {(xib, bi)} Nb i=1. We utilize the concatenated physical and neural network parameters \u03be = [\u03bb,\u03b8]. With the approximate solution u\u0303(x;\u03b8) parameterized by \u03b8, the PDE operator Nx, and the boundary operator Bx, we can define the corresponding observation operator Gu, Gf , and Gb:\nGu(\u03be) = [u\u0303(x1u;\u03b8), ..., u\u0303(xNuu ;\u03b8)], (3.12) Gf (\u03be) = [Nx(u\u0303(x1f ;\u03b8);\u03bb), ...,Nx(u\u0303(x Nf f ;\u03b8);\u03bb)], (3.13) Gb(\u03be) = [Bx(u\u0303(x1u;\u03b8);\u03bb), ...,Bx(u\u0303(x Nb b ;\u03b8);\u03bb)]. (3.14)\nGiven these notations, we now define our measurement vector y and the corresponding measurement operator G(\u03be) under the context of EKI as follows:\ny = [u, f ,b], (3.15)\nG(\u03be) = [Gu(\u03be),Gf (\u03be),Gb(\u03be)]. (3.16)\n10\nAfter identifying each component in the EKI setting, we employ the version of EKI in (3.7)-(3.9) to infer the parameters of B-PINNs. Following that, we can then compute the sample means and standard deviations of ensemble {\u03bb(j)}Ji=1 of the physical parameter \u03bb and ensemble of approximate solutions {u\u0303(x;\u03b8(j))}Jj=1 via (2.19)-(2.20) as described in the section 2.2.\nWe remark that in EKI B-PINN, the underlying physics laws are enforced as soft constraints on the residual points. This approach contrasts with the standard EKI measurement operator, which enforces the physics law as hard constraints imposed by the traditional numerical solvers.\nChoice of Covariance matrices Q and R. The choice of observation covariance R and parameter evolution Q are often important in the Ensemble Kalman type algorithms, including the EKI algorithms. In the Ensemble Kalman methods literature, several attempts to automatically estimate these covariance matrices have also been suggested (e.g., [6, 34, 1]), however, there is no standard approach to estimating these matrices. In practice, R is often assumed to be known or estimated empirically from the instrument error and representation error between the states and observations [44]. The matrix Q is more difficult to estimate, as its dimension is often much larger than the number of available measurements.\nIn this paper, we assume R corresponds to the covariance matrix chosen for the Gaussian likelihood p(y|\u03be) in Section 2.1, i.e.,\nR = \u03c32\u03b7uINu 0 00 \u03c32\u03b7f INf 0 0 0 \u03c32\u03b7bINb  . (3.17) The choice of Q is important in this setting as it prevents the collapse of the ensemble and improves the uncertainty quantification in the EKI. In this study, we assume Q takes the form\nQ =\n[ \u03c32\u03b8IN\u03b8 0\n0 \u03c32\u03bbIN\u03bb\n] , (3.18)\nwhere \u03c3\u03b8 and \u03c3\u03bb are standard deviations associated with the neural weight parameters \u03b8 and the physical parameters \u03bb, respectively.\nStopping Criterion. We consider a stopping criterion based on the Discrepancy Principle [27, 29, 24, 38], originally utilized as a stopping criterion in iterative regularized Gauss-Newton methods, which has been used as a common choice in many EKI formulations. The discrepancy principle suggests an acceptable choice of \u03be for the inverse problem (3.1) can be made when\n||R\u22121/2(y \u2212 G(\u03be))|| \u2264 ||R\u22121/2(y \u2212 G(\u03be\u2020))||, (3.19)\nwhere \u03be\u2020 is the true solution of (3.1). This choice avoids instabilities in the solution and provides a criterion for stopping the EKI iteration. As the \u03be\u2020 is not known, often it is stated as\n||R\u22121/2(y \u2212G(\u03be))|| \u2264 \u03b7 (3.20)\nwhere \u03b7 > 0 is some stopping threshold. In the context of EKI, a sample mean-based discrepancy principle stopping criterion has been employed [27]:\u2225\u2225\u2225\u2225\u2225\u2225R\u22121/2 y \u2212 1 J J\u2211 j=1 G(\u03be(j)i ) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b7. (3.21)\n11\nThe choice of \u03b7 is dependent on R, which can be an issue if a reasonable choice of R is not clear, such as in the case with residual points Df and boundary points Db in the context of B-PINNs. Instead, we consider the relative change in the discrepancy over several iterations. By defining Di as the ith discrepancy metric as follows:\nDi = \u2225\u2225\u2225\u2225\u2225\u2225R\u22121/2 y \u2212 1 J J\u2211 j=1 G(\u03be(j)i ) \u2225\u2225\u2225\u2225\u2225\u2225 , (3.22) We shall stop at the iteration when the relative improvement of Di over a fixed iteration window of length W does not improve by more than \u03c4 , i.e.,\nmax j\u2208{i\u2212W,...,i} |Dj \u2212Di| Di < \u03c4. (3.23)\nRemark 3.1. If an EKI iteration results in a failure state which occurs in the\nfirst several iterations, we can reinitialize the ensemble from the prior \u03be (j) i \u223c p(\u03be).\nComplexity Analysis. At each EKI iteration in Algorithm 2 has following computational complexity of O(JNyN\u03be +N3y + JN2y ) detailed as follows:\n\u2022 O(JN2y ) - Construction of matrix C y\u0302y\u0302 in (3.10) \u2022 O(JN\u03beNy) - Construction of matrix C \u03be\u0302y\u0302 in (3.11) \u2022 O(N3y +JN2y +JN\u03beNy) - Evaluation of C \u03be\u0302y\u0302(C y\u0302y\u0302 +R)\u22121(y\u0302 (j) i +\u03b7 (j) i ) in (3.9).\nAs a result, the computational complexity of EKI grows linearly with both parameter dimension and ensemble size and thus can easily scale to high-dimensional inverse problems efficiently. Nevertheless, the method does scale cubically with data dimension and hence is computationally prohibitive when naively applied to large data sets. In this scenario, mini-batching, as in standard neural network optimization problems, can be used to reduce the computational cost [32]. Furthermore, it is important to note that the EKI requires storing an ensemble of J neural network parameter sets, which can be memory-demanding for large ensemble sizes and large networks. Dimension reduction techniques might be helpful to address this issue [16].\n4. Numerical Examples. In this section, we shall demonstrate the applicability and performance of EKI B-PINNs via various numerical examples. We also compare our approach with a variant of the HMC B-PINN to assess the inference accuracy and computational efficiency of our proposed method.\nFor each example, we generated a synthetic data set, Du, by solving the corresponding problem and corrupting the solution with i.i.d zero-mean Gaussian noise, N (0, \u03c3u). To demonstrate the robustness of the EKI method, we considered two noise levels: \u03c3u = 0.1 and \u03c3u = 0.01. We generate residual points Df by evaluating f at locations generated using Latin hypercube sampling over the problem domain for 2D problems and equally spaced over the domain for 1D problems. For 2D problems, boundary points Db are placed equally spaced over the boundary. Boundary and residual points are assumed to be noise-free in this paper.\nFor all examples, the neural network architecture used for both B-PINNs consists of 2 hidden layers with 50 neurons in each layer and the tanh activation function. Neural network parameter dimension N\u03b8 for each type of example can be seen in Table 4.1. For each problem, we assume the standard deviations for the boundary and residual likelihood in (3.17): \u03c3\u03b7b = 0.01 and \u03c3\u03b7f = 0.01 for the B-PINNs unless otherwise specified. Furthermore, we assume that the measurement noise level \u03c3u is\n12\nAlgorithm 3 Ensemble Kalman Inversion (EKI) B-PINNs\n1: Input: y (observations), Q (parameter covariance), R (observation covariance), W (stopping window), \u03c4 (stopping threshold) 2: Initialize prior samples for j = 1, ..., J :\n\u03be (j) 0 \u223c p(\u03be).\n3: for i = 1, ..., I do 4: Obtain i th prior parameter and measurement ensembles for j = 1, ..., J :\n(j) i \u223c N (0, Q).\n\u03be\u0302 (j) i = \u03be (j) i + (j) i . y\u0302 (j) i = Gi(\u03be\u0302 (j) i ).\n5: Evaluate sample mean and covariance terms:\n\u03be\u0304i = 1\nJ J\u2211 j=1 \u03be\u0302 (j) i .\ny\u0304i = 1\nJ J\u2211 j=1 y\u0302 (j) i .\nC y\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (y\u0302 (j) i \u2212 y\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T .\nC \u03be\u0302y\u0302i = 1\nJ \u2212 1 J\u2211 j=1 (\u03be\u0302 (j) i \u2212 \u03be\u0304i)(y\u0302 (j) i \u2212 y\u0304i) T .\n6: Update posterior ensemble for j = 1, ..., J :\n\u03b7 (j) i \u223c N (0, R),\n\u03be (j) i = \u03be\u0302 (j) i + C \u03be\u0302y i (C yy i +R) \u22121(y \u2212 y\u0302ji + \u03b7 (j) i ).\n7: Check discrepancy\nDi = \u2225\u2225\u2225\u2225\u2225\u2225R1/2 y \u2212 1 J J\u2211 j=1 G(\u03be(j)i ) \u2225\u2225\u2225\u2225\u2225\u2225 , 8: if max\nj\u2208{i\u2212W,...,i} |Dj \u2212Di|/Di < \u03c4 then\n9: I := i 10: Break 11: end if 12: end for 13: Return: \u03be\n(1) I , ..., \u03be (J) I\n13\nknown for each example and set \u03c3\u03b7u = \u03c3u. Finally, unless otherwise specified, we use physical parameter prior \u03bb \u223c N (0, IN\u03bb) for both B-PINNs.\nFor the EKI B-PINNs, we choose an ensemble size of J = 1000 and stopping criterion parameters from (3.23): W = 25 and \u03c4 = 0.05. Additionally, the artificial dynamics standard deviations for the parameters from (3.18) are chosen to be \u03c3\u03bb = 0.1 and \u03c3\u03b8 = 0.002. For the HMC B-PINNs, the leapfrog step L = 50 and the initial step size \u03b4t = 0.1 adaptively tuned to reach an acceptance rate of 60% during burn-in steps as in [50]. We draw a total of 1000 samples following 1000 burn-in steps.\nMetrics. We examine the accuracy of the forward solution approximation and physical parameter estimation from the B-PINNs in the following metrics over an independent test set {xit} Nt i=1:\neu = \u221a\u221a\u221a\u221a\u2211Nti=1 \u2223\u2223u(xit)\u2212 u\u0304(xit)\u2223\u22232\u2211Nt i=1 \u2223\u2223u(xit)\u2223\u22232 , e\u03bb = \u2223\u2223\u03bb\u2212 \u03bb\u0304\u2223\u2223 |\u03bb| , (4.1)\nwhere u(x) and \u03bb are the reference solution and the reference physical parameters. The sample means of the forward solution approximation u\u0304 and the physical parameter \u03bb\u0304 are computed from the B-PINNs as defined in (2.19). The mean predictions are obtained in the final iteration for the EKI B-PINNs and over the 1000 samples generated for the HMC B-PINN. Furthermore, we assess the quality of our uncertainty estimates by examining the sample standard deviation of the estimated forward solutions and physical parameters \u03c3\u03bb and \u03c3u\u0303 defined in (2.20).\nTo demonstrate the efficiency of our proposed method, we compare the walltime of the EKI B-PINN and HMC B-PINN (including both burn-in and training time) experiments averaged over 10 trials. The average iterations utilized over 10 trials are also presented for the EKI B-PINN tests. All experiments use the JAX [9] library in single-precision floating-point on a single GPU (Tesla T4 GPU with 16 GB of memory).\n4.1. 1D linear Poisson equation. We first consider the 1D linear Poisson equation motivated by [10] as follows:\nuxx \u2212 k cos(x) = 0, x \u2208 [0, 8], (4.2) u(0) = 0, u(8) = k cos(8), (4.3)\nwhere the exact solution is u(x) = k cos(x). Assuming the unknown constant k = 1.0, we generated Nu = 8 equally spaced measurements over [0, 8] for measurements u, excluding the boundary points, and Nb = 2 boundary points at x = 0 and x = 8. Additionally, Nf = 100 equally spaced residual points are utilized.\nDue to the linearity of the solution with respect to k, one can derive a Gaussian \u201creference\u201d posterior distribution for parameter k conditioned on knowledge of\n14\nthe correct solution parameterization u\u0303(x; k) = k cos(x) [10]. In the case where the forward solution and boundary standard deviations are equal (i.e., \u03c3\u03b7u = \u03c3\u03b7b), if we denote for simplicity Du as containing both forward solution and boundary data, the distribution takes the form:\np(k|Du) \u221d exp ( \u2212 (\u2211Nu i=1(u i \u2212 k cos(xiu))2\n2\u03c32\u03b7u +\n(k \u2212 k0)2\n2\u03c32k\n)) , (4.4)\nwhere k0, \u03c3k are the prior mean and standard deviation of k. In this example, we choose k0 = 0, \u03c3k = 1 and consider the noise level \u03c3u = 0.01. We present the \u201creference\u201d density and density estimates of the estimated posterior of k in Fig 4.1. Both approaches deliver comparable inference results for the posterior distribution of k compared to the \u201creference\u201d density.\nTable 4.2 provides the mean and one standard deviation of the posterior k approximations. The uncertainty estimates of k for both B-PINNs show that the true value of k is within one standard deviation of the posterior mean. Table 4.3 shows the mean relative error of the solution and parameters, with corresponding walltime and the number of EKI iterations utilized. From the table, it is clear that the EKI B-PINN can achieve comparable approximation quality to HMC B-PINN but with a 8-fold speed-up.\n4.2. 1D nonlinear Poisson equation. We now consider a 1D nonlinear Poisson equation as presented in [48] as follows:\n\u03bbuxx + k tanh(u) = f, x \u2208 [\u22120.7, 0.7], (4.5)\nwhere \u03bb = 0.01. Here, k = 0.7 is the unknown physical parameter to be inferred. By assuming the true solution u(x) = sin3(6x), the right-hand side f and boundary conditions can be analytically constructed. The objective of this problem is to infer the physical parameter k and the forward solution u, along with corresponding uncertainty estimates, using Nu = 6 measurements u equally spaced over the spatial domain, Nb = 2 boundary points b, and Nf = 32 equally spaced PDE residual points f .\nTable 4.4 compares the posterior sample mean and one standard deviation of k obtained from the EKI B-PINN and HMC B-PINN. Both methods accurately capture\n15\nthe true value of k = 0.7 within one standard deviation of the mean for both noise levels. Figure 4.2 compares the sample mean and standard deviation of the surrogate based on both approaches. The mean of the EKI B-PINN is in good agreement with the reference solution. The one standard deviation bound captures most of the error in the mean prediction.\nTable 4.5 reports the relative errors of the forward solution and parameter estimates using EKI B-PINN and HMC B-PINN for two noise levels, along with the corresponding walltime and number of EKI iterations. Both methods are reasonably accurate, with estimates of k within 1% error for \u03c3u = 0.01 and 5% error for \u03c3u = 0.1. Additionally, the relative errors of the mean forward solution eu for both B-PINNs reach similar levels of accuracy, approximately 2% and 10% for \u03c3u = 0.01 and \u03c3u = 0.1 respectively. Nevertheless, the EKI B-PINN achieves comparable inference results to the HMC B-PINN, but approximately 9 times faster.\n16\n17\n4.3. 2D nonlinear diffusion-reaction equation. Next, we examine the 2D nonlinear diffusion-reaction equation in [48] as follows:\n\u03bb\u2206u+ ku2 = f, (x, y) \u2208 [\u22121, 1]2, (4.6) u(x,\u22121) = u(x, 1) = 0, (4.7) u(\u22121, y) = u(1, y) = 0, (4.8)\nwhere \u03bb = 0.01 is known. Here, k = 1 is an unknown physical parameter, and the ground truth solution u(x, y) = sin(\u03c0x) sin(\u03c0y). We construct the source term f to satisfy the PDE with the given solution. For this problem, we have Nu = 100 measurements u and Nf = 100 residual points f both sampled via Latin Hypercube sampling over the spatial domain. Additionally, we have Nb = 100 boundary points b, which are equally spaced over the boundary. As in the previous example, we aim to estimate k and u with uncertainty estimates. The solution u and measurements u can be seen in Figure 4.3.\nTable 4.6 shows the one standard deviation confidence interval of the B-PINN estimates for parameter k. Notably, the ground truth k = 1.0 falls within one standard deviation of the mean estimates for both noise levels. Additionally, Figure 4.4 shows the sample mean, one standard deviation, and the error of the forward surrogate based on both approaches. Both B-PINNs provide reasonably good mean estimates of the true solution for two measurement noise levels. Moreover, the standard deviation for both B-PINNs increases as the measurement noise level increases as expected, indicating that the uncertainty estimates provided are plausible. Although the standard deviations by the EKI B-PINN somewhat differ from those of the HMC B-PINN, both\nmethods appear to agree on the rough locations of regions with higher uncertainty. For instance, when \u03c3u = 0.01, For example, when \u03c3u = 0.01, both methods exhibit large peaks around (x, y) = (\u22121, 0), (\u22120.3, 0.7), (0.9, 0.9), (1,\u22121). Similarly, major peaks can be seen around at (x, y) = (0.8, 0.8), (\u22120.8,\u22120.8), (\u22120.8, 0.8), (0.8,\u22120.8) for both methods when \u03c3u = 0.1.\nThe relative error of the mean estimates of u and k and walltime for the B-PINN methods are presented in Table 4.7. The mean approximations of k for both B-PINNs show approximation errors around 1% and 5% of the ground truth for \u03c3u = 0.01 and \u03c3u = 0.1, respectively. Similarly, the sample mean approximation of the forward surrogate u achieved a relative error less than 2% and 5%, respectively. The EKI B-PINN approximates the forward solution and physical parameter reasonably well, with mean estimates comparable to the HMC. Nonetheless, the EKI B-PINN provides inference approximately 18 times faster than the HMC B-PINN.\n19\n4.4. Kraichnan-Orszag system. We next consider the Kraichnan-Orszag model [50] consisting of three coupled nonlinear ODEs describing the temporal evolution of a system composed of several interacting inviscid shear waves:\ndu1 dt \u2212 au2u3 = 0, (4.9) du2 dt \u2212 bu1u3 = 0, (4.10) du3 dt + (a+ b)u1u2 = 0, (4.11) u1(0) = 1.0, u2 = 0.8, u3(0) = 0.5, (4.12)\nwhere u1, u2, and u3 are solutions to the above system of ODEs and a and b are unknown physical parameters. We choose a = b = 1 in this example. We place 12 equally spaced data points over t \u2208 [1, 10], and observe u1 and u3 at each of these locations, and u2 at the first 7 locations, and thus, Nu = 31 for this example. We also utilize Nf = 300 residual points (with 100 points equally spaced over t \u2208 [0, 10] for each equation in the system of ODEs). Initial conditions for the ODE are assumed to be unknown, thus Nb = 0. As in [50], we place a Gaussian prior on a and b such that a \u223c N (0, 2) and b \u223c N (0, 2). Our goal is to estimate u1, u2, u3, and parameters a and b with uncertainty estimates.\nTable 4.8 shows the mean and one standard deviation of the estimated a and b. For the small noise level \u03c3u = 0.01, the parameter estimates capture the truth b within one standard deviation and a within two standard deviations of the mean. For \u03c3u = 0.1 noise level, both estimates find b within two standard deviations of the mean. However,a is less well approximated for both B-PINNs due to bias in the mean approximation. Furthermore, Figure 4.5 presents the approximation of the forward solutions (u1, u2, u3) with the means and one standard deviation bands. For \u03c3 = 0.01, the forward predictions closely match the true solutions with narrow uncertainty bands for the B-PINNs. For the case \u03c3 = 0.1, the deviations between the mean prediction of u1 and the ground truth are more pronounced, particularly at locations where the standard deviation is also larger, suggesting both B-PINNs offer informative uncertainty estimates.\nTable 4.9 presents the mean relative errors for parameter estimations and forward approximations. The mean approximations of the EKI B-PINN are accurate and comparable to those of the HMC B-PINN for both noise levels, with the exception of parameter a for \u03c3u = 0.1, where both B-PINNs show less accuracy. However, the EKI method is approximately 25 times faster than the HMC method in this example.\n20\n\u03c3u = 0.01\n\u03c3u = 0.1\n21\n.\n4.5. Burgers\u2019 Equation. We now consider the following 1D time-dependent Burgers\u2019 equation motivated by [42]:\nut + uux \u2212 \u03bduxx = 0, x \u2208 [\u22121, 1], (4.13) u(x, 0) = \u2212 sin(\u03c0x), x \u2208 [\u22121, 1], (4.14) u(\u22121, t) = u(1, t) = 0, (4.15)\nwhere \u03bd = 0.1\u03c0 is an unknown physical parameter, and t \u2208 [0, 3 \u03c0 ]. A reference solution u for (4.13) is found in [5], where we evaluate on a 256 \u00d7 100 equally spaced grid over the domain. We randomly sampled Nu = 100 measurements u from the solution grid and placed Nb = 75 equally spaced boundary points over the boundary, which is visualized with the solution in Figure 4.6. Residual points f were sampled using Latin hypercube sampling over (x, t) \u2208 [\u22121, 1] \u00d7 [0, 3\u03c0 ] with Nf = 100. Instead of directly approximating \u03bd, we approximate the transformed parameter log \u03bd and place the prior log \u03bd \u223c N (0, 3). Additionally, for this example, we choose the standard deviation of the likelihood for the residual \u03c3f = 0.1. In this example, we aim to infer \u03bd and u with corresponding uncertainty estimates.\nTable 4.10 shows the posterior mean and one standard deviation of the estimated parameter \u03bd. Both methods provide accurate mean approximations of the parameter, and the true \u03bd is contained within the two standard deviation bounds for \u03c3u = 0.01, and lies just outside the one standard deviation confidence interval from the EKI BPINN. For \u03c3u = 0.1, EKI B-PINN does provide a slightly better mean estimate, but it underestimates the uncertainty compared to the HMC.\nFigure 4.7 displays the posterior mean and standard deviation of u estimated by both methods, as well as their corresponding approximation error. As expected, the uncertainty in both B-PINNs increases with the measurement noise level. Notably, larger uncertainty is clustered around x = 0, and greater errors are observed in the same region in the error plot, which suggests that the uncertainty estimates are informative as an error indicator.\nTable 4.11 provides mean relative error for \u03bd and u. Both methods can achieve reasonably good accuracy with relative errors of less than 2% for both \u03bd and u when \u03c3u = 0.01 and 8% for \u03c3 = 0.1. The inference results from EKI are obtained approximately 10 times faster than the HMC method.\n4.6. Diffusion-Reaction Equation with Source Localization. Finally, we consider the following source inversion problem for a two-dimensional linear diffusion-\n23\nreaction equation inspired by the example [48]:\n\u2212\u03bb\u2206u\u2212 f2 = f1, (x, y) \u2208 [0, 1]2, (4.16) u(0, y) = u(1, y) = 0, (4.17)\nu(x, 0) = u(x, 1) = 0, (4.18)\nwhere \u03bb = 0.02 is the known diffusivity and f1(x) = 0.1 sin(\u03c0x) sin(\u03c0y) is a known forcing. The goal is to infer the location parameters of the field f2, corresponding to three contaminant sources of the unknown location in the following equation:\nf2(x) = 3\u2211 i=1 ki exp ( \u2212||x\u2212 xi|| 2 2`2 ) . (4.19)\nHere, k = (k1, k2, k3) = (2,\u22123, 0.5) and ` = 0.15 are known constants and parameters x1 = (0.3, 0.3), x2 = (0.75, 0.75), x3 = (0.2, 0.7) are parameters to be recovered. The prior distributions on the locations xi = (xi, yi) are chosen to be log-normal, that is log(xi) \u223c N (0, 1) and log(yi) \u223c N (0, 1) for i = 1, 2, 3. The measurements u are generated by solving (4.16) using the solvepde function in Matlab with 1893 triangle meshes and sampling randomly Nu = 100 points from among the nodes and Nb = 100 equally spaced points along the domain\u2019s boundary, shown in Figure 4.8. In addition, Nf = 100 residual points were obtained via Latin Hypercube Sampling within the domain.\nThe posterior mean and standard deviation of the locations x1, x2 and x3 in Table 4.12 show an accurate estimation of most of the centers of contaminants, with all\n24\nvalues falling within contained within two standard deviations and most within one standard deviation of the mean estimates. The corresponding solution obtained for both B-PINNs, with the first two moments and approximation error shown in Figure 4.9, shows high accuracy regarding the mean prediction. Although the uncertainty estimates for the surrogate solutions by both B-PINNs somewhat differ for both noise\n25\nlevels, their magnitudes are comparable over the domain for both noise levels. Furthermore, when \u03c3u = 0.01, both methods identify some similar bulk regions with higher uncertainty, particularly near (x, y) = (0.5, 0.25) and (0.75, 0.75). Similarly for \u03c3u = 0.1, the regions of higher uncertainty show rough similarities between the two methods, with larger uncertainties clustering near the right diagonal of the domain.\nTable 4.13 compares the mean relative errors of physical parameters xi and forward solution u for two noise levels, as well as the walltime and number of EKI iterations. Consistent with the previous examples, EKI-based inference is approximately 25-fold faster than that of the HMC.\n5. Summary. This paper presents a new and efficient inference method for BPINNs, utilizing Ensemble Kalman Inversion (EKI) to infer physical and neural network parameters. We demonstrate the applicability and performance of our proposed approach using several benchmark problems. Interestingly, while EKI methodology theoretically only provides unbiased inference with Gaussian priors and linear operators, our results show that it can still provide reasonable inference in non-linear and non-Gaussian settings, which is consistent with findings in other literature [8, 19]. In all examples, our proposed method delivers comparable inference results to HMCbased B-PINNs, but with around 8-30 times speedup. Furthermore, EKI B-PINNs can provide informative uncertainty estimates for physical model parameters and forward predictions at different noise levels comparable to the results of HMC B-PINNs. In cases where more detailed uncertainty quantification is necessary, our proposed approach can serve as a good initialization for other inference algorithms, such as HMC, to produce better results with reduced computational costs.\nBesides, it is worth noting that our study did not investigate the case of a large dataset or residual points. In such cases, naive approaches to EKI would be computationally expensive due to the cubic scaling of EKI with the size of the dataset. In such a case, mini-batching techniques proposed in [32] for EKI to train NNs with large datasets, may help to overcome this challenge. Finally, we acknowledge that the EKI requires storing an ensemble of J neural network parameter sets, which can be memory-demanding for large ensemble sizes and large networks. In this situation, Dimension reduction techniques may help address this issue. We plan to investigate this strategy in future work.\n26"
        }
    ],
    "title": "EFFICIENT BAYESIAN PHYSICS INFORMED NEURAL NETWORKS FOR INVERSE PROBLEMS VIA ENSEMBLE KALMAN INVERSION",
    "year": 2023
}