{
    "abstractText": "\u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Abstract\u2014 Accurate road surface classification is crucial for autonomous vehicles (AVs) to optimize driving conditions, enhance safety, and enable advanced road mapping. However, deep learning models for road surface classification suffer from poor generalization when tested on unseen datasets. To update these models with new information, also the original training dataset must be taken into account, in order to avoid catastrophic forgetting. This is, however, inefficient if not impossible, e.g., when the data is collected in streams or large amounts. To overcome this limitation and enable fast and efficient cross-dataset adaptation, we propose to employ continual learning finetuning methods designed to retain past knowledge while adapting to new data, thus effectively avoiding forgetting. Experimental results demonstrate the superiority of this approach over naive finetuning, achieving performance close to fresh retraining. While solving this known problem, we also provide a general description of how the same technique can be adopted in other AV scenarios. We highlight the potential computational and economic benefits that a continual-based adaptation can bring to the AV industry, while also reducing greenhouse emissions due to unnecessary joint retraining.",
    "authors": [
        {
            "affiliations": [],
            "name": "Paolo Cudrano"
        },
        {
            "affiliations": [],
            "name": "Matteo Bellusci"
        },
        {
            "affiliations": [],
            "name": "Giuseppe Macino"
        },
        {
            "affiliations": [],
            "name": "Matteo Matteucci"
        }
    ],
    "id": "SP:f8b2dfc98fe884aca12bbeb9e30047b136f7b2be",
    "references": [
        {
            "authors": [
                "T. Tommasi",
                "N. Patricia",
                "B. Caputo",
                "T. Tuytelaars"
            ],
            "title": "A Deeper Look at Dataset Bias",
            "venue": "Domain Adaptation in Computer Vision Applications. Springer International Publishing, 2017, pp. 37\u201355. 1",
            "year": 2017
        },
        {
            "authors": [
                "D. Pessach",
                "E. Shmueli"
            ],
            "title": "A review on fairness in machine learning",
            "venue": "ACM Comput. Surv., vol. 55, no. 3, Feb 2022. 1",
            "year": 2022
        },
        {
            "authors": [
                "A. Torralba",
                "A.A. Efros"
            ],
            "title": "Unbiased look at dataset bias",
            "venue": "CVPR 2011. IEEE, 2011, pp. 1521\u20131528. 1",
            "year": 2011
        },
        {
            "authors": [
                "T. Rateke",
                "K.A. Justen",
                "A. v. Wangenheim"
            ],
            "title": "Road Surface Classification with Images Captured From Low-cost Camera - Road Traversing Knowledge (RTK) Dataset",
            "venue": "Revista de Inform\u00e1tica Te\u00f3rica e Aplicada, vol. 26, no. 3, pp. 50\u201364, Nov. 2019, number: 3. 1, 2, 4, 5",
            "year": 2019
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "R. Urtasun"
            ],
            "title": "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2012. 1, 2, 4",
            "year": 2012
        },
        {
            "authors": [
                "P.Y. Shinzato"
            ],
            "title": "CaRINA dataset: An emerging-country urban scenario benchmark for road detection systems",
            "venue": "2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 2016. 1, 2, 4",
            "year": 2016
        },
        {
            "authors": [
                "R.M. French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in cognitive sciences, vol. 3, no. 4, pp. 128\u2013135, 1999. 1",
            "year": 1999
        },
        {
            "authors": [
                "V. Pereira",
                "S. Tamura",
                "S. Hayamizu",
                "H. Fukai"
            ],
            "title": "Classification of paved and unpaved road image using convolutional neural network for road condition inspection system",
            "venue": "2018 5th International Conference on Advanced Informatics: Concept Theory and Applications (ICAICTA), 2018, pp. 165\u2013169. 2",
            "year": 2018
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, 2015. 2",
            "year": 2015
        },
        {
            "authors": [
                "M. Nolte",
                "N. Kister",
                "M. Maurer"
            ],
            "title": "Assessment of deep convolutional neural networks for road surface classification",
            "venue": "2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018, pp. 381\u2013386. 2",
            "year": 2018
        },
        {
            "authors": [
                "W. Maddern",
                "G. Pascoe",
                "C. Linegar",
                "P. Newman"
            ],
            "title": "1 Year, 1000km: The Oxford RobotCar Dataset",
            "venue": "The International Journal of Robotics Research (IJRR), vol. 36, no. 1, pp. 3\u201315, 2017. 2",
            "year": 2017
        },
        {
            "authors": [
                "J. Gesnouin",
                "S. Pechberti",
                "B. Stanciulescu",
                "F. Moutarde"
            ],
            "title": "Assessing Cross-dataset Generalization of Pedestrian Crossing Predictors",
            "venue": "2022 IEEE Intelligent Vehicles Symposium (IV), Jun. 2022, pp. 419\u2013 426. 2",
            "year": 2022
        },
        {
            "authors": [
                "K. Shaheen",
                "M.A. Hanif",
                "O. Hasan",
                "M. Shafique"
            ],
            "title": "Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks",
            "venue": "Journal of Intelligent & Robotic Systems, vol. 105, no. 1, p. 9, May 2022. 2",
            "year": 2022
        },
        {
            "authors": [
                "E. Verwimp"
            ],
            "title": "CLAD: A realistic Continual Learning benchmark for Autonomous Driving",
            "venue": "Neural Networks, vol. 161, pp. 659\u2013669, 2023. 2",
            "year": 2023
        },
        {
            "authors": [
                "H. Zhang",
                "F. Mueller"
            ],
            "title": "CLAIRE: Enabling Continual Learning for Real-time Autonomous Driving with a Dual-head Architecture",
            "venue": "2022 IEEE 25th International Symposium On Real-Time Distributed Computing (ISORC), May 2022, pp. 1\u201310. 2",
            "year": 2022
        },
        {
            "authors": [
                "T. Kalb",
                "M. Roschani",
                "M. Ruf",
                "J. Beyerer"
            ],
            "title": "Continual Learning for Class- and Domain-Incremental Semantic Segmentation",
            "venue": "2021 IEEE Intelligent Vehicles Symposium (IV), Jul. 2021, pp. 1345\u20131351. 2",
            "year": 2021
        },
        {
            "authors": [
                "T. Kalb",
                "B. Mauthe",
                "J. Beyerer"
            ],
            "title": "Improving Replay-Based Continual Semantic Segmentation with Smart Data Selection",
            "venue": "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), Oct. 2022, pp. 1114\u20131121. 2",
            "year": 2022
        },
        {
            "authors": [
                "J.-A. Term\u00f6hlen",
                "M. Klingner",
                "L.J. Brettin",
                "N.M. Schmidt",
                "T. Fingscheidt"
            ],
            "title": "Continual Unsupervised Domain Adaptation for Semantic Segmentation by Online Frequency Domain Style Transfer",
            "venue": "2021 IEEE International Intelligent Transportation Systems Conference (ITSC), Sep. 2021, pp. 2881\u20132888. 2",
            "year": 2021
        },
        {
            "authors": [
                "M.-J. Tsai",
                "Z. Cui",
                "C. Liu",
                "H. Yang",
                "Y. Wang"
            ],
            "title": "An Incremental Learning-based Framework for Non-stationary Traffic Representations Clustering and Forecasting",
            "venue": "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), Oct. 2022, pp. 3237\u20133242. 2",
            "year": 2022
        },
        {
            "authors": [
                "J. Kirkpatrick"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521\u20133526, 2017. 2, 3",
            "year": 2017
        },
        {
            "authors": [
                "J. Sun",
                "S. Kousik",
                "D. Fridovich-Keil",
                "M. Schwager"
            ],
            "title": "Self- Supervised Traffic Advisors: Distributed, Multi-view Traffic Prediction for Smart Cities",
            "venue": "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), Oct. 2022, pp. 917\u2013922. 2",
            "year": 2022
        },
        {
            "authors": [
                "T. Lesort",
                "V. Lomonaco",
                "A. Stoian",
                "D. Maltoni",
                "D. Filliat",
                "N. D\u0131\u0301az- Rodr\u0131\u0301guez"
            ],
            "title": "Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",
            "venue": "Information fusion, vol. 58, pp. 52\u201368, 2020. 3",
            "year": 2020
        },
        {
            "authors": [
                "L. Pellegrini",
                "G. Graffieti",
                "V. Lomonaco",
                "D. Maltoni"
            ],
            "title": "Latent replay for real-time continual learning",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 10 203\u201310 209. 3",
            "year": 2020
        },
        {
            "authors": [
                "S.-A. Rebuffi",
                "A. Kolesnikov",
                "G. Sperl",
                "C.H. Lampert"
            ],
            "title": "iCaRL: Incremental Classifier and Representation Learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 3",
            "year": 2017
        },
        {
            "authors": [
                "H. Shin",
                "J.K. Lee",
                "J. Kim",
                "J. Kim"
            ],
            "title": "Continual learning with deep generative replay",
            "venue": "Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., 2017. 3",
            "year": 2017
        },
        {
            "authors": [
                "D. Lopez-Paz",
                "M.A. Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., 2017. 3",
            "year": 2017
        },
        {
            "authors": [
                "M. De Lange"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3366\u20133385, 2022. 3, 4, 5",
            "year": 2022
        },
        {
            "authors": [
                "A.A. Rusu"
            ],
            "title": "Progressive neural networks",
            "venue": "arXiv preprint arXiv:1606.04671, 2016. 3",
            "year": 2016
        },
        {
            "authors": [
                "J. Serra",
                "D. Suris",
                "M. Miron",
                "A. Karatzoglou"
            ],
            "title": "Overcoming catastrophic forgetting with hard attention to the task",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 80. PMLR, 10\u201315 Jul 2018, pp. 4548\u20134557. 3",
            "year": 2018
        },
        {
            "authors": [
                "R. Hadsell",
                "D. Rao",
                "A.A. Rusu",
                "R. Pascanu"
            ],
            "title": "Embracing change: Continual learning in deep neural networks",
            "venue": "Trends in cognitive sciences, vol. 24, no. 12, pp. 1028\u20131040, 2020. 3",
            "year": 2020
        },
        {
            "authors": [
                "G.I. Parisi",
                "R. Kemker",
                "J.L. Part",
                "C. Kanan",
                "S. Wermter"
            ],
            "title": "Continual lifelong learning with neural networks: A review",
            "venue": "Neural networks, vol. 113, pp. 54\u201371, 2019. 3",
            "year": 2019
        },
        {
            "authors": [
                "F. Zenke",
                "B. Poole",
                "S. Ganguli"
            ],
            "title": "Continual learning through synaptic intelligence",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 70. PMLR, 06\u201311 Aug 2017, pp. 3987\u20133995. 3",
            "year": 2017
        },
        {
            "authors": [
                "H. Jung",
                "J. Ju",
                "M. Jung",
                "J. Kim"
            ],
            "title": "Less-forgetful learning for domain expansion in deep neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018. 3, 4",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke"
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc., 2019. 5",
            "year": 2019
        },
        {
            "authors": [
                "V. Lomonaco"
            ],
            "title": "Avalanche: an end-to-end library for continual learning",
            "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, ser. 2nd Continual Learning in Computer Vision Workshop, 2021. 5",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "\u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nI. INTRODUCTION\nAutonomous vehicles (AVs) have gained significant attention in recent years, with tremendous advancements in various aspects of their operation. One crucial aspect is the detection and understanding of the road environment, which includes the ability to classify different types of road surfaces. Road surface classification is critical for AVs as it provides valuable information about the road material, allowing the vehicle to adapt its driving operating conditions accordingly. For instance, when traversing an unpaved rural road, the vehicle can modulate its velocity and adjust its steering to navigate safely through the challenging terrain. Similarly, on a cobblestone road, the presence of rain can increase the risk of skidding, necessitating proactive measures to avoid potential hazards. In addition, road surface classification facilitates the creation of high-definition (HD) maps, enabling AVs to receive information about road conditions in advance. With this information, vehicles can plan their routes more effectively, enhancing the overall driving experience. Moreover, knowledge of the pavement material is crucial when developing realistic simulations for testing and validating AVs and Advanced Driver Assistance Systems (ADAS). Indeed, this knowledge enables the recreation of the driving scenario with higher fidelity.\n1,2Department of Electronics Information and Bioengineering, Politecnico di Milano, p.zza Leonardo da Vinci 32, Milan, Italy name.surname @ { polimi.it1, mail.polimi.it2}\nThis paper is supported by \u201cSustainable Mobility Center (Centro Nazionale per la Mobilita\u0300 Sostenibile \u2013 CNMS)\u201d project funded by the European Union NextGenerationEU program within the PNRR, Mission 4 Component 2 Investment 1.4.\nTo tackle the road surface classification problem, current approaches rely on deep learning techniques due to their exceptional ability to extract intricate features from visual data. However, deep learning models heavily rely on the quality and diversity of their training dataset, and several issues can arise when applying these models to real-world scenarios [1]. For instance, biased or unrepresentative training data can lead to models that perform poorly or unfairly [2]. Additionally, the lack of diversity in the training data can limit the ability of models to generalize to unseen situations or novel datasets. It is known that the distribution shift between different datasets can indeed significantly impact model performance, as models trained on one dataset often fail to generalize when tested on other datasets (Fig. 1a\u2013b), i.e., they show poor cross-dataset generalization [3].\nIn the context of AVs, a large number of datasets have been collected. For the task of road surface classification, in particular, three datasets are publicly available and contain all the necessary information: RTK [4], KITTI [5], and CaRINA [6]. It has been shown in the literature [4] that existing deep learning models trained on one of these three datasets exhibit inadequate generalization when tested on the remaining two. This limitation poses a significant obstacle to the deployment of robust road surface classification systems in autonomous vehicles, as it highlights how these models cannot operate reliably in all plausible real-world scenarios.\nAddressing this cross-dataset generalization problem is critical. A straightforward approach would be to perform finetuning on the model using the target dataset. However, finetuning only on novel data leads to catastrophic forgetting [7], i.e., the model\u2019s performance on previously learned tasks quickly deteriorates as it becomes optimized only for the new data (Fig. 1c). A consequent idea would be to include the original dataset in the data used for finetuning, forcing the model to remember past data while learning from novel samples. Including the entire original dataset in the training, however, can be computationally inefficient, especially when dealing with large historical datasets. Additionally, access to past datasets may not even be feasible in settings of online training, or it may be forbidden due to privacy restrictions.\nTo overcome this limitation, an alternative solution is necessary: updating the model solely based on the new data, yet avoiding forgetting past knowledge (Fig. 1d). This problem is precisely the focus of continual learning (CL), a research field studying how to mitigate catastrophic forgetting.\nIn this work, we exploit one such continual learning strategy to solve the problem of dataset adaptation in road surface classification. In doing so, our contribution is twofold. First, we propose a practical solution to the known problem of poor\nar X\niv :2\n30 9.\n02 21\n0v 1\n[ cs\n.C V\n] 5\nS ep\n2 02\n3\ncross-dataset generalization in road surface classification, showing how it is possible to efficiently update a model using only new data. Second, we showcase the applicability of continual learning strategies not only to this specific task, but to any AV task that necessitates updating a model with new data effectively yet efficiently."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Road surface classification has been addressed by several works in the literature. Pereira et al. [8] distinguish between paved and unpaved road surfaces using a Convolutional Neural Network (CNN) based on VGG [9]. They train their deep model with smartphone-collected images acquired under several weather conditions. Nolte et al. [10], instead, compare CNN architectures at classifying the road surface (asphalt, dirt, grass, wet, cobblestone, snow) under different weather conditions, exploiting various datasets as Oxford RoboCar [11] and KITTI [5]. Using a custom CNN architecture, Rateke et al. [4] classify the road material (asphalt, paved, unpaved), and then proceed to analyze road quality and conditions (e.g., presence of potholes) with an ad-hoc system. They train their road material classifier using monocular images from KITTI [5], CaRINA [6], and to properly train on unpaved conditions, they additionally propose a new purposefully-collected dataset, RTK. In the same work, Rateke et al. [4] also report cross-dataset generalization issues when training on one single dataset and testing on the other two. They propose to mitigate this problem by training their model jointly on all three datasets. This, however, is not only inefficient, but it implies that, if a criticality is discovered in the future, retraining on all available datasets will be necessary again in order to fix it.\nSimilar poor cross-generalization performances are re-\nported by Gesnouin et al. [12] in the context of pedestrian crossing detection, a completely different task in autonomous vehicle perception. This shows that the problem of poor cross-dataset generalization is widespread and hinders the reliability of potentially any AV perception system. A systematic approach must be found to solve it consistently.\nAs continual learning gains attention in the deep learning community, applications in industrial and real-world setups are emerging, including works on other autonomous driving problems. Shaheen et al. [13] propose a review of the literature and discuss emerging applications and related challenges for autonomous vehicles, including insights on scene recognition, lane keeping, and Reinforcement Learning (RL)based vehicle control. Verwimp et al. [14], instead, recently proposed a new benchmark for continual learning, specifically designed for autonomous vehicle tasks. Zhang and Mueller [15] assess the feasibility of online continual learning for on-board vehicle devices, proposing a lightweight object detection architecture. Kalb et al. [16] adapt the formulation of semantic segmentation to a continual learning setup, validating their work on autonomous driving benchmarks. In a successive work, Kalb et al. [17] further investigate the performance of replay techniques when dealing with segmentation tasks, field previously almost unexplored. In segmentation, Termo\u0308hlen et al. [18] use frequency-based style transfer to perform continual domain adaptation and contrast catastrophic forgetting. Also traffic monitoring and forecasting has seen interest in early continual learning works. Among these, Tsai et al. [19] perform traffic forecasting using an online version of EWC [20], with application to traffic pattern discovery during COVID-19 road regulation changes. Sun et al. [21], instead, apply continual techniques to a camera-based traffic advisory system."
        },
        {
            "heading": "III. CONTINUAL LEARNING",
            "text": "Before framing the problem of road surface classification in the context of continual learning, we briefly present the field and its current developments.\nContinual Learning (CL) aims at making learning systems able to acquire knowledge through time from a possiblyinfinite stream of data, without forgetting past learned information. The main problem faced by CL is catastrophic forgetting, i.e., the inability of deep learning models to consistently retain past knowledge when trained on new data.\nThis characteristic is rooted in the optimization procedure adopted to train these models. Let us consider, without loss of generality, the most simple scenario of a deep learning model (or network), whose parameters (or weights) are optimized (or trained) in a fully supervised setting on real-world highdimensional data, e.g., camera images. Let us also assume we have a mechanism to sample input data and associated target outputs from their real-world statistical distribution.\nTo find the best weights for our model, we start acquiring a random data sample and its associated target output, and use our model with its current weights to predict an output. We can then compare this prediction to its associated target by means of a loss function (e.g., a distance measure), which assesses the model\u2019s performance. As model and loss function are smooth, by chain rule we can compute the gradient of this loss with respect to each model weight, and perform a step of gradient descent (or variations) in such direction. Doing so will slightly pull the weights of our model towards a configuration that performs better on the current data sample.\nRepeating this procedure over different samples, the weight updates give rise to a tug-of-war in the weight space, pulling weights in the optimal direction for each data instance. We can expect that, after a large number of samples, the weights converge to a region of compromise that performs adequately over all data samples. It can be shown that this indeed happens, but only if our sampling procedure generates i.i.d. (independent and identically distributed) samples over the entire data distribution. Intuitively, this means that the samples must span over the entire distribution and that regions of such distribution are eventually revisited.\nClearly, this condition is strict and unrealistic for realworld data, but to approach it, two steps are always taken. First, a very large amount of samples is used in the optimization, in order to best approximate the entire data distribution; and second, these samples are shuffled, in order to break any inherent dependency. To carry on these two operations practically, these samples are preemptively collected and stored in the form of a dataset. It is for this reason that, trying to run this procedure on an already-trained model and sampling only from a new dataset, inevitably leads to catastrophic forgetting, as we are sampling only from a portion of the entire data distribution of interest, thus violating the above condition. Intuitively, this is because the tug-of-war mechanism internal to the optimization is not balanced, and will end up favoring the performance of the\nmodel only on newly-seen data, at the expense of older ones. Although solutions for catastrophic forgetting are still currently under research, several training strategies have been proposed in the literature to mitigate it, finding a tradeoff for the stability-plasticity dilemma [22]. Such strategies are traditionally categorized into three classes: replay-based, architecture-based, and regularization-based [22]. In recent times, hybrid versions [23] are also emerging.\nReplay-based strategies, such as iCaRL [24], DGR [25], and GEM [26], represent the most straightforward approaches. They focus on storing in a memory buffer a few of the seen samples during training, in order to mix them with the new data when learning future experiences. In this way, they recreate a simile-i.i.d. setup and thus maintain good performance on past experiences. The main focus in this direction is devoted, of course, to the selection of as few most representative samples as possible, in order to maintain low memory consumption. Generative models and latent replay are also commonly adopted as rehearsal strategies [27].\nArchitecture-based strategies, instead, physically modify the network architecture to account for the incoming new information. A simple baseline consists in adding a new head to the network every time a new experience is encountered. This expedient untangles stability and plasticity, as it learns custom weights for each experience, at the cost of rapidlyincreasing memory consumption. Among others, examples are progressive neural networks (PNN) [28] and HAT [29].\nRegularization-based strategies, at last, deal with catastrophic forgetting by constraining the weight updates through regularized losses, in order to prevent large changes of the weights more responsible for modeling past data distributions. In a sense, these techniques deal with the root of catastrophic forgetting, controlling the tug-of-war mechanism and dealing with the problem of credit assignment [30]. Based on theoretical neuroscience considerations on synaptic consolidation [30], [31], these approaches provide the advantage of having low extra-memory requirements. Among these methods, Elastic Weight Consolidation (EWC) [20] limits weights\u2019 gradients proportionally to their importance for previous experiences. These importances are estimated by computing their Fisher information matrix at the end of each experience. Synaptic Intelligence (SI) [32], instead, estimates similar weight importances in an online manner, allowing for a smooth online transition between consecutive experiences. These strategies, however, must be applied from the beginning of the training, and thus cannot be adopted with pretrained models. Conversely, Less-Forgetful Learning (LFL) [33] directly uses the new data as a proxy for the old ones. In this way, it is able to regularize the embedding space generated by the last layer of the classifier."
        },
        {
            "heading": "IV. CONTINUAL ADAPTATION TO NEW DATA",
            "text": "With a background in CL, we can now frame the problem of dataset adaptation in the context of continual learning. Specifically, we focus our attention on the task of road surface classification, although the following formulation is general to other AV applications.\nIn road surface classification, we consider a deep network F\u03b8 of parameters \u03b8 that has the task of classifying the road surface between three categories: asphalt, unpaved (e.g., gravel roads) and paved (other than asphalt, e.g., cobblestone). Let us assume that F is first trained on dataset D0. We can denote this trained network with F\u03b8 = F\u03b8\u22170 , where \u03b8 \u2217 0 represents the optimal parameters obtained training on D0. Notice that D0 could be a large pretraining dataset and might be unavailable at a later time. Our objective is to adapt the model weights \u03b8 to a new dataset Di such that, while new knowledge is acquired, the network retains high performance also on all previously seen datasets (D0, . . . , Di\u22121). Here, we use i to index experiences, which correspond with a new dataset being presented to the network.\nIn a general continual learning setup, at every new experience, changes can occur in the distribution of the input data, the distribution of the targets, or the set of target labels. In dataset adaptation, however, we consider only changes in the input data distribution. This setup is also known in the literature as domain-incremental learning [27]. Nevertheless, our formulation can be adapted for task-incremental, classincremental, and data-incremental setups [27].\nAmong all possible strategies proposed in the literature, for the task of dataset adaptation we rely on Less-Forgetful Learning (LFL), proposed by Jung et al. [33]. In the following, we summarize its working mechanisms and highlight its main advantages over the alternatives."
        },
        {
            "heading": "A. Less-Forgetful Learning (LFL)",
            "text": "Less-Forgetful Learning (LFL) [33] is a regularizationbased strategy perfect for the problem of continual domain adaptation, i.e., avoiding catastrophic forgetting due to input domain shifts. LFL can be applied on pretrained models asis, as it does not require any additional computation during training on the first dataset. This is a crucial feature in the context of this work, as we aim to rapidly adapt already existing models as new information becomes available.\nConsidering a deep feed-forward neural network with a final softmax classification layer, the authors of LFL consider the top layer FL as a linear classifier, while the rest of the network FL\u22121 is considered as a feature extractor, mapping the input data to a linearly separable embedding space. Building on this consideration, the authors then highlight two key desiderata for their continual strategy:\nA. The decision boundaries outlined by the top layer FL\nmust remain unchanged between different experiences. B. The embedding of data from past experiences should\nnot move significantly when updating the network. Clearly, B. cannot be guaranteed in general, as it requires constant access to past datasets. Nevertheless, the authors find that using data from new experiences as a proxy for the old ones still provides good performances. As a consequence, given a model F\u03b8 = F\u03b8\u22170 , pretrained on datasets D0, their strategy for updating F\u03b8 using dataset D1 prescribes to: 1) Copy the current optimal network weights \u03b81 := \u03b8\u22170 . 2) Freeze the top softmax layer FL (to achieve A.).\n3) Train F\u03b8 on dataset D1 with loss:\nL(x; \u03b81, \u03b8\u22170) = Lc(x; \u03b81) + \u03bbeLe(x; \u03b81, \u03b8\u22170) +R(\u03b81), (1)\nwhere x \u2208 D1, R is a regularization term (such as an L2 penalty), and Le (enforcing B.) is:\nLe(x; \u03b81, \u03b8\u22170) = 1\n2 \u2225\u2225\u2225FL\u22121\u03b8\u22170 (x)\u2212 FL\u22121\u03b81 (x)\u2225\u2225\u22252 . (2) This procedure can be repeated for any further new dataset Di becoming available over time. In the end, F\u03b8 will have converged to a configuration that has acquired as much knowledge as possible from the last dataset, without forgetting what was learned from past datasets."
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experimental setup",
            "text": "To validate the efficacy of our methodology in achieving efficient dataset adaptation, we follow the same experimental setup proposed by Rateke et al. [4]. In particular, we replicate their findings that the task of road surface classification presents poor cross-dataset generalization, and we show how the continual adaptation strategy presented in Sec. IV significantly mitigates the problem without needing to store and process past data.\nTo maintain replicability, we employ the CNN architecture described in their work [4]. Moreover, we employ the same three datasets: RTK [4], KITTI [5], and CaRINA [6]. Each dataset is divided into train and test splits. Fig. 2 displays a few samples of the images present in each dataset, highlighting the three possible output labels: asphalt, unpaved, and paved. In their work, [4] consider RTK as the first training source (D0 in our notation) because of its larger size, more suitable for the initial training; KITTI and CaRINA instead, containing fewer samples, are considered as secondary datasets (D1, D2). When training only on RTK and testing on KITTI and CaRINA, Rateke et al. report\nsignificantly lower metrics on the latter two, highlighting poor cross-dataset generalization. For this reason, they then repeat the experiment training jointly on all three datasets, achieving satisfactory performances on all three test sets, at the cost of a longer and more expensive training. We replicate their results and extend their experimental setup to evaluate the performance of the reference model when trained using two baselines (naive and joint training [27]) and the LFL continual learning strategy. In particular: Naive finetuning. After our model has been trained on\nRTK, we continue finetuning it, first on KITTI and then on CaRINA. This replicates the findings of [4] and represents a lower bound for our expected performance, as it is the best setup for catastrophic forgetting to occur. LFL (continual adaptation). After our model has been trained on RTK, we use the continual learning strategy LFL (Sec. IV-A) to finetune it first on KITTI and then on CaRINA. Notice that LFL does not need to be applied when training on RTK. Joint training. We discard our model trained only on RTK, and retrain it from scratch over the combination of all three datasets, as proposed in [4]. This represents an upper bound to our expected performance as, merging and shuffling all available data, no forgetting can occur.\nOur entire experimental setup is implemented in PyTorch [34], using the Avalanche framework [35]. We train each experience for 30 epochs, using an SGD optimizer with learning rate of 0.002 and \u03bbe = 1. These values have been finetuned to yield the best results and thus propose a fair evaluation. All experiments are executed on an Nvidia Quadro RTX 6000 GPU."
        },
        {
            "heading": "B. Experimental results",
            "text": "We evaluate the performance of the reference model in terms of AUROC and F1-score, with the former being the most indicative as it balances precision and recall and is not affected by class imbalance.\nTo evaluate the impact of forgetting, we compute these metrics after training on the initial dataset (RTK), and after adding each additional dataset (KITTI, CaRINA). These metrics are computed individually on the test splits of each dataset. In this way, we can also evaluate the performances yielded on KITTI and CaRINA even before they are seen by the model, showing the model\u2019s cross-dataset performance.\nTables I, II and III report the results of this analysis for Naive, LFL, and Joint strategies respectively. Notice that the Naive strategy can be considered as an approximate lower bound for performances, while the Joint strategy as an approximate upper bound.\nFrom Tables I and II, we notice that both Naive and LFL strategies report the same performance when training on the first dataset, RTK. This is because LFL does not intervene during the first training, thus both strategies perform the same exact computation at this stage. We notice that while the model has great performance on RTK, it generalizes poorly to both unseen datasets KITTI and CaRINA, despite the classification task being the same. This replicates the poor cross-dataset generalization reported by Rateke et al. [4].\nWhen finetuning on the second dataset (KITTI), Naive and LFL present different behaviors. In both cases, the performance on KITTI rises, as the model is adapting to its data distribution. However, Naive leads to a significant drop in performance on the RTK test set, going from an AUROC of 0.9932 to 0.7526. This is a clear manifestation of catastrophic forgetting: the model has adapted to the new data distribution but has lost performance on the previous distribution. LFL, on the contrary, displays only a minor perturbation, and its AUROC performance on RTK remains above 0.97 even when learning from a new dataset. The same trend is noticeable when we move to the third dataset (CaRINA), as the AUROC on RTK drops further to 0.5972 under the Naive strategy. This confirms that this strategy leads to significant catastrophic forgetting, while LFL maintains a high performance throughout the model\u2019s lifetime.\nWhen comparing to the upper bound performance given by the Joint strategy in Table IV, we notice that the final model obtained through sequential updates achieves comparable performance when adopting the LFL strategy. Indeed, the model obtained with LFL reports a difference of at most 2.38% AUROC on any dataset. The Naive strategy, instead,\nleads to drops in AUROC of 38.80% on RTK, significantly impacting the reliability of the system on past data.\nAs current CL strategies are not perfectly mature, we realize that the performance of LFL remains slightly inferior to a Joint training (Table IV). However, if datasets related to past experiences are not available, this is the best we can achieve, and it improves significantly over the Naive approach. Moreover, even if we do have access to all past data, we might still want to consider whether to use it entirely. Indeed, we must also take into account the speedup induced by continual adaptation. In our experiments, finetuning only on the last dataset was found to be 5 times faster than training using all past data. In this work, the dataset dimensions are modest, and thus the computation required could still be handled. However, with large-scale datasets, the saved time is magnified significantly. As a result, when dealing with similar problems of dataset adaptation, we advise practitioners to evaluate the trade-off between the slight performance loss experienced by continual adaptation and the efficiency gained in terms of computation resources.\nIncidentally, we notice from Table IV that, on the last dataset (CaRINA), Naive and LFL strategies achieve slightly higher performance than Joint. Intuitively, this is due to how each strategy deals with the stability-plasticity tradeoff. Naive and LFL can focus their optimization only on new data, thus fitting it slightly better than Joint, which is instead fed random samples from all three datasets.\nFor additional reference, we report the trend during the entire training of the AUROC and F1 metrics computed on the union of the test sets (Fig. 3). As mentioned above, the Joint strategy serves as an upper bound on the performance. We notice that, while the Naive strategy accumulates a significant gap as new datasets are introduced, LFL maintains almost perfect performance throughout the entire training."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this work, we addressed the problem of cross-dataset adaptation in road surface classification. We showed how continual learning strategies can be adopted to update a given model when new data becomes available, without the need to store and use past datasets, yet avoiding catastrophic forgetting. The efficacy of continual learning for crossdataset adaptation is confirmed by our experiments, which also highlighted the occurrence of strong forgetting when adopting a naive finetuning approach. While contributing to tackling this open problem in the field of autonomous vehicles, we believe our formulation provides also the basic tools necessary to apply our approach to other AV tasks. Indeed, the need for updating a model with new data is particularly important for improving the robustness of perception systems when deployed in the real world. With this in mind, we encourage the adoption of continual learning techniques not only in future research, but also in industrial settings, in order to save the computational, economic, and logistic resources due to unnecessary large-scale retraining."
        }
    ],
    "title": "Continual Cross-Dataset Adaptation in Road Surface Classification",
    "year": 2023
}