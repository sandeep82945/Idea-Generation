Robots and artificial agents that interact with humans should be able to do so without bias and inequity, but facial perception systems have notoriously been found to work more poorly for certain groups of people than others. In our work, we aim to build a system that can perceive humans in a more transparent and inclusive manner. Specifically, we focus on dynamic expressions on the human face, which are difficult to collect for a broad set of people due to privacy concerns and the fact that faces are inherently identifiable. Furthermore, datasets collected from the Internet are not necessarily representative of the general population. We address this problem by offering a Sim2Real approach in which we use a suite of 3D simulated human models that enables us to create an auditable synthetic dataset covering 1) underrepresented facial expressions, outside of the six basic emotions, such as confusion; 2) ethnic or gender minority groups; and 3) a wide range of viewing angles that a robot may encounter a human in the real world. By augmenting a small dynamic emotional expression dataset containing 123 samples with a synthetic dataset containing 4536 samples, we achieved an improvement in accuracy of 15% on our own dataset and 11% on an external benchmark dataset, compared to the performance of the same model architecture without synthetic training data. We also show that this additional step improves accuracy specifically for racial minorities when the architecture’s feature extraction weights are trained from scratch. ii. methodology The methodology behind our Sim2Real approach is explained in this section. An overview of our dataset generation and preparation, as well as an overview of our deep-learningbased dynamic facial expression recognition model, is provided. The synthetic data pretraining step of the Sim2real approach is explored in Section III. a. dataset generation In this section, we describe the collection of in-thewild emotionally expressive videos and the generation of synthetic videos using a suite of simulated humans. We focus this study on confusion, a dynamic social signal which is underrepresented in datasets and lacks examples on the web [15], yet is common in HRI [30]. 1) Collection of in-the-wild confusion, anger, and disgust videos: Confusion is an affective state conveyed through varied and multiple expressions, as are both anger and disgust. Some of these expressions (such as frowning) are common between the three, resulting in some expressions being easily mistaken as another emotion, possibly due to these three emotions appearing very close together in the CMA [29]. We therefore focus on making a dataset for these three emotions. We collected short video clips (1-3s) from YouTube.com and Giphy.com using search tags such as “angry”, “confused”, and “disgust” reactions to gather human facial expressions of our desired social signals, as shown in Fig. 2. This search resulted in 153 clips. Each video was then labeled for the conveying facial expression, by two annotators identifying with Canadian culture (inter-rater agreement kappa score=.88), and low confidence videos were discarded. We created a multi-ethnicity dataset of real human videos expressing the three social signals of confusion (41 videos), anger (41 videos), and disgust (41 videos). The final dataset contains 123 videos, of which 26 are of non-Caucasian individuals. 2) Generation of augmented dynamic social signal video dataset: The task of creating desired social signals videos is made possible using the MakeHuman toolkit [4] and the FACSHuman plugin [12]. MakeHuman toolkit is an open-source and free 3D computer graphics toolset designed for prototyping human-like models. FACSHuman offers the possibility of manipulating the Action Units (AU) presented in the Facial Action Coding System (FACS) [9] on the 3D models created in the MakeHuman software. This manipulation of AUs is a key component of our Sim2Real process. FACSHuman enabled us to generate social signal animations that can be rendered into videos or frames on a selected human virtual model from any viewing angle. We created a script plugin that used this capability to render 4536 synthetic videos from the combination of 24 human virtual models, 21 social signals (facial movement animations), and 9 viewing angles. Creation of a suite of simulated humans: The overarching vision of this work is to create a large, auditable suite of human models to represent people from many different backgrounds. As a first step, we create 24 simulated human adult models balanced on gender and four different ethnicities (Caucasian, Black, Asian, and Hispanic). The MassProduce plugin within the MakeHuman application was then used to create several randomly generated human models of multiple ethnicities and different ages and skin colors. Out of all those generated human-like models, we selected 24 models for our study based on the realism of action unit manipulation on the model (8 samples are shown in Fig. 3). We chose to use 3D models as we hope to eventually use them in HRI simulators, to create expressive virtual humans with facial expressions. These models are provided on Github1 so that researchers can also import the 24 virtual humans with 21 dynamic expressions (7 social signals per emotion), to replay them in front of their virtual robot. 1https://github.com/sabaak95/confusionDetection
Multiple social signals per emotion: We used the FACSHuman software to create 21 different social signal animations, 7 for each emotional class. These animations convey multiple variations of social cues of confusion, anger, and disgust. These 21 social signals were manually animated over 25 frames and were created via inspection of the in-the-wild real human dataset. For example, Fig. 4 shows the AUs that were used to create dynamic confusion social signals. Varied AU combinations and sequences were used to animate the 21 social signals, validated by an annotator with Canadian culture. An example is a side-eye movement confusion state made by a timed sequence of the following AUs: AU61, AU62, AU61. While future work should automatically perform the animation creation process from video data, the manual animation creation step in this study allows us to validate the Sim2Real portion given human-level feature extraction, enabling us to identify specific underlying social signals for each emotion (7 for each emotion), which are now available for use in our 3D models. The dataset is available for download.2
Multiple viewing angles: As robots may view a human from varied angles, it is important that our generated dataset incorporate varied perspectives. Our dataset was therefore expanded by creating videos of the same facial gesture from 9 viewing angles, to make our network invariant to the face viewing angle, as shown in Fig. 5. The camera movement included horizontal rotations of −40,−20, 0, 20, 40 degrees and vertical rotations of −30, 15, 0, 15, 30 degrees. The
2https://www.rosielab.ca/datasets/ confusion-in-the-wild
nine combinations of (Hrotation, Vrotation)={(−40,−30), (−20,−15), (0, 0), (20, 15), (40, 30), (40,−30), (20,−15), (20,−15), (40,−30)} were selected as our viewing angles in degrees, as shown in Fig. 5. b. data preparation In order to refine the data and remove any unimportant or unrelated information in the images, we used Multi-task CNN (MTCNN) to detect and crop the faces before feeding frames to our network [41] and resized images to 160 × 160. Additional transforms were also applied to the images randomly on each epoch, including cropping, perspective, affine transform, horizontal flip, and color transforms (shown in Fig. 6).We ensured that the same transformations were applied to all the frames from the same video. c. model architecture We developed a basic framework for our video classification problem to test the Sim2Real strategy. Existing work can extract valuable frame-based facial features from a face image, such as FaceNet [31] and OpenFace [2]; one of these models can be used to first extract each frames’ facial features. After frame-based feature extraction, we model the problem as Time Series Classification. Sections II-C.1, IIC.2 are dedicated for further exposition on our selections for this architecture. 1) Facial Feature Extraction Network: We used the pretrained FaceNet [31] architecture as our facial feature extractor. FaceNet uses an InceptionResnetV1 architecture trained on the VGGFace2 dataset. Each video is given to FaceNet frame by frame, and the output feature arrays are concatenated together across the time dimension to create a multivariate time series array. 2) Time Series Classifier Network: Further processing of this output requires a time series classification algorithm. K-Nearest Neighbor (KNN) algorithm with Dynamic Time Warping (DTW) [39] metric is one of the earliest techniques for this task that is still used, specially when working with relatively small datasets. Many machine learning algorithms have also been applied to this problem, such as ResNet and FCN [37]. However, we opted to use InceptionTime [10] as our classifier because it has proven to be a versatile and promising machine learning solution for many Time Series Classification tasks, based on its performance results on the UCR [7] benchmark collection datasets. 3) Proposed and Baseline Architectures: Our proposed DNN structure is the combination of FaceNet [31] and InceptionTime128 [10]. This structure is shown in Fig. 7. This model is referred to here as FN+INC25 or FN+INC64. The two numbers of 25 and 64 indicate the required number of frames for the video input. Videos shorter than the indicated number are padded to the required input length, and the longer videos are cropped. This procedure is explained in Section III. We included two instances of these models in our experiments. In one instance, the FaceNet weights are frozen. In the main instance, the FaceNet weights are not frozen, and are tuned alongside the InceptionTime weights. We hypothesized that the addition of synthetic data would allow us to tailor the FaceNet weights in favor of this dataset. The second DNN architecture included in our experiments is I3D [6], an advanced video classification method that applies combined temporal and spatial processing using 3D convolutional layers. This addition enabled us to evaluate the problem using a model not already affected by previous facial information knowledge. Finally, we included a KNN classifier architecture applicable to the small real dataset. This model uses FaceNet [31] for frame facial feature extraction and a KNN with DTW [39]
metric as the video classifier. We propose this baseline model, FN+KNN, for comparison. iii. experiments and results In this section, we evaluate our network on the created real dataset. We performed several experiments varying the architecture, input length, and the use of only synthetic, synthetic plus real, or only real training data. We used 5- fold cross-validation on the real dataset to compare the performance of different approaches, with one fold consisting primarily of expressions by non-Caucasian individuals. We explored three training strategies for our experiments. In the first strategy, the algorithms are only trained on the small real dataset. The baseline KNN model was tested under this strategy. In the second strategy, the networks are first trained on synthetic data, then fine-tuned on the real dataset. The second strategy was developed to add and assess the addition of synthetic data. Third, the strategy was to combine the real training data set with the synthetic dataset and pass them to the network alongside each other. This strategy was designed to explore if a higher performance could be achieved by creating ratioed synthetic and real data training. Instead of combining the whole synthetic dataset with the real data, we trained the network with one-fourth of the synthetic dataset. In the next test, the ratio of the synthetic dataset was set to half. Finally, in the last test, the ratio of the synthetic dataset was set to one, meaning the whole synthetic dataset was included. One important factor in our training and testing processes is setting the input video length to a fixed number of frames L. Input videos shorter than the set length were looped until they reached length L. The way we dealt with longer videos differed depending on the training phase. In the training phase, we randomly selected L consecutive frames from the lengthier videos. For a video with N frames, frames n to n+L− 1 are cropped. The n is selected randomly on each epoch between 0 and N −L. However, in the testing phase, we only selected the middle L frames as the representative sequence in each video. In our experiment, we set L to two values: 25 and 64. We selected 25 because the number of frames in our synthetic videos was 25. The choice of 64 was reliant on two factors. First, 64 was long enough to include a majority of the input video while small enough to keep computational cost and time consumption adequate. Second, the I3D network used in some of our experiments was designed for 64 frame inputs. In the following subsections, we elaborate on the latter two training strategies: (i) fine-tuning the synthetic trained network on real data and combined synthetic. (ii) Combined synthetic and real data training. a. fine-tuning the synthetic trained network on real data In this experiment, the model was first trained on the synthetic dataset alone. The simulated human models were randomly divided into two sets of 19 and 5 models. All of the generated videos using simulated human models in the larger set were used for training, and those in the smaller set were used for validation. The respective numbers for the
videos in the training and validation data were 3591 and 945. The training was done over 20 epochs with the learning rate of 10−4 and the categorical cross-entropy loss function. The batch size was set to 8. After the training on synthetic data, we fine-tuned the model on the four selected training folds of the real dataset, over 50 epochs. The model is tested on the remaining single test fold. This operation is repeated 5 times each time a new fold is selected as the test fold. Learning rates and parameters were chosen empirically. The results averaged over all 5 runs for these experiments are shown in Table I. FZ specifies the instances where FaceNet weights were frozen to treat FaceNet purely as a feature extraction network, with weights of the rest of the network updated during training. Table I also includes experiments in which the synthetic data training step was skipped to highlight its effect. Additionally, we compared our methods with the baseline FN+KNN classifier applied only to the real data. Our results show that the models trained on synthetic data outperformed their counterparts only trained on the real data. The unfrozen FN+IN25 model achieved an
89% accuracy on the real data when trained on synthetic data. In the frozen weights instances, the inception models perform similarly to the KNN models when trained only on the real data. However, the addition of synthetic data training improved the FZ model accuracy up to 83% in the case of FN(FZ)+INC64. This addition also significantly impacted the I3D model, and its accuracy of 83% outperforms all models not influenced by the synthetic data. Interestingly, this model even outperforms the FZ models trained on the synthetic data. This is quite impressive because I3D was designed for video action recognition tasks. Unlike the other models, the I3D had no prior information about the facial features. In Table II we explored the effect of unfreezing the FaceNet weights and the addition of synthetic data on the non-Caucasian data fold. We created one test fold which included 25 videos of the underrepresented ethnicities. The FN(FZ)+IN models trained without synthetic data are highlighted as the base models in this table. This table shows that the addition of synthetic data combined with unfreezing of the pre-trained weights has the highest impact on the correct classification of the non-Caucasian data samples (24% increase). The addition of synthetic data alone has a limited beneficial impact; it can not alter the dataset bias effect of the original dataset on which the FaceNet was trained. b. combining synthetic and real data for training We designed another experiment to investigate how the accuracy changes with the addition of synthetic data. A portion of the synthetic data was randomly selected and combined with the real training data to create a new data set. The model was trained on this new training data and tested only on the real test data sample. We applied this training strategy to our most satisfactory model, input length 25 FaceNet + InceptionTime. In our first experiment, we set the ratio of selected synthetic data portion to 0.25. This ratio was doubled in the next experiment and doubled again in the last one. In each epoch bratio× 24c human-like models were randomly selected. For every selected human-like model, out of the nine videos of that human-like model expressing a specific expression from multiple angles, only one was chosen randomly. This selection method means that only bratio× 24c × 21 synthetic videos are used in the models training alongside the real data in that epoch. This number equals 126 for the ratio of 0.25, roughly equal to the number of real training videos. The selected human-like models and viewing angles were refreshed at the start of each epoch. The results for these experiments are shown in Table III. These results show that doubling the synthetic data ratio from 0.5 to 1 increases the model’s performance. However, this does not apply to the change from 0.25 to 0.5. In the case of FaceNet+Inception64, the synthetic ratio of 0.25 results in the highest performing network. This model achieved a 94% accuracy, which shows an 18% increase over the performance of the same model trained without the synthetic data. The combined confusion matrix of all the folds for this highest performing network is shown in Fig. 8. c. evaluating the sim2real approach on gifgif dataset To evaluate the generalization of our Sim2Real approach and model, we selected an external dataset for validation, GIFGIF [28].3 As previously noted, there are currently no video datasets with confusion samples [38]. GIFGIF [28] has video-level annotations and contains 2 of our emotions of interest (“anger” and “disgust”). This dataset is a collection of 3,858 cropped short videos with annotation scores for 17 emotions. We used GIFGIF API to get the first 400 highest-ranking videos for the “disgust” emotion. These videos were filtered down to 75 based on the following criteria: 1) contains a human face reaction video, 2) must not hold a higher score in other categories. Similarly, we chose the top 75 “anger” videos. The Arousal-Valence distribution of all these 150 samples is displayed in Fig. 9. The Arousal-Valence values are extracted using Emonet [35]. Fig. 9 suggests that this evaluation dataset is severely challenging. For this evaluation, no additional training was performed. 3We also considered AffWild, EmoReact, ElderReact, but their data did not contain anger or confusion, or their annotation schemes were not directly comparable with our data (e.g., frame-based). CK+ [21], Oulu-Casia [42], and MMI [26] were also not selected since they are all acted/posed and we focus on in-the-wild interactions. We tested the two FN+IN25 models presented in Table I on this dataset. One model was trained on our real dataset, another model was pretrained on the synthetic dataset then trained on the real dataset. The GIFGIF dataset was used as a test dataset for these models. The results for this experiment are shown in Table IV. The FN+IN25 model pretrained on synthetic data achieved a 75% accuracy. Out of 150 videos, this model misclassified 7 disgust videos and 2 anger as confusion. The same model without synthetic pretraining achieved a 64% accuracy and misclassified 14 disgust and 4 anger videos as confusion. Therefore, without any additional transfer learning, we showed that our Sim2Real approach improved FN+INC25 performance on this dynamic FER task by 11%. iv. discussion and limitations In this section, we elaborate on insights that we found while doing experiments and after analyzing the results. Our experiments showed that additional synthetic data is similar to have an extensive dataset, and the generalization of the final model is increased. An interesting finding in our experiments was that all the models with frozen FaceNet weights performed worse than their counterparts with unfrozen weights or even I3D. This was especially the case when considering non-Caucasian samples, which was an unexpected result because FaceNet was trained on a vast face recognition dataset. This shows
that although the FaceNet feature embedding performs well on facial recognition tasks, it may not be entirely related to the facial changes of a specific emotional expression. However, more research is needed to investigate these hypotheses. Another interesting point was the misclassification of specific samples that were revealed after we looked deeper into the results. These samples were classified wrongly even in our best model with an average accuracy of 94%, Fig. 10 show three examples of the eight wrongly classified videos from all folds. From left to right, each column corresponds to the first, middle, and last frame. The incorrect classification of the first video might be related to the minimal movement of the face. The generated synthetic dataset that we used lacks fully static samples. Additionally, the main concept behind the proposed model was the focus on dynamic movements. The incorrect prediction of the second video relates to its head movement. The dynamic movement of the expression is done over frames involved with head movement. This adds fluctuation to the inception model’s multivariant timeseries input that may not relate to the emotion. The OpenFace algorithm [2] uses perspective transform to make all of the input images have a frontal face view. The addition of this step may help deal with these types of videos. However, we believe the ultimate solution is in designing a model that can predict from shorter video snippets inputs. Poor prediction of the third video relates to the movement of hand midway through the video. Face occlusion is a challenge in FER, and even though new studies focus on reducing or removing the occlusions [25], [24], their advancement has been minimal. An interesting notion observed in the annotation of the real dataset was that annotators had trouble distinguishing between disgust and confusion in some cases. However, when the audio was played alongside the video, this confusion was resolved. This could mean that the next step for more inclusive and accurate facial expression recognition systems could incorporate audiovisual data processing. One of the main components of this work was the generation of synthetic data. The MakeHuman application limitation highly affects this component. More advanced applications can be used for this task to generate more
realistic synthetic datasets, and to explore other variations including age, non-binary gender, or conditions impacting facial development. Another point worth mentioning is that while we understand that the relatively small size of our dataset (synthetically generated dataset plus the real human dataset gathered from YouTube and Giphy) might be a limiting factor, this is sufficient to illustrate the proposed approach as a proof-of-concept. v. conclusions and future work We showed that our Sim2Real approach improves FN+INC64 performance on our dynamic FER task by 11- 18%, up to 94% on our internal test dataset, and up to 75% on a previously unseen dataset, compared to the performance of the same model architecture without synthetic training data. This performance was achieved by mixed synthetic and real data training. Additionally, it was shown that the proposed FN+INC model along with our Sim2Real approach is less sensitive to dataset ethnicity bias.