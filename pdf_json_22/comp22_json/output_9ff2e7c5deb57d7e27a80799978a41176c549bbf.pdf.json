{
    "abstractText": "We study the first gradient descent step on the first-layer parametersW in a two-layer neural network: f(x) = 1 \u221a N a>\u03c3(W>x), where W \u2208 Rd\u00d7N ,a \u2208 R are randomly initialized, and the training objective is the empirical MSE loss: 1 n \u2211n i=1(f(xi)\u2212yi) . In the proportional asymptotic limit where n, d,N \u2192\u221e at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 \u201cspike\u201d, which results in an alignment between the first-layer weights and the linear component of the teacher model f\u2217. To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on W with learning rate \u03b7, when f\u2217 is a single-index model. We consider two scalings of the first step learning rate \u03b7. For small \u03b7, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large \u03b7, we prove that for certain f\u2217, the same ridge estimator on trained features can go beyond this \u201clinear regime\u201d and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jimmy Ba"
        },
        {
            "affiliations": [],
            "name": "Murat A. Erdogdu"
        },
        {
            "affiliations": [],
            "name": "Taiji Suzuki"
        },
        {
            "affiliations": [],
            "name": "Zhichao Wang"
        },
        {
            "affiliations": [],
            "name": "Denny Wu"
        },
        {
            "affiliations": [],
            "name": "Greg Yang"
        }
    ],
    "id": "SP:76ec7dbb9d0e242276a0df5101cc4813245baec2",
    "references": [
        {
            "authors": [
                "Emmanuel Abbe",
                "Enric Boix Adsera",
                "Matthew Brennan",
                "Guy Bresler",
                "Dheeraj Nagaraj"
            ],
            "title": "The staircase property: How hierarchical structure can guide deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Emmanuel Abbe",
                "Enric Boix-Adsera",
                "Theodor Misiakiewicz"
            ],
            "title": "The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks",
            "venue": "arXiv preprint arXiv:2202.08658,",
            "year": 2022
        },
        {
            "authors": [
                "Radoslaw Adamczak"
            ],
            "title": "A note on the hanson-wright inequality for random vectors with dependencies",
            "venue": "Electronic Communications in Probability,",
            "year": 2015
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Simon S Du",
                "Wei Hu",
                "Zhiyuan Li",
                "Russ R Salakhutdinov",
                "Ruosong Wang"
            ],
            "title": "On exact computation with an infinitely wide neural net",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gerard Ben Arous",
                "Reza Gheissari",
                "Aukosh Jagannath"
            ],
            "title": "Online stochastic gradient descent on non-convex losses from high-dimensional inference",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Adlam",
                "Jeffrey Pennington"
            ],
            "title": "The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li"
            ],
            "title": "What can resnet learn efficiently, going beyond kernels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li"
            ],
            "title": "Backward feature correction: How deep learning performs deep learning",
            "venue": "arXiv preprint arXiv:2001.04413,",
            "year": 2020
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Yingyu Liang"
            ],
            "title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Francis Bach"
            ],
            "title": "Breaking the curse of dimensionality with convex neural networks",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Francis Bach"
            ],
            "title": "Learning Theory from First Principles",
            "year": 2023
        },
        {
            "authors": [
                "Jinho Baik",
                "G\u00e9rard Ben Arous",
                "Sandrine"
            ],
            "title": "P\u00e9ch\u00e9. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices",
            "venue": "The Annals of Probability,",
            "year": 2005
        },
        {
            "authors": [
                "Florent Benaych-Georges",
                "Raj Rao Nadakuditi"
            ],
            "title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices",
            "venue": "Advances in Mathematics,",
            "year": 2011
        },
        {
            "authors": [
                "Florent Benaych-Georges",
                "Raj Rao Nadakuditi"
            ],
            "title": "The singular values and vectors of low rank perturbations of large rectangular random matrices",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2012
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Daniel Hsu",
                "Siyuan Ma",
                "Soumik Mandal"
            ],
            "title": "Reconciling modern machinelearning practice and the classical bias\u2013variance trade-off",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Yu Bai",
                "Jason D. Lee"
            ],
            "title": "Beyond linearization: On quadratic and higher-order approximation of wide neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi",
                "Pascal Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "venue": "Oxford university press,",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Bodin",
                "Nicolas Macris"
            ],
            "title": "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Andrea Montanari",
                "Alexander Rakhlin"
            ],
            "title": "Deep learning: a statistical viewpoint",
            "venue": "Acta numerica,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Benigni",
                "Sandrine P\u00e9ch\u00e9"
            ],
            "title": "Eigenvalue distribution of some nonlinear models of random matrices",
            "venue": "Electronic Journal of Probability,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Benigni",
                "Sandrine P\u00e9ch\u00e9"
            ],
            "title": "Largest eigenvalues of the conjugate kernel of single-layered neural networks",
            "venue": "arXiv preprint arXiv:2201.04753,",
            "year": 2022
        },
        {
            "authors": [
                "Zhi-Dong Bai",
                "Jack W Silverstein"
            ],
            "title": "No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices",
            "venue": "The Annals of Probability,",
            "year": 1998
        },
        {
            "authors": [
                "Zhidong Bai",
                "Jack W Silverstein"
            ],
            "title": "Spectral analysis of large dimensional random matrices, volume 20",
            "year": 2010
        },
        {
            "authors": [
                "Mireille Capitaine"
            ],
            "title": "Limiting eigenvectors of outliers for spiked information-plus-noise type matrices",
            "venue": "In Se\u0301minaire de Probabilite\u0301s XLIX,",
            "year": 2018
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Francis Bach"
            ],
            "title": "On the global convergence of gradient descent for overparameterized models using optimal transport",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Francis Bach"
            ],
            "title": "Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Shuxiao Chen",
                "Hangfeng He",
                "Weijie Su"
            ],
            "title": "Label-aware neural tangent kernel: Toward better generalization and local elasticity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Simran Kaur",
                "Yuanzhi Li",
                "J Zico Kolter",
                "Ameet Talwalkar"
            ],
            "title": "Gradient descent on neural networks typically occurs at the edge of stability",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Niladri S Chatterji",
                "Philip M Long",
                "Peter L Bartlett"
            ],
            "title": "When does gradient descent with logistic loss find interpolating two-layer networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Edouard Oyallon",
                "Francis Bach"
            ],
            "title": "On lazy training in differentiable programming",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xiuyuan Cheng",
                "Amit Singer"
            ],
            "title": "The spectrum of random inner-product kernel matrices",
            "venue": "Random Matrices: Theory and Applications,",
            "year": 2013
        },
        {
            "authors": [
                "Nello Cristianini",
                "John Shawe-Taylor",
                "Andre Elisseeff",
                "Jaz Kandola"
            ],
            "title": "On kernel-target alignment",
            "venue": "Advances in neural information processing systems,",
            "year": 2001
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Ethan Dyer",
                "Guy Gur-Ari"
            ],
            "title": "Asymptotics of wide networks from feynman diagrams",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Oussama Dhifallah",
                "Yue M Lu"
            ],
            "title": "A precise performance analysis of learning with random features",
            "venue": "arXiv preprint arXiv:2008.11904,",
            "year": 2020
        },
        {
            "authors": [
                "Amit Daniely",
                "Eran Malach"
            ],
            "title": "Learning parities with neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yen Do",
                "Van Vu"
            ],
            "title": "The spectrum of random kernel matrices: universality results for rough and varying kernels",
            "venue": "Random Matrices: Theory and Applications,",
            "year": 2013
        },
        {
            "authors": [
                "Edgar Dobriban",
                "Stefan Wager"
            ],
            "title": "High-dimensional asymptotics of prediction: Ridge regression and classification",
            "venue": "The Annals of Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Konstantin Donhauser",
                "Mingqi Wu",
                "Fanny Yang"
            ],
            "title": "How rotational invariance of common kernels prevents generalization in high dimensions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Simon S. Du",
                "Xiyu Zhai",
                "Barnabas Poczos",
                "Aarti Singh"
            ],
            "title": "Gradient descent provably optimizes over-parameterized neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Noureddine El Karoui"
            ],
            "title": "The spectrum of kernel random matrices",
            "venue": "The Annals of Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Noureddine El Karoui"
            ],
            "title": "On the impact of predictor geometry on the performance on highdimensional ridge-regularized generalized robust regression estimators",
            "venue": "Probability Theory and Related Fields,",
            "year": 2018
        },
        {
            "authors": [
                "Spencer Frei",
                "Niladri S Chatterji",
                "Peter L Bartlett"
            ],
            "title": "Random feature amplification: Feature learning and generalization in neural networks",
            "venue": "arXiv preprint arXiv:2202.07626,",
            "year": 2022
        },
        {
            "authors": [
                "Stanislav Fort",
                "Gintare Karolina Dziugaite",
                "Mansheej Paul",
                "Sepideh Kharaghani",
                "Daniel M Roy",
                "Surya Ganguli"
            ],
            "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhou Fan",
                "Andrea Montanari"
            ],
            "title": "The spectral norm of random inner-product kernel matrices",
            "venue": "Probability Theory and Related Fields,",
            "year": 2019
        },
        {
            "authors": [
                "Reza Rashidi Far",
                "Tamer Oraby",
                "Wlodzimierz Bryc",
                "Roland Speicher"
            ],
            "title": "Spectra of large block matrices",
            "venue": "arXiv preprint cs/0610045,",
            "year": 2006
        },
        {
            "authors": [
                "Zhou Fan",
                "Zhichao Wang"
            ],
            "title": "Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Sharad Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Federica Gerace",
                "Bruno Loureiro",
                "Florent Krzakala",
                "Marc M\u00e9zard",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Generalisation error in learning with random features and the hidden manifold model",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Goldt",
                "Bruno Loureiro",
                "Galen Reeves",
                "Florent Krzakala",
                "Marc M\u00e9zard",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "The gaussian equivalence of generative models for learning with shallow neural networks",
            "venue": "Proceedings of Machine Learning Research vol,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Goldt",
                "Marc M\u00e9zard",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Modeling the influence of data structure on learning in neural networks: The hidden manifold model",
            "venue": "Physical Review X,",
            "year": 2020
        },
        {
            "authors": [
                "Behrooz Ghorbani",
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Limitations of lazy training of two-layers neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Behrooz Ghorbani",
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "When do neural networks outperform kernel methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Behrooz Ghorbani",
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Linearized twolayers neural networks in high dimension",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yehoram Gordon"
            ],
            "title": "On milman\u2019s inequality and random subspaces which escape through a mesh in R",
            "venue": "In Geometric aspects of functional analysis,",
            "year": 1988
        },
        {
            "authors": [
                "Mario Geiger",
                "Stefano Spigler",
                "Arthur Jacot",
                "Matthieu Wyart"
            ],
            "title": "Disentangling feature and lazy training in deep neural networks",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2020
        },
        {
            "authors": [
                "Karl Hajjar",
                "L\u00e9n\u00e4\u0131c Chizat",
                "Christophe Giraud"
            ],
            "title": "Training integrable parameterizations of deep neural networks in the infinite-width limit",
            "venue": "arXiv preprint arXiv:2110.15596,",
            "year": 2021
        },
        {
            "authors": [
                "J William Helton",
                "Reza Rashidi Far",
                "Roland Speicher"
            ],
            "title": "Operator-valued semicircular elements: solving a quadratic matrix equation with positivity constraints",
            "year": 2007
        },
        {
            "authors": [
                "Hong Hu",
                "Yue M Lu"
            ],
            "title": "Universality laws for high-dimensional learning with random features",
            "venue": "arXiv preprint arXiv:2009.07669,",
            "year": 2020
        },
        {
            "authors": [
                "J William Helton",
                "Tobias Mai",
                "Roland Speicher"
            ],
            "title": "Applications of realizations (aka linearizations) to free probability",
            "venue": "Journal of Functional Analysis,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaoyang Huang",
                "Horng-Tzer Yau"
            ],
            "title": "Dynamics of deep neural networks and neural tangent hierarchy",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Masaaki Imaizumi",
                "Kenji Fukumizu"
            ],
            "title": "Deep neural networks learn non-smooth functions effectively",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Stanislaw Jastrzebski",
                "Maciej Szymczak",
                "Stanislav Fort",
                "Devansh Arpit",
                "Jacek Tabor",
                "Kyunghyun Cho",
                "Krzysztof Geras"
            ],
            "title": "The break-even point on optimization trajectories of deep neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ziwei Ji",
                "Matus Telgarsky"
            ],
            "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Stefani Karp",
                "Ezra Winston",
                "Yuanzhi Li",
                "Aarti Singh"
            ],
            "title": "Local signal adaptivity: Provable feature learning in neural networks beyond kernels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Yasaman Bahri",
                "Ethan Dyer",
                "Jascha Sohl-Dickstein",
                "Guy Gur-Ari"
            ],
            "title": "The large learning rate phase of deep learning: the catapult mechanism",
            "venue": "arXiv preprint arXiv:2003.02218,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenyu Liao",
                "Romain Couillet",
                "Michael W Mahoney"
            ],
            "title": "A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bruno Loureiro",
                "Cedric Gerbelot",
                "Hugo Cui",
                "Sebastian Goldt",
                "Florent Krzakala",
                "Marc Mezard",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Learning curves of generic features maps for realistic datasets with a teacher-student model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Cosme Louart",
                "Zhenyu Liao",
                "Romain Couillet"
            ],
            "title": "A random matrix approach to neural networks",
            "venue": "The Annals of Applied Probability,",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Leclerc",
                "Aleksander Madry"
            ],
            "title": "The two regimes of deep network training",
            "venue": "arXiv preprint arXiv:2002.10376,",
            "year": 2020
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Tengyu Ma",
                "Hongyang R Zhang"
            ],
            "title": "Learning over-parametrized two-layer neural networks beyond ntk",
            "venue": "In Conference on learning theory,",
            "year": 2020
        },
        {
            "authors": [
                "Tengyuan Liang",
                "Alexander Rakhlin"
            ],
            "title": "Just interpolate: Kernel \u201cridgeless\u201d regression can generalize",
            "venue": "The Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Colin Wei",
                "Tengyu Ma"
            ],
            "title": "Towards explaining the regularization effect of initial large learning rate in training neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Elizabeth S Meckes"
            ],
            "title": "The random matrix theory of the classical compact groups, volume 218",
            "year": 2019
        },
        {
            "authors": [
                "Eran Malach",
                "Pritish Kamath",
                "Emmanuel Abbe",
                "Nathan Srebro"
            ],
            "title": "Quantifying the benefit of using differentiable learning over tangent kernels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Song Mei",
                "Andrea Montanari"
            ],
            "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Song Mei",
                "Andrea Montanari",
                "Phan-Minh Nguyen"
            ],
            "title": "A mean field view of the landscape of two-layer neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "James A Mingo",
                "Roland Speicher"
            ],
            "title": "Free probability and random matrices, volume",
            "year": 2017
        },
        {
            "authors": [
                "Andrea Montanari",
                "Basil Saeed"
            ],
            "title": "Universality of empirical risk minimization",
            "venue": "arXiv preprint arXiv:2202.08832,",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Montanari",
                "Yiqiao Zhong"
            ],
            "title": "The interpolation phase transition in neural networks: Memorization and generalization under lazy training",
            "venue": "arXiv preprint arXiv:2007.12826v1,",
            "year": 2020
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Bayesian learning for neural networks, volume 118",
            "venue": "Springer Science & Business Media,",
            "year": 1995
        },
        {
            "authors": [
                "Phan-Minh Nguyen"
            ],
            "title": "Analysis of feature learning in weight-tied autoencoders via the mean field lens",
            "venue": "arXiv preprint arXiv:2102.08373,",
            "year": 2021
        },
        {
            "authors": [
                "Atsushi Nitanda",
                "Taiji Suzuki"
            ],
            "title": "Stochastic particle gradient descent for infinite ensembles",
            "venue": "arXiv preprint arXiv:1712.05438,",
            "year": 2017
        },
        {
            "authors": [
                "Atsushi Nitanda",
                "Denny Wu",
                "Taiji Suzuki"
            ],
            "title": "Convex analysis of the mean field langevin dynamics",
            "venue": "arXiv preprint arXiv:2201.10469,",
            "year": 2022
        },
        {
            "authors": [
                "Guillermo Ortiz-Jim\u00e9nez",
                "Seyed-Mohsen Moosavi-Dezfooli",
                "Pascal Frossard"
            ],
            "title": "What can linearized neural networks actually say about generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Samet Oymak",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks",
            "venue": "IEEE Journal on Selected Areas in Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Pesme",
                "Loucas Pillaud-Vivien",
                "Nicolas Flammarion"
            ],
            "title": "Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Pratik Worah"
            ],
            "title": "Nonlinear random matrix theory for deep learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Maria Refinetti",
                "Sebastian Goldt",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Classifying highdimensional gaussian mixtures: Where kernel methods fail and neural networks succeed",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "In Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Taiji Suzuki",
                "Shunta Akiyama"
            ],
            "title": "Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods",
            "year": 2012
        },
        {
            "authors": [
                "Johannes Schmidt-Hieber"
            ],
            "title": "Nonparametric regression using deep neural networks with relu activation function",
            "venue": "The Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Gilbert W Stewart"
            ],
            "title": "Matrix perturbation theory",
            "year": 1990
        },
        {
            "authors": [
                "Taiji Suzuki"
            ],
            "title": "Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality",
            "venue": "arXiv preprint arXiv:1810.08033,",
            "year": 2018
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Ben Adlam",
                "Jeffrey Pennington"
            ],
            "title": "Covariate shift in high-dimensional random feature regression",
            "venue": "arXiv preprint arXiv:2111.08234,",
            "year": 2021
        },
        {
            "authors": [
                "Christos Thrampoulidis",
                "Samet Oymak",
                "Babak Hassibi"
            ],
            "title": "Regularized linear regression: A precise analysis of the estimation error",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Rodrigo Veiga",
                "Ludovic Stephan",
                "Bruno Loureiro",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks",
            "venue": "arXiv preprint arXiv:2202.00293,",
            "year": 2022
        },
        {
            "authors": [
                "Blake Woodworth",
                "Suriya Gunasekar",
                "Jason D Lee",
                "Edward Moroshko",
                "Pedro Savarese",
                "Itay Golan",
                "Daniel Soudry",
                "Nathan Srebro"
            ],
            "title": "Kernel and rich regimes in overparametrized models",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Colin Wei",
                "Jason D Lee",
                "Qiang Liu",
                "Tengyu Ma"
            ],
            "title": "Regularization matters: Generalization and optimization of neural nets vs their induced kernel",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Denny Wu",
                "Ji Xu"
            ],
            "title": "On the optimal weighted `2 regularization in overparameterized linear regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhichao Wang",
                "Yizhe Zhu"
            ],
            "title": "Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks",
            "venue": "arXiv preprint arXiv:2109.09304,",
            "year": 2021
        },
        {
            "authors": [
                "Greg Yang"
            ],
            "title": "Tensor programs iii: Neural matrix laws",
            "venue": "arXiv preprint arXiv:2009.10685,",
            "year": 2020
        },
        {
            "authors": [
                "Greg Yang",
                "Edward J Hu"
            ],
            "title": "Feature learning in infinite-width neural networks",
            "venue": "arXiv preprint arXiv:2011.14522,",
            "year": 2020
        },
        {
            "authors": [
                "Gilad Yehudai",
                "Ohad Shamir"
            ],
            "title": "On the power and limitations of random features for understanding neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "N a>\u03c3(W>x), where W \u2208 Rd\u00d7N ,a \u2208 RN are randomly initialized, and the training objective\nis the empirical MSE loss: 1 n \u2211n i=1(f(xi)\u2212yi)\n2. In the proportional asymptotic limit where n, d,N \u2192\u221e at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 \u201cspike\u201d, which results in an alignment between the first-layer weights and the linear component of the teacher model f\u2217. To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on W with learning rate \u03b7, when f\u2217 is a single-index model. We consider two scalings of the first step learning rate \u03b7. For small \u03b7, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large \u03b7, we prove that for certain f\u2217, the same ridge estimator on trained features can go beyond this \u201clinear regime\u201d and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training."
        },
        {
            "heading": "1 Introduction",
            "text": "We consider the training of a fully-connected two-layer neural network (NN) with N neurons,\nfNN(x) = 1\u221a N N\u2211 i=1 ai\u03c3(\u3008x,wi\u3009) = 1\u221a N a>\u03c3(W>x), (1.1)\nwhere x \u2208 Rd,W \u2208 Rd\u00d7N ,a \u2208 RN , \u03c3 is the nonlinear activation function applied entry-wise, and the training objective is to minimize the (potentially `2-regularized) empirical risk. Our analysis will be made in the proportional asymptotic limit, i.e., the number of training data n, the input dimensionality d, and the number of features (neurons) N jointly tend to infinity. Intuitively, this regime reflects the setting where the network width and data size are comparable, which is consistent with practical choices of model scaling.\nWhen the first layer W is fixed and only the second layer a is optimized, we arrive at a kernel model, where the kernel defined by features x\u2192 \u03c3(W>x) (often called the hidden representation) is referred to as the conjugate kernel (CK) [Nea95]. When W is randomly initialized, this model is an example of the random features (RF) model [RR08]. The training and test performance of RF regression has been extensively studied in the proportional limit [LLC18, MM22]. These precise characterizations reveal interesting phenomena also present in practical deep learning, such as the non-monotonic risk curve [BHMM19].\nHowever, RF models do not fully explain the empirical success of neural networks: one crucial advantage of deep learning is the ability to learn useful features [GDDM14, DCLT18] that \u201cadapt\u201d to the learning problem [Suz18]. In fact, recent works have shown that such adaptivity enables NNs optimized by gradient\n\u2217University of Toronto and Vector Institute for Artificial Intelligence. {jba,erdogdu,dennywu}@cs.toronto.edu. \u2020University of Tokyo and RIKEN Center for Advanced Intelligence Project. taiji@mist.i.u-tokyo.ac.jp. \u2021University of California, San Diego. zhw036@ucsd.edu. \u00a7Microsoft Research AI. gregyang@microsoft.com.\nar X\niv :2\n20 5.\n01 44\n5v 1\n[ st\nat .M\nL ]\n3 M\nay 2\n02 2\ndescent to outperform a wide range of linear/kernel estimators [AZL19, GMMM19]. While many explanations of this separation between NNs and kernel models have been proposed, our starting point is the empirical finding that \u201cnon-kernel\u201d behavior often occurs in the early phase of NN optimization, especially under large learning rate [JSF+20, FDP+20]. The goal of this work is to answer the following question:\nCan we precisely capture the presence of feature learning in the early phase of gradient descent training, and demonstrate its improvement over the initial (fixed) kernel in the proportional limit?"
        },
        {
            "heading": "1.1 Contributions",
            "text": "Motivated by the above observations, we investigate a simplified scenario of the \u201cearly phase\u201d of learning: how the first gradient step on the first-layer parameters W impacts the representation of the two-layer NN (1.1). Specifically, we consider the regression setting with the squared (MSE) loss, and a student-teacher model in the proportional asymptotic limit; we characterize the prediction risk of the kernel ridge regression estimator on top of the first-layer CK feature x\u2192 \u03c3(W>x), before and after the gradient descent step1 on the empirical risk (starting from Gaussian initialization). Our findings can be summarized as follows.\n\u2022 In Section 3, we show that the first gradient step on W is approximately rank-1; hence under appropriate learning rate, the updated weight matrix exhibits a information (spike) plus noise (bulk) structure.\n\u2022 As a result, the isolated singular vector of the weight matrix aligns with the linear component of target function (teacher) f\u2217, and the top eigenvector of the CK matrix aligns with the training labels y.\nNext in Section 4 we study how the aforementioned alignment improves the kernel. We consider a more specialized setting where the teacher f\u2217 is a single-index model, in which case the prediction risk of a large class of RF/kernel ridge regression estimators is lower-bounded by the L2-norm of the \u201cnonlinear\u201d component the teacher \u2016P>1f\u2217\u20162L2 , i.e., they can only learn linear functions on the input. After taking one gradient step on W , we compute the CK ridge estimator using separate training data, and compare its prediction risk against this linear lower bound. Our analysis will be made under two choices of learning rate scalings (see Figure 1):\n\u2022 Small lr: \u03b7 = \u0398(1). In Section 4.2, we extend the Gaussian Equivalence Theorem (GET) in [HL20] to the updated feature map trained via multiple gradient descent steps on W with learning rate \u03b7 = \u0398(1); this allows us to precisely characterize the prediction risk using random matrix theoretical tools. We prove that after one gradient step, the ridge regression estimator\non the learned CK features already exhibits nontrivial improvement over the initial RF ridge model, but it remains in the \u201clinear regime\u201d and cannot outperform the best linear estimator on the input. \u2022 Large lr: \u03b7 = \u0398( \u221a N). In Section 4.3, we analyze a larger learning rate that coincides with the maximal\nupdate parameterization in [YH20]. For certain target functions f\u2217, we prove that kernel ridge regression after one feature learning step can achieve lower risk than the lower bound \u2016P>1f\u2217\u20162L2 , and thus outperform a wide range of kernel ridge estimators (including the neural tangent kernel of (1.1))."
        },
        {
            "heading": "1.2 Related Works",
            "text": "Asymptotics of Kernel Regression. A plethora of recent works provided precise performance analysis of RF and kernel models in the proportional limit [MM22, GLK+20, DL20, LCM20, AP20]. These results typically build upon analyses of the spectrum of kernel matrices, a key ingredient in which is the \u201clinearization\u201d of nonlinear random matrices via Taylor expansion [EK10] or orthogonal polynomials [CS13, PW17].\nConsequently, a large class of kernel models are essentially linear in the proportional limit [LR20, BMR21]. In the case of RF models, similar property is captured by the Gaussian equivalence theorem [GMKZ20, HL20,\n1Some of our results also apply to multiple gradient steps on the first layer W , which we specify in the sequel.\nGLR+21], which roughly states that RF estimators achieve the same prediction risk as a (noisy) linear model. For input on unit sphere, [GMMM21, MMM21] showed that sample size n = \u2126(d2) is required to go beyond this \u201clinear\u201d regime. As we will see in certain settings, such limitation can also be overcome (in the n d scaling) by training the feature map for one gradient step with sufficiently large learning rate.\nAdvantage of NNs over Fixed Kernels. It is well-known that under certain initialization, the learning dynamics of overparameterized NNs can be described by the neural tangent kernel (NTK) [JGH18]. However, the NTK description essentially \u201cfreezes\u201d the model around its initialization [COB19], and thus does not explain the presence of feature learning in NNs [YH20].\nIn fact, various works have shown that deep learning is more powerful than kernel methods in terms of approximation and estimation ability [Bac17, Suz18, IF19, SH20, GMMM20]. Moreover, in some specialized settings, NNs optimized with gradient-based methods can outperform the NTK (or more generally any kernel estimators) in terms of generalization error [AZL19, WLLM19, GMMM19, LMZ20, DM20, SA20, AZL20, RGKZ21, KWLS21, ABAB+21] (see [MKAS21, Table 2] for survey). These results often require careful analysis of the landscape (e.g., properties of global optimum) or optimization dynamics; in contrast, our goal is to precisely characterize the first gradient step and demonstrate a similar separation.\nEarly Phase of NN Optimization. Recent empirical studies suggest that properties of the final trained model is strongly influenced by the early stage of optimization [GAS19, LM20, PPVF21], and the NTK evolves most rapidly in the first few epochs [FDP+20]. Large learning rate in the initial steps can impact the conditioning of loss surface [JSF+20, CKL+21] and potentially improve the generalization performance [LWM19, LBD+20]. Under structural assumptions on the data, it has been proved that one gradient step with sufficiently large learning rate can drastically decrease the training loss [CLB21], extract task-relevant features [DM20, FCB22], or escape the trivial stationary point at initialization [HCG21]. While these works also highlight the benefit of one feature learning step2, to our knowledge this advantage has not been precisely characterized in the proportional regime (where the performance of RF models has been extensively studied)."
        },
        {
            "heading": "2 Problem Setup and Basic Assumptions",
            "text": "Notations. Throughout this paper, \u2016 \u00b7 \u2016 denotes the `2 norm for vectors and the `2 \u2192 `2 operator norm for matrices, and \u2016 \u00b7 \u2016F is the Frobenius norm. For matrix M \u2208 Rn\u00d7n, tr(M) = 1n Tr(M) is the normalized trace. Od(\u00b7) and od(\u00b7) stand for the standard big-O and little-o notations, where the subscript highlights the asymptotic variable; we write O\u0303(\u00b7) when the (poly-)logarithmic factors are ignored. Od,P(\u00b7) (resp. od,P(\u00b7)) represents big-O (resp. little-o) in probability as d\u2192\u221e. \u2126(\u00b7),\u0398(\u00b7) are defined analogously. \u0393 is the standard Gaussian distribution in Rd. Given f : Rd \u2192 R, we denote its Lp-norm w.r.t. \u0393 as \u2016f\u2016Lp(Rd,\u0393), which we abbreviate as \u2016f\u2016Lp when the context it clear. \u00b5MP\u03b3 is the Marchenko\u2013Pastur distribution with ratio \u03b3."
        },
        {
            "heading": "2.1 Training Procedure",
            "text": "Gradient Descent on the 1st Layer. Given training examples {(xi, yi)}ni=1, we learn the two-layer NN (1.1) by minimizing the empirical risk: L(f) = 1n \u2211n i=1 `(f(xi), yi), where ` is the squared loss `(x, y) = 1 2 (x \u2212 y)\n2. As previously remarked, fixing the first layer W at random initialization and learning the second layer a yields RF model, which is a convex problem with a closed-form solution. In contrast, we are interested in learning the feature map (representation); hence we first fix a (at initialization) and perform gradient descent on W . We write the initialized first-layer weights as W 0, and the weights after t gradient steps as W t. The gradient update with learning rate \u03b7 > 0 is given as: W t+1 = W t + \u03b7 \u221a N \u00b7Gt, where\nGt := 1 n X> [( 1\u221a N ( y \u2212 1\u221a N \u03c3(XW t)a ) a> ) \u03c3\u2032(XW t) ] , (2.1)\nfor t \u2208 N, in which is the Hadamard product, \u03c3\u2032 is the derivative of \u03c3 (acting entry-wise), and we denoted the input feature matrix X \u2208 Rn\u00d7d, and the corresponding label vector y \u2208 Rn. We remark that the\u221a N -scaling in front of the learning rate \u03b7 is due to the 1\u221a\nN -prefactor in our definition of two-layer NN (1.1).\n2We however note that the \u201cearly phase\u201d is not always sufficient: for certain teacher model f\u2217, (stochastic) gradient descent may exhibit a long initial \u201csearch\u201d stage before nontrivial alignment can be achieved, see [AGJ21, VSL+22].\nRidge Regression for the 2nd Layer. After obtaining the updated weights W 1, we evaluate the quality of the new CK features by computing the prediction risk of the kernel ridge regression estimator on top of the first-layer representation. Note that if ridge regression is performed on the same data X, then after one feature learning step, W 1 is no longer independent of X, which significantly complicates the analysis. To circumvent this difficulty, we estimate the regression coefficients a\u0302 using a new set of training data {x\u0303i, y\u0303i}ni=1, which for simplicity we assume to have the same size as the original dataset. This can be interpreted as the representation is \u201cpretrained\u201d on separate data before the ridge estimator is learned.\nDenote the feature matrix on the fresh training set {X\u0303, y\u0303} as \u03a6 := 1\u221a N \u03c3(X\u0303W 1) \u2208 Rn\u00d7N , the CK ridge\nregression estimator is given by f\u0302(x) = 1\u221a N a\u0302>\u03c3 ( W>1 x ) , where a\u0302 = argmina { 1 n\u2016y\u0303 \u2212\u03a6a\u2016 2 + \u03bbN \u2016a\u2016 2 } ."
        },
        {
            "heading": "2.2 Main Assumptions",
            "text": "Given a target function (ground truth) f\u2217 and a learned model f\u0302 , we evaluate the model performance using the prediction risk: R(f\u0302) = Ex(f\u0302(x) \u2212 f\u2217(x))2 = \u2016f\u0302 \u2212 f\u2217\u20162L2 , where the expectation is taken over the test data from the same training distribution. Our analysis will be made under the following assumptions.\nAssumption 1.\n1. Proportional Limit. n, d,N \u2192\u221e, n/d\u2192 \u03c81, N/d\u2192 \u03c82, where \u03c81, \u03c82 \u2208 (0,\u221e).\n2. Student-teacher Setup. Labels are generated as yi = f \u2217(xi) + \u03b5i, where xi i.i.d.\u223c N (0, I), \u03b5i is i.i.d. subGaussian noise with mean 0 and variance \u03c32\u03b5 , and the teacher f\n\u2217 is \u03bb\u03c3-Lipschitz with \u2016f\u2217\u2016L2 = \u0398d(1). 3. Normalized Activation. The nonlinear activation \u03c3 has \u03bb\u03c3-bounded first three derivatives almost\nsurely. In addition, the activation function satisfies E[\u03c3(z)] = 0, E[z\u03c3(z)] 6= 0, for z \u223c N (0, 1).\n4. Gaussian Initialization. \u221a d \u00b7 [W 0]ij i.i.d.\u223c N (0, 1), \u221a N \u00b7 [a]j\ni.i.d.\u223c N (0, 1), for all i \u2208 [d], j \u2208 [N ]. Remark. Following [HL20], we assume smooth and centered activation to simplify the computation; Section 4 provides empirical evidence that our results hold beyond this condition (see also [LGC+21]). We expect that the Gaussian input assumption may be replaced by weaker orthogonality conditions as in [FW20].\nUnder Assumption 1, increasing the sample size corresponds to enlarging \u03c81, and increasing the network width corresponds to enlarging \u03c82. The proportional scaling of n, d,N (also referred to as the \u201clinear-width\u201d regime) implies that the model width is not significantly larger than the training set size, in contrast to the polynomial overparameterization often required in NTK analyses [DZPS19], which may be less realistic for practical settings.\nImportantly, the initialization of our two-layer NN (1.1) resembles the mean-field parameterization [MMN18, CB18]: the second layer is divided by an additional \u221a N -factor compared to the kernel (NTK) scaling \u2014 this ensures that fNN(x) = od,P(1) at initialization and enables feature learning (see [YH20, Corollary 3.10]). As an illustrative example, in Figure 2 we plot the gradient descent trajectory of the first-layer parameters W in two coordinates. Observe that under the mean-field parameterization (main figure), the neurons travel away from the initialization and align with the target function (black dashed lines), whereas in the NTK parameterization (subfigure, which omits the 1\u221a\nN -prefactor), the parameters remain\nclose to their initialization and hence do not learn useful features."
        },
        {
            "heading": "2.3 Lower Bound for Kernel Ridge Regression",
            "text": "To illustrate the benefit of feature learning, we compare the prediction risk of ridge regression on the trained CK (after one gradient step) against the ridge estimator on the initial RF kernels. Specifically, given training data {xi, yi}ni=1, we consider the following class of kernel models for comparison.\n\u2022 Random Features Model. We introduce two RF kernels associated with the two-layer NN (1.1) at initialization: the conjugate kernel (CK) defined by features \u03c6CK(x) = 1\u221a N \u03c3(W>0 x) \u2208 RN , and the\nneural tangent kernel (NTK) [JGH18] defined by features \u03c6NTK(x) = 1\u221a Nd Vec ( \u03c3\u2032(W>0 x)x >) \u2208 RNd. Given feature map RF \u2208 {CK,NTK}, the RF ridge regression estimator can be written as\nf\u0302RF(x) = \u3008\u03c6RF(x), a\u0302\u3009, a\u0302 = argmin a\u2208RN { 1 n n\u2211 i=1 (yi \u2212 \u3008\u03c6RF(xi),a\u3009)2 + \u03bb N \u2016a\u20162 } . (2.2)\n\u2022 Rotationally Invariant Kernel Model. Consider the inner-product kernel: k(x,y) = g ( \u3008x,y\u3009 d ) , and\nEuclidean distance kernel: k(x,y) = g ( \u2016x\u2212y\u20162\nd\n) , where g satisfies certain smoothness conditions as in\n[EK10]. Denote the associated RKHS as H, and [K]ij = k(xi,xj). The kernel ridge estimator is given by\nf\u0302ker = argmin f\u2208H { 1 n n\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162H } \u21d2 f\u0302ker(x) = k(x,X)>(K + \u03bbI)\u22121y. (2.3)\nWe denote the prediction risk of the above kernel estimators as RCK(\u03bb),RNTK(\u03bb),Rker(\u03bb), respectively. The following lower bound is a simple combination of known results from [EK10, HL20, MZ20, BMR21].\nProposition 1 (Informal). Under Assumptions 1 and 2,\ninf \u03bb>0\nmin{RCK(\u03bb),RNTK(\u03bb),Rker(\u03bb)} \u2265 \u2016P>1f\u2217\u20162L2 + od,P(1), (2.4)\nwhere P>1 denotes the projector orthogonal to constant and linear functions in L 2(Rd,\u0393).\nThis proposition implies that in the proportional limit, ridge regression on the RF or rotationally invariant kernels defined above does not outperform the best linear estimator on the input data \u2014 it cannot achieve negligible prediction risk unless the target function is linear (i.e., \u2016P>1f\u2217\u2016L2 = 0). In Section 4, we compare the prediction risk of the ridge estimator on trained features against this lower bound."
        },
        {
            "heading": "3 How Does One Gradient Step Change the Weights?",
            "text": "In this section, we study the properties of the updated weight matrix W 1 in the two-layer NN (1.1). We first show that the first gradient step on W can be approximated by a rank-1 matrix, which contains information of the training labels y. Based on this property, we provide a signal (spike) plus noise (bulk) decomposition of W 1, and prove that the isolated singular vector is aligned to the linear component of the teacher f \u2217."
        },
        {
            "heading": "3.1 Almost Rank-1 Property of the Gradient Matrix",
            "text": "We utilize the orthogonal decomposition of the activation function \u03c3 (note that \u03c3 is normalized by Assumption 1 so that E[\u03c3(z)] = 0). Define the coefficients\n\u00b51 = E[z\u03c3(z)], \u00b52 = \u221a E[\u03c3(z)2]\u2212 \u00b521, where z \u223c N (0, 1).\nThis implies that \u03c3(z) = \u00b51z + \u03c3\u22a5(z), where E[\u03c3\u22a5(z)] = E[z\u03c3\u22a5(z)] = 0, and E[\u03c3\u22a5(z)2] = \u00b522. When \u00b51 6= 0 (again due to Assumption 1), we have the following characterization of the first gradient step G0 in (2.1). Proposition 2. Define G0 = 1\n\u03b7 \u221a N (W 1\u2212W 0) and a rank-1 matrix A := \u00b51n\u221aNX >ya>. Under Assumption\n1, there exist some constants c, C > 0 such that for all large n,N, d, with probability at least 1\u2212 ne\u2212c log2 n,\n\u2016G0 \u2212A\u2016 \u2264 C log2 n\u221a\nn \u00b7 \u2016G0\u2016.\nProposition 2 suggests that the first-step gradient can be approximated in operator norm by a rank-1 matrix A; thus, when the learning rate is reasonably large, we expect a \u201cspike\u201d to appear in the updated weight matrix W 1. Intuitively, since this rank-1 direction relates to the label vector y, the resulting W 1 may be \u201caligned\u201d to the target function f\u2217. This intuition is confirmed in the next subsection.\nScaling of Learning Rate \u03b7. Before we analyze the alignment property, it is important to specify an appropriate learning rate \u03b7 such that change in the first-layer weights after one gradient descent step is neither insignificant nor unreasonably large. From Assumption 1 we know that for proportional n, d,N , the initial weight matrix satisfies \u2016W 0\u2016 = \u0398d,P(1), \u2016W 0\u2016F = \u0398d,P( \u221a d), and due to Proposition 2, the first\ngradient step satisfies \u221a N\u2016G0\u2016 = \u0398d,P(1), \u221a N\u2016G0\u2016F = \u0398d,P(1).\nIn other words, if we write \u03b7 = \u0398(N\u03b1), then \u03b1 \u2265 0 is required so that the change in the weight matrix is non-negligible (one may verify that for \u03b7 = od(1), the test performance of kernel ridge regression remains unchanged after one GD step). On the other hand, when \u03b1 > 1/2, the gradient \u201coverwhelms\u201d the initialized parameters W 0, and the preactivation feature \u3008x,wi\u3009 in the NN (1.1) becomes unbounded as N \u2192\u221e. This motivates us to consider the following two regimes of learning rate scaling.\nSmall lr: \u03b7 = \u0398(1) \u21d2 \u2016W 1 \u2212W 0\u2016 \u2016W 0\u2016 (3.1) Large lr: \u03b7 = \u0398( \u221a N) \u21d2 \u2016W 1 \u2212W 0\u2016F \u2016W 0\u2016F (3.2)\nThe following subsection and Section 4.2 consider the setting where \u03b7 = \u0398(1), which is parallel to common practice in NN optimization3. Whereas in Section 4.3 we analyze the larger step size \u03b7 = \u0398( \u221a N), which resembles the learning rate scaling in the maximal update parameterization in [YH20]; in particular, using Lemma 14 in Appendix B.1 one can easily verify that given data point x \u223c N (0, I), the change in each coordinate of the feature vector is roughly of the same order as its initialized magnitude, that is, for i \u2208 [N ],\u2223\u2223\u03c3(W>1 x)\u2212 \u03c3(W>0 x)\u2223\u2223i \u2223\u2223\u03c3(W>0 x)\u2223\u2223i = \u0398\u0303(1) with probability 1 as N \u2192\u221e."
        },
        {
            "heading": "3.2 Alignment with the Target Function",
            "text": "Under Assumption 1, we may utilize the following orthogonal decomposition of the target function f\u2217,\nf\u2217(x) = \u00b5\u22170 + \u00b5 \u2217 1\u3008x,\u03b2\u2217\u3009+ P>1f\u2217(x), \u00b5\u22171\u03b2\u2217 = E[xf\u2217(x)], (3.3)\nwhere P>1 is the projector orthogonal to constant and linear functions in L 2(Rd,\u0393), which implies that E[P>1f\u2217(x)] = 0,E[xP>1f\u2217(x)] = 0 (e.g., see [BMR21, Section 4.3]). As d\u2192\u221e, quantities defined in (3.3) satisfy \u2016\u03b2\u2217\u2016 = 1, \u2016P>1f\u2217\u2016L2 \u2192 \u00b5\u22172, where \u00b5\u22170, \u00b5\u22171, \u00b5\u22172 are bounded constants. Intuitively, \u00b5\u22170, \u00b5\u22171, and \u00b5\u22172 can be interpreted as the \u201cmagnitude\u201d of the constant, linear, and nonlinear components of f\u2217, respectively.\nA Spiked Model for W 1. When \u03b7 = \u0398(1) in (3.1), we show a BBP phase transition (named after Baik, Ben Arous, Pe\u0301che\u0301 [BAP05]) for the leading singular value of W 1, and quantify the alignment between the corresponding singular vector u1 and the linear component of target function \u03b2\u2217. It is worth noting that in our analysis, the signal \u03b2\u2217 is \u201chidden\u201d in the rank-one perturbation A defined in Proposition 2; thus our setting is different from the usual low-rank signal-plus-noise models (e.g. [BGN11, BGN12, Cap18]), and the alignment we aim to quantify |\u3008u1,\u03b2\u2217\u3009| does not directly follow from classical results on the BBP transition. Theorem 3. Given Assumption 1 and fixed \u03b7 = \u0398(1), we define \u00b5\u0304 = limd\u2192\u221e\u2016f\u2217\u2016L2(Rd,\u0393), and\n\u03b81 := \u221a \u00b5\u03042\u03c8\u221211 + \u00b5 \u22172 1 \u00b7 \u00b51\u03b7, \u03b82 := \u00b51\u00b5\u22171\u03b7. (3.4)\nThen the leading singular value s1(W 1) and the corresponding left singular vector u1 satisfy\ns1(W 1)\u2192\n\u221a (1 + \u03b821)(\u03c82 + \u03b8 2 1)\n\u03b821 , |\u3008u1,\u03b2\u2217\u3009|2 \u2192 \u03b822 \u03b821\n( 1\u2212 \u03c82 + \u03b8 2 1\n\u03b821(\u03b8 2 1 + 1)\n) ,\nif \u03b81 > \u03c8 1/4 2 ; otherwise, s1(W 1)\u2192 1 + \u221a \u03c82 and |\u3008u1,\u03b2\u2217\u3009| \u2192 0, in probability, as n,N, d\u2192\u221e.\nRemark. While the above proposition only describes the isolated singular value/vector, due to the almost rank-1 property of G0, one can easily verify that the limiting spectrum of first-layer weights, namely the \u201cbulk\u201d, remains unchanged after the gradient update, and for any fixed i > 1, |si(W 1)\u2212 si(W 0)| = od,P(1).\n3Heuristically speaking, the updated NN under \u03b7 = \u0398(1) remains close to the \u201ckernel regime\u201d, in the sense that each neuron does not travel far away from the initialization, i.e., as N \u2192\u221e, \u2223\u2223[W 1 \u2212W 0]ij\u2223\u2223 \u2223\u2223[W 0]ij\u2223\u2223 for all i, j with high probability.\nWe make the following observations. Beyond the threshold \u03b81 > \u03c8 1/4 2 , increasing the learning rate \u03b7 enlarges the leading singular value (spike) s1(W 1). As for the overlap, one can numerically verify |\u3008u1,\u03b2\u2217\u3009|2 is upper-bounded by \u03b842\u2212\u03c82 \u03b822(\u03b8 2 2+1) < 1 (obtained when \u03c81 = n/d \u2192 \u221e), from which we deduce that better alignment is achieved when we take a bigger step, or when the nonlinearity \u03c3 and target f\u2217 have larger linear components (i.e., larger \u00b51, \u00b5 \u2217 1).\nTheorem 3 is numerically verified in Figure 3. Observe that after one gradient step with \u03b7 = \u0398(1), the bulk of the spectrum of W remains unchanged and is given by the Marchenko-Pastur law (red), but a spike may appear (prediction from Theorem 3 is indicated by marker \u201c\u00d7\u201d) when \u03b7 exceeds a certain threshold; furthermore, the corresponding singular vector u1 aligns with the linear component \u03b2\u2217 of the target function, as shown in the subfigure (see also Figure 8(a)). We investigate the impact of this alignment on the performance of kernel ridge regression in Section 4.\nA Spiked Model for CK? While our result only characterizes the weight matrix W , it may also reveal interesting properties of the CK matrix. In particular, [HL20, Lemma 5] in combination with Lemma 14 imply that for odd activation \u03c3, the expected feature matrix (after one gradient step with \u03b7 = \u0398(1)) satisfies\u2225\u2225\u03a3\u03a6 \u2212\u03a3\u03a6\u2225\u2225 P\u2192 0, where \u03a3\u03a6 = Ex[\u03c3(W>1 x)\u03c3(x>W 1)], \u03a3\u03a6 = \u00b521W>1 W 1 + \u00b522I. Consequently, Theorem 3 implies the same BBP transition for \u03a3\u03a6. When the population \u03a3\u03a6 contains a spike, it is natural to expect the empirical CK matrix to exhibit a similar transition, which we conjecture that the Gaussian equivalence property (see Section 4.1) can precisely capture.\nConjecture 4. Assume \u03c3 is an odd function4 in addition to Assumption 1, and \u03b7 = \u0398(1). Given new\ntraining data/labels X\u0303, y\u0303 (independent of W 1), define \u03a6 = 1\u221a N \u03c3(X\u0303W 1), \u03a6\u0304 = 1\u221a N\n( \u00b51X\u0303W 1 + \u00b52Z ) , where\n[Z]i,j i.i.d.\u223c N (0, 1), and denote the left leading singular vectors of \u03a6, \u03a6\u0304 as u1, u\u03041, respectively. We conjecture\u2223\u2223si(\u03a6)\u2212 si(\u03a6\u0304)\u2223\u2223 = od,P(1), \u2200i \u2208 [n]; |\u3008u1, y\u0303/\u2016y\u0303\u2016\u3009|2 = |\u3008u\u03041, y\u0303/\u2016y\u0303\u2016\u3009|2 + od,P(1).\nThe conjecture predicts both the eigenvalues of the CK matrix and the overlap between its spike eigenvector and the training labels. In Figure 4 we plot the eigenvalue histogram of the CK matrix after one gradient step with \u03b7 = \u0398(1), which we denote as CK1 = \u03a6\u03a6\n>. Observe that the bulk of the spectrum remains unchanged compared to CK0, which can be analytically computed (red). On the other hand, similar to W 1, an isolated eigenvalue (spike) appears in CK1, the location of which can be predicted by the Gaussian equivalent model in Conjecture 4 (marker \u201c\u00d7\u201d).\nFurthermore, in our student-teacher setting, we observe that the isolated eigenvector (top principal component PC) of CK1 correlates with the training labels y\u0303 \u2013 this is also captured by the Gaussian equivalent model, as shown in the subfigure of Figure 4. This demonstrates that the alignment phenomenon reported in [FW20, Figure 3] already occurs after one gradient step. We note that similar alignment between the training labels and the principal components of the (trained) NTK has also been empirically observed [CHS20, OJMDF21], and it is argued that such overlap may improve optimization or generalization.\n4The odd activation \u03c3 ensures that the initialized CK0 does not contain \u201cuniformative\u201d spikes \u2013 see [BP22]."
        },
        {
            "heading": "4 Do the Learned Features Improve Generalization?",
            "text": "Thus far we have shown that after one gradient step, the first-layer weights align with the linear component of the teacher model. Intuitively, since the learned feature map x \u2192 \u03c3(W>1 x) \u201cadapts\u201d to the teacher f\u2217, we may expect the ridge regression estimator on the trained CK to achieve better performance. In this section we confirm this intuition in a concrete example: we consider the setting where f\u2217 is a single-index model, and compare the CK prediction risk before and after one gradient descent step on W .\nAssumption 2 (Single-index/one-neuron Teacher). f\u2217(x) = \u03c3\u2217(\u3008x,\u03b2\u2217\u3009), where \u03b2\u2217 \u2208 Rd is a deterministic signal with \u2016\u03b2\u2217\u2016 = 1, and \u03c3\u2217 is Lipschitz with \u00b5\u22170 = 0, \u00b5\u22171 6= 0 defined in (3.3).\nRemark. The single-index setting has been extensively studied in the proportional regime [GLK+20, DL20, HL20], and it is an instance of the \u201chidden manifold model\u201d [GMKZ20]. However, most prior works only considered training the coefficients a on top of fixed feature map (e.g., defined by randomly initialized W 0), and such RF models cannot learn a single-index f\u2217 efficiently in high dimensions [YS19].\nAs stated in Section 2.3, the RF ridge estimator defined by the two-layer NN (1.1) has \u2126(1) prediction risk unless \u03c3\u2217 is a linear function. Here our goal is to demonstrate that the trained CK model can outperform the initial RF and potentially the kernel lower bound (2.4). We first introduce the Gaussian equivalence property which will be useful in the computation of prediction risk."
        },
        {
            "heading": "4.1 The Gaussian Equivalence Property",
            "text": "The Gaussian equivalence theorem (GET) implies that the prediction risk of a nonlinear kernel model can be the same as that of a noisy linear model. Specifically, recall the prediction risk of the ridge estimator:\nRF(\u03bb) = Ex ( \u3008\u03c6F(x), a\u0302\u03bb\u3009 \u2212 f\u2217(x) )2 , a\u0302\u03bb = argmina { 1 n n\u2211 i=1 (yi \u2212 \u3008\u03c6F(xi),a\u3009)2 + \u03bb N \u2016a\u20162 } , (4.1)\nwhere F \u2208 {CK,GE} indicates the choice of feature map, which is either the nonlinear CK feature \u03c6CK(x) = 1\u221a N \u03c3(W>x), or the Gaussian equivalent (GE) feature \u03c6GE(x) = 1\u221a N ( \u00b51W >x+ \u00b52z )\nwhere z \u223c N (0, I) independent of x, W . In the following, we take W to be the updated weights after one or more GD steps.\nThe Gaussian equivalence refers to the universality phenomenonRCK(\u03bb) \u2248 RGE(\u03bb). For RF models (2.2), the GET has been rigorously proved in [HL20, MS22]. Furthermore, [GLR+21, LGC+21] provided empirical evidence that such equivalence holds in much more general feature maps, including the representation of certain pretrained NNs (e.g., see [LGC+21, Figure 4]). Since our setting goes beyond RF model and cannot be covered by prior results, we first establish the GET for our trained feature map under small learning rate.\nTheorem 5. Given Assumptions 1, 2, and in addition assume the activation \u03c3 is an odd function. If the learning of Wt in (2.1) and estimation of a\u0302\u03bb in (4.1) are performed on independent training data X and X\u0303, respectively, then for any fixed t \u2208 N, the GET holds after the first-layer weights are optimized for t gradient steps with learning rate \u03b7 = \u0398(1); that is, for trained CK feature \u03c6CK(x) = 1\u221a N \u03c3(W>t x) and \u03bb > 0,\n|RCK(\u03bb)\u2212RGE(\u03bb)| = od,P(1). (4.2)\nThis is to say, for learning rate \u03b7 = \u0398(1), the Gaussian equivalent model provides an accurate description of the prediction risk of ridge regression (on the trained CK) at any fixed time step t, although most of our analysis deals with t = 1. The important observation is that even though the trained weights W t are no longer i.i.d., the Gaussian equivalence property can still hold when W t \u2212W 0 remains \u201csmall\u201d (in some norm, see (C.3) for details), which entails that the neurons remain nearly orthogonal to one another.\nImplications of Gaussian Equivalence. Under the GET, we can equivalently compute RGE(\u03bb), the prediction risk of ridge regression on noisy Gaussian features \u03c6GE, which can be characterized using standard tools such as the Gaussian comparison inequalities [Gor88, TOH15]. Theorem 5 is empirically validated in Figure 5(c), in which we run gradient descent on W with small learning rate for 50 steps, and compute the\nprediction risk of the ridge regression estimator on the CK at each step; observe that empirical values match the analytic predictions5 in the early phase of training. We however emphasize that Theorem 5 does not allow for the number of training steps t to grow with the training set size n; in fact, in Appendix A.1 we empirically observe that the GET may fail if we train the first-layer weights longer.\nOn the other hand, the GET also implies that the kernel estimator is essentially \u201clinear\u201d in high dimensions. For the squared loss, it is straightforward to verify that the Gaussian equivalent model cannot learn the nonlinear component of the target function P>1f \u2217 as follows.\nFact 6. Under the same assumptions as Theorem 5, RGE(\u03bb) \u2265 \u2016P>1f\u2217\u20162L2 for any \u03c81, \u03c82 and \u03bb > 0.\nHence, when \u03b7 = \u0398(1), even though training the first-layer W for just one step leads to non-trivial improvement over the initial RF ridge estimator (which we precisely quantify in Section 4.2), the learned CK cannot outperform the best linear model on the input features. In other words, to (possibly) learn a nonlinear f\u2217, the trained feature map needs to violate the GET. In the case of one gradient step on W , this amounts to using a sufficiently large step size, which we analyze in Section 4.3."
        },
        {
            "heading": "4.2 \u03b7 = \u0398(1): Improvement Over the Initial CK",
            "text": "While the Gaussian equivalence property allows us to compute the asymptotic prediction risk after multiple gradient steps with \u03b7 = \u0398(1), the precise expressions can be opaque and not amenable to interpretation or quantitative characterization. Fortunately for the first gradient step, the risk calculation can be simplified by the rank-1 approximation of the gradient matrix G0 shown in Section 3.1. Therefore, in this subsection we focus on t = 1 and analyze how the trained features improves over the initialized RF. To quantify the discrepancy in the prediction risk (4.1), we write R0(\u03bb) as the prediction risk of the initialized RF ridge regression estimator (on the feature map x \u2192 \u03c3(W>0 x)), and R1(\u03bb) as the prediction risk of the ridge estimator on the new feature map x\u2192 \u03c3(W>1 x) after one feature learning step.\nImportantly, due to the alignment between the trained features and the teacher model f\u2217 demonstrated in Section 3, we cannot simply apply a rotation invariance argument (e.g., [MM22, Lemma 9.2]) to remove the dependency on the true parameters \u03b2\u2217 and reduce the prediction risk to trace of certain rational functions of the kernel matrix; in other words, knowing the spectrum (or the Stieltjes transform) of the CK is not sufficient. Instead, we utilize the GET and the almost rank-1 property of G0 in Proposition 2, which, in combination with techniques from operator-valued free probability theory [MS17], enables us to obtain the asymptotic expression of the difference in the prediction risk before and after one gradient step.\nTheorem 7. Under the same assumptions as Theorem 5 and \u03b7 = \u0398(1), we have\nR0(\u03bb)\u2212R1(\u03bb) P\u2192 \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) \u2265 0,\nwhere \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is defined by (C.28) in Appendix C.3. \u03b4 is a non-negative function of \u03b7, \u03bb, \u03c81, \u03c82 \u2208 (0,+\u221e) with parameters \u00b5\u22171, \u00b51, \u00b52, and it vanishes if and only if (at least) one of \u00b5\u22171, \u00b51 and \u03b7 is zero.\nRemark. Performance of the initial RF ridge estimator R0(\u03bb) has been characterized by many prior works (e.g., [GLK+20, MM22]); hence the precise asymptotics of \u03b4 provided in Theorem 7 allows us to explicitly compute the asymptotic prediction risk of the CK model after one feature learning step R1(\u03bb).\nTheorem 7 confirms our intuition that training the first-layer parameters improves the CK model, as shown in Figure 5(a)(b). Remarkably, this improvement (\u03b4 > 0) holds for any \u03c81, \u03c82 \u2208 (0,\u221e), that is, taking one gradient step (with learning rate \u03b7 = \u0398(1)) is always beneficial, even when the training set size n is small. Moreover, we do not require the student and teacher models to have the same nonlinearity \u2014 a non-vanishing decrease in the prediction risk of CK ridge regression is present as long as \u00b51, \u00b5 \u2217 1 6= 0. On the other hand, the GET (in particular Fact 6) also implies an upper bound on the possible improvement: \u03b4 \u2264 R0(\u03bb)\u2212 \u00b5\u221722 as n, d,N \u2192\u221e; this is to say, the trained CK remains in the \u201clinear\u201d regime.\nNow we consider the following special cases where the expression of \u03b4 can be further simplified.\n5In Figure 5(c), we compute certain quantities involved in the optimization problem (e.g., spectrum of W ) using finitedimensional matrices, following [LGC+21]; hence our analytic curves are not entirely \u201casymptotic\u201d.\nLarge Sample Limit. We first analyze the setting where the sample size n is larger than any constant times d, that is, we let n, d,N \u2192\u221e proportionally, and then take the limit \u03c81 \u2192\u221e. In this regime, since a large number of training data is used to compute the gradient for the first-layer parameters, we intuitively expect the benefit of feature learning to be more pronounced, and a larger step may be more beneficial.\nProposition 8. Consider the large sample regime: \u03c81 \u2192\u221e, \u03c82 \u2208 (0,\u221e). Under the same assumptions as Theorem 5 and \u03b7 = \u0398(1), lim\u03c81\u2192\u221e \u03b4(\u03b7, \u03bb, \u03c81, \u03c82), defined in (C.32), is (i) non-negative, (ii) vanishing if and only if one of \u00b5,\u00b5 \u2217 1, \u03b7 is zero, and (iii) increasing with respect to the learning rate \u03b7.\nProposition 8 predicts that the prediction risk R1(\u03bb) further decreases as we use a larger learning rate \u03b7, which is empirically verified in Figure 5(a). We note that the large learning rate setting (\u03b7 = \u0398( \u221a N)) in Section 4.3 cannot be covered by the above proposition by increasing \u03b7, as here \u03b7 does not scale with N .\nLarge Width Limit. We also address the highly overparameterized regime, i.e., \u03c82 \u2192 \u221e. In this limit, the initialized CK model approaches the kernel ridge regression estimator, the prediction risk of which is still lower bounded by \u2016P>1f\u2217\u20162L2 due to Proposition 1. The following proposition indicates that the advantage of one-step feature learning becomes negligible in this large width setting.\nProposition 9. Consider the large width regime: \u03c81 \u2208 (0,\u221e), \u03c82 \u2192\u221e. Then under the same assumptions as Theorem 5 and \u03b7 = \u0398(1), we have lim\u03c82\u2192\u221e \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = 0.\nProposition 9 agrees with Figure 5(b), where we see that the risk improvement is more prominent when the width N is not too large. One explanation is that as \u03c82 = N/d increases, the initial CK already achieves lower prediction risk (e.g., see [MM22, Figure 4]), so the benefit of feature learning becomes less significant.\n4.3 \u03b7 = \u0398( \u221a N): Improvement Over the Kernel Lower Bound Now we take one gradient step with large learning rate \u03b7 = \u0398( \u221a N), which matches the asymptotic order of the Frobenius norm of the gradient G0 and that of the initialized weight matrix W 0 as in (3.2). Note that after absorbing the prefactors, this learning rate scaling is analogous to the maximum update parameterization [YH20], which admits a feature learning limit; specifically, the change in each coordinate of the feature vector [\u03c3(W>x)]i is \u0398\u0303d,P(1), which has roughly the same magnitude as its value at initialization.\nDue to the large step size, the columns of the updated weight matrix W 1 are no longer near-orthogonal, which is an important property used in existing analyses of the Gaussian equivalence (e.g., see Proposition 22 or [HL20, Equation (66)]). Indeed, we will see that in this regime, the ridge regression estimator on the trained CK features is no longer \u201clinear\u201d and can potentially outperform the kernel lower bound (2.4) in the proportional limit. However, in the absence of GET, it is difficult to derive the precise asymptotics of the CK model. As an alternative, in this subsection we establish an upper bound on the prediction risk R1(\u03bb), which we then compare against the kernel ridge lower bound.\nExistence of \u201cGood\u201d Solution. Given the trained first-layer weights W 1, we first construct a secondlayer a\u0303 for which the prediction risk can be easily upper-bounded. For a pair of nonlinearities (\u03c3, \u03c3\u2217), we introduce a scalar quantity \u03c4\u2217 which is the optimum of the following minimization problem:\n\u03c4\u2217 := inf \u03ba\u2208R\nE\u03be1 [( \u03c3\u2217(\u03be1)\u2212 E\u03be2\u03c3(\u03ba\u03be1 + \u03be2) )2] , (4.3)\nwhere \u03be1, \u03be2 i.i.d.\u223c N (0, 1). We write \u03ba\u2217 as an optimal value at which \u03c4\u2217 is attained (when \u03c4\u2217 is not achieved by finite \u03ba, the same argument holds by introducing a small tolerance factor > 0 in \u03c4\u2217; see Appendix D.2). Roughly speaking, \u03c4\u2217 approximates the prediction risk of a specific student model which takes the form of an average over subset of neurons (after one feature learning step); in particular, the first term on the RHS of (4.3) containing \u03c3\u2217 corresponds to the teacher f\u2217, and the second term E\u03be2 represents the constructed student model. The following lemma shows that we can find some a\u0303 on the trained CK features whose prediction risk is approximately \u03c4\u2217, under the additional assumption that the activation function \u03c3 is bounded.\nLemma 10 (Informal). Given Assumptions 1 and 2, and assume further that \u03c3 is bounded. Then after one gradient step on W with \u03b7 = \u0398( \u221a N), there exists some second-layer coefficients a\u0303 such that the constructed student model f\u0303(x) = 1\u221a N a\u0303>\u03c3(W>1 x) achieves prediction risk \u201cclose\u201d to \u03c4 \u2217 when \u03c81 = n/d is large.\nIt is worth noting that the definition of \u03c4\u2217 does not involve the specific value of learning rate \u03b7. This is because for any choice of \u03b7 = \u0398( \u221a N), due to the Gaussian initialization of ai, we can find a subset of weights that receive a \u201cgood\u201d learning rate (with high probability) such that the corresponding neurons are useful in learning the teacher model. In addition, observe that \u03c4\u2217 is a simple Gaussian integral which can be numerically or analytically computed (see Appendix D.2 for some examples). For instance, when \u03c3 = \u03c3\u2217 = erf, one can easily verify that \u03ba\u2217 = \u221a 3 and \u03c4\u2217 = 0.\nPrediction Risk of Ridge Regression. Having established the existence of a \u201cgood\u201d student model f\u0303 that achieves prediction risk close to \u03c4\u2217 defined in (4.3), we can now prove an upper bound for the prediction risk of the ridge regression estimator on trained CK features R1(\u03bb) in terms of \u03c4\u2217. Theorem 11. Under the same assumptions as Lemma 10, after one gradient step on W with \u03b7 = \u0398( \u221a N), there exist constants C,\u03c8\u22171 > 0 such that for any n/d > \u03c8 \u2217 1 , the ridge regression estimator (4.1) satisfies\nR1(\u03bb) \u2264 10\u03c4\u2217 + C (\u221a \u03c4\u2217 \u00b7 \u221a d n + d n ) ,\nwith probability 1 as n, d,N \u2192\u221e, if we choose the ridge penalty: n\u03b5\u22121 < N\u22121\u03bb < n\u2212\u03b5 for some small \u03b5 > 0.\nWhile Theorem 11 does not provide exact expression of the prediction risk, the upper bound still allows us to compare the prediction risk of CK ridge regression before and after one large gradient step. In particular, if \u2016P>1f\u2217\u20162L2 \u2265 10\u03c4\u2217 (the constant 10 is not optimized), we know that the trained CK can outperform the kernel lower bound (2.4) (hence also the initialized CK) in the proportional limit, when the ratio \u03c81 = n/d is sufficiently large. The following corollary provides two examples of this separation (see Figure 6).\nCorollary 12. Under the same conditions as Theorem 11, there exists some constant \u03c8\u22171 such that for any \u03c81 > \u03c8 \u2217 1 , the following holds with probability 1 when n, d,N \u2192\u221e proportionally:\n\u2022 For \u03c3 = \u03c3\u2217 = erf, R1(\u03bb) = O(d/n), which vanishes if \u03c81 is large. \u2022 For \u03c3 = \u03c3\u2217 = tanh, we have R1(\u03bb) < \u2016P>1f\u2217\u20162L2 .\nIn the two examples outlined above, training the features by taking one large gradient step on the first-layer parameters can lead\nto substantial improvement in the performance of the CK model. In fact, the new ridge regression estimator may outperform a wide range of kernel models outlined in Section 2.3. However, we emphasize that this separation is only present in specific pairs of (\u03c3, \u03c3\u2217) for which \u03c4\u2217 is small enough. In general settings, learning a good representation likely requires more than one gradient step (even if f\u2217 is a simple single-index model)."
        },
        {
            "heading": "5 Discussion and Conclusion",
            "text": "We investigated how the conjugate kernel of a two-layer neural network (1.1) benefits from feature learning in an idealized student-teacher setting, where the first-layer parameters W are updated by one gradient descent step on the empirical risk. Based on the approximate low-rank property of the gradient matrix, we established a signal-plus-noise decomposition for the updated weight matrix W 1, and quantified the improvement in the prediction risk of conjugate kernel ridge regression under two different scalings of firststep learning rate \u03b7. To the best of our knowledge, this is the first work that rigorously characterizes the precise asymptotics of kernel models (defined by neural networks) in the presence of feature learning.\nWe outline a few limitations of our current analysis as well as future directions.\n\u2022 Dependence between W 1 and X. One of our crucial assumptions is that the trained weight matrix W 1 is independent of the data X\u0303 on which the CK is computed. While this does not cover the important scenario where feature learning and kernel evaluation are performed on the same data, our setting is very natural in the analysis of pretrained models or transfer learning, which would be an interesting extension. \u2022 Scaling of Learning Rate. Our findings in Section 4 illustrate that \u03b7= \u0398(1) and \u03b7= \u0398( \u221a N) result in\ndrastically different behavior. One natural question to ask is whether there exists a \u201cphase transition\u201d in between the two regimes (see Figure 6) that dictates whether the GET holds. Interestingly, [RGKZ21] showed that instead of breaking the near-orthogonality of weight matrix W (via large gradient step), one can also introduce sufficiently large low-rank shifts to the input X to enable the initial RF estimator to fit a nonlinear f\u2217. Intuitively, this may be due to the \u201cdual\u201d relation of X and W in the CK model.\n\u2022 Rigorous Analysis of CK Spike. In Section 3.2 we put forward a Gaussian equivalence hypothesis on the isolated eigenvalue/eigenvector of the trained CK matrix (see Figure 4); understanding whether and when such property holds is an important research direction."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank (in alphabetical order) Konstantin Donhauser, Zhou Fan, Hong Hu, Masaaki Imaizumi, Ryo Karakida, Bruno Loureiro, Yue M. Lu, Atsushi Nitanda, Sejun Park, Ji Xu, Yiqiao Zhong for discussions and feedback on the manuscript.\nJB was supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program and Amazon Research Award. MAE was supported by NSERC Grant [2019-06167], Connaught New Researcher Award, CIFAR AI Chairs program, and CIFAR AI Catalyst grant. TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST. ZW was supported by NSF Grant DMS-2055340. Part of this work was completed when DW interned at Microsoft Research (hosted by GY)."
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2"
        },
        {
            "heading": "2 Problem Setup and Basic Assumptions 3",
            "text": "2.1 Training Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Main Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3 Lower Bound for Kernel Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
        },
        {
            "heading": "3 How Does One Gradient Step Change the Weights? 5",
            "text": "3.1 Almost Rank-1 Property of the Gradient Matrix . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.2 Alignment with the Target Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
        },
        {
            "heading": "4 Do the Learned Features Improve Generalization? 8",
            "text": "4.1 The Gaussian Equivalence Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n4.2 \u03b7 = \u0398(1): Improvement Over the Initial CK . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.3 \u03b7 = \u0398( \u221a N): Improvement Over the Kernel Lower Bound . . . . . . . . . . . . . . . . . . . . 10"
        },
        {
            "heading": "5 Discussion and Conclusion 12",
            "text": ""
        },
        {
            "heading": "A Background and Additional Results 20",
            "text": "A.1 Additional Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nA.2 Additional Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.3 Linearity of Kernel Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
        },
        {
            "heading": "B Proof for the Weight Matrix 23",
            "text": "B.1 Norm Control of Gradient Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.2 Calculation of Alignment with Target Function . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
        },
        {
            "heading": "C Proof for Small Learning Rate (\u03b7 = \u0398(1)) 39",
            "text": "C.1 Gaussian Equivalence for Trained Feature Map . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nC.2 Prediction Risk of the Gaussian Equivalent Model . . . . . . . . . . . . . . . . . . . . . . . . 43\nC.3 Precise Characterization of Prediction Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nC.4 Analysis of Special Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nD Proof for Large Learning Rate (\u03b7 = \u0398( \u221a N)) 59\nD.1 Refined Properties of the First-step Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nD.2 Constructing the \u201cOracle\u201d Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nD.3 Prediction Risk of Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67"
        },
        {
            "heading": "A Background and Additional Results",
            "text": ""
        },
        {
            "heading": "A.1 Additional Experiments",
            "text": "Failure Cases of GET. It is worth noting that Theorem 5 does not apply to the setting where t scales with n, d,N . Because of our mean-field parameterization, the first-layer weight W needs to travel sufficiently far away from initialization to achieve small training loss (see Figure 2). Hence in our experimental simulations (where n, d,N are large but finite), as the number of steps t or learning rate \u03b7 increases, we expect the Gaussian equivalence predictions to become inaccurate at some point. This transition is empirically demonstrated in Figure 7(a). Observe that for larger t, the GET predictions overestimate the test loss; one possible explanation is that the trained kernel can learn nonlinear functions (which we show in Section 4.3 for one gradient step with \u03b7 = \u0398( \u221a N) and specific choices of f\u2217), which the GET cannot capture.\nWe provide additional empirical evidence on this explanation in Figure 7(b). To track the learning of the linear and nonlinear components of f\u2217, we recall the orthogonal decomposition:\nf\u2217(x) = \u00b5\u22170 + \u00b5 \u2217 1\u3008x,\u03b2\u2217\u3009\ufe38 \ufe37\ufe37 \ufe38\nf\u2217L(x)\n+P>1f \u2217(x)\ufe38 \ufe37\ufe37 \ufe38\nf\u2217NL(x)\n.\nDenote the CK ridge regression estimator on the feature map after t gradient steps x\u2192 \u03c3(W>t x) as f t\u03bb. We estimate the following alignment quantities (we normalize f\u2217L and f \u2217 NL to have unit L 2-norm):\nLinear component: \u2329 f\u2217L, f t \u03bb \u232a L2(Rd,\u0393). Nonlinear component: \u2329 f\u2217NL, f t \u03bb \u232a L2(Rd,\u0393). (A.1)\nIn Figure 7(b), we observe that the student model f t\u03bb first aligns with the linear component of the teacher model f\u2217L; on the other hand, when the student model begins to learn the nonlinear component f \u2217 NL (at \u223c30 gradient steps), the Gaussian equivalent predictions (Figure 7(a)) overestimate the prediction risk.\nSingular Vector Alignment (Theorem 3). In Figure 8(a), we compute the overlap between the leading eigenvector of W 1 and the linear component of the teacher model \u03b2\u2217. Observe that the empirical simulations (dots) closely match the analytic predictions of Theorem 3 (solid curves). Also, note that increasing the learning rate \u03b7 or the sample size \u03c81 = n/d both lead to greater alignment with the teacher model.\nLarge Learning Rate (SoftPlus). In Figure 8(b) we repeat the large learning rate experiment in Section 4.3 for a different nonlinearity \u03c3 = \u03c3\u2217 = SoftPlus, for which \u03c4\u2217 \u2248 0.03 > 0, and hence the upper bound in Theorem 11 is non-vanishing. In this case, we observe that the prediction risk of the CK ridge regression\nmodel (after one feature learning step) is also non-vanishing even when the step size is large; this indicates that although we do not provide precise asymptotic characterization in Theorem 11, the upper-bounding quantity \u03c4\u2217 in (4.3) has predictive power on the actual prediction risk.\nKernel Target Alignment. In Section 3.2, we observed that the trained CK aligns with training labels. Here we provide additional empirical evidence by tracking the Kernel Target Alignment (KTA) [CSTEK01] between the CK and training labels during training. Specifically, we compute the following quantity at each gradient step t, which takes value between 0 and 1,\nKTA = \u3008CKt,yy>\u3009 \u2016CKt\u2016F \u2016y\u2016 2 , (A.2)\nwhere CKt denotes the CK matrix defined by W t. Figure 8(c) shows the KTA for two-layer NN under our mean-field parameterization and also the NTK parameterization (which omits the 1\u221a\nN -prefactor in (1.1)).\nWe optimize the first-layer weights W until the training loss reaches 10\u22122 for both settings, and compute the KTA on the training and test data at gradient step. Observe that the trained CK in the mean-field model aligns with both the training and test labels (purple), whereas the NN in the kernel regime does not exhibit such alignment (orange)."
        },
        {
            "heading": "A.2 Additional Related Works",
            "text": "The Kernel Regime and Beyond. The neural tangent kernel (NTK) [JGH18] describes the learning dynamics of wide neural network under specific parameter scaling. Such description is based on linearizing the NN around its initialization, and the limiting kernel can be computed for various architectures [ADH+19, Yan20]. Thanks to strong convexity of the kernel objective, global convergence rate guarantees of gradient descent can be established [DZPS19, JT20]. As mentioned in Section 1.2, this first-order Taylor expansion fails to explain the adaptivity of NNs; therefore, recent works also analyzed higher-order approximations of the training dynamics [DGA20, HY20]. Noticeably, a quadratic model (i.e., second-order approximation) can outperform kernel (NTK) estimators in certain settings [AZLL19, BL20].\nIn contrast to the aforementioned local approximations (via Taylor expansion and truncation), the meanfield regime (e.g., [NS17, MMN18, CB18]) deals with a different scaling limit under which the evolution of parameters can be described by some partial differential equation (for comparison between regimes see [WGL+20, GSJW20]). While the mean-field limit can capture the presence of feature learning [CB20, Ngu21], quantitative guarantees often require additional conditions such as KL regularization [NWS22, Chi22]. Note\nthat our parameterization (1.1) mirrors the mean-field scaling, but we circumvent the difficulty of analyzing the nonlinear PDE because only the \u201cearly phase\u201d (one gradient step) is considered.\nFinally, we highlight two concurrent papers that studied the mean-field dynamics of two-layer NNs (under one-pass SGD) in the high-dimensional asymptotic regime, and showed learnability results for certain target functions. [ABAM22] established a separation between NNs and kernel methods in learning \u201cstaircase-like\u201d functions on hypercube; [VSL+22] analyzed how the model width and step size impact the learning of a well-specified two-layer NN teacher model.\nSpectrum of Kernel Random Matrices. Kernel matrices in the proportional regime was first analyzed by [EK10] through Taylor expansion, and later their limiting spectra were fully described by [CS13, DV13, FM19]. As an extension of kernel random matrices, the CK matrix has also been studied in [PW17, Pe\u0301c19, BP21, BP22] and [LLC18, FW20, WZ21], using the moment method and the Stieltjes transform method, respectively. In addition, the spectrum and concentration behavior of the NTK matrix were elaborated in [MZ20, FW20, WZ21]. We remark that based on these prior results on the NTK of two-layer NNs, one can check our large learning rate \u03b7 = \u0398( \u221a N) satisfies \u221a N\u03b7 \u00b7 \u03bbmax(F ) = \u0398d,P(1), where F is the Fisher information matrix ; heuristically speaking, this means that the chosen step size is not unreasonably large (under first-order approximation of the landscape)."
        },
        {
            "heading": "A.3 Linearity of Kernel Ridge Regression",
            "text": "As previously mentioned, our kernel ridge regression lower bound (Proposition 1) is a simple combination of existing results, which we briefly outline below.\nLinear Regression on Input. We first discuss the prediction risk of the ridge regression estimator on the input features. Recall that under Assumptions 1 and 2, we may write: f\u2217(x) = \u00b5\u22171\u3008x,\u03b2\u2217\u3009 + P>1f\u2217(x). Given the ridge regression estimator on the input features: \u03b8\u0302Lin , (X >X + \u03bbnId) \u22121X>y, we have the following bias-variance decomposition,\nRLin(\u03bb) =Ex ( f\u2217(x)\u2212 x>(X>X + \u03bbnId)\u22121X>f\u2217(X) )2 \ufe38 \ufe37\ufe37 \ufe38\nBias\n+\u03c32\u03b5 Tr ( (X>X + \u03bbnId) \u22122X>X ) \ufe38 \ufe37\ufe37 \ufe38\nVariance\n+od,P(1).\nFollowing a similar computation as [BMR21, Theorem 4.13] and using the asymptotic formulae in [DW18, WX20], we can derive the following expression,\nRLin(\u03bb) P\u2192 m\u0304 \u2032(\u2212\u03bb) m\u03042(\u2212\u03bb) \u00b7 \u00b5 \u22172 1 (1 + m\u0304(\u2212\u03bb))2 + (\u03c32\u03b5 + \u00b5 \u22172 2 ) \u00b7 ( m\u0304\u2032(\u2212\u03bb) m\u03042(\u2212\u03bb) \u2212 1 ) + \u00b5\u221722 , (A.3)\nwhere m\u0304(\u2212\u03bb) > 0 is the Stieltjes transform of the limiting eigenvalue distribution of 1nXX >. Observe that RLin(\u03bb) \u2265 \u00b5\u221722 . In addition, as shown in [DW18, WX20], the optimal ridge regularization and the corresponding prediction risk can be written as\n\u03bbopt = \u03c32\u03b5 + \u00b5 \u22172 2\n\u03c81\u00b5\u221721 , RLin(\u03bbopt)\nP\u2192 \u03c3 2 \u03b5 + \u00b5 \u22172 2\n\u03bboptm\u0304(\u2212\u03bbopt) \u2212 \u03c32\u03b5 . (A.4)\nLower Bound for RF/Kernel Ridge Regression. First note that for RF models (2.2), the lower bound \u00b5\u221722 is directly implied by the GET [HL20] under Assumptions 1 and 2 (see Fact 6). For inner-product kernels 6 in (2.3), if g : R\u2192 R is a smooth function in a neighborhood of 0, then the same lower bound can be obtained from [BMR21, Theorem 4.13] (observe that the bias term is lower bounded by \u2016P>1f\u2217\u20162L2). Finally, for the (first-layer) NTK, the kernel ridge regression estimator is given as\nf\u0302NTK(x) =g >(K + \u03bbI)\u22121y,\nwhere gi = 1\nNd N\u2211 k=1 \u3008x,xi\u3009\u03c3\u2032(\u3008x,wk\u3009)\u03c3\u2032(\u3008xi,wk\u3009),\n6Similar result can also be shown for Euclidean distance kernels following the analysis in [EK10, Theorem 2.2].\nand Kij = 1\nNd N\u2211 k=1 \u3008xi,xj\u3009\u03c3\u2032(\u3008xi,wk\u3009)\u03c3\u2032(\u3008xj ,wk\u3009).\nDefine the orthogonal decomposition \u03c3\u2032(z) = b0 + \u03c3 \u2032 \u22a5(z), where b0 = \u00b51 = E[\u03c3\u2032(z)], b21 = E[\u03c3\u2032(z)2]\u2212 b20, for z \u223c N (0, 1). Similar to [AP20, MZ20], we make the following \u201clineaized\u201d substitutions:\ng \u2248 g\u0304 , 1 d \u00b7 b20Xx, K \u2248 K\u0304 , 1 d \u00b7 b20XX > + b21I.\nThe error of this linear approximation has been studied in [MZ20, Lemma B.8] and [WZ21, Theorem 2.7], which, together with [BMR21, Theorem 4.13], entail the following equivalence under Assumption 1,\nRNTK(\u03bb) = RLin ( \u03bb+ b21 b20\u03c81 ) + od,P(1),\nwhere RLin is the prediction risk of the ridge regression estimator on the input features defined in (A.3). Hence, the linear lower bound (2.4) directly applies; in fact, the prediction risk is lower-bounded by the optimal ridge regression estimator on the input (A.4).\nKernel Lower Bound under Polynomial Scaling. For high-dimensional input x uniform on sphere or hypercube, [GMMM21, MMM21] showed that RF and kernel ridge estimators can learn at most a degree-k polynomial when n = O ( dk+1\u2212\u03b5 ) ; for the proportional scaling, this implies our lower bound \u2016P>1f\u2217\u20162L2 (but under different input assumptions). [DWY21] provided a similar result for more general data distributions and a class of rotation invariant kernels based on power series expansion, but the dependence on k is not sharp enough to cover the linear lower bound in Proposition 1."
        },
        {
            "heading": "B Proof for the Weight Matrix",
            "text": ""
        },
        {
            "heading": "B.1 Norm Control of Gradient Matrix",
            "text": "In this section we establish a few important properties of the gradient matrix defined in (2.1). For simplicity, we derive the results for the squared loss, but one may check that the same characterization holds for any differentiable loss function ` with Lipschitz derivative (w.r.t. both arguments), such as the logistic loss, for which the gradient update on the first layer at step t is given by\nW t+1 \u2212W t = \u2212\u03b7 \u221a N \u00b7 n\u2211 i=1 1 n \u22022` ( yi, \u03c3(x >W t) a\u221a N ) \u00b7 xi ( \u03c3\u2032(x>i W t) a>\u221a N ) , (B.1)\nwhere \u22022 refers to the partial derivative w.r.t. the second argument in `. In the following, for any t \u2208 N and i \u2208 [N ], we will always use wti with both subscript and superscript to indicate the i-th column of the weight matrix W t at time step t.\nFor our later analysis, a key quantity to control is the entry-wise 2-\u221e matrix norm defined as\n\u2016M\u20162,\u221e := max 1\u2264i\u2264N \u2016mi\u2016,\nfor any matrix M \u2208 Rd\u00d7N with the i-th column mi \u2208 Rd and 1 \u2264 i \u2264 N . It is straightforward to verify that\n\u2016M\u20162,\u221e \u2264 \u2016M\u2016 \u2264 \u2016M\u2016F \u2264 \u221a N\u2016M\u20162,\u221e.\nIn addition, for the Hadamard product with rank-1 matrix, we have the following property.\nFact 13. For m \u2208 Rm,n \u2208 Rn,M \u2208 Rm\u00d7n, we can write mn> M = diag(m)M diag(n), and\u2225\u2225mn> M\u2225\u2225 \u2264 \u2016diag(m)\u2016 \u00b7 \u2016M\u2016 \u00b7 \u2016diag(n)\u2016 = \u2016m\u2016\u221e\u2016M\u2016\u2016n\u2016\u221e."
        },
        {
            "heading": "B.1.1 Norm Bounds for the First Gradient Step",
            "text": "We begin with the first gradient step. Recall the definition of the gradient matrix under the squared loss (we omit the learning rate \u03b7 and prefactor \u221a N):\nG0 = \u2212 1 n X> [( 1\u221a N ( 1\u221a N \u03c3(XW 0)a\u2212 y ) a> ) \u03c3\u2032(XW 0) ] (B.2)\n= 1 n \u00b7 \u00b51\u221a N X>ya>\ufe38 \ufe37\ufe37 \ufe38 A + 1 n \u00b7 1\u221a N X>\n( ya> \u03c3\u2032\u22a5(XW 0) ) \ufe38 \ufe37\ufe37 \ufe38\nB\n\u2212 1 n \u00b7 1 N X>\n( \u03c3(XW 0)aa > \u03c3\u2032(XW 0) )\n\ufe38 \ufe37\ufe37 \ufe38 C ,\nwhere we utilized the orthogonal decomposition: \u03c3\u2032(z) = \u00b51 + \u03c3 \u2032 \u22a5(z). Due to Stein\u2019s lemma, we know that E[z\u03c3(z)] = E[\u03c3\u2032(z)] = \u00b51, and hence E[\u03c3\u2032\u22a5(z)] = 0 for z \u223c N (0, 1). The following lemma provides norm control for the above decomposition.\nLemma 14. Assume that f\u2217 \u2208 L2(Rd,\u0393), and both f\u2217 and \u03c3 are Lipschitz functions. Then (i) E\u2016A\u20162,\u221e \u2264 E\u2016A\u2016 \u2264 E\u2016A\u2016F \u2264 C \u221a d nN + 1 N ,\n(iii) E\u2016C\u2016 \u2264 E\u2016C\u2016F \u2264 C N \u221a 1 + dn .\nFurthermore, we have the following probability bounds. (i) P ( \u2016A\u2016F \u2265 C (\u221a d nN + \u221a 1 N )) \u2264 C \u2032 ( e\u2212cn + e\u2212cN ) ,\nP ( \u2016A\u2016F \u2264 C \u221a d nN ) \u2264 C \u2032 ( e \u2212cmin { nd2 (n2+d2) , ndn+d } + e\u2212cN + e\u2212cn ) , and\nP ( \u2016A\u20162,\u221e \u2265 C ( \u221a n+ \u221a d) logn\nN \u221a n\n) \u2264 C \u2032 ( e\u2212c ( \u221a n+ \u221a d)2 n log 2 n + e\u2212cn +Ne\u2212c log 2 n ) .\n(ii) P ( \u2016B\u2016 \u2265 C ( \u221a n+ \u221a d)( \u221a n+ \u221a N) log2 n\nn \u221a Nd\n) \u2264 C \u2032 ( (n+N)e\u2212c log 2 n + e\u2212( \u221a n+ \u221a d)2 + e\u2212cN + e\u2212cd ) ,\nP ( \u2016B\u2016F \u2265 C \u221a n+ \u221a d\u221a\nnN\n) \u2264 C \u2032 ( e\u2212cn + e\u2212cN + e\u2212c( \u221a n+ \u221a d)2 ) .\n(iii) P ( \u2016C\u2016F \u2265 C ( \u221a d+ \u221a n) logn logN\u221a nN ) \u2264 C \u2032 ( Ne\u2212cN + ne\u2212cd + ne\u2212c log 2 n +Ne\u2212c log 2N ) .\nHere all constants C,C \u2032, c > 0 only depend on \u03bb\u03c3, \u00b51, \u03c3\u03b5 and \u2016f\u2217\u2016L2(Rd,\u0393).\nRemark. In Lemma 14, we do not use the proportional scaling in Assumption 1 to simplify the expressions. This is because the dependence on n, d,N needs to be tracked separately in some of our calculations.\nProof. We analyze the three matrices of interest separately.\nPart (i). We first upper-bound \u2016A\u20162F . Notice that\nn \u221a N\n\u00b51 \u2016A\u2016F \u2264\u2016X>f\u2217(X)a>\u2016F + \u2016X>\u03b5a>\u2016F\n\u2264\u2016X\u2016(\u2016f\u2217(X)\u2016+ \u2016\u03b5\u2016)\u2016a\u2016. (B.3)\nWe know that Gaussian random matrices and vectors satisfy\nE\u2016\u03b5\u20162 = \u03c32\u03b5n, E\u2016f\u2217(X)\u20162 = n\u2016f\u2217\u20162L2(Rd,\u0393), (B.4)\nE\u2016a\u20162 = 1, E\u2016X\u20162 \u2264 C0(n+ d), (B.5)\nwhere the last inequality is from [Ver18, Exercise 4.6.2]. Based on Cauchy-Schwarz inequality, we can employ (B.4) and (B.5) to obtain\nE\u2016A\u20162,\u221e \u2264 E\u2016A\u2016 \u2264 E\u2016A\u2016F \u2264 C1\n\u221a d\nnN +\n1\nN ,\nwhere constant C1 > 0 only depends on \u00b51, \u03c3\u03b5 and \u2016f\u2217\u2016L2(Rd,\u0393). As for the probability bound, we use the Lipschitz concentration property (e.g., see [Ver18, Theorem 5.2.2]) of \u2016a\u2016, \u2016\u03b5\u2016 and \u2016f\u2217(X)\u2016, and apply [Ver18, Corollary 7.3.3] for \u2016X\u2016 to obtain\nP ( \u2016\u03b5\u2016 \u2265 \u03c3\u03b5 \u221a n ) \u2264 2e\u2212cn, P ( |\u2016a\u2016 \u2212 1| \u2265 1\n2\n) \u22642e\u2212cN , (B.6)\nP (\u2223\u2223\u2016f\u2217(X)\u2016 \u2212 \u2016f\u2217\u2016L2(Rd,\u0393)\u221an\u2223\u2223 \u2265 12\u2016f\u2217\u2016L2(Rd,\u0393)\u221an ) \u22642e\u2212cn, (B.7)\nP ( \u2016X\u2016 \u2265 \u221a n+ \u221a d+ t ) \u22642e\u2212ct 2 , (B.8)\nfor any t \u2265 0. Hence, from (B.3), we arrive at\nP ( \u2016A\u2016F \u2265 \u221a d\nnN +\n\u221a 1\nN + t\n) \u2264 4 ( e\u2212cn + e\u2212cN + e\u2212ct 2nN ) .\nNote that the same probability bounds also applies to \u2016A\u2016 and \u2016A\u20162,\u221e. Thus, we may take t = \u221a 1 N to obtain the desired result. Now we provide lower bounds for \u03b5>XX>\u03b5 and f\u2217(X)>XX>\u03b5. First, we define events A1, A2 and A3 by\nA1 := {\u2223\u2223\u2223Tr(XX>)\u2212 nd\u2223\u2223\u2223 \u2264 nd\n2\n} , A2 := { \u2016X\u2016 \u2264 \u221a d+ 2 \u221a n } ,\nA3 := { \u2016f\u2217(X)\u2016 \u2264 1\n2 \u2016f\u2217\u2016L2(Rd,\u0393)\n\u221a n } .\nWe know that P(A1),P(A2),P(A3) \u2265 1\u2212 2e\u2212cn by Bernstein\u2019s inequality, the Lipschitz Gaussian concentration inequality, and [Ver18, Corollary 7.3.3]. Condition on A1 \u2229 A2, by the Hanson-Wright inequality,\nP ( \u03b5>XX>\u03b5 \u2264 \u03c3 2 \u03b5\n2 nd\u2212 t \u2223\u2223\u2223 A1 \u2229 A2) \u2264 2e\u2212cmin{ t2n(n2+d2) , tn+d}. Choosing t = \u03c32\u03b5nd/4, we have\nP ( \u03b5>XX>\u03b5 \u2264 \u03c3 2 \u03b5\n8 nd\n) \u2264 2e\u2212cmin { nd2 (n2+d2) , ndn+d } + 4e\u2212cn. (B.9)\nSimilarly, by the general Hoeffding inequality, one can easily see that P (\u2223\u2223\u2223f\u2217(X)>XX>\u03b5\u2223\u2223\u2223 \u2265 t \u2223\u2223\u2223 A2 \u2229 A3) \u2264 2e\u2212 ct2n(n2+d2) .\nThus, again, by (B.6), we obtain P (\u2223\u2223\u2223f\u2217(X)>XX>\u03b5\u2223\u2223\u2223 \u2265 \u03c32\u03b5\n32 nd\n) \u2264 2e\u2212 cnd2 (n2+d2) + 2e\u2212cN + 4d\u2212cn. (B.10)\nAlso, since the operator norm has the following lower bound \u2016X>ya>\u2016 = \u2016a\u2016\u2016X>y\u2016 = \u2016a\u2016 ( y>XX>y )1/2 \u2265 \u2016a\u2016 ( \u03b5>XX>\u03b5+ 2f\u2217(X)>XX>\u03b5 )1/2 ,\nby (B.6), (B.9) and (B.10), we arrive at P ( n2N\n\u00b521 \u2016A\u20162 \u2264 \u03c3\n2 \u03b5\n16 nd\n) \u2264 16 ( e \u2212cmin { nd2 (n2+d2) , ndn+d } + e\u2212cN + e\u2212cn ) .\nAs for the last inequality on \u2016A\u20162,\u221e, by definition we know that\n\u2016A\u20162,\u221e \u2264 \u00b51\nn \u221a N \u2016X\u2016(\u2016f\u2217(X)\u2016+ \u2016\u03b5\u2016)\u2016a\u2016\u221e.\nThe desired result can be obtained from the tail bound on the sup-norm of Gaussian random vector, P ( \u2016a\u2016\u221e \u2264 t/ \u221a N ) \u2265 1\u2212 2Ne\u2212ct2 , in combination with (B.6), (B.7) and (B.8).\nPart (ii). As a result of Fact 13, we have\n\u2016B\u2016 \u2264 1 n \u221a N \u2016X\u2016\u2016a\u2016\u221e(\u2016f\u2217(X)\u2016\u221e + \u2016\u03b5\u2016\u221e)\u2016\u03c3\u2032\u22a5(XW 0)\u2016. (B.11)\nWe first control the operator norm of the random feature matrix \u03c3\u2032\u22a5(XW 0). Since \u03c3 \u2032 \u22a5 is centered,\n[FW20, Lemma D.4] implies that P ( \u2016\u03c3\u2032\u22a5(XW 0)\u2016 \u2265 C( \u221a n+ \u221a N)\u03bb\u03c3B,AB ) \u2264 2e\u2212cN ,\nwhere event AB is defined by\nAB := { \u2016W 0\u2016 \u2264 B,\nN\u2211 i=1 (\u2016w0i \u20162 \u2212 1)2 \u2264 B2 } ,\ngiven any constant B > 0. Hence, we have P ( \u2016\u03c3\u2032\u22a5(XW 0)\u2016 \u2265 C( \u221a n+ \u221a N)\u03bb\u03c3B ) \u2264 2e\u2212cN + P(AcB). (B.12)\nNext, we estimate the failure probability of event AcB . By Bernstein\u2019s inequality, for any t \u2265 0, we have\nP ( |\u2016w01\u20162 \u2212 1|2 \u2265 t2 ) \u2264 2e\u2212cdmin{t 2,t},\nwhere we write w01 as the first column of W 0 (and similarly for all w 0 i ). Following the proof of Proposition 3.3 in [FW20], we can obtain that\nP ( N\u2211 i=1 ( \u2016w0i \u20162 \u2212 1 )2 \u2265 4t2) \u2264 2eN log 5\u2212cdmin{t2,t}, (B.13) for any t \u2265 0. Besides, inequality (B.8) implies that for any t \u2265 0,\nP ( \u2016W 0\u2016 \u2264 c\u2032 \u221a N\nd\n) \u2265 1\u2212 2e\u2212cd.\nBy choosing t = c\u2032 \u221a\nN d in (B.13) and B := c\n\u2032 \u221a\nN d for sufficient large c \u2032 > 0, we can claim that there exists\nsufficient large constant c > 0 such that\nP(AcB) \u2264 2e\u2212cd + 2e\u2212cN .\nCombining (B.12) and the above inequality, we have\nP ( \u2016\u03c3\u2032\u22a5(XW 0)\u2016 \u2265 C( \u221a n+ \u221a N) \u221a N\nd\n) \u2264 4e\u2212cN + 2e\u2212cd. (B.14)\nIn addition, the following tail bound is due to property of (sub-)Gaussian random variables: P ( \u2016a\u2016\u221e \u2264 t1/ \u221a N ) \u2265 1\u2212 2Ne\u2212ct 2 1 , P(\u2016\u03b5\u2016\u221e \u2264 t2) \u2265 1\u2212 2ne\u2212ct 2 2 , (B.15)\nfor any t1, t2 \u2265 0. Because f\u2217 is Lipschitz, f\u2217(X) is a sub-Gaussian random vector with similar tail bound\nP(\u2016f\u2217(X)\u2016\u221e \u2264 t2) \u2265 1\u2212 2ne\u2212ct 2 2 .\nLet t1 = t2 = log n. Applying all these three tail bounds (B.14) and (B.8), (B.11) gives us the first part of the probability bound in (ii). As for the second part, following the observation\n\u2016B\u2016F \u2264 \u00b51\nn \u221a N \u2016X\u2016\u2016ya> \u03c3\u2032\u22a5(XW 0)\u2016F \u2264\n\u00b51\u03bb\u03c3 n \u221a N \u2016X\u2016\u2016y\u2016\u2016a\u2016,\nwe can adopt (B.6), (B.7) and (B.8) to conclude the second probability bound.\nPart (iii). Finally, we analyze the lower-order term C. Recall the definitions X = [X\u0303, . . . ,xn] >, W 0 = [w01, . . . ,w 0 N ] and a = [a1, . . . , aN ] >. We first observe that\nE\u2016\u03c3(XW 0)aa> \u03c3\u2032(XW 0)\u20162F \u2264 \u03bb2\u03c3 n\u2211 j=1 N\u2211 k=1 E ( N\u2211 i=1 aiak\u03c3(x > j w 0 i ) )2 ,\n=\u03bb2\u03c3 n\u2211 j=1 N\u2211 k=1 N\u2211 i,l=1 E [ alaia 2 k\u03c3(x > j w 0 i )\u03c3(x > j w 0 l ) ] = \u03bb2\u03c3 n\u2211 j=1 N\u2211 k=1 N\u2211 i=1 E [ a2i a 2 k\u03c3(x > j w 0 i ) 2 ] ,\n\u2264 C \u2032\nN2 n\u2211 j=1 N\u2211 k=1 N\u2211 i=1 E [ \u03c3(x>j w 0 i ) 2 ] \u2264 C \u2032\u2032n, (B.16)\nwhere the last inequality can be deduced by E[\u03c3(x>w)2] = Ew [ Ex[\u03c3(x>w)2] ] = Ew [ Ez[\u03c3(\u2016w\u2016z)2] ] \u22642Ew [ Ez(\u03c3(\u2016w\u2016z)\u2212 \u03c3(z))2 ] + 2Ew [ Ez[\u03c3(z)2]\n] \u22642\u03bb2\u03c3Ew [ (\u2016w\u2016 \u2212 1)2 ] + 2Ew [ Ez[\u03c3(z)2] ] \u2264 4\u03bb2\u03c3 + Ez[\u03c3(z)2],\nwhich is uniformly bounded by a constant. Therefore, by (B.5) and (B.16), we get\nE\u2016C\u2016] \u2264 E\u2016C\u2016F \u2264 1 nN E[\u2016X\u20162] 12E[\u2016\u03c3(XW 0)aa> \u03c3\u2032(XW 0)\u20162F ] 1 2 \u2264 C3 N\n\u221a 1 + d\nn .\nAs for the tail control, because of Fact 13, we consider the following upper-bound,\n\u2016C\u2016 \u2264 \u2016C\u2016F \u2264 1\nnN \u2016X\u2016\u2016\u03c3(XW 0)a\u2016\u221e\u2016a\u2016\u221e\u2016\u03c3 \u2032(XW 0)\u2016F \u2264 \u03bb\u03c3\u221a nN \u2016X\u2016\u2016\u03c3(XW 0)a\u2016\u221e\u2016a\u2016\u221e, (B.17)\nwhere the last inequality is due to |\u03c3\u2032| being upper-bounded by \u03bb\u03c3. To control \u2016\u03c3(XW 0)a\u2016\u221e, note that since a is centered by Assumption 1, we can apply Bernstein\ninequality for a and W conditioned on the event M := {\u2223\u2223\u2223\u2016xi\u2016/\u221ad\u2212 1\u2223\u2223\u2223 \u2264 1/2, i \u2208 [n]}. Conventionally, we\ndenote \u2016\u00b7\u2016\u03c82 as the sub-Gaussian norm. Since \u2225\u2225\u2016xi\u2016\u2212\u221ad\u2225\u2225\u03c82 is bounded by some absolute constant ([Ver18, Theorem 3.1.1]), we know that P(M) \u2265 1\u2212 ne\u2212cd.\nNotice that for any j \u2208 [n], \u03c3(x>j W 0)a = \u2211N i=1 ai\u03c3(x > j w 0 i ) is the sum of N independent and centered sub-Exponential random variables, where, in terms of [FW20, Lemma D.5], the sub-Exponential norm \u2016\u00b7\u2016\u03c81 of each term is bounded by the sub-Gaussian norm of the entries as follows,\n\u2016ai\u03c3(x>j w0i )\u2016\u03c81 \u2264 \u2016ai\u2016\u03c82\u2016\u03c3(x>j w0i )\u2016\u03c82 \u2264 C\u03bb\u03c3\u221a N \u2016xj\u2016\u221a d \u2264 3C\u03bb\u03c3 2 \u221a N ,\nfor some absolute constant C. Thus, by Bernstein inequality [Ver18, Theorem 2.8.1], for each j \u2208 [n],\nP ( |\u03c3(x>j W 0)a| \u2265 log n ) \u2264 2e\u2212c(logn) 2 .\nThen we take the union over all xj and obtain \u2016\u03c3(XW 0)a\u2016\u221e \u2264 log n with probability at least 1 \u2212 2ne\u2212c(logn) 2 . Hence, by (B.8), (B.15) and (B.17), we get\nP ( \u2016C\u2016F \u2265 ( \u221a d+ \u221a n+ t) log n logN\u221a\nnN\n) \u2264 2ne\u2212c(logn) 2 + 2Ne\u2212c(logN) 2 + ne\u2212cd + 2e\u2212ct 2 + 2Ne\u2212cN .\nPart (iii) is established by choosing t = \u221a d. This concludes the proof of the lemma.\nProposition 2 is a direct consequence of the above norm bounds.\nProof of Proposition 2. Notice that G0 \u2212A = B + C. In the proportional regime, by Lemma 14, there exist universal constants C, c > 0 such that\nP ( \u2016G0 \u2212A\u2016 \u2264 C log2 n\nn\n) \u2265 1\u2212 ne\u2212c log 2 n.\nOn the other hand, part (i) in Lemma 14 implies that P ( \u2016A\u2016 \u2265 C\u221a\nn\n) \u2265 1\u2212 e\u2212cn,\nfor some constant c, C > 0. Here we used the fact \u2016A\u2016 = \u2016A\u2016F because it is a rank-one matrix. Conditioning on the two events stated above, we have\n\u2016G0 \u2212A\u2016 \u2264 C\u221a n log2 n\u221a n \u2264 log 2 n\u221a n \u2016A\u2016 \u2264 log 2 n\u221a n (\u2016G0\u2016+ \u2016G0 \u2212A\u2016).\nAs long as n is sufficiently large such that log 2 n\u221a n < 12 , we can obtain\nP ( \u2016G0 \u2212A\u2016 \u2264\n2 log2 n\u221a n \u2016G0\u2016\n) \u2265 1\u2212 ne\u2212c log 2 n \u2212 e\u2212cn,\nwhich completes the proof."
        },
        {
            "heading": "B.1.2 Decomposition of Matrix A",
            "text": "Using the orthogonal decomposition (3.3), we can further decompose the rank-1 matrix A as follows\nA = 1 n \u00b7 \u00b51\u00b5 \u2217 1\u221a N X>X\u03b2\u2217a\n>\ufe38 \ufe37\ufe37 \ufe38 A1\n+ 1 n \u00b7 \u00b51\u221a N X>(\u00b5\u221701 + P>1f \u2217(X) + \u03b5)a>\ufe38 \ufe37\ufe37 \ufe38 A2 , (B.18)\nwhere we denote P>1f \u2217(X) := [P>1f \u2217(x1), . . . ,P>1f \u2217(xn)] > \u2208 Rn. Similar to the previous Lemma 14, we have the following norm bound.\nLemma 15. Assume that target function f\u2217 \u2208 L4(Rd,\u0393) is a Lipschitz function. We have (i) E\u2016A1\u2016F \u2264 C\u221aN ( 1 + dn ) and P ( \u2016A1\u2016F \u2265 C ( 1\u221a N + d n \u221a N )) \u2264 C \u2032(e\u2212cN + e\u2212cn);\n(ii) E\u2016A2\u2016F \u2264 C \u221a d Nn , and when n \u2265 d,\nP ( \u2016A2\u20162F \u2265 Cd\nnN\n) \u2264 C \u2032(e\u2212c \u221a n + e\u2212cN + ne\u2212cd + d\u22121), (B.19)\nfor some constants C,C \u2032, c > 0 that only depend on \u00b51, \u03c3\u03b5 and f \u2217.\nProof. For simplicity, we denote P>1f \u2217(x) by f\u2217NL(x) and P>1f \u2217(X) by f\u2217NL \u2208 Rn.\nPart (i). The expectation follows from (B.5) and the following inequality,\n\u2016A1\u2016F \u2264 \u00b51\u00b5\n\u2217 1\nn \u221a N \u2016X\u20162\u2016\u03b2\u2217\u2016\u2016a\u2016 =\n\u00b51\u00b5 \u2217 1 n \u221a N \u2016X\u20162\u2016a\u2016.\nThe probability bound also follows from the same argument as Lemma 14.\nPart (ii). Following the proof of part (i) in Lemma 14, we can further decompose \u2016A\u2016F into\n\u2016A2\u2016F \u2264 \u00b51\nn \u221a N \u2016X\u2016\u2016a\u2016\n( \u00b5\u22170 \u221a n+ \u2016f\u2217NL\u2016+ \u2016\u03b5\u2016 ) .\nSince P>1f \u2217 is a Lipschitz function as well, we can again apply the Lipschitz concentration (B.7). Hence, combining (B.8), (B.6) and (B.7), one can conclude the bound on the expectation of \u2016A2\u2016F . For the tail bound, we consider matrices\nA\u20322 := \u00b51 n \u221a N X>f\u2217NLa >, A\u2032\u20322 := \u00b51 n \u221a N X>\u03b5a>, A\u2032\u2032\u20322 :=\n\u00b5\u22170\u00b51\nn \u221a N X>1a>,\nwhose squared Frobenius norms are given by\n\u2016A\u20322\u20162F = \u00b521 n2N a>af\u2217>NLXX >f\u2217NL, \u2016A \u2032\u2032 2\u20162F = \u00b521 n2N a>a\u03b5>XX>\u03b5, \u2016A\u2032\u2032\u20322 \u20162F = \u00b5\u221720 \u00b5 2 1 n2N a>a1>XX>1.\nRecall that (B.6) implies P ( \u2016a\u20162 \u2265 4 ) \u2264 2e\u2212cN . (B.20)\nLet us first addressA\u2032\u20322 . Due to (B.20), it suffices to control \u03b5 >XX>\u03b5, whose expectation with respect to \u03b5 is \u03c32\u03b5 Tr(XX >), and E[Tr(XX>)] = nd. Recalling the Lipschitz Gaussian concentration for \u2016X\u2016F and (B.6),\nwe know that for some constant c > 0, P(A\u03b5) \u2265 1\u2212 4e\u2212cd, where A\u03b5 := {\u2016X\u2016F \u2264 \u221a nd, \u2016X\u2016 \u2264 \u221a n+ \u221a d}. This directly implies that\n\u2016XX>\u2016F \u2264 \u2016X\u2016F \u2016X\u2016 \u2264 \u221a nd (\u221a n+ \u221a d ) ,\nconditioned on event A\u03b5. Thus, the Hanson-Wright inequality (Theorem 6.2.1 [Ver18]) indicates that P ( \u03b5>XX>\u03b5 \u2265 t+ 4\u03c32\u03b5nd ) \u2264 P ( \u03b5>XX>\u03b5 \u2265 t+ \u03c32\u03b5nd \u2223\u2223\u2223 A\u03b5)+ P(Ac\u03b5) \u2264P (\u2223\u2223\u2223\u03b5>XX>\u03b5\u2212 \u03c32\u03b5\u2016X\u20162F \u2223\u2223\u2223 \u2265 t \u2223\u2223\u2223 A\u03b5)+ 4e\u2212cd \u2264 2e\u2212cmin { t2 nd(n+d) , t ( \u221a d+ \u221a n)2 } + 4e\u2212cd.\nThus, by choosing t = nd and employing (B.20), we have P ( \u2016A\u2032\u20322\u20162F \u2265 Cd\nnN\n) \u2264 6e\u2212cd + 2e\u2212cN ,\nwhere we simplified the expression using the assumption that n \u2265 d. Next we analyze \u2016A\u20322\u20162F . Notice that E[f\u2217NL(x1)] = 0 and E[x1f\u2217NL(x1)] = 0. Since f \u2217 NL is a random vector with independent mean-zero sub-Gaussian coordinates and f\u2217 \u2208 L2(Rd,\u0393), we know that\nP (\u2223\u2223\u2223\u2223 1n\u2016f\u2217NL\u20162 \u2212 \u2016f\u2217NL\u20162L2(Rd,\u0393) \u2223\u2223\u2223\u2223 \u2264 n\u22121/4) \u2265 1\u2212 2e\u2212c\u221an. (B.21) We can further decompose \u2016A\u20322\u20162F into two parts:\n\u2016A\u20322\u20162F = \u00b521 n2N \u2016a\u20162 n\u2211 i6=j x>i xjf \u2217 NL(xi)f\n\u2217 NL(xj)\ufe38 \ufe37\ufe37 \ufe38\nJ1\n+ \u00b521 n2N \u2016a\u20162 n\u2211 i=1\n\u2016xi\u20162(f\u2217NL(xi))2\ufe38 \ufe37\ufe37 \ufe38 J2 .\nSince E[xif\u2217NL(xi)] = 0 for 1 \u2264 i \u2264 n, we deduce that E[J1] = 0 and\nVar(J1) = n\u2211 i6=j E [ (x>i xj) 2(f\u2217NL(xi)f \u2217 NL(xj)) 2 ] + 2 n\u2211 i 6=j 6=k E [ x>i xjx > k xjf \u2217 NL(xi)f \u2217 NL(xk)(f \u2217 NL(xj)) 2 ]\n+ n\u2211 i6=j 6=k 6=l E [ x>i xjx > k xlf \u2217 NL(xi)f \u2217 NL(xk)f \u2217 NL(xl)f \u2217 NL(xj) ]\n= n\u2211 i6=j E [ (x>i xj) 2(f\u2217NL(xi)f \u2217 NL(xj)) 2 ] \u2264 n2E[(x>1 x2)4]1/2\u2016f\u2217NL\u20164L4(Rd,\u0393). (B.22)\nOn the other hand, Bernstein\u2019s inequality [Ver18, Theorem 2.8.1] indicates that for all 1 \u2264 i 6= j \u2264 n,\nP ( |x>i xj | \u2265 t ) , P ( |\u2016xi\u20162 \u2212 d| \u2265 t ) \u2264 2e\u2212cmin { t2 d ,t } , (B.23)\nwhich yields the sub-exponential condition for x>1 x2. Thus, based on [BLM13, Theorem 2.3], we can obtain moment bounds for x>1 x2, namely E[(x>1 x2)4] . d2, whence Var(J1) \u2264 Cn2d for some constant C > 0. By Chebyshev\u2019s inequality, we deduce that\nP \u2223\u2223\u2223 \u00b521 n2N n\u2211 i6=j x>i xjf \u2217 NL(xi)f \u2217 NL(xj) \u2223\u2223\u2223 > t  \u2264 Cd n2N2t2 , (B.24)\nfor all t > 0. As for J2, we apply (B.21) and (B.23) to all \u2016xi\u2016. Letting t = d in (B.23) and taking union bounds for all 1 \u2264 i \u2264 n, we obtain\nP ( \u2016xi\u20162 \u2264 2d, \u22001 \u2264 i \u2264 n ) \u2265 1\u2212 2ne\u2212cd. (B.25)\nHence, the above equation and (B.21) lead the following bound\nP ( \u00b521 n2N n\u2211 i=1 \u2016xi\u20162(f\u2217NL(xi))2 \u2264 Cd nN ) \u2265 1\u2212 2e\u2212c \u221a n \u2212 ne\u2212cd. (B.26)\nTherefore, by letting t = dnN in (B.24) and combining (B.20) and (B.26), we can conclude that\nP ( \u2016A\u20322\u20162F \u2265 Cd\nnN\n) \u2264 2e\u2212c \u221a n + ne\u2212cd + 2e\u2212cN + c\nd , (B.27)\nfor some constant C, c > 0.\nFinally for A\u2032\u2032\u20322 , we may employ a similar decomposition as A \u2032 2,\u2225\u2225A\u2032\u2032\u20322 \u2225\u22252F = \u00b5\u221720 \u00b521n2N \u2016a\u20162 n\u2211 i 6=j\nx>i xj\ufe38 \ufe37\ufe37 \ufe38 K1\n+ \u00b5\u221720 \u00b5 2 1\nn2N \u2016a\u20162 n\u2211 i=1\n\u2016xi\u20162\ufe38 \ufe37\ufe37 \ufe38 K2 .\nFollowing the same computation as (B.22) and (B.24), we may control the tail of K1 via Chebyshev\u2019s inequality due to E[K1] = 0, Var(K1) . n2d. Whereas the bound on K2 follows from (B.20) and the union bound (B.25). We omit the details for this part. Combining these estimates, we know that \u2225\u2225A\u2032\u2032\u20322 \u2225\u22252F also obeys the same tail bound as (B.27). The proof of (B.19) is completed by combining the above calculations."
        },
        {
            "heading": "B.1.3 Multiple Gradient Steps",
            "text": "Finally, we show via induction that the estimates in Lemma 14 still hold after t gradient steps with learning \u03b7 = \u0398(1) for fixed t \u2208 N (note that we do not scale the number of steps t jointly with n, d,N). For this lemma, we directly consider the proportional limit, that is, we do not keep track of the exact constants and dependence on n, d,N separately for simplicity.\nLemma 16. Under Assumption 1, given any fixed t \u2208 N and learning rate \u03b7 = \u0398(1), the weight matrix after t gradient steps W t defined in (2.1) satisfies:\nP(\u2016W t \u2212W 0\u2016 \u2265 C) \u2264 exp(\u2212cN);\nP ( \u2016W t \u2212W 0\u20162,\u221e \u2265\nC log2N\u221a N\n) \u2264 exp ( \u2212c log2N ) ;\nP(\u2016W t \u2212W 0\u2016F \u2265 C) \u2264 exp(\u2212cN),\nfor some positive constants c, C.\nProof. For the induction hypothesis, we assume that (under the proportional scaling in Assumption 1) after t gradient steps with learning rate \u03b7 = \u0398(1), the weight matrix satisfies P(\u2016W t \u2212W 0\u2016 \u2265 C) \u2264 exp(\u2212cN), P ( \u2016W t \u2212W 0\u20162,\u221e \u2265 C log2N\u221a N ) \u2264 exp ( \u2212c log2N ) , P(\u2016W t \u2212W 0\u2016F \u2265 C) \u2264 exp(\u2212cN). Our goal is to show that the same high-probability statements also hold for W t+1 (for some different constants c \u2032, C \u2032).\nWe first control the difference in the prediction of the trained neural network compared to the initialized model. Following the same argument as [OS20, Setion 6.6.1], we know that\n\u2016ft(X)\u2016 \u2264 \u2016f0(X)\u2016+ \u2016ft(X)\u2212 f0(X)\u2016 . \u2016f0(X)\u2016+ 1\u221a N \u2016a\u2016\u2016X\u2016\u2016W t \u2212W 0\u2016F . (B.28)\nNote that \u2016W t \u2212W 0\u2016F = O(1) with high probability due to the induction hypothesis. We now compute the next gradient update Gt (we drop the learning rate \u03b7 = \u0398(1)).\nGt = \u2212 1 n X> [( 1\u221a N ( 1\u221a N \u03c3(XW t)a\u2212 y ) a> ) \u03c3\u2032(XW t) ] = 1\nn \u00b7 \u00b51\u221a N X>(y \u2212 ft(X))a>\ufe38 \ufe37\ufe37 \ufe38\nAt\n+ 1 n \u00b7 1\u221a N X>\n( (y \u2212 ft(X))a> \u03c3\u2032\u22a5(XW t) ) \ufe38 \ufe37\ufe37 \ufe38\nBt\n.\nFor At, following the same argument as Lemma 14, we have\u2225\u2225At\u2225\u2225 = \u2225\u2225At\u2225\u2225 F . 1\nn \u221a N \u2016X\u2016(\u2016y\u2016+ \u2016ft(X)\u2016)\u2016a\u2016.\u2225\u2225At\u2225\u2225\n2,\u221e . 1\nn \u221a N \u2016X\u2016(\u2016y\u2016+ \u2016ft(X)\u2016)\u2016a\u2016\u221e.\nNow recall that \u2016f0(X)\u2016 \u2264 1\u221aN \u2016a\u2016\u2016\u03c3(XW 0)\u2016. Combining the norm control of a in (B.6), (B.15), the norm control of y due to (B.7) and (B.15), the operator norm bound on X and \u2016\u03c3(XW 0)\u2016 given in (B.8) and (B.14) (where we applied [FW20, Lemma D.4] to the matrix \u03c3(XW 0), since \u03c3 is centered), and the upper bound on \u2016ft(X)\u2016 given in (B.28), we arrive at\nP (\u2225\u2225At\u2225\u2225 \u2265 C \u2032\u221a\nN\n) \u2264 exp(\u2212c\u2032N), P (\u2225\u2225At\u2225\u2225 2,\u221e \u2265 C \u2032 logN\u221a N ) \u2264 exp ( \u2212c\u2032 log2N ) ,\nfor large enough N and constants c\u2032, C \u2032 > 0. Similarly for Bt, we have\u2225\u2225Bt\u2225\u2225 2,\u221e \u2264 \u2225\u2225Bt\u2225\u2225 . 1 n \u221a N \u2016X\u2016(\u2016y\u2016\u221e + \u2016ft(X)\u2016\u221e)\u2016a\u2016\u221e\u2016\u03c3\n\u2032 \u22a5(XW t)\u2016,\u2225\u2225Bt\u2225\u2225\nF .\n1\nn \u221a N \u2016X\u2016(\u2016y\u2016+ \u2016ft(X)\u2016)\u2016a\u2016max i,j |\u03c3\u2032\u22a5(XW t)|i,j .\nAgain using [OS20, Setion 6.6.1], we have\n\u2016\u03c3\u2032\u22a5(XW t)\u2016 \u2264 \u2016\u03c3\u2032\u22a5(XW 0)\u2016+ \u2016\u03c3\u2032\u22a5(XW t)\u2212 \u03c3\u2032\u22a5(XW 0)\u2016 . \u2016\u03c3\u2032\u22a5(XW 0)\u2016+ \u2016X\u2016\u2016W t \u2212W 0\u2016F .\nThanks to the norm control of X in (B.8), the norm control of y and ft(X) from (B.6), (B.7), (B.15), and (B.28), and the operator norm of the CK matrix given in (B.14), we get\nP (\u2225\u2225Bt\u2225\u2225 \u2265 C \u2032 log2N\nN\n) \u2264 exp ( \u2212c\u2032 log2N ) , P (\u2225\u2225Bt\u2225\u2225 F \u2265 C \u2032 \u221a N ) \u2264 exp(\u2212c\u2032N),\nfor large enough N . Consequently, given the induction hypothesis, we know that for the next time step (t+ 1) with learning rate \u03b7 = \u0398(1), there exist some constants c\u2032, C \u2032 such that\nP(\u2016W t+1 \u2212W 0\u2016 \u2265 C \u2032) \u2264 exp(\u2212c\u2032N); P ( \u2016W t+1 \u2212W 0\u20162,\u221e \u2265\nC \u2032 log2N\u221a N\n) \u2264 exp ( \u2212c\u2032 log2N ) ;\nP(\u2016W t+1 \u2212W 0\u2016F \u2265 C \u2032) \u2264 exp(\u2212c\u2032N).\nNote that constants c\u2032, C \u2032 > 0 may depend on t but do not rely on n, d,N . We therefore conclude that the above statements hold true for any finite t \u2208 N."
        },
        {
            "heading": "B.2 Calculation of Alignment with Target Function",
            "text": "In this section we prove Theorem 3. We first characterize certain quadratic forms which will appear in many parts of our analysis."
        },
        {
            "heading": "B.2.1 Concentration of Quadratic Forms",
            "text": "The following lemma is a direct adaptation from Lemma 2.7 and Lemma A.1 in [BS98]. We also refer readers to section B.5 in [BS10] for more details.\nLemma 17. Given any deterministic matrix D \u2208 Rd\u00d7d and x \u223c N (0, I) in Rd, we have that for any p \u2265 1,\nE [\u2223\u2223x>Dx\u2212 TrD\u2223\u2223p] \u2264 Cp(3p/2 + (2p\u2212 1)!!)(Tr(DD>))p/2,\nwhere Cp > 0 is a universal constant. Furthermore, if D is a non-negative definite matrix, then we have E [\u2223\u2223x>Dx\u2223\u2223p] \u2264 Cp((TrD)p + (2p\u2212 1)!! Tr(Dp)).\nEquipped with Lemma 17, we introduce a quadratic concentration lemma specialized to our setting.\nLemma 18. Define u := \u03b7\u00b51n X >y where y = f\u2217(X) + \u03b5. Under the same assumptions as Theorem 3, consider any deterministic matrix D \u2208 Rd\u00d7d with \u2016D\u2016 \u2264 C uniformly for some constant C > 0. Then, as n/d\u2192 \u03c81 proportionally, we have that\u2223\u2223\u2223u>Du\u2212 (\u03b821 \u2212 \u03b822) trD \u2212 \u03b822\u03b2>\u2217D\u03b2\u2217\u2223\u2223\u2223, \u2223\u2223\u2223\u03b2>\u2217Du\u2212 \u03b82\u03b2>\u2217D\u03b2\u2217\u2223\u2223\u2223 P\u2192 0, where \u03b81 and \u03b82 are defined in (3.4). In addition, recalling that the nonlinear part of the target function is given as f\u2217NL(x) := f\n\u2217(x)\u2212 \u00b5\u22170 \u2212 \u00b5\u22171\u3008x,\u03b2\u2217\u3009, we have that\u2223\u2223\u2223\u2223 1n\u03b2>\u2217DX>f\u2217NL(X) \u2223\u2223\u2223\u2223 P\u2192 0. (B.29)\nProof. We first consider the concentration for u>Du. Note that X> = [x1, . . . ,xn] has i.i.d. columns. Hence, we can expand the first quadratic form as follows:\nu>Du = \u03b72\u00b521 n2 n\u2211 i,j=1 x>j Dxi(f \u2217(xi) + \u03b5i)(f \u2217(xj) + \u03b5j).\nDenote vi := xi(f \u2217(xi) + \u03b5i), for 1 \u2264 i \u2264 n. By condition (3.3) for f\u2217, all vectors vi are i.i.d. with E[vi] = \u00b5\u22171\u03b2\u2217. Let us first compute the expectation of this quadratic form\nE[u>Du] = \u03b72\u00b521 n2 n\u2211 i,j=1 ( E [ x>i Dxjf \u2217(xi)f \u2217(xj) ] + E [ \u03b5i\u03b5jx > i Dxjf \u2217(xi)f \u2217(xj) ])\n= \u03b72\u00b521 n\n( E[x>1 Dx1f\u22172(x1)] + \u03c32\u03b5 TrD ) + \u03b72\u00b521 n2 \u2211 i 6=j \u00b5\u221721 \u03b2 > \u2217D\u03b2\u2217. (B.30)\nNotice that \u2223\u2223E[x>1 Dx1f\u22172(x1)]\u2212 E[f\u22172(x1)] TrD\u2223\u2223 \u2264 E[\u2223\u2223x>1 Dx1 \u2212 TrD\u2223\u2223f\u22172(x1)] \u2264 E\n[\u2223\u2223x>1 Dx1 \u2212 TrD\u2223\u22232]1/2E[f\u22174(x1)]1/2 (i)\u2264 C\u2016D\u2016F \u2264 C\u221ad\u2016D\u2016, (B.31) where (i) is due to Lemma 17 and \u2016f\u2217\u2016L4(Rd,\u0393) \u2264 Cf uniformly for some constant Cf > 0, since f\u2217 is Lipschitz by Assumption 1. Therefore, recalling the definitions of \u03b81 and \u03b82 in (3.4), we arrive at\nE[u>Du]\u2212 ( \u03b821 \u2212 \u03b822 ) trD \u2212 \u03b822\u03b2 > \u2217D\u03b2\u2217 \u2192 0,\nas n\u2192\u221e and n/d\u2192 \u03c81. Next, we claim that this quadratic form u>Du concentrates around its expectation in L2. For i 6= j \u2208 [n], denote qij := v>i Dvj \u2212 \u00b5\u221721 \u03b2 > \u2217D\u03b2\u2217 and qii := v > i Dvi \u2212 ( \u00b5\u03042 + \u03c32\u03b5 ) TrD, hence E[qij ] = 0 and, by (B.30) and (B.31), 1nE[qii]\u2192 0 as n\u2192\u221e. In particular,\nE[q12q13] =\u2212 ( \u00b5\u221721 \u03b2 > \u2217D\u03b2\u2217 )2 + \u00b5\u221721 \u03b2 > \u2217DE[v1v>1 ]D\u03b2\u2217\n=\u00b5\u221721 E [ x>1 D >\u03b2\u2217\u03b2 > \u2217Dx1 ( f\u22172(x1) + \u03b5 2 1 ) \u2212 \u00b5\u221721 \u03b2 > \u2217D\u03b2\u2217\u03b2 > \u2217D\u03b2\u2217 ] \u2264\u00b5\u221741 \u2016D\u20162 + \u00b5\u221721 E [( x>1 D >\u03b2\u2217\u03b2 > \u2217Dx1 )2]1/2( E [ f\u22174(x1) ]1/2 + E [ \u03b541 ]1/2) \u2264 C\u2016D\u20162, (B.32)\nwhere the last inequality is from Lemma 17 and the uniform boundedness of \u2016f\u2217\u2016L4(Rd,\u0393). In addition, since |TrD| \u2264 d\u2016D\u2016, according to Lemma 17, we get\nE[q211] = E [( (f\u2217(x1) + \u03b51) 2( x>1 Dx1 \u2212 TrD ) + TrD \u00b7 ( (f\u2217(x1) + \u03b51) 2 \u2212 \u00b5\u03042 \u2212 \u03c32\u03b5 ))2]\n\u2264 2E[ ( x>1 Dx1 \u2212 TrD )4 ]1/2E[(f\u2217(x1) + \u03b51)8]1/2 + 2E[(f\u2217(x1) + \u03b51)4](TrD)2\n\u2264 C ( Tr ( DD> ) + (TrD) 2 ) \u2264 Cd2\u2016D\u2016, (B.33)\nwhere we used the fact that \u2016f\u2217\u2016L8(Rd,\u0393) is uniformly bounded (which is due to Lipschitz assumption on f\u2217). Analogously, following the above estimations, we have\nE[q212] =Var(v>1 Dv2) \u2264 E [ x>1 D >x2x > 2 Dx1(f \u2217(x1) + \u03b51) 2 (f\u2217(x2) + \u03b52) 2 ]\n\u2264 E [( x>1 D >x2x > 2 Dx1 )2]1/2 E [ (f\u2217(x1) + \u03b51) 4 ] \u2264 CEx2 [ Ex1 [( x>1 D >x2x > 2 Dx1 )2]]1/2 (ii)\n. Ex2 [ (TrD>x2x > 2 D) 2 + TrD>x2x > 2 DD >x2x > 2 D ]1/2 = Ex2 [ 2(x>2 D >Dx2) 2 ]1/2\n(iii)\n. (( Tr ( D>D ))2 + Tr ( (D>D)2 )) . d\u2016D\u20162, (B.34)\nwhere both (ii) and (iii) are deduced from Lemma 17 since D>x2x > 2 D and D >D are both semi-positive definite. Combining the bounds for E[q12q13], E[q212] and E[q211], we conclude that\nE [\u2223\u2223\u2223u>Du\u2212 (\u03b821 \u2212 \u03b822) trD \u2212 \u03b822\u03b2>\u2217D\u03b2\u2217\u2223\u2223\u22232]\n\u2264 2E  \u2223\u2223\u2223\u2223\u2223\u2223\u03b7 2\u00b521 n2 \u2211 i 6=j v>i Dvj \u2212 \u03b822\u03b2 > \u2217D\u03b2\u2217 \u2223\u2223\u2223\u2223\u2223\u2223 2 + 2E \u2223\u2223\u2223\u2223\u2223\u03b72\u00b521n2 n\u2211 i=1 v>i Dvi \u2212 ( \u03b821 \u2212 \u03b822 ) trD \u2223\u2223\u2223\u2223\u2223 2 \n\u2264 C\u03b7 4\u00b541 n4 E (\u2211 i 6=j qij )2+ C\u03b74\u00b541 n4 E [ n\u2211 i=1 \u2223\u2223v>i Dvi \u2212 (\u00b5\u03042 + \u03c32\u03b5)TrD\u2223\u22232 ]\n(iv) \u2264 C\u03b74\u00b541 ( 1\nn2 E[q212] +\n1 n E[q12q13] + 1 n3 E[q211]\n) . 1\nn \u2192 0, (B.35)\nas n\u2192\u221e, where (iv) is obtained by Lemma 2.2 in [BS98] because v>i Dvi are i.i.d. for i \u2208 [n], and the last inequality (B.35) follows from (B.32), (B.33) and (B.34). This yields the convergence in probability.\nFor the second part \u03b2>\u2217Du, notice that\n\u03b2>\u2217Du\u2212 \u03b82\u03b2 > \u2217D\u03b2\u2217 = \u03b7\u00b51 n n\u2211 i=1 ( \u03b2>\u2217Dxi(f \u2217(xi) + \u03b5i)\u2212 \u00b5\u22171\u03b2 > \u2217D\u03b2\u2217 ) is a sample mean of i.i.d. centered random variables. Therefore by Lemma 17, we have\nE [\u2223\u2223\u2223\u03b2>\u2217Du\u2212 \u03b82\u03b2>\u2217D\u03b2\u2217\u2223\u2223\u22232] \u2264 \u03b72\u00b521n E(\u03b2>\u2217Dxi(f\u2217(xi) + \u03b5i))2\n\u2264 \u03b7 2\u00b521 n\nE [( \u03b2>\u2217Dxi )4]1/2 E [ ((f\u2217(xi) + \u03b5i)) 4 ]1/2 \u2264 C n E [( x>i D >\u03b2\u2217\u03b2 > \u2217Dxi )2]1/2 . 1\nn\n(( Tr ( D>\u03b2\u2217\u03b2 > \u2217D ))2 + Tr ( D>\u03b2\u2217\u03b2 > \u2217DD >\u03b2\u2217\u03b2 > \u2217D ))1/2 . \u2016D\u20162\nn \u2192 0.\nHence, this L2 convergence completes the proof. Finally, note that the proof of (B.29) is identical to the above calculation, where we can apply E[xif\u2217NL(xi)] = 0 for i \u2208 [n]."
        },
        {
            "heading": "B.2.2 Analysis of Spike in Weight Matrix",
            "text": "Useful Lemmas. Observe that the limiting eigenvalue distribution of W>0 W 0 is the Marchenko\u2013Pastur distribution \u00b5MP\u03c82 with parameter \u03c82; let m(z) be the Stieltjes transform of \u00b5 MP \u03c82 . Also, we denote the limiting eigenvalue distribution for W 0W > 0 by \u00b5\u0304 MP \u03c82\nwhose Stieltjes transform is m\u0304(z), which is referred to as the companion transform. The relation between m(z) and m\u0304(z) is given as\n\u03c82m(z) = m\u0304(z) + 1\u2212 \u03c82 z . (B.36)\nMoreover, m(z) is uniquely determined by the fixed-point equation\nz\u03c82m 2(z)\u2212 (1\u2212 \u03c82 \u2212 z)m(z) + 1 = 0, (B.37)\nfor z \u2208 C \\ supp(\u00b5MP\u03c82 ). For more details on Stieltjes transform of \u00b5 MP \u03c82 , we refer to [BS10, Chapter 3].\nLemma 19. Following the above notions, we define the resolvent Q0(z) := (W 0W > 0 \u2212 zI)\u22121, for\nz \u2208 \u2126 := { z \u2208 C : Re(z) > ( 1 + \u221a \u03c82 )2 + } , (B.38)\nand any small > 0. Then, under the same assumptions of Theorem 3, for all sufficiently large N , \u2016Q0(z)\u2016 \u2264 2/ uniformly for all z \u2208 \u2126 . Proof. Write z = x+ iy, with x > ( 1+ \u221a \u03c82 )2\n+ . For any i \u2208 [N ], let \u03bbi be the i-th eigenvalue of W 0W>0 . Then we have \u2223\u2223\u2223\u2223 1z \u2212 \u03bbi \u2223\u2223\u2223\u2223 \u2264 1\u221a(x\u2212 \u03bbi)2 + y2 \u2264 1|x\u2212 \u03bbi| \u2264 1|x\u2212 \u03bb1| .\nBy Theorem 5.11 in [BS10], for sufficiently large N , \u2223\u2223\u2223\u03bb1 \u2212 (1 +\u221a\u03c82)2\u2223\u2223\u2223 \u2264 /2. Hence, 1/|z \u2212 \u03bbi| \u2264 /2 for i \u2208 [N ] and \u2016Q0(z)\u2016 \u2264 2/ for all z \u2208 \u2126 .\nThe following lemma characterizes the asymptotics of certain quantities in terms of the Stieltjes transform which will be useful in the subsequent analysis.\nLemma 20. Recall the definition u = \u03b7\u00b51n X >y. Under the same assumptions as Theorem 3, for any > 0,\na>W>0 Q0(z)u\u2192 0, \u03b2 > \u2217 Q0(z)W 0a\u2192 0, (B.39) u>Q0(z)u\u2192 \u03b821m\u0304(z), \u03b2 > \u2217 Q0(z)u\u2192 \u03b82m\u0304(z), u>Q0(z)2u\u2192 \u03b821m\u0304\u2032(z) (B.40) a>W>0 Q0(z)W 0a\u2192 1 + zm(z), a>W > 0 Q0(z) 2W 0a\u2192 m(z) + zm\u2032(z), (B.41)\nin probability as n/d\u2192 \u03c81 and N/d\u2192 \u03c82, uniformly on any compact subset of \u2126 defined in (B.38), where scalars \u03b81 and \u03b82 are defined in Theorem 3.\nProof. Firstly note that (B.39) directly follows from Lemma 19 and Hoeffding\u2019s inequality for a. The remaining concentration statements will be established by applying Lemma 18 to different choices ofD and the Hanson-Wright inequality for a. In particular, due to Lemma 19, we know that \u2016Q0(z)\u2016, \u2016W > 0 Q0(z)W 0\u2016, \u2016Q0(z)2\u2016 and \u2016W > 0 Q0(z)\n2W 0\u2016 are all uniformly bounded on \u2126 for large N . Take D = Q0(z) in Lemma 18, we obtain that \u2223\u2223\u2223u>Q0(z)u\u2212 (\u03b821 \u2212 \u03b822) trQ0(z)\u2212 \u03b822\u03b2>\u2217 Q0(z)\u03b2\u2217\u2223\u2223\u2223 P\u2192 0, uniformly for z \u2208 \u2126 . Moreover, we may treat \u03b2\u2217 as uniformly distributed on Sd\u22121 due to the rotational invariance of W 0W > 0 . Thus, |\u03b2 > \u2217 Q0(z)\u03b2\u2217 \u2212 trQ0(z)| \u2192 0 in probability uniformly for z \u2208 \u2126 (the detailed statement will be elaborated by Lemma 27 and 28 in Section C.3). By the Marchenko\u2013Pastur law [BS10, Theorem 3.10] and (B.36), we can conclude that u>Q0(z)u converges to \u03b8 2 1m\u0304(z) in probability for any z \u2208 \u2126 . In addition, Arzela\u0300-Ascoli theorem implies that this convergence in probability holds uniformly on any compact subset of \u2126 . One can analogously verify the remaining statements in (B.40). Finally, note that with Lemma 19, the Hanson-Wright inequality for normalized Gaussian vector a enables us to establish (B.41) directly. To complete the proof, we point out that\ntr ( W>0 Q0(z)W 0 ) P\u2192 1 \u03c82 \u222b x x\u2212 z d\u00b5\u0304MP\u03c82 (x) = 1 + zm(z),\ntr ( W>0 Q0(z) 2W 0 ) P\u2192 1 \u03c82 \u222b x (x\u2212 z)2 d\u00b5\u0304MP\u03c82 (x) = m(z) + zm \u2032(z),\nuniformly on any compact subset of \u2126 , due to (B.36), Lemma 7.4 of [DW18] and Lemma 2.14 of [BS10].\nFinally, we recall the following control of singular values (also referred to as Weyl\u2019s inequality).\nLemma 21 (Theorem A.46 of [BS10]). Let C and D \u2208 Rn\u00d7m be two complex matrices with singular values s1(C) \u2265 s2(C) \u2265 \u00b7 \u00b7 \u00b7 \u2265 sr(C) and s1(D) \u2265 s2(D) \u2265 \u00b7 \u00b7 \u00b7 \u2265 sr(D) where r = min{n,m}. Then for any 1 \u2264 k \u2264 r, the difference in the k-th singular values of C and D satisfies\n|sk(C)\u2212 sk(D)| \u2264 \u2016C \u2212D\u2016.\nWe are now ready to prove Theorem 3.\nProof of Theorem 3. Denote W\u0303 1 := W 0 + \u03b7 \u221a NA, where \u03b7 \u221a NA = \u03b7\u00b51n X >ya>. Lemma 14 implies that as n/d\u2192 \u03c81 and N/d\u2192 \u03c82, \u2016W 1\u2212W\u0303 1\u2016 \u2192 0 almost surely. Thanks to Lemma 21, the top i-th singular value si(W 1) coincides with si(W\u0303 1) asymptotically for any fixed i \u2265 1. Hence we first prove Theorem 3 for W\u0303 1 in lieu of the original W 1; this is equivalent to considering the leading eigenvalue \u03bb\u0302 := s1(W\u0303 1) 2 of\nW\u0303 1W\u0303 > 1 and its corresponding eigenvector denoted as u\u03031. Note that s1(W\u0303 1) is the leading singular value\nof a rectangular Gaussian random matrix W 0 plus an independent rank-one perturbation \u03b7 \u221a NA.\nFrom the definition of W\u0303 1, we have the decomposition\nW\u0303 1W\u0303 > 1 = W 0W > 0 + [ u W 0a ][\u2016a\u20162 1 1 0 ][ u>\na>W>0\n] . (B.42)\nAlso, by [BGN11, Section 6.2.1.] and [BS10, Section 5.2], we know that lim inf s1(W\u0303 1) \u2265 1+ \u221a \u03c82, s1(W 0)\u2192 1 + \u221a \u03c82, and si(W 0), si(W\u0303 1)\u2192 1 + \u221a \u03c82 for any fixed i > 1.\nTo analyze the isolated eigenvalue and the corresponding eigenvector for W\u0303 1W\u0303 > 1 , we follow the approach in [BGN11, BGN12]. It is straightforward to verify that the isolated eigenvalue of W\u0303 1W\u0303 > 1 outside the spectrum of W 0W > 0 is the solution x \u2208 R to the following equation:\ndetQ0(x) ( W\u0303 1W\u0303 > 1 \u2212 xI ) = 0.\nBy (B.42), the equality det(AB) = det(A) det(B), and the Sylvester\u2019s determinant identity det(I +AB) = det(I +BA) for A,B of appropriate dimensions, the above equations has the same solution as\nPn(x) := det\n( I + ( \u2016a\u20162 1\n1 0\n)( u>\na>W>0\n) Q0(x) ( u W 0a )) = 0,\nwhere Pn(x) is the determinant of a 2-by-2 matrix. Notice that \u2016a\u2016 \u2192 1 almost surely as N \u2192\u221e. Hence in terms of Lemma 20, we know that given > 0, as n, d,N \u2192\u221e proportionally, Pn(z)\u2192 P (z) in probability uniformly on any compact subset of \u2126 , where P (z) := 1\u2212 \u03b821zm(z)m\u0304(z). Next we establish the convergence of the roots of Pn(z) to the roots of P (z) on \u2126 \u2229 R. Due to (B.36) and (B.37), we know that\nP (z) = 1 + \u03b821(1 + zm(z)). (B.43) Now we compute the root of P (z) on ( (1 + \u221a \u03c82) 2,+\u221e ) . From (B.37) we have\nz = zm(z)\nzm(z) + 1 \u2212 \u03c82zm(z),\nwhich implies that the root of P (z) on the real line is given as \u03bb0 := (1+\u03b821)(\u03c82+\u03b8 2 1)\n\u03b821 . Based on the expression\nof m(z) and (B.37), we have\nlim x\u2198(1+ \u221a \u03c82)2\nxm(x) = \u22121 + \u221a \u03c82\u221a\n\u03c82 .\nNote that zm(z) is an increasing mapping from ( (1 + \u221a \u03c82) 2,+\u221e ) to ( \u2212 1+ \u221a \u03c82\u221a \u03c82 ,\u22121 ) . Thus, as long as \u03b81 > \u03c8 1/4 2 , by selecting a sufficient small > 0, we can obtain that \u03bb0 is a root in \u2126 \u2229 R. By Hurwitz\u2019s theorem and the uniform convergence of Pn(z), we know that the root of Pn(z), which is exactly the isolated eigenvalue \u03bb\u0302 for W 1W > 1 outside the spectrum of W > 0 W 0, is converging to \u03bb0 in probability. On the other hand, if \u03b81 \u2264 \u03c81/42 , there is no root of P (z) in ((1 + \u221a \u03c82)\n2,+\u221e), hence the largest eigenvalue \u03bb\u0302 for W 1W>1 is no greater than (1+ \u221a \u03c82) 2 + , for any > 0; because of the lower bound on \u03bb\u0302, in this case \u03bb\u0302\u2192 (1+ \u221a \u03c82) 2 which is at the right-edge of the support of \u00b5MP\u03c82 .\nNext we consider the isolated eigenvector u\u03031. By definition, \u03bb\u0302u\u03031 = W 1W > 1 u\u03031, which, together with\n(B.42), yields the identity\n0 = (W\u0303 0W\u0303 > 0 \u2212 \u03bb\u0302I)u\u03031 + [ u W 0a ][\u2016a\u20162 1 1 0 ][ u>\na>W>0\n] u\u03031.\nSince \u03bb\u0302 does not reside in the spectrum of W 0W > 0 , we can further write\n0 = ( I +Q0(\u03bb\u0302) [ u W 0a ][\u2016a\u20162 1 1 0 ][ u>\na>W>0\n]) u\u03031. (B.44)\nBy multiplying [u,W 0a] > from the left hand side of the above equality, we arrive at 0 = Mn(\u03bb\u0302) [ v\u03021 v\u03022 ] , where the 2-by-2 matrix is given as\nMn(z) := I +\n[ u>Q0(z)u u >Q0(z)W 0a\na>W>0 Q0(z)u a >W>0 Q0(z)W 0a\n][ \u2016a\u20162 1\n1 0\n] ,\nand v\u03021 := u >u\u03031, v\u03022 := a >W>0 u\u03031. This implies that the vector [ v\u03021 v\u03022 ] belongs to the kernel of 2 \u00d7 2 matrix Mn(\u03bb\u0302). Hence, the relation between v\u03021 and v\u03022 is determined by( \u2016a\u20162a>W>0 Q0(\u03bb\u0302)u+ a>W > 0 Q0(\u03bb\u0302)W 0a ) v\u03021 + ( 1 + a>W>0 Q0(\u03bb\u0302)u ) v\u03022 = 0. (B.45)\nMoreover, by definition we have\n\u03bb\u0302u\u03031 = W 1W > 1 u\u03031 = W 0W > 0 u\u03031 + ( v\u03021\u2016a\u20162 + v\u03022 ) u+ v\u03021 \u00b7W 0a.\nWith \u2016u\u03031\u20162 = 1, the above implies that 1 = (( v\u03021\u2016a\u20162 + v\u03022 ) u> + v\u03021 \u00b7 a>W>0 ) Q0(\u03bb\u0302) 2 (( v\u03021\u2016a\u20162 + v\u03022 ) u+ v\u03021 \u00b7W 0a ) = ( v\u03021\u2016a\u20162 + v\u03022 )2 u>Q0(\u03bb\u0302) 2u+ v\u030221 \u00b7 a>W > 0 Q0(\u03bb\u0302) 2W 0a+ 2v\u03021 ( v\u03021\u2016a\u20162 + v\u03022 ) u>Q0(\u03bb\u0302) 2W 0a. (B.46)\nIn addition, multiplying \u03b2>\u2217 from the left hand side of (B.44) yields\nu\u0303>1 \u03b2\u2217 = \u2212 ( v\u03021 ( \u2016a\u20162\u03b2>\u2217 Q0(\u03bb\u0302)u+ \u03b2 > \u2217 Q0(\u03bb\u0302)W 0a ) + v\u03022 \u00b7 \u03b2>\u2217 Q0(\u03bb\u0302)u ) . (B.47)\nOur goal is to describe the asymptotic behavior of u\u0303>1 \u03b2\u2217 using (B.45), (B.46) and (B.47). As a side remark, our quantity of interest u\u0303>1 \u03b2\u2217 is different from the eigenvector alignments u\u0303 > 1 u addressed in prior works [BGN12], so we further introduce (B.47) for our purpose.\nCase I: \u03b81 > \u03c8 1/4 2 . We first consider the scenario \u03b81 > \u03c8 1/4 2 , where \u03bb\u0302\u2192 \u03bb0 in probability and \u03bb0 is outside the support of \u00b5MP\u03c82 . For sufficiently small > 0 and all large n, \u03bb\u0302 \u2208 \u2126 , and thus Lemma 20 gives\na>W>0 Q0(\u03bb\u0302)u\u2192 0, \u03b2 > \u2217 Q0(\u03bb\u0302)W 0a\u2192 0, u>Q0(\u03bb\u0302)2W 0a\u2192 0, a>W>0 Q0(\u03bb\u0302) 2W 0a\u2192 m(\u03bb0) + \u03bb0m\u2032(\u03bb0), u>Q0(\u03bb\u0302)2u\u2192 \u03b821m\u0304\u2032(\u03bb0), \u03b2>\u2217 Q0(\u03bb\u0302)u\u2192 \u03b82m\u0304(\u03bb0), a>W > 0 Q0(\u03bb\u0302)W 0a\u2192 1 + \u03bb0m(\u03bb0), u>Q0(\u03bb\u0302)u\u2192 \u03b821m\u0304(\u03bb0),\nin probability as n, d,N \u2192 \u221e proportionally. Notice that both v\u03021 and v\u03022 are uniformly bounded by some constants with high probability since \u2016a\u2016 \u2192 1, \u2016W 0\u2016 \u2192 (1 + \u221a \u03c82) almost surely, and Lemma 18 implies \u2016u\u2016 \u2192 \u03b81 in probability as n, d,N \u2192 \u221e. Therefore, when n/d \u2192 \u03c81 and N/d \u2192 \u03c82, (B.45) and (B.46) provide the limits of v\u03021 and v\u03022, which we denote by v1 and v2, respectively. More precisely,\nv2 = \u2212(1 + \u03bb0m(\u03bb0))v1, v21 = 1\n\u03b821m\u0304 \u2032(\u03bb0)\u03bb20m(\u03bb0) 2 +m(\u03bb0) + \u03bb0m\u2032(\u03bb0) .\nNow by (B.47), we know that\n(u\u0303>1 \u03b2\u2217) 2 P\u2192 \u03b822m\u0304(\u03bb0)2(v1 + v2)2 =\n\u03b822m\u0304(\u03bb0) 2\u03bb20m(\u03bb0) 2\n\u03b821m\u0304 \u2032(\u03bb0)\u03bb20m(\u03bb0) 2 +m(\u03bb0) + \u03bb0m\u2032(\u03bb0) .\nThus, we can apply formula (B.36), condition P (\u03bb0) = 0 in (B.43) and the following well-known facts of the Stieltjes transform (e.g., see [BS10]) with \u03bb0 = (1 + \u03b8 2 1)(\u03c82 + \u03b8 2 1)/\u03b8 2 1:\n\u03bb0m(\u03bb0) = \u22121 \u03b821 \u2212 1, m(\u03bb0) = \u22121 \u03c82 + \u03b821 ,\n\u03bb0m\u0304(\u03bb0) = \u2212\u03c82 \u03b821 \u2212 1, m\u0304(\u03bb0) = \u22121 1 + \u03b821 ,\n\u03bb0m \u2032(\u03bb0) =\n\u03b821(\u03b8 2 1 + 1)\n(\u03c82 + \u03b821)(\u03b8 4 1 \u2212 \u03c82)\n, \u03bb0m\u0304 \u2032(\u03bb0) =\n\u03b821(\u03b8 2 1 + \u03c82)\n(1 + \u03b821)(\u03b8 4 1 \u2212 \u03c82)\n,\nto conclude that asymptotic alignment between the linear component \u03b2\u2217 of the teacher model and leading left singular vector W\u0303 1 satisfies\n(u\u0303>1 \u03b2\u2217) 2 P\u2192 \u03b8\n2 2(\u03b8 4 1 \u2212 \u03c82) \u03b841(\u03b8 2 1 + 1) = \u03b822 \u03b821\n( 1\u2212 \u03c82 + \u03b8 2 1\n\u03b821(\u03b8 2 1 + 1)\n) ,\nas n, d,N \u2192\u221e proportionally and when \u03b841 > \u03c82. This establishes the alignment between \u03b2\u2217 and u\u03031. Now we return to the left singular vector u1 of the original W 1 using Davis-Kahan sin \u03b8 [Ste90, Theorem 4.4]:\n\u2016u1 \u2212 u\u03031\u2016 \u2264 \u221a\n2\u2016W 1 \u2212 W\u0303 1\u2016 \u03b4 \u2212 \u2016W 1 \u2212 W\u0303 1\u2016 ,\nwhere \u03b4 := s1(W\u0303 1) \u2212 s2(W\u0303 1). When \u03b841 > \u03c82, s1(W\u0303 1) will stay outside of the bulk whereas s2(W\u0303 1) will stick to right edge of the bulk. Therefore, \u03b4 has a uniform lower bound and eventually \u2016u1\u2212 u\u03031\u2016 \u2192 0, which implies that (u>1 \u03b2\u2217) 2 has the same limit as (u\u0303>1 \u03b2\u2217) 2.\nCase II: \u03b841 \u2264 \u03c82. On the other hand, if \u03b841 \u2264 \u03c82, we have proved that \u03bb\u0302 is approaching to the right-edge of the bulk of \u00b5MP\u03c82 . In fact, with probability one, \u03bb1 < \u03bb\u0302, where \u03bb1 is the largest eigenvalue of W 0W > 0 ; this\nis because detMn(z) = ( u>Q0(z)W 0a+ 1 )2 + u>Q0(z)u ( \u2016a\u20162 \u2212 a>W>0 Q0(z)W 0a ) satisfies\nlim z\u2192+\u221e detMn(z) = 1, lim z\u2198\u03bb1\ndetMn(z) = \u2212\u221e,\nwhile detMn(\u03bb\u0302) = 0. Hence by the definition of Mn(z), detMn(\u03bb\u0302) = ( 1 + a>W>0 Q0(\u03bb\u0302)u )2 \u2212 \u03bb\u0302 \u00b7 a>W>0 Q0(\u03bb\u0302)W 0a \u00b7 u>Q0(\u03bb\u0302)u = 0. (B.48)\nAlso, by the Cauchy\u2013Schwarz inequality, we have\n\u03bb\u0302 =\n( 1 + a>W>0 Q0(\u03bb\u0302)u )2 a>W>0 Q0(\u03bb\u0302)W 0a \u00b7 u>Q0(\u03bb\u0302)u \u2264 ( 1 +\n1\na>W>0 Q0(\u03bb\u0302)u\n)2 .\nSince \u03bb\u0302\u2192 (1 + \u221a \u03c82) 2, given any small \u2208 (0, \u221a \u03c82), for all sufficiently large N , we have\n(1 + \u221a \u03c82 \u2212 )2 \u2264 ( 1 +\n1\na>W>0 Q0(\u03bb\u0302)u\n)2 ,\nwhich indicates that a>W>0 Q0(\u03bb\u0302)u \u2208 ( 1 \u22122\u2212 \u221a \u03c82+ , 1\u221a \u03c82\u2212 ) . Therefore, for all large N , |a>W>0 Q0(\u03bb\u0302)u| is bounded by some universal constant related to \u03c82. Then by (B.48), we can conclude a >W>0 Q0(\u03bb\u0302)W 0a \u00b7 u>Q0(\u03bb\u0302)u is also asymptotically bounded by some constant. On the other hand, since all eigenvalues of W 0W > 0 is smaller than \u03bb\u0302, we obtain\n\u2212u>Q0(\u03bb\u0302)u > 1\n\u03bb\u0302 .\nThis directly implies \u2212a>W>0 Q0(\u03bb\u0302)W 0a has a constant upper bound for all large N . Following the proofs of [BGN11, Theorem 2.3] and [BGN12, Theorem 2.10] (with slight modifications of Lemma A.2 and Proposition A.3 in [BGN11]), it is straightforward to control the following quadratic forms by verifying the weak convergence of certain weighted spectral measures in combination with the Portmanteau theorem:\nlim inf n\u2192\u221e\nu>Q0(\u03bb\u0302) 2u \u2265 lim\nz\u2198(1+ \u221a \u03c82)2\n\u03b821m\u0304 \u2032(z) = +\u221e, (B.49)\nlim inf n\u2192\u221e\na>W>0 Q0(z) 2W 0a \u2265 lim\nz\u2198(1+ \u221a \u03c82)2\n( m(z) + zm\u2032(z) ) = +\u221e, (B.50)\nlim inf n\u2192\u221e \u2212a>W>0 Q0(z)W 0a \u2265 lim z\u2198(1+ \u221a \u03c82)2\n( \u2212 zm(z)\u2212 1 ) =\n1\u221a \u03c82 .\nConsequently, we know that \u2212a>W>0 Q0(z)W 0a \u2208 ( 1/ \u221a \u03c82, C(1 + \u221a \u03c82) 2 ) for some constant C > 0 and all\nlarge N . Hence \u2212u>Q0(\u03bb\u0302)u \u2208 ( 1/(1 + \u221a \u03c82) 2, C \u221a \u03c82 ) . Now notice that(\n\u03b2>\u2217 Q0(\u03bb\u0302)u )2 \u2264 ( \u2212\u03b2>\u2217 Q0(\u03bb\u0302)\u03b2\u2217 )( \u2212u>Q0(\u03bb\u0302)u ) .\nIn addition, by definition we have\n\u2212a>W>0 Q0(z)W 0a = \u2212\u2016a\u20162 + \u03bb\u0302a> ( \u03bb\u0302I \u2212W>0 W 0 )\u22121 a.\nBy the same rotation invariance argument as in Lemma 20, we may assume \u03b2\u2217 \u223c Unif(Sd\u22121). Thus we can apply the Hanson-Wright inequality for a and \u03b2\u2217 to show that \u2212\u03b2 > \u2217 Q0(\u03bb\u0302)\u03b2\u2217 and a > ( \u03bb\u0302I \u2212W>0 W 0 )\u22121 a have comparable limits as N,n, d \u2192 \u221e proportionally. This directly implies that \u2212\u03b2>\u2217 Q0(\u03bb\u0302)\u03b2\u2217 also has a uniform upper bound for all large d. We conclude that as n, d,N \u2192 \u221e proportionally, |\u03b2>\u2217 Q0(\u03bb\u0302)u| is eventually bounded by some constant from above.\nOn the other hand, as n/d\u2192 \u03c81 and N/d\u2192 \u03c82, from (B.46), (B.49) and (B.50) we know that v\u03021, v\u03022 \u2192 0. This allows us to conclude that u\u0303>1 \u03b2\u2217 \u2192 0. Finally, to translate the result back to u1 which is the left singular vector of W 1, we denote R := W 1W > 1 \u2212 W\u0303 1W\u0303 > 1 . Recall that Lemma 14 ensures \u2016R\u2016 \u2192 0 almost surely. Hence, we may repeat above computations for \u03bb\u0302u1 = W 1W > 1 u1, where with a slight abuse of notation we still denote \u03bb\u0302 as the largest eigenvalue of W 1W > 1 . In this case, (B.47) needs to be modified as\nu>1 \u03b2\u2217 = \u2212 ( v\u03021 ( \u2016a\u20162\u03b2>\u2217 Q0(\u03bb\u0302)u+ \u03b2 > \u2217 Q0(\u03bb\u0302)W 0a ) + v\u03022 \u00b7 \u03b2>\u2217 Q0(\u03bb\u0302)u ) \u2212 \u03b2>\u2217 Q0(\u03bb\u0302)Ru1.\nSince \u2016R\u2016 \u2192 0, by a simple adaptation of Lemma A.2 in [BGN11], one can directly verify \u03b2>\u2217 Q0(\u03bb\u0302)Ru1 \u2192 0, as N,n, d \u2192 \u221e proportionally. Hence we conclude that u>1 \u03b2\u2217 \u2192 0 if \u03b841 \u2264 \u03c82. The theorem is established by combining the above cases."
        },
        {
            "heading": "C Proof for Small Learning Rate (\u03b7 = \u0398(1))",
            "text": ""
        },
        {
            "heading": "C.1 Gaussian Equivalence for Trained Feature Map",
            "text": "The Gaussian Equivalence Property. To validate Theorem 5, we follow the proof strategy of [HL20], which established the GET for RF models using the Lindeberg approach and leave-one-out arguments [EK18]. We remark that concurrent to our work, [MS22] proved the Gaussian equivalence property for a larger model class under an assumed central limit theorem, which is verified for two-layer RF or NTK models, and thus cannot directly imply our results on the trained features.\nWe first introduce the notations used in this section. Given weight matrix W and input x, we define the feature vector \u03c6x = 1\u221a N \u03c3(W>x) \u2208 RN ; similarly, given training data matrix X\u0303 \u2208 Rn\u00d7d, the kernel feature matrix is given as \u03a6 = 1\u221a N \u03c3(X\u0303W ) \u2208 Rn\u00d7N . Also, the linearized noisy Gaussian feature can be written as: \u03c6\u0304x = 1\u221a N ( \u00b51W >x+ \u00b52z ) , and the corresponding matrix \u03a6\u0304 = 1\u221a N ( \u00b51X\u0303W + \u00b52Z ) , where z, [Z]i i.i.d.\u223c N (0, I) for i \u2208 [n]. We emphasize that in our analysis W does not depend on X\u0303; for notational simplicity, in this subsection we omit the accent in X\u0303.\nWe establish the Gaussian equivalence property (Theorem 5) for kernel regression with respect to certain trained feature map under general convex loss ` satisfying Assumption (A.4) in [HL20]. Consider the estimators obtained from `2-regularized empirical risk minimization:\na\u0302 , arg mina { 1 n n\u2211 i=1 `(yi, \u3008a,\u03c6i\u3009) + \u03bb N \u2016a\u201622 } , (C.1)\na\u0304 , arg mina { 1 n n\u2211 i=1 `(yi, \u3008a, \u03c6\u0304i\u3009) + \u03bb N \u2016a\u201622 } , (C.2)\nwhere we abbreviated \u03c6i = \u03c6xi = 1\u221a N \u03c3(W>xi), \u03c6\u0304i = \u03c6\u0304xi = 1\u221a N\n( \u00b51W >xi + \u00b52zi ) for i \u2208 [n].\nIn our setting, the first-layer weight W is no longer the initialized random matrix W 0. However, we can still write the weight matrix as a perturbed version of W 0, i.e., W = W 0 +\u2206, where \u2206 \u2208 Rd\u00d7N corresponds to the update to the weights (possibly multiple gradient steps as in Lemma 16) that is independent of the training data X for ridge regression (e.g., the weight matrix and the ridge regression estimator are trained on separate data). We aim to show that under suitable conditions on \u2206, the Gaussian equivalence theorem holds for the kernel model defined by the perturbed features x \u2192 1\u221a\nN \u03c3(x>W ). Throughout this section,\nwe take W to be the trained first layer W t for t \u2208 N according to (2.1). Define the set of weight matrices perturbed from the Gaussian initialization W 0 as\nW := { W = W 0 + \u2206 \u2208 Rd\u00d7N : \u2016\u2206\u2016 = O(1), \u2016\u2206\u20162,\u221e = O ( polylog d\u221a\nd\n)} . (C.3)\nNote that for learning rate \u03b7 = \u0398(1), we can verify thatW is a high-probability event after any finite number of gradient steps, as characterized in Lemma 14 and 16. The following proposition is a reformulation and extension of [HL20, Theorem 1], stating that the Gaussian equivalence property holds as long as W remains \u201cclose\u201d to the initialization W 0.\nProposition 22. Under Assumptions 1 and 2, and P(W) \u2265 1 \u2212 exp ( \u2212c log2N ) for some c > 0, we have that as n, d,N \u2192\u221e proportionally,\nEx(f\u2217(x)\u2212 \u3008\u03c6x, a\u0302\u3009) 2 = (1 + od,P(1)) \u00b7 Ex ( f\u2217(x)\u2212 \u3008\u03c6\u0304x, a\u0304\u3009 )2 ,\nwhere a\u0302 and a\u0304 are defined in (C.1) and (C.2).\nFrom Proposition 22 we know that Theorem 5 holds if the optimized weight matrix W falls into the set W with sufficiently high probability. This condition is in turn verified by Lemma 14 and 16. Also note that in our setting of MSE loss and \u03bb > 0, the RHS of the above equation is bounded in probability.\nCentral Limit Theorem for Trained Features. Recall the single-index teacher assumption: yi = \u03c3\u2217(\u3008xi,\u03b2\u2217\u3009) + \u03b5i for i \u2208 [n]. Observe that for W \u2208 W, the following near-orthogonality condition between the neurons holds with high probability\n\u2016W \u2016 = O(1), and max i6=j {\u3008wi,wj\u3009, \u3008wi,\u03b2\u2217\u3009} = O\n( polylog d\u221a\nd\n) . (C.4)\nImportantly, for W satisfying the near-orthogonality condition (C.4), we can utilize the following central limit theorem from [HL20] derived via Stein\u2019s method.\nProposition 23 (Theorem 2 in [HL20]). Given Assumptions 1 and 2, suppose that the activation \u03c3 is an odd function. Let {\u03d5d(x; y)} be a sequence of two-dimensional test functions, where |\u03d5d(x; y)|, |\u03d5\u2032d(x; y)| \u2264 Bd(y)(1 + |x|)K for some function Bd and constant K \u2265 1, then for W satisfying (C.4), and fixed vectors \u03b1 \u2208 RN ,\u03b2 \u2208 Rd with \u2016\u03b2\u2016 = 1, we have\u2223\u2223\u2223E\u03d5d(\u03c6>x\u03b1;x>\u03b2)\u2212 E\u03d5d(\u03c6\u0304>x\u03b1;x>\u03b2)\u2223\u2223\u2223 = O(polylogN\u221a\nN E[Bd(z)4]1/4\n( 1 + \u2016\u03b1\u20162\u221e + ( 1\u221a N \u2016\u03b1\u2016 )K\u2032)) ,\nwhere z \u223c N (0, 1), and K \u2032 only depends on constant K.\nWe remark that our assumption of odd activation in Theorem 5 is required by the above Proposition 23, and we believe it could be removed with some extra work. Also, to verify the GET, we take \u03d5d to be the test function defined in [HL20, Equation (50)]. In our case, by [HL20, Lemma 25] we know that there exists a function B satisfying the growth condition such that E[B(z)4] is bounded. Therefore, in order to apply Proposition 23 and obtain the Gaussian equivalence theorem (see derivation in [HL20, Section 2]), we only need to control the `2-norm and `\u221e-norm of certain vector \u03b1 of interest. The following subsection establishes the required norm bound.\nNorm Control Along the Interpolation Path. Following [HL20], we construct an interpolating sequence between the nonlinear and linear features model. For any 0 \u2264 k \u2264 n, we define\ng\u2217k , arg ming\u2208RN { k\u2211 i=1 `(yi, \u3008g, \u03c6\u0304i\u3009) + n\u2211 j=k+1 `(yj , \u3008g,\u03c6j\u3009) + n N ( \u03bb\u2016g\u201622 +Q(g) )} , (C.5)\nwhere we introduce a perturbation term Q(g) , \u03b31g > ( \u00b521W >W + \u00b522I ) g + \u03b32\u00b51 \u221a N\u03b2>\u2217Wg.\nNote that when \u03b31 = \u03b32 = 0, setting k = 0 recovers the estimator on nonlinear features a\u0302, and similarly, setting k = n gives the estimator on the linear Gaussian features a\u0304.\nWe remark that the perturbation Q(g) allows us to compute the prediction risk by taking the derivative of the objective w.r.t. \u03b31, \u03b32 around 0 \u2014 see [HL20, Proposition 1] for details. Note that when \u2016W \u2016 = \u0398(1), we may choose \u03b3\u2217 = Nn \u00b7 \u03bb/4\n\u00b521\u2016W \u2016 2+\u00b522 > 0 such that for |\u03b31| \u2264 \u03b3\u2217, |\u03b32| \u2264 1, the overall objective (C.5) is \u03bb 2 -strongly convex (i.e., the strongly-convex regularizer dominates the concave part of Q(g) when \u03b31 < 0).\nWhile most of the statements in [HL20] hold for deterministic weight matrices satisfying (C.4), the `\u221e- norm bound relies on the (sub-)Gaussian property of W and thus only applies to RF models. The following lemma establishes a high probability upper bound on the `\u221e-norm of g \u2217 k on our trained feature map.\nLemma 24. Given Assumptions 1 and 2, if we further assume that 1 \u2212 P(W) \u2264 exp ( \u2212c log2N ) for some constant c > 0, then there exists some constant c\u2032 > 0 such that for any 0 \u2264 k \u2264 n,\nP(\u2016g\u2217k\u2016\u221e \u2265 polylogN) \u2264 exp ( \u2212c\u2032 log2N ) .\nProof. We follow the proof of [HL20, Lemma 23] and first analyze one coordinate of g\u2217k defined by (C.5), which WLOG we select to be the last coordinate. For concise notation, we instead augment the weight matrix with an (N + 1)-th column and study the corresponding [g\u2217k]N+1. Denote the weight vector wN+1 = w0N+1 + \u03b4N+1, where w 0 N+1 is the (N + 1)-th column of the initialized W 0, and \u03b4 is the perturbation (i.e., gradient update for W ).\nTo further simplify the notation, we define ri \u2208 RN , where ri = 1\u221aN ( \u00b51W >xi + \u00b52zi ) , zi\ni.i.d.\u223c N (0, I) for i \u2264 k, and ri = 1\u221aN \u03c3(W\n>xi) for k < i \u2264 n. Recall that W = W 0 + \u2206, in which the initialization [W 0]i,j = N (0, d\u22121); we denote the i-th feature vector at initialization W 0 by r0i . In addition, we define f \u2208 Rn to represent the feature vector at the last coordinate, i.e., fi = [f ]i = 1\u221aN ( \u00b51x > i wN+1 + \u00b52zi ) , zi i.i.d.\u223c N (0, 1) for i \u2264 k, and fi = [f ]i = 1\u221aN \u03c3(x > i wN+1) for k < i \u2264 n; similarly, we introduce a superscript in f0 \u2208 Rn to denote the features produced by the initial w0N+1. The (N + 1)-th coordinate of interest, which we denote as u\u2217, can be written as the solution to the following optimization problem,\nu\u2217 = argmin u min g n\u2211 i=1 ` ( r>i g + fiu; yt ) + n N ( \u03bb\u2016g\u20162 +Q(g) + \u03bbu2 + q(u) + ( 2\u03b31\u00b5 2 1w > N+1Wg ) u ) ,\nwhere we defined q(u) = \u03b31 ( \u00b521\u2016wN+1\u2016 2 + \u00b522 ) u2 + \u03b32 ( \u00b51 \u221a N\u03b2>\u2217 wN+1 ) u.\nBy [HL20, Equation (249)], we know that for W \u2208 W,\n|u\u2217| . 1 \u03bb \u2223\u2223\u2223\u2223\u22232\u03b31\u00b521w>N+1Wg\u2217k + \u03b32\u00b51\u221aN\u03b2>\u2217 wN+1 + n\u2211 i=1 `\u2032 ( r>i g \u2217 k; yi ) fi \u2223\u2223\u2223\u2223\u2223. (C.6) We control each term on the right hand side of (C.6) separately. Note that W \u2208 W implies that \u2016\u03b4N+1\u2016 = O (\npolylogd\u221a d\n) due to the definition (C.3). Since \u2223\u2223\u2223\u03b2>\u2217 wN+1\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u03b2>\u2217 w0N+1\u2223\u2223\u2223+\u2016\u03b4N+1\u2016\u2016\u03b2\u2217\u2016, by combining [HL20, Equation (252)] and our assumption that \u2016\u03b2\u2217\u2016 = 1, we know that for some constant c1 > 0 and large N ,\nP (\u2223\u2223\u2223\u221aN\u03b2>\u2217 wN+1\u2223\u2223\u2223 \u2265 polylogN) \u2264 exp(\u2212c1 log2N).\nSimilarly, \u2223\u2223w>N+1Wg\u2217k\u2223\u2223 \u2264 \u2223\u2223\u2223w0>N+1Wg\u2217k\u2223\u2223\u2223 + \u2016\u03b4N+1\u2016\u2016Wg\u2217k\u2016, and therefore by [HL20, Lemma 17] (note that the lemma only requires W to satisfy (C.4)), we have\nP (\u2223\u2223w>N+1Wg\u2217k\u2223\u2223 \u2265 polylogN) \u2264 exp(\u2212c2 log2N),\nfor some constant c2 > 0.\nTo control the sum of `\u2032 in (C.6), for simplicity we define \u03b8\u2217 \u2208 Rn, where \u03b8\u2217i = [\u03b8 \u2217]i = ` \u2032(r>i g\u2217k; yi) for i \u2208 [n]. Notice that\n\u2223\u2223x>i wj \u2212 x>i w0j \u2223\u2223 = \u2223\u2223x>i \u03b4j\u2223\u2223. Due to the assumed independence between X and \u2206, and the assumption on P(W), we know that\n\u2223\u2223x>i \u03b4j\u2223\u2223 . \u2016\u03b4j\u2016 \u00b7 logN = O(polylogN\u221aN ) with high probability. In addition, since the activation function \u03c3 is Lipschitz, for k < i \u2264 n, we may take a union bound over the weight vectors wj and obtain\nP (\u221a N \u2225\u2225ri \u2212 r0i\u2225\u2225 \u2265 polylogN) \u2264 N \u00b7 exp(\u2212c3 log2N), (C.7)\nfor some c3 > 0. The case where i \u2264 k (i.e., the features are linear) follows from the exact same argument. Also, because of \u2223\u2223r>i g\u2217k\u2223\u2223 \u2264 \u2223\u2223r0>i g\u2217k\u2223\u2223+ \u2225\u2225ri \u2212 r0i\u2225\u2225\u2016g\u2217k\u2016, we know that [HL20, Equation (257)], [HL20, Lemma 17], and (C.7) together ensure that\nP(|\u03b8\u2217i | \u2265 polylogN) \u2264 exp ( \u2212c4 log2N ) , (C.8)\nfor some constant c4 > 0 and large enough N . Now we can control \u2223\u2223\u2211n i=1 ` \u2032(r>i g\u2217k; yi)fi\u2223\u2223 in (C.6). Again using the Lipschitz property of activation \u03c3, we get\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 `\u2032 ( r>i g \u2217 k; yi ) (fi \u2212 f0i + f0i )\n\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 `\u2032 ( r>i g \u2217 k; yi ) f0i \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 `\u2032 ( r>i g \u2217 k; yi )( fi \u2212 f0i )\u2223\u2223\u2223\u2223\u2223 .\n\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b8\u2217i f 0 i \u2223\u2223\u2223\u2223\u2223+ 1\u221aN n\u2211 i=1 |\u03b8\u2217i | \u00b7 \u2223\u2223x>i \u03b4N+1\u2223\u2223.\nGiven (C.8) (which implies that 1\u221a N \u2016\u03b8\u2217\u2016 = O(polylogN) with high probability), it has been shown in [HL20, Proof of Lemma 23] that P (\u2223\u2223\u2211n\ni=1 \u03b8 \u2217 i f 0 i \u2223\u2223 \u2265 polylogN) \u2264 exp(\u2212c5 log2N) for some constant c5 > 0. Hence, by taking union bound over the failure events |\u03b8\u2217i | \u2265 polylogN and \u221a N \u00b7\n\u2223\u2223x>i \u03b4N+1\u2223\u2223 \u2265 polylogN , we arrive at the following high probability upper bound on u\u2217 in terms of (C.6):\nP(|u\u2217| \u2265 polylogN) \u2264 n2N \u00b7 exp ( \u2212c6 log2N ) ,\nfor some constant c6 > 0 and all large N . Finally, since the assumption on \u2016\u2206\u20162,\u221e implies control of \u2016\u03b4i\u2016 for all i \u2208 [N ], we complete the proof by a union bound over the N coordinates.\nPutting Things Together. Denote the optimal value of objective (C.5) by\nR\u2217k , ming\u2208RN\n{ 1\nn k\u2211 i=1 `(yi, \u3008g, \u03c6\u0304i\u3009) + 1 n n\u2211 j=k+1 `(yj , \u3008g,\u03c6j\u3009) + 1 N ( \u03bb\u2016g\u201622 +Q(g)\n)}\nFrom [HL20, Section 2.3], we know that Proposition 23 and Lemma 24 imply that for any W \u2208 W and 1 \u2264 k \u2264 n, the discrepancy due to one swap can be bounded as\u2223\u2223\u2223E[\u03c8(R\u2217k)]\u2212 E[\u03c8(R\u2217k\u22121)]\u2223\u2223\u2223 = O(polylogNN3/2 ) ,\nfor bounded test function \u03c8 with bounded first and second derivatives. As there are n = \u0398(N) total swaps to be made, we can obtain the desired Gaussian equivalence ([HL20, Theorem 1]) if the failure probability (1\u2212 P(W)) is sufficiently small. Hence we can conclude Proposition 22. Proof of Theorem 5. Finally, we establish Theorem 5 by verifying that in our setting the eventW occurs with high probability. For one gradient step on the squared loss with learning rate \u03b7 = \u0398(1), Lemma 14 together with \u2016\u03b2\u2217\u2016 = 1 entail that for proportional n, d,N , there exists some constant c, C > 0 such that\nP(\u2016W 1\u2016 \u2265 C) \u2264 exp(\u2212cd), P (\nmax i 6=j \u2223\u2223\u3008w1i ,w1j \u3009\u2223\u2223 \u2265 C log2 d\u221a d ) \u2264 exp ( \u2212c log2 d ) ,\nP (\nmax i \u2223\u2223\u3008w1i ,\u03b2\u2217\u3009\u2223\u2223 \u2265 C log2 d\u221a d ) \u2264 exp ( \u2212c log2 d ) ,\nwhere w1i stands for the i-th column of W 1 for i \u2208 [N ]. For multiple gradient steps with \u03b7 = \u0398(1), Lemma 16 implies similar tail probability bounds. In addition, under Assumptions 1 and 2, when \u03bb > 0, it is straightforward to verify that prediction risk of the Gaussian equivalent model RGE(\u03bb) P\u2192 C\u03bb for some finite constant C\u03bb > 0 as n,N, d\u2192\u221e proportionally. Theorem 5 therefore follows from Proposition 22 (or equivalently, Equation (16) in [HL20, Theorem 1])."
        },
        {
            "heading": "C.2 Prediction Risk of the Gaussian Equivalent Model",
            "text": "Now we compute the prediction risk of the CK ridge estimator on the feature map after one gradient step x\u2192 \u03c3(W>1 x). We restrict ourselves to the squared loss, the optimal solution of which is given by:\na\u0302 = arg mina =\n( \u03a6>\u03a6 + \u03bbn\nN I\n)\u22121 \u03a6>y\u0303,\nwhere \u03a6 = 1\u221a N \u03c3(X\u0303W 1) \u2208 Rn\u00d7N , X\u0303 \u2208 Rn\u00d7d denotes a new batch of training data independent of W 1, and y\u0303 = f\u2217(X\u0303) + \u03b5\u0303 \u2208 Rn is the corresponding training labels (following the same Assumption 1). Also, recall the following Gaussian covariates model:\n\u03a6\u0304 , 1\u221a N\n( \u00b51X\u0303W 1 + \u00b52Z ) \u2208 Rn\u00d7N ; a\u0304 , ( \u03a6\u0304 > \u03a6\u0304 + \u03bbn\nN I\n)\u22121 \u03a6\u0304 > y\u0303.\nwhere [Z]ij \u223c N (0, 1) independent of X\u0303 and W 1. Due to the Gaussian equivalence property (4.2), we can analyze the prediction risk of the Gaussian covariates model, which we denote as RGE(\u03bb).\nBias-variance Decomposition. The following lemma simplifies the prediction risk RGE(\u03bb) and separates the bias (due to learning the teacher f\u2217) and variance (due to the label noise \u03b5\u0303).\nLemma 25. Under Assumptions 1 and 2, we have\nRGE(\u03bb)\u2212 (B1 +B2 + V ) P\u2192 0,\nwhere the bias and variance terms are given as\nB1 = \u00b5 \u22172 1 + \u00b5 \u22172 2 \u2212\n2\u00b51\u00b5 \u2217 1\u221a\nN \u03b2>\u2217W 1\n( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0304 > f\u2217. (C.9)\nB2 = f \u2217>\u03a6\u0304 ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0304 > f\u2217. (C.10)\nV = \u03c32\u03b5 Tr (( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a3\u0302\u03a6 ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a3\u03a6 ) . (C.11)\nand we defined \u03bb\u0303 = \u03bbnN , \u03a3\u0302\u03a6 = \u03a6\u0304 > \u03a6\u0304,\u03a3\u03a6 = 1 N ( \u00b521W > 1 W 1 + \u00b5 2 2I ) , and [f\u2217]i = f \u2217(x\u0303i).\nProof. First note that RGE is given by [HL20, Equation (57)]:\nRGE = Ex ( \u03c3\u2217(x>\u03b2\u2217)\u2212 \u03c6\u0304 > x a\u0304 )2\n=Ez1,z2 (\u03c3\u2217(z1)\u2212 \u00b51\u221a N \u03b2>\u2217W 1a\u0304 \u00b7 z1 + \u221a 1 N a\u0304>(\u00b521W > 1 W 1 + \u00b5 2 2I \u2212 \u00b521W > 1 \u03b2\u2217\u03b2 > \u2217W 1)a\u0304 \u00b7 z2 )2(C.12) where z1, z2\ni.i.d.\u223c N (0, 1). Because of the independence between z1, z2, we only need to show the following as n, d,N \u2192\u221e proportionally:\n1\u221a N \u03b2>\u2217W 1\n( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0304 > \u03b5\u0303 P\u2192 0,\nf\u2217>\u03a6\u0304 ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0304 > \u03b5\u0303 P\u2192 0.\nBoth equations directly follow from the general Hoeffding inequality for \u03b5\u0303 (e.g., see Theorem 2.6.3 [Ver18])\nsince both \u2225\u2225\u2225\u2225\u03b2>\u2217W 1(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u03a6\u0304>\u2225\u2225\u2225\u2225 and \u2225\u2225\u2225\u2225\u221aN \u00b7 f\u2217>\u03a6\u0304(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u03a3\u03a6(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u03a6\u0304>\u2225\u2225\u2225\u2225 are bounded by some constant with high probability when \u03bb > 0.\nAlso, the risk lower bound for the Gaussian equivalent model is a direct consequence of (C.12).\nProof of Fact 6. Under Assumptions 1 and 2, we may write \u03c3\u2217(z) = \u00b5\u22171z + \u03c3 \u2217 \u22a5(z), where Ez[z\u03c3\u2217\u22a5(z)] = 0,Ez[\u03c3\u2217\u22a5(z)2] = \u00b5\u221722 for z \u223c N (0, 1). Hence from (C.12) we know that\nRGE \u2265 Ez1 ( \u03c3\u2217(z1)\u2212\n\u00b51\u221a N \u03b2>\u2217W 1a\u0304 \u00b7 z1\n)2 = ( \u00b5\u22171 \u2212\n\u00b51\u221a N \u03b2>\u2217W 1a\u0304\n)2 + \u00b5\u221722 .\nThis implies that RGE \u2265 \u2016P>1f\u2217\u20162L2 = \u00b5\u221722 with probability one as d\u2192\u221e.\nIn the following sections, we compare the bias and variance terms given in (C.9), (C.10) and (C.11) before and after one feature learning step. We first simplify the calculation by showing that the values of these equations remain asymptotically unchanged if we remove certain low-order terms.\nStability of the Bias and Variance. We now control the errors in the bias and variance terms after ignoring the lower-order terms in the weight matrix.\nRecall that W 1 = W 0 + \u03b7 \u221a NG0; we introduce W\u0303 := W 0 + \u03b7 \u221a NA, in which we ignored the terms B and C in the gradient matrix (B.1). We also denote the corresponding CK features and kernel matrix\nas \u03a6\u0303 := 1\u221a N\n( \u00b51X\u0303W\u0303 + \u00b52Z ) , \u03a3\u0303\u03a6 := \u03a6\u0303 > \u03a6\u0303, and the bias terms as B\u03031, B\u03032 (parallel to (C.9) and (C.10)).\nFinally, we write the initial random feature matrix as \u03a6\u03040 := 1\u221a N\n( \u00b51X\u0303W 0 + \u00b52Z ) , \u03a3\u0302\u03a60 := \u03a6\u0304 > 0 \u03a6\u03040, and\nrefer to the variance of the initialized RF ridge estimator as V0.\nLemma 26. Given Assumptions 1, 2 and \u03bb > 0. Then for \u03b7 = \u0398(1), we have\n|B1 \u2212 B\u03031| = od,P(1), |B2 \u2212 B\u03032| = od,P(1), |V \u2212 V0| = od,P(1).\nProof. To start with, recall that the operator norms of all matricesW 1,W 0, W\u0303 , \u03a6\u0304, \u03a6\u03040 and \u03a6\u0303 are uniformly bounded by some constants with high probability. We first consider the change in Frobenius norm of firstlayer W to analyze the difference between V and V0. By Lemma 14, standard calculation yields:\u2225\u2225\u2225W>1 W 1 \u2212W>0 W 0\u2225\u2225\u2225\nF = Od,P(1); \u2225\u2225\u03a6\u0304\u2212 \u03a6\u03040\u2225\u2225F = Od,P(1); \u2225\u2225\u2225\u03a3\u0302\u03a6 \u2212 \u03a3\u0302\u03a60\u2225\u2225\u2225F = Od,P(1). Utilizing the above estimates, we obtain\u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u03a6\u0304> \u2212 (\u03a3\u0302\u03a60 + \u03bb\u0303I)\u22121\u03a6\u0304>0 \u2225\u2225\u2225\u2225\nF \u2264 \u2225\u2225\u03a6\u0304\u2212 \u03a6\u03040\u2225\u2225F\u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u2225\u2225\u2225\u2225+ \u2225\u2225\u03a6\u03040\u2225\u2225\u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121 \u2212 (\u03a3\u0302\u03a60 + \u03bb\u0303I)\u22121\u2225\u2225\u2225\u2225\nF\n(i) = Od,P(1).\nwhere (i) is due to our assumption that \u03bb > 0. Denote M := ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0304 >\nand likewise M0 :=( \u03a3\u0302\u03a60 + \u03bb\u0303I )\u22121 \u03a6\u0304 > 0 . Then we have\n|V \u2212 V0| (ii) . 1\nN \u2223\u2223\u2223Tr(MM>(\u00b521W>1 W 1 + \u00b522I)\u2212M0M>0 (\u00b521W>0 W 0 + \u00b522I))\u2223\u2223\u2223 . 1\nN \u2225\u2225\u2225M0M>0 \u2225\u2225\u2225 F \u00b7 \u2225\u2225\u2225W>1 W 1 \u2212W>0 W 0\u2225\u2225\u2225 F + 1 N \u2225\u2225\u2225MM> \u2212M0M>0 \u2225\u2225\u2225 F \u00b7 \u2225\u2225\u2225\u00b521W>1 W 1 + \u00b522I\u2225\u2225\u2225 F = od,P(1),\nas n, d,N \u2192\u221e at comparable rate, where we dropped the constant \u03c32\u03b5 in (ii). For the bias terms, we consider perturbation on W 1 in the operator norm. Again, Lemma 14 entails that\u2225\u2225\u2225W>1 W 1 \u2212 W\u0303>W\u0303\u2225\u2225\u2225 = od,P(1); \u2225\u2225\u2225\u03a6\u0304\u2212 \u03a6\u0303\u2225\u2225\u2225 = od,P(1); \u2225\u2225\u2225\u03a3\u0302\u03a6 \u2212 \u03a3\u0303\u03a6\u2225\u2225\u2225 = od,P(1). Define M\u0303 := ( \u03a3\u0303\u03a6 + \u03bb\u0303I )\u22121 \u03a6\u0303 > . Following the same procedure, we obtain the operator norm control\n\u2225\u2225\u2225M \u2212 M\u0303\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u03a6\u0304> \u2212 (\u03a3\u0303\u03a6 + \u03bb\u0303I)\u22121\u03a6\u0303>\u2225\u2225\u2225\u2225 \u2264 \u2016\u03a6\u0304\u2212 \u03a6\u0303\u2016\n\u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121\u2225\u2225\u2225\u2225+ \u2016\u03a6\u0303\u2016\u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bb\u0303I)\u22121 \u2212 (\u03a3\u0303\u03a6 + \u03bb\u0303I)\u22121\u2225\u2225\u2225\u2225 = od,P(1). Based on this result, it is straightforward to show that\n|B1 \u2212 B\u03031| . \u2225\u2225\u2225W 1 \u2212 W\u0303\u2225\u2225\u2225\u2016M\u0303\u2016+ \u2016W 1\u2016\u2225\u2225\u2225M \u2212 M\u0303\u2225\u2225\u2225 = od,P(1).\nSimilarly, for B2, we have\n|B2 \u2212 B\u03032| . 1 N \u2016f\u2217\u20162 \u00b7 \u2225\u2225\u2225M>(\u00b521W>1 W 1 + \u00b522I)M \u2212 M\u0303>(\u00b521W\u0303>W\u0303 + \u00b522I)M\u0303\u2225\u2225\u2225 (iii)\n. Od,P(1) \u00b7 ( (\u2016M\u2016+ \u2016M\u0303\u2016) \u2225\u2225\u2225W>1 W 1\u2225\u2225\u2225\u2225\u2225\u2225M \u2212 M\u0303\u2225\u2225\u2225+ \u2016M\u0303\u20162\u2225\u2225\u2225W>1 W 1 \u2212 W\u0303>W\u0303\u2225\u2225\u2225) = od,P(1),\nwhere in (iii) we used the fact that \u03c3\u2217 is Lipschitz and \u2016\u03b2\u2217\u2016 = 1 (for example see [BMR21, Lemma A.12]). The statement is proved by combining all the above calculations.\nLemma 26 entails that the variance term in the risk does not change after one gradient step with \u03b7 = \u0398(1), and for the bias terms, we may consider the rank-1 approximation of the gradient matrix studied in Proposition 2 instead. In the following section, we use this property to simplify the risk expressions."
        },
        {
            "heading": "C.3 Precise Characterization of Prediction Risk",
            "text": "Now we compute the asymptotic expressions of the bias and variance terms defined in Lemma 25. As previously remarked, due to the dependence between the feature matrix \u03a6 and the teacher \u03b2\u2217, we cannot naively employ a rotation invariance argument to simplify the calculation (as in [MM22]). Instead, based on the Gaussian equivalence property, we first make use of the Woodbury formula to separate the low-rank terms in the risk expressions. In particular, because of Lemma 14 and Lemma 26, we may simply consider the rank-one approximation of the first-step gradient: W 1 = W 0+ua >, where u = \u00b51\u03b7n X >y and y = f\u2217(X)+\u03b5 satisfying Assumptions 1 and 2. Notice here u, X\u0303, W 0 and a are mutually independent. To distinguish the terms in the CK ridge regression estimator using the initial weights W 0 and the trained weights W 1, in this section we denote\n\u03a6\u0304 := 1\u221a N\n( \u00b51X\u0303W 1 + \u00b52Z ) , \u03a60 :=\n1\u221a N\n( \u00b51X\u0303W 0 + \u00b52Z ) ,\n\u03a3\u0302\u03a6 := \u03a6\u0304 > \u03a6\u0304, \u03a3\u0302\u03a60 := \u03a6 > 0 \u03a60 \u2208 RN\u00d7N , R := ( \u03a3\u0302\u03a6 + \u03bb\u0303I )\u22121 , R0 := ( \u03a3\u0302\u03a60 + \u03bb\u0303I )\u22121 ,\n\u03a3\u03a6 := 1\nN\n( \u00b521W > 1 W 1 + \u00b5 2 2I ) , \u03a3\u03a60 := 1\nN\n( \u00b521W > 0 W 0 + \u00b5 2 2I ) .\n(C.13)\nAlso, we write f\u2217 := f\u2217(X\u0303) \u2208 Rn, which can be decomposed into\nf\u2217 = \u00b5\u22171X\u0303\u03b2\u2217 + f \u2217 NL, (C.14)\nwhere [f\u2217NL]i = P>1f \u2217(x\u0303i) (recall that \u00b5 \u2217 0 = 0 by Assumption 2). Furthermore, we introduce the following terms which will be important in the decomposition of the prediction risk:\nT1 := a >R0a, T2 := \u00b521 N u>X\u0303 > \u03a60R0\u03a6 > 0 X\u0303u, T3 := \u00b521 N u>X\u0303 > X\u0303u, T4 := \u00b5 \u2217 1\u03b2 > \u2217 u, T5 := \u00b521\u00b5 \u2217 1\nN \u03b2>\u2217 X\u0303 > X\u0303u, T\u03035 := \u00b521 N f\u2217>NLX\u0303u,\nT6 := \u00b51\u00b5\n\u2217 1\n2 \u221a N \u03b2>\u2217\n( W 0R0\u03a6 > 0 X\u0303 + X\u0303 > \u03a60R0W > 0 ) u, T\u03036 := \u00b51\n2 \u221a N f\u2217>NL\u03a60R0W > 0 u, (C.15)\nT7 := \u00b521\u00b5 \u2217 1\nN u>X\u0303\n> \u03a60R0\u03a6 > 0 X\u0303\u03b2\u2217, T\u03037 := \u00b521 N u>X\u0303 > \u03a60R0\u03a6 > 0 f \u2217 NL,\nT8 := N\n\u00b521 a>R0\u03a3\u03a60R0a, T9 :=\n\u00b51 2 \u221a N u> ( W 0R0\u03a6 > 0 X\u0303 + X\u0303 > \u03a60R0W > 0 ) u,\nT11 := u >X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 X\u0303u, T10 := \u2016u\u20162,\nT12 := \u00b5 \u2217 1u >X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 X\u0303\u03b2\u2217, T\u030312 := u >X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 f \u2217 NL.\nIn the following subsections we will characterize the limiting value of each Ti as n, d,N \u2192\u221e proportionally."
        },
        {
            "heading": "C.3.1 Concentration and Simplification",
            "text": "In the following lemma, we show that each Ti will concentrate around some T 0 i given by\nT 01 := trR0, T 0 2 := \u00b521 N \u03b821 tr\n( X\u0303 >\n\u03a60R0\u03a6 > 0 X\u0303\n) ,\nT 03 := \u00b521\u03b8 2 1 N tr ( X\u0303 > X\u0303 ) , T 04 := \u00b5 \u2217 1\u03b82, T 05 := \u00b521\u00b5 \u2217 1\u03b82 N tr ( X\u0303 > X\u0303 ) , T 06 := \u00b51\u00b5 \u2217 1\u03b82\u221a N tr ( W 0R0\u03a6 > 0 X\u0303 ) , T 07 := \u00b521\u00b5 \u2217 1\u03b82 N tr ( X\u0303 > \u03a60R0\u03a6 > 0 X\u0303 ) , T 08 := N \u00b521 tr ( R0\u03a3\u03a60R0 ) , T 09 := \u00b51\u03b8\n2 1\u221a N\ntr ( W 0R0\u03a6 > 0 X\u0303 ) , T 010 := \u03b8 2 1,\nT 011 := \u03b8 2 1 tr ( X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 X\u0303 ) , T 012 := \u00b5 \u2217 1\u03b82 tr ( X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 X\u0303 ) ,\n(C.16)\nwhere scalars \u03b81 and \u03b82 are defined in Theorem 3. In what follows, we first prove that each T\u0303i in (C.15) vanishes in probability for i = 5, 6, 7, 12. Then we extend Lemma 18 to cover the case where matrix D is also random to establish the concentrations for all Ti\u2019s in (C.15).\nLemma 27. Under Assumptions 1 and 2, as n, d,N \u2192\u221e proportionally, we have\n|T\u03035|, |T\u03036|, |T\u03037|, |T\u030312| P\u2192 0.\nProof. To simplify the presentation, we take D \u2208 Rn\u00d7d, which involves independent Gaussian random matrices W 0, X\u0303 and Z, to be any of the following matrices:\n\u00b521\u221a N X\u0303, \u00b51 2 \u03a60R0W 0, \u00b521\u221a N \u03a60R0\u03a6 > 0 X\u0303, 1\u221a N \u03a60R0(\u00b5 2 1W > 0 W 0 + \u00b5 2 2I)R0\u03a6 > 0 X\u0303.\nIt is clear that all the T\u0303i of interest can be written as 1\u221a N f\u2217>NLDu for different choices of D. As a first observation, one can verify that \u2016D\u2016 \u2264 C with high probability, for some constant C > 0, as n,N, d \u2192 \u221e proportionally. Moreover, by Lemma A.12 in [BMR21], with probability at least 1\u2212 Cn\u22121/4, we have\u2223\u2223\u2016f\u2217NL\u20162/n\u2212 \u2016f\u2217NL\u20162L2 \u2223\u2223 \u2264 n\u22123/8. Hence, 1\u221a\nN \u2016f\u2217NL\u2016 \u2264 C uniformly with high probability. Notice that\n1\u221a N f\u2217>NLDu = \u00b51\u03b7 n \u221a N\n( f\u2217>NLDX >f\u2217(X) + f\u2217>NLDX >\u03b5 ) . (C.17)\nSince \u03b5 is independent with all other random variables, we can easily show the second term \u00b51\u03b7 n \u221a N f\u2217>NLDX >\u03b5\nis negligible asymptotically. In particular, notice that X/ \u221a n is also bounded by some constant with high probability. Thus, by the general Hoeffding inequality,\nP (\u2223\u2223\u2223\u2223 \u00b51\u03b7n\u221aN f\u2217>NLDX>\u03b5 \u2223\u2223\u2223\u2223 \u2265 n\u22121/4) \u2264 2e\u2212c\u221an, which implies that the second term in (C.17) converges to zero in probability. Hence we only need to control\n\u00b51\u03b7\nn \u221a N f\u2217>NLDX\n>f\u2217(X) = \u00b51\u03b7\nn \u221a N \u03c3\u2217NL(\u03b2 > \u2217 X\u0303 > )DX>\u03c3\u2217(X\u03b2\u2217), (C.18)\nwhere \u03c3\u2217NL(x) := \u03c3 \u2217(x)\u2212 \u00b5\u22171x. Our analysis can be divided into the following three steps.\nStep 1: Simplification via Rotation Invariance. Following an argument similar to Lemma 9.1 in [MM22] and Lemma 6.1 in [MZ20], we claim that for (C.18), it suffices to consider \u03b2\u2217 uniform on sphere and independent of X, X\u0303,W 0 and Z, that is, \u03b2\u2217 \u223c Unif(Sd\u22121). In particular, given Haar-distributed orthogonal matrix O \u2208 Rd\u00d7d, we apply this random rotation to W 0, X\u0303 > ,X> and \u03b2\u2217 respectively in (C.18). Since OW 0 d = W 0, XO d = X, and X\u0303O d = X\u0303, after replacing the deterministic \u03b2\u2217 by the random \u03b2\u2217 \u223c Unif(Sd\u22121), one can easily verify that the quantities of interest in (C.18) are unchanged in distribution. For example consider the quantity T\u03035. Denote G(X, X\u0303,\u03b2\u2217) = f \u2217> NLX\u0303X >f\u2217(X); we see that G(X, X\u0303,\u03b2\u2217) = G(XO, X\u0303O,O\u03b2\u2217) d = G(X, X\u0303,O\u03b2\u2217), and thus we can assume \u03b2\u2217 is uniform on sphere. Computation of the remaining terms follow from similar procedure. Hence, without loss of generality, in the following, we view \u03b2\u2217 as a uniformly random vector on unit sphere Sd\u22121. For properties of random orthogonal matrices, we refer the reader to [Mec19].\nStep 2: Nonlinear Hanson-Wright Inequality. Now, condition on the event\nE := {\u2016D\u2016, \u2016X\u0303\u2016/ \u221a N, \u2016X\u2016/ \u221a N \u2264 C},\nfor some constant C > 0, we can apply the nonlinear Hanson-wright inequality ([WZ21, Theorem 3.4.]) for \u03b2\u2217 \u223c Unif(Sd\u22121). In particular, we can rewrite (C.18) as a quadratic form:\n\u00b51\u03b7\nn \u221a N f\u2217>NLDX\n>f\u2217(X) = \u00b51\u03b7 n \u221a N q> [ 0 DX> 0 0 ] q,\nwhere we denoted q := [ \u03c3\u2217NL ( \u03b2>\u2217 X\u0303 >) , \u03c3\u2217 ( \u03b2>\u2217X >)]> \u2208 R2n. From Corollary 5.4 of [Mec19] we know the vector \u03b2\u2217 \u223c Unif(Sd\u22121) satisfies\nP(|f(\u03b2\u2217)\u2212 E[f(\u03b2\u2217)]| > t) \u2264 e\u2212cdt 2 ,\nfor 1-Lipschitz function f : Sd\u22121 \u2192 R; this is to say, \u03b2\u2217 satisfies the convex concentration property with parameter 1/ \u221a d defined in [Ada15]. Since both \u03c3\u2217 and \u03c3\u2217NL are \u03bb\u03c3-Lipschitz, based on [Ada15, Theorem 2.5] (or analogously [WZ21, Theorem 3.4]), only considering the randomness of \u03b2\u2217, we have\nP (\u2223\u2223\u2223\u2223 \u00b51\u03b7n\u221aN f\u2217>NLDX>f\u2217(X)\u2212 \u00b51\u03b7n\u221aN TrDX>\u03a8 \u2223\u2223\u2223\u2223 \u2265 tn\u221aN )\n\u2264 2 exp \u2212 1 C min  d2t24\u03bb4\u03c3(\u2016X\u20164 + \u2016X\u0303\u20164)\u2016DX>\u20162F , dt \u03bb2\u03c3 ( \u2016X\u20162 + \u2016X\u0303\u20162 ) \u2016DX>\u2016  \n+ 2 exp \u2212 dt2 16\u03bb2\u03c3 ( \u2016X\u20162 + \u2016X\u0303\u20162 ) \u2016DX>\u20162\u2016E\u03b2\u2217 [q]\u20162 , where \u03a8 := E\u03b2\u2217 [ \u03c3\u2217(X\u03b2\u2217)\u03c3 \u2217 NL(\u03b2 > \u2217 X\u0303 > ) ] \u2208 Rn\u00d7n is an \u201cexpected\u201d kernel matrix. By the Lipschitz property of \u03c3\u2217, we have \u2016E\u03b2\u2217 [q]\u2016 2 \u2264 \u2016X\u20162+\u2016X\u0303\u20162 \u2264 Cd under the event E . Moreover, note that under E , \u2016X\u2016, \u2016X\u0303\u2016 \u2264\nC \u221a d, \u2016D\u2016 \u2264 C, and \u2016X\u20162F \u2264 Cnd for some constant C > 0. Setting t = nN1/4 in the above probability bound, one can obtain\nP (\u2223\u2223\u2223\u2223 \u00b51\u03b7n\u221aN f\u2217>NLDX>f\u2217(X)\u2212 \u00b51\u03b7n\u221aN TrDX>\u03a8 \u2223\u2223\u2223\u2223 \u2265 1N1/4 ) \u2264 4e\u2212c \u221a N ,\nfor some constant c > 0. Therefore, (C.18) concentrates around \u00b51\u03b7 n \u221a N TrDX>\u03a8 with high probability. Thus it remains to show that \u00b51\u03b7 n \u221a N TrDX>\u03a8 is vanishing in probability.\nStep 3: Estimations of Expected Kernel. Notice that on the event E ,\u2223\u2223\u2223\u2223 \u00b51\u03b7n\u221aN TrDX>\u03a8 \u2223\u2223\u2223\u2223 \u2264 \u00b51\u03b7n\u221aN \u2016DX>\u2016F \u2016\u03a8\u2016F \u2264 \u00b51\u03b7n\u221aN \u2016D\u2016\u2016X>\u2016F \u2016\u03a8\u2016F \u2264 C\u221aN \u2016\u03a8\u2016F .\nHence we aim to control the entry-wise magnitude of \u03a8. We denote columns of X> and X\u0303 > by X> := [x1, . . . ,xn] and X\u0303 >\n:= [x\u03031, . . . , x\u0303n], respectively. Notice that for 1 \u2264 i, j \u2264 n, entry of the expected kernel matrix \u03a8 is given as \u03a8i,j = E\u03b2\u2217 [ \u03c3\u2217(\u03b2>\u2217 xi)\u03c3 \u2217 NL(\u03b2 > \u2217 x\u0303j) ] .\nDefine the event M := {\u2223\u2223\u2223\u2016xi\u2016/\u221ad\u2212 1\u2223\u2223\u2223, \u2223\u2223\u2223\u2016x\u0303i\u2016/\u221ad\u2212 1\u2223\u2223\u2223, \u3008xi, x\u0303j\u3009 \u2264 C log d/\u221ad, i, j \u2208 [n]}. One can verify that M holds with high probability based on concentration of Gaussian random vectors. Conditioned on event M, we aim to control |\u03a8i,j | for 1 \u2264 i, j \u2264 n. In the following, we follow the arguments in [MZ20, Appendix A.4] to establish the desired claim. Without loss of generality, we may change the coordinate and take the direction of x\u0303j as e1, namely x\u0303j = \u2016x\u0303j\u2016e1. Hence, \u03b2\u2217 \u223c Unif(Sd\u22121) can be rewritten as \u03b2\u2217 = w1e1 + \u221a 1\u2212 w21w, where w1 := \u03b2 > \u2217 e1, and w := [0, w\u0303\n>]> \u2208 Rd with w\u0303 \u223c Unif(Sd\u22122) independent of w1. Note that w1 \u2208 [\u22121, 1] and \u221a dw1 converges weakly to the standard Gaussian distribution. Analogous to the proof of Lemma A.5 in [MZ20], we make the decomposition\n\u03b2>\u2217 xi = w1 \u2329 xi,\nx\u0303j \u2016x\u0303j\u2016 \u232a \ufe38 \ufe37\ufe37 \ufe38\n\u03b4d\n+ \u221a 1\u2212 w21\u2016xi\u2016\u221a d\u2212 1\ufe38 \ufe37\ufe37 \ufe38 1+ d \u2329 xi \u2016xi\u2016 , \u221a d\u2212 1w \u232a \ufe38 \ufe37\ufe37 \ufe38 \u03bed .\nTherefore, we have for any i, j \u2208 [n],\n\u03a8i,j = Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1)Ew[\u03c3\u2217(\u03b4d + (1 + d)\u03bed)]]. (C.19)\nNow condition on event M, the Lipschitz property of \u03c3\u2217 entails that\u2223\u2223\u2223\u2223Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1)\u03b4d]\u2212 Ew1[\u03c3\u2217NL(\u221adw1)\u221adw1]\u2329 xi\u221ad , x\u0303j\u2016x\u0303j\u2016 \u232a\u2223\u2223\u2223\u2223\n\u2264 Ew1 [dw21] \u2223\u2223\u2223\u2223\u2329 xi\u221ad , x\u0303j\u2016x\u0303j\u2016 \u232a\u2223\u2223\u2223\u2223 \u00b7 \u2223\u2223\u2223\u22231\u2212 \u2016x\u0303j\u2016\u221ad \u2223\u2223\u2223\u2223 . (log d)2d , (C.20)\nwhere we also applied Lemma 4.9 of [MZ20] to obtain |Ew1 [dw21]\u2212 1| . (log d)2/d. Similarly,\n|Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1)\u03b4d]| . (log d)2\nd + Ew1\n[ \u03c3\u2217NL( \u221a dw1) \u221a dw1 ] log d\u221a d . (log d)2 d ,\nsince Ew1 [ \u03c3\u2217NL( \u221a dw1) \u221a dw1 ] = E\u03be\u223cN (0,1)[\u03c3\u2217NL(\u03be)\u03be] +Od ( (log d)2/d ) and E\u03be\u223cN (0,1)[\u03c3\u2217NL(\u03be)\u03be] = 0. Next, note\nthat \u2223\u2223\u22231\u2212\u221a1\u2212 w21\u2223\u2223\u2223 \u2264 12w21, and d = 1\u2212\u2016xi\u2016/\u221ad\u2212 1 +(1\u2212\u221a1\u2212 w21)\u2016xi\u2016/\u221ad\u2212 1. Thus under eventM,\n|Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1) d]| (C.21) \u2264 \u2223\u2223\u2223\u22231\u2212 \u2016xi\u2016\u221ad\u2212 1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223Ew1[\u03c3\u2217NL(\u2016x\u0303j\u2016w1)\u2212 \u03c3\u2217NL(\u221adw1)]\u2223\u2223\u2223 + \u2223\u2223\u2223\u22231\u2212 \u2016xi\u2016\u221ad\u2212 1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223Ew1[\u03c3\u2217NL(\u221adw1)]\u2223\u2223\u2223+ \u2016xi\u2016\u221ad\u2212 1\n\u2223\u2223\u2223\u2223Ew1[|\u03c3\u2217NL(\u2016x\u0303j\u2016w1)| \u00b7 \u2223\u2223\u2223\u22231\u2212\u221a1\u2212 w21\u2223\u2223\u2223\u2223]\u2223\u2223\u2223\u2223 \u2264 C(log d) 2\nd \u03bb\u03c3E[|\n\u221a dw1|] + C(log d)3\nd3/2 + C 2d E[( \u221a dw1) 4]1/2Ew1 [\u03c3\u22172NL(\u2016x\u0303j\u2016w1)]1/2 . (log d)2 d ,\nwhere we used E\u03be\u223cN (0,1)[\u03c3\u2217NL(\u03be)] = 0 and Lemma A.9 of [MZ20]. Consequently, by (C.19),\n|[\u03a8]i,j \u2212 Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1)]Ew[\u03c3\u2217(\u03bed)]|\n. |Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1)\u03b4d]|+ |Ew1 [\u03c3\u2217NL(\u2016x\u0303j\u2016w1) d]Ew[\u03bed]|+ Ew1,w [ (\u03b4d + d\u03bed) 2 ]\n. (log d)2\nd (1 + Ew[|\u03bed|]) + Ew1 [\u03b42d] + Ew1 [ 2d]Ew[\u03be2d] .\n(log d)2\nd ,\nwhere we repeatedly applied Lemma A.9 of [MZ20], (C.20), and (C.21). Meanwhile, notice that Ew[\u03c3\u2217(\u03bed)] = E\u03be\u223cN (0,1)[\u03c3\u2217(\u03be)] +Od((log d)2/d). We conclude that with high probability, the following holds\nmax 1\u2264i,j\u2264n\n|[\u03a8]i,j | . (log d)2\nd .\nThus, as d\u2192\u221e, \u2016\u03a8\u2016F / \u221a N . (log d)2/ \u221a d with high probability, which finally implies that \u00b51\u03b7\nn \u221a N\nTrDX>\u03a8\nconverges to zero in probability as n, d,N \u2192\u221e proportionally. This completes the proof.\nFinally, we use the following simplification of quadratic forms to obtain the desired T 0i .\nLemma 28. Consider a random matrix D \u2208 Rd\u00d7d that does not rely on \u03b2\u2217 and is rotational invariant in distribution, namely D d = O>DO for any random rotational matrix O \u2208 Rd\u00d7d. Assume that \u2016D\u2016 \u2264 C with high probability for some universal constant C > 0. Then as d\u2192\u221e,\u2223\u2223\u2223\u03b2>\u2217D\u03b2\u2217 \u2212 trD\u2223\u2223\u2223 P\u2192 0, Also as a corollary, we have\n\u2223\u2223Ti \u2212 T 0i \u2223\u2223 P\u2192 0 as n, d,N \u2192 \u221e proportionally for all 1 \u2264 i \u2264 12, where Ti, T 0i are defined in (C.15) and (C.16).\nProof. Given any rotational matrix O \u2208 Rd\u00d7d following the Haar distribution, notice that \u03b2>\u2217D\u03b2\u2217 = \u03b2 \u2032> \u2217 O >DO\u03b2\u2032\u2217 d = \u03b2 \u2032> \u2217 D\u03b2 \u2032 \u2217, where \u03b2 \u2032 \u2217 := O\n>\u03b2\u2217. Consequently, we can equivalently take \u03b2\u2217 to be a random vector uniformly distributed on the unit sphere Sd\u22121. Again recall that \u03b2\u2217 \u223c Unif(Sd\u22121) satisfies the convex concentration property\nP(|f(\u03b2\u2217)\u2212 E[f(\u03b2\u2217)]| > t) \u2264 e\u2212cdt 2\nfor 1-Lipschitz f . Therefore, conditioned on the event \u2016D\u2016 \u2264 C, by Theorem 2.5 in [Ada15], one can conclude that \u2223\u2223\u2223\u03b2>\u2217D\u03b2\u2217 \u2212 E\u03b2\u2217 [\u03b2>\u2217D\u03b2\u2217]\u2223\u2223\u2223 P\u2192 0. Finally, note that E\u03b2\u2217 [\u03b2>\u2217D\u03b2\u2217] = trD because the covariance of the uniform random vector on Sd\u22121 is 1dI; this concludes the proof. Convergence of each Ti to the corresponding T 0i follows from a direct application of Lemma 18 and this lemma."
        },
        {
            "heading": "C.3.2 Risk Calculation via Linear Pencils",
            "text": "In this section, we derive analytic expressions of the terms Ti defined in (C.15) as n, d,N \u2192\u221e proportionally. In particular, the exact values are described by self-consistent equations defined in the following proposition.\nProposition 29. Given Assumption 1 and \u03bb > 0. For each Ti defined in (C.15) and 1 \u2264 i \u2264 12, we have\nTi \u2192 \u03c4i,\nin probability, as n/d\u2192 \u03c81 and N/d\u2192 \u03c82, where \u03c4i\u2019s are defined as follows\n\u03c41 := \u03c81 \u03c82 m1 + ( \u03c82 \u03c81 \u2212 1 ) 1 \u03bb , \u03c42 := \u00b5 2 1\u03b8 2 1 \u03c81 \u03c82 ( 1\u2212 \u03bb\u03c81 \u03c82 m2 ) , \u03c43 := \u00b5 2 1\u03b8 2 1 \u03c81 \u03c82 ,\n\u03c44 := \u00b5 \u2217 1\u03b82, \u03c45 := \u00b5 2 1\u00b5 \u2217 1\u03b82 \u03c81 \u03c82 , \u03c46 := \u00b5 \u2217 1\u03b82\n( 1\u2212 m2\nm1\n) ,\n\u03c47 := \u00b5 2 1\u00b5 \u2217 1\u03b82 \u03c81 \u03c82\n( 1\u2212 \u03bb\u03c81\n\u03c82 m2\n) , \u03c48 := m1 + \u03c81 \u03c82 \u03bbm\u20321(\n\u00b51 \u03c81 \u03c82 \u03bbm1\n)2 , \u03c49 := \u03b821(1\u2212 m2m1 ) ,\n\u03c410 := \u03b8 2 1, \u03c411 := \u03b8 2 1\n( 1\u2212 2m2\nm1 \u2212 m\n\u2032 2 m21\n) , \u03c412 := \u00b5 \u2217 1\u03b82 ( 1\u2212 2m2\nm1 \u2212 m\n\u2032 2 m21\n) .\nAll scalars \u03c4i\u2019s are only determined by parameters \u03c81, \u03c82, \u03b7, \u00b51, \u00b52, \u03bb, and m1,m2,m \u2032 1,m \u2032 2. Here, m1 := m1 ( \u03bb\u03c81\u03c82 ) , m\u20321 := m \u2032 1 ( \u03bb\u03c81\u03c82 ) , m2 := m2 ( \u03bb\u03c81\u03c82 ) and m\u20322 := m \u2032 2 ( \u03bb\u03c81\u03c82 ) , where m1(z) and m2(z) \u2208 C+ \u222aR+ are the solutions to the following self-consistent equations for z \u2208 C+ \u222a R+, 1\n\u03c81 (m1(z)\u2212m2(z))(\u00b522m1(z) + \u00b521m2(z)) + \u00b521m1(z)m2(z)(zm1(z)\u2212 1) = 0, (C.22)\n\u03c82 \u03c81\n( \u00b521m1(z)m2(z) + 1\n\u03c81 (m2(z)\u2212m1(z))\n) + \u00b521m1(z)m2(z)(zm1(z)\u2212 1) = 0. (C.23)\nProof. First note that due to Lemma 28, it suffices to consider the limits of T 0i instead. Convergence of T3, T4, T5, and T10 directly follows from Lemma 18, Lemma 28 and the Marchenko-Pastur law for 1 nX\u0303 > X\u0303. In addition, T7, T9 and T12 are analogous to T2, T6 and T11, respectively. To characterize the remaining T1, T2, T6, T8 and T11, we adopt the linear pencil method in basis of operator-valued free probability theory [FOBS06, HFS07, MS17, HMS18]. Specifically, the linear pencil allows us to relate the quantities of interest to the trace of certain large block matrices; in our case, variants of T1, T2, T6, T8, T11 have already appeared in prior constructions from [AP20, BM21, TAP21], which we build upon in the following calculation.\nFor z \u2208 C+\u222aR+, let us defineR0(z) := ( \u03a3\u0302\u03a60 + zI )\u22121 and R\u03040(z) := ( \u03a60\u03a6 > 0 + zI )\u22121 \u2208 Rn\u00d7n. Note that due to the Gaussian equivalent property, as n,N, d\u2192\u221e at comparable rate, the limit of tr R\u03040(z) is exactly the Stieltjes transform of the limiting spectrum of the (nonlinear) CK, namely \u03a6\u03a6> \u2208 Rn\u00d7n, evaluated at\u2212z. We denote m1(z) := limn\u2192\u221e tr R\u03040(z). Similarly, the limit of trR0(z) is the companion Stieltjes transform of m1(z), as \u03a60\u03a6 > 0 and \u03a6 > 0 \u03a60 have the same non-zero eigenvalues. We denote \u03c41(z) := limn\u2192\u221e trR0(z). The defined Stieltjes transforms will be evaluated at z = \u03c81\u03c82\u03bb. Also recall the the following relationship between R0(z) and R\u03040(z),\n\u03c41(z) = \u03c81 \u03c82 m1(z) +\n( 1\u2212 \u03c81\n\u03c82\n) 1\nz . (C.24)\nAnalogously, we introduce the following quantities: for any z \u2208 C+ \u222a R+, as n,N, d\u2192\u221e proportionally\nm2(z) := lim n\u2192\u221e tr\n( 1\nd X\u0303X\u0303\n> R\u03040(z) ) , \u03c42(z) : = lim\nn\u2192\u221e tr\n( 1\nn X\u0303 > \u03a60R0(z)\u03a6 > 0 X\u0303\n) ,\n\u03c46(z) := lim n\u2192\u221e 1\u221a N\ntr ( W 0R0(z)\u03a6 > 0 X\u0303 ) , \u03c48(z) : = lim n\u2192\u221e tr ( R0(z) ( \u00b521W > 0 W 0 + \u00b5 2 2I )) .\nIt is straightforward to verify all the above limits exist and are finite. Finally, in the following analysis we will repeatedly make use of the following identities:\nR0(z)\u03a6 > 0 =\u03a6 > 0 R\u03040(z), (C.25)\nX\u0303 >\n\u03a60R0(z)\u03a6 > 0 X\u0303 =X\u0303 > X\u0303 \u2212 zX\u0303>R\u03040(z)X\u0303. (C.26)\nAnalysis of T1 and T2. Note that m1(z) and m2(z) defined in (C.22) and (C.23) have been characterized in prior works, such as Proposition 1 of [AP20]; in particular, since we are only interested in the CK, we can simply set \u03c3W2 = 0 in [AP20] (which considered the sum of the CK and the first-layer NTK). Therefore, from (C.24) we obtain \u03c41 = \u03c41 ( \u03c81 \u03c82 \u03bb ) from m1 := m1 ( \u03c81 \u03c82 \u03bb ) . As for the limit of T2, (C.26) indicates\n\u03c42(z) = 1\u2212 zm2(z), since tr ( X\u0303X\u0303 > /d ) \u2192 1, as n,N, d\u2192\u221e. Thus,\n\u03c42 = \u00b5 2 1\u03b8 2 1 \u03c81 \u03c82 (1\u2212 zm2(z)),\nwith z = \u03c81\u03c82\u03bb. \u03c47 can also be derived in similar fashion.\nAnalysis of T6. For T6, we utilize the computations in Appendix I.6.1 of [TAP21] by setting the covariance \u03a3 = I. More precisely, based on Equations (S370) and (S418) in [TAP21],\n\u00b51\u03c46(z) = 1\u2212GK \u22121 6,6 = z\u00b521\u03c81m1(z)\u03c41(z)\n1 + z\u00b521\u03c81m1(z)\u03c41(z)\n(i) = z\u00b521\u03c81m1(z)\u03c41(z) (ii) = 1\u2212 m2(z)\nm1(z) , (C.27)\nwhere (i) and (ii) are both due to (C.23) and (C.24). Hence we obtain the formulae of \u03c46 and \u03c49.\nAnalysis of T8. Recall the following derivative trick of the Stieltjes transform,\n\u2202 \u2202z tr ( R0(z) ( \u00b521W > 0 W 0 + \u00b5 2 2I )) = \u2212 tr ( R0(z) ( \u00b521W > 0 W 0 + \u00b5 2 2I ) R0(z) ) .\nDefine \u03c4\u0303(z) = limn\u2192\u221e tr ( R0(z) ( \u00b521W > 0 W 0 + \u00b5 2 2I )) . Following the proof of Lemma 7.4 in [DW18], we can\napply Lemma 2.14 of [BS10] and Vitali\u2019s theorem to claim that\n\u2212\u03c4\u0303 \u2032(z) = lim n\u2192\u221e\nN tr ( R0(z)\u03a3\u03a60R0(z) ) .\nHence, we need to first calculate \u03c4\u0303(z). By definition of \u03c41(z), Equations (S376) and (S412) in [TAP21],\n\u03c4\u0303(z) = \u00b522\u03c41(z) + \u00b521\u03c41\n1 + z\u00b521\u03c81m1(z)\u03c41(z)\n(iii) = \u03c41(z) ( \u00b522 + \u00b5 2 1 m2(z)\nm1(z) ) (iv) =\n\u00b521 z ( \u03c81 \u03c82 zm1(z) + ( 1\u2212 \u03c81 \u03c82 ))( \u00b522 \u00b521 + m2(z) m1(z) ) (v) = 1\n\u03c81z m1(z)\u2212m2(z) m1(z)m2(z) ( \u00b522 \u00b521 + m2(z) m1(z) ) (vi) =\n1\nzm1(z) \u2212 1,\nwhere (iii) and (v) are due to (C.23) and (C.24), (iv) comes from (C.24), (vi) is based on (C.22). We arrive at \u03c48 = \u2212 1\u00b521 \u03c4\u0303 \u2032(z) with z = \u03c81\u03c82\u03bb.\nAnalysis of T11. Once again by setting \u03c3W2 = 0 in [AP20], we can directly employ the computation of E32 in Section S4.3.4 of [AP20] to derive \u03c411 and \u03c412. In particular, due to (C.25),\nE32 = tr ( X\u0303 > R\u03040(z) ( \u00b522 N \u03a60\u03a6 > 0 + \u00b521 N \u03a60W > 0 W 0\u03a6 > 0 ) R\u03040(z)X\u0303 ) = tr ( X\u0303 >\n\u03a60R0(z) ( \u00b522 N I + \u00b521 N W>0 W 0 ) R0(z)\u03a6 > 0 X\u0303 ) = tr ( X\u0303 > \u03a60R0(z)\u03a3\u03a60R0(z)\u03a6 > 0 X\u0303 ) .\nNote that Equation (S148) in [AP20] established that\nlim n\u2192\u221e\nE32 = 1\u2212 2m2(z) m1(z) \u2212 m\n\u2032 2(z)\nm1(z)2 ,\nwhence, letting z = \u03c81\u03c82\u03bb, we obtain \u03c411 and \u03c412.\nHaving obtained the asymptotic expressions of each term in the decomposition of the prediction risk, we can now compute the difference in the prediction risk of CK ridge regression before and after one gradient descent step, i.e., R0(\u03bb)\u2212R1(\u03bb) in Theorem 7. The following statement is the complete version of Theorem 7.\nTheorem 30. Given Assumptions 1 and 2, consider \u03c81, \u03c82 \u2208 (0,+\u221e). Fix \u03b7 = \u0398(1) and \u03bb > 0. Denote R0(\u03bb) and R1(\u03bb) as the prediction risk of CK ridge regression in (4.1) using initial weight W 0 and first-step updated W 1, respectively. Then the difference between these two prediction risk values satisfies\nR0(\u03bb)\u2212R1(\u03bb) P\u2192 \u03b4(\u03b7, \u03bb, \u03c81, \u03c82),\nwhere \u03b4 is a non-negative function of \u03b7, \u03bb, \u03c81 and \u03c82 \u2208 (0,+\u221e) with parameters \u00b5\u22171, \u00b51, \u00b52 given as\n\u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) \u03c41(\u03c42 \u2212 \u03c43)\u2212 1 \u2212 \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) + (\u03c47 \u2212 \u03c45) 2\u03c48 (\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 . (C.28)\nHere the scalars \u03c4i\u2019s are defined in Proposition 29. Furthermore, \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = 0 if and only if at least one of \u00b5\u22171, \u00b51 and \u03b7 is zero.\nProof. Due to Lemma 26 (or the decomposition (C.9), (C.10) and (C.11)), we can see that variance V is unchanged after one gradient descent step with \u03b7 = \u0398(1). Hence we only need to analyze the changes in (C.9) and (C.10). Also, due to Lemma 26 and the proof of Theorem 3, we can ignore B and C in W 1 and take W 1 := W 0 + ua >, where u = \u00b51\u03b7n X >y and y = f\u2217(X) + \u03b5, without changing the bias terms.\nSeparation of Low-rank Terms. First note that if \u00b51 = 0, then u = 0 and therefore R0(\u03bb) = R1(\u03bb) as n \u2192 \u221e. In the following, we take \u00b51 6= 0 which implies that \u03b81 defined in Theorem 3 will not vanish. Now we aim to extract the low-rank perturbation ua> from bias terms (C.9) and (C.10). We adhere to the notions in (C.13), (C.15) and (C.16) and define D := T1(T2 \u2212 T3) \u2212 1. Similar to [MM22, Lemma C.1], we use the following linearization trick to separate the gradient step ua> from the matrices R, \u03a6\u0304,\u03a3\u03a6 and W 1.\nDefine b := \u00b51\u221a N X\u0303u and c := \u03a6>0 b; observe that T2 = c >R0c, T3 = b >b, and\n\u03a3\u0302\u03a6 = \u03a3\u0302\u03a60 + [ a c ][T3 1 1 0 ][ a> c> ] .\nTherefore, by the Sherman-Morrison-Woodbury formula and Hanson-Wright inequality, we have\nR = R0 \u2212\u2206aa \u2212\u2206cc + \u2206ac + \u2206ca + od,P(1),\nwhere we further defined\n\u2206aa := T2 \u2212 T3 D R0aa >R0, \u2206cc := T1 D R0cc >R0, \u2206ca := 1\nD R0ca\n>R0, \u2206ac := 1\nD R0ac\n>R0.\nWe decompose the subtracted term in (C.9) into\nB1,1 : = \u2212 2\u00b51\u00b5 \u22172 1\u221a\nN \u03b2>\u2217W 1R\u03a6\u0304 > X\u0303\u03b2\u2217, B NL 1,1 := \u2212\n2\u00b51\u00b5 \u22172 1\u221a\nN \u03b2>\u2217W 1R\u03a6\u0304 > X\u0303f\u2217NL,\nB01,1 : = \u2212 2\u00b51\u00b5 \u22172 1\u221a\nN \u03b2>\u2217W 0R0\u03a6 > 0 X\u0303\u03b2\u2217, B 0 2 := f \u2217>X\u0303 > \u03a60R0\u03a3\u03a60R0\u03a6 > 0 X\u0303f \u2217,\nwhere f\u2217NL and f \u2217 are defined in (C.14). By repeatedly applying the Hanson-Wright inequality (since a is centered and independent of all other terms) and Lemma 27, we can obtain\nB1,1 = B 0 1,1 \u2212 2T1(T7 \u2212 T5)(T4 \u2212 T6)/D + od,P(1), BNL1,1 = od,P(1).\nNow we denote \u2206ua := \u00b521 NW > 0 ua >, \u2206au := \u2206 > ua and \u2206aua := \u00b521T10 N aa >. Hence,\n\u03a3\u03a6 = \u03a3\u03a60 + \u2206ua + \u2206au + \u2206aua.\nAnalogously, we can decompose B2 in (C.10) as follows\nB2 = \u00b5 \u22172 1 \u03b2 > \u2217 X\u0303 > \u03a6\u0304R\u03a3\u03a6R\u03a6\u0304 > X\u0303\u03b2\u2217\n= \u00b5\u221721 \u03b2 > \u2217 X\u0303 > \u03a60(R0 \u2212\u2206cc + \u2206ca)\u03a3\u03a6(R0 \u2212\u2206cc + \u2206ac)\u03a6>0 X\u0303\u03b2\u2217\n+ 2\u00b5\u221721 \u03b2 > \u2217 X\u0303 > \u03a60(R0 \u2212\u2206cc + \u2206ca)\u03a3\u03a6(R0 \u2212\u2206aa + \u2206ca)ab>X\u0303\u03b2\u2217\n+ \u00b5\u221721 \u03b2 > \u2217 X\u0303 > ba>(R0 \u2212\u2206aa + \u2206ac)\u03a3\u03a6(R0 \u2212\u2206aa + \u2206ca)ab>X\u0303\u03b2\u2217 + od,P(1)\n= B02 + 2T1(T7 \u2212 T5)(T6 \u2212 T12)\nD + (T7 \u2212 T5)2(T 21 T11 + T 21 T10 + T8 \u2212 2T 21 T9) D2 + od,P(1),\nwhere we repeatedly make use of Lemma 27 and the concentration for a to simplify the computations. Therefore, one can obtain\nR0(\u03bb)\u2212R1(\u03bb) = B01,1 \u2212B1,1 +B02 \u2212B2\n= 2T1(T7 \u2212 T5)(T4 + T12 \u2212 2T6) D \u2212 (T7 \u2212 T5) 2(T 21 T11 + T 2 1 T10 + T8 \u2212 2T 21 T9) D2 + od,P(1).\nOn the other hand, from Proposition 29 we know that\nR0(\u03bb)\u2212R1(\u03bb) P\u2192 2\u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) \u03c41(\u03c42 \u2212 \u03c43)\u2212 1 \u2212 (\u03c47 \u2212 \u03c45) 2(\u03c421 \u03c411 + \u03c4 2 1 \u03c410 + \u03c48 \u2212 2\u03c421 \u03c49)\n(\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2\ufe38 \ufe37\ufe37 \ufe38 ,\u03b4(\u03b7,\u03bb,\u03c81,\u03c82)\n,\nwhere the right hand side is the quantity of interest \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) defined in Theorem 7. Also observe the following equivalences from Proposition 29,\n\u00b5\u22171\u03b82(\u03c42 \u2212 \u03c43) = \u03b821(\u03c47 \u2212 \u03c45), \u00b5\u22171\u03b82(\u03c411 + \u03c410 \u2212 2\u03c49) =\u03b821(\u03c44 + \u03c412 \u2212 2\u03c46).\nHence, we can simplify \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) as follows\n\u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = \u2212 (\u03c47 \u2212 \u03c45)2\u03c48\n(\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2\n+ \u03c421 (\u03c42 \u2212 \u03c43)(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46)\u2212 \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46)\u2212 \u03c421 (\u03c47 \u2212 \u03c45)2(\u03c44 + \u03c412 \u2212 2\u03c46)\n(\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2\n=\u2212 (\u03c47 \u2212 \u03c45) 2\u03c48 (\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 + \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) \u03c41(\u03c42 \u2212 \u03c43)\u2212 1 \u2212 \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) (\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 = \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46)\n\u03c41(\u03c42 \u2212 \u03c43)\u2212 1 \u2212 \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) + (\u03c47 \u2212 \u03c45) 2\u03c48 (\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 . (C.29)\nNon-negativity of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82). Finally, we validate that the function \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is non-negative on variables \u03b7, \u03bb, \u03c81 and \u03c82 \u2208 (0,+\u221e). Observe that the formula of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) in (C.29) is decomposed into two parts. From Proposition 29 we know that \u03c41 and m1 are the limits of trR0(z) and tr R\u03040(z) evaluated at z = \u03c81\u03bb/\u03c82; this indicates that \u03c41 \u2208 (0, \u03c82/\u03bb\u03c81] is non-negative. For the same reason, m2 \u2208 (0, \u03c82/\u03bb\u03c81] and \u2212m\u20321,\u2212m\u20322 \u2208 (0, \u03c822/\u03bb2\u03c821 ]. Also due to Proposition 29, we have\n\u03c42 \u2212 \u03c43 = \u2212\u00b521\u03b821 ( \u03c81 \u03c82 )2 \u03bbm2 \u2264 0, \u03c47 \u2212 \u03c45 = \u2212\u00b521\u00b5\u22171\u03b82 ( \u03c81 \u03c82 )2 \u03bbm2 \u2264 0, \u03c44 + \u03c412 \u2212 2\u03c46 = \u2212\u00b5\u22171\u03b82 m\u20322 m21 \u2265 0, \u03c411 + \u03c410 \u2212 2\u03c49 = \u2212\u03b821 m\u20322 m21 \u2265 0,\n\u03c48 = 1\nm1\n1\n\u00b521\u03bb 2 ( \u03c82 \u03c81 )2 + m\u20321 m21 ( \u03c82 \u03c81 ) 1 \u00b521\u03bb .\n(C.30)\nTherefore, \u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) \u2264 0 and \u03c41(\u03c42 \u2212 \u03c43) \u2212 1 \u2264 \u22121. This entails that the first part of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is non-negative:\n\u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) \u03c41(\u03c42 \u2212 \u03c43)\u2212 1 \u2265 0.\nAs for the second part, it suffices to evaluate \u2206 := \u03c41(\u03c44 + \u03c412 \u2212 2\u03c46) + (\u03c47 \u2212 \u03c45)\u03c48 since\n\u2212\u03c41(\u03c47 \u2212 \u03c45)(\u03c44 + \u03c412 \u2212 2\u03c46) + (\u03c47 \u2212 \u03c45) 2\u03c48\n(\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 = (\u03c45 \u2212 \u03c47)\u2206 (\u03c41(\u03c42 \u2212 \u03c43)\u2212 1)2 .\nPlugging in quantities in (C.30) with z = \u03bb\u03c81/\u03c82, we have \u2206 =\u2212 \u00b5\u22171\u03b82 ( \u03c81 \u03c82 m2 zm21 (m1 + zm \u2032 1) + \u03c41m \u2032 2 m21 ) =\u2212 \u00b5\n\u2217 1\u03b82 zm21 ( \u03c81 \u03c82 (m1m2 + zm2m \u2032 1 + zm1m \u2032 2) + ( 1\u2212 \u03c81 \u03c82 ) m\u20322 ) =\u2212 \u00b5\n\u2217 1\u03b82 zm21 d dz \u2223\u2223\u2223\u2223 z=\u03bb\u03c81/\u03c82 ( \u03c81 \u03c82 zm1(z)m2(z) + ( 1\u2212 \u03c81 \u03c82 ) m2(z) ) (i) = \u2212 \u00b5 \u2217 1\u03b82\nzm21\u03c81\u00b5 2 1\nd\ndz \u2223\u2223\u2223\u2223 z=\u03bb\u03c81/\u03c82 ( 1\u2212 m2(z) m1(z) ) (ii) = \u2212 \u00b5 \u2217 1\u03b82\nzm21\u00b5 2 1\nd\ndz \u2223\u2223\u2223\u2223 z=\u03bb\u03c81/\u03c82 z\u00b521m1(z)\u03c41(z),\nwhere (i) and (ii) are due to (C.23) and (C.27), respectively. By Lemma A.1 in [TAP21], we know function z\u00b521m1(z)\u03c41(z) has non-positive derivative when z > 0. This implies that \u2206 \u2265 0 and hence the second part of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is also non-negative.\nFinally, we note that when \u00b5\u22171 = 0, the function \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = 0. This is because\n\u03c47 \u2212 \u03c45 = \u2212\u00b521\u00b5\u22171\u03b82\u03c821\u03bbm2/\u03c822 = 0,\nwhen \u00b5\u22171 = 0. Whereas when \u03b7 = 0, we know that \u03b81 = \u03b82 = 0, which entails \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is also vanishing. Also observe that in (C.30), m1,m2,m \u2032 1,m \u2032 2, \u03c41 are all positive. Hence we conclude that if \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) = 0, then at least one of \u03b7, \u00b51\u00b5 \u2217 1 must be zero."
        },
        {
            "heading": "C.4 Analysis of Special Cases",
            "text": "While the previous subsection provides explicit formulae of \u03b4, the expressions are rather complicated due to the self-consistent equations (C.22) and (C.23). In this section we consider two special cases: the large sample limit \u03c81 \u2192\u221e and the large width limit \u03c82 \u2192\u221e, where the calculation simplifies and enables us to further characterize properties of \u03b4. In both cases, we start with Theorem 30 and take one of aspect ratios (\u03c81 or \u03c82) to infinity.\nC.4.1 Case I: Large sample limit\nIn this subsection we prove Proposition 8. We introduce two positive parameters\ns1 :=\n\u222b 1\n\u00b521x+ \u00b5 2 2 + \u03bb\nd\u00b5MP\u03c82 (x), s2 :=\n\u222b 1\n(\u00b521x+ \u00b5 2 2 + \u03bb)\n2 d\u00b5 MP \u03c82 (x), (C.31)\nwhere \u00b5MP\u03c82 is Marchenko\u2013Pastur distribution with rate \u03c82 \u2208 (0,\u221e). Now we consider the large-sample limit: \u03c81 \u2192\u221e, \u03c82 \u2208 (0,\u221e). The following statement is the formal version of Proposition 8, and compared to the general result (Theorem 30), this special case admits a more explicit formula only determined by s1 and s2.\nTheorem 31 (Large sample limit). Under the same assumptions as Theorem 30 and take \u03c81 \u2192 \u221e. Then the difference between the prediction risks before and after one feature learning step R0(\u03bb)\u2212R1(\u03bb) satisfies\nlim \u03c81\u2192\u221e lim n,d,N\u2192\u221e\n(R0(\u03bb)\u2212R1(\u03bb)) =: \u03b4(\u03b7, \u03bb,\u221e, \u03c82) = \u00b5\u221721 ( AB\nA+ 1 +\nC\n(A+ 1)2\n) , (C.32)\nin probability, where\nA :=\u00b521\u03b8 2 2s1(1 + \u03c82(\u00b5 2 2 + \u03bb)s1 \u2212 \u03c82), (C.33)\nB :=1\u2212 \u03c82 + \u03c82\u03bb(\u00b522 + \u03bb)s2 + \u00b522\u03c82s1, (C.34) C :=\u03bb\u00b521\u03b8 2 2(1 + \u03c82(\u00b5 2 2 + \u03bb)s1 \u2212 \u03c82) ( 2(\u00b522 + \u03bb)\u03c82s1s2 \u2212 \u03c82s21 + s2(1\u2212 \u03c82) ) . (C.35)\nIn this case \u03b4(\u03b7, \u03bb,\u221e, \u03c82) is a non-negative function of \u03b7, \u03bb, \u03c82 \u2208 (0,+\u221e), and \u03b4 = 0 if and only if one of \u00b51, \u00b5 \u2217 1, \u03b7 is zero. Furthermore, \u03b4(\u03b7, \u03bb,\u221e, \u03c82) is increasing with respect to the learning rate \u03b7 \u2265 0.\nProof. Following Theorem 30, it suffices to consider the limit of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) when \u03c81 \u2192\u221e. This reduces to simplifying the asymptotics of \u03c4i\u2019s defined in Proposition 29, as \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) is determined by \u03c4i\u2019s in Theorem 30. We aim to prove the following:\n\u03c81 \u03c82 \u03c41 \u2192 s1,\n\u03c82 \u00b521\u03c81 \u03c42 \u2192 \u03b822\u03c82(1\u2212 (\u00b522 + \u03bb)s1),\n\u03c46 \u2192 \u00b5\u22171\u03b82\u03c82(1\u2212 (\u00b522 + \u03bb)s1), \u03c82 \u00b521\u03c81 \u03c47 \u2192 \u00b5\u22171\u03b82\u03c82(1\u2212 (\u00b522 + \u03bb)s1), \u00b521\u03c8 2 1\n\u03c822 \u03c48 \u2192 s1 \u2212 \u03bbs2, \u03c49 \u2192 \u03b822\u03c82(1\u2212 (\u00b522 + \u03bb)s1),\n\u03c411 \u2192 \u03b822\u03c82(1\u2212 (\u00b522 + 2\u03bb)s1 + \u03bb(\u00b522 + \u03bb)s2), \u03c412 \u2192 \u00b5\u22171\u03b82\u03c82(1\u2212 (\u00b522 + 2\u03bb)s1 + \u03bb(\u00b522 + \u03bb)s2),\n(C.36)\nas \u03c81 \u2192 \u221e, where s1 and s2 are defined in (C.31). The trivial cases when \u00b51, \u03b7 = 0 have been studied in Theorem 30. So, WLOG, we assume \u00b51, \u03b7 > 0 in the following derivations.\nRecall the definitions of \u03c41(z),m1(z) and m2(z). One can easily see that m1,m2 \u2192 0 as \u03c81 \u2192 \u221e. For any z \u2265 0, (C.22) and (C.23) can be written as follows\n\u03c81(zm1(z)\u2212 1) + \u03c81\u00b521zm1(z)\u03c41(z) + \u00b522 \u00b521\n( 1\u2212 1\n1 + \u03c81\u00b521zm1(z)\u03c41(z)\n) = 0,\nm2(z) = m1(z) 1 + \u03c81\u00b521m1(z) ( \u03c81 \u03c82 (zm1(z)\u2212 1) + 1 ) = m1(z) 1 + \u03c81\u00b521zm1(z)\u03c41(z) . (C.37)\nNotice that \u03c41(z) = lim trR0(z), and based on [FW20, Theorem 3.4], for any z \u2265 0,\ntrR0(z) = N\nn tr\n( 1\nn\n( \u00b51X\u0303W 0 + \u00b52Z )( \u00b51X\u0303W 0 + \u00b52Z ) + N\nn zI\n)\u22121 ,\nwhere, by exchanging W 0 and X\u0303, the limit of the right hand side coincides with the Stieltjes transform \u00b5MP\u03c82/\u03c81 ( \u00b522 + \u00b5 2 1 \u00b7 \u00b5MP\u03c82 ) at \u2212z, as n/d \u2192 \u03c81 and N/d \u2192 \u03c82. Denote this Stieltjes transform at point \u2212z by m\u0303(z). Then as n/d\u2192 \u03c81 and N/d\u2192 \u03c82, we have\n\u03c41(z) = \u03c82 \u03c81 m\u0303 ( \u03c82 \u03c81 z ) . (C.38)\nTherefore, when \u03c81 \u2192\u221e, the measure \u00b5MP\u03c82/\u03c81 ( \u00b522 + \u00b5 2 1 \u00b7 \u00b5MP\u03c82 ) reduces to a deformed Marchenko\u2013Pastur\nlaw ( \u00b522 + \u00b5 2 1 \u00b7 \u00b5MP\u03c82 ) ; hence by the definition of s1,\n\u03c81 \u03c82 \u03c41 \u2192 m\u0303(\u03bb) = s1, (C.39)\nwhich verifies the first statement in (C.36). Now recall the value of interest z = \u03bb\u03c81\u03c82 . Due to the relationship between \u03c41 and m1, it is straightforward to deduce that \u03bb \u03c81 \u03c82 m1 \u2192 1 as \u03c81 \u2192 \u221e. As for \u03c42, in terms of (C.37), we have\n\u03c82 \u00b521\u03c81 \u03c42 = \u03b8 2 1\n( 1\u2212 \u03bb\u03c81\n\u03c82 m2\n) = \u03b821 ( 1\u2212\n\u03bb\u03c81\u03c82m1\n1 + \u03c81\u00b521\u03bb \u03c81 \u03c82 m1\u03c41\n) \u2192 \u03b8 2 1\u00b5 2 1\u03c82s1\n1 + \u03c82\u00b521s1 , (C.40)\nas \u03c81 \u2192 \u221e. Define z\u0303 = \u2212(\u00b522 + \u03bb)/\u00b521 < 0 and consider the Stieltjes transform m(z\u0303) > 0 of \u00b5MP\u03c82 , which satisfies the self-consistent equation\n\u03c82z\u0303m 2(z\u0303)\u2212m(z\u0303) + \u03c82m(z\u0303) + z\u0303m(z\u0303) + 1 = 0, (C.41)\nwhich can be rewritten as follows\nz\u0303m(z\u0303) + 1 = 1\n1\u2212 z\u0303 \u2212 \u03c82z\u0303m(z\u0303) > 0. (C.42)\nNote that s1 = m(z\u0303)/\u00b5 2 1. By (C.41), we can simplify (C.40) to obtain \u03c82 \u00b521\u03c81 \u03c42 \u2192 \u03b822\u03c82(1\u2212 (\u00b522 + \u03bb)s1) when \u03c81 \u2192\u221e. The calculation of \u03c46, \u03c47 and \u03c49 are essentially the same as \u03c42 based on properties of the Stieltjes transform (C.37) and (C.41), the details of which we omit.\nNext we compute the limit of m\u20321. Taking derivative with respect to z at both sides of (C.38), we arrive at \u03c821 \u03c822 \u03c4 \u20321 \u2192 \u2212s2; here \u03c4 \u20321 represents the derivative \u03c4 \u20321(z) at z = \u03bb\u03c81 \u03c82 . Combining this relation and (C.39),(C.24), we can deduce that\n\u00b521\u03c8 2 1\n\u03c822 \u03c48 =\n\u03c82 \u03c81\n( \u03c41 +\n\u03c82 \u03c81\u03bb ( \u03c81 \u03c82 \u2212 1 )) + \u03bb ( \u03c4 \u20321 + ( 1\u2212 \u03c81\u03c82 ) \u03c822 \u03c821\u03bb 2 ) \u03bb2 ( \u03c82 \u03c81 ( \u03c41 + \u03c82 \u03c81\u03bb ( \u03c81 \u03c82 \u2212 1 )))2 = \u03c81\u03c82 \u03c41 + \u03bb ( \u03c81 \u03c82 )2 \u03c4 \u20321( \u03bb\u03c41 + 1\u2212 \u03c82\u03c81 )2 \u2192 s1 \u2212 \u03bbs2.\nNote that here we used \u03c41 \u2192 0 when \u03c81 \u2192\u221e. Lastly, for \u03c411 and \u03c412, by (C.37),\nm\u20322 m21 = m\u20321 \u2212 \u03c82\u00b521\u03bb \u03c821 \u03c822 \u03c4 \u20321m 2 1 \u2212 \u03c81\u00b521m21\u03c41\nm21 ( 1 + \u03c82\u00b521\u03bb \u03c81 \u03c82 m1\u03c41 )2 = \u03c81 \u03c82 \u03c4 \u20321 + ( 1\u2212 \u03c81\u03c82 ) \u03c82 \u03c81\u03bb2 \u2212 \u03c82\u00b521 \u03c821 \u03c822 \u03bb\u03c4 \u20321 ( \u03c41 + 1 \u03bb \u2212 \u03c82 \u03c81\u03bb\n)2 ( \u03c41 + 1 \u03bb \u2212 \u03c82 \u03c81\u03bb )2( 1 + \u03c82\u00b521\u03bb \u03c81 \u03c82 m1\u03c41 )2 \u2212 \u03c81\u00b521\u03c41( 1 + \u03c82\u00b521\u03bb \u03c81 \u03c82 m1\u03c41\n)2 \u21921\u2212 \u03c82\u03bb\u00b5 2 1s2 + \u00b5 2 1\u03c82s1\n(1 + \u03c82\u00b521s1) 2 , as \u03c81 \u2192\u221e,\nwhere we applied the previously established convergence of \u03c41 and \u03c4 \u2032 1. Also recall that (C.40) implies that m2/m1 converges to 1/ ( 1 + \u03c82\u00b5 2 1s1 ) as \u03c81 \u2192\u221e. Together with the convergence of \u03c49 in (C.36), we get\n\u03c411 \u2192 \u03b822\u03c82(1\u2212 (\u00b522 + \u03bb)s1) + \u2212\u03c82\u03bb\u00b521s2\n(1 + \u03c82\u00b521s1) 2 .\nMeanwhile, we also know that s2 = m \u2032(z\u0303)/\u00b541, where z\u0303 = \u2212(\u00b522 + \u03bb)/\u00b521, and m(z\u0303) is the Stieltjes transform \u00b5MP\u03c82 . Hence by (C.41)(C.42) and taking derivative in (C.41), we have\n\u2212 \u03c82\u03bb\u00b5 2 1s2\n(1 + \u03c82\u00b521s1) 2 = \u03c82\u03bb\n( (\u00b522 + \u03bb)s2 \u2212 s1 ) ,\nwhich implies the convergence of \u03c411 in (C.36).\nAs a result, by replacing \u03c4i\u2019s in (C.28) with the corresponding reparameterized \u03c4i\u2019s in (C.36), we arrive at the following expression of \u03b4:\nlim \u03c81\u2192\u221e lim n,d,N\u2192\u221e\n(R0(\u03bb)\u2212R1(\u03bb)) = \u00b5\u221721\n( \u00b521\u03b8 2 2s1\u03b1\u03b2\n\u00b521\u03b8 2 2s1\u03b1\u2212 1\n\u2212 \u00b5 2 1\u03b8 2 2\u03b1\u03b3\n(\u00b521\u03b8 2 2s1\u03b1\u2212 1)\n2\n) , (C.43)\nin probability, where we defined\n\u03b1 :=\u03c82 \u2212 1\u2212 \u03c82(\u00b522 + \u03bb)s1, (C.44)\n\u03b2 :=1\u2212 \u03c82 + \u03c82\u00b522s1 + \u03bb\u03c82(\u00b522 + \u03bb)s2, (C.45) \u03b3 :=s1\u03b2 + \u03b1(s1 \u2212 \u03bbs2) = \u03bb ( 2(\u00b522 + \u03bb)\u03c82s1s2 \u2212 \u03c82s21 + s2(1\u2212 \u03c82) ) . (C.46)\nBy definitions of A,B,C in (C.33), (C.34) and (C.35), we can see that A = \u2212\u00b521\u03b822\u03b1s1, B = \u03b2 and C = \u2212\u00b521\u03b822\u03b1\u03b3; this leads to the equivalent expression\nlim \u03c81\u2192\u221e lim n,d,N\u2192\u221e\n(R0(\u03bb)\u2212R1(\u03bb)) = \u00b5\u221721 ( AB\nA+ 1 +\nC\n(A+ 1)2\n) =: \u03b4(\u03b7, \u03bb,\u221e, \u03c82).\nNow we claim that A,B,C are all non-negative, for any \u03b7, \u03bb, \u03c82 \u2265 0. With a slight abuse of terminology, in the following we denote z = \u2212(\u00b522 + \u03bb)/\u00b521 < 0. Recall that s1 = m(z)/\u00b521 and s2 = m\u2032(z)/\u00b541; We can therefore simplify (C.44), (C.45) and (C.46) as follows\n\u03b1 =\u03c82 \u2212 1 + \u03c82zm(z) (i) = \u22121 + zm(z)\nm(z) < 0,\n\u03b2 =1\u2212 \u03bb \u00b521 \u03c82(m(z) + zm \u2032(z))\u2212 \u03c82(zm(z) + 1), \u03b3 = \u03bb\n\u00b541\n( (1\u2212 \u03c82)m\u2032(z)\u2212 2\u03c82zm(z)m\u2032(z)\u2212 \u03c82m2(z) ) (ii) = \u03bb\n\u00b541 (m(z) + zm\u2032(z)),\nwhere (i) is due to (C.42) and (ii) is obtained by taking derivative with respect to z in (C.41). In addition,\nm(z) + zm\u2032(z) =\n\u222b \u00b541x\n(\u00b521x+ \u00b5 2 2 + \u03bb)\n2 d\u00b5MP\u03c82 (x) > 0,\nwhich implies that \u03b3 > 0. We also denote the companion Stieltjes transform of m(z) by m\u0304(z), which is the Stieltjes transform of the limiting eigenvalue distribution for W 0W > 0 . Recall the following relation between m(z) and m\u0304(z): m\u0304(z) + 1z = \u03c82 ( m(z) + 1z ) . Since m(z) + zm\u2032(z) is positive, we can deduce that\n\u03b2 \u22651\u2212 \u03bb+ \u00b5 2 2\n\u00b521 \u03c82(m(z) + zm\n\u2032(z))\u2212 \u03c82(zm(z) + 1)\n=1 + \u03c82 ( zm(z) + z2m\u2032(z) ) \u2212 \u03c82(zm(z) + 1) = 1 + \u03c82 ( z2m\u2032(z)\u2212 1 ) = z2m\u0304\u2032(z) > 0,\nwhere the last equality is obtained by taking derivative of (B.36) on both sides with respect to z. In summary, we have shown that \u03b1 < 0 and \u03b2, \u03b3 > 0 when \u03bb, \u00b51 > 0. Hence by definition, A,B,C are all non-negative and so is \u03b4(\u03b7, \u03bb,\u221e, \u03c82).\nFinally, we verify that \u03b4(\u03b7, \u03bb,\u221e, \u03c82) is an increasing function of \u03b7 \u2265 0. Observe that \u03b7 only appears in \u03b82 in the expression of \u03b4(\u03b7, \u03bb,\u221e, \u03c82) in (C.43). Hence, it suffices to take the derivative of \u03b4(\u03b7, \u03bb,\u221e, \u03c82) with respect to \u03b82 and verify that this partial derivative is positive. One can check that\n\u2202\n\u2202\u03b7 \u03b4(\u03b7, \u03bb,\u221e, \u03c82) = 2\u03b82\u00b51\u00b5\u22171 \u00b7\n\u00b541\u03b8 2 2\u03b1 2s1(\u03b3 \u2212 s1\u03b2) + \u00b521\u03b1(s1\u03b2 + \u03b3)) (\u00b521\u03b8 2 2s1\u03b1\u2212 1) 3 .\nBy the definition of \u03b3 in (C.46), we have (\u03b3 \u2212 s1\u03b2) = \u03b1(s1\u2212 \u03bbs2). Also from (C.31) we know that \u03bbs2 \u2264 s1. Finally, recall that \u03b1 < 0 and \u03b2, \u03b3 > 0; this implies \u03b4(\u03b7, \u03bb,\u221e, \u03c82) is increasing with regard to \u03b7 \u2208 [0,+\u221e) and completes the proof.\nC.4.2 Case II: Highly overparameterized regime\nNext we consider the large width limit \u03c82 \u2192\u221e and establish Proposition 9. Proof of Proposition 9. We first highlight that the constant \u00b52 > 0 since the activation \u03c3 is a nonlinear function. Similar to the proof of Theorem 31, we need to consider the limits of \u03c4i defined in Proposition 29\nas \u03c82 \u2192\u221e. We first study the asymptotics of m1,m2,m\u20321 and m\u20322 as \u03c82 \u2192\u221e.Recall that in Proposition 29, m1(z) is the Stieltjes transform of limiting spectrum of CK matrix at \u2212z. In fact, the limiting eigenvalue distribution of CK is \u00b5MP\u03c81/\u03c82 ( \u00b522 + \u00b5 2 1 \u00b7 \u00b5MP\u03c81 ) , which has been analyzed in [FW20, Theorem 3.4]. Therefore due to [FW20, Equation (6)], we know that for any z \u2208 C+ \u222aR+, m1(z) satisfies the self-consistent equation\nm1(z) =\n\u222b d\u00b5MP\u03c81 (x)\n(\u00b521x+ \u00b5 2 2) ( 1\u2212 \u03c81\u03c82 + \u03c81 \u03c82 zm1(z) ) + z . (C.47)\nNote that 0 \u2264 zm1(z) \u2264 1, for all z \u2265 0, and thus 0 \u2264 \u03c81\u03c82 zm1(z) \u2264 \u03c81 \u03c82 . On the other hand, \u00b5MP\u03c81 is compactly supported. Therefore, by taking z = \u03c81\u03bb/\u03c82 and letting \u03c82 \u2192\u221e at both sides of (C.47), we arrive at\nlim \u03c82\u2192\u221e m1 =\n\u222b d\u00b5MP\u03c81 (x)\n\u00b521x+ \u00b5 2 2\n\u2208 (0, 1/\u00b522),\nwhich is a finite positive value determined by \u03c81, \u00b51, \u00b52. With this in mind, we conclude that lim\u03c82\u2192\u221em2 is also finite, since m2(z) is determined by (C.37) and we can take z = \u03c81\u03bb/\u03c82 with \u03c82 \u2192\u221e. In addition, since 0 \u2264 \u2212zm\u20321(z) \u2264 m1(z) for any z \u2265 0, we may take the derivative with respect to z at both sides of (C.47) to obtain m\u20321(z), and take z = \u03c81\u03bb/\u03c82 and \u03c82 \u2192\u221e to conclude that the limit of m\u20321 is finite as well. Similarly, by taking derivative with respect to z in (C.37), one can also verify that as \u03c82 \u2192\u221e, the limit of m\u20322 remains finite. From these estimates we know that\n\u03c42 \u2212 \u03c43 =\u2212 \u00b521\u03b821 ( \u03c81 \u03c82 )2 \u03bbm2,\n\u03c47 \u2212 \u03c45 =\u2212 \u00b521\u00b5\u22171\u03b82 ( \u03c81 \u03c82 )2 \u03bbm2,(\n\u03c81 \u03c82\n)3 \u03c48 = 1\nm1\n1\n\u00b521\u03bb 2 ( \u03c81 \u03c82 ) + m\u20321 m21 ( \u03c81 \u03c82 )2 1 \u00b521\u03bb ,\nare vanishing as \u03c82 \u2192\u221e, whereas\n\u03c44 + \u03c412 \u2212 2\u03c46 = \u2212\u00b5\u22171\u03b82 m\u20322 m21 , \u03c411 + \u03c410 \u2212 2\u03c49 = \u2212\u03b821 m\u20322 m21 , \u03c81 \u03c82 \u03c41 = ( \u03c81 \u03c82 )2 m1 + ( 1\u2212 \u03c81 \u03c82 ) 1 \u03bb\nwill converge to some finite values. The proposition is established based on the definition of \u03b4(\u03b7, \u03bb, \u03c81, \u03c82) with the help of the above statements.\nD Proof for Large Learning Rate (\u03b7 = \u0398( \u221a N))\nIn this section we restrict ourselves to a single-index target function (generalized linear model): f\u2217(x) = \u03c3\u2217(\u3008x,\u03b2\u2217\u3009), and study the impact of one gradient step with large learning rate \u03b7 = \u0398( \u221a N). For simplicity,\nwe denote \u03b7 = \u03b7\u0304 \u221a N where \u03b7\u0304 > 0 is a fixed constant not depending on N .\nAs the Gaussian equivalence property is no longer applicable, we instead establish an upper bound on the prediction risk of the CK ridge estimator. Our proof is divided into two parts: (i) we show that there exists an \u201coracle\u201d second-layer a\u0303 that achieves small prediction risk \u03c4\u2217 when n/d is large; (ii) based on \u03c4\u2217, we provide an upper bound on the prediction risk when the second layer is estimated via ridge regression.\nHere we provide a short summary on the construction of a\u0303 and upper bound on the prediction risk. \u2022 We first introduce fr(x) := 1|Ar| \u2211 i\u2208Ar \u03c3 ( \u3008x,w1i \u3009 ) , which is the average of a subset of neurons in Ar \u2282 [N ]\ndefined in (D.5). Intuitively, this subset of neurons approximately matches the target direction \u03b2\u2217. This averaging corresponds to setting the second-layer a\u0303i = \u221a N |Ar| for all i \u2208 Ar.\n\u2022 We show that fr can be approximated up to \u0398(d/n)-error by an \u201cexpected\u201d single-index model f\u0304(x) := Ew\u223cN (0, I/d)[\u03c3(\u3008w + c\u03b2\u2217,x\u3009)], for some c \u2208 R that depends on the learning rate and nonlinearities. To bound this substitution error, we establish a more refined control of gradient norm in Section D.1.\n\u2022 By choosing an \u201coptimal\u201d subset Ar, we simplify the prediction risk of f\u0304 into the one-dimensional expectation \u03c4\u2217 defined in (4.3). This provides a high-probability upper bound of the prediction risk of the constructed a\u0303 up to \u0398(d/n)-error.\nAfter constructing some a\u0303 that achieves reasonable test performance, we can then show that the prediction risk of CK ridge regression estimator with trained weight W 1 is also upper-bounded by \u03c4\n\u2217 when n d. This result is established in Section D.3 and follows from classical analysis of kernel ridge regression."
        },
        {
            "heading": "D.1 Refined Properties of the First-step Gradient",
            "text": "Recall that W 1 = W 0 + \u03b7 \u221a NG0, where G0 = A1 +A2 +B +C is defined in Lemma 14 and 15, and the full-rank term B is given as\nB = 1 n \u00b7 1\u221a N X>\n( ya> \u03c3\u2032\u22a5(XW 0) ) .\nWe first refine the estimate on the Frobenius norm of certain submatrix of B; the choice of such submatrices will be explained in Section D.2.\nLemma 32. Given Assumptions 1 and 2, take Br \u2208 Rd\u00d7Nr which is a submatrix of B via selecting any Nr \u2208 [N ] columns in B, and let ar \u2208 RNr be the corresponding 2nd layer coefficients. If entries of ar are uniformly bounded by \u03b1/ \u221a N , then for any \u03b5 \u2208 (0, 1/4), we have\nE\u2016Br\u20162F \u2264 C0\u03b1 2Nr N\n( 1\nNd 1 2\u2212\u03b5\n+ d\nnN\n) , (D.1)\nand\nP ( N\nNr \u2016Br\u20162F \u2264\nC1\nNd 1 4\u2212\u03b5\n+ C2d\nNn\n) \u2265 1\u2212 \u03b1 2\nd 1 4\n\u2212 \u03b1 4\nn , (D.2)\nwhere constants C0, C1, C2 > 0 only depend on \u03bb\u03c3 and \u2016f\u2217\u2016L2(Rd,\u0393).\nProof. Let X> = (x1,x2, . . . ,xn) and y > = (y1, . . . , yn). Then, matrix Br can be written as\nBr = 1\nn \u221a N n\u2211 i=1 yixi\u03c3 \u2032 \u22a5(x > i W r 0) diag(ar),\nwhere W r0 \u2208 Rd\u00d7Nr is a submatrix of W 0 by choosing any Nr columns of W 0, and ar \u2208 RNr is the corresponding second layer (note that by assumption \u2016ar\u2016\u221e \u2264 \u03b1/ \u221a N). Hence,\n\u2016Br\u20162F = Tr(BrB > r ) =\n1\nn2N n\u2211 i,j=1 yiyj Tr [ xi\u03c3 \u2032 \u22a5(x > i W r 0) diag(ar) 2\u03c3\u2032\u22a5(x > j W r 0) >x>j ] = 1\nn2N n\u2211 i,j=1 yiyj ( \u03c3\u2032\u22a5(x > i W r 0) diag(ar) 2\u03c3\u2032\u22a5(x > j W r 0) >x>j xi ) = 1\nn2N n\u2211 i 6=j yiyj ( \u03c3\u2032\u22a5(x > i W r 0) diag(ar) 2\u03c3\u2032\u22a5(x > j W r 0) >x>j xi ) + 1\nn2N n\u2211 i=1 y2i ( \u03c3\u2032\u22a5(x > i W r 0) diag(ar) 2\u03c3\u2032\u22a5(W r> 0 xi)\u2016xi\u20162 ) =: J1 + J2.\nHere, J1 represents the sum for distinct i 6= j \u2208 [n] and J2 is the sum when i = j \u2208 [n]. Therefore,\nE[\u2016Br\u20162F ] \u2264 \u03b12Nr N n(n\u2212 1) n2N\nE [ f\u2217(x1)f \u2217(x2)\u03c3 \u2032 \u22a5(x > 1 w)\u03c3 \u2032 \u22a5(x > 2 w)x > 2 x1 ]\n+ \u03b12Nr nN2\nE [ (f\u2217(x1) 2 + \u03c32\u03b5)\u03c3 \u2032 \u22a5(x > 1 w) 2\u2016x1\u20162 ] ,\nwhere w \u223c N (0, I) independent of a and X. We compute the aforementioned expectations as follows\nN\n\u03b12Nr E[\u2016B\u20162F ] \u2264\n1\nN \u2223\u2223E[f\u2217(x1)f\u2217(x2)\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)x>2 x1]\u2223\u2223 + 1\nnN E [ f\u2217(x1) 2\u03c3\u2032\u22a5(x > 1 w) 2\u2016x1\u20162 ] + \u03c32\u03b5 nN E [ \u03c3\u2032\u22a5(x > 1 w) 2\u2016x1\u20162 ] =: I1 + I2 + I3.\nTo verify (D.1), we in turn control I1, I2 and I3. Since the target function is a single-index model f\u2217(x) = \u03c3\u2217(\u3008x,\u03b2\u2217\u3009) and \u03c3\u2217 is Lipschitz, it is clear that f\u2217 belongs to L2(Rd,\u0393). Besides, x1,x2 are two independent standard Gaussian random vectors. Therefore, Ex1,x2 [ |f\u2217(x1)f\u2217(x2)|2 ] = \u2016f\u2217\u20164L2(Rd,\u0393). Now given any t \u2208 (0, 1), define event by\nAt := { |x>1 x2| d \u2264 t, \u2223\u2223\u2223\u2223\u2016xi\u2016\u221ad \u2212 1 \u2223\u2223\u2223\u2223 \u2264 t, for i = 1, 2}. Using the same rotational invariance argument as Step 1 in the proof of Lemma 27, WLOG, we can further consider \u03b2\u2217 \u223c Unif(Sd\u22121) independent of x1,x2 and w, because x1,x2 and w are rotationally invariant in distribution. Thus, for I1, we have\nI1 = 1\nN \u2223\u2223\u2223Ex1,x2[E\u03b2\u2217 [\u03c3\u2217(\u03b2>\u2217 x1)\u03c3\u2217(\u03b2>\u2217 x2)]Ew[\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)]x>2 x1]\u2223\u2223\u2223 \u2264 1 N Ex1,x2 [\u2223\u2223\u2223E\u03b2\u2217 [\u03c3\u2217(\u03b2>\u2217 x1)\u03c3\u2217(\u03b2>\u2217 x2)]\u2223\u2223\u2223 \u00b7 \u2223\u2223Ew[\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)]\u2223\u2223 \u00b7 \u2223\u2223x>2 x1\u2223\u2223].\nSince E\u03be\u223cN (0,1)[\u03c3\u2217(\u03be)] = 0, we can adopt Lemma A.5 of [MZ20] (with a slight modification) to conclude that conditioned on event At, we have \u2223\u2223\u2223E\u03b2\u2217 [\u03c3\u2217(\u03b2>\u2217 x1)\u03c3\u2217(\u03b2>\u2217 x2)]\u2223\u2223\u2223 \u2264 Ct, for some constant C > 0. In addition, based on Lemma D.3 and G.1 in [FW20], we can show the following inequality for any t \u2208 (0, 1) under event At:\u2223\u2223Ew[\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)]\u2223\u2223 \u2264 Ct. (D.3) Let \u03b61 := w >x1 and \u03b62 := w >x2. Conditioned on x1,x2, we know that\n(\u03b61, \u03b62) \u223c N ( 0, [ \u2016x1\u20162/d x>1 x2/d x>1 x2/d \u2016x2\u20162/d ]) .\nNow we make the reparameterization: \u03b61 = \u03b31\u03be1, \u03b62 = \u03b32\u03be2 + \u03bd2\u03be1, where \u03be1, \u03be2 i.i.d.\u223c N (0, 1) are independent to x1 and x2, and\n\u03b31 := \u2016x1\u2016\u221a d , \u03b32 := \u221a \u2016x2\u20162 d \u2212 (x > 1 x2) 2 d\u2016x1\u20162 , \u03bd2 := x>1 x2\u221a d\u2016x1\u2016 .\nFor i = 1, 2, by Taylor expansion of \u03c3(\u03b6i) around \u03bei (note that \u03c3 is differentiable by assumption), there exists a random variable \u03b7i between \u03b6i and \u03bei such that\n\u03c3\u2032\u22a5(\u03b6i) = \u03c3 \u2032 \u22a5(\u03bei) + \u03c3 \u2032\u2032(\u03b7i)(\u03b6i \u2212 \u03bei),\nwhere we use \u03c3\u2032\u2032\u22a5 = \u03c3 \u2032\u2032. Because E[\u03c3\u2032\u22a5(\u03bei)] = 0 and |\u03c3\u2032\u22a5(x)|, |\u03c3\u2032\u2032(x)| \u2264 \u03bb\u03c3 almost surely for all x, we have\n|Ew[\u03c3\u2032\u22a5(\u03b61)\u03c3\u2032\u22a5(\u03b62)]| \u2264 C(|\u03bd2|+ |\u03b31 \u2212 1|) \u2264 Ct,\non the event At with t \u2208 (0, 1), where C > 0 is a constant depending on \u03bb\u03c3. This concludes (D.3).\nAlso, note the probability bound for Gaussian random vector x1 and x2 implies that P(Act) \u2264 4 exp ( \u2212dt2 ) .\nFrom the above arguments, we can bound the first term I1 via the following steps:\nI1 \u2264 1\nN Ex1,x2 [\u2223\u2223\u2223E\u03b2\u2217 [\u03c3\u2217(\u03b2>\u2217 x1)\u03c3\u2217(\u03b2>\u2217 x2)]\u2223\u2223\u2223 \u00b7 \u2223\u2223Ew[\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)]\u2223\u2223 \u00b7 \u2223\u2223x>2 x1\u2223\u2223 \u00b7 1At] + 1\nN Ex1,x2 [\u2223\u2223\u2223E\u03b2\u2217 [\u03c3\u2217(\u03b2>\u2217 x1)\u03c3\u2217(\u03b2>\u2217 x2)]\u2223\u2223\u2223 \u00b7 \u2223\u2223Ew[\u03c3\u2032\u22a5(x>1 w)\u03c3\u2032\u22a5(x>2 w)]\u2223\u2223 \u00b7 \u2223\u2223x>2 x1\u2223\u2223 \u00b7 1Act ] \u2264Ct 3d\nN + \u03bb2\u03c3 N E[f\u2217(x1)2f\u2217(x2)2] 1 2E [\u2223\u2223x>2 x1\u2223\u222321Act ] 12\n\u2264Ct 3d N + \u03bb2\u03c3 N E [\u2223\u2223x>2 x1\u2223\u22234] 14E[1Act ] 14 \u2264Ct 3d\nN + 4\u03bb2\u03c3 N\nE [ \u2016x1\u20164 ] 1 2 e \u2212dt2 4 = C\u03bb2\u03c3t 3d\nN +\n12\u03bb2\u03c3d\nN e \u2212dt2 4 ,\nwhere we used \u2016\u03c3\u2032\u22a5\u2016\u221e \u2264 \u03bb\u03c3 and the fact that E[f\u2217(x1)2]1/2 = \u2016f\u2217\u2016L2(R,\u0393) is finite. For any \u03b5 \u2208 (0, 1/2), if we choose t = d\u03b5\u22121/2, then we can conclude I1 \u2264 C dN d\n\u03b5\u22123/2, for all large d, sufficiently large constant C > 0 and sufficient small \u03b5. Next we consider I2 and I3. Notice that\nI2 \u2264 \u03bb2\u03c3 nN E[f\u2217(x1)4] 1 2E[\u2016x1\u20164] 1 2 \u2264 3C\u03bb 2 \u03c3d nN ,\nbecause \u03c3\u2217 is Lipschitz and f\u2217 \u2208 L4(Rd,\u0393). Following the same computation, we also have I3 \u2264 \u03bb 2 \u03c3d nN . This establishes a bound for E[[\u2016B\u20162F ] in (D.1). For the tail control (D.2), recall that \u2016Br\u20162F = J1 +J2 where E[|J1|] \u2264 \u03b12Nr N I1 and E[J2] \u2264 \u03b12Nr N (I2 +I3). Hence Markov\u2019s inequality and the upper bound for I1 implies that\nP ( N\nNr |J1| \u2265 t\n) \u2264 C\u03b1 2\ntd 1 2\u2212\u03b5N\n.\nBy choosing t = C/Nd 1 4\u2212\u03b5, we conclude that NNr |J1| cannot exceed C/Nd 1 4\u2212\u03b5 with probability at least 1 \u2212 \u03b12/d 14 , for any \u03b5 \u2208 (0, 1/4). As for J2, since \u03c3\u2032\u22a5 is uniformly bounded by \u03bb\u03c3 and all entries of ar are bounded by \u03b1/ \u221a N , we have\nN Nr |J2| \u2264\n\u03bb2\u03c3\u03b1 2 Nn2 n\u2211 i=1 y2i \u2016xi\u20162 =: J \u20322.\nSimilarly for I2 and I3, it is easy to check |E[J \u20322]| \u2264 3\u03b1 2Cd Nn . Besides,\nVar(J \u20322) = \u03bb4\u03c3\u03b1 4\nN2n3 Var(y21\u2016x1\u20162) \u2264\n\u03bb4\u03c3\u03b1 4 N2n3 E[y41\u2016x1\u20164] \u2264 c\u03b14d2 N2n3 ,\nwhere constant c > 0 only depends on \u03bb\u03c3 and \u2016f\u2217\u2016L8(R,\u0393). By Chebyshev\u2019s inequality,\nP(|J \u20322 \u2212 E[J \u20322]| > t) \u2264 c\u03b14d2\nt2N2n3 .\nLetting t = \u221a cd/Nn, we arrive at\nP ( N\nNr J2 \u2264\n\u221a cd Nn + 3Cd Nn\n) \u2265 P ( J \u20322 \u2264 \u221a cd\nNn +\n3Cd\nNn\n) \u2265 1\u2212 \u03b1 4\nn .\nWe conclude (D.2) by combining the above estimates of J1 and J2."
        },
        {
            "heading": "D.2 Constructing the \u201cOracle\u201d Estimator",
            "text": "In this subsection we prove the following lemma related to Lemma 10. Lemma 33 (Reformulation of Lemma 10). Suppose Assumptions 1 and 2 hold, \u03b7 = \u0398( \u221a N) and the activation \u03c3 is bounded. Then given any \u03b5 > 0, for N sufficiently large, there exists some constant C and second-layer a\u0303 such that the model f\u0303(x) = 1\u221a\nN a\u0303>\u03c3(W>1 x) has prediction risk\nR(f\u0303) \u2264 \u03c4\u2217 + C ( \u221a \u03c4\u2217 \u00b7 \u221a d\nn + d n\n) + \u03b5+ od,P(1), (D.4)\nwhere the scalar \u03c4\u2217 is defined in (4.3).\nWe first introduce a constant \u03b1 (independent to N). Recall that [a]i = ai i.i.d.\u223c N ( 0, N\u22121 ) for i \u2208 [N ].\nFor any \u03b1 \u2208 R, define the subset of initialized weights: A\u03b1r = { i \u2208 [N ] : \u2223\u2223\u2223\u221aN \u00b7 ai \u2212 \u03b1\u2223\u2223\u2223 \u2264 N\u2212r}, for any given r > 0. (D.5) The size of the subset is given by |A\u03b1r | = \u2211N i=1 1|\u221aNai\u2212\u03b1|\u2264N\u2212r , and hence its expectation is E|A \u03b1 r | =\nN \u00b7 Ez\u223cN (0,1) [ 1|z\u2212\u03b1|\u2264N\u2212r ] = C(\u03b1)N1\u2212r for some constant C(\u03b1) \u221d exp ( \u2212\u03b12 ) . By Hoeffding\u2019s inequality,\nP(||A\u03b1r | \u2212 E|A\u03b1r || \u2265 t) \u2264 2 exp ( \u22122t 2\nN\n) . (D.6)\nHence we may conclude that for any r \u2208 (0, 1/2) and large enough N , |A\u03b1r | = \u0398d,P(N1\u2212r) with probability at least 1\u2212 2 exp ( \u2212c log2N ) . This is to say, for any constant \u03b1, we know that with high probability, there exist a large number of initialized second-layer coefficients ai\u2019s that are close to \u03b1. We specify our choice of \u03b1 \u2208 R via (4.3) in the subsequent analysis.\nRank-1 Approximation of the Gradient. Denote Nr := |A\u03b1r | for some constant \u03b1, and ir \u2208 [N ] as the index such that ir \u2208 A\u03b1r . We define fr as an average over neurons with indices ir \u2208 A\u03b1r , and fA as an approximation of fr in which the first-step gradient matrix G0 in (B.2) is replaced by the rank-1 matrix A1 defined in (B.18):\nfr(x) := 1\nNr \u2211 ir\u2208A\u03b1r \u03c3 ( \u3008x,w1ir \u3009 ) ; fA(x) := 1 Nr \u2211 ir\u2208A\u03b1r \u03c3 ( \u3008x,wAir \u3009 ) , (D.7)\nwhere w1ir is the ir-th neuron in W 1, w A i = w 0 i + \u03b7 \u221a N [A1]i, [A1]i is the i-th column of A1 and w 0 i \u2208 Rd is the corresponding initial neuron in W 0. Applying the Lipschitz property of the activation function, one can control Ex[(fA(x)\u2212 fr(x))2] as follows\n|fA(x)\u2212 fr(x)| . 1\nNr \u2211 ir\u2208A\u03b1r \u2223\u2223\u3008w1ir \u2212wAir ,x\u3009\u2223\u2223 = \u03b7 \u221a N Nr \u2211 ir\u2208A\u03b1r |\u3008\u03b4ir ,x\u3009|.\nwhere the \u201cresidual\u201d is entry-wisely defined as [\u03b4i]j := [A2 +B +C]ji for i \u2208 [N ] and j \u2208 [d]. Recall that A2,B and C have been analyzed in Lemmas 14 and 15. Let us further denote Ar \u2208 Rd\u00d7Nr as a submatrix of A2 by selecting all ir \u2208 A\u03b1r columns of A2. Similarly, we choose Br,Cr \u2208 Rd\u00d7Nr as submatrices of B,C related to A\u03b1r , respectively. Using Lemma 15 applied to the submatrix, we have\nP ( N\nNr \u2016Ar\u20162F \u2265\nCd\nnN\n) \u2264 C \u2032 ( ne\u2212c \u221a n + 1\nd\n) . (D.8)\nMoreover, by definition of A\u03b1r , all air \u2019s are close to \u03b1\u221aN for ir \u2208 A \u03b1 r ; thus Lemma 32 (in particular (D.2)) can be directly applied to Br. As for Cr, since \u2016Cr\u2016F \u2264 \u2016C\u2016F , we use part (iii) in Lemma 14 to obtain\nP ( \u2016Cr\u2016F \u2265 C log n logN\nN\n) \u2264 C \u2032 ( ne\u2212c log 2 n +Ne\u2212c log 2N ) . (D.9)\nWith these concentration estimates, we know that when n > d,\nEx[(fA(x)\u2212 fr(x))2] . Ex  \u03b7\u221aN\nNr \u2211 ir\u2208A\u03b1r |\u3008\u03b4ir ,x\u3009|\n2  = \u03b72N\nN2r Ex  \u2211 ir,jr\u2208A\u03b1r \u2223\u2223\u2223\u03b4>irx\u2223\u2223\u2223\u2223\u2223\u2223\u03b4>jrx\u2223\u2223\u2223 \n\u2264\u03b7 2N\nN2r \u2211 ir,jr\u2208A\u03b1r Ex [( \u03b4>irx )2] 12 Ex [( \u03b4>jrx )2] 12 = \u03b72N N2r \u2211 ir,jr\u2208A\u03b1r \u2016\u03b4ir\u2016\u2016\u03b4jr\u2016\n= \u03b72N\nN2r  \u2211 ir\u2208A\u03b1r \u2016\u03b4ir\u2016 2 \u2264 \u03b72N Nr ( \u2016Ar\u20162F + \u2016Br\u20162F + \u2016Cr\u20162F ) . d n + 1 d 1 4\u2212\u03b5 + log2 n log2N Nr , (D.10)\nwith probability at least 1 \u2212 c ( \u03b12\nd 1 4\n+ \u03b1 4\nn + 1\u221a N + ne\u2212c log 2 n +Ne\u2212 log\n2N )\nfor some constant c > 0; this is\ndue to the defined step size \u03b7 = \u0398( \u221a N), (D.8), (D.9) in (D.2) of Lemma 32 outlined above. In (D.10), we ignore the constants in the upper bound since we are only interested in the rate with respect to n, d,N .\nSimplification under \u201cPopulation\u201d Gradient. Recall the definition of the single-index teacher: f\u2217(x) = \u03c3\u2217(\u3008x,\u03b2\u2217\u3009), and the definition of rank-1 matrix A1 = 1n \u00b7 \u00b51\u00b5 \u2217 1\u221a N X>X\u03b2\u2217a >. Define v = \u03b7\u00b51\u00b5 \u2217 1 n \u221a N X>X\u03b2\u2217 \u2208 Rd, we can write\nfA(x) = 1\nNr \u2211 ir\u2208A\u03b1r \u03c3 ( \u3008wir + \u221a Nairv,x\u3009 ) , f\u0303A(x) := 1 Nr \u2211 ir\u2208A\u03b1r \u03c3(\u3008wir + \u03b1v,x\u3009),\nwhere we dropped the superscript in the initialized weights w0ir to simplify the notation. Note that the difference between fA and f\u0303A is that the each second-layer coefficient ai is replaced by the same scalar \u03b1.\nBy the definition of A\u03b1r and the Lipschitz property of \u03c3, one can obtain\u2223\u2223\u2223fA(x)\u2212 f\u0303A(x)\u2223\u2223\u2223 . 1 Nr \u2211 ir\u2208A\u03b1r \u03b7N\u2212r\u221a N \u00b7 \u2223\u2223\u2223\u2223\u2329 1nX>X\u03b2\u2217,x \u232a\u2223\u2223\u2223\u2223 . N\u2212r \u00b7 \u2223\u2223\u2223\u2223\u2329 1nX>X\u03b2\u2217,x \u232a\u2223\u2223\u2223\u2223. (D.11)\nNow define v\u0304 := \u03b7\u00b51\u00b5 \u2217 1\u221a\nN \u03b2\u2217 = \u03b7\u0304\u00b51\u00b5\n\u2217 1\u03b2\u2217, which corresponds to the \u201cpopulation\u201d version of v, and denote\nf\u0304A(x) := 1\nNr \u2211 ir\u2208A\u03b1r \u03c3(\u3008wir + \u03b1v\u0304,x\u3009). (D.12)\nSimilar to (D.11), we have\u2223\u2223\u2223f\u0304A(x)\u2212 f\u0303A(x)\u2223\u2223\u2223 . 1 Nr \u2211 ir\u2208A\u03b1r \u03b7\u221a N \u2223\u2223\u2223\u2223\u2329( 1nX>X \u2212 I ) \u03b2\u2217,x \u232a\u2223\u2223\u2223\u2223 . \u2223\u2223\u2223\u2223\u2329( 1nX>X \u2212 I ) \u03b2\u2217,x \u232a\u2223\u2223\u2223\u2223. (D.13) Combining the inequalities (D.11) and (D.13), we know that for some constant C,\nEx[(fA(x)\u2212 f\u0304A(x))2] . N\u22122r \u00b7 Ex (\u2329 1\nn X>X\u03b2\u2217,x\n\u232a)2 + Ex (\u2329( 1\nn X>X \u2212 I\n) \u03b2\u2217,x \u232a)2 \u2264 ( N\u22122r \u2225\u2225\u2225\u2225 1nX>X \u2225\u2225\u2225\u22252 + \u2225\u2225\u2225\u2225 1nX>X \u2212 I \u2225\u2225\u2225\u22252 ) \u00b7 \u2016\u03b2\u2217\u2016 2\n. (( 1 + d\nn\n) N\u22122r + d\nn\n) ,\nwhere the last inequality holds with probability at least 1\u2212 exp(\u2212cd) for some universal constant c > 0, due to the operator norm bound and concentration of the sample covariance matrix 1nX\n>X (for instance see [Ver18, Theorem 4.6.1]).\nNow we take the expectation of f\u0304A over initial weight wir in (D.12) to define\nf\u0304(x) := Ew\u223cN (0, d\u22121I)[\u03c3(\u3008w + \u03b1v\u0304,x\u3009)].\nNote that for fixed x, \u3008w,x\u3009 \u223c N (0, \u2016x\u20162/d). Since \u03c3 is \u03bb\u03c3-Lipschitz, by the Hoeffding bound on subGaussian random variables, conditioned on x, we have\nP (\u2223\u2223f\u0304A(x)\u2212 f\u0304(x)\u2223\u2223 > t \u2223\u2223x) \u2264 2 exp(\u2212 t2Nr\n2\u03bb2\u03c3 \u00b7 \u2016x\u2016 2 2/d\n) , (D.14)\nAlso notice that\nEw(f\u0304(x)\u2212 f\u0304A(x))2 = \u222b \u221e\n0\nP (\u2223\u2223f\u0304A(x)\u2212 f\u0304(x)\u2223\u22232 > t \u2223\u2223x) dt\n\u2264 \u222b \u221e\n0\n2 exp ( \u2212 tNr\n2\u03bb2\u03c3 \u00b7 \u2016x\u2016 2 2/d\n) dt = 4\u03bb2\u03c3\u2016x\u20162\nNrd .\nThus, by taking expectation over x in the above bound, we know that E(f\u0304(x)\u2212 f\u0304A(x))2 \u2264 4\u03bb 2 \u03c3E[\u2016x\u2016 2] Nrd = 4\u03bb2\u03c3 Nr\n. By Markov\u2019s inequality, we have\nP ( Ex(f\u0304(x)\u2212 f\u0304A(x))2 \u2265 t ) \u2264 E(f\u0304(x)\u2212 f\u0304A(x)) 2\nt \u2264 4\u03bb\n2 \u03c3\nNrt . (D.15)\nHence we deduce that Ex(f\u0304(x)\u2212 f\u0304A(x))2 \u2264 4\u03bb 2 \u03c3\u221a Nr with probability 1\u2212 1\u221a Nr .\nObserve that f\u0304 is given by an expectation over w in a single-index model. To calculate its difference from the true model: Ex(f\u0304(x) \u2212 f\u2217(x))2, first recall the assumption that \u2016\u03b2\u2217\u2016 = 1, and w \u223c N (0, I/d), x \u223c N (0, I). Denote \u03be1 := \u3008x,\u03b2\u2217\u3009 \u223c N (0, 1) and, condition on x, \u3008x,w\u3009 d = \u03be2\u2016x\u2016/ \u221a d, where \u03be2 \u223c N (0, 1) independent of \u03be1. Since \u03b7/ \u221a N = \u03b7\u0304, we can write \u03ba := \u03b1\u03b7\u00b51\u00b5 \u2217 1\u221a\nN = \u03b1\u03b7\u0304\u00b51\u00b5\n\u2217 1 \u2208 R. Following these definitions,\nwe have f\u0304(x) = E\u03be2 [\u03c3(\u03be2\u2016x\u2016/ \u221a d+ \u03ba\u03be1)], and\nEx(f\u0304(x)\u2212 f\u2217(x))2 = E\u03be1 ( \u03c3\u2217(\u03be1)\u2212 E\u03be2 [\u03c3(\u03ba\u03be1 + \u03be2\u2016x\u2016/ \u221a d)] )2 . (D.16)\nIn addition, given \u03ba \u2208 R, we introduce a scalar quantity\n\u03c4 := E\u03be1 ( \u03c3\u2217(\u03be1)\u2212 E\u03be2 [\u03c3(\u03ba\u03be1 + \u03be2)] )2 . (D.17)\nNote that \u03c3\u2217 \u2208 L2(R,\u0393) and \u03c3 is uniformly bounded by assumption; one can easily check that \u03c4 is uniformly bounded for all \u03ba \u2208 R. Hence \u03c4 defined above is always finite. We now show that the difference between \u03c4 and Ex(f\u0304(x)\u2212 f\u2217(x))2 is asymptotically negligible, again using the Lipschitz property of \u03c3,\u2223\u2223\u2223\u03c3(\u03ba\u03be1 + \u03be2)\u2212 \u03c3(\u03ba\u03be1 + \u03be2\u2016x\u2016/\u221ad)\u2223\u2223\u2223 . \u2223\u2223\u2223\u22231\u2212 \u2016x\u2016\u221ad\n\u2223\u2223\u2223\u2223 \u00b7 |\u03be2|. Since \u03c3\u2217 \u2208 L2(R,\u0393), and \u03c3 is uniformly bounded and Lipschitz, based on (D.16) and (D.17), we can apply the Cauchy-Schwarz inequality to get\u2223\u2223\u03c4 \u2212 Ex(f\u0304(x)\u2212 f\u2217(x))2\u2223\u2223\n. E [( \u03c3(\u03ba\u03be1 + \u03be2)\u2212 \u03c3(\u03ba\u03be1 + \u03be2\u2016x\u2016/ \u221a d) )2] 12\n. E[\u03be22 ] 1 2E [\u2223\u2223\u2223\u22231\u2212 \u2016x\u2016\u221ad \u2223\u2223\u2223\u22232 ] 1 2 \u2264 C\u221a 2d , (D.18)\nwhere the last inequality is due to property of the sub-Gaussian norm \u2016\u2016x\u2016/ \u221a d \u2212 1\u2016\u03c82 \u2264 C/ \u221a d (see e.g. [Ver18, Theorem 3.1.1]) for some universal constant C > 0.\nProof of Lemma 33. Based on above calculations, we now control the prediction risk of f\u0303 by combining the substitution errors, where f\u0303 = fr is constructed as the average over subset A\u03b1r defined in (D.7).\nGiven any \u03b1 \u2208 R and r \u2208 (0, 1/2), we define the subset A\u03b1r and the corresponding f\u0303(x) = fr(x) = 1\u221a N a\u0303>\u03c3(W>1 x), where the second-layer a\u0303 is given as [a\u0303]i = \u221a N/Nr if i \u2208 A\u03b1r , otherwise [a\u0303]i = 0. Moreover, (D.6) implies that Nr = \u0398d,P(N 1\u2212r) with probability at least 1 \u2212 exp ( \u2212 log2N ) . Therefore, together with (D.10), (D.14), (D.15), and (D.18), we know that\nEx(fr(x)\u2212 f\u0304(x))2 \u2264 Cd\nn + od,P(1); Ex(f\u2217(x)\u2212 f\u0304(x))2 = \u03c4 + od(1),\nas n, d,N \u2192\u221e, for some constant C > 0. By the Cauchy-Schwarz inequality,\nEx(f\u2217(x)\u2212 fr(x))2 \u2264 \u03c4 + C ( \u221a \u03c4 \u00b7 \u221a d\nn + d n\n) + od,P(1),\nwhere the failure probability only relates to r, \u03b1,N, d, n and is vanishing as N, d, n\u2192\u221e. For simplicity, we only keep the leading orders and ignore the subordinate terms in the exact probability bounds.\nNote that the above characterization holds for any finite \u03b1; since our goal is to construct an estimator fr that achieves as small prediction risk as possible, we optimize over \u03b1 \u2208 R by defining\n\u03c4\u2217 := inf \u03b1\u2208R\nE\u03be1 ( \u03c3\u2217(\u03be1)\u2212 E\u03be2 ( \u03c3 ( \u03b1\u03b7\u0304\u00b51\u00b5 \u2217 1 \u00b7 \u03be1 + \u03be2 )))2 , \u03c4\u2217\u03b5 := \u03c4 \u2217 + \u03b5,\nwhere \u03b5 \u2265 0 is a small constant. This definition of \u03c4\u2217 is identical to (4.3) and is always finite because \u03c4 defined in (D.17) is uniformly bounded and non-negative (observe that optimizing over \u03ba or \u03b1 \u2208 R are equivalent, since we can reparameterize \u03ba = \u03b1\u03b7\u0304\u00b51\u00b5 \u2217 1 where \u00b51, \u00b5 \u2217 1 6= 0). When \u03c4\u2217 is attained at some finite \u03b1, then we may simply set \u03b5 = 0 and define\n\u03b1\u2217 := argmin \u03b1\u2208R\nE\u03be1 ( \u03c3\u2217(\u03be1)\u2212 E\u03be2 ( \u03c3 ( \u03b1\u03b7\u0304\u00b51\u00b5 \u2217 1 \u00b7 \u03be1 + \u03be2 )))2 .\nOtherwise, observe that as a bounded and continuous function of \u03b1 on the real line, \u03c4(\u03b1) := E\u03be1 ( \u03c3\u2217(\u03be1) \u2212\nE\u03be2 ( \u03c3 ( \u03b1\u03b7\u0304\u00b51\u00b5 \u2217 1 \u00b7 \u03be1 + \u03be2 )))2 will approach its minimum at infinity. Therefore, in this case, for any \u03b5 > 0, we can find some finite \u03b1\u2217\u03b5 such that \u03c4(\u03b1 \u2217 \u03b5) \u2264 \u03c4\u2217\u03b5 = \u03c4\u2217 + \u03b5; hence, we may set \u03b1 = \u03b1\u2217\u03b5 and conclude the proof. Finally, note that given nonlinearities \u03c3 and \u03c3\u2217 (which determine the relation between \u03b5 and \u03b1\u03b5), we can take \u03b5 \u2192 0 at a slow enough rate as n, d,N \u2192 \u221e, as long as C(\u03b1) \u00b7 N1\u2212r \u2192 \u221e. Thus we also obtain an asymptotic version of Lemma 33: with probability one, there exists some second-layer a\u0303 such that the prediction risk of the corresponding student model f\u0303(x) = 1\u221a\nN a\u0303>\u03c3(W>1 x) satisfies\nR(f\u0303) \u2264 \u03c4\u2217 + C ( \u221a \u03c4\u2217 \u00b7 \u221a d\nn + d n\n) , (D.19)\nfor some constant C > 0, as n, d,N \u2192\u221e proportionally.\nThe above analysis illustrates that because of the Gaussian initialization of ai, for any \u03b7 = \u0398( \u221a N), we can find a subset of neurons A\u03b1r that receive a \u201cgood\u201d learning rate, in the sense that the corresponding (sub-) network defined by fr can achieve the prediction risk close to \u03c4 \u2217 \u03b5 when n d.\nSome Examples. Equation (D.4) reduces the prediction risk of our constructed fr to a one-dimensional Gaussian integral, which can be numerically evaluated for pairs of (\u03c3, \u03c3\u2217). Denote \u03ba\u2217 = \u03b1\u2217\u03b7\u0304\u00b51\u00b5 \u2217 1, we give a few examples in which we set \u03b5 = 0 and the corresponding \u03c4\u2217 is small. Note that due to Assumptions 1 and 2, choices of \u03c3 and \u03c3\u2217 considered below are centered with respect to standard Gaussian measure \u0393.\n\u2022 \u03c3 = \u03c3\u2217 = erf. Note that for c1, c2 \u2208 R, Ez\u223cN (0,1)[erf(c1z + c2)] = erf (\nc2\u221a 1+2c21\n) . Hence we can choose\n\u03ba\u2217 = \u221a 3, and the corresponding minimum value \u03c4\u2217 = 0.\n\u2022 \u03c3 = \u03c3\u2217 = tanh. Numerical integration yields \u03c4\u2217 \u2248 3\u00d7 10\u22124, \u03ba\u2217 \u2248 1.6.\n\u2022 \u03c3 = \u03c3\u2217 = SoftPlus. Numerical integration yields \u03c4\u2217 \u2248 0.03, \u03ba\u2217 \u2248 0.96.\n\u2022 \u03c3 = ReLU, \u03c3\u2217 = SoftPlus. Numerical integration yields \u03c4\u2217 \u2248 0.09, \u03ba\u2217 \u2248 0.94.\nObserve that in all the above examples, \u03c4\u2217 can be obtained by some finite \u03b1\u2217 (or equivalently \u03ba\u2217). In the following analysis of kernel ridge regression, we drop the small constant \u03b5 in Lemma 33 and directly apply the asymptotic statement given in (D.19).\nRemark. We make the following remarks on the calculation of \u03c4\u2217 in (4.3).\n\u2022 When \u03c3 = \u03c3\u2217, we intuitively expect \u03c4\u2217 to be small when the nonlinearity is smooth such that it is to some extent unchanged under Gaussian convolution (when \u03ba is chosen appropriately).\n\u2022 Adding weight decay with strength \u03bb < 1 to the first-layer parameters W 0 simply corresponds to multiplying \u03be2 in the definition of \u03c4 (D.17) by a factor of (1\u2212 \u03bb)."
        },
        {
            "heading": "D.3 Prediction Risk of Ridge Regression",
            "text": "In this section we prove Theorem 11. Recall that we aim to upper-bound the prediction risk of the CK ridge regression estimator defined as\nf\u0302(x) = \u2329 1\u221a\nN \u03c3(W>1 x), a\u0302\n\u232a , where a\u0302 := ( \u03a6>\u03a6 + \u03bbnI )\u22121 \u03a6>y\u0303, \u03a6 :=\n1\u221a N \u03c3(X\u0303W 1), (D.20)\nwhere {X\u0303, y\u0303} is a new set of training data independent of W . For concise notation, in this section we rescale the ridge parameter in (4.1) by replacing \u03bbN with \u03bb.\nGiven feature map x \u2192 1\u221a N \u03c3(W>1 x) conditioned on first layer weights W 1, we denote the associated Hilbert space as H. Note that H is a finite-dimensional reproducing kernel Hilbert space and is hence closed; we define the optimal predictor in the RKHS as f\u030c := argminf\u2208H Ex(f(x) \u2212 f\u2217(x))2, which takes the form of f\u030c(x) = \u3008 1\u221a\nN \u03c3(W>1 x), a\u030c\u3009 for some a\u030c \u2208 RN . In addition, we may write the orthogonal decomposition in\nL2(Rd,\u0393): f\u2217(x) = f\u030c(x) + f\u22a5(x). By definition of f\u22a5, we have \u2016f\u22a5\u20162L2 = Ex[f\u22a5(x)2] \u2264 R(h) = \u2016f\u2217 \u2212 h\u2016 2 L2 , for any h \u2208 H and x \u223c N (0, 1). Finally, from Assumption 2 we know that \u2016f\u2217\u2016L2 is bounded by some constant, and thus \u2016f\u22a5\u2016L2 is also bounded.\nWe are interested in the prediction risk of the CK ridge regression estimator denoted as R1(\u03bb). We first define the following quantities which R1(\u03bb) can be decomposed into (see Lemma 35):\nB1 := Ex ( f\u2217(x)\u2212 f\u030c(x) )2 ,\nB2 := Ex ( f\u030c(x)\u2212 1\nn \u03c6x\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>f\u030c )2 ,\nV1 := 1 n2 \u03b5\u0303>\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>\u03b5\u0303,\nV2 := 1 n2 f>\u22a5\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>f\u22a5,\n(D.21)\nwhere the i-th entry of vector f\u030c and f\u22a5 are given by [f\u030c ]i = f\u030c(x\u0303i), [f\u22a5]i = f\u22a5(x\u0303i), respectively, and \u03a3\u0302\u03a6 := 1 n\u03a6 >\u03a6, \u03a3\u03a6 := 1 NEx [ \u03c3(W>1 x)\u03c3(W > 1 x) > ] . Also, \u03c6x := 1\u221a N \u03c3(x>W 1) for x \u2208 Rd, which gives \u03a6> = [\u03c6>x\u03031 , . . . ,\u03c6 > x\u0303i , . . . ,\u03c6 > x\u0303n ], where x\u0303 > i is the i-th row of X\u0303. To simplify the notation, we omit the accent in x\u0303, \u03b5\u0303 when the context is clear. In the following subsections, to control R1(\u03bb), we provide high-probability upper-bounds for B1, B2, V1 and V2 separately.\nConcentration of Feature Covariance. We begin by defining a concentration event A on the empirical feature matrix \u03a3\u0302\u03a6, under which the prediction risk can be controlled. We modify the proof of [Ver18, Theorem 4.7.1] to obtain a normalized version of the concentration for CK matrix as follows.\nLemma 34. Under Assumptions 1, 2 and using the above notations, there exists some constant c > 0 such that the following holds7\nP (\u2225\u2225\u2225(\u03a3\u03a6 + \u03bbI)\u22121/2(\u03a3\u03a6 \u2212 \u03a3\u0302\u03a6)(\u03a3\u03a6 + \u03bbI)\u22121/2\u2225\u2225\u2225 \u2265 2K2 \u00b7\u221aN n ) \u2264 2 exp ( \u2212c \u221a N ) ,\nfor all large n > N , where K := \u03bb\u03c3\u221a N \u2016W 1\u2016F .\nProof. First observe that the null space of \u03a3\u03a6 contains the null space of \u03a3\u0302\u03a6. Also, notice that \u03a3\u0302\u03a6 is a sample covariance matrix taking the form of\n\u03a3\u0302 = 1\nn n\u2211 i=1 \u03c6x\u0303i\u03c6 > x\u0303i ,\nwhere the covariance of \u03c6x\u0303i is \u03a3\u03a6. This entails that there exists some independent isotropic random vector zi such that \u03c6x\u0303i = \u03a3 1/2 \u03a6 zi. We first show that \u03c6x\u0303i is a sub-Gaussian random vector in R N . Consider any unit vector r \u2208 RN . Let f(x) := \u3008\u03c6x, r\u3009, we can easily validate that f(x) is a \u03bb\u03c3\u221aN \u2016W 1\u2016F -Lipschitz function. Therefore, by Gaussian Lipschitz concentration we know that the sub-Gaussian norm of f(x\u0303i) is at least K = \u03bb\u03c3\u221a\nN \u2016W 1\u2016F , which also implies the sub-Gaussian norms of \u03c6x\u0303i and zi. DenoteRn := 1 n \u2211n i=1 ziz > i \u2212IN .\nFrom [Ver18, Theorem 4.6.1] we know that with probability at least 1\u2212 2e\u2212ct, \u2016Rn\u2016 \u2264 K2 (\u221a N\nn + t\u221a n\n) ,\nfor all large n,N . This proposition is proved by setting t = \u221a N and noting that\u2225\u2225\u2225(\u03a3\u03a6 + \u03bbI)\u22121/2(\u03a3\u03a6 \u2212 \u03a3\u0302\u03a6)(\u03a3\u03a6 + \u03bbI)\u22121/2\u2225\u2225\u2225 \u2264 \u2016Rn\u2016.\nFrom Lemma 14 we know that when n, d,N are proportional and \u03b7 = \u0398( \u221a N), there exist some constants c, C such that P ( \u2016W 1\u2016F \u2265 C \u221a N ) \u2264 exp(\u2212cN). We denote t = 2C2N/n and consider sufficiently large n (but still proportional to d) such that t < 1. Now given fixed \u03bb > 0, we define the concentration event\nA\u03bb = { \u2212tI 4 (\u03a3\u03a6 + \u03bbI)\u22121/2(\u03a3\u03a6 \u2212 \u03a3\u0302\u03a6)(\u03a3\u03a6 + \u03bbI)\u22121/2 4 tI } .\nSimilarly, for the \u201cridgeless\u201d case \u03bb = 0, we define A0 = { \u2212tI 4 \u03a3\u22121/2\u03a6 (\u03a3\u03a6 \u2212 \u03a3\u0302\u03a6)\u03a3 \u22121/2 \u03a6 4 tI } .\nLemma 34 entails that both A\u03bb and A0 hold with probability at least 1\u2212 2e\u2212c \u221a N . Following the remark on [Bac23, Lemma 7.1], under events A\u03bb and A0, we can obtain that\u2225\u2225\u2225\u03a3\u22121/2\u03a6 (\u03a3\u0302\u03a6 \u2212\u03a3\u03a6)\u03a3\u22121/2\u03a6 \u2225\u2225\u2225 \u2264 t, (D.22) and (1\u2212 t)(\u03a3\u03a6 \u2212 \u03a3\u0302\u03a6) 4 t(\u03a3\u0302\u03a6 + \u03bbI), which implies that\n\u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 4 t\n1\u2212 t I + \u03a3\u0302\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 4 1\n1\u2212 t I,\n7Note that for \u03bb = 0, the LHS of the inequality may be interpreted as a pseudo-inverse.\nsince \u2225\u2225\u2225\u2225\u03a3\u0302\u03a6(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u2225\u2225\u2225\u2225 \u2264 1. Analogously, we claim that (\u03a3\u0302\u03a6 + \u03bbI)\u22121/2\u03a3\u03a6(\u03a3\u0302\u03a6 + \u03bbI)\u22121/2 4 11\u2212tI. Thus, under events A\u03bb and A0, we know that\u2225\u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a31/2\u03a6 \u2225\u2225\u2225\u2225, \u2225\u2225\u2225\u2225\u03a3\u03a6(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u2225\u2225\u2225\u2225 \u2264 11\u2212 t . (D.23) We now control B1, B2, V1, V2 under the high probability events A\u03bb and A0.\nControlling B1, B2. By the definition of f\u030c , we have\nB1 = inf f\u2208H\nEx(f\u2217(x)\u2212 f(x))2 \u2264 Ex(f\u2217(x)\u2212 fr(x))2 = R(f\u0303), (D.24)\nwhere fr = f\u0303 \u2208 H is the estimator we constructed in Lemma 33. Note that the upper bound R(f\u0303) has already been characterized in (D.19) in the previous subsection.\nAs for B2, since f\u030c = 1\u221a N \u03c3(X\u0303W 1)a\u030c, simple calculation yields,\nB2 = Tr\n(( I \u2212 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u0302\u03a6 )> \u03a3\u03a6 ( I \u2212 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u0302\u03a6 ) a\u030ca\u030c> )\n= \u03bb2 \u2329 a\u030c, ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 a\u030c \u232a .\nFollowing [Bac23, Proposition 7.2], we define a\u03bb = \u03a3\u03a6(\u03a3\u03a6 + \u03bbI) \u22121 a\u030c and obtain\nB2 \u2264 2\u03bb2 \u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u03a6 + \u03bbI)\u22121a\u030c\u2225\u2225\u22252 + 2\u2225\u2225\u2225\u2225\u03a31/2\u03a6 ((\u03a3\u03a6 + \u03bbI)\u22121\u03a3\u03a6 \u2212 (\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a3\u0302\u03a6)a\u030c\u2225\u2225\u2225\u22252.\nIn addition, note that ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u0302\u03a6 \u2212 (\u03a3\u03a6 + \u03bbI)\u22121\u03a3\u03a6\n= ( \u03a3\u0302\u03a6 + \u03bbI )\u22121( \u03a3\u0302\u03a6 \u2212\u03a3\u03a6 ) + [( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u2212 (\u03a3\u03a6 + \u03bbI)\u22121 ] \u03a3\u03a6\n=\u03bb ( \u03a3\u0302\u03a6 + \u03bbI )\u22121( \u03a3\u0302\u03a6 \u2212\u03a3\u03a6 ) (\u03a3\u03a6 + \u03bbI) \u22121 .\nTherefore, we know that under events A\u03bb and A0,\u2225\u2225\u2225\u2225\u03a31/2\u03a6 ((\u03a3\u03a6 + \u03bbI)\u22121\u03a3\u03a6 \u2212 (\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a3\u0302\u03a6)a\u030c\u2225\u2225\u2225\u22252 = \u03bb2\n\u2225\u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u0302\u03a6 + \u03bbI)\u22121(\u03a3\u0302\u03a6 \u2212\u03a3\u03a6)(\u03a3\u03a6 + \u03bbI)\u22121a\u030c\u2225\u2225\u2225\u22252 \u2264 \u2225\u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a31/2\u03a6 \u2225\u2225\u2225\u22252 \u00b7 \u2225\u2225\u2225\u03a3\u22121/2\u03a6 (\u03a3\u0302\u03a6 \u2212\u03a3\u03a6)\u03a3\u22121/2\u03a6 \u2225\u2225\u22252 \u00b7 \u03bb2\u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u03a6 + \u03bbI)\u22121a\u030c\u2225\u2225\u22252\n(i) \u2264 t 2 (1\u2212 t)2 \u00b7 \u03bb2 \u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u03a6 + \u03bbI)\u22121a\u030c\u2225\u2225\u22252,\nwhere (i) follows from the definition of the concentration events A, (D.22) and (D.23). Finally, from [Bac23, Lemma 7.2], we have\n\u03bb2 \u2225\u2225\u2225\u03a31/2\u03a6 (\u03a3\u03a6 + \u03bbI)\u22121a\u030c\u2225\u2225\u22252 \u2264 \u03bb\u2329a\u030c, (\u03a3\u03a6 + \u03bbI)\u22121\u03a3\u03a6a\u030c\u232a\n= inf f\u2208H\n{ \u2016f \u2212 f\u030c\u20162L2 + \u03bb\u2016f\u2016 2 H } \u2264 2\u2016f\u2217 \u2212 fr\u20162L2 + \u03bb\u2016fr\u20162H, (D.25)\nwhere the last step is a triangle inequality due to \u2016f\u2217 \u2212 f\u030c\u20162L2 \u2264 \u2016f\u2217 \u2212 fr\u20162L2 .\nControlling V1, V2. For V1, note that under event A\u03bb,\nV1 = 1 n2 \u03b5\u0303>\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>\u03b5\u0303\n\u2264 \u2225\u2225\u2225\u2225 1n\u03b5>\u03a6 \u2225\u2225\u2225\u22252 \u00b7 \u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a3\u03a6\u2225\u2225\u2225\u2225 \u00b7 \u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u2225\u2225\u2225\u2225 (ii). 1\u03bb(1\u2212 t) \u00b7 \u2225\u2225\u2225\u2225 1n\u03a6>\u03b5\u0303 \u2225\u2225\u2225\u22252, where \u03a6 is defined in (D.20), and (ii) is based on the concentration property for A\u03bb given in (D.23). Denote \u03c7ik := [\u03c6k]i \u00b7 \u03b5\u0303k whence [ \u03a6>\u03b5\u0303 ] i = \u2211n k=1 \u03c7 i k. Note that E[\u03c7ik\u03c7 j k] = 0, E[(\u03c7ik)2] . \u03c32\u03b5 N for any k \u2208 [n] and i 6= j \u2208 [N ], due to the assumptions on label noise and bounded activation \u03c3. Therefore, by Markov\u2019s inequality, for any x > 0, we have\nP ( 1\nn2 \u2016\u03a6>\u03b5\u0303\u20162 \u2265 x\n) \u2264 E\u2016\u03a6 >\u03b5\u0303\u20162\nn2x . \u03c32\u03b5 nx . (D.26)\nSimilarly for V2, under event A\u03bb, we have\nV2 = 1 n2 f>\u22a5\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>f\u22a5\n\u2264 \u2225\u2225\u2225\u2225 1nf>\u22a5\u03a6 \u2225\u2225\u2225\u22252 \u00b7 \u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u03a3\u03a6\u2225\u2225\u2225\u2225 \u00b7 \u2225\u2225\u2225\u2225(\u03a3\u0302\u03a6 + \u03bbI)\u22121\u2225\u2225\u2225\u2225 . 1\u03bb(1\u2212 t) \u00b7 \u2225\u2225\u2225\u2225 1n\u03a6>f\u22a5 \u2225\u2225\u2225\u22252. Recall that E[\u03c6xf\u22a5(x)] = 0 due to the orthogonality condition. Hence we may apply the exact same argument as V1 to obtain an upper bound similar to (D.26); by Markov\u2019s inequality,\nP ( 1\nn2 \u2016\u03a6>f\u22a5\u20162 \u2265 x\n) \u2264 E\u2016\u03a6 >f\u22a5\u20162\nn2x\n(iii) . \u2016f\u22a5\u20162L2 nx , (D.27)\nwhere (iii) is due to the boundedness of \u03c3 and \u2016f\u22a5\u2016L2 . Combining V1 and V2, and taking x = Cn\u03b5\u22121 in (D.26) and (D.27), for some C > 0 and any small \u03b5 > 0, we arrive at\nV1 + V2 . \u03c32\u03b5 + \u2016f\u22a5\u2016 2 L2\nn1\u2212\u03b5\u03bb(1\u2212 t) , (D.28)\nwith probability at least 1\u2212 n\u2212\u03b5.\nPutting Things Together. The following lemma provides a decomposition of the prediction risk R1(\u03bb) in terms of B1, B2, V1, V2 analyzed above.\nLemma 35. Under the same assumptions as Lemma 10, if we choose \u03bb = \u2126(n\u03b5\u22121) for small \u03b5 > 0, then the prediction risk of the CK ridge estimator admits the following upper bound\nR1(\u03bb) \u2264 B1 +B2 + 2 \u221a B1B2 + od,P(1),\nwhere B1, B2 are defined in (D.21).\nProof. Based on the definition of prediction risk, we have R1(\u03bb) = Ex (( f\u2217(x)\u2212 f\u030c(x) ) + ( f\u030c(x)\u2212 1\nn \u03c6x\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>y\u0303 ))2 \u2264 Ex ( f\u2217(x)\u2212 f\u030c(x)\n)2\ufe38 \ufe37\ufe37 \ufe38 B1 +2 \u221a B1S1 + S1,\nwhere we defined\nS1 := Ex ( f\u030c(x)\u2212 1\nn \u03c6x\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>(f\u030c + f\u22a5 + \u03b5\u0303) )2\n\u2264 Ex ( f\u030c(x)\u2212 1\nn \u03c6x\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>f\u030c )2 \ufe38 \ufe37\ufe37 \ufe38\nB2\n+2 \u221a B2S2 + S2,\nin which\nS2 := 1 n2 \u03b5\u0303>\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>\u03b5\u0303\ufe38 \ufe37\ufe37 \ufe38\nV1\n+ 1\nn2 f>\u22a5\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>f\u22a5\ufe38 \ufe37\ufe37 \ufe38\nV2\n+ 2\nn2 f>\u22a5\u03a6\n( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a3\u03a6 ( \u03a3\u0302\u03a6 + \u03bbI )\u22121 \u03a6>\u03b5\u0303\n\u2264 2(V1 + V2).\nRecall that Lemma 34 entails that events A\u03bb and A0 occur with high probability, for constant t \u2208 (0, 1). Hence from (D.28) we know that for \u03bb = \u2126(n\u03b5\u22121) with small \u03b5 > 0, V1 + V2 = od,P(1), and thus S2 is vanishing when n, d,N \u2192 \u221e proportionally. On the other hand, (D.24) and (D.25) entail that B1 and B2 are both finite. The claim is established by combining the calculations.\nProof of Theorem 11. Since Lemma 34 ensures that events A\u03bb and A0 happens with high probability for fixed t \u2208 (0, 1), if we set \u03bb = \u2126(n\u03b5\u22121) for some small \u03b5 > 0, then Lemma 35 entails\nR1(\u03bb) \u2264 B1 +B2 + 2 \u221a B1B2 + od,P(1),\nwhere B1 \u2264 \u2016f\u2217 \u2212 fr\u20162L2 , B2 \u2264 2 ( 1 + t2\n(1\u2212 t)2\n) \u00b7 (\n2\u2016f\u2217 \u2212 fr\u20162L2 + \u03bb\u2016fr\u2016 2 H ) + od,P(1),\nin which fr is defined by (D.7) in the proof of Lemma 33. Here, we applied the upper bounds on B1 in (D.24) and B2 in (D.25). Since \u2016f\u2217\u2212 fr\u20162L2 = R(f\u0303), by (D.19) we know that as n, d,N \u2192\u221e, with probability one,\n\u2016f\u2217 \u2212 fr\u20162L2 \u2264 \u03c4\u2217 + C (\u221a \u03c4\u2217 \u00b7 \u221a d n + d n ) , (D.29)\nfor some constant C > 0. Finally, recall that in the proof of Lemma 33, we constructed an estimator fr \u2208 H with \u2016fr\u20162H = \u2016a\u0303\u20162 = N/|A\u03b1r | = \u0398d,P(Nr), for 0 < r < 1/2. In other words, \u03bb\u2016fr\u2016 2 H = od,P(1) as long as Nr\u03bb\u2192 0 as n, d,N \u2192\u221e; this provides a way to choose r \u2208 (0, 1/2) given \u03bb. Now from Lemma 34 we know that there exists some constant \u03c8\u22171 such that both A\u03bb and A0 hold with high probability for t < 0.1 when n/d > \u03c8\u22171 . In this case, given any \u03bb = n \u2212\u03c1 for some \u03c1 \u2208 (0, 1), we know that\nR1(\u03bb) \u2264 B1 +B2 + 2 \u221a B1B2 + od,P(1)\n\u2264 \u2016f\u2217 \u2212 fr\u20162L2 + 4 ( 1 + t2\n(1\u2212 t)2\n) \u00b7 \u2016f\u2217 \u2212 fr\u20162L2 + 4 \u221a 1 +\nt2\n(1\u2212 t)2 \u00b7 \u2016f\u2217 \u2212 fr\u20162L2 + od,P(1).\nFinally, due to the upper-bound (D.29), we conclude that\nR1(\u03bb) \u2264 10\u03c4\u2217 + C \u2032 (\u221a \u03c4\u2217 \u00b7 \u221a d n + d n ) ,\nwith probability one as n, d,N \u2192\u221e proportionally and n/d > \u03c8\u22171 , where \u03c4\u2217 is defined in (4.3)."
        }
    ],
    "title": "High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation",
    "year": 2022
}