{
    "abstractText": "For black-box attacks, the gap between the substitute model and the victim model is usually large, which manifests as a weak attack performance. Motivated by the observation that the transferability of adversarial examples can be improved by attacking diverse models simultaneously, model augmentation methods which simulate different models by using transformed images are proposed. However, existing transformations for spatial domain do not translate to significantly diverse augmented models. To tackle this issue, we propose a novel spectrum simulation attack to craft more transferable adversarial examples against both normally trained and defense models. Specifically, we apply a spectrum transformation to the input and thus perform the model augmentation in the frequency domain. We theoretically prove that the transformation derived from frequency domain leads to a diverse spectrum saliency map, an indicator we proposed to reflect the diversity of substitute models. Notably, our method can be generally combined with existing attacks. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method, e.g., attacking nine stateof-the-art defense models with an average success rate of 95.4%. Our code is available in https://github.com/yuyang-long/SSA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuyang Long"
        },
        {
            "affiliations": [],
            "name": "Qilong Zhang"
        },
        {
            "affiliations": [],
            "name": "Boheng Zeng"
        },
        {
            "affiliations": [],
            "name": "Lianli Gao"
        },
        {
            "affiliations": [],
            "name": "Xianglong Liu"
        },
        {
            "affiliations": [],
            "name": "Jian Zhang"
        },
        {
            "affiliations": [],
            "name": "Jingkuan Song"
        }
    ],
    "id": "SP:b692fe3f65b1686a2a5a0989bac4dbed50b6c2c1",
    "references": [
        {
            "authors": [
                "N. Ahmed",
                "T.R. Natarajan",
                "K.R. Rao"
            ],
            "title": "Discrete cosine transform",
            "venue": "IEEE Trans. Computers 23, 90\u201393",
            "year": 1974
        },
        {
            "authors": [
                "M. Bojarski",
                "D.D. Testa",
                "D. Dworakowski",
                "B. Firner",
                "B. Flepp",
                "P. Goyal",
                "L.D. Jackel",
                "M. Monfort",
                "U. Muller",
                "J. Zhang",
                "X. Zhang",
                "J. Zhao",
                "K. Zieba"
            ],
            "title": "End to end learning for self-driving cars",
            "venue": "CoRR abs/1604.07316",
            "year": 2016
        },
        {
            "authors": [
                "N. Carlini",
                "D.A. Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "Symposium on Security and Privacy",
            "year": 2017
        },
        {
            "authors": [
                "Y. Dong",
                "F. Liao",
                "T. Pang",
                "H. Su",
                "J. Zhu",
                "X. Hu",
                "J. Li"
            ],
            "title": "Boosting adversarial attacks with momentum",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "Y. Dong",
                "T. Pang",
                "H. Su",
                "J. Zhu"
            ],
            "title": "Evading defenses to transferable adversarial examples by translation-invariant attacks",
            "venue": "CVPR",
            "year": 2019
        },
        {
            "authors": [
                "R. Duan",
                "Y. Chen",
                "D. Niu",
                "Y. Yang",
                "A.K. Qin",
                "Y. He"
            ],
            "title": "Advdrop: Adversarial attack to dnns by dropping information",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Dziugaite",
                "G. Karolina",
                "Z. Ghahramani",
                "D.M. Roy."
            ],
            "title": "A study of the effect of jpg compression on adversarial images",
            "venue": "CoRR abs/1608.00853",
            "year": 2016
        },
        {
            "authors": [
                "A.A. Efros",
                "W.T. Freeman"
            ],
            "title": "Image quilting for texture synthesis and transfer",
            "venue": "SIGGRAPH",
            "year": 2001
        },
        {
            "authors": [
                "L. Gao",
                "Y. Cheng",
                "Q. Zhang",
                "X. Xu",
                "J. Song"
            ],
            "title": "Feature space targeted attacks by statistic alignment",
            "venue": "IJCAI",
            "year": 2021
        },
        {
            "authors": [
                "L. Gao",
                "Q. Zhang",
                "J. Song",
                "X. Liu",
                "H.T. Shen"
            ],
            "title": "Patch-wise attack for fooling deep neural network",
            "venue": "ECCV",
            "year": 2020
        },
        {
            "authors": [
                "L. Gao",
                "Q. Zhang",
                "J. Song",
                "H.T. Shen"
            ],
            "title": "Patch-wise++ perturbation for adversarial targeted attacks",
            "venue": "CoRR abs/2012.15503",
            "year": 2020
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "ICLR",
            "year": 2015
        },
        {
            "authors": [
                "C. Guo",
                "J.S. Frank",
                "K.Q. Weinberger"
            ],
            "title": "Low frequency adversarial perturbation",
            "venue": "Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019. vol. 115, pp. 1127\u20131137",
            "year": 2019
        },
        {
            "authors": [
                "C. Guo",
                "M. Rana",
                "M. Ciss\u00e9",
                "L. van der Maaten"
            ],
            "title": "Countering adversarial images using input transformations",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR. pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. van der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "CVPR",
            "year": 2017
        },
        {
            "authors": [
                "J. Jia",
                "X. Cao",
                "B. Wang",
                "N.Z. Gong"
            ],
            "title": "Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing",
            "venue": "ICLR",
            "year": 2020
        },
        {
            "authors": [
                "A. Kurakin",
                "I.J. Goodfellow",
                "S. Bengio"
            ],
            "title": "Adversarial machine learning at scale",
            "venue": "ICLR",
            "year": 2017
        },
        {
            "authors": [
                "J. Li",
                "R. Ji",
                "H. Liu",
                "J. Liu",
                "B. Zhong",
                "C. Deng",
                "Q. Tian"
            ],
            "title": "Projection & probability-driven black-box attack",
            "venue": "CVPR",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "J. Li",
                "Y. Chen",
                "S. Ye",
                "Y. He",
                "S. Wang",
                "H. Su",
                "H. Xue"
            ],
            "title": "QAIR: practical query-efficient black-box attacks for image retrieval",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "F. Liao",
                "M. Liang",
                "Y. Dong",
                "T. Pang",
                "X. Hu",
                "J. Zhu"
            ],
            "title": "Defense against adversarial attacks using high-level representation guided denoiser",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "J. Lin",
                "C. Song",
                "K. He",
                "L. Wang",
                "J.E. Hopcroft"
            ],
            "title": "Nesterov accelerated gradient and scale invariance for adversarial attacks",
            "venue": "ICLR",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liu",
                "X. Chen",
                "C. Liu",
                "D. Song"
            ],
            "title": "Delving into transferable adversarial examples and black-box attacks",
            "venue": "ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Y. Liu",
                "Y. Cheng",
                "L. Gao",
                "X. Liu",
                "Q. Zhang",
                "J. Song"
            ],
            "title": "Practical evaluation of adversarial robustness via adaptive auto attack",
            "venue": "CoRR abs/2203.05154",
            "year": 2022
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "authors": [
                "X. Mao",
                "Y. Chen",
                "S. Wang",
                "H. Su",
                "Y. He",
                "H. Xue"
            ],
            "title": "Composite adversarial attacks",
            "venue": "AAAI",
            "year": 2021
        },
        {
            "authors": [
                "S. Moosavi-Dezfooli",
                "A. Fawzi",
                "O. Fawzi",
                "P. Frossard"
            ],
            "title": "Universal adversarial perturbations",
            "venue": "CVPR",
            "year": 2017
        },
        {
            "authors": [
                "S. Moosavi-Dezfooli",
                "A. Fawzi",
                "P. Frossard"
            ],
            "title": "Deepfool: A simple and accurate method to fool deep neural networks",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "M. Naseer",
                "S.H. Khan",
                "M. Hayat",
                "F.S. Khan",
                "F. Porikli"
            ],
            "title": "A self-supervised approach for adversarial robustness",
            "venue": "CVPR",
            "year": 2020
        },
        {
            "authors": [
                "M. Naseer",
                "S.H. Khan",
                "M.H. Khan",
                "F.S. Khan",
                "F. Porikli"
            ],
            "title": "Cross-domain transferability of adversarial perturbations",
            "venue": "NeurPIS",
            "year": 2019
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)",
            "venue": "Doklady AN USSR",
            "year": 1983
        },
        {
            "authors": [
                "N. Papernot",
                "P.D. McDaniel",
                "I.J. Goodfellow",
                "S. Jha",
                "Z.B. Celik",
                "A. Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "Karri, R., Sinanoglu, O., Sadeghi, A., Yi, X. (eds.) ACM",
            "year": 2017
        },
        {
            "authors": [
                "L.I. Rudin",
                "S. Osher",
                "E. Fatemi"
            ],
            "title": "Nonlinear total variation based noise removal algorithms",
            "venue": "Physica D: nonlinear phenomena",
            "year": 1992
        },
        {
            "authors": [
                "A.E. Sallab",
                "M. Abdou",
                "E. Perot",
                "S.K. Yogamani"
            ],
            "title": "Deep reinforcement learning framework for autonomous driving",
            "venue": "CoRR abs/1704.02532",
            "year": 2017
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Gradcam: Visual explanations from deep networks via gradient-based localization",
            "venue": "ICCV",
            "year": 2017
        },
        {
            "authors": [
                "Y. Sharma",
                "G.W. Ding",
                "M.A. Brubaker"
            ],
            "title": "On the effectiveness of low frequency perturbations",
            "venue": "IJCAI. pp. 3389\u20133396",
            "year": 2019
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke",
                "A.A. Alemi"
            ],
            "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
            "venue": "AAAI",
            "year": 2017
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I.J. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "ICLR",
            "year": 2014
        },
        {
            "authors": [
                "Y. Taigman",
                "M. Yang",
                "M. Ranzato",
                "L. Wolf"
            ],
            "title": "Deepface: Closing the gap to humanlevel performance in face verification",
            "venue": "CVPR",
            "year": 2014
        },
        {
            "authors": [
                "A. Thomas",
                "O. Elibol"
            ],
            "title": "Defense against adversarial attacks-3rd place",
            "venue": "https:// github.com/anlthms/nips-2017/blob/master/poster/defense.pdf",
            "year": 2017
        },
        {
            "authors": [
                "F. Tram\u00e8r",
                "A. Kurakin",
                "I.J. Goodfellow",
                "D. Boneh",
                "P.D. McDaniel"
            ],
            "title": "Ensemble adversarial training: Attacks and defenses",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "Y. Wang",
                "Z. Zhou",
                "X. Ji",
                "D. Gong",
                "J. Zhou",
                "Z. Li",
                "W. Liu"
            ],
            "title": "Cosface: Large margin cosine loss for deep face recognition",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "X. Wu",
                "Z. Huang",
                "E.P. Xing"
            ],
            "title": "High-frequency component helps explain the generalization of convolutional neural networks",
            "venue": "CVPR. pp. 8681\u20138691",
            "year": 2020
        },
        {
            "authors": [
                "X. Wang",
                "K. He"
            ],
            "title": "Enhancing the transferability of adversarial attacks through variance tuning",
            "venue": "CVPR. pp. 1924\u20131933",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "X. He",
                "J. Wang",
                "K. He"
            ],
            "title": "Admix: Enhancing the transferability of adversarial attacks",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "H. Guo",
                "Z. Zhang",
                "W. Liu",
                "Z. Qin",
                "K. Ren"
            ],
            "title": "Feature importance-aware transferable adversarial attacks",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Yang",
                "A. Shrivastava",
                "V. Rawal",
                "Z. Ding"
            ],
            "title": "Towards frequency-based explanation for robust CNN",
            "venue": "CoRR abs/2005.03141",
            "year": 2020
        },
        {
            "authors": [
                "D. Wu",
                "Y. Wang",
                "S. Xia",
                "J. Bailey",
                "X. Ma"
            ],
            "title": "Skip connections matter: On the transferability of adversarial examples generated with resnets",
            "venue": "ICLR",
            "year": 2020
        },
        {
            "authors": [
                "W. Wu",
                "Y. Su",
                "M.R. Lyu",
                "I. King"
            ],
            "title": "Improving the transferability of adversarial samples with adversarial transformations",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "C. Xie",
                "J. Wang",
                "Z. Zhang",
                "Z. Ren",
                "A.L. Yuille"
            ],
            "title": "Mitigating adversarial effects through randomization",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "authors": [
                "C. Xie",
                "Y. Wu",
                "L. van der Maaten",
                "A.L. Yuille",
                "K. He"
            ],
            "title": "Feature denoising for improving adversarial robustness",
            "venue": "CVPR",
            "year": 2019
        },
        {
            "authors": [
                "C. Xie",
                "Z. Zhang",
                "Y. Zhou",
                "S. Bai",
                "J. Wang",
                "Z. Ren",
                "A.L. Yuille"
            ],
            "title": "Improving transferability of adversarial examples with input diversity",
            "venue": "CVPR",
            "year": 2019
        },
        {
            "authors": [
                "D. Yin",
                "R.G. Lopes",
                "J. Shlens",
                "E.D. Cubuk",
                "J. Gilmer"
            ],
            "title": "A fourier perspective on model robustness in computer vision",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "J. Song",
                "L. Gao",
                "Y. Liu",
                "H.T. Shen"
            ],
            "title": "Progressive meta-learning with curriculum",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhang",
                "X. Li",
                "Y. Chen",
                "J. Song",
                "L. Gao",
                "Y. He",
                "H. Xue"
            ],
            "title": "Beyond imagenet attack: Towards crafting adversarial examples for black-box domains",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhang",
                "C. Zhang",
                "C. Li",
                "J. Song",
                "L. Gao",
                "H.T. Shen"
            ],
            "title": "Practical nobox adversarial attacks with training-free hybrid image transformation",
            "venue": "CoRR abs/2203.04607",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhang",
                "X. Zhu",
                "J. Song",
                "L. Gao",
                "H.T. Shen"
            ],
            "title": "Staircase sign method for boosting adversarial attacks",
            "venue": "CoRR abs/2104.09722",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhao",
                "Z. Liu",
                "M.A. Larson"
            ],
            "title": "Towards large yet imperceptible adversarial image perturbations with perceptual color distance",
            "venue": "CVPR",
            "year": 2020
        },
        {
            "authors": [
                "J. Zou",
                "Z. Pan",
                "J. Qiu",
                "X. Liu",
                "T. Rui",
                "W. Li"
            ],
            "title": "Improving the transferability of adversarial examples with resized-diverse-inputs, diversity-ensemble and region fitting",
            "venue": "ECCV",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Adversarial examples, Model augmentation, Transferability"
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, deep neural networks (DNNs) have achieved a considerable success in the field of computer vision, e.g., image classification [15,16,55], face recognition [40,43] and self-driving [2,34]. Nevertheless, there are still many concerns regarding the stability of neural networks. As demonstrated in prior works [39,12], adversarial examples which merely add human-imperceptible perturbations on clean images can easily fool state-of-the-art DNNs. Therefore, to help improve the robustness of DNNs, crafting adversarial examples to cover as many blind spots of DNNs as possible is necessary.\n\u2217Corresponding author\nar X\niv :2\n20 7.\n05 38\n2v 1\n[ cs\n.C V\n] 1\n2 Ju\nIn general, settings of adversarial attacks can be divided into white-box and black-box. For the former [3,28,59,26], an adversary has access to the model, e.g., model architecture and parameters are known. Therefore, adversarial examples can be directly crafted by the gradient (w.r.t. the input) of the victim model, and thus achieving a high success rate. However, white-box attack is usually impracticable in real-world applications because an adversary is impossible to obtain all information about a victim model. To overcome this limitation, a common practice of black-box attacks [4,53,10] turns to investigate the inherent cross-model transferability of adversarial examples. Typically, an adversary crafts adversarial examples via a substitute model (a.k.a. white-box model), and then transfers them to a victim model (a.k.a. black-box model) for attacking.\nHowever, the gap between the substitute model and the victim model is usually large, which manifests as the low transferability of adversarial examples. Although attacking diverse models simultaneously can boost the transferability, collecting a large number of diverse models is difficult and training a model from scratch is also time-consuming. To tackle this issue, model augmentation [22] is proposed. In particular, typical model augmentation approaches [53,5,22] aim to simulate diverse models by applying loss-preserving transformations to inputs. Yet, all of existing works investigate relationships of different models in spatial domain, which may overlook the essential differences between them.\nTo better uncover the differences among models, we introduce the spectrum saliency map (see Sec. 3.2) from a frequency domain perspective since represen-\ntation of images in this domain have a fixed pattern [1,54], e.g., low-frequency components of an image correspond to its contour. Specifically, the spectrum saliency map is defined as the gradient of model loss function w.r.t. the frequency spectrum of input image. As illustrated in Figure 1 (d\u223cg), spectrum saliency maps of different models significantly vary from each other, which clearly reveals that each model has different interests in the same frequency component.\nMotivated by this, we consider tuning the spectrum saliency map to simulate more diverse substitute models, thus generating more transferable adversarial examples. To that end, we propose a spectrum transformation based on (discrete cosine transform) DCT and (inverse discrete cosine transform) IDCT techniques [1] to diversify input images. We theoretically prove that this spectrum transformation can generate diverse spectrum saliency maps and thus simulate diverse substitute models. As demonstrated in Figure 1 (a\u223cc), after averaging results of diverse augmented models, only our resulting spectrum saliency map can cover almost all those of other models. This demonstrates our proposed spectrum transformation can effectively narrow the gap between the substitute model and victim model. To sum up, our main contributions are as follows:\n1) We discover that augmented models derived from the spatial domain transformations are not significantly diverse, which may limit the transferability of adversarial examples.\n2) To overcome this limitation, we introduce the spectrum saliency map (based on a frequency domain perspective) to investigate the differences among models. Inspired by our finds, we propose a novel Spectrum Simulation Attack to effectively narrow the gap between the substitute model and victim model.\n3) Extensive experiments on the ImageNet dataset highlight the effectiveness of our proposed method. Remarkably, compared to state-of-the-art transferbased attacks, our method improves the attack success rate by 6.3%\u223c12.2% for normally trained models and 5.6%\u223c23.1% for defense models."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Adversarial Attacks",
            "text": "Since Szegedy et al. [39] discover the existence of adversarial examples, various attack algorithms [12,18,3,28,32,27,59,30,6,20,58,50,56,57,24] have been proposed to investigate the vulnerability of DNNs. Among all attack branches, FGSM-based black-box attacks [12,18,4,53,10,11,49,9] which resort to the transferability of adversarial examples are one of the most efficient families. Therefore, in this paper, we mainly focus on this family to boost adversarial attacks.\nTo enhance the transferability of adversarial examples, it is crucial to avoid getting trapped in a poor local optimum of the substitute model. Towards this end, Dong et al. [4] adopt the momentum term at each iteration of I-FGSM [18] to stabilize update direction. Lin et al. [22] further adapt Nesterov accelerated gradient [31] into the iterative attacks with the aim of effectively looking ahead. Gao et al. [10] propose patch-wise perturbations to better cover the discriminate region of images. In addition to considering better optimization algorithms,\nmodel augmentation [22] is also an effective strategy. Xie et al. [53] introduce a random transformation to the input, thus improving the transferability. Dong et al. [5] shift the input to create a series of translated images and approximately estimate the overall gradient to mitigate the problem of over-reliance on the substitute model. Lin et al. [22] leverage the scale-invariant property of DNNs and thus average the gradients with respect to different scaled images to update adversarial examples. Zou et al. [60] modify DI-FGSM [53] to promote TI-FGSM [5] by generating multi-scale gradients. Wang et al. [45] consider the gradient variance along momentum optimization path to avoid overfitting. Wang et al. [47] average the gradients with respect to feature maps to disrupt important object-aware features. Wang et al. [46] average the gradients of a set of admixed images, which are the input image admixed with a small portion of other images while maintaining the label of the input image. Wu et al. [50] utilizes an adversarial transformation network to find a better transformation for adversarial attacks in the spatial domain."
        },
        {
            "heading": "2.2 Frequency-based Analysis and Attacks",
            "text": "Several works [54,44,36,6,48] have analyzed DNNs from a frequency domain perspective. Wang et al. [44] notice DNNs\u2019 ability in capturing high-frequency components of an image which are almost imperceptible to humans. Dong et al. [54] find that naturally trained models are highly sensitive to additive perturbations in high frequencies, and both Gaussian data augmentation and adversarial training can significantly improve robustness against high-frequency noises.\nIn addition, there also exists several adversarial attacks based on frequency domain. Guo et al. [13] propose a LF attack that only leverages the low-frequency components of an image, which shows that low-frequency components also play a significant role in model prediction as high-frequency components. Sharma et al. [36] demonstrate that defense models based on adversarial training are less sensitive to high-frequency perturbations but still vulnerable to low-frequency perturbations. Duan et al. [6] propose the AdvDrop attack which generates adversarial examples by dropping existing details of clean images in frequency domain. Unlike these works that perturb a subset of frequency components, our method aims to narrow the gap between models by frequency-based analysis."
        },
        {
            "heading": "2.3 Adversarial Defenses",
            "text": "To mitigate the threat of adversarial examples, numerous adversarial defense techniques have been proposed in recent years. One popular and promising way is adversarial training [12,25] which leverages adversarial examples to augment the training data during the training phase. Trame\u0300r et al. [42] introduce ensemble adversarial training, which decouples the generation of adversarial examples from the model being trained, to yield models with stronger robustness to black-box attacks. Xie et al. [52] inject blocks that can denoise the intermediate features into the network, and then end-to-end train it on adversarial examples to learn to reduce perturbations in feature maps.\nAlthough adversarial training is the most effective strategy to improve the robustness of the model at present, it inevitably suffers from time-consuming training costs and is expensive to be applied to large-scale datasets and complex DNNs. To avoid this issue, many works try to cure the infection of adversarial perturbations before feeding to DNNs. Guo et al. [14] utilize multiple input transformations (e.g., JPEG compression [7], total variance minimization [33] and image quilting [8]) to recover from the adversarial perturbations. Liao et al. [21] propose high-level representation guided denoiser (HGD) to suppress the influence of adversarial perturbation. Xie et al. [51] mitigate adversarial effects through random resizing and padding (R&P). Cohen et al. [17] leverage the classifier with Gaussian data augmentation to create a provably robust classifier.\nIn addition, researchers also try to combine the benefits of adversarial training and input pre-processing methods to further improve the robustness of DNNs. NeurIPS-r3 solution [41] propose a two-step procedure which first process images with a series of transformations (e.g., rotation, zoom and sheer) and then pass the outputs through an ensemble of adversarially trained models to obtain the overall prediction. Naseer et al. [29] design a Neural Representation Purifier (NRP) model that learns to clean adversarial perturbed images based on the automatically derived supervision."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first give the basic definition of our task in Sec. 3.1, and then introduce our motivation in Sec. 3.2. Based on the motivation, we provide a detailed description of the proposed method - Spectrum Transformation (Sec. 3.3). Finally, we introduce our overall attack algorithm in Sec. 3.4."
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "Formally, let f\u03b8 : x \u2192 y denotes a classification model, where \u03b8, x and y indicate the parameters of the model, input clean image and true label, respectively. Our goal is to craft an adversarial perturbation \u03b4 so that the resulting adversarial example x\u2032 = x+\u03b4 can successfully mislead the classifier, i.e., f\u03b8(x\n\u2032) \u0338= y (a.k.a. non-targeted attack). To ensure an input is minimally changed, an adversarial example should be in the \u2113p-norm ball centered at x with radius \u03f5. Following previous works [4,53,5,10,46,47,9], we focus on the \u2113\u221e-norm in this paper. Therefore, the generation of adversarial examples can be formulated as the following optimization problem:\nargmax x\u2032\nJ(x\u2032, y; \u03b8), s.t. \u2225\u03b4\u2225\u221e \u2264 \u03f5, (1)\nwhere J(x\u2032, y; \u03b8) is usually the cross-entropy loss. However, it is impractical to directly optimize Eq. 1 via the victim model f\u03b8 under the black-box manner because its parameter \u03b8 is inaccessible. To overcome this limitation, a common practice is to craft adversarial examples via the accessible substitute model f\u03d5\nand relying on the transferability to fool the victim model. Taking I-FGSM [18] as an example, adversarial examples at iteration t+ 1 can be expressed as:\nx\u2032t+1 = clipx,\u03f5{x\u2032t + \u03b1 \u00b7 sign ( \u2207x\u2032tJ (x \u2032 t, y;\u03d5) ) }, (2)\nwhere clipx,\u03f5(\u00b7) denotes an element-wise clipping operation to ensure x\u2032 \u2208 [x\u2212 \u03f5,x+ \u03f5], and \u03b1 is the step size."
        },
        {
            "heading": "3.2 Spectrum Saliency Map",
            "text": "In order to effectively narrow the gap between models, it is important to uncover the essential differences between them. Recently, various attack methods [12,18,4,53,5,10,60,22,45,46] have been proposed to boost the transferability of adversarial examples. Among these algorithms, model augmentation [22] is one of the most effective strategies. However, existing works (e.g., [5,22]) usually augment substitute models by applying loss-preserving transformations in the spatial domain, which might ignore essential differences among models and reduce the diversity of substitute models. Intuitively, different models usually focus on similar spatial regions of each input image since location of key objects in images is fixed. By contrast, as demonstrated in previous work [48,54,44], different models usually rely on different frequency components of each input image when making decisions.\nMotivated by this, we turn to explore correlations among models from a perspective of frequency domain. Specifically, we adopt DCT to transform input images x from the spatial domain to the frequency domain. The mathematical definition of the DCT (denoted as D(\u00b7)4 in the following) can be simplified as:\nD(x) = AxAT, (3)\nwhere A is an orthogonal matrix and thus AAT is equal to the identity matrix I. Formally, low-frequency components whose amplitudes are high tend to be concentrated in the upper left corner of a spectrum, and high-frequency components are located in the remaining area. Obviously, the pattern of frequency domain is more fixed compared with diverse representations of images in spatial domain (more visualizations can be found in supplementary Sec. D.1). Therefore, we propose a spectrum saliency map S\u03d5 to mine sensitive points of different models f\u03d5, which is defined as:\nS\u03d5 = \u2202J(DI(D(x)), y;\u03d5)\n\u2202D(x) , (4)\nwhere DI(\u00b7) denotes the IDCT which can recover the input image from frequency domain back to spatial domain. Note that both the DCT and the IDCT are lossless, i.e., DI(D(x)) = ATD(x)A = x.\nFrom the visualization result of S\u03d5 shown in Figure 1, we observe that frequency components of interest usually varies from model to model. Hence, the spectrum saliency map can serve as an indicator to reflect a specific model.\n4In the implementation, DCT is applied to each color channel independently."
        },
        {
            "heading": "3.3 Spectrum Transformation",
            "text": "The analysis above motivates us that if we can simulate augmented models with a similar spectrum saliency map to victim model, the gap between the substitute model and victim model can be significantly narrowed and adversarial examples can be more transferable. Lemma 1. Assume both B1 and B2 are n-by-n matrix and B1 is invertible, then there must exist an n-by-n matrix C that can make B1 \u00d7 C = B2.\nLemma 1 shows that it is possible to make two matrices (note the essence of spectrum saliency map is also a matrix) equal in the form of a matrix transformation. However, the spectrum saliency map of vicitm model is usually not available under black-box setting. Moreover, spectrum saliency map of substitute model is high-dimensional and not guaranteed to be invertible. To tackle this problem, we propose a random spectrum transformation T (\u00b7) which decomposes matrix multiplication into matrix addition and Hadamard product to get diverse spectrums. Specifically, in combination with the DCT/IDCT, our T (\u00b7) can be expressed as:\nT (x) = DI((D(x) +D(\u03be))\u2299M), (5) = DI(D(x+ \u03be)\u2299M) (6)\nwhere \u2299 denotes Hadamard product, \u03be \u223c N (0, \u03c32I) and each element of M \u223c U(1\u2212\u03c1, 1+\u03c1) are random variants sampled from Gaussian distribution and Uniform distribution, respectively. In practice, common DCT/IDCT paradigm [6,19], i.e., splitting the image into several blocks before applying DCT, not works well for boosting transferability (see the ablation study for experimental details). Therefore, we apply DCT on the whole image in our experiments and visualization of transformation outputs can be found in supplementary Sec. D.2.\nFormally, T (\u00b7) is capable of yielding diverse spectrum saliency maps (we also provide proof in supplementary Sec. A) which can reflect the diversity of substitute models, and meanwhile, narrowing the gap with the victim model. As illustrated in Figure 1, previously proposed transformations [22,46] in the spatial domain (i.e., (b & c)) is less effective for generating diverse spectrum saliency maps, which may lead to a weaker model augmentation. In contrast, with our proposed spectrum transformation, resulting spectrum saliency map (i.e., (a)) can cover almost all those of other models."
        },
        {
            "heading": "3.4 Attack Algorithm",
            "text": "In Sec. 3.3, we have introduced our proposed spectrum transformation. This method could be integrated with any gradient-based attacks. For instance, in combination with I-FGSM [18] (i.e., Eq. 2), we propose the Spectrum Simulation Iterative Fast Gradient Sign Method (S2I-FGSM). The algorithm is detailed in Algorithm 1. Technically, our attack can be mainly divided into three steps. First, in lines 3-6, we apply our spectrum transformation T (\u00b7) to the input image x\u2032t so that the gradient g\u2032i obtained from the substitute model is approximately equal\nAlgorithm 1: S2I-FGSM\nInput : A classifier f with parameters \u03d5; loss function J ; a clean image x with ground-truth label y; iterations T ; L\u221e constraint \u03f5; spectrum transformation times N ; tunning factor \u03c1; std \u03c3 of noise \u03be. Output: The adversarial example x\u2032\n1 \u03b1 = \u03f5/T , x\u20320 = x 2 for t = 0 \u2192 T \u2212 1 do 3 for i = 1 \u2192 N do 4 Get transformation output T (x\u2032t) using Eq. 6 5 Gradient calculate g\u2032i = \u2207x\u2032tJ(T (x \u2032 t), y;\u03d5) 6 end 7 Average gradient: g\u2032 = 1 N \u2211N i=1 g \u2032 i 8 x\u2032t+1 = clipx,\u03f5 {x\u2032t + \u03b1 \u00b7 sign(g\u2032)} 9 x\u2032t+1 = clip(x \u2032 t+1, 0, 1)"
        },
        {
            "heading": "10 end",
            "text": "11 x\u2032 = x\u2032T 12 return x\u2032\nto the result obtained from a new model, i.e., model augmentation. Second, in line 7, we averageN augmented models\u2019 gradients to obtain a more stable update direction g\u2032. Finally, in line 8, we update adversarial examples x\u2032t+1 of iteration t+ 1. In short, the above process can be summarised in the following formula:\nx\u2032t+1 = clipx,\u03f5{x\u2032t + \u03b1 \u00b7 sign( 1\nN N\u2211 i=1 \u2207x\u2032tJ(T (x \u2032 t), y;\u03d5))}. (7)\nThe resulting adversarial examples are shown in Figure 2. Compared with IFGSM [18] and SI-FGSM [22], our proposed S2I-FGSM can craft more threatening adversarial examples for fooling black-box models."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Dataset. Following previous works [4,5,9,10], we conduct our experiments on the ImageNet-compatible dataset5, which contains 1000 images with resolution 299\u00d7 299\u00d7 3.\nModels.We choose six popular normally trained models, including Inceptionv3 (Inc-v3) [38], Inception-v4 (Inc-v4) [37], Inception-Resnet-v2 (IncRes-v2) [37], Resnet-v2-50 (Res-50), Resnet-v2-101 (Res-101) and Resnet-v2-152 (Res-152) [15]. For defenses, we consider nine defense models (i.e., Inc-v3ens3, Inc-v3ens4,\n5https://github.com/cleverhans-lab/cleverhans/tree/master/cleverhans_\nv3.1.0/examples/nips17_adversarial_competition/dataset\nIncRes-v2ens [42], HGD [21], R&P [51], NIPS-r3 [41], JPEG [14], RS [17] and NRP [29]) that are robust against black-box attacks.\nCompetitor. To show the effectiveness of our proposed spectrum simulation attack, we compare it with diverse state-of-the-art attack methods, including MI-FGSM [4], DI-FGSM [53], TI-FGSM [5], PI-FGSM [10], SI-NI-FGSM [22], VT-FGSM [45], FI-FGSM [47] and Admix [46]. Besides, we also compare the combined version of these methods, e.g., TI-DIM (combined version of TI-FGSM, MI-FGSM and DI-FGSM) and SI-NI-TI-DIM.\nParameter Settings. In all experiments, the maximum perturbation \u03f5 = 16, the iteration T = 10, and the step size \u03b1 = \u03f5/T = 1.6. For MI-FGSM, we set the decay factor \u00b5 = 1.0. For DI-FGSM, we set the transformation probability p = 0.5. For TI-FGSM, we set the kernel length k = 7. For PI-FGSM, we set the amplification factor \u03b2 = 10, project factor \u03b3 = 16 and the kernel length kw = 3 for normally trained models, kw = 7 for defense models. For SI-NI-FGSM, we set the number of copies m1 = 5. For VT-FGSM, we set the hyper-parameter \u03b2 = 1.5, number of sampling examples is 20. For FI-FGSM, the drop probability pd = 0.3 for normally trained models and pd = 0.1 for defense models, and the\nensemble number is 30. For Admix, we set number of copies m1 = 5 6 , sample number m2 = 3 and the admix ratio \u03b7 = 0.2. For our proposed S 2I-FGSM, we set the tuning factor \u03c1 = 0.5 for M , the standard deviation \u03c3 of \u03be is simply set to the value of \u03f5, and the number of spectrum transformations N = 20 (discussions about \u03c1, \u03c3 and N can be found in supplementary Sec. B). The parameter settings for the combined version are the same."
        },
        {
            "heading": "4.2 Attack Normally Trained Models",
            "text": "In this section, we investigate the vulnerability of normally trained models. We first compare S2I-FGSM with MI-FGSM [4], DI-FGSM [53], PI-FGSM [10] to verify the effectiveness of our method in Table 1. A first glance shows that S2I-FGSM consistently surpasses well-known baseline attacks on all black-box models. For example, when attacking against Inc-v3, MI-FGSM, DI-FGSM and PI-FGSM only successfully transfer 47.2%, 38.2% and 49.6% adversarial examples to IncRes-v2, while our S2I-FGSM can achieve a much higher success rate\n6Note that Admix is equipped with SI-FGSM by default.\nof 58.9%. This convincingly validates the high effectiveness of our proposed method against normally trained models.\nBesides, we also report the results for methods with the momentum term [4]. As displayed in Table 1, the performance gap between our proposed method and state-of-the-art approaches is still large. Notably, adversarial examples crafted by our proposed S2I-MI-FGSM are capable of getting 88.8% success rate on average, which outperforms SI-NI-FGSM, VT-MI-FGSM and FI-MI-FGSM by 7.3%, 12.2% and 6.3%, respectively. This also demonstrates that the combination of our method and existing attacks can significantly enhance the transferability of adversarial examples."
        },
        {
            "heading": "4.3 Attack Defense Models",
            "text": "Although many attack methods can easily fool normally trained models, they may fail in attacking models with the defense mechanism. To further verify the superiority of our method, we conduct a series of experiments against defense models. Given that the vanilla versions of attacks are less effective for defense\nmodels, we consider the stronger DIM, TI-DIM, PI-TI-DI-FGSM, SI-NI-TI-DIM, VT-TI-DIM, FI-TI-DIM and Admix-TI-DIM as competitors to our proposed S2I-TI-DIM, S2I-SI-DIM and S2I-SI-TI-DIM.\nSingle-Model Attacks. We first investigate the transferability of adversarial examples crafted via a single substitute model. From the results of Table 2, we can observe that our algorithm can significantly boost existing attacks. For example, suppose we generate adversarial examples via Inc-v3, TI-DIM only achieves an average success rate of 39.0% on the nine defense models, while our proposed S2I-TI-DIM can yield about 2\u00d7 transferability, i.e., outperforms TIDIM by 35.7%. This demonstrates the remarkable effectiveness of our proposed method against defense models.\nEnsemble-based Attacks. We also report the results for attacking an ensemble of models simultaneously [23] to demonstrate the effectiveness of our proposed method. In particular, the adversarial examples are crafted via an ensemble of Inc-v3, Inc-v4, IncRes-v2 and Res-152. Similar to the results of Table 2, our S2I-SI-TI-DIM displayed in Table 3 still consistently surpass state-of-the-art approaches. Remarkably, S2I-SI-TI-DIM is capable of obtaining 95.4% success rate on average, which outperforms SI-NI-TI-DIM, VT-TI-DIM, FI-TI-DIM and Admix-TI-DIM by 23.1%, 12.4%, 16.1%, 17.1% and 5.6%, respectively. This also reveals that current defense mechanisms are still vulnerable to well-design adversarial examples and far from the need of real security."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "In this section, we analyze the impact of different aspects of our method: Frequency domain vs. Spatial domain. For our proposed S2I-FGSM, transformation is applied in the frequency domain. To verify that frequency domain transformation (i.e., our spectrum transformation) is more potent in narrowing the gap between models than spatial domain transformation (i.e., remove the DCT/IDCT in spectrum transformation), we conduction an ablation study. As depicted in Figure 3 (left), regardless of what substitute models are attacked, the transferability of adversarial examples crafted based on frequency domain transformation is consistently higher than that of spatial domain transformation.\nNotably, when attacking against Inc-v3, the attack based on frequency domain transformation (i.e., S2I-FGSM) outperforms the attack based on spatial domain transformation by a large margin of 15.0%. This convincingly validates that frequency domain can capture more essential differences among models, thus yielding more diverse substitute models than spatial domain.\nEffect of \u03be and M . To analyze the effect of each random variant (i.e., \u03be and M) in our spectrum transformation, we conduct the experiment in Figure 3 (right). From this result, we observe that both \u03be and M are useful for enhancing the transferability of adversarial examples. It is because both of them can manipulate the spectrum saliency map to a certain extent, albeit from different aspects of implementation. Therefore, by leveraging them simultaneously, our proposed spectrum transformation can simulate a more diverse substitute model, thus significantly boosting attacks.\nOn the block size of DCT/IDCT. Previous works [6,19] usually started by splitting images into small blocks with size n\u00d7n and then apply DCT/IDCT. However, it is not clear that this paradigm is appropriate for our approach. Therefore, in this part, we investigate the impact of block size on the transferability. Specifically, we tune the block size from 8\u00d78 to 299\u00d7299 (full image size) and report the attack success rates of S2I-FGSM in Figure 4. From this result, we observe that larger blocks are more suited to our approach. Particularly, the attack success rates reach peak when the size of the block is the same as the full image size. Therefore, in our experiment, we do not split the image beforehand and directly apply DCT/IDCT on the full image to get its spectrum (we also provide time analysis of DCT/IDCT in supplementary Sec. C).\nAttention shift. To better understand the effectiveness of our attack, we apply Grad-CAM [35] to compare attention maps of clean images with those of adversarial examples. As illustrated in Figure 5, our proposed method can effectively shift the model\u2019s attention from the key object to other mismatched regions. Consequently, the victim model inevitably captures other irrelevant features, thus leading to misclassification."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a Spectrum Simulation Attack to boost adversarial attacks from a frequency domain perspective. Our work gives a novel insight into model augmentation, which narrows the gap between the substitute model and victim model by a set of spectrum transformation images. We also conduct a detailed ablation study to clearly illustrate the effect of each component. Compared with traditional model augmentation attacks in spatial domain, extensive experiments demonstrate the significant effectiveness of our method, which outperforms state-of-the-art transfer-based attacks by a large margin."
        },
        {
            "heading": "6 Acknowledge",
            "text": "This work is supported by the National Natural Science Foundation of China (Grant No. 62122018, No. 61772116, No. 61872064, No. U20B2063)."
        },
        {
            "heading": "A Proof",
            "text": "Proposition 1. Our proposed spectrum transformation can generate diverse spectrum saliency maps and thus simulate diverse substitute models. Proof . According to Lagrange\u2019s mean value theorem:\n\u2202J(x1, y;\u03d5)\n\u2202x1 =\n\u2202J(x2, y;\u03d5)\n\u2202x2 +K, (8)\nwhere K = \u2202 2J(\u03b6,y;\u03d5)\n\u2202\u03b62 (x1 \u2212 x2), \u03b6 \u2208 [x2,x1]. Without spectrum transformation function T (\u00b7), spectrum saliency map:\nS\u03d5 = \u2202J(DI(D(x)), y;\u03d5)\n\u2202D(x) , (9)\nafter applying our proposed spectrum transformation function T (\u00b7), the resulting spectrum saliency map:\nS\u2032\u03d5 = \u2202J(T (x), y;\u03d5)\n\u2202D(x) , (10)\nwhere T (x) = DI((D(x) +D(\u03be))\u2299M) LetD1 denotes\n\u2202J(DI(D(x)),y;\u03d5) \u2202DI(D(x)) andD2 denotes \u2202DI(D(x)) \u2202D(x) , then S\u03d5 = D1D2\n(according to chain rule). After applying T (\u00b7) to x, resulting spectrum saliency map S\u2032\u03d5 can be expressed as:\nS\u2032\u03d5 = D \u2032 1D \u2032 2 \u2299M , (11)\nwhere\nD\u20321 = \u2202J(DI(D(x+ \u03be)\u2299M), y;\u03d5)\n\u2202DI(D(x+ \u03be)\u2299M) , (12)\nD\u20322 = \u2202DI(D(x+ \u03be)\u2299M) \u2202(D(x+ \u03be)\u2299M)) . (13)\nBased on Eq. 8, we can formally formulate S\u2032\u03d5 to be:\nS\u2032\u03d5 = (D1 +K1)(D2 +K2)\u2299M , = (S\u03d5 +K \u2032)\u2299M , (14)\nwhere K1 and K2 are two specific matrices, and K \u2032 = D1K2+D2K1+K1K2. Eq. 14 clearly demonstrates that our proposed transformation T (\u00b7) is capable of simulating a different spectrum saliency map."
        },
        {
            "heading": "B On the Hyper-Parameters Settings",
            "text": "We first study the influence of the hyper-parameters(i.e., standard deviation (std) \u03c3 of noise \u03be, tuning factor \u03c1 of matrix M , number N of spectrum transformations) for the proposed Spectrum Simulation Attack method.\nB.1 On the Standard Deviation \u03c3 of Noise \u03be\nIn Figure 6, we report the attack success rates of S2I-FGSM for different std \u03c3. Adversarial examples are crafted via Inc-v3 with N = 20 and \u03c1 = 0.5. Particularly, \u03c3 = 0 means no noise is added to the input. A first glance shows that for normally trained models, the attack success rates increase gradually as \u03c3 increases and then tend to decrease when \u03c3 exceeds 16. Also when \u03c3 = 16, the defense models can achieve relatively high attack success rates. Therefore, we set \u03c3 = 16 in our paper.\nB.2 On the Tuning Factor \u03c1 of Matrix M\nIn this section, we study the effect of tuning factor \u03c1 for our S2I-FGSM in Figure 7. Adversarial examples are crafted via Inc-v3 with N = 20 and \u03c3 = 16. Particularly, \u03c1 = 0 means there is no tuning on the spectrum. Similarly, as \u03c1 increases, the degree of spectrum transformation becomes stronger and the attack success rates gradually increase and peak at \u03c1 = 0.5. If we continue to increase \u03c1 (i.e. \u03c1 > 0.5), the attack success rates will decrease which may be attributed to the excessive spectrum transformation. To achieve better transferability, we choose \u03c1 = 0.5 in our paper.\nB.3 On the Number N of Spectrum Transformations.\nIn this section, we study the effect of number N of spectrum transformations for our S2I-FGSM in Figure 8. Adversarial examples are crafted via Inc-v3 with \u03c1 = 0.5 and \u03c3 = 16. As shown in Figure 8, when N = 1, our method performs\nonly one spectrum transformation and achieves the lowest transferability. As N increases, the transferability of adversarial examples is significantly enhanced at first, and turns to increase slowly after N exceeds 20. It also demonstrates that our spectrum transformation can effectively narrow the gap between the substitute model and victim model. It is worth noting that larger N implies expensive computational overhead, as we need more forward and backward propagation for gradient computation at each iteration. To balance the transferability and computational overhead, we choose N = 20 in our paper."
        },
        {
            "heading": "C Time Analysis of DCT/IDCT",
            "text": "In our experiments, we directly apply DCT/IDCT on the full image which is a time-consuming operation. Therefore, in this section we analyze the time consumption of DCT/IDCT. In Tab.4 we show the average time of an adversarial example generated by S2I-FGSM and the average time of DCT/IDCT among it. For example, let IncRes-v2 be the substitute model, S2I-FGSM takes an average of 3.78s to produce an adversarial example, of which DCT/IDCT takes up 0.58s (only accounts for 15.3% of all overheads). The experiment is conducted on RTX 3090 GPUs."
        },
        {
            "heading": "D Additional Results",
            "text": "D.1 Spatial Domain Transformation Analysis\nIn this section, we further validate our point that analysis on spatial domain cannot well reflect the gap between models. To support our point, we first define spatial saliency map S\u0302\u03d5 as:\nS\u0302\u03d5 = \u2202J(x, y;\u03d5)\n\u2202x , (15)\nwhich is similar to our proposed spectrum saliency map S\u03d5 in Eq. 4. Then we flip the image horizontally (spatial domain transformation) and analyze their spatial saliency map and frequency saliency map. As shown in Figure 9, although spatial saliency maps between raw image and fliped image vary greatly, the changes in frequency spectrum and frequency saliency map (an indicator reflecting the characteristics of models) are small. Thus, analysis on spatial domain is unreliable and can hardly reflect the gap between models.\nD.2 Spectrum Transformation Images\nTo better understand the process of our method, we visualize the outputs of spectrum transformation. Specifically, we perform several spectrum transformations on input images and show the resulting spectrum transformation outputs in Figure 10. This figure shows that spectrum transformation just modifies colors of image and does not change its semantic information."
        }
    ],
    "title": "Frequency Domain Model Augmentation for Adversarial Attack",
    "year": 2022
}