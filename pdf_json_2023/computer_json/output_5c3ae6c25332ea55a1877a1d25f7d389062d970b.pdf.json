{
    "abstractText": "We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLPMixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of ground truths. Results show MLP-SRGAN results in sharper edges, less blurring, preserves more texture and fine-anatomical detail, with fewer parameters, faster training/evaluation time, and smaller model size than existing methods. Code for MLP-SRGAN training and inference, data generators, models and no-reference image quality metrics will be available at https://github.com/IAMLAB-Ryerson/MLP-SRGAN.",
    "authors": [
        {
            "affiliations": [],
            "name": "Samir Mitha"
        },
        {
            "affiliations": [],
            "name": "Seungho Choe"
        },
        {
            "affiliations": [],
            "name": "Pejman Jahbedar Maralani"
        },
        {
            "affiliations": [],
            "name": "Alan R. Moody"
        }
    ],
    "id": "SP:b9a31f6b86be1d59ea507549c00f9c700643683d",
    "references": [
        {
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ],
            "title": "Photo-Realistic Single Image Super- Resolution Using a Generative Adversarial Network",
            "year": 2017
        },
        {
            "authors": [
                "Chao Dong",
                "Chen Change Loy",
                "Kaiming He",
                "Xiaoou Tang"
            ],
            "title": "Image Super-Resolution Using Deep Convolutional Networks, July 2015",
            "year": 2015
        },
        {
            "authors": [
                "Xintao Wang",
                "Ke Yu",
                "Shixiang Wu",
                "Jinjin Gu",
                "Yihao Liu",
                "Chao Dong",
                "Yu Qiao",
                "Chen Change Loy"
            ],
            "title": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
            "venue": "Computer Vision \u2013 ECCV 2018 Workshops, Lecture Notes in Computer Science,",
            "year": 2018
        },
        {
            "authors": [
                "Yuchong Gu",
                "Zitao Zeng",
                "Haibin Chen",
                "Jun Wei",
                "Yaqin Zhang",
                "Binghui Chen",
                "Yingqin Li",
                "Yujuan Qin",
                "Qing Xie",
                "Zhuoren Jiang",
                "Yao Lu"
            ],
            "title": "MedSRGAN: medical images super-resolution using generative adversarial networks",
            "venue": "Multimedia Tools and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey D. Rudie",
                "Tyler Gleason",
                "Matthew J. Barkovich",
                "David M. Wilson",
                "Ajit Shankaranarayanan",
                "Tao Zhang",
                "Long Wang",
                "Enhao Gong",
                "Greg Zaharchuk",
                "Javier E. Villanueva-Meyer"
            ],
            "title": "Clinical Assessment of Deep Learning\u2013based Super-Resolution for 3D Volumetric Brain MRI. Radiology",
            "venue": "Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Andreas Steiner",
                "Daniel Keysers",
                "Jakob Uszkoreit",
                "Mario Lucic",
                "Alexey Dosovitskiy"
            ],
            "title": "MLP-Mixer: An all-MLP Architecture for Vision, June 2021",
            "year": 2021
        },
        {
            "authors": [
                "George Cazenavette",
                "Manuel Ladron De Guevara"
            ],
            "title": "MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation, August 2021",
            "year": 2021
        },
        {
            "authors": [
                "Osman Semih Kayhan",
                "Jan C. van Gemert"
            ],
            "title": "On translation invariance in cnns: Convolutional layers can exploit absolute spatial location",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Olivier Commowick",
                "Audrey Istace",
                "Micha\u00ebl Kain",
                "Baptiste Laurent",
                "Florent Leray",
                "Mathieu Simon",
                "Sorina Camarasu Pop",
                "Pascal Girard",
                "Roxana Am\u00e9li",
                "Jean-Christophe Ferr\u00e9",
                "Anne Kerbrat",
                "Thomas Tourdias",
                "Fr\u00e9d\u00e9ric Cervenansky",
                "Tristan Glatard",
                "J\u00e9r\u00e9my Beaumont",
                "Senan Doyle",
                "Florence Forbes",
                "Jesse Knight",
                "April Khademi",
                "Amirreza Mahbod",
                "Chunliang Wang",
                "Richard McKinley",
                "Franca Wagner",
                "John Muschelli",
                "Elizabeth Sweeney",
                "Eloy Roura",
                "Xavier Llad\u00f3",
                "Michel M. Santos",
                "Wellington P. Santos",
                "Abel G. Silva-Filho",
                "Xavier Tomas-Fernandez",
                "H\u00e9l\u00e8ne Urien",
                "Isabelle Bloch",
                "Sergi Valverde",
                "Mariano Cabezas",
                "Francisco Javier Vera-Olmos",
                "Norberto Malpica",
                "Charles Guttmann",
                "Sandra Vukusic",
                "Gilles Edan",
                "Michel Dojat",
                "Martin Styner",
                "Simon K. Warfield",
                "Fran\u00e7ois Cotton",
                "Christian Barillot"
            ],
            "title": "Objective Evaluation of Multiple Sclerosis Lesion Segmentation using a Data Management and Processing Infrastructure",
            "venue": "Scientific Reports,",
            "year": 2018
        },
        {
            "authors": [
                "Jean-Claude Tardif",
                "J David Spence",
                "Therese M Heinonen",
                "Alan Moody",
                "Josephine Pressacco",
                "Richard Frayne",
                "Philippe L\u2019allier",
                "Benjamin J W Chow",
                "Matthias Friedrich",
                "Sandra E Black",
                "Aaron Fenster ans Brian Rutt",
                "Rob Beanlands"
            ],
            "title": "Atherosclerosis imaging and the Canadian Atherosclerosis Imaging Network",
            "venue": "Canadian Journal of Cardiology,",
            "year": 2013
        },
        {
            "authors": [
                "Zia Mohaddes",
                "Samir Das",
                "Rida Abou-Haidar",
                "Mouna Safi-Harab",
                "David Blader",
                "Jessica Callegaro",
                "Charlie Henri- Bellemare",
                "Jingla-Fri Tunteng",
                "Leigh Evans",
                "Tara Campbell",
                "Derek Lo",
                "Pierre-Emmanuel Morin",
                "Victor Whitehead",
                "Howard Chertkow",
                "Alan C. Evans"
            ],
            "title": "National Neuroinformatics Framework for Canadian Consortium on Neurodegeneration in Aging (CCNA)",
            "venue": "Frontiers in Neuroinformatics,",
            "year": 2018
        },
        {
            "authors": [
                "Clifford R Jack Jr.",
                "Matt A Bernstein",
                "Nick C Fox",
                "Paul Thompson",
                "Gene Alexander",
                "Danielle Harvey",
                "Bret Borowski",
                "Paula J Britson",
                "Jennifer L. Whitwell",
                "Chadwick Ward"
            ],
            "title": "The alzheimer\u2019s disease neuroimaging initiative (adni): Mri methods",
            "venue": "Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine,",
            "year": 2008
        },
        {
            "authors": [
                "Brittany Reiche",
                "Alan R. Moody",
                "April Khademi"
            ],
            "title": "Pathology-preserving intensity standardization framework for multi-institutional FLAIR MRI datasets",
            "venue": "Magnetic Resonance Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ],
            "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution, July 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuchen Fan",
                "Jianchao Yang",
                "Ning Xu",
                "Zhaowen Wang",
                "Xinchao Wang",
                "Thomas Huang"
            ],
            "title": "Wide Activation for Efficient and Accurate Image Super-Resolution, December 2018",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords Super resolution, upsampling, MRI, image quality, SRGAN, MLP-Mixer"
        },
        {
            "heading": "1 Introduction",
            "text": "Fluid attenuation inversion recovery (FLAIR) MRI is used to diagnose neurological disease including dementia and cerebrovascular disease (CVD). 2D FLAIR acquisitions usually have thick slices, which limits direct comparison with other MRI sequences, or longitudinal scans. Moreover, inputs to deep-learning and registration tools may require specific spatial resolutions. In the past, bilinear and bicubic interpolation methods were used to change the spatial resolution of images. More recently, super resolution (SR) methods have been proposed for natural images that retain photorealism [1]. Convolutional neural networks (CNN) have been used to learn the mapping between the low- and high-resolution images for SR applications [2]. The perceptual output of SRCNN surpasses traditional methods such as bilinear and bicubic interpolation. More recently, generative adversarial networks (GAN) for image super-resolution (SRGAN) \u2217Keenan Research Center, St. Michael\u2019s Hospital, Toronto, ON, CAN \u2020Institute of Biomedical Engineering, Science and Technology (iBEST), Toronto, ON, CAN\nar X\niv :2\n30 3.\n06 29\n8v 1\n[ cs\n.C V\nhave been gaining traction as they maintain photo-realism in natural images for 4\u00d7 upscaling [1]. SRGAN [1] aims to recover finer texture details with a perceptual loss that combines adversarial and content losses. Enhanced SRGAN [3] introduced the Residual-in-Residual Dense Block without batch normalization and relativistic GANs that lets the discriminator predict relative realness. It produced better visual quality with more realistic and natural textures than SRGAN and is current state-of-the-art. SR has also been utilized in medical imaging with similar networks and loss functions [3] - [4], which can restore small structures (i.e. septum pellucidum) [5] and better texture detail [4]. Although these methods show promise, photorealism, resolving fine-details and maintaining texture remain problems when upscaling using SR for medical images. Existing methods can create anatomical inaccuracies, blurring, smoothing, artefacts, etc. This is especially true for FLAIR MRI with thick slices causing notable interpolation errors and blurring between object boundaries when upsampled. Existing networks also upscale in 2 dimensions, but for FLAIR MRI, we are concerned with upsampling along the (single) slice dimension. Lastly, methods based strictly on CNNs have short-comings, in that CNNs require a lot of data to train and can fail to encode position and orientation information which may be important for retaining texture or fine-details. To overcome these challenges, we propose a novel architecture called MLP-SRGAN, which is a single-dimension SRGAN that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upscale FLAIR MRI volumes. This is the first time an MLP-Mixer is employed in an SR application. Convolution-free networks are gaining attention in computer vision applications to overcome CNN limitations [6] [7]. The MLP-Mixer [6] architecture has been purposed as one such solution, and studies show multi-layered perceptrons are enough for visual learning. MLP-Mixer architectures are based entirely on MLPs repeatedly applied across either spatial locations or feature channels. They accept a sequence of linearly projected patches (tokens) that maintains dimensionality. Channel-mixing MLP communicates between channels, and token-mixing MLPs operate on each token independently. CNNs have been shown to have a dependence on spatial location, which can affect the outcome of vision applications [8]. The channel mixing functionality of the MLP-Mixer helps to reduce this spatial dependency introduced from convolutional layers. Combining MLP-Mixers with convolutional layers for vision tasks have been done before [7]. The benefit of using MLP-Mixers in conjunction with convolutional layers is a reduction of parameters and faster compute time, while still achieving similar performance to CNN-based solutions. For FLAIR MRI we complete upsampling along the slice dimension to interpolate thick slices. We propose a novel block known as the Residual MLP-Mixer in Residual Dense Block, which is used in an SRResNet-like architecture to upscale images by 4\u00d7 over a single dimension. We also propose a selective downsampling Block, which uses convolutional layers to select relevant pixels from the fully upscaled images and provide intelligent anti-aliasing for a one-dimensional upscale. The selective downsampling block also allows the output of the network to be scaled to the desired resolution. Results are compared to five popular deep learning-based SR methods and bicubic interpolation. Networks were tested on four multicentre FLAIR MRI datasets and results were compared using peak-signal-to-noise-ratio (PSNR), structural similarity index measure (SSIM), and three novel no-reference image metrics based on sharpness, entropy, and low frequency of the discrete wavelet transform."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Network Architecture",
            "text": "We propose a new architecture called MLP-SRGAN which contains a combination of MLP-Mixer blocks and convolutions in a generator network (Figure 1) and a CNN-based discriminator (Figure 2). The CNN-based discriminator network allows the generator network to converge faster during training and ensures outputs are photorealistic. The network accepts any input image resolution, and can render any output resolution, which permits us to take advantage of upscaling in a single dimension to address thick slices in FLAIR MRI."
        },
        {
            "heading": "2.1.1 Generator",
            "text": "The generator network consists of MLP-Mixer blocks with a reshaping layer, a linear connecting layer, MLP encoder, followed by layer normalization, and finally another reshaping layer. A residual connection is added from the input of the MLP-Mixer block to the output to ensure information from lower layers is maintained which improves super resolution performance [3]. Three residual MLP-Mixer blocks are serially connected with a residual connection from the input of the first block to the output of the last block, to create the residual MLP in residual dense block (RMRDB). A varying number of RMRDB are connected together sequentially to form a SRResNet-like architecture, which is connected to convolution blocks and upsampling layers to create a 4\u00d7 upscaled image. The 4\u00d7 upscaled image is then input into a new block called the selective downsampling block, which provides for a highly flexible mechanism to control the output resolution. The selective downsampling block consists of a convolutional layer followed by leaky ReLU activation as shown in Figure 1. By strategically selecting kernel size and stride of the convolutional portion of the selective downsampling\nblock, the output resolution of the network can be intelligently downsampled by an integer value. By changing the kernel size and stride of the downsampling block, and changing the number of upsampling layers, the output resolution of the volume can be scaled to any value. The flexibility of the network allows for upscaling to higher resolutions by increasing the number of RMRDBs, upsampling layers, and selective downsampling layers respectively. For this experiment, kernel size of 5x5 is chosen to ensure information from the upscaled low resolution input is contained within each convolution operation. A stride of 2x1 ensures the convolution output size is scaled down by a factor of 2 in a single dimension for each selective downsampling layer. Using this kernel and stride size allows the network to achieve a 4\u00d7 upscale over the slice direction, thereby focusing the model on the thick slices, while still maintaining the original image quality in the other dimensions."
        },
        {
            "heading": "2.1.2 Discriminator",
            "text": "The discriminator consists of convolutions, batch normalization, and leaky ReLU activations similar to [1], with the final dense layers and sigmoid activation replaced with a convolution layer to save memory."
        },
        {
            "heading": "2.2 Loss Functions",
            "text": "The loss functions and loss function parameters used are similar to those in [3]. The generator uses perceptual loss, content loss, and adversarial loss\nLgen = \u03bb1Lpercep + \u03bb2Lcontent + \u03bb3Ladv (1)\nwhere \u03bb1 = 1, \u03bb2 = 0.01, and \u03bb3 = 0.005. The perceptual loss uses features from the last convolution layer of VGG19. Mean absolute error is computed from features of the high resolution image IHR and generated super resolution image ISR to create a perceptual loss\nLpercep = N\u2211 i=1 |vgg19(IHR)\u2212 vgg19(ISR)| N\n(2)\nwhere vgg19 is the VGG19 feature extraction network. The content loss is the mean absolute error between the pixel values of the original high resolution image IHR and the generated super resolution image ISR. The adversarial loss for the generator is the binary cross entropy of 1 minus the relativistic average discriminator output, while the discriminator loss is the binary cross entropy of the discriminator output:\nLadv = \u2212 log(1\u2212D(IHR, ISR))\u2212 log(D(ISR, IJR)) (3)\nLdiscr = \u2212 log(D(IHR, ISR))\u2212 log(1\u2212D(ISR, IHR)) (4) where D is the discriminator network. The generator and discriminator are trained alternately during each iteration of training."
        },
        {
            "heading": "2.3 Image Quality Metrics",
            "text": "This section presents the image quality metrics used in this work. Let I(x, y) represent an image where (x, y) \u2208 Z2 are the spatial coordinates and L is number of graylevels with I \u2208 [0, L\u2212 1]. The human perceptual metreics are the PSNR, SSIM and the structural image quality metrics are Shannon\u2019s Entropy, Sharpness and Wavelet Low (Blurriness). The equations for each metric are summarized below.\nName Metric Peak Signal-to-Noise-Ratio (PSNR) PSNR = 10 \u00b7 log10 ( MAX2I MSE ) Structural Similarity Index Measure (SSIM) SSIM = (\n2\u00b5I1\u00b5I2+c1)(2\u03c3I1I2+c2)( \u00b52I1 +\u00b52I2 +c1 )( \u03c32I1 +\u03c32I2 +c2 ) Shannon\u2019s Entropy H(I) = \u2212 \u2211L\u22121 i=0 p (I) log p (I)\nSharpness IS(I) = 1N \u2211 x,yHsobel(x, y) \u2217 I(x, y)\nBlurriness (Wavelet Low) Low(I) = \u22115 s=1 A2s(x,y) N\n\u2022 PSNR: MAXI represents the maximum intensity given the image\u2019s bit depth, while MSE is the mean squared error between the generated and ground truth images.\n\u2022 SSIM: (\u00b5I1 ,\u00b5I2 ) and (\u03c32I1 , \u03c3 2 I2 ) are the mean and variance of the generated and ground truth images. \u03c3I1I2 is the covariance between generated and ground truth images. c1 and c2 are constants that ensure the metric does not exceed 1.\n\u2022 Shannon\u2019s Entropy: p(I) is the probability distribution (normalized number of occurences) of intensity I . Entropy measures the randomness or noise levels in an image and a high value indicates more rapid intensity variations.\n\u2022 Sharpness Hsobel(x, y) is the 2D sobel filter used for edge detection, \u2217 represents the convolution operation, and N is the total number of pixels in the image. High quality images have high contrast and sharp edges which in turn, have large edge magnitudes and an overall high Sharpness metric.\n\u2022 Blurriness: the low frequency band (approximation coefficients) of the DWT are specified by As(x, y), where s is the scale (decomposition level). Five levels (s = 5) were used with a Daubechies wavelet. This metric considers the energy in the low frequency bands. In images with more blurring, there is higher energy in the low frequency approximations, and the Wavelet metric would be high."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Data",
            "text": "Training data is from the MSSEG2 challenge [9] and consists of 80 high resolution FLAIR volumes of 256\u00d7256\u00d7256, with 0.9766mm\u00d70.9766mm\u00d70.5-1.0mm resolution from GE, Philips and Siemens scanners. Sixty-four (64) volumes are used for training/validation, and the remainder 16 are used for testing. Volumes are split into individual slices in the sagittal plane with blank slices removed for training, resulting in a total of 10,940 sagittal slices used to train each network. The original high-resolution (HR) slices are used as ground truth, and low-resolution (LR) images are\ncreated by downsampling the original images to 256\u00d764 with bicubic interpolation. All models under go a five fold cross validation so all 80 volumes are used for testing purposes without data leakage. One hundred (100) volumes randomly sampled from three additional multicentre, clinical data sets are used as hold out, unseen datasets. The datasets are from the Canadian Atherosclerosis Imaging Network (CAIN) dataset [10], the Canadian Consortium of Neurodegeneration and Aging (CCNA) dataset [11], and the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) [12]. Acquisition parameters are shown in Table 1. All volumes are intensity normalized [13]."
        },
        {
            "heading": "3.2 Training Details",
            "text": "To fairly compare with other super resolution experiments, the proposed network performs a 4\u00d7 upscale. A batch size of 8 is used, with low resolution inputs sized to 256\u00d764 and output images sized to 256\u00d7256. Adam optimizer with default PyTorch parameters, a learning rate of 2e-4 and a decay beginning in the 100th epoch are used. The network uses a pre-trained VGG19 network for the perceptual loss. Since this network requires a 3-channel input image, the input images are duplicated into 3 channels. The VGG19 pre-trained model requires images to be normalized to a specific intensity range and default VGG19 parameters are used \u00b5R = 0.485, \u00b5G = 0.456, \u00b5B = 0.406, and \u03c3R = 0.229, \u03c3G = 0.224, \u03c3B = 0.225. The network interpolation strategy proposed by [3] is used to train the generator, where the generator network is trained alone for 500 iterations using only the content loss. This strategy of warm up iterations helps to improve stability once the perceptual and adversarial loss when the discriminator is added as the generator is initialized with weights that help to maintain intensity."
        },
        {
            "heading": "3.3 Image Quality Metrics",
            "text": "Several image quality metrics are used to evaluate upsampling performance. Peak Signal-to-Noise-Ratio (PSNR) and Structural Similarity Index Measure (SSIM) were used as they traditional metrics used in the literature. We also define three new no-reference based metrics to quantify noise/randomness, sharpness, and blurriness in Appendix A: Table 7. The first metric uses Shannon\u2019s entropy to measure noise/randomness in the upscaled images - a random image would have higher entropy. The second metric is Sharpness which measures the average magnitude of the image\u2019s edge content from the Sobel gradient - an image with high contrast and sharp edges would have a large Sharpness metric. The third metric measures the energy of the low frequency bands from the wavelet decomposition to quantify image blurriness - blurry images have high wavelet energy."
        },
        {
            "heading": "4 Results",
            "text": "Performance of MLP-SRGAN was compared to bicubic interpolation, EDSR [14], WDSR [15], SRGAN [1], ESRGAN [3], and SRCNN [2]. MLP-SRGAN was tested with varying number of RMRDBs, represented with (D-N), where N is the number of RMRDBs in the network. MLP-SRGAN (D-1) was also tested without a discriminator, using only the generator and excluding adversarial loss. Training time, evaluation time, model size, and trainable parameters is shown in Table 2. MLP-SRGAN has lower training time and approximately same inference time compared to ESRGAN. Increasing the number of RMRDBs increases the number of parameters, training/inference time. To determine whether performance is different between two models, paired-tests are performed on log-transformed metrics. See Figure 3 for generated images from select models in MSSEG test data. Generated images for all methods and datasets are shown in Figure 10. Images generated by MLP-SRGAN produce noticeable sharpness in edge content (less blur), with more texture compared to other methods, including ESRGAN. There are differences in small anatomical structures between ESRGAN and MLP-SRGAN. As shown in sulci/gyri (top in Figure 3), the MLP-SRGAN more accurately represents anatomical details of the original image and has high quality upsampling for fine details. Using MLP-SRGAN, fine details of the cerebellum (bottom) closely follow the original ground truth image, including shape/edges/texture. ESRGAN output shows fine-details are lost, increased blur and there is lower shape and texture correspondence with the original. The high quality generation of fine details, anatomical accuracy, and improved texture of the MLP-Model is retaining small details.\nPerformance metrics for the in-distribution MSSEG2 dataset are shown in Table 4 (average over five folds). The distribution of all the metrics is shown in Figure 7 and 8. The ground truth metrics are computed from the high resolution ground truth images and serve as gold standards for PSNR, SSIM, edge quality (Sharpness), noisiness (Entropy) and blurriness (Wavelet). For perfect upsampling, the metrics from the generated images would be identical to those from the ground truths. MLP-SRGAN without the discriminator performed the worst (most blur (Wavelet) similar to bicubic, highest randomness (Entropy) and Sharpness most dissimilar). As a result, MLP-SRGAN (No Discr) is not considered further. Images generated by MLP-SRGAN have image quality metrics closer to the gold standard metrics, shown as bold in the table (and red line in the box plots). This shows a discriminator helps to resolve fine-details and improve realness of the images.\nTop models are MLP-SRGAN (D-1) and MLP-SRGAN (D-3). Since MLP-SRGAN (D-1) is more computationally efficient, we choose to further analyze this model and benchmark it against ESRGAN (current state-of-the-art SR method). Metrics from Table 4 show images generated from MLP-SRGAN (D-1) are more similar to ground truth (smallest metric difference) compared to ESRGAN. T-tests were used to compare metrics between MLP-SRGAN and ESRGAN and p-values are shown in Table 3. Over all metrics, there were statistical differences between MLP-SRGAN and ESRGAN (p < 0.05) for SSIM, PSNR, and Entropy, indicating a significant improvement in image quality as explained by these metrics. These trends are supported by histograms of the metrics in Figure 5 and Figure 6. We hypothesize the fine anatomical details and texture preservation (which we observed visually), combined with sharper edges, less blurriness and more smoothness, contributes to these differences. There were no differences between MLP-SRGAN and ESRGAN with respect to the Sharpness or Wavelet measures (they have overlapping histograms). MLP-SRGAN (D-1) achieves these results with fewer parameters, 2.26\u00d7 faster training time, 2.20\u00d7 evaluation time, and 0.54\u00d7 smaller model size compared to ESRGAN. The MLP-SRGAN (D-1), ESRGAN and bicubic interpolation methods are evaluated further on held-out (unseen)\nclinical datasets (CAIN, ADNI, CCNA). The clinical datasets do not have HR ground truths and therefore, we cannot compute PSNR or SSIM. Instead, we use the no-reference metrics Entropy, Sharpness and Wavelet and the distributions are shown in Figure 4. T-tests comparing MLP-SRGAN to ESRGAN are shown in Table 6 for ADNI, Table 5 for CAIN and Table 7 for CCNA. MLP-SRGAN (D-1) has similar Sharpness to ESRGAN, with lower Entropy and Wavelet metrics. Lower Entropy indicates smoother textures (less noise/randomness) in upscaled images for MLP-SRGAN (D-1).There are significant differences in Sharpness, Entropy and Wavelet between MLP-SRGAN and ESRGAN for the CAIN and CCNA datasets. In ADNI, there are some metrics that were similar, including Wavelet and Sharpness, but differences for Entropy. Entropy was significantly lower and different in MSSEG as well as closer to the ground truth on MSSEG, indicating this may be a good metric for future studies on texture differences between images. A lower Wavelet feature for MLP-SRGAN (D-1) indicates lower energy in the low frequency bands (less blurriness) which can be attributed to the intelligent upsampling along the (thick) slice dimension. This was statistically the same as ESRGAN, so blur may be similar. Since these are global metrics it may be hard to quantify local differences. In the future, we will examine metrics that analyze more local features, fine anatomical structures and texture."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed a novel architecture called MLP-SRGAN for upscaling FLAIR MRI images in a single dimension. The proposed method consists of a combination of MLP-Mixers and convolutions in the generator network and convolutions in the discriminator network. The reconfigurable RMRDB blocks, upsampling layers, and selective downsampling layers allow the network to be scaled to higher output resolutions. MLP-SRGAN (D-1) has significantly better\nperformance (p < 0.05) on testing sets, faster training and evaluation times compared to state-of-the-art methods such as ESRGAN. Visual analysis shows better retaining of texture, small anatomical details, with less blurring and noise, and higher quality edges in images generated from MLP-SRGAN. We hypothesize the MLP-Mixer blocks are able to retain fine-features by learning mappings from raw image pixels, which have a spatial dependency (compared to CNNs that fail to encode position and orientation information)."
        },
        {
            "heading": "Acknowledgments",
            "text": "We acknowledge the Natural Sciences and Engineering Research Council (NSERC) of Canada (Discovery Grant), Alzheimer\u2019s Society Research Program (ASRP) (New Investigator Grant) and the Ontario Government (Early Researcher Award) for funding this research."
        },
        {
            "heading": "A Supplemental Data",
            "text": "Table 5: P-values for t-tests comparing MLP-SRGAN and ESRGAN for CAIN\nSharpness Entropy Wavelet MLP-SRGAN (D-1) (No Discr) <0.01 <0.01 0.9\nMLP-SRGAN (D-1) 0.01 <0.01 <0.01 MLP-SRGAN (D-3) 0.05 <0.01 <0.01 MLP-SRGAN (D-5) <0.01 <0.01 <0.01\nTable 6: P-values for t-tests comparing MLP-SRGAN and ESRGAN for ADNI"
        }
    ],
    "title": "MLP-SRGAN: A SINGLE-DIMENSION SUPER RESOLUTION GAN USING MLP-MIXER",
    "year": 2023
}