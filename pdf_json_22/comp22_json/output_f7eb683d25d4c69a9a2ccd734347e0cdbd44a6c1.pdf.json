{
    "abstractText": "Limited by the locality of convolutional neural networks, most existing local features description methods only learn local descriptors with local information and lack awareness of global and surrounding spatial context. In this work, we focus on making local descriptors \u201clook wider to describe better\u201d by learning local Descriptors with More Than just Local information (MTLDesc). Specifically, we resort to context augmentation and spatial attention mechanisms to make our MTLDesc obtain non-local awareness. First, Adaptive Global Context Augmented Module and Diverse Local Context Augmented Module are proposed to construct robust local descriptors with context information from global to local. Second, Consistent Attention Weighted Triplet Loss is designed to integrate spatial attention awareness into both optimization and matching stages of local descriptors learning. Third, Local Features Detection with Feature Pyramid is given to obtain more stable and accurate keypoints localization. With the above innovations, the performance of our MTLDesc significantly surpasses the prior state-of-the-art local descriptors on HPatches, Aachen Day-Night localization and InLoc indoor localization benchmarks.[Code release]",
    "authors": [
        {
            "affiliations": [],
            "name": "Changwei Wang"
        },
        {
            "affiliations": [],
            "name": "Rongtao Xu"
        },
        {
            "affiliations": [],
            "name": "Yuyang Zhang"
        },
        {
            "affiliations": [],
            "name": "Shibiao Xu"
        },
        {
            "affiliations": [],
            "name": "Weiliang Meng"
        },
        {
            "affiliations": [],
            "name": "Bin Fan"
        },
        {
            "affiliations": [],
            "name": "Xiaopeng Zhang"
        }
    ],
    "id": "SP:a309b18a3735574d6725d396db223c1a42fd9cb4",
    "references": [
        {
            "authors": [
                "V. Balntas",
                "K. Lenc",
                "A. Vedaldi",
                "K. Mikolajczyk"
            ],
            "title": "HPatches: A benchmark and evaluation of handcrafted and learned local descriptors",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5173\u20135182.",
            "year": 2017
        },
        {
            "authors": [
                "A. Barroso-Laguna",
                "E. Riba",
                "D. Ponsa",
                "K. Mikolajczyk"
            ],
            "title": "Key",
            "venue": "net: Keypoint detection by handcrafted and learned cnn filters. In Proceedings of the IEEE International Conference on Computer Vision, 5836\u20135844.",
            "year": 2019
        },
        {
            "authors": [
                "B. Cao",
                "A. Araujo",
                "J. Sim"
            ],
            "title": "Unifying deep local and global features for image search",
            "venue": "European Conference on Computer Vision, 726\u2013743. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "N. Carion",
                "F. Massa",
                "G. Synnaeve",
                "N. Usunier",
                "A. Kirillov",
                "S. Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "European Conference on Computer Vision, 213\u2013229. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "L.-C. Chen",
                "G. Papandreou",
                "I. Kokkinos",
                "K. Murphy",
                "A.L. Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 40(4): 834\u2013848.",
            "year": 2017
        },
        {
            "authors": [
                "J. Dai",
                "H. Qi",
                "Y. Xiong",
                "Y. Li",
                "G. Zhang",
                "H. Hu",
                "Y. Wei"
            ],
            "title": "Deformable convolutional networks",
            "venue": "Proceedings of the IEEE international conference on computer vision, 764\u2013773.",
            "year": 2017
        },
        {
            "authors": [
                "D. DeTone",
                "T. Malisiewicz",
                "A. Rabinovich"
            ],
            "title": "Superpoint: Self-supervised interest point detection and description",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 224\u2013236.",
            "year": 2018
        },
        {
            "authors": [
                "M. Dusmanu",
                "I. Rocco",
                "T. Pajdla",
                "M. Pollefeys",
                "J. Sivic",
                "A. Torii",
                "T. Sattler"
            ],
            "title": "D2-Net: A Trainable CNN for Joint Detection and Description of Local Features",
            "venue": "CVPR 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Germain",
                "G. Bourmaud",
                "V. Lepetit"
            ],
            "title": "S2DNet: Learning Image Features for Accurate Sparse-to-Dense Matching",
            "venue": "European Conference on Computer Vision, 626\u2013643. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "B. Hariharan",
                "P. Arbelaez",
                "R. Girshick",
                "J. Malik"
            ],
            "title": "Object instance segmentation and fine-grained localization using hypercolumns",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 39(4): 627\u2013639.",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "G. Hinton"
            ],
            "title": "Using relaxation to find a puppet",
            "venue": "Proceedings of the 2nd Summer Conference on Artificial Intelligence and Simulation of Behaviour, 148\u2013157.",
            "year": 1976
        },
        {
            "authors": [
                "Y. Kalantidis",
                "C. Mellina",
                "S. Osindero"
            ],
            "title": "Crossdimensional weighting for aggregated deep convolutional features",
            "venue": "European conference on computer vision, 685\u2013 701. Springer.",
            "year": 2016
        },
        {
            "authors": [
                "C.-Y. Lee",
                "S. Xie",
                "P. Gallagher",
                "Z. Zhang",
                "Z. Tu"
            ],
            "title": "Deeply-supervised nets",
            "venue": "Artificial intelligence and statistics, 562\u2013570. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Li",
                "N. Snavely"
            ],
            "title": "Megadepth: Learning singleview depth prediction from internet photos",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2041\u20132050.",
            "year": 2018
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 3431\u20133440.",
            "year": 2015
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scaleinvariant keypoints",
            "venue": "International journal of computer vision, 60(2): 91\u2013110.",
            "year": 2004
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scaleinvariant keypoints",
            "venue": "International journal of computer vision, 60(2): 91\u2013110.",
            "year": 2004
        },
        {
            "authors": [
                "Z. Luo",
                "T. Shen",
                "L. Zhou",
                "J. Zhang",
                "Y. Yao",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Contextdesc: Local descriptor augmentation with cross-modality context",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2527\u20132536.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Luo",
                "L. Zhou",
                "X. Bai",
                "H. Chen",
                "J. Zhang",
                "Y. Yao",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Aslfeat: Learning local features of accurate shape and localization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6589\u20136598.",
            "year": 2020
        },
        {
            "authors": [
                "K. Mikolajczyk",
                "C. Schmid"
            ],
            "title": "A performance evaluation of local descriptors",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 27(10): 1615\u20131630.",
            "year": 2005
        },
        {
            "authors": [
                "A. Mishchuk",
                "D. Mishkin",
                "F. Radenovic",
                "J. Matas"
            ],
            "title": "Working hard to know your neighbor\u2019s margins: Local descriptor learning loss",
            "venue": "Advances in Neural Information Processing Systems, 4826\u20134837.",
            "year": 2017
        },
        {
            "authors": [
                "H. Noh",
                "A. Araujo",
                "J. Sim",
                "T. Weyand",
                "B. Han"
            ],
            "title": "Large-scale image retrieval with attentive deep local features",
            "venue": "Proceedings of the IEEE international conference on computer vision, 3456\u20133465.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ono",
                "E. Trulls",
                "P. Fua",
                "K.M. Yi"
            ],
            "title": "LF-Net: learning local features from images",
            "venue": "Advances in neural information processing systems, 6234\u20136244.",
            "year": 2018
        },
        {
            "authors": [
                "J. Revaud",
                "C. De Souza",
                "M. Humenberger",
                "P. Weinzaepfel"
            ],
            "title": "R2D2: Reliable and Repeatable Detector and Descriptor",
            "venue": "Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alch\u00e9-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 32. Curran",
            "year": 2019
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In",
            "year": 2015
        },
        {
            "authors": [
                "P.-E. Sarlin",
                "C. Cadena",
                "R. Siegwart",
                "M. Dymczyk"
            ],
            "title": "From coarse to fine: Robust hierarchical localization at large scale",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12716\u201312725.",
            "year": 2019
        },
        {
            "authors": [
                "T. Sattler",
                "T. Weyand",
                "B. Leibe",
                "L. Kobbelt"
            ],
            "title": "Image Retrieval for Image-Based Localization Revisited",
            "venue": "BMVC.",
            "year": 2012
        },
        {
            "authors": [
                "J.L. Schonberger",
                "J.-M. Frahm"
            ],
            "title": "Structure-frommotion revisited",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4104\u20134113.",
            "year": 2016
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556.",
            "year": 2014
        },
        {
            "authors": [
                "H. Taira",
                "M. Okutomi",
                "T. Sattler",
                "M. Cimpoi",
                "M. Pollefeys",
                "J. Sivic",
                "T. Pajdla",
                "A. Torii"
            ],
            "title": "InLoc: Indoor visual localization with dense matching and view synthesis",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7199\u20137209.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Tian",
                "A. Barroso Laguna",
                "T. Ng",
                "V. Balntas",
                "K. Mikolajczyk"
            ],
            "title": "HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss",
            "venue": "Advances in Neural Information Processing Systems, 33.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "B. Fan",
                "F. Wu"
            ],
            "title": "L2-net: Deep learning of discriminative patch descriptor in euclidean space",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 661\u2013669.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Tian",
                "X. Yu",
                "B. Fan",
                "F. Wu",
                "H. Heijnen",
                "V. Balntas"
            ],
            "title": "SOSNet: Second order similarity regularization for local descriptor learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 11016\u201311025.",
            "year": 2019
        },
        {
            "authors": [
                "G. Tolias",
                "T. Jenicek",
                "O. Chum"
            ],
            "title": "Learning and aggregating deep local descriptors for instance-level recognition",
            "venue": "European Conference on Computer Vision, 460\u2013 477. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "M. Tyszkiewicz",
                "P. Fua",
                "E. Trulls"
            ],
            "title": "DISK: Learning local features with policy gradient",
            "venue": "Advances in Neural Information Processing Systems, 33.",
            "year": 2020
        },
        {
            "authors": [
                "H. Wang",
                "Y. Zhu",
                "B. Green",
                "H. Adam",
                "A. Yuille",
                "L.-C. Chen"
            ],
            "title": "Axial-deeplab: Stand-alone axialattention for panoptic segmentation",
            "venue": "European Conference on Computer Vision, 108\u2013126. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Wang",
                "X. Zhou",
                "B. Hariharan",
                "N. Snavely"
            ],
            "title": "Learning Feature Descriptors using Camera Pose Supervision",
            "venue": "Proc. European Conference on Computer Vision (ECCV).",
            "year": 2020
        },
        {
            "authors": [
                "B. Wu",
                "C. Xu",
                "X. Dai",
                "A. Wan",
                "P. Zhang",
                "M. Tomizuka",
                "K. Keutzer",
                "P. Vajda"
            ],
            "title": "Visual transformers: Tokenbased image representation and processing for computer vision",
            "venue": "arXiv preprint arXiv:2006.03677.",
            "year": 2020
        },
        {
            "authors": [
                "C. Yu",
                "J. Wang",
                "C. Peng",
                "C. Gao",
                "G. Yu",
                "N. Sang"
            ],
            "title": "Bisenet: Bilateral segmentation network for real-time semantic segmentation",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 325\u2013341.",
            "year": 2018
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun"
            ],
            "title": "Multi-scale context aggregation by dilated convolutions",
            "venue": "arXiv preprint arXiv:1511.07122. Zhang, Y.; Wang, J.; Xu, S.; Liu, X.; and Zhang, X. 2020. MLIFeat: Multi-level information fusion based deep local features. In Proceedings of the Asian Conference on Com-",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Local descriptors currently play a key role in various vision applications such as image matching, image retrieval, SfM, SLAM, and visual localization. With the industry\u2019s rapid development, these applications must deal with more complex and challenging scenarios (various conditions such as day, night, and seasons). As the local features detection and description are the critical for these applications, there is an urgent need to further boost their performance.\nGeoffrey Hinton said that \u201clocal ambiguities have to be resolved by finding the best global interpretation\u201d in his first paper (Hinton 1976). This idea still holds true in local descriptors learning. There are two main weaknesses for learning local descriptors only using limited local visual information: i) Ambiguity regions with repetitive patterns (texture, color, shape, etc.) is difficult to be distinguished only\n*These authors contributed equally. \u2020Shibiao Xu and Weiliang Meng are the corresponding authors\n(shibiaoxu@bupt.edu.cn, weiliang.meng@ia.ac.cn). Copyright \u00a9 2022\nby local information, as shown in the first row (trees) and the fourth row (river and ground) in Fig. 1 (b); ii) Local visual information becomes unreliable and indistinguishable for challenging scenes with large illumination and viewpoint changes, which will lead to massive incorrect matches (the second and fourth rows of Fig. 1 (b)). On this account, robust non-local context can be employed to better distinguish challenging local regions. We propose Context Augmentation and Consistent Attention Weighting to look wider for describing better, enabling our descriptors to gain awareness beyond the local region, in turn to effectively mitigate above weaknesses as shown in Fig. 1 (d).\nCNN-based backbones like L2Net (Tian, Fan, and Wu 2017) and VGG (Simonyan and Zisserman 2014) are widely adopted by the local descriptors learning methods. Due to the inherent locality of CNN, features can only be extracted in a limited receptive field. Although some of these meth-\nar X\niv :2\n20 3.\n07 00\n3v 1\n[ cs\n.C V\n] 1\n4 M\nar 2\nods (Luo et al. 2019, 2020; Tyszkiewicz, Fua, and Trulls 2020) can implicitly alleviate this problem by extracting features in a larger receptive field, they still only considered the context in a fixed patch-wise neighborhood. By contrast, our method further utilizes the context of the global and different receptive fields by the well-designed Adaptive Global Context Augmented Module (AGCA) and Diverse Local Context Augmented Module (DLCA).\nIn addition, the attention mechanism is also an effective way to use non-local information. When humans observe and describe images, they will quickly analyze spatial information and focus their attention on some key regions. Some image retrieval practices (Cao, Araujo, and Sim 2020) have indicated that the network can also obtain similar awareness with the attention mechanism. Based on the above inspiration, we design a new Consistent Attention Mechanism for both optimization and matching of local descriptors by a well-designed Consistent Attention Weighted Triplet Loss.\nFurthermore, as the localization accuracy of keypoints also affects the results of local features matching, we propose a Local Features Detection with Feature Pyramid based on the classical scale-space (Lowe 2004a) and deep supervision (Lee et al. 2015) to obtain more stable and accurate keypoints localization. Besides, to meet the demand of practical applications, running speed should also be addressed. Therefore, the whole method is carefully designed to be as fast as possible. In summary, there are four main contributions in this paper: 1) We devise AGCA and DLCA modules to aggregate effectively from global to local context information for local descriptors learning. 2) We propose a novel Consistent Attention Weighted Triplet Loss to introduce spatial consistent attention awareness in both optimization and matching of local descriptors. 3) We present the Local Features Detection with Feature Pyramid to improve the localization accuracy of keypoints. 4) We provide a realtime solution for local features learning named MTLDesc, which achieves state-of-the-art on HPatches, Aachen-DayNight and InLoc benchmarks."
        },
        {
            "heading": "Related Works",
            "text": "Local Descriptors Learning: Hand-crafted local descriptors are widely used in computer vision, and are comprehensively evaluated in (Mikolajczyk and Schmid 2005). Current deep learning based descriptors can roughly fall into two categories: patch-based descriptors and dense descriptors. Patch-based descriptors (Tian, Fan, and Wu 2017; Mishchuk et al. 2017; Tian et al. 2019; Luo et al. 2019) extract descriptors from corresponding patches of the detected keypoints (e.g. SIFT (Lowe 2004b)), while dense descriptors (DeTone, Malisiewicz, and Rabinovich 2018; Revaud et al. 2019; Dusmanu et al. 2019; Revaud et al. 2019; Luo et al. 2020; Wang et al. 2020b; Tyszkiewicz, Fua, and Trulls 2020) usually use a fully convolutional neural network (Long, Shelhamer, and Darrell 2015) to extract dense feature descriptors for the whole image in one forward pass. Dense descriptors have achieved good performance in image matching and long-term visual localization, showing great potential for practical applications. In contrast to these prior\nworks, we recommend the introduction of non-local information include both context and spatial attention awareness to local descriptors learning, aiming to make the local descriptors \u201clook wider to describe better\u201d.\nContext Awareness: Context awareness is essential for pixel-level tasks (Yu et al. 2018), but it has not been attracted widespread attention in local descriptors learning. ContextDesc (Luo et al. 2019) aggregates the cross-modality context for local descriptors, including visual context from a ResNet-50 (He et al. 2016) branch, and geometric context from keypoints distribution. Patch-based ContextDesc depends on a large network and needs to obtain the geometric context after additional keypoints detection calculation, so it consumes more computing time and memory space. Some CNN-based dense descriptors implicitly improve context awareness by increasing the receptive field. ASLFeat (Luo et al. 2020) proposes to use deformable convolution (Dai et al. 2017) to extract descriptors with shape context, MLIFeat (Zhang et al. 2020) utilizes hypercolumns (Hariharan et al. 2016) to fuse multi-level features, while DISK (Tyszkiewicz, Fua, and Trulls 2020) employs UNet-like backbone (Ronneberger, Fischer, and Brox 2015) to fuse multi-scale context information. However, all these descriptors only aggregate the context of a fixed receptive field and do not consider the global context. Recently, Visual Transformers (Wu et al. 2020) has shown the ability to aggregate global context in some computer vision applications (Carion et al. 2020; Wang et al. 2020a). In this work, we make the network get comprehensive context awareness from global to local. On the one hand, we use visual transformer and a learnable Gated Map to adaptively embed the global context and location information into local descriptors. On the other hand, we propose to flexibly learn local descriptors through surrounding contexts with different receptive fields.\nAttention Mechanism: As a non-local awareness, spatial attention has been successfully applied to the learning of image-level global descriptors (Kalantidis, Mellina, and Osindero 2016; Noh et al. 2017; Cao, Araujo, and Sim 2020; Tolias, Jenicek, and Chum 2020) for image retrieval. In these methods, spatial attention is used as the weight of local descriptors, and global descriptors are derived from local descriptors through weighted summation. However, directly applying the local descriptors of these methods to image matching produces poor results, as reported by (Revaud et al. 2019; Dusmanu et al. 2019). This may be caused by the lack of supervision of local pixel correspondence. However, attention mechanisms in these methods are optimized with the supervision of image-level and it is not suitable for pixel correspondence supervision. In contrast, we proposes a special consistent attention mechanism to improve the optimization and matching of local descriptors for image matching."
        },
        {
            "heading": "Method",
            "text": "Our MTLDesc employs a fully convolutional network encoder as the shared backbone for both local features description and detection. The encoder consists of 3 \u00d7 3 convolutional layers, relu layers, and max-pooling layers. For a h\u00d7w image I ,C1(h\u00d7w) ,C2(h/2\u00d7w/2),C3(h/4\u00d7w/4),\nand C4(h/8\u00d7w/8) feature maps are obtained after four sequential sub-encoders. MTLDesc detects keypoints (Fig. 5) and extracts corresponding local descriptors (Fig. 2) at the same time, and the two parts use the same shared backbone network."
        },
        {
            "heading": "Local Descriptors with Non-local Information",
            "text": "I. Local Descriptors with Context Augmentation: We propose Adaptive Global Context Augmented Module and Diverse Local Context Augmented Module to implement context augmentation from global to local, as shown in Fig. 2 (a), while Fig. 2 (b) shows the difference between the two modules about the receptive field, which leads them to aggregate context from different perspectives.\n(1) Adaptive Global Context Augmented Module. Different image regions usually have different degrees of demand for global context. For regions that are difficult to describe only with local information (e.g. weak or repeated textures regions), the global context can introduce more spatial information to make it more discriminative. But for regions with good distinguishable local information, directly adding the global context may bring some noise. Our Adaptive Global Context Augmented Module is designed to adaptively introduce global context for local descriptors. Specifically, we take the feature maps C4 through adaptive average pooling to obtain a fixed-size feature map (64 \u00d7 64) as the input of the module. Compared with previous visual transformers, our method can effectively reduce the computational complexity and adapt to images of any size. Following (Wu et al. 2020), we perform tokenization by reshaping the input into a sequence of flattened 2D patches Xp, and each patch is of size 16\u00d7 16. We map the vectorized patches Xp a latent 128-dimensional embedding space using a trainable linear projection. In order to make the local descriptors obtain the spatial position information relative to the global, we learn specific position embeddings which are added to the patch embeddings to retain positional information as follows: Z0 = [X1pE;X 2 pE; ...;X N p E] + Epos, where E is the\npatch embedding projection and Epos is the position embedding. After Z0 passes through 8 transformer layers, the hidden features with global context are obtained. The structure of transformer layer is shown in Fig. 2 (d). Where MSA denotes Multihead Self-Attention and MLP denotes MultiLayer Perceptron block (Wu et al. 2020). After reshaping, we can get the patch-wise descriptors with the global context. Another branch of the module predicts a gated map to mask regions that do not require a global context supplement. We implement the gating mechanism through the relu activation function, and the visualization result of gated map is shown in Fig. 2 (c). Finally, we merge the global context filtered by the gated map into local descriptors.\n(2) Diverse Local Context Augmented Module. The surrounding context is also crucial for local descriptors learning. We design a simple and effective Diverse Local Context Augmented Module to extract diverse surrounding contexts. Unlike most previous CNN-based local descriptors which only deploy the top-layer feature maps to extract descriptors, we recommend using all feature maps derived from the backbone to construct descriptors. Specifically, the C1, C2, C3, C4 are interpolated to the same spatial size and then aggregated together to get Ccat. The size is set to 1/4 of the input image I as it gives a good trade-off between accuracy and speed. It improves the utilization of feature maps and integrates information of different scales. To obtain diverse surrounding contexts, we decouple the descriptor into four 32-dimensional sub-descriptors and learn them in different description spaces respectively. This ensures that the sub-descriptors remain independent and diversity to contain more information. Specifically, we use 1\u00d7 1 Conv and three 3\u00d7 3 dilated Conv (Yu and Koltun 2015) with dilation rates of 6, 12 and 18 respectively to derive descriptors from different receptive fields. After being concatenated and added to Draw, the final dense descriptor D is obtained. The surrounding context from different receptive fields further stimulates the representation ability of local descriptors.\nII. Local Descriptors with Consistent Attention Weighting: To further overcome the limitation of keypoints description only based on local information, we make the network imitate humans to obtain the awareness and insight of spatial information with a consistent attention mechanism. We claim that the proposed Consistent Attention has three types of properties: i) The same regions in different images have consistent attention scores. ii) Representative regions are given higher attention scores, as these regions easily match to inliers while distinguishable to outliers. iii) Descriptors from regions with high attention scores are optimized first. We now explain how we design the module and loss for applying consistent attention to make the optimization and matching of local descriptors better.\nConsistent Attention Weighting Module is shown in Fig 2 (a). Specifically, Ccat is averaged across the channel dimension, and the attention map W is predicted from this averaged feature map via the 3\u00d7 3 Conv + SoftPlus.\nConsistent Attention weighted Triplet Loss is designed to jointly optimize local descriptors and consistent attention. Considering an image pair (I, I \u2032), the dense descriptors D,D\u2032 and attention maps W,W \u2032 of I, I \u2032 can be extracted by our MTLDesc, as shown in Fig. 2 (a). Given the sampled points set P of size N in I and their corresponding points P \u2032 in I \u2032 , the corresponding descriptors of P, P \u2032 are denoted as di and d\u2032i, i \u2208 1 . . . N . The corresponding score of the descriptor di on the attention map W is \u03c9i, so the attention weighted descriptor is defined as xi = \u03c9i \u00b7 di . For xi, its positive distance ||xi||+ is defined as:\n||xi||+ = ||\u03c9i \u00b7 di \u2212 \u03c9\u2032i \u00b7 d\u2032i||2, (1) and its hardest negative distance ||xi||\u2212 is defined as:\n||xi||\u2212 = min j\u22081...N,j 6=i (||\u03c9i \u00b7 di \u2212 \u03c9\u2032j \u00b7 d\u2032j ||2). (2)\nThe Consistent Attention Weighted Triplet Loss LAtrip can be defined as:\nLAtrip(x) = e \u03c9/T\u2211N\ni=1 e \u03c9i/T\nmax(0, ||x||+\u2212 ||x||\u2212+1), (3)\nwhere \u03c9 is the attention score corresponding to x, and T is a smoothing factor. T is used to adjust the effect of attention weight on loss. We will explore the impact of T in next\n(1) Consistent Attention in Optimization: First, we will discuss the difference between our LAtrip and standard triplet loss in the optimization direction. It is easy to validate the optimization of the L2 normalized descriptor\u2019s distance degenerates to angle\u2019s optimization (Tian et al. 2020), meaning that the common standard triplet loss only has the optimization of the angle component as shown in Fig. 4 (a). However, the optimization direction of our LAtrip is decoupled into the component of the descriptor d (angle) optimization and the component of the consistent attention \u03c9 (weight) optimization in Fig. 4 (b). For descriptors, LAtrip still optimizes the angle between them. For consistent attention, as shown in Fig. 4 (b), the attention scores of positive samples tend to convergent while the attention scores of negative samples tend to divergent. This trend will lead to the consistent distribution of attention scores (i.e., property i)) in corresponding regions of image pairs. The details are shown in Fig. 3.\nSecond, we will further explore the optimization goal of consistent attention based on the above discussion. As shown in Eq. 3, the consistent attention \u03c9 is affected by both triplet loss term max(0, ||x||+\u2212||x||\u2212+1) which only provides consistency as mentioned before and softmax term\ne \u03c9/T\u2211N\ni=1 e \u03c9i/T\nin optimization. To minimize the loss, the soft-\nmax term causes the network tends to give larger attention \u03c9 to samples with smaller triplet loss term. These samples\nusually have large ||x||\u2212 and small ||x||+, so they are more suitable for matching. To minimize the total loss, these two conditions need to be met together, meaning that consistent attention has properties i) and ii) as shown in Fig. 3.\nThird, we will explore the role of attention score \u03c9 in the optimization of descriptor d by analyzing the gradient. Given weighted descriptor x = \u03c9 \u00b7 d and positive sample x+, we can get the gradient of the positive distance to the descriptor d by partial derivative as following:\n\u2202||x\u2212 x+||2 \u2202d = \u03c9 \u00b7 x\u2212 x + ||x\u2212 x+||2 . (5)\nIn Eq. 5, it is obvious that the gradient of descriptors is weighted by the attention score. Moreover, as shown in Eq. 3, the gradient of d is also weighted by the softmax term in the whole loss. Note that softmax term is also proportional to w, so in summary, the sample with high attention score will contribute more gradients relatively (i.e., property iii)). Obviously, not every sample\u2019s descriptor is worthy of equal optimization. Forcing learning descriptions on pixels that are not suitable as descriptors (e.g. sky, grass and waves) will bring noise and lead to sub-optimal results. Therefore, the local descriptor can be optimized more flexibly and selectively with the help of consistent attention weighting. (2) Consistent Attention in Matching: Interestingly, we find that attention-weighted local descriptors are also more suitable for matching. As shown in Fig. 3, the corresponding regions have similar consistent attention scores in different images, so consistent attention can be used as a prior information in the local descriptors matching. It is evident that regions with high attention scores are more likely to be successfully matched with the same high scores regions in another image. Thus, the weighted descriptor has a smaller matching space and lead to higher matching accuracy."
        },
        {
            "heading": "Local Features Detection with Feature Pyramid",
            "text": "We adopt pixel-wise classification to train keypoints detector with pseudo-keypoints supervision. Different from SuperPoint, we recommend predicting keypoints at different scale spaces by proposed Local Features Detection with Feature Pyramid. Specifically, we set four detection headers to predict the keypoint heatmaps respectively as shown in Fig 5. In order to combine the predict results, we interpolate the predicted heatmaps of different scales to the image size h\u00d7 w. We set four learnable weights to fuse heatmaps of different scales to predict final keypoints and calculate the loss. Each detection header can receive direct supervision of detector loss, which can be considered as deep supervision (Lee et al.\n2015). Our method also conforms to the famous scale-space theory (Lowe 2004a) which is accepted by many methods (Barroso-Laguna et al. 2019; Luo et al. 2020), with the difference from them that we directly predict keypoints through supervised learning without additional statistics and calculations. We use the weighted binary cross-entropy loss as the detector loss since there is an extreme imbalance in the number of keypoints and non-keypoints. Given the predicted keypoints heatmap K \u2208 Rh\u00d7w and pseudo-ground truth label G \u2208 Rh\u00d7w, the detector loss is defined as:\nLbce(k, g) = \u2212\u03bbglog(k)\u2212 (1\u2212 g)log(1\u2212 k), (6)\nLdet = 1\nhw h,w\u2211 u,v Lbce(Ku,v, Gu,v) (7)\nwhere the weight \u03bb is empirically set to 200."
        },
        {
            "heading": "Training Strategy and Implementation Details",
            "text": "Data Preparation: We use MegaDepth (Li and Snavely 2018) to generate the training data with dense pixel-wise correspondences. MegaDepth dataset contains image pairs with known pose and depth information from 196 different scenes. Following the settings in D2-Net, we take 118 scenes from all scenes as the training set. We randomly select 100 image pairs from each scene, and intercept 400 \u00d7 400 image blocks from the original images for the training. Thus, we get 11, 800 image pairs with dense pixel correspondence. This part of the data contains real complex transformations, which are difficult to collect but closer to practical applications. Besides, we use random homography to synthesize more diverse image pairs to supplement the training data inspired by SuperPoint, further enriching the whole transformation types. In summary, our training data consists of 23, 600 image pairs in total. We compare our dataset settings with advanced methods in the appendix. Keypoints Supervision with Distillation: We employ distillation to get the pseudo-keypoints ground truth directly from an off-the-shelf trained SuperPoint (teacher model). To obtain reliable and more pseudo-labeling of keypoints, we use iterative homographic adaptation to obtain the probability map of keypoints heatmap. We refer the readers to the work (DeTone, Malisiewicz, and Rabinovich 2018) for more details. Correspondences Supervision with Keypoints Heatmap Guidance: Obviously, not all locations are equally important. Forcing the network to train the descriptors in meaningless areas may lead to impaired performance. To get enough keypoints-specific and distributed diversely correspondences, we design a novel keypoints guided correspondences sampling method. Specifically, for each image pair I1, I2: 1) Predict keypoints heatmaps M1,M2 with a trained SuperPoint model. 2) Synthesize M \u20321 from M2 based on the transformation between the image pair I1, I2. 3) Generate a compound keypoints heatmap by M = M1 +M \u20321 and divide this heatmap M into 40 \u00d7 40 grids. 4) Take the point with the largest score of each grid on M to obtain a candidate point set Q. 5) Apply the non-maximum suppression (NMS) to Q and select the top 400 points to construct the refined descriptor correspondences P .\nImplementations: To optimize the keypoints detection and description jointly, the total loss is composed of detector loss Ldet and descriptor loss Ldes, which is formulated as:\nLtotal = Ldet + Ldes. (8) The Adam optimizer with poly learning rate policy is used\nto optimize the network, and the learning rate decays from 0.001. The training image size is set to 400 \u00d7 400 with the training batch size 12. The whole training process typically converges in 30 epochs and takes about 14 hours with a single NVIDIA Titan V GPU. During the testing, the detection threshold \u03b1 is empirically set to 0.9 and non-maximum suppression (NMS) radius to 4 to balance the keypoints\u2019 number and reliability. Our method implemented by Pytorch1 runs at 24 FPS (real time) on 480 \u00d7 640 images with a single NVIDIA Titan V GPU."
        },
        {
            "heading": "Experiments",
            "text": ""
        },
        {
            "heading": "Comparisons on Image Matching",
            "text": "Dataset and Metrics: We use the popular HPatches benchmark (Balntas et al. 2017) for ablation studies and comparisons. Following previous methods, we use 108 sequences with viewpoint or illumination variations after excluding high-resolution sequences from 116 available sequences. The entire benchmark includes 56 sequences with changes in viewpoint and 52 sequences with changes in illumination. We use three standard metrics for evaluation: 1) Mean matching accuracy (MMA) is the average percentage of correct matches in image pairs under different matching error thresholds. 2) Match score (M.S.) is the ratio of the correct match to the total number of keypoints estimated in the shared view, following the definition in (Revaud et al. 2019). 3) Accuracy of homography (HA) ( (DeTone, Malisiewicz, and Rabinovich 2018)) is used to compare the estimated homography of image pairs with its corresponding ground truth. Comprehensive Ablation Studies: Ablation studies are reported in Tab. 1. Our proposed data construction method is used to re-implement the SuperPoint named our impl. as a stronger baseline with higher MMA and M.S. scores compared to the SuperPoint orig. After applying proposed FP Keypoints, all metrics have been significantly improved, due to more accurate keypoints localization. Almost all metrics get steady improvement after incrementally applying AGCA and DLCA. Note that the Gated Map in AGCA can further improve the performance, especially for M.S. score. In addition, we also compared some related operations that can implicitly improve context awareness including DCN in (Luo et al. 2020), Hypercolumns in (Zhang et al. 2020), UNetlike backbone in (Tyszkiewicz, Fua, and Trulls 2020) and ASPP in (Chen et al. 2017). Our proposed Context Augmentation is superior to these alternative context augmentation operations. Furthermore, all metrics still have a significant improvement with only using consistent attention in optimization (CA in Optimization) and not using consistent attention in matching. This indicates that our proposed Consistent Attention Weighted Triplet Loss can inde-\n1We also implemented our method by using Mindspore (https://www.mindspore.cn/) and observed similar performance.\npendently improve the performance of the descriptors substantially. Different from the above settings, the consistent attention weighted descriptors (CA in Matching) are also used for matching to obtain an MMA score of 78.66. Our MTLDesc significantly exceeds the current state-of-the-art DISK (Tyszkiewicz, Fua, and Trulls 2020) on all metrics.\nImpact of Smoothing factor T: The T is used to adjust the effect of consistent attention on the loss function. When T becomes larger, the weight e\n\u03c9/T\u2211N i=1 e \u03c9i/T in Eq. 3 is smoothed\nand the weights of different samples are closer. On the contrary, the weights will be concentrated on some better samples (easier to optimize) with a smaller T . It will cause the network to only optimize better samples and the optimization of other normal samples is undermined. In the evaluation, these non-optimized normal samples lead to fewer possible matches (including correct and incorrect matches) and more correct matches in possible matches. The increase in the proportion of correct matches in possible matches will lead to a higher MMA. However, the reduced number of possible matches also contains some correct matches, which leads to a lower M.S. score. To obtain both accurate and dense local features matching results, it is necessary to balance M.S. and MMA by adjusting the value of parameter T .\nAs observed in Fig. 6, the best balance between M.S. and MMA is achieved when T is set to 15. Comparisons with Advanced Local Descriptors: In Fig. 7, we compare our MTLDesc with advanced local descriptors (Ono et al. 2018; Luo et al. 2019; Noh et al. 2017; DeTone, Malisiewicz, and Rabinovich 2018; Dusmanu et al. 2019; Revaud et al. 2019; Luo et al. 2020; Zhang et al. 2020; Germain, Bourmaud, and Lepetit 2020; Wang et al. 2020b; Tyszkiewicz, Fua, and Trulls 2020) on HPatches benchmark. All methods use the optimal configuration and results reported in their papers, while MTLDesc notably outperforms these methods in all thresholds under overall MMA. Although the performance of the recent DISK is closest to our method, we should note that when DISK uses 2 K keypoints (equivalent to our 1.5 K), the performance is much lower than our method. It is worth mentioning that DISK does not exceed our method even if using unfair 8 K keypoints."
        },
        {
            "heading": "Comparisons on Visual Localization",
            "text": "Dataset and Evaluation: We resort to Aachen Day-Night visual localization v1.1 (Sattler et al. 2012) and InLoc indoor visual localization (Taira et al. 2018) benchmarks to further demonstrate the effectiveness of our MTLDesc. For a fair comparison, all methods use the same image matching pairs provided by the benchmarks and the same evaluation pipelines except for local features. The number of maximum local features of all methods is limited to 20 K as reported in the previous methods. For Aachen, our evaluation is performed via a localization pipeline based on COLMAP (Schonberger and Frahm 2016). For InLoc, we evaluate all methods based on HLOC (Sarlin et al. 2019). See supplementary material for more details. Results Analysis: The comparison results are shown in Tab 2. Our MTLDesc evidently outperforms other local descriptors under the tolerance (0.25m, 2\u25e6 and 0.5m, 5\u25e6) and achieves competitive performance under the tolerance (5m, 10\u25e6 or 1m, 10\u25e6) on both Aachen outdoor and InLoc indoor benchmarks, validating the effectiveness of our MTLDesc for visual localization task, especially under highprecision requirements. Our MTLDesc only deploys a relatively small training dataset (1/40 of ASLFeat and does not include any indoor scenes data) and a short descriptor repre-\nAachen Day-Night v1.1 Benchmark\nMethod Dim Features Correctly localized queries 0.25m,2\u25e6 0.5m,5\u25e6 5m,10\u25e6\nROOT-SIFT 128 11 K 53.4 62.3 72.3 DSP-SIFT 128 11 K 40.3 47.6 51.3 SuperPoint 256 7 K 68.1 85.9 94.8\nD2Net 512 14 K 67.0 86.4 97.4 R2D2 128 10 K 70.7 85.3 96.9 ASLFeat 128 10 K 71.2 85.9 96.9 CAPS + SuperPoint 256 7 K 71.2 86.4 97.9\nsentation (128 vs 256, 512) to achieve the state-of-the-art performance, revealing the great potential for further improvement."
        },
        {
            "heading": "Conclusion",
            "text": "In this work, we propose a novel method named MTLDesc to cope with the local features detection and description simultaneously. In order to make our descriptor \u201clook wider to describe better\u201d, the Context Augmentation and Consistent Attention Weighting is designed to give descriptors a context awareness beyond the local region, while the Local Features Detection with Feature Pyramid is presented to obtain accurate and reliable keypoints localization. We have conducted thorough experiments on the standard HPatches, Aachen and InLoc benchmark, and validate our MTLDesc can achieve state-of-the-art performance among local descriptors."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China (Nos. 61620106003, 61971418, U2003109, 62171321, 62071157, 62162044 and 61771026) and in part by the Open Research Fund of Key Laboratory of Space Utilization, Chinese Academy of Sciences (No. LSU-KFJJ-2021-05), Open Research Projects of Zhejiang Lab (No. 2021KE0AB07), and this work was partially sponsored by CAAI-Huawei MindSpore Open Fund."
        }
    ],
    "title": "MTLDesc: Looking Wider to Describe Better",
    "year": 2022
}