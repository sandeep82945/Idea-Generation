{
    "abstractText": "Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models\u2019 performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. These factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-totext tasks (e.g., summarization), it is less studied in the context of vision-based text generation. In this work, we conduct a detailed human evaluation of the factuality in video captioning and collect two annotated factuality datasets. We find that 57.0% of the model-generated sentences have factual errors, indicating it is a severe problem in this field. However, existing evaluation metrics are mainly based on ngram matching and show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning. The datasets and metrics will be released to promote future research for video captioning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hui Liu"
        },
        {
            "affiliations": [],
            "name": "Xiaojun Wan"
        }
    ],
    "id": "SP:07d263576b5a8d2c027072b17926b8d491d50003",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
            "year": 2005
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "venue": "arXiv preprint arXiv:2302.04023,",
            "year": 2023
        },
        {
            "authors": [
                "Ashwin Devaraj",
                "William Sheffield",
                "Byron C Wallace",
                "Junyi Jessy Li"
            ],
            "title": "Evaluating factuality in text simplification",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Esin Durmus",
                "He He",
                "Mona Diab"
            ],
            "title": "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo FR Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych"
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Simon Ging",
                "Mohammadreza Zolfaghari",
                "Hamed Pirsiavash",
                "Thomas Brox"
            ],
            "title": "Coot: Cooperative hierarchical transformer for video-text representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tejas Gokhale",
                "Abhishek Chaudhary",
                "Pratyay Banerjee",
                "Chitta Baral",
                "Yezhou Yang"
            ],
            "title": "Semantically distributed robust optimization for vision-and-language inference. In Findings of the Association for Computational Linguistics: ACL",
            "year": 2022
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Clipscore: A reference-free evaluation metric for image captioning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend"
            ],
            "title": "Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alphareliability",
            "year": 2011
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Dense-captioning events in videos",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jie Lei",
                "Liwei Wang",
                "Yelong Shen",
                "Dong Yu",
                "Tamara Berg",
                "Mohit Bansal"
            ],
            "title": "Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Botian Shi",
                "Haoyang Huang",
                "Nan Duan",
                "Tianrui Li",
                "Jason Li",
                "Taroon Bharti",
                "Ming Zhou"
            ],
            "title": "Univl: A unified video and language pre-training model for multimodal understanding and generation",
            "venue": "arXiv preprint arXiv:2002.06353,",
            "year": 2002
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald"
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata"
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov"
            ],
            "title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Boxiao Pan",
                "Haoye Cai",
                "De-An Huang",
                "Kuan-Hui Lee",
                "Adrien Gaidon",
                "Ehsan Adeli",
                "Juan Carlos Niebles"
            ],
            "title": "Spatio-temporal graph for video captioning with knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "title": "Object hallucination in image captioning",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Yaya Shi",
                "Xu Yang",
                "Haiyang Xu",
                "Chunfeng Yuan",
                "Bing Li",
                "Weiming Hu",
                "Zheng-Jun Zha"
            ],
            "title": "Emscore: Evaluating video captioning via coarse-grained and fine-grained embedding matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuqing Song",
                "Shizhe Chen",
                "Qin Jin"
            ],
            "title": "Towards diverse paragraph captioning for untrimmed videos",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Subhashini Venugopalan",
                "Marcus Rohrbach",
                "Jeffrey Donahue",
                "Raymond Mooney",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "title": "Sequence to sequence-video to text",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis"
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang"
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Teng Wang",
                "Ruimao Zhang",
                "Zhichao Lu",
                "Feng Zheng",
                "Ran Cheng",
                "Ping Luo"
            ],
            "title": "End-to-end dense video captioning with parallel decoding",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao"
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Luowei Zhou",
                "Chenliang Xu",
                "Jason J Corso"
            ],
            "title": "Towards automatic learning of procedures from web instructional videos",
            "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Video captioning is a challenging cross-modal task that aims to describe videos with natural language sentences. With the rapid development of social media platforms such as Youtube and Tiktok, video plays a more important role in our daily life. Video captioning has received much attention in computer vision and natural language processing communities. Substantial progress has been made to generate descriptions for videos that contain a single event [31, 23] or multiple events [38, 15]. However, like other text generation tasks, video captioning models risk introducing factual errors not supported by the input video. Examples are shown in Table 1. This paper defines factual errors (or hallucinations) as follows: a span of caption text that contradicts the video or describes something not appearing in the video.\nA caption with factual errors can cause misunderstandings of the video content. Sometimes, factual errors make the generated captions completely unusable.\nFactual consistency evaluation has received much research attention in text-to-text tasks, including summarization [19, 14], knowledge-grounded dialogue [10], text simplification [4], and large language models[3]. Nevertheless, it is less studied in vision-to-text tasks, especially video captioning. Therefore, this work focuses on the research gap in the factuality evaluation of video captioning.\nRecently, more works have focused on videos with multiple events [29, 34], and it may bring more factual errors. So we choose multiple events video captioning for our eval-\nar X\niv :2\n30 3.\n02 96\n1v 1\n[ cs\n.C V\n] 6\nM ar\n2 02\nuation. We use ActivityNet Captions [13] and YouCook2 [37] as our video datasets, for they are the most common datasets for this task. Then we carefully select five recent models on each dataset to generate video captions. The models differ in model framework, pretrained features, and input signals. After collecting the videos and captions, we design a factuality annotation protocol and conduct our human annotation. In the end, we obtain two human-annotated factuality datasets ActivityNet-Fact (200 videos, 3,152 sentences) and YouCook2-Fact (100 videos, 3,400 sentences).\nAfter analyzing the human annotation, we find that factual error (hallucination) is a severe problem in video captioning. To sum up, there are 87.8% of the paragraphs, 57.0% of the sentences, and 15.2% of the words have factual errors. There are different types of factual errors, including person-related errors, action-related errors, objectrelated errors and so on.\nSince hallucination is a severe problem in video captioning, we test to what extent existing automatic evaluation metrics can measure the factuality of video captions. We test commonly used video caption metrics like BLEU[24], ROUGE[16], METEOR[2], CIDEr[30], and model-based metrics BERTScore[36] and EMScore[28]. We find that most existing metrics correlate poorly with human judgment.\nWe try to find a better metric to evaluate the factuality in video captioning. Inspired by EMScore[28], we also leverage the CLIP model to encode video frames and captions. Considering the CLIP model is trained on image-text pairs, it may have a gap transferring to video factuality evaluation. So we automatically construct a training set using text augmentation skills and finetune CLIP on it. Our new metric FactVC (Factual consistency for Video Captioning) outperforms all previous metrics, achieving a higher correlation with human factuality annotation.\nThe main contributions of this work are as follows:\n\u2022 We conduct the first thorough factuality evaluation on video captioning. We find that hallucination is a severe problem in this field while existing evaluation metrics can hardly measure it.\n\u2022 We design a factuality annotation protocol and collect two human-annotated factuality datasets for video captioning.\n\u2022 We propose a new factuality metric FactVC, which achieves a much higher correlation with human annotation on video captioning, and it can be further transferred to evaluate the factuality of image captioning.\n\u2022 All the datasets, metrics, and code will be released for further research."
        },
        {
            "heading": "2. Related Work",
            "text": "Factuality evaluation is first proposed in the field of document summarization. Maynez et al. [19] conducted a human annotation on the XSUM dataset [21] and found that more than 70% of summaries generated by summarization models have factual errors. Other human annotations[32, 22] reach similar conclusions. To measure the factual consistency, researchers proposed different metrics, which can be roughly divided into Entailment-based metrics [6, 14] and QA-based metrics [5, 32]. Inspired by the works in summarization, factuality evaluation is studied for other tasks, including knowledge-grounded dialogue [10], text simplification [4] and large language models [3].\nFor vision-based text generation tasks, the most widely used metrics are based on n-gram matching between reference captions and generated captions, including BLEU[24], ROUGE[16], METEOR[2], and CIDEr[30]. They are sensitive to lexical variation and cannot match deeper semantics between captions and vision inputs. Recently, there are model-based metrics such as BERTScore[36], CLIPScore[9], EMScore[28]. They leverage large-scale pretrained models to compute a matching score, even without requiring reference captions.\nLittle work pays attention to the hallucination problem in vision-based text generation. CHAIR [27] proposes an image relevance metric to evaluate the object hallucination in image captioning. However, they restrict their evaluation to 80 MSCOCO objects and a single type of hallucination. EMScore[28] is designed for the overall evaluation of video captioning, and it also shows the potential to identify hallucinating captions. However, it is tested on a non-real dataset and is not specifically designed for factuality evaluation."
        },
        {
            "heading": "3. Human Annotation",
            "text": "Considering there does not exist factuality annotation of video captioning, we decide to construct our own datasets. We use ActivityNet Captions [13] and YouCook2 [37] as our source video datasets and select five recent models for each dataset to generate captions. Then we design a factuality annotation protocol and conduct our human annotation."
        },
        {
            "heading": "3.1. Datasets",
            "text": "ActivityNet Captions[13] contains 20k untrimmed videos of various human activities. On average, each video lasts 120s and has 3.65 human-annotated sentences. Previous works[15, 7, 29] report results on the ae-test split (2,457 videos). In this work, we randomly sample 200 videos from the ae-test split for human annotation. YouCook2 [37] contains 2,000 long untrimmed videos from 89 cooking recipes. On average, each video lasts 320s and has 7.70 humanannotated sentences. Previous works[15, 7, 18] report results on the val split (457 videos). We randomly sample\n100 videos from the val split for human annotation."
        },
        {
            "heading": "3.2. Captioning Models",
            "text": "We select five recent captioning models for each dataset and obtain the output captions on the sampled videos. For ActivityNet Captions, the selected models include: MART [15]: a recurrent transformer model that uses a memory module to augment the transformer architecture; COOT [7]: it first hierarchically trains video-text representation on the ActivityNet dataset, and then uses the representation to train MART model; PDVC-gt [34]: it uses parallel decoding to generate dense video captioning. The above three models need the human-annotated event timestamps to generate each sentence; PDVC-pred [34]: similar to PDVCgt, but it uses predicted event timestamps to generate sentences; Song [29]: it eschews the event detection stage and directly generates paragraphs for untrimmed videos. For YouCook2, the selected models include: VTrans[38]: a transformer architecture for video captioning; MART[15]; COOT[7]; COOT-100m[7]: it uses the pretrained features on the large-scale Howto100M dataset [20] to train COOT model; UniVL[18]: a unified video and language pretraining model pretrained on Howto100M dataset. The above models are different in model framework, input signals, pretrained features, and pretraining scales, as shown in Table 2."
        },
        {
            "heading": "3.3. Annotation Protocol",
            "text": "Before conducting human annotation, we design an annotation protocol to instruct annotators on how to measure and label the factuality of video captions. For the factuality annotation in summarization, annotators often give a binary label 0/1 for each summary sentence, indicating whether the sentence is factual or not [19, 32, 14]. However, the video\ncaptions have hierarchical structures (paragraph-sentenceword), and we want to obtain the factuality annotation for different granularity.\nSo we design a new annotation protocol. Annotators are asked to give three levels of factuality annotation for each video caption. Paragraph-level: For each paragraph, annotators need to give a factuality Likert scale from 1 to 5, where 1 means the paragraph has many severe factual errors, and 5 means there are no obvious factual errors; Sentence-level: For each sentence, annotators give a label 1 if it has factual errors else label 0; Word-level: Within each sentence, annotators need to mark phrases and words that have factual errors. Another critical issue is that we focus on whether the caption has factual errors given the video (Precision) and do not care whether the caption describes the video completely (Recall). Please refer to Appendix A for the complete annotation protocol and examples."
        },
        {
            "heading": "3.4. Annotation Procedure",
            "text": "According to previous works [14, 32], the inter-annotator agreement through the crowdsourcing platform is relatively low. To make our annotation more reliable, we hire three graduate students as our annotators. We provide them with a detailed instruction document and several annotation examples so that they can fully understand the annotation protocol. The annotations are checked multiple times during the annotation process. If there are any problems, we give suggestions to help annotators improve the annotations. The annotations will be adopted only when an annotator completes all videos and passes every check, ensuring the annotations\u2019 quality and consistency. We collect three annotations for each video caption and combine them to get the final annotation. For paragraph-level annotation, if there exists a majority score, we use it as the final score; otherwise, we use the median score. For sentence-level and word-level annotation, we use the majority label as the final label.\nWe quantified the degree of inter-annotator agreement using Krippendorff\u2019s alpha coefficient [12]. On the ActivityNet dataset, the inter-annotator interval metrics are 0.741, 0.647, and 0.564 for paragraph-level, sentencelevel, and word-level annotations respectively. On the YouCook2 dataset, the inter-annotator interval metrics are 0.783, 0.766, 0.688 for paragraph-level, sentence-level, and word-level annotations respectively. The metrics show a substantial agreement between annotators. The agreement for word-level annotation is relatively low because it has more uncertainty and ambiguity."
        },
        {
            "heading": "4. Annotation Analysis",
            "text": ""
        },
        {
            "heading": "4.1. Datasets Statistics",
            "text": "Based on sampled ActivityNet and YouCook2 videos, we collect two annotated factuality datasets ActivityNet-\nFact and YouCook2-Fact. The ActivityNet-Fact dataset contains 1,000 paragraphs, 3,152 sentences, and 40,461 words, among which 82.7% of the paragraphs, 52.7% of the sentences, and 14.0% of the words have factual errors. The YouCook2-Fact dataset contains 500 paragraphs, 3,400 sentences, and 24,903 words, among which 98% of the paragraphs, 60.9% of the sentences, and 17.1% of the words have factual errors. This indicates that factual error is a severe problem in video captioning and should attract more research attention."
        },
        {
            "heading": "4.2. Factual Error Type Analysis",
            "text": "FRANK[22] propose a typology of factual errors towards summarization and annotate their data with specific factual errors. Although we do not annotate the factual error type during human annotation, we conduct a post-analysis with our annotated datasets. We collect phrases/words marked as factual errors appearing at least twice and classify them into different error categories. The results are shown in Table 3. We can see that the factual errors in video captioning are various. For ActivityNet-Fact, the most common factual error categories are Person, Action,\nand Object, which count for 83.5% of the total factual errors. For YouCook2-Fact, Object is the dominant category, which counts for 92.7% of the total factual errors."
        },
        {
            "heading": "4.3. Model Evaluation",
            "text": "We evaluate the selected models using automatic metrics and our factuality annotation. The results are shown in Table 4. On the ActivityNet-Fact dataset, the PDVC-gt model performs best on both automatic metrics (except for RougeL) and factuality annotation. On YouCook2-Fact dataset, the UniVL model performs best. Combing the model characteristics in Table 2, we can draw several conclusions: 1) Pretraining (especially large-scale pretraining) can improve both the automatic metrics and factuality (compare MART vs. COOT, they use the same model framework but with different features). 2) The timestamps information is helpful in improving automatic metrics but less helpful in improving factuality (compare PDVC-gt with PDVC-pred, PDVC-gt use annotated event timestamps). 3) Model framework is the most important factor for factuality. In addition, Table 4 shows that the automatic metrics are not always consistent with factuality annotation."
        },
        {
            "heading": "5. Metric Analysis",
            "text": "Now that factual errors broadly exist in video captions, we want to know to what extent existing metrics can measure the factuality of video captions. We test the correlation between automatic metrics and human annotation (for sentence/word-level annotation, we use the ratio of factual sentences and words as annotation score). We test model-free metrics BLEU[24], ROUGE[16], METEOR[2], CIDEr[30], and model-based metrics BERTScore[36] and recently proposed EMScore[28] 1. The results are shown\n1For EMScore, we use the ViT-B/16 CLIP model, which performs bet-\nin Table 5 and Table 6. To our surprise, the most commonly used metrics for video captioning, such as Bleu4 and CIDEr, correlate poorly with factuality annotation. METEOR and Bleu2 perform relatively better but still show a weak correlation with factuality annotation. As for modelbased metrics, BERTScore shows little superior to METEOR on two datasets, indicating that just introducing large-scale text-pretrained model is not enough. The recently proposed EMScore, which leverages the image-text pretrained model CLIP[25], shows a higher correlation with human annotation. In addition, it can evaluate video captions using the input videos, with no need for human-written\nter than the default ViT-B/32 CLIP model.\ncaptions. We attribute the advantage of EMScore to largescale cross-modal pretraining."
        },
        {
            "heading": "6. FactVC Metric",
            "text": "Although EMScore achieves a good correlation with human factuality annotation, it has two drawbacks: 1) EMScore uses the pretrained model CLIP, which is trained on image-text pairs from the Internet, and it may not transfer well to the video captioning data; 2) EMScore is designed for evaluating the overall quality of the video caption, not specifically designed for factuality evaluation. As a result, we propose a new metric FactVC (Factual consistency for Video Captioning). We first automatically construct a factuality training set using text augmentation skills and then use it to finetune the CLIP model. We also improve the calculation of the similarity score so that it is more suitable for factuality evaluation."
        },
        {
            "heading": "6.1. Training Data",
            "text": "Collecting a large-scale training dataset through human annotation is expensive and time-consuming. Inspired by [14, 8], we decide to construct our training set automatically using text augmentation skills. Given a video V together with a human-annotated caption sentence T , we use a set of text transformation functions to augment the dataset. The transformation functions include positive transformations (T +) which ensure the new sentence is factually correct and negative transformations (T \u2212) which introduce factual errors into the sentence.\nThe positive transformations include: 1)Paraphrasing: we generate paraphrases using the back-translation method. We use the Google Translation API 2 and use German and French as middle language; 2) Simplification: we use a tool 3 to simplify complex and compound sentences into simple sentences.\nThe negative transformations include: 1) Person Swap: we design a set of rules to change person words\u2019 gender, age, and pronoun; 2) Action Swap: we collect a common action set and apply deletion and insertion; 3) Object Swap: we collect a common object set and apply object substitution; 4) Adjective Swap: we swap adjectives (color, numerical words, etc.) in original sentences; 5) Poor Generation: we simulate the poor generation sentences by inserting \u201dUNK\u201d word and redundancy phrases. We design the negative transformations according to the factual errors shown in Table 3.\nWe first apply positive transformations to obtain positive sentences and then apply negative transformations to them to get negative sentences. Finally, we collect a set of data samples (V, T+, T\u2212), where V means the input video, T+\n2https://translate.google.com/ 3https://github.com/garain/Sentence-Simplification\nmeans a fact-consistent sentence, and T\u2212 means the corresponding fact-inconsistent sentence. A detailed description of the data generation process is in Appendix B."
        },
        {
            "heading": "6.2. CLIP Finetuning",
            "text": "We only finetune the projection layers of the pretrained CLIP model. Given a batch of data {(Vi, T+i , T \u2212 i )}Bi=1 with a batch size of B, we first use the CLIP model to calculate the similarities between each video and text:\ns+i,j = cos  1 |Vi| |Vi|\u2211 k=1 Ev(fik), Et(T + j )  (1) where s+i,j means the similarity score between Vi and T + j , cos means cosine similarity, fik is the k-th frame of video Vi, |Vi| is the sampled frame number, Ev and Et are the vision encoder and text encoder of CLIP model. The similarity score s\u2212i,j between Vi and T \u2212 i is computed similarly.\nThen we finetune the CLIP model using the following loss function:\nLcoarse = \u2212 B\u2211 i=1 exp(s+i,i)\u2211B j=1(exp(s + i,j))\n(2)\nLfine = B\u2211 i=1 max(0,M \u2212 s+i,i + s \u2212 i,i) (3)\nL = Lcoarse + \u03bbLfine (4)\nwhere Lcoarse is a cross-entropy loss to learn whether the video content and text are matched, Lfine is a hinge loss to learn to assign a higher score to the fact-consistent text. M and \u03bb are hyper-parameters."
        },
        {
            "heading": "6.3. Score Calculation",
            "text": "With the finetuned CLIP model, we can calculate the factuality score FactVC as follows:\nFactV C(T, V ) = (1\u2212 \u03b1)S(T, V )c + \u03b1S(T, V )pf (5)\nS(T, V )c = cos( 1\n|V | |V |\u2211 k=1 Ev(fk), Et(T )) (6)\nS(T, V )pf = 1 |T | \u2211 xj\u2208T max fi\u2208V cos(Ev(fi), Et(xj)) (7)\nwhere \u03b1 is a balance factor, S(T, V )c is the coarse-grained similarity score between video V and sentence T . S(T, V )pf is the precision-based fine-grained similarity score computed between each frame f and each word x. Please refer to [28] for more technical details. Similar to EMScore, FactVC can use video V , human-written caption T \u2217, or both (V, T \u2217) as reference.\nWe improve the calculation of FactVC towards EMScore in two aspects: 1) we use the precision-based score instead\nof the F-value-based score, for factuality is more related to the precision of video captions; 2) we introduce a parameter \u03b1 to balance the coarse-grained score and the fine-grained score, we set it to 0.75 to favor more on fine-grained score."
        },
        {
            "heading": "7. Experiments",
            "text": ""
        },
        {
            "heading": "7.1. Comparison with Other Metrics",
            "text": "We compare FactVC metric to other automatic metrics including Bleu[24], ROUGE[16], CIDEr[30], METEOR[2], BERTScore[36], Entailment[14]4, EMScore[28].\nThe results are shown in Table 7. We omit the metrics with worse correlation here (Bleu, ROUGE, CIDEr), and\n4Similar to FactCC[14], we use our training set to train an entailment model, and use the entailment probability as the factual score.\nyou can check them in Tables 5 and 6. From the table, EMScore performs better than METEOR, BERTScore, and the Entailment-based metric, indicating the usefulness of pretrained model CLIP. However, EMScore performs relatively poorly using video as the reference. Our FactVC metric, on the other hand, shows a much better performance in this setting. Compared to other metrics, FactVC shows the highest correlation with human annotation in all settings."
        },
        {
            "heading": "7.2. Ablation Study",
            "text": "We conduct an ablation study to test the effectiveness of each component in FactVC. The results are shown in Table 8. FactVC(no finetune) removes the finetuning process and shows an obvious performance degradation. FactVC(Lcoarse) only uses Lcoarse to finetune CLIP and FactVC(Lfine) only uses Lfine to finetune CLIP. From the\nresults, we find that Lcoarse can ensure stable finetuning, only using Lfine is not a good choice, but it can help video encoding together with Lcoarse. FactVC(F-value) uses the F-value-based score instead of the precision-based score, showing a performance degradation. This proves that factual consistency is more related to the precision of video captions. FactVC(\u03b1 = 0.5) sets \u03b1 to 0.5 in eq (5) and it is inferior to FactVC with \u03b1 = 0.75. This shows that the finegrained score is more important in Factuality evaluation."
        },
        {
            "heading": "7.3. Experiments on ActivityNet-FOIL",
            "text": "EMScore[28] introduced the ActivityNet-FOIL dataset by injecting foil visual concepts into the original captions from ActivityNet Captions ae-test split. It contains 1,900 correct-foil paragraph pairs, and at least one sentence in the foil paragraph contains a foil visual concept. This experiment uses different metrics to evaluate the correct-foil paragraph pairs and compute the pairwise ranking accuracy. The results are shown in Table 9. We can see that when just using video as the reference, FactVC is better than previous metrics. When using both video and text as the reference, FactVC achieves the highest accuracy of 94.3%."
        },
        {
            "heading": "7.4. Cross-Dataset Experiments",
            "text": "We conduct a cross-dataset experiment to test the generalizability of the FactVC metric. We use different datasets to finetune CLIP model and test them on ActivityNet-Fact and YouCook2-Fact datasets. The results are shown in Table 10. Compared to the CLIP model without finetuning, our finetuned method can obviously improve the metric performance. Even training on a different dataset, the FactVC metric still performs well. Considering the huge domain gap between ActivityNet (ANet) and YouCook2 (You2) datasets, our FactVC metric has good generalizability on different video categories and textual styles.\nFor more implementation details, experiments and qualitative analysis, please refer to Appendix C and D."
        },
        {
            "heading": "7.5. Transferring to Image Captioning",
            "text": "According to the above experiments, FactVC performs well in evaluating the factuality of video captioning. We want to know whether our method can transfer to image captioning. So we additionally collect an annotated factuality dataset MSCOCO-Fact based on 200 MSCOCO[17] test images and five recent image captioning models\u2019 outputs. Unlike video captioning where a caption is a multisentence paragraph, an image caption is a single sentence. We collect three kinds of factuality annotation for each image caption: Likert (1-5 factuality score), Binary (0 or 1,\nindicating whether the sentence has a factual error), Word (whether each word has a factual error, and we use the ratio of factual words as word-level annotation score).\nWe test the correlation between image captioning metrics and human factuality annotation. Results are shown in Table 11. CLIPScore and RefCLIPScore[9] are CLIPbased metrics for image captioning. CLIPScore\u2217 and RefCLIPScore\u2217 use our video-finetuned CLIP model. We further construct a training set from image caption data using the same text augmentation skills, and finetune the CLIP model to get CLIPScore\u2217\u2217 and RefCLIPScore\u2217\u2217. From Table 11, we can see that with our finetuned method, CLIPScore and RefCLIPScore can better measure the factuality of image captions. More details of the MSCOCO-Fact dataset are in Appendix E."
        },
        {
            "heading": "8. Conclusion",
            "text": "In this work, we focus on the factuality evaluation in video captioning. We first collect two human-annotated factuality datasets for video captioning and find that hallucination is a severe problem in video captioning, with 57.0% of the model-generated sentences having different kinds of factual errors. However, most existing metrics show little correlation with human annotation. So we propose a new factuality metric FactVC. It is trained on an automaticallyconstructed training set and correlates much better with the factuality annotation. Experiments also show the potential of our method in evaluating image captions. Although factual consistency is a hot research topic in text-to-text tasks, it is less studied in video captioning. We hope our work can fill this research gap and promote further research in video captioning."
        },
        {
            "heading": "A. Annotation Protocol and examples",
            "text": "The detailed annotation instructions and protocol we provided to the annotators are shown in Table 12. Two annotation examples are provided in Table 13.\nWe paid the annotators 12 dollars per hour, more than the local average minimum wage for human annotation. We checked that all content in the datasets contained no personal information about the annotators."
        },
        {
            "heading": "B. Training Dataset Generation",
            "text": "A detailed description of the data generation process is shown in Algorithm 1.\nAlgorithm 1 The algorithm to generate dataset Require: S - set of videos V and captions T T + - set of positive transformations T \u2212 - set of negative transformations\nfunction GENERATE DATA(S, T +, T \u2212) P \u2190 \u2205 . set of positive data for (V, T ) in S do P \u2190 P \u2229 {(V, T )} for fn in T + do\nT+ \u2190 fn(T ) P \u2190 P \u2229 {(V, T+)}\nend for end for D \u2190 \u2205 . set of data pairs for (V, T+) in P do\nfor fn in T \u2212 do T\u2212 \u2190 fn(T+) D \u2190 D \u2229 {(V, T+, T\u2212)}\nend for end for\nend function return D"
        },
        {
            "heading": "C. Experiments",
            "text": "C.1. Implementation details\nWe use the training split of ActivityNet Captions and YouCook2 to construct our training and validation set. The training and validation set size are 44,820 and 5,180 for ActivityNet and 18,029 and 1,971 for YouCook2. For CLIP finetuning, we start with the pretrained ViT-B/16 CLIP model. We sample three frames from each video clip uniformly. We set the margin M in Eq (3) to 5.0 and the loss weight \u03bb in Eq (4) to 0.1. We finetune the projection layers of the CLIP model for three epochs with a batch size of 256 and learning rate of 5e \u2212 5. During score calculation,\nwe set the balance factor \u03b1 in Eq (5) to 0.75, favoring finegrained scores more than coarse-grained ones. Regarding the complexity cost, we finetune CLIP 3 epochs, which cost 4-6 hours on a single 2080Ti GPU card. We will keep the above settings unless otherwise stated.\nC.2. Extra ablation test\nWe explore the impact of the \u03bb value in Eq (4). The results are shown in Table 14. Note that when \u03bb = 0, we only use Lcoarse to finetune the CLIP model. From the table, FactVC performs better when choosing a \u03bb between [0.1, 0.3]. It makes the CLIP model make use of both Lcoarse and Lfine.\nConsidering our FactVC is based on the image-text pretrained model CLIP[25], we want to explore the impact of using different CLIP models. The results are shown in Table 15. We test FactVC performance on ActivityNet-Fact with different CLIP models without finetuning in this experiment. The table shows that among ResNet-based CLIP models, RN50 and RN50x16 perform best; among ViTbased CLIP models, ViT-B/16 performs best. This leads us to the conclusion that a larger CLIP model does not necessarily perform better on factuality evaluation. As a result, we use the relatively small ViT-B/16 CLIP model in this work.\nC.3. Model Ranking\nEvaluation metrics are often reported at the system level to compare the performance of different models, and a reliable metric should be consistent with human judgment. We test the performance of the five models on the ActivityNetFact dataset using EMScore and FactVC and report the average scores. The results are shown in Table 16. All the metrics are scaled to [0, 1]. Compared to Sentence-level factuality annotation, EMScore ranks the COOT, PDVCgt, and PDVC-pred models differently. In contrast, FactVC ranks them consistently with human annotation."
        },
        {
            "heading": "D. FactVC Qualitative Analysis",
            "text": "We show several video captioning evaluation examples in Table 17 and 18. The annotation scores and metric scores are scaled in [0, 1] for comparison. In video 1, the generated caption has no obvious factual errors. BERTScore, EMScore, and FactVC assign relatively high scores, while FactVC shows the most confidence. In video 2, the generated caption has a minor factual error, but it has a poor overlap with the references. All text-reference-based metrics give low scores, while FactVC gives a more reasonable factual score. In video 3, the generated caption has many different kinds of factual errors. However, probably because of the semantic overlap, BERTScore and EMScore give it high scores. Our FactVC gives a relatively low score. In video 4, the generated caption is full of severe factual errors,\nand FactVC correctly gives a very low score. The examples show that our FactVC metric performs best in measuring\nthe factuality of video captions.\nVideo content:\nA man is seen speaking to the camera and leads into him holding up a pair of tools. The man then begins ironing the shirt while speaking to the camera. He continues to iron the iron and ends by showing off the finished product. Paragraph score: 2\nA woman is seen ironing a pair of pants on an ironing board while speaking to the camera. She continues ironing the pants and ends by showing off the shirt. Paragraph score: 2\nA woman is ironing a shirt on an ironing board. She shows off a pair of pants. She then irons the shirt on the ironing board. Paragraph score: 4\nA woman is seen speaking to the camera and leads into a large iron of a large iron. The woman then begins ironing the shirt and irons the iron. The woman continues to iron the iron and shows off the iron. Paragraph score: 3\nShe then shows the iron the iron and continues to use the iron. She then irons the ironing the shirt and begins ironing the pants. A woman is standing in a kitchen talking to the camera. Paragraph score: 3\nVideo content:\nAdd chickpeas lemon juice and lemon juice to a bowl. Add flour salt pepper and a spoon of chicken breast and mix. Add chopped tomatoes chopped spring onions and a little salt and pepper. Toss and mix everything together. Paragraph score: 2\nPour macaroni and milk on the pasta. Add pasta sugar salt pepper and vinegar to the salad. Add salt and pepper and mix. Mix the salad. Paragraph score: 4\nPour boiled macaroni and boiled macaroni in a bowl. Add some mayonnaise and blend until smooth. Add diced celery and minced garlic to a bowl. Mix everything together. Paragraph score: 4\nAdd 1 cup of chopped green onions and 1 cup of chopped green onions. Plate the meat with the sauce and bread crumbs. Add diced onion celery celery and mint to the food processor. Mix the ingredients in the bowl. Paragraph score: 2\nAdd pasta to a bowl. Mix mayonnaise mayonnaise mayonnaise salt and pepper. Add the cabbage celery and red bell pepper to the cabbage. Toss the salad. Paragraph score: 4\nTable 13: Annotation examples. For each example, we show the video content, five paragraph captions, and the paragraph factuality scores. The phrases/words that are not factual are marked in red."
        },
        {
            "heading": "E. MSCOCO-Fact Dataset",
            "text": "In order to test our method on image captioning, we additionally collect a human-annotated factuality dataset MSCOCO-Fact. We sample 200 MSCOCO[17] images from Karpathy test split[11]. We select five image captioning models: BUTD[1]: a popular image captioning model using bottom-up and top-down attention; BUTDsc[1]: use self-critical sequence training method[26] to train BUTD model; VinVL[35]: a large-scale pretrained captioning model; OFA-base, OFA-huge[33]: a unified multimodal pretrained model that achieves the SOTA perfor-\nmance on MSCOCO caption task. We additionally annotate the factuality of human-generated image captions.\nWe use a similar annotation protocol as video captioning. We collect three kinds of factuality annotation for each image caption sentence: Likert (1-5 factuality score), Binary (0 or 1, indicating whether the sentence has a factual error), Word (whether each word has a factual error). The MSCOCO-Fact dataset contains 1,200 sentences and 11,703 words, among which 29.3% of the sentences and 6.2% of the words have factual errors. Compared to the video captioning dataset ActivityNet-Fact and YouCook2Fact, MSCOCO-Fact suffers less from factual errors.\nWe evaluate the selected models using both automatic metrics and factuality annotation. The results are shown in Table 19. The SOTA image captioning model OFA-huge performs best, and its factuality even outperforms humanwritten captions. The pretrained models (OFA, VinVL) show an obvious advantage over non-pretraining models\n(BUTD). Self-critical sequence training method[26] can improve the automatic metrics but may harm the captions\u2019 factuality."
        }
    ],
    "title": "Models See Hallucinations: Evaluating the Factuality in Video Captioning",
    "year": 2023
}