{
    "abstractText": "This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning. We analytically show that the diversity in neurons\u2019 integration/relaxation dynamics improves an RSNN\u2019s ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve E , defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activity compared to a homogeneous RSNN.",
    "authors": [
        {
            "affiliations": [],
            "name": "SYNAPTIC DYNAM"
        },
        {
            "affiliations": [],
            "name": "DESIGN PRINCIPLES"
        },
        {
            "affiliations": [],
            "name": "Biswadeep Chakraborty"
        }
    ],
    "id": "SP:b9be6878f421b985736766857b3e16cb2db4438e",
    "references": [
        {
            "authors": [
                "Larry F Abbott",
                "Peter Dayan"
            ],
            "title": "The effect of correlated variability on the accuracy of a population code",
            "venue": "Neural computation,",
            "year": 1999
        },
        {
            "authors": [
                "Pau Vilimelis Aceituno",
                "Gang Yan",
                "Yang-Yu Liu"
            ],
            "title": "Tailoring echo state networks for optimal learning. iscience",
            "year": 2020
        },
        {
            "authors": [
                "Robert M Blumenthal",
                "Ronald K Getoor"
            ],
            "title": "Some theorems on stable processes",
            "venue": "Transactions of the American Mathematical Society,",
            "year": 1960
        },
        {
            "authors": [
                "Martin Boerlin",
                "Sophie Den\u00e8ve"
            ],
            "title": "Spike-based population coding and working memory",
            "venue": "PLoS computational biology,",
            "year": 2011
        },
        {
            "authors": [
                "Martin Boerlin",
                "Christian K Machens",
                "Sophie Den\u00e8ve"
            ],
            "title": "Predictive coding of dynamical variables in balanced spiking networks",
            "venue": "PLoS computational biology,",
            "year": 2013
        },
        {
            "authors": [
                "Ralph Bourdoukan",
                "David Barrett",
                "Sophie Deneve",
                "Christian K Machens"
            ],
            "title": "Learning optimal spike-based representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Nicolas Brunel"
            ],
            "title": "Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons",
            "venue": "Journal of computational neuroscience,",
            "year": 2000
        },
        {
            "authors": [
                "Gy\u00f6rgy Buzs\u00e1ki",
                "Kenji Mizuseki"
            ],
            "title": "The log-dynamic brain: how skewed distributions affect network operations",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Biswadeep Chakraborty",
                "Saibal Mukhopadhyay"
            ],
            "title": "Characterization of generalizability of spike time dependent plasticity trained spiking neural networks",
            "venue": "arXiv preprint arXiv:2105.14677,",
            "year": 2021
        },
        {
            "authors": [
                "Biswadeep Chakraborty",
                "Saibal Mukhopadhyay"
            ],
            "title": "Brain-inspired spiking neural network for online unsupervised time series prediction",
            "venue": "arXiv preprint arXiv:2304.04697,",
            "year": 2023
        },
        {
            "authors": [
                "Biswadeep Chakraborty",
                "Saibal Mukhopadhyay"
            ],
            "title": "Heterogeneous recurrent spiking neural network for spatio-temporal classification",
            "venue": "Frontiers in Neuroscience,",
            "year": 2023
        },
        {
            "authors": [
                "Biswadeep Chakraborty",
                "Xueyuan She",
                "Saibal Mukhopadhyay"
            ],
            "title": "A fully spiking hybrid neural network for energy-efficient object detection",
            "venue": "arXiv preprint arXiv:2104.10719,",
            "year": 2021
        },
        {
            "authors": [
                "Biswadeep Chakraborty",
                "Uday Kamal",
                "Xueyuan She",
                "Saurabh Dash",
                "Saibal Mukhopadhyay"
            ],
            "title": "Brain-inspired spatiotemporal processing algorithms for efficient event-based perception",
            "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE),",
            "year": 2023
        },
        {
            "authors": [
                "Ashesh Chattopadhyay",
                "Pedram Hassanzadeh",
                "Devika Subramanian"
            ],
            "title": "Data-driven predictions of a multiscale lorenz 96 chaotic system using machine-learning methods: reservoir computing, artificial neural network, and long short-term memory network",
            "venue": "Nonlinear Processes in Geophysics,",
            "year": 2020
        },
        {
            "authors": [
                "Julien Chevallier",
                "Mar\u00eda Jos\u00e9 C\u00e1ceres",
                "Marie Doumic",
                "Patricia Reynaud-Bouret"
            ],
            "title": "Microscopic approach of a time elapsed neural model",
            "venue": "Mathematical Models and Methods in Applied Sciences,",
            "year": 2015
        },
        {
            "authors": [
                "ES Chornoboy",
                "LP Schramm",
                "AF Karr"
            ],
            "title": "Maximum likelihood identification of neural point process systems",
            "venue": "Biological cybernetics,",
            "year": 1988
        },
        {
            "authors": [
                "Lee Cossell",
                "Maria Florencia Iacaruso",
                "Dylan R Muir",
                "Rachael Houlton",
                "Elie N Sader",
                "Ho Ko",
                "Sonja B Hofer",
                "Thomas D Mrsic-Flogel"
            ],
            "title": "Functional organization of excitatory synaptic strength in primary visual cortex",
            "venue": "Nature, 518(7539):399\u2013403,",
            "year": 2015
        },
        {
            "authors": [
                "Ian Covert",
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "Understanding global feature contributions with additive importance measures",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Cramer",
                "Yannik Stradmann",
                "Johannes Schemmel",
                "Friedemann Zenke"
            ],
            "title": "The heidelberg spiking data sets for the systematic evaluation of spiking neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sophie Den\u00e8ve",
                "Christian K Machens"
            ],
            "title": "Efficient codes and balanced networks",
            "venue": "Nature neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Alain Destexhe",
                "Eve Marder"
            ],
            "title": "Plasticity in single neuron and circuit",
            "venue": "computations. Nature,",
            "year": 2004
        },
        {
            "authors": [
                "John K Douglass",
                "Lon Wilkens",
                "Eleni Pantazelou",
                "Frank Moss"
            ],
            "title": "Noise enhancement of information transfer in crayfish mechanoreceptors by stochastic resonance",
            "year": 1993
        },
        {
            "authors": [
                "C\u00e9line Duval",
                "Eric Lu\u00e7on",
                "Christophe Pouzat"
            ],
            "title": "Interacting hawkes processes with multiplicative inhibition",
            "venue": "Stochastic Processes and their Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Igor Farka\u0161",
                "Peter Gergel"
            ],
            "title": "Maximizing memory capacity of echo state networks with orthogonalized reservoirs",
            "venue": "In 2017 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "Igor Farka\u0161",
                "Radom\u00edr Bos\u00e1k",
                "Peter Gergel"
            ],
            "title": "Computational analysis of memory capacity in echo state networks",
            "venue": "Neural Networks,",
            "year": 2016
        },
        {
            "authors": [
                "Jean Feydy",
                "Thibault S\u00e9journ\u00e9",
                "Fran\u00e7ois-Xavier Vialard",
                "Shun-ichi Amari",
                "Alain Trouv\u00e9",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Interpolating between optimal transport and mmd using sinkhorn divergences",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Peter I Frazier"
            ],
            "title": "A tutorial on bayesian optimization",
            "venue": "arXiv preprint arXiv:1807.02811,",
            "year": 2018
        },
        {
            "authors": [
                "Antonio Galves",
                "Eva L\u00f6cherbach"
            ],
            "title": "Modeling networks of spiking neurons as interacting processes with memory of variable length",
            "venue": "Journal de la Socie\u0301te\u0301 Franc\u0327aise de Statistique,",
            "year": 2016
        },
        {
            "authors": [
                "Erol Gelenbe",
                "Zhi-Hong Mao",
                "Yan-Da Li"
            ],
            "title": "Function approximation with spiked random networks",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 1999
        },
        {
            "authors": [
                "Felipe Gerhard",
                "Moritz Deger",
                "Wilson Truccolo"
            ],
            "title": "On the stability and dynamics of stochastic spiking neuron models: Nonlinear hawkes process and point process glms",
            "venue": "PLoS computational biology,",
            "year": 2017
        },
        {
            "authors": [
                "Wulfram Gerstner",
                "Werner M Kistler"
            ],
            "title": "Mathematical formulations of hebbian learning",
            "venue": "Biological cybernetics,",
            "year": 2002
        },
        {
            "authors": [
                "Aditya Gilra",
                "Wulfram Gerstner"
            ],
            "title": "Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network",
            "venue": "Elife, 6:e28295,",
            "year": 2017
        },
        {
            "authors": [
                "Mirko Goldmann",
                "Felix K\u00f6ster",
                "Kathy L\u00fcdge",
                "Serhiy Yanchuk"
            ],
            "title": "Deep time-delay reservoir computing: Dynamics and memory capacity",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan W Gouwens",
                "Staci A Sorensen",
                "Jim Berg",
                "Changkyu Lee",
                "Tim Jarsky",
                "Jonathan Ting",
                "Susan M Sunkin",
                "David Feng",
                "Costas A Anastassiou",
                "Eliza Barkan"
            ],
            "title": "Classification of electrophysiological and morphological neuron types in the mouse visual cortex",
            "venue": "Nature neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "David Hansel",
                "Germ\u00e1n Mato",
                "Claude Meunier"
            ],
            "title": "Synchrony in excitatory neural networks",
            "venue": "Neural computation,",
            "year": 1995
        },
        {
            "authors": [
                "Niels Richard Hansen",
                "Patricia Reynaud-Bouret",
                "Vincent Rivoirard"
            ],
            "title": "Lasso and probabilistic inequalities for multivariate point processes",
            "year": 2015
        },
        {
            "authors": [
                "Hatsuo Hayashi",
                "Jun Igarashi"
            ],
            "title": "Ltd windows of the stdp learning rule and synaptic connections having a large transmission delay enable robust sequence learning amid background noise",
            "venue": "Cognitive neurodynamics,",
            "year": 2009
        },
        {
            "authors": [
                "Nicolangelo Iannella",
                "Andrew D. Back"
            ],
            "title": "A spiking neural network architecture for nonlinear function approximation",
            "venue": "Neural Networks,",
            "year": 2001
        },
        {
            "authors": [
                "Herbert Jaeger"
            ],
            "title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note",
            "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,",
            "year": 2001
        },
        {
            "authors": [
                "Herbert Jaeger"
            ],
            "title": "Short term memory in echo state networks. gmd-report 152",
            "venue": "In GMD-German National Research Institute for Computer Science",
            "year": 2002
        },
        {
            "authors": [
                "Herbert Jaeger"
            ],
            "title": "Short term memory in echo state networks, volume 5. GMD-Forschungszentrum Informationstechnik",
            "venue": "Bremen, Germany,",
            "year": 2001
        },
        {
            "authors": [
                "Mina A Khoei",
                "Amirreza Yousefzadeh",
                "Arash Pourtaherian",
                "Orlando Moreira",
                "Jonathan Tapson"
            ],
            "title": "Sparnet: Sparse asynchronous neural network execution for energy efficient inference",
            "venue": "In 2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),",
            "year": 2020
        },
        {
            "authors": [
                "Daehyun Kim",
                "Biswadeep Chakraborty",
                "Xueyuan She",
                "Edward Lee",
                "Beomseok Kang",
                "Saibal Mukhopadhyay"
            ],
            "title": "Moneta: A processing-in-memory-based hardware platform for the hybrid convolutional spiking neural network with online learning",
            "venue": "Frontiers in Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Seijoon Kim",
                "Seongsik Park",
                "Byunggook Na",
                "Sungroh Yoon"
            ],
            "title": "Spiking-yolo: Spiking neural network for energy-efficient object detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Martin Korte",
                "Dietmar Schmitz"
            ],
            "title": "Cellular and system biology of memory: timing, molecules, and beyond",
            "venue": "Physiological reviews,",
            "year": 2016
        },
        {
            "authors": [
                "\u0141ukasz Ku\u015bmierz",
                "Shun Ogawa",
                "Taro Toyoizumi"
            ],
            "title": "Edge of chaos and avalanches in neural networks with heavy-tailed synaptic weight distribution",
            "venue": "Physical Review Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Yasaman Bahri",
                "Roman Novak",
                "Samuel S Schoenholz",
                "Jeffrey Pennington",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Deep neural networks as gaussian processes",
            "venue": "arXiv preprint arXiv:1711.00165,",
            "year": 2017
        },
        {
            "authors": [
                "Hongmin Li",
                "Hanchao Liu",
                "Xiangyang Ji",
                "Guoqi Li",
                "Luping Shi"
            ],
            "title": "Cifar10-dvs: an event-stream dataset for object classification",
            "venue": "Frontiers in neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Eva L\u00f6cherbach"
            ],
            "title": "Spiking neurons: interacting hawkes processes, mean field limits and oscillations",
            "venue": "ESAIM: Proceedings and Surveys,",
            "year": 2017
        },
        {
            "authors": [
                "Edward N Lorenz"
            ],
            "title": "Deterministic nonperiodic flow",
            "venue": "Journal of atmospheric sciences,",
            "year": 1963
        },
        {
            "authors": [
                "Edward N Lorenz"
            ],
            "title": "Predictability: A problem partly solved",
            "venue": "In Proc. Seminar on predictability,",
            "year": 1996
        },
        {
            "authors": [
                "Cheng Ly"
            ],
            "title": "Firing rate dynamics in recurrent spiking neural networks with intrinsic and network heterogeneity",
            "venue": "Journal of computational neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "Rosario N Mantegna",
                "H Eugene Stanley"
            ],
            "title": "Scaling behaviour in the dynamics of an economic index",
            "venue": "Nature, 376(6535):46\u201349,",
            "year": 1995
        },
        {
            "authors": [
                "Eve Marder",
                "Adam L Taylor"
            ],
            "title": "Multiple models to capture the variability in biological neurons and networks",
            "venue": "Nature neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Cyrille Mascart"
            ],
            "title": "Efficient simulation of point processes with applications to neurosciences",
            "venue": "PhD thesis, Universite\u0301 Co\u0302te d\u2019Azur,",
            "year": 2021
        },
        {
            "authors": [
                "Sam McKenzie",
                "Roman Husz\u00e1r",
                "Daniel F English",
                "Kanghwan Kim",
                "Fletcher Christensen",
                "Euisik Yoon",
                "Gy\u00f6rgy Buzs\u00e1ki"
            ],
            "title": "Preexisting hippocampal network dynamics constrain optogenetically induced place",
            "venue": "fields. Neuron,",
            "year": 2021
        },
        {
            "authors": [
                "Fernando Montani",
                "Robin AA Ince",
                "Riccardo Senatore",
                "Ehsan Arabzadeh",
                "Mathew E Diamond",
                "Stefano Panzeri"
            ],
            "title": "The impact of high-order interactions on the rate of synchronous discharge and information transmission in somatosensory cortex",
            "venue": "Philosophical Transactions of the Royal Society A: Mathematical,",
            "year": 1901
        },
        {
            "authors": [
                "Emre O Neftci",
                "Hesham Mostafa",
                "Friedemann Zenke"
            ],
            "title": "Surrogate gradient learning in spiking neural networks",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2019
        },
        {
            "authors": [
                "Erkki Oja"
            ],
            "title": "Simplified neuron model as a principal component analyzer",
            "venue": "Journal of mathematical biology,",
            "year": 1982
        },
        {
            "authors": [
                "Erkki Oja"
            ],
            "title": "Neural networks, principal components, and subspaces",
            "venue": "International journal of neural systems,",
            "year": 1989
        },
        {
            "authors": [
                "Ankita Paul",
                "Stefan Wagner",
                "Anup Das"
            ],
            "title": "Learning in feedback-driven recurrent spiking neural networks using full-force training",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Perez-Nieves",
                "Vincent CH Leung",
                "Pier Luigi Dragotti",
                "Dan FM Goodman"
            ],
            "title": "Neural heterogeneity promotes robust learning",
            "venue": "Nature communications,",
            "year": 2021
        },
        {
            "authors": [
                "Balint Petro",
                "Nikola Kasabov",
                "Rita M Kiss"
            ],
            "title": "Selection and optimization of temporal spike encoding methods for spiking neural networks",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Pfaffelhuber",
                "Stefan Rotter",
                "Jakob Stiefel"
            ],
            "title": "Mean-field limits for non-linear hawkes processes with excitation and inhibition",
            "venue": "Stochastic Processes and their Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Filip Ponulak",
                "Andrzej Kasinski"
            ],
            "title": "Introduction to spiking neural networks: Information processing, learning and applications",
            "venue": "Acta neurobiologiae experimentalis,",
            "year": 2011
        },
        {
            "authors": [
                "R Rossi Pool",
                "Germ\u00e1n Mato"
            ],
            "title": "Spike-timing-dependent plasticity and reliability optimization: the role of neuron dynamics",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Steven A Prescott",
                "Yves De Koninck",
                "Terrence J Sejnowski"
            ],
            "title": "Biophysical basis for three distinct dynamical mechanisms of action potential initiation",
            "venue": "PLoS computational biology,",
            "year": 2008
        },
        {
            "authors": [
                "Ryan Pyle",
                "Robert Rosenbaum"
            ],
            "title": "Spatiotemporal dynamics and reliable computations in recurrent spiking neural networks",
            "venue": "Physical review letters,",
            "year": 2017
        },
        {
            "authors": [
                "Nitin Rathi",
                "Amogh Agrawal",
                "Chankyu Lee",
                "Adarsh Kumar Kosta",
                "Kaushik Roy"
            ],
            "title": "Exploring spike-based learning for neuromorphic computing: Prospects and perspectives",
            "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE),",
            "year": 2021
        },
        {
            "authors": [
                "Patricia Reynaud-Bouret",
                "Vincent Rivoirard",
                "Franck Grammont",
                "Christine Tuleau-Malot"
            ],
            "title": "Goodness-of-fit tests and nonparametric adaptive estimation for spike train analysis",
            "venue": "The Journal of Mathematical Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Xueyuan She",
                "Saurabh Dash",
                "Saibal Mukhopadhyay"
            ],
            "title": "Sequence approximation using feedforward spiking neural network for spatiotemporal learning: Theory and optimization methods",
            "venue": "In Proceedings of 10th International Conference on Learning Representations, ICLR (forthcoming),",
            "year": 2022
        },
        {
            "authors": [
                "Michael F Shlesinger",
                "BJ West",
                "Joseph Klafter"
            ],
            "title": "L\u00e9vy dynamics of enhanced diffusion: Application to turbulence",
            "venue": "Physical Review Letters,",
            "year": 1987
        },
        {
            "authors": [
                "Umut Simsekli",
                "Levent Sagun",
                "Mert Gurbuzbalaban"
            ],
            "title": "A tail-index analysis of stochastic gradient noise in deep neural networks",
            "year": 1901
        },
        {
            "authors": [
                "Umut Simsekli",
                "Ozan Sener",
                "George Deligiannidis",
                "Murat A Erdogdu"
            ],
            "title": "Hausdorff dimension, heavy tails, and generalization in neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "P Jesper Sjostrom",
                "Ede A Rancz",
                "Arnd Roth",
                "Michael Hausser"
            ],
            "title": "Dendritic excitability and synaptic plasticity",
            "venue": "Physiological reviews,",
            "year": 2008
        },
        {
            "authors": [
                "Martino Sorbaro",
                "Qian Liu",
                "Massimo Bortone",
                "Sadique Sheik"
            ],
            "title": "Optimizing the energy consumption of spiking neural networks for neuromorphic applications",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Soures",
                "Dhireesha Kudithipudi"
            ],
            "title": "Spiking reservoir networks: Brain-inspired recurrent algorithms that use random, fixed synaptic strengths",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2019
        },
        {
            "authors": [
                "Gopalakrishnan Srinivasan",
                "Kaushik Roy"
            ],
            "title": "Restocnet: Residual stochastic binary convolutional spiking neural network for memory-efficient neuromorphic computing",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Staude",
                "Stefan Rotter",
                "Sonja Gr\u00fcn"
            ],
            "title": "Cubic: cumulant based inference of higher-order correlations in massively parallel spike trains",
            "venue": "Journal of computational neuroscience,",
            "year": 2010
        },
        {
            "authors": [
                "Tobias Thornes",
                "Peter D\u00fcben",
                "Tim Palmer"
            ],
            "title": "On the use of scale-dependent precision in earth system modelling",
            "venue": "Quarterly Journal of the Royal Meteorological Society,",
            "year": 2017
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Yuan Xie",
                "Luping Shi"
            ],
            "title": "Direct training for spiking neural networks: Faster, larger, better",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Bojian Yin",
                "Federico Corradi",
                "Sander M Boht\u00e9"
            ],
            "title": "Effective and efficient computation with multipletimescale spiking recurrent neural networks",
            "venue": "In International Conference on Neuromorphic Systems 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Wenrui Zhang",
                "Peng Li"
            ],
            "title": "Temporal spike sequence learning via backpropagation for deep spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yan Zhou",
                "Yaochu Jin",
                "Jinliang Ding"
            ],
            "title": "Surrogate-assisted evolutionary search of spiking neural architectures in liquid state",
            "venue": "machines. Neurocomputing,",
            "year": 2020
        },
        {
            "authors": [
                "Perez et al.Perez-Nieves"
            ],
            "title": "2021). Thus, instead of searching for the individual parameters themselves, we use a modified BO to estimate parameter distributions for the LIF neurons and the STDP dynamics. After learning the optimal distributions, we simply sample from the distribution to get the distribution of hyperparameters used in the model. To learn the probability distribution of the data, we modify BO\u2019s surrogate model and acquisition function",
            "year": 2021
        },
        {
            "authors": [
                "modified BO"
            ],
            "title": "We want to learn the optimal distribution of hyperparameters x\u2032, which maximizes the performance. It is to be noted here that for higher-dimensional metric spaces, we use the Sinkhorn distance as a regularized version of the Wasserstein distance to approximate the Wasserstein distance (Feydy et al., 2019). D1\u2236k are the points evaluated by the objective function. The GP will estimate the mean \u03bc\u20d7Dk\u2236n",
            "year": 2019
        },
        {
            "authors": [
                "DVS dataset Li"
            ],
            "title": "Impact of Heterogeneity on Covariance: We plot the covariance matrices for different levels of heterogeneity J (Eq. 46) for a small network with 50 neurons. The covariance matrix is calculated by taking the average neuronal states before the appearance of the first spike in the final layer",
            "year": 2017
        },
        {
            "authors": [
                "Deneve"
            ],
            "title": "Den\u00e8ve & Machens (2016) discussed the inefficiency of irregular Poisson rate encoding in the brain. The authors argue that the Poisson point process, which we use to model the spike firing rate, is extremely inefficient as it exponentially increases the number of spikes required to convey information",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning. We analytically show that the diversity in neurons\u2019 integration/relaxation dynamics improves an RSNN\u2019s ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve E , defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activity compared to a homogeneous RSNN."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Spiking neural networks (SNNs) (Ponulak & Kasinski, 2011) use unsupervised bio-inspired neurons and synaptic connections, trainable with either biological learning rules such as spike-timingdependent plasticity (STDP) (Gerstner & Kistler, 2002; Chakraborty & Mukhopadhyay, 2023a) or supervised statistical learning algorithms such as surrogate gradient (Neftci et al., 2019). Empirical results on standard SNNs also show good performance for various tasks Chakraborty et al. (2023), including spatiotemporal data classification, (Lee et al., 2017; Khoei et al., 2020), sequence-to-sequence mapping (Zhang & Li, 2020), object detection (Chakraborty et al., 2021; Kim et al., 2020), and universal function approximation (Gelenbe et al., 1999; Iannella & Back, 2001). An important motivation for the application of SNN in machine learning (ML) is the sparsity in the firing (activation) of the neurons, which reduces energy dissipation during inference (Wu et al., 2019). Many prior works have empirically shown that SNN has lower firing activity than artificial neural networks and can improve energy efficiency (Kim et al., 2022; Srinivasan & Roy, 2019). However, there are very few analytical studies on how to reduce the spiking activity of an SNN while maintaining its learning performance. Understanding and optimizing the relations between spiking activity and performance will be key to designing energy-efficient SNNs for complex ML tasks.\nIn this paper, we derive analytical results and present design principles from optimizing the spiking activity of a recurrent SNN (RSNN) while maintaining prediction performance. Most SNN research in ML considers a simplified network model with a homogeneous population of neurons and synapses (homogeneous RSNN (MRSNN)) where all neurons have uniform integration/relaxation dynamics, and all synapses use the same long-term potentiation (LTP) and long-term depression (LTD) dynamics in STDP learning rules. On the contrary, neurobiological studies have shown that a brain has a wide variety of neurons and synapses with varying firing and plasticity dynamics, respectively (Destexhe & Marder, 2004; Gouwens et al., 2019; Hansel et al., 1995; Prescott et al., 2008). We show that optimizing neuronal and synaptic heterogeneity will be key to simultaneously reducing spiking activity while improving performance.\nar X\niv :2\n30 2.\n11 61\n8v 2\n[ cs\n.A I]\n5 A\nug 2\n02 3\nWe define the spike efficiency E of an RSNN as the ratio of its memory capacity C and average spiking activity S\u0303. Given a fixed number of neurons and synapses, a higher C implies a network can learn more patterns and hence, perform better in classification or prediction tasks (Aceituno et al., 2020; Goldmann et al., 2020); a lower spiking rate implies that a network is less active, and hence, will consume less energy while making inferences (Sorbaro et al., 2020; Rathi et al., 2021). We analytically show that a Heterogeneous Recurrent SNN (HRSNN) model leads to a more spike-efficient learning architecture by reducing spiking activity while improving C (i.e., performance) of the learning models. In particular, we make the following contributions to the theoretical understanding of an HRSNN.\n\u2022 We prove that for a finite number of neurons, models with heterogeneity among the neuronal dynamics has higher memory capacity C.\n\u2022 We prove that heterogeneity in the synaptic dynamics reduces the spiking activity of neurons while maintaining C. Hence, a model with heterogeneous synaptic dynamics has a lesser firing rate than a model with homogeneous synaptic dynamics.\n\u2022 We connect the preceding results to prove that simultaneously using heterogeneity in neurons and synapses, as in an HRSNN, improves the spike efficiency of a network.\nWe empirically characterize HRSNN considering the tasks of (a) classifying time series ( Spoken Heidelberg Digits (SHD)) and (b) predicting the evolution of a dynamical system (a modified chaotic Lorenz system). The theoretical results are used to develop an HRSNN architecture where a modified Bayesian Optimization (BO) is used to determine the optimal distribution of neuron and synaptic parameters to maximize E . HRSNN exhibits a better performance (higher classification accuracy and lower NRMSE loss) with a lesser average spike count S\u0303 than MRSNN.\nRelated Works Inspired by the biological observations, recent empirical studies showed potential for improving SNN performance with heterogeneous neuron dynamics(Perez-Nieves et al., 2021; Chakraborty & Mukhopadhyay, 2023b). However, there is a lack of theoretical understanding of why heterogeneity improves SNN performance, which is critical for optimizing SNNs for complex tasks. She et al. (2022) have analytically studied the universal sequence approximation capabilities of a feedforward network of neurons with varying dynamics. However, they did not consider heterogeneity in plasticity dynamics, and the results are applicable only for a feed-forward SNN and do not extend to recurrent SNNs (RSNN). The recurrence is not only a fundamental component of a biological brain (Soures & Kudithipudi, 2019), but as a machine learning (ML) model, RSNN also shows good performance in modeling spatiotemporal and nonlinear dynamics (Pyle & Rosenbaum, 2017; Gilra & Gerstner, 2017). Hence, it is critical to understand whether heterogeneity can improve learning in an RSNN. To the best of our knowledge, this is the first work that analytically studies the impact of heterogeneity in synaptic and neuronal dynamics in an RSNN. This work shows that only using neuronal heterogeneity improves performance and does not impact spiking activity. The number of spikes required for the computation increases exponentially with the number of neurons. Therefore, simultaneously analyzing and optimizing neuronal and synaptic heterogeneity, as demonstrated in this work, is critical to design an energy-efficient recurrent SNN."
        },
        {
            "heading": "2 PRELIMINARIES AND DEFINITIONS",
            "text": "We now define the key terms used in the paper. Table 1 summarizes the key notations used in this paper. Figure 1 shows the general structure of the HRSNN model with heterogeneity in both the LIF neurons and the STDP dynamics. It is to be noted here that there are a few assumptions we use for the rest of the paper: Firstly, the heterogeneous network hyperparameters are estimated before the training and inference. The hyperparameters are frozen after estimation and do not change during the model evaluation. Secondly, this paper introduces neuronal and synaptic dynamics heterogeneity by using a distribution of specific parameters. However, other parameters can also be chosen, which might lead to more interesting/better performance or characteristics. We assume a mean-field model where the synaptic weights converge for the analytical proofs. In addition, it must be noted that LIF neurons have been shown to demonstrate different states Brunel (2000). Hence, for the analytical study of the network, we use the mean-field theory to analyze the collective behavior of a dynamical system comprising many interacting particles.\nHeterogeneous LIF Neurons We use the Leaky Integrate and Fire (LIF) neuron model in all our simulations. In this model, the membrane potential of the i-th neuron ui(t) varies over time as:\n\u03c4m dvi(t)\ndt = \u2212 (vi(t) \u2212 vrest) + Ii(t) (1)\nwhere \u03c4m is the membrane time constant, vrest is the resting potential and Ii is the input current. When the membrane potential reaches the threshold value vth a spike is emitted, vi(t) resets to the reset potential vr and then enters a refractory period where the neuron cannot spike. Spikes emitted by the j th neuron at a finite set of times {tj} can be formalized as a spike train Si(t) =\u2211 \u03b4 (t \u2212 ti).\nLet the recurrent layer of an RSNN be R. We incorporate heterogeneity in the LIF neurons by using different membrane time constants \u03c4m,i and threshold voltages vth,i for each LIF neuron i in R. This gives a distribution of time constants and threshold voltages of the LIF neurons inR."
        },
        {
            "heading": "Heterogeneous STDP:",
            "text": "The STDP rule for updating a synaptic weight (\u2206w) is defined by Pool & Mato (2011):\n\u2206w(\u2206t) = \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 A+(w)e \u2212 \u2223\u2206t\u2223\u03c4+ if \u2206t \u2265 0 \u2212A\u2212(w)e \u2212 \u2223\u2206t\u2223\u03c4\u2212 if \u2206t < 0\ns.t.,A+(w) = \u03b7+ (wmax \u2212w) ,A\u2212(w) = \u03b7\u2212 (w \u2212wmin)\n(2) where \u2206t = tpost \u2212 tpre is the time difference between the post-synaptic spike and the pre-synaptic one, with synaptic time-constant \u03c4\u00b1. In heterogeneous STDP, we use an ensemble of values from a distribution for \u03c4\u00b1 and the scaling functions \u03b7\u00b1."
        },
        {
            "heading": "Heterogeneity:",
            "text": "We define heterogeneity as a measure of the variability of the hyperparameters in an RSNN that gives rise to an ensemble of neuronal dynamics.\nEntropy is used to measure population diversity. Assuming that the random variable for the hyperparameters X follows a multivariate Gaussian Distribution (X \u223c N (\u00b5,\u03a3)), then the differential entropy of x on the multivariate Gaussian distribution, isH(x) = n\n2 ln(2\u03c0) +\n1 2 ln \u2223\u03a3\u2223 + n 2 . Now, if we take\nany density function q(x) that satisfies \u222b q(x)xixjdx = \u03a3ij and p = N (0,\u03a3), thenH(q) \u2264H(p). (Proof in Suppl. Sec. A) The Gaussian distribution maximizes the entropy for a given covariance. Hence, the log-determinant of the covariance matrix bounds entropy. Thus, for the rest of the paper, we use the determinant of the covariance matrix to measure the heterogeneity of the network.\nMemory Capacity: Given an input signal x(t), the memory capacity C of a trained RSNN model is defined as a measure for the ability of the model to store and recall previous inputs fed into the network (Jaeger, 2001; Jaeger et al., 2001). In this paper, we use C as a measure of the performance of the model, which is based on the network\u2019s ability to retrieve past information (for various delays) from the reservoir using the linear combinations of reservoir unit activations observed at the output. Intuitively, HRSNN can be interpreted as a set of coupled filters that extract features from the input signal. The final readout selects the right combination of those features for classification or prediction. First, the \u03c4 -delay C measures the performance of the RC for the task of reconstructing the delayed version of model input x(t) at delay \u03c4 (i.e., x(t \u2212 \u03c4) ) and is defined as the squared correlation coefficient between the desired output ( \u03c4 -time-step delayed input signal, x(t \u2212 \u03c4)) and the observed network output y\u03c4(t), given as:\nC = lim \u03c4max\u2192\u221e\n\u03c4max \u2211 \u03c4=1 C(\u03c4) = lim \u03c4max\u2192\u221e \u03c4max \u2211 \u03c4=1 Cov2 (x(t \u2212 \u03c4), y\u03c4(t)) Var(x(t))Var (y\u03c4(t)) , \u03c4 \u2208 N, (3)\nwhere Cov(\u22c5) and Var(\u22c5) denote the covariance function and variance function, respectively. The y\u03c4(t) is the model output in this reconstruction task. C measures the ability of RSNN to reconstruct precisely the past information of the model input. Thus, increasing C indicates the network is capable of learning a greater number of past input patterns, which in turn, helps in increasing the performance of the model. For the simulations, we use \u03c4max = 100.\nSpike-Efficiency: Given an input signal x(t), the spike-efficiency (E) of a trained RSNN model is defined as the ratio of the memory capacity C to the average total spike count per neuron S\u0303.\nE is an analytical measure used to compare how C and hence the model\u2019s performance is improved with per unit spike activity in the model. Ideally, we want to design a system with high C using fewer spikes. Hence we define E as the ratio of the memory capacity using NR neurons C(NR) to the average number of spike activations per neuron (S\u0303) and is given as:\nE = C(NR) \u2211NRi=1 Si\nNR\n, Si = \u222b T\n0 si(t)dt \u2248 Npost\nT\n\u222b \u221e tref t\u03a6idt (4)\nwhere Npost is the number of postsynaptic neurons, \u03a6i is the inter-spike interval spike frequency for neuron i, and T is the total time. It is to be noted here that the total spike count S is obtained by counting the total number of spikes in all the neurons in the recurrent layer until the emission of the first spike at the readout layer."
        },
        {
            "heading": "3 HETEROGENEOUS RSNN: ANALYTICAL RESULTS",
            "text": "We present three main analytical findings. Firstly, neuronal dynamic heterogeneity increases memory capacity by capturing more principal components from the input space, leading to better performance and improved C. Secondly, STDP dynamic heterogeneity decreases spike activation without affecting C, providing better orthogonalization among the recurrent network states and a more efficient representation of the input space, lowering higher-order correlation in spike trains. This makes the model more spike-efficient since the higher-order correlation progressively decreases the information available through neural population (Montani et al., 2009; Abbott & Dayan, 1999). Finally, incorporating heterogeneity in both neuron and STDP dynamics boosts the C to spike activity ratio, i.e., E , which enhances performance while reducing spike counts.\nMemory Capacity: The performance of an RSNN depends on its ability to retain the memory of previous inputs. To quantify the relationship between the recurrent layer dynamics and C, we note that extracting information from the recurrent layer is made using a combination of the neuronal states. Hence, more linearly independent neurons would offer more variable states and, thus, more extended memory.\nLemma 3.1.1: The state of the neuron can be written as follows: ri(t) = NR\n\u2211 k=0\nNR\n\u2211 n=1\n\u03bbkn \u27e8v \u22121 n ,w in\u27e9 (vn)i x(t \u2212 k), where vn,v \u22121 n \u2208 V are, respectively, the left and right\neigenvectors of W, win are the input weights, and \u03bbkn \u2208 \u03bb belongs to the diagonal matrix containing\nthe eigenvalues of W; ai = [ai,0, ai,1, . . .] represents the coefficients that the previous inputs xt = [x(t), x(t \u2212 1), . . .] have on ri(t).\nShort Proof: (See Suppl. Sec. B for full proof) As discussed by Aceituno et al. (2020), the state of the neuron can be represented as r(t) =Wr(t \u2212 1) +win x(t), where win are the input weights. We can simplify this using the coefficients of the previous inputs and plug this term into the covariance between two neurons. Hence, writing the input coefficients a as a function of the eigenvalues of W,\nr(t) = \u221e \u2211 k=0 Wkwinx(t\u2212k) = \u221e \u2211 k=0 (V\u039bkV\u22121)winx(t\u2212k)\u21d2 ri(t) = NR \u2211 k=0 NR \u2211 n=1 \u03bbkn \u27e8v \u22121 n ,w in\u27e9 (vn)i x(t\u2212k)\u220e\nTheorem 1: If the memory capacity of the HRSNN and MRSNN networks are denoted by CH and CM respectively, then, CH \u2265 CM , where the heterogeneity in the neuronal parametersH varies inversely to the correlation among the neuronal states measured as \u2211NRn=1\u2211 NR m=1Cov 2 (xn(t), xm(t)) which in turn varies inversely with C.\nIntuitive Proof: (See Suppl. Sec. B for full proof) Aceituno et al. (2020) showed that the C increases when the variance along the projections of the input into the recurrent layer are uniformly distributed. We show that this can be achieved efficiently by using heterogeneity in the LIF dynamics. More formally, let us express the projection in terms of the state space of the recurrent layer. We show that\nthe raw variance in the neuronal states J can be written as J = \u2211\nNR n=1 \u03bb 2 n(\u03a3)\n(\u2211 NR n=1 \u03bbn(\u03a3))\n2 where \u03bbn(\u03a3) is\nthe nth eigenvalue of \u03a3. We further show that with higherH, the magnitude of the eigenvalues of W decreases and hence leads to a higher J . Now, we project the inputs into orthogonal directions of the network state space and model the system as r(t) = \u221e \u2211 \u03c4=1 a\u03c4x(t\u2212\u03c4)+\u03b5r(t) where the vectors a\u03c4 \u2208 RN are correspond to the linearly extractable effect of x(t \u2212 \u03c4) onto r(t) and \u03b5r(t) is the nonlinear contribution of all the inputs onto the state of r(t). First, we show that C increases when the variance along the projections of the input into the recurrent layer is more uniform. Intuitively, the variances at directions a\u03c4 must fit into the variances of the state space, and since the projections are orthogonal, the variances must be along orthogonal directions. Hence, we show that increasing the correlation among the neuronal states increases the variance of the eigenvalues, which would decrease our memory bound C\u2217. We show that heterogeneity is inversely proportional to NR\n\u2211 n=1\nCov2 (xn(t), xm(t)).\nWe see that increasing the correlations between neuronal states decreases the heterogeneity of the eigenvalues, which reduces C. We show that the variance in the neuronal states is bounded by the determinant of the covariance between the states; hence, covariance increases when the neurons become correlated. AsH increases, neuronal correlation decreases. Aceituno et al. (2020) proved that the neuronal state correlation is inversely related to C. Hence, for HRSNN, withH > 0, CH \u2265 CM . \u220e\nSpiking Efficiency We analytically prove that the average firing rate of HRSNN is lesser than the average firing rate of the MRSNN model by considering a subnetwork of the HRSNN network and modeling the pre-and post-synaptic spike trains using a nonlinear interactive Hawkes process with inhibition, as discussed by Duval et al. (2022). The details of the model are discussed in Suppl. Sec. B.\nLemma 3.2.1: If the neuronal firing rate of the HRSNN network with only heterogeneity in LTP/LTD dynamics of STDP is represented as \u03a6R and that of MRSNN represented as \u03a6M , then the HRSNN model promotes sparsity in the neural firing which can be represented as \u03a6R < \u03a6M .\nShort Proof: (See Suppl. Sec. B for full proof ) We show that the average firing rate of the model with heterogeneous STDP (LTP/LTD) dynamics (averaged over the population of neurons) is lesser than the corresponding average neuronal activation rate for a model with homogeneous STDP dynamics. We prove this by taking a sub-network of the HRSNN model. Now, we model the input spike trains of the pre-synaptic neurons using a multivariate interactive, nonlinear Hawkes process with multiplicative inhibition. Let us consider a population of neurons of size N that is divided into population A (excitatory) and population B (inhibitory). We use a particular instance of the model given in terms of a family of counting processes (Z1t , . . . , Z NA t ) (population A) and (Z NA+1 t , . . . , Z N t ) (population\nB ) with coupled conditional stochastic intensities given respectively by \u03bbA and \u03bbB as follows:\n\u03bbA,Nt \u2236= \u03a6A \u239b\n\u239d\n1\nN \u2211 j\u2208A \u222b\nt\u2212\n0 h1(t \u2212 u)dZ\nj u\n\u239e \u23a0 \u03a6B\u2192A \u239b \u239d 1 N \u2211 j\u2208B \u222b\nt\u2212\n0 h2(t \u2212 u)dZ\nj u\n\u239e\n\u23a0\n\u03bbB,Nt \u2236= \u03a6B \u239b\n\u239d\n1\nN \u2211 j\u2208B \u222b\nt\u2212\n0 h3(t \u2212 u)dZ\nj u\n\u239e \u23a0 +\u03a6A\u2192B \u239b \u239d 1 N \u2211 j\u2208A \u222b\nt\u2212\n0 h4(t \u2212 u)dZ\nj u\n\u239e \u23a0 (5)\nwhere A,B are the populations of the excitatory and inhibitory neurons, respectively, \u03bbit is the intensity of neuron i,\u03a6i a positive function denoting the firing rate, and hj\u2192i(t) is the synaptic kernel associated with the synapse between neurons j and i. Hence, we show that the heterogeneous STDP dynamics increase the synaptic noise due to the heavy tail behavior of the system. This increased synaptic noise leads to a reduction in the number of spikes of the post-synaptic neuron. Intuitively, a heterogeneous STDP leads to a non-uniform scaling of correlated spike-trains leading to decorrelation. Hence, we can say that heterogeneous STDP models have learned a better-orthogonalized subspace representation, leading to better encoding of the input space with fewer spikes. \u220e\nTheorem 2: For a given number of neurons NR, the spike efficiency of the model E = C(NR)\nS\u0303 for\nHRSNN (ER) is greater than MRSNN (EM ) i.e., ER \u2265 EM\nShort Proof: (See Suppl. Sec. B for full proof) First, using Lemma 3.2.1, we show that the number of spikes decreases when we use heterogeneity in the LTP/LTD Dynamics. Hence, we compare the efficiencies of HRSNN with that of MRSNN as follows:\nER EM = CR(NR) \u00d7 S\u0303M S\u0303R \u00d7 CM(NR) =\n\u2211 NR \u03c4=1 Cov2(x(t\u2212\u03c4),aR\u03c4 rR(t)) Var(aR\u03c4 rR(t)) \u00d7 \u221e \u222b\ntref\nt\u03a6Rdt\n\u2211 NR \u03c4=1 Cov2(x(t\u2212\u03c4),aM\u03c4 rM (t)) Var(aM\u03c4 rM (t)) \u00d7\n\u221e \u222b tref t\u03a6Mdt\n(6)\nSince SR \u2264 SM and also,the covariance increases when the neurons become correlated, and as neuronal correlation decreases,HX increases (Theorem 1), we see that ER/EM \u2265 1\u21d2 ER \u2265 EM \u220e\nOptimal Heterogeneity using Bayesian Optimization for Distributions To get optimal heterogeneity in the neuron and STDP dynamics, we use a modified Bayesian Optimization (BO) technique. However, using BO for high-dimensional problems remains a significant challenge. In our case, optimizing HRSNN model parameters for 5000 neurons requires the optimization of two parameters per neuron and four parameters per STDP synapse, where standard BO fails to converge to an optimal solution. However, the parameters to be optimized are correlated and can be drawn from a probability distribution as shown by Perez-Nieves et al. (2021). Thus, we design a modified BO to estimate parameter distributions instead of individual parameters for the LIF neurons and the STDP synapses, for which we modify the BO\u2019s surrogate model and acquisition function. This makes our modified BO highly scalable over all the variables (dimensions) used. The loss for the surrogate model\u2019s update is calculated using the Wasserstein distance between the parameter distributions. We use the modified Matern function on the Wasserstein metric space as a kernel function for the BO problem. The detailed BO methods are discussed in Suppl. Sec. A. BO uses a Gaussian process to model the distribution of an objective function and an acquisition function to decide points to evaluate. For data points x \u2208 X and the corresponding output y \u2208 Y , an SNN with network structure V and neuron parameters W acts as a function fV,W(x) that maps input data x to y. The optimization problem can be defined as: minV,W \u2211x\u2208X,y\u2208Y L (y, fV,W(x)) where V is the set of hyperparameters of the neurons inR andW is the multi-variate distribution constituting the distributions of: (i) the membrane time constants \u03c4m\u2212E , \u03c4m\u2212I of LIF neurons, (ii) the scaling function constants (A+,A\u2212) and (iii) the decay time constants \u03c4+, \u03c4\u2212 for the STDP learning rule in SRR."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "Model and Architecture We empirically verify our analytical results using HRSNN for classification and prediction tasks. Fig. 2 shows the overall architecture of the prediction model. Using a rate-encoding methodology, the time-series data is encoded to a series of spike trains. This highdimensional spike train acts as the input to HRSNN. The output spike trains from HRSNN act as the\ninput to a decoder and a readout layer that finally gives the prediction results. For the classification task, we use a similar method. However, we do not use the decoding layer for the signal but directly feed the output spike signals from HRSNN into the fully connected layer. The complete details of the models used and description of the different modules used in Fig. 2 is discussed in Suppl. Sec. A.\nDatasets: Classification: We use the Spoken Heidelberg Digits (SHD) spiking dataset to benchmark the HRSNN model with other standard spiking neural networks (Cramer et al., 2020). Prediction: We use a multiscale Lorenz 96 system (Lorenz, 1996) which is a set of coupled nonlinear ODEs and an extension of Lorenz\u2019s original model for multiscale chaotic variability of weather and climate systems which we use as a testbed for the prediction capabilities of the HRSNN model (Thornes et al., 2017). Further details on both datasets are provided in Suppl. Sec. A.\nBayesian Optimization Ablation Studies: First, we perform an ablation study of BO for the following three cases: (i) Using Memory Capacity C as the objective function (ii) Using Average Spike Count S\u0303 as the objective function and (iii) Using E as the objective function. We optimize both LIF neuron parameter distribution and STDP dynamics distributions for each. We plot C, S\u0303, the empirical spike efficiency E\u0302 , and the observed RMSE of the model obtained from BO with different numbers of neurons. The results for classification and prediction problems are shown in Fig. 3(a) and (b), respectively. Ideally, we want to design networks with high C and low spike count, i.e., models in the upper right corner of the graph. The observed results show that BO using E as the objective gives the best accuracy with the fewest spikes. Thus, we can say that this model has learned a better-orthogonalized subspace representation, leading to better encoding of the input space with fewer spikes. Hence, for the remainder of this paper, we focus on this BO model, keeping the E as the objective function. This Bayesian Optimization process to search for the optimal hyperparameters of the model is performed before training and inference using the model and is generally equivalent to the network architecture search process used in deep learning. Once we have these optimal hyper-parameters, we freeze these hyperparameters, learn (unsupervised) the network parameters\n(i.e., synaptic weights) of the HRSNN while using the frozen hyperparameters, and generate the final HRSNN model for inference. In other words, the hyperparameters, like the distribution of membrane time constants or the distribution of synaptic time constants for STDP, are fixed during the learning and inference. Further details of the Bayesian Optimization procedure, including the parameterized variables and the final searched distributions of the hyperparameters, are shown in Suppl. Sec. A, where we also discuss the convergence analysis of the three different BOs discussed above.\nHeterogeneity Parameter Importance: We use SAGE (Shapley Additive Global importancE) (Covert et al., 2020), a game-theoretic approach to understand black-box models to calculate the significance of adding heterogeneity to each parameter for improving C and S\u0303. SAGE summarizes the importance of each feature based on the predictive power it contributes and considers complex feature interactions using the principles of Shapley value, with a higher SAGE value signifying a more important feature. We tested the HRSNN model using SAGE on the Lorenz96 and the SHD datasets. The results are shown in Fig. 4. We see that \u03c4m has the greatest SAGE values for C, signifying that it has the greatest impact on improving C when heterogeneity is added. Conversely, we see that heterogeneous STDP parameters (viz., \u03c4\u00b1, \u03b7\u00b1) play a more critical role in determining the average neuronal spike activation. Hence, we confirm the notions proved in Sec. 3 that heterogeneity in neuronal dynamics improves the C while heterogeneity in STDP dynamics improves the spike count. Thus, we need to optimize the heterogeneity of both to achieve maximum E .\nResults: We perform an ablation study to evaluate the performance of the HRSNN model and compare it to standard BP-based spiking models. We study the performances of both the SHD dataset for classification and the Lorenz system for prediction. The results are shown in Table 2. We compare the Normalized Root Mean Squared Error (NRMSE) loss (prediction), Accuracy (classification), Average Spike Count S\u0303 and the application level empirical spiking efficiency E\u0302 calculated as 1\nNRMSE \u00d7 S\u0303 (prediction) and Accuracy S\u0303 (classification). We perform the experiments\nusing 5000 neurons inR on both classification and prediction datasets. We see that the HRSNN model with heterogeneous LIF and heterogeneous STDP outperforms other HRSNN and MRSNN models in terms of NRMSE scores while keeping the S\u0303 much lower than HRSNN with heterogeneous LIF and homogeneous STDP. From the experiments, we can conclude that the heterogeneous LIF neurons have the greatest contribution to improving the model\u2019s performance. In contrast, heterogeneity in STDP has the most significant impact on a spike-efficient representation of the data. HRSNN with heterogeneous LIF and STDP leverages the best of both worlds by achieving the best RMSE with low spike activations, as seen from Table 2. Further detailed results on limited training data are added in Suppl. Sec. A. We also compare the generalizability of the HRSNN vs. MRSNN models, where we empirically show that the heterogeneity in STDP dynamics helps improve the overall model\u2019s generalizability. In addition, we discuss how HRSNN reduces the effect of higher-order correlations, thereby giving rise to a more efficient representation of the state space."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper analytically and empirically proved that heterogeneity in neuronal (LIF) and synaptic (STDP) dynamics leads to an unsupervised RSNN with more memory capacity, reduced spiking count, and hence, better spiking efficiency. We show that HRSNN can achieve similar performance as an MRSNN but with sparse spiking leading to the improved energy efficiency of the network. In conclusion, this work establishes important mathematical properties of an RSNN for neuromorphic machine learning applications like time series classification and prediction. However, it is interesting to note that the mathematical results from this paper also conform to the recent neurobiological research that suggests that the brain has large variability between the types of neurons and learning methods. For example, intrinsic biophysical properties of neurons, like densities and properties of ionic channels, vary significantly between neurons where the variance in synaptic learning rules invokes reliable and efficient signal processing in several animals (Marder & Taylor, 2011; Douglass et al., 1993). Experiments in different brain regions and diverse neuronal types have revealed a wide range of STDP forms with varying neuronal dynamics that vary in plasticity direction, temporal dependence, and the involvement of signaling pathways (Sjostrom et al., 2008; Korte & Schmitz, 2016). Thus, heterogeneity is essential in encoding and decoding stimuli in biological systems. In conclusion, this work establishes connections between the mathematical properties of an RSNN for neuromorphic machine learning applications like time series classification and prediction with neurobiological observations. There are some key limitations to the analyses in this paper. First, the properties discussed are derived independently. An important extension will be to consider all factors simultaneously. Second, we assumed an idealized spiking network where the memory capacity is used to measure its performance, and the spike count measures the energy. Also, we mainly focused on the properties of RSNN trained using STDP. An interesting connection between synchronization and heterogeneous STDP remains a topic that needs to be studied further - whether we can optimally engineer the synchronization properties to improve the model\u2019s performance. Finally, the empirical evaluations were presented for the prediction task on a single dataset. More experimental evaluations, including other tasks and datasets, will strengthen the empirical validations."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This work is supported by the Army Research Office and was accomplished under Grant Number W911NF-19-1-0447. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government."
        },
        {
            "heading": "Supplementary Section",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Supplementary Section A 15",
            "text": ""
        },
        {
            "heading": "B Supplementary Section B 24",
            "text": ""
        },
        {
            "heading": "C Supplementary Section C 35",
            "text": ""
        },
        {
            "heading": "A SUPPLEMENTARY SECTION A",
            "text": ""
        },
        {
            "heading": "A.1 EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "A.1.1 HRSNN AND MRSNN",
            "text": "LIF Neurons STDP parameters HRSNN Heterogeneous Heterogeneous MRSNN Homogeneous Homogeneous\nThe models used in this paper were the Heterogeneous Recurrent SNN (HRSNN) and the Homogeneous Recurrent SNN (MRSNN). Both models use STDP as the learning method. For MRSNN, we use STDP with uniform parameters for all the synapses. However, for HRSNN, we use a distribution for each parameter to get a rich class of diverse LTP/LTD dynamics. But, at the core, all the training is done using STDP."
        },
        {
            "heading": "A.1.2 LIF NEURON NUMERICAL IMPLEMENTATION",
            "text": "To implement the LIF model, we discretize time into multiples of a small-time step \u2206t so that spikes can only happen at multiples of \u2206t. (Cramer et al., 2020; Perez-Nieves et al., 2021) Thus, we can approximately solve Eq. 1 as\nvi(t+\u2206t) = vi[t+1] = \u03b2 (vi[t] \u2212 v0)+v0+(1\u2212\u03b2)Ii[t]\u2212(vth \u2212 vr)Si[t] s.t. \u03b2 = exp(\u2212\u2206t/\u03c4m) (7)\nIt is to be noted here that we use this approximation for numerically solving the LIF neurons. Hence, although we use continuous notations for the remainder of the paper, it is to be noted that we use the discrete form discussed here for numerical solutions."
        },
        {
            "heading": "A.1.3 SPIKE CODING",
            "text": "Encoding: For the RSNN to process our time series, the signal must be represented as spikes. We use a temporal encoding technique for representing signals in this paper. The spikes are only generated whenever the signal changes in value. The implementation of the temporal encoding used in this research is based on the Step-Forward (SF) algorithm (Petro et al., 2019). The percentage of neurons to input the spikes to (\u03b1) is also chosen to provide good recurrent layer dynamics.\nDecoding: To represent the recurrent state, we use an exponentially decreasing rate decoding strategy by taking the sum of all the spikes s over the last \u03c4 timesteps into account as follows:\nxXi (t) = \u03c4\n\u2211 n=0\n\u03b3nsi(t \u2212 n) \u2200i \u2208 E\nwhere X denotes the model representation. The parameters \u03c4 and \u03b3 are balanced to optimize the memory size of the stored data (e.g., \u03c4 \u2264 50 ) and its containment of information, which includes adjusting \u03c4 to the pace at which the temporal data is presented and processed. The state of the recurrent layer will be only based on the output of excitatory neurons. Thus, it is crucial for the discount \u03b3 not to be too small, as it possibly flattens older values in the window to 0, making part of the sliding window unusable. Recent spikes hardly affect the recurrent layer state when setting \u03b3 too high in combination with a large window size. This causes the decoder to react too late to recent information provided by the recurrent layer and complicates the learning process of the readout layer."
        },
        {
            "heading": "A.1.4 READOUT",
            "text": "After the initialization of the recurrent layer, the readout is the only component of the LSM with trainable parameters. It consists of a single fully connected layer for regression or classification. The readout does not have to be any deeper, as the output of the recurrent layer is already a highdimensional representation of the processed input. x and y present the continuous signals of the time series T and model representation X .\nxT (t + k) = yT (t) \u2248 y\u0302X(t) = f\u03b8 (x X (t))\nThe mean squared error (MSE) is used as the loss function to train the readout, and the network was trained using the stochastic optimizer Adam (Kingma & Ba, 2014).\nL (yT , y\u0302X) = 1\nn\nn\n\u2211 i=0 (yTi \u2212 y\u0302 X i )\n2"
        },
        {
            "heading": "A.1.5 DATASETS",
            "text": "Lorenz96: (Lorenz, 1996)Our objective is more clearly demonstrated using the canonical chaotic system we will use as a test bed for the prediction capabilities of the HRSNN model. We use a multiscale Lorenz 96 system which is a set of coupled nonlinear ODEs and an extension of Lorenz\u2019s original model (Thornes et al., 2017), (Chattopadhyay et al., 2020).\ndXk dt = Xk\u22121 (Xk+1 \u2212Xk\u22122) + F \u2212 hc b \u03a3jYj,k\ndYj,k\ndt = \u2212 cbYj+1,k (Yj+2,k \u2212 Yj\u22121,k) \u2212 cYj,k +\nhc\nb Xk \u2212\nhe\nd \u03a3iZi,j,k\ndZi,j,k\ndt =edZi\u22121,j,k (Zi+1,j,k \u2212Zi\u22122,j,k) \u2212 geZi,j,k +\nhe\nd Yj,k (8)\nThis set of coupled nonlinear ordinary differential equations (ODEs) is a three-tier extension of Lorenz\u2019s original model (Lorenz, 1963) and has been proposed by Thornes et al.Thornes et al. (2017) as a fitting prototype for multiscale chaotic variability of the weather and climate system and a useful test bed for novel methods. In these equations, F = 20 is a large-scale forcing that makes the system highly chaotic, and b = c = e = d = g = 10 and h = 1 are tuned to produce appropriate spatiotemporal variability. For this paper, we focus on predicting Y axes, which have relatively moderate amplitudes compared to X,Z and demonstrate high-frequency variability and intermittency, which makes the\nprediction problem difficult. It is to be noted here that the Lorenz 96 is a complex, difficult dataset for climate modeling. A snippet of the time series is shown in Fig. 5.\nSHD dataset: We use the Spoken Heidelberg Digits spiking dataset to benchmark the HRSNN model with other standard spiking neural networks (Cramer et al., 2020). It was created based on the Heidelberg Digits (HD) audio dataset which comprises 20 classes of spoken digits from zero to nine in English and German, spoken by 12 individuals. For training and evaluation, the dataset (10420 samples) is split into a training set (8156 samples) and test set (2264 samples). To apply our RSNNs, we converted all audio samples into 250- by-700 binary matrices. For this, all samples fit within a 1 the second window, shorter samples were padded with zeros, and longer samples were cut by removing the tail. Spikes were then binned in time bins, both of sizes 10ms and 4ms; for the RSNNs, the presence or non-presence of any spikes in the time bin is noted as a single binary event."
        },
        {
            "heading": "A.1.6 HYPERPARAMETERS",
            "text": "The hyperparameters used in this paper are summarized in Table 4"
        },
        {
            "heading": "A.2 MAXIMUM ENTROPY DISTRIBUTION",
            "text": "This subsection proves that the maximum entropy distribution with a fixed covariance matrix is Gaussian.\nLemma: Let q(r) be any density satisfying \u222b q(r)xixjdr = \u03a3ij . Let p = N (0,\u03a3). Then h(q) \u2264 h(p) Proof.\n0 \u2264 KL(q\u2225p) = \u222b q(r) log q(r)\np(r) dr\n= \u2212h(q) \u2212 \u222b q(r) log p(r)dr\n= \u2212h(q) \u2212 \u222b p(r) log p(r)dr\n= \u2212h(q) + h(p)\nsince q and p yield the same moments for the quadratic form encoded by log p(r)."
        },
        {
            "heading": "A.3 OPTIMAL HYPERPARAMETER SELECTION USING BAYESIAN OPTIMIZATION",
            "text": "Most recent research in Bayesian Optimization (BO) applications is limited to low-dimensional problems, as BO fails catastrophically when generalizing to high-dimensional problems (Frazier, 2018). However, in this paper, we aim to use BO to optimize the neuronal and synaptic parameters of a heterogeneous RSNN model. This BO problem thus entails a huge number of hyperparameters to be optimized; hence, using standard BO algorithms remains a significant challenge. Hence, to overcome this issue, we used a novel BO algorithm based on the assumption that our hyperparameters to be optimized are not completely random and uncorrelated but can be thought of as being drawn from a probability distribution as shown by Perez et al.Perez-Nieves et al. (2021). Thus, instead of searching for the individual parameters themselves, we use a modified BO to estimate parameter distributions for the LIF neurons and the STDP dynamics. After learning the optimal distributions, we simply sample from the distribution to get the distribution of hyperparameters used in the model. To learn the probability distribution of the data, we modify BO\u2019s surrogate model and acquisition function to treat the parameter distributions instead of individual variables. This makes our modified BO highly scalable over all the variables (dimensions) used. The loss for the surrogate model\u2019s update is calculated using the Wasserstein distance between the parameter distributions.\nBO uses a Gaussian process to model the distribution of an objective function and an acquisition function to decide on points to evaluate. For data points in a target dataset x \u2208X and the corresponding label y \u2208 Y , an SNN with network structure V and neuron parametersW acts as a function fV,W(x) that maps input data x to predicted label y\u0303. The optimization problem in this work is defined as\nmin V,W \u2211x\u2208X,y\u2208Y L (y, fV,W(x)) (9)\nwhere V is the set of hyperparameters of the neurons in R (Details of hyperparameters given in the Supplementary) andW is the multi-variate distribution constituting the distributions of (i) the membrane time constants \u03c4m\u2212E , \u03c4m\u2212I of the LIF neurons, (ii) the scaling function constants (A+,A\u2212) and (iii) the decay time constants \u03c4+, \u03c4\u2212 for the STDP learning rule in SRR.\nAgain, BO needs a prior distribution of the objective function f(x\u20d7) on the given data D1\u2236k = {x\u20d71\u2236k, f (x\u20d71\u2236k)} . In the Gaussian Process (GP)-based BO, we assume that the prior distribution of f (x\u20d71\u2236k) follows the multivariate Gaussian distribution, which follows a GP with mean \u00b5\u20d7D1\u2236k and covariance \u03a3\u20d7D1\u2236k . Thus, we estimate \u03a3\u20d7D1\u2236k using the modified Matern kernel function. We use the loss function as d(x,x\u2032), which is the Wasserstein distance between the multivariate distributions of the different parameters. That is, given two distributions of hyperparameters x1, x2, the distance between these two distributions (given as d(x1, x2) is used as the loss function in the Matern kernel for the modified BO. We want to learn the optimal distribution of hyperparameters x\u2032, which maximizes the performance. It is to be noted here that for higher-dimensional metric spaces, we use the Sinkhorn distance as a regularized version of the Wasserstein distance to approximate the Wasserstein distance (Feydy et al., 2019).\nD1\u2236k are the points evaluated by the objective function. The GP will estimate the mean \u00b5\u20d7Dk\u2236n and variance \u03c3\u20d7Dk\u2236n for the rest unevaluated data Dk\u2236n. The acquisition function used in this work is the expected improvement (EI) of the prediction fitness as:\nEI (x\u20d7k\u2236n) = (\u00b5\u20d7Dk\u2236n \u2212 f (xbest ))\u03a6(Z\u20d7) + \u03c3\u20d7Dk\u2236n\u03d5(Z\u20d7) (10)\nwhere \u03a6(\u22c5) and \u03d5(\u22c5) denote the probability distribution function and the cumulative distribution function of the prior distributions, respectively. f (xbest ) =max f (x\u20d71\u2236k) is the maximum value that\nhas been evaluated by the original function f in all evaluated data D1\u2236k and Z\u20d7 = \u00b5\u20d7Dk\u2236n\u2212f(xbest )\n\u03c3\u20d7Dk\u2236n . BO\nwill choose the data xj = argmax{EI (x\u20d7k\u2236n) ;xj \u2286 x\u20d7k\u2236n} as the next point to be evaluated using the original objective function."
        },
        {
            "heading": "A.3.1 OPTIMIZED HYPERPARAMETERS",
            "text": "The list of the hyperparameters optimized using the Bayesian Optimization technique is shown in Table 5. We also show the range of the hyperparameters used and the initial values. In addition to this, Table 6 enlist the final optimized distributions of the STDP and the LIF parameters obtained using BO."
        },
        {
            "heading": "A.3.2 CONVERGENCE ANALYSIS",
            "text": "We compare the convergence analysis of the three Bayesian Optimization techniques and the results are shown in Fig. 6. Each of the experiments was repeated five times and the mean and variance of the observations are shown in the Figure. It is to be noted here that since we define the BO as a minimization principle, we minimize 1C , S\u0303 and 1 E ."
        },
        {
            "heading": "A.4 COMPARING BAYESIAN OPTIMIZATION OBJECTIVE FUNCTIONS",
            "text": "We show the results of Bayesian Optimization results for the three cases we are considering in this paper for both the classification and prediction problems. The results for the classification problem are shown in Table 7. We tabulate the memory capacity, the average spike count and the observed accuracy for the three BO cases. Similarly, the results for the prediction problem are shown in Table 8. In that case, we tabulate the memory capacity, the average spike count and the observed NRMSE for the three BO cases. We rerun each of the experiments 5 times and report the mean and standard deviation of the results obtained."
        },
        {
            "heading": "A.5 COMPARING THE GENERALIZABILITY",
            "text": "We observed that increasing the neuronal heterogeneity increases the memory capacity of the network. However, this increment in the memory capacity might lead to a model which overfits the training data. However, the heterogeneous STDP model with varying synaptic dynamics gives rise to a heavy-tailed Feller process. Recent works Simsekli et al. (2020), Chakraborty & Mukhopadhyay (2021) show that the Hausdorff dimension of the trajectories of the sample paths of the learning algorithm can control the generalization error. This is intimately linked to the tail behavior of the driving process. The authors showed that heavier-tailed processes achieve better generalization. Thus, the tail index of the process can be used as a notion of capacity metric that estimates the generalization error, which does not necessarily grow with the number of parameters. The authors discuss that the stochastic process for the synaptic weights behaves like a L\u00e9vy motion around a local point. Because of this locally regular behavior, the Hausdorff dimension can be bounded by the Blumenthal-Getoor (BG) index (Blumenthal & Getoor, 1960), which depends on the tail behavior of the L\u00e9vy process. Thus, we can use the BG index as a bound for the Hausdorff dimension of the trajectories from the STDP\nlearning process. Now, as the Hausdorff dimension is a measure of the generalization error and is also controlled by the tail behavior of the process, heavier tails imply less generalization error. In this paper, we empirically study the generalization ability of the HRSNN network using the BG index as a metric. We did the experiments on the 4 ablation study models for the classification task on the SHD dataset, and the results are reported in Table 9. From the table, we see that the heterogeneity in STDP improves the generalization error the most, while the heterogeneity in the LIF neurons increases the training and testing accuracies."
        },
        {
            "heading": "A.6 RESULTS ON LIMITED TRAINING DATA",
            "text": "We have trained the models with limited training data. We observe that the HRSNN model with heterogeneous LIF and STDP dynamics not only has better testing accuracy but also shows better generalization behavior when compared to other homogeneous RSNN or the other ablation heterogeneous models (with heterogeneity in only one of them). Also, we see that the HRSNN model with heterogeneous STDP shows distinctly better generalization ability than the generalization ability of\nSHD SSC CIFAR10 DVS Training Accuracy\n(A)\nTesting Accuracy\n(B)\nGeneralization Error \u2223A \u2212B\u2223 Training Accuracy (A) Testing Accuracy (B) Generalization Error \u2223A \u2212B\u2223 Training Accuracy (A) Testing Accuracy (B) Generalization Error \u2223A \u2212B\u2223\nHom LIF Hom STDP 86.92 \u00b1 1.35 72.89 \u00b1 1.85 14.03 \u00b1 1.67 74.69 \u00b1 1.72 47.94 \u00b1 1.94 26.75 \u00b1 1.42 82.41 \u00b1 1.8 65.33 \u00b1 3.41 17.08 \u00b1 1.35 Hom LIF Het STDP 85.76 \u00b1 1.27 73.91 \u00b1 1.49 11.85 \u00b1 1.25 76.79 \u00b1 1.58 52.96 \u00b1 1.73 23.86 \u00b1 1.29 83.48 \u00b1 1.52 67.06 \u00b1 2.97 16.42 \u00b1 1.24 Het LIF Hom STDP 95.29 \u00b1 1.16 78.36 \u00b1 1.42 16.93 \u00b1 1.13 84.26 \u00b1 1.33 55.11 \u00b1 1.65 29.15 \u00b1 1.12 86.93 \u00b1 1.79 68.37 \u00b1 3.05 18.56 \u00b1 1.42 Het LIF Het STDP 94.07 \u00b1 1.03 80.01 \u00b1 1.13 14.06 \u00b1 1.02 86.41 \u00b1 1.49 59.28 \u00b1 1.35 27.13 \u00b1 0.97 87.49 \u00b1 1.76 70.54 \u00b1 1.82 16.95 \u00b1 1.38\nHRSNN with heterogeneous LIF neurons. On the other hand, the latter showcases significantly higher training and testing accuracy compared to the former model. This can be interpreted as follows: since heterogeneous LIF dynamics increase the memory capacity, it leads to an overfitting of the data. Heterogeneous STDP dynamics help in obtaining more generalizable solutions from this. Each has its own downsides; however, using HRSNN with both heterogeneous LIF and STDP dynamics shows better performance and generalization abilities, as seen from Table 10."
        },
        {
            "heading": "A.7 FURTHER EVALUATIONS",
            "text": "In Section B, we argued that as the heterogeneity in the neuronal parameters increases, the covariance decreases; hence the neurons become less correlated. In this section, we give empirical results to support the theory. We tested the model on more complex datasets - (i) The Spiking Heidelberg Digits (SHD) dataset (ii) the Spiking Speech Command (SSC) dataset are both audio-based classification datasets for which input spikes and output labels are provided Cramer et al. (2020) and (iii) CIFAR10 DVS dataset Li et al. (2017).\n\u2022 Impact of Heterogeneity on Covariance: We plot the covariance matrices for different levels of heterogeneity J (Eq. 46) for a small network with 50 neurons. The covariance matrix is calculated by taking the average neuronal states before the appearance of the first spike in the final layer. We see that as the heterogeneity in the neuronal parameters increases, the correlation between the neurons decreases. The results are shown in Fig. 7\n\u2022 Impact of Heterogeneity on Principal Components: From the covariance plots, we see that increasing J . reduces the correlation between neurons. We also plot the probability density functions of the eigenvalues of the covariance matrix of the neurons with increasing heterogeneity in the neuronal parameters. We see that with higher heterogeneity in the neuronal parameters J , the distribution of the eigenvalues of the covariance becomes flatter. This signifies that the covariance matrix has a lower variance for higher J . A flatter distribution also indicates that a larger number of principal components are active. This supports our hypothesis that heterogeneity in the neuronal parameters increases the number of principal components and helps increase the model\u2019s memory capacity. The result is shown in this Fig. 8\n\u2022 Impact of Heterogeneity in STDP on Firing Rate: We plot the mean firing rate of the neurons for the four types of HRSNNs and MRSNN with homogeneous LIF and STDP dynamics. We plot the results for a smaller network with 100 neurons and a Poisson input process. The MRSNN model shows a much higher firing rate, especially at a higher frequency, demonstrating that MRSNN requires significantly more spikes than the HRSNN model. The result is shown in Fig. 9\n\u2022 Coupling Strength: We note here that in this paper, we use (homogeneous or heterogeneous) STDP to learn the synaptic conductance connecting various neurons in the SNN. Therefore, we do not control the synaptic coupling strength as independent variables and hence, cannot perform control experiments with various extents of coupling strength. An interesting future extension of the results will be quantifying the coupling strength for HRSNN with heterogeneity in LIF and STDP dynamics. We can leverage McKenzie et al.McKenzie et al. (2021), where the authors proposed statistical tools to estimate synaptic coupling dynamics from spike-spike correlations."
        },
        {
            "heading": "B SUPPLEMENTARY SECTION B",
            "text": ""
        },
        {
            "heading": "B.1 APPROXIMATIONS AND ASSUMPTIONS",
            "text": "We make several approximations and assumptions for this section\u2019s theoretical analysis of the heterogeneous RSNN networks. Firstly, it must be noted that in this paper, the analytical relations are derived by taking the heterogeneity individually. i.e., when we consider heterogeneity in the neuronal parameters, we consider homogeneous STDP dynamics and vice-versa. In addition to this, we assume diffusion approximation. That is, if a neuron receives Poissonian uncorrelated input spike trains and the contribution of a single synaptic connection is small compared to the distance between reset and threshold w \u226a (V\u0398 \u2212 V0), the random input can be approximated by Gaussian white noise with mean \u00b5 and noise intensity \u03c32. This approximation does not hold if the network features highly correlated activity or receives strong external input common to many neurons. Also, we assume a fast/slow synaptic regime in which the synaptic time constant \u03c4s is much shorter/longer than the membrane time constant \u03c4m. In this work, we consider a mean-field approximation of the HRSNN network with heterogeneity in the parameters of the LIF neurons and the STDP dynamics independently."
        },
        {
            "heading": "B.2 MEAN-FIELD REDUCTION MODEL OF HRSNN",
            "text": "In this section, we model the HRSNN network using heterogeneity in only the LIF neuron parameters. Following the works of Ly et al.Ly (2015), we can write the equations for the excitatory neurons indexed by j \u2208 {1,2, . . . ,Ne} are:\n\u03c4m dvj\ndt = \u2212vj \u2212 gie(t) (vj \u2212 EI) \u2212 gee(t) (vj \u2212 EE) + \u03c3E\u03b7j(t) (11) vj (t \u2217 ) \u2265 \u03b8j( refractory period )\u21d2 vj (t\u2217 + \u03c4ref) = 0 (12)\n\u03c4n d\u03b7j\ndt = \u2212\u03b7j +\n\u221a \u03c4n\u03bej(t) (13)\ngee(t) = qj \u03b3ee\npeeNe \u2211 j\u2032\u2208{ presyn E cells} Gj\u2032(t) (14)\ngei(t) = \u03b3ei\npeiNi \u2211 k\u2032\u2208{ presyn I cells} Gk\u2032(t) (15)\n\u03c4d dGj\ndt = \u2212Gj +Aj (16)\n\u03c4r dAj\ndt = \u2212Aj + \u03c4r\u03b1\u2211\nl\n\u03b4 (t \u2212 tl) (17)\nwhere the inhibitory and excitatory reversal potentials are EI , and EE , respectively, with EI < 0 < EE . \u03bej(t) are uncorrelated white noise processes, pxy is the proportion of neuron type y (randomly chosen) that provides presynaptic input to neuron type x (x, y \u2208 {e, i}). The second line in the equations describes the refractory period at spike time t\u2217. When the neuron\u2019s voltage crosses threshold \u03b8j , the neuron goes into a refractory period for \u03c4ref where the voltage is undefined, after which we set the neuron\u2019s voltage to 0. In the last equation, tl denotes the spike times of the j th excitatory neuron. Now, for the mean-field analysis, we use qji to model the synaptic heterogeneity between the pre-and post-synaptic neurons by modulating the synaptic conductance for both the excitatory and inhibitory neurons.\nWe note here the numerical assumptions for the mean-field analysis:\n1. finite size effects are negligible (N e/i\u226b 1 )\n2. the firing rate of presynaptic neurons is governed by a Poisson process\n3. the population firing rate averaged over q and \u03c4m is a good approximation to the average presynaptic input rate and\n4. a single p.d.f. function is sufficient to describe the population behavior) (finite N )\nSimilarly, for the inhibitory neurons indexed by k \u2208 {1,2, . . . ,Ni}, the equations are:\n\u03c4m dvk dt = \u2212vk \u2212 gii(t) (vk \u2212 EI) \u2212 gei(t) (vk \u2212 EE) + \u03c3I\u03b7k(t) (18) vk (t \u2217 ) \u2265 1( refractory period )\u21d2 vj (t\u2217 + \u03c4ref) = 0 (19)\n\u03c4n d\u03b7k dt = \u2212\u03b7k + \u221a \u03c4n\u03bek(t) (20)\ngie(t) = qj \u03b3ie\npieNe \u2211 k\u2032\u2208{ presyn I cells} Gk\u2032(t) (21)\ngii(t) = \u03b3ii\npiiNi \u2211 k\u2032\u2208 {presyn I cells} Gk\u2032(t) (22)\n\u03c4d dGk dt = \u2212Gk +Ak (23)\n\u03c4r dAk dt = \u2212Ak + \u03c4r\u03b1\u2211\nl\n\u03b4 (t \u2212 tl) (24)\nPlease refer to the paper by Ly et al. Ly (2015) for details regarding the equations. Since the recurrent coupled stochastic network is difficult to describe theoretically, we use population density methods, where an equation determines the probability of a neuron being in a particular state. The variables in the populations are determined using distribution functions. The two forms of heterogeneity introduce a large number of dimensions. For simplicity, one can track a family of probability density functions for each (qj , \u03c4j) pair for each neuron. The subsequent equations are a good approximation to the HRSNN network with the following assumptions: (i) finite size effects are negligible (Ne/i \u226b 1) (ii) the firing rate of presynaptic neurons is governed by a Poisson process (iii) the population firing rate averaged over q and \u03c4m is a good approximation to the average presynaptic input rate (iv) a single p.d.f. function is sufficient to describe the population behavior, and the heterogeneity is driven by (qj , \u03c4m, j) For each pair of values (qj , \u03c4m, j), the probability density function \u03c1 is defined by:\n\u222b \u2126 \u03c1 (vE ,wE , vI ,wI , t)dvEdwEdvIdwI = Pr((vE(t),wE(t), vI(t),wI(t)) \u2208 \u2126) (25)\nwhere wX denotes the other states variables of the corresponding neuron type X \u2208 {E, I}, consisting of conductance, colored noise: wX = (gX , aX , \u03b7X). The evolution of the p.d.f.\u2019s is governed by a continuity equation and boundary conditions:\n\u2202\u03c1 \u2202t = \u2212\u2207 \u22c5 J (26)\nJ \u2236= (JvE , JgE , JaE , J\u03b7E , JvI , JgI , JaI , J\u03b7I ) (27)\nJvE \u2236= \u2212 1\n\u03c4m [vE + q\u03b3eigI (vE \u2212 EI) + q\u03b3eegE (vE \u2212 EE) + \u03c3E\u03b7E]\u03c1 (28)\nJvI \u2236= \u2212 1\n\u03c4m [vI + \u03b3iigI (vI \u2212 EI) + \u03b3iegE (vI \u2212 EE) + \u03c3I\u03b7I]\u03c1 (29)\nJgX \u2236= \u2212 1\n\u03c4d [gX \u2212 aX]\u03c1 (30)\nJaX \u2236= \u2212 aX \u03c4r + vX(t)\u222b\naX\naX\u2212\u03b1X \u03c1 (. . . , a\u2032X , . . .)da \u2032 X (31)\nJ\u03b7X \u2236= \u2212 1\n\u03c4n \u03b7X\u03c1 +\n1\n\u03c4n\n\u22022\u03c1 \u2202\u03b72X (32)\nvX(t) \u2236= y 1\n\u03c4m JvXdwXdqd\u03c4m (33)\nJwX \u2223 \u2202wX = 0 (34)\nThe definitions of gXY in the LIF neuron equations defined above result in a total conductance of \u03b3XY gY on average.\nWe describe an insightful analytic reduction that captures how the range of excitatory firing rates changes in different regimes. We focus on only the excitatory neurons, which have fewer state variables if the inhibitory population is ignored or assumed to be known.\nLet us denote the approximate excitatory firing rate(s) vE as r. The deterministic firing rate of the equation\n\u03c4m dvE dt = \u2212vE \u2212 qg\u0303I (vE \u2212 EI) \u2212 qg\u0303E (vE \u2212 EE) + \u03b7\u0303E (35)\nis given by\nr0(q, \u03c4m; w\u0303E) = \u23a7\u23aa\u23aa\u23aa \u23a8 \u23aa\u23aa\u23aa\u23a9 0, if q(g\u0303EEE+g\u0303IEI)+\u03b7\u0303E 1+q(g\u0303E+g\u0303I) \u2264 \u03b8 1+q(g\u0303E+g\u0303I) \u03c4m(g\u0303\u2217EEE+g\u0303IEI)+\u03b7\u0303E if q(g\u0303EEE+g\u0303IEI)+\u03b7\u0303E 1+q(g\u0303E+g\u0303I) > \u03b8\n(36)\nWe define: g\u0303E \u2236= \u03b3eegE , g\u0303I \u2236= \u03b3eigI , \u03b7\u0303E \u2236= \u03c3E\u03b7E . Finally, the given state variables are integrated against their marginal density to get:\nr(q, \u03b8) = E [ r0\n1 + r0\u03c4ref ] = \u222b r0 1 + r0\u03c4ref \u03c1\u0303 (g\u0303E , g\u0303I , \u03b7\u0303E)dw\u0303E (37)\nThere is a slight abuse of notation because the auxiliary variables aX effect the conductances but are not written in the previous equation; the emphasis is on how (g\u0303E , g\u0303I , \u03b7\u0303E) directly effects r. Since the external noise is applied indiscriminately, \u03b7\u0303E is independent of the other variables and the marginal density factors into:\n\u03c1\u0303 (g\u0303E , g\u0303I , \u03b7\u0303E) = \u03c1\u0303 (g\u0303E , g\u0303I) e\u2212(\u03b7\u0303E/\u03c3E) 2\n\u03c3E \u221a \u03c0\n(38)\nHowever, \u03c1\u0303 (g\u0303E , g\u0303I) is still not analytically tractable, leading us to rely on Monte Carlo simulations to numerically estimate \u03c1\u0303 (g\u0303E , g\u0303I).\nIt must be noted here that this is a reduction model for the HRSNN network with many simplifying assumptions. It is not a complete mean-field derivation of the HRSNN model with heterogeneous LIF neurons, and heterogeneous STDP dynamics is a fascinating research question but beyond the scope of this paper."
        },
        {
            "heading": "B.3 ANALYTICAL RESULTS",
            "text": "Analytical Results of Memory Capacity Neuroscience networks of spiking neurons are increasingly used to understand mechanisms underlying phenomena observed in electrophysiological recordings.\nThere are two complementary strategies for studying such a recurrent network of spiking neurons - (a) numerical simulations and (b) analytical methods using mean field models. With numerical simulations, we can simulate any network model without any approximation. However, this method typically works in high dimensional parameter space and is, thus, hard to interpret. Also, it is generally hard to characterize parameter regions where specific behaviors are found using numerical simulations. On the other hand, with analytical calculations, we obtain deeper insights into mechanisms underlying specific behaviors and can obtain critical parameters that control specific behaviors. So, now, we analytically study the variance of the estimated memory capacity with the change in the heterogeneity of neuronal parameters. We plot the change in the estimated memory capacity C, calculated using Eq. 3. We plot this with respect to the neuronal heterogeneityH, measured using the entropy of the neuronal parameters for the HRSNN model. The result is plotted in Fig. 10(a). We use a HRSNN model with NR = 1000 and sequences of 4,000 random inputs chosen from U[\u22121; 1]. We see that, as predicted, the memory capacity of the model increases linearly with the increase in heterogeneity within the limits of the application, as proved in Theorem 1. The error bars in Fig. 10(a) represent the standard deviation of the observations.\nAnalytical Study of Spike Efficiency We calculate the average firing rate of the heterogeneous spiking neural network for the prediction task during inference, and the results are shown in Fig 10(b). Using heterogeneity in the STDP parameters reduces the average number of spiking activations while keeping the memory capacity almost equal. This result shows that Heterogeneous STDP leads to sparse activation of neurons, as proved in Theorem 2.\nComparison with Neuroscience Works: We compare the analytical results obtained with some of the standard recurrent LIF network models in the literature. Brunel et al.Brunel (2000) analytically study the dynamics of sparsely connected a network of sparsely connected excitatory and inhibitory integrate-and-fire neurons. The authors showed the existence of a diverse set of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. In this paper, we use heterogeneity in the LIF neurons. This leads to a diverse set of states for the neurons, which consequently helps orthogonalize the state space dynamics to increase the information stored in the memory of the network.\nDeneve et al. Den\u00e8ve & Machens (2016) discussed the inefficiency of irregular Poisson rate encoding in the brain. The authors argue that the Poisson point process, which we use to model the spike firing rate, is extremely inefficient as it exponentially increases the number of spikes required to convey information. The authors further discuss that a continuum exists between loosely balanced and tightly balanced spike-coding networks in neuroscience. Though loosely balanced networks are inefficient, they are cheap in terms of the number of connections per neuron and structure (Boerlin et al., 2013; Boerlin & Den\u00e8ve, 2011; Bourdoukan et al., 2012). On the other hand, tightly-balanced spike-coding networks are highly efficient but extremely structured, dense connections that STDP rules must constantly maintain. For the HRSNN model, since we are engineering an artificial spiking neural network model, our network is highly structured and constantly updated using the heterogeneous STDP rules. Thus, we might say that the HRSNN model is a tightly-coupled network that helps in an efficient transfer of information. This hypothesis is supported by the results shown in Table 2, where the HRSNN model shows a higher performance using a lesser number of spikes."
        },
        {
            "heading": "B.4 MEMORY CAPACITY",
            "text": "Let x(t) \u2208 U (where \u2212\u221e < t < +\u221e and U \u2282 R is a compact interval) be a single-channel stationary input signal. Assume that we have an RSNN, specified by its internal weight matrix W, its input weight vector win and the unit output functions f , f out . The network receives x(t) at its input unit. For a given delay \u03c4 and an output unit y\u03c4 with connection weight vector wout\u03c4 we consider the determination coefficient\nd [wout\u03c4 ] (x(t \u2212 \u03c4), y\u03c4(t)) =\n= d(x(t \u2212 \u03c4),wout\u03c4 ( x(t) r(t) )\n= Cov2 (x(t \u2212 \u03c4), y\u03c4(t))\n\u03c32(x(t))\u03c32 (y\u03c4(t))\nwhere Cov denotes covariance and \u03c32 variance. The \u03c4 -delay Memory capacity of the network is\ndefined by C\u03c4 =max wout\u03c4 d [wout\u03c4 ] (x(t \u2212 \u03c4), y\u03c4(t)). The Memory capacity of the network is C = \u221e \u2211 \u03c4=1 C\u03c4 . The determination coefficient of two signals is the squared correlation coefficient. It ranges between 0 and 1 and represents the fraction of variance explainable in one signal by the other. Thus, the Memory capacity measures how much variance of the delayed input signal can be recovered from optimally trained output units, summed over all delays. Note that the output units do not interfere; arbitrarily, many output units y\u03c4 can be attached to the same network.\nThe performance of the heterogeneous network model derives from its ability to retain the memory of previous inputs. To quantify the relationship between the recurrent layer dynamics and the memory capacity, we note that the extraction of information from the recurrent layer is made through a linear combination of the neurons\u2019 states. Hence, more linearly independent neurons would offer more variable states and, thus, more extended memory.\nFor reservoir computing (RC), Jaeger et al. Jaeger (2002) shows that C is bounded by the reservoir network size of the linear RC with the identity activation function and the independent and identically distributed (i.i.d.) model input. Memory capacity ( C) is used to quantify the memory of RSNN. Such memory capacity measures the ability of RC to reconstruct precisely the past information of the model input. Also, the network\u2019s structural properties can greatly impact the C of the linear RC. Now, the question arises what is the need to maximize the memory capacity of the network? The C normally serves as a global index to quantify the memory property of the network. To comprehensively examine the memory property deeply, the local measurement of its memory property is indispensable. Thus, maximizing the C acts as an estimator for better prediction results of the trained network.\nSince the first-order approximation of the model is linear, the heterogeneity between state variables depends on all the eigenvalues of the adjacency matrix, with a larger mean eigenvalue meaning higher heterogeneity. Hence we can use the eigenvalues {\u03bbi} of the weight matrix W to quantify approximately how fast the input decays in the recurrent layer. In other words, the eigenvalues of W should be related to the memory capacity of the heterogeneous neural network model. Indeed, we find that the average eigenvalue modulus: \u27e8\u2223\u03bb\u2223\u27e9 = 1/NR\u2211NRi=1 \u2223\u03bbi\u2223 strongly correlates withH and therefore with C as well. Note that, instead of C andH, \u27e8\u2223\u03bb\u2223\u27e9 is much easier to compute and is solely determined by the recurrent layer network.\nThe memory capacity reflects the precision with which previous inputs can be recovered. The nonlinearity of the recurrent layer and other far-in-the-past inputs induce noise that complicates recovery. Thus, similar to the analysis done by Aceituno et al.Aceituno et al. (2020) for Echo state networks, the variance of the linear part of the recurrent layer is placed to maximize the recoverable information. Thus, the inputs are projected into orthogonal directions of the recurrent layer state space to not add noise to each other. The variance spread across the different dimensions should be evenly distributed within those orthogonal directions, quantified by the neurons\u2019 covariance.\nWe start by noticing that the linear nature of the projection vector wout implies that we are treating the system as\nr(t) = \u221e \u2211 \u03c4=0 a\u03c4x(t \u2212 \u03c4) + \u03b5(t) (39)\nwhere the vectors a\u03c4 \u2208 RNR correspond to the linearly extractable effect of x(t \u2212 \u03c4) onto r(t) and \u03b5(t) is the nonlinear contribution of all the inputs onto the state of r(t).\nPrevious works have shown that linear recurrent layers have more extended memory, but nonlinearity is needed to perform interesting computations. Here we show that for a fixed ratio of the nonlinearity, greater heterogeneity leads to a lesser neuronal correlation, leading to a higher memory capacity.\nTo maintain this trade-off between linear and non-linear behavior, we will assume that linear and non-linear strengths distribution is fixed. This can be achieved if we impose that the probabilities of the neuron states do not change, meaning that the mean, variance, and other moments of the neuron outputs are unchanged; hence, the strength of the non-linear effects is unchanged. A first constraint can also be obtained from the maintained strength of the linear side of Eq.39\nVar( \u221e \u2211 \u03c4=1 a\u03c4x(t \u2212 \u03c4)) = c (40)\nwhere c is a constant.\nLemma 3.1.1: The state of the neuron can be written as follows:\nri(t) = NR\n\u2211 k=0\nNR\n\u2211 n=1\n\u03bbkn \u27e8v \u22121 n ,w in\u27e9 (vn)i x(t \u2212 k) (41)\nwhere vn,v\u22121n \u2208V are, respectively, the left and right eigenvectors of W, and \u03bb k n \u2208 \u03bb belongs to the diagonal matrix containing the eigenvalues of W; ai = [ai,0, ai,1, . . .] represents the coefficients that the previous inputs xt = [x(t), x(t \u2212 1), . . .] have on ri(t).\nProof: We build on the work of Aceituno et al.Aceituno et al. (2020) where they showed that higher heterogeneity among the neuronal states implies higher memory capacity. Here we aim to show that as the number of neurons NR in the recurrent layer decreases, heterogeneity increases the spectral radius. More formally, the spectral radius \u2223\u03bbn\u2223 is directly proportional to H as NR decreases. We express the state of a neuron ri(t) as\nri(t) = \u221e \u2211 k=0 (W kwin) i x(t \u2212 k) = \u221e \u2211 k=0 ai,kx(t \u2212 k) = \u27e8ai,xt\u27e9 (42)\nwhere the vector ai = [ai,0, ai,1, . . .] represents the coefficients that the previous inputs xt = [x(t), x(t \u2212 1), . . .] have on ri(t). We can then plug this into the covariance between two neurons,\nCov (ri, rj) = lim T\u2192\u221e\n1\nT t+T \u2211 q=t \u27e8ai,xq\u27e9 \u27e8aj ,xq\u27e9\n= \u27e8ai,aj\u27e9 lim T\u2192\u221e\n1\nT\nT\n\u2211 qi=0\nT\n\u2211 qj=0 \u27e8xqi ,xqj \u27e9\n= \u27e8ai,aj\u27e9 lim T\u2192\u221e\n1\nT\nT\n\u2211 q=0 \u27e8xq,xq\u27e9\n= \u27e8ai,aj\u27e9 \u00d7E [x2(t)] = \u27e8ai,aj\u27e9 (43)\nNow we write ai as a function of the eigenvalues of W. Using the eigenvalue decomposition of the weight matrix W, we rewrite the state of the neuron as follows:\nri(t) = NR\n\u2211 k=0\nNR\n\u2211 n=1\n\u03bbkn \u27e8v \u22121 n ,w in\u27e9 (vn)i x(t \u2212 k) (44)\nwhere vn,v\u22121n \u2208V are, respectively, the left and right eigenvectors of W, and \u03bb k n \u2208 \u03bb belongs to the diagonal matrix containing the eigenvalues of W; ai = [ai,0, ai,1, . . .] represents the coefficients that the previous inputs xt = [x(t), x(t \u2212 1), . . .] have on ri(t). \u220e\nTheorem 1: If the memory capacity of the HRSNN and MRSNN networks are denoted by CH and CM respectively, then, CH \u2265 CM , where the heterogeneity in the neuronal parametersH varies inversely to the correlation among the neuronal states measured as \u2211NRn=1\u2211 NR m=1Cov 2 (xn(t), xm(t)) which in turn varies inversely with C.\nProof: As shown by Aceituno et al.Aceituno et al. (2020), the memory capacity increases when the variance along the projections of the input into the recurrent layer state has higher heterogeneity. This can be expressed in terms of the state space of the recurrent layer. Now, we aim to project the inputs into orthogonal directions of the network state space. Thus, we model the system as\nr(t) = \u221e \u2211 \u03c4=1 a\u03c4x(t \u2212 \u03c4) + \u03b5(t) (45)\nwhere the vectors a\u03c4 \u2208 RN correspond to the linearly extractable effect of x(t \u2212 \u03c4) onto r(t) and \u03b5(t) is the nonlinear contribution of all the inputs onto the state of r(t).\nSince our goal is to have a variance as homogeneous as possible along with the directions of a\u03c4 , we need a variance that is as homogeneous along with orthogonal directions, where the vectors a\u03c4 \u2208 RN correspond to the linearly extractable effect of the input variable x(t) onto the states of the neurons (r(t)). Since the eigenvectors of \u03a3 preserve orthogonality across the covariance matrix \u03a3, the new variances are given by the eigenvalues of the covariance matrix, \u03bbn(\u03a3). Thus, we work on the distribution of the eigenvalues of the covariance matrix. Specifically, we want to show that increasing the heterogeneity in the neuronal membrane time constants decreases the correlation between the neuron states, which decreases the variance of the neuronal states of the eigenvalues, which would increase the memory capacity C. We quantify the heterogeneity using the mean with respect to the square root of the raw variance of the eigenvalues of the covariance matrix given by\nJ = \u2211\nNR n=1 \u03bb 2 n(\u03a3)\n(\u2211 NR n=1 \u03bbn(\u03a3))\n2 (46)\nwhere \u03bbn(\u03a3) is the nth eigenvalue of \u03a3. To get an intuition of how this metric reflects the heterogeneity in the neuronal parameters, consider the case of two eigenvalues \u03bb1, \u03bb2; when \u03bb1 = \u03bb2-very homogeneous \u2212 then J = 1\n2 , but when \u03bb1 > 0, \u03bb2 = 0\u2212 heterogeneity is more and hence, J = 1. The\nmembrane time constant is given by the product of the membrane resistance Rm and membrane capacitance Cm, such that \u03c4m = RmCm. Rm is the inverse of the permeability; the higher the permeability, the lower the resistance, and vice versa. Thus, the lower the time constant, the faster or more rapidly a membrane will respond to a stimulus. The effects of the time constant on propagation velocity will become clear below. Hence, variability in the membrane time constants will lead to variability in the propagation velocity of action potentials.\nNow,\n( NR\n\u2211 n=1\n\u03bbn(\u03a3))\n2\n= (tr[\u03a3])2 = ( NR\n\u2211 n=1\nVar (rn(t))) 2 (47)\nwhich is constant by the assumption that the probability distributions of the neuron activities are fixed. Hence we can focus on the value of \u2211NRn=1 \u03bb 2 n(\u03a3) which is true since\n\u03a3ken(\u03a3) = \u03bbn(\u03a3)\u03a3 k\u22121en(\u03a3) = \u03bbkn(\u03a3)en(\u03a3)\u21d2\nNR\n\u2211 n=1\n\u03bb2n(\u03a3) = tr [\u03a3 2] (48)\nwhere en(\u03a3) and \u03bbn(\u03a3) are, resp. the nth eigenvector and eigenvalue of \u03a3. Hence, we can compute this by decomposing the square of the covariance matrix as follows:\nNR\n\u2211 n=1\n\u03bb2n(\u03a3) = NR\n\u2211 n=1\nNR\n\u2211 m=1\n\u03a3nm\u03a3mn = NR\n\u2211 n=1\nNR\n\u2211 m=1\nCov2 (xn(t), xm(t)) (49)\nwhere \u03a3ij are the factor matrices obtained using Cholesky decomposition of \u03a3. Thus, \u2211NRn=1 \u03bb 2 n(\u03a3) increases as the neurons become more correlated; hence heterogeneity decreases.\nThus, from Eqs. 46, 49 we can write the heterogeneity as inversely proportional to \u2211 NR n=1Cov 2 (xn(t), xm(t)). We see that increasing the correlations between neuronal states decreases the heterogeneity of the eigenvalues, which would reduce the memory capacity of the model. We show that the determinant of the covariance between neuronal parameters bounds the heterogeneity. Thus, as H increases \u2192 covariance decreases \u2192 neurons become less correlated. Aceituno et al.Aceituno et al. (2020) proved that the neuronal state correlation is inversely related to the memory capacity of the network. Hence, we claim that asH increases, the memory capacity C also increases. Hence, for HRSNN, withH > 0, CH \u2265 CM . \u220e"
        },
        {
            "heading": "B.5 SPIKING EFFICIENCY",
            "text": "In this section, we model the spiking activity using a point process called the multivariate Point process model. A point process is a collection of random points on some underlying mathematical space, such as the real line, the Cartesian plane, or more abstract spaces.\nThe notion of using point process models, especially the interactive Hawkes processes, to model the spiking dynamics of LIF network dynamics has been studied in the literature previously (L\u00f6cherbach,\n2017; Galves & L\u00f6cherbach, 2016; Mascart, 2021; Pfaffelhuber et al., 2022). We leverage these results to prove that heterogeneity in the synaptic dynamics can help reduce the spike count, as already discussed in the paper. We highlighted the key assumptions used in deriving the results in the Suppl. Sec. C. We apologize if there is still confusion, and we will add more in-depth discussion in the final manuscript as discussed below. In their paper, Locherbach et al.L\u00f6cherbach (2017) survey some aspects of the study of Hawkes processes in high dimensions to model biological neural systems and study their long-term behavior. Galves et al.Galves & L\u00f6cherbach (2016) provided an overview of point processes used as stochastic models for interacting neurons in discrete and continuous time. Similarly, Hawkes processes have met a recent interest in the mathematical neuroscience literature for their ability to model the dependence of a neuron\u2019s activity in the network\u2019s history (Mascart, 2021; Pfaffelhuber et al., 2022; Galves & L\u00f6cherbach, 2016; Gerhard et al., 2017; Zhou et al., 2020; Duval et al., 2022). Other works have also used a nonlinear interactive Hawkes process to model spiking neural networks with excitatory and inhibitory neurons (Chevallier et al., 2015; Chornoboy et al., 1988; Hansen et al., 2015; Reynaud-Bouret et al., 2014). Drawing from these works, we use a microscopic model describing a large network of interacting neurons that can generate oscillations in a macroscopic frame. In the model, the activity of each neuron is represented by a point process indicating the successive times at which the neuron emits a spike, where each realization of this point process is the spike train. We take the spiking intensity of a neuron as the probability of emitting a spike during the next instant, depending on the history of the neuron and the activity of other neurons in the network. The neurons interact through their synapses. This means that a spike of a pre-synaptic neuron leads to an increase of the membrane potential of the post-synaptic neuron if the synapse is excitatory or a decrease if the synapse is inhibitory, possibly after some delay, like the process of synaptic integration. The neuron fires a spike when the membrane potential reaches a certain upper threshold. Thus, excitatory inputs from the neurons in the network increase the firing intensity, and inhibitory inputs decrease it. Hawkes processes provide good models of this synaptic integration phenomenon by the structure of their intensity processes. This paper uses a general class of mean-field interacting Hawkes processes, modeling the reciprocal interactions between a population of excitatory neurons and a population of inhibitory neurons.\nLet us consider a subsection of the HRSNN network as shown in Fig. 11 denoted by Nx. We use the multivariate Point process model to create a probabilistic model that relates the inner structure of the sub-network and its spiking activity. In this model, each neuron i has a background spiking intensity \u03bdi caused by neurons outside the network. We know that when a neuron spikes, it impacts its spiking activity and the spiking activity of its output neurons. The impact of a neuron j on neuron i is modeled by a real function hj\u2192i(t). This impact can be excitatory or inhibitory depending on whether the pre-synaptic neuron is excitatory or inhibitory, as shown in Fig. 11. While the spikes from excitatory neurons try to excite another spike, spikes originating from inhibitory neurons try to inhibit the spiking of the cascading neuron.\nA Hawkes process is a point process in which each point is commonly associated with event occurrences in time, where every event time impacts the probability that other events will take place subsequently. These processes are characterized by the conditional intensity function, seen as an instantaneous measure of the probability of event occurrences. A Hawkes process is a point process in which each point is commonly associated with event occurrences. In this past-dependent model, every event time impacts the probability that other events take place subsequently. These processes are characterized by the conditional intensity function, seen as an instantaneous measure of the probability of event occurrences. Although the self-exciting Hawkes process remains widely studied, there has been a growing interest in modeling the opposite effect, known as inhibition, in which the apparition of certain events lowers the probability of observing an event. In practice, this amounts to considering negative kernel functions. To maintain the positivity of the intensity function, a non-linear operator is added to the expression, which in turn entails the loss of the cluster representation. This model is known as the non-linear Hawkes process, where the existence of such processes was proved via construction using bi-dimensional marked Poisson processes. The general Hawkes framework can be written as:\n\u03bbit = \u03a6i \u239b\n\u239d \u2211 j\u2208Si,E \u222b\nt\n0 hj\u2192i(t \u2212 u)dZju\n\u239e \u23a0 , (50)\nwhere \u03bbit is the intensity of neuron i,\u03a6i a positive function, Zj,t is the counting process associated with neuron j, hj\u2192i(t) is the synaptic kernel associated with the synapse between neurons j and i.\nTo simplify the notation, we can rewrite Eq. 50 as\n\u03bbi(t) = \u03a6i (\u2211 k\u2208I \u222b(0,t) hki(t \u2212 s)dZk(s)) . (51)\nwhere hik(t \u2212 s) measures the influence of neuron k on neuron i and how this influence vanishes with the time. More precisely, hik(t \u2212 s) describes how a spike of neuron k lying back t \u2212 s time units in the past influences the present spiking rate at time t.\nThe goal of using heterogeneity in the STDP dynamics is to get better orthogonalization among the recurrent network states to lower higher-order correlations in spike trains. Studies have shown that the correlation of higher order progressively decreases the information available through neural population (Montani et al., 2009; Abbott & Dayan, 1999). Since we are trying to engineer a spikeefficient model, we leverage the heterogeneity in the STDP dynamics to reduce the higher-order correlations. The hypothesis is that using heterogeneity in STDP helps us orthogonalize the recurrent layer that can help us achieve an efficient representation of the input spike patterns with fewer spikes. This may be interpreted as the recurrent layer acting as an orthogonal bases function where inputs are projected onto these bases. Thus, having orthogonal bases can efficiently map inputs without much loss. While heterogeneous LIF neurons help us increase the number of principal components, thereby enabling us to store a greater subclass of features, heterogeneous STDP helps us efficiently encode this orthogonalization of the recurrent layer, resulting in fewer spikes compared to a homogeneous RSNN. Thus, in effect, heterogeneous STDP parameters can learn the output more precisely, which is projected back into the recurrent network. One of the primary reasons why heterogeneous STDP helps project the input to orthogonal activations of the recurrent network can be attributed to the distribution of LTD dynamics, as this increases the competition and helps distribute the input projection to multiple principal components. We discuss that the heterogeneous LTP/LTD dynamics in STDP lead to fewer spikes in the transmission of information.\nLemma 3.2.1: If the neuronal firing rate of the HRSNN network with only heterogeneity in LTP/LTD dynamics of STDP is represented as \u03a6R and that of MRSNN represented as \u03a6M , then the HRSNN model promotes sparsity in the neural firing which can be represented as \u03a6R < \u03a6M .\nProof: In this lemma, we show that the average firing rate of the model with heterogeneous STDP (LTP/LTD) dynamics (averaged over the population of neurons) is lesser than the corresponding average neuronal activation rate for a model with homogeneous STDP dynamics. We prove this by taking a sub-network of the HRSNN model as illustrated by Fig. 11. Now, we model the input spike trains of the pre-synaptic neurons using a multivariate interactive, nonlinear Hawkes process with multiplicative inhibition (Duval et al., 2022).\nWe consider a population of neurons of size N that is divided into population A (excitatory) with size NA \u2236= \u03b1N and a population B (inhibitory) with size NB = (1 \u2212 \u03b1)N . A particular instance of the model is then given in terms of a family of counting processes (Z1t , . . . , Z NA t ) (population A) and (ZNA+1t , . . . , Z N t ) (population B ) with coupled conditional stochastic intensities given respectively by \u03bbA and \u03bbB . Consider on a filtered probability space (\u2126,F , (Ft)t\u22650 ,P) an independent family of i.i.d. Poisson measures (\u03c0i( ds, dz), i \u2208 {1, . . . ,N}) with intensity measure ds \u00d7 dz on [0,\u221e) \u00d7 [0,\u221e). Let (x, y)\u21a6 F (x, y) and (x, y)\u21a6 G(x, y) two nonnegative functions defined on (0,\u221e)2.\nWe assume that F and G satisfy\nF (x, y) = \u03a6A(x)\u03a6B\u2192A(y),G(x, y) = \u03a6B(x) +\u03a6A\u2192B(y),\nwhere \u03a6A,\u03a6B\u2192A,\u03a6B and \u03a6A\u2192B are nonnegative functions, each of them globally Lipschitz with \u03a6B\u2192A bounded (and with no loss of generality we assume 0 \u2264 \u03a6B\u2192A \u2264 1 ).\nLet us consider the family of c\u00e0dl\u00e0g (Ft)t\u22650 point processes (Z i t)t\u22650,i=1,...,N given by\nZit = \u222b t\n0 \u222b\n\u221e\n0 1z\u2a7d\u03bbis\u03c0i( ds, dz), i = 1, . . . ,N,\nwhere the intensity \u03bbi, i = 1, . . . ,N , is given as:\n\u03bbA,Nt \u2236= \u03a6A \u239b\n\u239d\n1\nN \u2211 j\u2208A \u222b\nt\u2212\n0 h1(t \u2212 u)dZ\nj u\n\u239e \u23a0 \u03a6B\u2192A \u239b \u239d 1 N \u2211 j\u2208B \u222b\nt\u2212\n0 h2(t \u2212 u)dZ\nj u\n\u239e \u23a0 (52)\n\u03bbB,Nt \u2236= \u03a6B \u239b\n\u239d\n1\nN \u2211 j\u2208B \u222b\nt\u2212\n0 h3(t \u2212 u)dZ\nj u\n\u239e \u23a0 +\u03a6A\u2192B \u239b \u239d 1 N \u2211 j\u2208A \u222b\nt\u2212\n0 h4(t \u2212 u)dZ\nj u\n\u239e \u23a0 (53)\n, where A&B are the populations of the excitatory and inhibitory neurons, respectively.\nThe dynamics given by Eq. 53 is of Hawkes type: each particle\u2019s intensity depends on the whole system\u2019s history, through memory kernels hi, i = 1, . . . ,4 and firing rate functions \u03a6A and \u03a6B . The multiplicative influence of inhibitory population B onto population A, is represented using the inhibition kernel \u03a6B\u2192A which is a decreasing nonnegative function on [0,+\u221e), with \u03a6B\u2192A(0) = 1 and \u03a6B\u2192A(x) \u00d0\u2192\nx\u2192\u221e 0- i.e., activity of population A should decrease as activity of population B rises. The model secondly incorporates retroaction from population A onto population B, which is supposed to be mostly additive, although possibly modulated by a nonlinear feedback kernel \u03a6A\u2192B .\nNow, without loss of generality we assume that \u03a6A and \u03a6B are linear - i.e., \u03a6A(x) = \u00b5A+x,\u03a6B(x) = \u00b5B + x,x \u2265 0, where \u00b5A, \u00b5B \u2265 0, and hi \u2265 0 for i = 1, . . . ,4.\nHence, Eq. 53 becomes\n\u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 \u03bbAt = (\u00b5A + \u03b1 \u222b t 0 h1(t \u2212 u)\u03bb A u du)\u03a6B\u2192A ((1 \u2212 \u03b1) \u222b t 0 h2(t \u2212 u)\u03bb B u du) , \u03bbBt = \u00b5B + (1 \u2212 \u03b1) \u222b t 0 h3(t \u2212 u)\u03bb B u du +\u03a6A\u2192B (\u03b1 \u222b t 0 h4(t \u2212 u)\u03bb A u du) .\n(54)\nFor heterogeneous neuron populations, there exists an asymmetry of the weights. Based on balanced spiking neural networks with heterogeneous connection strengths, previous works have revealed that such heterogeneous networks possess heavy-tailed L\u00e9vy fluctuations (Shlesinger et al., 1987; Mantegna & Stanley, 1995; Cossell et al., 2015). The heterogeneous heavy-tailed distributions of synaptic weights have been fitted to lognormal distributions (Buzs\u00e1ki & Mizuseki, 2014; Kus\u0301mierz et al., 2020). We model the inputs to neuron i \u2208 E as:\nli(t) =WEX\u03c4mE \u2211 j\u2208X cijsj(t) +WEE\u03c4mE \u2211 j\u2208E cijsj(t) \u2212WEI\u03c4mE\u2211 j\u2208I cijsj(t) (55)\n= \u00b51E +\u2206\u00b5i + \u03b7i(t) (56)\nwhere \u00b5 denotes the mean inputs such that \u00b5E = K\u03c4mE (WEXrX +WEErE \u2212WEIrl); \u2206\u00b5i = \u2019quenched\u2019 fluctuations (from neuron to neuron) with variance \u27e8\u2206\u00b52\u27e9E = K\u03c42mE (W 2 EX (r 2 X +\u2206r 2 X) +W 2 EE (r 2 E +\u2206r 2 E) +W 2 EI (r 2 I +\u2206r 2 I)) due to random connectivity. Finally, \u03b7i denotes temporal fluctuations due to spiking activity. We assume that the pre-synaptic neurons fire as using the interactive Hawkes process described above.\nConsider a case where \u00b5A \u226b 1, \u00b5B = 0 and h = 1[0,\u03b8] \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 \u03bbAt \u2236= (\u00b5A + \u03b1 \u222b t 0 h1(t \u2212 u)d\u03bb A u )\u03a6B\u2192A ((1 \u2212 \u03b1) \u222b t 0 h2(t \u2212 u)d\u03bb B u ) , \u03bbBt \u2236= (1 \u2212 \u03b1) \u222b t 0 h3(t \u2212 u)d\u03bb B u + \u03b1 \u222b t 0 h4(t \u2212 u)d\u03bb A u .\n(57)\nIn a normal case, the excitatory and inhibitory populations follow the following steps: (1) t \u2248 0, \u03bbAt \u2248 \u00b5A is high and \u03bbBt \u2248 0 is small (2) Feedback from A to B \u2236 \u03bb B t increases (3) Inhibition of B to A : when \u03bbBt gets high, \u03a6B\u2192A reduces \u03bb A t (4) h4 has compact support: after a time \u03b84,B no longer feels the influence of A : intensity of B is back to \u00b5B \u2248 0 and A to its normal high activity \u00b5A (State 1)\nThis leads to oscillations which lead to spikes. However, heterogeneity in the synaptic dynamics increases the stochasticity of the pre-synaptic spike arrival. Thus, due to the heterogeneity, \u03a6B\u2192A promotes the system in the inhibition state (state 3) and inhibits the system\u2019s movement to system 4 and system 1, thereby creating a spike. Hence, \u03a6AR < \u03a6 A M . Similarly, for the inhibitory neurons, we can show that \u03a6BR < \u03a6 B M . Thus, we get \u03a6R < \u03a6M \u220e\nThis lemma might be interpreted as the heterogeneous STDP dynamics increasing the synaptic noise, which reduces the number of spikes of the post-synaptic neuron. A heterogeneous STDP leads to a non-uniform scaling of correlated spike trains leading to de-correlation. Hence, we can say that heterogeneous STDP models have learned a better-orthogonalized subspace representation, leading to a better encoding of the input space with fewer spikes.\nIt is to be mentioned here that the synaptic noise might be thought of as analogous to the stochasticity in the gradient descent algorithm. As recently proved by Simsekli et al. (Simsekli et al., 2020; 2019), stochasticity plays an important role in the generalization ability of the model. We might interpret the synaptic noise in the heterogeneous STDP to play a similar role and helps in better generalizability of the HRSNN model. This hypothesis is empirically proven in Supplementary Section A. However, a detailed theoretical analysis would be a very interesting direction for future work.\nTheorem 2: For a given number of neurons NR, the spike efficiency of the model E = C(NR)\nS for\nHRSNN (ER) is greater than MRSNN (EM ) i.e., ER \u2265 EM\nProof: To study the effect of the spike time when the weight wk changes, we look into the expected value of the time difference in the post-synaptic spikes, which is given as:\nE [\u2206tpost ] = E [tpost \u2212 tpost ] = (E [tpost ] \u2212 tpost )Pr[s] (58)\nwhere Pr[\u2203s] is the probability of occurrence of the post-synaptic spike. Thus, the expected input to the neuron at time t(E [i(t)]), which comprises of its excitatory and inhibitory components E [ie(t)] ,E [ii(t)] can be expressed as:\nE [\u2206i(t)] =\u2206E [ie(t)] \u2212\u2206E [ii(t)] for t < tpost (59)\nwhere E [ie(t)] = \u03c1e \u222b \u221e\n0 \u00b5we(w, t)dw ; E [ii(t)] = \u03c1i \u222b\n\u221e\n0 \u00b5wi(w, t)dw (60)\nwhere \u03c1e, \u03c1i are the rates of incoming spikes and \u00b5we(w, t), \u00b5wi(w, t) the probabilities of the weights associated to time t. Now, considering the case for RSNNs with homogeneous STDP (M ) and with heterogeneous STDP (R), the difference in the variances of the two populations is given as:\n\u2206Var[VM ] \u2212\u2206Var[VR] =\u2206\u222b t\n\u2212\u221e [E [i2M(t)] \u2212 (E [i 2 R(t)] \u2212E[iR(t)] 2)]dt (61)\nSince t < tpost , STDP potentiates both inhibitory and excitatory synapses, so \u2206E [i2i (t)] > 0,\u2206E [i2e(t)] > 0. The term E[iM(t)]2 = 0 by the symmetry of the weights, and it is maintained at zero by the symmetry of the STDP. But for heterogeneous neuron populations, as described above, there exists an asymmetry of the weights. Based on balanced spiking neural networks with heterogeneous connection strengths, previous works have revealed that such heterogeneous networks possess heavy-tailed, L\u00e9vy fluctuations (Shlesinger et al., 1987; Mantegna & Stanley, 1995; Cossell et al., 2015). This implies E[iR(t)]2 > 0\u21d2\u2206Var[VR] <\u2206Var[v(t)M ]We calculate the number of post-synaptic spikes triggered when the stimulus is present. Now, representing the spike rate of the HRSNN and the MRSNN as \u03a6R,\u03a6M resp.,\n\u222b\nt\n0 \u03a6R(t)dt \u2264 \u222b\nt\n0 \u03a6M(t)\u21d2 SR = NR\nT\nt\u0302ISIR \u2264 NR\nT\nt\u0302ISIM = SM (62)\nThus, spikes decrease when we use heterogeneity in the LTP/LTD Dynamics. Hence, we compare the efficiencies of the HRSNN with that of MRSNN as follows:\nER EM = MR(NR) \u00d7 SM SR \u00d7MM(NR) =\n\u2211 NR \u03c4=1 Cov2(x(t\u2212\u03c4),aR\u03c4 rR(t)) Var(aR\u03c4 rR(t)) \u00d7 \u221e \u222b\ntref\nt\u03a6Rdt\n\u2211 NR \u03c4=1 Cov2(x(t\u2212\u03c4),aM\u03c4 rM (t)) Var(aM\u03c4 rM (t)) \u00d7\n\u221e \u222b tref t\u03a6Mdt\n(63)\nSince SR \u2264 SM and also,the covariance increases when the neurons become correlated, and as neuronal correlation decreases,H increases (Theorem 1), we see that EREM \u2265 1\u21d2 ER \u2265 EM \u220e"
        },
        {
            "heading": "C SUPPLEMENTARY SECTION C",
            "text": ""
        },
        {
            "heading": "C.1 HIGHER ORDER CORRELATION",
            "text": "In this paper, we took inspiration from results in reservoir computing, which show that we can maximize memory capacity using orthogonalization among reservoir states in the case of reservoir computers (Farka\u0161 & Gergel\u2019, 2017; Farka\u0161 et al., 2016). The goal of using heterogeneous STDP dynamics is to get better orthogonalized recurrent network states to achieve more efficient information transfer with lower higher-order correlations in spike trains. Recent studies (Montani et al., 2009; Abbott & Dayan, 1999) have shown that the correlation of higher order progressively decreases the information available through the neural population. The decrease in information becomes larger as the interaction order grows. Since we are trying to engineer a spike-efficient model, we leverage the heterogeneity in neuronal parameters to reduce the higher-order correlations. The hypothesis is that an orthogonal recurrent layer can help us efficiently represent the input spike patterns with fewer spikes. This may be interpreted as the recurrent layer acting as an orthogonal bases function where the inputs are projected onto these bases. Thus, having orthogonal bases can efficiently map the inputs without much loss. The heterogeneous STDP helps us efficiently achieve this orthogonalization of the recurrent layer, resulting in a lesser voltage variance across the neuron population. This leads to fewer spikes (since the mean is constant) compared to a homogeneous RSNN. Thus, in effect, heterogeneous STDP parameters can learn the output more precisely, which is projected back into the recurrent network. Hence, using heterogeneous STDP parameters leads to a better orthogonalization among the neuronal states and hence, a higher C.\nIn this paper, we show that using a distribution of LTP/LTD dynamics in the STDP parameters helps us in mappings the input onto the orthogonal activations of the recurrent network to capture the principal components of the input signal. The LTD dynamics play an important role in determining the orthogonality of neuronal activations. LTD windows of the STDP rules enable robust sequence learning amid background noise in cooperation with a large signal transmission delay between neurons and a theta rhythm (Hayashi & Igarashi, 2009). The LTD window in the range of positive spike-timing plays an important role in preventing noise influences with sequence learning. Oja (Oja, 1982; 1989) showed that the LIF neuron\u2019s time constant is very fast compared to the time constant of learning in which the weights wji change. The learning is assumed to take place according to the STDP type conjunction of the inputs \u03bei and the integrated effect of the inputs, \u03bdj , with an additional forgetting term attributed to the LTD dynamics: dwji\ndt = \u03b1\u03bdj\u03bei \u2212 f (\u03bdj , \u03bei,wji) In the\ncase of homogeneous STDP, f(.) is a constant; hence, the model can only efficiently learn the first principal component of the input. However, quite interesting functions emerge when considering STDP to have a distribution. This also helps us determine the next principal components other than the first one. Hence the diversity in the different LTD dynamics increases the competition and helps that not all inputs are mapped to the first principle component. Thus, the diversity in the LTD dynamics helps in projecting the input to orthogonal activations of the recurrent network.\nNow, for homogeneous RSNNs, several higher-order correlations, which according to our hypothesis, arise because of the poor orthogonalization among the network states. This results in the redundancies of spikes for encoding the same information. In this paper, we use heterogeneous STDP dynamics to learn an efficient orthogonal representation of the state space, which result in the network learning the same patterns but using fewer spikes. (theorem: 3) We also show that heterogeneity in the neuronal parameters decreases the neuronal correlation (theorem 1 And fig 2a). Thus, since heterogeneity results in better orthogonalization among the neuronal states, it results in fewer higher-order correlations. Moreover, recent studies have shown that the correlation of higher order progressively\ndecreases the information available through the neural population, and the decrease in information becomes larger as the interaction order grows. Since we are trying to engineer an efficient model, we aim to reduce the higher-order correlations using heterogeneity in neuronal parameters (as shown in Theorem 1). In addition to this, to verify this, we used CuBIC (Staude et al., 2010), a cumulantbased inference of higher-order correlations in massively parallel spike trains. The details of the experimental methodology are given in Supplementary Section C. The outcome of CuBIC is a lower bound \u03be\u0302 on the order of correlation in the spiking activity of large groups of simultaneously recorded neurons. CuBIC can provide statistical evidence for large correlated groups without the discouraging requirements on a sample size that direct tests for higher-order correlations have to meet. This is achieved by exploiting constraining relations among correlations of different orders. However, it must be noted that CuBIC is not designed to estimate the order of correlation directly; the inferred lower bound might not always correspond to the maximal order of correlation present in a given data set."
        }
    ],
    "year": 2023
}