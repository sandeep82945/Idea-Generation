{
    "abstractText": "In the field of Automated Planning there is often the need for a set of planning problems from a particular domain, e.g., to be used as training data for Machine Learning or as benchmarks in planning competitions. In most cases, these problems are created either by hand or by a domain-specific generator, putting a burden on the human designers. In this paper we propose NeSIG, to the best of our knowledge the first domain-independent method for automatically generating planning problems that are valid, diverse and difficult to solve. We formulate problem generation as a Markov Decision Process and train two generative policies with Deep Reinforcement Learning to generate problems with the desired properties. We conduct experiments on several classical domains, comparing our method with handcrafted domain-specific generators that generate valid and diverse problems but do not optimize difficulty. The results show NeSIG is able to automatically generate valid problems of greater difficulty than the competitor approaches, while maintaining good diversity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Carlos N\u00fa\u00f1ez-Molina"
        },
        {
            "affiliations": [],
            "name": "Juan Fern\u00e1ndez-Olivares"
        }
    ],
    "id": "SP:d69d75300ab87189abbe234fe1dc6a8da638186e",
    "references": [
        {
            "authors": [
                "Barcel\u00f3 et al",
                "2020] Pablo Barcel\u00f3",
                "Egor V Kostylev",
                "Mikael Monet",
                "Jorge P\u00e9rez",
                "Juan Reutter",
                "Juan-Pablo Silva"
            ],
            "title": "The logical expressiveness of graph neural",
            "year": 2020
        },
        {
            "authors": [
                "Blai Bonet",
                "H\u00e9ctor Geffner. Planning as heuristic search"
            ],
            "title": "Artificial Intelligence",
            "venue": "129(12):5\u201333,",
            "year": 2001
        },
        {
            "authors": [
                "Isabel Cenamor",
                "Alberto Pozanco"
            ],
            "title": "Insights from the 2018 ipc benchmarks",
            "venue": "ICAPS 2019 Workshop on the International Planning Competition (WIPC), pages 8\u201314,",
            "year": 2019
        },
        {
            "authors": [
                "Eldan Cohen",
                "J Christopher Beck"
            ],
            "title": "Problem difficulty and the phase transition in heuristic search",
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Honghua Dong",
                "Jiayuan Mao",
                "Tian Lin",
                "Chong Wang",
                "Lihong Li",
                "Denny Zhou"
            ],
            "title": "Neural logic machines",
            "venue": "arXiv preprint arXiv:1904.11694,",
            "year": 2019
        },
        {
            "authors": [
                "Fawcett et al",
                "2014] Chris Fawcett",
                "Mauro Vallati",
                "Frank Hutter",
                "J\u00f6rg Hoffmann",
                "Holger H Hoos",
                "Kevin LeytonBrown"
            ],
            "title": "Improved features for runtime prediction of domain-independent planners",
            "venue": "In Twenty-Fourth International Conference on Automated Planning and Scheduling,",
            "year": 2014
        },
        {
            "authors": [
                "Alan Fern",
                "Sung Wook Yoon",
                "Robert Givan. Learning domain-specific control knowledge from random walks"
            ],
            "title": "In ICAPS",
            "venue": "pages 191\u2013199,",
            "year": 2004
        },
        {
            "authors": [
                "Raquel Fuentetaja",
                "Tom\u00e1s De la Rosa"
            ],
            "title": "A planning-based approach for generating planning problems",
            "venue": "Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Malik Ghallab",
                "Dana Nau",
                "Paolo Traverso"
            ],
            "title": "Automated planning and acting",
            "venue": "Cambridge University Press,",
            "year": 2016
        },
        {
            "authors": [
                "Patrik Haslum",
                "Nir Lipovetzky",
                "Daniele Magazzeni",
                "Christian Muise. An introduction to the planning domain definition language"
            ],
            "title": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
            "venue": "13(2):1\u2013187,",
            "year": 2019
        },
        {
            "authors": [
                "Malte Helmert",
                "Carmel Domshlak"
            ],
            "title": "Landmarks",
            "venue": "critical paths and abstractions: what\u2019s the difference anyway? In Nineteenth International Conference on Automated Planning and Scheduling,",
            "year": 2009
        },
        {
            "authors": [
                "Malte Helmert. The fast downward planning system"
            ],
            "title": "Journal of Artificial Intelligence Research",
            "venue": "26:191\u2013246,",
            "year": 2006
        },
        {
            "authors": [
                "J\u00f6rg Hoffmann"
            ],
            "title": "Ff: The fast-forward planning system",
            "venue": "AI magazine, 22(3):57\u201357,",
            "year": 2001
        },
        {
            "authors": [
                "Chad Hogg",
                "H\u00e9ctor Munoz-Avila",
                "Ugur Kuter"
            ],
            "title": "Htn-maker: Learning htns with minimal additional knowledge engineering required",
            "venue": "AAAI, pages 950\u2013956,",
            "year": 2008
        },
        {
            "authors": [
                "Michael Katz",
                "Shirin Sohrabi"
            ],
            "title": "Generating data in planning: Sas planning tasks of a given causal structure",
            "venue": "HSDIP 2020, page 41,",
            "year": 2020
        },
        {
            "authors": [
                "Ofir Marom",
                "Benjamin Rosman. Utilising uncertainty for efficient learning of likelyadmissible heuristics. In ICAPS"
            ],
            "title": "volume 30",
            "venue": "pages 560\u2013 568,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas L McCluskey",
                "Tiago S Vaquero",
                "Mauro Vallati"
            ],
            "title": "Engineering knowledge for automated planning: Towards a notion of quality",
            "venue": "Proceedings of the Knowledge Capture Conference, pages 1\u2013 8,",
            "year": 2017
        },
        {
            "authors": [
                "Hanna M Pasula",
                "Luke S Zettlemoyer",
                "Leslie Pack Kaelbling. Learning symbolic models of stochastic domains"
            ],
            "title": "Journal of Artificial Intelligence Research",
            "venue": "29:309\u2013352,",
            "year": 2007
        },
        {
            "authors": [
                "Silvia Richter",
                "Matthias Westphal"
            ],
            "title": "The lama planner: Guiding cost-based anytime planning with landmarks",
            "venue": "Journal of Artificial Intelligence Research, 39:127\u2013177,",
            "year": 2010
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Jos\u00e9 \u00c1 Segura-Muros",
                "Ra\u00fal P\u00e9rez",
                "Juan Fern\u00e1ndez-Olivares. Discovering relational",
                "numerical expressions from plan traces for learning action models"
            ],
            "title": "Applied Intelligence",
            "venue": "51(11):7973\u20137989,",
            "year": 2021
        },
        {
            "authors": [
                "W M Shen",
                "H A Simon"
            ],
            "title": "Rule creation and rule learning through environmental exploration",
            "venue": "IJCAI, pages 675\u2013680. Morgan Kaufmann",
            "year": 1989
        },
        {
            "authors": [
                "William Shen",
                "Felipe Trevizan",
                "Sylvie Thi\u00e9baux. Learning domain-independent planning heuristics with hypergraph networks. In ICAPS"
            ],
            "title": "volume 30",
            "venue": "pages 574\u2013584,",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Alvaro Torralba",
                "Jendrik Seipp",
                "Silvan Sievers. Automatic instance generation for classical planning. In Proceedings of the International Conference on Automated Planning",
                "Scheduling"
            ],
            "title": "volume 31",
            "venue": "pages 376\u2013384,",
            "year": 2021
        },
        {
            "authors": [
                "Mauro Vallati",
                "Lukas Chrpa",
                "Marek Grze\u015b",
                "Thomas Leo McCluskey",
                "Mark Roberts",
                "Scott Sanner"
            ],
            "title": "et al",
            "venue": "The 2014 international planning competition: Progress and trends. Ai Magazine, 36(3):90\u201398,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automated Planning (AP) [Ghallab et al., 2016] is a subfield of Artificial Intelligence devoted to providing goal-oriented, deliberative behaviour to both physical and virtual agents, e.g. robots and video game automated players. An automated planner receives as input the description of the planning task to solve, containing the environment dynamics, initial state and goal. It then carries out a search process in order to find a plan (sequence of actions) which achieves the task goal starting from its initial state. Planning tasks are usually described in a declarative, first-order logic (FOL) language such as PDDL (Planning Domain Definition Language) [Haslum et al., 2019]. The PDDL description consists of a planning domain, containing the environment dynamics, and a planning problem/instance, containing the initial state and goal to achieve. This encoding allows the reuse of the same planning domain for planning tasks with different initial state and/or goal but which share their environment dynamics.\nThroughout the years, many works [Shen and Simon, 1989; Pasula et al., 2007; Segura-Muros et al., 2021] have tried to automatically learn planning domains from data, in order to alleviate the burden on domain designers. Nevertheless, the task of generating planning problems has received less attention. In most cases, they need to be created by hand or produced by hard-coded, domain-specific instance generators, which requires great effort from the human designers. Having a large set of planning problems is useful for several reasons. The main one is that many approaches which apply Machine Learning (ML) to AP employ planning problems as training data, such as those for learning heuristics [Shen et al., 2020] and HTN domains [Hogg et al., 2008]. Additionally, problems are used as benchmarks in planning competitions [Vallati et al., 2015] and result useful for domain validation [McCluskey et al., 2017], i.e., ensuring that the planning domain faithfully represents the environment dynamics.\nIn this work, we address the problem of automatically generating sets of planning instances for a given domain, in order to overcome the drawbacks presented by manual problem generation approaches. The generated problems must exhibit three main properties:\n\u2022 Validity. Valid problems must meet two criteria. Firstly, the initial state of the problem must represent a consistent (possible) situation of the world (e.g., an object can only be in a single place at the same time). These consistency constraints are not encoded in the PDDL description. Therefore, the human designer must first decide on these rules and then either craft problems which satisfy them or hard-code them into an instance generator (so only consistent problems are generated). Secondly, valid problems must be solvable, i.e., there must exist at least one valid plan from the initial state to the goal.\n\u2022 Quality. This property is defined by the users, according to the characteristics they want the generated problems to exhibit. In this work, we will use difficulty as the only quality measure, i.e., the goal is to generate problems as hard to solve as possible (in terms of planning time) by an automated planner, given some limit on problem size. We have chosen difficulty as the measure to optimize because it plays a central role in AP, where great effort has been devoted to studying problem difficulty [Cohen and Beck, 2017] and developing efficient algorithms for\nar X\niv :2\n30 1.\n10 28\n0v 1\n[ cs\n.A I]\n2 4\nJa n\n20 23\nsolving difficult problems [Bonet and Geffner, 2001].\n\u2022 Diversity. The generated problems should also be different from one another, according to some similarity/distance metric. In other words, they should be sampled from the entire problem subspace that satisfies the validity and quality requirements.\nWe propose NeSIG (Neuro-Symbolic Instance Generator), which to the best of our knowledge is the first domainindependent method for automatically generating planning problems that are at the same time valid, diverse and of good quality (i.e., difficult to solve). NeSIG receives as inputs the PDDL description of the domain, a set of consistency constraints which generated problems must satisfy, the maximum size of the problems to generate and a list with the predicates and object types which can appear in the problem goals. It then learns to generate problems for the domain provided as input so that they are valid, diverse and difficult to solve. The scope of this work is limited to typed-STRIPS domains with existential and negative preconditions1.\nWe formulate problem generation as a Markov Decision Process (MDP) [Sutton and Barto, 2018], in which a problem is generated in a series of steps. Firstly, the initial state of the problem is generated by successively adding objects and atoms to an initially empty state. Then, a series of domain actions (i.e., the actions present in the planning domain) are executed from the generated initial state to arrive at a goal state, where a subset of the atoms are selected to form the problem goal, according to the goal types and predicates provided by the user. We use Reinforcement Learning (RL) [Sutton and Barto, 2018] to train two generative policies, one for generating the problem initial state and one for the goal. These policies are learned by Neural Logic Machines (NLMs) [Dong et al., 2019], a neuro-symbolic deep neural network architecture capable of inductive learning and logic reasoning. We test our method on two classical planning domains, comparing the problems generated with those obtained by instance generators programmed for those particular domains. Our results show NeSIG obtains valid and diverse problems of greater difficulty than the competitor approaches while reducing human intervention, as it removes the need to design the problems manually or program a domain-specific generator."
        },
        {
            "heading": "2 Related Work",
            "text": "Several works have proposed domain-independent methods for planning problem generation but, to this date, none of them have been able to generate problems that are simultaneously valid, of good quality and diverse. [Fern et al., 2004] proposes a random-walk approach to generate planning problems. It randomly creates an initial state si and executes n actions at random to arrive at state sg . Then, it selects a subset of the atoms of sg , which constitutes the goal g, and returns the planning problem (si, g). Although the problems generated with this method are always solvable, they may\n1We have used Lifted PDDL (https://github.com/AIPlanning/lifted-pddl) as our parser, which only provides support for this subset of ADL.\nnot exhibit the other properties (consistency, quality and diversity), as they are generated at random. [Fuentetaja and De la Rosa, 2012] also employs a random-walk approach but, unlike the previous work, it uses semantics-related information provided by the user to guarantee the consistency of the problems obtained. Thus, this method always generates valid problems but provides no guarantees about their diversity or quality, since they are also generated at random. [Marom and Rosman, 2020] follows a different approach. It starts from a predefined goal state and performs a backward search for the initial state. The problems obtained are used to learn a planning heuristic. The proposed method estimates its uncertainty and uses this value to search for problems with the right difficulty for training the heuristic. Hence, this method is able to obtain valid and quality problems. However, it only works for domains with a single goal and for which there exists an inverse transition model, i.e., for every action a that transitions from state s to s\u2032 must exist an inverse action a\u2032 that goes from s\u2032 to s, which needs to be provided to the method.\nFinally, it is worth to mention several works that address a similar problem to the one tackled in this paper. [Katz and Sohrabi, 2020] addresses the problem of generating diverse and difficult planning tasks with different causal graphs. This work generates complete planning tasks (planning domain and problem) whereas NeSIG generates planning problems for the particular domain chosen by the user. [Torralba et al., 2021] proposes Autoscale, a method for obtaining valid and diverse planning problems with graded difficulty for their use in planning competitions. However, unlike our method, Autoscale does not generate problems on its own. Instead, it relies on the existence of domain-specific instance generators, selecting a set of problems with graded difficulty among the ones they generate. Therefore, Autoscale can be considered complementary to our approach, as it could be used to select problems among the ones NeSIG generates."
        },
        {
            "heading": "3 Background",
            "text": "This section briefly explains some background concepts. We first describe how planning tasks are represented and then explain how NLMs work."
        },
        {
            "heading": "3.1 Planning Task Representation",
            "text": "A planning task is a tuple formed by a planning domain and a planning problem (also known as an instance). Both the domain and problem are often represented in a formal FOLbased language such as PDDL [Haslum et al., 2019]. In PDDL, the domain encodes the existing object types, predicates and actions available to solve the task, detailing for each action its parameters (variables), preconditions (conditions which must be true for the action to be applicable) and effects (how the action modifies the state). This information is encoded in lifted form, i.e., in terms of FOL variables which can be instantiated (grounded) on objects. On the other hand, the PDDL problem encodes the objects present in that particular instance, the set of atoms which are true at the initial state, and the goal to achieve, represented as a FOL formula (usually as just a conjunction of atoms)."
        },
        {
            "heading": "3.2 Neural Logic Machines",
            "text": "A Neural Logic Machine (NLM) [Dong et al., 2019] is a deep neural network capable of learning from FOL data and performing logic reasoning. An NLM receives as input a set of predicates grounded on a set of objects. Then, it sequentially applies first-order rules to obtain a different set of output predicates instantiated on the same objects. Input predicates are represented as binary tensors containing the truth value for each grounding of the predicate on the set of objects. Given some input predicate p, if p(oi, oj , ok) is true (where i, j, k represent object indexes), then its associated tensor will contain a value of 1 at the (i, j, k) position. Output predicates and those inferred internally by the NLM are also represented as tensors, but they contain real values between 0 and 1. The NLM operates with these tensors by using neural modules that approximate boolean rules (and, or, not) and quantifications (\u2200 and \u2203), being expressive enough to realize a set of Horn clauses. Therefore, NLMs are more expressive than alternative architectures such as Graph Neural Networks [Barcelo\u0301 et al., 2020], which is why they are used in this work."
        },
        {
            "heading": "4 Neuro-Symbolic Instance Generation",
            "text": "In this section we describe our method, shown in Figure 1. NeSIG takes as inputs a PDDL planning domain, a method for evaluating the consistency of the problems generated (which we refer to as the consistency validator) and some extra information, corresponding to the maximum size of the problems to generate and a list with the predicates and object types which can appear in the problem goals. It then learns to generate problems for that particular domain so that they are valid, diverse and difficult to solve (see Figure 1a)."
        },
        {
            "heading": "4.1 Problem Generation as MDP",
            "text": "We propose to generate problems of the form (si, g), where si is the problem initial state and g is the problem goal, via an iterative process which first generates si and then g. Firstly, the initial state generation policy starts from an empty state (with no objects or atoms) and successively adds new objects and atoms to construct si. Then, the goal generation policy starts at si and successively executes domain actions (i.e., the actions present in the planning domain) to arrive at a goal state sg . Finally, g is obtained by selecting a subset of the atoms in sg , according to the goal predicates and object types specified by the user. For example, in the domain known as logistics, the goal only contains atoms of the form at(package,*), i.e., atoms corresponding to predicate at where the object instantiated on the first position is of type package. This entire process is depicted in Figure 1b and a handcrafted example is given in the Appendix. It can be formulated as an MDP (S,A, app, r, t):\n\u2022 S is the state space of the MDP. In our case, states correspond to (incomplete or fully-generated) planning problems (sic, sgc). We use the subindex c (current) to denote when the initial state sic and goal state sgc may not be completely generated yet.\n\u2022 A is the action space, while app : S\u00d7A\u2192 {0, 1} is the applicability function which determines if an action can\nbe executed at a state or not. The set of applicable actionsAapp is different for the initial state and goal generation phases. In the initial state generation phase, Aapp corresponds to adding a new atom to the initial state sic which preserves the continuous consistency constraints (see Section 4.2). The objects this new atom is instantiated on can already be present in sic or not. If they are not, we refer to them as virtual objects, and are added to sic along with their corresponding atom. For example, if the applicable action add ontable(b1) is selected and the object b1 does not exist in sic, then both the atom ontable(b1) and the object b1 will be added to sic. Thus, instantiating atoms on virtual objects is the mechanism we use to add new objects to the problem. In the goal generation phase, Aapp is the subset of actions in the planning domain for which their preconditions are met at the current goal state sgc.\n\u2022 r : S \u00d7 A \u2192 R is the reward function, accounting for the validity and quality of the problems. Since all the generated problems are solvable (as sg is obtained by executing domain actions from si), we only need to consider the consistency aspect of the validity. To do so, the user must provide a method (consistency validator) to check the consistency of the initial state sic associated with a given MDP state. It is used to penalize (i.e., give a negative reward) the initial state generation policy for selecting actions resulting in inconsistent states. Once a problem has been completely generated, it is solved with different automated planners. Then, the policies receive a final reward directly proportional to the resolution difficulty of the problem, i.e., the average number of nodes expanded by the planners.\n\u2022 t : S \u00d7 A\u2192 S is the transition function. In our setting, t is deterministic and returns the next MDP state s\u2032 resulting from executing an applicable action at the current MDP state s."
        },
        {
            "heading": "4.2 Consistency Validator",
            "text": "NeSIG must learn to generate problems which are consistent, in addition to being diverse and difficult to solve. Consistency arises from the semantics of the planning domain, which is not encoded in its PDDL description. For example, in the blocksworld domain, the PDDL description tells us that blocks can be placed on top of each other (predicate on(x,y), where x and y are of type block). However, it does not inform us that blocks are supposed to only be on top of a single block (in FOL, \u2200x on(x, y) \u2192 @z (on(x, z) \u2227 z 6= y)). The PDDL description does not encode these consistency requirements because, in most cases, PDDL problems are created by hand, so it is the duty of the human expert to design problems which encode situations that could arise in the system the PDDL domain represents.\nNonetheless, if problems are generated automatically this is no longer the case. Therefore, a different approach must be followed to enforce these consistency constraints or, else, inconsistent problems may be generated. In the case of domainspecific generators, these constraints are hard-coded in the ad hoc procedure used to generate the problems. Since NeSIG is\ndomain-independent, we must resort to a different method. The user must provide a set of rules used to evaluate the consistency of (incomplete or fully-generated) problem initial states sic. We let the user encode the consistency rules in their preferred way (e.g., as a procedure in Python) and treat them as a black-box method, which we refer to as the consistency validator. This method must receive as input(s) sic (or sic along with some atom a), and output whether sic (or the state resulting from adding a to sic) is consistent or not. However implemented, the consistency validator must be able to separately evaluate two different types of state consistency: continuous consistency and eventual consistency.\nContinuous consistency. These type of consistency rules encode those constraints which must be continuously satisfied during the entire initial state generation phase. In other words, if sic does not meet the continuous-consistency rules, there exists no sequence of MDP actions (i.e., atoms and objects to add to sic) which can be applied to sic to restore its consistency. An example continuous-consistency rule in blocksworld is the following: a block can never be on top of more than one block. If an initial state sic does not meet this rule (e.g., sic contains the atoms on(A,B) and on(A,C)), the only way to restore its consistency would be to either remove on(A,B) or on(A,C) from sic. However, in our generation method, once an atom has been added to sic it cannot be removed. Therefore, if sic is continuous-inconsistent it will always be, no matter what we do.\nEventual consistency. On the other hand, this type of consistency rules encode those constraints which only need to be eventually satisfied once the initial state has been completely generated. If an incomplete initial state sic does not meet an eventual-consistency rule, there must exist at least one sequence of MDP actions which, when applied to sic, transform it into another state s\u2032ic which satisfies the eventualconsistency rule. An example eventual-consistency rule in blocksworld is: if a block X does not have another block on top of it, X is clear, i.e., sic must contain the atom clear(X). If an initial state sic does not meet this rule (i.e., sic only con-\ntains the atoms ontable(A) and on(B,A) but not clear(B)), we can apply the MDP action add clear(B) to sic resulting in a state s\u2032ic which now satisfies the rule. Therefore, if sic is eventual-inconsistent, it may become consistent depending on the MDP actions we apply to it."
        },
        {
            "heading": "4.3 Generative Policies",
            "text": "We use two policies for guiding problem generation. One policy generates the initial state si of each problem, whereas the other generates its goal g. The generative policies are encoded by NLMs and trained with RL to generate consistent, diverse and difficult problems for the corresponding domain.\nInitial State Generation Policy The initial state generation policy (Figure 1c) starts from an empty initial state sic. It then successively adds atoms and objects to sic until it is completely generated. At each step, it receives sic encoded in a suitable form for the policy NLM, i.e., as a list of tensors representing the atoms and objects in sic. The set of objects in sic contains both the objects which are actually present in the state and the set of virtual objects which could be added to it as part of the next atom. This set of virtual objects contains nt objects of each type t in the domain, where nt is equal to the maximum number of occurrences of objects of type t on the same predicate. For example, the set of virtual objects for blocksworld contains two objects of type block, as this is the only type in the domain and there is a predicate (on) which is instantiated on two objects of type block. We also input extra 0-arity, real-valued predicates (i.e., predicates whose value is a real number between 0 and 1) to the NLM encoding the total number of objects, number of objects of each type and number of atoms of each predicate type in sic, normalized by the maximum problem size, and extra 1-arity predicates representing the type of each object and whether it is virtual or not.\nThe NLM receives all this information and outputs a probability distribution over the atoms which can be added to sic. We mask out those tensor positions (i.e., set their value\nto 0) corresponding to invalid atoms, i.e., atoms instantiated on objects of the incorrect type or those which result in a continuous-inconsistent initial state s\u2032ic when added to sic, where the latter condition is checked with the consistency validator. An atom a is sampled from the output probability distribution and it is added to sic in addition to all the virtual objects (if any) it is instantiated on. In addition to the atom probabilities, the NLM also outputs a termination condition probability. The initial state generation policy keeps adding atoms to sic until one of the following conditions is met: 1) the termination condition is sampled or 2) sic has reached the maximum allowed size (i.e., number of atoms), which is provided by the user. Once the problem initial state si has been completely generated, the consistency validator checks if it meets the eventual-consistency rules. If it does, the goal generation phase can start. Otherwise, the policy receives a negative reward as penalty and the problem is discarded.\nGoal generation policy The goal generation policy (Figure 1d) starts from a completely-generated initial state si, i.e., at the start of the goal generation phase the goal state sgc is equal to si. The policy then successively applies domain actions to sgc until the goal state is completely generated. At each step, it receives both si and sgc encoded in a suitable form for the policy NLM, i.e., as a list of tensors representing the atoms and objects in si and sgc. We obtain a single encoding for the pair (si, sgc) by appending the tensor representations of both states. This is possible because the same objects that are present in si are also at sgc, and no virtual objects are added. For example, if si={ontable(A), on(B,A), clear(B), handempty()} and sgc={ontable(B), on(A,B), clear(A), handempty()}, then (si, sgc)={ontablei(A), oni(B,A), cleari(B), handemptyi(), ontableg(B), ong(A,B), clearg(A), handemptyg()}, where subindexes i and g denote predicates belonging to si and sgc, respectively.\nThe NLM receives the input information and outputs a probability distribution over ground domain actions (i.e., the actions encoded in the PDDL domain instantiated on the objects present in sgc). We mask out all the actions except those whose parameters are instantiated on objects of the correct type and their preconditions are met at sgc. Then, a ground action a is sampled from the output distribution and it is applied to sgc, resulting in a new goal state. In addition to the action probabilities, the NLM also outputs a termination condition probability. The goal generation policy keeps applying actions to sgc until one of the following conditions is met: 1) the termination condition is sampled or 2) the policy has already sampled a certain number of actions, where this number must be provided by the user.\nOnce the goal state sg has been completely generated, the problem goal g is obtained by selecting a subset of the atoms present in sg , according to the goal predicates and object types provided by the user. At this point, the problem (si, g) has already been generated. This problem is then solved by a set of automated planners, and its difficulty is calculated as the normalized average of the number of nodes each planner needed to expand in order to solve the problem. This difficulty is then provided as reward to both the initial state and\ngoal generation policies, to encourage the generation of challenging problems.\nPolicy training with RL We train both generative policies with the RL algorithm known as Proximal Policy Optimization (PPO) [Schulman et al., 2017]. Since PPO is an actor-critic algorithm, we need to employ an additional critic NLM for each policy, whose sole purpose is to evaluate the current state (sic in the case of the initial state generation policy and (si, sgc) in the case of the goal policy). The two policies are trained simultaneously in an end-to-end fashion. The initial state generation policy receives rewards measuring the eventual consistency and difficulty of the problems generated, whereas the goal generation policy only receives rewards measuring problem difficulty.\nThe reward signal used to train the generative policies motivates consistency and difficulty, but ignores diversity. In order to motivate diversity, we leverage the entropy term included in the loss function optimized by PPO. This term encourages policies with great entropy, i.e., highly stochastic policies, which result in diverse problems. To calculate the entropy term, we employ two different probability distributions. On the one hand, we use the ground probability distribution, which is the distribution output by the policy NLMs, from which the MDP action to apply to the current state is sampled. On the other hand, we take this distribution and sum the probabilities of all the atoms/ground actions for each predicate/lifted action in the domain, obtaining the lifted probability distribution. The entropy term is simply the average of the entropies of the two distributions."
        },
        {
            "heading": "5 Experimentation",
            "text": "In this section we explain the experiments carried out in this paper. Firstly, we detail the experimental setup, in which we compare the problems generated by our method, NeSIG, with those obtained by domain-specific generators in two classical planning domains. Then, we analyse the results obtained, showing that NeSIG successfully learns to generate valid, diverse and difficult problems for both domains."
        },
        {
            "heading": "5.1 Experimental setup",
            "text": "We perform experiments on two well-known classical planning domains: blocksworld and logistics. In blocksworld, a set of stackable blocks need to be re-assembled with a gripper. In logistics, the goal is to move a set of packages across locations and cities using airplanes and trucks. We train NeSIG separately on each domain, using the same NLM architecture and hyperparameter values for PPO except for the number of training iterations.2 A list with the specific NLM architecture and PPO hyperparameters can be found in the Appendix.\nIn order to speed up training, we take advantage of the ability of NLMs to generalize to problems of larger size [Dong et al., 2019]. During training, NeSIG is tasked with generating problems of small size, whose initial state contains 15 atoms at most. Once trained, we use it to generate larger problems, with 20, 25, 30, 35, and 40 atoms at most. By training on\n2The full code and data used in this paper can be found at https://github.com/ari-dasci/S-PlanningProblemGeneration.\nsmall problems, the NLMs inference time and time needed to check problem consistency is reduced. Additionally, the generated problems can be efficiently solved with state-of-the-art planners, which makes possible to calculate their difficulty during training (needed to obtain the reward for training the policies) without having to resort to methods for approximating problem difficulty, e.g., [Fawcett et al., 2014]. By following this approach, we were able to train NeSIG in 24 hours for blocksworld and 31 hours for logistics, on a system with a Ryzen 5 3600X CPU and RTX 2060 GPU.\nWe compare the problems generated by NeSIG with those obtained by domain-specific instance generators3. They implement a hard-coded procedure that generates problems at random but which always meet the consistency constraints, i.e., they optimize validity and diversity but not difficulty. These generators expect as input a set of parameter values that control the characteristics of the problems to generate (i.e., number of cities in logistics). In order to avoid any unnecessary bias and generate problems as diverse as possible, we set each parameter value at random, by sampling from an uniform distribution where the minimum value is either 0 or 1, depending on the parameter type, and the maximum value is equal to the maximum size of the problems to generate.4 We enforce two additional rules to make sure that the comparison between NeSIG and the generators is fair. Firstly, since problem size impacts difficulty, given a maximum problem size of n we discard all the problems obtained by the generators whose size is smaller than n \u2212 2. Secondly, the set of consistent problems which can be generated should be the same for NeSIG and the generators. Therefore, we have chosen the consistency rules used by NeSIG for each domain so that they closely match those hard-coded in the generators. These rules can be found in the Appendix.\nFor each domain (blocksworld and logistics) and (maximum) problem size (15, 20, 25, 30, 35 and 40 atoms), we generate 50 problems with NeSIG and the corresponding instance generator. We compare three properties of the problems generated: difficulty, diversity and generation time. In order to measure difficulty, each problem is solved with three of the automated planners provided by FastDownward (FD) [Helmert, 2006]. In logistics, we use LAMA-first [Richter and Westphal, 2010] (alias lama-first in FD), FastForward (FF) [Hoffmann, 2001] and weighted A* search with the LMcut heuristic [Helmert and Domshlak, 2009]. In blocksworld, we tried to use the same three planners but run out of memory when trying to solve some of the hardest problems generated. Therefore, we resort to LAMA-first, lazy-greedy search with the FF heuristic and lazy-greedy search with the additive heuristic [Bonet and Geffner, 2001] for the blocksworld domain. For each generated problem, we obtain the number of nodes each planner needed to expand to solve it, and normalize these values by dividing by the mean number of nodes expanded by each planner to solve the problems in the domain. We then calculate problem difficulty as the average number\n3https://github.com/AI-Planning/pddl-generators 4The only exception to this rule is the parameter num cities for the logistics instance generator, whose minimum value is set to 2 as we train NeSIG to generate logistics problems with at least 2 cities.\nof nodes expanded by the three corresponding planners. For measuring problem diversity, we follow the approach detailed in [Cenamor and Pozanco, 2019]. This work extracts a set of planning features from each problem, measuring different properties such as the number of goal atoms, causal graph variables and mutexes. These features are used to measure problem distance and, therefore, diversity. Nonetheless, we employ the set of features proposed in [Fawcett et al., 2014], as this set contains more features. Additionally, we discard those features which measure extraction time, since they could introduce artificial diversity if employed. The diversity of the problems is then calculated as the average distance between each problem and the rest.\nFinally, we measure generation time, as the number of seconds each approach needed to generate each problem on average. In the case of NeSIG, we also take into consideration the time spent on generating inconsistent problems which are later discarded. Therefore, the more inconsistent problems NeSIG generates, the larger its generation time will be."
        },
        {
            "heading": "5.2 Analysis of results",
            "text": "The experiment results are shown in Table 1. It can be observed that NeSIG generates problems of higher difficulty than the instance generators. In blocksworld, it generates problems which are on average 66% more difficult than those obtained by the instance generators for problem sizes 15, 20 and 25. For larger problems, the difference in difficulty enlarges. It obtains problems 4.67 times more difficult than the generator for size 30, 6.48 times for size 35 and 2.88 times for size 40. These results show that NeSIG is not only able to generate hard problems for blocksworld, but it also generalizes to larger problems than those it was trained on, as the difference in difficulty between NeSIG and the instance generator actually grows with problem size, which constitutes a notable result. In logistics, NeSIG generates problems approximately twice as difficult as those obtained by the generator for sizes 15, 20 and 25. For larger problem sizes, nonetheless, the difference in difficulty is smaller. It generates problems 38% more difficult than the generator for size 30, 24% more difficult for size 35 and 18% more difficult for size 40. Therefore, in the logistics domain NeSIG shows good generalization abilities for problems with up to 25 atoms, i.e., 67% larger than those used during training, with difficulty degrading for problems of larger size.\nRegarding diversity, NeSIG generates problems in blocksworld which are 22% as diverse, on average, as those obtained by the generator, for problem sizes 15, 20 and 25. As it occurred with the difficulty, the diversity of the problems obtained in blocksworld improves with their size. For problem sizes between 30 and 40, NeSIG generates problems which are approximately 60% as diverse as those obtained by the generator. In logistics, NeSIG generates the most diverse problems for size 15, with 67% the diversity of the generator. For larger sizes, it generates problems which are approximately half as diverse as the generator problems. In order to put these results into perspective, it should be noted that the instance generators generate problems completely at random and, thus, their diversity can be regarded as (close to) maximal. For this reason, generating problems half as diverse as\nthe instance generators is quite a notable result, since NeSIG tries to strike a balance between diversity and difficulty.\nRegarding generation time, NeSIG spends on average 0.46s to generate each blocksworld problem of size 15. This time increases by a factor of 1.75 on average every time we add five atoms to the problem size, resulting in an average generation time of 7.04s for a blocksworld problem with 40 atoms. Generation times are larger in logistics, ranging from 1.29s for a problem with 15 atoms to 31.52s for a problem with 40 atoms. Nonetheless, they scale similarly, by a factor of 1.9 every five atoms on average. These generation times are much larger than those obtained by the instance generators, which are able to generate a blocksworld problem in 0.19s and a logistics problem in 0.12s on average, while spending a similar amount of time regardless of problem size. Nevertheless, NeSIG generation times can still be considered small in absolute numbers, and could be further reduced by employing better hardware and exploiting the fact that several problems can be generated in parallel.\nIn light of the results obtained, we conclude that our proposed method, NeSIG, is able to generate valid, diverse and difficult problems in reasonable time for both blocksworld and logistics. Additionally, it shows good generalization abilities to larger problems, being able to generalize to problems with 40 atoms in blocksworld and 25 atoms in logistics. We consider these to be remarkable results, especially taking into consideration that our method is domainindependent whereas the instance generators we have compared it against are tailored to each specific domain. For example, the blocksworld generator uses an ad hoc formula to make sure that every consistent state has the same probability of being generated, whereas the logistics generator obtains the goal by randomly shuffling the packages in the initial state, knowing in advance that such a goal will always be achievable. Therefore, in order to program both instance generators careful design and exhaustive domain knowledge was needed, beyond that required to encode the consistency constraints. On the other hand, our method only needs the consistency rules, which can be encoded however the user likes, thus reducing human effort. Furthermore, the instance\ngenerators follow a naive approach for generating challenging problems, as they do so by simply increasing the number of problem objects (e.g., the number of blocks in blocksworld and number of cities and packages in logistics). Instead, NeSIG optimizes difficulty in a clever way, as it automatically learns domain-specific information (i.e., what makes a hard problem for a particular domain) and exploits it to generate challenging problems given a limit on their size."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work we introduced NeSIG, to the best of our knowledge the first domain-independent method for the automatic generation of planning problems that are at the same time valid, diverse and difficult to solve. We formulated problem generation as an MDP, training two policies with RL so that they learn to generate problems with the desired properties. Both policies were encoded by NLMs, a neuro-symbolic deep neural network architecture capable of working with FOL data. We tested our approach on two classical planning domains, comparing the problems generated with those obtained by domain-specific instance generators. The results obtained show NeSIG generates valid and diverse problems of greater difficulty than the instance generators, being able to generalize to larger problems than those it was trained on. Additionally, our method only needs as inputs the PDDL domain and set of consistency rules, along with some extra information (problem size and goal types and predicates). Therefore, it reduces the human effort needed to generate planning problems, when compared to instance generators which need to be programmed for each particular domain.\nIt is important to note that the choice of consistency constraints and quality metric to optimize is totally arbitrary. In future work, we want to harness the flexibility of our method by optimizing different quality metrics (e.g., plan length) and trying different sets of consistency rules for the same domain. Additionally, we would like to extend the experimentation to additional domains and generate problems of larger size, improving NeSIG so that it is able to generalize to problems several times larger than those used during training."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "This work has been partially funded by the Andalusian Collaborative project PYC20 RE 049 UGR and B-TIC-668UGR20 with FEDER funds.\nWe want to express our deep gratitude to Masataro Asai, for his suggestion to use NLMs in our work; Simon Sta\u030ahlberg, for providing the implementation of ACR-GNNs used in a previous version of this work; Mauro Vallati and the rest of authors of [Fawcett et al., 2014], in addition to Sergio Jime\u0301nez Celorrio, for their advice on how to measure problem difficulty; Michael Katz, for his advice on how to measure problem diversity; Jiayuan Mao and the rest of authors of [Dong et al., 2019], for their helpful advice on NLMs; and, finally, Christian Muise and the FD community, for their invaluable help on the use of the FD planning system."
        },
        {
            "heading": "A Problem generation example",
            "text": "In this appendix we provide a simple, handcrafted example of our problem generation method that illustrates how a single planning problem is created from start to finish. For this example, we will use blocksworld as our domain and, at each step (state), we will assume a random action is chosen from the set of applicable actions Aapp. In the initial state generation phase, an action a \u2208 Aapp corresponds to adding an atom to the (current) initial state sic, where this atom is obtained by instantiating a domain predicate (on, ontable, handempty, holding and clear in blocksworld) on objects of the correct type (block type in blocksworld, as all predicates are only instantiated on objects of this type) so that sic remains continuous-consistent. These objects can be in sic or not. In the latter case, we call them virtual objects and they are added to sic along with their corresponding atom. In the goal generation phase, an action a \u2208 Aapp corresponds to executing a domain action (stack, unstack, pickup and putdown in blocksworld) in the (current) goal state sgc, modifying the atoms in sgc according to the add and delete effects of a, which are encoded in the PDDL description of the domain. The selected action a must be grounded, i.e., instantiated, on objects of the correct type (block type in blocksworld, as all actions are applied to objects of this type) present in sgc and, also, its preconditions must be true in sgc.\nAdditionally, we will assume we randomly choose when to stop generating the states sic and sgc. In a real scenario, the generative policies would be in charge of both selecting the next action to execute and when to stop generating sic and sgc (i.e., sample the termination condition). We represent the states of the MDP, corresponding to (incomplete or fully-generated) planning problems, as a tuple (sic, sgc). We represent sic and sgc as another tuple (O,P ), whereO is a set containing the objects present in the state (with their respective types), and P is a set containing the atoms of the state. We now detail the process followed to generate the example problem:\n1. The generation process starts at an empty state ( , ), where both sic and sgc are empty, and the initial state generation phase begins. The selected action is add ontable(o1), where o1 is an object of type block. Since o1 corresponds to a virtual object, we\nalso need to add it to sic. Thus, the resulting state is ( ({block o1}, {ontable(o1)}), ), which corresponds to a continuous-consistent state.\n2. The action add on(o2, o1) is selected, where o2 is a virtual object of type block. The resulting state is (({block o1, block o2}, {ontable(o1), on(o2, o1)}), ), which corresponds to a continuous-consistent state.\n3. The action add clear(o2) is selected and the resulting state is ( ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2)}), ), which corresponds to a continuousconsistent state.\n4. The action add handempty() is selected and the resulting state is ( ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2), handempty()}), ), which corresponds to a continuous-consistent state.\n5. The termination condition is sampled, so the initial state generation phase concludes, i.e., si = sic. Then, the consistency validator checks if si meets the eventual-consistency rules, which it does (otherwise, si would be discarded and no goal would be generated). Therefore, the goal generation phase can start. The goal state is initialized to the initial state, i.e., sgc = si, so the resulting state is ( ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2), handempty()}), ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2), handempty()})).\n6. As we are now in the goal generation phase, the set of applicable actions Aapp corresponds to the domain actions whose preconditions are met in sgc. Assume the action unstack(o2, o1) is selected. Then, the current goal state sgc is modified with the effects of the selected action. Thus, the next state is ( ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2), handempty()}), ({block o1, block o2}, {ontable(o1), holding(o2), clear(o1)})).\n7. The action putdown(o2) is selected and the resulting state is ( ({block o1, block o2}, {ontable(o1), on(o2, o1), clear(o2), handempty()}), ({block o1, block o2}, {ontable(o1), clear(o1), clear(o2), handempty(), ontable(o2)})).\n8. The termination condition is sampled, so the goal generation phase concludes, i.e., sg = sgc. Then, the problem goal g is obtained by selecting a subset of the atoms in sg according to the goal types and predicates given by the user. Let us assume the goal types and predicates list is {ontable(block), on(block, block)}, i.e., g must only contain atoms of predicate type on or ontable instantiated on objects of type block. If that is the case, the goal is g = {ontable(o1), ontable(o2)} and NeSIG outputs the problem (si, g), which is shown in Listing 1.\nListing 1 Example problem generated with NeSIG. (define (problem example_blocksworld_problem)\n(:domain blocksworld)\n(:objects o1 o2 - block)\n(:init (ontable o1) (on o2 o1) (clear o2) (handempty))\n(:goal (ontable o1) (ontable o2)) )"
        },
        {
            "heading": "B Hyperparameter values",
            "text": "In this appendix we provide a comprehensive list with all the NeSIG hyperparameters used in this work. We have used the same hyperparameter values for both blocksworld and logistics. The exceptions to this rule are: the number of training iterations (1500 for blocksworld and 2000 for logistics, chosen by plotting the training metrics and halting training once reward stopped increasing significantly) and the goal types and predicates ({on(block, block)} for blocksworld and {at(package, location)}5 for logistics, since each domain contains a different set of predicates and object types). The remaining hyperparameter values are shown in Table 2.\n5We take into consideration type inheritance. Therefore, the list {at(package, location)} is equivalent to {at(package, location), at(package, airport)}, since type airport inherits from location."
        },
        {
            "heading": "C Consistency rules",
            "text": "In this appendix we detail the rules encoded in the consistency validator for the blocksworld and logistics domains. We differentiate between continuous consistency rules and eventual consistency ones. Continuous consistency rules are checked every time the initial state generation policy needs to select the next action a \u2208 Aapp to apply to the current initial state sic (i.e., the next atom to add to sic). The consistency validator receives as inputs a possible atom a \u2208 A (instantiated on objects of the correct type) and a continuous-consistent initial state sic, and returns whether the continuous consistency of sic is preserved when a is added to it. Once the initial state si has been completely generated, we need to check whether it is eventual-consistent. The consistency validator receives si as input and returns whether it satisfies all the eventual consistency rules. We do not assume any particular encoding for the continuous and eventual consistency rules. Instead, we let the user decide which encoding suits them best. In our implementation, we have represented the consistency rules as Python procedures, differentiating between the type of rules (continuous vs eventual consistency) and domain (blocksworld vs logistics). Their pseudo-code is shown in Listings 2, 3, 4 and 5.\nListing 2 Continuous consistency rules for blocksworld. # Input: current initial state (as a set of\nstate_objects and state_atoms) and the atom to add (as its predicate type (atom_type) and the list of objects (atom_objs) obj_1, obj_2, ..., obj_n it is instantiated on)\n# The atom must not be instantiated on repeated objects if len(atom_objs) != len(set(atom_objs)): return False\n# The atom must not already be in the initial state if atom in state_atoms: return False\n# ontable(obj_1) if atom_type == ontable:\n# obj_1 must be a virtual object return obj_1 not in state_objects\n# on(obj_1, obj_2) elif atom_type == on:\n# obj_1 must be a virtual object if obj_1 in state_objects:\nreturn False\n# obj_2 must be a real object if obj_2 not in state_objects:\nreturn False\n# obj_2 must not appear in an atom of type clear or holding if clear(obj_2) in state_atoms or holding(obj_2) in state_atoms:\nreturn False\n# obj_2 does not have a block on top of it # \u2019*\u2019 is a wildcard representing any object in state_objects return on(*,obj_2) not in state_atoms\n# clear(obj_1) elif atom_type == clear:\n# obj_1 must be a real object if obj_1 not in state_objects:\nreturn False\n# obj_1 does not appear in an atom of type holding if holding(obj_1) in state_atoms:\nreturn False\n# obj_1 does not have a block on top of it return on(*,obj_1) not in state_atoms\n# holding(obj_1) elif atom_type == holding:\n# obj_1 must be a virtual object if obj_1 in state_objects:\nreturn False\n# The state does not contain an atom of type holding or handempty return holding(*) not in state_atoms and handempty() not in state_atoms\n# handempty() elif atom_type == handempty:\n# The state does not contain an atom of type holding (there is no need to check whether handempty() is in state_atoms as the state contains no repeated atoms) return holding(*) not in state_atoms\n# Atom of non-existing predicate type else:\nreturn False\nListing 3 Eventual consistency rules for blocksworld. # Input: current initial state (as a set of\nstate_objects and state_atoms)\n# The initial state must contain at least an atom of type ontable, on and clear if ontable(*) not in state_atoms or on(*,*) not in state_atoms or clear(*) not in state_atoms: return False\n# The block on top of each tower is clear, i.e., for all blocks x appearing in atoms of type ontable or on which do not have another block y on top must have an atom clear(x) for x in state_objects: if (ontable(x) in state_atoms or on(x,*) in state_atoms) and on(*,x) not in\nstate_atoms: if clear(x) not in state_atoms:\nreturn False\n# The state must either have an atom of type holding or handempty return holding(*) in state_atoms or handempty() in state_atoms\nListing 4 Continuous consistency rules for logistics. # Input: current initial state (as a set of\nstate_objects and state_atoms) and the atom to add (as its predicate type (atom_type) and the list of objects (atom_objs) obj_1, obj_2, ..., obj_n it is instantiated on). The type of an object obj_i is given by the expression obj_type(obj_i)\n# The atom must not be instantiated on repeated objects if len(atom_objs) != len(set(atom_objs)): return False\n# The atom must not already be in the initial state if atom in state_atoms: return False\n# incity(obj_1, obj_2) if atom_type == incity:\n# obj_1 must be a virtual object if obj_1 in state_objects:\nreturn False\n# If obj_2 is a virtual object (i.e., corresponds to a new city), obj_1 must be of type airport (since the first location of every city is always an airport) if obj_2 not in state_objects: return obj_type(obj_1) == airport else:\nreturn True\n# at(obj_1,obj_2) elif atom_type == at:\n# obj_1 must be a virtual object if obj_1 in state_objects:\nreturn False\n# obj_2 must be a real object if obj_2 not in state_objects:\nreturn False\n# obj_1 must be of type package, truck or airplane if obj_type(obj_1) not in {package, truck, airplane}:\nreturn False\n# If obj_1 is of type airplane, obj_2 must be of type airport (i.e., airplanes must be placed at airports)\nif obj_type(obj_1) == airplane: return obj_type(obj_2) == airport else: return True\n# in(obj_1, obj_2) elif atom_type == in:\n# The initial state must not contain atoms of type in return False\n# Atom of non-existing predicate type else:\nreturn False\nListing 5 Eventual consistency rules for logistics. # Input: current initial state (as a set of\nstate_objects and state_atoms)\n# Obtain the type of each object in the initial state state_objects_types = [obj_type(obj) for obj in state_objects]\n# The initial state must contain at least an atom of type incity and at if incity(*,*) not in state_atoms or at(*,*) not in state_atoms: return False\n# The initial state must contain at least one package and one airplane if package not in state_objects_types or airplane not in state_objects_types: return False\n# Each city must contain at least one truck (in one of its locations) cities = [obj for obj in state_objects if obj_type(obj)==city]\nfor curr_city in cities: city_locations = [obj for obj in state_objects if incity(obj,curr_city) in state_atoms]\ntrucks_in_city_locations = [obj_1 for at(obj_1, obj_2) in state_atoms if obj_2 in city_locations and obj_type(obj_1) == truck]\nif len(trucks_in_city_locations) == 0: return False\n# The initial state must contain at least two cities return state_objects_types.count(city) >= 2"
        }
    ],
    "title": "NeSIG: A Neuro-Symbolic Method for Learning to Generate Planning Problems",
    "year": 2023
}