{
    "abstractText": "Node classification is a fundamental graph-based task that aims to predict the classes of unlabeled nodes, for which GraphNeural Networks (GNNs) are the state-of-the-art methods. Current GNNs assume that nodes in the training set contribute equally during training. However, the quality of training nodes varies greatly, and the performance of GNNs could be harmed by two types of low-quality training nodes: (1) inter-class nodes situated near class boundaries that lack the typical characteristics of their corresponding classes. BecauseGNNs are data-driven approaches, training on these nodes could degrade the accuracy. (2) mislabeled nodes. In real-world graphs, nodes are oftenmislabeled, which can significantly degrade the robustness of GNNs. To mitigate the detrimental effect of the low-quality training nodes, we present CLNode, which employs a selective training strategy to train GNN based on the quality of nodes. Specifically, we first design a multi-perspective difficulty measurer to accuratelymeasure the quality of training nodes. Then, based on the measured qualities, we employ a training scheduler that selects appropriate training nodes to train GNN in each epoch. To evaluate the effectiveness of CLNode, we conduct extensive experiments by incorporating it in six representative backboneGNNs. Experimental results on real-world networks demonstrate that CLNode is a general framework that can be combined with various GNNs to improve their accuracy and robustness. CCS CONCEPTS \u2022 Theory of computation\u2192 Graph algorithms analysis; \u2022 Information systems\u2192 Social networks; \u2022 Computing methodologies\u2192 Neural networks. Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM \u201923, February 27-March 3, 2023, Singapore, Singapore \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-9407-9/23/02. . . $15.00 https://doi.org/10.1145/3539597.3570385",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaowen Wei"
        },
        {
            "affiliations": [],
            "name": "Xiuwen Gong"
        },
        {
            "affiliations": [],
            "name": "Yibing Zhan"
        },
        {
            "affiliations": [],
            "name": "Bo Du"
        },
        {
            "affiliations": [],
            "name": "Yong Luo"
        },
        {
            "affiliations": [],
            "name": "Wenbin Hu"
        }
    ],
    "id": "SP:6c1b8a6d95e1a4af2dfbb7ab6c7184daf1e9f225",
    "references": [
        {
            "authors": [
                "Yoshua Bengio",
                "J\u00e9r\u00f4me Louradour",
                "Ronan Collobert",
                "Jason Weston"
            ],
            "title": "Curriculum learning",
            "venue": "In Proceedings of the 26th annual international conference on machine learning",
            "year": 2009
        },
        {
            "authors": [
                "Joan Bruna",
                "Wojciech Zaremba",
                "Arthur Szlam",
                "Yann LeCun"
            ],
            "title": "Spectral networks and locally connected networks on graphs",
            "year": 2013
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Enyan Dai",
                "Charu Aggarwal",
                "Suhang Wang"
            ],
            "title": "Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
            "year": 2021
        },
        {
            "authors": [
                "Miryam de Lhoneux",
                "Sheng Zhang",
                "Anders S\u00f8gaard"
            ],
            "title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Yingtong Dou"
            ],
            "title": "Robust Graph Learning for Misbehavior Detection",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "arXiv preprint arXiv:1903.02428",
            "year": 2019
        },
        {
            "authors": [
                "Guy Hacohen",
                "Daphna Weinshall"
            ],
            "title": "On the power of curriculum learning in training deep networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Dongxiao He",
                "Xinxin You",
                "Zhiyong Feng",
                "Di Jin",
                "Xue Yang",
                "andWeixiong Zhang"
            ],
            "title": "A network-specific Markov random field approach to community detection",
            "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "Mengda Huang",
                "Yang Liu",
                "Xiang Ao",
                "Kuan Li",
                "Jianfeng Chi",
                "Jinghua Feng",
                "Hao Yang",
                "Qing He"
            ],
            "title": "AUC-oriented Graph Neural Network for Fraud Detection",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yuge Huang",
                "Yuhan Wang",
                "Ying Tai",
                "Xiaoming Liu",
                "Pengcheng Shen",
                "Shaoxin Li",
                "Jilin Li",
                "Feiyue Huang"
            ],
            "title": "Curricularface: adaptive curriculum learning loss for deep face recognition",
            "venue": "In proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Ziyang Liu",
                "Weihao Li",
                "Dongxiao He",
                "Weixiong Zhang"
            ],
            "title": "Graph convolutional networks meet markov random fields: Semi-supervised community detection in attribute networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Dongkwan Kim",
                "Alice Oh"
            ],
            "title": "How to find your friendly neighborhood: Graph attention design with self-supervision",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Yayong Li",
                "Jie Yin",
                "Ling Chen"
            ],
            "title": "Unified robust training for graph neural networks against label noise",
            "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data",
            "year": 2021
        },
        {
            "authors": [
                "Yangdi Lu",
                "Yang Bo",
                "Wenbo He"
            ],
            "title": "An Ensemble Model for Combating Label Noise",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data",
            "year": 2022
        },
        {
            "authors": [
                "Yueming Lyu",
                "Ivor W. Tsang"
            ],
            "title": "Curriculum Loss: Robust Learning and Generalization against LabelCorruption",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Hoang NT",
                "Choong Jun Jin",
                "Tsuyoshi Murata"
            ],
            "title": "Learning graph neural networks with noisy labels",
            "venue": "arXiv preprint arXiv:1905.01591",
            "year": 2019
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Otilia Stretcu",
                "Graham Neubig",
                "Barnabas Poczos",
                "Tom MMitchell"
            ],
            "title": "Competence-based curriculum learning for neural machine translation",
            "year": 2019
        },
        {
            "authors": [
                "Meng Qu",
                "Huiyu Cai",
                "Jian Tang"
            ],
            "title": "Neural Structured Prediction for Inductive Node Classification",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2022
        },
        {
            "authors": [
                "Prithviraj Sen",
                "Galileo Namata",
                "Mustafa Bilgic",
                "Lise Getoor",
                "Brian Galligher",
                "Tina Eliassi-Rad"
            ],
            "title": "Collective classification in network data",
            "venue": "AI magazine 29,",
            "year": 2008
        },
        {
            "authors": [
                "Oleksandr Shchur",
                "Maximilian Mumme",
                "Aleksandar Bojchevski",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Pitfalls of graph neural network evaluation",
            "venue": "arXiv preprint arXiv:1811.05868",
            "year": 2018
        },
        {
            "authors": [
                "Ke Sun",
                "Zhouchen Lin",
                "Zhanxing Zhu"
            ],
            "title": "Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Binghui Wang",
                "Jinyuan Jia",
                "Neil Zhenqiang Gong"
            ],
            "title": "Graph-based security and privacy analytics via collective classification with joint weight learning and propagation",
            "year": 2018
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Ming Zhou",
                "Zhenglu Yang"
            ],
            "title": "Curriculum pre-training for end-to-end speech translation",
            "year": 2020
        },
        {
            "authors": [
                "Peiyi Wang",
                "Liang Chen",
                "Tianyu Liu",
                "Damai Dai",
                "Yunbo Cao",
                "Baobao Chang",
                "Zhifang Sui"
            ],
            "title": "Hierarchical Curriculum Learning for AMR Parsing",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "WeiWang",
                "Ye Tian",
                "JiquanNgiam",
                "Yinfei Yang",
                "IsaacCaswell",
                "Zarana Parekh"
            ],
            "title": "Learning a Multi-Domain Curriculum for Neural Machine Translation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi"
            ],
            "title": "Curgraph: Curriculum learning for graph classification",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Daphna Weinshall",
                "Dan Amir"
            ],
            "title": "Theory of curriculum learning, with convex loss functions",
            "venue": "Journal of Machine Learning Research 21,",
            "year": 2020
        },
        {
            "authors": [
                "Daphna Weinshall",
                "Gad Cohen",
                "Dan Amir"
            ],
            "title": "Curriculum learning by transfer learning: Theory and experiments with deep networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "Felix Wu",
                "Amauri Souza",
                "Tianyi Zhang",
                "Christopher Fifty",
                "Tao Yu",
                "Kilian Weinberger"
            ],
            "title": "Simplifying graph convolutional networks",
            "venue": "In International conference on machine learning",
            "year": 2019
        },
        {
            "authors": [
                "Zonghan Wu",
                "Shirui Pan",
                "Fengwen Chen",
                "Guodong Long",
                "Chengqi Zhang",
                "S Yu Philip"
            ],
            "title": "A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems",
            "year": 2020
        },
        {
            "authors": [
                "Yiqing Xie",
                "Sha Li",
                "Carl Yang",
                "Raymond Chi Wing Wong",
                "Jiawei Han"
            ],
            "title": "When do gnns work: Understanding and improving neighborhood aggregation",
            "venue": "In IJCAI International Joint Conference on Artificial Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Keyulu Xu",
                "Chengtao Li",
                "Yonglong Tian",
                "Tomohiro Sonobe",
                "Ken-ichi Kawarabayashi",
                "Stefanie Jegelka"
            ],
            "title": "Representation learning on graphs with jumping knowledge networks",
            "venue": "In International conference on machine learning",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Zhang",
                "Yidong Wang",
                "Wenxin Hou",
                "HAO WU",
                "Jindong Wang",
                "Manabu Okumura",
                "Takahiro Shinozaki"
            ],
            "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ge Zhang",
                "Zhao Li",
                "JiamingHuang",
                "JiaWu",
                "ChuanZhou",
                "JianYang",
                "Jianliang Gao"
            ],
            "title": "efraudcom: An e-commerce fraud detection system via competitive graph neural networks",
            "venue": "ACM Transactions on Information Systems (TOIS) 40,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Zhang",
                "Peng Cui",
                "Wenwu Zhu"
            ],
            "title": "Deep learning on graphs: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhou",
                "ShengjieWang",
                "Jeff Bilmes"
            ],
            "title": "Robust CurriculumLearning: from clean label detection to noisy label self-correction",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 6.\n07 25\n8v 2\n[ cs\n.L G\n] 3\n0 D\nec 2\nNode classification is a fundamental graph-based task that aims to predict the classes of unlabeled nodes, for which GraphNeural Networks (GNNs) are the state-of-the-art methods. Current GNNs assume that nodes in the training set contribute equally during training. However, the quality of training nodes varies greatly, and the performance of GNNs could be harmed by two types of low-quality training nodes: (1) inter-class nodes situated near class boundaries that lack the typical characteristics of their corresponding classes. BecauseGNNs are data-driven approaches, training on these nodes could degrade the accuracy. (2) mislabeled nodes. In real-world graphs, nodes are oftenmislabeled, which can significantly degrade the robustness of GNNs. To mitigate the detrimental effect of the low-quality training nodes, we present CLNode, which employs a selective training strategy to train GNN based on the quality of nodes. Specifically, we first design a multi-perspective difficulty measurer to accuratelymeasure the quality of training nodes. Then, based on the measured qualities, we employ a training scheduler that selects appropriate training nodes to train GNN in each epoch. To evaluate the effectiveness of CLNode, we conduct extensive experiments by incorporating it in six representative backboneGNNs. Experimental results on real-world networks demonstrate that CLNode is a general framework that can be combined with various GNNs to improve their accuracy and robustness.\nCCS CONCEPTS\n\u2022 Theory of computation\u2192 Graph algorithms analysis; \u2022 Information systems\u2192 Social networks; \u2022 Computing methodologies\u2192 Neural networks.\n\u2217Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM \u201923, February 27-March 3, 2023, Singapore, Singapore \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-9407-9/23/02. . . $15.00 https://doi.org/10.1145/3539597.3570385\nKEYWORDS\nnode classification; curriculum learning; graph neural networks\nACM Reference Format: Xiaowen Wei, Xiuwen Gong, Yibing Zhan, Bo Du, Yong Luo, and Wenbin Hu. 2023. CLNode: Curriculum Learning for Node Classification. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM \u201923), February 27-March 3, 2023, Singapore, Singapore. ACM,NewYork, NY, USA, 9 pages. https://doi.org/10.1145/3539597.3570385"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Node classification is a fundamental graph-based task. Given a graph with limited labeled nodes (training nodes), the task aims to assign labels to unlabeled nodes [23]. The state-of-the-art node classificationmethods are Graph Neural Networks (GNNs) [35, 40]. Generally, GNNs update the node representations by aggregating the messages passed from their neighbors. Benefiting from this aggregation mechanism, GNNs learn low-dimensional node representations that preserve the topological information and node feature attributes, which are then used to predict the labels. Although many GNN-based node classification works [3, 10, 16, 22, 34] have been proposed, these works usually assume that all training nodes contribute equally. In fact, the quality of training nodes varies widely. Being data-driven approaches, GNNs exhibit degraded performance by training on the low-quality nodes.\nTo illustrate the quality of nodes, we define training nodeswhose\nrepresentations lack the typical characteristics of their label classes as difficult nodes, because it is difficult for GNNs to learn class characteristics from these low-quality nodes. In contrast, easy nodes refer to high-quality nodes that have the typical representations of their label classes. We illustrate difficult nodes and easy nodes using the paper citation network in Figure 1. As illustrated, the cross-field paper E1 connects papers from multiple classes. During neighborhood aggregation, E1 aggregates messages fromneighbors {E2, E3, E4, E5, E6}. By aggregating messages {E4 \u2192 E1, E5 \u2192 E1, E6 \u2192 E1} from classes {21, 22, 24}, E1 obtains an unclear representation that mixes characteristics of different classes, indicating that E1 is a difficult node. In contrast, all the aggregated messages of E15 are from class 24, which makes it an easy node. Therefore, the above observation raises the question of whether these unevenquality training nodes should be treated equally by GNNs.\nEasy nodes and difficult nodes play different roles during training. The representations of easy nodes are typical, and training on such nodes helps GNNs find clear decision boundaries; whereas, difficult nodes should be used carefully, as their representations lack the typical characteristics of their classes. There are two types of difficult nodes that degrade the performance of GNNs: (1) interclass nodes situated near class boundaries. By aggregating messages from neighbors, these nodes obtain unclear representations; as a result, training on these nodes degrades the accuracy of GNNs. (2)mislabeled nodes. Real-world graphs often contain label noise[5, 17, 20] and current GNNs are easily perturbed by training on these mislabeled nodes. Figure 2 shows the accuracy of GCN [16] on three paper citation networks [23], where the same number of difficult nodes or easy nodes are utilized for training. The node difficulty is evaluated using Eq.(11), which we will detail in Section 4. From the results, we can see that training on easy nodes leads to higher accuracy. For example, on the Cora network, if all training nodes are easy nodes, the accuracy is 71.8%, and the accuracy is only 23.6% when only difficult nodes are utilized. Based on the above analysis, mitigating the detrimental effect of difficult nodes can improve the accuracy and robustness of GNNs. In this paper, we introduce curriculum learning [1] to mitigate the effect of these low-quality training nodes.\nIn particular, curriculum learning is a training strategy that initially trains the machine learning models using an easier training subset and then gradually introduces more difficult samples. By excluding low-quality difficult samples during initial training, curriculum learning mitigates overfitting to data noise, and thus improves models\u2019 accuracy and robustness [19, 30, 41]. The most critical component of curriculum learning is the difficulty measurer, which estimates the difficulty (quality) of samples. In existing works, difficultymeasurers are often designed by observing the sample features; for example, sentence length is a popular difficulty measurer in NLP tasks because shorter sentences are often easier for models to learn [21]. However, difficulty cannot be measured directly from node features using a similar approach. One feasible way is to utilize the graph structure, e.g., if a node connects neighbors from multiple classes, it is likely to be an inter-class difficult node. However, in the node classification task, this is challenging due to the limited node labels.\nIn this paper, we attempt to address the above challenging problem by proposing a Curriculum Learning framework for Node Classification, called CLNode. The key idea behind CLNode is to enhance the performance of backbone GNN by incrementally introducing nodes into the training process, starting with easy nodes and progressing to harder ones. Specifically, we first propose to assign pseudo-labels to unlabeled nodes. With the help of label information, we design a multi-perspective difficulty measurer, in which two difficulty measurers from local and global perspectives are proposed to measure the difficulty of training nodes. The local difficulty measurer computes local label distribution to identify inter-class difficult nodes because their neighbors have diverse labels; the global difficulty measurer identifies mislabeled difficult nodes by analyzing the node feature. Based on the measured node difficulty, we propose a continuous training scheduler that selects appropriate training nodes in each epoch to mitigate the negative effect of difficult nodes. CLNode is a general framework that can be combined with various GNNs to improve their node classification performance. The key contributions of this paper are summarized as follows:\n\u2022 We propose CLNode, a novel curriculum learning frame-\nwork for node classification. CLNode first accurately identifies two types of difficult nodes, and then employs a selective training strategy to mitigate the detrimental effect of these nodes. \u2022 We demonstrate that CLNode can be directly plugged into\nexisting GNNs.Without increasing the time complexity, CLNode enhances backbone GNNs by simply feeding nodes to the training process in order from easy to difficult.\n\u2022 We conduct extensive experiments on five datasets. The re-\nsults demonstrate that comparedwith baselinemethodswithout curriculum learning, CLNode effectively improves the accuracies and enhances the robustness to label noise."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Node Classification and GNNs",
            "text": "Node classification [23] aims to predict labels for unlabeled nodes in a given graph. As a fundamental task on graphs, node classification has various applications, including fraud detection [7, 12, 39],\nsecurity and privacy analytics [27], and community detection [11, 14].\nRecently, GNNs have emerged as promising approaches for analyzing graph data. Due to the long history of GNNs, we refer readers to [35, 40] for a comprehensive review. Based on the definition of graph convolution, GNNs can be broadly divided into two categories, namely spectral-based [2, 16, 26] and spatial-based [10, 37]. Bruna et al. [2] first explore spectral-based GNNs by utilizing a spectral filter on the spectral space. In a follow-up work, GCN [16] simplifies the graph convolution operation. SGC[34] proposes to remove the nonlinearity in GCN and thereby speed up the model. Different from spectral-based methods, spatial-based methods define convolutions directly on graphs by performing operations on spatially close neighbors. GraphSAGE [10] is a general inductive framework that generates representations for nodes by sampling local neighbors. JK-Net [37] devises an alternative graph structure-based strategy to select neighbors for nodes. Although GNNs have achieved great success, they simply assume all training nodes to make equal contributions; consequently, training on the low-quality difficult nodes significantly degrades their accuracy and robustness."
        },
        {
            "heading": "2.2 Curriculum Learning",
            "text": "Inspired by the learning principle underlying human cognitive processes, curriculum learning [1] is proposed as a training strategy that trains machine learning models from easier samples to harder samples. Previous studies [1, 32, 33] have shown that curriculum learning improves generalization capacity and guides themodel towards a better parameter space. Motivated by this, scholars have exploited the power of curriculum learning in a wide range of fields, including computer vision (CV) [9, 13, 38], natural language processing (NLP) [6, 28, 29] and graph classification [31], etc. To the best of our knowledge, however, no work has yet attempted to apply curriculum learning to node classification."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 Notation",
            "text": "Let G = (V,E, - ) denote a graph, where V is the node set, E is the edge set, and - is the node feature matrix. The input feature of node 8 is G8 , and the neighborhood of node 8 is N(8). For the node classification task, a labeled node setV! = {E1, ..., E; } is given with .! denoting the input labels. C is the set of classes. The goal of node classification is to predict the labels of unlabeled nodes in the graph."
        },
        {
            "heading": "3.2 Graph Neural Networks",
            "text": "Generally, a GNN involves two key computations for each node 8 at every layer: (1) neighborhood aggregation: aggregating messages passed fromN(8). (2) update representation: updating 8\u2019s representation from its representation in the previous layer and the aggregated messages. Formally, the ;-th layer representation of node 8 is given by:\n\u210e;8 = Update(\u210e ;\u22121 8 ,Aggregate({\u210e ;\u22121 9 | 9 \u2208 N (8)})). (1)\nThe final node representation \u210e!8 , i.e., the output of the last layer, is used for various downstream tasks. For the node classification\ntask, after obtaining node representations, a multilayer perceptron is often used to map them to the predicted labels."
        },
        {
            "heading": "3.3 Curriculum Learning",
            "text": "Curriculum learningmitigates the detrimental effect of low-quality samples by using a curriculum to train the model. A curriculum is a sequence of training criteria < &1, ..., &C , ..., &) > over) training epochs. Each criterion &C is a training subset. The initial &1 consists of easier samples; as C increases, more difficult samples are gradually introduced into&C . In essence, designing such a curriculum for node classification requires us to design a difficulty measurer and a training scheduler. Here, the difficulty measurer estimates the difficulty of each training node; subsequently, based on the difficulty, the training scheduler generates &C at any training epoch C to train the model."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "In this section, we present the details of CLNode. As shown in Figure 3, CLNode comprises two components: (i) multi-perspective difficulty measurer (Figure 3(a)). We first perform a standard node classification to obtain additional label information, then two difficulty measurers from local and global perspectives are proposed to measure the node difficulty. (ii) continuous training scheduler (Figure 3(b)). After determining the node difficulty, we design a training scheduler to train backbone GNNwith easy nodes initially and continuously introduce harder training nodes. By paying less attention to difficult nodes, CLNode improves the accuracy and robustness of backbone GNN. We detail the components of CLNode in the following subsections."
        },
        {
            "heading": "4.1 Multi-perspective Difficulty Measurer",
            "text": "In general, neighborhood aggregation benefits from the homophily of graphs, i.e., a node 8\u2019s neighborsN(8) tend to have the same label as 8 . However, the difficult nodes violate the homophily; for example, the neighbors of an inter-class difficult node have diverse labels because they belong to multiple classes. Taking a step further, the difficulty of nodes can be measured with the help of label information. Therefore, the first step is to assign pseudo-labels to unlabeled nodes (see Figure 3(a)). Specifically, we first train a GNN 51 on the whole training set V! to perform a standard node classification. After the training process, 51 is used to get the pseudo-labels:\n= 51 (G), (2)\n.% = \"!% ( ), (3)\nwhere is the node representation matrix obtained by GNN 51 and .% is the pseudo-labels predicted by a multilayer perceptron. However, directly using .% to measure node difficulty may lead to inaccurate results, since .% of training nodes may be different from the input labels.! . Therefore, to better measure node difficulty, we retain the input labels for training nodes:\n.\u0303 [8] =\n{\n.! [8] , 8 \u2208 V! .% [8] , >C\u210e4AF8B4 . (4)\nSubsequently, to identify two types of difficult nodes, i.e., interclass nodes and mislabeled nodes, we propose two difficulty measurers to capture both local and global information for measuring the node difficulty."
        },
        {
            "heading": "4.1.1 Neighborhood-based Difficulty Measurer.",
            "text": "We first introduce how to identify difficult nodes from a local perspective. After obtaining .\u0303 , for each training node D , we calculate its difficulty with reference to the label distribution of its neighborhood. The first type of difficult nodes (inter-class nodes) have diverse neighbors that belong to multiple classes. In order to identify these inter-class difficult nodes, we calculate the diversity of neighborhood\u2019s labels:\n%2 (D) = |{.\u0303 [E] = 2 | E \u2208 N\u0302 (D)}|\n|N\u0302 (D) | , (5)\n;>20; (D) = \u2212 \u2211\n2\u2208\n%2 (D) ;>6(%2 (D)), (6)\nwhere N\u0302 (D) denotesN(D) \u222a{D} and %2 (D) denotes the proportion of the neighborhood N\u0302 (D) belonging to class 2 . A larger ;>20; indicates a more diverse neighborhood. Taking Figure 3(a) as an example, the ;>20; of node 1 is 0.54, which is much larger than\n;>20; (8) = 0, indicating that node 1 has more diverse neighbors than node 8. Nodes with larger ;>20; are more likely to be interclass nodes. As a result, during neighborhood aggregation, these nodes aggregate neighbors\u2019 features to get an unclear representation, making them difficult for GNNs to learn. By paying less attention to these difficult nodes, CLNode learns more useful information and effectively improves the accuracy of backbone GNNs."
        },
        {
            "heading": "4.1.2 Feature-based Difficulty Measurer.",
            "text": "Because the pseudo-labels could be inaccurate, mislabeled training nodes may not be identified using local information. For instance, consider the training node 7 in Figure 4, whose truth label is 23 but is mislabeled as 21. The label information of node 7 will affect the pseudo-labels of its neighbors. As a result, the pseudo-label of node 2 is likely to be predicted as the mislabeled class 21, thus the local label distribution of node 7 is consistent, from which we cannot identify it as a mislabeled node. Therefore, we propose to use global feature information to identify mislabeled nodes.\nNodes of the same class have similar features, e.g., in a paper citation network, papers in the same field tend to contain the same keywords. However, the mislabeled nodes violate this principle. For instance, in Figure 4, the mislabeled node 7 has low feature similarity to many nodes of its label class (e.g., node 10), since they do not in fact belong to the same class. Conversely, node 7 has high feature similarity to nodes in class 23(e.g., node 8). Therefore, by exploring the feature similarity, we can deduce that node 7 is likely to bemislabeled. The input feature- is sparse in high-dimensional space, instead, we use (see Eq.(2)) as the node feature to compute similarity. Let \u210eE denote the feature of node E , then the representative feature of class 2 is defined as the average of the node features in class 2:\nV2 = {E | .\u0303 [E] = 2}, (7)\n\u210e2 = Avg(\u210eE | E \u2208 V2 ), (8)\nwhereV2 denotes the nodes belonging to class 2 , and \u210e2 is the representative feature of class 2 . To identify mislabeled difficult nodes, for each training node D , we compute its feature similarity to the label class:\n( (D) = 4G? (\u210eD \u00b7 \u210e2D )\nmax2\u2208C 4G? (\u210eD \u00b7 \u210e2 ) , (9)\nwhere 2D denotes the label class of node D , ( (D) calculates the feature similarity between \u210eD and \u210e2D . Mislabeled nodes tend to have smaller ( (D) than correctly labeled nodes. Based on ( (D), the feature-based difficulty measurer is defined as:\n6;>10; (D) = 1 \u2212 ( (D). (10)\n6;>10; measures node difficulty from a global perspective. By using 6;>10; to identify mislabeled training nodes, CLNode selectively excludes these nodes from the training process, thus improving the robustness of the backbone GNNs to label noise. Considering two difficultymeasurers from local and global perspectives, we finally define the difficulty of D as:\n(D) = ;>20; (D) + U \u00b7 6;>10; (D), (11)\nwhereU is a hyper-parameter that controls theweight of 6;>10; (D)."
        },
        {
            "heading": "4.2 Continuous Training Scheduler",
            "text": "After measuring the node difficulty, we use a curriculum-based training strategy to train a better GNN model (see Figure 3(b)). To distinguish it from 51, we denote the model trained with curriculum as 52. We propose a continuous training scheduler to generate the easy-to-difficult curriculum. In more detail, we first sort the training setV! in ascending order of node difficulty; subsequently, a pacing function 6(C) is used to map each training epoch C to a scalar _C whose range is (0, 1], meaning that a proportion _C of the easiest training nodes are used as the training subset at the C-th epoch. Let _0 denote the initial proportion of the available easiest nodes, while) denotes the epoch when 6(C) reaches 1 for the first time. We consider three pacing functions, namely linear, root, and geometric:\n\u2022 linear:\n6(C) =<8=(1, _0 + (1 \u2212 _0) \u2217 C\n) ). (12)\n\u2022 root:\n6(C) =<8=(1,\n\u221a\n_20 + (1 \u2212 _ 2 0) \u2217\nC ) ). (13)\n\u2022 geometric:\n6(C) =<8=(1, 2;>62_0\u2212;>62_0\u2217 C ) ). (14)\nThe visualization of these three pacing functions is presented in Figure 5. As shown in the figure, the linear function increases the difficulty of training nodes at a uniform rate; the root function\nintroduces more difficult nodes in fewer epochs, while the geometric function trains for a greater number of epochs on the subset of easy nodes. By using the pacing function to continuously introduce training nodes into the training process, CLNode assigns appropriate training weights to nodes of different levels of difficulty. Specifically, the more difficult a training node is, the later it is introduced into the training process, meaning it has a smaller training weight.\nMoreover, we do not stop training immediately when C = ) , because at this time, the backbone GNN 52 may not have fully explored the knowledge of nodes which have been recently introduced. Instead, when C > ) , we use the whole training set to train 52 until the test accuracy on validation set converges."
        },
        {
            "heading": "4.3 Pseudo-code and Complexity Analysis",
            "text": "In this subsection, we present the pseudo-code of CLNode and explore its time complexity. The process of CLNode is detailed in Algorithm 1. Lines 2\u20137 describe the the process of measuring node difficulty and lines 8\u201317 describe the process of training the backbone GNN 52 with a curriculum. After the training process, 52 is finally used for node classification (see line 18). As the pseudo-code shows, CLNode is easy to be plugged into any backbone GNN, as it only changes the training set in each training epoch.\nFor the convenience of complexity analysis, we consider GCN as the backbone. The time complexity of an !-layer GCN in one epoch is$ (! |E| +! |V | 2), where is the number of node feature attributes.We assume that GCN converges after)1 epochs, thus its time complexity is$ ()1 \u00b7 (! |E| +! |V| 2)), which is also the time complexity of training 51. Next, the time complexity of measuring node difficulty is$ (;3 +; |C| ), where 3 is the average node degree. The time complexity of sortingV! is$ (; \u00b7;>6 ;). Finally, we analyze the time complexity of training 52. We first train) epochs using the curriculum, after whichwe train 52 with thewholeV! until convergence. The training of the first) epochs can be seen as pre-training 52 with high-quality training nodes. Therefore, 52 will converge before ) + )1 epochs. Because ; < |V| \u226a |E|, the upper bound on the time complexity of CLNode is$ ((2)1 +) ) \u00b7 (! |E| +! |V| 2)). In our experiments, we observe that the running time of CLNode is about twice that of the baseline GNN."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we first evaluate the improvement in accuracy achieved by CLNode over various backbone GNNs. Further experiments are\nAlgorithm 1: CLNode\nInput: A graph G = (V,E, - ), the labeled node setV! ,\nthe input labels .! , the backbone GNN model, the hyper-parameters U , _0, ) .\nOutput: The predicted labels .\u0302 .\n1 Initialize parameters of two GNN models 51 and 52; 2 Train 51 on (G,V!, .!); 3 Predict pseudo-labels .% with 51; 4 .\u0303 \u2190 Eq.(4);\n5 for D \u2208 V! do 6 Calculate node difficulty (D) \u2190 Eq.(11);\n7 end\n8 SortV! according to node difficulty in ascending order; 9 Let C = 1;\n10 while C < ) or not converge do 11 _C \u2190 6(C); 12 Generate training subsetVC \u2190V! [1, ..., \u230a_C \u00b7 ;\u230b] ; 13 Use 52 to predict the labels .C ; 14 Calculate loss L on {.C [E], .! [E] | E \u2208 VC }; 15 Back-propagation on 52 for minimizing L; 16 C \u2190 C + 1;"
        },
        {
            "heading": "17 end",
            "text": "18 Predict .\u0302 with 52;\nconducted on graphs with label noise to demonstrate the robustness of CLNode. Subsequently, we conduct ablation studies to verify the effectiveness of components in CLNode. Finally, we discuss the parameter sensitivity to hyper-parameters.\nWe conduct experiments on five benchmark datasets: Cora, Citeseer, PubMed [23], Amazon Computers (A-Computers), and Amazon Photo (A-Photo) [24]. Cora, CiteSeer, and PubMed are paper citation networks while A-Computers and A-Photo are product copurchase networks. Experiments are conducted on these datasets with random splits and standard splits. The random splits follow [25, 36] to randomly label a specific proportion of nodes as the training set, and the label rates are listed in Table 1; the standard splits follow [16, 26] in using 20 labeled nodes per class as the training set. In each dataset, we follow [26, 34] to use 500 nodes for validation and 1000 nodes for testing.\nWe use six popular GNNs as the backbone models, namely GCN [16], GraphSAGE [10], GAT [26], SuperGAT [15], JK-Net [37] and GCNII [3], which are representative of a broad range of GNNs. In more detail, GCN is a typical convolution-based GNN, GraphSAGE can be applied to inductive learning, GAT and SuperGAT use attention mechanism in neighborhood aggregation, while JK-Net and GCNII are deep GNNs. We use backbone GNNs without curriculum learning as baselines to explore the improvement achieved by CLNode. All models are implemented in PyTorch-geometric [8]. We use the Adam optimizer with a learning rate of 0.01 and the weight decay is 5 \u00d7 10\u22124. The hidden unit is fixed at 16 in paper citation networks and 64 in product co-purchase networks. We apply two graph convolutional layers for GCN, GAT, GraphSage, and SuperGAT, 6 layers for JK-Net, and 64 layers for GCNII. To facilitate fair comparison, the backbone GNNs\u2019 parameters of CLNode\nare the same as the baselines. For CLNode, U is fixed at 1 because we observe good performance at this value. We use the geometric pacing function by default. The hyper-parameter _0 is searched in the range of {0.25, 0.5, 0.75}, while the search space of) is {50, 100, 150}. The code is available at https://github.com/wxwmd/CLNode."
        },
        {
            "heading": "5.1 Node Classification",
            "text": "In this subsection, node classification experiments are conducted on five datasets. For each baseline GNN, we compare its original accuracy with the accuracy of being plugged into the CLNode framework. We conduct each experiment for ten trials to report the average test accuracy and standard deviation.\nTable 2 reports the experimental results under random splits. The results demonstrate that CLNode can be combined with six backbone GNNs and improve their accuracy on node classification. For example, on the Cora dataset, CLNode improves the test accuracy of backbone GNNs by 3.5% (GCN), 2.0% (GraphSAGE), 2.9% (GAT), 1.1% (SuperGAT), 2.8% (JK-Net), and 1.6% (GCNII). The results prove that CLNode effectively mitigates the detrimental effect of difficult nodes, thereby enabling more useful information to be learned from uneven-quality training nodes.\nMoreover, we conduct node classification experiments under different label rates. Table 3 shows the accuracy on Cora dataset at label rates of 1%, 2%, 3%, respectively. We observe that when there are fewer labeled training nodes, the improvement achieved by CLNode is more obvious. This is because when there are more training nodes, the detrimental effect of difficult nodes is mitigated by a large number of easy nodes; conversely, when there are fewer training nodes, difficult nodes easily mislead GNNs to learn the wrong knowledge. Therefore, by excluding difficult nodes from initial training, CLNode significantly improves the accuracy of GNNs at a low label rate. For many real-world graphs, the labeling process is tedious and costly, resulting in limited labels, and it would be highly beneficial to use CLNode in these situations."
        },
        {
            "heading": "5.2 Robustness to Noise",
            "text": "In this subsection, we investigate whether CLNode enhances the robustness of backbone GNNs to label noise. In a noisily labeled graph, the labels have a probability of ? to be flipped to other\nclasses, where ? denotes the noise rate. Following [5, 18], we corrupt the labels of the training and validation set with two kinds of label noise:\n\u2022 Uniform noise. The label has a probability of ? to be misla-\nbeled as any other class.\n\u2022 Pair noise. We assume that nodes in one class can only be\nmislabeled as their closest class; that is, labels have a probability ? to flip to their pair class.\nWe conduct experiments on Cora under standard splits and vary ? from {0, 5%,..., 30%} to compare the performance of CLNode and the baseline GNNs under different levels of noise. We only report the results using GCN and GAT as backbone GNNs because we have similar observations for otherGNNs. CLNode(GCN) and CLNode(GAT) denote the CLNodemethod using GCN and GAT as backbone GNN, respectively.\nThe results are shown in Figure 6, from which we observe that as the noise rate increases, the performance of all baselines drops dramatically. CLNode also suffers under conditions of increasing noise rate; however, when there is more noise in the graph, the performance gap between CLNode and the baseline increases. This observation demonstrates that CLNode effectively enhances the robustness of backbone GNNs to two kinds of label noise, since CLNode considers mislabeled training nodes as difficult nodes and selectively excludes them from the training process, while the baseline\nGNNs treat all training nodes as equal and consequently overfit to noise."
        },
        {
            "heading": "5.3 Ablation Study",
            "text": "In this subsection, we conduct ablation studies to explore the effectiveness of the multi-perspective difficulty measurer and the sensitivity of CLNode to different pacing functions. Ablation studies are conducted on three paper citation datasets under standard splits, where the graphs are corrupted by uniform label noise and the noise rate ? is set to 30%.\nFirst, to verify themulti-perspective difficultymeasurer benefits from combining the local and global information, we design two difficulty measurers to replace it:\n\u2022 Measuring difficulty only with local information, i.e., we\nonly use ;>20; to measure node difficulty.\n\u2022 Measuring difficulty only with global information, i.e., we\nonly use 6;>10; to measure node difficulty.\nWe use these two difficulty measurers for ablation studies; in the below, we refer to the ablated methods as CLNode(local) and CLNode(global), respectively. GCN is used as the baseline method. The results are reported in Table 4, from which we observe the following: (1) both CLNode(local) and CLNode(global) outperform the baseline method, which demonstrates that they measure the node difficulty from different perspectives, and thus mitigate the detrimental effect of different types of difficult nodes; (2) CLNode achieves the best results in all experiments, proving that by combining local and global perspectives to measure the node difficulty, CLNode effectively identifies two types of difficult nodes, thus enhancing the accuracy and robustness of backbone GNNs.\nIn Table 5, we evaluate the sensitivity of CLNode to three pacing functions: linear, root, and geometric. We find that the geometric pacing function has a slight advantage on all datasets. As shown in Figure 5, the geometric function trains for a greater number of epochs on the subset of easy nodes before introducing difficult nodes. Therefore, to mitigate the detrimental effect of difficult nodes, we believe that the high-confidence knowledge in easy nodes should be fully explored before more difficult nodes are introduced."
        },
        {
            "heading": "5.4 Parameter Sensitivity Analysis",
            "text": "Last but not least, we investigate how the hyper-parameters _0 and ) affect the performance of CLNode. _0 controls the initial number of training nodes, while) controls the speed at which difficult nodes are introduced to the training process. To explore the parameter sensitivity, we alter _0 and) from {0.1, 0.2,..., 0.9} and {20, 40,..., 200}, respectively. We use GCN as the backbone GNN and report the results on Cora under random splits. The results in Figure 7 show the following: (1) Generally, with increasing _0, the performance tends to first increase and then decrease; specifically, the performance is relatively good when _0 is between 0.3 and 0.7. A too small _0 results in few training nodes in the initial training process, meaning that the model cannot learn efficiently. In contrast, an overly large _0 introduces difficult nodes during initial training and thus degrades the accuracy. (2) Similarly, as ) increases, the test accuracy tends to first increase and then decrease. A too small ) will quickly introduce more difficult nodes, thus degrading the backbone GNN\u2019s performance; conversely, an extremely large ) causes the backbone GNN to be trained mainly on the easy subset, causing a loss of the information contained in difficult nodes."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we study the problem of training GNNs on unevenquality training nodes. Current GNNs assume that all training nodes contribute equally during training; as a result, difficult nodes degrade their accuracy and robustness. To address these issues, we propose a novel framework CLNode to mitigate the detrimental effect of difficult nodes. Specifically, we design a multi-perspective difficulty measurer to accurately measure node difficulty using local and global information. Based on these measurements, a continuous training scheduler is proposed to feed nodes to the training progress in an easy-to-difficult curriculum. Extensive experiments on five benchmark datasets demonstrate that CLNode is a general framework that can be combined with six representative backbone GNNs to improve their accuracy. Further experiments are conducted on noisily labeled graphs to prove that CLNode enhances backbone GNNs\u2019 robustness. An interesting future direction to expand the current work is to explore the application of curriculum learning to more graph-related tasks, e.g., link prediction."
        },
        {
            "heading": "7 ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the Natural Science Foundation of China (Nos. 61976162, 82174230), Artificial Intelligence Innovation Project ofWuhan Science and Technology Bureau (No.20 22010702040070), Science and Technology Major Project of Hubei Province (Next Generation AI Technologies) (No. 2019AEA170), and Joint Fund for TranslationalMedicine and Interdisciplinary Research of Zhongnan Hospital ofWuhanUniversity (No. ZNJC202016)."
        }
    ],
    "title": "CLNode: Curriculum Learning for Node Classification",
    "year": 2023
}