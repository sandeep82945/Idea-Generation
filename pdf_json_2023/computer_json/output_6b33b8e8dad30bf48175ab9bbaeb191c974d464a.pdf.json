{
    "abstractText": "Detecting concealed objects presents a significant challenge for human and artificial intelligent systems. Detecting concealed objects task necessitates a high level of human attention and cognitive effort to complete the task successfully. Thus, in this study, we use concealed objects as stimuli for our decision-making experimental paradigms to quantify participants\u2019 decision-making performance. We applied a deep learning model, Bi-directional Long Short Term Memory (BiLSTM), to predict the participant\u2019s decision accuracy by using their electroencephalogram (EEG) signals as input. The classifier model demonstrated high accuracy, reaching 96.1% with an epoching time range of 500 ms following the stimulus event onset. The results revealed that the parietal-occipital brain region provides highly informative information for the classifier in the concealed visual searching tasks. Furthermore, the neural mechanism underlying the concealed visual-searching and decision-making process was explained by analyzing serial EEG components. The findings of this study could contribute to the development of a fault alert system, which has the potential to improve human decision-making performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xuan-The Tran"
        },
        {
            "affiliations": [],
            "name": "Thomas (Tien-Thong"
        },
        {
            "affiliations": [],
            "name": "Chin-Teng Lin"
        }
    ],
    "id": "SP:671cab4e320f464b6bbb16dcee93ef496c2c5920",
    "references": [
        {
            "authors": [
                "Y. Si",
                "X. Wu",
                "F. Li",
                "L. Zhang",
                "K. Duan",
                "P. Li",
                "L. Song",
                "Y. Jiang",
                "T. Zhang",
                "Y. Zhang"
            ],
            "title": "Different decision-making responses occupy different brain networks for information processing: a study based on eeg and tms",
            "venue": "Cerebral Cortex, vol. 29, no. 10, pp. 4119\u2013 4129, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T.V. Wiecki",
                "I. Sofer",
                "M.J. Frank"
            ],
            "title": "Hddm: Hierarchical bayesian estimation of the drift-diffusion model in python",
            "venue": "Frontiers in neuroinformatics, p. 14, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "D.-P. Fan",
                "G.-P. Ji",
                "M.-M. Cheng",
                "L. Shao"
            ],
            "title": "Concealed object detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Bhattacharyya",
                "D. Valeriani",
                "C. Cinel",
                "L. Citi",
                "R. Poli"
            ],
            "title": "Target detection in video feeds with selected dyads and groups assisted by collaborative brain-computer interfaces",
            "venue": "2019 9th International IEEE/EMBS Conference on Neural Engineering (NER). IEEE, 2019, pp. 159\u2013162.",
            "year": 2019
        },
        {
            "authors": [
                "D. Valeriani",
                "C. Cinel",
                "R. Poli"
            ],
            "title": "Group augmentation in realistic visual-search decisions via a hybrid brain-computer interface",
            "venue": "Scientific reports, vol. 7, no. 1, p. 7772, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Valeriani",
                "R. Poli"
            ],
            "title": "Cyborg groups enhance face recognition in crowded environments",
            "venue": "PloS one, vol. 14, no. 3, p. e0212935, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C.-T. Lin",
                "T.-T.N. Do"
            ],
            "title": "Direct-sense brain\u2013computer interfaces and wearable computers",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51, no. 1, pp. 298\u2013312, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Delorme",
                "S. Makeig"
            ],
            "title": "EEGLAB: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis",
            "venue": "Journal of neuroscience methods, vol. 134, no. 1, pp. 9\u201321, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "T.-T.N. Do",
                "C.-T. Lin",
                "K. Gramann"
            ],
            "title": "Human brain dynamics in active spatial navigation",
            "venue": "Scientific Reports, vol. 11, no. 1, pp. 1\u201312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Gramfort",
                "M. Luessi",
                "E. Larson",
                "D.A. Engemann",
                "D. Strohmeier",
                "C. Brodbeck",
                "L. Parkkonen",
                "M.S. H\u00e4m\u00e4l\u00e4inen"
            ],
            "title": "MNE software for processing MEG and EEG data",
            "venue": "NeuroImage, vol. 86, pp. 446\u2013 460, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Selimbeyoglu",
                "Y. Keskin-Ergen",
                "T. Demiralp"
            ],
            "title": "What if you are not sure? electroencephalographic correlates of subjective confidence level about a decision",
            "venue": "Clinical Neurophysiology, vol. 123, no. 6, pp. 1158\u20131167, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Berchicci",
                "D. Spinelli",
                "F. Di Russo"
            ],
            "title": "New insights into old waves. matching stimulus-and response-locked erps on the same timewindow",
            "venue": "Biological psychology, vol. 117, pp. 202\u2013215, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "\u00c1. Darriba",
                "F. Waszak"
            ],
            "title": "Predictions through evidence accumulation over time",
            "venue": "Scientific reports, vol. 8, no. 1, pp. 1\u201315, 2018.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nDecision-making is a ubiquitous aspect of daily life and is crucial for human survival. The decision-making process comprises three primary stages: decision-making preparation, decision-making itself, and decision-making evaluation [1]. Early decision-making prediction means we predict the human decision in the decision-making preparation stage. Early decision-making is also a critical component of a fault alert system, which aims to identify the possibility of human error before a decision is made. Task complexity and individual variability both play a role in the decision-making process. Neurocognitive research has made strides in quantifying the decision-making process through behavioural parameters such as reaction time and decision accuracy. Hierarchical Drift Diffusion Models (HDDM) [2] are commonly employed in the analysis of decision-making behaviour. However, the HDDM model is primarily descriptive and lacks explanatory power regarding the underlying mechanisms of the decision-making process. Additionally, the HDDM model is typically utilized in the context of discrimination-based decision-making tasks.\nComputer vision techniques divide the objects of detection tasks into two categories: salient and concealed objects. Salient objects, which are transparent and easy to detect, contrast with concealed objects which pose a greater challenge for human observers and machine learning algorithms.\n1X.-T. T., T. D. and C.-T. L. are with the University of Technology Sydney, Faculty of Engineering and Information Technology, Australian Artificial Intelligence Institute, GrapheneX-UTS Human-centric AI Centre, 15 Broadway, Ultimo, New South Wales 2007, Australia.\nConcealed objects are often similar to the image background in terms of colour, texture, and shape. The most popular use case of concealed object detection is in medical endoscopic imaging, such as polyp segmentation and lung infection segmentation [3]. Concealed object detection can also be used for tasks such as detecting locusts to prevent invasions and identifying rare species.\nA number of decision-making studies have employed visual searching tasks as a means of investigation. The Patrol [4] paradigms utilize short videos in which participants are required to detect whether a soldier is wearing a helmet or a cap. The Realistic Search paradigm [5] employs multiple objects images that contain penguins and possibly a polar bear, and the participants\u2019 task is detecting the presence of a polar bear. The Suspect Detection experimental paradigm [6] requires participants to detect human appearance in the stimulus images. All of these experimental paradigms involve discrimination tasks that use binary stimuli, where the target either appears or does not appear in videos or images. Participants are also required to make binary decisions (yes or no, target or non-target). However, the use of binary stimuli can lead to the decision-making results being influenced by the randomness of participants\u2019 responses, with a chance level of 1/2, particularly if participants are not fully engaged in the task. Our paradigm offers six options corresponding to the six possible positions of an object within the image, with a chance level of 1/6. As a result, the likelihood of correct answers by chance is lower (1/6 < 1/2).\nNeuroimaging techniques, such as electroencephalography (EEG), functional magnetic resonance imaging (fMRI), and electrocorticography (ECoG), have the potential to provide insight to explain the decision-making process in the human brain [7]. These methods are able to capture brain activity while participants perform decision-making tasks. Furthermore, advanced data-driven techniques can be applied to neuroimaging data to classify or predict human decisions prior to their actual realization. Therefore, in this research, we employed concealed objects as stimuli in our experimental paradigm to elicit such brain activity.\nThe objective of this study is to propose a novel visual searching experimental paradigm for decision-making research. The paradigm involves participants detecting concealed objects and making decisions regarding the object\u2019s location within the images. This approach aims to minimize the number of arbitrary decisions and fortunate trials. In addition, using a Bidirectional Long Short Term Memory model with various EEG temporal epoch lengths allows early prediction of the participant\u2019s correct or incorrect decisions.\nThis work is licensed under a Creative Commons Attribution 3.0 License. For more information, see http://creativecommons.org/licenses/by/3.0/\n20 23\n4 5t\nh A\nnn ua\nl I nt\ner na\ntio na\nl C on\nfe re\nnc e\nof th\ne IE\nEE E\nng in\nee rin\ng in\nM ed\nic in\ne &\nB io\nlo gy\nS oc\nie ty\n(E M\nB C\n) | 9\n79 -8\n-3 50\n3- 24\n47 -1\n/2 3/\n$3 1.\n00 \u00a9\n20 23\nIE EE\n| D\nO I:\n10 .1\n10 9/\nEM B"
        },
        {
            "heading": "II. METHODS",
            "text": ""
        },
        {
            "heading": "A. Participants",
            "text": "This study recruited ten healthy participants (1 female, aged 20-38 years, mean \u00b1 SD: 25.1 \u00b1 4.2; 2 left-handed) who all had normal or corrected-to-normal vision. All participants provided informed consent, which was reviewed and approved by the ethical committee of the University of Technology Sydney, Australia (Approval Grant Number: UTS HREC REF NO. ETH22-7038).\nParticipants took part in four block tests using the same experimental paradigm with different object targets. Each block test comprised 50 trials and took 10 minutes to complete. Before the formal tests, participants completed a practice test comprising 10 trials to familiarize themselves with the task. Participants were given 5 minutes of break time between each test. The entire experiment lasted 2.5 hours, and participants received AUD 60 as compensation for their time.\nB. Image dataset and experimental paradigm\nTwo hundred fifty images were chosen from the publicly available concealed image dataset COD10K [3]. These images contain only one animal object that is challenging to detect. All images were resized to 1000 x 600 pixels in order to enhance visual clarity during the experimental task and to minimize head movement during the object-searching process.\nThe experimental paradigm is illustrated in Figure 1. Each trial began with the hint, which is the animal species, for 2 seconds, followed by a 1-second fixation period before the image containing the hinted object was presented for 3 seconds. The thin grid lines divided the image into six equally sized areas, and participants were instructed to locate and indicate which area the object was located in. Another fixation period of 1 second was presented before participants had 2 seconds to indicate their decision by pressing one of six keyboard buttons (1, 2, 3, 4, 5 and 6). The correct position of the object in the image was subsequently highlighted for 2 seconds before a 2-second resting period; after that, the next trial began. Each participant completed 200 trials, with the entire testing session lasting 1 hour."
        },
        {
            "heading": "C. EEG data recording and pre-processing",
            "text": "The EEG data were recorded with Neuroscan Synamps 2 amplifier and 64-channels Quik-Cap (Compumedics, Australia). The impedance in all channels was maintained below 5k\u2126. The recorded EEG data sampling rate is 1000Hz. A 29-inch screen was used, and the distance between the screen and the participant\u2019s chair was 40 cm. The image is shown in the centre of the screen, and participants did not need to move their heads when searching for the image\u2019s object. The EEGLAB toolbox v14.1.2 [8] was used for preprocessing recorded EEG data (adapted from [9]). The EEG data was first down-sampling to 250 Hz, a high-pass filter at 1 Hz and a line-noise removal was applied. EEG data were average referenced, and Adaptive Mixed Independent\nComponent Analysis (AMICA) was applied to remove the artifacts during the data recording.\nStimulus events were recorded during the presentation of the images and were subsequently employed to extract two hundred stimulus-based epochs. These epochs were then averaged (using the average method) for two conditions: correct and incorrect responses. Figure 2 compares the event-related potential (ERP) of the incorrect and correct evoked responses. The epoching and evoking process was implemented using the Python MNE toolbox v1.3.0 [10].\nIn order to investigate the effect of the epoching time range on decision-making classification, three stimulus-based epochs with time ranges of 125 ms, 250 ms, and 500 ms after the stimulus event onset were extracted. Since the parietaloccipital brain area processes the early visual information, we extract another dataset based on the parietal-occipital EEG channels (OZ, O1, O2, POZ, PO3, PO4, PZ, P1, P2) from these data segments. Finally, both parietal-occipital and whole brain (64-channels) datasets are used as the input for the classifier model separately.\nD. Classification\nA Bidirectional Long Short Term Memory (BiLSTM) model was utilized to classify correct and incorrect decisionmaking. Prior to this, the Standard Scaler method was applied to transform the EEG features. For each participant, 80% of the EEG dataset was randomly assigned as the training set, with the remaining 20% used for testing. Ridge regression (L2 regularization) was applied to the first Dense layer, and two Dropout layers (with a 30% rate) were used after each of the Bidirectional layers. Models were trained using the Adam optimizer and a learning rate of 0.0005. Early stopping was implemented with a patience of 15 epochs."
        },
        {
            "heading": "III. RESULTS AND DISCUSSION",
            "text": "As depicted in Figure 2, the comparison of ERP evoked responses in the OZ and POZ channels revealed a similar sequence of components (N170, P2, P3) in the EEG signal. However, differences in temporal and amplitude characteristics were observed. The N170 EEG component, which is known to be related to error-related potentials (ErrP) [11], reflects the moment when the participant has already detected an object and is considering whether or not it is the hinted object. The P2 EEG component has been theorized to reflect the final stage of the decision-making process, indicating the point at which sufficient action-related information has been gathered before a response is emitted [12]. In this experimental paradigm, the P2 EEG component may reflect the point at which the participant makes a decision regarding the location of the object in the image (positions 1 to 6). Finally, The P3 EEG component, which is related to decision confidence [13], reflects the point at which the participant reconsiders the object\u2019s position that they have made before.\nTables 1 and 2 present the classification results of all 10 participants, using three different epoch time ranges for all 64 channels and for 9 channels of the parietal-occipital region. Three epoch durations (0 - 125) ms, (0 - 250) ms,\nand (0-500) ms were selected as inputs for the Bidirectional Long Short Term Memory models based on the analysis of evoked comparisons. The period of (0 - 125) ms corresponds to the point at which participants are actively searching for the hinted object in the images, and it is likely that they have not yet made any decisions about the object. The period of (0 - 250) ms includes the N170 EEG component, which reflects the moment when participants are considering whether the detected object is the hinted object or not. This period may have some impact on the final decision of participants regarding the location of the observed object. By the time period of (0 - 500) ms, the three components (N170, P2, and P3) have already finished. Thus, the participant\u2019s final decision is almost completed at this point, and the classification result of the detected object is fully reported.\nThe mean classification accuracy when using all 64 chan-\nnels (89.4\u00b1 1.9) is higher than when using only 9 parietaloccipital channels (81.0 \u00b1 2.0). In both channel configurations, the mean classification accuracy increases as the epoching time duration increases. When using 64 channels as input for the classifier model, the highest classification accuracy was 96.1% (participant SS07) for the epoch time range (0 - 500) ms, while the highest accuracy for the shortest epoch time range (0 - 125) ms was 88.6% (participant 10). Similarly, when using 9 parietal-occipital channels as the classification dataset, the highest accuracy was 86.3% with an epoch time range (0 - 500) ms (participant 10). The highest classification results were achieved by participants SS07 (64 channels configuration) and SS10 (9 parietaloccipital region channels configuration). The classification results were more consistent when using 64 channels as opposed to 9 parietal-occipital region channels, and when\nthe epoch time range was higher. The classification results support the explanation of the effect of the three EEG components (N170, P2, and P3) on the participants\u2019 final decision-making."
        },
        {
            "heading": "COMPARISON OF CLASSIFICATION RESULTS FOR ALL PARTICIPANTS",
            "text": ""
        },
        {
            "heading": "COMPARISON OF CLASSIFICATION RESULTS FOR ALL PARTICIPANTS",
            "text": ""
        },
        {
            "heading": "PARIETAL-OCCIPITAL REGION",
            "text": ""
        },
        {
            "heading": "IV. CONCLUSIONS",
            "text": "In this study, we investigated the feasibility of participants\u2019 performance prediction in concealed object visual searching tasks using electroencephalography (EEG) signals. We employed a Bi-directional Long Short Term Memory (BiLTSM) classification model and used three different epoching time ranges (0 - 125 ms, 0 - 250 ms, and 0 - 500 ms) as input datasets. The classification was performed using both all 64 EEG channels and 9 parietal-occipital region channels. The results indicated that the mean classification accuracy was 89.4% when using all 64 channels and 81.0% when using only the 9 parietal-occipital region channels, suggesting the potential for early detection of human performance in challenging visual searching tasks. This can be beneficial in scenarios where people work in teams with other people or\nartificial intelligence systems. For future work, we will use different classification models with more epoch time ranges and EEG channels configuration to increase the accuracy of decision-making classification in the shortest time and with the lowest number of EEG channels."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This work was supported in part by the Australian Research Council (ARC) under discovery grant DP210101093 and DP220100803, and the UTS Human-Centric AI Centre funding sponsored by GrapheneX (2023-2031). Research was also sponsored in part by the Australia Defence Innovation Hub under Contract No. P18-650825, Australian Cooperative Research Centres Projects (CRC-P) Round 11 CRCPXI000007, US Office of Naval Research Global under Cooperative Agreement Number ONRG - NICOP - N6290919-1-2058, and AFOSR \u2013 DST Australian Autonomy Initiative agreement ID10134. We also thank the NSW Defence Innovation Network and NSW State Government of Australia for financial support in part of this research through grant DINPP2019 S1-03/09 and PP21-22.03.02."
        }
    ],
    "title": "Early Detection of Human Decision-Making in Concealed Object Visual Searching Tasks: An EEG-BiLSTM Study",
    "year": 2023
}