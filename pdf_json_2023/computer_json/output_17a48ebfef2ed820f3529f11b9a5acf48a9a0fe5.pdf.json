{
    "abstractText": "Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. Our codes are available at https://github.com/yuanqidu/LeftNet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weitao Du"
        },
        {
            "affiliations": [],
            "name": "Yuanqi Du"
        },
        {
            "affiliations": [],
            "name": "Limei Wang"
        },
        {
            "affiliations": [],
            "name": "Dieqiao Feng"
        },
        {
            "affiliations": [],
            "name": "Guifeng Wang"
        },
        {
            "affiliations": [],
            "name": "Shuiwang Ji"
        },
        {
            "affiliations": [],
            "name": "Carla P Gomes"
        },
        {
            "affiliations": [],
            "name": "Zhi-Ming Ma"
        }
    ],
    "id": "SP:ec666989406355f8f2f125a1e6cf48264abd7bf5",
    "references": [
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Anthony Simeonov",
                "Yilun Du",
                "Andrea Tagliasacchi",
                "Joshua B Tenenbaum",
                "Alberto Rodriguez",
                "Pulkit Agrawal",
                "Vincent Sitzmann"
            ],
            "title": "Neural descriptor fields: Se (3)-equivariant object representations for manipulation",
            "venue": "In 2022 International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Frank No\u00e9",
                "Alexandre Tkatchenko",
                "Klaus-Robert M\u00fcller",
                "Cecilia Clementi"
            ],
            "title": "Machine learning for molecular simulation",
            "venue": "Annual review of physical chemistry,",
            "year": 2020
        },
        {
            "authors": [
                "Lars Holdijk",
                "Yuanqi Du",
                "Priyank Jaini",
                "Ferry Hooft",
                "Bernd Ensing",
                "Max Welling"
            ],
            "title": "Path integral stochastic optimal control for sampling transition paths",
            "venue": "ICML",
            "year": 2022
        },
        {
            "authors": [
                "Michael M Bronstein",
                "Joan Bruna",
                "Yann LeCun",
                "Arthur Szlam",
                "Pierre Vandergheynst"
            ],
            "title": "Geometric deep learning: going beyond euclidean data",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Kenneth Atz",
                "Francesca Grisoni",
                "Gisbert Schneider"
            ],
            "title": "Geometric deep learning on molecular representations",
            "venue": "Nature Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyang Wang",
                "Meng Liu",
                "Youzhi Luo",
                "Zhao Xu",
                "Yaochen Xie",
                "Limei Wang",
                "Lei Cai",
                "Qi Qi",
                "Zhuoning Yuan",
                "Tianbao Yang"
            ],
            "title": "Advanced graph and sequence neural networks for molecular property prediction and drug",
            "venue": "discovery. Bioinformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Arne Schneuing",
                "Yuanqi Du",
                "Charles Harris",
                "Arian Jamasb",
                "Ilia Igashov",
                "Weitao Du",
                "Tom Blundell",
                "Pietro Li\u00f3",
                "Carla Gomes",
                "Max Welling"
            ],
            "title": "Structure-based drug design with equivariant diffusion models",
            "venue": "arXiv preprint arXiv:2210.13695,",
            "year": 2022
        },
        {
            "authors": [
                "Yuanqi Du",
                "Tianfan Fu",
                "Jimeng Sun",
                "Shengchao Liu"
            ],
            "title": "Molgensurvey: A systematic survey in machine learning models for molecule design",
            "venue": "arXiv preprint arXiv:2203.14500,",
            "year": 2022
        },
        {
            "authors": [
                "Michael M Bronstein",
                "Joan Bruna",
                "Taco Cohen",
                "Petar Veli\u010dkovi\u0107"
            ],
            "title": "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges",
            "venue": "arXiv preprint arXiv:2104.13478,",
            "year": 2021
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds",
            "venue": "arXiv preprint arXiv:1802.08219,",
            "year": 2018
        },
        {
            "authors": [
                "V\u0131ctor Garcia Satorras",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "E (n) equivariant graph neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Liu",
                "Limei Wang",
                "Meng Liu",
                "Yuchao Lin",
                "Xuan Zhang",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Spherical message passing for 3D molecular graphs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Peter W Battaglia",
                "Jessica B Hamrick",
                "Victor Bapst",
                "Alvaro Sanchez-Gonzalez",
                "Vinicius Zambaldi",
                "Mateusz Malinowski",
                "Andrea Tacchetti",
                "David Raposo",
                "Adam Santoro",
                "Ryan Faulkner"
            ],
            "title": "Relational inductive biases, deep learning, and graph networks",
            "venue": "arXiv preprint arXiv:1806.01261,",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Yang",
                "Kyle Swanson",
                "Wengong Jin",
                "Connor Coley",
                "Philipp Eiden",
                "Hua Gao",
                "Angel Guzman-Perez",
                "Timothy Hopper",
                "Brian Kelley",
                "Miriam Mathea"
            ],
            "title": "Analyzing learned molecular representations for property prediction",
            "venue": "Journal of chemical information and modeling,",
            "year": 2019
        },
        {
            "authors": [
                "Fabrizio Frasca",
                "Beatrice Bevilacqua",
                "Michael Bronstein",
                "Haggai Maron"
            ],
            "title": "Understanding and extending subgraph gnns by rethinking their symmetries",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chaitanya K Joshi",
                "Cristian Bodnar",
                "Simon V Mathis",
                "Taco Cohen",
                "Pietro Li\u00f2"
            ],
            "title": "On the expressive power of geometric graph neural networks",
            "venue": "In The First Learning on Graphs Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Elton P Hsu"
            ],
            "title": "Stochastic analysis on manifolds",
            "venue": "Number 38. American Mathematical Soc.,",
            "year": 2002
        },
        {
            "authors": [
                "Weitao Du",
                "He Zhang",
                "Yuanqi Du",
                "Qi Meng",
                "Wei Chen",
                "Nanning Zheng",
                "Bin Shao",
                "Tie- Yan Liu"
            ],
            "title": "Se (3) equivariant graph neural networks with complete local frames",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Limei Wang",
                "Yi Liu",
                "Yuchao Lin",
                "Haoran Liu",
                "Shuiwang Ji"
            ],
            "title": "ComENet: Towards complete and efficient message passing for 3D molecular graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "arXiv preprint arXiv:1810.00826,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Morris",
                "Yaron Lipman",
                "Haggai Maron",
                "Bastian Rieck",
                "Nils M Kriege",
                "Martin Grohe",
                "Matthias Fey",
                "Karsten Borgwardt"
            ],
            "title": "Weisfeiler and leman go machine learning: The story so far",
            "venue": "arXiv preprint arXiv:2112.09992,",
            "year": 2021
        },
        {
            "authors": [
                "Asiri Wijesinghe",
                "Qing Wang"
            ],
            "title": "A new perspective on\" how graph neural networks go beyond weisfeiler-lehman?",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kristof T Sch\u00fctt",
                "Huziel E Sauceda",
                "P-J Kindermans",
                "Alexandre Tkatchenko",
                "K-R M\u00fcller"
            ],
            "title": "Schnet\u2013a deep learning architecture for molecules and materials",
            "venue": "The Journal of Chemical Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Patricia Suriana",
                "Raphael John Lamarre Townshend",
                "Ron Dror"
            ],
            "title": "Learning from protein structure with geometric vector perceptrons",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kristof Sch\u00fctt",
                "Oliver Unke",
                "Michael Gastegger"
            ],
            "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Tess Smidt"
            ],
            "title": "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs",
            "venue": "arXiv preprint arXiv:2206.11990,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Florian Becker",
                "Stephan G\u00fcnnemann"
            ],
            "title": "GemNet: Universal directional graph neural networks for molecules",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific data,",
            "year": 2014
        },
        {
            "authors": [
                "Stefan Chmiela",
                "Alexandre Tkatchenko",
                "Huziel E Sauceda",
                "Igor Poltavsky",
                "Kristof T Sch\u00fctt",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Machine learning of accurate energy-conserving molecular force fields",
            "venue": "Science advances,",
            "year": 2017
        },
        {
            "authors": [
                "Anders S Christensen",
                "O Anatole Von Lilienfeld"
            ],
            "title": "On the role of gradients for machine learning of molecular energies and forces",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Janek Gro\u00df",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Directional message passing for molecular graphs",
            "venue": "arXiv preprint arXiv:2003.03123,",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Fuchs",
                "Daniel Worrall",
                "Volker Fischer",
                "Max Welling"
            ],
            "title": "Se (3)-transformers: 3d roto-translation equivariant attention networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Brandon Anderson",
                "Truong Son Hy",
                "Risi Kondor"
            ],
            "title": "Cormorant: Covariant molecular neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Marc Finzi",
                "Samuel Stanton",
                "Pavel Izmailov",
                "Andrew Gordon Wilson"
            ],
            "title": "Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Tuan Le",
                "Frank No\u00e9",
                "Djork-Arn\u00e9 Clevert"
            ],
            "title": "Equivariant graph attention networks for molecular property prediction",
            "venue": "arXiv preprint arXiv:2202.09891,",
            "year": 2022
        },
        {
            "authors": [
                "Albert Musaelian",
                "Simon Batzner",
                "Anders Johansson",
                "Lixin Sun",
                "Cameron J Owen",
                "Mordechai Kornbluth",
                "Boris Kozinsky"
            ],
            "title": "Learning local equivariant representations for large-scale atomistic dynamics",
            "venue": "Nature Communications,",
            "year": 2023
        },
        {
            "authors": [
                "D\u00e1vid P\u00e9ter Kov\u00e1cs",
                "Cas van der Oord",
                "Jiri Kucera",
                "Alice EA Allen",
                "Daniel J Cole",
                "Christoph Ortner",
                "G\u00e1bor Cs\u00e1nyi"
            ],
            "title": "Linear atomic cluster expansion force fields for organic molecules: beyond rmse",
            "venue": "Journal of chemical theory and computation,",
            "year": 2021
        },
        {
            "authors": [
                "Felix A Faber",
                "Anders S Christensen",
                "Bing Huang",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Alchemical and structural distribution based representation for universal quantum machine learning",
            "venue": "The Journal of chemical physics,",
            "year": 2018
        },
        {
            "authors": [
                "Albert P Bart\u00f3k",
                "Mike C Payne",
                "Risi Kondor",
                "G\u00e1bor Cs\u00e1nyi"
            ],
            "title": "Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons",
            "venue": "Physical review letters,",
            "year": 2010
        },
        {
            "authors": [
                "Xiang Gao",
                "Farhad Ramezanghorbani",
                "Olexandr Isayev",
                "Justin S Smith",
                "Adrian E Roitberg"
            ],
            "title": "Torchani: a free and open source pytorch-based deep learning implementation of the ani neural network potentials",
            "venue": "Journal of chemical information and modeling,",
            "year": 2020
        },
        {
            "authors": [
                "Ilyes Batatia",
                "Simon Batzner",
                "D\u00e1vid P\u00e9ter Kov\u00e1cs",
                "Albert Musaelian",
                "Gregor NC Simm",
                "Ralf Drautz",
                "Christoph Ortner",
                "Boris Kozinsky",
                "G\u00e1bor Cs\u00e1nyi"
            ],
            "title": "The design space of e (3)equivariant atom-centered interatomic potentials",
            "venue": "arXiv preprint arXiv:2205.06643,",
            "year": 2022
        },
        {
            "authors": [
                "Omri Puny",
                "Matan Atzmon",
                "Edward J Smith",
                "Ishan Misra",
                "Aditya Grover",
                "Heli Ben-Hamu",
                "Yaron Lipman"
            ],
            "title": "Frame averaging for invariant and equivariant network design",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Lingshen He",
                "Yiming Dong",
                "Yisen Wang",
                "Dacheng Tao",
                "Zhouchen Lin"
            ],
            "title": "Gauge equivariant transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wenbing Huang",
                "Jiaqi Han",
                "Yu Rong",
                "Tingyang Xu",
                "Fuchun Sun",
                "Junzhou Huang"
            ],
            "title": "Equivariant graph mechanics networks with constraints",
            "venue": "arXiv preprint arXiv:2203.06442,",
            "year": 2022
        },
        {
            "authors": [
                "Nadav Dym",
                "Haggai Maron"
            ],
            "title": "On the universality of rotation equivariant point cloud networks",
            "venue": "arXiv preprint arXiv:2010.02449,",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Gong",
                "Tian Xie",
                "Yang Shao-Horn",
                "Rafael Gomez-Bombarelli",
                "Jeffrey C Grossman"
            ],
            "title": "Examining graph neural networks for crystal structures: limitations and opportunities for capturing periodicity",
            "venue": "arXiv preprint arXiv:2208.05039,",
            "year": 2022
        },
        {
            "authors": [
                "Jonas K\u00f6hler",
                "Leon Klein",
                "Frank No\u00e9"
            ],
            "title": "Equivariant flows: exact likelihood generative learning for symmetric densities",
            "venue": "In International conference on machine learning,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 4.\n04 75\n7v 1\n[ cs\n.L G"
        },
        {
            "heading": "1 Introduction",
            "text": "The success of many deep neural networks can be attributed to their ability to respect physical symmetry, such as Convolutional Neural Networks (CNNs) [1] and Graph Neural Networks (GNNs) [2]. Specifically, CNNs encode translation equivariance, which is essential for tasks such as object detection. Similarly, GNNs encode permutation equivariance, which ensures that the node ordering does not affect the output node representations. , by aggregating neighboring messages. Modeling 3D objects, such as point clouds and molecules, is a fundamental problem with numerous applications, including robotics [3], molecular simulation [4, 5], and drug discovery [6\u201310]. Different from 2D pictures and graphs that only possess the translation [1] and permutation [2] symmetry, 3D objects intrinsically encode the complex SE(3)/E(3) symmetry [11], which makes their modeling a nontrivial task in the machine learning community.\nTo tackle this challenge, several approaches have been proposed to effectively encode 3D rotation and translation equivariance in the deep neural network architectures, such as TFN [12], EGNN [13], and SphereNet [14]. TFN leverages spherical harmonics to represent and update tensors equivariantly, while EGNN processes geometric information through vector update. On the other hand, SphereNet is invariant by encoding scalars like distances and angles. Despite rapid progress has\n\u2217Equal contribution.\nPreprint. Under review.\nbeen made on the empirical side, it\u2019s still unclear what 3D geometric information can equivariant graph neural networks capture and how the geometric information is integrated during the message passing process [15\u201317]. This type of analysis is crucial in designing expressive and efficient 3D GNNs, as it\u2019s usually a trade-off between encoding enough geometric information and preserving relatively low computation complexity. Put aside the SE(3)/E(3) symmetry, this problem is also crucial in analysing ordinary GNNs. For example, 1-hop based message passing graph neural networks [18] are computationally efficient while suffering from expressiveness bottlenecks (comparing with subgraph GNNs [19, 20]). On the other hand, finding a better trade-off for 3D GNNs is more challenging, since we must ensure that the message updating and aggregating process respects the SE(3)/E(3) symmetry.\nIn this paper, we attempt to discover better trade-offs between computational efficiency and expressiveness power for 3D GNNs by studying two specific questions: 1. What is the expressive power of invariant scalars in encoding 3D geometric patterns? 2. Is equivariance really necessarily for 3D GNNs? The first question relates to the design of node-wise geometric messages, and the second question relates to the design of equivariant (or invariant) aggregation. To tackle these two problems, we take a local-to-global approach. More precisely, we first define three types of 3D isomorphism to characterize local 3D structures: tree, triangular, and subgraph isomorphism, following a local hierarchy. As we will discuss in the related works section, our local hierarchy lies between the 1- hop and 2-hop geometric isomorphism defined in [21]. Then, we can measure the expressiveness power of 3D GNNs by their ability of differentiating non-isomorphic 3D structures in a similar way as the geometric WL tests in [21]. Under this theoretical framework, we summarize one essential ingredient for building expressive geometric messages on each node: local 3D substructure encoding (LSE), which allows an invariant realization. To answer the second question, we analyze whether local invariant features are sufficient for expressing global geometries by message aggregation, and it turns out that frame transition encoding (FTE) is crucial during the local to global process. Although FTE can be realized by invariant scalars, we further demonstrate that introducing equivariant messaging passing is more efficient. By connecting LSE and FTE modules, we are able to present a modular overview of 3D GNNs designs.\nIn realization of our theoretical findings, we propose LEFTNet that efficiently implements LSE and FTE (with equivariant tensor update) without sacrificing expressiveness. Empirical experiments on real-world scenarios, predicting scalar-valued property (e.g. energy) and vector-valued property (e.g. force) for molecules, demonstrate the effectiveness of LEFTNet."
        },
        {
            "heading": "2 Preliminary",
            "text": "In this section, we provide an overview of the mathematical foundations of E(3) and SE(3) symmetry, which is essential in modeling 3D data. We also summarize the message passing graph neural network framework, which enables the realization of E(3)/SE(3) equivariant models.\nEuclidean Symmetry. Our target is to incorporate Euclidean symmetry to ordinary permutationinvariant graph neural networks. The formal way of describing Euclidean symmetry is the group E(3) = O(3) \u22ca T (3), where O(3) corresponds to reflections (parity transformations) and rotations. For tasks that are anti-symmetric under reflections (e.g. chirality), we consider the subgroup SE(3) = SO(3) \u22ca T (3), where SO(3) is the group of rotations. We will use SE(3) in the rest of the paper for brevity except when it\u2019s necessary to emphasize reflections.\nEquivariance. A tensor-valued function f(x) is said to be equivariant with respect to SE(3) if for any translation or rotation g \u2208 SE(3) acting on x \u2208 R3, we have\nf(gx) =M(g)f(x),\nwhereM(\u00b7) is a matrix representation of SE(3) acting on tensors. See Appendix A for a general definition of tensor fields. In this paper, we will use bold letters to represent an equivariant tensor, e.g., x as a position vector. It is worth noting that when f(x) \u2208 R1 andM(g) \u2261 1 (the constant group representation), the equivariant function f(x) is also called an invariant scalar function.\nScalarization. Scalarization is a general technique that originated from differential geometry for realizing covariant operations on tensors [22]. Our method will apply a simple version of scalarization in R3 to transform equivariant quantities. At the heart of its realization is the notion of equivariant\northonormal frames, which consist of three orthonormal equivariant vectors:\nF := (e1, e2, e3).\nBased on F , we can build orthonormal equivariant frames for higher order tensors by taking tensor products\u2297, see Eq. 19 in Appendix. By taking the inner product between F and a given equivariant vector (tensor) x, we get a tuple of invariant scalars (see [23] for a proof):\nx\u2192 x\u0303 := (x \u00b7 e1, x \u00b7 e2, x \u00b7 e3), (1)\nand x\u0303 can be seen as the \u2018scalarized\u2019 coordinates of x.\nTensorization. Tensorization, on the other hand, is the \u2018reverse\u2019 process of scalarization. Given a tuple of scalars: (x1, x2, x3), tensorization creates an equivariant vector (tensor) out of F :\n(x1, x2, x3) Pairing \u2212\u2212\u2212\u2192 x := x1e1 + x2e2 + x3e3. (2)\nThe same procedure is extended to higher order cases, see Eq. 20 in Appendix.\nMessage Passing Scheme for Geometric Graphs. A geometric graph G is represented by G = (V,E). Here, vi \u2208 V denotes the set of nodes (vertices, atoms), and eij \u2208 E denotes the set of edges. For brevity, the edge feature attached on eij is also denoted by eij . Let X = (x1, . . . , xn) \u2208 Rn\u00d73 be the 3D point cloud of all nodes\u2019 equivariant positions, which determines the 3D geometric structure of G.\nA common machine learning tool for modeling graph-structured data is the Message Passing Neural Network (MPNN) [15]. A typical 1-hop MPNN framework consists of two phases: (1) message passing; (2) readout. Let hli, h l j denote the l-th layer\u2019s node features of source i and target j that also depend on the 3D positions (xi, xj), then the aggregated message is\nmli = \u2295\nj\u2208N (i)\nmij(h l(xi), h l(xj), e l ij), (3)\nand \u2295\nj\u2208N (i) is any permutation-invariant pooling operation between the 1-hop neighbors of i. We\nalso include the edge features elij into the message passing phase for completeness. 3D equivariant MPNNs (3D GNNs for short) require the message mi to be equivariant with respect to the geometric graph. That is, for an arbitrary edge eij :\nmij(h l(gxi), h l(gxj)) =M(g)mij(h l(xi), h l(xj)), (4)\nwhere g \u2208 SE(3) is acting on the whole geometric graph simultaneously: (x1, . . . , xn) \u2192 (gx1, . . . , gxn). For example, the invariant model ComENet [24] satisfies Eq. 4 by settingM(g) \u2261 1, and MACE [25] realized Eq. 4 for nonconstant irreducible group representationsM(g) through spherical harmonics and Clebsch-Gordan coefficients."
        },
        {
            "heading": "3 A Local Hierarchy of 3D Isomorphism",
            "text": "As presented in Section 2, defining expressive messages is an essential component for building powerful 3D GNNs. In this section, we develop a fine-grained characterization of local 3D structures and build its connection with the expressiveness of 3D GNNs.\nSince the celebrated work [26], a popular expressiveness test for permutation invariant graph neural networks is the 1-WL graph isomorphism test [27], and Wijesinghe and Wang [28] has shown that the 1-WL test is equivalent to the ability to discriminate the local subtree-isomorphism. It motivates us to develop a novel (local) 3D isomorphism for testing the expressive power of 3D GNNs. However, this task is nontrivial, since most of the previous settings for graph isomorphism are only applicable to 2D topological features. For 3D geometric shapes, we should take the SE(3) symmetry into account. Formally, two 3D geometric graphs X,Y are defined to be globally isomorphic, if there exists g \u2208 SE(3) such that Y = gX. (5) In other words, X and Y are essentially the same, if they can be transformed into each other through a series of rotations and translations. Inspired by Wijesinghe and Wang [28], now we introduce a novel hierarchy of SE(3) equivariant local isomorphism to measure the local similarity of 3D structures.\nLet Si denote the 3D subgraph (and the associated node features) of node i, which contains all edges in E if the end points are one-hop neighbors of i. For each edge eij \u2208 E, the mutual 3D substructure Si\u2212j is defined by the intersection of Si and Sj : Si\u2212j = Si \u2229 Sj .\nGiven two local subgraphs Si and Sj that correspond to two nodes i and j, we say Si is {-tree, -triangular, -subgraph} isometric to Sj , if there exists a bijective function f : Si \u2192 Sj such that hf(u) = hu for every node u \u2208 Si, and the following conditions hold respectively:\n\u2022 Tree Isometric: If there exists a collection of group elements giu \u2208 SE(3), such that (xf(u), xf(i)) = (giuxu, giuxi) for each edge eiu \u2208 Si;\n\u2022 Triangular Isometric: If there exists a collection of group elements giu \u2208 SE(3), such that the corresponding mutual 3D substructures satisfy: Sf(u)\u2212f(i) = giuSu\u2212i for each edge eiu \u2208 Si\u2212j ;\n\u2022 Subgraph Isometric: for any two adjacent nodes u, v \u2208 Si, f(u) and f(v) are also adjacent in Sj , and there exist a single group element gi \u2208 SE(3) such that giSi = Sj .\nNote that tree isomorphism only considers edges around a central node, which is of a tree shape. On the other hand, the mutual 3D substructure can be decomposed into a bunch of triangles (since it\u2019s contained in adjacent node triplets), which explains the name of triangular isomorphism.\nIn fact, the three isomorphisms form a hierarchy from micro to macro, in the sense that the following implication relation holds:\nSubgraph Isometric\u21d2 Triangular Isometric\u21d2 Tree Isometric\nThis is an obvious fact from the above definitions. To deduce the reverse implication relation, we provide a visualized example. Figure 1 shows two examples of local 3D structures: 1. the first one shares the same tree structure, but is not triangular-isomorphic; 2. the second one is triangularisomorphic but not subgraph-isomorphic. In conclusion, the following diagram holds:\nTree Isometric 6\u21d2 Triangular Isometric 6\u21d2 Subgraph Isometric\nOne way to formally connect the expressiveness power of a geometric GNN with their ability of differentiating geometric subgraphs is to define geometric WL tests, the reader can consult [21]. In this paper, we take an intuitive approach based on our nested 3D hierarchy. That is, if two 3D GNN algorithms A and B can differentiate all non-isomorphic local 3D shapes of tree (triangular) level, while A can differentiate at least two more 3D geometries which are non-isomorphic at triangular(subgraph) level than B, then we claim that algorithm A\u2019s expressiveness power is more powerful than B.\nSince tree isomorphism is determined by the one-hop Euclidean distance between neighbors, distinguishing local tree structures is relatively simple for ordinary 3D equivariant GNNs. For example, the standard baseline SchNet [29] is one instance of Eq. 3 by setting etij = RBF(d(xi, xj)), where RBF(\u00b7) is a set of radial basis functions. Although it is powerful enough for testing tree non-isomorphism (assuming that RBF(\u00b7) is injective), we prove in Appendix B that SchNet cannot distinguish non-isomorphic structures at the triangular level.\nOn the other hand, Wijesinghe and Wang [28] has shown that by leveraging the topological information extracted from local overlapping subgraphs, we can enhance the expressive power of GNNs\nto go beyond 2D sub-tree isomorphism. In our setting, the natural analogue of the overlapping subgraphs is exactly the mutual 3D substructures. Now we demonstrate how to merge the information from 3D substructures to the message passing framework (3). Given an SE(3)-invariant encoder \u03c6, define the 3D structure weights Aij := \u03c6(Si\u2212j) for each edge eij \u2208 E. Then, the message passing framework (3) is generalized to:\nmli = \u2295\nj\u2208N (i)\nmij(h l(xi), h l(xj), Aijh l(xj), e l ij). (6)\nFormula 6 is an efficient realization of enhancing 3D GNNs by injecting the mutual 3D substructures. However, a crucial question remains to be answered: Can the generalized message passing framework boost the expressive power of 3D GNNs? Under certain conditions, the following theorem provides an affirmative answer: Theorem 3.1. Suppose \u03c6 is a a universal SE(3)-invariant approximator of functions with respect to the mutual 3d structures Si\u2212j , then the collection of weights {{Aij}eij\u2208E} is able to differentiate local structures beyond tree isomorphism. Moreover, with additional injectivity assumptions (see Eq. 14), 3D GNNs based on the enhanced message passing framework 6 map at least two distinct local 3D subgraphs with isometric local tree structures to different representations.\nThis theorem confirms that the enhanced 3D GNN (formula 6) is more expressive than the SchNet baseline, at least in testing local non-isomorphic geometric graphs. The complete proof is left in Appendix B. The existence of such local invariant encoder \u03c6 is also proved by explicit construction. Note that there are other different perspectives on characterizing 3D structures, we will also briefly discuss them in Appendix B."
        },
        {
            "heading": "4 From Local to Global: The Missing Pieces",
            "text": "In the last section, we introduced a geometric local isomorphism hierarchy for testing the expressive power of 3D GNNs. Furthermore, we motivated adding a SE(3)-invariant encoder to improve the expressive power of one-hop 3D GNNs by scalarizing not only pairwise distances but also their mutual 3D structures in Theorem 3.1. However, to build a powerful 3D GNN, it remains to be analyzed how a 3D GNN acquires higher order (beyond 1-hop neighbors) information by accumulating local messages. A natural question arises: are invariant features enough for representing global geometric information?\nTo formally formulate this problem, we consider a two-hop aggregation case. From figure 2, the central atom a is connected with atoms b and c. Except for the common neighbor a, other atoms that connect to b and c form two 3D clusters, denoted by B, C. Suppose the groundtruth interaction potential of B and C imposed on atom a is described by a tensor-valued function fa(B,C). Since B and C are both beyond the 1-hop neighborhood of a, the information of fa(B,C) can only be acquired after two steps of message passing: 1. atoms b and c aggregate message separately from B and C; 2. the central atom a receives the aggregated message (which contains information of B and C) from its neighbors b and c.\nLet SB (SC) denote the collection of all invariant scalars created by B (C) . For example, SB contains all relative distances and angles within\nthe 3D structure B. Then, the following theorem holds: Theorem 4.1. Not all types of invariant interaction fa(B,C) can be expressed by inputting the union of two sets SB and SC. In other words, there exists E(3) invariant function fa(B,C), such that it cannot be expressed as functions of SB and SC: fa(B,C) 6= \u03c1(SB, SC) for an arbitrary invariant function \u03c1.\nThis theorem in essence tells us that naively aggregating \u2018local\u2019 scalar information from different clusters is not enough to approximate \u2018global\u2019 interactions, even if we only consider simple invariant interaction tasks. Different from the last section, where the local expressiveness is measured by the ability of classifying geometric shapes, we built regression functions that depend strictly more than the combination of local invariant scalars.\nIntuitively, the proof is based on the fact that all scalars in SB (SC) can be expressed through equivariant frames separately determined by B (C). However, the transition matrix between these two frames is not encoded in the aggregation, which causes information loss when aggregating geometric features from two sub-clusters. More importantly, the proof also revealed the missing information that causes the expressiveness gap: Frame Transition (FT).\nFrame Transition (FT). Formally, two orthonormal frames (ei1, e i 2, e i 3) and (e j 1, e j 2, e j 3) are connected by an orthogonal matrix Rij \u2208 SO(3):\n(ei1, e i 2, e i 3) = Rij(e j 1, e j 2, e j 3). (7)\nMoreover, it is easy to check that when (ei1, e i 2, e i 3) and (e j 1, e j 2, e j 3) are equivariant frames, all elements of Rxy are invariant scalars. Suppose i and j represent indexes of two connected atoms in a geometric graph, then the fundamental torsion angle \u03c4ij appeared in ComeNet [24] is just one element of Rij (see Appendix C).\nTowards filling this expressiveness gap, we can straightforwardly inject all invariant pairwise frame transition matrices (FT) into the model. Nevertheless, it imposes expensive computational cost when the number of local clusters is large (O(k2) pairs of FT for each node). Therefore, compared with pure invariant approaches, a more efficient way is to introduce equivariant tensor features for each node i, denoted by mi. By directly maintaining the equivariant frames in mi, we show in Appendix C that FT is easily derived through equivariant message passing.\nEquivariant Message Passing. Similarly with the standard one-hop message passing scheme 3, the aggregated tensor message mi from the l \u2212 1 layer to the l layer can be written as: ml\u22121i =\u2211\nj\u2208N(i) m l\u22121 j . Since summation does not break the symmetry rule, it is obvious that m l\u22121 i are still equivariant tensors. However, the nontrivial part lies in the design of the equivariant update function \u03c6:\nmli = \u03c6(m l\u22121 i ). (8)\nA good \u03c6 should have enough expressive power while preserving SE(3) equivariance. Here, we propose a novel way of updating scalar and tensor messages by performing node-wise scalarization and tensorization blocks (the FTE module of Figure 3). From the perspective of Eq. 4, m(xu) is transformed equivariantly as:\nm(gxu) = l\u2211\ni=0\nMi(g)mi(gxu), g \u2208 SE(3). (9)\nHere, m(xu) is decomposed to (m0(xu), . . . ,ml(xu)) according to different tensor types, and {Mi(g)}li=0 is a collection of different SE(3) tensor representations (see the precise definition in Appendix A).\nTo illustrate the benefit of aggregating equivariant messages from local patches, we study a simple case. Let fa(B,C) = hB \u00b7 hC be an invariant function of B and C (see Fig. 2), then fa can be calculated by a direction composition of scalar messages and equivariant vector messages: fa(B,C) = 1 2 [\u2016ma\u2016 2 \u2212\u2016hB\u2016 2 \u2212\u2016hC\u2016 2 ], where ma = hB + hC is an equivariant vector. Note that ma follows the local equivariant aggregation formula 8, and the other vectors\u2019 norm \u2016hB\u2016 and \u2016hC\u2016 are obtained through local scalarization on atoms b and c. As a comparison, it\u2019s worth mentioning that fa(B,C) can also be expressed by local scalarization with the additional transition matrix data RBC defined by Eq. 7. Let h\u0303B and h\u0303C be the scalarized coordinates with respect to two local equiv-\nariant frames FB and FC . Then fa(B,C) = 12 [\u2225 \u2225 \u2225R\u22121BC h\u0303B + h\u0303C \u2225 \u2225 \u2225 2 \u2212 \u2225 \u2225 \u2225h\u0303B \u2225 \u2225 \u2225 2 \u2212 \u2225 \u2225 \u2225h\u0303C \u2225 \u2225 \u2225 2 ] . However,\nit requires adding the rotation matrix RBC for each (B,C) pair, which is computationally expensive compared to directly implementing equivariant tensor updates."
        },
        {
            "heading": "5 Building an Efficient and Expressive Equivariant 3D GNN",
            "text": "We propose to leverage the full power of LSE and FTE along with a powerful tensor update module to push the limit of efficient and expressive 3D equivariant GNNs design.\nLSE Instantiation. We propose to apply edge-wise equivariant frames to encode the local 3D structures Si\u2212j . By definition, Si\u2212j contains edge eij , nodes i and j, and their common neighbors. We use the equivariant frame Fij built on eij (see the precise formula in Appendix D) to scalarize Si\u2212j . After scalarization (1), the equivariant coordinates of all nodes in Si\u2212j are transformed into invariant coordinates: {xk \u2192 x\u0303k for xk \u2208 Si\u2212j}. To encode these scalars sufficiently, we first weight each x\u0303k by the RBF distance embedding: x\u0303k \u2192 RBF(\u2016xk\u2016)\u2299MLP(x\u0303k) for each xk \u2208 Si\u2212j . Note that to preserve the permutation symmetry, the MLP is shared among the nodes. Finally, the 3D structure weight Aij is obtained by the average pooling of all node features.\nFTE Instantiation. We propose to introduce equivariant tensor message passing and update function for encoding local FT information. At initialization, let NFl(xi, xj) denote the embedded tensorvalued edge feature between i and j. We split it into two parts: 1. the scalar part SFl(xi, xj) for aggregating invariant messages; 2. the higher order tensor part TFl(xi, xj) for aggregating tensor messages. To transform TFl(xi, xj), we turn to the equivariant frame Fij once again. After scalar-\nization by Fij , TF l(xi, xj) becomes a tuple of scalars T\u0303F l (xi, xj), which is then transformed by MLP. Finally, we output arbitrary tensor messages through equivariant tensorization 20:\nT\u0303F l (xi, xj) Tensorize \u2212\u2212\u2212\u2212\u2212\u2192 Fij NFl+1(xi, xj).\nFurther details are provided in Appendix D. As we have discussed earlier, the node-wise tensor update function \u03c6 in Eq. 8 is also one of the guarantees for a powerful FTE. As a comparison, \u03c6 is usually a standard MLP for updating node features in 2D GNNs, which is a universal approximator of invariant functions. Previous works [13, 30] updated equivariant features by taking linear combinations and calculating the invariant norm of tensors, which may suffer from information loss. Then a natural question arises: Can we design an equivariant universal approximator for tensor update? We answer this question by introducing a novel node-wise frame. Consider node i with its position xi, let x\u0304i := 1N \u2211 xj\u2208N(xi) xj be the center of mass around xi\u2019s neighborhood. Then the orthonormal equivariant frame Fi := (ei1, e i 2, e i 3) with respect to xi is defined by\n( xi \u2212 x\u0304i \u2016xi \u2212 x\u0304i\u2016 , x\u0304i \u00d7 xi \u2016x\u0304i \u00d7 xi\u2016 , xi \u2212 x\u0304i \u2016xi \u2212 x\u0304i\u2016 \u00d7 x\u0304i \u00d7 xi \u2016x\u0304i \u00d7 xi\u2016 ). (10)\nFinally, we realize a powerful \u03c6 by the following theorem:\nTheorem 5.1. Equipped with an equivariant frame Fi for each node i, the equivariant function \u03c6 defined by the following composition is a universal approximator of tensor transformations: \u03c6 : Scalarization\u2192 MLP\u2192 Tensorization.\nThe proof is left in Appendix D.\nLEFTNet. An overview of our {LSE,FTE} enhanced efficient graph neural network (LEFTNet) is depicted in Figure 3. LEFTNet receives as input a collection of node embeddings {v01 , . . . , v 0 N}, which contain the atom types and 3D positions for each node: v0i = (zi, xi), where i \u2208 {1, . . . , N}. For each edge eij \u2208 E, we denote the associated equivariant features consisting of tensors by eij . During each messaging passing layer, the LSE module outputs the scalar weight coefficients Aij as enhanced invariant edge feature and feed into the interaction module. Moreover, scalarization and tensorization as two essential blocks are used in the equivariant update module that fulfills the function of FTE. The permutation equivariance of a geometric graph is automatically guaranteed for any message passing architecture, we provide a complete proof of SE(3)-equivariance for LEFTNet in Appendix D.\nSE(3) vs E(3) Equivariance. Besides explicitly fitting the SE(3) invariant molecular geometry probability distribution, modeling the energy surface of a molecule system is also a crucial task for molecule property prediction. However, the Hamiltonian energy function E of a molecule is invariant under refection transformation: Energy(X) = Energy(RX), for arbitrary reflection transformation R \u2208 E(3). In summary, there exist two different inductive biases for modeling 3D data: (1) SE(3) equivariance, e.g. chirality could turn a therapeutic drug to a killing toxin; (2) E(3) equivariance, e.g. energy remains the same under reflections.\nSince we implement SE(3) equivariant frames in LEFTNet, our algorithm is naturally SE(3) equivariant (and reflection anti-equivariant). However, our method is flexible to implement E(3) equivariant tasks as well. For E(3) equivariance, we can either replace our frames to E(3) equivariant frames, or modify the scalarization block by taking the absolute value: x \u2192 x\u0303 := (x \u00b7 e1, x \u00b7 e2, x \u00b7 e3) \ufe38 \ufe37\ufe37 \ufe38\nSE(3)\n\u2192 (x \u00b7 e1, |x \u00b7 e2|, x \u00b7 e3) \ufe38 \ufe37\ufe37 \ufe38\nE(3)\n. Intuitively, since the second vector e2 is a pseudo-\nvector, projections of any equivariant vectors along the e2 direction are not E(3) invariant until taking the absolute value.\nEfficiency. To analyze the efficiency of LEFTNet, suppose 3D graph G has n vertices, and its average node degree is k. Our algorithm consists of three phases: 1. Building equivariant frames and performing local scalarization; 2. Equivariant message passing; 3. Updating node-wise tensor features through scalarization and tensorization. Let l be the number of layers, then the computational complexity for each of our three phases are: 1. O(nk) for computing the frame and local (1-hop) 3D features; 2. O(nkl) for 1-hop neighborhood message aggregation; 3. O(nl) for node-wise tensorization and feature update."
        },
        {
            "heading": "6 Related Work",
            "text": "In light of the discussions in Section 3 and 4, we summarize two necessary ingredients for building expressive equivariant 3D GNNs: (1) local 3D substructure encodings (LSE), such that the local\nmessage is aware of different local 3D structures; (2) frame transition encodings (FTE), such that the 3D GNN is aware of the equivariant coordinate transformation between different local patches.\nWe review the previous 3D GNNs following this framework and summarize the results in Table 1. For a fair comparison, we also list the computational complexity as it is often a trade-off of expressiveness (see the detailed analysis at the end of the next section). For LSE, SphereNet [14] and GemNet [33] (implicitly) encode the local 3D substructures by introducing a computation-intensive 2-hop edge-based update. For FTE, most 3D GNNs with equivariant vector update are able to express the local frame transitions (FT). While EGNN [13] is an exception, because it only updates the position vector (i.e. one channel), which is insufficient to express the whole FT. In other words, whether the update function \u03c6 of (8) is powerful also affects the FT encoding. Except for equivariant update methods, models that encode torsion angle information also partially express FTE as illustrated in Appendix C. However, there is a trade-off between the efficiency and expressiveness in terms of number of hops considered for message passing.\nDifferent from our invariant realization of LSE, Batatia et al. [25] builds its framework by constructing complete equivariant polynomial basis with the help of spherical harmonics and tensor product, where the monomial variables depend on different nodes (bodies). On the other hand, we realize the function of LSE and FTE through the edgewise scalarization Aij and the equivariant message passing (see Fig. 3).\nRecently, Joshi et al. [21] propose a geometric k-WL test (GWL) to measure the expressiveness power of geometric GNN algorithms. On a high level, our tree isomorphism is equivalent to the 1-hop geometric isomorphism as proposed in GWL, and the fine-grained triangular isomorphism lies between the 1-hop and 2-hop geometric isomorphism as proposed in GWL. From the model design point of view, our realization of LSE is through local scalarization, whose expressiveness is guaranteed by the Kolmogorov representation theorem (see [34]) and the universal approximator property of MLP. Moreover, the key concepts of measuring the expreesive power in [21] are the body order and tensor order, which originate from classical inter-atomic potential theories and are of the equivariance nature. On the other hand, we discover the FTE as the \u2019missing\u2019 bridge connecting local invariant scalars and global geometric expressiveness, which (together with LSE on mutual 3D substructures) also reveals why the 1-hop scalarization implemented in ClofNet [23] is insufficient."
        },
        {
            "heading": "7 Experiments",
            "text": "We test the performance of LEFTNet on both scalar value (e.g. energy) and vector value (e.g. forces) prediction tasks. The scalar value prediction experiment is conducted on the QM9 dataset [35] which includes 134k small molecules with quantum property annotations; the vector value prediction experiment is conducted on the MD17 dataset [36] and the Revised MD17(rMD17) dataset [37] which includes the energies and forces of molecules. We compare our LEFTNet with a list of state-of-the-art equivariant (invariant) graph neural networks including SphereNet [14], PaiNN [31],\nEquiformer [32], GemNet [33], etc [29, 38, 12, 39, 40, 13, 15, 41\u201347].The results on rMD17 and ablation studies are listed in Appendix E."
        },
        {
            "heading": "7.1 QM9 - Scalar-valued Property Prediction",
            "text": "The QM9 dataset is a widely used dataset for predicting molecular properties. However, existing models are trained on different data splits. Specifically, Cormorant [40], EGNN [13], etc., use 100k, 18k, and 13k molecules for training, validation, and testing, while DimeNet [38], SphereNet [14], etc., split the data into 110k, 10k, and 11k. For a fair comparison with all baseline methods, we conduct experiments using both data splits. Experimental results are listed in Table 2. For the first data split, LEFTNet is the best on 7 out of the 12 properties and improves previous SOTA results by 20% on average. In addition, LEFTNet is the second best on 4 out of the other 5 tasks. Consistently, LEFTNet is the best or second best on 10 out of the 12 properties for the second split. These experimental results on both splits validate the effectiveness of LEFTNet on scalar-valued property prediction tasks. The ablation study in Appendix E shows that both LSE and FTE contribute to the final performance."
        },
        {
            "heading": "7.2 MD17 - Vector-valued Property Prediction",
            "text": "We evaluate the ability of LEFTNet to predict forces on the MD17 dataset. Following existing studies [29, 38, 14], we train a separate model for each of the 8 molecules. Both training and validation sets contain 1000 samples, and the rest are used for testing. Note that all baseline methods are trained on a joint loss of energies and forces, but different methods use different weights of force over energy (WoFE). For example, SchNet [29] sets WoEF as 100, while GemNet [33] uses a weight of 1000. For a fair comparison with existing studies, we conduct experiments on two widely used weights of 100 and 1000 following Liu et al. [14]. The results are summarized in Table 3. We can observe that when WoFE is 100, LEFTNet outperforms all baseline methods on 7 of the 8 molecules and improves previous SOTA results by 16% on average. In addition, LEFTNet can outperform all baseline methods on 6 of the 8 molecules when WoFE is 1000. These experimental results on MD17 demonstrate the performance of LEFTNet on vector-valued property prediction tasks. The ablation study in Appendix E also demonstrates that both LSE and FTE are important to the final results."
        },
        {
            "heading": "8 Limitation and Future Work",
            "text": "In this paper, we seek a general recipe for building 3D geometric graph deep learning algorithms. Considering common prior of 2D graphs, such as permutation symmetry, has been incorporated in off-the-shelf graph neural networks, we mainly focus on the E(3) and SE(3) symmetry specific to 3D geometric graphs. Despite our framework being general for modeling geometric objects, we only conducted experiments on commonly used molecular datasets. It\u2019s worth exploring datasets in other domains in the future.\nTo elucidate the future design space of equivariant GNNs, we propose two directions that are worth exploring. Firstly, our current algorithms consider fixed equivariant frames for performing aggregation and node updates. Inspired by the high body-order ACE approach [48] (for modeling atomcentered potentials), it is worth investigating in the future if equivariant frames that relate to many body (e.g., the PCA frame in [49]) can boost the performance of our algorithm. For example, to\nbuild the A-basis proposed in Puny et al. [49], we can replace our message aggregation Eq. 8 from summation to tensor product, which is also a valid pooling operation. Another direction is to explore geometric mesh graphs on manifolds M , where the local frame is defined on the tangent space of each point: F(x) \u2208 TxM . Since our scalarization technique (crucial for realizing LSE in LEFTNet) originates from differential geometry on frame bundles [22], it is reasonable to expect that our framework also works for manifold data [50, 51]."
        },
        {
            "heading": "A Supplementary Background",
            "text": "We briefly review the concept of (contravariant) tensor fields and their associated equivariant group representations.\nA s order (contravariant) tensor T on a vector space V is a multilinear map:\nT : V\u2217 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7V\u2217 \ufe38 \ufe37\ufe37 \ufe38\ns\n\u2192 R1,\nwhere V\u2217 denotes the dual space of V. In fact, there is a canonical \u2018multiplication\u2019 operation between two tensors. Define the tensor product S\u2297T of two tensors S and T to be a tensor of order r + s :\nS\u2297 T(v1, . . . , vr+s) = S(v1, . . . , vr)T(vr+1, . . . , vr+s). (11)\nwhere vi \u2208 V\u2217.\nFrom now on, we assume V = V \u2217 = R3. Note that when s = 1, T is exactly an equivariant vector. In practice, the tensor data in R3 is usually given by its coefficients under a Cartesian coordinate system. Take a second-order tensor as an example, assume we are given an orthonormal frame (basis) (e1, e2, e3) and its dual frame (e1, e2, e3), then the nine coefficients of T are given by\nTij = T(e i, ej), 1 \u2264 i, j \u2264 3.\nIn other words, we say the collection {Tij}1\u2264i,j\u22643 is a faithful representation of T in a fixed coordinate system:\nT = \u2211\ni,j\nTijei \u2297 ej . (12)\nOnce defined a tensor on R3, it\u2019s easy to extend it to a continuous manifold or a discrete graph. A tensor field of order s on a 3D graph G = (V,E) is a tensor-valued function f which assigns to each 3D node xi an order s tensor, denoted by f(xi).\nSE(3) Tensor Representations. Let V be a vector space, then the group SE(3) is said to act on V if there is a mapping \u03c6 : SE(3)\u00d7 V \u2192 V satisfying the following two conditions:\n1. if e \u2208 SE(3) is the identity element, then\n\u03c6(e, x) = x for \u2200x \u2208 V.\n2. if g1, g2 \u2208 SE(3), then\n\u03c6(g1, \u03c6(g2, x)) = \u03c6(g1g2, x) for \u2200x \u2208 V.\nIf we further require \u03c6(g, \u00b7) is a linear map for all g \u2208 SE(3), then \u03c6 becomes a group representation of SE(3). From now on, we only consider the rotation subgroup SO(3) and its group representations. When V = R3, there is a natural representation of SO(3) by rotating vectors in R3. In this way, an element g \u2208 SO(3) is identified with a Rotation matrix, denoted by {gji }1\u2264i,j\u22643.\nFrom the tensor definition (11), this natural representation on R3 induces a tensor representation on T . Still take T = {Tij}1\u2264i,j\u22643 as an example, we have\nTkl = \u2211\ni\n\u2211\nj\ngikg j l Tij , 1 \u2264 k, l \u2264 3, (13)\nfor \u2200g \u2208 SO(3). It\u2019s easy to check that (13) is indeed a SO(3) representation on the vector space spanned by second-order tensors.\nRelation with Spherical Harmonics. For the SO(3) group, all representations (including the tensor representations) can be decomposed as a direct sum of irreducible representations. For each type of irreducible representations, there is a subset of spherical harmonics formulating a basis for this specific representation. However, in terms of representing equivariant geometric quantities, the theorem in [52] claims that tensor representations and irreducible representations are equally powerful: They all form a complete basis in the space of continuous E(3) equivariant functions.\nAlgorithm 1 Invariant Design for LEFTNET.\n1: Input: Complete 3D gragh with equivariant positions X = (x1, . . . , xn) \u2208 Rn\u00d73, invariant node features hi \u2208 Rd, invariant relative distances dij \u2208 R1. 2: Centralize the positions: X\u2190 X\u2212 CoM(X). 3: for (i = 1; i < n; i++) do 4: for (j = 1; j < k; j ++) do 5: Compute edge-wise equivariant frames Fij via Eq. 17:\nFij = EquiFrame(xi, xj).\n6: Get the mutual 3D structure Si\u2212j , perform local scalarization:\ntij = Scalarize(Si\u2212j)\n7: Calculate the SE(3)-invariant structural coefficients: Aij = g(tij , dij) 8: Perform invariant message passing:\nmij = \u03c6m(hi, Aij \u2299 hj , dij)\n9: end for 10: Update invariant node features:\nhi = \u03c6h(hi, \u2211\nj\u2208N (i)\nmij)\n11: end for 12: Output: AvgPooling (h1, . . . , hn)."
        },
        {
            "heading": "B Related Proofs and Discussions of Section 3",
            "text": "In section 3, we proposed a novel hierarchy of local geometric isomorphisms that further motivates the design of incorporating the mutual 3D substructure\u2019s information into equivariant GNNs. Different from our fine-grained local characterization, a cocurrent work GWL [21]) proposes to measure geometric isomorphism from local to global by the k-hop partition.\nFrom another point of view, we essentially demonstrated that encoding mutual 3D substructures expands the capacity of the transformation function class with respect to an equivariant GNN. [24] put forward the Completeness concept for characterizing these transformation functions. However, it mainly concentrates on testing whether a function can discriminate global geometric isomorphism (in the sense of Eq. 5).\nDiscussion on the Completeness Concept. Following our terminology in the preliminary section, completeness of a transformation f can be translated into claiming that f is invariant among 3D graphs if and only if they are globally isomorphic (see definition (5)). Therefore, it\u2019s easy to refine the notion of completeness that adapts to our local version by replacing the global isomorphism to local isomorphism:\nf(X) = f(Y),\nif and only if X and Y are local {-tree, -triangular, -subgraph } isomorphic. Then, in terms of function class capacities, the following relation holds:\nGlobal complete \u2282 Subgraph complete \u2282 Triangular complete \u2282 Tree complete.\nNote that our equivalent description of complete transformation reveals the fact that the completeness concept in [24] is defined from the global 3D isomorphism point of view. Therefore, we shall claim that the above series of completeness notions belong to the structure completeness. Indeed, the theory developed in section 3 indicates that a GNN which can express structure complete functions may not be sufficient in expressing general tensor potential functions on a 3D graph.\nOn the other hand, a non-negligible proportion of 3D graph tasks may not be sensitive to the global 3D non-isomorphism. For example, some chemical properties (formulated as a function defined on molecular graphs) are characterized by local substructures [53]. In these scenarios, we are looking\nfor a geometric transformation f that is global non-complete, but (-tree, -triangular, -subgraph) local complete.\nProof of Theorem 3.1.\nProof. The first part of the theorem is proved by providing an explicit example. From the first 3D shapes of figure 1, the difference of the two triangular non-isomorphism shapes is indicated by an invariant function:\nd(xp, xm) = \u2016xp \u2212 xm\u2016 2 .\nNote that this function cannot be expressed by tree-level features, since there is no edge connecting xp and xm. However, since the position vectors xp and xm are included in the mutual 3D structure (corresponds to edge eiq), then they are ready to be scalarized by a local equivariant frame. Then quoting the universal approximation theorem from [23], there exists a corresponding invariant encoder \u03c6 that can approximate the function d(xk, xl). Since this function produces different output values for the two tree isometric but triangular non-isometric 3D shapes, we know that \u03c6 is able to distinguish 3D shapes beyond tree isomorphism.\nTo prove the second part and build up the injectivity condition, we first introduce the multi-set notation {{\u00b7}}, following [GIN]. A basic equivariant GNN based on our enhanced framework contains at least two steps: 1. Message passing, which is defined by (6); 2. Node-wise update:\nht+1i = MLP(m t i, h t i).\nFor simplicity, we denote the composition of the two steps by \u03a8. Then the additional injectivity condition is stated as follows:\n\u03a8({{hti, Ajih t i, , h t j |j \u2208 Ni}}, {{Aijh t j |j \u2208 Ni}}) (14)\nis injective, for each layer t and each node i. Note that this condition is realizable by adding weighted residue terms similar to [26]. Then, from the above condition, it\u2019s obvious that two non-identical collections of {{Aij}eij\u2208E} would yield two different feature vectors. Moreover, from the first part, there exist at least two distinct local 3D subgraphs with isometric local tree structure, such that the corresponding geometric weights {{Aij}eij\u2208E} that come out of the encoder \u03c6 are different.\nC Related Proofs and Discussions of Section 4\nTorsion Angle is Secretly Hidden in FT\nRecall the edge-wise (signed) torsion angle \u03c4ij involves the 1-hop atom pairs i and j and two 2-hop atoms k and l, then \u03c4ij is defined to be the dihedral angle between plane k\u2212 i\u2212 j and plane l\u2212 j\u2212 i. Although exhausting all torsion angles requires O(k2) complexity, [24] reduces the computation to O(k) order by selecting a canonical 2-hop atom k and l, which is enough for detecting the relative orientations between atoms (insufficient for general tasks like many body interactions).\nNow we show how \u03c4ij naturally appears as one of the derivatives from frame transition functions. For node i, define the equivariant frame Fi by\n(ei1, e i 2, e i 3) = (xi \u2212 xj, xi \u2212 xk, e i 1 \u00d7 e i 2).\nFi is normalized through the Gram-Schmidt algorithm. For node j, Fj is defined similarly by\n(ej1, e j 2, e j 3) = (xj \u2212 xi, xj \u2212 xl, e j 1 \u00d7 e j 2).\nThen following the transition formula (7),\nRij = (e i 1, e i 2, e i 3) \u00b7 (e j 1, e j 2, e j 3) T .\nNote that for an orthonormal matrix, its inverse equals its transpose. Then, by the standard definition of a dihedral angle, we have\n\u03c4ij = e i 3 \u00b7 e j 3 \u2261 Rij(3, 3).\nAlgorithm 2 Equivariant Design for LEFTNET.\n1: Input: Complete 3D gragh with equivariant positions X = (x1, . . . , xn) \u2208 Rn\u00d73, invariant node features hi \u2208 Rd, invariant relative distances dij \u2208 R1, equivariant edge features eij \u2208 R\nc. 2: Centralize the positions: X\u2190 X\u2212 CoM(X). 3: for (i = 1; i < n; i++) do 4: Compute node-wise equivariant frames Fi via Eq. 10 . 5: for (j = 1; j < k; j ++) do 6: Compute edge-wise equivariant frames Fij via Eq. 17:\nFij = EquiFrame(xi, xj)\n7: Get the mutual 3D structure Si\u2212j , perform local scalarization through Fij :\ntij = {Scalarize(Si\u2212j ,Fij)}\n8: Calculate the SE(3)-invariant structural coefficients: Aij = g(tij , dij) 9: Perform equivariant message passing as in Eq. 6:\nmij = \u03c6 1 m(hi, Aij \u2299 hj, dij) + \u03c6 2 m(hi, Aij \u2299 hj , dij) \u00b7 eij + \u03c6 3 m(hi, Aij \u2299 hj , dij) \u00b7 Fij\n10: end for 11: Equivariant message aggregation: mi = \u2211\nj\u2208N (i) mij ; 12: Transform equivariant node features through Fi:\nti = Scalarize(mi,Fi)\n13: Update invariant node features: hi = \u03c6h(hi, ti)\n14: Equivariant Output: Perform tensorization through Fi:\nhi = Tensorize(hi,Fi).\n15: end for\nIn conclusion, \u03c4ij is just one component of the transition matrix. However, to fully determine Rij , we still need another two angles (since a transition matrix is uniquely determined by three Euler angles).\nProof of Theorem 4.1.\nProof. This theorem is proved in two steps:\n1. The first step characterizes all scalars determined by (isolated) local clusters B and C through equivariant frames and local scalarization;\n2. The second step constructs a specific invariant function fa(B,C) that cannot be expressed by the local scalarization.\nLet G denote a 3D point cloud. Then, as it has been proved in Du et al. [23], once equipped with an equivariant frame FG, all equivariant features of G can be transformed to scalar features through scalarization without information loss. Following the convention in the main text, let G\u0303 be the output of performing scalarization on the original features G, then for any invariant function f(G), there exists a corresponding function f\u0303 such that\nf(G) = f\u0304(G\u0303). (15)\nSince SG is the collection of all invariant scalars produced by G, (15) implies that SG is generated by G\u0303, which is only a finite subset of SG. To apply the above insight to our current theorem, note that we have two different 3D clouds B and C. Therefore, we need to build two local equivariant frames FB = (eB1 , e B 2 , e B 3) andFC = (e C 1 , e C 2 , e C 3 ). The crucial point is the frameFB itself doesn\u2019t depend on\nC, and the scalarization through FB is only performed on the local point cloud B. Operations like scalarizing equivariant information of C through FB would break the assumptions of the theorem.\nNow we are ready to construct an explicit interaction potential fa(B,C):\nfa(B,C) := e B 1 \u00b7 e C 1 .\nSince fa is an inner product of two equivariant vectors, it\u2019s automatically an invariant function. Then we need to check whether fa(B,C) is a function of fa(SB, SC). Note that the equivariant building block of fa that relates to B is exactly eB1 . Then, following the above local scalarization principle, we scalarize eB1 through FB and get:\neB1 \u2192 e\u0303 C 1 = (1, 0, 0).\nSimilarly, eC1 is also transformed to a constant scalar tuple e\u0303 C 1 = (1, 0, 0) through FC. As constant inputs generate constant outputs, we conclude that the deduced local scalars can only approximate constant functions. However, since the local frames are changing as we vary the 3D structure of B and C, it\u2019s obvious that eB1 \u00b7 e C 1 is not a constant function of (B,C). Therefore, we finish the proof by contradiction.\nRealizing FT by Equivariant Messages: From the FT definition 7, each element of the 3 \u00d7 3 matrix Rij is calculated by\nRij(k, l) = e i k \u00b7 e j l . (16)\nNow we show how to reproduce Rij(k, l) through equivariant messages. Let the equivariant message mi be the following:\nmi := (e i 1, e i 2, e i 3) \u00b7\n\n 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n\n\nIt\u2019s easy to check that mi \u2208 R3\u00d79 consists of 9 equivariant vectors (multi-channels). For atom j, mj is defined symmetrically. For each node, we also store the scalar messages, e.g., \u2225 \u2225eik \u2225 \u2225 for 1 \u2264 k \u2264 3. Flattening the whole matrix Rij into a R1\u00d79 array, then Rij is obtained by simple summation and taking the vector norm:\n\u2016mi + mj\u2016 = {\u2225 \u2225 \u2225e i k + e j l \u2225 \u2225 \u2225 }\n1\u2264k,l\u22643 ,\nwhere the norm is taken for each column of mi + mj , such that \u2016mi + mj\u2016 \u2208 R1\u00d79. Then,\nRij = 1\n2\n[\u2225 \u2225 \u2225e i k + e j l \u2225 \u2225 \u2225 2 \u2212 \u2225 \u2225eik \u2225 \u2225 2 \u2212 \u2225 \u2225 \u2225e j l \u2225 \u2225 \u2225 2 ] .\nOur illustration also demonstrates the importance of keeping multi-channel tensor messages.\nRelation with Previous Equivariant Update Methods. Following the efficiency principle established in section 4, we don\u2019t encode the data of the transition matrices explicitly. Instead, we implement tensor messages to fill in the expressiveness gap. Among the tremendously different designs of equivariant graph neural networks, Sch\u00fctt et al. [31] is closely related to our equivariant updating method. By the above argument, the inner product operation for node i (see (9) of Sch\u00fctt et al. [31])\n< Uvi,Vvi >\ncan also be reinterpreted as a realization of the (aggregated) frame transition matrix (7).\nMoreover, since the equivariant vectors Uvi and Vvi are both aggregated vector features that belong to the same node i and the inner product operation between them is performed in the node-wise updating phase, Sch\u00fctt et al. [31] actually avoids the 2-hop O(k2) complexity of computing Rxy for all neighborhood node pairs (x, y) (while able to express the torsion angle implicitly). For our algorithm, we utilize the scalarization and tensorization in the node-wise updating phase. By the universal approximation theorem 5.1, our method can approximate any inner product operations."
        },
        {
            "heading": "D Related Proofs and Discussions of Section 5",
            "text": "Equivariant Frames and Higher Order Scalarization and Tensorization. Given an edge eij with two atom\u2019s positions (xi,xj), our edge-wise SE(3) equivariant frames Fij are defined by:\n(e1, e2, e3) = ( xi \u2212 xj \u2016xi \u2212 xj\u2016 , xi \u00d7 xj \u2016xi \u00d7 xj\u2016 , xi \u2212 xj \u2016xi \u2212 xj\u2016 \u00d7 xi \u00d7 xj \u2016xi \u00d7 xj\u2016 ). (17)\nTo make the frame translation invariant, we follow previous works [54, 55] by limiting the whole 3D conformers\u2019 space to a linear subspace where the center of mass (CoM) of the system (either the whole system or the sub-cluster where i and j belong to) is zero. On the other hand, building an E(3) frame requires an additional atom\u2019s position xk, which can be selected by K-Nearest Neightbor algorithm. Then if (xi,xj ,xk) spans the 3D space, we obtain an E(3) equivariant frame by performing the gram-schmidt orthogonalization.\nOnce we have an equivariant frame, every vector is a linear combination of the three orthogonal vectors in the frame. Moreover, the unique combination coefficients are exactly the \u2019scalarized\u2019 coordinates in (1). A similar procedure also applies to higher order tensors. Indeed, the vector frame F1 extends to a tensor frame Fr of arbitrary order r > 1:\nFr := {e11 \u2297 \u00b7 \u00b7 \u00b7 \u2297 e1r}1\u2264i1,...,ir\u22643. (18)\nSince the orthonormal frame Fr is complete in the sense that it spans the whole tensor space of order r, every r-th order tensor admits a unique decomposition:\nT = \u2211\n1\u2264i1,...,ir\u22643\nT i1,...,irei1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 eir . (19)\nIt\u2019s easy to prove that the collection {T i1,...,ir}1\u2264i1,...,ir\u22643 consists of invariant scalars. We call the process from T to {T i1,...,ir}1\u2264i1,...,ir\u22643 scalaraization.\nTensorization is the inverse of scalarization, in the sense that it sends scalars {T i1,...,ir}1\u2264i1,...,ir\u22643 to tensor T. Under the same frames we use during scalarization, the following diagram demonstrates the pipeline of producing L second-order tensors out of {T i1i2j }1\u2264i1,i2\u22643:\n{T1, . . . ,TL} =\n \n\n\n\nT 111 , T 12 1 , T 13 1 T 211 , T 22 1 , T 23 1 T 311 , T 32 1 , T 33 1\n\n , . . . ,\n\n\nT 11L , T 12 L , T 13 L T 21L , T 22 L , T 23 L T 31L , T 32 L , T 33 L\n\n\n \n\n\ufe38 \ufe37\ufe37 \ufe38\nL channels\n\u2299 [ e1 \u2297 e1, e1 \u2297 e2, e1 \u2297 e3 e2 \u2297 e1, e2 \u2297 e2, e2 \u2297 e3 e3 \u2297 e1, e3 \u2297 e2, e3 \u2297 e3 ] ,\n(20) where \u2299 denotes the element-wise product.\nProof of Theorem 5.1\nProof. The proof is based on the fact that Scalarization and Tensorization are invertible (see Appendix A.5 of Du et al. [23] ). In other words, we have the following commutative dia-\ngram: Tl\u22121 Tl\nT\u0303 l\u22121 T\u0303 l\u22121.\n\u03c1\nScalarize\nMLP\nTensorize Therefore, for each mapping \u03c1, we can always find a correspond-\ning\u2018scalarized\u2019 mapping \u03c1\u0303: \u03c1\u0303 := Tensorize \u25e6 \u03c1 \u25e6 Scalarize.\nNow we have turned from expressing equivariant \u03c1 to the invariant \u03c1\u0303. Note that MLP is a universal approximator of invariant functions, therefore we can always find a MLP to express \u03c1\u0303. By reserving the arrows, we finish the proof.\nProof of Equivariance for LEFTNet LEFTNet consists of multiple layers of LSE and FTE. LSE is realized by scalarization, and FTE is realized by scalarization and tensorization. Since the invariance of scalarization and the equivariance of tensorization have been proved, we finish the proof."
        },
        {
            "heading": "E Additional Experimental Results",
            "text": "Ablation Study. As discussed in Section 5, there are two main modules in LEFTNet, namely LSE and FTE. We conduct experiments on QM9 and MD17 to show the importance of each component. Experimental results are summarized in Table 4 and Table 5. The results show that using LSE can outperform the model without both LSE and FTE on all tasks. Adding FTE can further improve the performance. The results demonstrate the importance of LSE and FTE modules.\nResults on rMD17. Following [25], we conduct experiments on rMD17 to compare with recent studies. Results show that our LEFTNet can achieve comparable performance to state-of-the-art methods such as MACE and NequIP, while outperforming other baseline methods like GemNet and PaiNN.\nModel and training hyperparameters. Model and training hyperparameters for our method on different datasets are listed in Table 7."
        }
    ],
    "title": "A new perspective on building efficient and expressive 3D equivariant graph neural networks",
    "year": 2023
}