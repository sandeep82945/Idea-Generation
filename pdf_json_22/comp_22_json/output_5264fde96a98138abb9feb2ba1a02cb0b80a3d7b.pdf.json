{
    "abstractText": "Most speech enhancement (SE) models learn a point estimate and do not make use of uncertainty estimation in the learning process. In this paper, we show that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian negative log-likelihood (NLL) improves SE performance at no extra cost. During training, our approach augments a model learning complex spectral mapping with a temporary submodel to predict the covariance of the enhancement error at each time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the covariance introduces an undersampling effect, detrimental to SE performance. To mitigate undersampling, our approach inflates the uncertainty lower bound and weights each loss component with their uncertainty, effectively compensating severely undersampled components with more penalties. Our multivariate setting reveals common covariance assumptions such as scalar and diagonal matrices. By weakening these assumptions, we show that the NLL achieves superior performance compared to popular loss functions including the mean squared error (MSE), mean absolute error (MAE), and scale-invariant signal-to-distortion ratio (SI-SDR).",
    "authors": [
        {
            "affiliations": [],
            "name": "Kuan-Lin Chen"
        },
        {
            "affiliations": [],
            "name": "Daniel D. E. Wong"
        },
        {
            "affiliations": [],
            "name": "Ke Tan"
        },
        {
            "affiliations": [],
            "name": "Buye Xu"
        },
        {
            "affiliations": [],
            "name": "Anurag Kumar"
        },
        {
            "affiliations": [],
            "name": "Vamsi Krishna Ithapu"
        }
    ],
    "id": "SP:c8ac42d60296db875598a0682abe979c631423d3",
    "references": [
        {
            "authors": [
                "Y. Hsu",
                "Y. Lee",
                "M.R. Bai"
            ],
            "title": "Learning-based personal speech enhancement for teleconferencing by exploiting spatialspectral features",
            "venue": "ICASSP. IEEE, 2022, pp. 8787\u20138791.",
            "year": 2022
        },
        {
            "authors": [
                "L. Pisha",
                "J. Warchall",
                "T. Zubatiy",
                "S. Hamilton",
                "C.-H. Lee",
                "G. Chockalingam",
                "P.P. Mercier",
                "R. Gupta",
                "B.D. Rao",
                "H. Garudadri"
            ],
            "title": "A wearable, extensible, open-source platform for hearing healthcare research",
            "venue": "IEEE Access, vol. 7, pp. 162083\u2013 162101, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Pisha",
                "S. Hamilton",
                "D. Sengupta",
                "C.-H. Lee",
                "K.C. Vastare",
                "T. Zubatiy",
                "S. Luna",
                "C. Yalcin",
                "A. Grant",
                "R. Gupta",
                "G. Chockalingam",
                "B.D. Rao",
                "H. Garudadri"
            ],
            "title": "A wearable platform for research in augmented hearing",
            "venue": "Asilomar Conference on Signals, Systems, and Computers. IEEE, 2018, pp. 223\u2013227.",
            "year": 2018
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303\u2013314, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks, vol. 2, no. 5, pp. 359\u2013366, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "M. Telgarsky"
            ],
            "title": "Benefits of depth in neural networks",
            "venue": "Conference on Learning Theory. PMLR, 2016, pp. 1517\u20131539.",
            "year": 2016
        },
        {
            "authors": [
                "X. Lu",
                "Y. Tsao",
                "S. Matsuda",
                "C. Hori"
            ],
            "title": "Speech enhancement based on deep denoising autoencoder",
            "venue": "Interspeech, 2013, pp. 436\u2013440.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Xu",
                "J. Du",
                "L.-R. Dai",
                "C.-H. Lee"
            ],
            "title": "An experimental study on speech enhancement based on deep neural networks",
            "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Xu",
                "J. Du",
                "L.-R. Dai",
                "C.-H. Lee"
            ],
            "title": "A regression approach to speech enhancement based on deep neural networks",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D. Wang",
                "J. Chen"
            ],
            "title": "Supervised speech separation based on deep learning: An overview",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702\u20131726, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Tan",
                "D. Wang"
            ],
            "title": "A convolutional recurrent neural network for real-time speech enhancement",
            "venue": "Interspeech, 2018, pp. 3229\u20133233.",
            "year": 2018
        },
        {
            "authors": [
                "A. Pandey",
                "D. Wang"
            ],
            "title": "TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain",
            "venue": "ICASSP. IEEE, 2019, pp. 6875\u20136879.",
            "year": 2019
        },
        {
            "authors": [
                "K. Tan",
                "D. Wang"
            ],
            "title": "Learning complex spectral mapping with gated convolutional recurrent networks for monaural speech enhancement",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 380\u2013390, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Hu",
                "Y. Liu",
                "S. Lv",
                "M. Xing",
                "S. Zhang",
                "Y. Fu",
                "J. Wu",
                "B. Zhang",
                "L. Xie"
            ],
            "title": "DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement",
            "venue": "Interspeech, 2020, pp. 2472\u20132476.",
            "year": 2020
        },
        {
            "authors": [
                "X. Hao",
                "X. Su",
                "R. Horaud",
                "X. Li"
            ],
            "title": "Fullsubnet: A full-band and sub-band fusion model for real-time single-channel speech enhancement",
            "venue": "ICASSP. IEEE, 2021, pp. 6633\u20136637.",
            "year": 2021
        },
        {
            "authors": [
                "A. Li",
                "S. You",
                "G. Yu",
                "C. Zheng",
                "X. Li"
            ],
            "title": "Taylor, can you hear me now? A Taylor-unfolding framework for monaural speech enhancement",
            "venue": "International Joint Conference on Artificial Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Gawlikowski",
                "C.R.N. Tassi",
                "M. Ali",
                "J. Lee",
                "M. Humt",
                "J. Feng",
                "A. Kruspe",
                "R. Triebel",
                "P. Jung",
                "R. Roscher",
                "M. Shahzad",
                "W. Yang",
                "R. Bamler",
                "X.X. Zhu"
            ],
            "title": "A survey of uncertainty in deep neural networks",
            "venue": "arXiv preprint arXiv:2107.03342, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.-W. Fu",
                "T.-y. Hu",
                "Y. Tsao",
                "X. Lu"
            ],
            "title": "Complex spectrogram enhancement by convolutional neural network with multi-metrics learning",
            "venue": "International Workshop on Machine Learning for Signal Processing. IEEE, 2017, pp. 1\u20136.",
            "year": 2017
        },
        {
            "authors": [
                "D.A. Nix",
                "A.S. Weigend"
            ],
            "title": "Estimating the mean and variance of the target probability distribution",
            "venue": "International Conference on Neural Networks. IEEE, 1994, vol. 1, pp. 55\u2013 60.",
            "year": 1994
        },
        {
            "authors": [
                "A. Kendall",
                "Y. Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision",
            "venue": "Advances in Neural Information Processing Systems, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Amini",
                "W. Schwarting",
                "A. Soleimany",
                "D. Rus"
            ],
            "title": "Deep evidential regression",
            "venue": "Advances in Neural Information Processing Systems, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Fang",
                "T. Peer",
                "S. Wermter",
                "T. Gerkmann"
            ],
            "title": "Integrating statistical uncertainty into neural network-based speech enhancement",
            "venue": "ICASSP. IEEE, 2022, pp. 386\u2013390.",
            "year": 2022
        },
        {
            "authors": [
                "J. Le Roux",
                "S. Wisdom",
                "H. Erdogan",
                "J.R. Hershey"
            ],
            "title": "SDR\u2013 half-baked or well done",
            "venue": "ICASSP. IEEE, 2019, pp. 626\u2013 630.",
            "year": 2019
        },
        {
            "authors": [
                "M. Kolb\u00e6k",
                "Z.-H. Tan",
                "S.H. Jensen",
                "J. Jensen"
            ],
            "title": "On loss functions for supervised monaural time-domain speech enhancement",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 825\u2013838, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Seitzer",
                "A. Tavakoli",
                "D. Antic",
                "G. Martius"
            ],
            "title": "On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R.F. Astudillo",
                "D. Kolossa",
                "R. Orglmeister"
            ],
            "title": "Accounting for the uncertainty of speech estimates in the complex domain for minimum mean square error speech enhancement",
            "venue": "Interspeech, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "T. Gerkmann",
                "E. Vincent"
            ],
            "title": "Spectral masking and filtering",
            "venue": "Audio Source Separation and Speech Enhancement. 2018, pp. 65\u201385, John Wiley & Sons, Ltd.",
            "year": 2018
        },
        {
            "authors": [
                "C.K. Reddy",
                "H. Dubey",
                "K. Koishida",
                "A. Nair",
                "V. Gopal",
                "R. Cutler",
                "S. Braun",
                "H. Gamper",
                "R. Aichner",
                "S. Srinivasan"
            ],
            "title": "INTERSPEECH 2021 deep noise suppression challenge",
            "venue": "Interspeech, 2021, pp. 2796\u20132800.",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.W. Rix",
                "J.G. Beerends",
                "M.P. Hollier",
                "A.P. Hekstra"
            ],
            "title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs",
            "venue": "ICASSP. IEEE, 2001, vol. 2, pp. 749\u2013752.",
            "year": 2001
        },
        {
            "authors": [
                "C.H. Taal",
                "R.C. Hendriks",
                "R. Heusdens",
                "J. Jensen"
            ],
            "title": "An algorithm for intelligibility prediction of time\u2013frequency weighted noisy speech",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125\u20132136, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "P. Manocha",
                "A. Kumar"
            ],
            "title": "Speech quality assessment through MOS using non-matching references",
            "venue": "Interspeech, 2022, pp. 654\u2013658.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 1.\n08 62\n4v 3\n[ cs\n.S D\n] 8\nM ar\n2 02\ndo not make use of uncertainty estimation in the learning process. In this paper, we show that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian negative log-likelihood (NLL) improves SE performance at no extra cost. During training, our approach augments a model learning complex spectral mapping with a temporary submodel to predict the covariance of the enhancement error at each time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the covariance introduces an undersampling effect, detrimental to SE performance. To mitigate undersampling, our approach inflates the uncertainty lower bound and weights each loss component with their uncertainty, effectively compensating severely undersampled components with more penalties. Our multivariate setting reveals common covariance assumptions such as scalar and diagonal matrices. By weakening these assumptions, we show that the NLL achieves superior performance compared to popular loss functions including the mean squared error (MSE), mean absolute error (MAE), and scale-invariant signal-to-distortion ratio (SI-SDR).\nIndex Terms\u2014 Uncertainty, negative log-likelihood, neural net-\nworks, complex spectral mapping, speech enhancement\n1. INTRODUCTION\nSpeech enhancement (SE) aims at improving speech quality and intelligibility via recovering clean speech components from noisy recordings. It is an essential part of many applications such as teleconferencing [1], hearing aids [2], and augmented hearing systems [3]. Modern SE approaches usually train a deep neural network (DNN) model to minimize a loss function on a target speech representation. Because DNNs can be universal approximators [4, 5, 6] and capable of learning anything incentivized by the loss, designing DNN architectures on different target representations has been the most popular trend in SE. Literature in this area is vast, [7, 8, 9, 10, 11, 12, 13, 14, 15, 16] for example.\nDespite such progress, the most popular loss functions such as the MSE and MAE in SE make no or little use of uncertainty. We refer the reader to an excellent survey paper by Gawlikowski et al. [17] for a thorough discussion of uncertainty in DNNs. From a probabilistic point of view, one can derive a loss function from a probabilistic distribution subject to certain constraints. For example, minimizing the MSE loss is equivalent to maximizing a Gaussian likelihood that assumes homoscedastic uncertainty, meaning that the variance associated with each squared error is a constant. The MAE loss also follows the same logic but with a Laplacian distribution.\n\u2020Work done during an internship at Meta Reality Labs Research. \u2217 The corresponding author.\nTaking complex spectral mapping [18, 13] for instance, optimizing the MSE or MAE on the complex spectrogram implicitly assumes a constant variance of the enhancement error on the real and imaginary parts at every time-frequency (T-F) bin. In fact, such an assumption even extends to speech signals in dissimilar noise conditions, e.g., different signal-to-noise ratios (SNRs). Although these loss functions are easy to use, they could limit the learning capability of a DNN model due to the underlying constant variance assumption.\nThe first effort in the literature training a neural network to minimize a Gaussian NLL dates back to a seminal work by Nix and Weigend [19]. Although the Gaussian NLL has been earlier used in computer vision [20, 21], its potential in SE remained unexplored until a recent work by Fang et al. [22]. They showed that a hybrid loss combining the SI-SDR [23, 24] and the Gaussian NLL can outperform both MSE and SI-SDR losses. However, they also reported that minimizing a Gaussian NLL alone leads to inferior SE performance, highlighting the difficulty of using heteroscedastic uncertainty to improve perceptual scores in SE.\nIn this paper, we show that, at no extra cost in terms of compute, memory, and parameters, directly minimizing a Gaussian NLL yields significantly better SE performance than minimizing a conventional loss such as the MAE or MSE, and slightly better SE performance than the SI-SDR loss. To the best of our knowledge, this is the first successful study that achieves improved perceptual metric performance by directly using heteroscedastic uncertainty for SE. Inspired by recent progress in uncertainty estimation [25], we reveal the main optimization difficulty and propose two methods: i) covariance regularization and ii) uncertainty weighting to overcome such a hurdle. Experiments show that minimizing Gaussian NLLs using these methods consistently improves SE performance in terms of speech quality and objective intelligibility.\n2. PROBABILISTIC MODELS AND ASSUMPTIONS\nLet the received signal at a single microphone in the short-time Fourier transform (STFT) domain be yt,fr + iy t,f i \u2208 C for all (t, f) with the time frame index t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , T} and frequency bin index f \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , F}. Let y \u2208 R2TF be the vector representing every real part and imaginary part of the STFT representation of the received signal. We assume the clean signal is corrupted by additive noise, i.e., y = x + v where x and v are the clean signal random vector and noise random vector, respectively, from the probabilistic perspective. Now, we assume that the probability density of the clean signal given the received noisy signal and a conditional density model follows a multivariate Gaussian distribution\np (x|y;\u03c8) = exp\n(\n\u2212 1 2 [x\u2212 \u00b5\u0302\u03b8(y)] T \u03a3\u0302\u22121\u03c6 (y) [x\u2212 \u00b5\u0302\u03b8(y)] )\n\u221a (2\u03c0)n det \u03a3\u0302\u03c6(y) (1)\nwhere its conditional mean \u00b5\u0302\u03b8(y) and covariance \u03a3\u0302\u03c6(y) are directly learned from a dataset by a conditional density model f\u03c8 parameterized by \u03c8 = {\u03b8, \u03c6} in supervised speech enhancement. Fig. 1 illustrates the conditional density model f\u03c8 and its difference compared to conventional SE models that only estimate clean speech. The map f\u03c8 can be expressed as an augmented map consisting of an essential SE model f\u03b8 and a temporary submodel f\u03c6 such that\n[ \u00b5\u0302\u03b8(y)\nvec [ L\u0302\u03c6(y) ]\n]\n= [ f\u03b8(y) f\u03c6(y\u0303) ] = f\u03c8(y) (2)\nwhere f\u03c8 , f\u03b8 , and f\u03c6 are DNN models parameterized by \u03c8, \u03b8, and \u03c6, respectively. vec[\u00b7] is an operator that vectorizes a matrix. Because a valid covariance is symmetric positive semidefinite, the output of f\u03c6 must be constrained to satisfy the property. To avoid imposing such a constraint, we design the map f\u03c6 to estimate the lower Cholesky factor L\u0302\u03c6 of the covariance. The covariance can be later obtained by \u03a3\u0302\u03c6(y) = L\u0302\u03c6(y)L\u0302 T\n\u03c6(y). y\u0303 is the input feature of f\u03c6 and a function of y, which can be y, an intermediate representation produced by f\u03b8 , or a combination of both. One can design different DNN architectures for f\u03b8 and f\u03c6. For example, f\u03c8 can be an integrated DNN with two output branches, one for clean speech estimation and the other for covariance estimation, with shared weights between f\u03b8 and f\u03c6, i.e. \u03b8 \u2229 \u03c6 6= \u2205. Alternatively, f\u03b8 and f\u03c6 can be two separate DNNs, i.e. \u03b8 \u2229 \u03c6 = \u2205. Below we point out key features of our framework.\nRemark 1. The conditional mean \u00b5\u0302\u03b8(y) is the enhanced signal so the submodel f\u03c6 can be removed at inference time. Hence, one can use a much larger parameter set \u03c6 to design f\u03c6 without increasing the complexity of the SE model at inference time.\nRemark 2. The conditional covariance \u03a3\u0302\u03c6(y) is also referred to as the uncertainty in this paper. Its homoscedasticity or heteroscedasticity is determined by assumptions made for the structure of the covariance. For example, the covariance can be assumed as a scalar matrix, a diagonal matrix, or a block diagonal matrix (see \u00a73).\nRemark 3. The form of conditional density in (1) can be obtained by assuming x and v are drawn from two multivariate Gaussian distributions. A Wiener filter can be realized by estimating the mean and covariance of the joint distribution of x and v. However, this approach requires a model to estimate more parameters, and such an extra cost cannot be removed at inference time.\nGiven a dataset {xn, yn} N n=1 containing pairs of target clean signal xn and received noisy signal yn, we find the conditional mean \u00b5\u0302\u03b8(y) and covariance \u03a3\u0302\u03c6(y) maximizing the likelihood of the joint probability distribution p(x1, x2, \u00b7 \u00b7 \u00b7 , xN |y1, y2, \u00b7 \u00b7 \u00b7 , yN ;\u03c8) = \u220fN n=1 p (xn|yn;\u03c8) where we assume the data points are independent and identically distributed (i.i.d.).\n3. MULTIVARIATE GAUSSIAN NLL\nIntroducing the logarithmic function to the likelihood of the joint distribution and expanding terms according to (1), the maximization problem can be converted into minimizing the empirical risk using the following multivariate Gaussian NLL loss\n\u2113 Full x,y(\u03c8) = [x\u2212 \u00b5\u0302\u03b8(y)] T \u03a3\u0302\u22121\u03c6 (y) [x\u2212 \u00b5\u0302\u03b8(y)]+log det \u03a3\u0302\u03c6(y) (3)\nof which the first term is an affinely transformed squared error between clean and enhanced speech, and the second term is a logdeterminant term. Without imposing any assumptions on the covariance \u03a3\u0302\u03c6, the multivariate Gaussian NLL \u2113 Full x,y(\u03c8) in (3) uses a full matrix for the covariance. A full covariance matrix relaxes common assumptions such as uncorrelated real part and imaginary part at each T-F bin and uncorrelated T-F bins [26, 27]. Although \u2113Fullx,y(\u03c8) is the most generalized formulation for a Gaussian NLL, the number of output units of the submodel f\u03c6 is 4T 2F 2, leading to exceedingly high training complexity. Assumptions (\u00a73.1, \u00a73.2, and \u00a73.3) are made to sparsify the covariance matrix, which in turn, reduces the complexity of the submodel and makes training amenable."
        },
        {
            "heading": "3.1. Homoscedastic Uncertainty: An MSE Loss",
            "text": "If the covariance \u03a3\u0302\u03c6(y) is assumed to be a scalar matrix \u03a3\u0302\u03c6(y) = cI where c is a scalar constant and I is an identity matrix, then we actually assume the uncertainty is homoscedastic. The log-determinant term in (3) becomes a constant, and the affinely transformed squared error reduces to an MSE. In this case, minimizing the Gaussian NLL is equivalent to the empirical risk minimization using an MSE loss \u2113MSEx,y (\u03b8) = \u2016x\u2212\u00b5\u0302\u03b8(y)\u2016 2 2.Apparently, the submodel f\u03c6 is not needed for an MSE loss so the optimization is performed only on \u03b8. Many SE works fall into this category, e.g., [7, 8, 10, 12, 13]."
        },
        {
            "heading": "3.2. Heteroscedastic Uncertainty: A Diagonal Case",
            "text": "If every random variable in the random vector drawn from the conditional density p(x|y) is assumed to be uncorrelated with the others, then the covariance reduces to a diagonal matrix. In this case, the Gaussian NLL ignores uncertainties across different T-F bins and between real and imaginary parts, leading to the following uncorrelated Gaussian NLL loss\n\u2113 Diagonal x,y (\u03c8) =\n\u2211\nt,f\n\u2211\nk\u2208{r,i}\n[\nx t,f k \u2212 \u00b5\u0302 t,f k;\u03b8(y)\n\u03c3\u0302 t,f k;\u03c6(y)\n]2\n+ 2 log \u03c3\u0302t,fk;\u03c6(y) (4)\nwhere \u03c3\u0302 t,f r;\u03c6 and \u03c3\u0302 t,f i;\u03c6 denote the conditional standard deviation for the real and imaginary parts at (t, f) bin, \u00b5\u0302t,fr;\u03c6 and \u00b5\u0302 t,f i;\u03c6 denote their conditional means, and xt,fr and x t,f i denote the real and imaginary parts of x at (t, f) bin. In this case, the number of output units of the submodel f\u03c6 is 2TF . Note that the Gaussian NLL derived by Fang et al. [22] assumes circularly symmetric complex Gaussian distributions for both clean speech and noise. Such a circularly symmetric assumption is stronger than the assumption used in (4). Consequently, their Gaussian NLL only has a variance term associated with each T-F bin whereas our formulation in (4) allows the real and imaginary parts have their own variance."
        },
        {
            "heading": "3.3. Heteroscedastic Uncertainty: A Block Diagonal Case",
            "text": "We relax the uncorrelated assumption imposed between every real and imaginary part in \u00a73.2 to take more uncertainty into account.\nIn this case, the conditional covariance becomes a block diagonal matrix consisting of 2-by-2 blocks, giving the Gaussian NLL loss\n\u2113 Block x,y (\u03c8) =\n\u2211\nt,f\nd t,f \u03b8,x(y) T\n[ \u03a3\u0302t,f\u03c6 (y) ]\u22121 d t,f \u03b8,x(y) + log t t,f \u03c6 (y)\n\ufe38 \ufe37\ufe37 \ufe38\nz t,f x,y(\u03c8)\n(5)\nwhere t t,f \u03b8 (y) =\n[\n\u03c3\u0302 t,f r;\u03c6(y)\u03c3\u0302 t,f i;\u03c6(y)\n]2 \u2212 [\n\u03c3\u0302 t,f ri;\u03c6(y)\n]2\n, d t,f \u03b8 (y) =\n[\nxt,fr \u2212 \u00b5\u0302 t,f r;\u03b8(y) x t,f i \u2212 \u00b5\u0302 t,f i;\u03b8 (y)\n]\n, \u03a3\u0302t,f\u03c6 (y) =\n\n\n[\n\u03c3\u0302 t,f r;\u03c6(y)\n]2\n\u03c3\u0302 t,f ri;\u03c6(y)\n\u03c3\u0302 t,f ri;\u03c6(y)\n[\n\u03c3\u0302 t,f i;\u03c6(y)\n]2\n\n , and\n\u03c3\u0302 t,f ri;\u03c6 is the covariance between the real and imaginary parts at (t, f) bin. Compared to the uncorrelated case in \u00a73.2, the submodel f\u03c6 needs to additionally predict one more parameter at every (t, f) bin, resulting in a submodel with 3TF output units, while the inferencetime complexity of the SE model f\u03b8 remains the same as using an MSE loss or uncorrelated Gaussian NLL loss (Remark 1).\n4. ON MITIGATING UNDERSAMPLING\nThe optimization difficulty in minimizing a Gaussian NLL can be revealed by its average first-order derivative. Taking the uncorrelated Gaussian NLL for example, the expected first-order derivative of \u2113Digonalx,y with respect to \u00b5\u0302 t,f r;\u03b8 can be approximated by\nEx,y\n[\n\u2202\u2113Diagonalx,y\n\u2202\u00b5\u0302 t,f r;\u03b8\n]\n\u2248 2\nN\nN\u2211\nn=1\n\u00b5\u0302 t,f r;\u03b8(yn)\u2212 x t,f n,r\n[\n\u03c3\u0302 t,f r;\u03c6(yn)\n]2 . (6)\nGiven the unconstrained variance in the denominator, a larger variance makes the model f\u03b8 harder to converge to a clean component compared to a loss component with a smaller variance. This undersampling issue was pointed out in a recent work by Seitzer et al. [25], in which they proposed the \u03b2-NLL to mitigate undersampling. However, the \u03b2-NLL was only developed for the univariate Gaussian NLL. It can be used in (4) because the uncorrelated multivariate Gaussian NLL can be decomposed into many univariate Gaussian NNLs, while (5) can only be decomposed into many bivariate Gaussian NLLs, which requires a more generalized approach."
        },
        {
            "heading": "4.1. Covariance Regularization",
            "text": "Let \u03b4 > 0 be the lower bound of the eigenvalues of the Cholesy factor of the covariance matrix. The output of f\u03c6 is modified by\n[\nL\u0302 \u03b4 \u03c6(y)\n]\nmm = max\n{[ L\u0302\u03c6(y) ] mm , \u03b4 } . (7)\nfor all m \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 2TF} where L\u0302\u03b4\u03c6(y) is now the regularized output of f\u03c6. As the degree of undersampling is affected by the ratio of the largest variance to the smallest variance, suitably increasing \u03b4 can reduce the ratio and hence mitigate undersampling. However, a large \u03b4 can saturate uncertainties, driving the NLL toward the MSE."
        },
        {
            "heading": "4.2. Uncertainty Weighting",
            "text": "Because a large variance can make the gradient of a loss component small, assigning a larger weight for such a loss component would alleviate undersampling. This is the intuition of \u03b2-NLL. To extend it to a multivariate Gaussian NLL, we propose an uncertainty weighting approach, which assigns a larger weight for a loss component\naccording to the minimum eigenvalue of the covariance matrix. Applying such a strategy to (5) leads to the following loss function\n\u2113 \u03b2-Block x,y (\u03c8) =\n\u2211\nt,f\n\u03bbmin\n[ \u03a3\u0302t,f\u03c6 (y) ]\u03b2 z t,f x,y(\u03c8) (8)\nwhere \u03bbmin [\u00b7] gives the minimum eigenvalue which is treated as a constant. No gradients are propagated through \u03bbmin [\u00b7]. \u03b2 \u2208 [0, 1] is a hyperparameter controlling the degree of uncertainty weighting. When \u03b2 = 0, \u2113\u03b2-Blockx,y (\u03c8) reduces to \u2113 Block x,y (\u03c8).To mitigate undersampling while exploiting heteroscedastic uncertainty, we pick \u03b2 = 0.5, which is is also the suggested value of \u03b2-NLL.\n5. EXPERIMENTS\nThe DNS dataset [28] is used as the corpus for all experiments. By randomly mixing the speech and noise signals in the DNS dataset, we simulate our training, validation, and test sets, which consist of 500K, 1K, and 1.5K pairs of noisy and clean utterances, respectively. The SNR for each noisy utterance in the training and validation sets is randomly sampled between -5 and 5 dB. For the test set, -5, 0, and 5 dB SNRs are used, equally dividing the 1.5K utterances. Note that all test speakers are excluded from the training and validation sets and all utterances are sampled at 16 kHz, each of which is truncated to 10 seconds. The window size and hop size of STFT are 320 and 160 points, respectively, where the Hann window is used. We adopt the gated convolutional recurrent network (GCRN) [13] as the SE model f\u03b8 for investigation. Given that the original GCRN has an encoder-decoder architecture with long short-term memory (LSTM) in between, we formulate the temporary submodel f\u03c6 as an additional decoder that takes the output of the in-between LSTM as input. Hence the augmented model f\u03c8 formed by these two models is a GCRN with two distinct decoders. For comparison, we train three SE models f\u03b8 individually using the MAE, MSE, and SI-SDR loss (\u2113SI-SDR), respectively. Another model f\u03c8 is trained with the Gaussian NLL. At inference time, f\u03c8 drops f\u03c6, so all SE models for comparison have the same DNN architecture and number of parameters. The Adam [29] optimizer is adopted to train all the models. The learning rate is 0.0004 and the batch size is 128. All models are trained for 300 epochs. After each training epoch, the model is evaluated on the validation set, and the best model is determined by the validation results. We measure SE performance using multiple metrics, including wideband perceptual evaluation of speech quality (WB-PESQ) [30], short-time objective intelligibility (STOI) [31] (%), SI-SDR [23] (dB), and NORESQA-MOS [32] on the test set. Calibration of the Probabilistic Model: Each quantile-quantile (QQ) plot at a different frequency component in Fig. 2(a) compares the\npopulation of a clean speech with the predictive Gaussian distribution for the real part on a frequency component. The Q-Q plots for the imaginary part are shown in Fig. 2(b). All these Q-Q plots are close to the main diagonal, showing that calibration qualities seem to be acceptable. The predictive distribution is obtained by training an SE model with the Gaussian NLL \u2113\u03b2-Block using \u03b4 = 0.01 and \u03b2 = 0.5 and feeding a random noisy utterance at 5 dB SNR from the test set to the model. One can probably argue that such calibration qualities are sufficient to achieve SE performance improvements. Covariance Regularization and Uncertainty Weighting: Table 1 shows that the eigenvalue lower bound of the lower Cholesky factor \u03b4 plays an important role in minimizing Gaussian NLLs. When \u03b4 = 0.0001, we find that it is very difficult to optimize the Gaussian NLL, and the trained model completely fails to enhance speech. Such an issue can be significantly improved by increasing \u03b4. Taking the block diagonal case for example, \u03b4 = 0.001 gives an SE model with reasonable perceptual metric performance, and further increasing \u03b4 to 0.01 gives even better performance. On the other hand, Table 1 shows that applying the uncertainty weighting method with \u03b2 = 0.5 consistently improves SE performance for different eigenvalue lower bounds \u03b4 and covariance assumptions. Covariance Assumptions: Table 1 also shows that the Gaussian NLL using the block diagonal covariance assumption outperforms the Gaussian NLL using the diagonal covariance assumption for both \u03b2 = 0 and \u03b2 = 0.5 under \u03b4 = 0.01. These improvements show that modeling more heteroscedastic uncertainty is beneficial for SE. Note that modeling more uncertainty implicitly relaxes more assumptions for the loss function, which gives the SE model more flexibility to learn better complex spectral mapping. In Comparison to Losses without Exploiting Uncertainty: Ta-\nble 1 shows that the Gaussian NLL using the block diagonal covariance with \u03b4 = 0.01 and \u03b2 = 0.5 substantially outperforms the MAE and MSE loss functions that assume homoscedastic uncertainty. The Gaussian NLL also slightly outperforms the SI-SDR loss. To determine if the Gaussian NLL gives statistically significant improvements over the SI-SDR loss, we perform the paired Student\u2019s t-test that assumes the two-tailed distribution. The p-values for WBPESQ, STOI, and NORESQA-MOS are much less than 0.1%, implying that these improvements are statistically significant. It should be noted that, although the SI-SDR loss yields competitive perceptual metric performance, it does not preserve the level of the clean speech signal due to its scale invariance, which would require additional rescaling in real applications. In contrast, the proposed Gaussian NLL preserves the level of clean speech while achieving superior perceptual metric performance.\nA Hybrid Loss: Table 2 shows that a hybrid loss gives better performance than every single-task loss in Table 1, suggesting that combining the Gaussian NLL with the SI-SDR loss is indeed beneficial. Such a result supports a multi-task learning strategy for SE.\n6. CONCLUSION\nIn this study, we have developed a novel framework to improve SE performance by modeling uncertainty in the estimation. Specifically, we jointly optimize an SE model to learn complex spectral mapping and a temporary submodel to minimize a multivariate Gaussian NLL. In our multivariate setting, we reveal common covariance assumptions and propose to use a block diagonal assumption to leverage more heteroscedastic uncertainty for SE. To overcome the optimization difficulty induced by the multivariate Gaussian NLL, we propose two methods, covariance regularization and uncertainty weighting, to mitigate the undersampling effect. With these methods, the multivariate Gaussian NLL substantially outperforms conventional losses including the MAE and MSE, and slightly outperforms the SI-SDR. To our best knowledge, this study is the first to show that directly minimizing a Gaussian NLL can improve SE performance, with our approach. Furthermore, such improvements in SE are achieved without extra computational cost at inference time.\n7. REFERENCES\n[1] Y. Hsu, Y. Lee, and M. R. Bai, \u201cLearning-based personal\nspeech enhancement for teleconferencing by exploiting spatialspectral features,\u201d in ICASSP. IEEE, 2022, pp. 8787\u20138791.\n[2] L. Pisha, J. Warchall, T. Zubatiy, S. Hamilton, C.-H. Lee, G.\nChockalingam, P. P. Mercier, R. Gupta, B. D. Rao, and H. Garudadri, \u201cA wearable, extensible, open-source platform for hearing healthcare research,\u201d IEEE Access, vol. 7, pp. 162083\u2013 162101, 2019.\n[3] L. Pisha, S. Hamilton, D. Sengupta, C.-H. Lee, K. C. Vastare,\nT. Zubatiy, S. Luna, C. Yalcin, A. Grant, R. Gupta, G. Chockalingam, B. D. Rao, and H. Garudadri, \u201cA wearable platform for research in augmented hearing,\u201d in Asilomar Conference on Signals, Systems, and Computers. IEEE, 2018, pp. 223\u2013227.\n[4] G. Cybenko, \u201cApproximation by superpositions of a sigmoidal\nfunction,\u201d Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303\u2013314, 1989.\n[5] K. Hornik, M. Stinchcombe, and H. White, \u201cMultilayer feed-\nforward networks are universal approximators,\u201d Neural Networks, vol. 2, no. 5, pp. 359\u2013366, 1989.\n[6] M. Telgarsky, \u201cBenefits of depth in neural networks,\u201d in Con-\nference on Learning Theory. PMLR, 2016, pp. 1517\u20131539.\n[7] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, \u201cSpeech enhancement\nbased on deep denoising autoencoder.,\u201d in Interspeech, 2013, pp. 436\u2013440.\n[8] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cAn experimental study\non speech enhancement based on deep neural networks,\u201d IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2013.\n[9] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression ap-\nproach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2014.\n[10] D. Wang and J. Chen, \u201cSupervised speech separation based\non deep learning: An overview,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702\u20131726, 2018.\n[11] K. Tan and D. Wang, \u201cA convolutional recurrent neural net-\nwork for real-time speech enhancement.,\u201d in Interspeech, 2018, pp. 3229\u20133233.\n[12] A. Pandey and D. Wang, \u201cTCNN: Temporal convolutional neu-\nral network for real-time speech enhancement in the time domain,\u201d in ICASSP. IEEE, 2019, pp. 6875\u20136879.\n[13] K. Tan and D. Wang, \u201cLearning complex spectral map-\nping with gated convolutional recurrent networks for monaural speech enhancement,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 380\u2013390, 2019.\n[14] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B.\nZhang, and L. Xie, \u201cDCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement,\u201d in Interspeech, 2020, pp. 2472\u20132476.\n[15] X. Hao, X. Su, R. Horaud, and X. Li, \u201cFullsubnet: A full-band\nand sub-band fusion model for real-time single-channel speech enhancement,\u201d in ICASSP. IEEE, 2021, pp. 6633\u20136637.\n[16] A. Li, S. You, G. Yu, C. Zheng, and X. Li, \u201cTaylor, can you\nhear me now? A Taylor-unfolding framework for monaural speech enhancement,\u201d in International Joint Conference on Artificial Intelligence, 2022.\n[17] J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt, J.\nFeng, A. Kruspe, R. Triebel, P. Jung, R. Roscher, M. Shahzad, W. Yang, R. Bamler, and X. X. Zhu, \u201cA survey of uncertainty in deep neural networks,\u201d arXiv preprint arXiv:2107.03342, 2021.\n[18] S.-W. Fu, T.-y. Hu, Y. Tsao, and X. Lu, \u201cComplex spec-\ntrogram enhancement by convolutional neural network with multi-metrics learning,\u201d in International Workshop on Machine Learning for Signal Processing. IEEE, 2017, pp. 1\u20136.\n[19] D. A. Nix and A. S. Weigend, \u201cEstimating the mean and vari-\nance of the target probability distribution,\u201d in International Conference on Neural Networks. IEEE, 1994, vol. 1, pp. 55\u2013 60.\n[20] A. Kendall and Y. Gal, \u201cWhat uncertainties do we need in\nbayesian deep learning for computer vision?,\u201d in Advances in Neural Information Processing Systems, 2017.\n[21] A. Amini, W. Schwarting, A. Soleimany, and D. Rus, \u201cDeep\nevidential regression,\u201d in Advances in Neural Information Processing Systems, 2020.\n[22] H. Fang, T. Peer, S. Wermter, and T. Gerkmann, \u201cIntegrat-\ning statistical uncertainty into neural network-based speech enhancement,\u201d in ICASSP. IEEE, 2022, pp. 386\u2013390.\n[23] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSDR\u2013\nhalf-baked or well done?,\u201d in ICASSP. IEEE, 2019, pp. 626\u2013 630.\n[24] M. Kolb\u00e6k, Z.-H. Tan, S. H. Jensen, and J. Jensen, \u201cOn\nloss functions for supervised monaural time-domain speech enhancement,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 825\u2013838, 2020.\n[25] M. Seitzer, A. Tavakoli, D. Antic, and G. Martius, \u201cOn the pit-\nfalls of heteroscedastic uncertainty estimation with probabilistic neural networks,\u201d in International Conference on Learning Representations, 2021.\n[26] R. F. Astudillo, D. Kolossa, and R. Orglmeister, \u201cAccounting\nfor the uncertainty of speech estimates in the complex domain for minimum mean square error speech enhancement,\u201d in Interspeech, 2009.\n[27] T. Gerkmann and E. Vincent, \u201cSpectral masking and filtering,\u201d\nin Audio Source Separation and Speech Enhancement. 2018, pp. 65\u201385, John Wiley & Sons, Ltd.\n[28] C. K. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R.\nCutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, \u201cINTERSPEECH 2021 deep noise suppression challenge,\u201d in Interspeech, 2021, pp. 2796\u20132800.\n[29] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d in International Conference on Learning Representations, 2015.\n[30] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hek-\nstra, \u201cPerceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs,\u201d in ICASSP. IEEE, 2001, vol. 2, pp. 749\u2013752.\n[31] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen,\n\u201cAn algorithm for intelligibility prediction of time\u2013frequency weighted noisy speech,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125\u20132136, 2011.\n[32] P. Manocha and A. Kumar, \u201cSpeech quality assessment\nthrough MOS using non-matching references,\u201d in Interspeech, 2022, pp. 654\u2013658."
        }
    ],
    "title": "LEVERAGING HETEROSCEDASTIC UNCERTAINTY IN LEARNING COMPLEX SPECTRAL MAPPING FOR SINGLE-CHANNEL SPEECH ENHANCEMENT",
    "year": 2023
}