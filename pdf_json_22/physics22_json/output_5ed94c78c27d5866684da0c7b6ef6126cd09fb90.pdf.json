{
    "abstractText": "Missions to small celestial bodies rely heavily on optical feature tracking for characterization of and relative navigation around the target body. While deep learning has led to great advancements in feature detection and description, training and validating data-driven models for space applications is challenging due to the limited availability of large-scale, annotated datasets. This paper introduces AstroVision, a large-scale dataset comprised of 115,970 densely annotated, real images of 16 different small bodies captured during past and ongoing missions. We leverage AstroVision to develop a set of standardized benchmarks and conduct an exhaustive evaluation of both handcrafted and data-driven feature detection and description methods. Next, we employ AstroVision for end-to-end training of a state-of-the-art, deep feature detection and description network and demonstrate improved performance on multiple benchmarks. The full benchmarking pipeline and the dataset will be made publicly available to facilitate the advancement of computer vision algorithms for space applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Travis Drivera"
        },
        {
            "affiliations": [],
            "name": "Katherine A. Skinner"
        },
        {
            "affiliations": [],
            "name": "Mehregan Dor"
        },
        {
            "affiliations": [],
            "name": "Panagiotis Tsiotras"
        }
    ],
    "id": "SP:9d561c2c0f2361252aa92f3962968c21dce44cff",
    "references": [
        {
            "authors": [
                "A.F. Cheng",
                "A.S. Rivkin",
                "P. Michel",
                "J. Atchison",
                "O. Barnouin",
                "L. Benner",
                "N.L. Chabot",
                "C. Ernst",
                "E.G. Fahnestock",
                "M. Kueppers",
                "P. Pravec",
                "E. Rainey",
                "D.C. Richardson",
                "A.M. Stickle",
                "C. Thomas"
            ],
            "title": "AIDA DART asteroid deflection test: Planetary defense and science objectives",
            "venue": "Planetary and Space Science 157 ",
            "year": 2018
        },
        {
            "authors": [
                "D.D. Mazanek",
                "R.G. Merrill",
                "J.R. Brophy",
                "R.P. Mueller"
            ],
            "title": "Asteroid redirect mission concept: A bold approach for utilizing space resources",
            "venue": "Acta Astronautica 117 ",
            "year": 2015
        },
        {
            "authors": [
                "A.S. Rivkin",
                "F.E. DeMeo"
            ],
            "title": "How many hydrated NEOs are there",
            "venue": "J. of Geophysical Research: Planets 124 (1) ",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Barucci",
                "E. Dotto",
                "A.C. Levasseur-Regourd"
            ],
            "title": "Space missions to small bodies: asteroids and cometary nuclei",
            "venue": "Astronomy and Astrophysics Review 19 (48) ",
            "year": 2011
        },
        {
            "authors": [
                "C. Norman",
                "C. Miller",
                "R. Olds",
                "C. Mario",
                "E. Palmer",
                "O. Barnouin",
                "M. Daly",
                "J. Weirich",
                "J. Seabrook"
            ],
            "title": "C",
            "venue": "Bennett, et al., Autonomous navigation performance using natural feature tracking during the OSIRIS-REx touch-and-go sample collection event, Planetary Science 3 (101) ",
            "year": 2022
        },
        {
            "authors": [
                "R.W. Gaskell",
                "O.S. Barnouin-Jha",
                "D.J. Scheeres",
                "A.S. Konopliv",
                "T. Mukai",
                "S. Abe",
                "J. Saito",
                "M. Ishiguro",
                "T. Kubota",
                "T. Hashimoto",
                "J. Kawaguchi",
                "M. Yoshikawa",
                "K. Shirakawa",
                "T. Kominato",
                "N. Hirata",
                "H. Demura"
            ],
            "title": "Characterizing and navigating small bodies with imaging data",
            "venue": "Meteoritics & Planetary Science 43 (6) ",
            "year": 2008
        },
        {
            "authors": [
                "O. Barnouin",
                "M. Daly",
                "E. Palmer",
                "C. Johnson",
                "R. Gaskell",
                "M. Al Asad",
                "E. Bierhaus",
                "K. Craft",
                "C. Ernst",
                "R. Espiritu",
                "H. Nair",
                "G. Neumann",
                "L. Nguyen",
                "M. Nolan",
                "E. Mazarico",
                "M. Perry",
                "L. Philpott",
                "J. Roberts",
                "R. Steele",
                "J. Seabrook",
                "H. Susorney",
                "J. Weirich",
                "D. Lauretta"
            ],
            "title": "Digital terrain mapping by the OSIRIS- REx mission",
            "venue": "Planetary and Space Science 180 ",
            "year": 2020
        },
        {
            "authors": [
                "E.E. Palmer",
                "R. Gaskell",
                "M.G. Daly",
                "O.S. Barnouin",
                "C.D. Adam",
                "D.S. Lauretta"
            ],
            "title": "Practical stereophotoclinometry for modeling shape and topography on planetary missions",
            "venue": "Planetary Science 3 (102) ",
            "year": 2022
        },
        {
            "authors": [
                "P.G. Antreasian",
                "C.D. Adam",
                "K. Berry",
                "J. Geeraert",
                "K.M. Getzandanner",
                "D. Highsmith",
                "J.M. Leonard",
                "E.J. Lessac-Chenen",
                "A.H. Levine"
            ],
            "title": "J",
            "venue": "V. McAdams, et al., OSIRIS-REx proximity operations and navigation performance at Bennu, in: AIAA SciTech Forum",
            "year": 2022
        },
        {
            "authors": [
                "S. Bhaskaran",
                "S. Nandi",
                "S. Broschart",
                "M. Wallace",
                "L.A. Cangahuala"
            ],
            "title": "C",
            "venue": "0lson, Small body landings using autonomous onboard optical navigation, J. of the Astronautical Sciences 58 (3) ",
            "year": 2011
        },
        {
            "authors": [
                "M. Quadrelli",
                "L. Wood",
                "J. Riedel",
                "M. McHenry",
                "M. Aung",
                "L. Cangahuala",
                "R. Volpe",
                "P. Beauchamp",
                "J. Cutts"
            ],
            "title": "Guidance",
            "venue": "navigation, and control technology assessment for future planetary science missions, J. of Guidance, Control, and Dynamics 38 (7) ",
            "year": 2015
        },
        {
            "authors": [
                "I. Nesnas",
                "B.J. Hockman",
                "S. Bandopadhyay",
                "B.J. Morrell",
                "D.P. Lubey",
                "J. Villa",
                "D.S. Bayard",
                "A. Osmundson",
                "B. Jarvis",
                "M. Bersani",
                "S. Bhaskaran"
            ],
            "title": "Autonomous exploration of small bodies toward greater autonomy for deep space missions",
            "venue": "Frontiers in Robotics and AI 8 (650885) ",
            "year": 2021
        },
        {
            "authors": [
                "K.M. Getzandanner",
                "P.G. Antreasian",
                "M.C. Moreau",
                "J.M. Leonard",
                "C.D. Adam",
                "D. Wibben",
                "K. Berry",
                "D. Highsmith",
                "D. Lauretta"
            ],
            "title": "Small body proximity operations & TAG: Navigation experiences & lessons learned from the OSIRIS-REx mission",
            "venue": "in: AIAA SciTech Forum",
            "year": 2022
        },
        {
            "authors": [
                "K. Dennison"
            ],
            "title": "S",
            "venue": "D\u2019Amico, Comparing optical tracking techniques in distributed asteroid orbiter missions using ray-tracing, in: AAS/AIAA Space Flight Mechanics Meeting",
            "year": 2021
        },
        {
            "authors": [
                "B.J. Morrell",
                "J. Villa",
                "A. Havard"
            ],
            "title": "Automatic feature tracking on small bodies for autonomous approach",
            "venue": "in: ASCEND",
            "year": 2020
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "Int. J. of Computer Vision (IJCV) 60 (2) ",
            "year": 2004
        },
        {
            "authors": [
                "D. DeTone",
                "T. Malisiewicz",
                "A. Rabinovich"
            ],
            "title": "SuperPoint: Self-supervised interest point detection and description",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2018
        },
        {
            "authors": [
                "M. Dusmanu",
                "I. Rocco",
                "T. Pajdla",
                "M. Pollefeys",
                "J. Sivic",
                "A. Torii",
                "T. Sattler"
            ],
            "title": "D2-Net: A trainable CNN for joint description and detection of local features",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "J. Revaud",
                "C. De Souza",
                "M. Humenberger",
                "P. Weinzaepfel"
            ],
            "title": "R2D2: Reliable and repeatable detector and descriptor",
            "venue": "in: Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2019
        },
        {
            "authors": [
                "Z. Luo",
                "L. Zhou",
                "X. Bai",
                "H. Chen",
                "J. Zhang",
                "Y. Yao",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "ASLFeat: Learning local features of accurate shape and localization",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "J. Song",
                "D. Rondao",
                "N. Aouf"
            ],
            "title": "Deep learning-based spacecraft relative navigation methods: A survey",
            "venue": "Acta Astronautica 191 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Pugliatti",
                "M. Maestrini",
                "P. Di Lizia",
                "F. Topputo"
            ],
            "title": "On-board small-body semantic segmentation based on morphological features with U-Net",
            "venue": "in: AAS/AIAA Space Flight Mechanics Meeting",
            "year": 2021
        },
        {
            "authors": [
                "D. Zhou",
                "G. Sun",
                "J. Song",
                "W. Yao"
            ],
            "title": "2D vision-based tracking algorithm for general space non-cooperative objects",
            "venue": "Acta Astronautica 188 ",
            "year": 2021
        },
        {
            "authors": [
                "T. Fuchs",
                "D.R. Thompson",
                "B.D. Bue",
                "J. Castillo-Rogez",
                "S.A. Chien",
                "D. Gharibian",
                "K.L. Wagstaff"
            ],
            "title": "Enhanced flyby science with onboard computer vision: Tracking and surface feature detection at small bodies",
            "venue": "Earth and Space Science 2 (10) ",
            "year": 2015
        },
        {
            "authors": [
                "H. Lee",
                "H.-L. Choi",
                "D. Jung",
                "S. Choi"
            ],
            "title": "Deep neural networkbased landmark selection method for optical navigation on Lunar highlands",
            "venue": "IEEE Access 8 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Olds",
                "C. Miller",
                "C. Norman",
                "C. Mario",
                "K. Berry",
                "E. Palmer",
                "O. Barnouin",
                "M. Daly",
                "J. Weirich"
            ],
            "title": "J",
            "venue": "Seabrook, et al., The use of digital terrain models for natural feature tracking at asteroid Bennu, Planetary Science 3 (100) ",
            "year": 2022
        },
        {
            "authors": [
                "D.A. Lorenz",
                "R. Olds",
                "A. May",
                "C. Mario",
                "M.E. Perry",
                "E.E. Palmer",
                "M. Daly"
            ],
            "title": "Lessons learned from OSIRIS-REx autonomous navigation using natural feature tracking",
            "venue": "in: IEEE Aerospace Conf.",
            "year": 2017
        },
        {
            "authors": [
                "E. Rublee",
                "V. Rabaud",
                "K. Konolige",
                "G. Bradski"
            ],
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "venue": "in: IEEE Int. Conf. on Computer Vision (ICCV)",
            "year": 2011
        },
        {
            "authors": [
                "P.-E. Sarlin",
                "D. DeTone",
                "T. Malisiewicz",
                "A. Rabinovich"
            ],
            "title": "Super- Glue: Learning feature matching with graph neural networks",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "D. Nist\u00e9r"
            ],
            "title": "An efficient solution to the five-point relative pose problem",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI) 26 (6) ",
            "year": 2004
        },
        {
            "authors": [
                "D. Forsyth",
                "J. Ponce"
            ],
            "title": "Computer Vision: A Modern Approach",
            "venue": "(Second edition), Prentice Hall",
            "year": 2011
        },
        {
            "authors": [
                "C. Cadena",
                "L. Carlone",
                "H. Carrillo",
                "Y. Latif",
                "D. Scaramuzza",
                "J. Neira",
                "I. Reid",
                "J.J. Leonard"
            ],
            "title": "Past",
            "venue": "present, and future of simultaneous localization and mapping: Toward the robustperception age, IEEE Trans. on Robotics 32 (6) ",
            "year": 2016
        },
        {
            "authors": [
                "B.E. Tweddle",
                "A. Saenz-Otero",
                "J.J. Leonard",
                "D.W. Miller"
            ],
            "title": "Factor graph modeling of rigid-body dynamics for localization",
            "venue": "mapping, and parameter estimation of a spinning object in space, J. of Field Robotics 32 (6) ",
            "year": 2015
        },
        {
            "authors": [
                "T. Lindeberg"
            ],
            "title": "Scale-space theory: A basic tool for analyzing structures at different scales",
            "venue": "J. of Applied Statistics 21 (1-2) ",
            "year": 1994
        },
        {
            "authors": [
                "H. Bay",
                "A. Ess",
                "T. Tuytelaars",
                "L. Van Gool"
            ],
            "title": "Speeded-Up Robust Features (SURF)",
            "venue": "Computer Vision and Image Understanding 110 (3) ",
            "year": 2008
        },
        {
            "authors": [
                "E. Rosten",
                "T. Drummond"
            ],
            "title": "Fusing points and lines for high performance tracking",
            "venue": "in: IEEE Int. Conf. on Computer Vision (ICCV)",
            "year": 2005
        },
        {
            "authors": [
                "M. Calonder",
                "V. Lepetit",
                "C. Strecha",
                "P. Fua"
            ],
            "title": "BRIEF: Binary Robust Independent Elementary Features",
            "venue": "in: European Conf. on Computer Vision (ECCV)",
            "year": 2010
        },
        {
            "authors": [
                "Y. Ono",
                "E. Trulls",
                "P. Fua",
                "K.M. Yi"
            ],
            "title": "LF-Net: Learning local features from images",
            "venue": "in: Int. Conf. on Neural Information Processing Systems (NeurIPS)",
            "year": 2018
        },
        {
            "authors": [
                "Y. Verdie",
                "K. Yi",
                "P. Fua",
                "V. Lepetit"
            ],
            "title": "TILDE: A Temporally Invariant Learned DEtector",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "K.M. Yi",
                "Y. Verdie",
                "P. Fua",
                "V. Lepetit"
            ],
            "title": "Learning to assign orientations to feature points",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "E. Simo-Serra",
                "E. Trulls",
                "L. Ferraz",
                "I. Kokkinos",
                "P. Fua",
                "F. Moreno-Noguer"
            ],
            "title": "Discriminative learning of deep convolutional feature point descriptors",
            "venue": "in: IEEE Int. Conf. on Computer Vision (ICCV)",
            "year": 2015
        },
        {
            "authors": [
                "K.M. Yi",
                "E. Trulls",
                "V. Lepetit",
                "P. Fua"
            ],
            "title": "LIFT: Learned Invariant Feature Transform",
            "venue": "in: European Conf. on Computer Vision (ECCV)",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "Y. Lu",
                "S. Sclaroff"
            ],
            "title": "Local descriptors optimized for average precision",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "X. Zhu",
                "H. Hu",
                "S. Lin",
                "J. Dai"
            ],
            "title": "Deformable convnets v2: More deformable",
            "venue": "better results, in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "J. Zhang",
                "Y. Ren",
                "L. Zhou",
                "T. Fang",
                "L. Quan"
            ],
            "title": "BlendedMVS: A large-scale dataset for generalized multi-view stereo networks",
            "venue": "in: IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "T. Shen",
                "Z. Luo",
                "L. Zhou",
                "R. Zhang",
                "S. Zhu",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Matchable image retrieval by learning from surface reconstruction",
            "venue": "in: Asian Conf. on Computer Vision (ACCV)",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "J. Jiang",
                "G. Zhang"
            ],
            "title": "CraterIDNet: An end-to-end fully convolutional neural network for crater detection and identification in remotely sensed planetary images",
            "venue": "Remote Sensing 10 (7) ",
            "year": 2018
        },
        {
            "authors": [
                "A. Silburt",
                "M. Ali-Dib",
                "C. Zhu",
                "A. Jackson",
                "D. Valencia",
                "Y. Kissin",
                "D. Tamayo",
                "K. Menou"
            ],
            "title": "Lunar crater identification via deep learning",
            "venue": "Icarus 317 ",
            "year": 2019
        },
        {
            "authors": [
                "C.T. Russell"
            ],
            "title": "C",
            "venue": "A. Raymond, Dawn mission to Vesta and Ceres ",
            "year": 2011
        },
        {
            "authors": [
                "R. Park",
                "A. Vaughan",
                "A. Konopliv",
                "A. Ermakov",
                "N. Mastrodemos",
                "J. Castillo-Rogez",
                "S. Joy",
                "A. Nathues",
                "C. Polanskey"
            ],
            "title": "M",
            "venue": "Rayman, et al., High-resolution shape model of Ceres from stereophotoclinometry using Dawn imaging data, Icarus 319 ",
            "year": 2019
        },
        {
            "authors": [
                "R.W. Gaskell"
            ],
            "title": "SPC shape and topography of Vesta from Dawn imaging data",
            "venue": "in: AAS Division for Planetary Sciences Meeting # 44",
            "year": 2012
        },
        {
            "authors": [
                "M. Dougherty",
                "L. Esposito",
                "S.M. Krimigis"
            ],
            "title": "Saturn from Cassini- Huygens",
            "venue": "Springer",
            "year": 2009
        },
        {
            "authors": [
                "R.T. Daly",
                "C.M. Ernst",
                "R.W. Gaskell",
                "O.S. Barnouin",
                "P.C. Thomas"
            ],
            "title": "New stereophotoclinometry shape models for irregularly shaped Saturnian satellites",
            "venue": "in: Lunar and Planetary Science Conf.",
            "year": 2018
        },
        {
            "authors": [
                "A. Fujiwara",
                "J. Kawaguchi",
                "D. Yeomans",
                "M. Abe",
                "T. Mukai",
                "T. Okada",
                "J. Saito",
                "H. Yano",
                "M. Yoshikawa"
            ],
            "title": "D",
            "venue": "Scheeres, et al., The rubble-pile asteroid Itokawa as observed by Hayabusa, Science 312 (5778) ",
            "year": 2006
        },
        {
            "authors": [
                "Y. Tsuda",
                "T. Saiki",
                "F. Terui",
                "S. Nakazawa"
            ],
            "title": "M",
            "venue": "Yoshikawa, S.i. Watanabe, H. P. Team, Hayabusa2 mission status: Landing, roving and cratering on asteroid Ryugu, Acta Astronautica 171 ",
            "year": 2020
        },
        {
            "authors": [
                "J.-P. Bibring",
                "Y. Langevin",
                "J.F. Mustard",
                "F. Poulet",
                "R. Arvidson",
                "A. Gendrin",
                "B. Gondet",
                "N. Mangold",
                "P. Pinet"
            ],
            "title": "F",
            "venue": "Forget, et al., Global mineralogical and aqueous Mars history derived from OMEGA/Mars Express data, science 312 (5772) ",
            "year": 2006
        },
        {
            "authors": [
                "A.F. Cheng",
                "A. Santo",
                "K. Heeres",
                "J. Landshof",
                "R. Farquhar",
                "R. Gold",
                "S. Lee"
            ],
            "title": "Near-earth asteroid rendezvous: Mission overview",
            "venue": "J. of Geophysical Research: Planets 102 (E10) ",
            "year": 1997
        },
        {
            "authors": [
                "D. Lauretta",
                "S. Balram-Knutson",
                "E. Beshore",
                "W. Boynton"
            ],
            "title": "C",
            "venue": "Drouet d\u2019Aubigny, D. DellaGiustina, H. Enos, D. Golish, C. Hergenrother, E. Howell, et al., OSIRIS-REx: sample return from asteroid (101955) Bennu, Space Science Reviews 212 (1) ",
            "year": 2017
        },
        {
            "authors": [
                "O. Barnouin",
                "M. Daly",
                "E. Palmer",
                "R. Gaskell",
                "J. Weirich",
                "C. Johnson",
                "M. Al Asad",
                "J. Roberts",
                "M. Perry"
            ],
            "title": "H",
            "venue": "Susorney, et al., Shape of (101955) Bennu indicative of a rubble pile with internal stiffness, Nature Geoscience 12 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "M. Taylor",
                "N. Altobelli",
                "B. Buratti",
                "M. Choukroun"
            ],
            "title": "The Rosetta mission orbiter science overview: the comet phase",
            "venue": "Philosophical Trans. of the Royal Society A: Mathematical, Physical and Engineering Sciences 375 (2097) ",
            "year": 2017
        },
        {
            "authors": [
                "R. Schulz",
                "H. Sierks",
                "M. K\u00fcppers",
                "A. Accomazzo"
            ],
            "title": "Rosetta flyby at asteroid (21) Lutetia: An overview",
            "venue": "Planetary and Space Science 66 (1) ",
            "year": 2012
        },
        {
            "authors": [
                "R.W. Gaskell",
                "L. Jorda",
                "E. Palmer",
                "C. Jackman",
                "C. Capanna",
                "S. Hviid",
                "P. Guti\u00e9rrez"
            ],
            "title": "Comet 67P/CG: Preliminary shape and topography from SPC",
            "venue": "in: AAS/Division for Planetary Sciences Meeting, Vol. 46",
            "year": 2014
        },
        {
            "authors": [
                "N. Otsu"
            ],
            "title": "A threshold selection method from gray-level histograms",
            "venue": "IEEE Trans. on Systems, Man, and Cybernetics 9 (1) ",
            "year": 1979
        },
        {
            "authors": [
                "D.Q. Huynh"
            ],
            "title": "Metrics for 3d rotations: Comparison and analysis",
            "venue": "J. of Mathematical Imaging and Vision 35 (2) ",
            "year": 2009
        },
        {
            "authors": [
                "Q.-T. Luong",
                "O.D. Faugeras"
            ],
            "title": "The fundamental matrix: Theory",
            "venue": "algorithms, and stability analysis, Int. J. of Computer Vision (IJCV) 17 (1) ",
            "year": 1996
        },
        {
            "authors": [
                "P.H. Torr",
                "A. Zisserman",
                "S.J. Maybank"
            ],
            "title": "Robust detection of degenerate configurations while estimating the fundamental matrix",
            "venue": "Computer Vision and Image Understanding 71 (3) ",
            "year": 1998
        },
        {
            "authors": [
                "C. Choy",
                "J. Park",
                "V. Koltun"
            ],
            "title": "Fully convolutional geometric features",
            "venue": "in: IEEE Int. Conf. on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "S. Vassilvitskii",
                "D. Arthur"
            ],
            "title": "k-means++: The advantages of careful seeding",
            "venue": "in: ACM-SIAM Sym. on Discrete algorithms",
            "year": 2006
        },
        {
            "authors": [
                "G. Lentaris",
                "K. Maragos",
                "I. Stratakos",
                "L. Papadopoulos",
                "O. Papanikolaou",
                "D. Soudris",
                "M. Lourakis",
                "X. Zabulis",
                "D. Gonzalez- Arjona",
                "G. Furano"
            ],
            "title": "High-performance embedded computing in space: Evaluation of platforms for vision-based navigation",
            "venue": "J. of Aerospace Information Systems 15 (4) ",
            "year": 2018
        },
        {
            "authors": [
                "A.D. George",
                "C.M. Wilson"
            ],
            "title": "Onboard processing with hybrid and reconfigurable computing on small satellites",
            "venue": "Proceedings of the IEEE 106 (3) ",
            "year": 2018
        },
        {
            "authors": [
                "L. Kosmidis",
                "I. Rodriguez",
                "A. Jover",
                "S. Alcaide",
                "J. Lachaize",
                "J. Abella",
                "O. Notebaert",
                "F.J. Cazorla",
                "D. Steenari"
            ],
            "title": "GPU4S: Embedded GPUs in space - latest project updates",
            "venue": "Microprocessors and Microsystems 77 (103143) ",
            "year": 2020
        },
        {
            "authors": [
                "V. Kothari",
                "E. Liberis",
                "N.D. Lane"
            ],
            "title": "The final frontier: Deep learning in space",
            "venue": "in: Int. Workshop on Mobile Computing Systems and Applications",
            "year": 2020
        },
        {
            "authors": [
                "B. Knowles"
            ],
            "title": "Cassini Imaging Science Subsystem (ISS) data user\u2019s guide",
            "venue": "Tech. rep., Space Science Institute ",
            "year": 2018
        },
        {
            "authors": [
                "S. Schr\u00f6der",
                "P. Gutierrez-Marques"
            ],
            "title": "Dawn Framing Camera: Calibration pipeline",
            "venue": "Tech. Rep. DA-FC-MPAE-RP-272, Planetary Science Institute ",
            "year": 2013
        },
        {
            "authors": [
                "S. Murchie",
                "M. Robinson",
                "D. Domingue",
                "H. Li",
                "L. Prockter",
                "S. Hawkins",
                "W. Owen",
                "B. Clark",
                "N. Izenberg"
            ],
            "title": "Inflight calibration of the NEAR multispectral imager: II",
            "venue": "results from Eros approach and orbit, Icarus 155 (1) ",
            "year": 2002
        },
        {
            "authors": [
                "D. Golish",
                "B. Rizk"
            ],
            "title": "OSIRIS-REx camera suite calibration description",
            "venue": "Tech. rep., Planetary Science Institute ",
            "year": 2019
        },
        {
            "authors": [
                "B. Geiger",
                "M. Barthelemy"
            ],
            "title": "R",
            "venue": "Andr \u0301es, EAICD ROSETTA- NAVCAM, Tech. Rep. RO-SGS-IF-0001, European Space Agency ",
            "year": 2020
        },
        {
            "authors": [
                "G. Neukum",
                "R. Jaumann"
            ],
            "title": "HRSC: the High Resolution Stereo Camera of Mars Express",
            "venue": "Tech. Rep. SP-1240, European Space Agency ",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "Missions to small celestial bodies rely heavily on optical feature tracking for characterization of and relative navigation around the target body. While deep learning has led to great advancements in feature detection and description, training and validating data-driven models for space applications is challenging due to the limited availability of large-scale, annotated datasets. This paper introduces AstroVision, a large-scale dataset comprised of 115,970 densely annotated, real images of 16 different small bodies captured during past and ongoing missions. We leverage AstroVision to develop a set of standardized benchmarks and conduct an exhaustive evaluation of both handcrafted and data-driven feature detection and description methods. Next, we employ AstroVision for end-to-end training of a state-of-the-art, deep feature detection and description network and demonstrate improved performance on multiple benchmarks. The full benchmarking pipeline and the dataset will be made publicly available to facilitate the advancement of computer vision algorithms for space applications.\nKeywords: Keypoint Detection, Feature Description, Feature Tracking, Deep Learning, Computer Vision, Spacecraft Navigation, Small Bodies"
        },
        {
            "heading": "1. Introduction",
            "text": "There has been an increasing interest in missions to small bodies (e.g., asteroids, comets) due to their great scientific value, with four currently in operation (OSIRISREx, Hayabusa2, Lucy, DART) and two scheduled to launch over the next year (Psyche, Janus). In addition to planetary protection [1] and resource utilization [2, 3], small bodies are believed to be remnants from the solar system\u2019s formation, and studying their composition could provide insight into the solar system\u2019s evolution and the origins of organic life on Earth [4].\nFeature tracking is an integral component of current small body shape reconstruction and relative navigation methodologies. However, the current state-of-the-practice relies heavily on humans-in-the-loop. Specifically, human operators on the ground manually identify salient surface features from images acquired during an extensive characterization phase, where the definition of saliency usually undergoes multiple iterations [5]. Extracted features are then combined with a priori global shape and spacecraft pose (position and orientation) estimates and used to iteratively construct a collection of digital terrain maps (DTMs), local topography and albedo maps, through a method known as stereophotoclinometry (SPC) [6]. DTM\n\u2217Corresponding author Email address: travisdriver@gatech.edu (Travis Driver)\nconstruction typically involves extensive human-in-theloop verification and carefully designed image acquisition plans to achieve optimal results [7, 8]. These topographic features, along with global shape models, are critical for precision navigation and orbit determination for ground-based maneuvering and planning during data acquisition phases [9]. Moreover, upon satisfying strict accuracy and resolution requirements, a catalog of DTMs can be uplinked to the spacecraft and correlated with onboard images to produce an onboard navigation solution for execution of safety-critical maneuvers [10], e.g., during the OSIRIS-REx Touch-And-Go (TAG) sample collection event [5]. While this manual approach has achieved much success, its reliance on extensive human involvement for extended durations limits mission capabilities and increases operational costs [11, 12, 13].\nWhile automated feature tracking methods have been investigated to reduce reliance on current human-in-theloop practices for missions to small bodies [14, 15], these works have focused exclusively on traditional handcrafted features (e.g., SIFT [16]). More recently, feature detection and description methods that leverage deep convolutional neural networks (CNNs) have been shown to significantly outperform handcrafted methods when applied to terrestrial imagery, especially in scenarios involving considerable change in illumination, scale, and perspective [17, 18, 19, 20]. However, transferring recent advances in deep learning to small body science applications is challenging due to the unavailability of relevant, annotated\nPreprint submitted to Elsevier August 4, 2022\nar X\niv :2\n20 8.\n02 05\n3v 1\n[ as\ntr o-\nph .I\nM ]\n3 A\nug 2\n02 2\ndata [21]. To the best our knowledge, there exists no largescale, annotated dataset comprised entirely of real small body images. Indeed, previous work has relied entirely on simulated data [22, 23, 24], small sets (i.e., <150 images) of manually annotated real imagery [25], or datasets restricted to a single body [26]. Moreover, operation in space presents a unique set of environmental (e.g., dynamic hard lighting, self-similar surface features) and operational (e.g., significant scale and perspective change during approach) challenges that are likely not adequately captured in available datasets based on terrestrial imagery.\nThis paper presents AstroVision, a large-scale dataset comprised of 115,970 densely annotated, real images of 16 different small bodies from both legacy and ongoing deep space missions to bridge the terrestrial-to-extraterrestrial domain gap and facilitate the study of deep learning for autonomous navigation in the vicinity of a small body.\nThe contributions of this paper are as follows: (i) AstroVision is a first-of-a-kind dataset for vision-based tasks in the vicinity of a small body with special emphasis on feature tracking applications; (ii) we perform an exhaustive evaluation of both handcrafted and data-driven keypoint detection and feature description pipelines under challenging conditions on real imagery; (iii) we employ AstroVision for end-to-end training of a state-ofthe-art, deep feature detection and description network and demonstrate improved performance with respect to our benchmarks. We make our dataset, benchmarking pipeline, and trained models publicly available at https://github.com/astrovision."
        },
        {
            "heading": "2. Background",
            "text": "In the following subsections, we detail the feature tracking process (Section 2.1) and feature-based pose estimation methodologies (Section 2.2). For completeness, we also provide a brief overview of structure-from-motion in Section 2.3."
        },
        {
            "heading": "2.1. Feature Tracking",
            "text": "Robust tracking of salient image features is a critical component of current small body relative navigation methods, as the apparent displacement of tracked features between images can be leveraged to estimate the relative pose of the spacecraft as it moves around the body. In the context of optical feature tracking, saliency typically refers to the ability to detect and and precisely localize the feature under multiple viewing conditions (i.e., repeatability) and to the distinctiveness of the feature to ensure accurate matching between images (i.e., reliability) [18, 19]. The current state-of-the-practice for small body feature tracking leverages high-fidelity DTMs of salient surface regions as local feature representations, which require extensive human involvement and mission operations planning for accurate construction [7, 8]. Criteria for selecting salient features typically undergoes multiple iterations through\nDTMs Rendered Image\nCropped Image\nRender\nCorrelation\nSpacecraft pose Sun vector\nCorrelation Surface\n(a) DTM-based feature tracking. DTMs are rendered by leveraging a priori spacecraft pose and sun vector information, along with a photometric model, which is subsequently correlated with the input image to register a match. Adapted from [27].\nKeypoint Detection & Feature Description\nKeypoint Detection & Feature Description\nDescriptor Matching\nInput Image\nSaliency Map\n(b) Keypoint-based feature tracking. Keypoints, extracted from each images\u2019 saliency map, and their associated descriptors abstract away the image, and tracking is performed by matching local descriptors between images.\nFigure 1: Feature tracking paradigms.\ntesting and development of the DTMs [5]. Next, each DTM is combined with a priori estimates of the spacecraft\u2019s pose and Sun pointing vector, along with a photometric model, to yield a photorealistic rendering of the DTM with respect to the input image. Finally, tracking is performed by comparing the rendering against the input image near the expected feature location using normalized cross-correlation, where a match is declared if a significant correlation peak is detected [28, 5]. This process is illustrated in Figure 1a. The relative pose of the spacecraft when the image was taken can be computed using the registered matches and the a priori DTM position estimates. Therefore, this DTM-based method relies on the fidelity of the a priori data products and can only be utilized after the target body has been adequately observed and reconstructed at the required resolutions [10].\nIn this work we instead investigate approaches to feature tracking that rely on autonomous keypoint detection and feature description. Consider two images I : \u2126 \u2192 R and I \u2032 : \u2126\u2032 \u2192 R with pixel domains \u2126 \u2282 R2 and \u2126\u2032 \u2282 R2, respectively. Keypoints pk \u2208 \u2126 (p\u2032k \u2208 \u2126\u2032) localize salient regions in the image, which are typically extracted from a saliency map S : \u2126 \u2192 R. Saliency can be predefined (e.g., corners) and localized using image filtering methods or learned from data (see Section 3.1).\nFeature description is the task of forming a latent representation of the local image data at detected keypoints, where the latent representation commonly takes the form of a d-dimensional vector dk \u2208 Rd referred\nto as the descriptor associated with the keypoint pk. Consider, for instance, corresponding keypoints {pk}k\u2208K and {p\u2032k}k\u2208K\u2032 with correspondences defined by M := {(k, \u03c4(k)) | \u03c4 : K \u2194 K \u2032}. The overarching goal of feature description is to compute descriptors such that\nd(dl,d \u2032 l\u2032) < min ( min k 6=l d(dk,d \u2032 l\u2032), min k\u2032 6=l\u2032 d(dl,d \u2032 k\u2032) ) (1)\nfor all (l, l\u2032) \u2208 M, where d(\u00b7, \u00b7) is some distance metric. In words, feature description seeks to assign a descriptor to each keypoint such that descriptors of corresponding keypoints are closer together than those of other noncorresponding keypoints. Common metrics d(\u00b7, \u00b7) include the Euclidean distance, or the Hamming distance for binary descriptors [29]. We give an overview of different keypoint detection and feature description methodologies based on both handcrafted filtering approaches and deep learning in Section 3.1.\nFinally, feature tracking is conducted through detection of keypoints and matching of their corresponding descriptors between images. The objective defined in (1) elicits a straightforward descriptor matching criterion referred to as mutual nearest-neighbors (MNN):\nM := {\n(l, l\u2032) | d(dl,d\u2032l\u2032) < min k 6=l d(dk,d \u2032 l\u2032) } \u22c2{\n(l, l\u2032) | d(dl,d\u2032l\u2032) < min k\u2032 6=l\u2032 d(dl,d \u2032 k\u2032)\n} . (2)\nIn this work we leverage MNN with the Euclidean distance metric for feature matching between images. This keypoint-based tracking process is illustrated in Figure 1b. Exploiting recently developed matching approaches based on deep learning [30] will be the subject of future work."
        },
        {
            "heading": "2.2. Feature-based Pose Estimation",
            "text": "Consider a spacecraft equipped with a monocular camera navigating around a target small body. The relative pose between cameras can be estimated by tracking the apparent motion of salient surface landmarks between images. Formally, let B denote some body-fixed frame of the small body with origin B, and let Ci denote the camera frame at time index i with origin Ci.\nMoreover, let `Bk = [ `Bx,k ` B y,k ` B z,k ]> \u2208 R3 denote the vector from B to the kth surface landmark expressed in\nB, let qCik = [ qCix,k q Ci y,k q Ci z,k ]> \u2208 R3 denote the vector from Ci to the kth landmark expressed in Ci, and let p\n(i) k =\n[ u\n(i) k v (i) k\n]> \u2208 R2 denote the 2D image coordinates\nof the kth landmark observed by camera Ci, i.e., the keypoint.\nA landmark can be forward-projected onto the image\nplane via\np(i) k = \u03a0 ( `Bk , TCiB;K ) = 1\ndCik\n[ K |03\u00d71 ] TCiB` B k\n= 1\ndC i\nk\nKqCik (3)\nwhere dCik = q Ci z,k is the landmark depth in Ci, ` B k =[(\n`Bk\n)> 1 ]> \u2208 P3 and p(i)k = [( p (i) k )> 1 ]> \u2208 P2 denote\nthe homogeneous coordinates of `Bk and p (i) k , respectively, TCiB \u2208 SE(3) denotes the relative pose of B with respect to Ci:\nTCiB =\n[ RCiB r Ci BCi\n01\u00d73 1\n] , (4)\nand K is the camera calibration matrix:\nK = fx 0 cx0 fy cy 0 0 1  (5) where fx and fy are the focal lengths in the x- and ydirections of the camera frame, and (cx, cy) is the principal point of the camera. The geometry of the pinhole camera model is illustrated in Figure 2. Conversely, a 2D keypoint may be backward-projected into 3D coordinates via\n`Bk = \u03a0 \u22121 ( p (i) k , d Ci k , TCiB;K ) = T\u22121CiB [ dCik K \u22121p (i) k\n1 ] = TBCiq Ci k . (6)\nThen, given corresponding keypoints p (i) k and p (j) k observed by cameras Ci and Cj , respectively, the essential matrix E := [r\nCj CiCj\n]\u00d7RCjCi satisfies( p(j) k )> K\u2212>EK\u22121p(i) k = 0, (7)\nwhere we have assumed a shared camera matrix K for simplicity, and [\u00b7]\u00d7 denotes the skew-symmetric matrix crossproduct matrix, defined for any r \u2208 R3, such that\n[r]\u00d7 =  0 \u2212rz ryrz 0 \u2212rx \u2212ry rx 0  . (8) The well-known five-point algorithm [31] can be used to solve for E given five or more correspondences. Finally, RCjCi and r Cj CiCj (up to some unknown scale) can be estimated using singular value decomposition (SVD) of the associated essential matrix and by imposing the Cheirality constraint, i.e., triangulating the landmark associated with keypoints p (i) k , p (j) k and enforcing that the associated landmark lies in front of the cameras [32]."
        },
        {
            "heading": "2.3. Structure-from-Motion",
            "text": "In the structure-from-motion (SfM) or simultaneous localization and mapping (SLAM) setting, we are interested in simultaneously estimating a collection of camera poses T := {TCiB \u2208 SE(3) | i = 1, . . . ,m} and a network of landmarks (the map) L := {`Bk \u2208 R3 | k = 1, . . . , n}. Note that the SfM solution is innately expressed in some arbitrary body-fixed frame since most SfM techniques assume operation in a static scene, typically referred to as the\u201cworld\u201d frame [33]. SfM seeks the maximum a-posteriori (MAP) estimate of the poses T and landmarks L, given the (independent) keypoint measurements P := {p\u0302(i)k \u2208 R2 | i = 1, . . . ,m, k = 1, . . . , n}:\nT \u2217,L\u2217 = arg max T ,L p (T ,L | P) (9)\n\u221d arg max T ,L p (T ,L) p (P | T ,L) (10)\n= p (T ,L) \u220f i \u220f k p ( p\u0302 (i) k | TCiB, ` B k ) . (11)\nBy assuming measurements p\u0302 (i) k are corrupted by zeromean Gaussian noise, i.e., p\u0302 (i) k = p (i) k + \u03b7 (i) k where \u03b7 (i) k \u223c N (0,\u03a3(i)k ), we get\np ( p\u0302\n(i) k | TCiB, ` B k\n) \u221d exp { \u2016p\u0302(i)\nk \u2212\u03a0 ( `Bk , TCiB;K ) \u20162\n\u03a3 (i) k\n} ,\n(12) where \u2016e\u20162\u03a3 := e>\u03a3\u22121e. The MAP estimate can be formulated as the solution to a nonlinear least-squares problem by taking the negative logarithm of (11):\nT \u2217,L\u2217 = arg min T ,L \u2211 i,k \u2016p\u0302(i) k \u2212\u03a0 ( `Bk , TCiB;K ) \u20162 \u03a3 (i) k , (13)\nwhere we have omitted the priors p (T ,L) for conciseness and generality, which can be ignored if no prior information is assumed (i.e., p (T ,L) = const.) or can encode relative pose constraints via known dynamical models [34]. This process is commonly referred to as Bundle Adjustment (BA). Note that the optimization process of SPC decouples estimation of the poses and landmarks, i.e., a priori landmark position and camera pose estimates are passed back-and-forth between the pose determination and DTM construction steps, respectively, until convergence [6].\nIn this work, we focus on two-view pose estimation by estimating the essential matrix using the five-point algorithm. Future work will focus on incorporating our feature detection and description methods into a full SfM pipeline."
        },
        {
            "heading": "3. Related Work",
            "text": "In this section, we give an overview of both handcrafted and data-driven feature detection and description methods (Section 3.1), and then discuss existing datasets and benchmarks for vision tasks in the vicinity of a small body (Section 3.2) and data-driven relative navigation techniques (Section 3.3)."
        },
        {
            "heading": "3.1. Feature Detection and Description",
            "text": "Many computer vision algorithms rely on local image features. The seminal work of David Lowe\u2019s Scale Invariant Feature Transform (SIFT) [16] laid the foundation for the field, where he outlined a rigorous framework for identifying and describing image features. SIFT follows a detect-then-describe paradigm, whereby a series of predetermined (or handcrafted) filters are applied to the image for keypoint localization, followed by pooling and normalization of image gradients to form the descriptor. SIFT aims to extract features that are invariant to changes in scale, illumination, and rotation. Keypoints are extracted from local extrema of the saliency map derived by convolving the difference of Gaussians (DoG) kernel with the input image, as the DoG function provides a close approximation to the scale-normalized Laplacian of Gaussian function which has been shown to be scale invariant [35]. This detection scheme generally results in keypoints centered around large gradients in the image (e.g., edges, corners). Descriptors are then computed by pooling gradients in a local window of each keypoint into histograms according to their orientation, where a canonical orientation is assigned to each keypoint according to the dominant gradient orientation. The oriented histograms are then concatenated and normalized to form the descriptor vector. Speeded-up Robust Features (SURF) built upon the success of SIFT to enable more efficient feature detection and description by leveraging integral images to eliminate the need for computing the DoG [36]. Oriented FAST and Rotated BRIEF (ORB) has become a popular alternative\nto SIFT, especially for SLAM applications [29]. ORB is based on Features from Accelerated Segment Test (FAST) detectors [37] and Binary Robust Independent Elementary Features (BRIEF) descriptors [38] and outputs binary descriptor vectors, enabling more efficient matching.\nMore recently, feature detection and description methods that leverage deep convolutional neural networks (CNNs) have achieved state-of-the-art performance and have been shown to outperform handcrafted methods, especially in scenarios involving significant illumination, scale, and perspective change [39, 17, 19, 20]. The first data-driven methods focused on individual components of the full image processing pipeline, including keypoint detection [40], orientation estimation [41], and feature description [42]. Yi et al. [43] developed the first complete learning-based pipeline, Learned Invariant Feature Transform (LIFT). LIFT uses a patch-based Siamese training architecture and implements each component of the traditional feature detector and descriptor scheme sequentially using CNNs. The approach relies on an incremental training procedure to pretrain each subnetwork component individually, with a final training phase that optimizes over the entire network end-to-end. LFNet [39] proposed a sequential two-stage approach: the first stage learns keypoint detection and the second stage learns feature description. SuperPoint [17] developed a network composed of separate interest point and descriptor decoders that operate on a spatially reduced representation of the input image from a shared encoder network. Simulated data of simple geometric shapes is used to pre-train the interest point detector which is then combined with a random homographic warping procedure to train the network endto-end in a self-supervised fashion.\nTowards joint detection and description, the seminal work of D2-Net [18] proposed a detect-and-describe approach that trains a single deep CNN to detect and describe salient image features. Reliability (or distinctiveness) of descriptors is enforced through a triplet margin ranking loss term which is weighted according to soft detection scores to jointly enforce repeatability of detections. R2D2 [19] leverages the detect-and-describe paradigm to perform simultaneous feature detection and description, but repeatability and reliability are enforced in separate terms in the loss function. Repeatability is enforced through maximization of the cosine similarity of the detection scores of corresponding image patches, while reliability of the descriptors is learned through maximizing a differentiable approximation of the average precision [44] between corresponding patch descriptors. ASLFeat [20] builds upon the success of D2-Net and proposes a multilevel detection scheme to generate detection scores that enable more accurate keypoint localization, and leverages deformable convolutional networks (DCNs) [45] to model local geometric variations in the image and learn more transformation invariant features. ASLFeat is trained using the BlendedMVS [46] and GL3D [47] datasets, which contain 125,623 high-resolution images of 543 different scenes an-\nnotated with depth information using scene reconstructions from a dense SfM pipeline. Although the training data is exceptionally comprehensive, we seek to capitalize on the recent success of deep feature detection and description methods by training these models on domain-relevant data to increase feature tracking performance for missions to small bodies."
        },
        {
            "heading": "3.2. Datasets and Benchmarks for Vision Tasks in the Vicinity of a Small Body",
            "text": "Morrell et. al [15] and Dennison et. al [14] conduct an extensive evaluation of handcrafted feature extraction methods on synthetic images of comet 67P and asteroid 433 Eros, respectively, where SIFT demonstrates superior overall performance with respect to the algorithms studied. While the results are promising, the experiments were conducted in a controlled, simulated environment of a single target body, and their benchmarks were not made publicly available. Conversely, we benchmark both handcrafted and data-driven feature detection and description methods on real imagery of multiple small bodies with different surface characteristics and under varying illumination, scale, and perspective.\nWith respect to small body image datasets, we are only aware of the work by Zhou et. al [23, 24], which includes images of both mock-up and computer generated asteroid models. The authors fabricate in-house models to represent arbitrary small bodies as opposed to leveraging available models of asteroids observed from past or current small body missions. The authors do not apply their learned models on real mission imagery. In our work, we train and test our approach on real imagery."
        },
        {
            "heading": "3.3. Data-driven Relative Navigation",
            "text": "Fuchs et. al [25] train a random forest classifier on patches extracted from 119 images of the comets Hartley 2 and Tempel 1. However, significant performance degradation is observed when applied to unseen bodies, demonstrating the necessity to train models on data from a diverse set of small body instances. Pugliatti et. al [22] employ a custom U-Net for segmentation of small body images into a constrained set of classes (i.e., terminator, boulders, craters, surface, background) using synthetic images of 7 different small bodies (e.g., 101955 Bennu, 21 Lutetia). However, the performance suffers when applied to real images.\nData-driven crater detection has received much attention, especially for lunar applications. Wang et. al [48] leverages a lightweight CNN architecture pre-trained on Martian crater samples to extract feature maps, which are then fed into a fully convolutional architecture to perform crater detection. Detected craters are then matched against an a priori database for localization. Silburt et. al [49] implement a custom U-Net architecture to detect and identify craters from digital elevation maps (DEMs). Lee et al. [26] employ a CNN-based object detector to\ndiscriminate between a catalog of handpicked lunar surface landmarks while also predicting landmark detection probabilities as a function of the Sun\u2019s relative azimuth and elevation. The reliance on a catalog of known landmarks for navigation and the specification of craters as the most salient features limit the range of applications of this technology. Instead of explicitly specifying the featuresof-interest beforehand, we allow the network to learn the most salient features for a wide variety of surface characteristics."
        },
        {
            "heading": "4. The AstroVision Dataset",
            "text": "In this section, we present our novel small body image dataset, referred to as AstroVision, for training and evaluation of keypoint detection and feature description methods. AstroVision features over 110,000 real images of 16 small bodies from 8 missions, as shown in Figure 3. We describe the full data generation pipeline of AstroVision in the following subsections. Next, we develop a novel benchmarking suite (Section 5) and train a deep feature detection and description network (Section 6) using our novel dataset."
        },
        {
            "heading": "4.1. Image and Ancillary Data Extraction",
            "text": "AstroVision leverages publicly available images and ancillary data (i.e., camera pose, camera calibration, shape models) from both legacy and active small body science missions provided through NASA\u2019s Planetary Data System (PDS) [71] and maintained by NASA\u2019s Navigation and Ancillary Information Facility. High-fidelity shape models (i.e., watertight, 3D triangular surface meshes) are developed as part of the relative navigation pipeline of small body missions, as they are critical for characterization of the body and relative navigation in subsequent phases. Specifically, shape models for these missions are typically developed using SPC [6]. SPC leverages feature correspondences between images captured during an extended\ncharacterization phase procured by human operators on the ground. A network of landmarks is estimated using stereophotogrammetry and subsequently densified using photometric stereo techniques via a priori camera pose and sun pointing estimates and a reflectance model. The process yields high quality shape models that are precisely registered to the images and provide the foundation for our small body image dataset. For more details about the shape reconstruction and state estimation process, we refer the reader to [6], [10], and [7]. Moreover, information and references for the various missions, images, and shape models used in this work are provided in Table 1.\nImages provided by PDS are commonly stored using the Flexible Image Transport System (FITS), the standard data format used in astronomy, with pixel intensity values in units of either radiance (W s\u22121 m\u22122) or reflectance (unitless). We linearly scale pixel intensities to [0, 1] before converting to a grayscale Portable Network Graphics (PNG) image. Photometrically calibrated (e.g., flat field and dark current correction) images were utilized when available. Moreover, we provide undistorted images to ensure alignment with the depth maps by leveraging geometric distortion estimates derived during a meticulous calibration procedure conducted both on the ground and during flight by mission scientists. See Appendix A for specific calibration details for each mission."
        },
        {
            "heading": "4.2. Data Generation",
            "text": "The suite of AstroVision data products includes a landmark map, a depth map, and a mask for each image as shown in Figure 4. The landmark map provides a consistent, discrete set of tie-points for sparse correspondence computation and is derived by forward-projecting vertices from a medium-resolution (i.e., \u223c800k facets) shape model onto the image plane. We classify visible landmarks by tracing rays1 from the landmarks toward the camera origin and recording landmarks whose line-of-sight ray does not intersect the 3D model. The depth map provides a dense representation of the imaged surface and is computed by backward-projecting rays at each pixel in the image and recording the depth of the intersection between the ray and a high-resolution (i.e., \u223c3.2 million facets) shape model. Finally, the mask provides an estimate of the non-occluded portions of the imaged surface.\nIn order to generate the visibility masks, both global and dynamic intensity thresholding was used. For the more recent missions (i.e., Dawn, Hayabusa2, OSIRISREx, Rosetta), global thresholding was used. For some of the legacy missions (i.e., Cassini, Hayabusa, NEAR, Mars Express), variable vignetting was observed, primarily influenced by exposure time. Therefore, Otsu\u2019s method [72] was employed to compute a dynamic threshold for these instances. While illuminated pixels could have been computed by tracing the Sun\u2019s incident light ray, estimating\n1Ray tracing uses the Trimesh library: https://trimsh.org/\nthe mask independently of the ground truth scene geometry proved to be a useful tool for algorithmic outlier rejection, in addition to an extensive manual cleaning process. Specifically, we compute the ratio of the intersection area between the intensity mask and depth map and the total area of the mask as an alignment measure between the shape model and image, where a nominal value of 0.97 was empirically chosen. Moreover, we found that utilizing these intensity masks during training led to significant performance increases, which will be discussed further in Section 6.5."
        },
        {
            "heading": "5. Small Body Feature Benchmarks",
            "text": "In this section, we conduct a comprehensive evaluation of existing feature detection and description methods using the proposed AstroVision dataset. First, we detail our suite of performance metrics and verification procedures. Then, we present and discuss the benchmarking results."
        },
        {
            "heading": "5.1. Performance Metrics",
            "text": "We evaluate the matching performance on a per image pair basis using the standard metrics precision, recall, and accuracy. First, precision defines the inlier ratio of the putative matches (as determined by our verification process decsribed in the following section):\nprecision = # correct matches\n# putative matches . (14)\nSecond, recall describes the number of identified ground truth matches:\nrecall = # correct matches\n# ground truth matches . (15)\nThird, accuracy measures the matching performance with respect to the total number of computed features:\naccuracy = # correct matches & nonmatches\n# features . (16)\nWe classify correct nonmatches as keypoints which were not included in the set of putative or ground truth\nmatches, where we take the minimum of the number of such keypoints in each image in the pair [14].\nFinally, we compute the maximum of the angular error between the estimated and ground truth pose orientation and (unit) translation in degrees. Specifically, the angle of rotation between the estimated q\u0303CjCi and ground truth qCjCi relative orientation quaternions\nq := cos \u22121(\u3008q\u0303CjCi ,qCjCi\u3009 2 \u2212 1) (17)\nis used as a metric [73] for the orientation error, and t := cos \u22121 ( r\u0303 Cj CiCj \u00b7 rCjCiCj / \u2016r\u0303CjCiCj\u2016\u2016r Cj CiCj \u2016 ) (18)\nprovides a measure of the translation error. The final pose error metric is taken to be := max( q, t). The normalized cumulative error curve for is computed for each test sequence and the area under the curve (AUC) is reported for thresholds of 5\u25e6, 10\u25e6 and 20\u25e6. We compute AUC using the explicit integration procedure of [30] rather than coarse histograms."
        },
        {
            "heading": "5.2. Implementation",
            "text": "We evaluated the performance of ORB [29] and SIFT [16] as two representatives of handcrafted features. Three state-of-the-art data-driven features were selected that leverage different learning approaches (previously detailed in Section 2): SuperPoint [17], R2D2 [19], and ASLFeat [20]. We use the OpenCV implementations of ORB and SIFT and the open source implementations and pretrained models of the learned features made available by the respective authors. Each feature is limited to detect 5,000 keypoints and descriptors.\nGiven a set of keypoints and descriptors, putative matches are computed using MNN. Matches are verified by first backward-projecting (via Equation (6)) each keypoint in the first image into 3D world coordinates using the ground truth calibration and depth map. The 3D points are then forward-projected (via Equation (3)) into the second image, and matches are verified by checking that the projected image coordinates are within some distance \u03b3 to the keypoint of its matched feature, where we empirically chose a value of \u03b3 = 5 pixels. Ground truth matches are\nestimated in a similar way for computing recall, where a ground truth match is registered if there exists a keypoint within \u03b3 = 5 pixels of the projected image coordinate.\nFinally, poses are computed from the putative matches by first estimating the essential matrix using the five-point method [31], implemented in OpenCV\u2019s findEssentialMat function, and RANSAC with an inlier threshold of 1 pixel, followed by SVD of the essential matrix to determine the relative pose, implemented in OpenCV\u2019s recoverPose function. Evaluation is conducted for 2N randomly generated image pairs with at least 20% overlap with respect to the landmark map, where N is the number of images in the respective test dataset rounded up to the nearest multiple of 100, and metrics are averaged over all the image pairs."
        },
        {
            "heading": "5.3. Results & Discussion",
            "text": "We evaluated both handcrafted (i.e., ORB and SIFT) and data-driven (i.e., SuperPoint, R2D2, and ASLFeat) feature detection and description algorithms. These results are summarized in Table 2, and qualitative comparisons are provided in Figure 5. We also list the mean and median ground sample distance (GSD) for each dataset, i.e., the distance on the surface of the body covered by each pixel. SIFT demonstrates competitive performance on the Dawn and Cassini datasets, outperforming many of the data-driven methods, but suffers when applied to datasets with harsher illumination (i.e., Rosetta @ 67P, OSIRIS-REx @ 101955 Bennu). The efficacy of the orientation encoding of SIFT in certain scenarios can be seen in Figure 5a, although this behavior does not seem to be typical (see Figure 10). Superpoint achieves high recall but low precision and generally underperforms with respect to all other methods except ORB. Although R2D2 demonstrates high precision and accuracy, we found that the feature matches generally result in poor pose estimates. Finally, ASLFeat exhibits high precision, recall, and accuracy, which translates into generally superior relative pose estimates as indicated by the AUC score, and consistently ranks among the top performing methods with respect to all datasets. Therefore, we selected ASLFeat network for end-to-end training using the AstroVision data products. This is detailed in the next section.\nWe recognize the very low AUC values for all methods on the Cassini @ Mimas dataset. The relatively symmetric and homogeneous surface topology of Mimas generally resulted in low matching precision, and image pairs with high inlier ratios usually corresponded to pairs with relatively low baseline with respect to the radial imaging depth (e.g., Figure 5b) resulting in spurious relative translation estimates given even small amounts of measurement noise. Indeed, the Cassini @ Mimas images have a mean GSD of 1,176.2 m/pixel, almost four times that of the next highest value. We also observed correspondence configurations that resulted in ambiguous essential matrix estimates. This suggests that the points may lie close to a so-called critical surface [74], special surfaces which yield\nmultiple essential matrix estimates that satisfy Equation (7). Detection (e.g., via the iterative method presented in [75]) and rectification (e.g., by considering more views in a full SfM solution) of these degenerate configurations will be the subject of future work."
        },
        {
            "heading": "6. Learning Features from Small Body Imagery",
            "text": "In this section, we leverage the AstroVision dataset to train a deep feature detection and description network."
        },
        {
            "heading": "6.1. Network Architecture",
            "text": "Predicated on our evaluation benchmarks, we leverage the ASLFeat [20] network architecture. Given an image I \u2208 Rh\u00d7w\u00d7c, ASLFeat uses a single deep CNN to generate both a detection score (saliency) map S \u2208 Rh\u00d7w and a dense descriptor volume D \u2208 Rh/4\u00d7w/4\u00d7d.\nThe score map S is computed through aggregation of elements in intermediate feature maps Y (`) \u2208 Rh`\u00d7w`\u00d7b` , ` = 1, . . . , 3. Specifically, local peakiness over the channels Y (`) c , c = 1, . . . , b`, of the descriptor volume is used to compute channel-wise detection scores (dropping the ` subscript and superscript for conciseness):\n\u03b2cij = softplus ( ycij \u2212 1\nb \u2211 t ytij\n) (19)\nwhere ycij is the element at pixel (i, j) \u2208 {1, . . . , h} \u00d7 {1, . . . , w} in Yc and softplus(x) = log(1+exp(x)). Next, the local detection score is defined as\n\u03b1cij = softplus ycij \u2212 1|N (i, j)| \u2211 (i\u2032,j\u2032)\u2208N (i,j) yci\u2032j\u2032  (20) where N (i, j) is the set of 9 neighbors of the pixel (i, j) (including itself). The elements of the `th score map S(`) are computed as s (`) ij = maxc(\u03b1 c ij , \u03b2 c ij). Finally, each score map is bilinearly upsampled to the spatial resolution of the input image, and the elements in the final score map S are computed via a weighted average\nS = 1\u2211 ` w` \u2211 ` w`S (`), (21)\nwhere the weights w1, w2, w3 have been empirically set to 1, 2, 3, respectively.\nGiven correspondences M := {(k, \u03c4(k)) | \u03c4 : K \u2194 K \u2032} between keypoints {pk}k\u2208K and {p\u2032k\u2032}k\u2032\u2208K\u2032 extracted from images I and I \u2032, respectively, the total loss is formulated as\nL(D,D\u2032, S, S\u2032;M) = 1 |M| \u2211 (l,l\u2032)\u2208M sls \u2032 l\u2032\u2211 (k,k\u2032)\u2208M sks \u2032 k\u2032 m(dl,d \u2032 l\u2032). (22) where sk (s \u2032 k\u2032) is the detection score and dk (d \u2032 k\u2032) is the descriptor at keypoint pk (p \u2032 k\u2032), and m(\u00b7, \u00b7) is the descriptor\nreliability loss. Note that descriptors and detection scores at subpixel locations can be computed through (e.g., bilinear) interpolation of the score map S (S\u2032) and descriptor volume D (D\u2032). ASLFeat leverages a hardest-contrastive margin ranking loss [76] to enforce descriptor reliability:\nm(dl,d \u2032 l\u2032) = max (\u2016dl \u2212 d\u2032l\u2032\u2016 \u2212Mp, 0) +\nmax ( Mn \u2212min ( min k\u2032 6=l\u2032 \u2016dl \u2212 d\u2032k\u2032\u2016,min k 6=l \u2016dk \u2212 d\u2032l\u2032\u2016 ) , 0 ) ,\n(23)\nwhere Mp and Mn are the margins for positive and negative pairs, respectively.\nThe formulated loss L in Equation (22) produces a weighted average of the margin terms m over all matches based on their detection scores. Thus, in order for the loss to be minimized, the most distinctive correspondences (with a lower margin term) will get higher relative detection scores and vice versa."
        },
        {
            "heading": "6.2. Implementation Details",
            "text": "We train ASLFeat using a procedure similar to the original implementation [20]. The train/test split is shown in Table 3, where we use an approximate 90/10 split.\nTraining. The model is trained from scratch with ground truth cameras and depths from our AstroVision dataset. The relative perspective change between an image pair is limited during training, where the angle of rotation between the orientation quaternions of the respective images with respect to the body-fixed frame, as defined by Equation (17), is used as a metric for the relative perspective change between two images. We ignore image pairs with a value greater than q(qCiB,qCjB) = 60\n\u25e6. The training consumes \u223c800k image pairs resized to 480 \u00d7 480 using a batch size of 2. Ground truth matches for training are computed by first querying the landmark map for sparse correspondences. Dense matching is performed on image pairs with at least 128 shared landmarks by projecting a uniform grid of coordinates in the first image into the second image using the ground truth depth and calibration. Additionally, the visibility masks are used to remove matches that have keypoints in occluded regions of either\nimage. Learning gradients are computed for image pairs that have at least 128 matches, while a maximum of 512 randomly selected matches are used for back propagating gradients. Each input image is standardized to have zero mean and unit standard deviation. The SGD optimizer is used with momentum of 0.9, and an exponentially decaying learning rate is used with an initial value of 0.1. We use a two-stage training procedure as suggested by [20]. Specifically, all regular convolutions are trained for 400k iterations in the first stage of training. In the second stage, the DCNs are trained with the initial learning rate of 0.01 for another 400k iterations.\nTesting. Non-maximum suppression is applied (sized 3) to remove detections that are spatially too close. The position of the detected keypoints is improved using a local refinement and edge-elimination procedure over the detection score map following the approach used in SIFT [16]. The descriptors are then bilinearly interpolated at the refined (subpixel) positions. We select the top-k keypoints (nominally k = 5000) with respect to their detection scores, and empirically discard those whose scores are\nlower than 0.50."
        },
        {
            "heading": "6.3. Experiments",
            "text": "We withheld data corresponding to 4 different small bodies with variable surface characteristics from training, i.e., Cassini @ Epimetheus, Cassini @ Mimas, Hayabusa @ 25143 Itokawa, and Rosetta @ 21 Lutetia. In doing so, we test the networks ability to reliably compute features upon arrival at a previously unexplored small body. The network was also tested on held-out images of small bodies it saw during training. This emulates a scenario in which images obtained during earlier stages of a mission could be used to train the network for feature extraction in later phases of the mission. In order to minimize overlap between the train and test sets, we cluster images within each dataset according to the backward-projected 3D coordinates of the principle point in each image using k-means [77] with a value of k = 64. Seven of these clusters are held-out for testing while the remaining are used during training. A visualization of a subset of the clusters for the Dawn @ 1 Ceres dataset in shown in Figure 7. Matching and verification is conducted using the procedure described in Section 5.2."
        },
        {
            "heading": "6.4. Results & Discussion",
            "text": "The ASLFeat model trained on AstroVision data, i.e., ASLFeat-CVGBEDTRPJMU, is compared against the pretrained model. These results are shown in Table 4 and qualitative comparisons are shown in Figure 8. The model trained on AstroVision consistently outperforms the pretrained model with respect to precision, recall, accuracy, and AUC. Importantly, the AstroVision-trained model achieves increased matching performance on many of the novel testing instances, i.e., Cassini @ Epimetheus, Cassini @ Mimas, and Hayabusa @ 25143 Itokawa. Indeed, very little is known about the surface characteristics\nof a small body prior to arrival. Our model obtains higher precision and accuracy on all novel test instances with the exception of Rosetta @ 21 Lutetia. Despite the lower precision and recall on Rosetta @ 21 Lutetia, we are able to achieve significantly better pose estimates as indicated by the pose AUC metric. This is most likely due to the more uniform distribution of matches on the surface of the body, whereas the pretrained network primarily computes matches on the boundary of the body (see Figure 8d). Our model generally exhibits slightly lower recall, but achieves higher AUC on all novel test instances excluding Cassini @ Mimas.\nMoreover, ASLFeat-CVGBEDTRPJMU demonstrates impressive performance on the held-out images of the small bodies it saw during training. Our model demonstrates considerably higher performance with respect to all metrics on the Dawn @ 1 Ceres and Dawn @ 4 Vesta test sets. Intuitively, training on AstroVision data results in more conservative feature matching on the difficult OSIRIS-REx @ 101955 Bennu and Rosetta @ 67P test sets, as indicated by the higher precision and accuracy and lower recall and number of matches, which exhibit hard and rapidly changing illumination, significant perspective changes, and repetitive surface characteristics. We achieve slightly lower pose AUC as compared to the pretrained model for the OSIRIS-REx@ 101955 Bennu test set despite having higher precision and significantly higher accuracy. This is most likely due to the reduced number of matches, although this is primarily restricted to low precision image pairs as shown in Figure 9. Indeed, for difficult image pairs with precision close to zero, ASLFeat-CVGBEDTRPJMU features typically result in an order of magnitude fewer incorrect matches compared to the pretrained model. An example of this is provided in Figure 8g.\nWe experimented with training the network on OSIRISREx @ 101955 Bennu data only, referred to as ASLFeatB, as we suspected the network may be prioritizing discrimination of other feature classes more relevant to the other training instances due to the unique and challenging surface features of Bennu and the lower number of training images relative to some of the other missions (e.g., Dawn @ 1 Ceres, Rosetta @ 67P). Benchmarking results for this experiment are presented in Table 5. ASLFeat-B achieves increased performance with respect to all metrics compared to the pretrained model on the OSIRIS-REx @ 101955 Bennu dataset. We postulate that adding more small body instances with similar surface characteristics will increase performance.\nWe also compared matching precision against perspective and illumination changes in Figures 10 and 11. We leverage Equation (17) as a measure for perspective change, and\ns := cos \u22121(s\u0302Ci \u00b7 s\u0302Cj ) (24)\nas a measure of illumination change, where s\u0302Ci and s\u0302Cj denote the (unit) Sun vector in Ci and Cj , respectively. Our model exhibits superior invariance to both perspective\nTable 4: AstroVision-trained model compared to pretrained. Performance of the AstroVision-trained ASLFeat model compared to pretrained with respect to precision (P), recall (R), accuracy (A), and pose AUC in percentages. See Section 5.1 for metric definitions.\nAUC\nDataset # Images Feature # Matches P R A @5\u25e6 @10\u25e6 @20\u25e6\nCassini @ Epimetheus (Saturn XI)\u2020 133 ASLFeat 386 27.4 29.0 74.7 2.7 8.2 13.7\nASLFeat-CVGBEDTRPJMU 396 28.9 27.5 74.1 2.7 8.6 14.0\nCassini @ Mimas (Saturn I)\u2020 307 ASLFeat 372 21.8 15.7 65.3 0.2 0.2 0.3\nASLFeat-CVGBEDTRPJMU 328 23.6 14.9 67.1 0.0 0.1 0.2\nDawn @ 1 Ceres 3624 ASLFeat 1535 48.4 67.8 80.2 12.9 27.1 42.4\nASLFeat-CVGBEDTRPJMU 1514 52.8 71.5 82.1 15.9 31.6 46.9\nDawn @ 4 Vesta 2006 ASLFeat 1524 59.0 66.1 84.3 17.5 31.9 46.0\nASLFeat-CVGBEDTRPJMU 1412 70.3 69.7 87.4 17.5 33.0 48.7\nHayabusa @ 25143 Itokawa\u2020 603 ASLFeat 338 13.5 11.3 47.5 2.2 4.2 7.6\nASLFeat-CVGBEDTRPJMU 363 15.2 11.0 53.7 2.9 5.0 8.8\nOSIRIS-REx @ 101955 Bennu 1789 ASLFeat 1378 33.1 30.9 68.7 8.0 14.4 20.9\nASLFeat-CVGBEDTRPJMU 858 34.2 28.4 79.5 6.7 12.6 19.3\nRosetta @ 67P 3039 ASLFeat 1147 25.0 24.0 62.8 3.4 6.4 10.6\nASLFeat-CVGBEDTRPJMU 837 30.4 23.9 69.8 4.2 7.9 13.4\nRosetta @ 21 Lutetia\u2020 40 ASLFeat 970 42.9 35.0 71.9 6.0 12.1 23.8\nASLFeat-CVGBEDTRPJMU 778 41.3 31.1 76.3 8.4 13.2 22.3\n\u2020 No images of this body were included in the training set\nand illumination changes for all test sets.\nThe detection score maps S for the respective models, as described in Section 6.1, are visualized in Figure 12. It can been seen that the pretrained model repeatably places a high confidence to edges formed from hard shadowing and to features on the boundary between the body and deep space. Features in these regions are known to be rel-\nOSIRIS-REx @ 101955 Bennu\n0 20 40 60 80 10 0\n12 0\ns ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr ec\nisi on\nORB SIFT SuperPoint R2D2 ASLFeat ASLFeat-CVGBEDTRPJMU\nRosetta @ 67P\natively unreliable and not repeatable, as the appearance of these features can change dramatically due to deformation of the shadows, or become completely occluded, as the body rotates about its axis [15]. However, the model trained on AstroVision learns to assign a low confidence to these regions and gives a higher confidence to features corresponding to salient topographic structures such as rocky outcroppings and crater rims."
        },
        {
            "heading": "6.5. Ablation Study for Masking",
            "text": "We found that utilizing the visibility masks during training led to faster convergence and greatly increased overall performance. Specifically, a grid of image coordinates from the first image was projected into the second image using the ground truth calibrations of each camera and the depth map to generate a collection of ground truth matches during training. We mask keypoints according to the visibility masks of each image and ignore matches with keypoints in occluded regions during training. The results in Table 6 demonstrate how utilizing the visibility masks during training greatly improves the overall performance. It can be seen that there is a slight degradation in precision for the Dawn @ 1 Ceres dataset, which has significantly fewer shadowing occlusions as compared to the other datasets. This could indicate that exposing the network to training instances in occluded regions could benefit matching performance. Investigating training strategies that allow the network to effectively learn from occluded matches will be the subject of future work."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper we presented a first-of-a-kind dataset composed of densely annotated images of small celestial bodies acquired during past and ongoing missions. The AstroVision dataset was leveraged to develop a novel benchmark\nsuite for evaluation of feature detection and description methods on real remote imagery of small bodies. Moreover, we showed that leveraging the Astrovision data for training a deep feature detection and description network increases matching and pose estimation performance on small bodies with a wide variety of surface characteristics, including on bodies completely unseen during training. We believe that feature extraction based on deep learning is a promising alternative to current human-in-theloop practices used in state-of-the-practice small body 3D shape reconstruction methods, e.g., SPC [6]. Furthermore, pending ongoing advancements in space-grade multi-core\nprocessors [78, 79, 80, 81], deep learning approaches to feature extraction could feasibly be implemented for autonomous relative navigation onboard future spacecraft. Finally, we postulate that the use of AstroVision will extend beyond feature detection and description and enable the deployment of a variety of new deep learning methods for deep space applications, ultimately leading to a significant increase in small body science mission capabilities. The code, data, and trained models will be made available to the public at https://github.com/astrovision."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by a NASA Space Technology Graduate Research Opportunity. The authors would like to thank Kenneth Getzandanner and Andrew Liounis from NASA Goddard Space Flight Center for several helpful discussions, and Robert Gaskell from the Planetary Science Institute for providing detailed SPC shape models."
        },
        {
            "heading": "Appendix A. Photometric Calibration Details",
            "text": "Table A.7 defines the different types of photometric calibration applied to each of the datasets. Bias + Dark + Smear indicates that sensor bias subtraction, dark current (warm pixel) removal, and readout smear correction have been applied to the images. Radiometric indicates radiometric calibration was conducted to convert the raw sensor measurements to units of radiance or reflectance. Deblurred refers to applying a deblurring filter to the radiometrically calibrated images. More details can be found in the technical reports for the respective instrumentation: Cassini Imaging Science Subsystem (ISS) [82], Dawn Framing Camera [83], NEAR Multispectral Imager (MSI) [84], OSIRIS-REx Camera Suite (OCAMS) [85], Rosetta NavCam [86], and Mars Express High Resolution Stereo Camera (HRSC) [87]."
        }
    ],
    "title": "AstroVision: Towards Autonomous Feature Detection and Description for Missions to Small Bodies Using Deep Learning",
    "year": 2022
}