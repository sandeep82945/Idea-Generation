{
    "abstractText": "The paper addresses a problem of sequential bilateral bargaining with incomplete information. We proposed a decision model that helps agents to successfully bargain by performing indirect negotiation and learning the opponent\u2019s model. Methodologically the paper casts heuristically-motivated bargaining of a self-interested independent player into a framework of Bayesian learning and Markov decision processes. The special form of the reward implicitly motivates the players to negotiate indirectly, via closed-loop interaction. We illustrate the approach by applying our model to the Nash demand game, which is an abstract model of bargaining. The results indicate that the established negotiation: i) leads to coordinating players\u2019 actions; ii) results in maximising success rate of the game and iii) brings more individual profit to the players. INDEX TERMS Learning, Markov decision process, Nash demand game, Negotiation.",
    "authors": [
        {
            "affiliations": [],
            "name": "TATIANA V. GUY"
        },
        {
            "affiliations": [],
            "name": "JITKA HOMOLOV\u00c1"
        },
        {
            "affiliations": [],
            "name": "ALEKSEJ GAJ"
        }
    ],
    "id": "SP:086f9ddbc300f48fa1836177a87ea931c293d299",
    "references": [
        {
            "authors": [
                "Y.-Y. Yang",
                "X.-M. Xie"
            ],
            "title": "Research on the effectiveness of network negotiation based on evolutionary game model",
            "venue": "IEEE Access, vol. 8, pp. 194 623\u2013194 630, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. David",
                "E. Gerding",
                "D. Sarne",
                "O. Shehory"
            ],
            "title": "Agent-Mediated Electronic Commerce. Designing Trading Strategies and Mechanisms for Electronic Markets: AAMAS Workshop, AMEC 2009, Budapest, Hungary, May 12, 2009, and IJCAI Workshop",
            "venue": "TADA",
            "year": 2009
        },
        {
            "authors": [
                "L. Li",
                "K. Robert Lai",
                "S. Zhu"
            ],
            "title": "Data-driven behavior-based negotiation model for cyber-physical-social systems",
            "venue": "IEEE Access, vol. 7, pp. 83 319\u2013 83 331, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chakraborty",
                "T. Baarslag",
                "M. Kaisers"
            ],
            "title": "Automated peer-to-peer negotiation for energy contract settlements in residential cooperatives",
            "venue": "Applied Energy, vol. 259, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. L\u00f3pez",
                "B. Innocenti",
                "D. Busquets"
            ],
            "title": "A multiagent system for coordinating ambulances for emergency medical services",
            "venue": "IEEE Intelligent Systems, vol. 23, no. 5, pp. 50\u201357, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "T. Ito",
                "H. Hattori",
                "M. Zhang",
                "T. Matsuo"
            ],
            "title": "Rational, Robust, and Secure Negotiations in Multi-Agent Systems, ser. Studies in Computational Intelligence",
            "year": 2008
        },
        {
            "authors": [
                "K. Kang",
                "B. Qing Tan",
                "R.Y. Zhong"
            ],
            "title": "Multi-attribute negotiation mechanism for manufacturing service allocation in smart manufacturing",
            "venue": "Advanced Engineering Informatics, vol. 51, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Ryan",
                "J. Fokker",
                "S. Healy",
                "A. Amann"
            ],
            "title": "Dynamics of targeted ransomware negotiation",
            "venue": "IEEE Access, vol. 10, pp. 32 836\u201332 844, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Ren",
                "M. Zhang"
            ],
            "title": "A single issue negotiation model for agents bargaining in dynamic electronic markets",
            "venue": "Decis. Support Syst., vol. 60, p. 55\u201367, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. Kennan",
                "R. Wilson"
            ],
            "title": "Strategic Bargaining Models and Interpretation of Strike Data",
            "venue": "Journal of Applied Econometrics, vol. 4, no. S, pp. 87\u2013 130, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "A. Rubinstein"
            ],
            "title": "Perfect equilibrium in a bargaining model",
            "venue": "Econometrica, vol. 50, no. 1, pp. 97\u2013109, 1982.",
            "year": 1982
        },
        {
            "authors": [
                "J.F. Nash"
            ],
            "title": "The bargaining problem",
            "venue": "Econometrica, vol. 18, no. 2, pp. 155\u2013162, 1950.",
            "year": 1950
        },
        {
            "authors": [
                "H. Raiffa"
            ],
            "title": "The Art and Science of Negotiation",
            "year": 1982
        },
        {
            "authors": [
                "M.J. Osborne",
                "A. Rubinstein"
            ],
            "title": "A course in game theory",
            "year": 1994
        },
        {
            "authors": [
                "T. Ellingsen"
            ],
            "title": "The evolution of bargaining behavior",
            "venue": "The Quarterly Journal of Economics, vol. 112, no. 2, pp. 581\u2013602, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "S.G. Ficici",
                "A. Pfeffer"
            ],
            "title": "Modeling how humans reason about others with partial information",
            "venue": "Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1, ser. AAMAS \u201908. International Foundation for Autonomous Agents and Multiagent Systems, 2008, p. 315\u2013322.",
            "year": 2008
        },
        {
            "authors": [
                "R. Lin",
                "S. Kraus",
                "J. Wilkenfeld",
                "J. Barry"
            ],
            "title": "Negotiating with bounded rational agents in environments with incomplete information using an automated agent",
            "venue": "Artificial Intelligence, vol. 172, no. 6, pp. 823\u2013851, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "M.A. Neale",
                "M.H. Bazerman"
            ],
            "title": "Negotiator cognition and rationality: A behavioral decision theory perspective",
            "venue": "Organizational Behavior and Human Decision Processes, vol. 51, no. 2, pp. 157\u2013175, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "M. van\u2019t Wout",
                "L.J. Chang",
                "A.G. Sanfey"
            ],
            "title": "The influence of emotion regulation on social interactive decision-making",
            "venue": "Emotion, vol. 10, no. 6, pp. 815\u2013821, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "G.F. Loewenstein",
                "L. Thompson",
                "M.H. Bazerman"
            ],
            "title": "Social utility and decision making in interpersonal contexts",
            "venue": "Journal of Personality and Social Psycholog, vol. 57, no. 3, pp. 426\u2013441, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "T. Guy",
                "M. K\u00e1rn\u00fd",
                "A. Lintas",
                "A. Villa"
            ],
            "title": "Theoretical models of decision-making in the ultimatum game: Fairness vs. reason.",
            "venue": "Advances in Cognitive Neurodynamics (V),",
            "year": 2016
        },
        {
            "authors": [
                "R. Kulhav\u00fd",
                "M. K\u00e1rn\u00fd"
            ],
            "title": "Tracking of slowly varying parameters by directional forgetting",
            "venue": "IFAC Proceedings Volumes, vol. 17, no. 2, pp. 687\u2013 692, 1984, 9th IFAC World Congress: A Bridge Between Control Science and Technology, Budapest, Hungary, 2-6 July 1984.",
            "year": 1984
        },
        {
            "authors": [
                "B. An",
                "N. Gatti",
                "V. Lesser"
            ],
            "title": "Bilateral bargaining with one-sided two-type uncertainty",
            "venue": "Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02. IEEE Computer Society, 2009, p. 403\u2013410.",
            "year": 2009
        },
        {
            "authors": [
                "G. Haim",
                "Y.K. Gal",
                "B. An",
                "S. Kraus"
            ],
            "title": "Human\u2013computer negotiation in a three player market setting",
            "venue": "Artificial Intelligence, vol. 246, pp. 34\u201352, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Lin",
                "Y. Oshrat",
                "S. Kraus"
            ],
            "title": "Automated Agents that Proficiently Negotiate with People: Can We Keep People out of the Evaluation Loop",
            "year": 2012
        },
        {
            "authors": [
                "A. Quinn",
                "M. K\u00e1rn\u00fd",
                "T.V. Guy"
            ],
            "title": "Optimal design of priors constrained by external predictors",
            "venue": "International Journal of Approximate Reasoning, vol. 84, pp. 150\u2013158, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Homolov\u00e1",
                "E. Zugarov\u00e1",
                "T.V. Guy"
            ],
            "title": "On decentralized implicit negotiation in modified ultimatum game",
            "venue": "Multi-Agent Systems and Agreement Technologies, vol. 15, 2017, pp. 357\u2013369.",
            "year": 2017
        },
        {
            "authors": [
                "D. Cooper",
                "E. Dutcher"
            ],
            "title": "The dynamics of responder behavior in ultimatum games: A meta-study",
            "venue": "Experimental Economics, vol. 14, pp. 519\u2013546, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M. K\u00e1rn\u00fd",
                "T.V. Guy"
            ],
            "title": "On Support of Imperfect Bayesian Participants",
            "venue": "VOLUME",
            "year": 2012
        },
        {
            "authors": [
                "M.S. Fagundes",
                "S. Ossowski",
                "M. Luck",
                "S. Miles"
            ],
            "title": "Using normative markov decision processes for evaluating electronic contracts",
            "venue": "AI Commun., vol. 25, no. 1, p. 1\u201317, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P. Faratin",
                "C. Sierra",
                "N. Jennings"
            ],
            "title": "Using similarity criteria to make negotiation trade-offs",
            "venue": "Proceedings Fourth International Conference on MultiAgent Systems, 2000, pp. 119\u2013126.",
            "year": 2000
        },
        {
            "authors": [
                "D. Zeng",
                "K. Sycara"
            ],
            "title": "Bayesian learning in negotiation",
            "venue": "International Journal of Human-Computer Studies, vol. 48, no. 1, pp. 125\u2013141, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "L. Wu",
                "S. Chen",
                "X. Gao",
                "Y. Zheng",
                "J. Hao"
            ],
            "title": "Detecting and learning against unknown opponents for automated negotiations",
            "venue": "PRICAI 2021: Trends in Artificial Intelligence, D. Pham, T. Theeramunkong, G. Governatori, and F. Liu, Eds. Cham: Springer International Publishing, 2021, pp. 17\u201331.",
            "year": 2021
        },
        {
            "authors": [
                "T. Baarslag",
                "M.J.C. Hendrikx",
                "K.V. Hindriks",
                "C.M. Jonker"
            ],
            "title": "Learning about the opponent in automated bilateral negotiation: a comprehensive survey of opponent modeling techniques.",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2016
        },
        {
            "authors": [
                "U. Kiruthika",
                "T.S. Somasundaram",
                "S.K.S. Raja"
            ],
            "title": "Lifecycle Model of a Negotiation Agent: A Survey of Automated Negotiation Techniques",
            "venue": "Group Decision and Negotiation, vol. 29, no. 6, pp. 1239\u20131262, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Baarslag",
                "M. Kaisers",
                "E.H. Gerding",
                "C.M. Jonker",
                "J. Gratch"
            ],
            "title": "Self-sufficient, Self-directed, and Interdependent Negotiation Systems: A Roadmap Toward Autonomous Negotiation Agents",
            "year": 2022
        },
        {
            "authors": [
                "S. Fatima",
                "M. Wooldridge",
                "N. Jennings"
            ],
            "title": "Optimal negotiation of multiple issues in incomplete information settings",
            "venue": "Proc. of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004, N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., vol. 3, 2004, pp. 1080\u20131087.",
            "year": 2004
        },
        {
            "authors": [
                "D.E. Kr\u00f6hling",
                "O.J. Chiotti",
                "E.C. Mart\u00ednez"
            ],
            "title": "A context-aware approach to automated negotiation using reinforcement learning",
            "venue": "Advanced Engineering Informatics, vol. 47, p. 101229, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Lin",
                "Y. Gev",
                "S. Kraus"
            ],
            "title": "Bridging the gap: Face-to-face negotiations with an automated mediator",
            "venue": "IEEE Intelligent Systems, vol. 26, no. 6, pp. 40\u201347, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "B. An",
                "K.M. Sim",
                "C.Y. Miao",
                "Z.Q. Shen"
            ],
            "title": "Decision making of negotiation agents using markov chains",
            "venue": "Multiagent Grid Syst., vol. 4, no. 1, p. 5\u201323, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "S. Fatima",
                "M. Wooldridge",
                "N. Jennings"
            ],
            "title": "Approximate and online multi-issue negotiation",
            "venue": "Proc. of th 6th Inernational Joint Conference on Autonomous Agents and Multi-agent Systems, pp. 947\u2013954, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "F. Ren",
                "M. Zhang"
            ],
            "title": "A single issue negotiation model for agents bargaining in dynamic electronic markets",
            "venue": "Decision Support Systems, vol. 60, pp. 55\u201367, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Stern",
                "S. Kraus",
                "D. Sarne"
            ],
            "title": "A negotiating strategy for a hybrid goal function in multilateral negotiation",
            "venue": "CoRR, vol. abs/2201.04126, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Feng",
                "C. Tan",
                "J. Zhang",
                "Q. Zeng"
            ],
            "title": "Bargaining Game with Altruistic and Spiteful Preferences",
            "venue": "Group Decision and Negotiation, vol. 30, no. 2, pp. 277\u2013300, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Hindriks",
                "D. Tykhonov"
            ],
            "title": "Opponent modelling in automated multiissue negotiation using bayesian learning",
            "venue": "Proc. of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2008, p. 331\u2013338.",
            "year": 2008
        },
        {
            "authors": [
                "B. An",
                "V. Lesser",
                "K. Sim"
            ],
            "title": "Strategic agents for multi-resource negotiation",
            "venue": "Autonomous Agents and Multi-Agent Systems, pp. 1\u201340, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "J. Boger",
                "J. Hoey",
                "P. Poupart",
                "C. Boutilier",
                "G. Fernie",
                "A. Mihailidis"
            ],
            "title": "A planning system based on markov decision processes to guide people with dementia through activities of daily living",
            "venue": "IEEE Transactions on Information Technology in Biomedicine, vol. 10, no. 2, pp. 323\u2013333, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "M. Feng",
                "Y. Li"
            ],
            "title": "Predictive maintenance decision making based on reinforcement learning in multistage production systems",
            "venue": "IEEE Access, vol. 10, pp. 18 910\u201318 921, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Song",
                "C.-C. Liu",
                "J. Lawarree",
                "R. Dahlgren"
            ],
            "title": "Optimal electricity supply bidding by markov decision process",
            "venue": "IEEE Transactions on Power Systems, vol. 15, no. 2, pp. 618\u2013624, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "W.T. Scherer",
                "S. Adams",
                "P.A. Beling"
            ],
            "title": "On the practical art of state definitions for markov decision process construction",
            "venue": "IEEE Access, vol. 6, pp. 21 115\u201321 128, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W.B. Powell"
            ],
            "title": "Approximate Dynamic Programming, 2nd ed",
            "year": 2011
        },
        {
            "authors": [
                "V. Peterka"
            ],
            "title": "Bayesian approach to system identification",
            "venue": "In Trends and Progress in System Identification, P. Eykhoff, Ed. Pergamon Press, 1981, pp. 239\u2013304.",
            "year": 1981
        },
        {
            "authors": [
                "I. Kapoutsis",
                "R. Volkema",
                "A. Lampaki"
            ],
            "title": "Mind the first step: The intrapersonal effects of affect on the decision to initiate negotiations under bargaining power asymmetry",
            "venue": "Frontiers in Psychology, vol. 8, p. 1313, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Wu",
                "X. Liu",
                "J. Qin",
                "F. Herrera"
            ],
            "title": "Trust-consensus multiplex networks by combining trust social network analysis and consensus evolution methods in group decision-making",
            "venue": "IEEE Transactions on Fuzzy Systems, pp. 1\u20131, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Fiori",
                "A. Lintas",
                "S. Mesrobian",
                "A.E.P. Villa"
            ],
            "title": "Effect of Emotion and Personality on Deviation from Purely Rational Decision-Making",
            "venue": "VOLUME",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Learning, Markov decision process, Nash demand game, Negotiation.\nI. INTRODUCTION\nPOLITICS and business are considered traditionalspheres of human negotiation. The internet and modern means of communication have extended human negotiation to new domains such as social networks, deliberative democracy, e-commerce, cloud-based applications, [1], [2]. Besides, automatic bargaining and negotiation, being inevitable in modern cyber-physical-social systems [3], have been established in variety of applications, like network negotiation, energy trading [4] and traffic management [5], multi-robot systems [6], manufacturing service allocation [7] and newly in ransomware negotiation [8]. While solving negotiation task, agents must take into account incomplete information and strategically interact with other, human or artificial, agents. Majority of the existing research however assumes negotiation with non-human agents.\nHere we consider the simplest bilateral bargaining scenario with incomplete information often found in e-commerce [9]. A typical example is two self-interested agents (say, a buyer and a seller) bargaining on some goods or service. As soon as their price preferences differ, agents begin negotiations to achieve a mutually acceptable price. Either agent strives to satisfy own preferences as much as possible, but also has to take into account the opponent\u2019s preferences. Otherwise\nit is unlikely that an agreement can be reached1. Additional aspects of real-life bilateral bargaining to be considered are: i) multi-attribute negotiation when agents need to agree on goods/service characterised by several, possibly interrelated, attributes (say price of a product and terms of its delivery); ii) limited negotiation time as no agent can deliberate infinitely; iii) absence of moderator to coordinate the negotiation, so the agents must reach agreement themselves [11].\nThe negotiation has been widely addressed in diverse fields ranging from economy and sociology to computer science. An amount of works is much too large to survey them here. One can distinguish several main frameworks: game theoretic approach, negotiation protocols approach, evolutionary approach. Existing works however have different limitations preventing them from wide use. Game theoretic approach [12], [11], assumes that agents are perfectly rational and have common knowledge. Negotiation protocols approach, [13], needs the clear rules for negotiation, [14], and the results largely depend on the information available to the agents about each other. Evolutionary approach , [15], being inspired by biological evolution, finds optimal negotiation via trial-error and agents should have access to policy of their opponents and their profits. Some approaches are based on an agent-coordinator responsible for assigning goods or\n1for details on modelling bargaining, see for instance [10].\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nservices to agents. This coordinating (or planning) agent uses a negotiation mechanism to find the best share.\nWe consider a finite horizon bilateral sequential bargaining of two independent self-interested Bayesian decision making (DM) agents facing with incomplete information. The key aspects of the targeted solution are as follows.\n\u2022 Negotiation. The purpose of negotiation is to enable agents to coordinate their actions/decisions. Thus negotiation is a means to achieve coordinated behaviour of the agents. We consider the ability to negotiate an intrinsic part of an agent and treat it accordingly. The proposed solution allows indirect negotiation via information feedback and further leads to coordinated behaviour without conventional (explicit) negotiation. \u2022 Domain-independence. Existing solutions are either of domain-specific, [16], or domain-independent, [17]. The former ones may be more effective, but tailoring them to a new domain may often be useless. The ever-growing number of new applications make domain-tailored solutions less favourable. The considered Bayesian DM agent is inherently domainindependent. \u2022 Modelling and learning the opponent. Incomplete knowledge is given by uncertainty regarding the opponent\u2019s preferences and behaviour. This uncertainty may prevent agents from reaching mutually beneficial agreement as well as own DM goals. The proposed solution uses Bayesian approach to dynamically learn opponent\u2019s model based on observed actions (bids). \u2022 Bounded rationality. Assumption on perfect rationality used by game theoretic approach is not valid in reallife tasks. Moreover human agents often behave seemingly irrational due to cognitive or social factors [18]. Their DM is also influenced by emotional state [19] and personal traits [20]: self-interest, altruism, ability to cooperate. The proposed solution is general enough and has already proven to take into account humanlike factors [21]. Thus the approach can serve both an artificial agent and a human.\nOther important aspect of the negotiation problem concerns limited deliberation. Obviously, no agent can bargain indefinitely so the DM policy that is being designed must take that into account. It is hardly possible to set flexible limits on the length of negotiations, but we believe that the established internal feedback complemented by stopping rule can adaptively influence the length of negotiations. A natural decrease of the utility of goods/service over time can also be counteracted by introducing a kind of forgetting [22] in the utility function. Main contributions. The paper contributes to research on bilateral bargaining in distributed settings. We propose a self-interested probabilistic DM agent maximising expected utility, that is able to purposefully negotiate. The developed agent is domain-independent, can serve to either human or artificial agents and is equipped with the following abilities\n(which indicate major contributions):\n\u2022 Learning ability. To counteract incomplete knowledge and adapt to possible changes of its opponent, the agent is equipped with the learning ability. The algorithm is based on Bayesian approach and learns opponent\u2019s model from bargaining history, i.e. from the bids the opponent proposes during a negotiation. This allows to respect the opponent\u2019s dynamics as well as any other related uncertainty, cf. [23]. \u2022 Indirect negotiation. A key component of the proposed bargaining agent is a reward function that consists of two components. The first one respects a purely economical individual profit of the agent. The second component expresses degree to which bargaining agents exploit the game potential. It is important to note that the second component i) provides the agent with information feedback; ii) prompts the agents for indirect negotiation, and iii) set limits on the negotiation range. The trade-off between the individual profitability and the game potential is expressed by an agent-specific weight, cf. [24]. Naturally the opponent equipped with learning ability can model the weight and use this knowledge in next rounds. The weight expresses the agent preferences and partially reflects personal behavioural aspects of human bargaining. The latter opens an avenue for design of automated agents reflecting human traits, [25]. \u2022 Privacy preservation. The implicit nature of the resulting distributed interaction does not involve the exchange of any private data or models between players. Therefore the proposed approach fully preserves players\u2019 privacy.\nThe proposed solution also allows to incorporate prior knowledge of the opponent though does not require that. The methodology [26] makes it possible to use the available external or domain-specific knowledge to enrich the opponent\u2019s model. The paper also compares three types of prior knowledge reflecting typical cases and illustrate its use.\nThe paper continues our previous work [27] that assumes complete knowledge of the opponent model, which is rarely achievable in real-world applications. Thus the present paper focuses on learning the opponent model as well as on intrinsic motivation to cooperate. The last contribution of the work is that we have compared the performance of the proposed bargaining agents to agents employing heuristic models built on the extensive experimental meta-study [28]. Related research. The literature of negotiation constitutes a very large collection, and space limitations prevent it from being presented in its entirety here. Generally there are several models focusing on the explicit negotiation based either on game theory or negotiation theory. The proposed approach considers independent dynamical self-interested DM agents, with learning ability and special reward prompting indirect negotiation. The mentioned features are very practical and up to now missing within the otherwise well-elaborated and important area of the paper. Up to the authors best knowledge"
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nthere is no similar approach. We use probabilistic models [29] of bargaining agents that interact in a closed-loop and admit Markov decision processes as a modelling methodology, cf. [30]. The area of agent negotiation and opponent modelling has a lot of achievements, see for instance [31], [32] [33]. The comprehensive survey can be found in [34] and in [35]. The recent paper [36] discusses main challenges and promises in the area. Most research on negotiating concerns static environments and focused on i) developing utilitybased negotiation strategies for rational DM agents, see for instance [37], [38], and ii) creating agent-moderator helping DM agent in negotiation task, [17], [39], [40]. So far much less research describe negotiating in dynamic environment, see [41], [42]. The recent approach [43] uses a logistic regression for modelling the opponent, that requires collecting significant amount of data for learning and initialisation. Paper [44] uses a similar utility based on the bargaining principles though constructs a subgame that relies on the perfect equilibrium. The closely related work, dealt with opponent modelling, is probably [45]. It also employs Bayesian learning but relies on specific structure of preferences and policy of the opponent. Though work [46] also focuses on design of negotiation agents in dynamic and uncertain environments, it relies on a negotiation agent and proposes a set of heuristics to make negotiation decisions. Our model introduces an intrinsic mechanism that motivates the agent to negotiate while learning opponent\u2019s model via Bayesian approach. The resulting bargaining policy is optimal with respect to the resources available and individual preferences of the agents. It can also take into account human factors, which are important whenever human agents are involved.\nWe illustrate the approach using the Nash Demand Game (NDG) [12], a bilateral bargaining game for two players that should decide how to split given amount of money. The players simultaneously demand a certain portion of the amount they would like to get. The demand of one player is unknown to another one (an opponent). If the players\u2019 demands can be satisfied simultaneously, both players get the respective profit. Otherwise, they both get nothing. Despite its seeming simplicity, the NDG is a good model of dynamical resource allocation that achieves coordination without explicit negotiation. It also serves a big challenge for understanding human negotiation.\nThe remainder of the paper is organized as follows. Section II introduces notations and a mathematical background. Section III formulates the Nash Demand Game as MDP of a single player, introduces heuristic model of the opponent and prior models used in learning. Section IV describes and discuss simulated experiments. Section V and Section VI summarise the results obtained and outlines future research directions.\nII. PRELIMINARIES This section introduces and recalls necessary notions."
        },
        {
            "heading": "A. GENERAL CONVENTIONS",
            "text": "N, R set of natural numbers, set of real numbers xt \u2208 X value x from finite set X at discrete time t p(x) probability mass function of discrete random vari-\nable x p(x|y) probability mass function of x conditioned on y E[x|y] the expectation of x conditioned on y\nNote that no notational distinction is made between a random variable and its realisation."
        },
        {
            "heading": "B. MARKOV DECISION PROCESS",
            "text": "We model player\u2019s decision making in the NDG via Markov Decision Process (MDP) framework [47]. MDPs were first introduced and developed in the operations research and economics [48]. Since that MDP framework has been widely used to describe and solve decision-theoretic problems. MDP allows to capture the underlying stochastics omnipresent in application domain and also allows to respect multiple DM criteria. Typical examples of using MDP framework include medical applications [49], predictive maintenance [50], power systems [51], more examples see [52].\nThe overall scenario is as follows. An player interacts with the environment by taking actions to achieve its2 DM goal. The player is motivated by a reward it receives after each action taken. A finite state and action MDP is considered. Definition 1 (MDP): The fully observable MDP is characterised by {T,S,A, p, R}, where T = {1, 2, ..., N}, N \u2208 N, is a set of decision epochs; S is a finite set of all possible environment states and A denotes a finite set of all actions available to the player. Function p : S\u00d7S\u00d7A 7\u2192 [0, 1] is the transition model p(st+1|st, at) that moves the environment from state st \u2208 S to state st+1 \u2208 S after the agent took action at \u2208 A; R : S\u00d7S\u00d7A 7\u2192 R is a real-valued function representing the player\u2019s reward R(st+1, st, at) after taking action at \u2208 A in state st \u2208 S.\nThe transition model captures environment dynamics and is represented by a family of probability distributions p(st+1|st, at), each denotes the probability that at time t+ 1 the environment will move from st to st+1 when action at is executed. The state transitions obey Markov property: the distribution over states at time t+ 1 is independent of any previous state st\u2212j and action at\u2212j , j \u2264 1 for fixed st and at.\nThe player\u2019s preferences are described by a reward function, R. The aim of the player is to choose a sequence of actions in order to maximise the total expected sum of rewards as described in the following section."
        },
        {
            "heading": "C. OPTIMAL DECISION POLICY",
            "text": "The player chooses action at \u2208 A based on the randomised DM rule p(at|st) : S 7\u2192 A in each decision epoch t \u2208 T.\n2\"It\" is used as the generic pronoun. A device or an algorithm can be considered as the agent.\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nA sequence of DM rules forms DM policy \u03c0t,h at time t over decision horizon h \u2208 N, s\u03c4\u2208S, a\u03c4\u2208A:\n\u03c0t,h= { p(a\u03c4 |s\u03c4 ) \u2223\u2223\u2223s\u03c4 , a\u03c4 ,\u2211 a\u03c4\u2208A p(a\u03c4 |s\u03c4 )=1,\u2200s\u03c4\u2208S }t+h\u22121 \u03c4=t .\n(1) MDP with finite horizon h evaluates the quality of DM policy by expected total reward defined as follows:\nE [t+h\u22121\u2211 \u03c4=t R(s\u03c4+1, s\u03c4 , a\u03c4 )|st ] = (2)\nt+h\u22121\u2211 \u03c4=t \u2211 s\u03c4+1\u2208S s\u03c4\u2208S a\u03c4\u2208A R(s\u03c4+1, s\u03c4 , a\u03c4 )p(s\u03c4+1, s\u03c4 , a\u03c4 |st),\nwhere\np(s\u03c4+1, s\u03c4 , a\u03c4 |st) = p(s\u03c4+1|s\u03c4 , a\u03c4 )p(a\u03c4 |s\u03c4 )p(s\u03c4 |st).\nThe solution to MDP [47] is a sequence of DM rules,{ popt(a\u03c4 |s\u03c4 ) }t+h\u22121 \u03c4=t\n, that maximises the expected reward (2) and forms the optimal decision policy:\n\u03c0optt,h = arg max{\u03c0t,h}\u2208\u03c0 E [ t+h\u22121\u2211 \u03c4=t R(s\u03c4+1, s\u03c4 , a\u03c4 )|st ] , (3)\nwhere \u03c0 is a set of possible DM policies, see (1). The optimal policy (3) is computed by dynamic programming algorithm [48], [53], which requires knowledge of transition model p(s\u03c4+1|s\u03c4 , a\u03c4 ).\nD. LEARNING TRANSITION MODEL In bilateral bargaining, the transition model is a model of the opponent, that is, it predicts the opponent\u2019s reaction to the player\u2019s action. Generally it describes the dynamics of the opponent\u2019s decision making. In real-life tasks, opponent model p(st+1|st, at) is usually unknown to the player3. It reflects the player\u2019s knowledge about the behaviour of the opponent. Without lost of generality the model can be assumed timeinvariant, i.e. p(st+1|st, at) = p(st|st\u22121, at\u22121) and can be learned from the observed data.\nTo simplify the presentation, let us drop out the time index and introduce the following temporary notations: s\u2032 = st+1, s = st and a = at. The transition model then can be written p(s\u2032|s, a)4.\nWe consider a parametrised form of the opponent\u2019s model with time-invariant parameter \u03b8 \u2208 \u0398\np(s\u2032|s, a, \u03b8) = \u03b8s\u2032sa, \u03b8s\u2032sa \u2208 \u0398, (4)\nwhere \u0398 is a set of all possible \u03b8\u2019s and 0 \u2264 \u03b8s\u2032sa \u2264 1,\u2211 s\u2032\u2208S \u03b8s\u2032sa = 1, \u2200(s, a)|s \u2208 S and a \u2208 A. Thus, parameter \u03b8 in (4) is an array defining transition probabilities \u03b8s\u2032sa that opponent\u2019s state in the next time will equal s\u2032 whenever the\n3it can be partially known or incorrectly specified. 4The new notation is valid within Section II-D only.\nprevious state is s and the player takes action a. Our aim is to learn parameter \u03b8, (4).\nLet the player have belief b(\u03b8) about the opponent\u2019s dynamics expressed via the probability density function of the parameter \u03b8. While interacting with the opponent, the player updates belief about the parameter, b(\u03b8), to a new value, b\u2032(\u03b8), given observed transition (s\u2032, s, a) as follows, see [54]:\nb\u2032(\u03b8) \u221d b(\u03b8)p(s\u2032|s, a, \u03b8) = b(\u03b8)\u03b8s\u2032sa. (5)\nChoosing belief b(\u03b8) in conjugate form of Dirichlet distribution implies that the posterior (5) induced by Bayes\u2019 rule [54] is\nDir(\u03bd, \u03b8) \u221d \u220f s\u2032sa \u03b8 \u03bds\u2032sa\u22121 s\u2032sa . (6)\nIn (6) concentration parameter \u03bd > 0 is an array containing occurrences \u03bds\u2032sa > 0 of triples (s\u2032, s, a). Each observation of a triplet (s\u2032, s, a) increases the corresponding entry, \u03bds\u2032sa, by one.\nTherefore, after n \u2208 N observations {(s\u2032, s, a)}n\u2208N, update \u03bd\u2032s\u2032sa contains the actual occurrences of (s\n\u2032, s, a). Recalling (4), the expectation of (6) can be interpreted as Bayesian estimate of unknown parameter \u03b8 based on the observed data (i.e. transitions occurred):\nE [ p(s\u2032|s, a, \u03b8) \u2223\u2223\u2223\u2223\u03bd\u2032] = E[\u03b8s\u2032sa|\u03bd\u2032] = \u03bd\u2032s\u2032sa\u2211 s\u2032 \u03bd \u2032 s\u2032sa . (7)\nRecursive implementation of the prior statistics update is described in [55].\nA real-life dynamic decision making requires an efficient and feasible learning that can be performed online. Markov models belong to the exponential family for which exact estimation is feasible. The estimation and prediction within this family is very simple, especially with the conjugate prior in the form of Dirichlet distribution. The needed update of functions (probability density functions, see (5)) is given by the algebraic recursive update of the finite dimensional sufficient statistics. This clarifies applicability of this learning in combination with decision making."
        },
        {
            "heading": "III. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. MDP FORMALISATION OF NASH DEMAND GAME",
            "text": "The considered repetitive scenario of the game is as follows. Two structurally identical players A and B are bargaining on splitting an amount of money q \u2208 N. The roles of both players are the same. In each round, two stages are present: an action stage and a reward stage. During action stage, each player decides how much to claim from the total available amount. The players do not communicate and their interests can be competitive. At reward stage, the players announce their demanded shares, observe the demands of their opponents and reward is allocated. Note that in action stage each player has no information about their opponent\u2019s demand or preferences. The game runs for a fixed and known number of periods.\nLet q \u2208 N is a total amount to split. At the beginning of round t \u2208 T, each player k \u2208 {A,B} chooses action akt \u2208"
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nAk that is a demanded share of q in the round. The minimum demand equals 1 and the maximum is q \u2212 1. If the sum of demands is less than or equal to q, both players get what they asked for, otherwise the players get zero reward.\nPlayer\u2019s profit in round t \u2208 T equals the amount of money player receives2:\nzAt = a A t \u03c7(a A t , a B t ), zBt = a B t \u03c7(a A t , a B t ), (8)\nwhere zAt , z B t \u2208 Z are profits of A and B respectively. Z = {0, 1, 2, ..., q \u2212 1} is a set of possible profits in one game round, and\n\u03c7(aAt , a B t ) =\n{ 1 if aAt + a B t \u2264 q,\n0 if aAt + a B t > q.\n(9)\nThe addressed distributed bargaining does not consider communication between the players or any agent-moderator. To find a fully distributed solution, the game is described from a point of view of a stand-alone player. Let us now formulate the discussed bargaining task of a stand-alone player, say player A, as an MDP problem. Definition 2 (Bargaining as an MDP task): The bargaining scenario is modelled by tuple {T,S,A, p, R}, see Definition 1, where A = {1, 2, ..., q \u2212 1} is a set of possible actions; aAt \u2208 A is action of player A, i.e. a portion of q demanded by A at time t \u2208 T; st = (aAt\u22121, aBt\u22121) \u2208 S is a state observed by A at time t and p(st+1|aAt\u22121, st) is a transitional model that describes the state dynamics. Initial state s1 = (aA0 , a B 0 ) is preset to the same demand aA0 = a B 0 = a0.\nReward as motivation for negotiation. Let reward of player A be defined as follows:\nRAt = a A t (1\u2212\u03c9A)\u03c7(aAt , aBt )\u2212\u03c9A | q\u2212(aAt +aBt ) | . (10)\nThe first term in (10) is a pure economic profit of player A, cf. (8). The second term expresses efficiency of using the game potential at round t, i.e. whenever aAt +a B t < q some amount remains unclaimed and thus lost for the players. The same situation happens when an agreement is not reached and the entire amount q is lost.\nObviously reward (10) ensures that, given fixed aAt , player A will receive the maximum possible reward iff its opponent, B, demands q \u2212 aAt . The proposed form of reward, (10), \"connect\" A\u2019s action with that of B and thus encourages player A to indirectly negotiate with B during bargaining. The mechanism of dynamic indirect negotiation is as follows. Each player influences the amount left while their opponent observes this influence and changes their next demand. Let us assume that there is a tendency for some unclaimed amount to remain. Then, if one player has consumed a small portion of it, the other player will observe that and then may increase their demand in the next round. Another situation occurs when the joint claim of the players exceeds the available resources. Then any of the players may step back and reduce\n2Upper indexes indicate the player whom action or profit belongs to.\ntheir demand in the next round. This behaviour can again lead to a large unclaimed amount and affects the future demands of the players. In particular, the desire to minimise the unclaimed amount, | q \u2212 (aAt + aBt ) |, (10), forces player A to modify the current demand while taking into account the history of the opponent\u2019s claims. By doing so, in each round, each player dynamically adapts their demand to the foreseen demands of their opponent, that is indirectly negotiates with the opponent.\nWeight \u03c9A \u2208 [0, 1] in (10) reflects A\u2032s preferences between pure economic gain and exploiting the game\u2019s potential. The value \u03c9A = 0 implies player A considers pure economic profit only, while in case of \u03c9A = 1 player A cares about efficient use of the game potential. The A\u2032s reward (10) thus equals\nRAt =\n{ aAt \u2212 \u03c9A ( q \u2212 aBt ) if aAt + a B t \u2264 q,\n\u2212\u03c9A ( q \u2212 aBt ) otherwise.\n(11)\nDefinition 2 and considerations above describe DM of player A. Easy to see that the same considerations can be applied to formalise decision making of player B.\nThe conditional independence of the players\u2019 actions given by the game rules and the definition of the state, see Definition 2, imply\np(st+1|st, aAt ) = p(aAt |aAt\u22121, aBt\u22121)p(aBt |aAt\u22121, aBt\u22121). (12)\nFrom player A point of view, the first factor in (12) is a part of A\u2019s optimal policy while the second factor models DM of player B and can be recursively estimated using Bayesian paradigm [54] as described in Section II-D."
        },
        {
            "heading": "B. HEURISTIC MODEL OF OPPONENT",
            "text": "The proposed approach formalised and solved bilateral dynamic bargaining of learning self-interested player within MDP framework (Section II-C). To verify the approach we propose a probabilistic bargaining model for non-learning and non-optimising opponent. The model is based on the reported experimental evidence obtained with human-players, see [56], [28]. For simplicity here we consider player B is serving as an opponent to A.\nHeuristic behaviour of B reflects the dependence of its future demand on the results of the previous round. Once the previous round demands are incompatible, that is aAt\u22121 + aBt\u22121 > q, player B tends to decrease next demand. If there are unclaimed money left in the previous round, B, on the contrary, increases the next demand. The proportion (speed) of demands\u2019 increase/decrease may depend on personal traits (i.e. reflect the personality of B).\nThe remainder of this section introduces model that reflects the behaviour of an opposing player, B."
        },
        {
            "heading": "1) B had Low Demand in the Previous Round",
            "text": "Consider the previous demand of player B is low, i.e. less than the fair split would have been, aBt\u22121 \u2264 q 2 . The next de-\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nmand (in sense of its mean value) then depends on the success of the previous round, i.e. whether demands in the previous round were compatible or not. Below we distinguish these two cases and provide the respective probabilistic description of B\u2019s actions.\ni) Incompatible Demands (aAt\u22121 + aBt\u22121 > q): B tends to keep its next demand close to the previous one, aBt , as the previous demand of A was certainly much higher than aBt\u22121. Thus any further increase could cause players\u2019 demands to become incompatible again and implies zero profit. Therefore the new demand of player B can be modelled as follows:\np(aBt |aAt\u22121, aBt\u22121) \u221d exp\n( \u2212 ( aBt \u2212 aBt\u22121 )2 2\u03c32 ) (13)\nwhile aBt\u22121 \u2264 q 2 .\nii) Compatible Demands (aAt\u22121+aBt\u22121 \u2264 q): opponent B will proportionally increase the next demand, expecting A to do the same in order to fully distribute the entire available amount, q. In other words player who received less in the previous round would also ask for proportionally less unclaimed money and vice versa. A model of B describing the new demand is then\np(aBt |aAt\u22121, aBt\u22121) \u221d exp ( \u2212 K 2\u03c32 ) , (14)\nwith K= ( aBt \u2212 aBt\u22121 \u2212\naBt\u22121 aAt\u22121+a B t\u22121\n(q \u2212 aAt\u22121 \u2212 aBt\u22121) )2\nwhile aAt\u22121 + a B t\u22121 \u2264 q and aBt\u22121 \u2264 q 2 ."
        },
        {
            "heading": "2) B had High Demand in the Previous Round",
            "text": "Now let us consider a situation when the previous demand of B was high, i.e. its value was greater than the fair split would have been, aBt\u22121 > q 2 . Then B decreases/increases demand while keeping own share proportional to the previous round in order to fully distribute the entire amount. A player who received less in the last round would ask for less of proportionally less unclaimed money and vice versa. Then a model of B\u2032s new demand has the same form as (14).\nC. PRIOR MODELS USED IN LEARNING\nOur approach considers decision making of the player in question, A, who models behaviour of the opponent, B, and optimises own demand in order to maximise the accumulated profit. The ability to accurately predict the opponent\u2019s behaviour significantly affects the success of A\u2032s decision making, (12). To learn a model of the opponent, A follows the approach described in Section II-D. It exploits knowledge available in the form of a parameter prior that quantifies A\u2032s belief about dynamics of the opponent, B. Following Bayesian paradigm this prior will be gradually updated with new data accumulated, see Section II-D and [55]. The choice of prior model is important, especially when a number of\ngame rounds is limited. In implementation we use three prior models reflecting different knowledge A about B:\n\u2022 a uniform prior distribution. This model is used when A has little or no knowledge about the dynamics of B \u2022 prior model describing \"rational\" heuristic, see Section III-B. It is used when non-optimising B follows some heuristic and does not optimise. In that case prior model has the same structure as (13) or (14), but with different (larger) standard deviation \u03c3. \u2022 pre-trained prior model. The third way of building an a priori model mimics the natural learning process of human players, where the player first gathers some knowledge about the opponent\u2019s playing style and then updates this knowledge during the game. Practically it means we run game for 30 preliminary rounds and player A built prior model of B based on the data obtained during these rounds. This way of building prior is used whenever the both players optimise and learn."
        },
        {
            "heading": "IV. SIMULATED EXPERIMENTS",
            "text": "The proposed approach is illustrated with the Nash demand game, described in Section III-A, using simulated examples5.\nWe selected the most representative experiments from a much wider set of the experiments differing in the number of rounds and horizons. The selected experiments are long enough to perform learning (because very short runs will not be sufficient to learn the models used), while longer runs will add no significant information about the results.\nGoal of the experiments The goal was to analyse the impact of the proposed distributed solution and indirect negotiation and to verify that player employing the proposed DM policy is capable of achieving better results than heuristic player playing the same role. The main objectives of the performed experiments are:\n\u2022 illustrate the distributed DM approach in repetitive bargaining; \u2022 show that the proposed form of the reward function leads to an indirect negotiation and to a coordinated course of actions of both players, that is, to a more efficient allocation of the available limited resources; \u2022 demonstrate influence of weight \u03c9 in (10) \u2022 show that DM policy with indirect negotiation brings\nhigher profit to every player compare to the heuristic model.\nCommon settings of the experiments Each game has 60 rounds and optimisation horizon h \u2208 N equals 10 game rounds. The amount of money that players can split (if they reach an agreement) is q = 10 CZK per round. The reward (10) is evaluated for the optimal policy (3) resulted from the dynamic programming [48]. The initial state of each player s1 = (aA0 , a B 0 ) is preset to a A 0 = a B 0 = 3.\n5The examples were implemented in MATLAB, The MathWorks, Inc."
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nThe simulation is performed for 11 different values of weight \u03c9, (10). Weight (0 \u2264 \u03c9 \u2264 1) expresses a trade-off between the individual profitability and efficiency of using the game resources. It thus reflects the extent to which the player is negotiating. Zero value of \u03c9 in (10) models the situation when the player is interested only in economic profit. Other values of \u03c9 (0 < \u03c9 \u2264 1) correspond to cases when the player maximises the personal profit while minimising the unclaimed amount of money.\nExperiments performed The players used in the simulation are artificial agents with either heuristic DM model (see Section III-B) or proposed DM policy that optimises reward (10), see Section II-C. In each game at least one of the players uses the observed behaviour to update the opponent\u2019s model, see Section II-D. In order to display behaviour of our bargaining model, five typical cases were considered: Test 1 : Both players are non-learning. The player in ques-\ntion, A, is of the MDP type and uses the proposed DM policy optimising (10). Its opponent, B, behaves heuristically, see Section III-B.\nTest 2 : This case is similar to Test 1 but player A dynamically learns the opponent\u2019s model. Test 3 : Both players are of the MDP type and non-learning. They have no knowledge of their opponent and do not model it either (i.e. they use uniform model). Test 4 : Both players are of the MDP type, and use having non-informative prior for learning, see Section II-D. Test 5 : This case is similar to Test 4, but the players use informative priors (i.e. opponent model trained during the preliminary phase).\nApproach verification The players have played the game repeatedly with different settings. The results are summarised in graphs depicting individual cumulative profits of the players, total profit of the game, and success rates of game depending on the value of parameter \u03c9. The success rate is defined as a number of game rounds in which the players\u2019 demands were compatible and thus satisfied. In other words, the value of the success rate shows how successfully the players collaborated, i.e. respected the opponent\u2019s actions. High values indicate high collaboration. The results show minimum, mean and maximum values of the individual cumulative profits and the game success rate. Note that\n\u2022 The maximum success rate does not necessarily imply the maximum total profit of the game. \u2022 Compatibility of the players\u2019 claims does not guarantee zero unclaimed amount in the game. \u2022 It is not guaranteed either that the maximum profit will be obtained for the same value of the weight \u03c9. Thus the total maximum (minimum) profit of the game is not equal to the sum of the individual maximum (minimum) profit of the players."
        },
        {
            "heading": "A. TEST 1: A IS A NON-LEARNING MDP PLAYER, B BEHAVES HEURISTICALLY",
            "text": "Player B, behaves according to the heuristic model (13), (14) with \u03c32 = 1.\nPlayer A is of MDP type and uses DM policy (3) that optimises reward (10). In optimisation A uses model p(st+1|st, at) having structure of the heuristic model, see Section III-B, but with different parameter \u03c3 = 3. This imitates a situation when A has partial or vague knowledge of the opponent.\nCumulative profits of the players A and B are shown in Figure 1 and Figure 2. Total cumulative profit and success rate of the game as a function of parameter \u03c9A are shown in Figure 3 and Figure 4.\nThe players are successful in more than 51% of the rounds on average. The results show influence of parameter \u03c9A on profit: the higher the parameter, the higher the profits of individual players and the higher the total profit of the game. This indicates a positive effect of the second term (10), which prompts A to indirectly negotiate with B by minimising the unclaimed amount in each round. As a result the players start to implicitly cooperate.\nThe results show the saddle value of parameter \u03c9A = 0.5 that provides the minimum values of A\u2019s profit and success rate of the game. The maximum is reached for \u03c9A = 1. Obviously, optimising player A earned slightly less on average than non-optimising player B. It could be because player B used fixed decision making rules and A had to adapt to that."
        },
        {
            "heading": "B. TEST 2: A OPTIMISES AND LEARNS, B BEHAVES HEURISTICALLY",
            "text": "This experiment is similar to Test 1, see Section IV-A, i.e. player B behaves accordingly to the heuristic model, Section III-B, and player A uses optimal DM policy minimising the proposed reward, (10). Unlike Test 1, player A is learning. A considers a uniform prior as B\u2019s transition model and dynamically updates it via data gathered, see Section III-C.\nCumulative profit and success rate obtained in Test 2 are shown in Figures 5-8 and Table 2. Obviously the learning has a positive impact on the game results. On average, the players are successful in more then 66% of all rounds - the average success rate is about 15% higher than in Test 1, as is the cumulative profit. The minimum values for individual profits and overall success rate are significantly higher cf. Table 1. On the other hand, their maximum values have noticeably\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\ndecreased. The players have similar individual profits and their values weakly depend on parameter \u03c9A.\nC. TEST 3: BOTH PLAYERS OPTIMISE BUT NONE LEARNS This experiment considers both players are of MDP type and select DM policy maximising reward (10). However neither of the players is learning. They use a fixed uniform model (see Section III-C) that models the situation when there is no information about the opponent.\nCumulative profits and success rate of the game vs. param-\nFigure 3. Test 1 - Total Profit of Players.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOmega A\nS uc\nce ss\nR at\ne\nSuccess Rate of Games\nFigure 4. Test 1 - Success Rate of the Games.\neters \u03c9A and \u03c9B are shown in Figures 9-12 and Table 3. The results illustrate positive impact of i) optimal bargaining compare to heuristic behaviour, cf. results of Test 1 and Test 2 and ii) proposed reward (10) that prompts on indirect negotiation. Even with non-informative prior knowledge, the players get higher profit. If the players\u2019 weights are \u03c9A \u2265 0.5 and \u03c9B \u2265 0.5 the success rate is 100% and overall game profit gained is close to the maximum possible (600 CZK), see Table 3. By other words: when the players care about the optimal allocation of the resources (by assigning high weights to the second term in reward (10)), the bargaining is more profitable. On average, the players are successful in more than 76% of all rounds."
        },
        {
            "heading": "D. TEST 4: PLAYERS OPTIMISE AND LEARN WITH UNIFORM PRIOR",
            "text": "This experiment is similar to Test 3, i.e. both players are of MDP type and maximise reward (10). Unlike Test 3, the ability to learn the opponent\u2019s model has been added to the players. The agents dynamically enhance their non-"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\ninformative (uniform) priors based on the data observed during the game. Thus each player i) learns their opponent; ii) searches for optimal demand; iii) indirectly negotiates via minimising unshared resources.\nCumulative profits and success rate of the game in dependence on parameters \u03c9A and \u03c9B are shown in Figures 13-16 and Table 5.\nThe results show significant improvement due to the learning. The minimum values of the individual profits and the success rate decreased but their maximum values increased on average, see Table 4. The significant improvement oc-\nFigure 7. Test 2 - Overall profit of the players.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOmega A\nS uc\nce ss\nR at\ne\nSuccess Rate of Games\nFigure 8. Test 2 - Success rate of the games.\ncurred when \u03c9A \u2264 0.5 and \u03c9B \u2264 0.5, see Figures 13-15. Compare to Test 3, learning ability brought higher individual profits as well as higher success rates achieved for relatively low values of \u03c9. Thus learning helps even when the player\u2019s willingness to negotiate (expressed by \u03c9) is low."
        },
        {
            "heading": "E. TEST 5: INFORMATIVE PRIOR INFORMATION",
            "text": "This experiment is a modification of Test 4 with each of the players having meaningful prior knowledge of the opponent. First, to get informative prior, the players played 30 training\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nrounds during which they gained prior models of their opponents. Then, Test 3 has been performed with the resulting prior instead of uniform distribution.\nCumulative profits and success rate of the game in dependence on parameters \u03c9A and \u03c9B are shown in Figures 17-20 and Table 5.\nThe results show further improvement, see Table 5, cf. Tests 3-4. The minimum values of profits and success rate do not change but the maximum and mean values noticeably increased, cf. Test 4 (Section IV-D). The players achieve much higher individual profits for low values of weights \u03c9 because they coordinated their demands to make them almost always compatible (see Figure 20)."
        },
        {
            "heading": "V. DISCUSSION",
            "text": "Section IV describes simulation results obtained on the NDG. It can be seen that our DM model can help the players to effectively bargain and counteract the incomplete knowledge. The main advantages of the proposed DM model are as follows:\n\u2022 The proposed reward function respects individual eco-"
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nnomic profit of the bargaining agent and the unclaimed amount of money from the previous round. As the opponent\u2019s past actions enter the reward (10), the optimal policy of the agent implicitly respects them. And vice versa: the optimal policy of the opponent respects agent\u2019s actions. Hence both players are forced to implicitly cooperate. \u2022 The weight \u03c9 in (10) expresses trade-off between the individual profitability and efficiency of using game potential. At the same time it also reflects agent\u2019s preferences and partially style of playing (personal traits). High values of the weight in the player\u2019s reward (10) indicate a high interest of the agent in efficient use of game resources, i.e. in minimising the remaining unclaimed amount. In each round thus the reward encourages the agent to dynamically \"adapt\" its current demand to the\npredicted demand of the opponent. In the next round, the resulting profit6 together with the updated opponent\u2019s model, is used in (2), (3) to select a new demand. This is the essence of the proposed indirect dynamic negotiation. \u2022 Compared to the heuristic bargaining model, Section III-B, our optimal DM policy increased the mean value of the player\u2019s individual profit by more than 50% (in the case of an uninformative prior) and by about 65% (informative prior). \u2022 Learning significantly improves the bargaining results. However optimising but not learning agent can have worse individual results compare with the heuristic op-\n6which reflects the effect of the previous round and thus provides a feedback to the agent.\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nponent. The reason is that the optimising agent implicitly cooperates with the opponent during bargaining but does not use the correct opponent model for this7. On contrary, the opponent does not cooperate and it uses a fixed heuristic model. As a result, the agent\u2019s effort brings more profit to the opponent than to itself. \u2022 The best bargaining results were achieved if both players are learning and employ the proposed bargaining policy. Informative prior used in learning can significantly improve the agent\u2019s profit.\nThe proposed solution can be further extended i) to cover multi-issue bargaining; ii) to respect human non-rationality given by social and cognitive aspects; iii) to respect emotional state of the agent that has been proved to significantly\n7and therefore cannot predict the opponent\ninfluence DM [57]."
        },
        {
            "heading": "VI. CONCLUDING REMARKS",
            "text": "The paper addresses a problem of sequential bilateral bargaining with incomplete information. We proposed DM model that helps agents to successfully bargain by performing indirect negotiation and learning the opponent\u2019s model. Methodologically the paper casts heuristically-motivated bargaining of a self-interested independent agent into a framework of Bayesian learning and Markov decision processes. The proof of the main results is based on the standard methodology. However, the problem formulation and the gained solution are novel and practically important. The special form of the reward implicitly motivates the players to negotiate indirectly, via closed-loop interaction. At the"
        },
        {
            "heading": "12 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\nsame time the proposed method is privacy-preserving, since it does not require the exchange of data or models between the bargaining agents. We illustrate the approach by applying our model to the Nash demand game, which is an abstract model of bargaining. The paper provides our original formulation and solution of the practically important DM scenario. It presents the initial study that confirms that our formulation is meaningful and gives the promising results. The results indicate that the introduced DM model: i) leads to coordinating the players\u2019 actions and to their indirect negotiation; ii) results in maximising success rate of the game and iii) brings more individual profit to the players compare to the heuristic model.\nThe proposed bargaining policy minimises losses caused by: (i) insufficient use of the resources; (ii) demands that exceed the total resources available; and (iii) incomplete knowledge.\nThe results obtained indicate possibility to create a realistic and applicable methodology of cooperation and negotiation in flatly organised networks of interacting agents without a fixed structure, cf. [58]. We believe that our approach is suitable for non-cooperative, multi-agent networks, since we provide an easy way to implicit cooperation. The solution does not rely on a central authority and the proposed DM model outperforms a heuristic model whenever both agents are rational, learning and follow the optimal strategy.\nIn future work we would like :\n\u2022 to cover the multi-issue bargaining; \u2022 to extend the approach to a multi-agent settings; \u2022 to implement the approach for other bargaining rules\nthan NDG.\nFurther foreseen challenge is learning weights of individual players based on their bargaining history. The weights indirectly reflect agent\u2019s model of bargaining and preferences. Moreover the weights may depend on the agent\u2019s personality [59], which allows taking into account the influence of personality traits on decision making.\nACKNOWLEDGEMENT The authors would like to thank Miroslav K\u00e1rn\u00fd for his thorough feedback on an earlier draft and for useful discussions.\nReferences [1] Y.-Y. Yang and X.-M. Xie, \u201cResearch on the effectiveness of network\nnegotiation based on evolutionary game model,\u201d IEEE Access, vol. 8, pp. 194 623\u2013194 630, 2020. [2] E. David, E. Gerding, D. Sarne, and O. Shehory, Agent-Mediated Electronic Commerce. Designing Trading Strategies and Mechanisms for Electronic Markets: AAMAS Workshop, AMEC 2009, Budapest, Hungary, May 12, 2009, and IJCAI Workshop, TADA 2009, Pasadena, CA, USA, July 13, 2009, Selected and Revised Papers, ser. Lecture Notes in Business Information Processing. Springer Berlin Heidelberg, 2010. [3] L. Li, K. Robert Lai, and S. Zhu, \u201cData-driven behavior-based negotiation model for cyber-physical-social systems,\u201d IEEE Access, vol. 7, pp. 83 319\u2013 83 331, 2019. [4] S. Chakraborty, T. Baarslag, and M. Kaisers, \u201cAutomated peer-to-peer negotiation for energy contract settlements in residential cooperatives,\u201d Applied Energy, vol. 259, 2020.\n[5] B. L\u00f3pez, B. Innocenti, and D. Busquets, \u201cA multiagent system for coordinating ambulances for emergency medical services,\u201d IEEE Intelligent Systems, vol. 23, no. 5, pp. 50\u201357, 2008. [6] T. Ito, H. Hattori, M. Zhang, and T. Matsuo, Rational, Robust, and Secure Negotiations in Multi-Agent Systems, ser. Studies in Computational Intelligence. Springer Berlin Heidelberg, 2008. [7] K. Kang, B. Qing Tan, and R. Y. Zhong, \u201cMulti-attribute negotiation mechanism for manufacturing service allocation in smart manufacturing,\u201d Advanced Engineering Informatics, vol. 51, 2022. [8] P. Ryan, J. Fokker, S. Healy, and A. Amann, \u201cDynamics of targeted ransomware negotiation,\u201d IEEE Access, vol. 10, pp. 32 836\u201332 844, 2022. [9] F. Ren and M. Zhang, \u201cA single issue negotiation model for agents bargaining in dynamic electronic markets,\u201d Decis. Support Syst., vol. 60, p. 55\u201367, 2014. [10] J. Kennan and R. Wilson, \u201cStrategic Bargaining Models and Interpretation of Strike Data,\u201d Journal of Applied Econometrics, vol. 4, no. S, pp. 87\u2013 130, 1989. [11] A. Rubinstein, \u201cPerfect equilibrium in a bargaining model,\u201d Econometrica, vol. 50, no. 1, pp. 97\u2013109, 1982. [12] J. F. Nash, \u201cThe bargaining problem,\u201d Econometrica, vol. 18, no. 2, pp. 155\u2013162, 1950. [13] H. Raiffa, The Art and Science of Negotiation. University Press Cambridge, USA, 1982. [14] M. J. Osborne and A. Rubinstein, A course in game theory. Cambridge, USA: The MIT Press, 1994. [15] T. Ellingsen, \u201cThe evolution of bargaining behavior,\u201d The Quarterly Journal of Economics, vol. 112, no. 2, pp. 581\u2013602, 1997. [16] S. G. Ficici and A. Pfeffer, \u201cModeling how humans reason about others with partial information,\u201d in Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1, ser. AAMAS \u201908. International Foundation for Autonomous Agents and Multiagent Systems, 2008, p. 315\u2013322. [17] R. Lin, S. Kraus, J. Wilkenfeld, and J. Barry, \u201cNegotiating with bounded rational agents in environments with incomplete information using an automated agent,\u201d Artificial Intelligence, vol. 172, no. 6, pp. 823\u2013851, 2008. [18] M. A. Neale and M. H. Bazerman, \u201cNegotiator cognition and rationality: A behavioral decision theory perspective,\u201d Organizational Behavior and Human Decision Processes, vol. 51, no. 2, pp. 157\u2013175, 1992. [19] M. van\u2019t Wout, L. J. Chang, and A. G. Sanfey, \u201cThe influence of emotion regulation on social interactive decision-making,\u201d Emotion, vol. 10, no. 6, pp. 815\u2013821, 2010. [20] G. F. Loewenstein, L. Thompson, and M. H. Bazerman, \u201cSocial utility and decision making in interpersonal contexts,\u201d Journal of Personality and Social Psycholog, vol. 57, no. 3, pp. 426\u2013441, 1989. [21] T. Guy, M. K\u00e1rn\u00fd, A. Lintas, and A. Villa, \u201cTheoretical models of decision-making in the ultimatum game: Fairness vs. reason.\u201d in Advances in Cognitive Neurodynamics (V), P. X. Wang R., Ed. Springer, 2016. [22] R. Kulhav\u00fd and M. K\u00e1rn\u00fd, \u201cTracking of slowly varying parameters by directional forgetting,\u201d IFAC Proceedings Volumes, vol. 17, no. 2, pp. 687\u2013 692, 1984, 9th IFAC World Congress: A Bridge Between Control Science and Technology, Budapest, Hungary, 2-6 July 1984. [23] B. An, N. Gatti, and V. Lesser, \u201cBilateral bargaining with one-sided two-type uncertainty,\u201d in Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02. IEEE Computer Society, 2009, p. 403\u2013410. [24] G. Haim, Y. K. Gal, B. An, and S. Kraus, \u201cHuman\u2013computer negotiation in a three player market setting,\u201d Artificial Intelligence, vol. 246, pp. 34\u201352, 2017. [25] R. Lin, Y. Oshrat, and S. Kraus, Automated Agents that Proficiently Negotiate with People: Can We Keep People out of the Evaluation Loop. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 57\u201380. [26] A. Quinn, M. K\u00e1rn\u00fd, and T. V. Guy, \u201cOptimal design of priors constrained by external predictors,\u201d International Journal of Approximate Reasoning, vol. 84, pp. 150\u2013158, 2017. [27] J. Homolov\u00e1, E. Zugarov\u00e1, and T. V. Guy, \u201cOn decentralized implicit negotiation in modified ultimatum game,\u201d in Multi-Agent Systems and Agreement Technologies, vol. 15, 2017, pp. 357\u2013369. [28] D. Cooper and E. Dutcher, \u201cThe dynamics of responder behavior in ultimatum games: A meta-study,\u201d Experimental Economics, vol. 14, pp. 519\u2013546, 2010. [29] M. K\u00e1rn\u00fd and T. V. Guy, On Support of Imperfect Bayesian Participants. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 29\u201356.\nVOLUME 4, 2016 13\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGuy et al.: Indirect Dynamic Negotiation in the Nash Demand Game\n[30] M. S. Fagundes, S. Ossowski, M. Luck, and S. Miles, \u201cUsing normative markov decision processes for evaluating electronic contracts,\u201d AI Commun., vol. 25, no. 1, p. 1\u201317, 2012. [31] P. Faratin, C. Sierra, and N. Jennings, \u201cUsing similarity criteria to make negotiation trade-offs,\u201d in Proceedings Fourth International Conference on MultiAgent Systems, 2000, pp. 119\u2013126. [32] D. Zeng and K. Sycara, \u201cBayesian learning in negotiation,\u201d International Journal of Human-Computer Studies, vol. 48, no. 1, pp. 125\u2013141, 1998. [33] L. Wu, S. Chen, X. Gao, Y. Zheng, and J. Hao, \u201cDetecting and learning against unknown opponents for automated negotiations,\u201d in PRICAI 2021: Trends in Artificial Intelligence, D. Pham, T. Theeramunkong, G. Governatori, and F. Liu, Eds. Cham: Springer International Publishing, 2021, pp. 17\u201331. [34] T. Baarslag, M. J. C. Hendrikx, K. V. Hindriks, and C. M. Jonker, \u201cLearning about the opponent in automated bilateral negotiation: a comprehensive survey of opponent modeling techniques.\u201d Autonomous Agents and Multi-Agent Systems, vol. 30, p. 849\u2013898, 2016. [35] U. Kiruthika, T. S. Somasundaram, and S. K. S. Raja, \u201cLifecycle Model of a Negotiation Agent: A Survey of Automated Negotiation Techniques,\u201d Group Decision and Negotiation, vol. 29, no. 6, pp. 1239\u20131262, 2020. [36] T. Baarslag, M. Kaisers, E. H. Gerding, C. M. Jonker, and J. Gratch, Self-sufficient, Self-directed, and Interdependent Negotiation Systems: A Roadmap Toward Autonomous Negotiation Agents. Springer International Publishing, 2022, pp. 387\u2013406. [37] S. Fatima, M. Wooldridge, and N. Jennings, \u201cOptimal negotiation of multiple issues in incomplete information settings,\u201d in Proc. of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004, N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., vol. 3, 2004, pp. 1080\u20131087. [38] D. E. Kr\u00f6hling, O. J. Chiotti, and E. C. Mart\u00ednez, \u201cA context-aware approach to automated negotiation using reinforcement learning,\u201d Advanced Engineering Informatics, vol. 47, p. 101229, 2021. [39] R. Lin, Y. Gev, and S. Kraus, \u201cBridging the gap: Face-to-face negotiations with an automated mediator,\u201d IEEE Intelligent Systems, vol. 26, no. 6, pp. 40\u201347, 2011. [40] B. An, K. M. Sim, C. Y. Miao, and Z. Q. Shen, \u201cDecision making of negotiation agents using markov chains,\u201d Multiagent Grid Syst., vol. 4, no. 1, p. 5\u201323, 2008. [41] S. Fatima, M. Wooldridge, and N. Jennings, \u201cApproximate and online multi-issue negotiation,\u201d Proc. of th 6th Inernational Joint Conference on Autonomous Agents and Multi-agent Systems, pp. 947\u2013954, 2007. [42] F. Ren and M. Zhang, \u201cA single issue negotiation model for agents bargaining in dynamic electronic markets,\u201d Decision Support Systems, vol. 60, pp. 55\u201367, 2014. [43] A. Stern, S. Kraus, and D. Sarne, \u201cA negotiating strategy for a hybrid goal function in multilateral negotiation,\u201d CoRR, vol. abs/2201.04126, 2022. [44] Z. Feng, C. Tan, J. Zhang, and Q. Zeng, \u201cBargaining Game with Altruistic and Spiteful Preferences,\u201d Group Decision and Negotiation, vol. 30, no. 2, pp. 277\u2013300, 2021. [45] K. Hindriks and D. Tykhonov, \u201cOpponent modelling in automated multiissue negotiation using bayesian learning,\u201d in Proc. of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2008, p. 331\u2013338. [46] B. An, V. Lesser, and K. Sim, \u201cStrategic agents for multi-resource negotiation,\u201d Autonomous Agents and Multi-Agent Systems, pp. 1\u201340, 2010. [47] M. L. Puterman, Markov Decission Processes. John Wiley & Sons\u201e 1994. [48] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton Univer-\nsity Press, 1957. [49] J. Boger, J. Hoey, P. Poupart, C. Boutilier, G. Fernie, and A. Mihailidis,\n\u201cA planning system based on markov decision processes to guide people with dementia through activities of daily living,\u201d IEEE Transactions on Information Technology in Biomedicine, vol. 10, no. 2, pp. 323\u2013333, 2006. [50] M. Feng and Y. Li, \u201cPredictive maintenance decision making based on reinforcement learning in multistage production systems,\u201d IEEE Access, vol. 10, pp. 18 910\u201318 921, 2022. [51] H. Song, C.-C. Liu, J. Lawarree, and R. Dahlgren, \u201cOptimal electricity supply bidding by markov decision process,\u201d IEEE Transactions on Power Systems, vol. 15, no. 2, pp. 618\u2013624, 2000. [52] W. T. Scherer, S. Adams, and P. A. Beling, \u201cOn the practical art of state definitions for markov decision process construction,\u201d IEEE Access, vol. 6, pp. 21 115\u201321 128, 2018. [53] W. B. Powell, Approximate Dynamic Programming, 2nd ed. John Wiley and Sons, 2011.\n[54] V. Peterka, \u201cBayesian approach to system identification,\u201d in In Trends and Progress in System Identification, P. Eykhoff, Ed. Pergamon Press, 1981, pp. 239\u2013304. [55] M. K\u00e1rn\u00fd, J. B\u00f6hm, T. V. Guy, L. Jirsa, I. Nagy, P. Nedoma, and L. Tesar\u030c, Optimized Bayesian Dynamic Advising: Theory and Algorithms. Springer, 2006. [56] J. Henrich, R. Boyd, S. Bowles, C. Camerer, and E. Fehr, The Handbook of Experimental Economics. Princeton University Press, 2001. [57] I. Kapoutsis, R. Volkema, and A. Lampaki, \u201cMind the first step: The intrapersonal effects of affect on the decision to initiate negotiations under bargaining power asymmetry,\u201d Frontiers in Psychology, vol. 8, p. 1313, 2017. [58] T. Wu, X. Liu, J. Qin, and F. Herrera, \u201cTrust-consensus multiplex networks by combining trust social network analysis and consensus evolution methods in group decision-making,\u201d IEEE Transactions on Fuzzy Systems, pp. 1\u20131, 2022. [59] M. Fiori, A. Lintas, S. Mesrobian, and A. E. P. Villa, Effect of Emotion and Personality on Deviation from Purely Rational Decision-Making. Springer Berlin Heidelberg, 2013, pp. 129\u2013161.\nTATIANA V. GUY (SM\u201901) received the Dipl.Eng. degree in Control and Automation at Kiev Polytechnic Institute, the PhD degree in Cybernetics from Czech Technical University, Prague. She is currently with the Institute of Information Theory and Automation, Prague, since 2013 as Head of the Adaptive Systems Department. She has also an appointment as a researcher at the Czech University of Life Sciences. Her current research interests include distributed decision mak-\ning, multiagent systems, transfer learning.\nJITKA HOMOLOV\u00c1 received the BSc degree in financial mathematics from Charles University, Prague, Czech republic, the MSc degree and Ph.D degree from Czech Technical University, Prague, Czech republic both in Informatics. Since 2004 she has been with Institute of Information Theory and Automation, Prague, Czech Republic. Her main research interests include Bayesian identification, Markov decision processes, bargaining and multi-agent systems.\nALEKSEJ GAJ is currently pursuing the MSc degree in applied mathematics at Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University, Prague, Czech Republic. Since 2018 he has been involved in the research work at Adaptive Systems Department, Institute of Information Theory and Automation, Prague. His research interests include cooperation in multi-agent systems, use of quantum mechanics for decision making."
        },
        {
            "heading": "14 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        }
    ],
    "title": "Indirect Dynamic Negotiation in the Nash Demand Game",
    "year": 2022
}