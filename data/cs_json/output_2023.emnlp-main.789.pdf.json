{
    "abstractText": "Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression. For training the extractor, we propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions. Experimental results on the ZESHEL dataset demonstrate that the proposed method outperforms state-of-the-art models by a significant margin across all test domains, showing the effectiveness of keywordenhanced sparse retrieval. Code is available at https://github.com/HITsz-TMG/ Sparse-Retrieval-Fewshot-EL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yulin Chen"
        },
        {
            "affiliations": [],
            "name": "Zhenran Xu"
        },
        {
            "affiliations": [],
            "name": "Baotian Hu"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        }
    ],
    "id": "SP:831a022944870e48d8c76e7d00bc8b121fb52ea1",
    "references": [
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yue Dong",
                "John Wieting",
                "Pat Verga."
            ],
            "title": "Faithful to the document or to the world? mitigating hallucinations via entity-linked knowledge in abstractive summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Thibault Formal",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "Splade: Sparse lexical and expansion model for first stage ranking",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Heng Ji",
                "Joel Nothman."
            ],
            "title": "Overview of TACKBP2016 tri-lingual EDL and its impact on end-toend KBP",
            "venue": "Proceedings of the 2016 Text Analysis Conference, TAC 2016, Gaithersburg, Maryland, USA, November 14-15, 2016. NIST.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Phong Le",
                "Ivan Titov."
            ],
            "title": "Distant learning for entity linking with automatic noise detection",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4081\u2013 4090, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Haoran Li",
                "Yangqiu Song",
                "Lixin Fan."
            ],
            "title": "You don\u2019t know my favorite color: Preventing dialogue representations from revealing speakers\u2019 private personas",
            "venue": "arXiv preprint arXiv:2205.10228.",
            "year": 2022
        },
        {
            "authors": [
                "Xiuxing Li",
                "Zhenyu Li",
                "Zhengyan Zhang",
                "Ning Liu",
                "Haitao Yuan",
                "Wei Zhang",
                "Zhiyuan Liu",
                "Jianyong Wang."
            ],
            "title": "Effective few-shot named entity linking by meta-learning",
            "venue": "2022 IEEE 38th International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Lajanugen Logeswaran",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova",
                "Jacob Devlin",
                "Honglak Lee."
            ],
            "title": "Zero-shot entity linking by reading entity descriptions",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Xinyin Ma",
                "Yong Jiang",
                "Nguyen Bach",
                "Tao Wang",
                "Zhongqiang Huang",
                "Fei Huang",
                "Weiming Lu."
            ],
            "title": "MuVER: Improving first-stage entity retrieval with multi-view entity representations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        },
        {
            "authors": [
                "Kelong Mao",
                "Zhicheng Dou",
                "Hongjin Qian."
            ],
            "title": "Curriculum contrastive context denoising for fewshot conversational dense retrieval",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Ruiyang Ren",
                "Yingqi Qu",
                "Jing Liu",
                "Wayne Xin Zhao",
                "Qifei Wu",
                "Yuchen Ding",
                "Hua Wu",
                "Haifeng Wang",
                "Ji-Rong Wen."
            ],
            "title": "A thorough examination on zero-shot dense retrieval",
            "venue": "CoRR, abs/2204.12755.",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389. 12805",
            "year": 2009
        },
        {
            "authors": [
                "Kai Sun",
                "Richong Zhang",
                "Samuel Mensah",
                "Yongyi Mao",
                "Xudong Liu."
            ],
            "title": "A transformational biencoder with in-domain negative sampling for zero-shot entity linking",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1449\u20131458,",
            "year": 2022
        },
        {
            "authors": [
                "Weiqi Wang",
                "Tianqing Fang",
                "Baixuan Xu",
                "Chun Yi Louis Bo",
                "Yangqiu Song",
                "Lei Chen."
            ],
            "title": "Cat: A contextualized conceptualization and instantiation framework for commonsense reasoning",
            "venue": "arXiv preprint arXiv:2305.04808.",
            "year": 2023
        },
        {
            "authors": [
                "Ledell Wu",
                "Fabio Petroni",
                "Martin Josifoski",
                "Sebastian Riedel",
                "Luke Zettlemoyer."
            ],
            "title": "Scalable zeroshot entity linking with dense entity retrieval",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Mikel Artetxe",
                "Jingfei Du",
                "Danqi Chen",
                "Veselin Stoyanov."
            ],
            "title": "Prompting ELECTRA: Few-shot learning with discriminative pre-trained models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Peilin Yang",
                "Hui Fang",
                "Jimmy Lin."
            ],
            "title": "Anserini: Enabling the use of lucene for information retrieval research",
            "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201917, page",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12801\u201312806 December 6-10, 2023 \u00a92023 Association for Computational Linguistics\nEntity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression. For training the extractor, we propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions. Experimental results on the ZESHEL dataset demonstrate that the proposed method outperforms state-of-the-art models by a significant margin across all test domains, showing the effectiveness of keywordenhanced sparse retrieval. Code is available at https://github.com/HITsz-TMG/ Sparse-Retrieval-Fewshot-EL."
        },
        {
            "heading": "1 Introduction",
            "text": "Entity linking (EL) aligns entity mentions in documents with the corresponding entities in a knowledge base (KB), which is a crucial component of information extraction (Ji and Nothman, 2016). Typically, EL systems follow a \"retrieve and rerank\" pipeline (Logeswaran et al., 2019): Candidate Retrieval, where a small set of candidates are efficiently retrieved from a large number of entities, and Candidate Ranking, where candidates are ranked to find the most probable one. With the rapid development of pre-trained language models (PLM), EL has witnessed a radical paradigm shift towards dense retrieval (Ma et al., 2021; Sun et al.,\n*Both authors contributed equally to this work. \u2020Corresponding author.\n2022). Using bi-encoders and Approximate Nearest Neighbors (ANN) search has become the standard approach for initial entity retrieval, overcoming the long-standing vocabulary mismatch problem and showing promising performance gains.\nAlthough the bi-encoder has shown strong indomain performance, they typically require training on sufficient labeled data to perform well in a specific domain (Ren et al., 2022). However, such labeled data may be limited or expensive in new and specialized domains. As reported by Li et al. (2022b), in the absence of sufficient training data, the recall of bi-encoder significantly decreases, performing even worse than unsupervised sparse retrievers (e.g., BM25 (Robertson and Zaragoza, 2009)). Motivated by the above finding, in this work, instead of resorting to dense representations in the \u201csemantic space\u201d, we go back to the \u201clexical space\u201d and explore PLM-augmented sparse representations for few-shot EL.\nWhen applying BM25 to the first-stage retrieval, previous work only employs the mention string as query (Wu et al., 2020). However, this can be insufficient due to under-specified mentions (e.g., \u201chis embodiments\u201d in Figure 1), requiring additional context to formulate a more comprehensive query. While an intuitive solution could be incorporating all context into the query, this approach is suboptimal for two reasons: (i) it does not create mentionspecific queries, i.e., for all mentions in a document, their queries and retrieved entities are the same; (ii) it may introduce noise into queries (Mao et al., 2022), since not all words in the context are necessary for understanding the mention.\nThis paper introduces a keyword extractor to denoise the context for BM25 retrieval. The extraction is modeled as binary token classification, identifying which tokens in context should be added to the mention string. We employ a discriminative PLM, specifically ELECTRA (Clark et al., 2020), as the basis of our keyword extractor, and leverage\n12801\nits discriminator head to predict a score for each token. The top-k tokens with the highest scores are then added into the final query.\nTraining the extractor requires binary labels to distinguish whether a token is a keyword. The keywords are expected to be related to the mention and necessary for understanding it. We propose a distant supervision method to automatically generate training data, based on overlapping words between the mention context and the description of the corresponding entity. These overlapping words are then ranked based on their BM25 score, which measures the relevance of the word to the entity description. We select the top-k words with the highest scores as keywords.\nFollowing the few-shot experimental setting in Li et al. (2022b), We evaluate our approach on the ZESHEL dataset (Logeswaran et al., 2019). The results highlight our approach\u2019s superior performance across all test domains, with an average of 15.07% recall@64 improvement over the best dense retriever result. In addition to its superior performance, our method inherits the desirable properties of sparse representations such as efficiency of inverted indexes and interpretability.\nThe contributions of this work are threefold:\n\u2022 We are the first to explore PLM-augmented sparse retrieval in entity linking, especially in scenarios of insufficient data.\n\u2022 We propose a keyword extractor to denoise the mention context, and use the keywords to formulate the query for sparse retrieval, accompanied by a distant supervision method to generate training data.\n\u2022 We achieve the state-of-the-art performance across all test domains of ZESHEL, outperforming previous dense retrieval methods by a large margin."
        },
        {
            "heading": "2 Related work",
            "text": "Entity linking (EL) bridges the gap between knowledge and downstream tasks (Wang et al., 2023; Dong et al., 2022; Li et al., 2022a). Recent entity linking (EL) methods follow a two-stage \u201cretrieve and re-rank\u201d approach (Wu et al., 2020), and our work focuses on the first-stage retrieval. Traditional retrieval systems based on lexical matching struggle with vocabulary mismatch issues (Formal et al., 2021). With the advent of pre-trained language\nmodels, the bi-encoder has largely mitigated this problem, but its effectiveness depends on extensive annotation data (Ren et al., 2022). Without sufficient labeled data in specific domains, the recall of the bi-encoder drastically decreases (Li et al., 2022b), hence the need for further research about few-shot EL.\nFew-shot Entity Linking is recently proposed by Li et al. (2022b). They split the data in a specific domain of ZESHEL (Logeswaran et al., 2019) and provide few examples to train. They address the data scarcity problem with synthetic mention-entity pairs, and design a meta-learning mechanism to assign different weights to synthetic data. With this training strategy, the performance of bi-encoder is better than simply training with insufficient data. However, it still cannot surpass the performance of the unsupervised sparse retrieval method (i.e., BM25 in Table 2). As dense retrievers (e.g., the bi-encoder) and generative retrievers (e.g., GENRE (Cao et al., 2021)) are data-hungry, we go back to the \u201clexical space\u201d and resort to sparse retrievers for few-shot EL solutions."
        },
        {
            "heading": "3 Methodology",
            "text": "When applying BM25 to the first-stage retrieval of EL, previous work directly uses mention string as query. We propose a keyword extractor to denoise the context, and add the extracted keywords into the query. Then the expanded query is used for BM25 retrieval. In Section 3.1, we introduce our distant supervision method to generate keyword labels. In Section 3.2, we present the architecture, training and inference of our keyword extractor.\nPreprocessing. BM25 implementation generally comes with stopword removal (Yang et al., 2017). In this work, we remove the words which occur in more than 20% of entities\u2019 description documents, resulting in removal of uninformative words (such as \u201cwe\u201d, \u201cthe\u201d, etc.)."
        },
        {
            "heading": "3.1 Distant Supervision",
            "text": "Given a mention m, its context M and the description document E of its corresponding entity, the distant supervision method aims to gather keywords in the context M related to the description E. As shown in Figure 1(a), the overlapping words form a preliminary keyword set (denoted as K\u2217 = {w1, w2, . . . , w|K\u2217|}), i.e.,\nK\u2217 = words(M) \u2229 words(E) (1)\nwhere words(x) is the set of words in sequence x. The words in K\u2217 are further ranked according to the BM25 score between each word wi and the document E, calculated as follows:\ns(wi, E) = IDF(wi) \u00b7 f(wi, E) \u00b7 (k1 + 1)\nf(wi, E) + k1 \u00b7 (1\u2212 b+ b \u00b7 |E|avgdl) (2)\nwhere IDF(wi) represents the inverse document frequency of word wi, f(wi, E) denotes the frequency of word wi appearing in description E, |E| is the length of the description D, avgdl is the average document length in entity descriptions. The hyperparameters k1 and b are adjustable values, where we set k1 = 1.5 and b = 0.75. We select the top-k words with the highest scores as the final keyword set K for the mention m."
        },
        {
            "heading": "3.2 Keyword Extractor",
            "text": "Our keyword extractor aims to denoise the context through sequence labeling. Since discriminative pre-trained models can be easily adapted to tasks involving multi-token classification (Xia et al., 2022), we use ELECTRA as the backbone of our keyword extractor, as shown in Figure 1(b). For the mention m in its context M , the input \u03c4m is the word-pieces of the highlighted mention and its context:\n[CLS]Ml [START]m [END]Mr [SEP]\nwhere Ml and Mr are context before and after the mention m respectively. [START] and [END] are special tokens to tag the mention. The token embeddings from the last layer of the extractor T can be denoted as follows:\nH = T (\u03c4m) \u2208 RL\u00d7d (3)\nwhere L is the number of word-pieces in \u03c4m and d is the hidden dimension. Finally, the predicted score of the i-th token, denoted as s\u0302i, is calculated\nwith a discriminator head (i.e., a linear layer W ) and the sigmoid activation function:\ns\u0302i = Sigmoid(hiW ) (4)\nwhere hi \u2208 R1\u00d7d is i-th row of the matrix H . s\u0302i \u2208 [0.0, 1.0], and a higher score means the token is more likely to be a keyword.\nOptimization. In Section 3.1, for each mention m and its context M , we have obtained the keyword set K. The tokens of every word in K should be labeled as keywords. We optimize the extractor with a binary cross entropy loss, as below:\nL = \u2212 1 L\nL\u2211\ni=1\nsilog(s\u0302i) + (1\u2212 si)log(1\u2212 s\u0302i) (5)\nwhere si takes the value 1 if the i-th token should be labeled as a keyword, otherwise 0.\nInference. The keyword extractor predicts a score s\u0302i for every token. The top-k distinct tokens with the highest scores are selected for the keyword set K\u0302. The final query for BM25 combines the extracted keywords with the mention m:\nQ = words(m) \u222a K\u0302 (6) The top-n retrieved entities by BM25 are the result of our method."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Dataset and Evaluation Metric",
            "text": "For fair comparison with Li et al. (2022b), we follow their few-shot experimental settings, i.e.,\nchoose ZESHEL (Logeswaran et al., 2019) as the dataset, split the data as shown in Table 1 and use top-64 recall (i.e., Recall@64) as the evaluation metric."
        },
        {
            "heading": "4.2 Baselines",
            "text": "To evaluate our method, we compare it with the state-of-the-art sparse retriever (i.e., BM25 (Robertson and Zaragoza, 2009)), dense retrievers (i.e., BLINK (Wu et al., 2020), MUVER (Ma et al., 2021), MetaBLINK (Li et al., 2022b), DL4EL (Le and Titov, 2019)), and the generative retriever GENRE (Cao et al., 2021). More details of the baselines can be found in Appendix A. Implementation details are in Appendix B."
        },
        {
            "heading": "4.3 Results",
            "text": "Main results. Table 2 shows that our model outperforms all dense and generative retrieval methods across all domains, and achieves a 15.07% microaveraged Recall@64 improvement over the best reported result (i.e., MetaBLINK).\nComparison among sparse retrievers in Table 2 highlights the importance of mention-related context. BM25 (mention words) is apparently not the optimal solution since it ignores all context; BM25 (mention context) leverages all context and\nperforms even worse, indicating the importance of context denoising. The YuGiOh domain has the lowest recall of BM25 (mention words), showing a large vocabulary mismatch between mention string and entity documents. However, with mentionrelated context, our method addresses this issue effectively, resulting in an impressive 18.72% performance improvement in the YuGiOh domain.\nAblation study in Table 2, i.e. results of BM25 (mention words) and Ours (w/o mention words), demonstrates that the mention string and the keywords are both important to the final query. Deleting either of them in BM25\u2019s query will result in a significant performance drop. To further explore the contribution of keywords, we present the case study in Table 3. From these cases, we can find that the extracted keywords are semantically related to the mention, excluding the noise in context."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we focus on the first-stage retrieval in few-shot entity linking (EL). Given the significant performance drop in dense retrievers for few-shot scenarios, we revisit the sparse retrieval method and enhance its performance with a keyword extractor. We also propose a distant supervision ap-\nproach for automated training data generation. Our extensive experiments demonstrate our method\u2019s superior performance, with case studies showing denoised context and mention-related keywords. Future work may focus on more PLM-augmented sparse retrieval solutions for EL, expanding the query to words outside the context.\nLimitations\nDespite surpassing the previous state-of-the-art performance by a significant margin, our method exhibits certain limitations, primarily related to the constraint on the keywords. The formulated query effectively denoises the context, but is still limited to the scope of mention context. The vocabulary mismatch between the mention context and the entity description cannot be alleviated in our method. Expanding the query to words outside the context could be an interesting research direction."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Zifei Shan for discussions and the valuable feedback. This work is jointly supported by grants: Natural Science Foundation of China (No. 62006061, 82171475), Strategic Emerging Industry Development Special Funds of Shenzhen (No.JCYJ20200109113403826)."
        },
        {
            "heading": "A Baseline details",
            "text": "To evaluate the performance of our method, we compare with the following state-of-the-art retrievers that represent a diverse array of approaches.\n\u2022 BM25 (Robertson and Zaragoza, 2009), a traditional and strong sparse retrieval method. We consider two variants: BM25 (mention words), which simply uses the mention string as query, and BM25 (mention context), which uses all context as query.\n\u2022 BLINK (Wu et al., 2020) adopts the biencoder architecture for dense retrieval.\n\u2022 MUVER (Ma et al., 2021) extends the biencoder to multi-view entity representations and approximates the optimal view for mentions via a heuristic searching method.\n\u2022 GENRE (Cao et al., 2021) is an autoregressive retrieval method base on generating the title of candidates with constrained beam search.\n\u2022 MetaBLINK (Li et al., 2022b) proposes a weak supervision method to generate mentionentity pairs. These pairs are then used for training BLINK with a meta-learning mechanism.\n\u2022 DL4EL (Le and Titov, 2019) is a denoising method based on KL divergence to force the model to select high-quantity data. It can be combined with MetaBLINK\u2019s generated data to train BLINK.\nB Implementation Details\nOur model is implemented with PyTorch 1.10.0. The number of the model parameters is roughly 109M. All experiments are carried out on a single NVIDIA A100 GPU. We use Adam optimizer (Kingma and Ba, 2015) with weight decay set to 0.01, learning rate set to 2e-5. The batch size is 8, and the number of extracted keywords (denoted as k) is 32, For the convenience of comparison, the number of final retrieved entities (denoted as n) is 64. The mention context length is set to 128, with each left and right context having a length of 64. For each domain, we train for a total of 10 epochs, and choose the best checkpoint based on the performance of development set. The whole training process takes about 1.5 minutes for each domain.\nIn Table 2, the results of MUVER and GENRE are our reproduction based on their official Github repositories. These reproduction results, together with results of sparse retrievers, are from 5 runs of our implementation with different random seeds."
        }
    ],
    "title": "Revisiting Sparse Retrieval for Few-shot Entity Linking",
    "year": 2023
}