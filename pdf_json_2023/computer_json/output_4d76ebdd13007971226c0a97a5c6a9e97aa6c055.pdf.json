{
    "abstractText": "In many real-world scenarios, obtaining large amounts of labeled data can be a daunting task. Weakly supervised learning techniques have gained significant attention in recent years as an alternative to traditional supervised learning, as they enable training models using only a limited amount of labeled data. In this paper, the performance of a weakly supervised classifier to its fully supervised counterpart is compared on the task of defect detection. Experiments are conducted on a dataset of images containing defects, and evaluate the two classifiers based on their accuracy, precision, and recall. Our results show that the weakly supervised classifier achieves comparable performance to the supervised classifier, while requiring significantly less labeled data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vasileios Sevetlidis"
        },
        {
            "affiliations": [],
            "name": "George Pavlidis"
        },
        {
            "affiliations": [],
            "name": "Vasiliki Balaska"
        },
        {
            "affiliations": [],
            "name": "Athanasios Psomoulis"
        },
        {
            "affiliations": [],
            "name": "Spyridon Mouroutsos"
        },
        {
            "affiliations": [],
            "name": "Antonios Gasteratos"
        }
    ],
    "id": "SP:7be1b1e3f539fc1858d6948bdc1b28183b43cd7f",
    "references": [
        {
            "authors": [
                "C. Li",
                "J. Li",
                "Y. Li",
                "L. He",
                "X. Fu"
            ],
            "title": "and J",
            "venue": "Chen, \u201cFabric defect detection in textile manufacturing: a survey of the state of the art,\u201d Security and Communication Networks, vol. 2021, pp. 1\u201313",
            "year": 2021
        },
        {
            "authors": [
                "A. Castellani",
                "S. Schmitt"
            ],
            "title": "and S",
            "venue": "Squartini, \u201cReal-world anomaly detection by using digital twin systems and weakly supervised learning,\u201d IEEE Transactions on Industrial Informatics, vol. 17, no. 7, pp. 4733\u20134742",
            "year": 2020
        },
        {
            "authors": [
                "F. Psarommatis",
                "G. May",
                "P.-A. Dreyfus",
                "D. Kiritsis"
            ],
            "title": "Zero defect manufacturing: state-ofthe-art review",
            "venue": "shortcomings and future directions in research,\u201d International journal of production research, vol. 58, no. 1, pp. 1\u201317",
            "year": 2020
        },
        {
            "authors": [
                "F.K. Konstantinidis",
                "S. Sifnaios",
                "G. Tsimiklis",
                "S.G. Mouroutsos",
                "A. Amditis"
            ],
            "title": "and A",
            "venue": "Gasteratos, \u201cMulti-sensor cyber-physical sorting system (cpss) based on industry 4.0 principles: A multifunctional approach,\u201d Procedia Computer Science, vol. 217, pp. 227\u2013237",
            "year": 2023
        },
        {
            "authors": [
                "F.K. Konstantinidis",
                "N. Myrillas",
                "S.G. Mouroutsos",
                "D. Koulouriotis"
            ],
            "title": "and A",
            "venue": "Gasteratos, \u201cAssessment of industry 4.0 for modern manufacturing ecosystem: A systematic survey of surveys,\u201d Machines, vol. 10, no. 9, p. 746",
            "year": 2022
        },
        {
            "authors": [
                "V. Sevetlidis",
                "G. Pavlidis",
                "S. Mouroutsos"
            ],
            "title": "and A",
            "venue": "Gasteratos, \u201cTackling dataset bias with an automated collection of real-world samples,\u201d IEEE Access, vol. 10, pp. 126832\u2013126844",
            "year": 2022
        },
        {
            "authors": [
                "M. Zhang",
                "Y. Zhou",
                "J. Zhao",
                "Y. Man",
                "B. Liu"
            ],
            "title": "and R",
            "venue": "Yao, \u201cA survey of semi-and weakly supervised semantic segmentation of images,\u201d Artificial Intelligence Review, vol. 53, pp. 4259\u20134288",
            "year": 2020
        },
        {
            "authors": [
                "Z.-H. Zhou"
            ],
            "title": "A brief introduction to weakly supervised learning,",
            "venue": "National science review,",
            "year": 2018
        },
        {
            "authors": [
                "S. Guo",
                "W. Huang",
                "H. Zhang",
                "C. Zhuang",
                "D. Dong",
                "M.R. Scott"
            ],
            "title": "and D",
            "venue": "Huang, \u201cCurriculumnet: Weakly supervised learning from large-scale web images,\u201d in Proceedings of the European conference on computer vision (ECCV), pp. 135\u2013150",
            "year": 2018
        },
        {
            "authors": [
                "W. Shimoda",
                "K. Yanai"
            ],
            "title": "Distinct class-specific saliency maps for weakly supervised semantic segmentation,",
            "venue": "European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhu",
                "A.B. Goldberg"
            ],
            "title": "Introduction to Semi-Supervised Learning",
            "venue": "Springer Nature",
            "year": 2022
        },
        {
            "authors": [
                "F. Herrera",
                "S. Ventura",
                "R. Bello",
                "C. Cornelis",
                "A. Zafra",
                "D. S\u00e1nchez-Tarrag\u00f3",
                "S. Vluymans",
                "F. Herrera",
                "S. Ventura"
            ],
            "title": "R",
            "venue": "Bello, et al., Multiple instance learning. Springer",
            "year": 2016
        },
        {
            "authors": [
                "F. Ma",
                "D. Meng",
                "Q. Xie",
                "Z. Li"
            ],
            "title": "and X",
            "venue": "Dong, \u201cSelf-paced co-training,\u201d in International Conference on Machine Learning, pp. 2275\u20132284, PMLR",
            "year": 2017
        },
        {
            "authors": [
                "K. Jaskie",
                "A. Spanias"
            ],
            "title": "Positive unlabeled learning,",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "L. Xu",
                "S. Lv",
                "Y. Deng"
            ],
            "title": "and X",
            "venue": "Li, \u201cA weakly supervised surface defect detection based on convolutional neural network,\u201d IEEE Access, vol. 8, pp. 42285\u201342296",
            "year": 2020
        },
        {
            "authors": [
                "S. Niu",
                "H. Lin",
                "T. Niu",
                "B. Li"
            ],
            "title": "and X",
            "venue": "Wang, \u201cDefectgan: Weakly-supervised defect detection using generative adversarial network,\u201d in 2019 IEEE 15th international conference on automation science and engineering (CASE), pp. 127\u2013132, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "T.T.A. Pham",
                "D.K.T. Thoi",
                "H. Choi"
            ],
            "title": "and S",
            "venue": "Park, \u201cDefect detection in printed circuit boards using semi-supervised learning,\u201d Sensors, vol. 23, no. 6, p. 3246",
            "year": 2023
        },
        {
            "authors": [
                "Z. Zhang",
                "W. Liu"
            ],
            "title": "and X",
            "venue": "Sun, \u201cImage recognition of limited and imbalanced samples based on transfer learning methods for defects in welds,\u201d Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture, vol. 236, no. 12, pp. 1643\u20131652",
            "year": 2022
        },
        {
            "authors": [
                "F.T. Liu"
            ],
            "title": "K",
            "venue": "M. Ting, and Z.-H. Zhou, \u201cIsolation forest,\u201d in 2008 eighth ieee international conference on data mining, pp. 413\u2013422, IEEE",
            "year": 2008
        },
        {
            "authors": [
                "T. Schlagenhauf",
                "C.-P. Feuring",
                "J. Hillenbrand"
            ],
            "title": "and J",
            "venue": "Fleischer, \u201cCamera based ball screw spindle defect classification system: System zur kamerabasierten defekterkennung auf kugelgewindetriebspindeln,\u201d in Production at the leading edge of technology: Proceedings of the 9th Congress of the German Academic Association for Production Technology (WGP), September 30th-October 2nd, Hamburg 2019, pp. 503\u2013512, Springer",
            "year": 2019
        },
        {
            "authors": [
                "T. Schlagenhauf",
                "M. Landwehr"
            ],
            "title": "Industrial machine tool component surface defect dataset,",
            "venue": "Data in Brief,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "M. Cisse",
                "Y.N. Dauphin"
            ],
            "title": "and D",
            "venue": "Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d arXiv preprint arXiv:1710.09412",
            "year": 2017
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition,",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li"
            ],
            "title": "and L",
            "venue": "Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255, Ieee",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Defect detection is a critical task in many industries, from manufacturing [1] to healthcare[2]. Detecting defects early on can prevent quality issues and save costs by avoiding expensive rework or recalls [3]. In Industry 4.0, defect detection is even more critical, as automated inspection systems are increasingly being used to identify defects in real-time, ensuring that only products of the highest quality are delivered to customers [4]. However, obtaining large amounts of labeled data for training accurate defect detection models can be a significant challenge in many scenarios [5]. Manual labeling of data is time-consuming and expensive, while labeling errors can significantly affect the quality of the trained model [6].\nIn recent years, weakly supervised learning has emerged as a promising solution for training models using limited amounts of labeled data [7]. Unlike traditional supervised learning, which requires a large amount of annotated data, weakly supervised learning enables models to learn from only a fraction of the labeled or even from unlabeled data [8]. This approach has attracted significant attention due to its potential to reduce the reliance on manual annotation and improve the efficiency of the learning process. In particular, weakly supervised learning has shown promising results in defect detection tasks, where only a limited number of tagged samples are available.\nThis work explores the performance of a weakly supervised classifier compared to its fully supervised counterpart on the task of defect detection. The main objective is to evaluate the feasibility of weakly supervised learning in defect detection and compare it with the traditional supervised approach. Experiments are conducted on a real-world dataset of images containing defects and the two classifiers are evaluated based on their accuracy, precision, and recall. Additionally, the performance of the two classifiers under different amounts of labeled data is analyzed, to understand the effect of data availability on the performance of the models.\n\u2217vsevetli@athenarc.gr\nar X\niv :2\n30 3.\n15 09\n2v 1\n[ cs\n.L G\n] 2\n7 M\nar 2\n02 3\nValuable insights will be provided by the findings into the effectiveness of defect detection tasks using weakly supervised learning, and the strengths and limitations of this approach in real-world scenarios will be identified."
        },
        {
            "heading": "2 Related Work",
            "text": "Weakly supervised learning refers to a set of machine learning techniques that aim to train models using a limited amount of labeled data, or even without any labeled data at all [8]. In contrast to traditional supervised learning, which requires a large amount of labeled data for training, weakly supervised learning enables models to learn from weak supervision signals such as noisy labels, incomplete annotations, or partial information [9]. One common weakly supervised learning technique is weakly supervised classification, which aims to classify input data into predefined categories without relying on fully labeled samples. Instead, these classifiers are trained using only partially annotated data, such as images with image-level tags, rather than object-level bounding boxes or pixel-level annotations. The classifier can then localize the object of interest within the image and generate saliency maps [10]. Another popular technique is semi-supervised learning, which combines a small amount of labeled with a larger amount of unlabeled data to improve the model\u2019s performance. Semi-supervised learning techniques typically work by leveraging the inherent structure in the data to propagate the labels from the samples assigned with a label to the unlabeled ones [11]. In addition, there are other weakly supervised learning techniques, such as multi-instance learning [12], where each training instance consists of a bag of instances, some labeled as positive, negative, or unlabeled. Co-training [13], which trains two or more classifiers using different views of the data, aims to improve the overall performance of the models. Finally, positive-unlabeled learning is a machine learning framework that aims to learn a binary classification model from data from a single class, e.g., the positive samples are the only ones with a label [14].\nWeakly supervised learning has emerged as a promising approach for defect detection, which aims to identify the presence and location of defects in products or materials. Several studies have investigated the effectiveness of weakly supervised learning on this topic. For example, [15] proposed an approach to identify faults in steel plates. It uses image-level labels to train a classifier and generates saliency maps to localize the defects within the image. In their study, Niu et al. [16] investigated a surface segmentation method based on CycleGAN using a weakly supervised approach. The model was trained using image-level annotations and outperformed the supervised method on industrial datasets. [17] proposed a semi-supervised method to identify defective circuit boards (PCBs). It combined a small amount of labeled data with a large number of unlabeled ones to improve the model\u2019s performance. The approach achieved higher accuracy than its fully supervised counterpart. Another study, [18], proposed a multi-instance learning approach for defect detection in welding seam images, which achieved comparable performance to a fully supervised model using only a small amount of labeled data.\nOverall, these studies demonstrate the potential of weakly supervised learning for defect detection tasks and highlight the effectiveness of different these approaches which in general require a lot less annotated data than typical supervised learning methods."
        },
        {
            "heading": "3 Proposed Methodology",
            "text": "PU learning, also known as positive-unlabeled learning, is a subset of semi-supervised learning that deals with imbalanced classification problems where the negative samples are unknown or hard to obtain. The framework of PU learning is used in this work to simulate a weakly supervised learning setup. The framework is based on the assumption that the positive samples are accurately labeled, while the negative samples are partially labeled or unlabeled. Formally, assume there is a dataset of N samples, denoted as D = (x1, y1), (x2, y2), ..., (xN , yN ), where\nxi is the i th input feature vector and yi is the corresponding label. However, the labels are incomplete, meaning that some of the samples are only labeled as positive (y = 1), while others are either unlabeled or negative. The goal of PU learning is to build a binary classifier f(x) that can accurately predict the probability of a sample being positive, given the incomplete labels.\nThe proposed approach aims at using a small positive-labeled set with samples of a single class to assign labels to an unlabeled set which contains samples with arbitrary labels. The approach comprises a feature extractor, an anomaly scoring method, and a deep binary classifier, as shown in Figure 1. The feature extractor is a pre-trained deep architecture, and the anomaly scoring method is trained on the features extracted from the positive-labeled set. The resulting ranking of anomalous samples is used to create a counter-example class that is comparable to the positive-labeled set. Finally, a deep learning binary classifier is trained on the positive-labeled and counter-example sets using another deep learning model with a custom binary classifier attached.\nIn detail, the feature extractor is a deep learning architecture pre-trained on a large image dataset. The layers of the deep model are frozen, and no classification layers are attached to it, so its output can be used as feature vectors. In this work, a VGG-16 model is being used for this task. The images from the positive-labeled class are fed into the deep learning model, and the activation of its weights is taken as feature vectors. These feature vectors can then be used for a variety of downstream tasks, such as clustering or visualization.\nThe anomaly scoring method in the proposed approach is an Isolation Forest [19], an unsupervised machine-learning algorithm used for anomaly detection. It works by isolating anomalies in tree structures. Its decisions are based on the number of steps required to isolate them from other samples. In the framework of the proposed approach, this method is trained on the features of the positive-labeled samples and is used for scoring the feature vectors of the samples in the unlabeled set. The fewer the splits for given a sample, the less likely it is a normal instance. Thus a ranking from the most anomalous to the most normal sample is obtained.\nThe positive-labeled set only comprises a portion of the entire unlabeled set. If both sets were used without modification, the resulting dataset would be heavily imbalanced, which would make it unsuitable for training a supervised learning algorithm. However, a portion of the unlabeled set can be extracted that is equal in size to the positive-labeled set. By using the ranked anomaly scores, a counter-example class can be created, as the most anomalous samples are included in this set.\nFinally, a deep learning binary classifier is trained on the positive-labeled and the counterexample sets. Another VGG16 architecture is used with a custom binary classifier attached to it."
        },
        {
            "heading": "4 Experiments",
            "text": "This work involves comparing a supervised learning binary classifier that was trained using the entire dataset with different variants of the proposed weakly supervised approach. The variants test the performance of the approach using different initial populations in the positive-labeled class, ranging from 5% to 30% of the non-defective data depending on the experimental setup. All experiments used a 5-fold cross-validation and data split 80-20 between training and testing. It\u2019s worth noting that the supervised learning binary classifier and the binary classifier at the final step of the proposed approach share the same architecture to enable a direct comparison of their performance under the different learning frameworks."
        },
        {
            "heading": "4.1 Description of the dataset",
            "text": "A ball screw drive is a mechanical system that converts rotary motion into linear. It consists of a threaded shaft and a series of ball bearings that run along the threads. The ball nut contains a threaded hole that matches the thread on the ball screw and is held in place while the ball screw rotates. As the ball screw rotates, the ball bearings inside it move along the threads, pushing the nut along the shaft and creating linear motion. Ball screw drives are commonly used in applications requiring precise, repeatable linear motion, such as machine tools, robotics, and automation equipment [20].\nPitting is a type of surface damage that can occur in ball screws. It is a form of fatigue failure characterized by the formation of tiny, localized craters or pits on the surface of the ball screw. Pitting is typically caused by repeated cyclic loading of the ball screw, which can result in the accumulation of small cracks or defects in the material. Over time, these cracks can grow and merge, leading to the formation of pits on the surface of the ball screw. In this work, the goal is to distinguish Pitting from intact components or lubrication traces.\nSpecifically, the Ball Screw Defect for Classification (BSD) [21] dataset is publicly available1 and consists of 21,835 RGB images with a resolution of 150x150 pixels, depicting the surface of Ball Screw Drives. Among these images, 11,075 are without surface defects, while the remaining images show surface defects in the form of pittings. This distribution ensures\n1The dataset can be found here: https://publikationen.bibliothek.kit.edu/1000133819\nan equal representation of both classes in the dataset; a sample of BSD is shown in Figure 2. As mentioned before, it is essential to identify surface defects promptly to maintain machine availability. This dataset allows researchers and practitioners to train and test machine learning models to classify surface defects on machine tool elements accurately.\nThis dataset provides labels for all samples. In order to test the proposed approach in the weakly supervised setting, it needs to be adjusted accordingly. Thus, the data are split into a positive-labeled set comprising a small percentage of samples coming from one class, for example the non-defective class, and the remaining samples along with the unused class, e.g., the defective samples, become the unlabeled ones."
        },
        {
            "heading": "4.2 Data preprocessing",
            "text": "The images were rescaled from 150\u00d7 150 to 128\u00d7 128 and they were normalized so their values ranged in [0, 1]. The feature vectors were used as input at the anomaly detection method without further modifications. Dataset augmentation techniques were applied to the last step (binary classification) to combat inadequate training due to the small training set sizes. The augmentations were twofold: (a) random affine transformations, including rotations, vertical and horizontal flipping, shearing, cropping, and contast adjustment; and (b) sample mixing with MixUp [22] with default parameters."
        },
        {
            "heading": "4.3 Configuration",
            "text": "As aforementioned, the feature extractor is a VGG16 architecture [23] pre-trained on the ImageNet[24]. It was chosen because it is a prevalent deep-learning model which has been used numerous times for benchmarking since its inception. Moreover, it is easy to train and does not exhaust computational resources. The layers of the deep model are frozen, and no classification layers are attached to it. The model\u2019s output is flattened, so a feature vector is obtained R1\u00d78192.\nIsolation Forest was chosen because it is fast to train due to its random splits, adapts well in highly non-linear spaces, and it is easy to optimize its hyper-parameters (i.e., the number of estimators, maximum depth, contamination ratio, and maximum sample size) [19]. Hence, the number of estimators was set to 100; the depth was left at the default operation, the number of samples per estimator was 256, and the contamination fraction at C = 0.1.\nThe proposed approach2 also utilizes a VGG16 architecture as the binary classifier. It was the same architecture used by the authors who introduced the BSD dataset and set a baseline for supervised binary classification. The additional custom classification layers were two Dense, fully connected layers with ReLU activation, a Dropout layer of 0.2 rate between them, and a final fully connected layer with sigmoid activation. Using the same model and the proposed hyperparameters [20] allows for a direct comparison of the performance of our supervised learning instance, the weakly supervised one, and theirs. Note that all model parameters are trainable, and they were trained from scratch."
        },
        {
            "heading": "5 Results",
            "text": "The Table 1 provides a comparison of the performance metrics (accuracy, precision, recall, and F1-score) of a supervised learning approach versus the proposed approach with different amounts of positive-labeled data. The supervised learning approach achieves the highest accuracy, precision, recall, and F1-score, but requires a large amount of labeled data. On the other\n2The proposed method was built in Python using Keras as the deep learning backbone, and all experiments were executed on the same platform.\nTable 1: A comparison between supervised learning and weakly supervised performances for the BSD dataset. Also, the performance of the proposed method is evaluated for different initial populations.\nSupervised Learning Weakly Supervised Learning Positive-labeled 100% 100% 5% 10% 15% 20% 30% Accuracy (%) 91.5 [20] 96.68 (\u00b10.09) 80.35 (\u00b11.56) 88.35 (\u00b10.96) 90.39 (\u00b10.81) 93.29 (\u00b11.05) 93.42 (\u00b10.91) Precision (%) 93.68 [20] 96.09 (\u00b10.15) 76.34 (\u00b11.11) 87.34 (\u00b10.71) 93.22 (\u00b10.41) 93.12 (\u00b10.93) 93.02 (\u00b11.04) Recall (%) 89.00 [20] 97.03 (\u00b10.11) 81.65 (\u00b11.48) 88.29 (\u00b10.84) 88.51 (\u00b10.33) 90.86 (\u00b10.79) 91.23 (\u00b10.64)\nF1-score (%) 91.28 96.47 (\u00b10.18) 78.90 (\u00b11.33) 87.81 (\u00b10.80) 90.43 (\u00b10.74) 91.98 (\u00b10.99) 92.14 (\u00b10.83)\nhand, the weakly supervised learning approaches perform reasonably well even with a small amount of labeled data, but their performance is lower than the supervised learning approach.\nThe results in the table also show that increasing the percentage of positive-labeled data generally leads to better performance for the proposed approach. For instance, even with just 5% of positive-labeled data, the proposed approach achieves an accuracy score of around 80%. This suggests that the proposed weakly supervised learning method can be useful in scenarios where obtaining large amounts of labeled data is not feasible or expensive.\nIt is also worth noting that the performance of the weakly supervised learning approach generally improves as the percentage of positive-labeled data increases. For instance, the F1score increases from 78.90% to 92.14% as the percentage of positive-labeled data increases from 5% to 30%. The latter suggests that the proposed method can benefit from more labeled data, although they still may not match the performance of supervised learning methods.\nAdditionally, there is some variability in the performance of the weakly supervised learning approach across different percentages of positive-labeled data. For example, the precision scores increase as the percentage of positive-labeled data increases, but the recall scores show a small decrease in some cases. Overall, the table provides useful information about the trade-off between the amount of labeled data required for supervised learning versus the performance of weakly supervised learning approach with different amounts of positive-labeled data."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "Pitting can have a negative impact on the performance and reliability of ball screws. It can cause increased friction and wear, reduced accuracy, and increased noise levels. In severe cases, pitting can lead to complete failure of the ball screw [20]. The proposed approach of weakly supervised learning presents an effective solution for assigning labels to an unlabeled set with samples of arbitrary labels. By using a small positive-labeled set, a feature extractor, an anomaly scoring method, and a deep binary classifier, the approach is able to achieve impressive results. The feature extractor, a pre-trained deep architecture, is used to extract feature vectors, while the anomaly scoring method, Isolation Forest, is used to detect anomalies and rank samples. Finally, a deep learning binary classifier is trained on the positive-labeled and counter-example sets, resulting in accurate labeling of the unlabeled set.\nThis study highlights the potential of weakly supervised learning as a valuable alternative to traditional supervised learning methods, especially in scenarios where labeled data is scarce. Further investigations can be conducted on other image-based tasks and datasets to provide valuable insights into the effectiveness and generalizability of the approach. Moreover, incorporating domain knowledge or prior information into the weakly supervised learning framework can potentially enhance the performance of the method. Finally, exploring the potential of combining weakly supervised learning with other machine learning techniques, such as transfer learning or active learning, can lead to more effective and efficient approaches for solving real-world problems with limited labeled data."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research has been co-financed by the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH - CREATE - INNOVATE grant number T2EDK-01658."
        }
    ],
    "title": "Defect detection using weakly supervised learning",
    "year": 2023
}