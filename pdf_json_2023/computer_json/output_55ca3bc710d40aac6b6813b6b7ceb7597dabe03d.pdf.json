{
    "abstractText": "Effectively encoding layout information is a central problem in structured document understanding. Most existing methods rely heavily on millions of trainable parameters to learn the layout features of each word from Cartesian coordinates. However, two unresolved questions remain: (1) Is the Cartesian coordinate system the optimal choice for layout modeling? (2) Are massive learnable parameters truly necessary for layout representation? In this paper, we address these questions by proposing Layout Attention with Gaussian Biases (LAGaBi): Firstly, we find that polar coordinates provide a superior choice over Cartesian coordinates as they offer a measurement of both distance and angle between word pairs, capturing relative positions more effectively. Furthermore, by feeding the distances and angles into 2-D Gaussian kernels, we model intuitive inductive layout biases, i.e., the words closer within a document should receive more attention, which will act as the attention biases to revise the textual attention distribution. LAGaBi is modelagnostic and language-independent, which can be applied to a range of transformer-based models, such as the text pre-training models from the BERT series and the LayoutLM series that incorporate visual features. Experimental results on three widely used benchmarks demonstrate that, despite reducing the number of layout parameters from millions to 48, LAGaBi achieves competitive or even superior performance. Our code is available on GitHub1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xi Zhu"
        },
        {
            "affiliations": [],
            "name": "Xue Han"
        },
        {
            "affiliations": [],
            "name": "Shuyuan Peng"
        },
        {
            "affiliations": [],
            "name": "Shuo Lei"
        },
        {
            "affiliations": [],
            "name": "Chao Deng"
        },
        {
            "affiliations": [],
            "name": "Junlan Feng"
        },
        {
            "affiliations": [],
            "name": "JIUTIAN Team"
        }
    ],
    "id": "SP:71a77ef538521d2ea2eeeab03624505e148fbef2",
    "references": [
        {
            "authors": [
                "Srikar Appalaraju",
                "Bhavan Jasani",
                "Bhargava Urala Kota",
                "Yusheng Xie",
                "R Manmatha."
            ],
            "title": "Docformer: End-to-end transformer for document understanding",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, pages 993\u20131003.",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Ming Zhou."
            ],
            "title": "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "arXiv",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "\u00c9douard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Xue Han",
                "Yi-Tong Wang",
                "Jun-Lan Feng",
                "Chao Deng",
                "Zhan-Heng Chen",
                "Yu-An Huang",
                "Hui Su",
                "Lun Hu",
                "Peng-Wei Hu."
            ],
            "title": "A survey of transformerbased multimodal pre-trained modals",
            "venue": "Neurocomputing, 515:89\u2013106.",
            "year": 2023
        },
        {
            "authors": [
                "Teakgyu Hong",
                "Donghyun Kim",
                "Mingi Ji",
                "Wonseok Hwang",
                "Daehyun Nam",
                "Sungrae Park."
            ],
            "title": "Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents",
            "venue": "Proceedings of the AAAI Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document ai with unified text and image masking",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, pages 4083\u20134091.",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Jaume",
                "Hazim Kemal Ekenel",
                "JeanPhilippe Thiran."
            ],
            "title": "Funsd: A dataset for form understanding in noisy scanned documents",
            "venue": "2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), volume 2, pages",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Chen-Yu Lee",
                "Chun-Liang Li",
                "Timothy Dozat",
                "Vincent Perot",
                "Guolong Su",
                "Nan Hua",
                "Joshua Ainslie",
                "Renshen Wang",
                "Yasuhisa Fujii",
                "Tomas Pfister"
            ],
            "title": "Formnet: Structural encoding beyond sequential modeling in form document information",
            "year": 2022
        },
        {
            "authors": [
                "David Lewis",
                "Gady Agam",
                "Shlomo Argamon",
                "Ophir Frieder",
                "David Grossman",
                "Jefferson Heard."
            ],
            "title": "Building a test collection for complex document information processing",
            "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Re-",
            "year": 2006
        },
        {
            "authors": [
                "Chenliang Li",
                "Bin Bi",
                "Ming Yan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si."
            ],
            "title": "Structurallm: Structural pre-training for form understanding",
            "venue": "arXiv preprint arXiv:2105.11210. 7781",
            "year": 2021
        },
        {
            "authors": [
                "Peizhao Li",
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Hongfu Liu."
            ],
            "title": "Selfdoc: Self-supervised document representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2021
        },
        {
            "authors": [
                "Yulin Li",
                "Yuxi Qian",
                "Yuechen Yu",
                "Xiameng Qin",
                "Chengquan Zhang",
                "Yan Liu",
                "Kun Yao",
                "Junyu Han",
                "Jingtuo Liu",
                "Errui Ding."
            ],
            "title": "Structext: Structured text understanding with multi-modal transformers",
            "venue": "Proceedings of the 29th ACM International",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Chuwei Luo",
                "Changxu Cheng",
                "Qi Zheng",
                "Cong Yao."
            ],
            "title": "Geolayoutlm: Geometric pre-training for visual information extraction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7092\u20137101.",
            "year": 2023
        },
        {
            "authors": [
                "Seunghyun Park",
                "Seung Shin",
                "Bado Lee",
                "Junyeop Lee",
                "Jaeheung Surh",
                "Minjoon Seo",
                "Hwalsuk Lee."
            ],
            "title": "Cord: a consolidated receipt dataset for post-ocr parsing",
            "venue": "Workshop on Document Intelligence at NeurIPS 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Qiming Peng",
                "Yinxu Pan",
                "Wenjin Wang",
                "Bin Luo",
                "Zhenyu Zhang",
                "Zhengjie Huang",
                "Yuhui Cao",
                "Weichong Yin",
                "Yongfeng Chen",
                "Yin Zhang"
            ],
            "title": "Ernie-layout: Layout knowledge enhanced pretraining for visually-rich document understanding",
            "year": 2022
        },
        {
            "authors": [
                "Rafa\u0142 Powalski",
                "\u0141ukasz Borchmann",
                "Dawid Jurkiewicz",
                "Tomasz Dwojak",
                "Micha\u0142 Pietruszka",
                "Gabriela Pa\u0142ka."
            ],
            "title": "Going full-tilt boogie on document understanding with text-image-layout transformer",
            "venue": "Document Analysis and Recognition\u2013ICDAR 2021:",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "arXiv preprint arXiv:2108.12409.",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Jiapeng Wang",
                "Lianwen Jin",
                "Kai Ding."
            ],
            "title": "Lilt: A simple yet effective language-independent layout transformer for structured document understanding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Wenjin Wang",
                "Zhengjie Huang",
                "Bin Luo",
                "Qianglong Chen",
                "Qiming Peng",
                "Yinxu Pan",
                "Weichong Yin",
                "Shikun Feng",
                "Yu Sun",
                "Dianhai Yu"
            ],
            "title": "2022b. mmlayout: Multi-grained multimodal transformer for document understanding",
            "year": 2022
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Wanxiang Che"
            ],
            "title": "2021a. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
            "venue": "In Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Layoutlm: Pre-training of text and layout for document image understanding",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data",
            "year": 2020
        },
        {
            "authors": [
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Furu Wei."
            ],
            "title": "Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding",
            "venue": "arXiv preprint arXiv:2104.08836.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7773\u20137784 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Structured document understanding (SDU) has gained significant research attention in the field of intelligent document processing (Park et al., 2019; Jaume et al., 2019; Han et al., 2023). It focuses on extracting layout structures and contents from scanned or digital documents, leading to enhanced\n\u2217Junlan Feng is the corresponding author. 1 https://github.com/zxilucky/LAGaBi\nperformance in several downstream tasks like form comprehension and receipt understanding.\nUnlike conventional text understanding (Liu et al., 2019; Vaswani et al., 2017; Kenton and Toutanova, 2019), SDU goes beyond comprehending serialized text and requires the ability to interpret documents with diverse layouts (Xu et al., 2020; Huang et al., 2022; Powalski et al., 2021; Li et al., 2021a; Wang et al., 2022a). Documents with varying layouts often contain text fields positioned in different ways. To take advantage of existing pre-trained language models, early methods (Xu et al., 2020; Li et al., 2021a,c; Appalaraju et al., 2021; Chi et al., 2020) propose to directly add 2-D position embedding to the word embedding for each word as input for Transformer. The position embedding encode the 2-D absolute coordinates (x0, y0, x1, y1) of each word in the document through multi position encoding layers, where (x0, y0) represents the upper left point and (x1, y1) represents the lower right point of the bounding box for each word. Some researches (Powalski et al., 2021; Hong et al., 2022; Lee et al., 2022; Xu et al., 2021a; Huang et al., 2022) further proposed that absolute positions are inefficient for representing the spatial relationships between words. They employ relative positions between words to encode spatial relationships. For example, Hong et al.(2022) map the relative positions into embeddings, which are then multiplied with the semantic embedding of the word to calculate inter-word layout scores. This score is incorporated into the self-attention layers to combine semantic and layout features.\nDespite the significant progress made, we argue that current methods, whether based on absolute or relative positioning, heavily rely on a large number of trainable parameters for position embeddings from Cartesian coordinates, often comprising millions of parameters. This raises two unexplored questions: (1) Is the Cartesian coordinate system the optimal choice for layout modeling? (2) Are\n7773\nmassive learnable layout parameters truly necessary? Concerning the former, various coordinate systems exist, including Cartesian, polar, and spherical coordinates, yet previous research has solely focused on Cartesian coordinates. Regarding the latter, it is intuitively expected that words closer within a document should receive more attention. However, this simple inductive bias may not be effectively learned solely through gradient updates.\nIn this paper, we present a unified investigation of the two aforementioned problems. Regarding the choice of coordinate systems, we find that polar coordinates offer a more efficient representation than Cartesian coordinates for expressing relative positions. By computing the differences in distance and angle between two words in polar space, polar coordinates outperform their Cartesian counterparts by providing extra angle information. For layout learning, we discover that layout modeling can be achieved by a specific distribution: words closer in space receive higher layout scores, eliminating the need for extra position embeddings. Combining these two choices, we propose LAGaBi (Layout Attention with Gaussian Biases). LAGaBi formulates pairwise spatial relationships between tokens using the distance and angle in the polar coordinate system. Moreover, the distance and angle are fed into a 2-D Gaussian distribution to output a layout score. We choose the Gaussian distribution because it guarantees that the layout score decreases when either the distance or angle variables increase, making it the most commonly used distribution for this purpose. The layout score is then incorporated into the original self-attention as attention bias, resulting in a revised distribution that considers both text and layout features. We introduce trainable Gaussian kernels to better align the semantic and layout scores at different scales, adding just 4\u00d7 attention heads extra parameters. For instance, based on RoBERTa (Liu et al., 2019) with 12 attention heads, there are only 48 additional parameters that need to be learned for encoding layout features.\nExtensive experiments demonstrate that LAGaBi achieves remarkable performance on diverse SDU benchmarks, including FUNSD (Jaume et al., 2019), CORD (Park et al., 2019), and XFUND (Xu et al., 2021b), across both monolingual and multilingual scenarios. LAGaBi emerges as a versatile module that seamlessly integrates with transformerbased language models, such as BERT (Kenton and Toutanova, 2019), RoBERTa, and InfoXLM (Chi\net al., 2020), empowering them to effectively process structured documents and achieve significant performance gains of up to 27.01 points. Additionally, LAGaBi can be incorporated into complex SDU models that leverage visual features, such as LayoutLM (Xu et al., 2020), LayoutLMv2 (Xu et al., 2021a), and LayoutLMv3 (Huang et al., 2022), leading to further performance enhancements and establishing new state-of-the-art results."
        },
        {
            "heading": "2 Related Works",
            "text": "Significant progress has recently been made by using the Transformer-based pre-trained model (PTM) to learn the cross-modality interaction between textual and layout information, which has been demonstrated to be critical for structured document understanding (Xu et al., 2020; Huang et al., 2022; Powalski et al., 2021; Li et al., 2021a; Wang et al., 2022a). LayoutLM (Xu et al., 2020) modified the input of BERT (Kenton and Toutanova, 2019) by adding position embedding layers to encode word-level 2-D coordinates, while StructualLM (Li et al., 2021a) proposed to encode segment-level positions. LiLT (Wang et al., 2022a) encoded text and layout using two different transformer layers separately and adopted bi-directional attention to fuse them. Besides, there are a series of works that use multi-modal transformers to model text, layout, and image simultaneously, such as SelfDoc (Li et al., 2021b), DocFormer (Appalaraju et al., 2021), StrucTexT (Li et al., 2021c), ERNIE-Layout (Peng et al., 2022), mmLayout (Wang et al., 2022b).\nMost of the above approaches encode the 2-D absolute positions, ignoring the critical relative spatial relationships between words that are essential to textual semantic understanding. Hong et al.(2022) proposed to encode the spatial relationships as relative position embeddings, which are then multiplied with the semantic embedding of the token to calculate inter-word layout scores.This score is incorporated into the self-attention to combine semantic and layout features. GeoLayoutLM (Luo et al., 2023) introduces geometric relations and brand-new geometric pre-training tasks in different levels for learning the geometric layout representation, whose geometric relations are largely dependent on some pre-defined manual rules. TITL (Powalski et al., 2021) encodes the relative positions between words in a simpler manner, and it adopts linear layers to convert the 2-D discrete distance (implemented through bucketing)\nbetween words into attention biases motivated by T5 (Raffel et al., 2020). Such relative attention biases have also been employed by LayoutLMv2 (Xu et al., 2021a) and LayoutLMv3 (Huang et al., 2022), yielding notable results. Different from them, we model the relative positions from a new perspective, i.e. introducing polar coordinates with distance and angle that are more efficient in representing relative spatial relationships. Furthermore, we employ the extremely lightweight Gaussian kernels to encode polar coordinates into attention biases, offering a more streamlined approach with fewer trainable parameters and aligning better with human intuition compared to linear layers."
        },
        {
            "heading": "3 Methodology",
            "text": "Structured documents typically contain both textual words and layouts, where textual words denote the main content of the document, while layouts record the organizational form of text. Optical character recognition (OCR) as a classical technique for image document parsing can recognize the text as well as its locations. Formally, given a document D, OCR identifies the text words\nW = {wi}Ni=1 and their associated layout positions C = {ci}Ni=1, where N is the number of words while ci = [x0i , y 0 i , x 1 i , y 1 i ] represents the left-top and right-bottom coordinates of the bounding box that contains the ith word. An ideal model for document understanding should take both text words and their layout positions into consideration.\nThe key to structured document comprehension, building upon the achievements of text understanding techniques such as Transformer (Vaswani et al., 2017), lies in effectively representing document layouts while remaining compatible with language representation. Therefore, there are two crucial problems that need to be solved: 1. How to model document layouts efficiently? 2. How to integrate the layouts into Transformer to guide textual semantic understanding of the document content?\nIn this paper, we introduce a novel layout modeling method for structured document understanding, namely Layout Attention with Gaussian Biases (LAGaBi). As shown in Figure 1, it mainly consists of two parts: modeling relative positions with polar coordinates and layout attention with Gaussian biases. The former is responsible for modeling the 2-D relative positions between words by po-\nlar coordinates which are capable of representing inter-word spatial relationships; The latter focuses on transforming polar coordinates into attention biases, which will modify the original semantic query-key attentions between words into a more suitable distribution that accurately captures the underlying layout structure of documents."
        },
        {
            "heading": "3.1 Spatial Relationships with Polar Coordinates",
            "text": "In a structured document, the relative spatial relationships between words are shown to be more important than their absolute coordinates, which can assist humans in better understanding pair-wise semantic dependencies. For example, tokens in the same line generally have stronger semantic associations with each other, while tokens that are farther away and on different lines are more difficult to form strong associations. Although some previous works (Powalski et al., 2021; Xu et al., 2021a; Li et al., 2021a) have proposed to model the relative distances, which acted as learnable attention biases, they still rely on learnable positional embeddings and only utilize the relative horizontal and vertical distances in Cartesian coordinate system.\nDifferent from previous works, we propose to capture inter-word spatial relationships through polar coordinates, in which both orientations and distances can be preserved. For each query token, we can build a polar coordinate system centered at its position, and calculate the polar coordinates (spatial relationships) of its keys. More concretely, the position of the query token is regarded as the reference point (pole), and the horizontal direction in the current Cartesian coordinate system is set as the reference direction (polar axis) following the most common reading habit, i.e., left-to-right and top-to-down. Formally, given a query token with its 2-D coordinates ci as the pole, the polar coordinates uij = (\u03c1ij , \u03b8ij) of the jth key in this document page can be calculated as below:\n\u03c1ij = \u221a (xj \u2212 xi)2 + (yj \u2212 yi)2 (1)\n\u03b8ij = tan \u22121((yj \u2212 yi)/(xj \u2212 xi)) (2)\nwhere \u03c1ij \u2208 [0, 1] and \u03b8ij \u2208 [\u2212\u03c0/2, \u03c0/2] denote the distance and angle (orientation) from the ith token to the jth token respectively, and (xi, yi) are the normalized coordinates of the top-left point of the ith bounding box. For instance, in Figure 1, when taking \"REPORT\" as the reference point, the\nspatial relationship between the words \"FORM\" and \"REPORT\" can be represented as a polar coordinate (0.064, 0), indicating a distance of 0.064 and an angle of 0 degrees, while that between the words \"YEAR\" and \"REPORT\" is (0.297, 1.432)."
        },
        {
            "heading": "3.2 Layout Attention with Gaussian Biases",
            "text": "How to use the essential relative spatial relationships to guide the model to perceive layout information is a problem worth exploring. Inspired by ALiBi (Press et al., 2021) and T5 bias (Raffel et al., 2020) that encode the 1-D relative position information as attention biases upon the query-key scores instead of positional embedding, we propose to revise attention scores/distribution with 2-D attention biases that integrate spatial relationships. Specifically, the attention score in a single-head self-attention can be modified as:\naij = exp(qikTj / \u221a dk + \u03b1 (g(uij)\u2212 1))\u2211N\nj=1 exp(qik T j / \u221a dk + \u03b1 (g(uij)\u2212 1))\n(3) where qi is the ith query vetor, qi is the jth key vetor, and dk is the dimension of the attention head. g(uij) denotes the attention biases, which is derived from the 2-D Gaussian kernel with learnable parameters based on polar coordinates u indicating spatial relationships. The Gaussian kernel ensures that words farther within the document are assigned a smaller layout score. Therefore, by incorporating a reversed term (g(uij)\u2212 1), we can significantly penalize the attention scores of query-key pairs that are farther, while making only slight revisions to the scores of closer pairs. \u03b1 is a hyper-parameter that makes a trade-off between semantic association and spatial dependency. It denotes how much the spatial relationship between the key and the query contributes to their semantic association. In particular, a convenient formula of g(u) is:\ng(u) = exp(\u22121 2 (u \u2212 \u00b5)T\u03a3\u22121(u \u2212 \u00b5)) (4)\nwhere \u03a3 and \u00b5 are learnable 2 \u00d7 2 and 2 \u00d7 1 covariance matrix and mean vector of a Gaussian kernel, respectively. We further restrict the covariances to have diagonal form, resulting in 2 \u00d7 2 parameters per kernel for each attention head. Note that the Gaussian kernels are different across different attention heads, but are shared across different self-attention layers. Thus, there is a total of 2 \u00d7 2 \u00d7 Nheads learnable parameters for our attention biases, where Nheads denotes the attention head\nnumber in each self-attention layer. For example, taking the RoBERTa base as the backbone, there are 48 parameters that need to be learned. Notably, we only include layout information in the keys and queries but not in the values, ensuring that the text semantics are not corrupted. Properties of our LAGaBi: (1) It is easy to implement and can be adapted to any transformer-based model without changing its structure. (2) The position embedding layer is discarded, and there are very few parameters to learn, which can be done during the fine-tuning stage, so it is quite efficient. (3) It decouples layout and text understanding, allowing the potential of language models to be fully exploited in structured document understanding."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Pre-training Dataset. Following LayoutLM (Xu et al., 2020), we also pre-train our model using the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006), which is a large-scale dataset with over 11 million scanned document images. Only 1 million of them are used for fast pre-training. We pre-process each document page using Tesseract2, an open-source OCR engine, to retrieve the textual contents as well as their layouts. We normalize the coordinates of each token to integers in the range of 0 to 1000 and add an empty bounding box [0, 0, 0, 0] to the special tokens [CLS], [SEP], and [PAD]. Fine-tuning Datasets. We evaluate our method on both monolingual (English) and multilingual document information extraction datasets listed below. FUNSD (Jaume et al., 2019) is a form dataset that uses forms to extract and organize textual information. It contains 199 documents, 149 of which are for training and 50 of which are for testing. CORD (Park et al., 2019) is a dataset for receipt key information extraction that includes 800, 100, and 100 receipts for training, validating, and testing, respectively. XFUND (Xu et al., 2021b) is a multilingual version of FUNSD with 8 languages, each language containing 199 instances (149 for training and 50 for testing) as FUNSD."
        },
        {
            "heading": "4.2 Implemention Details",
            "text": "Our approach is model-agnostic and languageindependent, which can be applied to a range of transformer-based models. In this paper, we have evaluated our method based on three kinds\n2https://github.com/tesseract-ocr/tesseract\nof baselines: 1) monolingual models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), 2) multilingual model (InfoXLM (Chi et al., 2020)), and 3) document understanding models (LayoutLM (Xu et al., 2020), LayoutLMv2 (Xu et al., 2021a), and LayoutLMv3 (Huang et al., 2022)). Gaussian kernels will be included in the self-attention blocks of each model, which describe the relative spatial relationship between tokens. Pre-training. We only conduct pre-training tasks for the two monolingual models: BERT+LAGaBi and RoBERTa+LAGaBi. We initialize the weight of them with the corresponding baselines, except the Gaussian kernels. The parameters of the Gaussian kernels, namely covariance matrixes and mean vectors, are randomly initialized. Both models are simply supervised by masked language modeling (MLM) loss during pre-training. Adam optimizer (Kingma and Ba, 2014) is adopted with a learning rate of 5e \u2212 5, weight decay of 1e \u2212 2 and (\u03b21, \u03b22) = (0.9, 0.999). The batch size is set to 128 and all the two models are trained for 200,000 steps on 8 NVIDIA v100 32GB GPUs. Fine-tuning. In this paper, we mainly focus on the document understanding task of semantic entity labeling, which aims at assigning each semantic entity a BIO label. We add a token-level classification layer upon the base models (including monolingual, multilingual, and document understanding models) to predict the BIO labels for this task. Word-level F1 score is adopted as the evaluation metric. The fine-tuning process takes 2000 steps using a batch size of 16 and the Adam optimizer with a learning rate of 5e-5 for FUNSD and 7e-5 for CORD and XFUND. Fine-tuning configurations for document understanding models follow their official releases. Hyper-parameter \u03b1 is set to 4 for all experiments, which has been tuned on the CORD\u2019s val set."
        },
        {
            "heading": "4.3 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.3.1 Performance on monolingual datasets",
            "text": "We first evaluate our method on the monolingual form and receipt understanding datasets. From the results shown in Table 1, we can observe that: (1) The lightweight LAGaBi enables simple integration with language models, allowing them to effectively process structured documents. For example, without any pre-training on document data, RoBERTa+LAGaBi has achieved 84.84% and 95.97% F1 scores on FUNSD and CORD, surpassing baseline model RoBERTa by 18.38% and\n2.43% respectively. RoBERTa+LAGaBi also outperforms several representative document understanding models such as LayoutLM (Xu et al., 2020) and LayoutLMv2 (Xu et al., 2021a). The results show that LAGaBi is a powerful method for capturing essential layout features, allowing language models to be easily extended to adapt to structured document understanding tasks. (2) Pre-training brings profits. After pre-training on 1 million unlabeled document data, our method exhibits extra improvements. RoBERTa+LAGaBi with pre-train surpasses all other approaches except LayoutLMv3 (Huang et al., 2022). While there is still a minor difference on FUNSD between our model and LayoutLMv3, our method is significantly easier to implement, introducing only 48 extra learnable parameters to the vanilla Transformers structure, making it more computationally efficient and flexible. (3) The LAGaBi could also be seamlessly coupled with other layout embedding and multimodal-based document understanding models, improving their performance even further. Performance improvements on LayoutLM (Xu et al., 2020), LayoutLMv2 (Xu et al., 2021a) and LayoutLmv3 (Huang et al., 2022)) are obvious, with F1 score gains of 8.10%, 5.49%, 0.71% on FUNSD and 0.62%, 2.10%, 0.49% on CORD. The process\nis particularly efficient since it only requires finetuning based on the published pre-trained weights rather than pre-training from scratch. Furthermore, by combining LAGaBi with the top-performing LayoutLMv3 model, we achieve new state-of-theart results on both FUNSD and CORD datasets."
        },
        {
            "heading": "4.3.2 Performance on multilingual dataset",
            "text": "Following the multi-lingual LayoutXLM (Xu et al., 2021b) and LiLT (Wang et al., 2022a), we also evaluate our method based on InfoXLM (Chi et al., 2020) on three sub-tasks: language-specific finetuning, multi-task fine-tuning, and zero-shot transfer learning. We first perform fine-tuning based on the Gaussian kernels with random initialization. For a fair comparison, following LiLT, we also adopt the pre-trained Gaussian kernels for further fine-tuning, and we employ the Gaussian kernels in RoBERTa+LAGaBi which have been pre-trained on 1M monolingual document data in Sec 4.3.1. Results on XFUND are shown in Table 2.\nLAGaBi is also valid in multilingual scenarios, allowing multilingual language models to understand structured documents and outperform existing best-performing methods. LAGaBi can largely increase the performance of the multilingual language model InfoXLM on all three tasks, regardless of whether the Gaussian kernels are\npre-trained or not. InfoXLM+LAGaBi using the pre-trained Gaussian kernels outperforms the topperforming method LiLT on both language-specific and multi-task fine-tuning tasks, with average F1 scores of 84.06% and 87.22%, demonstrating the efficacy of LAGaBi in multilingual scenarios. On the zero-shot transfer learning task, LAGaBi fell slightly behind its counterparts. This may be due to the inherent gaps between different languages, such as differences in reading order and semantic density. For example, English usually uses spaces to separate words and has uneven word lengths, while Chinese appears as a tighter sequence with smaller semantic units (i.e., characters). Such layout knowledge learned from a specific language shows limited contributions to other languages. This phenomenon also proves the effectiveness of LAGaBi in modeling layouts, i.e. it actually has acquired the layout knowledge for a specific language after fine-tuning on the corresponding data."
        },
        {
            "heading": "4.4 Ablation Studies",
            "text": "To investigate the impacts of our learnable Gaussian kernels and polar coordinates, we have con-\nducted extensive ablation experiments based on several RoBERTa variants equipped with different layout encoding mechanisms (e.g. layout embedding layers, linear bias layers, and fixed/learnable Gaussian Kernels), and spatial relationships (e.g. distance, angle, and 2D-xy distance). All the variants are evaluated without further pre-training, and only fine-tuned for 2000 steps on FUNSD and CORD. Results on FUNSD\u2019s test set and CORD\u2019s validation set are shown in Table 3. From the result, we can observe that LAGaBi with learnable Gaussian kernels and polar coordinates (#8) can significantly outperform the baseline (#1) and the model with linear layout embedding layers (#2), indicating that models encoding layouts as attention biases are superior to layout embedding-based methods.\nImpact of the Gaussian kernels. To study the effects of various methods for converting polar coordinates to attention biases, we compared linear layers (#3), fixed Gaussian kernels (#4), and learnable Gaussian kernels (#8). The results demonstrate that linear layers are far less effective than Gaussian kernels, which is likely due to the fact that the Gaussian kernels are more in line with human\nintuition than linear layers. Learnable Gaussian kernels (#8) also achieve better performance than fixed Gaussian kernels whose mean is 0 and variance is 1 (#4), since the learnable Gaussian kernels enjoy better flexibility to adapt to different formats. Impact of the polar coordinates. Polar coordinates, which consist of two elements: distance and angle, is a typical technique for describing spatial relationships. We analyzed the effects of distance (#5) and angle (#6), as well as the classical 2-D relative horizontal and vertical distances proposed by TITL (Powalski et al., 2021) (#7). The results suggest that the model with angle information (#6) is more effective than the model with distance information (#5 and #7). We hypothesize that this is because angles are less impacted by size scaling than distances, but they are more sensitive to location changes. Furthermore, LAGaBi achieves much better performance than the model that employs horizontal and vertical distances, which further reveals the superiority of polar coordinates."
        },
        {
            "heading": "4.5 Analysis",
            "text": "Impact of \u03b1. Hyper-parameter \u03b1 makes a tradeoff between semantic and layout contributions when computing pair-wise attention scores in our LABaBi, which is important. We conduct several experiments with different \u03b1 settings to study the impact of \u03b1 based on RoBERTa without any further pre-training. F1 scores on CORD\u2019s validation set are listed in Table 4, showing the model achieves its best performance when \u03b1 = 4.\nVisualization analysis. We visualize the wordlevel attention maps of the baseline RoBERTa and our RoBERTa+LAGaBi. Due to space limitations, in this paper, we only show the attention maps of the first 8 words in the input sequence. The case in Figure 2 is from the test set of FUNSD. As shown in Figure 2, the RoBERTa incorrectly associates the character \"R.\" with all the words, while most other words are treated as unrelated. According to the attention map in the RoBERTa+LAGaBi, greater attention scores arise between \"R.\" and \"F.\", \"J.\" and \"D.\", all of which are placed in the signature area, whereas attention scores between remote irrelevant words such as \"DATE\" and \"STRU\" are zeros. This demonstrates that LAGaBi indeed learns more accurate semantic associations by incorporating layout information. More examples and detailed analysis can be seen in Appendix.A."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a model-agnostic and language-independent method that leverages Layout Attention with Gaussian Biases to encode the relative spatial positions for structured document understanding (SDU). Specifically, we first model the inter-word spatial relationships using polar coordinates. Then the query-key attention scores are revised by the Gaussian biases that are related to their spatial relationships. Our method can be applied to a series of Transformer-based models with extremely few parameters, improving their performance for SDU tasks. Experiments based on six transformer-based SDU models and three monolingual/multilingual benchmarks fully demonstrate the effectiveness of our proposal. This research provides new ideas for structured document understanding tasks, which are expected to promote the efficient development of document intelligence.\nLimitations\nDespite the superior performance exhibited by LAGaBi, it does have some limitations. Firstly, in our experiments with the LayoutLM series that integrate multi-modal features, LAGaBi was only fine-tuned for validation without pre-training. We believe that leveraging multi-modal pre-training could further improve LAGaBi\u2019s performance based on LayoutLM, and this will be explored in future investigations. Secondly, although we have empirically demonstrated the effectiveness of polar coordinates and Gaussian distribution in layout learning, our motivation is driven by a simple intuition rather than rigorous mathematical proof."
        },
        {
            "heading": "Acknowledgements",
            "text": "We sincerely thank all the anonymous reviewers for their valuable comments and suggestions."
        },
        {
            "heading": "A Appendix",
            "text": "Figure 3 in this section shows the attention distribution of four additional examples on FUNSD. From all the examples, it can be observed that the RoBERTa model typically treats each word in isolation, while RoBERTa+LAGaBi can learn the more accurate correlations between different words based on their layout relationships.\nWe also visualize the word-level attention maps of LayoutLMv3 and LayoutLMv3+LAGaBi. LayoutLMv3 is currently the top-performing method for structured document understanding, which utilizes the 2-D relative positions through linear attention biases. From the attention maps shown in Figure 4, we can observe that LayoutLMv3+LAGaBi refers more to layout information when modeling the inter-word semantic correlations, while LayoutLMv3 is relatively independent. For example, in the third sample of Figure 4, LAGaBi learns more dense associations among \u201cTiers\", \u201cII.\", and \u201c&\" in the neighborhood than LayoutLMv3, which is in line with human intuition."
        }
    ],
    "title": "Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding",
    "year": 2023
}