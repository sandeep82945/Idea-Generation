{
    "abstractText": "A camera begins to sense light the moment we press the shutter button. During the exposure interval, relative motion between the scene and the camera causes motion blur, a common undesirable visual artifact. This paper presents ECIR, which converts a blurry image into a sharp video represented as a parametric function from time to intensity. ECIR leverages events as an auxiliary input. We discuss how to exploit the temporal event structure to construct the parametric bases. We demonstrate how to train a deep learning model to predict the function coefficients. To improve the appearance consistency, we further introduce a refinement module to propagate visual features among consecutive frames. Compared to state-of-the-art event-enhanced deblurring approaches, E-CIR generates smoother and more realistic results. The implementation of E-CIR is available at https://github.com/chensong1995/E-CIR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Song"
        },
        {
            "affiliations": [],
            "name": "Qixing Huang"
        },
        {
            "affiliations": [],
            "name": "Chandrajit Bajaj"
        }
    ],
    "id": "SP:71c641195bfbf0bdb8440ddb556bd9c411a7e48b",
    "references": [
        {
            "authors": [
                "S Derin Babacan",
                "Rafael Molina",
                "Minh N Do",
                "Aggelos K Katsaggelos"
            ],
            "title": "Bayesian blind deconvolution with general sparse image priors",
            "venue": "In European conference on computer vision,",
            "year": 2012
        },
        {
            "authors": [
                "Christian Brandli",
                "Raphael Berner",
                "Minhao Yang",
                "Shih-Chii Liu",
                "Tobi Delbruck"
            ],
            "title": "A 240 \u00d7 180 130 db 3 \u03bcs latency global shutter spatiotemporal vision sensor",
            "venue": "IEEE Journal of Solid-State Circuits,",
            "year": 2014
        },
        {
            "authors": [
                "Jose Caballero",
                "Christian Ledig",
                "Andrew Aitken",
                "Alejandro Acosta",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ],
            "title": "Realtime video super-resolution with spatio-temporal networks and motion compensation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Pablo Rodrigo Gantier Cadena",
                "Yeqiang Qian",
                "Chunxiang Wang",
                "Ming Yang"
            ],
            "title": "Spade-e2vid: Spatially-adaptive denormalization for event-based video reconstruction",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Rob Fergus",
                "Barun Singh",
                "Aaron Hertzmann",
                "Sam T Roweis",
                "William T Freeman"
            ],
            "title": "Removing camera shake from a single photograph",
            "venue": "In ACM SIGGRAPH 2006 Papers,",
            "year": 2006
        },
        {
            "authors": [
                "DA Fish",
                "AM Brinicombe",
                "ER Pike",
                "JG Walker"
            ],
            "title": "Blind deconvolution by means of the richardson\u2013lucy algorithm",
            "venue": "JOSA A,",
            "year": 1995
        },
        {
            "authors": [
                "Jin Han",
                "Yixin Yang",
                "Chu Zhou",
                "Chao Xu",
                "Boxin Shi"
            ],
            "title": "Evintsr-net: Event guided multiple latent frames reconstruction and super-resolution",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhe Jiang",
                "Yu Zhang",
                "Dongqing Zou",
                "Jimmy Ren",
                "Jiancheng Lv",
                "Yebin Liu"
            ],
            "title": "Learning event-based motion deblurring",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Meiguang Jin",
                "Givi Meishvili",
                "Paolo Favaro"
            ],
            "title": "Learning to extract a video sequence from a single motion-blurred image",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Neel Joshi",
                "C Lawrence Zitnick",
                "Richard Szeliski",
                "David J Kriegman"
            ],
            "title": "Image deblurring and denoising using color priors",
            "venue": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Sang Ku Kim",
                "Sang Rae Park",
                "Joon Ki Paik"
            ],
            "title": "Simultaneous out-of-focus blur estimation and restoration for digital auto-focusing system",
            "venue": "IEEE Transactions on Consumer Electronics,",
            "year": 1998
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Dilip Krishnan",
                "Rob Fergus"
            ],
            "title": "Fast image deconvolution using hyper-laplacian priors",
            "venue": "Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Orest Kupyn",
                "Volodymyr Budzan",
                "Mykola Mykhailych",
                "Dmytro Mishkin",
                "Ji\u0159\u0131\u0301 Matas"
            ],
            "title": "Deblurgan: Blind motion deblurring using conditional adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Orest Kupyn",
                "Tetiana Martyniuk",
                "Junru Wu",
                "Zhangyang Wang"
            ],
            "title": "Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre Lacoste",
                "Alexandra Luccioni",
                "Victor Schmidt",
                "Thomas Dandres"
            ],
            "title": "Quantifying the carbon emissions of machine learning",
            "venue": "arXiv preprint arXiv:1910.09700,",
            "year": 1910
        },
        {
            "authors": [
                "Anat Levin",
                "Rob Fergus",
                "Fr\u00e9do Durand",
                "William T Freeman"
            ],
            "title": "Image and depth from a conventional camera with a coded aperture",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2007
        },
        {
            "authors": [
                "Sheng Li",
                "Fengxiang He",
                "Bo Du",
                "Lefei Zhang",
                "Yonghao Xu",
                "Dacheng Tao"
            ],
            "title": "Fast spatio-temporal residual network for video super-resolution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "P. Lichtsteiner",
                "C. Posch",
                "T. Delbruck"
            ],
            "title": "A 128 x 128 120db 30mw asynchronous vision sensor that responds to relative intensity change",
            "venue": "IEEE International Solid State Circuits Conference - Digest of Technical Papers,",
            "year": 2006
        },
        {
            "authors": [
                "Patrick Lichtsteiner",
                "Christoph Posch",
                "Tobi Delbruck"
            ],
            "title": "A 128\u00d7 128 120 db 15 \u03bcs latency asynchronous temporal contrast vision sensor",
            "venue": "IEEE Journal of Solid-State Circuits,",
            "year": 2008
        },
        {
            "authors": [
                "Songnan Lin",
                "Jiawei Zhang",
                "Jinshan Pan",
                "Zhe Jiang",
                "Dongqing Zou",
                "Yongtian Wang",
                "Jing Chen",
                "Jimmy Ren"
            ],
            "title": "Learning event-driven video deblurring and interpolation",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Niklaus",
                "Long Mai",
                "Feng Liu"
            ],
            "title": "Video frame interpolation via adaptive separable convolution",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Genady Paikin",
                "Yotam Ater",
                "Roy Shaul",
                "Evgeny Soloveichik"
            ],
            "title": "Efi-net: Video frame interpolation from fusion of events and frames",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2021
        },
        {
            "authors": [
                "Liyuan Pan",
                "Richard Hartley",
                "Cedric Scheerlinck",
                "Miaomiao Liu",
                "Xin Yu",
                "Yuchao Dai"
            ],
            "title": "High frame rate video reconstruction based on an event camera",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Liyuan Pan",
                "Cedric Scheerlinck",
                "Xin Yu",
                "Richard Hartley",
                "Miaomiao Liu",
                "Yuchao Dai"
            ],
            "title": "Bringing a blurry frame alive at high frame-rate with an event camera",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern 14 Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Sunghyun Park",
                "Kangyeol Kim",
                "Junsoo Lee",
                "Jaegul Choo",
                "Joonseok Lee",
                "Sookyung Kim",
                "Edward Choi"
            ],
            "title": "Vid-ode: Continuous-time video generation with neural ordinary differential equation",
            "venue": "arXiv preprint arXiv:2010.08188,",
            "year": 2020
        },
        {
            "authors": [
                "Daniele Perrone",
                "Paolo Favaro"
            ],
            "title": "Total variation blind deconvolution: The devil is in the details",
            "venue": "In 2014 IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Kuldeep Purohit",
                "Anshul Shah",
                "AN Rajagopalan"
            ],
            "title": "Bringing alive blurred moments",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Henri Rebecq",
                "Daniel Gehrig",
                "Davide Scaramuzza"
            ],
            "title": "ESIM: an open event camera simulator",
            "venue": "Conf. on Robotics Learning (CoRL),",
            "year": 2018
        },
        {
            "authors": [
                "Henri Rebecq",
                "Ren\u00e9 Ranftl",
                "Vladlen Koltun",
                "Davide Scaramuzza"
            ],
            "title": "Events-to-video: Bringing modern computer vision to event cameras",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Henri Rebecq",
                "Ren\u00e9 Ranftl",
                "Vladlen Koltun",
                "Davide Scaramuzza"
            ],
            "title": "High speed and high dynamic range video with an event camera",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (T-PAMI),",
            "year": 2019
        },
        {
            "authors": [
                "William Hadley Richardson"
            ],
            "title": "Bayesian-based iterative method of image restoration",
            "venue": "JoSA, 62(1):55\u201359,",
            "year": 1972
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computer-assisted intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Qi Shan",
                "Jiaya Jia",
                "Aseem Agarwala"
            ],
            "title": "High-quality motion deblurring from a single image",
            "venue": "Acm transactions on graphics (tog),",
            "year": 2008
        },
        {
            "authors": [
                "Wei Shang",
                "Dongwei Ren",
                "Dongqing Zou",
                "Jimmy S Ren",
                "Ping Luo",
                "Wangmeng Zuo"
            ],
            "title": "Bringing events into video deblurring with non-consecutively blurry frames",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yapeng Tian",
                "Yulun Zhang",
                "Yun Fu",
                "Chenliang Xu"
            ],
            "title": "Tdan: Temporally deformable alignment network for video super-resolution",
            "venue": "arXiv preprint arXiv:1812.02898,",
            "year": 2018
        },
        {
            "authors": [
                "Yapeng Tian",
                "Yulun Zhang",
                "Yun Fu",
                "Chenliang Xu"
            ],
            "title": "Tdan: Temporally-deformable alignment network for video super-resolution",
            "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Stepan Tulyakov",
                "Daniel Gehrig",
                "Stamatios Georgoulis",
                "Julius Erbach",
                "Mathias Gehrig",
                "Yuanyou Li",
                "Davide Scaramuzza"
            ],
            "title": "Time lens: Event-based video frame interpolation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Bishan Wang",
                "Jingwei He",
                "Lei Yu",
                "Gui-Song Xia",
                "Wen Yang"
            ],
            "title": "Event enhanced high-quality image recovery",
            "venue": "In European Conference on Computer Vision. Springer,",
            "year": 2020
        },
        {
            "authors": [
                "Fang Xu",
                "Lei Yu",
                "Bishan Wang",
                "Wen Yang",
                "Gui-Song Xia",
                "Xu Jia",
                "Zhendong Qiao",
                "Jianzhuang Liu"
            ],
            "title": "Motion deblurring with real events",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Li Xu",
                "Jiaya Jia"
            ],
            "title": "Two-phase kernel estimation for robust motion deblurring",
            "venue": "In European conference on computer vision,",
            "year": 2010
        },
        {
            "authors": [
                "Li Xu",
                "Shicheng Zheng",
                "Jiaya Jia"
            ],
            "title": "Unnatural l0 sparse representation for natural image deblurring",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Limeng Zhang",
                "Hongguang Zhang",
                "Chenyang Zhu",
                "Shasha Guo",
                "Jihua Chen",
                "Lei Wang"
            ],
            "title": "Fine-grained video deblurring with event camera",
            "venue": "In International Conference on Multimedia Modeling,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The shutter speed, or the length of the exposure interval, controls how much light reaches the image sensor from the environment. If the exposure interval is too short, the camera only has the time to capture very few photons. Consequently, the resulting image is not only unilluminated but also lacks fine details. On the other hand, if the exposure interval is too long, the relative motion between the scene and the camera may potentially be very significant. The resulting image is then the temporal average of a moving trajectory, causing blurry artifacts. Traditionally, it is presumed that any motion during the exposure interval, including both camera shake and subject movement, is unwanted and should therefore be removed. Over the past several decades, researchers have studied extensively how to convert a blurry image into a sharp one [1,5,6,10,11,13\u201315,17,28,33,35,42,43]. It is only until recently when several works that reconstruct the complete motion trajectory have received profound attention [9, 29]. These works introduce algorithms that convert a blurry image into a sharp video describing the exact movement that causes the blurry artifact.\nSharp video reconstruction is an ill-posed problem because there are infinitely many motion trajectories whose temporal averages correspond to the same blurry frame. To compensate for the ambiguity, previous works [7, 8, 21, 25, 26, 36, 40, 41, 44] exploit event data as an auxiliary input, which provides additional information during the exposure interval at a finer temporal resolution, as shown in Figure 1. Even with the event input, difficult challenges remain. The events fail to capture the complete motion information. The video reconstruction quality is determined not only by the appearance of each individual frame but also the temporal smoothness. The immense density of events creates another obstacle for effective and efficient processing. The success of video deblurring depends on how the blurry image, the events, and priors about video sequences are integrated together. This calls for suitable video representations and prediction algorithms.\nThis paper makes fundamental contributions in video representations and methodologies for recovering accurate and temporally consistent videos. Specifically, we propose a continuous video representation whose coefficients are highly interpretable and easy to learn, due to their strong correlation to the events. For every pixel (x, y), we rep-\nar X\niv :2\n20 3.\n01 93\n5v 1\n[ ee\nss .I\nV ]\n3 M\nar 2\n02 2\nresent its intensity as a parametric polynomial function Lxy(t), allowing us to render the sharp image at any given timestamp t during the exposure interval. We show how to choose the polynomial bases such that the derivative of Lxy(t) interpolates the significant intensity changes. We also demonstrate how to train a deep neural network that regresses the polynomial coefficients. Instead of processing the video as a volume and implicitly encoding motions in convolutional filters, our approach explicitly asks the model to elaborate the motions that have already been described by the events. To further polish the frame quality, we introduce a refinement module that propagates the visual features among consecutive frames, which can be trained in an end-to-end manner with the rest of the model. The proposed regress-and-refine paradigm nicely combines the strength of recurrent modules for enforcing temporal smoothness and the strength of regression for drifting avoidance.\nWe quantitatively evaluate our method on the synthetic REDS dataset [22]. In terms of reconstruction quality, ECIR achieves an MSE of 0.114, representing a 37.4% improvement from state-of-the-art algorithms. We also present a qualitative evaluation on the real captures provided by Pan et al. [26]. Compared with baseline approaches, our method is less noisy, more realistic, and temporally smoother.\nIn summary, our key contributions are:\n1. We represent a video by per-pixel parametric polynomials. We discuss why this representation integrates easily with the event mechanism by showing the parallelism between function derivatives and events.\n2. From a blurry image and its associated events in the exposure interval, we demonstrate how to use a deep learning model to predict a sharp video represented by the proposed parametric polynomials.\n3. To overcome the limitations of the polynomial representation, we discuss how to formulate a refinement objective and encourage the temporal propagation of sharp visual features.\n4. We provide source code and documentation for converting the original REDS dataset into the event format. This clears the vagueness of the evaluation dataset in previous works and establishes an opensource benchmark for future comparisons."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Event-Enhanced Deblurring",
            "text": "First commercialized in 2006 [19], event cameras are an emerging type of vision sensor that models the environment evolution as intensity changes and represents the scene as events. Each event is a 4-tuple (x, y, t, p) that contains the location, time, and polarity of an intensity change. This\nsimple representation allows event cameras to support a fast data rate (up to 1 MHz), orders of magnitude higher than the frame rate of conventional cameras. The density of events during the exposure interval provides valuable motion information to explain the blurred image.\nPan et al. propose the Event-based Double Integral (EDI) model [25, 26] that analytically reconstructs a high framerate sharp video from a blurry frame and its associated events. Jiang et al. [8] formulate a Maximum-a-Posteriori problem and solve for the latent sharp images under the Markov assumption with the help of deep neural networks. Lin et al. [21] believe it is inadequate to calculate the intensity residual between sharp and blurry frames directly from the event threshold and propose to predict the intensity residual using deep learning. Meanwhile, the structural similarity between the EDI model and the blur kernel formulation has inspired Wang et al. [40] to represent sharp images as sparse codes in a learnable dictionary and optimize them using an iterative network. Shang et al. [36] assume that the input sequence contains a mixture of blurry and sharp frames and propose to wrap sharp frames to deblur the blurry frame. Zhang et al. [44] emphasize the temporal correlation among consecutive frames and design a multipatch convolutional LSTM to exploit such correlation. Han et al. [7] extend this idea by modeling the intensity residual between neighboring sharp frames. Xu et al. [41] also identify the importance of temporal correlation and propose to utilize the optical flow estimation instead.\nClosely related to deblurring, event-enhanced frame interpolation has also attracted increasing attention [24, 39]. While both tasks aim at constructing a high frame-rate video, frame interpolation methods typically assume the input frames are free of motion blur. Several works have attempted to reconstruct a high frame-rate video directly from the events without the conventional frame input [4, 31, 32] as well. However, these event-only methods are less robust than their dual-input counterparts [21, 44]."
        },
        {
            "heading": "2.2. Video Representation",
            "text": "To the best of our knowledge, most existing works in computer vision process videos as discrete collections of frames. The only exception is Vid-ODE [27], which represent videos by continuous latent states. The latent state can be evaluated at any given timestamp, allowing the video to be rendered with an infinitely high frame rate.\nWith the help of per-pixel parametric polynomials, our proposed representation also supports infinitely high rate rendering and enjoys two additional advantages. First, the polynomial bases are chosen to closely mimic the event mechanism, which makes the algorithm robust to domain differences between synthetic training data and real testing data. Second, the polynomial coefficients are more interpretable than the latent code hidden inside a deep network.\nThis allows humans to easily explain and debug the model."
        },
        {
            "heading": "3. Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1. Event Camera Model",
            "text": "Let Lxy(t) be the latent intensity of pixel (x, y) at time t. In the natural logarithmic space, the temporal contrast between tref and t is given by [19]:\n\u2206ln[Lxy(t)] = ln[Lxy(t)]\u2212 ln[Lxy(tref)] (1)\nwhere tref denotes the timestamp of the last event associated with pixel (x, y). The magnitude of \u2206ln[Lxy(t)] determines whether the hardware produces an event. Let (x, y, t, p) denote an event, where p \u2208 {\u22121,+1} is the polarity of the intensity change [19]:\np =  +1 \u2206ln[Lxy(t)] \u2265 c+ 0 (no event) c\u2212 < \u2206ln[Lxy(t)] < c+\n\u22121 \u2206ln[Lxy(t)] \u2264 c\u2212 (2)\nHere, c+ and c\u2212 are thresholds controlling the sensitivity of positive and negative events, respectively. It is commonly assumed that c+ and c\u2212 are stochastic variables [20, 30].\nDuring an exposure interval [\u2212T2 , T 2 ] with length T , let Bh\u00d7w = {Bxy} be the blurry output from the conventional camera. The relation between the blurry frame and the latent frames is given by temporal averaging [26, 30]:\nBxy = 1\nT\n\u222b T 2\n\u2212T2 Lxy(t)dt (3)"
        },
        {
            "heading": "3.2. Task Description",
            "text": "The input to the task has two components:\n1. The blurry intensity Bxy for all pixels (x, y);\n2. A collection of events during the exposure interval {ei = (xi, yi, ti, pi)| \u2212 T2 \u2264 ti \u2264 T 2 }.\nGiven an arbitrary timestamp t \u2208 [\u2212T2 , T 2 ], the goal of the task is to construct the corresponding latent frame Lh\u00d7w(t) = {Lxy(t)}."
        },
        {
            "heading": "3.3. Challenges",
            "text": "Intensity reconstruction is a highly ill-posed problem because there are infinitely many motion trajectories whose temporal averages correspond to the same blurry frame. Specifically, we face three challenges:\nFirst, the events fails to capture complete motion information during the exposure interval. Equation (2) states that when an event happens, the magnitude of \u2206ln[Lxy(t)] is greater than the event threshold. It remains unclear exactly how much \u2206ln[Lxy(t)] exceeds the threshold. Imagine a\nscene with two edges moving in the same pattern. The first edge has a strong contrast to the background and generates significant intensity changes for its movement. The second edge has a weak contrast to the background and generates small intensity changes. Suppose these two sets of intensity changes both exceed the event threshold. This means the number of events generated by the camera is determined only by the edge length. These two edges will yield the same number of events, as long as their lengths are equal, even though the absolute change in intensity of the first edge is several times higher than that of the second edge.\nSecond, the reconstruction quality is determined not only by the appearance of each individual frame but also the temporal smoothness. A naive model that independently optimizes each latent frame\u2019s quality may lead to unrealistic motion trajectories and cause frequent jitter. We refer readers to the supplementary animations for a demonstration of how some of our baseline approaches fail to solve this issue.\nThird, the event format is incompatible with popular deep learning models. One possible remedy is to aggregate the events into a histogram.This method ignores the event camera model and trains a network as if the inputs are merely some unexplained features. It remains unclear how to properly instill human knowledge about the bio-inspired event mechanism, such as the correlation between an event and an intensity change, into the model design.\nWhile previous works fail to address some or all of these challenges, the next section discusses how the proposed ECIR handles them effectively."
        },
        {
            "heading": "4. Method",
            "text": ""
        },
        {
            "heading": "4.1. Parametric Intensity Function",
            "text": "For each pixel (x, y), we propose to approximate the function Lxy(t) as a continuously differentiable mapping from the time domain to the intensity domain: [\u2212T2 , T 2 ] \u2192 [0, 1]. Inspired by the Taylor\u2019s theorem, we parameterize the mapping as a degree-n polynomial. Let \u03b10, \u03b11, \u03b12, \u00b7 \u00b7 \u00b7 , \u03b1n be the n+ 1 polynomial coefficients. The simplest parameterization uses the standard polynomial bases:\nLxy(t) = \u03b10 + \u03b11t+ \u03b12t 2 + \u00b7 \u00b7 \u00b7+ \u03b1ntn (4)\nMost real-life videos do not have frequent oscillations. This means a realistic intensity curve does not have very high-order derivative information. For a small change in t, the variation in Lxy(t) should not be too big. The fact that |\u03b10| \u2248 |\u03b11| > |\u03b12| > \u00b7 \u00b7 \u00b7 |\u03b1n| suggests that the standard representation is prone to numerical issues since high-order coefficients are expected to be very close to zero.\nThe temporal derivative of Lxy(t) reveals how the intensity changes across time. The events associated with pixel (x, y) provide a set of timestamps where the intensity change considerably. The derivatives at these timestamps\nare expected to have significant magnitudes. The key idea of our proposed parameterization is to interpolate the temporal derivative of the intensity signal at event timestamps. The number of events associated with each pixel is different, presenting a challenge to efficient computation. To address this issue, we extract a fixed number of n keypoints for each pixel, regardless of how many events the pixel initially possesses. The details of our keypoint extraction algorithm are presented in Figure 2(a). This algorithm ensures the selected keypoints are in correspondence to the event timestamps and as distant to each other as possible. The use of the uniform pivots further establishes spatial consistency in the keypoint choices among different pixels. Let the set of n keypoints for pixel (x, y) be:\nKxy = {(ti, dLxy dt (ti))|1 \u2264 i \u2264 n} (5)\nwhere \u2212T2 \u2264 t1 < t2 < \u00b7 \u00b7 \u00b7 < tn \u2264 T 2 . As shown in Figure 2(b), we parameterize the intensity derivative dLxydt (t) as the polynomial interpolation of these n keypoints:\ndLxy dt (t) = n\u2211 i=1 dLxy dt (ti) \u00b7 \u03b2xyi(t) (6)\nHere, \u03b2xyi(t), 1 \u2264 i \u2264 n are the Lagrange bases of degree n\u2212 1:\n\u03b2xyi(t) = (t\u2212 t1) \u00b7 \u00b7 \u00b7 (t\u2212 ti\u22121)(t\u2212 ti+1) \u00b7 \u00b7 \u00b7 (t\u2212 tn)\n(ti \u2212 t1) \u00b7 \u00b7 \u00b7 (ti \u2212 ti\u22121)(ti \u2212 ti+1) \u00b7 \u00b7 \u00b7 (ti \u2212 tn)\nThe Lagrange bases have the characteristic that \u03b2xyi(ti) = 1 and \u2200j 6= i, \u03b2xyj(ti) = 0. This ensures the\ncontinuous function dLxydt (t) passes through the n discrete keypoints (ti, dLxy dt (ti)), where 1 \u2264 i \u2264 n.\nWe can then recover the primitive intensity signal Lxy(t) from its derivative dLxydt (t) by taking the indefinite integral:\nLxy(t) = \u222b dLxy dt (t)dt+ axy (7)\nwhere axy is a constant that can be solved from Equation (3).\nCompared to the conventional frame-based representation, the main advantage of the proposed polynomial representation is that it closely mimics the event mechanism. We leverage event timestamps to construct the polynomial bases and allow the polynomial coefficients to be interpreted as the intensity changes that trigger the input events. The regression target of our model, the polynomial coefficients, is therefore highly correlated to the input events. While the input events characterize the locations of the edge features, the output polynomial coefficients reveal exactly how significant the edges are. Section 5.4 presents an empirical verification of the advantage of our representation."
        },
        {
            "heading": "4.2. Prediction Pipeline",
            "text": "We illustrate the overall prediction pipeline in Figure 3, which consists of the initialization stage and the refinement stage. The initialization stage regresses the polynomial coefficients, evaluates predicted parametric functions, and obtains coarse video reconstruction. The refinement stage further polishes the details in the initial reconstruction by learning and enforcing motion priors. This methodology nicely combines the strength of motion priors for recovering temporally smooth videos and the strength of regressing the volumetric output to avoid drifting. Initialization: Polynomial Coefficient Regression. We assemble the n keypoint timestamps associated with each pixel into an n\u00d7h\u00d7w tensor K, where h\u00d7w is the spatial frame resolution. Following eSL-Net [40], we voxelize the event stream by creating an m\u00d7h\u00d7w histogram tensor E, wherem = 40 is the number of temporal bins. We adopt the U-Net [34] architecture as the backbone prediction network. As shown in Figure 3, the U-Net takes the blurry frame, the keypoints, and the events as input and regresses the polynomial coefficients for Lxy(t) represented under the Lagrange bases of its derivative. Let t = {ti} be the d-dimensional vector collecting all timestamps of interest. With the help of Equation (7), the predicted coefficients allow us to reconstruct d initial frames {L\u0302(ti)}. At training time, d = 14 is set to the number of available ground-truth latent frames. At inference time, d can be an arbitrary positive integer. Refinement: Temporal Feature Propagation. When played as a video, the initialization results show an accurate motion reconstruction. Different parts of the scene move according to their respectively trajectories. Nonetheless, we\nAlgorithm 1 Refinement Input: Initial reconstruction: {L\u0302(ti)|1 \u2264 i \u2264 d} Input: All events: {(x, y, t, p)| \u2212 T2 \u2264 t \u2264 T 2 } Output: Final reconstruction: {L(ti)|1 \u2264 i \u2264 d} 1: loop Imax iterations 2: for i = 1 to d\u2212 1 do . Residual Prediction 3: E\u2190 VOXELIZE(events from ti to ti+1) 4: if i == 1 then 5: Ri \u2190 gR\u03b81(E, L\u0302(ti), L\u0302(ti+1), 6: \u2207L\u0302(ti),\u2207L\u0302(ti+1)) 7: else 8: Ri \u2190 gR\u03b82(Ri\u22121,E, L\u0302(ti), L\u0302(ti+1), 9: \u2207L\u0302(ti),\u2207L\u0302(ti+1)) 10: end if 11: end for 12: for i = 1 to d do . Apply Updates 13: Ai \u2190 gA\u03c6 (L\u0302(ti)) 14: Di \u2190 \u2202f(R1,\u00b7\u00b7\u00b7 ,Rd\u22121,L\u0302(t1),\u00b7\u00b7\u00b7 ,L\u0302(td),L(t1),\u00b7\u00b7\u00b7 ,L(td))\u2202L(ti) 15: L\u0302(ti)\u2190 L\u0302(ti)\u2212Ai Di 16: end for 17: end loop 18: for i = 1 to d do . Final Polishing 19: L(ti)\u2190 gL\u03b3 (L\u0302(ti)) 20: end for\nobserve that the initial reconstruction occasionally fails to recover temporally consistent features. This initial temporal inconsistency is expected and addressed by the refinement.\nThere are two factors that contribute to the temporal inconsistency. First, the polynomial function is a continuous signal that smooths sharp features and makes them visually blurry. Second, visual features may only move during a small part of the exposure interval. When the feature is actively moving, the reconstruction is usually sharp because there are input events in the spatial neighborhood depicting the edge locations. When the feature is not moving, how-\never, the reconstruction becomes blurry due to the lack of associated events and difficulty for volumetric filters used in regression to capture motion priors that are critical for deblurring.\nIn the refinement stage, we solve the first issue by optimizing the frames independent of polynomial formulation. We solve the second issue by encouraging visual features to propagate between consecutive frames (i.e., enforcing motion priors), a popular technique used extensively in video synthesis via optical flows [3, 41], residuals [7, 18], or deformable convolutional kernels [37, 38]. Details of our refinement process are presented as Algorithm 1. Specifically, we use a recurrent network to predict the residual Ri between adjacent frames L(ti) and L(ti+1). The inputs to the network include the previous residual Ri\u22121 between frames L(ti\u22121) and L(ti), the events from ti to ti+1, the initial reconstructions L\u0302(ti) and L\u0302(ti+1), as well as their spatial gradients\u2207L\u0302(ti) and\u2207L\u0302(ti+1). Algorithm 1 refers this recurrent network as gR\u03b81 (for the residual prediction between the first two frames) and gR\u03b82 (for the rest of residuals).\nThe recurrent architecture allows the residual to be gradually updated according to the relevant events and intensity reconstruction. We augment the inputs to include\u2207L\u0302(ti) = (dL\u0302(ti)dx , dL\u0302(ti) dy ) and \u2207L\u0302(ti+1) = ( dL\u0302(ti+1) dx , dL\u0302(ti+1) dy ). This is because both the spatial gradients and the temporal residuals are highly related to the edge features.\nConsider the objective function in Equation (8), where L(\u00b7, \u00b7) represents the distance between two matrices.\nf = d\u22121\u2211 i=1 L(L(ti) + Ri,L(ti+1)) + \u03bb d\u2211 i=1 L(L(ti), L\u0302(ti))\n(8) The free variables are the refined frames L(ti)\u2019s. The first objective term ensures the refinement output follows the residual flow. The second term discourages the refinement from deviating too far from the initialization. The trade-off parameter \u03bb balances these two terms. We expect\nboth terms to have small residuals throughout the optimization process and the final result to be in local proximity to the initial frames. This leads us to choose the L2-distance as L(\u00b7, \u00b7) for its numerical stability. As shown in Algorithm 1, we apply gradient descent to update the refinement result for Imax iterations. Different pixels may have different step sizes for the update, which are predicted by a convolutional network referred to as gA\u03c6 . After the gradient descent terminates, we use another convolutional network gL\u03b3 to perform final polishing on each individual frame."
        },
        {
            "heading": "4.3. Training Objective",
            "text": "Derivative Loss. We use Ld to supervise the output polynomial coefficients directly in the derivative domain:\nLd = |( dL\ndt )gt \u2212 (\ndL dt )pred|1 (9)\nPrimitive Loss. We first recover the primitive intensity signal from Equation (7) and then use Lp to supervise the polynomial coefficients indirectly in the primitive domain:\nLp = |Lgt \u2212 Lpred|1 (10)\nRefinement Loss. We use Lref to supervise the refinement output:\nLref = \u2211 t |Lgt(t)\u2212 Lpred(t)|1 (11)\nResidual Loss. We use Lres to supervise the residual prediction in the refinement stage. Note that Lres uses weighted L1-norm with \u03c1 = 5 because the intensity residual between consecutive frames is sparse.\nLres = \u2211 i (exp(\u03c1 \u00b7 |Rigt |1) |Rigt \u2212Ripred |1) (12)\nTotal Objective. The final training objective is the weighted sum of the losses introduced above:\nLtotal = \u03bbdLd + \u03bbpLp + \u03bbrefLref + \u03bbresLres (13)\nwhere \u03bbd, \u03bbp, \u03bbref, \u03bbres are constant trade-off factors. The U-Net and the prediction networks in the refinement stage (gR\u03b81 , g R \u03b82 , gA\u03c6 , g L \u03b3 ) are trained in an end-to-end manner."
        },
        {
            "heading": "5. Evaluation",
            "text": ""
        },
        {
            "heading": "5.1. Datasets",
            "text": "REDS [22] is a standard deblurring benchmark dataset designed for conventional cameras. The dataset contains 240 training videos and 30 validation videos and is publicly available under the CC BY 4.0 license. These videos are captured at 120 fps and are sharp and clear. We use the frame interpolation algorithm [23] to further increase the frame rate to 960 fps. After that, we convert the videos\nto grayscale and resize the frames to 240\u00d7180, consistent with the DAVIS2401 [2] sensor resolution. We apply the ESIM [30] simulator to generate events and synthesize blurry frames according to Equation (3) with an exposure interval of 120 milliseconds. We use least-squares to fit polynomial coefficients to the resized 960 fps sharp frames during each exposure interval. These coefficients are used to supervise network training. This process is an effort to reproduce the synthetic dataset described by Wang et al. [40], who have released the model weights after training but have not disclosed data processing or training scripts."
        },
        {
            "heading": "5.2. Baseline Approaches",
            "text": "For quantitative evaluation, we compare our model with eSL-Net [40] using both the official weights released by Wang et al. and the weights re-trained on our synthetic data. We take EDI [26] as an additional baseline model. At the submission time, they are the only two approaches with open-source implementation available online.\nFor qualitative evaluation, we also visualize the results on real event captures provided by Pan et al. [26]. The blurriness of real captures comes from the physical sensor instead of simulated temporal averaging. Therefore, there is a lack of \u201cground-truth\u201d sharp images, and we are unable to numerically evaluate the performance of each algorithm."
        },
        {
            "heading": "5.3. Training Details",
            "text": "The trade-off parameters for the derivative (Ld), primitive (Lp), refinement (Lref), and residual (Lres) losses are 1, 10, 10, and 0.5, respectively. We use the Adam [12] optimizer with a batch size of 96 and train the network for 50 epochs. The initial learning rate is 0.0001 and is halved at the end of the 20th and the 40th epochs. The degree of the polynomial functions we use is n = 10."
        },
        {
            "heading": "5.4. Analysis of Results",
            "text": "Baseline Comparison. We present a quantitative evaluation of our approach in Table 1. Compared to baseline models, our method obtains lower MSE, higher PSNR, and higher SSIM. Specifically, the proposed E-CIR improves the current state-of-the-art algorithm by 37.4% in MSE, 17.8% in PSNR, and 23.3% in SSIM.\nQualitatively, we compare E-CIR with baseline models in Figure 4 and Figure 5. Visually, our results are not only\n1DAVIS240 is a popular camera that records grayscale conventional frames and events simultaneously.\nsharper but also less noisy than the EDI [25] reconstruction. The blurriness of EDI can be explained by their use of total variation as an image quality prior, which penalizes spatial edge features. By contrast, our method implicitly learns a deep prior from the sharp images in the training dataset. In terms of the noises, EDI assumes the event threshold can precisely characterize all intensity changes. As discussed in Section 3.3, this assumption is inaccurate and susceptible to artifacts. Compared to eSL-Net [40], our model does not over exaggerate visual features. As shown in the second row of Figure 4, the eSL-Net output contains a distorted patch in the bottom-right corner, an overemphasis of the floor tile. The excessive amount of noise makes eSL-Net unfavorable in the quantitative evaluation, even though subjectively, it reconstructs sharper frames than EDI. We also invite readers to watch our supplementary animations. These animations demonstrate that our method generates temporally smooth reconstruction, while the baseline approaches suffer from trajectory discontinuity to varying degrees. Ablation Study. The input to our model has two components: the blurry frame and the events. Prior works have approached the video reconstruction problem using both the blurry frame alone [9, 29] and the events alone [4, 31, 32]. We begin our ablation study by examining the benefit of using a dual-stream input. As shown the first three rows in Table 2, removing either input from the pipeline leads to noticeable performance degradation. For example, the MSE of the dual-input model is 6.7% lower than the event-only model and 30.6% lower than the frame-only model. This suggests that conventional frames and events are complementary to each other, and our proposed E-CIR is able to take collaborative advantage of the combined information.\nTo examine if the polynomial video representation is indeed superior to the traditional frame-based representation, we use the same U-Net architecture to regress the d = 14 ground-truth sharp frames directly. The comparison between the third and fourth rows in Table 2 shows that our proposed polynomial representation outperforms the framebased baseline by 8.1% in MSE. This result demonstrates power of explicit derivative modeling in event data.\nFinally, we examine the effectiveness of refinement. Between the third and the fifth row in Table 2, we observe that the refinement module improves the initialization by\n8.8% in MSE. The supplementary material includes an animated comparison between the initialization and refinement frames. While the initialization results successfully recovers the motion trajectory, the visual features are occasionally not sharp enough. The refinement algorithm is able to sharpen the initial results by encouraging high-quality visual features to propagate among consecutive frames."
        },
        {
            "heading": "6. Limitations",
            "text": "We point out that standard image quality metrics have a negative bias towards approaches that generate sharp but noisy results, such as eSL-Net [40]. Ideally, we would like to separate the visual signal and the noise and measure them independently. Practically, we have to resort to distancebased metrics without the disentanglement. Readers are encouraged to compare our approach with the baselines visually while using the quantitative evaluation as a reference."
        },
        {
            "heading": "7. Conclusion",
            "text": "This paper introduces E-CIR, a novel event-enhanced deblurring approach that represents the intensity signal as a continuous parametric function. Experiments show that ECIR outperforms current state-of-the-art models in reconstruction quality. In the future, we plan to extend the approach and integrate spatial continuity into the formulation. Another possible direction is to explore probabilistic inference since each blurry image corresponds to more than one possible realistic motion trajectory."
        },
        {
            "heading": "8. Acknowledgement",
            "text": "We would like to sincerely appreciate Jiajia Yu (RPI), Jiacheng Chen (SFU), Zhenpei Yang (UT Austin), Bo Sun (UT Austin), Siming Yan (UT Austin), Haitao Yang (UT Austin), Jiaru Song (UT Austin), Yan He (Rice), and Ao Li (Yext) for their constructive criticism and valuable suggestions. Additionally, Qixing Huang acknowledges the support from NSF Career IIS-2047677 and NSF HDR TRIPODS1934932. Chandrajit Bajaj acknowledges the support from the NIH (DK129979), in part from the Peter O\u2019Donnell Foundation, and in part from a grant from the Army Research Office accomplished under Cooperative Agreement Number W911NF-19-2-0333."
        },
        {
            "heading": "9. Supplementary Material",
            "text": "The source code of E-CIR, scripts and step-by-step instructions to converting the REDS [22] dataset to the event format are available at https://github.com/chensong1995/ECIR. Additional animations can be viewed at http://songc.me/ecir/visualization.html. The static version of these additional results are presented in the remaining pages of this .pdf file."
        },
        {
            "heading": "10. Potential Negative Societal Impact",
            "text": "In our experiments, we train E-CIR using three Tesla V100-SXM2-32GB GPUs for approximately 100 hours. The total emission is estimated to be 38.88 kgCO2eq, equivalent to 157.2 km driven by an average car. This estimation is conducted using the Machine Learning Impact calculator presented in [16]. To mitigate repetitive labor and negative environmental impact in future research, we have released our open-source implementation together with trained network weights."
        }
    ],
    "title": "E-CIR: Event-Enhanced Continuous Intensity Recovery",
    "year": 2022
}