{
    "abstractText": "While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLMs)-based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency. Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhenlan Ji"
        },
        {
            "affiliations": [],
            "name": "Pingchuan Ma"
        },
        {
            "affiliations": [],
            "name": "Zongjie Li"
        },
        {
            "affiliations": [],
            "name": "Shuai Wang"
        }
    ],
    "id": "SP:caace4b205a250101950cb215bd91a2bd56b7b1c",
    "references": [
        {
            "authors": [
                "A. Aggarwal",
                "J. Sun",
                "N. Peng"
            ],
            "title": "Towards robust nlg bias evaluation with syntactically-diverse prompts",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "J. Austin",
                "A. Odena",
                "M. Nye",
                "M. Bosma",
                "H. Michalewski",
                "D. Dohan",
                "E. Jiang",
                "C. Cai",
                "M. Terry",
                "Q Le"
            ],
            "title": "Program synthesis with large language models",
            "year": 2021
        },
        {
            "authors": [
                "T. Baluta",
                "S. Shen",
                "S. Hitarth",
                "S. Tople",
                "P. Saxena"
            ],
            "title": "Membership inference attacks and generalization: A causal perspective",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "M. Barrett",
                "J. Bingel",
                "F. Keller",
                "A. S\u00f8gaard"
            ],
            "title": "Weakly supervised part-of-speech tagging using eyetracking data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2016
        },
        {
            "authors": [
                "S. Bills",
                "N. Cammarata",
                "D. Mossing",
                "H. Tillman",
                "L. Gao",
                "G. Goh",
                "I. Sutskever",
                "J. Leike",
                "J. Wu",
                "W. Saunders"
            ],
            "title": "Language models can explain neurons in language models. https://openaipublic.blob",
            "venue": "core.windows.net/neuron-explainer/paper/index.html",
            "year": 2023
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell",
                "S. Agarwal",
                "A. Herbert-Voss",
                "G. Krueger",
                "T. Henighan",
                "R. Child",
                "A. Ramesh",
                "D. Ziegler",
                "J. Wu",
                "C. Winter",
                "C. Hesse",
                "M. Chen",
                "E. Sigler",
                "M. Litwin",
                "S. Gray",
                "B. Chess",
                "J. Clark",
                "C. Berner",
                "S. McCandlish",
                "A. Radford",
                "I. Sutskever",
                "D. Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "M. Chen",
                "J. Tworek",
                "H. Jun",
                "Q. Yuan",
                "Pinto",
                "H.P. d. O",
                "J. Kaplan",
                "H. Edwards",
                "Y. Burda",
                "N. Joseph",
                "G Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "V. Chernozhukov",
                "D. Chetverikov",
                "M. Demirer",
                "E. Duflo",
                "C. Hansen",
                "W. Newey",
                "J. Robins"
            ],
            "title": "Double/debiased machine learning for treatment and causal parameters",
            "year": 2016
        },
        {
            "authors": [
                "D.M. Chickering"
            ],
            "title": "Learning equivalence classes of bayesian-network structures",
            "venue": "Journal of Machine Learning Research",
            "year": 2002
        },
        {
            "authors": [
                "K. Collins-Thompson"
            ],
            "title": "Computational assessment of text readability: A survey of current and future research",
            "venue": "ITL-International Journal of Applied Linguistics 165,",
            "year": 2014
        },
        {
            "authors": [
                "J. Cussens",
                "M. J\u00e4rvisalo",
                "J.H. Korhonen",
                "M. Bartlett"
            ],
            "title": "Bayesian network structure learning with integer programming: Polytopes, facets and complexity",
            "venue": "JAIR",
            "year": 2017
        },
        {
            "authors": [
                "Y. Du",
                "S. Li",
                "A. Torralba",
                "J.B. Tenenbaum",
                "I. andMordatch"
            ],
            "title": "Improving factuality and reasoning in language models through multiagent debate",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "C. Dubslaff",
                "K. Weis",
                "C. Baier",
                "S. Apel"
            ],
            "title": "Causality in configurable software systems",
            "venue": "In Proceedings of the 44th International Conference on Software Engineering",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "X. Gao",
                "M. Mirchev",
                "A. Roychoudhury",
                "S.H. Tan"
            ],
            "title": "Automated repair of programs from large language models",
            "venue": "IEEE/ACM 45th International Conference on Software Engineering (ICSE) (2023),",
            "year": 2023
        },
        {
            "authors": [
                "A. Fariha",
                "S. Nath",
                "A. andMeliou"
            ],
            "title": "Causality-guided adaptive interventional debugging",
            "venue": "In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data",
            "year": 2020
        },
        {
            "authors": [
                "L. Feng",
                "N. Elhadad",
                "M. Huenerfauth"
            ],
            "title": "Cognitively motivated features for readability assessment",
            "venue": "In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL",
            "year": 2009
        },
        {
            "authors": [
                "C.A. Furia",
                "R. Torkar",
                "R. Feldt"
            ],
            "title": "Towards causal analysis of empirical software engineering data: The impact of programming languages on coding competitions",
            "year": 2023
        },
        {
            "authors": [
                "C. Green"
            ],
            "title": "Application of theorem proving to problem solving",
            "venue": "In Readings in Artificial Intelligence. Elsevier,",
            "year": 1981
        },
        {
            "authors": [
                "S. Gulwani",
                "O. Polozov",
                "R Singh"
            ],
            "title": "Program synthesis. Foundations and Trends\u00ae in Programming Languages",
            "year": 2017
        },
        {
            "authors": [
                "D. Heckerman",
                "D. Geiger",
                "D.M. Chickering"
            ],
            "title": "Learning bayesian networks: The combination of knowledge and statistical data. Machine Learning",
            "year": 1995
        },
        {
            "authors": [
                "D. Hendrycks",
                "S. Basart",
                "S. Kadavath",
                "M. Mazeika",
                "A. Arora",
                "E. Guo",
                "C. Burns",
                "S. Puranik",
                "H. He",
                "D. Song",
                "J. Steinhardt"
            ],
            "title": "Measuring coding challenge competence with apps",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Hsiao",
                "C.-H",
                "S. Narayanasamy",
                "E.M.I. Khan",
                "C.L. Pereira",
                "G.A. Pokam"
            ],
            "title": "Asyncclock: Scalable inference of asynchronous event causality",
            "venue": "ACM SIGPLAN Notices 52,",
            "year": 2017
        },
        {
            "authors": [
                "H. Husain",
                "Wu",
                "H.-H",
                "T. Gazit",
                "M. Allamanis",
                "M. Brockschmidt"
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "year": 1909
        },
        {
            "authors": [
                "Y. Ishibashi",
                "D. Bollegala",
                "K. Sudoh",
                "S. Nakamura"
            ],
            "title": "Evaluating the robustness of discrete prompts",
            "venue": "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Z. Ji",
                "P. Ma",
                "Y. Yuan",
                "S. andWang"
            ],
            "title": "Cc: Causality-aware coverage criterion for deep neural networks",
            "venue": "IEEE/ACM 45th International Conference on Software Engineering (ICSE) (2023),",
            "year": 2023
        },
        {
            "authors": [
                "W. Jiao",
                "W. Wang",
                "Huang",
                "J.-t",
                "X. Wang",
                "Z. Tu"
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "B. Johnson",
                "Y. Brun",
                "A. andMeliou"
            ],
            "title": "Causal testing: understanding defects\u2019 root causes",
            "venue": "In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
            "year": 2020
        },
        {
            "authors": [
                "L. Kuhn",
                "Y. Gal",
                "S. Farquhar"
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "H. Le",
                "Y. Wang",
                "A.D. Gotmare",
                "S. Savarese",
                "S.C.H. Hoi"
            ],
            "title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "B.W. Lee",
                "Y.S. Jang",
                "J. Lee"
            ],
            "title": "Pushing on text readability assessment: A transformer meets handcrafted linguistic features",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "B.W. Lee",
                "J. Lee"
            ],
            "title": "LFTK: Handcrafted features in computational linguistics",
            "venue": "In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA",
            "year": 2023
        },
        {
            "authors": [
                "Y. Li",
                "D. Choi",
                "J. Chung",
                "N. Kushman",
                "J. Schrittwieser",
                "R. Leblond",
                "T. Eccles",
                "J. Keeling",
                "F. Gimeno",
                "A. Dal Lago",
                "T. Hubert",
                "P. Choy",
                "C. de Masson d\u2019Autume",
                "I. Babuschkin",
                "X. Chen",
                "Huang",
                "P.-S",
                "J. Welbl",
                "S. Gowal",
                "A. Cherepanov",
                "J. Molloy",
                "D. Mankowitz",
                "E. Sutherland Robson",
                "P. Kohli",
                "N. de Freitas",
                "K. Kavukcuoglu",
                "O. Vinyals"
            ],
            "title": "Competition-level code generation with alphacode",
            "venue": "arXiv preprint arXiv:2203.07814",
            "year": 2022
        },
        {
            "authors": [
                "Z. Li",
                "C. Wang",
                "Z. Liu",
                "H. Wang",
                "S. Wang",
                "C. Gao"
            ],
            "title": "Cctest: Testing and repairing code completion",
            "year": 2023
        },
        {
            "authors": [
                "P. Liu",
                "W. Yuan",
                "J. Fu",
                "Z. Jiang",
                "H. Hayashi",
                "G. Neubig"
            ],
            "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv",
            "year": 2023
        },
        {
            "authors": [
                "Y. Liu",
                "C. Tantithamthavorn",
                "L. Li"
            ],
            "title": "On the reliability and explainability of automated code generation approaches",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "L. Lorch",
                "J. Rothfuss",
                "B. Sch\u00f6lkopf",
                "A. Krause"
            ],
            "title": "Dibs: Differentiable bayesian structure learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "S. Lu",
                "D. Guo",
                "S. Ren",
                "J. Huang",
                "A. Svyatkovskiy",
                "A. Blanco",
                "C. Clement",
                "D. Drain",
                "D. Jiang",
                "D Tang"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
            "year": 2021
        },
        {
            "authors": [
                "X. Lu"
            ],
            "title": "Automatic analysis of syntactic complexity in second language writing",
            "venue": "International journal of corpus linguistics 15,",
            "year": 2010
        },
        {
            "authors": [
                "Z. Manna",
                "R.J. Waldinger"
            ],
            "title": "Toward automatic program synthesis",
            "venue": "Communications of the ACM 14,",
            "year": 1971
        },
        {
            "authors": [
                "C.D. Manning",
                "M. Surdeanu",
                "J. Bauer",
                "J.R. Finkel",
                "S. Bethard",
                "D. McClosky"
            ],
            "title": "The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations",
            "year": 2014
        },
        {
            "authors": [
                "S. Ouyang",
                "J.M. Zhang",
                "M. Harman",
                "M. andWang"
            ],
            "title": "Llm is like a box of chocolates: the non-determinism of chatgpt in code generation",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "Zhu",
                "W.-J"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
            "year": 2002
        },
        {
            "authors": [
                "H. Pearce",
                "B. Ahmad",
                "B. Tan",
                "B. Dolan-Gavitt",
                "R. Karri"
            ],
            "title": "Asleep at the keyboard? assessing the security of github copilot\u2019s code contributions",
            "venue": "IEEE Symposium on Security and Privacy (SP) (2022),",
            "year": 2022
        },
        {
            "authors": [
                "J. Peters",
                "D. Janzing",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Elements of causal inference: foundations and learning algorithms",
            "year": 2017
        },
        {
            "authors": [
                "M. Post"
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "In Proceedings of the Third Conference on Machine Translation: Research Papers (Belgium, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "R. Pryzant",
                "D. Iter",
                "J. Li",
                "Y.T. Lee",
                "C. Zhu",
                "M. Zeng"
            ],
            "title": "Automatic prompt optimization with\u201d gradient descent",
            "year": 2023
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research 21,",
            "year": 2020
        },
        {
            "authors": [
                "S. Ren",
                "D. Guo",
                "S. Lu",
                "L. Zhou",
                "S. Liu",
                "D. Tang",
                "N. Sundaresan",
                "M. Zhou",
                "A. Blanco",
                "S. Ma"
            ],
            "title": "Codebleu: a method for automatic evaluation of code synthesis",
            "year": 2009
        },
        {
            "authors": [
                "V. Sanh",
                "A. Webson",
                "C. Raffel",
                "S.H. Bach",
                "L. Sutawika",
                "Z. Alyafeai",
                "A. Chaffin",
                "A. Stiegler",
                "A. Raja",
                "M. Dey",
                "M.S. Bari",
                "C. Xu",
                "U. Thakker",
                "S.S. Sharma",
                "E. Szczechla",
                "T. Kim",
                "G. Chhablani",
                "N.V. Nayak",
                "D. Datta",
                "J. Chang",
                "M.T. Jiang",
                "H. Wang",
                "M. Manica",
                "S. Shen",
                "Z.X. Yong",
                "H. Pandey",
                "R. Bawden",
                "T. Wang",
                "T. Neeraj",
                "J. Rozen",
                "A. Sharma",
                "A. Santilli",
                "T. F\u00e9vry",
                "J.A. Fries",
                "R. Teehan",
                "T.L. Scao",
                "S. Biderman",
                "L. Gao",
                "T. Wolf",
                "A.M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M. Scanagatta",
                "C.P. de Campos",
                "G. Corani",
                "M. Zaffalon"
            ],
            "title": "Learning bayesian networks with thousands of variables",
            "venue": "NIPS",
            "year": 2015
        },
        {
            "authors": [
                "J. Siebert"
            ],
            "title": "Applications of statistical causal inference in software engineering",
            "venue": "Information and Software Technology",
            "year": 2023
        },
        {
            "authors": [
                "K. Singhal",
                "S. Azizi",
                "T. Tu",
                "S.S. Mahdavi",
                "J. Wei",
                "H.W. Chung",
                "N. Scales",
                "A. Tanwani",
                "H. Cole-Lewis",
                "S Pfohl"
            ],
            "title": "Large language models encode clinical knowledge",
            "year": 2023
        },
        {
            "authors": [
                "A. Solar-Lezama"
            ],
            "title": "Program synthesis by sketching",
            "venue": "University of California, Berkeley,",
            "year": 2008
        },
        {
            "authors": [
                "B. Sun",
                "J. Sun",
                "L.H. Pham",
                "J. Shi"
            ],
            "title": "Causality-based neural network repair",
            "venue": "In Proceedings of the 44th International Conference on Software Engineering",
            "year": 2022
        },
        {
            "authors": [
                "Z. Sun",
                "Q. Zhu",
                "L. Mou",
                "Y. Xiong",
                "G. Li",
                "L. Zhang"
            ],
            "title": "A grammar-based structural cnn decoder for code generation",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence (2019),",
            "year": 2019
        },
        {
            "authors": [
                "I. Tsamardinos",
                "L.E. Brown",
                "C.F. Aliferis"
            ],
            "title": "The max-min hill-climbing bayesian network structure learning algorithm",
            "venue": "Machine learning 65,",
            "year": 2006
        },
        {
            "authors": [
                "M. Tsunoda",
                "S. Amasaki"
            ],
            "title": "On software productivity analysis with propensity score matching",
            "venue": "ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) (2017),",
            "year": 2017
        },
        {
            "authors": [
                "S. Varshney",
                "M. andMehrotra"
            ],
            "title": "Search based software test data generation for structural testing: a perspective",
            "venue": "ACM SIGSOFT Software Engineering Notes 38,",
            "year": 2013
        },
        {
            "authors": [
                "C. Wang",
                "Y. Yang",
                "C. Gao",
                "Y. Peng",
                "H. Zhang",
                "M.R. Lyu"
            ],
            "title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence",
            "venue": "In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "J. Wei",
                "D. Schuurmans",
                "Q.V. Le",
                "E.H. Chi",
                "D. Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "W. Wang",
                "S. Joty",
                "S.C. Hoi"
            ],
            "title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "B. Wei",
                "G. Li",
                "X. Xia",
                "Z. Fu",
                "Z. Jin"
            ],
            "title": "Code generation as a dual task of code summarization. Advances in neural information processing systems",
            "year": 2019
        },
        {
            "authors": [
                "J. Wei",
                "Y. Tay",
                "R. Bommasani",
                "C. Raffel",
                "B. Zoph",
                "S. Borgeaud",
                "D. Yogatama",
                "M. Bosma",
                "D. Zhou",
                "D Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "year": 2022
        },
        {
            "authors": [
                "J. Wei",
                "X. Wang",
                "D. Schuurmans",
                "M. Bosma",
                "E. Chi",
                "Q. Le",
                "D. Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "M. Xia",
                "E. Kochmar",
                "T. Briscoe"
            ],
            "title": "Text readability assessment for second language learners",
            "venue": "arXiv preprint arXiv:1906.07580",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xiao",
                "P.P. Liang",
                "U. Bhatt",
                "W. Neiswanger",
                "R. Salakhutdinov",
                "Morency",
                "L.-P"
            ],
            "title": "Uncertainty quantification with pre-trained language models: A large-scale empirical analysis",
            "year": 2022
        },
        {
            "authors": [
                "C. Yang",
                "X. Wang",
                "Y. Lu",
                "H. Liu",
                "Q.V. Le",
                "D. Zhou",
                "X. Chen"
            ],
            "title": "Large language models as optimizers",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "S. Yao",
                "D. Yu",
                "J. Zhao",
                "I. Shafran",
                "T.L. Griffiths",
                "Y. Cao",
                "K. Narasimhan"
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "P. Yin",
                "G. Neubig"
            ],
            "title": "A syntactic neural model for general-purpose code generation",
            "venue": "arXiv preprint arXiv:1704.01696",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yu",
                "J. Chen",
                "T. Gao",
                "M. Yu"
            ],
            "title": "Dag-gnn: Dag structure learning with graph neural networks",
            "venue": "In International Conference on Machine Learning (2019),",
            "year": 2019
        },
        {
            "authors": [
                "M. Zhang",
                "J. Sun"
            ],
            "title": "Adaptive fairness improvement based on causality analysis",
            "venue": "In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "J. Yang",
                "Y. Yuan",
                "Yao",
                "A.C.-C"
            ],
            "title": "Cumulative reasoning with large language models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "X. Zheng",
                "B. Aragam",
                "P.K. Ravikumar",
                "E.P. Xing"
            ],
            "title": "Dags with no tears: Continuous optimization for structure learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhou",
                "A.I. Muresanu",
                "Z. Han",
                "K. Paster",
                "S. Pitis",
                "H. Chan",
                "J. Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 1910
        },
        {
            "authors": [
                "D.M. Ziegler",
                "N. Stiennon",
                "J. Wu",
                "T.B. Brown",
                "A. Radford",
                "D. Amodei",
                "P. Christiano",
                "G. Irving"
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords Causality, Large Language Models, Code Generation, Explainability"
        },
        {
            "heading": "1 Introduction",
            "text": "Code generation serves as one of the most central problems in automated software engineering. It enables machines to program automatically to satisfy human intent expressed in the form of some specification, usually in the form of natural language. The problem of code generation has been studied for decades, and has been applied as the basis of many different important domains, such as program synthesis, program repair, and fuzz testing. In recent years, code generation has achieved great progress in both academia and industry. In particular, we observe that large language models (LLMs) have been applied to support code generation, and have achieved impressive results. For example, GPT-4 has been applied to generate code from natural language, whose coding ability is comparable to that of humans [49]. To date, many LLMs are already seamlessly integrated into the developer\u2019s IDE for commercial usage, like GitHub Copilot, Amazon CodeWhisperer, and Tabnine.\nDespite the great progress, we notice that the quality of the generated code, especially in the era of LLMs-based code generation, fluctuates under the variation of natural language specifications. For instance, as will be shown shortly, the way of expressing the same intent in natural language (i.e., prompt) can lead to significantly different code outputs. As a result, the behavior of LLMs-based code generation is opaque and uninterruptible, which hinders the broader adoption of LLMs-based code generation in practice. For instance, it has been reported that GitHub Copilot generates code is highly unstable [40] or even generates code containing security vulnerabilities [52].\n\u2217Corresponding author.\nar X\niv :2\n31 0.\n06 68\n0v 1\n[ cs\n.S E\n] 1\n0 O\nct 2\nTo date, effectively evaluating and explaining the code generation capability of LLMs is challenging. While the research community has proposed several benchmarks to evaluate the code generation capability of LLMs, such as CodeSearchNet [30] and HumanEval [14], those benchmarks focus primarily on the surface level metrics (e.g., the BLEU score) or functional metrics like the pass rate. However, they fail to capture the interplay between the prompt and the generated code, which is critical to understanding the behavior of LLMs-based code generation. While some recent work has proposed approaches to explain the outcome of code generation models [42], they extensively rely on the model\u2019s internal mechanics, which are not available for the most common usage of LLMs-based code generation (i.e., via API). Indeed, to the best of our knowledge, there is no existing work that systematically analyzes prompts\u2019 impact on the code generated by LLM.\nInspired by the recent progress in causality analysis [53] and its application in software engineering [65, 32, 61], this paper explores a causality analysis-based approach to explaining LLM-based code generation. Our method offers a systematic and human-understandable explanation, by establishing the causal relations between the LLM prompts and the generated code. Technically, given the text nature of the prompt and code, it is challenging to represent the prompt and the generated code in a canonical form that is amenable to causal analysis. To this end, we first propose a novel quantification scheme to represent the prompt and the generated code via a number of linguistic features (for prompt) and code features (for generated code). Then, we recast the task of conducting causality analysis between natural language and code as analyzing causality analysis among these numerical features. To systematically explore how the diverse form of prompts affects the generated code, we employ an LLM-based rephrasing technique (e.g., \u201cmake it longer\u201d, \u201cmake it more technical\u201d) to generate diverse prompts for the same intent. We then apply a state-of-the-art causality analysis algorithm, namely DiBS [43], to identify the causal relations among these features and estimate the average treatment effect (ATE) of each rephrasing instruction on the generated code. The ATE indicates the average difference of a feature (e.g., BLEU score) in the generated code when the prompt is rephrased (e.g., to be lengthy) compared to the original prompt. We then use the ATE as a building block to enable holistic analysis that can explain the potential causes on a metric of interest (e.g., BLEU score) in the generated code. For instance, we can identify which linguistic features in the prompt that are most likely to cause the LLM to generate high-quality code and those that are most likely to cause the LLM to generate low-quality code. This actionable insights provide firm and handy guidance to developers to tune the prompt to generate high-quality code.\nTo gauge the effectiveness of our approach, we conduct extensive experiments using three representative LLMs, GPTNeo [28], GPT-3.5-Turbo (ChatGPT) [5], and GPT-4. This empirical study enables a comprehensive analysis of the trade-offs and leads to a number of intriguing findings over LLM prompts and code outputs. First, as expected, the usage of certain keywords/concepts may notablely affect the pattern of produced code output, highlighting the need for a systematic and automated approach to deciding optimal concept/keyword usage for particular scenarios. Second, certain keywords, such as Fluent and Formal, deliver visible \u201ctradeoffs\u201d in the generated code, which may become a practical obstacle for developers to tune the prompt. Moreover, the reaction of LLMs to prompts with certain properties may be unreasonable, revealing some severe problems in the LLMs (e.g., overfitting to the training data). This observation highlights the importance of taking both human-understandable and other subtle factors into account when calibrating the LLM prompt. Last, we demonstrate a versatile and important application of our framework to improve prompts. With empirical assessment, we show the selected prompts offer superior code generation quality compared to the original prompt, illustrating the potential of our approach in assisting developers in refining the prompt. To conclude, this paper makes the following contributions:\n\u2022 Given LLM-based code generation, a cornerstone and vastly-used task in software engineering, this paper for the first time uses causality analysis as a principled approach to analyzing how prompts affect the LLM-based code generation process. \u2022 Technically, we propose a set of domain-specific design considerations to enable accurate and comprehensive\ncausality analysis in the context of LLM-based code generation. We also design a novel approach to identify optimal prompts that facilitate instructing LLMs to generate high-quality code. \u2022 We conduct extensive evaluations over datasets and different LLMs, and we obtain actionable suggestions for users\nto understand and enhance the quality of the LLM-based generated code.\nCode and Data. We provide the code, data, and other supplementary materials of this research at [7]. We will maintain the released artifacts to ensure reproducibility and facilitate future research."
        },
        {
            "heading": "2 Preliminary and Motivation",
            "text": ""
        },
        {
            "heading": "2.1 LLM and Prompt Engineering",
            "text": "LLM. In recent years, LLMs have experienced a surge in popularity and adoption across various scenarios owing to their promising performance and high flexiblity on a diverse range of tasks. LLMs are typically trained on large\ncorpora of text data using self-supervised learning, and can be fine-tuned on specific downstream tasks with only a few examples (see below). To date, LLMs like ChatGPT [5], which contain over 100 billion parameters, have demonstrated strong capabilities on tasks such as language translation [33], neuron explanation [12], and even clinical decision [62].\nPrompt Engineering. One of the key factors that contribute to the success of LLMs is the input prompt, which is a text or template that provides task-specific knowledge to the model. The prompt is typically concatenated with the input text to form the final input to the model. In the task of code generation, the prompt usually refers to a textual or templated representation that encompasses the high-level specifications of the desired code. Conventional ML models are mainly employed following the \u201cpre-train & fine-tune\u201d paradigm which entails training models on general tasks and subsequently fine-tuning them for specific downstream tasks. In contrast, scaled LLMs exhibit properties amenable to few-shot learning, where prompts can strongly steer model towards producing answers for desired tasks. This facilitates an era of \u201cpre-train & prompt,\u201d with properly designed prompts becoming the key to elicit the full potential of LLMs. To date, various methods that improve LLM performance via prompts have been proposed, including few-shot learning [13, 59], chain-of-thought [71, 75], tree-of-thought [79], and debate LLM [19]. Consequently, the design and selection of prompts have rapidly gained importance, spearheading a new research area dubbed \u201cprompt engineering\u201d in LLM research [41]."
        },
        {
            "heading": "2.2 Code Generation",
            "text": "Code generation, also known as program synthesis, is a fundamental software engineering technique [26]. Early code generation task was typically formulated as a search-based problem [25, 46, 63], with the search space constructed by constraints. These constraints are derived from the high-level specification and the target language grammar. Although these approaches can handle simple tasks, their low generalizability prevents them from being applicable in real-world circumstances. Deep learning has led to novel neural network-based code generation methods. The use of various neural architectures such as recurrent neural networks (RNNs) [80], convolutional neural networks (CNNs) [66], and transformers [28] has achieved promising results in code generation tasks, including text-to-code and code-to-code [44]. This paper focuses on generating code from natural language text [28, 39], with Python3 as the target programming language for its high popularity [73, 36, 21].Our approach, however, is generalizable to other settings as shown in Sec. 8.\nRecent years have witnessed a rising interest in applying LLMs to code generation [74, 40, 9]. Although code generation with LLMs also benefits from prompt engineering, the output quality is prone to being compromised by the unstable nature of prompts (see example in Fig. 1). Moreover, unlike humans who are capable of correctly comprehending statements with subtle errors, the low tolerance of compilers and operating systems for bugs and performance issues further exacerbates the problem of prompt instability. In addition, the complexity of the natural language text that constitutes the prompt makes it inherently difficult to understand which characteristics (e.g., the average length of sentences in the prompt) of the prompt contribute to the output quality. All of these issues motivate us to investigate the relations between the prompt and the output quality of LLMs for code generation."
        },
        {
            "heading": "2.3 Causality Analysis",
            "text": "As a canonical technique that is extensively applied to analyze complex software systems [65, 82, 20], causality analysis is proficient at disentangling the complex relations between various factors into an intuitive causal graph with high interpretability. Typically, causality analysis comprises two phases: causal discovery and analysis based on the formed causal graph.\nCausal Discovery. Causal discovery seeks to infer causal relations between variables from observational data. These inferred causal relations are often represented as a directed acyclic graph (DAG), referred to as Causal Graph. Formally,\nDefinition 1 (Causal Graph). A causal graph is a DAG with nodes V and edges E, i.e., G = (V, E), where each node X (X \u2208 V) represents a random variable and each edge X \u2192 Y (X,Y \u2208 V) represents a directed causal relation from X to Y. PaG(X) denotes the parent nodes of X in G, i.e., PaG(X) = {N |N \u2192 X \u2208 E}. Definition 2 (Endogenous and Exogenous Nodes). Nodes in the causal graph can be categorized into two distinct groups based on the presence or absence of their parent nodes: endogenous nodes, whose values are determined by other nodes (i.e., parent nodes) in the graph, and exogenous nodes, which derive their values from external factors.\nThe edges of the causal graph represent causal relations, which are distinct from the commonly known correlations. The latter indicates merely that two variables are statistically correlated, whereas the former indicates that one variable causes the other. Suppose, for example, that there are three variables, X, Y , and Z, and that the corresponding causal graph is X \u2190 Z \u2192 Y . Here, Z is the common cause of X and Y , also known as a confounder. In this case, X and Y are\ncorrelated because they are simultaneously affected by Z. X and Y are not causally related, however, because X is not the cause of Y and vice versa. This disparity between causal relations and correlation necessitates the use of causal discovery algorithms. These algorithms can precisely infer the causal relations between variables from observational data and then construct the causal graph.\nAnalysis based on Causal Graphs. After obtaining the causal graph, further analysis can be conducted. The prerequisite for the analysis, however, is that all conclusions derived from the graph\u2019s properties must be consistent with the real world. global Markov assumption [53, 48], which is widely assumed in the field of causal analysis [64, 53], resolves this issue.\nDefinition 3 (Global Markov Assumption). Given a distribution P and its corresponding causal graph G, P and G satisfy the global Markov assumption if X yG Y | Z =\u21d2 X yP Y | Z.\nX, Y , and Z are three disjoint sets of variables, which also represent three nodes/node sets in the causal graph. X yP Y | Z signifies that X and Y are conditionally independent given Z in P, reflecting the real-world causal relations. And X yG Y | Z denotes that X and Y are separated by Z in G, also known as d-separation [53]. On causal graphs, both qualitative and quantitative analysis can be conducted. Qualitative analysis typically leverages structural information contained within the causal graph. By traversing the graph, all factors that have a causal effect on the target variable can be identified. Furthermore, quantitative analysis, also known as causal inference, aims to measure the causal effect of one variable X on another variable Y , by answering counterfactual questions such as \u201cwhat would happen to Y if X were set to a different value?\u201d. In this paper, both qualitative and quantitative analysis are performed on the causal graph for different purposes, which will be detailed in Sec. 4."
        },
        {
            "heading": "3 Research Motivation and Pilot Study",
            "text": ""
        },
        {
            "heading": "3.1 Effect of Prompt in Code Generation",
            "text": "As reviewed in Sec. 2, while recent work has shown promising results in prompt engineering, the design of prompts is still largely a manual process that relies on human expertise. Thus, evaluating the quality of prompts remains an open problem [70, 8], let alone providing a systematic guideline for prompt design. Overall, considering the high flexibility of prompts in natural language, we see that the effects of different types of prompts on LLM performance are not well understood. Specifically, the criteria for assessing prompt quality are unclear, making it challenging to determine whether one prompt is superior to another before actually executing it. Moreover, the effects of prompts are often non-linear and non-monotonic [31], making it difficult to predict the performance of a prompt solely based on its design.\nFig. 1 presents an example of how the performance of a prompt can be affected by subtle modifications in its design, leading to highly confusing and non-monotonic results. In this case, LLM is instructed to generate a program that returns the first letter of each word in a given string. The relatively unintelligible prompt in Fig. 1(a) misleads the LLM into generating a program that capitalizes the initial letter of each word. In contrast, the more explicit instructions provided by the prompt in Fig. 1(b) direct the LLM to generate the correct program. Evidently, this distinction was not anticipated by the designer, as the generated code for two semantically equivalent prompts should be identical. This example illustrates the importance of comprehending the mechanism of prompts\u2019 effect on code generation. Consequently, this work aims to answer this research question:\nHow to systematically establish the prompt\u2019s influence on LLM-based code generation?"
        },
        {
            "heading": "3.2 Prompt Adjustment via LLM-Based Rephrasing",
            "text": "Following the observation in Fig. 1, we interpret that \u201cprompt adjustment\u201d is critical, as a minor change in the prompt may substantially improve the performance of a given LLM from generating incorrect code (Fig. 1(a)) to correct code (Fig. 1(b)). Thus, the intuition is that by properly adjusting the prompts, we expect to observe and characterize the relations between the prompt and the generated code in depth. Nonetheless, prompt adjustment is itself challenging [85, 56, 78]. Generally, a desirable prompt adjustment method shall notably alter one or more properties of the prompt while preserving its semantics. In natural language processing, a straightforward method to achieve this goal is to replace words with their synonyms. However, we argue that this is not an optimal way in our research context. Indeed, the frequent occurrence of lengthy text in real-world prompts and the abundance of synonyms in natural language jointly construct an enormous search space, making the exhaustive search for the expected prompts impractical.\nInstruction Pass Rate Syntax Error Rate Stability Short -0.0029 +0.0090 +0.0534 Long +0.0095 -0.0013 -0.0184 Formal +0.0061 -0.0067 +0.0011 N/A 0.0000 +0.0026 +0.0290\ntions. Then, all rephrased programming problems are fed into LLM to generate code, which is then compared to the code generated from the original prompt. The results show that rephrasing instructions can indeed affect the generated code and, more importantly, convey different intentions to the LLM, which are reflected in the varied change trends of code metrics.2 Thus, this work adopts LLM-based rephrasing as the basic operator and aims to answer the following research challenge:\nHow to effectively explore the prompt space and systematically adjust the prompt to achieve the desired effect on the generated code?\n3.3 Analysis for Complex Relationships in Code Generation\n\ud835\udc4d\n\ud835\udc4b = 2\ud835\udc4d + \ud835\udc4f + \ud835\udf16\n\ud835\udc4b \ud835\udc4c \ud835\udc4c = 10\ud835\udc4b + \ud835\udc4d + \ud835\udc4f + \ud835\udf16\nit affects both X and Y . Besides, b1 and b2 are two constants, and \u03f51 and \u03f52 denote random noise with zero mean. The standard procedure for predicting the value of Y is to train a regression model taking X and Z as inputs. This trained model is capable of achieving a high level of accuracy, but it may not capture the correct quantitative relation between X and Y . For example, Y = 21Z + 10b1 + b2 is a \u201cperfect\u201d model to offer 100% accuracy in estimating.3 However, it is evidently not the appropriate model for examining the effect of the prompt (X) on the generated code (Y). Any downstream analysis based on this model will inevitably lead to an incorrect conclusion that X has no effect on Y . This problem will be further exacerbated in the real-world scenario of LLM-based code generation, where hundreds of variables, which reflect the properties of the prompt, the generated code, and the programming question, are involved.\n2In short, the pass rate is the percentage of generated code that passes the test cases, the syntax error rate is the percentage of generated code that contains syntax errors, and the stability is the percentage of generated code that is identical to other code generated from the same prompt. To clarify, we aim to demonstrate that rephrasing instructions can affect the generated code, as reflected in the change trends of these metrics. \u201cRephrasing\u201d however does not necessarily mean \u201cimproving\u201d the generated code.\n3All noise terms are omitted here because their mean is zero.\nAdditionally, the uncertainty of the LLM\u2019s output further complicates the trial-and-error process of empirical analysis [35, 77, 50]. In particular, the generated code for a given prompt is stochastic rather than deterministic. Hence, it is contended that the assessment of the generated code\u2019s quality should be based on the distribution rather than the outcomes of singular or a few executions. To conclude, we attempt to address the following issue in this paper:"
        },
        {
            "heading": "4 Design",
            "text": "Fig. 3 presents the overview of our study, where two primary phases are involved: data collection and analysis. In the data collection phase, we begin by rephrasing prompts (the green blocks in Fig. 3), which represent questions of programming tasks. Then, both the rephrased and the original question prompts are fed into two subtasks. The first subtask is to quantify the characteristics of prompts, which is accomplished by extracting linguistic features from the natural language text in these questions. In another subtask, questions are fed into an LLM to generate code. We further collect a variety of performance metrics from the generated code, including the code\u2019s correctness, readability, etc. Together, the linguistic features of the prompt and the performance metrics of the generated code (two blue blocks in Fig. 3) constitute the data for the analysis phase.\nDuring the analysis phase, we first use the causal discovery algorithm to construct a causal graph that represents the causal relations between all the variables that we collected. A series of further analyses are conducted on the causal graph in an effort to identify general principles for code generation prompt design. Last, a prompt optimization algorithm is employed based on the causal graph to determine the optimal prompt setting that will generate highquality code with the highest probability.\nOverall, the workflow in Fig. 3 is designed to answer the research questions raised in Sec. 3.1 and address subsequent technical challenges. Specifically, Sec. 4.1 proposes a systematic approach for quantifying the characteristics of prompts. Then, Sec. 4.2 addresses the challenge of prompt adjustment raised in Sec. 3.2. Afterwards, Sec. 4.3 adopt causality analysis to disentangle the complex relations between the numerous variables involved in this study. Finally, Sec. 4.4 proposes an algorithm for learning and explaining prompts\u2019 effect on code generation, thereby addressing the issue discussed in Sec. 3.3."
        },
        {
            "heading": "4.1 Prompt Quantification",
            "text": "As discussed in Sec. 3, the criteria for assessing prompt quality are generally unclear, making it impractical to compare two prompts prior to feeding them to LLMs. To address this, we propose to quantify the characteristics of prompts using linguistic features. Linguistic features are the building blocks of natural language. They are the elements that make up the structure of a language, such as morphology, syntax, and semantics. In order to quantify the prompt for large language models, it is necessary to extract these linguistic features from the natural language text of the prompts. This can be done using various techniques, such as part-of-speech tagging [11, 76], named entity recognition [23], and dependency parsing [47].\nIn this paper, we follow Lee et al. [37, 38], one state-of-the-art study on linguistic feature in natural language processing, to extract linguistic features from prompts. Specifically, a total of 255 linguistic features are extracted from prompts, including lexical features (e.g., count of nouns, verbs, adjectives, etc.), syntactic features (e.g., phrasal features [45], average height of parsed dependency tree, etc.), and semantic features (e.g., semantic richness [37], word familiarity features [17], etc.). The complete list of linguistic features can be found in the Appendix of Lee et al. [37].\nBy quantifying the linguistic features of a given prompt, we can better understand the underlying structure and meaning of the text, allowing us to more accurately capture the nuances and complexities of natural language and providing a more systematic guideline for prompt design. Further, we presume that these vast amounts of linguistic features are capable of sufficiently capturing all interactions between prompts and other variables (including rephrase instructions and generated code), thereby resolving the unmeasurability issue of prompt discussed in Sec. 3."
        },
        {
            "heading": "4.2 Rephrase Generation",
            "text": "To explore the impact of prompt design on code generation, we propose rephrasing the prompt using LLM. In comparison to rule-based mutation, rephrase serves a more flexible and holistic mutation primitive; it can generate a greater variety of prompts and substantially improve the prompt diversity. A greater variety of prompts indicates that the value space for each linguistic feature has been thoroughly explored. Hence, we presume that the diversity of prompts is beneficial to the observability of the linguistic features, which is crucial to the success of subsequent analysis.\nInspired by mature rephrasing tools, such as QuillBot [6], we designed a prompt template for programming question rephrasing, as shown in Fig. 4. Note that, for simplicity, we use meta-prompt to refer to the prompt used for programming question rephrasing. All of these meta-prompts are refined and optimized by the state-of-the-art LLM, GPT-4.0, in an effort to enhance the quality of the rephrased prompts and subsequently escalate the quality of the generated code.\nThis template also includes a set of rephrasing intentions that can direct the LLM to rephrase the provided programming question in a particular manner. These rephrasing intentions are classified as instruction, role, and scenario. Instruction provides the LLM with explicit instructions, such as make it short and make it fluent. For role and scenario, they are designed to provide context for the LLM, like as a student and in a programming competition to unleash the LLM\u2019s creativity. Furthermore, pre-defined intentions from various categories can be combined to form a more complex rephrasing intention. These rephrasing intentions are intended to exhaustively cover applicable rephrasing preferences for the programming question rephrasing task.\nWe designed a total of 22 rephrasing intentions, which can be combined to form over one hundred filled-in content for the meta-prompt template; our tentative study shows that exploring all these combinations results in excessive cost in the experiment. Therefore, we select six instructions, three roles, and three scenarios based on the expert\u2019s experience and the results of the pilot study. The selected rephrasing intentions are marked in grey in Fig. 4(b). It is worth noting that the design rephrasing intention is not limited to those shown in Fig. 4(b). Additional rephrasing intention can be added to the template to expand the diversity of the prompts. This will be discussed in Sec. 8."
        },
        {
            "heading": "4.3 Causal Analysis",
            "text": "Causal analysis is a powerful tool for comprehending the intricate relations between variables. This section describes the methods we employed to construct a causal graph (causal discovery) and conduct quantitative analysis on the graph (causal inference).\nCausal Discovery. This study involves a variety of variables, which can be categorized into three groups: meta-prompt variables (0/1 variables flagging whether users select a particular rephrasing intention listed in Fig. 4(b) or not), linguistic feature variables, and code metric variables. By employing causal discovery, it is possible to disentangle the complex relations between variables and learn a causal graph, where the nodes represent the variables and the edges represent the causal relations between nodes. In this study, we employ a state-of-the-art causal discovery algorithm, DiBS [43]. As a gradient-based causal discovery algorithm, DiBS transforms the causal graph construction procedure into a differentiable optimization problem, thereby substantially enhancing the efficacy of causal discovery and ensuring the plausibility of the learned causal graph [43].\nHowever, the large number of variables involved in this study makes constructing causal graphs exceedingly difficult. To address this challenge, we propose a two-step causal discovery approach, as shown in Fig. 5. Holistically, the three aforementioned groups of variables propagate in the following order: The meta-prompt determines the output of the rephrase generation task, affecting the linguistic features of the rephrased programming question, which, in turn, affects the generated code\u2019s metrics. Consequently, the causal graph can be split into two subgraphs, which can be\nlearned separately. After learning the two subgraphs independently, the complete causal graph can be obtained by combining them directly.\nCausal Inference. After learning the causal graph, we conduct quantitative analysis on the graph to identify the causal effect of one variable on another. Taking Fig. 2 as an example, the quantitative analysis seeks to know the magnitude of the change in Y (the output variable) when X (the treatment variable) changes from x1 to x2 while blocking the effect of confounding variable Z. Here, x1 and x2 are two specific values of X. One or both of them may not be observed in the data. Therefore, they are counterfactual values, denoted as doX = x1 and doX = x2, respectively. The magnitude\nof Y\u2019s average change reflects the causal effect of X on Y . This is referred to as the average treatment effect (ATE) within the context of causality analysis [53, 48]. Below, we present the formal definition of ATE: Definition 4 (ATE). ATE of treatment variable X on output variable Y is defined as:\nATE = E[Y | do(X = x1)] \u2212 E[Y | do(X = x2)] (1)\nwhere terms containing do(\u00b7) operator are known as causal estimand, which cannot be estimated from observational data directly.\nWe employ double machine learning (DML) [15] to estimate causal estimands in Eq. 1. DML is a state-of-the-art method and has been extensively applied in the causal analysis literature [4]. It first uses two machine learning models to estimate treatment variables and output variables, respectively, from confounders. The causal effect of the treatment variables on the output variables can then be determined by training another machine learning model that predicts the relations between the residuals of the two models. In this study, we employ the DML implementation in the EconML package [4]."
        },
        {
            "heading": "4.4 Establishing Prompt Effect on Code Generation",
            "text": "Algorithm 1: Analysis Input: Causal Graph G, Meta-prompt Variable M, Linguistic Features Ll = (L1, \u00b7 \u00b7 \u00b7 , Ln), Code Metrics Cl = (C1, \u00b7 \u00b7 \u00b7 ,Cn) Output: Dictionary D of Influenced Code Metrics with Explanation for the Effect\n1 D\u2190 {}; 2 foreach C \u2208 Cl do 3 ATECM \u2190 E[C | do(M = 1)] \u2212 E[C | do(M = 0)] ; // Computing ATE of M over C 4 end /* Find the top 3 code metrics that are most affected by M */ 5 Sort Cl in descending order according to abs(ATECM); 6 CS \u2190 Top3Affected(Cl); 7 foreach C \u2208 Cl do 8 LP \u2190 IdentifyAncestors(G,C, Ll); 9 foreach L \u2208 LP do\n10 lM=0 \u2190 E[L|M = 0]; lM=1 \u2190 E[L|M = 1]; 11 ATECL \u2190 E[C | do(L = lM=1)] \u2212 E[C | do(L = lM=0)]; 12 end /* Find the linguistic features that are mainly responsible for the effect of M on C */ 13 D[C]\u2190 SortSelect(LP); 14 end 15 return D\nAlg. 1 presents the prompt effect analysis procedure. The inputs for this algorithm are a causal graph G, a meta-prompt variable M, linguistic features Ll, and code metrics Cl. Typically, the meta-prompt variable M refers to a binary (0/1) variable flagging if a particular rephrasing intention is selected or not (\u201c1\u201d denotes selection). Moreover, a set of binary variables indicating if multiple rephrasing intentions are employed or not can also be supported by this algorithm (by invoking Alg. 1 for multiple times). Here, the set of binary variables simulates the situation where a user combines multiple rephrasing intentions to rephrase a question, as described in Sec. 4.2.\nHolistically, Alg. 1 comprises two steps, learning the effect and explaining the effect. In the first step, we compute the average treatment effect (ATE) of the meta-prompt variable M on each code metric C (lines 2-4) and then select the top three code metrics that are most affected by M (lines 5-6). This step is intended to quantify the extent of M\u2019s effect on code generation. In the second step, we attempt to explain how the changes in meta-prompt M are propagated to the selected code metrics CS (lines 7-13). Specifically, we first query the causal graph to find all the ancestor linguistic features of the chosen code metric C (line 8), as they are potential mediators of the effect of M on C. Then, we intervene on each ancestor individually to calculate their ATE on C (lines 10-12). Based on the calculated ATE, we determine the linguistic features primarily responsible for the effect of M on C (lines 13). Finally, we return a dictionary D containing the top three influenced code metrics and their corresponding explanations (line 15)."
        },
        {
            "heading": "5 Experiment Setup",
            "text": "Scripts required to conduct the entire study is written in Python3 with about 2.4K lines of code. We run all experiments on a server with AMD Ryzen 3970X CPU and one NVIDIA RTX 3090 GPU."
        },
        {
            "heading": "5.1 Datasets & Model",
            "text": "Datasets. Our study is conducted on one of the most popular text-to-code datasets, APPS [28]. It is exclusively designed for Python code generation and contains 10K Python problems with difficulty annotations. Each problem is associated with a few correct solutions and a set of test cases. To learn high-quality causal graphs, we sample and generate 6K data points from this dataset.4 Note that CodeContests [39] and other datasets are also compatible with our framework. The promisingly high generalizability of our framework to other datasets will be discussed in Sec. 8.\nModels. Three LLMs are used in our experiments: a pre-trained GPT-Neo (2.7B) model by Hendrycks et al. [28], GPT-3.5-Turbo (ChatGPT), and GPT-4. These three models denote the relatively tiny but powerful LLM, the most widely-used LLM, and the state-of-the-art LLM, respectively. For ChatGPT series models (GPT-3.5-Turbo and GPT4), we use the official API supported by Azure. Except for the number of generated solutions, which is set to 3, and the maximum length of generated solutions, which is set to 2,000 to handle long solutions, all parameters are set to their default values. In this study, each model generates code for the 6K data points and GPT-3.5-Turbo rephrases the prompt. The reason for this setup is twofold. First, compared to code generation, prompt rephrasing is a relatively simple task for for which GPT-3.5-Turbo is sufficient. Our preliminary observation shows that the more powerful but expensive GPT-4 is an \u201coverkill\u201d in this task, while the tiny GPT-Neo cannot handle it plausibly. Second, in this study, we focus on the code generation task and the rephrasing itself is quite standard and not our primary focus."
        },
        {
            "heading": "5.2 Code Metrics",
            "text": "Code generation is naturally multi-faceted. For example, generated code should be correct but fail to adhere to the coding style (e.g., PEP8 for Python). To comprehensively evaluate the code generation capability of LLMs, we employ a set of code metrics to measure the quality of generated code. These metrics are categorized into five categories: correctness, diversity, overhead, readability, and security, as reported in Table 2. The rationale behind this categorization is that these metrics are orthogonal to each other and can be used to evaluate the code generation capability of LLMs from different perspectives. For these metrics, we use the implementation provided by [55, 72, 44, 40]."
        },
        {
            "heading": "6 Evaluation",
            "text": "In this section, we evaluate the efficacy and value of our analysis framework, as described in Sec. 4. To assure the correctness of subsequent analysis, we first evaluate the accuracy of the learned causal graphs, which serve as the foundation of our framework (RQ1). Then, by querying the learned causal graphs, we obtain several valuable and indepth insights into LLM-based code generation (RQ2). Finally, we adopt the learned graphs to guide the adjustment of the prompt, thereby further demonstrating the value of our framework (RQ3).\nnodes edges GPT-Neo 45 192 GPT-3.5-Turbo 50 177 GPT-4 45 180"
        },
        {
            "heading": "6.1 RQ1: Verification of the Causal Graphs",
            "text": "As the cornerstone of our analysis framework, the accuracy of the learned causal graphs is critical. In this section, we endeavor to verify the accuracy of the learned causal graphs. However, the lack of a ground-truth causal graph poses an obstacle to verification. To overcome it, we propose a comprehensive and efficient verification strategy that uses the learned causal graphs to predict each downstream variable (code metrics listed in Table 2 in our case) and then compares the predictions to the actual values. Idealistically, the learned graph would depict the true causal relationships between each pair of variables, thereby precisely depicting the propagation of changes from meta-prompt variables to code metrics. Consequently, the predictive potential of the learned graph should correctly reflect its accuracy. For each code metric, we report the R2 score and mean squared error (MSE). The R2 score quantifies the proportion of the variance in the target variables (code metrics) that can be predicted from the input variables (meta-prompt variables and the linguistic properties of the original prompt), with a higher R2 score indicating a better predictive capability. For MSE, it measures the average of the squares of the errors; the lower the value, the better.\nTable 4 reports the verification results. From a holistic perspective, The high R2 scores and low MSE values indicate that the learned causal graphs have a strong predictive capability. In most cases, the R2 scores are above 0.8, and the MSE values are below 0.2. The only exception is the semgrep count metric, which represents the number of security issues detected by Semgrep [2] in the generated code. On this metric, the graphs for GPT-3.5-Turbo and GPT-4 perform poorly, whereas the graph for GPT-Neo attains a perfect score (R2=1.0 and MSE=0.0). We investigate this phenomenon and discover that the semgrep count has a low variance. Specifically, zero security issues are detected for code generated by GPT-Neo, while 13 security issues are detected on GPT-3.5-Turbo and four on GPT-4. We presume that GPT-Neo has been fine-tuned to the point of relative overfitting on APPS, a clean dataset, resulting in no detected security flaws. For ChatGPT series models, we attribute the impressive performance to its innovative training paradigm, reinforcement learning from human feedback (RLHF) [86]. In the subsequent analysis, the semgrep count metric will be omitted as no meaningful insights can be obtained from it.\nThere are several other interesting observations when comparing the performance of the learned graphs for different models. In general, the graphs for ChatGPT series models have a similar performance pattern, as the differences between their R2 scores and MSE values are typically small (less than 0.05). Given that GPT-4.0 is derived from GPT-3.5-Turbo, this is not surprising. In contrast, the graphs for GPT-Neo display a substantially distinct performance pattern. In particular, the graph performs outstandingly on similarity-based metrics (e.g., gold sim CodeBLEU). This phenomenon is interpreted as a consequence of the overfitting of GPT-Neo on APPS. Instead of \u201cthinking\u201d and then writing code like human programmers, we interpret that GPT-Neo is more likely to memorize a large number of tiny code snippets from training data and then combine them according to the requirements. Such characteristics render GPT-Neo insensitive to changes introduced by meta-prompt variables, preserving the stability and similarity of the generated code to the ground truth. Fig. 6 illustrates this phenomenon. In this example, altering the count target from lowercase letters to uppercase letters has no effect on the generated code. This stickiness to the original ground truth is also reflected in the low variance of similarity-based metrics, thereby reducing the predictive difficulty for the learned graph."
        },
        {
            "heading": "6.2 RQ2: Analysis based on the Causal Graphs",
            "text": "In this section, we assess our analysis framework by employing it to learn the extent of the impact of meta-prompt variables on code metrics; this reflects how code metrics can be affected by prompts. Alg. 1 is adopted to achieve this objective. Since we divide the meta-prompt variables into three groups \u2014 instruction, role, and scenario \u2014 we report the results for each group separately. Specifically, Fig. 7 presents results for the instruction group, and Fig. 8 shows results for the role and scenario groups. Each row in these figures represents a meta-prompt variable (e.g., Long; see Fig. 4 for details), with three sub-rows representing the three code metrics that are mostly affected by the meta-prompt variable. Code metrics are highlighted in red and blue colors. The former implies that the meta-prompt variable has a negative effect on the code metric, whereas the latter indicates a positive effect. Besides, cells marked in gray contain the primarily responsible linguistic variables for their left-hand side code metrics. For simplicity, we attach the primarily responsible linguistic variables for each code metric only in Fig. 7, while omitting them in Fig. 8. Readers are encouraged to refer to the repository[7] for the complete results.\nThe straightforward conclusion from Fig. 7 is that each meta-prompt variable tends to influence distinct code metrics in different causal graphs. For example, the mostly affected code metric for meta-prompt variable Short is syn err in GPT-Neo, while it is gold sim B in GPT-3.5-Turbo, and pass rate in GPT-4. We attribute this notable difference to the distinction between the three learned graphs, as mentioned in Table 3\u2019s discussion. However, there are also some common patterns, particularly in the graphs belonging to the GPT series models. Typically, the meta-prompt variables Long and Formal have a negative effect on the generated code, decreasing its similarity to the ground truth. In contrast, Short and Fluent are beneficial. In fact, these four meta-prompt variables constitute two pairs of opposites: Long vs. Short and Formal vs. Fluent. For Long vs. Short, we attribute this phenomenon to the fact that the former tends to load rephrasings with unnecessary and even misleading information, whereas the latter tends to summarize and then extracts the most essential information. For Formal vs. Fluent, we presume that Formal complicates the programming logic while Fluent simplifies it.\nFrom the perspective of the linguistic variables, we observe that, in the majority of instances, certain linguistic variables serve as the primary responsible variables. root det var occurs 12 times in the case of GPT-Neo. This variable represents the value of the total number of unique determiners divided by the square root of the total number of determiners. The unexpectedly high frequency of this variable is counterintuitive, as one may not expect it to have a substantial impact on the generated code. In line with our discussion in Sec. 6.1, we assume that this unusual\nfrom the meta-prompt variable, while blue indicates a positive effect. The right gray cells record the two primarily responsible linguistic variables for the meta-prompt effect.\noccurrence is consistent with the overfitting characteristics of GPT-Neo. In contrast, simp ttr and features related to named entities occur most frequently for GPT series models. The former indicates the degree of lexical variation of the text, and the latter determines the complexity of the information contained in the text; both are intuitive and reasonable.\nSome additional interesting conclusions can be derived from Fig. 8. Compared to the instruction group, the effect of the role and scenario groups on the generated code in the cases of GPT series models tends to be more positive. In\nparticular, these two groups of variables achieve 13 out of 18 positive effects on GPT-3.5-turbo and 15 out of 18 on GPT-4. This result is consistent with previous research on prompt engineering [75, 19, 83], indicating that role-playing and scenario setting are efficient ways to enhance the performance of LLM-based code generation. GPT-Neo, on the other hand, performs poorly on these two categories of variables, indicating that role-playing and scenario setting are inappropriate for models of its size.\nFindings: There are several interesting findings from the analysis that are worth mentioning.\n1. Treat the LLM as an \u201cignorant idler.\u201d When you try to improve the performance of LLM-based task by rephrasing the prompt, you should avoid extending it or stating it in a formal way. Instead, you should remove redundant information and make it easier to understand. 2. Take advantage of the analysis pipeline illustrated in this work, and watch out for the responsible linguistic variables. When some unreasonable linguistic variables frequently work as the primarily responsible variables for prompt effect, be careful. This phenomenon indicates that the LLM is failing to react reasonably to the prompt according to their linguistic properties. In other words, it may be a sign of overfitting. 3. In general, using role-playing and scenario setting denotes a universal and effective way to improve the performance of LLM for models with sufficient capacity. However, for tiny models, using simple rephrasing instructions like \u201cmake it more concise\u201d can often be more effective."
        },
        {
            "heading": "7 Downstream Application",
            "text": "In this section, we present a prospective downstream application on the basis of our approach in improving the prompt to generate high-quality code. Specifically, the procedure is based on the genetic algorithm which is widely used in the literature of search-based software engineering, such as test case generation [69]. The genetic algorithm is a metaheuristic search algorithm that mimics the process of natural selection. It operates on a population of candidate solutions and iteratively evolves the population to find the optimal solution. Below, we describe how these procedures are concretized in our context.\nFitness Function. The goal of the procedure is to find the optimal rephrasing instructions that maximize the probability of generating high-quality code. First, as a straightforward solution, we may directly use the objective metric (e.g., BLEU score) as the fitness function and search for the optimal rephrasing instructions that maximize the objective metric. However, this solution is infeasible in our context. The reason is that the objective metric relies on the ground truth to evaluate the quality of generated code. Even for the metrics that do not require the ground truth (e.g., black count), the solution is still highly costly as it requires the generation of code for each candidate solution. In contrast, our causality analysis-based approach actually constitutes a lightweight surrogate for the objective metric and is free from the ground truth. Specifically, based on the causal graph, we can estimate the expected value of the objective metric of a candidate solution without involving the actual code generation process. This surrogate is much more efficient than the objective metric and can be used as the fitness function in the genetic algorithm. More importantly, the surrogate is also expected to be more stable than actual code generation, as it is asymptotically unbiased and free from the infamous non-determinism issues in LLM [50].\nGenetic Representation, Modification and Selection. We use a genetic algorithm to find the rephrasing instructions that can maximize a given objective metric. The instructions are represented as a binary string, with each bit indicating the selection status of the corresponding instruction. For example, \u201cmake it short\u201d is represented as 100000, and combining it with \u201cmake it fluent\u201d is 110000. The algorithm operates on these binary strings to find the best instructions. In each generation, each vector is evaluated using a predefined fitness function. Only the top-N candidates, based on fitness scores, move to the next generation (i.e., \u201csurvive\u201d). The new generation is formed using these top-N vectors. Two standard operators in the genetic algorithm, Crossover and Mutation, are applied to create new offspring vectors from any two parent vectors. Specifically, two new vectors are generated using a two-point crossover. In addition to Crossover, Mutation in our context is implemented to randomly alter bits in a vector to enhance the population\u2019s genetic diversity.\nResult. We apply the genetic algorithm to the APPS dataset and use the BLEU score as the objective metric. Because the rephrasing intentions have been refined and optimized by GPT-4, we consider single rephrasing instructions to be an adequate baseline. We evaluate our approach against the best single rephrasing-intent instruction and the original prompt. The results are shown in Table 5. We can see that our approach outperforms the single rephrasing instruction and the original prompt in most cases. The only exception is in the case of GPT-Neo, where the overfitting problem may be the cause (see related discussion in Sec. 6). Overall, we interpret the results as highly encouraging, suggesting that our algorithm for improving the prompt to generate high-quality code is effective."
        },
        {
            "heading": "8 Discussion",
            "text": "Extensibility. While the current technical pipeline is primarily applied over the task of generating Python3 code, it is extensible to other languages. Overall, our approach is generally language agnostic; while a few prompts are language specific, it is easy to see that such prompts can be constructed when considering other languages. For example, the prompt def foo(): can be replaced with function foo() { for JavaScript, or int foo() { for C. Besides, our approach is also extensible to other rephrasing instructions. For example, we can add the rephrasing instruction \u2018\u2018make it more object-oriented\u2019\u2019 to the current set of rephrasing instructions. The only requirement is that the rephrasing instruction should be of reasonable difficulty for LLMs to understand and implement.\nThreats to Validity. Our study is subject to the following threats to validity. First, our study primarily focuses on the GPT-family models, namely, GPT-Neo (2.7B), GPT-3.5-Turbo, and GPT-4. While these models are among the most popular LLMs, there are however other LLMs that are not investigated in this study such as the T5 model [57]. Second, our study is conducted on APPS [28], which is currently one of the most popular datasets for code generation. However, there are other datasets that are not investigated in this study such as the HumanEval [14] dataset. Third, our study is conducted on Python3 code generation and focuses on a limited number of rephrasing instructions. However, given the popularity of Python3 and the extensibility of our approach, we believe our findings are representative for common usage scenarios and are generalizable to other languages and rephrasing instructions, as discussed above in \u201cExtensibility.\u201d"
        },
        {
            "heading": "9 Related Work",
            "text": "Theoretical Causality Analysis. Causality analysis has been studied in statistics and machine learning for decades [53]. In the past few decades, there has been a surge of interest in learning causal relations from observational data [54]. Algorithms from various categories typically employ different strategies for learning the causal graphs. Constraint-based algorithms are the most classical algorithms in this area. For instance, the Peter and Clark (PC) algorithm begins with a complete graph and deletes edges using conditional independence from hypothesis tests [64]. Score-based algorithms recover causal graphs by maximizing a pre-defined scoring criterion, such as BDe(u) [27]. Representative score-based algorithms includes HC [67], GES [16], BLIP [60] and GOBNILP [18]. Recently, gradient-based algorithms have been proposed. For example, NOTEARS [84] and DAG-GNN [81] seek to recover the data generation process while adhering to acyclicity constraints. These advancements in the theoretical aspect of causality analysis are orthogonal to our work. In this paper, we focus on the application of it in evaluating and explaining LLM-based code generation.\nApplication of Causality Analysis in Software Engineering. Recently, it has been shown that causality analysis can be applied to various software engineering tasks [61], including debugging [22, 20], root cause analysis [34], data race detection [29], and also deep neural network (DNN) testing and repairing [65, 82, 32]. Besides, causality analysis has been used for understanding empirical software engineering data and uncover the underlying causal relations. For example, causality analysis has been used to understand the impact of programming language on code competitions [24] or various factors that affect software productivity [68]. In this paper, we advocate the application of causality analysis for understanding the role of distinct prompts in LLM-based code generation."
        },
        {
            "heading": "10 Conclusion",
            "text": "In line with the prosperous adoption of LLMs in code generation, this paper proposes a novel causality analysis-based approach to systematically analyze the relations between the LLM input prompts and the generated code. Our solution facilitates the evaluation and explanation of LLM-based code generation, and provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt. Our work can serve as a roadmap for users to comprehend and improve the quality of LLM-generated code."
        }
    ],
    "title": "Benchmarking and Explaining Large LanguageModel-based Code Generation: A Causality-Centric Approach",
    "year": 2023
}