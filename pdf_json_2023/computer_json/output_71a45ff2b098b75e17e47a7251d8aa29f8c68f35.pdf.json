{
    "abstractText": "Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution, rather than specific weights, is trained. This meta predictive learning is successfully validated on classifying handwritten digits where pixels are input to the network in sequence, and moreover on the toy and real language corpus. Our model reveals that most of the connections become deterministic after learning, while the output connections have a higher level of variability. The performance of the resulting network ensemble changes continuously with data load, further improving with more training data, in analogy with the emergent behavior of large language models. Therefore, our model provides a starting point to investigate the connection among brain computation, next-token prediction and general intelligence. \u2217Equal contribution. \u2020Electronic address: huanghp7@mail.sysu.edu.cn 1 ar X iv :2 30 9. 04 10 6v 2 [ cs .C L ] 9 O ct 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Chan Li"
        },
        {
            "affiliations": [],
            "name": "Junbin Qiu"
        },
        {
            "affiliations": [],
            "name": "Haiping Huang"
        }
    ],
    "id": "SP:29f7afff612efd7e470692da8e63da086687e2aa",
    "references": [
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "John A. Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuan-Fang Li",
                "Scott M. Lundberg",
                "Harsha Nori",
                "Hamid Palangi",
                "Marco Tulio Ribeiro",
                "Yi Zhang"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Haiping Huang"
            ],
            "title": "Eight challenges in developing theory of intelligence",
            "year": 2023
        },
        {
            "authors": [
                "Rajesh P.N. Rao",
                "Dana H. Ballard"
            ],
            "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects",
            "venue": "Nature Neuroscience,",
            "year": 1999
        },
        {
            "authors": [
                "Yanping Huang",
                "Rajesh P.N. Rao"
            ],
            "title": "Predictive coding",
            "venue": "WIREs Cognitive Science,",
            "year": 2011
        },
        {
            "authors": [
                "James C.R. Whittington",
                "Rafal Bogacz"
            ],
            "title": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity",
            "venue": "Neural Computation,",
            "year": 2017
        },
        {
            "authors": [
                "Beren Millidge",
                "Anil Seth",
                "Christopher L Buckley"
            ],
            "title": "Predictive coding: a theoretical and experimental review",
            "year": 2021
        },
        {
            "authors": [
                "Karl Friston"
            ],
            "title": "Does predictive coding have a future",
            "venue": "Nature Neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "W. Martin Usrey",
                "Karl J. Friston"
            ],
            "title": "Canonical microcircuits for predictive",
            "venue": "coding. Neuron,",
            "year": 2012
        },
        {
            "authors": [
                "Yusi Chen",
                "Huanqiu Zhang",
                "Terrence J. Sejnowski"
            ],
            "title": "Hippocampus as a generative circuit for predictive coding of future",
            "year": 2022
        },
        {
            "authors": [
                "Lin Wang",
                "Lotte Schoot",
                "Trevor Brothers",
                "Edward Alexander",
                "Lena Warnke",
                "Minjae Kim",
                "Sheraz Khan",
                "Matti H\u00e4m\u00e4l\u00e4inen",
                "Gina R Kuperberg"
            ],
            "title": "Predictive coding across the left fronto-temporal hierarchy during language comprehension",
            "venue": "Cerebral Cortex,",
            "year": 2023
        },
        {
            "authors": [
                "Siavash Golkar",
                "Tiberiu Tesileanu",
                "Yanis Bahroun",
                "Anirvan Sengupta",
                "Dmitri Chklovskii"
            ],
            "title": "Constrained predictive coding as a biologically plausible model of the cortical hierarchy",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tommaso Salvatori",
                "Yuhang Song",
                "Thomas Lukasiewicz",
                "Rafal Bogacz",
                "Zhenghua Xu"
            ],
            "title": "Predictive coding can do exact backpropagation on convolutional and recurrent neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Beren Millidge",
                "Tommaso Salvatori",
                "Yuhang Song",
                "Rafal Bogacz",
                "Thomas Lukasiewicz"
            ],
            "title": "Predictive coding: towards a future of deep learning beyond backpropagation",
            "year": 2022
        },
        {
            "authors": [
                "Zhen-Ye Huang",
                "Ruyi Zhou",
                "Miao Huang",
                "Hai-Jun Zhou"
            ],
            "title": "Energy\u2013information tradeoff induces continuous and discontinuous phase transitions in lateral predictive coding",
            "year": 2023
        },
        {
            "authors": [
                "Alexandre Pouget",
                "Jeffrey M Beck",
                "Wei Ji Ma",
                "Peter E Latham"
            ],
            "title": "Probabilistic brains: knowns and unknowns",
            "venue": "Nature neuroscience,",
            "year": 2013
        },
        {
            "authors": [
                "Haruo Kasai",
                "Noam E. Ziv",
                "Hitoshi Okazaki",
                "Sho Yagishita",
                "Taro Toyoizumi"
            ],
            "title": "Spine dynamics in the brain, mental disorders and artificial neural networks",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Tommaso Salvatori",
                "Ankur Mali",
                "Christopher L. Buckley",
                "Thomas Lukasiewicz",
                "Rajesh P.N. Rao",
                "Karl Friston",
                "Alexander Ororbia"
            ],
            "title": "Brain-inspired computational intelligence via predictive coding",
            "year": 2023
        },
        {
            "authors": [
                "Wenxuan Zou",
                "Chan Li",
                "Haiping Huang"
            ],
            "title": "Ensemble perspective for understanding temporal credit assignment",
            "venue": "Physical Review E,",
            "year": 2023
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "Jurgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "Yoshua Bengio",
                "R\u00e9jean Ducharme",
                "Pascal Vincent",
                "Christian Janvin"
            ],
            "title": "A neural probabilistic language model",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2003
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2014
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
            "year": 2014
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "KyungHyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "year": 2014
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "In ICLR 2015 : International Conference on Learning Representations",
            "year": 2015
        },
        {
            "authors": [
                "Yair Lakretz",
                "German Kruszewski",
                "Theo Desbordes",
                "Dieuwke Hupkes",
                "Stanislas Dehaene",
                "Marco Baroni"
            ],
            "title": "The emergence of number and syntax units in lstm language models",
            "year": 1903
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Mary Ann Marcinkiewicz",
                "Beatrice Santorini"
            ],
            "title": "Building a large annotated corpus of english: The penn treebank",
            "venue": "Comput. Linguist.,",
            "year": 1993
        },
        {
            "authors": [
                "Chan Li",
                "Haiping Huang"
            ],
            "title": "Learning credit assignment",
            "venue": "Physical Review Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Charlotte Caucheteux",
                "Jean-R\u00e9mi King"
            ],
            "title": "Brains and algorithms partially converge in natural language processing",
            "venue": "Communications Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Mahowald",
                "Anna A. Ivanova",
                "Idan A. Blank",
                "Nancy Kanwisher",
                "Joshua B. Tenenbaum",
                "Evelina Fedorenko"
            ],
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "year": 2023
        },
        {
            "authors": [
                "A. Aldo Faisal",
                "Luc P.J. Selen",
                "Daniel M. Wolpert"
            ],
            "title": "Noise in the nervous system",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2008
        },
        {
            "authors": [
                "Robert Rosenbaum"
            ],
            "title": "On the relationship between predictive coding and backpropagation",
            "venue": "Plos one,",
            "year": 2022
        },
        {
            "authors": [
                "Jo\u00e3o Sacramento",
                "Rui Ponte Costa",
                "Yoshua Bengio",
                "Walter Senn"
            ],
            "title": "Dendritic cortical microcircuits approximate the backpropagation algorithm",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Boris Barbour",
                "Nicolas Brunel",
                "Vincent Hakim",
                "Jean-Pierre Nadal"
            ],
            "title": "What can we learn from synaptic weight distributions",
            "venue": "Trends in Neurosciences,",
            "year": 2007
        },
        {
            "authors": [
                "Haiping Huang"
            ],
            "title": "Role of zero synapses in unsupervised feature learning",
            "venue": "Journal of Physics A: Mathematical and Theoretical,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2014
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2,",
            "year": 2013
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler",
                "Ed H. Chi",
                "Tatsunori Hashimoto",
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Haiping Huang"
            ],
            "title": "Statistical Mechanics of Neural Networks",
            "year": 2022
        },
        {
            "authors": [
                "Luca Pinchetti",
                "Tommaso Salvatori",
                "Yordan Yordanov",
                "Beren Millidge",
                "Yuhang Song",
                "Thomas Lukasiewicz"
            ],
            "title": "Predictive coding beyond gaussian distributions. arXiv:2211.03481, 2022",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Terrence J. Sejnowski"
            ],
            "title": "Large Language Models and the Reverse Turing Test",
            "venue": "Neural Computation,",
            "year": 2023
        },
        {
            "authors": [
                "Jie Zhao",
                "Biwei Xie",
                "Xingquan Li"
            ],
            "title": "Weight uncertainty in transformer network for the traveling salesman problem",
            "venue": "In 2023 International Symposium of Electronics Design Automation (ISEDA),",
            "year": 2023
        },
        {
            "authors": [
                "Nicolas Zucchet",
                "Seijin Kobayashi",
                "Yassir Akram",
                "Johannes von Oswald",
                "Maxime Larcher",
                "Angelika Steger",
                "Joao Sacramento"
            ],
            "title": "Gated recurrent neural networks discover attention",
            "year": 2023
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are RNNs: Fast autoregressive transformers with linear attention",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Large language models based on self-attention mechanisms have achieved astonishing perfor-\nmances not only in natural language itself, but also in a variety of tasks of different nature.\nHowever, regarding processing language, our human brain may not operate using the same prin-\nciple. Then, a debate is established on the connection between brain computation and artificial\nself-supervision adopted in large language models. One of most influential hypothesis in brain\ncomputation is the predictive coding framework, which proposes to minimize the prediction error\nby local learning. However, the role of predictive coding and the associated credit assignment in\nlanguage processing remains unknown. Here, we propose a mean-field learning model within the\npredictive coding framework, assuming that the synaptic weight of each connection follows a spike\nand slab distribution, and only the distribution, rather than specific weights, is trained. This meta\npredictive learning is successfully validated on classifying handwritten digits where pixels are input\nto the network in sequence, and moreover on the toy and real language corpus. Our model reveals\nthat most of the connections become deterministic after learning, while the output connections\nhave a higher level of variability. The performance of the resulting network ensemble changes con-\ntinuously with data load, further improving with more training data, in analogy with the emergent\nbehavior of large language models. Therefore, our model provides a starting point to investigate\nthe connection among brain computation, next-token prediction and general intelligence.\n\u2217Equal contribution. \u2020Electronic address: huanghp7@mail.sysu.edu.cn\nar X\niv :2\n30 9.\n04 10\n6v 2\n[ cs\n.C L\n] 9\nO ct\nI. INTRODUCTION\nLarge language models (LLMs) based on transformer structures greatly boost both industrial and academic interests in artificial general intelligence [1]. LLMs are able to achieve state-of-art performances in a variety of different tasks, only trained by next-token prediction. The transformer structure computes self-attention scores to capture statistical correlations among input tokens in parallel, which is in stark contrast to brain-like recurrent computation based on synaptic feedback in temporal depth (e.g., a short working memory). In addition, LLMs typically require a sizable number of corpus to trigger emergence of intelligence, compared to the the fact that much less data is needed for a child to acquire linguistic ability. Therefore, it is necessary to establish a mechanistic model of language processing to understand the biological plausible mechanism and underlying physics law governing phase transitions, through statistical patterns of model hyperparameters [2].\nIn brain science, predictive coding is one of the most influential hypothesis that can implement hierarchical information processing [3, 4]. The predictive coding derives the neuroplasticity rule based on local error signal [5], whose goal is to minimize the surprise between the prediction and belief of a generative model of the outside world [6]. The framework of predictive coding has several benefits for theoretical research. First, the framework can be derived from the first principle that the brain is a biological machine of optimizing neural dynamics and synaptic connections to maximize the evidence of its internal model of the outside world [7]. Second, this principle shares exactly the same spirit adopted in variational free energy framework [6]. Recently, there appeared intense interests in studying the biological implementation of this hypothesis [8\u201310], in developing algorithmic applications [11\u201313], and in studying the trade-off between energy minimization and information robustness in a linear model of lateral predictive coding [14].\nThe predictive coding postulates that the cortex carries out a predictive model, where the incoming sensory signals are predicted by using prediction-error driven learning and inference. In this sense, predictive coding is a nice framework to model language processing. However, weight uncertainty is commonly observed in neural circuits [15, 16], e.g., synaptic transmission is stochastic, and spine size is subject to fluctuation. But these effects were not taken into account in previous studies of predictive coding, as remarked in a recent review [17]. In addition, the weight uncertainty was recently studied in recurrent neural\nnetworks [18], inspiring fluctuation-driven synaptic plasticity. Therefore, exploring how the weight uncertainty affects predictive coding in language processing, will help to establish a mechanistic model of language processing to understand the biological plausible mechanism and underlying physics law governing phase transitions, through associated statistical patterns of model hyperparameters. In this work, we derive a mean-field learning rule for predictive coding in recurrent neural networks (RNNs), which is a fundamental structure for nature language processing [19\u201325], and we assume that each direction of connection follows a weight distribution incorporating weight sparsity and variance. We thus call this rule meta predictive learning (MPL). This framework is tested first on the classification of MNIST dataset [26] where pixels in an image are divided into groups, and then a toy language dataset, where we can have a thorough exploration of algorithmic capabilities, and finally a language corpus in the real world (Penn Treebank corpus [27]).\nOur proposed MPL achieves equal or even better performance compared with traditional methods in all three tasks, showing the advantage of ensemble predictive coding, since examples of single networks can be readily sampled from the trained distribution [18, 28]. By analyzing the distribution of hyperparameters, we are able to find that most connections are deterministic in the input and recurrent layer, while the output layer has a higher level of variability. The observation that the output connections bear a higher level of variability is a universal result in all three tasks, which may particularly connect to the generative function of the language processing model. The network performance changes non-linearly and continuously with data load \u03b1 = M N , where M is the training data size and N is the number of neurons in the circuit, and we found that the critical point is given by \u03b1c \u2248 0.02, beyond which a chance level of prediction is absent. With increasing the size of training data, the performance further improves until a perfect learning is achieved. We can then test the resulting network to generate text of arbitrary length (to create something is a first step to understand that thing), and the generated text follows perfectly the grammatical rule set before training. In addition, our MPL is able to accomplish comparable performances in the Penn Treebank corpus with other training methods in RNN, although the framework is less accurate than the transformer structure, which thereby calls for further studies about the mechanistic difference between biological learning and non-biological transformer learning, and how the latter one can inspire discovery of new fundamental elements of computation that can realize logical and mathematical reasoning in many different tasks [29, 30]."
        },
        {
            "heading": "II. METHOD",
            "text": "Here we consider meta predictive learning in a vanilla recurrent neural network (RNN), which processes a time-dependent sequence x with time length T . The input signal of Nin dimension is first mapped to the recurrent reservoir of N neurons by an input weight matrix win \u2208 RN\u00d7Nin , whose element winij indicates the connection weight value from neuron j in the input to the reservoir neuron i. The neurons in the reservoir interact with each other with reciprocal connections w \u2208 RN\u00d7N , where elements wij specify the directional coupling from neuron j to neuron i, and moreover wij \u0338= wji. The self-connectivity wii is also included, and can be learned without imposing any prior knowledge [18]. The internal neural dynamics ri(t) is read out via the output weight w out \u2208 RNout\u00d7N . In the predictive learning setting, r is interpreted as a belief state when x is observed as a sensory input, which can be continuously updated to match the actual prediction whose dynamics reads as follows,\nhi(t) = N\u2211 j=1 wijf (rj(t\u2212 1)) + Nin\u2211 j=1 winijxj(t),\nyi(t) = \u03d5 ( N\u2211 j=1 woutij f (rj(t)) ) ,\n(1)\nwhere yi(t) is the i-th component of the network output, f(\u00b7) denotes the non-linear activation function, and we use the ReLU function for all tasks. \u03d5(\u00b7) is the output nonlinear function, and we use softmax function to specify the probability over all classes, which is defined as \u03d5(zk(t)) = ezk(t)\u2211 j e zj(t) . The belief state r(t) is updated for all time steps up to the sequence length to minimize the prediction error between r and h, which will be detailed below. Note that r(0) = 0, and we fix the belief of the output node ry = y\u0302, where y\u0302 denotes the label of input x. Generally speaking, all beliefs can be initialized to random values.\nThe core idea of the proposed MPL is assuming the distribution of the network parameters\nis subject to the following spike and slab (SaS) form [18, 28],\nP ( winij ) = \u03c0inij \u03b4 ( winij ) + ( 1 \u2212 \u03c0inij ) N  minij Nin ( 1 \u2212 \u03c0inij ) , \u039einij Nin ( 1 \u2212 \u03c0inij )  ,\nP (wij) = \u03c0ij\u03b4 (wij) + (1 \u2212 \u03c0ij)N (\nmij N (1 \u2212 \u03c0ij) , \u039eij N (1 \u2212 \u03c0ij)\n) ,\nP ( woutki ) = \u03c0outki \u03b4 ( woutki ) + ( 1 \u2212 \u03c0outki ) N\n( moutki\nN ( 1 \u2212 \u03c0outki ) , \u039eoutki N ( 1 \u2212 \u03c0outki\n)) . (2)\nNote that N(1 \u2212 \u03c0) specifies the mean degree of each neuron in the reservoir, as 1 \u2212 \u03c0 specifies the synaptic connection probability, which is biological plausible due to unreliable stochastic noise [31]. The first and second moments of elements w\u2113ij (\u2113 is in, out, or recurrent depending on the context; for the recurrent context, the item has no super- or subscript) can be derived as \u00b5\u2113ij = m\u2113ij N\u2113 , \u03f1\u2113ij = (m\u2113ij) 2\nN2\u2113 (1\u2212\u03c0\u2113ij) +\n\u039e\u2113ij N\u2113 , respectively. Note that the mean and\nvariance of the Gaussian slab are scaled by the number of mean synaptic connections, such that the prediction of neuron is of the order one.\nConsidering the statistics of synaptic weights and a large number of afferent projections for each neuron in Eq. (1), which is true in real neural circuits [32], we can reasonably assume the prediction hi(t) (\u2200i) follows an evolving Gaussian distribution whose mean and variance are defined by Gi = G rec i + G in i and \u2206 1 i = (\u2206 in i ) 2 + (\u2206reci ) 2, respectively. This is intuitively the result of central limit theorem. The statistics of readout neural currents can be derived in a similar way. Therefore, the mean-field dynamics of this model can be written as\nhi(t + 1) = G rec i (t) + G in i (t + 1) + \u03f5 1 i (t + 1) \u221a( \u2206ini (t + 1) )2 + (\u2206reci (t)) 2,\nyk(t) = \u03d5 ( Goutk (t) + \u03f5 2 k(t)\u2206 out k (t) ) ,\n(3)\nwhere the superscript in \u03f5 indicates different types of standard Gaussian random variables\u2014 one for reservoir neurons (with superscript 1) and the other for readout neurons (with superscript 2). By definition, {\u03f51,2i (t)} are both time and neuron index dependent. Given \u00b5\u2113ij and \u03f1 \u2113 ij, the mean currents together with the associated fluctuations are derived below,\nGini (t+ 1) = \u2211 j \u00b5inijxj(t+ 1)\nGreci (t+ 1) = \u2211 j \u00b5ijf (rj(t+ 1))\nGoutk (t+ 1) = \u2211 j \u00b5outkj f (rj(t+ 1))\n( \u2206ini (t+ 1) )2 = \u2211 j ( \u03f1inij \u2212 ( \u00b5inij )2) (xj(t+ 1)) 2 (\u2206reci (t+ 1)) 2 =\n\u2211 j ( \u03f1ij \u2212 (\u00b5ij)2 ) (f (rj(t+ 1))) 2\n( \u2206outk (t+ 1) )2 = \u2211 j ( \u03f1outkj \u2212 ( \u00b5outkj )2) (f (rj(t+ 1))) 2 .\n(4)\nThe prediction dynamics (Eq. (1) or Eq. (3) in the meta learning context) can be inter-\npreted as perceptual inference, widely used in energy-based optimization of brain dynamics [7], while the learning given below is called the neuroplasticity. Both processes minimize exactly the same energy (or variational free energy in general [5]).\nPredictive learning can be derived from a temporally hierarchical Gaussian probabilistic principle [4, 5], where the objective function is given by the negative log-likelihood of the joint neural-state distribution. To optimize this objective function, we apply a mean-field approximation of the joint distribution and an additionally Laplace approximation that leads to Gaussian forms [17]. We give a brief interpretation in appendix A. In essence, the predictive learning maximizing this log-likelihood aims to minimize the following energy cost [12, 33],\nF = 2\u2211\nj=1 T\u2211 t=1 Ej(t). (5)\nThis energy function is exactly the variational free energy in the above Gaussian probabilistic principle. The choice of Ej(t) depends on the problem at hand. If the network produces an output at every time step as in the language model, E1(t) = 1 2 \u2225r(t) \u2212 h(t)\u22252, and E2(t) =\n\u2212 \u2211\ni y\u0302i(t) ln (yi(t)). However, if the network only makes the final decision in the last time\nstep as in the classification task, i.e., yk(T ) = \u03d5 (G out k (T ) + \u03f5 2 k(T )\u2206 out k (T )), we then have the energy terms E1(t) = 1 2 \u2225r(t)\u2212h(t)\u22252 for t = 1, . . . , T \u22121, E1(T ) = 0, and E2(t) = 0 for t < T ,\nE2(T ) = \u2212 \u2211 i y\u0302i ln (yi(T )). Moreover, we define the prediction error E \u2032 1(t) = r(t)\u2212h(t) and E \u20322(t) = ry(t)\u2212y(t). This error can be propagated along the dendritic connections in neural circuits [34]. In mathematical sense, the prediction errors can be interpreted as the gradients of the above energy cost.\nIn essence, the predictive learning consists of three phases: inference phase, learning phase, and prediction phase (see Eq. (1), and in the current meta-learning, Eq. (3) is used). We next show the predictive learning details for the language processing, while other applications can be readily adapted. First of all, during the inference phase, the belief r(t) is\nupdated to minimize the energy function F with the following increment,\n\u2206ri(t \u2032) = \u2212\u03b3 \u2202F\n\u2202ri(t\u2032)\n= \u2212\u03b3 \u2202E1(t \u2032)\n\u2202ri(t\u2032) \u2212 \u03b3\n\u2202 \u2211\nt\u0338=t\u2032 E1(t) \u2202ri(t\u2032) \u2212 \u03b3 \u2202E2(t\n\u2032)\n\u2202ri(t\u2032) = \u2212\u03b3E \u20321,i(t\u2032) + \u03b3 \u2211 j E \u20321,j(t \u2032 + 1) \u2202hj(t \u2032 + 1) \u2202ri(t\u2032)\n+ \u03b3 \u2211 j E \u20322,j(t \u2032) \u2202[Goutj (t \u2032) + \u03f52j (t \u2032)\u2206outj (t \u2032)] \u2202ri(t\u2032)\n= \u2212\u03b3E \u20321,i(t\u2032) + \u03b3f \u2032 ( ri(t \u2032) )\u2211\nj\nE \u20321,j(t \u2032 + 1)\u00b5ji\n+ \u03b3 \u2211 j E \u20322,j(t \u2032)\u00b5outji f \u2032(ri(t \u2032)) + \u03b3 \u2211 j E \u20321,j(t \u2032 + 1)\u03f5\u03021ji + \u03b3 \u2211 j E \u20322,j(t \u2032)\u03f5\u03022ji,\n(6)\nwhere \u03b3 indicates the learning rate for the inference phase (we choose \u03b3 = 0.1 for all tasks), \u03f5\u03021ji = \u03f5 1 j (t\n\u2032 + 1) (\u03f1ji\u2212(\u00b5ji)2)f \u2032(ri(t\u2032))f(ri(t\u2032))\u221a (\u2206inj (t\u2032+1)) 2 +(\u2206recj (t\u2032)) 2 , and \u03f5\u0302 2 ji = \u03f5 2 j (t \u2032)\n( \u03f1outji \u2212(\u00b5outji ) 2 ) f \u2032(ri(t\u2032))f(ri(t\u2032))\n\u2206outj (t \u2032)\n. It\nis evident that the last two terms in Eq. (6) are related to the fluctuations caused by the network statistics. The interplay between the network statistics and the prediction errors governs the belief dynamics, which was not considered in previous studies. We emphasize this intrinsic property of neural dynamics is due to ongoing fluctuations of synaptic weights in the presence of circuit noise [31]. Equation (6) thus addresses how the neural belief is shaped under the fluctuating circuit environment.\nThe goal of this inference process is to find the best configuration of belief for synaptic weight modifications (aforementioned neuroplasticity). When the decrease of the energy F becomes stable, e.g., |F t \u2212 F t\u22121| < 0.1, or when a maximal number of iterations (n in our algorithm 1) is approached, the learning phase starts, i.e., the hyperparameters [m\u2113,\u03c0\u2113,\u039e\u2113] are updated based on the local error signal E \u2032j (t) with the following increments,\n\u2206m\u2113ij = \u2212\u03b7 \u2202F \u2202m\u2113ij = \u2212\u03b7 \u2211 t E \u2032\u2113\u2032,i(t) [ \u2212 1 N\u2113 \u03be\u2113j \u2212 \u03f5\u2113 \u2032 i (t) m\u2113ij\u03c0 \u2113 ij ( \u03be\u2113j )2 (N \u2113)2(1\u2212 \u03c0\u2113ij) \u221a \u2206\u2113 \u2032 i ] ,\n\u2206\u03c0\u2113ij = \u2212\u03b7 \u2202F \u2202\u03c0\u2113ij = \u2212\u03b7 \u2211 t E \u2032\u2113\u2032,i(t) [ \u2212\u03f5\u2113\u2032i (t)\n( m\u2113ij )2 ( \u03be\u2113j )2\n2(N\u2113)2 ( 1\u2212 \u03c0\u2113ij )2\u221a \u2206\u2113 \u2032 i\n] ,\n\u2206\u039e\u2113ij = \u2212\u03b7 \u2202F \u2202\u039e\u2113ij = \u2212\u03b7 \u2211 t E \u2032\u2113\u2032,i(t) [ \u2212\u03f5\u2113\u2032i (t) ( \u03be\u2113j )2 2N\u2113 \u221a \u2206\u2113 \u2032 i ] ,\n(7)\nwhere \u03b7 denotes the learning rate for the learning phase, \u22061i = ( \u2206ini (t) )2 + (\u2206reci (t\u2212 1)) 2\nand \u22062i = (\u2206 out i (t)) 2. To derive Eq. (7), the chain rule and mean-field dynamics [Eq. (3)] are used. The meaning of superscripts depends on the network structure where the computation is carried out. If \u2113 = in, \u2113\u2032 = 1, \u03be\u2113j = xj(t), N\u2113 = Nin; if \u2113 indicates the recurrent reservoir, \u2113\u2032 = 1, \u03be\u2113j = f(rj(t \u2212 1)), N\u2113 = N ; if \u2113 = out, \u2113\u2032 = 2, \u03be\u2113j = f(rj(t)), N\u2113 = N . For an easy comprehension, we summarize all mathematical items and associated explanations in appendix D. The dynamics of \u03c0 and \u039e is purely driven by the synaptic fluctuation, while the m dynamics is contributed by the activity (belief or sensory observation) and the synaptic fluctuation. The m yields impacts on \u03c0 and \u039e as well. Note that the vanilla predictive coding does not take into account synaptic fluctuations (see also appendix C), which is indeed ubiquitous in neural circuits [16]. One typical source is that the synaptic noise results from noisy biochemical processes underlying synaptic transmission, while the other source is the fluctuation of spine sizes in the neocortex, and the existence of silent synapses [35, 36].\nIn practice, implementation of the meta learning rule in Eq. (7) follows immediately the inference phase, where the update of belief r has made F converge. To improve the prediction performance, the inference and learning phases are repeated a number of times. Prediction phase is carried out after a round of inference-learning loop, to test the model\u2019s generalization performance. Three phases can be concisely represented by the pseudocode in Alg. 1. Codes to reproduce the numerical results provided in the next section are available in our GitHub [37].\nAlgorithm 1 Meta predictive learning algorithm 1: # Inference\n2: Given: input x, label y\u0302, randomly initialized belief r, ry = y\u0302, standard Gaussian variables \u03f5 1\nand \u03f52\n3: for iter = 1, . . . , n do\n4: for t = 1, . . . , T do 5: hi(t + 1) = G rec i (t) + G in i (t + 1) + \u03f5 1 i (t + 1) \u221a( \u2206ini (t + 1) )2 + (\u2206reci (t)) 2;\n6: yk(t) = \u03d5 ( Goutk (t) + \u03f5 2 k(t)\u2206 out k (t) ) ; 7: r(t) = r(t) + \u2206r(t).\n8: end for\n9: end for\n10: # Learning\n11: for \u2113 = in, out, recurent do\n12: for t = 1, . . . , T do 13: m\u2113 = m\u2113 + \u2206m\u2113; 14: \u03c0\u2113 = \u03c0\u2113 + \u2206\u03c0\u2113; 15: \u039e\u2113 = \u039e\u2113 + \u2206\u039e\u2113.\n16: end for\n17: end for\n18: Output: r.\n19: # Prediction 20: Given: test data x, converged belief r, another set of standard Gaussian variables \u03f51 and \u03f52\n21: for t = 1, . . . , T do 22: hi(t + 1) = G rec i (t) + G in i (t + 1) + \u03f5 1 i (t + 1) \u221a( \u2206ini (t + 1) )2 + (\u2206reci (t)) 2;\n23: yk(t) = \u03d5 ( Goutk (t) + \u03f5 2 k(t)\u2206 out k (t) ) ; 24: end for\n25: Output: y."
        },
        {
            "heading": "III. RESULTS AND DISCUSSION",
            "text": "In this section, we first apply the MPL in the digit classification task, where an MNIST digit of 784 pixels is divided into a sequence of pixels, and subgroups of 28 pixels are input to the network at each single time step. As a proof of concept, the first example is to show our framework can be applied to any computational tasks of temporal structures. Then, we extend the application to two language processing tasks; one is at the toy level and the other is the real corpus.\nA. MNIST digit classification\nThe recurrent neural network is trained to classify an input image after 28 time steps, seeing 28 pixels at each time step. This task requires long-term memory, because the recurrent neural network makes the final decision only after seeing all the pixels, and the information in the previous time steps (up to 28 steps before) must be stored and processed in the last step. To carry out this task, we use a vanilla RNN with N = 100 recurrent neurons, Nin = 28 input units and Nout = 10 output nodes indicating the output class in the one-hot form. The entire dataset is divided into several batches, and we use stochastic gradient descent (SGD) in the learning phase to update hyperparameters [m\u2113,\u03c0\u2113,\u039e\u2113], and adam optimizer is applied [38]. Despite working on the network ensemble level and the fact that weight uncertainty must be taken into account during inference, learning and prediction phases, our model can achieve better and more stable performances than the predictive coding without any distribution training [Fig. 1 (a)]. As expected, the overall energy F consistently decreases during training and reaches the point near zero in the late stage of training.\nThe macroscopic behavior of the network is corroborated by the statistical pattern of model hyperparameters underlying synaptic weights. The weight uncertainty characterized by hyperparameters [\u039e\u2113,\u03c0\u2113] decreases over training, showing that the weight is becoming\nmore deterministic, and we use the average value, e.g., \u27e8\u039ein\u27e9 = 1N\u00d7Nin \u2211 ij \u039e in ij to compute the average uncertainty level (for the mean m, we take its absolute value before the average is carried out). Interestingly, the uncertainty level is highest in the output layer, which is in striking contrast to the results obtained by a generalized backpropagation through\ntime (rather than local learning guided by prediction error considered in the current work) at the ensemble level [18] where the uncertainty is highest in the recurrent layer. From the predictive coding perspective, the readout weight has more flexibility to extract the information in the reservoir, which may be due to the local nature of learning that is driven by minimizing the prediction error. This remarks that a more biological plausible training may lead to different interpretations of the same computational tasks as implemented in neural circuits. Therefore, to reveal biological mechanisms, a biological plausible training is a necessary ingredient.\nB. Toy language model\nReal language corpus is commonly complicated, and is not simple for theoretical studies. To build a metaphor of the complicated natural language, we set up a generative process where a text (a stream of tokens) is generated through a fixed rule (similar to grammar).\nFollowing this setting, the artificial corpus consists of M texts of length T each, and each text is composed of letters from a, b, c, . . . , z. A periodic boundary is applied. For example, a single sample x = {a, c, g, i, ...} is generated according to the grammatical rule that starting from letter \u2032a\u2032, only the letter \u2032c\u2032 or \u2032e\u2032 which is located two letters or four letters next to \u2032a\u2032 (with equal probabilities) can follow \u2032a\u2032, and the case of two consecutive \u2032c\u2032 is not allowed. This rule for generating toy language is just a simple model of real corpus, but non-trivial enough for a neural network to learn the embedded rule. The generated examples (letter sequences) are shown to the neural network, which is required to discover the rule by our meta predictive learning working on next-letter prediction. After training, the network is tested by generating a sequence of arbitrary length following the same rule. A hierarchical compositional structure can also be incorporated into the generation process, but we leave this more interesting case to future studies based on this toy setting.\nA RNN with N = 100, Nin = 26, Nout = 26 is trained on the full dataset following the above rule, with a total of 26624 (calculated as 26\u00d72T\u22121) sequences of length T = 11 (other values of T can also be similarly studied), and SGD with adam optimizer is applied [38]. To detect a possible phase transition with increasing data size, we can use an increasing portion of the entire dataset (i.e., M < 26624). Each letter can be encoded into one-hot form before being input to the network, while the readout implements a decoding of the neural activity into one-hot form as well. Because of simplicity in our letter space, we do not need word embedding as commonly used in language processing [39]. In Fig. 2 (a), we can easily generate a sequence of arbitrary length, by supplying a network with the letter generated in the previous time step, and the trained network (in the ensemble sense) successfully generates sequences following the ground truth rule. Interestingly, the well-trained network also generates sequences with length T > 11 following the same rule, suggesting the possibility that the network output could be creative to generate new grammatically correct sequences.\nTo study the emergence behavior of this simplified language model, we define the correct letter ratio to characterize the language generating ability of our model. After training, the network instance (sampled from the ensemble) is required to generate 26 sequences of length T = 11 whose first letters are one of all 26 letters of the alphabet, and the correct letter ratio is defined as the average ratio of correctly predicted letters. For example, the sequence [\u2032a\u2032,\u2032 c\u2032,\u2032 e\u2032,\u2032 g\u2032,\u2032 k\u2032,\u2032m\u2032,\u2032 o\u2032,\u2032 s\u2032,\u2032 w\u2032,\u2032 a\u2032,\u2032 z\u2032] has 9 correctly predictions, with ratio 0.9 (in total the\nnetwork has to predict 10 letters) for this single sequence. Therefore, the correct letter ratio indicates the language generating ability of the network ensemble, with a maximal value of 1 (100%). In Fig. 2 (b), we can easily see that the correct letter ratio first remains at a very low level (close to chance level) if the data load \u03b1 = M N is small, i.e., the generated sequences are random when \u03b1 < 0.02. Beyond this threshold, the performance continuously improves, exhibiting the phenomenon of a second-order phase transition, which coincides qualitatively with empirical results of emergence discovered in large language models [40, 41]. The scaling exponent of the correct letter ratio (order parameter in statistical mechanics [42]) around the transition point is about 1.14. A rigorous derivation of this critical exponent is left for future analytic works. Training RNN with different network sizes yields qualitatively same behavior, but a larger network size makes the transition sharper. After the transition, the network assigns the correctly predicted letter with a larger probability than other letter candidates, while the possibilities for other letters are significantly suppressed (see Fig. 3). Another important characteristic is that, the learning with increasing data occurs first rapidly, followed by a slow period, and finally the performance is saturated to the perfect\ngeneralization of the language rule. This may be interpreted as a hierarchical decoding of the information embedded in the noisy (stochasticity in the generation process) sequences. We further remark that, after a full dataset of sequences with fixed length (e.g., T = 11) is trained, the network is able to generate the grammatically correct letter sequences of arbitrary length [see the inset of Fig. 2 (c)]. The energy of the language model is also decreasing with training until getting stationary, which emphasizes the important role of energy-based model to understand recurrent language processing. A further extension of meta predic-\ntive learning to transformer structure is possible, as the Gaussian assumption used in the standard predictive coding can be gone beyond in a recent work [43].\nTo study the properties of this simplified language model, we plot the distribution of hyperparameters [\u03c0,m,\u039e] for the input layer, output layer, and recurrent layer, respectively. The distribution of [\u03c0,\u039e] has the L-shape in all layers, while the output layer allows for more variability in both sparsity and variance of the Gaussian slab, which is characterized\nby a slightly broader distribution of [\u03c0,\u039e]. Extremes \u03c0 = 0, \u03c0 = 1 and \u039e = 0 have particular physics significance. \u03c0 = 0 indicates the connection has no sparsity, and thus carries important information for the task. The spike mass at \u03c0 = 1 implies that the connection is always zero, and thus is not important for the task, but none of the connections of our model belong to this case. \u039e = 0 shows the corresponding connection is deterministic, because the corresponding Gaussian distribution reduces to a Dirac delta peak. This result is also observed in the 28 by 28 MNIST classification task. The distribution of hyperparameter m is broadest in the output layer, ranging from \u2212200 to 200, showing the higher-level variability in the connection weight of the output layer. This phenomenon may have close relationship with the fact that the embedded rule can only be retrieved by using a highly heterogeneous weighting of each neuron\u2019s activity in the reservoir, which is particularly interesting from the perspective of neural decoding of language information and probabilistic computation in a biological plausible setting [10, 15, 30], since our embedded rule is actually a probabilistic generative rule mixed with a predefined grammatical structure.\nC. Experiments on natural language\nIn this section, we apply our MPL algorithm to a more complex language corpus, i.e., Penn Treebank (PTB) corpus [27], which contains nearly 50000 sentences collected from the Wall Street Journal. The PTB is one of the most known and used corpus for word-level language modeling.\nDue to the large space of the corresponding vocabulary, the corpus need to be preprocessed using word embedding techniques before sending the sentences into the network [39]. Here we describe the main steps. The first step is to use a tokenizer tool to split the sentences into tokens and replace the useless words or characters with a special token named <unk>, indicating an unknown token. In addition, the tokens that appeared less than five times in the whole corpus will be replaced with <unk> to help the network concentrate on the major tokens of high frequency. Next, we collect all tokens to generate a vocabulary to store all the different tokens. However, directly inputting the tokens (treated as one-hot vectors) into the network is inconvenient when the size of vocabulary is large. Hence, we set up a look-up table called embedding layer, to transform every token into vectors in a low-dimensional feature space via neural networks. The training goal is to\nlearn word vector representations that are able to predict the nearby words. Rows of the trained encoding matrix gives the distributed representation of words. The embedding layer is trained by traditional back-propagation algorithm [39], while both the recurrent reservoir and the readout layer are trained by our MPL as described above (or other alternatives if comparison among algorithms is made).\nThe overall energy for the predictive learning is given below,\nF = 1 2 \u2211 t ||E \u2032rec(t)||2 + L, (8)\nwhere E \u2032rec denotes the prediction error for the recurrent reservoir, L(y, ry) = \u2212 \u2211\nt \u2211 i(ry)i(t) ln yi(t) is related to the readout error. In order to measure the accuracy of\nthe language model, we use perplexity metric which measures how well the model predicts the next token, i.e., the uncertainty about the prediction, precisely given by [20]\nppl = [ T\u220f i=1 p(wi|wi\u22121, \u00b7 \u00b7 \u00b7 , w0) ]\u2212 1 T . (9)\nIt is intuitive that minimizing the perplexity is equivalent to maximizing the probability of a corpus which is composed of T words indicated by {w0, w1, . . . , wT}. Because the output of our model yi(t) is actually the prediction probability in the non-linear softmax form, we can recast the perplexity as ppl = eL, where L represents the cross-entropy objective used to train the network.\nWe apply our MPL to model this real corpus with comparison among other competitive algorithms (see Fig. 5). The test perplexity obtained by backpropagation through time with/without meta learning reaches a similar level to those obtained by standard predictive coding and our MPL method, after tens of epochs. A salient feature is that, those trainings without the SaS premise, get easily overfitted at later stages of training. For comparison, the transformer network with self-attention blocks (see appendix B for details), achieves the lowest perplexity among all considered methods, which demonstrates that the current biological recurrent computation has still a large gap to the artificial transformer computation, where the input tokens are not shown to the network in sequence, but in a form of single block, such that the self-attention is able to integrate information from different parts of the input. This also implies that, new elements, e.g., attention mechanisms, possibly in\nto-be-revealed biological forms, might be added to our current framework, to minimize the gap on one hand, and on the other hand to develop a biological computational model of intelligent systems that can handle natural language, particularly without relying on long working memory and an astonishingly large corpus.\nTo study the network behavior, we plot the distribution of hyperparameters m, \u03c0, \u039e when the RNN network is trained with the MPL method, as shown in the Fig. 6. We find that the mean weight m for all layers is symmetrically distributed around zero, with a relatively narrow distribution. The distribution of \u03c0 for all layers is of an L-shape and peaks at \u03c0 = 0, indicating a dense network is favored and formed after learning. The distribution of \u039e is of the U-shape and has two peaks. One peak is at \u039e = 0, indicating that these weights are deterministic and could only take a single value of m, and the other peak is at \u039e \u2243 0.01, indicating that the corresponding connection can carry a range of candidate values. Currently, it remains unknown how to relate these microscopic details of the network structure to the decoding of the semantic information in the corpus. It is thus important in future works to design analytically tractable model of language processing bridging neurophysiological plausibility and superperformance observed in the state-of-theart architectures, which would help to uncover key neuron, synapse, and circuit motif types in the human brain."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "Predictive coding is a prediction-error driven learning with local updates, performing a joint process of both inference and learning, thereby being a potential candidate for how the brain builds an internal generative model of the complex evolving outside world [2]. We take the predictive coding within the language processing context, which is currently attracting an intense research interests due to ChatGPT [1]. We address a meta predictive learning mechanism in recurrent neural networks encoding and predicting tokens in text sequences, in the presence of uncertainty. A continuous phase transition is revealed in our model, and perfect generation can be achieved after a sufficient number of training sequences are provided. Therefore, our toy model provides a good starting point to dissect the mechanism of learning in language processing [2].\nOur MPL framework is relatively not prone to overfitting in training real corpus. In\naddition, there emerge intriguing statistical patterns of hyperparameters in our networks. However, it remains unclear how these statistical properties explain the performance (e.g., accuracy in next-token predictions) of the recurrent computation., which highly resembles what occurs in human brains. In contrast, the self-attention leveraged in transformer networks is not biological (e.g., not recurrent and non-local learning). Nevertheless, the transformer structure leads to emergence of intelligence to some extent, and in particular the phenomenon of in-context learning, where the trained network can perform novel tasks by a prompt of example demonstrations without any further learning. The ability of in-context learning emerges by only scaling models and computation costs [41]. The deviation from known brain computation for language processing triggers a hot debate on what the nature of intelligence is [44], and whether the intelligence can be achieved by next-token prediction [30]. More precisely, how a compressed representation of hierarchical compositional structure in linguistic data can be achieved by biological learning (or resulting in multi-task performances beyond transformer) remains largely mysterious. Our current study shows that meta predictive learning for language processing may be a fruitful route towards this goal.\nA recent work demonstrated that the weight uncertainty with the form of SaS structure can be also incorporated into the transformer [45]. In addition, gated recurrent neural networks with multiplicative mechanisms were recently shown to be able to learn to implement linear self-attention [46]. Furthermore, the relationship between linear transformers allowing for faster autoregressive learning and RNNs was established in a recent work [47]. Taken together, our current work would be a starting point to establish the bridge between the biological learning (towards the science of specialized brain circuits) and transformer learning within the seminal predictive coding hypothesis, which can be put in the theoretically solid variational free energy minimization conceptual framework.\nAcknowledgments\nThis research was supported by the National Natural Science Foundation of China for Grant Number 12122515 (H.H.), and Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices (No. 2022B1212010008), and Guangdong Basic and Applied Basic Research Foundation (Grant No. 2023B1515040023).\nAppendix A: Interpretation of predictive coding as variational free energy mini-\nmization\nFor a recurrent dynamics, we write the neural activity at each step as a latent variable r(t), and then it is reasonable to assume the joint probability of a trajectory can be written into the following Markovian form,\nP (r(0), . . . , r(T )) = P (r(0)) T\u220f t=1 P (r(t)|r(t\u2212 1)). (A1)\nWe further assume a Gaussian form for the transition probability P (r(t)|r(t \u2212 1)) = N (r(t);h(t), \u03c32t I), where h(t) = wf(r(t \u2212 1)), and \u03c32t is a time-dependent variance, and for simplicity, we can set the variance to one without loss of generality, as the mere effect is leading to a rescaled cost function below. This Gaussian form can be obtained as an approximation, by using the Laplace approximation even if the transition probability is of other forms. The goal is to optimize the negative log-likelihood of the joint distribution, defined by\nF = \u2212 lnP (r(0), . . . , r(T )) = 1 2 \u2211 t \u2225r(t)\u2212 h(t)\u22252 \u03c32t + const, (A2)\nwhich corresponds exactly to the cost function of predictive coding if we treat \u03c32t = 1 and neglect the constant term.\nAppendix B: Transformer model\nA transformer network consists of an embedding layer, encoder blocks, and decoder blocks [48]. In analogy to the RNN model, all tokens (one-hot vectors) are transformed into representations X \u2208 Rd\u00d7T by an embedding layer, where d denotes the dimension of embedding space and T denotes the sequence length. As a clear difference from the RNN training, the input to the transformer is an entire X matrix, rather than one column each step for RNN. Note that we have not considered the position encoding scheme (e.g., adding a vector of sinusoids of different frequencies and phases to encode position of a word in a sentence) in our model.\nAn encoder block includes two parts. The first part is the self-attention mechanism,\naiming to evaluate the correlations among words in the input block X. To this end, we introduce three trainable matrices, namely, query Q, key K, and value V . Then, a linear transformation of the input is applied.\nQ =WQ \u00b7X, K =WK \u00b7X, V =WV \u00b7X,\n(B1)\nwhere WQ,WK \u2208 Rdh\u00d7d and WV \u2208 Rd\u00d7d are transformation matrices, and dh is the internal size of the attention operation. Therefore, we define Xt as the t-th column of X, and then we can define three vectors, namely kt = WKXt, vt = WvXt, and qt = WQXt. Then, the t-th column of the self-attention matrix SA(X) is given by\nattn(t) = T\u2211 i=1 \u03b1i(t)vi,\n\u03b1i(t) = ek\n\u22a4 i qt/ \u221a dh\u2211T\nj=1 e k\u22a4j qt/\n\u221a dh ,\n(B2)\nwhere \u03b1i(t) is a softmax operation containing information about the pairwise interactions between tokens. The normalization factor \u221a dh is required to retain relevant quantities in the exponential function being of the order one.\nThe second part is two feed-forward layers with skip connection, i.e.,\nz1 =SA(X) +X (residual layer 1) z2 =ReLU (W1 \u00b7 z1 + b1) (feed-forward layer 1) z3 =W2 \u00b7 z2 + b2 (feed-forward layer 2) zout =z1 + z3 (residual layer 2)\n(B3)\nwhere W1,W2 and b1, b2 are weights and biases of the two feed-forward layers. The output representations zout can be considered to be the input of the next encoder block. Here, we use the single headed attention transformer, and do not use the layer normalization, which scales each element of a vector by the mean and variance of all elements in that vector.\nOur used transformer model has only one encoder block and one decoder layer. The\ndecoder layer is a linear layer (the readout layer), where the output representations zout can be translated into the probability of the next token, which have the same function as the readout layer of the RNN model. The dimension of representations d = 300 for all models in Figure 5. For four RNN models, the number of neurons in the recurrent reservoir is N = 512. For the transformer model, it is convenient to set the hidden dimension dh = d = 300. The training parameters for all models are set to be the same. The batch size is 128 and the learning rate is 0.001. We have chosen the Adam algorithm as our training optimizer [38].\nAppendix C: The vanilla predictive learning algorithm\nThe vanilla predictive learning algorithm is a simplified version of our meta-predictive learning algorithm, without considering weight uncertainty. Hence, setting \u03c0 = 0 and \u039e = 0 in Eq. (6) and Eq. (7) in the main text leads to the following update equations for belief and weights.\n\u2206ri(t \u2032) = \u2212\u03b3E \u20321,i(t\u2032) + \u03b3f \u2032(ri(t\u2032)) \u2211 j E \u20321,j(t \u2032 + 1)wji + \u03b3f \u2032(ri(t \u2032)) \u2211 j E \u20322,j(t \u2032)woutji , (C1)\nand\n\u2206w\u2113ij = \u03b7\nN\u2113 \u2211 t E \u2032\u2113\u2032,i(t)\u03be \u2113 j , (C2)\nwhere the definition of \u2113, \u2113\u2032, \u03be\u2113j and N\u2113 bear the same meaning as in the main text (see Table I). We present the pseudocode of the vanilla predictive learning algorithm in Alg. 2.\nAlgorithm 2 Vanilla predictive coding algorithm 1: # Inference\n2: Given: input x, label y\u0302, randomly initialized belief r, ry = y\u0302 3: for i = 1, . . . , n do\n4: for t = 1, . . . , T do 5: hi(t) = \u2211N j=1wijf (rj(t\u2212 1)) + \u2211Nin j=1 w in ij xj(t);\n6: yi(t) = \u03d5 (\u2211N j=1w out ij f (rj(t)) ) ; 7: r(t) = r(t) + \u2206r(t).\n8: end for\n9: end for\n10: # Learning\n11: for \u2113 = in, out, recurrent do\n12: for t = 1, . . . , T do 13: w\u2113 = w\u2113 + \u2206w\u2113.\n14: end for\n15: end for\n16: Output: r\n17: # Prediction\n18: Given: test data x, converged belief r\n19: for t = 1, . . . , T do 20: hi(t) = \u2211N j=1wijf (rj(t\u2212 1)) + \u2211Nin j=1 w in ij xj(t)\n21: yi(t) = \u03d5 (\u2211N j=1w out ij f (rj(t)) ) 22: end for\n23: Output: y\nAppendix D: Mathematical items used in the main text and associated explanations\nWe list mathematical items used in the main text and associated explanations to help\nreaders go through the paper smoothly, as shown in the table I.\n[1] Se\u0301bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid\nPalangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv:2303.12712, 2023.\n[2] Haiping Huang. Eight challenges in developing theory of intelligence. arXiv:2306.11232, 2023.\n[3] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional\ninterpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1):79\u201387,\n1999.\n[4] Yanping Huang and Rajesh P. N. Rao. Predictive coding. WIREs Cognitive Science, 2(5):580\u2013\n593, 2011.\n[5] James C. R. Whittington and Rafal Bogacz. An Approximation of the Error Backpropagation\nAlgorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity. Neural\nComputation, 29(5):1229\u20131262, 2017.\n[6] Beren Millidge, Anil Seth, and Christopher L Buckley. Predictive coding: a theoretical and\nexperimental review. arXiv:2107.12979, 2021.\n[7] Karl Friston. Does predictive coding have a future? Nature Neuroscience, 21(8):1019\u20131021,\n2018.\n[8] Rick A Adams George R. Mangun Pascal Fries Andree M. Bastos, W. Martin Usrey and\nKarl J. Friston. Canonical microcircuits for predictive coding. Neuron, 76:695\u2013711, 2012.\n[9] Yusi Chen, Huanqiu Zhang, and Terrence J. Sejnowski. Hippocampus as a generative circuit\nfor predictive coding of future sequences. bioRxiv, 2022.\n[10] Lin Wang, Lotte Schoot, Trevor Brothers, Edward Alexander, Lena Warnke, Minjae Kim,\nSheraz Khan, Matti Ha\u0308ma\u0308la\u0308inen, and Gina R Kuperberg. Predictive coding across the left\nfronto-temporal hierarchy during language comprehension. Cerebral Cortex, 33(8):4478\u20134497,\n2023.\n[11] Siavash Golkar, Tiberiu Tesileanu, Yanis Bahroun, Anirvan Sengupta, and Dmitri Chklovskii.\nConstrained predictive coding as a biologically plausible model of the cortical hierarchy. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 14155\u201314169. Curran Associates,\nInc., 2022.\n[12] Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, Rafal Bogacz, and Zhenghua Xu.\nPredictive coding can do exact backpropagation on convolutional and recurrent neural net-\nworks. arXiv:2103.03725, 2021.\n[13] Beren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz.\nPredictive coding: towards a future of deep learning beyond backpropagation?\narXiv:2202.09467, 2022.\n[14] Zhen-Ye Huang, Ruyi Zhou, Miao Huang, and Hai-Jun Zhou. Energy\u2013information trade-\noff induces continuous and discontinuous phase transitions in lateral predictive coding.\narXiv:2302.11681, 2023.\n[15] Alexandre Pouget, Jeffrey M Beck, Wei Ji Ma, and Peter E Latham. Probabilistic brains:\nknowns and unknowns. Nature neuroscience, 16(9):1170\u20131178, 2013.\n[16] Haruo Kasai, Noam E. Ziv, Hitoshi Okazaki, Sho Yagishita, and Taro Toyoizumi. Spine\ndynamics in the brain, mental disorders and artificial neural networks. Nature Reviews Neu-\nroscience, 22(7):407\u2013422, 2021.\n[17] Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N.\nRao, Karl Friston, and Alexander Ororbia. Brain-inspired computational intelligence via\npredictive coding. 2023.\n[18] Wenxuan Zou, Chan Li, and Haiping Huang. Ensemble perspective for understanding tem-\nporal credit assignment. Physical Review E, 107(2):024307, 2023.\n[19] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation,\n9(8):1735\u20131780, 1997.\n[20] Yoshua Bengio, Re\u0301jean Ducharme, Pascal Vincent, and Christian Janvin. A neural proba-\nbilistic language model. J. Mach. Learn. Res., 3:1137\u20131155, 2003.\n[21] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems 27, pages 3104\u20133112, 2014.\n[22] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv:1406.1078, 2014.\n[23] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling. arXiv:1412.3555, 2014.\n[24] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR 2015 : International Conference on Learning\nRepresentations 2015, 2015.\n[25] Yair Lakretz, German Kruszewski, Theo Desbordes, Dieuwke Hupkes, Stanislas Dehaene,\nand Marco Baroni. The emergence of number and syntax units in lstm language models.\narXiv:1903.07435, 2019.\n[26] Y. LeCun, The MNIST database of handwritten digits, retrieved from\nhttp://yann.lecun.com/exdb/mnist.\n[27] Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large anno-\ntated corpus of english: The penn treebank. Comput. Linguist., 19(2):313\u2013330, 1993.\n[28] Chan Li and Haiping Huang. Learning credit assignment. Physical Review Letters,\n125(17):178301, 2020.\n[29] Charlotte Caucheteux and Jean-Re\u0301mi King. Brains and algorithms partially converge in\nnatural language processing. Communications Biology, 5:134, 2022.\n[30] Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum,\nand Evelina Fedorenko. Dissociating language and thought in large language models: a cog-\nnitive perspective. arXiv:2301.06627, 2023.\n[31] A. Aldo Faisal, Luc P. J. Selen, and Daniel M. Wolpert. Noise in the nervous system. Nature\nReviews Neuroscience, 9(4):292\u2013303, 2008.\n[32] Liqun Luo. Architectures of neuronal circuits. Science, 373(6559):1103, 2021.\n[33] Robert Rosenbaum. On the relationship between predictive coding and backpropagation. Plos\none, 17(3):e0266102, 2022.\n[34] Joa\u0303o Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical micro-\ncircuits approximate the backpropagation algorithm. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Pro-\ncessing Systems, volume 31, page 8721\u20138732. Curran Associates, Inc., 2018.\n[35] Boris Barbour, Nicolas Brunel, Vincent Hakim, and Jean-Pierre Nadal. What can we learn\nfrom synaptic weight distributions? Trends in Neurosciences, 30:622\u2013629, 2007.\n[36] Haiping Huang. Role of zero synapses in unsupervised feature learning. Journal of Physics\nA: Mathematical and Theoretical, 51(8):08LT01, 2018.\n[37] https://github.com/Qjbtiger/Meta-predictive-coding.\n[38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\narXiv:1412.6980, 2014.\n[39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Proceedings of the 26th\nInternational Conference on Neural Information Processing Systems - Volume 2, NIPS\u201913,\npages 3111\u20133119, Red Hook, NY, USA, 2013. Curran Associates Inc.\n[40] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv:2001.08361, 2020.\n[41] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. Transactions on Machine Learning Research, 2022.\n[42] Haiping Huang. Statistical Mechanics of Neural Networks. Springer, Singapore, 2022.\n[43] Luca Pinchetti, Tommaso Salvatori, Yordan Yordanov, Beren Millidge, Yuhang Song, and\nThomas Lukasiewicz. Predictive coding beyond gaussian distributions. arXiv:2211.03481,\n2022. in NeurIPS 2022.\n[44] Terrence J. Sejnowski. Large Language Models and the Reverse Turing Test. Neural Compu-\ntation, 35(3):309\u2013342, 2023.\n[45] Jie Zhao, Biwei Xie, and Xingquan Li. Weight uncertainty in transformer network for the trav-\neling salesman problem. In 2023 International Symposium of Electronics Design Automation\n(ISEDA), pages 219\u2013224, 2023.\n[46] Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes von Oswald, Maxime Larcher,\nAngelika Steger, and Joao Sacramento. Gated recurrent neural networks discover attention.\n2309.01775, 2023.\n[47] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc\u0327ois Fleuret. Transformers\nare RNNs: Fast autoregressive transformers with linear attention. In Hal Daume\u0301 III and Aarti\nSingh, editors, Proceedings of the 37th International Conference on Machine Learning, volume\n119 of Proceedings of Machine Learning Research, pages 5156\u20135165. PMLR, 2020.\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS\u201917, pages 6000\u2013\n6010, Red Hook, NY, USA, 2017. Curran Associates Inc."
        },
        {
            "heading": "Output Layer",
            "text": ""
        },
        {
            "heading": "Hidden Layer",
            "text": ""
        },
        {
            "heading": "Input Layer",
            "text": ""
        }
    ],
    "title": "Meta predictive learning model of languages in neural circuits",
    "year": 2023
}