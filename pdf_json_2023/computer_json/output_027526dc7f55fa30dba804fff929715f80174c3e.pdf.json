{
    "abstractText": "The accuracy of learning-based optical flow estimation models heavily relies on the realism of the training datasets. Current approaches for generating such datasets either employ synthetic data or generate images with limited realism. However, the domain gap of these data with real-world scenes constrains the generalization of the trained model to real-world applications. To address this issue, we investigate generating realistic optical flow datasets from realworld images. Firstly, to generate highly realistic new images, we construct a layered depth representation, known as multiplane images (MPI), from single-view images. This allows us to generate novel view images that are highly realistic. To generate optical flow maps that correspond accurately to the new image, we calculate the optical flows of each plane using the camera matrix and plane depths. We then project these layered optical flows into the output optical flow map with volume rendering. Secondly, to ensure the realism of motion, we present an independent object motion module that can separate the camera and dynamic object motion in MPI. This module addresses the deficiency in MPI-based single-view methods, where optical flow is generated only by camera motion and does not account for any object movement. We additionally devise a depthaware inpainting module to merge new images with dynamic objects and address unnatural motion occlusions. We show the superior performance of our method through extensive experiments on real-world datasets. Moreover, our approach achieves state-of-the-art performance in both unsupervised and supervised training of learning-based models. The code will be made publicly available at: https: //github.com/Sharpiless/MPI-Flow .",
    "authors": [
        {
            "affiliations": [],
            "name": "Yingping Liang"
        },
        {
            "affiliations": [],
            "name": "Jiaming Liu"
        },
        {
            "affiliations": [],
            "name": "Debing Zhang"
        },
        {
            "affiliations": [],
            "name": "Ying Fu"
        }
    ],
    "id": "SP:6c3ed00bbde4fed7878d07e3f40b5d3eda1a5a5d",
    "references": [
        {
            "authors": [
                "Filippo Aleotti",
                "Matteo Poggi",
                "Stefano Mattoccia"
            ],
            "title": "Learning optical flow from still images",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Simon Baker",
                "Daniel Scharstein",
                "JP Lewis",
                "Stefan Roth",
                "Michael J Black",
                "Richard Szeliski"
            ],
            "title": "A database and evaluation methodology for optical flow",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2011
        },
        {
            "authors": [
                "Thomas Brox",
                "Christoph Bregler",
                "Jitendra Malik"
            ],
            "title": "Large displacement optical flow",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "D.J. Butler",
                "J. Wulff",
                "G.B. Stanley",
                "M.J. Black"
            ],
            "title": "A naturalistic open source movie for optical flow evaluation",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2012
        },
        {
            "authors": [
                "Daniel J Butler",
                "Jonas Wulff",
                "Garrett B Stanley",
                "Michael J Black"
            ],
            "title": "A naturalistic open source movie for optical flow evaluation",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2012
        },
        {
            "authors": [
                "Gustavo Sutter P. Carvalho",
                "Diogo Carbonera Luvizon",
                "Antonio Joia Neto",
                "Andr\u00e9 G.C. Pacheco",
                "Ot\u00e1vio Augusto Bizetto Penatti"
            ],
            "title": "Learning multiplane images from single views with self-supervision",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Maskedattention mask transformer for universal image segmentation",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Jiyu Cheng",
                "Yuxiang Sun",
                "Max Q-H Meng"
            ],
            "title": "Improving monocular visual slam in dynamic environments: an optical-flow-based approach",
            "venue": "Advanced Robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip Hausser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick Van Der Smagt",
                "Daniel Cremers",
                "Thomas Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "In the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Bo Du",
                "Shihan Cai",
                "Chen Wu"
            ],
            "title": "Object tracking in satellite videos based on a multiframe optical flow tracker (j-stars)",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Yunhui Han",
                "Kunming Luo",
                "Ao Luo",
                "Jiangyu Liu",
                "Haoqiang Fan",
                "Guiming Luo",
                "Shuaicheng Liu"
            ],
            "title": "Realflow: Em-based realistic optical flow dataset generation from videos",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Yuxuan Han",
                "Ruicheng Wang",
                "Jiaolong Yang"
            ],
            "title": "Single-view view synthesis in the wild with learned adaptive multiplane images",
            "venue": "In ACM SIGGRAPH,",
            "year": 2022
        },
        {
            "authors": [
                "Toshihide Hanari",
                "Kuniaki Kawabata",
                "Keita Nakamura"
            ],
            "title": "Image selection method from image sequence to improve computational efficiency of 3d reconstruction: Analysis of inter-image displacement based on optical flow for evaluating 3d reconstruction performance",
            "venue": "In the IEEE International Symposium on System Integration (SII),",
            "year": 2022
        },
        {
            "authors": [
                "Anders Heyden",
                "Marc Pollefeys"
            ],
            "title": "Multiple view geometry",
            "venue": "Emerging topics in computer vision,",
            "year": 2005
        },
        {
            "authors": [
                "Tak-Wai Hui",
                "Xiaoou Tang",
                "Chen Change Loy"
            ],
            "title": "Liteflownet: A lightweight convolutional neural network for optical flow estimation",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Junhwa Hur",
                "Stefan Roth"
            ],
            "title": "Iterative residual refinement for joint optical flow and occlusion estimation",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Eddy Ilg",
                "Nikolaus Mayer",
                "Tonmoy Saikia",
                "Margret Keuper",
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Woobin Im",
                "Tae-Kyun Kim",
                "Sung-Eui Yoon"
            ],
            "title": "Unsupervised learning of optical flow with deep feature similarity",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Woobin Im",
                "Sebin Lee",
                "Sung-Eui Yoon"
            ],
            "title": "Semisupervised learning of optical flow by flow supervisor",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Jisoo Jeong",
                "Hong Cai",
                "Risheek Garrepalli",
                "Fatih Porikli"
            ],
            "title": "Distractflow: Improving optical flow estima- 10 tion via realistic distractions and pseudo-labeling",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Rico Jonschkowski",
                "Austin Stone",
                "Jonathan T Barron",
                "Ariel Gordon",
                "Kurt Konolige",
                "Anelia Angelova"
            ],
            "title": "What matters in unsupervised optical flow",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Art\u00far I K\u00e1roly",
                "Ren\u00e1ta Nagyn\u00e9 Elek",
                "Tam\u00e1s Haidegger",
                "K\u00e1roly Sz\u00e9ll",
                "P\u00e9ter Galambos"
            ],
            "title": "Optical flowbased segmentation of moving objects for mobile robot navigation using pre-trained deep learning models",
            "venue": "In the IEEE International Conference on Systems, Man and Cybernetics (SMC),",
            "year": 2019
        },
        {
            "authors": [
                "Filippos Kokkinos",
                "Iasonas Kokkinos"
            ],
            "title": "Learning monocular 3d reconstruction of articulated categories from motion",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Sheng Lai",
                "Jia-Bin Huang",
                "Ming-Hsuan Yang"
            ],
            "title": "Semi-supervised learning for optical flow with generative adversarial networks",
            "venue": "Advances in Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Jiaxin Li",
                "Zijian Feng",
                "Qi She",
                "Henghui Ding",
                "Changhu Wang",
                "Gim Hee Lee"
            ],
            "title": "Mine: Towards continuous depth mpi with nerf for novel view synthesis",
            "venue": "In the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2014
        },
        {
            "authors": [
                "Liang Liu",
                "Jiangning Zhang",
                "Ruifei He",
                "Yong Liu",
                "Yabiao Wang",
                "Ying Tai",
                "Donghao Luo",
                "Chengjie Wang",
                "Jilin Li",
                "Feiyue Huang"
            ],
            "title": "Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Yubao Liu",
                "Jun Miura"
            ],
            "title": "Rdmo-slam: Real-time visual slam for dynamic environments using semantic label prediction with optical flow",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Lombardi",
                "Tomas Simon",
                "Jason Saragih",
                "Gabriel Schwartz",
                "Andreas Lehrmann",
                "Yaser Sheikh"
            ],
            "title": "Neural volumes: Learning dynamic renderable volumes from images",
            "venue": "arXiv preprint arXiv:1906.07751,",
            "year": 1906
        },
        {
            "authors": [
                "Ao Luo",
                "Fan Yang",
                "Xin Li",
                "Shuaicheng Liu"
            ],
            "title": "Learning optical flow with kernel patch attention",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Ao Luo",
                "Fan Yang",
                "Kunming Luo",
                "Xin Li",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "title": "Learning optical flow with adaptive graph reasoning",
            "venue": "In the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Kunming Luo",
                "Chuan Wang",
                "Shuaicheng Liu",
                "Haoqiang Fan",
                "Jue Wang",
                "Jian Sun"
            ],
            "title": "Upflow: Upsampling pyramid for unsupervised optical flow learning",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Moritz Menze",
                "Andreas Geiger"
            ],
            "title": "Object scene flow for autonomous vehicles",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Chaerin Min",
                "Taehyun Kim",
                "Jongwoo Lim"
            ],
            "title": "Metalearning for adaptation of deep optical flow networks",
            "venue": "In the IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2023
        },
        {
            "authors": [
                "Federico Perazzi",
                "Jordi Pont-Tuset",
                "Brian McWilliams",
                "Luc Van Gool",
                "Markus Gross",
                "Alexander Sorkine-Hornung"
            ],
            "title": "A benchmark dataset and evaluation methodology for video object segmentation",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Katrin Lasinger",
                "David Hafner",
                "Konrad Schindler",
                "Vladlen Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zeroshot cross-dataset transfer",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
            "year": 2020
        },
        {
            "authors": [
                "Chris Rockwell",
                "David F Fouhey",
                "Justin Johnson"
            ],
            "title": "Pixelsynth: Generating a 3d-consistent experience from a single image",
            "venue": "In the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Geometry-free view synthesis: Transformers and no 3d priors",
            "venue": "In the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Nitin J Sanket",
                "Chahat Deep Singh",
                "Cornelia Ferm\u00fcller",
                "Yiannis Aloimonos"
            ],
            "title": "Prgflow: Unified swap-aware deep global optical flow for aerial robot navigation",
            "venue": "Electronics Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Austin Stone",
                "Daniel Maurer",
                "Alper Ayvaci",
                "Anelia Angelova",
                "Rico Jonschkowski"
            ],
            "title": "Smurf: Selfteaching multi-frame unsupervised raft with fullimage warping",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Deqing Sun",
                "Daniel Vlasic",
                "Charles Herrmann",
                "Varun Jampani",
                "Michael Krainin",
                "Huiwen Chang",
                "Ramin Zabih",
                "William T Freeman",
                "Ce Liu"
            ],
            "title": "Autoflow: Learning a better training set for optical flow",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Deqing Sun",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Jan Kautz"
            ],
            "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Alexandru Telea"
            ],
            "title": "An image inpainting technique based on the fast marching method",
            "venue": "Journal of graphics tools,",
            "year": 2004
        },
        {
            "authors": [
                "Richard Tucker",
                "Noah Snavely"
            ],
            "title": "Single-view view synthesis with multiplane images",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Shubham Tulsiani",
                "Richard Tucker",
                "Noah Snavely"
            ],
            "title": "Layer-structured 3d scene inference via view synthesis",
            "venue": "In the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Philippe Weinzaepfel",
                "Jerome Revaud",
                "Zaid Harchaoui",
                "Cordelia Schmid"
            ],
            "title": "Deepflow: Large displacement optical flow with deep matching",
            "venue": "In the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2013
        },
        {
            "authors": [
                "Christopher Zach",
                "Thomas Pock",
                "Horst Bischof"
            ],
            "title": "A duality based approach for realtime tv-l 1 optical flow",
            "venue": "In Pattern Recognition,",
            "year": 2007
        },
        {
            "authors": [
                "Jimuyang Zhang",
                "Sanping Zhou",
                "Xin Chang",
                "Fangbin Wan",
                "Jinjun Wang",
                "Yang Wu",
                "Dong Huang"
            ],
            "title": "Multiple object tracking by flowing and fusing",
            "venue": "arXiv preprint arXiv:2001.11180,",
            "year": 2020
        },
        {
            "authors": [
                "Tianwei Zhang",
                "Huayan Zhang",
                "Yang Li",
                "Yoshihiko Nakamura",
                "Lei Zhang"
            ],
            "title": "Flowfusion: Dynamic dense rgb-d slam based on optical flow",
            "venue": "In the IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Tinghui Zhou",
                "Richard Tucker",
                "John Flynn",
                "Graham Fyffe",
                "Noah Snavely"
            ],
            "title": "Stereo magnification: Learning view synthesis using multiplane images",
            "venue": "In ACM SIGGRAPH,",
            "year": 2018
        },
        {
            "authors": [
                "Yuemei Zhou",
                "Gaochang Wu",
                "Ying Fu",
                "Kun Li",
                "Yebin Liu"
            ],
            "title": "Cross-mpi: Cross-scale stereo for image super-resolution using multiplane images",
            "venue": "In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Optical flow refers to the precise calculation of per-pixel motion between consecutive video frames. Its applications span a wide range of fields, including object tracking [10, 51], robot navigation [23, 41], three-dimensional (3D)\n*Corresponding Author: fuying@bit.edu.cn\nreconstruction [24, 14], and visual simultaneous localization and mapping (SLAM) [52, 8, 29]. In recent years, with the rapid development of neural networks, learning-based methods [44, 45] have demonstrated significant advances compared to traditional model-based algorithms [3, 49, 50]. Conventional practices primarily rely on synthetic data, as demonstrated by [9, 18, 4]. Synthetic data contains exact optical flow labels and animated images. However, the domain gap between synthetic and real data hinders its further improvements in real-world applications.\nRecent studies have aimed to extract optical flow from real-world data by employing hand-made special hardware [2, 11, 34]. However, the rigidly controlled and inefficient collection procedure limits their applicability. To address this issue, Depthstillation [1], and RealFlow [12] have been proposed, which project each pixel in the real-world image onto the novel view frame with the help of random motions\nar X\niv :2\n30 9.\n06 71\n4v 1\n[ cs\n.C V\n] 1\n3 Se\np 20\n23\nof virtual cameras or estimated flows. Nonetheless, both methods are limited by the lack of image realism, leading to issues such as collisions, holes, and artifacts, as illustrated in Figure 1. These limitations constrain the real-world performance of learning-based optical flow models [44, 45].\nTo achieve higher image realism, we turn our attention to the use of single-view Multiplane Images (MPI) [53, 47, 54, 6, 13]. This line of work demonstrates remarkable single-view image rendering capabilities and effectively reduces collisions, holes, and artifacts commonly found in previous methods [1, 12]. These advancements contribute to higher image realism, prompting a natural question: Can high-realistic MPI methods be adapted to generate high-quality optical flow datasets for training purposes?\nTo this end, we propose MPI-Flow, aiming to generate realistic optical flow datasets from real-world images. Specifically, we first review the image synthesis pipeline of MPI and devise an optical flow generation pipeline along with image synthesis. In this step, we build an MPI by warping single-view image features onto each layered plane with the predicted color and density. The color and density then be mapped into a realistic new image via volume rendering. With the layered planes, we extract optical flows with virtual camera motions from the rendered image and the real image. Second, as the MPI can only be applied in static scenes, which yield limited motion realism, we propose an independent object motion module and a depthaware inpainting module to tackle this issue. The independent object motion module decouples dynamic objects from static scenes and applies different virtual camera matrices to calculate the motion of both dynamic and static parts. The depth-aware inpainting module is introduced to remove the object occlusion in the synthesized new image.\nWith MPI-Flow, a large number of single-view images can be used to generate large-scale training datasets with realistic images and motions. This enables learning-based optical flow models better generalization to a wide range of real-world scenes. Extensive experiments on real datasets demonstrate the effectiveness of our approach. In summary, our main contributions are as follows:\n\u2022 We are the first to present a novel MPI-based optical flow dataset generation framework, namely MPI-Flow, which can significantly improve the realism of the generated images and motion.\n\u2022 We present a novel independent object motion module for modeling dynamic objects in MPI, which can model realistic optical flow from camera motion and object motion simultaneously.\n\u2022 We design a depth-aware inpainting module for realistic image inpainting, which can remove unnatural motion occlusions in generated images."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we review the most relevant studies on optical flow networks, optical flow dataset generation, and novel view image synthesis methods. Supervised Optical Flow Network. Early methods train deep neural networks to match patches across images [49]. FlowNet [9] first trains convolutional neural networks on the synthetic datasets with optical flow. Moreover, the follow-up methods [16, 17, 18, 32, 31] with advanced modules and network architectures make a significant improvement in supervised optical flow learning, with RAFT [45] representing state-of-the-art. However, generalization remains a cause for concern due to the domain gap between synthetic datasets and real-world applications. To address this problem, our work focuses on generating realistic optical flow datasets from real-world images. Dataset Generation for Optical Flow. The use of fluorescent texture to record motions in real-world scenes is first described in [2] to obtain flow maps. KITTI [11, 34] provides sophisticated training data through complex lidar and camera setups. However, the aforementioned real-world datasets have limited quantities and constrained scenes, making it difficult for models trained using deep supervised learning to generalize to more expansive scenes. Synthesized training pairs, such as those in Flyingchairs [9] and Flyingthings [18], have shown promise for supervised learning. However, moving animated image patches cannot accurately match real-world scenes, leading to domain gaps. AutoFlow [43] introduces a learning-based approach for generating training data by hyper-parameters searching. However, AutoFlow relies on optical flow labels for domain adaptation, which is not practical in most scenarios where ground truth labels are unavailable.\nTwo recent works have proposed methods for generating training datasets based on real-world images or videos. The first, called Depthstillation [1], synthesizes paired images by estimating depth and optical flows from a single still image. Optical flows are calculated based on the virtual camera pose and depth. The second method, called RealFlow [12], synthesizes intermediate frames between two frames using estimated optical flows with RAFT. However, both methods use naive image synthesis techniques that fail to meet the demand for realism criteria due to hole-filling and artifacts. In contrast, our method improves on this approach by using a well-designed and modified multiplane image (MPI) technique to obtain realistic images. Novel View Synthesis. View synthesis methods aim to generate new images from arbitrary viewpoints by utilizing a given scene. Several classical approaches [48, 30, 53, 35] have been proposed that utilize multiple views of a scene to render novel views with geometric consistency. However, synthesizing novel views from a single image remains challenging due to the limited scene information available.\nPixelsynth [39] and Geometry-Free View Synthesis [40] address this challenge by optimizing the synthesizing model using multi-view supervision. However, their generalization to in-the-wild scenes is hindered by the lack of largescale multi-view datasets. Single-view MPI [47] and MINE [26] decompose the scene into multiple layers and utilize an inpainting network to extend each occluded layer. Additionally, AdaMPI [13] addresses complex 3D scene structures through a novel plane adjustment network. These MPIbased methods have demonstrated success in synthesizing realistic images, and thus, we adopt multiplane images as our basic synthesis tool. However, to the best of our knowledge, there are currently no publicly available methods for generating optical flow datasets from MPI. To extract optical flows from MPI, we propose a novel pipeline that differs from previous MPI-based image synthesis methods by utilizing layered depths and virtual camera poses. Additionally, to enhance the realism of the generated optical flow dataset, we introduce an independent object motion module for static and dynamic decoupling, as well as a depth-aware inpainting module to remove unnatural occupations."
        },
        {
            "heading": "3. The Proposed MPI-Flow",
            "text": "In this section, we first briefly review the basics of our motivation and formulation for novel view image generation. Then we introduce the optical flow generation\npipeline. Next, we present the details of two crucial components of our approach, including independent object motions and depth-aware inpainting."
        },
        {
            "heading": "3.1. Motivation and Formulation",
            "text": "Our goal is to generate a realistic novel view image It \u2208 RH\u00d7W\u00d73 and the corresponding optical flow maps Fs\u2192t \u2208 RH\u00d7W\u00d72 from single-view image Is \u2208 RH\u00d7W\u00d73. H and W are the height and width of the image, respectively. The two-dimensional array on the optical flow Fs\u2192t represents the change of the corresponding pixel from image Is to image It. The input image, generated image, and optical flow together form a training pair.\nTo generate training pair, previous works [1, 12] wrap pixels from image Is to image It with estimated flows. This inevitably leads to holes and artifacts in the image It, which damages the image realism. Recent work [47, 26, 13] on Multiplane Images (MPI) reveals that the layered depth representation of the single-view image can significantly improve the realism of the generated novel view image.\nWe aim to tackle the image realism challenges in our methods and meanwhile enhance the optical flow realism and motion realism. Accordingly, we present an MPI-based optical flow dataset generation method, namely MPI-Flow. Figure 2 shows the MPI-Flow framework for training pair generation. To construct MPI, given the input image Is,\nan off-the-shelf monocular depth estimation network [38] is used to estimate its depth map. Then we use a neural network to construct N fronto-parallel RGB\u03c3 planes with color, density, and depth predicted by neural networks in the rays under novel viewpoints.\nTo decouple dynamic objects, an instance segmentation network [7] gives the object mask. Then we use bilinear interpolation to obtain the object masks under two viewpoints respectively. Using object masks and constructed MPI, we use volume rendering to render the separate novel view images, optical flow maps, and depths of dynamic objects and the static scene, respectively. The optical flow Fs\u2192t can be obtained simply by adding the optical flows of the objects and the scene. However, due to different viewpoints, merging new images results in vacant areas and false occlusion. To this end, we design a depth-aware inpainting module, using rendered depths and object masks to fill the holes and repair false occlusion in the synthesized new image It."
        },
        {
            "heading": "3.2. Optical Flow Data Generation",
            "text": "Novel View Image from MPI. To render realistic image It under a target viewpoint \u00b5t, we use pixel warping from the source-view MPI in a differentiable manner. Specifically, we use a neural network F as in [13] to construct N frontoparallel RGB\u03c3 planes under source viewpoint \u00b5s with color channels cn, density channel \u03c3n, and depth dn from the input image Is and its depth map Ds as:\n{(cn,\u03c3n,dn)}Nn=1 = F(Is,Ds), (1)\nwhere N is a predefined parameter that represents the number of planes in MPI. Each pixel (xt, yt) on the novel view image plane can be mapped to pixel (xs, ys) on n-th source MPI plane via homography function [15]:\n[xs, ys, 1] T \u223c K ( R \u2212 tn T\ndn\n) K\u22121 [xt, yt, 1] T , (2)\nwhere R and t are the rotation and translation from the source viewpoints \u00b5s to the target viewpoints \u00b5t, K is the camera intrinsic, and n = [0, 0, 1] is the normal vector. Thus, the color c\u2032n and density \u03c3\u2032n of each new plane for the novel view It can be obtained via bilinear sampling. We use discrete intersection points between new planes and arbitrary rays passing through the scene and estimate integrals:\nIt = N\u2211\nn=1\n( c\u2032n\u03b1 \u2032 n n\u22121\u220f m=1 (1\u2212\u03b1\u2032m) ) , (3)\nwhere \u03b1\u2032n = exp (\u2212\u03b4n\u03c3\u2032n) and \u03b4n is the distance map between plane n and n+1 and we set the initial depth of MPI planes uniformly spaced in disparity as in [13].\nOptical Flow from MPI. Although MPI-based methods synthesize realistic images, reliable optical flow maps are\nalso needed to train learning-based optical flow estimation models. Therefore, we propose adding an additional optical channel in each plane. To this end, we compute the optical flow on the n-th plane at pixel [xs, ys] of source image Is by fn = [xt \u2212xs, yt \u2212 ys] with a backward-warp process in terms of the inverse equivalent form of Equation (2):\n[xt, yt, 1] T \u223c K ( R\u2020 \u2212 t \u2020nT\ndn\n) K\u22121 [xs, ys, 1] T , (4)\nwhere xs and ys are uniformly sampled from a H\u00d7W grid. R\u2020 and t\u2020 are the inverses of R and t, respectively.\nTo make sure that the optical flow maps match the novel view image It perfectly, we propose to render Fs\u2192t as in Equation (3) in terms of volume rendering:\nFs\u2192t = N\u2211\nn=1\n( fn\u03b1n\nn\u22121\u220f m=1 (1\u2212\u03b1m)\n) , (5)\nwhere fn \u2208 RH\u00d7W\u00d72 is the optical flow maps on the n-th plane of image Is. The pipeline implemented thus far models the optical flows resulting from camera motion without considering the potential presence of independently dynamic objects. However, real-world scenes are highly likely to contain such objects. Not incorporating their motions can lead to domain gaps by unrealistic optical flows.\nIndependent Object Motions. To model more realistic motions, we propose applying separate virtual motions to objects and static backgrounds extracted from the scene. Therefore, we utilize an instance segmentation network \u2126 [7] for extracting the main object in the source image Is as:\nMs = \u2126(Is) \u2208 RH\u00d7W , (6)\nwhere Ms is a binary mask to indicate the region of the object. To model the motions of the object Ms in the scene, we construct separate viewpoints, including camera motion \u00b5scet and object motion \u00b5 obj t . We then obtain the rendered scene novel view Iscet and object novel view I obj t as in Equation (3). The separate optical flows, Fsces\u2192t and F obj s\u2192t can also be obtained as in Equation (5). The optical flows in Fs\u2192t are mixed by the values in Fsces\u2192t and F obj s\u2192t in terms of mask Ms to get the final optical flow maps containing camera motion and dynamic objects for training.\nWe can then use the bilinear interpolation to get the new object masks Mt and Mobjt under the new viewpoints \u00b5t and \u00b5objt via bilinear sampling from Ms. Pixels in the new merged image are also selected according to the content masks 1 \u2212 Mt and Mobjt from Iscet and I obj t . Then, a simple inpainting strategy [46] is used to fill the empty area in the new image with an inpainting mask calculated by Mfill = Mt \u2299 (1\u2212 Mobjt ).\nDepth-Aware Inpainting Although merged images give a realistic visual effect, depth changes caused by camera motion and object motion can also cause unnatural occlusions. To solve this problem, we use volume rendering to obtain the depth Dscet of the scene novel view:\nDt = N\u2211\nn=1\n( d\u2032n\u03b1 \u2032 n n\u22121\u220f m=1 (1\u2212\u03b1\u2032m) ) , (7)\nand the depth of the object novel view Dobjt can be obtained in the same way. We then utilize both depths to compute the occupation mask between the novel views:\nMocc = (1\u2212 Mt)\u2299 Mobjt \u2299 (Dt < D obj t ), (8)\nwhich indicates the background areas in front of the object. Therefore, we are able to restore the coincidence area between the object and the background in the new image It.\nFigure 3 provides a detailed illustration of the incremental effects of MPI-Flow with and without independent object motion and depth-aware inpainting. Novel view images and optical flows from single-view images can be generated with MPI-Flow and only camera motion, as shown in Figures 3(a) and 3(b). However, camera motion alone does not match the complex optical flows in real-world scenes. To address this issue, we introduce an independent object motion module, as shown in Figure 3(c), to ensure motion realism. To further enhance motion realism and address occlusion caused by object motion, we apply the depth-aware inpainting module, as shown in Figure 3(d)."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Datasets",
            "text": "FlyingChairs [9] and FlyingThings3D [18] are both popular synthetic datasets that train optical flow models. As a standard practice, we use \u201cCh\u201d and \u201cTh\u201d respectively to represent the two datasets, and \u201cCh\u2192Th\u201d means training first on \u201cCh\u201d and fine-tuning on \u201cTh\u201d. By default, we use the RAFT pre-trained on \u201cCh\u2192Th\u201d to be fine-tuned on the generated datasets and evaluated on labeled datasets. COCO [27] is a collection of single still images and ground truth with labels for object detection or panoptic segmentation tasks. We sample 20k single-view still images from the train2017 split following Depthstillation [1] to generate virtual images and optical flow maps. DAVIS [37] provides high-resolution videos and it is widely used for video object segmentation. We use all the 10581 images of the unsupervised 2019 challenge to generate datasets by MPI-Flow and other state-of-the-art optical flow dataset generation methods. KITTI2012 [11] and KITTI2015 [34] are well-known benchmarks for optical flow estimation. There are multiview extensions (4,000 for training and 3,989 for testing)\ndatasets with no ground truth. We use the multi-view extension images (training and testing) of KITTI 2015 to generate datasets, separately. By default, we evaluate the trained models on KITTI 12 training set and KITTI 15 training set in the tables following [1] and [12], abbreviated as \u201cKITTI 15\u201d and \u201cKITTI 12\u201d in the following tables. Sintel [5] is derived from the open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB, while the disparity is grayscale. Although not a real-world dataset, we use it to verify the model\u2019s generalization across domains."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "Firstly, we provide a description of the learning-based optical flow estimation models that were utilized in our experiments. Subsequently, we outline the experimental parameters and setup along with the evaluation formulation. Optical Flow networks. To evaluate how effective our generated data are at training optical flow models, we select RAFT [45], which represents state-of-the-art architecture for supervised optical flow and has excellent generalization capability. By default, we train RAFT on generated data for 200K steps with a learning rate of 1\u00d7 10\u22124 and weight decay of 1\u00d710\u22125, batch size of 6, and 288\u00d7960 image crops. This configuration is the default setting of RAFT fitting on KITTI with two GPUs but four times the number of training steps, following [12]. For the rest of the setup, we use the official implementation of RAFT without any modifications. All evaluations are performed on a single NVIDIA\nGeForce RTX 3090 GPU 1. Virtual Camera Motion. To generate the novel view images from COCO and DAVIS, we adopt the same settings in [1] to build the virtual camera. For KITTI, we empirically build the camera motion with three scalars where tx, ty are in [\u22120.2, 0.2] and tz are in [0.1, 0.35]. We build the camera rotation with three Euler angles ax, ay, az in [\u2212 \u03c090 , \u03c0 90 ]. We use single camera motion for each image from COCO but multiple camera motions (\u00d74 by default) from DAVIS and KITTI as in [12], due to the small number of images and the homogeneity of the scene in video data. We show how the number of camera motions impacts the optical flow network performance in the discussion. Evaluation Metrics. We report evaluation results on the average End-Point Error (EPE) and two error rates, respectively the percentage of pixels with an absolute error greater than 3 (> 3) or both absolute and relative errors greater than 3 and 5% respectively (Fl) on all pixels."
        },
        {
            "heading": "4.3. Comparison with State-of-the-art Methods",
            "text": "In this section, we evaluate the effectiveness of the MPIFlow generation pipeline on public benchmarks. We will highlight the best results in bold and underline the secondbest if necessary among methods trained in fair conditions. Comparison with Dataset Generation Methods. As a method for generating datasets from real-world images, we compare MPI-Flow with Depthstillation [1] and RealFlow [12], which are representative works in real-world dataset generation. In order to evaluate the effectiveness\n1RAFT with the same parameters loaded on different GPUs yield slightly different evaluation results. Therefore, to ensure fair comparison, we download the official model weights with the best performance provided by the compared methods and evaluate them on the same GPU.\nof MPI-Flow, we follow the procedures of Depthstillation and RealFlow to construct training sets from four different datasets, including COCO, DAVIS, KITTI 15 multi-view train set, and KITTI 15 multi-view test set. To ensure fair comparisons, we conduct experiments with a similar number of training sets as our competitors. Specifically, for DAVIS and KITTI, we generate four motions for each image to match RealFlow, which trains RAFT for four EM iterations with four times the amount of data. For COCO, we follow the exact same setup as Depthstillation. Since Depthstillation does not provide details for KITTI 15 train, we use its default settings to obtain the results. Trained models are evaluated on the training sets of Sintel, KITTI 12, and KITTI 15. We report the evaluation results of models with the best performance in the paper for Depthstillation and RealFlow. Furthermore, we conduct crossdataset experiments where RAFT is trained with one generated dataset and evaluated with another.\nAs shown in Table 1, even with the same amount of data, our approach gains significant improvements and generalization over multiple datasets. When trained and tested with\nthe same KITTI 15 Train image source, our EPE outperforms the second-best by a remarkable 0.29. When trained and tested with different image sources, our MPI-Flow demonstrates clear improvements over the competitors on almost all the evaluation settings. Notably, MPI-Flow achieves much better performance, even though RealFlow requires two consecutive frames to generate datasets, while\nMPI-Flow needs only one still image. It is worth comparing MPI-Flow with Depthstillation under the same settings to evaluate their performance. For this comparison, we use the exact same settings as Depthstillation. Specifically, we generate MPI-Flow datasets with 1) no object motion, 2) the same camera parameters, 3) one camera motion per image, and 4) without pre-training on Ch\u2192Th. The datasets generated are named MF-DAVIS and MF-COCO, while dCOCO and dDAVIS are generated from Depthstillation. The models are trained from scratch with the respective datasets. RealFlow is not included in this table as it requires a pre-trained optical flow model to generate datasets. The evaluation results are shown in Table 2. Our method outperforms Depthstillation with significant improvements on both COCO and DAVIS, demonstrating the importance of image realism. Comparison with Unsupervised Methods. Another way to utilize real-world data is through unsupervised learning, which learns optical flow pixels directly without the need for optical flow labels. In order to further demonstrate the effectiveness of MPI-Flow, we compare our method with the existing literature on unsupervised methods. The results of this comparison can be seen in Table 3. All methods are evaluated under the condition that only images from\nthe evaluation set could be used, without access to ground truth labels. For evaluation, we train the RAFT on our generated dataset with images from the KITTI 15 training set. Our MPI-Flow outperform all unsupervised methods in terms of EPE on both the KITTI 12 training set and KITTI 15 training set, with no need for any unsupervised constraints. However, our method performs better on EPE but has slightly lower Fl than SMURF, mainly because SMURF employs multiple frames for training. Comparison with Supervised Methods. To further prove the effectiveness of MPI-Flow, we use KITTI 15 train set to fine-tune RAFT pre-trained by our generated dataset with images from KITTI 15 test set. The evaluation results on KITTI 15 train and KITTI 15 test are shown in Table 5. We achieve state-of-art performance on KITTI 2015 test benchmark compared to supervised methods on training RAFT. Qualitative Results. Figure 4 show the generated images from the methods utilizing real-world images, as presented in Table 1. In this comparison, we use images from the KITTI 15 dataset as source image input. The images generated by RealFlow [12] and Depthstillation [1] with artifacts degrade the image realism. In contrast, MPI-Flow generates more realistic images than the other two methods. More results are shown in Figures 5 and 6."
        },
        {
            "heading": "4.4. Discussion",
            "text": "To verify the effectiveness of the proposed MPI-Flow, we discuss the performance of models trained with generated datasets with different settings. We show more discus-\nsion and evaluation results in the supplementary material. Object Motion and Depth-Aware Inpainting. We conduct a series of ablation studies to analyze the impact of different choices in our proposed MPI-Flow for new image synthesis, including object motion, depth-aware inpainting, and multiple objects. \u201cMultiple objects\u201d indicates multiple moving objects in each image. To measure the impact of these factors, we generate new images from the KITTI 15 training set to train RAFT and evaluate them on the KITTI 12 training set and KITTI 15 training set. Because there are multiple combinations of these factors, we test by incrementally adding components of our approach, as shown in Table 4. In the first row, we show the performance achieved by generating new images without dynamic objects, thus assuming that optical flow comes from camera motion only. Then we incrementally add single-object motion, depthaware inpainting, and multi-object motion to model a more realistic scenario. There are considerable improvements in almost all datasets on various metrics, except for the KITTI 12 training set, possibly due to the infrequent dynamic object motion in this dataset. The EPE on KITTI 15 remains relatively stable within the normal margin after adding the depth-aware inpainting module and the multi-object trick. Camera Motion Parameters. We also conduct a series of\nablation studies to analyze the impact of different parameters on camera motions. We first show the effect of tz , which indicates the range of moving forward and backward distances, as shown in Table 4. We set the minimum tz to 0.1 by default, considering that most cameras on vehicles only move forward in the KITTI dataset. We also show the effect of tx and ty , representing the distance from left to right and from up to down, respectively. Because there are multiple combinations of parameters, we only test a specific parameter of our method in isolation. Settings used by default are underlined. The results show that a more reasonable range of camera motion leads to better performance. Model Architectures The default model used in our paper is RFAT [45]. Table 10 shows that our proposed MPI-Flow also works for PWC-Net [44] compared with depthstillation [1] in a fair setting. We generate data from COCO and DAVIS and train PWC-Net, in which the results are still better than PWC-Net trained on Ch\u2192Th or datasets generated from depthstillation. Amount of Virtual Motions. We can generate multiple camera motions with multiple dynamic objects for any given single image and thus a variety of paired images and ground-truth optical flow maps. Thus we can increase the number of camera motions to generate more data. Table 6 shows the impact of different amounts of camera motions on model performance. Interestingly, MPI-Flow with 4 mo-\ntions per image already allows for strong generalization to real domains, outperforming the results achieved using synthetic datasets shown in the previous evaluation results. Increasing the motions per image by factors 10 and 20 both lead to better performance on KITTI 12 and KITTI 15 compared to 4 and 1. Using 40 motions per image gives the best performance on KITTI 15 in terms of Fl. It indicates that a more variegate image content in the generated dataset may be beneficial for generalization to real applications. Table 7 shows the effect of amounts of dynamic objects on model performance. Increasing the number of dynamic objects improves the performance of the model on KITTI 12 but slightly decreases it on KITTI 15 in terms of EPE. Quantity of Source Images The number of source images affects the scene diversity of the generated dataset. Empirically, more source images will be more conducive to model learning, as verified in Table 11. We verify the effect of the number of source images on the MF-COCO. The model performance is significantly improved by increasing the number of source images."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we present a new framework for generating optical flow datasets, which addresses two main challenges: image realism and motion realism. Firstly, we propose an MPI-based image rendering pipeline that generates realistic images with corresponding optical flows from novel viewpoints. This pipeline utilizes volume rendering to address image artifacts and holes, leading to more realistic images. Secondly, we introduce an independent object motion module that separates dynamic objects from the static scene. By decoupling object motion, we further improve motion realism. Additionally, we design a depth-aware inpainting module that handles unnatural occlusions caused by object motion in the generated images. Through these novel designs, our approach achieves superior performance on realworld datasets compared to both unsupervised and supervised methods for training learning-based models.\nAcknowledgments This work was supported by the National Natural Science Foundation of China (12202048, 62171038, and 62171042), the R&D Program of Beijing Municipal Education Commission (KZ202211417048), and the Fundamental Research Funds for the Central Universities."
        }
    ],
    "title": "MPI-Flow: Learning Realistic Optical Flow with Multiplane Images",
    "year": 2023
}