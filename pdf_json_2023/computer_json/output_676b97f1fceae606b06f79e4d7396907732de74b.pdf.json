{
    "abstractText": "Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. In this direction, we present the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\u00f6dinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. We also show that the training of generative adversarial networks includes a hidden data-optimization sub-problem, which induces the SD flow under certain choices of loss function when the discriminator is optimal. As a result, the SD flow provides a theoretical link between model classes that individually address the three challenges of the generative modeling trilemma\u2014high sample quality, mode coverage, and fast sampling\u2014thereby setting the stage for a unified approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Romann M. Weber"
        }
    ],
    "id": "SP:96d271022adeb94d971553e2565bb59e297b4d7f",
    "references": [
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Michael Arbel",
                "Anna Korba",
                "Adil Salim",
                "Arthur Gretton"
            ],
            "title": "Maximum mean discrepancy gradient flow",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Ba",
                "Murat A Erdogdu",
                "Marzyeh Ghassemi",
                "Shengyang Sun",
                "Taiji Suzuki",
                "Denny Wu",
                "Tianzong Zhang"
            ],
            "title": "Understanding the variance collapse of svgd in high dimensions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Beyer",
                "Jonathan Goldstein",
                "Raghu Ramakrishnan",
                "Uri Shaft"
            ],
            "title": "When is \u201cnearest neighbor",
            "venue": "Database Theory\u2014ICDT\u201999: 7th International Conference Jerusalem, Israel, January",
            "year": 1999
        },
        {
            "authors": [
                "Giovanni Bussi",
                "Michele Parrinello"
            ],
            "title": "Accurate sampling using langevin dynamics",
            "venue": "Physical Review E,",
            "year": 2007
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tianrong Chen",
                "Guan-Horng Liu",
                "Evangelos Theodorou"
            ],
            "title": "Likelihood training of schr\u00f6dinger bridge using forward-backward sdes theory",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yongxin Chen",
                "Tryphon T Georgiou",
                "Michele Pavon"
            ],
            "title": "Optimal steering of a linear stochastic system to a final probability distribution, part i",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2015
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "James Thornton",
                "Jeremy Heng",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of machine learning research,",
            "year": 2011
        },
        {
            "authors": [
                "Bradley Efron"
            ],
            "title": "Tweedie\u2019s formula and selection bias",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "Martin Ehrendorfer"
            ],
            "title": "The liouville equation and its potential usefulness for the prediction of forecast",
            "venue": "skill. part i: Theory. Monthly Weather Review,",
            "year": 1994
        },
        {
            "authors": [
                "Xingdong Feng",
                "Yuan Gao",
                "Jian Huang",
                "Yuling Jiao",
                "Xu Liu"
            ],
            "title": "Relative entropy gradient sampler for unnormalized distributions",
            "venue": "arXiv preprint arXiv:2110.02787,",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gao",
                "Yuling Jiao",
                "Yang Wang",
                "Yao Wang",
                "Can Yang",
                "Shunkang Zhang"
            ],
            "title": "Deep generative learning via variational gradient flow",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yuan Gao",
                "Jian Huang",
                "Yuling Jiao",
                "Jin Liu",
                "Xiliang Lu",
                "Zhijian Yang"
            ],
            "title": "Deep generative learning via euler particle transport",
            "venue": "In Mathematical and Scientific Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow"
            ],
            "title": "Nips 2016 tutorial: Generative adversarial networks",
            "venue": "arXiv preprint arXiv:1701.00160,",
            "year": 2016
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "arXiv preprint arXiv:1406.2661,",
            "year": 2014
        },
        {
            "authors": [
                "Jackson Gorham",
                "Lester Mackey"
            ],
            "title": "Measuring sample quality with stein\u2019s method",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Will Grathwohl",
                "Ricky TQ Chen",
                "Jesse Bettencourt",
                "Ilya Sutskever",
                "David Duvenaud"
            ],
            "title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models",
            "venue": "arXiv preprint arXiv:1810.01367,",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M Borgwardt",
                "Malte J Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Peter Dayan"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Richard Jordan",
                "David Kinderlehrer",
                "Felix Otto"
            ],
            "title": "The variational formulation of the fokker\u2013planck equation",
            "venue": "SIAM journal on mathematical analysis,",
            "year": 1998
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "arXiv preprint arXiv:2206.00364,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wu Lin",
                "Mohammad Emtiyaz Khan",
                "Mark Schmidt"
            ],
            "title": "Stein\u2019s lemma for the reparameterization trick with exponential family mixtures",
            "year": 1910
        },
        {
            "authors": [
                "Qiang Liu",
                "Dilin Wang"
            ],
            "title": "Stein variational gradient descent: A general purpose bayesian inference algorithm",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Qiang Liu",
                "Jason Lee",
                "Michael Jordan"
            ],
            "title": "A kernelized stein discrepancy for goodness-of-fit tests",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "David JC MacKay"
            ],
            "title": "Information theory, inference and learning algorithms",
            "venue": "Cambridge university press,",
            "year": 2003
        },
        {
            "authors": [
                "Dimitra Maoutsa",
                "Manfred Opper"
            ],
            "title": "Deterministic particle flows for constraining stochastic nonlinear systems",
            "venue": "Physical Review Research,",
            "year": 2022
        },
        {
            "authors": [
                "Dimitra Maoutsa",
                "Sebastian Reich",
                "Manfred Opper"
            ],
            "title": "Interacting particle solutions of fokker\u2013planck equations through gradient\u2013log\u2013density estimation",
            "year": 2020
        },
        {
            "authors": [
                "Henry P McKean Jr."
            ],
            "title": "A class of markov processes associated with nonlinear parabolic equations",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1907
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "George Papamakarios",
                "Eric Nalisnick",
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Balaji Lakshminarayanan"
            ],
            "title": "Normalizing flows for probabilistic modeling and inference",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Hubert Ramsauer",
                "Bernhard Sch\u00e4fl",
                "Johannes Lehner",
                "Philipp Seidl",
                "Michael Widrich",
                "Thomas Adler",
                "Lukas Gruber",
                "Markus Holzleitner",
                "Milena Pavlovi\u0107",
                "Geir Kjetil Sandve"
            ],
            "title": "Hopfield networks is all you need",
            "year": 2008
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Erwin Schr\u00f6dinger"
            ],
            "title": "Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique",
            "venue": "In Annales de l\u2019institut Henri Poincare\u0301,",
            "year": 1932
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved techniques for training score-based generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Diederik P Kingma"
            ],
            "title": "How to train your energy-based models",
            "venue": "arXiv preprint arXiv:2101.03288,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Bharath Sriperumbudur",
                "Kenji Fukumizu",
                "Arthur Gretton",
                "Aapo Hyv\u00e4rinen",
                "Revant Kumar"
            ],
            "title": "Density estimation in infinite dimensional exponential families",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Bharath K Sriperumbudur",
                "Kenji Fukumizu",
                "Gert RG Lanckriet"
            ],
            "title": "Universality, characteristic kernels and rkhs embedding of measures",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Charles M Stein"
            ],
            "title": "Estimation of the mean of a multivariate normal distribution",
            "venue": "The annals of Statistics,",
            "year": 1981
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Taiji Suzuki",
                "Takafumi Kanamori"
            ],
            "title": "Density ratio estimation in machine learning",
            "year": 2012
        },
        {
            "authors": [
                "Yifei Wang",
                "Peng Chen",
                "Wuchen Li"
            ],
            "title": "Projected wasserstein gradient descent for high-dimensional bayesian inference",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification,",
            "year": 2022
        },
        {
            "authors": [
                "Romann M Weber"
            ],
            "title": "Exploiting the hidden tasks of gans: Making implicit subproblems explicit",
            "venue": "arXiv preprint arXiv:2101.11863,",
            "year": 2021
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In Proceedings of the 28th international conference on machine learning",
            "year": 2011
        },
        {
            "authors": [
                "2023. Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising",
            "year": 2023
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "2022) to derive the optimal denoiser for p. We define the denoising loss by E(Dp; \u03c3) = Ex\u223cpE\u03b5\u223cN",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. In this direction, we present the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\u00f6dinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. We also show that the training of generative adversarial networks includes a hidden data-optimization sub-problem, which induces the SD flow under certain choices of loss function when the discriminator is optimal. As a result, the SD flow provides a theoretical link between model classes that individually address the three challenges of the generative modeling trilemma\u2014high sample quality, mode coverage, and fast sampling\u2014thereby setting the stage for a unified approach."
        },
        {
            "heading": "1 Introduction",
            "text": "The goal of implicit generative modeling (IGM) is to create synthetic data samples that are indistinguishable from those drawn from a target distribution. A variety of approaches exist in the literature that address this problem from the perspective of the dynamics imposed upon the synthetic data during sampling or training. In particle-optimization sampling methods such as Langevin dynamics (Bussi & Parrinello, 2007; Welling & Teh, 2011), Hamiltonian Monte Carlo (MacKay, 2003), and Stein variational gradient descent (Liu & Wang, 2016), synthetic particles are drawn from a source distribution and perturbed over a number of steps until they resemble particles drawn from the target distribution. Separately, parametric models have been developed that either perform these perturbations implicitly under the hood, as in the case of normalizing flows (Rezende & Mohamed, 2015; Papamakarios et al., 2021), or do so explicitly during inference, as in the case of diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021).\nThis would seem to contrast with the training of parametric models for single-step generation by learning a mapping between input drawn from a noise distribution and synthetic output resembling the target data. Perhaps the most famous example of such a model is a generative adversarial network (GAN) (Goodfellow et al., 2014), which leverages a separately trained discriminator to guide the generator toward more targetlike output. However, we will show that GAN training contains a hidden sub-problem that induces a flow on the generated data that is completely determined by the loss being optimized. Consequently, this and various other IGM methods can be understood in terms of the dynamics imposed upon the synthetic data.\nThe question then becomes one of asking what the optimal dynamics might be for the IGM task. In this direction, we present the score difference (SD)\u2014the difference in the gradients of the log-densities of the target and source data distributions with respect to the data\u2014as the flow direction that optimally reduces the KL divergence between them at each step. We then argue that we can sidestep directly working with the source and target distributions in favor of operating on convenient proxy distributions with common support, which we show are aligned if and only if the original source and target distributions are aligned.\nar X\niv :2\n30 4.\n12 90\n6v 2\n[ cs\n.L G\n] 1\n8 Ju\nl 2 02\n3\nWe derive the score difference from the analysis of the dynamical systems that govern probability flow. But we also show that the score difference is hidden within or relates to various other IGM approaches, most notably denoising diffusion models and GANs, under certain conditions. We also outline a flexible algorithmic approach for leveraging SD flow when working with any source and target distributions, with no restrictions placed on either distribution.\nOur aim is to provide an accessible treatment of a complex topic with important connections to various areas of generative modeling and variational inference. The paper is organized around its key contributions as follows:\n1. In Section 2, we derive the score-difference (SD) flow from the study of probability flow dynamical systems and show that SD flow optimally reduces the Kullback-Leibler (KL) divergence between the source and target distributions and solves the Schr\u00f6dinger bridge problem.\n2. In Section 3, we consider modified proxy distributions for the source and target distributions, which have common support and are generally easier to manage and estimate than the unmodified distributions. We outline a method for aligning these proxies and show that this alignment occurs if and only if the unmodified distributions are aligned.\n3. In Section 4, we draw a connection between SD flow and denoising diffusion models and show that they are equivalent under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution.\n4. In Section 5, we show that GAN generator training is composed of two sub-problems, a particleoptimization step that induces a flow determined by the loss being optimized and a modeloptimization step, in which the flow-perturbed particles are fit by the generator via regression. We then show that the SD flow is induced in GANs in the particle-optimization step under certain conditions and choices of loss function.\n5. In Section 6, we present flexible algorithms for applying SD flow to both direct sampling (particle optimization) and parametric generator training (model optimization).\n6. In Section 7, we report experiments on our kernel-based implementation of SD flow, including comparisons to maximum mean discrepancy (MMD) gradient flow and Stein variational gradient descent (SVGD) under a variety of conditions.\nWe conclude in Section 8. In the appendices we provide supplemental information, discuss theoretical links to MMD gradient flow and SVGD, and report additional experimental results."
        },
        {
            "heading": "2 Probability Flow and the Score Difference",
            "text": ""
        },
        {
            "heading": "2.1 Derivation from Stochastic Differential Equations",
            "text": "Consider data x \u2208 Rd drawn from a base distribution q = q0. We can describe a dynamical system that perturbs the data and evolves its distribution q0 \u2192 qt over time by the stochastic differential equation\ndx = \u00b5(x, t)dt + \u03c3(t)d\u03c9, (1)\nwhere \u00b5 : Rd \u00d7 R \u2192 Rd is a drift coefficient, \u03c3(t) is a diffusion coefficient, and d\u03c9 denotes the standard Wiener process (Song et al., 2020).\nWhen \u00b5(x, t) = \u03c3(t) 2\n2 \u2207x log p(x), the diffusion in equation 1 describes Langevin dynamics, which for a suitable decreasing noise schedule \u03c3(t) can be shown to produce samples from a target distribution p as t\u2192\u221e (Bussi & Parrinello, 2007; Welling & Teh, 2011). We indicate this convergence by writing qt \u21dd p.\nAs the data points x are perturbed over time t, the distribution q0 evolves to qt. This evolution is described by the Fokker-Planck equation (Risken, 1996),\n\u2202qt(x) \u2202t\n= \u2212 d\u2211\ni=1\n\u2202\n\u2202xi [\u00b5i(x, t)qt(x)] + d\u2211 i=1 d\u2211 j=1 \u22022 \u2202xi\u2202xj [Dij(x, t)qt(x)]\n= \u2212\u2207x \u00b7 [\u00b5i(x, t)qt(x)] + \u03c3(t)2\n2 \u2207 2 xqt(x)\n= \u2212\u03c3(t) 2\n2 \u2207x \u00b7 [qt(x)\u2207x log p(x)\u2212\u2207xqt(x)]\n= \u2212\u03c3(t) 2\n2 \u2207x \u00b7 [qt(x) (\u2207x log p(x)\u2212\u2207x log qt(x))] ,\n(2)\nwhere D(x, t) is an isotropic diffusion tensor with elements Dij(x, t) = \u03c3(t) 2\n2 if i = j and Dij(x, t) = 0 otherwise, \u22072 = \u2207 \u00b7\u2207 represents the Laplacian operator,1 and \u03c3(t) and \u00b5(x, t) are defined as above. When qt = p, equation 2 vanishes, and the evolution of qt stops. When the drift term \u00b5(x, t) is defined as above, namely as the gradient of a potential,2 Jordan et al. (1998) show that the dynamics in equation 1 prescribe a direction of steepest descent on a free-energy functional with respect to the Wasserstein metric.\nA result due to Anderson (1982) shows that the forward dynamics in equation 1 can be reversed, effectively undoing the evolution of q to p. These reverse dynamics are given by\ndx = [ \u00b5(x, t)\u2212 \u03c3(t)2\u2207x log qt(x) ] dt + \u03c3(t)d\u03c9\u0302, (3)\nwhere now dt is a negative time step, and \u03c9\u0302 is a time-reversed Wiener process. The reverse of a diffusion process is therefore another diffusion process.\nRemarkably, there is a deterministic process with the same marginal densities as those prescribed by equation 3. The corresponding dynamics are given by the probability flow ODE (Maoutsa et al., 2020; Song et al., 2020),\ndx = [ \u00b5(x, t)\u2212 \u03c3(t) 2 2 \u2207x log qt(x) ] dt. (4)\nIf we substitute in the Langevin drift term \u00b5(x, t) = \u03c3(t) 2\n2 \u2207x log p(x) from above, equation 4 becomes\ndx = \u03c3(t) 2\n2 [\u2207x log p(x)\u2212\u2207x log qt(x)] dt. (5)\nSince dt is assumed to be a negative time step in the reverse process, equation 5 as written provides the dynamics of the forward process\u2014pushing qt toward the target distribution p\u2014when dt is positive. Equation 5 represents the score-difference (SD) flow of a probability distribution qt evolving toward p (or away from p, depending on the sign). Note that this is no longer a diffusion but rather defines a deterministic trajectory.\nCombining equation 2 and equation 5 yields\n\u2202qt(x) \u2202t\n= \u2212\u2207x \u00b7 [ qt(x)\ndx dt\n] . (6)\nThis is the Liouville equation (Ehrendorfer, 1994; Maoutsa et al., 2020), which describes the evolution of deterministic systems analogously to how the Fokker-Planck equation describes stochastic systems.\n1Many authors denote the Laplacian by \u2206, but we have reserved its use for discrete-time differences. 2Here the potential is given by U(x) = \u2212 log p(x)."
        },
        {
            "heading": "2.2 SD Flow Optimally Reduces KL Divergence",
            "text": "A continuous flow dx = f(x)dt can be approximated by defining a transformation T (x) = x+\u03b5f(x) for some small step \u03b5 > 0.3 If x \u223c q and x\u2032 = T (x) \u223c q[T ], then Liu & Wang (2016) show that the Kullback-Leibler (KL) divergence between q[T ] and p,\nDKL(q[T ]\u2225p) = Ex\u223cq[T ] [ log q[T ](x)\u2212 log p(x) ] , (7)\nvaries according to its functional derivative,\n\u2207\u03b5DKL(q[T ]\u2225p)|\u03b5=0 = \u2212Ex\u223cq[T ] [Tr (Apf(x))] , (8)\nwhere Apf(x) = \u2207x log p(x)f(x)\u22a4 +\u2207xf(x) (9)\nis the Stein operator (Gorham & Mackey, 2015).\nBy applying Stein\u2019s identity (Stein, 1981; Lin et al., 2019) to equation 8, we obtain Ex\u223cq[T ] [Tr (Apf(x))] = Ex\u223cq[T ] [ \u2207x log p(x)\u22a4f(x) ] \u2212 Ex\u223cq[T ] [ \u2207x log q[T ](x)\u22a4f(x) ] = Ex\u223cq[T ] [( \u2207x log p(x)\u2212\u2207x log q[T ](x)\n)\u22a4 f(x)] , (10) which is the inner product of the score difference and the flow vector f(x).4 Maximizing the reduction in the KL divergence (equation 8) corresponds to maximizing this inner product. Since the inner product of two vectors is maximized when they are parallel, choosing f(x) to output a vector parallel to the score difference will decrease the KL divergence as fast as possible. We can also see from equation 8 and equation 10 that the decrease in the KL divergence is then proportional to the Fisher divergence,\nDF(q[T ]\u2225p) = Ex\u223cq[T ] [ \u2225\u2207x log p(x)\u2212\u2207x log q[T ](x)\u22252 ] , (11)\nwhich, never being negative, shows that moving along the SD flow in sufficiently small steps will not increase the KL divergence.\nWe note that the SD flow also naturally emerges from variational gradient flows defined over f -divergences (Gao et al., 2019; Feng et al., 2021; Gao et al., 2022). Specifically, Gao et al. (2019) show that the negative gradient of the first variation of the f -divergence functional evaluated at q defines a flow that minimizes the divergence. When that f -divergence is the KL divergence, DKL(q\u2225p), the first variation is given by \u03b4F [q]/\u03b4q = log q(x) \u2212 log p(x) + 1, whose negative gradient is the SD flow. Dynamics closely resembling the SD flow also emerge in the study of optimal interventions for constraining stochastic interacting particle systems in Kullback-Leibler control problems (Maoutsa & Opper, 2022)."
        },
        {
            "heading": "2.3 SD Flow Solves the Schr\u00f6dinger Bridge Problem",
            "text": "The Schr\u00f6dinger bridge (SB) problem considers the most likely evolution between two marginal densities q and p over time t \u2208 [0, T ] (Chen et al., 2015). Specifically, the problem solves\nP \u2217 = arg min P \u2208\u03a0(q,p) DKL(P\u2225Q), (12)\nwhere \u03a0(q, p) is the collection of densities with marginals P0 = q and PT = p, and Q is a reference diffusion with initial density q that causes the solution to be unique (Winkler et al., 2023).5\nChen et al. (2021) show that solutions to the SB problem correspond to the following forward and backward SDEs:\ndx = [\u00b5(x, t) + \u03c32(t)\u2207 log \u03a8(x, t)]dt + \u03c3(t)d\u03c9 (13) dx = [\u00b5(x, t)\u2212 \u03c32(t)\u2207 log \u03a8\u0302(x, t)]dt + \u03c3(t)d\u03c9\u0302, (14)\n3If \u03b5 is sufficiently small, then the Jacobian of T is of full rank, meaning that the transformation is bijective. 4We assume the mild condition that f(x)p(x) \u2192 0 and f(x)q[T ](x) \u2192 0 as \u2225x\u2225 \u2192 \u221e. 5Note that we are considering the diffusion from q to p for consistency with the Langevin example in Section 2.1.\nwhere \u03a8 and \u03a8\u0302 are non-negative potentials (or Schr\u00f6dinger factors) such that \u03a8(x, t)\u03a8\u0302(x, t) = qt(x), and all other notation is as defined in Section 2.1. If we set \u03a8(x, t) = 1 and \u03a8\u0302(x, t) = qt(x), then the necessary conditions on the potentials are met, and the backward SDE (14) matches equation 3, from which the probability flow ODE in equation 4 and the SD flow in equation 5 directly follow. Therefore, SD flow defines a Schr\u00f6dinger bridge between the source and target distributions q and p."
        },
        {
            "heading": "3 Applying SD Flow to Proxy Distributions",
            "text": "One of the difficulties in applying Langevin dynamics or other score-based methods is the requirement that we have access to the true score of the target distribution, \u2207x log p(x), which is almost never available in practice. It is also the case that when operating in the ambient space of x \u2208 Rd, the score may not be well defined in areas of limited support if the data exist on a lower-dimensional manifold, which is generally assumed for a variety of data types of interest, such as image data. A large literature has emerged that is dedicated to the estimation of this score or the design of training procedures that are equivalent to estimating it (Hyv\u00e4rinen & Dayan, 2005; Song & Ermon, 2020; Song & Kingma, 2021; Karras et al., 2022).\nApplying SD flow would appear to be at least twice as difficult, since instead of one score to estimate, now we have two. The distribution qt is also changing over time, so even a reasonably good estimate at one time would have to be discarded and re-estimated at another. Our approach will be to essentially ignore p and qt and work instead with modified proxy distributions that are easier to estimate and manage. Importantly, aligning these proxy distributions will automatically align the unmodified source and target distributions."
        },
        {
            "heading": "3.1 Aligning Proxy Distributions",
            "text": "We can assess the alignment of two distributions q and p by computing a statistical distance between them.6 Although this quantity is not always a true \u201cdistance\u201d in a strict mathematical sense, it will have the two key properties that (1) D(q\u2225p) \u2265 0 for all distributions p, q and (2) D(q\u2225p) = D(p\u2225q) = 0 if and only if p = q. Perhaps the best-known statistical distance is the KL divergence, DKL(q\u2225p) (equation 7), although it can diverge to infinity if p and q have unequal support.\nOne way to equalize the support of two distributions is to corrupt their data with additive noise defined over the whole space Rd. Let us assume a Gaussian noise model. The distribution of z = x + \u03c3\u03f5, with x \u223c p and \u03f5 \u223c N (0, I), is given by the convolution p\u0303 = p \u2217 N (0, \u03c32I):\np\u0303(z) = p(z; \u03c3) = \u222b Rd p(x)N (z; x, \u03c32I) dx\n= Ex\u223cp [ N (z; x, \u03c32I) ] ,\n(15)\nwith q\u0303(z) = q(z; \u03c3) = Ey\u223cq [ N (z; y, \u03c32I) ] defined analogously. Although DKL(q\u0303\u2225p\u0303) \u2264 DKL(q\u2225p) and DKL(q\u0303\u2225p\u0303) \u2192 0 as \u03c3 \u2192 \u221e (Sriperumbudur et al., 2017), it is easy to show that DKL(q\u0303\u2225p\u0303) = 0 if and only if q = p (Zhang et al., 2020). As a result, aligning the proxy distributions q\u0303 and p\u0303 is equivalent to aligning the generative distribution q with the target distribution p.\nWe have shown that moving parallel to the score difference optimally reduces the KL divergence, so we define an SD flow between q\u0303 and p\u0303 to align q with p. The score corresponding to p\u0303(z) = p(z; \u03c3) (15) is\n\u2207z log p(z; \u03c3) = \u2207zp(z; \u03c3)\np(z; \u03c3)\n= Ex\u223cp\n[ \u2207zN (z; x, \u03c32I) ] Ex\u223cp [N (z; x, \u03c32I)]\n= 1 \u03c32\n( Ex\u223cp [ N (z; x, \u03c32I)x ] Ex\u223cp [N (z; x, \u03c32I)] \u2212 z ) .\n(16)\n6We will suppress the time index on q here for convenience.\nThe score for q\u0303(z) = q(z; \u03c3) is derived in the same way for z = y + \u03c3\u03f5, with y \u223c q. This leads to the following expression for the score difference:\n\u2207z log p(z; \u03c3)\u2212\u2207z log q(z; \u03c3) = 1 \u03c32 ( Ex\u223cp [K\u03c3(z, x)x] Ex\u223cp [K\u03c3(z, x)] \u2212 Ey\u223cq [K\u03c3(z, y)y] Ey\u223cq [K\u03c3(z, y)] ) , (17)\nwhere K\u03c3(z, x) = exp ( \u2212\u2225z\u2212x\u2225 2\n2\u03c32\n) is the Gaussian kernel.7\nIf we set the noise level according to the schedule \u03c3(t) = \u03c3, then the variance term cancels from equation 5, leading to the update\n\u2206z = \u03b72 [ Ex\u223cp [K\u03c3(z, x)x] Ex\u223cp [K\u03c3(z, x)] \u2212 Ey\u223cq [K\u03c3(z, y)y] Ey\u223cq [K\u03c3(z, y)] ] (18)\nfor some \u03b7 > 0 defining the step size.\nWe treat the dynamics of y \u223c q(y) as being the same as the dynamics of z \u223c q\u0303(z) and set \u2206y = \u2206z under the following rationale: At time t, we draw yt \u223c qt(y) and \u03f5 \u223c N (0, I) to form yt +\u03c3\u03f5 = zt \u223c q\u0303(z). We then perturb zt according to equation 18, forming zt + \u2206zt = zt+1 \u223c q\u0303t+1(z), which is closer to the distribution p\u0303(z) than the point zt was. However, there is a second way to obtain the point zt+1 \u223c q\u0303t+1(z), which is to first create yt + \u2206yt = yt+1 \u223c qt+1(y) via the (unknown) update \u2206yt and then corrupt it with noise to form yt+1 + \u03c3\u03f5 = zt+1 \u223c q\u0303t+1(z).8 If we think of our random noise \u03f5 as coming from a queue,9 then the value of \u03f5 is the same in both scenarios. The only difference is that \u03f5 is drawn from the queue at time t in the first scenario and is saved until time t + 1 in the second. As a result, we can equate the two definitions of zt+1 and write\nzt+1\ufe37 \ufe38\ufe38 \ufe37 yt + \u2206yt\ufe38 \ufe37\ufe37 \ufe38\nyt+1\n+\u03c3\u03f5 = zt+1\ufe37 \ufe38\ufe38 \ufe37 yt + \u03c3\u03f5\ufe38 \ufe37\ufe37 \ufe38\nzt\n+\u2206zt, (19)\nwhich, when we cancel like terms, leads to \u2206yt = \u2206zt. In this case the update \u2206y does not necessarily equal the score difference of the \u201cclean\u201d distributions, \u2207y log p(y)\u2212\u2207y log q(y), but is rather the perturbation to the clean data required for optimally aligning the proxies q\u0303 and p\u0303. Recalling that the alignment of the proxy distributions implies the alignment of the original distributions, the update \u2206y still serves to align q with p."
        },
        {
            "heading": "3.2 Limitations and Alternative Formulations of SD Flow",
            "text": "In the limit of infinite data, equation 17 is exact. But applying this formulation in a large-data setting can be computationally expensive, and estimates using smaller batches may suffer from low accuracy, especially in high dimensions. Recent work by Ba et al. (2021) shows that SVGD, which has a kernel-based specification similar to our description of SD flow in Section 3.1, tends to underestimate the target variance unless the number of particles is greater than the dimensionality. However, those authors note that MMD gradient descent (Arbel et al., 2019), which is closer in formulation to SD flow (see Appendix B.3), does not have this limitation. Additionally, recent work by Wang et al. (2022) proposed a Wasserstein gradient descent method essentially identical to the SD flow formulation in Section 3.1, albeit operating in a projected space, which the authors claim scales favorably to high dimensions.\nIn any case, it is useful to consider each term of equation 17 as a module that can be swapped out for another estimate, depending on the problem setting. For example, we can rewrite equation 17 in the equivalent form\n\u2207z log p(z; \u03c3)\u2212\u2207z log q(z; \u03c3) = 1 \u03c32 [E[x|z]\u2212 E[y|z]] (20)\n= 1 \u03c32 [ D\u2217p(z; \u03c3)\u2212D\u2217qt(z; \u03c3) ] , (21)\nwhere D\u2217p(z; \u03c3) and D\u2217qt(z; \u03c3) are the optimal denoising models for the distributions p and qt, respectively, when corrupted by Gaussian noise at level \u03c3. A simple derivation of this result is possible by rearranging\n7This is possible because the normalization constant of the normal distribution cancels from the numerator and denominator. 8Here we assume a constant noise schedule, \u03c3(t) = \u03c3 for all t. 9This is the case with random number tables as well as our computers\u2019 pseudorandom number generators.\nTweedie\u2019s formula (Efron, 2011),10 but we provide a separate proof of optimality in Appendix B.1. This formulation is particularly well-suited to high-dimensional applications, as it admits the use of specialized U-net architectures that form the backbone of modern denoising diffusion models (Karras et al., 2022).\nAs a practical consideration, the denoiser corresponding to the target data would need to be trained only once, while the denoiser for the generative distribution would, at least in principle, need to be retrained after each step along the flow. This is not in itself a major limitation, since the iterative fine-tuning of the generative-distribution denoiser Dqt is no more onerous than the standard practice of training a GAN generator and discriminator in alternating steps, although it does create a potential burden on resources by requiring a second denoiser to be loaded in memory. However, since we actually observe y \u223c qt before corrupting it to form z = y + \u03c3\u03f5, we can replace D\u2217qt(z; \u03c3) = E[y|z] with y in equation 21,\n11 leading to the update\ny \u2190 (1\u2212 \u03c1)y + \u03c1D\u2217p(z; \u03c3) (22)\nfor some small step size \u03c1. In Section 4, we show that this approximate formulation of SD flow is equivalent to the reverse process in denoising diffusion models."
        },
        {
            "heading": "4 Relation to Denoising Diffusion Models",
            "text": "In diffusion modeling, data from the target distribution p is corrupted in a forward diffusion process by Gaussian noise under the scale and noise schedules \u03b1t = \u03b1(t) and \u03c3t = \u03c3(t), respectively. Then for z0 = x \u223c p, the conditional distribution at time t relative to that at time s < t is given by\np(zt|zs) = N (\u03b1t|szs, \u03c32t|sI),\nwhere \u03b1t|s = \u03b1t/\u03b1s and \u03c32t|s = \u03c32t \u2212 \u03b12t|s\u03c32s (Kingma et al., 2021).\nThe hard part is inferring the reverse diffusion process, p(zs|zt), which is intractable unless also conditioned on z0 = x: p(zs|zt, x) = N (\u00b5s|t, \u03c32s|t), where \u00b5s|t = (\u03b1t|s\u03c32s/\u03c32t )zt + (\u03b1s\u03c32t|s/\u03c32t )x and \u03c32s|t = \u03c32t|s\u03c32s/\u03c32t . In practice, x is replaced by D(zt; \u03c3t), the output of a denoising model.12\nIf we let \u03b1s = \u03b1t = 1 for all s, t, then\nzs = \u03c32s \u03c32t\nzt + ( 1\u2212 \u03c3 2 s\n\u03c32t\n) D(zt; \u03c3t) + \u03c3s|t\u03f5s (23)\n= (1\u2212 \u03c1)zt + \u03c1D(zt; \u03c3t) + \u221a \u03c1\u03c3s\u03f5s (24)\nfor \u03f5s \u223c N (0, I) and \u03c1 = 1 \u2212 \u03c32s/\u03c32t . Recalling that in our framework, zt = yt + \u03c3t\u03f5t \u223c qt(zt; \u03c3t) for all t, equation 24 becomes\nzs = (1\u2212 \u03c1)yt + \u03c1D(zt; \u03c3t) + \u221a \u03c1\u03c3s\u03f5s + (1\u2212 \u03c1)\u03c3t\u03f5t\n= (1\u2212 \u03c1)yt + \u03c1D(zt; \u03c3t) + \u221a \u03c1\u03c32s + (1\u2212 \u03c1)2\u03c32t \u03f5\n= (1\u2212 \u03c1)yt + \u03c1D(zt; \u03c3t)\ufe38 \ufe37\ufe37 \ufe38 ys +\n\u221a( 1\u2212 \u03c3 2 s\n\u03c32t\n) \u03c32s + ( \u03c32s \u03c32t )2 \u03c32t\ufe38 \ufe37\ufe37 \ufe38\n\u03c3s\n\u03f5\n= ys + \u03c3s\u03f5,\n(25)\nwhere \u03f5, \u03f5s, \u03f5t \u223c N (0, I) and ys follows from equation 22. In other words, the updating process under SD flow is equivalent to the denoising diffusion reverse process under the substitution described in Section 3.2.\n10Tweedie\u2019s formula states that E[x|z] = z + \u03c32\u2207z log p(z; \u03c3). 11This is an approximation, since D\u2217qt (z; \u03c3) = E[y|z] will not necessarily equal y and may be closer to a local mean for large \u03c3.12In alternative but equivalent implementations, the error between x and zt is predicted by a parametric model \u03f5\u03b8(zt; t)."
        },
        {
            "heading": "5 Implicit Flows in Generative Adversarial Networks",
            "text": ""
        },
        {
            "heading": "5.1 Decomposing Generator Training into Sub-problems",
            "text": "When training a generative model g\u03b8, we define a loss L, which is a scalar function that quantifies the discrepancy between the current model output and the target distribution. We typically treat this loss as a function of the parameters \u03b8 \u2208 Rn and then optimize \u03b8 to minimize L via gradient13 descent at some learning rate \u03b7 > 0:\n\u03b8\u2032 = \u03b8 \u2212 \u03b7 (\n\u2202L \u2202\u03b8\n)\u22a4 . (26)\nHowever, the loss L is also a function of the generated data y = g\u03b8(\u03be) \u2208 Rd, which is itself a function of either \u03be \u2208 Rl or \u03b8 \u2208 Rn, depending on our perspective. This perspective can be made explicit by decomposing the derivative of the loss via the multivariate chain rule,\n\u2202L \u2202\u03b8\ufe38\ufe37\ufe37\ufe38 1\u00d7n = \u2202L \u2202y\ufe38\ufe37\ufe37\ufe38 1\u00d7d \u2202y \u2202\u03b8\ufe38\ufe37\ufe37\ufe38 d\u00d7n . (27)\nThis allows us to consider each component of the decomposition as corresponding to its own sub-problem.\nIn the first sub-problem, we perturb the generated data y in the direction of the negative gradient,\ny\u2032 = y \u2212 \u03bb1 (\n\u2202L \u2202y\n)\u22a4 , (28)\nwhere \u03bb1 > 0 is some small step size. Intuitively, the perturbed data y\u2032 corresponds to a potential output of the generator that would have a lower loss, but we can also interpret it as resulting from a gradient flow on the synthetic data. In GANs, this flow serves to approximately \u201cinvert\u201d the discriminator (Weber, 2021).\nIn the second sub-problem, we update the generator parameters \u03b8 by regressing the new, perturbed target y\u2032 on the original generator input \u03be via the least-squares loss,\nJ = 12\u2225g\u03b8(\u03be)\u2212 y \u2032\u22252 = 12\u2225y \u2212 y \u2032\u22252. (29)\nPutting the pieces together leads to the parameter update\n\u03b8\u2032 = \u03b8 \u2212 \u03bb2 (\n\u2202J \u2202\u03b8\n)\u22a4 (30)\n= \u03b8 \u2212 \u03bb2 (\n\u2202g\u03b8(\u03be) \u2202\u03b8\n)\u22a4 (g\u03b8(\u03be)\u2212 y\u2032) (31)\n= \u03b8 \u2212 \u03bb2 ( \u2202y\n\u2202\u03b8\n)\u22a4 (y \u2212 y\u2032) (32)\n= \u03b8 \u2212 \u03bb1\u03bb2 ( \u2202y\n\u2202\u03b8 )\u22a4( \u2202L \u2202y )\u22a4 (33)\n= \u03b8 \u2212 \u03bb1\u03bb2 (\n\u2202L \u2202\u03b8\n)\u22a4 , (34)\nwhich is is equal to the standard gradient update of \u03b8 under the original loss L (equation 26) with step size \u03b7 = \u03bb1\u03bb2. Here equation 33 follows from equation 32 by rearranging and substituting equation 28, while equation 34 follows from equation 33 via equation 27.\n13We treat the gradient as the transpose of the derivative.\nAlthough this decomposition is a direct consequence of gradient descent, it shows that hidden within generator training are two sub-problems with separate control options (their learning rates, for instance), each of which may be easier to conceptualize and handle than the original problem. In particular, we see that the model-optimization step of generator training is preceded, at least implicitly, by a particle-optimization step that prescribes a flow in the ambient data space Rd, regardless of the overall loss being optimized. This suggests that we can treat this particle-optimization step as a target-generation module that can be swapped out in favor of other procedures, such as SD flow. We note that this interpretation is consistent with recently proposed flow-based methods for training parametric generators (Gao et al., 2019; 2022). Furthermore, it suggests that a wide variety of generative models can be understood in terms of the dynamics imposed on the generated data during training."
        },
        {
            "heading": "5.2 SD Flow in GANs",
            "text": "Many GAN generators employ the widely used non-saturating loss (Goodfellow, 2016) given by\nL(\u03b8) = \u2212E\u03be\u223cp0 [log f(g\u03b8(\u03be))] \u2248 \u2212 1 |B| \u2211 y\u2208B\u223cqt log f(y), (35)\nwhere \u03be \u2208 Rl is a random noise input to the generator drawn from a prior distribution p0 and f : Rd \u2192 (0, 1) is a separately trained discriminator that estimates the probability that its argument is real data coming from a target distribution p (in which case f \u2248 1), as opposed to fake data coming from the generator distribution qt (in which case f \u2248 0). We note that in every practical case we are working with an empirical estimate of the expectation over a finite batch of generated data y = g\u03b8(\u03be) collected in the set B. Intuitively, the aim of the loss given by equation 35 is to tune the parameters \u03b8 to maximize the discriminator\u2019s assessment of generated data as real.\nIt can be shown that, if the prior probabilities of coming from either p or qt are equal, the Bayes optimal classifier ft is given by\nft(y) = p(y)\np(y) + qt(y) , (36)\nwhere we have included the time subscript to indicate the optimal discriminator\u2019s dependence on the changing distribution qt. An optimal discriminator is often assumed in the analysis of GANs but almost never holds in actual practice.\nWhen implemented as a neural network, the discriminator ft usually terminates with a sigmoid activation,\nft(y) = 1\n1 + exp[\u2212ht(y)] , (37)\nwhere ht(y) is the pre-activation output of the discriminator ft. Equating equation 36 and equation 37, we see that in the case of an optimal discriminator, ht(y) = log p(y) \u2212 log qt(y), whose gradient is the score difference,\n\u2207yht(y) = \u2207y log p(y)\u2212\u2207y log qt(y). (38)\nWhen trained using the non-saturating loss (equation 35), the gradient flow induced on a point y is\n\u2212\u2207yL = 1 |B| \u2207yft(y) ft(y) + 1 |B| \u2211 y\u2032\u2208B\\y \u2207yft(y\u2032) ft(y\u2032)\ufe38 \ufe37\ufe37 \ufe38\n0\n= 1 |B| (1\u2212 ft(y))\u2207yht(y)\n\u221d (1\u2212 ft(y)) [\u2207y log p(y)\u2212\u2207y log qt(y)] .\n(39)\nSince [1 \u2212 ft(y)] > 0 for all y, taking the results of Section 5.1 into account, we see that standard GAN training with an optimal discriminator consistently pushes the generated data toward the target data in a direction parallel to the score difference (equation 38).\nWe can also consider an alternative to the non-saturating loss that focuses on the sum of the discriminator\u2019s pre-activation outputs for generated data,\nLalt = \u2212 \u2211 y\u2208B ht(y), (40)\nwhich induces a flow exactly equal to the score difference when the discriminator is optimal:\n\u2212\u2207yLalt = \u2207yht(y) + \u2211\ny\u2032\u2208B\\y \u2207yht(y\u2032)\ufe38 \ufe37\ufe37 \ufe38 0\n= \u2207y log p(y)\u2212\u2207y log qt(y). (41)"
        },
        {
            "heading": "6 Algorithms",
            "text": "In this section we provide pseudocode algorithms for two main applications of SD flow: (1) direct sampling via particle optimization, which transports a set of particles from a source distribution to match a target distribution, interpolating between the distributions in the process, and (2) the model optimization of a parametric generator by (a) progressively perturbing generator output toward the target distribution and then (b) regressing those perturbed targets on the generator input. In Section 5, we showed that the training of a GAN generator can be decomposed into steps (a) and (b). This creates the opportunity to replace a separately trained discriminator with another target-generation method, such as SD flow, in step (a).\nThe algorithms reflect that SD flow has more than one representation or specification. There is the kernelbased specification (Section 3.1) derived from considering noise-injected proxy distributions; there is the denoiser-based specification (Section 3.2), which exploits a link between SD flow and diffusion models; and there is the density-ratio specification (Section 5) as estimated via a discriminator, such as one would use in GAN training. Other specifications and estimation methods are possible, especially considering the fundamental role played by the density ratio in machine learning applications (Sugiyama et al., 2012)."
        },
        {
            "heading": "6.1 Particle Optimization",
            "text": "Algorithm 1 Particle optimization with SD flow Input: Target data {xi}Ni=1 \u223c p, base (prior) data {yj}Mj=1 \u223c q0, noise schedule \u03c3(t), step schedule \u03b7(t) repeat\nDraw data batches x \u223c p and y \u223c qt Draw noise \u03f5 \u223c N (0, I) and perturb data: z = y + \u03c3(t)\u03f5 # Calculate SD (equation 18, 21, or 38). \u2206z \u221d Ex\u223cp[K\u03c3(t)(z,x)x] Ex\u223cp[K\u03c3(t)(z,x)] \u2212 Ey\u223cqt [K\u03c3(t)(z,y)y] Ey\u223cqt [K\u03c3(t)(z,y)]\n= D\u2217p(z; \u03c3(t))\u2212D\u2217qt(z; \u03c3(t)) \u221d \u2207zht(z) # Move (clean) data in SD direction. y \u2190 y + \u03b7(t)\u2206z\nuntil Terminated\nIn the particle-optimization application (Algorithm 1), a sample is generated by perturbing a single batch of \u201cbase data,\u201d much like one would via Langevin dynamics or Hamiltonian Monte Carlo. Here we interpret the base data as being drawn from q0 and evolving to the distribution qt \u21dd p through iterative perturbation."
        },
        {
            "heading": "6.2 Model Optimization",
            "text": "Ordinary regression problems involve paired training data {(\u03be, y)} implicitly defining the mapping g : \u03be 7\u2192 y to be learned. In the case of generative modeling, where the aim is to map data from a prior noise distribution to a target distribution, no a priori pairing exists. This requires the mapping to be learned indirectly. With GANs, the sub-problem interpretation of Section 5.1 is that the discriminator provides this pairing by associating a noise input \u03be with a perturbed output y\u2032 that then serves as a regression target.\nAlgorithm 2 Model optimization with SD flow Input: Target data {xi}Ni=1 \u223c p, noise input for generator \u03be \u223c p0, initialized generator g\u03b8 (\u03b8 \u2208 Rn), noise schedule \u03c3(t), step schedule \u03b7(t), learning rate schedule \u03bb(t) repeat\nDraw data batches x \u223c p and \u03be \u223c p0 Generate sample y = g\u03b8(\u03be) Draw noise \u03f5 \u223c N (0, I) and perturb data: z = y + \u03c3(t)\u03f5 # Calculate SD (equation 18, 21, or 38). \u2206z \u221d Ex\u223cp[K\u03c3(t)(z,x)x] Ex\u223cp[K\u03c3(t)(z,x)] \u2212 Ey\u223cqt [K\u03c3(t)(z,y)y] Ey\u223cqt [K\u03c3(t)(z,y)]\n= D\u2217p(z; \u03c3(t))\u2212D\u2217qt(z; \u03c3(t)) \u221d \u2207zht(z) # Move (clean) data in SD direction. y \u2190 y + \u03b7(t)\u2206z # Update generator parameters via regression. \u03b8 \u2190 \u03b8 \u2212 \u03bb(t)2 \u2207\u03b8\u2225g\u03b8(\u03be)\u2212 y\u2225 2\nuntil Terminated\nThe model-optimization application (Algorithm 2) trains a generator with unpaired data via regression on perturbed targets that move progressively closer to the target distribution. Here we interpret q0 as the distribution of the output of the generator g\u03b8 before training. As the generator regresses on perturbed targets, its output distribution changes to qt at time step t as governed by the Liouville equation.14 When the choice of SD flow is the gradient of the pre-activation discriminator output, \u2207zht(z) (equation 38), this algorithm is equivalent to GAN training with the alternative loss described in equation 40."
        },
        {
            "heading": "7 Experiments",
            "text": "Here we report several experiments on toy data. We focus in this section on particle-optimization applications using the kernel-based definition of SD flow (Section 3.1). This allows us to compare the performance of SD flow against two other kernel-based particle-optimization algorithms, namely MMD gradient flow (Arbel et al., 2019) and Stein variational gradient descent (SVGD) (Liu & Wang, 2016). Consistent with those works, we focus on low-dimensional data, although we present additional results on moderately high-dimensional data in a model-optimization application in Appendix C.4.\nWe leave a thorough exploration of our alternative specifications of SD flow (Section 3.2) on high-dimensional image data to follow-up work, since beyond the considerable computational and data demands in that setting, there are many architectural design choices and training tricks behind the current state of the art in image generation, which fall outside the scope of this paper. Nevertheless, several suggestions for applying SD flow to high-dimensional data, such as images, are given in Section 3.2. The general procedures given in Algorithms 1 and 2 still apply in the high-dimensional case."
        },
        {
            "heading": "7.1 Experimental Conditions",
            "text": "For the experiments reported in this section, we vary the conditions below, which we describe along with the labels used in Tables 1 and 2. All methods were tested with a default learning rate of \u03b7 = 0.1.\nADAGRAD: The published implementation of SVGD (Liu & Wang, 2016) relies on the adaptive gradient optimization algorithm AdaGrad (Duchi et al., 2011). We tested all algorithms using AdaGrad versus standard stochastic gradient descent (SGD).\nBATCH: The moderate size of our toy data sets allows us to test both batch-based and full-data versions of the algorithms. Batch sizes of 128 were used in the batch condition.\nCONST: In Section 4, we showed that an approximation of SD flow is equivalent to denoising diffusion under a decreasing noise schedule. However, we also showed in Section 3.1 that we can work entirely\n14See Section 2.1 and Appendix B.2.\nwith noise-corrupted proxy distributions, which allows for a constant noise schedule. Both MMD gradient flow and SVGD can be interpreted in the constant-noise setting, so we tested all algorithms with both constant and varying noise schedules. When using a constant noise schedule, we used the method described by Liu & Wang (2016), which determines the kernel bandwidth as a function of the median squared pairwise distance among the source (base) data and the number of points.15 In the non-constant setting, we used a modified cosine noise schedule to interpolate between a maximum and minimum variance: \u03c32(t) = \u03c32max cos(\u03c0t/2) for t \u2208 [0, tmax], with tmax = 2/\u03c0 cos\u22121(\u03c32min/\u03c32max).\nANNEAL: Although the kernel-based realization of SD flow is motivated by considering noise-annealed proxy distributions (Section 3.1), annealing is not necessary for SD flow to converge. Further, since SVGD does not anneal data during training, and MMD gradient flow introduces annealing only as a regularization technique, we tested all algorithms in both annealed and non-annealed conditions.\nOFFSET: In our experiments in R3, we introduced the condition of initializing the base distribution either away from the center of the target distribution (offset) or centered at the target distribution\u2019s mean.\nA discussion of the relationship between SD flow and MMD gradient flow is given in Appendix B.3 and between SD flow and SVGD in Appendix B.4. That discussion provides additional context behind some of the experimental conditions described above."
        },
        {
            "heading": "7.2 Results",
            "text": "To measure distribution alignment, we define a mean characteristic function distance (CFD),\nDCF(p\u2225qt) = 1 K K\u2211 k=1 |Ex\u223cp [ exp(i\u03c9\u22a4k x) ] \u2212 Ey\u223cqt [ exp(i\u03c9\u22a4k y) ] |,\nwhere i = \u221a \u22121 is the imaginary unit. DCF(p\u2225qt) is the mean absolute difference between the empirical characteristic functions of p and qt evaluated at K frequencies (K = 256 in our case) \u03c9k \u2208 Rd drawn from a normal distribution. For the reporting in Tables 1 and 2, we establish convergence in the following way: We compute the CFD for two independent random samples of the target distribution and record the maximum value over 1000 trials, which provides a threshold for the CFD estimated from finite data when the distributions are equal. We say that an algorithm has converged if the CFD between the synthetic and target distributions falls below this threshold."
        },
        {
            "heading": "7.2.1 Fitting a 25-Gaussian Grid",
            "text": "For our first particle-optimization experiment, we created a target distribution of 1024 points drawn from a mixture of 25 spherical Gaussians arranged on a grid in R2 and initialized 1024 points from a spherical Gaussian base distribution at a large distance from the target distribution but closest to its top-left component. (See Figure 1.) For the non-constant condition, we used a cosine noise schedule (described above) with \u03c32max = 10 and \u03c32min = 0.5. The results of this experiment are summarized in Table 1. Note that SD flow is the only algorithm to converge in every condition and that each algorithm consistently either converges or fails to converge in each condition."
        },
        {
            "heading": "7.2.2 Fitting a Gaussian Mixture",
            "text": "In this experiment, our target \u201cmystery distribution\u201d p is a mixture of 30 Gaussians in R3 arranged in the shape of a question mark. (See Figure 2.) In addition to the experimental conditions tested in Section 7.2.1, we also tested initializing the base distribution offset from the target distribution versus centered at the target distribution\u2019s mean. For the non-constant condition, we used a cosine noise schedule (described above) with \u03c32max = 4 and \u03c32min = 0.5. The convergence results are given in Table 2. Once again, SD flow is the only algorithm to converge in every condition.\n15\u03c32 = 2 \u00d7 median[D2(y, y\u2032)]/ log(N + 1)"
        },
        {
            "heading": "ADAGRAD BATCH CONST ANNEAL SD (OURS) MMD SVGD",
            "text": ""
        },
        {
            "heading": "7.2.3 Discussion of Experimental Results",
            "text": "Our experiments show that there are conditions under which all tested algorithms successfully and consistently transport particles from a source distribution to a target distribution. However, only SD flow showed itself to be robust to all experimental conditions. SVGD converged in every condition in which AdaGrad was employed in our experiments, where its performance was extremely close to that of SD flow. However, SVGD consistently failed to converge in any condition in which AdaGrad was not employed, while SD flow did converge in these conditions. MMD gradient flow fared worse in general and exhibited a tendency to cause a subset of the synthetic data to diverge.16 We note, however, that noise plays a different role in our setup than it does in the work of Arbel et al. (2019), where it serves a regularizing purpose.\nThe convergence results reported in Tables 1 and 2 were determined by comparing the minimum CFD achieved by an algorithm after 1000 iterations to a threshold empirically determined by multiple comparisons of independent random draws of the target distribution. Failure to meet this threshold could mean that the algorithm failed to steer the synthetic data toward the target distribution at all or simply failed to do so in the allotted number of iterations. For completeness, we provide the average minimum CFD values corresponding to Tables 1 and 2 in Appendix C.1. Additional experimental results are also reported in Appendix C."
        },
        {
            "heading": "8 Concluding Remarks",
            "text": "In this work, we presented the score-difference (SD) flow as an optimal trajectory for aligning a source distribution with a target distribution. We also showed that this alignment can be performed by working entirely with proxy distributions formed by smoothing the data with additive noise. As a result, while the SD flow defines a deterministic trajectory, its application to noise-injected points adds a stochastic component.\n16See Remark 1 in Appendix B.3.\nSD flow can be used as a direct sampling technique, in which case a sample of base data is converted to a sample from the target distribution via the flow. It can also be used in the training of parametric generative models, in which case generator output is perturbed by the flow to provide the generator with regression targets guaranteed to be closer to the target distribution than the previous output. Unlike most other scorebased methods, there are no restrictions on the choice of base or prior distributions. Consistent with this advantage, we have shown that SD flow forms a Schr\u00f6dinger bridge (Schr\u00f6dinger, 1932; De Bortoli et al., 2021) between source and target distributions.\nWe have shown that SD flow emerges in both GANs and diffusion models under certain conditions. The conditions for GANs include the presence of an optimal discriminator, which is often unattainable when training with finite, categorically labeled data. By contrast, diffusion models have the comparatively easier task of learning to denoise an image, a task for which the ground truth is more readily represented in the training data. Modern neural denoising architectures that employ attention provide another edge, since they have shown themselves to be extraordinarily capable of capturing patterns in data due to their errorcorrection and pattern-retrieval characteristics reminiscent of Hopfield networks (Ramsauer et al., 2020).\nSD flow supplies a link between IGM methods\u2014namely GANs and diffusion models\u2014that collectively perform well on all three desiderata of the so-called generative learning trilemma (Xiao et al., 2021): high sample quality, mode coverage, and fast sampling. Inserting SD flow as an alternative, non-adversarial target-generation module within generator training,17 replacing the need for an optimal discriminator, could lead to the development of \u201ctriple threat\u201d models that produce high-quality, diverse output in a single inference step. The various alternatives we have described in this paper for representing SD flow\u2014namely those based on kernels, denoisers, and density ratios\u2014suggest a variety of opportunities for integrating this approach into a number of IGM frameworks. We look forward to further developments in this direction.\n17See Section 5.1 and Appendix C.4."
        },
        {
            "heading": "ADAGRAD BATCH CONST ANNEAL OFFSET SD (OURS) MMD SVGD",
            "text": "Broader Impact Statement\nImplicit generative modeling in the image and text domains has matured to the point of producing output with an unprecedented level of realism, with other modalities not far behind. It is getting increasingly difficult to tell real data from fake, which is exciting when one reflects on how far the field has come in the past few years but also disturbing when one considers the consequences of advanced IGM methods in the hands of bad actors. We promote the responsible development and use of IGM not only with respect to its deployment but also its training, which should only use data with the proper usage rights secured. We also support continuing research into detecting generated or manipulated data in the effort of counteracting the misuse of this technology and minimizing the societal effects of any misuse."
        },
        {
            "heading": "A Guide to the Appendices",
            "text": "In Appendix B.1, we show that the score difference corresponds to the difference between the outputs of optimal denoisers corresponding to the target (p) and current synthetic (qt) distributions. In Appendix B.2, we describe the evolution of the generative distribution of a GAN under any loss. In Appendix B.3, we draw a connection between the kernel definition of SD flow and maximum mean discrepancy (MMD) gradient flow (Arbel et al., 2019). In Appendix B.4, we draw a connection between SD flow and Stein variational gradient descent (SVGD). Additional experimental results are reported in Appendix C."
        },
        {
            "heading": "B Supplemental Results",
            "text": ""
        },
        {
            "heading": "B.1 The Score Difference as the Difference of Optimal Denoisers",
            "text": "We follow an approach similar to that found in Appendix B.3 of Karras et al. (2022) to derive the optimal denoiser for p. We define the denoising loss by\nE(Dp; \u03c3) = Ex\u223cpE\u03f5\u223cN (0,\u03c32I) [ \u2225Dp(x + \u03f5; \u03c3)\u2212 x\u22252 ] = Ex\u223cpEz\u223cN (x,\u03c32I) [ \u2225Dp(z; \u03c3)\u2212 x\u22252\n] = Ez\u223cN (x,\u03c32I)Ex\u223cp [ \u2225Dp(z; \u03c3)\u2212 x\u22252\n] = \u222b Rd Ex\u223cp [ N (z; x, \u03c32I)\u2225Dp(z; \u03c3)\u2212 x\u22252\n]\ufe38 \ufe37\ufe37 \ufe38 E(Dp;z,\u03c3) dz.\n(42)\nWe can optimize E(Dp; \u03c3) by minimizing the integrand E(Dp; z, \u03c3) pointwise. The optimal denoiser is then given by\nD\u2217p(z; \u03c3) = arg min Dp(z;\u03c3) E(Dp; z, \u03c3), (43)\nwhich defines a convex optimization problem. We can then find the global minimum by setting \u2207Dp(z;\u03c3)E(Dp; z, \u03c3) to zero, leading to\nEx\u223cp [ N (z; x, \u03c32I)\u2207Dp(z;\u03c3)\u2225Dp(z; \u03c3)\u2212 x\u2225 2] = 0 2Dp(z; \u03c3)Ex\u223cp [ N (z; x, \u03c32I) ] = 2Ex\u223cp [ N (z; x, \u03c32I)x\n] D\u2217p(z; \u03c3) = Ex\u223cp [ N (z; x, \u03c32I)x ] Ex\u223cp [N (z; x, \u03c32I)] ,\n(44)\nthe final term of which is equal to the first term inside the brackets in equation 18 when K\u03c3 is the Gaussian kernel. The derivation for D\u2217qt(z; \u03c3) is identical, which leads to the result."
        },
        {
            "heading": "B.2 GAN Dynamics Under General Losses",
            "text": "Since the negative gradient of the GAN loss defines a flow on the generated data y, which the generator fits via regression, we can track the evolution of the synthetic distribution qt within the context of normalizing flows (Rezende & Mohamed, 2015; Papamakarios et al., 2021). In particular, the dynamics induced by a\ngenerator loss constitute, in the limit of arbitrarily small steps, a continuous normalizing flow whose effect on the synthetic (generated) data distribution is governed by the Liouville equation (see equation 6, Section 2.1),\n\u2202qt(yt) \u2202t = \u2207yt \u00b7 [qt(yt)\u2207yL] , (45)\na continuity equation with solution\nqt(yt) = q0(y0) exp (\u222b t\n0 Tr [HL(y\u03c4 )] d\u03c4\n) = q0(y0) exp (\u222b t 0 \u22072y\u03c4L d\u03c4 ) , (46)\nwhere HL(y\u03c4 ) is the Hessian matrix of the loss L evaluated at y\u03c4 , whose trace is the Laplacian \u22072yL =\u2211 i \u2202\n2/\u2202y2iL. Taking the logarithm of both sides of equation 46 yields the solution to the instantaneous change of variables differential equation (Chen et al., 2018; Grathwohl et al., 2018). Note that this evolution holds for any generator loss L and does not make any assumptions about an optimal discriminator."
        },
        {
            "heading": "B.3 Relation to MMD Gradient Flow",
            "text": "In the study of reproducing kernel Hilbert spaces (RKHS), the Gaussian kernel K\u03c3(z, x) is known as a characteristic kernel (Sriperumbudur et al., 2011). This means that the mapping \u03c6p(z) = Ex\u223cp[K\u03c3(z, x)] is injective, and \u03c6p(z) = \u03c6q(z) for all z if and only if p = q. This forms the basis of the maximum mean discrepancy (MMD), which is equal to the Hilbert space norm \u2225Wp,q\u2225H, where\nWp,q(z) = \u03c6q(z)\u2212 \u03c6p(z) = Ey\u223cq[K\u03c3(z, y)]\u2212 Ex\u223cp[K\u03c3(z, x)] (47)\nis known as the witness function (Gretton et al., 2012; Arbel et al., 2019).\nIn the theory of optimal transport, we wish to efficiently transport \u201cmass\u201d from an initial distribution q0 to a target distribution p, which we can do by defining a flow from q0 to p via intermediate distributions qt. One such flow is defined by the solution to\n\u2202qt \u2202t = \u2207 \u00b7 [qt\u2207Wp,qt ] , (48)\nanother instance of the Liouville equation (6, 45) that defines a McKean-Vlasov process (McKean Jr, 1966) with dynamics\ndzt = \u2212\u2207ztWp,qt(zt) dt = (Ex\u223cp[\u2207ztK\u03c3(zt, x)]\u2212 Ey\u223cq[\u2207ztK\u03c3(zt, y)]) dt,\n(49)\nwhere z0 \u223c q0. The results of Section 3.1 suggest that, in the limit of infinite data, this direction is proportional to \u2207ztp(zt; \u03c3)\u2212\u2207ztqt(zt; \u03c3).\nFor the Gaussian kernel, we have \u2207ztK\u03c3(zt, x) = K\u03c3(zt, x) (\nx\u2212 zt \u03c32\n) ,\nand for discrete time and finite data we can write equation 49 as\n\u2206zt = 1 N N\u2211 i=1 [ K\u03c3(zt, xi) ( xi \u2212 zt \u03c32 )] \u2212 1 M M\u2211 j=1 [ K\u03c3(zt, yj) ( yj \u2212 zt \u03c32 )] (50)\n= N\u2211\ni=1 w\n(p) i xi \u2212 M\u2211 j=1 w (qt) j yj +  M\u2211 j=1 w (qt) j \u2212 N\u2211 i=1 w (p) i  zt, (51) where w(p)i = K\u03c3(zt, xi)/(N\u03c32) and w (qt) j = K\u03c3(zt, yj)/(M\u03c32). This process defines the MMD gradient flow (Arbel et al., 2019). The kernel version of SD flow (equation 18) can also be written in the form\nof equation 51 by setting w(p)i = 12 K\u03c3(zt, xi)/ \u2211N i=1 K\u03c3(zt, xi) and w (qt) j = 12 K\u03c3(zt, xi)/ \u2211M j=1 K\u03c3(zt, yj), which causes the zt term to vanish.\nThere are practical consequences of this difference in weighting schemes between methods, which put the MMD gradient flow at a disadvantage in some conditions, as discussed in the following remark.\nRemark 1 If p and qt are far apart, for a point zt = y + \u03c3\u03f5, with y \u223c qt and a small kernel bandwidth \u03c3, equation 51 shows that under the MMD framework \u2211 j w (qt) j \u2248 1/(M\u03c32) and \u2211 i w (p) i \u2248 0. This suggests that under these conditions, the MMD gradient flow direction \u2206zMMDt will be nearly parallel to zt \u2212 y = \u03c3\u03f5, which would have the effect of increasing the variance of qt while not necessarily pushing it toward p. Evidence of this variance-exploding effect emerged in our experiments and also apparent in the authors\u2019 original paper (e.g. Appendix G.2 therein). Normalizing the weights in equation 51 to sum to one resolves this issue, however, and MMD gradient flow and SD flow then become equivalent.\nRemark 2 Our method prescribes that we inject noise at a level equal to the kernel bandwidth in order to sample from the noise-smoothed proxy distribution. In contrast, with MMD gradient flow the noise level is a separate parameter that essentially controls a regularization effect. In that case, this added noise is typically at a level far greater than that of the kernel bandwidth, which remains fixed during training."
        },
        {
            "heading": "B.4 Relation to Stein Variational Gradient Descent (SVGD)",
            "text": "Another statistical distance that measures the discrepancy between distributions p and q is the Stein discrepancy (Gorham & Mackey, 2015),\nDS(p\u2225q) = Ex\u223cq[Tr (Apf(x))], (52)\nwhere Ap is the Stein operator defined in equation 9, and f is a function that vanishes on the boundary of the support of p, q or (equivalently) behaves such that f(x)p(x)\u2192 0 and f(x)q(x)\u2192 0 as \u2225x\u2225 \u2192 \u221e.\nThe discrepancy 52 vanishes if and only if p = q, which can be seen by expanding out equation 52 as in equation 10, Ex\u223cq[Tr (Apf(x))] = Ex\u223cq [ (\u2207x log p(x)\u2212\u2207x log q(x))\u22a4 f(x) ] , (53)\nsince for nontrivial functions f , the above vanishes only when \u2207x log p(x) = \u2207x log q(x). When f is restricted to bounded functions within a reproducing kernel Hilbert space (RKHS), one obtains the kernel Stein discrepancy (Liu et al., 2016).\nAs reported in Section 2.2, Liu & Wang (2016) establish a link between the kernel Stein discrepancy and the variation in the KL divergence, which leads to their derivation of Stein variational gradient descent (SVGD) as an optimal direction for reducing the KL divergence between q and p when operating in a RKHS:\n\u03d5(z) = Ex\u223cq [\u2207x log p(x)K(z, x) +\u2207xK(z, x)] = Ex\u223cq [(\u2207x log p(x)\u2212\u2207x log q(x)) K(z, x)] ,\n(54)\nwhich shows SVGD to be the kernel-weighted average of the score difference. Ba et al. (2021) provide a separate analysis of the connection between SVGD and MMD gradient flow."
        },
        {
            "heading": "C Additional Experimental Results",
            "text": ""
        },
        {
            "heading": "C.1 Average CFD Values for Particle-Optimization Experiments",
            "text": "In Sections 7.2.1 and 7.2.2, we reported convergence probabilities for SD flow, MMD gradient flow, and SVGD under various experimental conditions. Convergence was defined as achieving a minimum characteristic function distance (CFD) below a threshold empirically determined by multiple comparisons of independent copies of the target distribution (0.0651 for the 25-Gaussian grid problem in R2 and 0.0612 for the 30-Gaussian \u201cmystery distribution\u201d in R3). In Tables 3 and 4, we report average CFD minimum values achieved by the algorithms after 1000 steps over five separate trials."
        },
        {
            "heading": "ADAGRAD BATCH CONST ANNEAL SD (OURS) MMD SVGD",
            "text": ""
        },
        {
            "heading": "C.2 Data-Set Interpolation",
            "text": "Standard score-based generative modeling is designed such that the end of the forward process is a Gaussian distribution. While this has the advantage of defining a prior that is easy to sample from for the reverse, generative process, it limits the flexibility of the method. Recent work on approximating a Schr\u00f6dinger bridge between source and target distributions (De Bortoli et al., 2021) relaxes this limitation, but the method itself is relatively complicated.\nSD flow, on the other hand, is (in our opinion) a simpler and more intuitive method that also solves the Schr\u00f6dinger bridge problem (see Section 2.3) and places no restrictions on the distributions p and q. It is therefore also capable of performing interpolation between arbitrary data sets. The results of one such interpolation experiment are shown in Figure 3. The figure actually shows two interpolation experiments: The first evolves 1024 points of the \u201cSwiss roll\u201d data toward the \u201cmystery\u201d distribution (Section 7.2.2) in R3, while the second evolves from the \u201cmystery\u201d distribution to the \u201cSwiss roll.\u201d The same cosine variance schedule as in Section 7.2.2 was employed."
        },
        {
            "heading": "C.3 Nearest-Neighbor Analysis",
            "text": "It is important to note that SD flow does not cause the synthetic data to collapse to nearest neighbors in the target distribution. In Figure 4, we show the distribution of distances from points in the synthetic distribution to their first nearest neighbors in the target distribution (shaded in green) for the particleoptimization experiment described in Section 7.2.2. Note the overlap with the distribution of distances between target data points and their first nearest neighbors (excluding themselves) in the target distribution. Overfitting to the target data would result in a large concentration of mass near zero for the synthetic data."
        },
        {
            "heading": "C.4 Model Optimization",
            "text": "Although we do not run experiments on high-dimensional image data in the present work for the reasons described in Section 7, we report here an experiment using the model-optimization application (Algorithm 2) on \u201chigh\u201d-dimensional data in R50. Here the scare quotes acknowledge that this dimensionality is far lower"
        },
        {
            "heading": "ADAGRAD BATCH CONST ANNEAL OFFSET SD (OURS) MMD SVGD",
            "text": "than the thousands to millions of dimensions typical in high-resolution image data, but it is high enough to exhibit the problematic, intuition-challenging characteristics of high-dimensional data in general.\nSpecifically, it is well known that as data dimensionality grows, the ratio of the distance of a point to its farthest neighbor, Dmax, and the distance to its nearest neighbor, Dmin, tends toward unity. The ratio Dmax/Dmin drops precipitously in lower dimensions before leveling off at around 30 dimensions and very slowly approaching an asymptote of one afterward (Beyer et al., 1999). We therefore chose R50 as a reasonable setting to challenge our approach in high dimensions.\nWe generated a ground-truth target distribution by randomly populating a 50 \u00d7 25 matrix B with values drawn from N (0, 0.25I) and a 50-vector \u00b5 with values drawn from N (10, I). Target data samples from N (\u00b5, BB\u22a4) were then generated18 by drawing samples \u03be \u223c N (0, I) \u2208 R25 and forming x = B\u03be + \u00b5. The\n18Technically, this is not completely well defined as a normal distribution, since BB\u22a4 is not of full rank.\nmodel parameters to be learned were a 50 \u00d7 25 matrix B\u0302, initialized from N (0, 0.01I), and a 50-vector \u00b5\u0302, initialized to all zeros. This model can be interpreted as a single-layer linear neural network, but it is most important to note that it exactly matches the capacity of the data-generating model.\nIf we retained the input vectors \u03be \u2208 \u039e for the outputs x \u2208 X, then the task of learning the parameters would be fairly straightforward in the context of a regression problem on paired data {(\u03be, x)}. But in the general IGM problem, we have only unpaired data to work with, so we assume that all information about the target data inputs is unavailable.\nWe performed 1000 steps of SD flow using Algorithm 2 with a constant noise schedule of 10 times the average distance of the initial synthetic (base) distribution to first nearest neighbors in the target distribution\n(corresponding to \u03c32 > 700),19 with a batch size of 1024, an SD flow step size of \u03b7 = 1, and a regression learning rate of \u03bb = 10\u22123. Other than brief experimentation to set reasonable values, no effort was made to optimize these hyperparameters.\nThe results of this experiment are shown in Figure 5. Despite (or perhaps because of ) a massive and constant injection of noise, SD flow successfully fit the target distribution. Analysis of nearest neighbors once again showed that SD flow did not overfit to the target distribution, although there was a very slight shift toward lower distances between synthetic data and their nearest neighbors in the target distribution as compared with the target data\u2019s nearest-neighbor distances relative to itself.\nThis experiment provides a basic proof of concept for a much more general procedure, one that can benefit from more sophisticated model architectures specifically suited to the problem at hand. For instance, for image generation, the kernel-based specification of SD flow from Section 3.1 can be exchanged for the denoising-based specification from Section 3.2, allowing one to take advantage of attention-equipped U-net denoising architectures. Furthermore, methods can be mixed and matched: A denoising model can be employed for the p score component while a kernel-based estimate can be used for the qt score component, for example.\n19We found that using a higher amount of noise somewhat improved the convergence profile of the algorithm."
        }
    ],
    "title": "The Score-Difference Flow for Implicit Generative Modeling",
    "year": 2023
}