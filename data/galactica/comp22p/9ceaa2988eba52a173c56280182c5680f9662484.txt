Since its conclusions The time series early classification task as commonly understood may not be a meaningful problem to solve. All current research efforts that address this problem will be condemned to being overwhelmed by false positives if actually deployed in a real-world setting. Of course, false positives are a fact of life for any machine learning problem. However, the unique claims of immediate actionability mean that these false positives will have a cost, and the false positives may be many orders of magnitude more common than true positives. In addition, virtually all the algorithms are making the assumption that the data they are seeing now is normalized relative to data that only exists in the future. All those algorithms are condemned to producing mostly false negatives. We believe that the issue is not with the proposed algorithms per se. The issue is that the definition of the problem itself is intrinsically underspecified and vague. The following are our recommendations to bring clarity to the ETSC area:
• An effort should be made to provide a concrete, testable, falsifiable, and useful definition of early classification of time series. While the current authors have no interest in providing this definition (in any case, a consortium of researchers would be better), we believe that any such definition would, at a minimum, have to consider:
1) The cost of a false positive for the actionable class(es) vs. the cost of a false negative [12], [19]. Even if the only early action taken is to sound an alarm, false alarm fatigue is known to have a high cost [21]. 2) The probability that the domain of interest contains prefixes, inclusions, and homophones that resemble the actionable class(es). 3) The prior probability of seeing a member of the actionable class(es). 4) The appropriateness of the normalization assumptions for the domain. • Anyone proposing an ETSC model needs to carefully explain what the model offers beyond simply classification with trivial awareness that not all datapoints matter (recall Fig. 9). • It is hard to see how any genuine progress could be made without access to a real-world publicly available dataset(s) that could benefit from the
more concrete definition. The overreliance on the UCR datasets seems to have led the community astray here. Proxy datasets and synthetic datasets do have their place in research, especially in fledging areas. However, we are now two decades and many dozens of papers into this area. It is hard to overemphasize the last point. If no realworld publicly available dataset(s) where some form of ETSC is useful can be obtained, this seems tantamount to saying that there is no problem to solve, and the community should stop publishing on this topic. It is stunning to think of the ease with which a grad student can obtain seismic data recorded on Mars, or the mitochondria DNA of a mammoth that has been extinct for a million years, yet everyone publishing on ETSC must resort to proxy datasets. appendix a ON THE TERM EARLY CLASSIFICATION The term “Early Classification” is unfortunately overloaded and vague. There are several tasks that might be named as such, which do not fall under the purview of this paper. For example:
• Suppose that a boiler is rated for at most 200 psi. If a sensor detects increasing pressure readings: 180, 181, 182, …, it would make perfect sense to sound an early warning that the pressure may approach 200 psi. Note that this setting only considers the value of a time series, not the shape of the time series. The same is true for many medical domains: if a person’s BMI is measured monthly and begins to creep up to 20, 21, 22, …, it might be better for a doctor to suggest an intervention before it reaches 25. But again, only the value, not the shape matters. • Monitoring of batch processes is a slight generalization of the above. At every time point in a single run (plus or minus some “wiggle room” that can be modeled [25]), we know what range of values are acceptable. If the reading begins to drift outside that range, we can sound an alarm. Once again, this problem only considers the value of a time series, not the shape of the time series. • Suppose that a chicken engaging in dustbathing more than 40 times a day is required to be culled by local ordinance (because dustbathing is often caused by the presence of mites or other pests) [26]. If we detect 10 bouts of dustbathing one day and 25 the next day, we may want to take some early intervention. Note that this setting only considers the frequency of (fully observed, not “early” observed) behaviors. More generally, there may be other problems that have been labeled “early classification” by someone. We make no claims about such work. Our claims are limited to the sense of early classification used in [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], where the prefix of the shape of the time series is assumed to contain information that we can act upon before seeing the remainder of the shape. appendix b objections to our claims Given the unusual nature of our claims, we solicited feedback from the community while writing this paper. We did this by writing to every author that published a paper on ETSC, and by general postings on discussion boards such as www.reddit.com/r/MachineLearning/
Most of the feedback has been (gratefully) incorporated into the main text. Here, we respond to a few questions that are worth addressing but would spoil the flow of our paper:
• Q) Doesn’t the fact that there are commercial predictive text algorithms for handwriting tell us that the prefix/ inclusion/homophone problems can be overcome? A) These systems are not doing predictive classification based on words, they are classifying individual letters, and then using classic ASCII predictive text algorithms. Moreover, as the Google help page notes “Stand-alone symbols that are just a line (1/l/I) or circle (o/O/0) can be difficult to distinguish” [27], exactly because those groups of symbols appear as homophones in time series space. • Q) Your claim “it is not clear any of them could ever work in a real-world setting” seems too strong. A) Let us clarify what it means for a model to “work” here. Simply producing plots like Fig. 8 is not sufficient. Every event we are trying to detect has a cost. For concreteness, let us consider petrochemical engineering, and say the target event is the undesirable foaming of a distillation column. Assume it costs $1,000 to clean out the apparatus after such an event. Let us further imagine that if we get “early” notice that this is about to happen, we can warn an engineer to throttle some valve, and stop the damage. This action must also have some cost, let us say $200. Thus, in order for an ETSC model to be said to work, it must at least break even, producing at least one true positive for every five false positives. A handful of ETSC papers do have costs built into their models [12], [19], but they only test on UCR datasets and never estimate costs for any real-world applications. The results shown in this paper suggest that the vast majority of positives will be false positives. For example, we applied the model in [2] to the GunPoint problem, with the exemplars inserted in between long stretches of random walks, and we see thousands of false positives for every true positive (see [28]). • Q) Doesn’t the homophone problem imply that all time series classification is hard, not just ETSC? A) Yes, it does to some extent. Even if you ignore the issues of early classification, and consider only classic time series classification, the UCR datasets seem to have led to an illusion of progress. However, at least some applications do bypass this problem. For example, there are many papers on using the time series obtained from the sensors in a Wii Remote to classify gestures as inputs to the
system. Normally, the user presses a button that indicates “start classifying” and releases it once the gesture is recognized. This means that the algorithm is not asked to deal with spurious data that might be thousands of times more frequent than target data. Such uses of time series classification do largely fit into the UCR format assumptions. Likewise, objects that come from the spectrogram and (converted from 2D) shape datatypes are presented as discrete vectors, not part of a stream. • Q) I don’t see why z-normalization would be imperative in all real problems. A) We think this question has been addressed in [24] and elsewhere by the community. However, in brief: it is meaningful to compare time series based on z-normalized shape; it is sometimes meaningful to compare time series based on mean value; but it is almost never meaningful to cluster on both at the same time (which is equivalent to comparing nonnormalized time series with shape measures). The reason is that even small differences in the mean (and/or the standard deviation) completely drown out any shape information. In other words, for non-normalized data, dist(mean(a), mean(b)) ∝ dist(a, b), where dist is the Euclidean distance or DTW, etc. To summarize, if z-normalization is not important in your domain, it is virtually certain that the shapes do not matter — only the absolute values do. We make no claim about such situations other than the obvious empirical observation that such domains are very rare. 