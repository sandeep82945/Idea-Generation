This work presents ‘BanglaNLG,’ a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language. We aggregate six challenging conditional text generation tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue generation in the process. Furthermore, using a clean corpus of 27.5 GB of Bangla data, we pretrain ‘BanglaT5’, a sequenceto-sequence Transformer language model for Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks, outperforming several multilingual models by up to 9% absolute gain and 32% relative gain. We are making the new dialogue dataset and the BanglaT5 model publicly available at https://github. com/csebuetnlp/BanglaNLG in the hope of advancing future research on Bangla NLG. 1 conclusion & future works NLP research in low-resource languages is lagging behind due to the lack of reliable benchmarks and datasets. To facilitate the development, evaluation, and comparison of new NLG models, we introduced a multi-task evaluation benchmark for Bangla NLG, a widely spoken yet low-resource language. We presented BanglaT5, a pretrained NLG model in Bangla, setting new state-of-the-art results with BanglaT5. We strongly believe that our contributions in this work will help the Bangla NLP community benchmark NLG tasks more easily under a unified setup. Limitations
Although Bhattacharjee et al. (2022) claimed that Bangla2B+, the pretraining corpus for BanglaT5, had been carefully filtered for offensive or unwanted texts, they alerted that there might be small amounts of these contents may be present, which can result in bias or toxicity in the pretrained model. We, therefore, recommend using BanglaT5 with caution, especially for real-world deployment. Ethics Statement
License The TyDiQA dataset (Clark et al., 2020) is released under the Apache License 2.0, allowing modifications and distribution. All other pretraining and fine-tuning datasets are released under the
Creative Commons Attribution-NonCommercialShareAlike 4.0 International License (CC BY-NCSA 4.0), which allows modifications and distributions for non-commercial research purposes. We strictly adhere to these licenses and will release BanglaT5 and BanglaNLG benchmark resources under CC BY-NC-SA 4.0. Annotation Expert translators who provide translation services for renowned Bangla newspapers were hired to translate the evaluation sets of the dialogue dataset. Each translated sentence was further assessed for quality by another expert. It was again translated by the original translator if found to be of low quality. If the re-translation was found to be of low quality, it was then translated by the other expert. The experts were paid hourly as per standard rates in local currency. Hallucinated Text It is well-known that text generation models can hallucinate outputs that may not necessarily be faithful to the original input (Maynez et al., 2020). Though the texts may be fluent and human-like, the hallucinations may be factually inconsistent and impact the outputs negatively. BanglaT5 may be susceptible to the same kinds of hallucinations. Carbon Footprint We avoided using large models for pretraining and fine-tuning, reducing their environmental impacts. BanglaT5 was trained for about 30 days on Google v3 TPUs. Google’s TPUs are specifically designed for machine learning, which makes them up to five times more efficient than GPUs. Assuming 0.080kg carbon emission per kWh,5 the pretraining would emit fewer than 100kg carbon into the environment, far below most computationally demanding models. All fine-tuning experiments were done on a desktop machine with an 8-core Intel Core-i7 11700k CPU and NVIDIA RTX 3090 GPU, and no single run except machine translation took more than 12 hours, which amounts to fewer than 0.5kg carbon emission. On average, machine translation runs took three days each, emitting less than 3kg of carbon. acknowledgements We would like to thank the Research and Innovation Centre for Science and Engineering (RISE), BUET, for funding the project and Google TPU Research Cloud (TRC) program for providing cloud support. 5https://blog.google/technology/ai/ minimizing-carbon-footprint/ a multi-turn dialogue scores In Table 3, we mention BLEU-1, BLEU-2, BLEU3, and BLEU-4 scores for different models in the multi-turn dialogue generation task. b cross-lingual capabilities of banglat5 Despite being a monolingual model pretrained on heavily filtered Bangla data, BanglaT5 exhibits strong cross-lingual abilities, particularly in the machine translation (MT) task. In addition to the quality and size of the fine-tuning dataset, this performance can also be attributed to the presence of a significant amount of non-Bangla tokens (∼10.3%) in the BanglaT5 vocabulary. Since Bhattacharjee et al. (2022) curated the Bangla2B+ corpus by document-level language filtering, these documents preserve foreign text sequences occurring in the Bangla documents. We deliberately maintain these tokens while training the vocabulary of BanglaT5, using a relatively high character coverage. Our rationale behind doing this was to capture code-switching and allow better generalization across languages co-occurring with Bangla, as well as romanized forms of Bangla texts during fine-tuning, which is reflected in the MT results. However, it should be noted that the quality and size of fine-tuning data are essential for a strong cross-lingual performance since the mere existence of foreign tokens in the vocabulary is not enough to produce meaningful generation performance, as demonstrated by the poor performance in the cross-lingual summarization (XLS) task. This phenomenon has been studied in-depth by Blevins and Zettlemoyer (2022) in the context of pretrained language models in English, where they showed that these models develop strong
cross-lingual transfer capabilities due to the nonnegligible amount of foreign text present in the pretraining data and robustness to UNK tokens during fine-tuning.