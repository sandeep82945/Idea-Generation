{
    "abstractText": "The recently proposed camouflaged object detection (COD) attempts to segment objects that are visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic similarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To deal with these problems, we propose a mixed-scale triplet network, ZoomNet, which mimics the behavior of humans when observing vague images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully explores imperceptible clues between the candidate objects and background surroundings. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization constraint, uncertainty-aware loss, to promote the model to accurately produce predictions with higher confidence in candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the existing 23 state-of-the-art methods on four public datasets. Besides, the superior performance over the recent cuttingedge models on the SOD task also verifies the effectiveness and generality of our model. The code will be available at https://github.com/lartpang/ZoomNet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Youwei Pang"
        },
        {
            "affiliations": [],
            "name": "Xiaoqi Zhao"
        },
        {
            "affiliations": [],
            "name": "Tian-Zhu Xiang"
        },
        {
            "affiliations": [],
            "name": "Lihe Zhang"
        },
        {
            "affiliations": [],
            "name": "Huchuan Lu"
        }
    ],
    "id": "SP:db84a3fff52cf6d1c05e54b7cb934d4dba3e37c0",
    "references": [
        {
            "authors": [
                "Radhakrishna Achanta",
                "Sheila Hemami",
                "Francisco Estrada",
                "Sabine S\u00fcsstrunk"
            ],
            "title": "Frequency-tuned salient region detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, number CONF,",
            "year": 2009
        },
        {
            "authors": [
                "Edward Adelson",
                "Charles Anderson",
                "James Bergen",
                "Peter Burt",
                "Joan Ogden"
            ],
            "title": "Pyramid methods in image processing",
            "venue": "RCA Eng., 29,",
            "year": 1983
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Shuhan Chen",
                "Xiuli Tan",
                "Ben Wang",
                "Xuelong Hu"
            ],
            "title": "Reverse attention for salient object detection",
            "venue": "In Proceedings of European Conference on Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Zuyao Chen",
                "Qianqian Xu",
                "Runmin Cong",
                "Qingming Huang"
            ],
            "title": "Global context-aware progressive aggregation network for salient object detection",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ming-Ming Cheng",
                "Yun Liu",
                "Tao Li",
                "Ali Borji"
            ],
            "title": "Structure-measure: A new way to evaluate foreground maps",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Cheng Gong",
                "Yang Cao",
                "Bo Ren",
                "Ming- Ming Cheng",
                "Ali Borji"
            ],
            "title": "Enhanced-alignment measure for binary foreground map evaluation",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Ming-Ming Cheng",
                "Ling Shao"
            ],
            "title": "Concealed object detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Guolei Sun",
                "Ming-Ming Cheng",
                "Jianbing Shen",
                "Ling Shao"
            ],
            "title": "Camouflaged object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Tao Zhou",
                "Geng Chen",
                "Huazhu Fu",
                "Jianbing Shen",
                "Ling Shao"
            ],
            "title": "Pranet: Parallel reverse attention network for polyp segmentation",
            "venue": "In International Conference on Medical Image Computing and Computer- Assisted Intervention,",
            "year": 2020
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Tao Zhou",
                "Ge-Peng Ji",
                "Yi Zhou",
                "Geng Chen",
                "Huazhu Fu",
                "Jianbing Shen",
                "Ling Shao"
            ],
            "title": "Inf-net: Automatic covid-19 lung infection segmentation from ct images",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Shang-Hua Gao",
                "Yong-Qiang Tan",
                "Ming-Ming Cheng",
                "Chengze Lu",
                "Yunpeng Chen",
                "Shuicheng Yan"
            ],
            "title": "Highly ef- 14 ficient salient object detection with 100k parameters",
            "venue": "Proceedings of European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Wei Ji",
                "Ge Yan",
                "Jingjing Li",
                "Yongri Piao",
                "Shunyu Yao",
                "Miao Zhang",
                "Li Cheng",
                "Huchuan Lu"
            ],
            "title": "Dmra: Depth-induced multi-scale recurrent attention network for rgb-d saliency detection",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Trung-Nghia Le",
                "Tam V. Nguyen",
                "Zhongliang Nie",
                "Minh- Triet Tran",
                "Akihiro Sugimoto"
            ],
            "title": "Anabranch network for camouflaged object segmentation",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2019
        },
        {
            "authors": [
                "Aixuan Li",
                "Jing Zhang",
                "Yunqiu Lyu",
                "Bowen Liu",
                "Tong Zhang",
                "Yuchao Dai"
            ],
            "title": "Uncertainty-aware joint salient object and camouflaged object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Guanbin Li",
                "Yizhou Yu"
            ],
            "title": "Visual saliency based on multiscale deep features",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Tony Lindeberg"
            ],
            "title": "Scale-Space Theory in Computer Vision, pages 187\u2013226",
            "year": 1994
        },
        {
            "authors": [
                "Tony Lindeberg"
            ],
            "title": "Feature detection with automatic scale selection",
            "venue": "International Journal of Computer Vision,",
            "year": 1998
        },
        {
            "authors": [
                "Jiang-Jiang Liu",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "title": "Dynamic feature integration for simultaneous detection of salient object, edge and skeleton",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Jiang-Jiang Liu",
                "Qibin Hou",
                "Ming-Ming Cheng",
                "Jiashi Feng",
                "Jianmin Jiang"
            ],
            "title": "A simple pooling-based design for realtime salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Nian Liu",
                "Junwei Han",
                "Ming-Hsuan Yang"
            ],
            "title": "Picanet: Learning pixel-wise contextual attention for saliency detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nian Liu",
                "Ni Zhang",
                "Kaiyuan Wan",
                "Ling Shao",
                "Junwei Han"
            ],
            "title": "Visual saliency transformer",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yun Liu",
                "Xin-Yu Zhang",
                "Jia-Wang Bian",
                "Le Zhang",
                "Ming-Ming Cheng"
            ],
            "title": "SAMNet: Stereoscopically attentive multi-scale network for lightweight salient object detection",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Zhiming Luo",
                "Akshaya Mishra",
                "Andrew Achkar",
                "Justin Eichel",
                "Shaozi Li",
                "Pierre-Marc Jodoin"
            ],
            "title": "Non-local deep features for salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yunqiu Lyu",
                "Jing Zhang",
                "Yuchao Dai",
                "Aixuan Li",
                "Bowen Liu",
                "Nick Barnes",
                "Deng-Ping Fan"
            ],
            "title": "Simultaneously localize, segment and rank the camouflaged objects",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ran Margolin",
                "Lihi Zelnik-Manor",
                "Ayellet Tal"
            ],
            "title": "How to evaluate foreground maps",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Haiyang Mei",
                "Ge-Peng Ji",
                "Ziqi Wei",
                "Xin Yang",
                "Xiaopeng Wei",
                "Deng-Ping Fan"
            ],
            "title": "Camouflaged object segmentation with distraction mining",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Youwei Pang"
            ],
            "title": "Pysodevaltoolkit: A python-based evaluation toolbox for salient object detection and camouflaged object detection",
            "venue": "https://github.com/lartpang/ PySODEvalToolkit,",
            "year": 2020
        },
        {
            "authors": [
                "Youwei Pang"
            ],
            "title": "Pysodmetrics: A simple and efficient implementation of sod metrcis",
            "venue": "https://github.com/ lartpang/PySODMetrics,",
            "year": 2020
        },
        {
            "authors": [
                "Youwei Pang",
                "Lihe Zhang",
                "Xiaoqi Zhao",
                "Huchuan Lu"
            ],
            "title": "Hierarchical dynamic filtering network for rgb-d salient object detection",
            "venue": "In Proceedings of European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Youwei Pang",
                "Xiaoqi Zhao",
                "Lihe Zhang",
                "Huchuan Lu"
            ],
            "title": "Multi-scale interactive network for salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo P\u00e9rez-de la Fuente",
                "Xavier Delcl\u00f2s",
                "Enrique Pe\u00f1alver",
                "Mariela Speranza",
                "Jacek Wierzchos",
                "Carmen Ascaso",
                "Michael S Engel"
            ],
            "title": "Early evolution and ecology of camouflage",
            "venue": "in insects. PNAS,",
            "year": 2012
        },
        {
            "authors": [
                "Xuebin Qin",
                "Zichen Zhang",
                "Chenyang Huang",
                "Chao Gao",
                "Masood Dehghan",
                "Martin Jagersand. Basnet"
            ],
            "title": "Boundaryaware salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: towards real-time object detection with region proposal networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Przemys\u0142aw Skurowski",
                "Hassan Abdulameer",
                "Jakub B\u0142aszczyk",
                "Tomasz Depta",
                "Adam Kornacki",
                "Przemys\u0142aw Kozie\u0142"
            ],
            "title": "Animal camouflage analysis: Chameleon database, 2017",
            "venue": "http://kgwisc.aei.polsl.pl/index. php / pl / dataset / 63 - animal - camouflage analysis",
            "year": 2017
        },
        {
            "authors": [
                "Martin Stevens",
                "Sami Merilaita"
            ],
            "title": "Animal camouflage: Current issues and new perspectives",
            "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences, 364:423\u20137,",
            "year": 2008
        },
        {
            "authors": [
                "Yujia Sun",
                "Geng Chen",
                "Tao Zhou",
                "Yi Zhang",
                "Nian Liu"
            ],
            "title": "Context-aware cross-level fusion network for camouflaged object detection",
            "venue": "International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Lijun Wang",
                "Huchuan Lu",
                "Yifan Wang",
                "Mengyang Feng",
                "Dong Wang",
                "Baocai Yin",
                "Xiang Ruan"
            ],
            "title": "Learning to detect salient objects with image-level supervision",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Wenguan Wang",
                "Shuyang Zhao",
                "Jianbing Shen",
                "Steven CH Hoi",
                "Ali Borji"
            ],
            "title": "Salient object detection with pyramid attention and salient edges",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jun Wei",
                "Shuhui Wang",
                "Qingming Huang"
            ],
            "title": "F3net: Fusion, feedback and focus for salient object detection",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Wei",
                "Shuhui Wang",
                "Zhe Wu",
                "Chi Su",
                "Qingming Huang",
                "Qi Tian"
            ],
            "title": "Label decoupling framework for salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "A. Witkin"
            ],
            "title": "Scale-space filtering: A new approach to multiscale description",
            "venue": "In International Conference on Acoustics, Speech and Signal Processing,",
            "year": 1984
        },
        {
            "authors": [
                "Runmin Wu",
                "Mengyang Feng",
                "Wenlong Guan",
                "Dong Wang",
                "Huchuan Lu",
                "Errui Ding"
            ],
            "title": "A mutual learning method for salient object detection with intertwined multi-supervision",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Wu",
                "Li Su",
                "Qingming Huang"
            ],
            "title": "Cascaded partial decoder for fast and accurate salient object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Wu",
                "Li Su",
                "Qingming Huang"
            ],
            "title": "Stacked cross refinement network for edge-aware salient object detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Binwei Xu",
                "Haoran Liang",
                "Ronghua Liang",
                "Peng Chen"
            ],
            "title": "Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection",
            "venue": "AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jinnan Yan",
                "Trung-Nghia Le",
                "Khanh-Duy Nguyen",
                "Minh- Triet Tran",
                "Thanh-Toan Do",
                "Tam V. Nguyen"
            ],
            "title": "Mirrornet: Bio-inspired camouflaged object segmentation",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "Qiong Yan",
                "Li Xu",
                "Jianping Shi",
                "Jiaya Jia"
            ],
            "title": "Hierarchical saliency detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Chuan Yang",
                "Lihe Zhang",
                "Huchuan Lu",
                "Xiang Ruan",
                "Ming-Hsuan Yang"
            ],
            "title": "Saliency detection via graph-based manifold ranking",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Fan Yang",
                "Qiang Zhai",
                "Xin Li",
                "Rui Huang",
                "Ao Luo",
                "Hong Cheng",
                "Deng-Ping Fan"
            ],
            "title": "Uncertainty-guided transformer reasoning for camouflaged object detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zi-Hang Jiang",
                "Francis E.H. Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Zeng",
                "Pingping Zhang",
                "Jianming Zhang",
                "Zhe Lin",
                "Huchuan Lu"
            ],
            "title": "Towards high-resolution salient object detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Qiang Zhai",
                "Xin Li",
                "Fan Yang",
                "Chenglizhao Chen",
                "Hong Cheng",
                "Deng-Ping Fan"
            ],
            "title": "Mutual graph learning for camouflaged object detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jing Zhang",
                "Deng-Ping Fan",
                "Yuchao Dai",
                "Saeed Anwar",
                "Fatemeh Sadat Saleh",
                "Tong Zhang",
                "Nick Barnes"
            ],
            "title": "Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jing Zhang",
                "Xin Yu",
                "Aixuan Li",
                "Peipei Song",
                "Bowen Liu",
                "Yuchao Dai"
            ],
            "title": "Weakly-supervised salient object detection via scribble annotations",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Miao Zhang",
                "Tingwei Liu",
                "Yongri Piao",
                "Shunyu Yao",
                "Huchuan Lu"
            ],
            "title": "Auto-MSFNet: Search Multi-Scale Fusion Network for Salient Object Detection, page 667\u2013676",
            "year": 2021
        },
        {
            "authors": [
                "Jia-Xing Zhao",
                "Jiang-Jiang Liu",
                "Deng-Ping Fan",
                "Yang Cao",
                "Jufeng Yang",
                "Ming-Ming Cheng"
            ],
            "title": "Egnet: Edge guidance network for salient object detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Zhao",
                "Xiangqian Wu"
            ],
            "title": "Pyramid feature attention network for saliency detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoqi Zhao",
                "Youwei Pang",
                "Jiaxing Yang",
                "Lihe Zhang",
                "Huchuan Lu"
            ],
            "title": "Multi-source fusion and automatic predictor selection for zero-shot video object segmentation",
            "venue": "In Proceedings of the ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoqi Zhao",
                "Youwei Pang",
                "Lihe Zhang",
                "Huchuan Lu",
                "Xiang Ruan"
            ],
            "title": "Self-supervised pretraining for rgb-d salient object detection",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoqi Zhao",
                "Youwei Pang",
                "Lihe Zhang",
                "Huchuan Lu",
                "Lei Zhang"
            ],
            "title": "Suppress and balance: A simple gated network for salient object detection",
            "venue": "In Proceedings of European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoqi Zhao",
                "Lihe Zhang",
                "Huchuan Lu"
            ],
            "title": "Automatic polyp segmentation via multi-scale subtraction network",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoqi Zhao",
                "Lihe Zhang",
                "Youwei Pang",
                "Huchuan Lu",
                "Lei Zhang"
            ],
            "title": "A single stream network for robust and real-time rgb-d salient object detection",
            "venue": "In Proceedings of European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Zhirui Zhao",
                "Changqun Xia",
                "Chenxi Xie",
                "Jia Li"
            ],
            "title": "Complementary Trilateral Decoder for Fast and Accurate Salient Object Detection, page 4967\u20134975",
            "venue": "Association for Computing Machinery,",
            "year": 2021
        },
        {
            "authors": [
                "Yunfei Zheng",
                "Xiongwei Zhang",
                "Feng Wang",
                "Tieyong Cao",
                "Meng Sun",
                "Xiaobing Wang"
            ],
            "title": "Detection of people with camouflage pattern via dense deconvolution network",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2019
        },
        {
            "authors": [
                "Huajun Zhou",
                "Xiaohua Xie",
                "Jian-Huang Lai",
                "Zixuan Chen",
                "Lingxiao Yang"
            ],
            "title": "Interactive two-stream decoder for accurate and fast saliency detection",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Camouflaged objects are often \u201cseamlessly\u201d integrated into the environment by changing their appearance, coloration or pattern to avoid detection, such as chameleons, cuttlefishes and flatfishes. This is mainly due to their self-protection mechanism in the harsh living environment.\n\u2020These authors contributed equally to this work. *Corresponding author.\nBroadly speaking, camouflaged objects also refer to the objects that are extremely small in size, highly similar to the background, or heavily obscured. They subtly hide themselves in the surroundings, making them difficult to be found, e.g., soldiers wearing camouflaged uniforms and lions hiding in the grass. Camouflaged object detection (COD) is far more complex and challenging than traditional salient object detection or other object segmentation. Recently, it has attracted ever-growing research interest from the computer vision community and facilitates many valuable real-life applications, such as search and rescue [8], species discovery [36], and medical image analysis [10, 11, 68].\nRecently, numerous deep learning-based methods have been proposed and achieved significant progress. Nevertheless, they are still struggled to accurately and reliably detect camouflaged objects, due to visual insignificance of camouflaged objects, and high diversity in scale, appearance\nar X\niv :2\n20 3.\n02 68\n8v 1\n[ cs\n.C V\n] 5\nM ar\n2 02\nand occlusion. By observing our experiments, it is found that the current COD detectors are susceptible to distractors from background surroundings. Thus it is difficult to excavate discriminative and subtle semantic cues for camouflaged objects, resulting in the inability to clearly segment the camouflaged objects from the chaotic background and the predictions of some uncertain (low-confidence) regions. Taking these into mind, in this paper, we summarize the COD issue into two aspects: 1) How to accurately locate camouflaged objects under conditions of inconspicuous appearance and various scales? 2) How to suppress the obvious interference from the background and infer camouflaged objects more reliably? Intuitively, to accurately find the vague or camouflaged objects in the scene, humans may try to refer to and compare the changes in the shape or appearance at different scales by zooming in and out (rescaling) the image. This specific behavior pattern of human beings motivates us to identify camouflaged objects by mimicking the zooming in and out strategy.\nWith this inspiration, in this paper, we propose a mixedscale triplet network, ZoomNet, which significantly improves the existing camouflaged object detection performance. Firstly, for accurate object location, we employ scale space theory [20,21,48] to imitate zooming in and out strategy. Specifically, we design two key modules, i.e., the scale integration unit (SIU) and the hierarchical mixed-scale unit (HMU). As shown in Fig. 1, our model extracts differentiated camouflaged object features at different \u201czoom\u201d scales using the triplet architecture, then adopts SIUs to screen and aggregate scale-specific features, and utilizes HMUs to further reorganize and enhance mixed-scale features. Thus, our model is able to mine the accurate and subtle semantic clues between objects and background under the mixed scales, and produce accurate predictions. Besides, we use the shared weight strategy, which achieves a good balance of efficiency and effectiveness. Secondly, it is related to reliable prediction in complex scenarios. Although the object is accurately located, the indistinguishable texture and background will easily bring negative effects to the model learning, e.g. predicting uncertain/ambiguity regions, which greatly reduces the detection performance and cannot be ignored. This can be seen in Fig. 6 (Row 3 and 4) and Fig. 1 in the supp. To this end, we design an uncertainty-aware loss (UAL) to guide the model training, which is only based on the prior knowledge that a good COD prediction should have a clear polarization trend. Its GT-independent characteristic makes it suitable for enhancing the GT-based BCE loss. This targeted enhancement strategy can force the network to optimize the prediction of the uncertain regions during the training process, enabling our ZoomNet to distinguish the uncertain regions and segment the camouflaged objects reliably.\nOur contributions can be summarized as follows: 1)\nFor the COD task, we propose a mixed-scale triplet network, ZoomNet, which can credibly capture the objects in complex scenes by characterizing and unifying the scalespecific appearance features at different \u201czoom\u201d scales and the purposeful optimization strategy. 2) To obtain the discriminative feature representation of camouflaged objects, we design SIUs and HMUs to distill, aggregate and strengthen the scale-specific and subtle semantic representation for accurate COD. 3) We propose a simple yet effective optimization enhancement strategy, UAL, which can significantly suppress the uncertainty and interference from the background without increasing extra parameters. 4) Our model greatly surpasses recent 23 state-of-the-art methods under seven metrics on four COD datasets. Furthermore, it shows good generalization in the SOD task and the superior performance compared with the existing SOD methods."
        },
        {
            "heading": "2. Related Work",
            "text": "Camouflaged Object. The study of camouflage has a long history in biology. This behavior of creatures in nature can be regarded as the result of natural selection and adaptation. In fact, in human life and other parts of society, it also has a profound impact, e.g., arts, popular culture, and design. More details can be found in [42]. In the field of computer vision, research on camouflaged objects is often associated with salient object detection (SOD), which mainly deals with those salient and easily observed objects in the scene. In general, saliency models are designed for the general observation paradigm (i.e., finding visually prominent objects). They are not suitable for the specific observation (i.e., finding concealed objects). Therefore, it is necessary to establish models based on the essential requirements and specific data of the task to learn the special knowledge. Camouflaged Object Detection (COD). Different from the traditional SOD task, the COD pays more attention to the undetectable objects (mainly because of too small size, occlusion, concealment or self-disguise). Due to the differences in the attributes of the objects of interest, the goals of the two tasks are different. The difficulty and complexity of the COD far exceed the SOD due to the high similarity between the object and the environment. Some valuable attempts have been made in recent years. Recent works [16, 29, 59] construct the multi-task learning framework in the prediction process of camouflaged objects and introduce some auxiliary tasks like classification and edge detection. Some uncertainty-aware methods [17, 56] are proposed to model and cope with the uncertainty in data annotation or COD data itself. In the other two methods [31, 43], contextual feature learning also plays an important role. There are also a number of bio-inspired methods, such as [9, 53]. They capture camouflaged objects by imitating the behavior process of hunters or changing the viewpoint of the scene. Although our method can also be at-\ntributed to the last category, ours is different from the above methods. Our method simulates the behavior of humans to understand complex images by zooming in and out strategy. The proposed method explores the scale-specific and imperceptible semantic features under the mixed scales for accurate predictions, with the supervision of BCE and our proposed uncertainty-aware loss. Accordingly, our method achieves a more comprehensive understanding of the scene, and accurately and robustly segments the camouflaged objects from the complex background, which even can be transferred to the SOD task effectively and smoothly.\nScale Space Integration. The scale-space theory aims to promote an optimal understanding of image structure, which is an extremely effective and theoretically sound framework for addressing naturally occurring scale variations. Its ideas have been widely used in computer vision, including the image pyramid [2] and the feature pyramid [19]. Due to the structural and semantic differences at different scales, the corresponding features play different roles. However, the commonly-used inverted pyramidlike feature extraction structures [13, 38, 65] often cause the feature representation to lose too much texture and appearance details, which are unfavorable for dense prediction tasks [27, 39] that emphasize the integrity of regions and edges. Thus, some recent CNN-based COD methods [9,31,43,59] and SOD methods [15,23,34,35,66,67,69] explore the combination strategy of inter-layer features to enhance the feature representation. These bring some positive gains for accurate localization and segmentation of objects. However, for the COD task, the existing approaches overlook the performance bottleneck caused by the ambiguity of the structural information of the data itself that makes it difficult to be fully perceived at a single scale.\nDifferent from them, we mimic the zoom strategy to synchronously consider differentiated relationships between object and background at multiple scales, thereby fully perceiving the camouflaged objects and confusing scenes. Besides, we also further explore the fine-grained feature scale space between channels."
        },
        {
            "heading": "3. Proposed Method",
            "text": "In this section, we first elaborate on the overall architecture of the proposed ZoomNet, and then present the details of each module and the uncertainty-aware loss."
        },
        {
            "heading": "3.1. Overall Architecture",
            "text": "The overall architecture of the proposed ZoomNet is illustrated in Fig. 2. Inspired by the zoom strategy from human beings when observing confusing scenes, we argue that different zoom scales often contain their specific information. Aggregating the differentiated information on different scales will benefit exploring the inconspicuous yet valuable clues from confusing scenarios, thus facilitating COD. To implement it, intuitively, we resort to the image pyramid. Specifically, we customize an image pyramid based on the single scale input to identify the camouflaged objects. The scales are divided into a main scale (i.e. the input scale) and two auxiliary scales. The latter is obtained by re-scaling the former to imitate the operation of zooming in and out. We utilize the shared triplet feature encoder to extract features on different scales and feed them to the scale merging layer. To integrate these features that contain rich scale-specific information, we design a series of scale integration units (SIUs) based on the attention-aware filtering mechanism. Thus, these auxiliary scales are integrated into the main scale, i.e., information aggregation of \u201czoom in and out\u201d op-\neration. This will largely enhance the model to distill critical and informative semantic cues for capturing difficultto-detect camouflaged objects. After that, we construct hierarchical mixed-scale units (HMUs) to gradually integrate multi-level features in a top-down manner to enhance the mixed-scale feature representation. It further increases the receptive field range and diversifies feature representation within the module. The captured fine-grained and mixedscale clues promote the model to accurately segment the camouflaged objects in the chaotic scenes. Besides, to overcome the uncertainty in the prediction caused by the inherent complexity of the data, we design an uncertainty-aware loss (UAL) to assist the BCE loss, enabling the model to distinguish these uncertain regions and produce an accurate and reliable prediction."
        },
        {
            "heading": "3.2. Triplet Feature Encoder",
            "text": "We start by extracting deep features through a shared triplet feature encoder for the group-wise inputs, which consists of the feature extraction and the channel compression networks, i.e. E-Net and C-Net. For the trade-off between efficiency and effectiveness, the main scale and the two auxiliary scales are empirically set to 1.0\u00d7, 1.5\u00d7 and 0.5\u00d7. E-Net is constituted by the commonly-used ResNet50 [14] that is removed the structure after \u201clayer4\u201d. CNet is cascaded to further optimize computation and obtain a more compact feature. For more details about it, please see the supp. Thus, three sets of 64-channel feature maps corresponding to three input scales are produced, i.e., {fki }5i=1, k \u2208 {0.5, 1.0, 1.5}. Next, these features are fed successively to the scale merging layer and the hierarchical mixed-scale decoder for subsequent processing."
        },
        {
            "heading": "3.3. Scale Merging Layer",
            "text": "We design an attention-based SIU to screen (weight) and combine scale-specific information, as shown in Fig. 3. Several such units make up the scale merging layer. Through filtering and aggregation, the expression of different scales is self-adaptively highlighted. Before scale integration, the features f1.5i and f 0.5 i are first resized to be consistent resolution with the main scale feature f1.0i . Specif-\nically, for f1.5i , we use a hybrid structure of \u201cmax-pooling + average-pooling\u201d to down-sample it, which helps to preserve the effective and diverse responses for camouflaged objects in high-resolution features. For f0.5i , we directly upsample it by the bi-linear interpolation. Then, these features are fed into the \u201cattention generator\u201d, and a three-channel feature map is calculated through a series of convolutional layers. After a softmax activation layer, the attention map Ak (k \u2208 {0.5, 1.0, 1.5}) corresponding to each scale can be obtained and used as respective weights for the final integration. The process is formulated as:\nAi = softmax(\u03a8( [ U(f0.5i ), f1.0i ,D(f1.5i ) ] , \u03c6)),\nfi = A 0.5 i \u00b7 U(f0.5i ) +A1.0i \u00b7 f1.0i +A1.5i \u00b7 D(f1.5i ),\n(1)\nwhere \u03a8(?, \u03c6) indicates the stacked \u201cConv-BN-ReLU\u201d layers in the attention generator, and \u03c6 means the parameters of these layers. [?] represents the concatenation operation. D and U refer to the hybrid pooling and bi-linear interpolation operations mentioned above, respectively. Note that some operations before and after the sampling operation are not shown in Equ. 1 for simplicity but can be seen in Fig. 3. These designs aim to selectively aggregate the scale-specific information to explore subtle but critical semantic cues at different scales, boosting the feature representation."
        },
        {
            "heading": "3.4. Hierarchical Mixed-scale Decoder",
            "text": "After SIUs, the auxiliary-scale information is integrated into the main-scale branch. Similar to the multi-scale case, different channels also contain differentiated semantics. Thus, it is necessary to excavate valuable clues contained in different channels. To this end, we design HMUs to conduct information interaction and feature refinement between channels, which strengthen features from coarsegrained group-wise iteration to fine-grained channel-wise modulation in the decoder, as depicted in Fig. 4. The input f\u0302i of the HMUi contains the multi-scale fused feature\nfi from the SIUi and the feature f\u0303i+1 from the HMUi+1:\nf\u0302i = fi + U(f\u0303i+1). (2)\nGroup-wise Iteration. We adopt 1 \u00d7 1 convolution to extend the channel number of feature map f\u0302i. The features are then divided into G groups {gj}Gj=1 along the channel dimension. Feature interaction between groups is carried out in an iterative manner. Specifically, the first group {g1} is split into three feature sets {g\u2032k1}3k=1 after a convolution block. Among them, the g\u203211 is adopted for information exchange with the next group, and the other two are used for channel-wise modulation. In the jth (1 < j < G) group, the feature gj is concatenated with the feature g\u2032 1 j\u22121 from the previous group along the channel, followed by a convolution block and a split operation, which similarly divides this feature group into three feature sets. It is noted that the output of the group G with the similar input form to the previous groups only contains g\u20322G and g \u20323 G. Such an iterative mixing strategy strives to learn the critical clues from different channels and obtain a powerful feature representation. From another perspective, the iterative structure in HMU can be equivalent to a kernel pyramid structure. Channel-wise Modulation. The features [{g\u20322j}Gj=1] are concatenated and converted into the feature modulation vector \u03b1 by a small convolutional network, which is employed to weight another concatenated feature [{g\u20323j}Gj=1]. The weighted feature is then processed by a convolutional layer, which is defined as:\nf\u0303i = A(f\u0302i +N (T (\u03b1 \u00b7 [{g\u2032 3 j}Gj=1]))), (3)\nwhere A, N and T represent the activation layer, the normalization layer and the convolutional layer, respectively.\nBased on five cascaded HMUs and several stacked convolutional layers, a single-channel logits map is obtained. The final confidence map P that highlights the camouflaged objects is then generated by a sigmoid function."
        },
        {
            "heading": "3.5. Loss Functions",
            "text": "The binary cross entropy loss (BCEL) is widely used in various image segmentation tasks and its mathematical form is li,jBCEL = \u2212gi,j logpi,j \u2212 (1\u2212 gi,j) log(1\u2212 pi,j), where gi,j \u2208 {0, 1} and pi,j \u2208 [0, 1] denote the ground truth and the predicted value at position (i, j), respectively. As shown in Fig. 6, due to the complexity of the COD data, if trained only under the BCEL, the model produces serious ambiguity and uncertainty in the prediction and fails to accurately capture objects, of which both will reduce the reliability of COD. To force the model to enhance \u201cconfidence\u201d in decision-making and increase the penalty for fuzzy prediction, we design a strong constraint as the auxiliary of the BCEL, i.e., the uncertainty-aware loss (UAL).\nIn the final probability map of the camouflaged object, the pixel value range is [0, 1], where 0 means the pixel belongs to the background, and 1 means it belongs to the camouflaged object. Therefore, the closer the predicted value is to 0.5, the more uncertain the determination about the property of the pixel is. To optimize it, a direct way is to use the ambiguity as the supplementary loss for these difficult samples. To this end, we first need to define the ambiguity measure of the pixel x, which maximizes at x = 0.5 and\nminimizes at x = 0 or x = 1. And as a loss, the function should be smooth and continuous with only a finite number of non-differentiable points. For brevity, we empirically consider two forms, \u03a6\u03b1pow(x) = 1\u2212 |2x\u2212 1|\u03b1 based on the power function and \u03a6\u03b1exp(x) = e\n\u2212(\u03b1(x\u22120.5))2 based on the exponential function. Besides, inspired by the form of the weighted BCE loss, we also try to use \u03c9 = 1 + \u03a62pow(x) as the weight of BCE loss to increase the loss of hard pixels. After massive experiments (Sec. 4.3), the proposed UAL is formulated as li,jUAL = 1 \u2212 |2pi,j \u2212 1|2. Finally, the total loss function can be written as:\nL = LBCEL + \u03bb\u00d7 LUAL, (4)\nwhere \u03bb is the balance coefficient and we design three adjustment strategies of \u03bb, i.e., a fixed constant value, an increasing linear strategy, and an increasing cosine strategy in Sec. 4.3. The different forms and corresponding results are listed in the supp. From the results, we find that the increasing strategies, especially \u201ccosine\u201d, do achieve better performance. So, the cosine strategy is used by default."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experiment Setup",
            "text": "Datasets. We use four COD datasets, CAMO [16], CHAMELEON [41], COD10K [9] and NC4K [29]. CAMO consists of 1,250 camouflaged and 1,250 non-camouflaged images. CHAMELEON contains 76 hand-annotated images. COD10K includes 5,066 camouflaged, 3,000 background and 1,934 non-camouflaged images. NC4K is\nanother large-scale COD testing dataset including 4,121 images from the Internet. Following the data partition of [9,29,31,59], we use all images with camouflaged objects in the experiments, in which 3,040 images from COD10K and 1,000 images from CAMO are used for training, and the rest ones for testing. Besides, we also show the performance on five SOD datasets in the supp. Evaluation Criteria. For COD and SOD, we use seven common metrics for evaluation based on [32,33], including S-measure [6] (Sm), weighted F-measure [30] (F\u03c9\u03b2 ), mean absolute error (MAE), F-measure [1] (F\u03b2), E-measure [7] (Em), precision-recall (PR) curve and F\u03b2-threshold curve (F\u03b2 curve). The curves can be found in the supp. Implementation Details. The proposed ZoomNet is implemented with PyTorch. As the settings in recent methods [9,29,31,59], the encoder is initialized with the parameters of ResNet-50 pretrained on ImageNet, and the remaining parts are randomly initialized. SGD with momentum 0.9 and weight decay 0.0005 is chosen as the optimizer. The learning rate is initialized to 0.05 and follows a linear warmup and linear decay strategy. The entire model is trained for 40 epochs with a batch size of 8 in an end-to-end manner on an NVIDIA 2080Ti GPU. During training and inference, the main scale is 384 \u00d7 384. Random flipping and rotating are employed to augment the training data."
        },
        {
            "heading": "4.2. Comparisons with State-of-the-arts",
            "text": "COD is an emerging field, so we introduce some methods for salient object detection and medical image segmentation for comparison. The results of all these methods come from existing public data or are generated by models\nthat are retrained based on the code released by the authors. Quantitative Evaluation. Tab. 1 shows the detailed comparison results. It can be seen that the proposed model consistently and significantly surpasses recent methods on all datasets without relying on any post-processing tricks. Compared with the recent best COD method UJSC, although it introduces extra SOD data for training and has suppressed other existing methods, our method still shows the obvious performance improvement on these datasets. Especially, our approach has more advantages on the metrics F\u03c9\u03b2 , MAE, and F\u03b2 . On four datasets, the proposed method averagely outperforms the second-best method C2F-Net by 19.3% in terms of MAE and the average gains in terms of F\u03c9\u03b2 and F\u03b2 are 4%. Besides, PR and F\u03b2 curves shown in the supp. also demonstrate the effectiveness of the proposed method. The flatness of the F\u03b2 curve reflects the consistency and uniformity of the prediction. Our curves are almost horizontal, which can be attributed to the effect of the proposed UAL. It drives the predictions to be more polarized and reduces the ambiguity. Qualitative Evaluation. Visual comparisons of different methods on several typical samples are shown in Fig. 5. They present the complexity in different aspects, such as big objects (Col. 1), middle objects (Col. 2-8), small objects (Col. 9-13), occlusions (Col. 2 and 10), background interference (Col. 10-13), and indefinable boundaries (Col. 1, 2, 6-13). These results intuitively show the superior performance of the proposed method. In addition, it can be noticed that our predictions have clearer and more complete object regions and sharper contours."
        },
        {
            "heading": "4.3. Ablation Studies",
            "text": "In this section, we perform comprehensive ablation analyses on different components. Because COD10K is the most widely-used large-scale COD dataset, and contains various objects and scenes, all subsequent ablation experiments are carried out on it. Effectiveness of SIUs and HMUs. In the proposed model, both the SIU and the HMU are very important structures. We install them one by one on the baseline model to eval-\nuate their performance. The results are shown in Tab. 2. Our baseline \u00ac and other models  and \u00b0 only use the inputs of the main scale. As can be seen, our baseline shows a good performance, probably due to the proper training setup and the more reasonable network architecture detailed in the supp. From \u00ac-\u00af, it can be seen that the two proposed modules make a significant contribution to the performance when compared to the baseline. Besides, the results in Fig. 6 show that the two modules can benefit each other and reduce their errors (e.g., Col. 1, 2, 5 and 7) to locate and distinguish objects more accurately. These components effectively help the model to excavate and distill the critical and valuable semantics and improve the capability of distinguishing hard objects. Under the cooperation between the proposed modules and loss functions, ZoomNet can completely capture the camouflaged objects of different scales and generate the predictions with higher contrast and consistency. In addition, in Tab. 2, the model \u00b0 is a simple extension of the baseline model \u00ac using some standard convolutional blocks, to make the similar number of parameters and FLOPs with \u00af. The model \u00af still achieves better performance, which reflects the effectiveness of the proposed modules and the rationality of the design. Number of Groups in HMUs. In Tab. 2, we also show the effects of different group numbers in the proposed HMU. It can be seen from the results that the best performance appears when the number of groups is equal to 6. Also, it achieves a good balance between performance and efficiency. So, in other experiments, we set the number of"
        },
        {
            "heading": "1.1, 1.2, 1.3 1/8, 1/4, 1/2 \u2014 \u2014 \u2014 \u2014 \u2014",
            "text": "groups in each HMU to 6. Mixed-scale Input Scheme. Our model is designed to mimic the behavior of \u201dZoom In&Out\u201d. The feature expression is enriched by combining the scale-specific information from different scales. In Fig. 1, the intermediate features and attention maps show that our mixed-scale scheme plays a positive and important role in locating the camouflaged object. Considering that the objects in COD10K are mainly small objects [9], which may limit the role of 0.5\u00d7 input to some extent, we list average results on COD10K and CAMO in Tab. 3. The proposed scheme performs better than the single-scale one and simply mixed one. This verifies the rationality of such a design for the COD task. Options of Setting \u03bb. We compare three strategies and the results are listed in the supp., in which the increasing cosine strategy achieves the best performance. This may be due to the advantage of its smooth change process. This smooth intensity warm-up strategy of UAL motivates the model to take advantage of UAL in improving the learning process and to mitigate the possible negative interference of UAL on BCEL due to the lower accuracy of the model during the early stage of training. Forms of UAL. Different forms of UAL are listed in Tab. 4 and the corresponding curves are illustrated in the supp. As can be seen, Form 1.5 has a more balanced performance. Also, it is worth noting that, when approaching 0 or 1, the form which can maintain a larger gradient will obtain better performance in terms of F\u03c9\u03b2 , MAE and F\u03b2 . This may provide some reference for designing a better loss. Effectiveness of UAL. The results of Fig. 6 intuitively show that the UAL greatly reduces the ambiguity caused by the interference from the background. Besides, we visualize the histogram maps of all results on CHAMELEON and the intermediate features from different stages in the decoder in the supp. In the stacked histogram map \u201cw/o UAL\u201d, a large number of pixels appear in the middle area, which corresponds to more visually blurred/uncertain pre-\ndictions. Besides, in the corresponding feature visualization, especially in the region inside the red box, there is clear background interference due to the complex scenarios and blurred edges, which are extremely prone to yield false positive predictions. However, when UAL is introduced, it can be seen that the middle interval of \u201cw UAL\u201d is flatter than the one of \u201cw/o UAL\u201d, that is, most pixel values approach two extremes. And the feature maps become more discriminative and present a more compact and complete response in the regions of camouflaged objects."
        },
        {
            "heading": "4.4. Discussion on SOD and COD",
            "text": "It can be seen from the experiments in the supp. that our method not only performs well on COD, but also shows outstanding performance on SOD. Considering the difference between these two tasks, we may wonder why our method consistently performs well on such two seemingly different tasks. We attribute this to the generality and rationality of the designed structure. In fact, SOD and COD have a clear commonality, i.e., the accurate segmentation has to depend on multi-scale and category-free discriminative features. By integrating rich scale-specific features, our model can extract critical and informative cues from scenes and objects, which helps precise localization and smooth segmentation of objects. In addition, the proposed UAL can mitigate the ambiguity of predictions caused by the inherent complexity of scenes. Although the objects in SOD are salient, it can also benefit from UAL due to the vagueness introduced by the CNN model itself in the detailed information recovery process. All of these components are built on the common demand of the two tasks, which provides a solid foundation for the performance."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose the ZoomNet by imitating the behavior of human beings to zoom in and out on images. This process actually considers the differentiated expressions about the scene from different scales, which helps to improve the understanding and judgment of camouflaged objects. We first filter and aggregate scale-specific features through the scale merging layer to enhance feature representation. Next, in the hierarchical mixed-scale decoder, the strategies of grouping, mixing and fusion further mine the mixed-scale semantics. Lastly, we introduce the uncertainty-aware loss to penalize the ambiguity of the prediction. Extensive experiments verify the effectiveness of the proposed method in both the COD and SOD tasks with superior performance to existing state-of-the-art methods. Acknowledgements This work was supported by the National Natural Science Foundation of China #61876202 and #61829102, the Liaoning Natural Science Foundation #2021-KF-12-10, and the Fundamental Research Funds for the Central Universities #DUT20ZD212.\nThis appendix will introduce more details that cannot be expanded in the main text, while showing the performance on SOD."
        },
        {
            "heading": "A. Model Details",
            "text": "A.1. E-Net\nE-Net is based on the feature extraction part of ResNet50 [14] and the layers after the \u201clayer4\u201d are removed. We collect the feature maps before passing the first maxpooling layer and the output feature maps of \u201clayer1\u201d, \u201clayer2\u201d, \u201clayer3\u201d and \u201clayer4\u201d as the output feature maps of the E-Net. The numbers of channels corresponding to them are 64, 256, 512, 1024, and 2048, respectively.\nA.2. C-Net\nFollowing the setting of the method [67], in C-Net, we use an ASPP [3] simplified according to our needs as the feature compression layer corresponding to the \u201clayer4\u201d of E-Net and other layers are simply composed of an independent \u201cConv3\u00d73-BN-ReLU\u201d (3\u00d73 CBR) unit. The numbers of output channels of all levels are set to 64 in our models.\nThe ASPP layer is composed of five CBR branches. The kernel sizes and dilation rates of them are 1, 3, 3, 3, 1 and 1, 2, 5, 7, 1. All convolution operations use the padding to ensure that the input and output sizes are consistent. A\nglobal average pooling operation and an up-sampling operation are used before and after the second 1\u00d7 1 CBR branch to capture the global context information and restore it to the original size. All results of the five branches are concatenated along the channel dimension and fused by a 3\u00d7 3 CBR unit to obtain the output."
        },
        {
            "heading": "A.3. Decoder Framework",
            "text": "The decoder networks of our models in all experiments follow the same framework as shown in Fig. 8. Before being\n1 class StackedCBRBlock(nn.Sequential): 2 def __init__(self, in_c, out_c, num_blocks=1, kernel_size=3): 3 super().__init__() 4 self.kernel_setting = dict(kernel_size=kernel_size, stride=1, padding=kernel_size // 2) 5 cs = [in_c] + [out_c] * num_blocks 6 self.channel_pairs = tuple(self.slide_win_select(cs, win_size=2, win_stride=1, drop_last=True)) 7 for i, (i_c, o_c) in enumerate(self.channel_pairs): 8 self.add_module(name=f\"cbr_{i}\", module=CBR(i_c, o_c, **self.kernel_setting)) 9 @staticmethod\n10 def slide_win_select(items, win_size=1, win_stride=1, drop_last=False): 11 i = 0 12 while i + win_size <= len(items): 13 yield items[i: i + win_size] 14 i += win_stride 15 if not drop_last: 16 yield items[i: i + win_size]\nListing 1. Code of stacked CBR units.\nfed into the fusion unit (FU), the up-sampled deeper feature map is directly added to the shallow feature map.\nIn our all experiments, Nf and Nl are set to 1. The numbers of input & output channels of the last 3 \u00d7 3 CBR unit are 64 and 32, respectively. The number of output channels of the \u201cConv1 \u00d7 1\u201d is 1 and a sigmoid layer is cascaded to convert the logits map to the prediction. In the decoder of the proposed ZoomNet, the FU is set to the HMU and the other layers remain the same."
        },
        {
            "heading": "A.4. Baseline Model",
            "text": "In the ablation study, we introduce a simple encoderdecoder network as our baseline model to evaluate the performance of different proposed components. It contains a feature extraction network \u201cE-Net\u201d, a simple multi-level feature compression convolutional network \u201cC-Net\u201d, and a basic convolutional decoder where the FU is set to the 3\u00d73 CBR unit. In the following text, \u201cCBR1-5\u201d are used to refer to these five units."
        },
        {
            "heading": "A.5. Model \u00b0",
            "text": "In Tab. 2 of the main text, based on the baseline model \u00ac, we construct the model \u00b0 with the similar amount of parameters and FLOPs to \u00af to reflect the effectiveness of the method and the rationality of the design. For increasing the number of parameters and FLOPs, we made the following modifications to the baseline model \u00ac:\n\u2022 The number of output channels of all levels of C-Net: 64\u2192 128. \u2022 The number of input/output channels of CBR1-5 units of the basic convolutional decoder: 64\u2192 128. \u2022 The number of input channels of the last CBR unit: 64\u2192 128. \u2022 The number of CBR units (Nf and Nl) of all levels of the basic convolutional decoder: 1\u2192 3. \u2022 The kernel size of the convolution operation in all lev-\nthe stacked CBR units used here is listed in List. 1.\nAlgorithm 1 The iteration struction in the HMU\nInput: {gj}Gj=1: feature groups; G \u2265 2: the number of groups;C = 32: the number of channels in a single feature group gj ; S: splitting operation; TCo\u00d7Ci : stacked CBR units with initial input and final output channel numbers of Ci and Co as listed in List. 1; C: concatenation operation; Output: {g\u20322j}Gj=1: the feature set for generating the modulation vector \u03b1; {g\u20323j}Gj=1: the feature set used to be modulated and generate the final output of the HMU;\n1: for i\u2190 1, G do 2: if i = 1 then . Group 1 3: g\u2032\n1 i , g \u20322 i , g \u20323 i \u2190 S(T i3C\u00d7C(gi));\n4: g\u2032 1 prev \u2190 g\u2032 1 i ; 5: else if i = G then . Group G 6: g\u2032\n2 i , g \u20323 i \u2190 S(T i2C\u00d72C(C(gi, g\u2032 1 prev)));\n7: else . Group i, 1 < i < G 8: g\u2032\n1 i , g \u20322 i , g \u20323 i \u2190 S(T i3C\u00d72C(C(gi, g\u2032 1 prev)));\n9: g\u2032 1 prev \u2190 g\u2032 1 i ;\n10: end if 11: end for\nC ou\nnt"
        },
        {
            "heading": "B. HMU: Perspective of Kernel Pyramid",
            "text": "The iteration structure of feature groups in HMU is actually equivalent to an integrated multi-path kernel pyramid structure with partial parameter sharing. In order to understand this intuitively, we highlight the feature information flow of different groups in the iterative structure in Fig. 11. Specifically, the 3 \u00d7 3 CBR unit corresponding to the feature group in the iteration structure can be split according to the output feature groups. As shown in the \u201cIntegrated Kernel Pyramid\u201d on the left of Fig. 11, each original CBR unit with an output channel number of 3C is converted to three independent CBR units with a shared input. And the numbers of output channels of them are C. When we further decouple the integrated form on the left into the form on the right, we can clearly see that the information flow paths corresponding to different feature groups each form a multi-branch kernel pyramid structure and there are some shared parameters between these pyramids.\nAs mentioned in the main text of the paper, some of the channels in the output feature of each branch are used together to generate the modulation vector. It not only weights the channels inside each branch, but also weights different branches. If viewed from the aforementioned perspective of\nthe kernel pyramid, such an operation can be seen as a relative modulation of the different kernel pyramids contained in the iterative structure of the HMU.\nBesides, in our HMU, C is set to 32. The number of channels of the final output feature of the HMU is the same as the input feature, both are 64. We also list the algorithm of the iteration structure in Alg. 1 to present the process more clearly and to complement the related statement in the main text."
        },
        {
            "heading": "C. More Comparisons",
            "text": ""
        },
        {
            "heading": "C.1. PR & F\u03b2 curves of COD Methods",
            "text": "In Fig. 12, we show the PR & F\u03b2 curves of different methods on four COD datasets. The red curve represents our method."
        },
        {
            "heading": "C.2. Comparisons of Param. & FLOPs",
            "text": "In Tab. 5, we list the number of parameters and FLOPs of existing COD methods and ours. Our method provides a performance-robust solution with the second-smallest amount of parameters for the COD task. But there may be still some redundancy in the design of the inference structure. The adopted explicit scale-independent design may bring additional inference cost. We will explore and improve this in future work.\nC.3. Intermediate Feature Maps of the Decoder\nWe show the intermediate feature maps from different stages of the decoder in Fig. 7."
        },
        {
            "heading": "C.4. Effectiveness of UAL",
            "text": "In Fig. 9, we visualize the histogram maps of all results on CHAMELEON [41]."
        },
        {
            "heading": "C.5. Different Forms of \u03bb",
            "text": "The different adjustment functions of the coefficient \u03bb and their results of UAL are list in Tab. 6."
        },
        {
            "heading": "C.6. Different Forms of UAL",
            "text": "The different forms of UAL are shown in Fig. 10."
        },
        {
            "heading": "C.7. Performance in More Complex Scenes",
            "text": "Actually, COD10K-TE is a very representative test dataset with rich and diverse scenarios and objects. Be-\nsides, there is also a very complex small-scale dataset CPD1K [71]. Tab. 7 shows the results of our method and some state-of-the-art competitors (all are trained without CPD1K-TR). The test results on CPD1K-TE can reflect the adaptability of the model to complex scenarios. The experiment shows the superior performance of our method in more complex scenarios."
        },
        {
            "heading": "D. Experiments on SOD",
            "text": "In order to show good generalization and further verify the rationality of the structural design, we evaluate the proposed model on the SOD task."
        },
        {
            "heading": "D.1. Datasets",
            "text": "Our experiment on SOD is based on the existing five SOD datasets, DUT-OMRON [55] (5168), DUTS [44] (10553 + 5017), ECSSD [54] (1000), HKU-IS [18] (4447) and Pascal-S [18] (850). We only use the training set of DUTS for training. During the test phase, we use the remaining data for inference.\nD.2. Implementation Details\nFor a fair comparison on SOD, the proposed model is retrained on DUTS [44] following the training strategies and techniques of [35,46,47,51,64]. The learning rate is initial-\nized to 0.05 and follows a linear warm-up and linear decay strategy. And the main scale is changed to 352 \u00d7 352 to achieve a trade-off between performance and speed. The model tends to converge after 50 epochs with a batch size of 22."
        },
        {
            "heading": "D.3. Comparisons with State-of-the-arts",
            "text": "We compare the proposed model with 22 existing methods. All the results are listed in Tab. 8 and shown in Fig. 13. Our model outperforms all these competitors, which shows that the proposed model can deal with the more general binary segmentation task."
        },
        {
            "heading": "E. Limitations and Future Work",
            "text": "Although our ZoomNet provides a powerful and effective solution for the COD task, some limitations still exist and are worth exploring further.\n1. In the current work, the shared feature extraction structure explicitly collects complementary information from different scales on the image pyramid, which is designed to mimic the behavior of zooming in and out. But for human beings, the process of information extraction and integration should be implicit and internalized in the process of knowledge learning. Moreover, this explicit scale-independent design also brings the additional inference cost. Although our method has\nachieved good performance on COD and SOD tasks, the inference speed is still slightly slower than the current fastest method, C2FNet [43].\n2. Besides, there is still room for improvement in the way of mining effective clues from small-scale features in SIU.\nIn future work, we will try to further simplify the inference structure to make it more in line with the actual human decision-making process and optimize the ability of our method to extract contextual cues from small-scale features."
        }
    ],
    "title": "Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection",
    "year": 2023
}