{
    "abstractText": "Building and implementing ethical AI systems that benefit the whole society is cost-intensive and a multi-faceted task fraught with potential problems. While computer science focuses mostly on the technical questions to mitigate social issues, social science addresses citizens\u2019 perceptions to elucidate social and political demands that influence the societal implementation of AI systems. Thus, in this study, we explore the salience of AI issues in the public with an emphasis on ethical criteria to investigate whether it is likely that ethical AI is actively requested by the population. Between May 2020 and April 2021, we conducted 15 surveys asking the German population about the most important AI-related issues (total of N=14,988 respondents). Our results show that the majority of respondents were not concerned with AI at all. However, it can be seen that general interest in AI and a higher educational level are predictive of some engagement with AI. Among those, who reported having thought about AI, specific applications (e.g., autonomous driving) were by far the most mentioned topics. Ethical issues are voiced only by a small subset of citizens with fairness, accountability, and transparency being the least mentioned ones. These have been identified in several ethical guidelines (including the EU Commission\u2019s proposal) as key elements for the development of ethical AI. The salience of ethical issues affects the behavioral intentions of citizens in the way that they 1) tend to avoid AI technology and 2) engage in public discussions about AI. We conclude that the low level of ethical implications may pose a serious problem for the actual implementation of ethical AI for the Common Good and emphasize that those who are presumably most affected by ethical issues of AI are especially unaware of ethical risks. Yet, once ethical AI is top of the mind, there is some potential for activism.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kimon Kieslich"
        }
    ],
    "id": "SP:ae74d5bf2cbff8f9ee0ae99187f53c42ab663184",
    "references": [
        {
            "authors": [
                "W. Serrin"
            ],
            "title": "The watchdog role",
            "year": 2005
        },
        {
            "authors": [
                "N. Bostrom"
            ],
            "title": "Superintelligence: Paths, dangers, strategies",
            "year": 2016
        },
        {
            "authors": [
                "M. v=8 Carradore"
            ],
            "title": "People\u2019s attitudes towards the use of robots in the social services: A multilevel analysis",
            "year": 2021
        },
        {
            "authors": [
                "C. Cath",
                "S. Wachter",
                "B. Mittelstadt",
                "M. Taddeo",
                "L. Floridi"
            ],
            "title": "Artificial intelligence and the \u2019good society\u2019",
            "venue": "eurobarometer data. International Journal of Social Robotics,",
            "year": 2018
        },
        {
            "authors": [
                "H. Choung",
                "P. David",
                "A. Ross"
            ],
            "title": "Trust and ethics in ai",
            "year": 2022
        },
        {
            "authors": [
                "und sonderbefragungen"
            ],
            "title": "Unpublished. doi: 10.13140/RG.2.2.36703.53928 Elzayn, H",
            "year": 2020
        },
        {
            "authors": [
                "E. Horvitz"
            ],
            "title": "Long-term trends in the public perception of artificial intelligence",
            "year": 2017
        },
        {
            "authors": [
                "nlebk",
                "db=nlabk",
                "V. AN=112529 Fietta",
                "F. Zecchinato",
                "B. Di Stasi",
                "M. Polato",
                "M. Monaro"
            ],
            "title": "Dissociation between users",
            "year": 2021
        },
        {
            "authors": [
                "C.B. Frey",
                "M.A. Osborne"
            ],
            "title": "The future of employment: How susceptible are jobs to computerisation",
            "year": 2017
        },
        {
            "authors": [
                "P.A. Hancock"
            ],
            "title": "Avoiding adverse autonomous agent actions. Human\u2013Computer Interaction, 1\u201326",
            "year": 2021
        },
        {
            "authors": [
                "K. Krippendorff"
            ],
            "title": "Answering the call for a standard reliability measure for coding data",
            "year": 2007
        },
        {
            "authors": [
                "A.O. Hirschman"
            ],
            "title": "Exit, voice, and loyalty: Responses to decline in firms",
            "venue": "Communication Methods and Measures,",
            "year": 1931
        },
        {
            "authors": [
                "T. Hartwig",
                "N. Takanashi",
                "H.M. Yokoyama"
            ],
            "title": "Octagon measurement: Public attitudes",
            "year": 2022
        },
        {
            "authors": [
                "A. Jobin",
                "M. Ienca",
                "E. Vayena"
            ],
            "title": "The global landscape of ai ethics guidelines",
            "venue": "ethics. International Journal of Human\u2013Computer Interaction,",
            "year": 2019
        },
        {
            "authors": [
                "A. Jungherr",
                "O. Posegga",
                "J. An"
            ],
            "title": "Discursive power in contemporary media systems: A comparative",
            "year": 2019
        },
        {
            "authors": [
                "K. 2021.1976642 Kieslich",
                "B. Keller",
                "C. Starke"
            ],
            "title": "Artificial intelligence ethics by design. evaluating public perception",
            "year": 2022
        },
        {
            "authors": [
                "K. Kieslich",
                "C. Starke",
                "P. Dosenovic",
                "B. Keller",
                "F. Marcinkowski"
            ],
            "title": "Artificial intelligence and discrimination",
            "year": 2020
        },
        {
            "authors": [
                "P. K\u00f6nig",
                "S. Wurster",
                "M.B. Siewert"
            ],
            "title": "Consumers are willing to pay a price for explainable",
            "year": 2021
        },
        {
            "authors": [
                "R. Bernhaupt"
            ],
            "title": "What is ai literacy? competencies and design considerations",
            "venue": "ai. SSRN Electronic Journal",
            "year": 2020
        },
        {
            "authors": [
                "M. L\u00fcnich",
                "K. Kieslich"
            ],
            "title": "Exploring the roles of trust and social group preference on the legitimacy",
            "year": 2022
        },
        {
            "authors": [
                "F. Marcinkowski",
                "K. Kieslich",
                "C. Starke",
                "M. L\u00fcnich"
            ],
            "title": "Implications of ai (un-)fairness in higher education",
            "year": 2020
        },
        {
            "authors": [
                "D.L. Shaw"
            ],
            "title": "The agenda-setting function of mass media",
            "venue": "Public Opinion Quarterly,",
            "year": 1972
        },
        {
            "authors": [
                "A. McNamara",
                "J. Smith",
                "E. Murphy-Hill"
            ],
            "title": "Does acm\u2019s code of ethics change ethical decision making",
            "year": 2018
        },
        {
            "authors": [
                "R. elementsofai.com/ Nichols",
                "G. Martinez"
            ],
            "title": "Political economy of media industries: Global transformations and challenges",
            "year": 2020
        },
        {
            "authors": [
                "B.V. Lewenstein"
            ],
            "title": "The public and nanotechnology: How citizens make sense of emerging",
            "year": 2005
        },
        {
            "authors": [
                "D. Shin"
            ],
            "title": "The effects of explainability and causability on perception, trust, and acceptance: Implications",
            "year": 2021
        },
        {
            "authors": [
                "D. Shin"
            ],
            "title": "How do people judge the credibility of algorithmic sources? AI & SOCIETY",
            "year": 2021
        },
        {
            "authors": [
                "D. 1007/s00146-021-01158-4 Shin"
            ],
            "title": "The perception of humanness in conversational journalism: An algorithmic information-processing",
            "year": 2021
        },
        {
            "authors": [
                "S. Sun",
                "Y. Zhai",
                "B. Shen",
                "Y. Chen"
            ],
            "title": "Newspaper coverage of artificial intelligence: A perspective",
            "year": 2020
        },
        {
            "authors": [
                "M. Vergeer"
            ],
            "title": "Artificial intelligence in the dutch press: An analysis of topics and trends",
            "venue": "California Management Review,",
            "year": 1986
        },
        {
            "authors": [
                "Sussex",
                "K. Hartmann"
            ],
            "title": "Policy formation, termination and the multiple streams framework",
            "year": 2021
        },
        {
            "authors": [
                "Chan",
                "C.-h",
                "M.S. Sch\u00e4fer"
            ],
            "title": "Contested chinese dreams of ai? public discourse about artificial",
            "year": 2020
        },
        {
            "authors": [
                "A. Dafoe"
            ],
            "title": "Artificial intelligence: American attitudes and trends",
            "venue": "SSRN Electronic Journal",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords AI ethics \u00b7 issue salience \u00b7 ethical guidelines \u00b7 survey \u00b7 political engagement"
        },
        {
            "heading": "1 Introduction",
            "text": "With the rapid development of AI technology, great threats to society, some of them even existential, can be expected to materialize and society has to be prepared to react to those changes (Hancock, 2021). This cautionary sentiment is shared by other scholars, albeit with a focus on distinct detrimental consequences (Bostrom, 2016; Floridi et al., 2018).\nThus, fairness, accountability, and transparency are increasingly discussed in the scientific community as are other related concepts that aim for Artificial Intelligence (AI) development for the Common Good. The Common Good encapsulates the ambition that AI should benefit everyone or \u201cbe good for all\u201d, not only the ones who invest in or deploy the technology (Berendt, 2019, p. 45). While normatively building ethical AI systems in the interest of everybody\u2014 which is often formulated as a central aim, for example by the EU (European Commission, 2019)\u2014research has shown that there exist considerable obstacles and costs on the way to achievement. For instance, according to Elzayn and Fish (2020) market interests can be in opposition to developing fair systems as developing such systems requires additional resources and is cost-intensive. Aside from economic interests, political interests may not necessarily prioritize ethical AI systems either (Crawford, 2021). Political actors often rather argue that AI development must be pushed forward for the sake of international competition, aiming at certain economical or scientific goals (Hagendorff, 2020). Concerning\nar X\niv :2\n20 7.\n14 08\n6v 1\n[ cs\n.C Y\n] 2\n8 Ju\nthe primary aim of politics to achieve and maintain power or the primary goal of economics to optimize businesses and grow, these actors are not necessarily aiming at a broad and society-including discussion about ethical AI which may oppose their primary goals (Bareis & Katzenbach, 2021; Hagendorff, 2020).\nThis paper starts from the assumption that neither academic debates nor political promises or declarations of intent alone will lead to shaping AI in the public interest. On the contrary, given the inherent logic of politics and economics in capitalist society, the claim for ethical AI will have to be asserted against the interests of economic and political elites. Regardless of the (inter)national economic and political contexts in which these interests operate, this can only succeed if a significant mass of citizens actively requests ethical principles in their roles as users and consumers of these technologies, or if they demand credible policies for ethical AI as voters. Hence, as a proposed safety measure against detrimental consequences of the emerging technology, one may demand the implementation of AI systems that a priori fulfill certain ethical criteria. Though, as argued above, the fulfillment of respective calls for ethical AI does not follow automatically but must be\u2014at least to some extent\u2014expressed by societal groups or the broader public in the form of articulated claims and requests to the political decision-making bodies. Thus, civil society has to issue convincing incentives for economic or political actors to invest in AI systems that benefit the Common Good (Kieslich, 2021). Consequently, there must be public awareness of social problems for them to lead to political actions. While we have some knowledge of how citizens react to the ethical design of AI (Kieslich, Keller, & Starke, 2022; K\u00f6nig, Wurster, & Siewert, 2021; Shin, 2021a, 2021b, 2021c; Shin, Zhong, & Biocca, 2020), we only have little to no knowledge about the salience of ethical AI among the public in the first place. Hence, we ask, what issues are top of citizens\u2019 minds and, in particular, how prominent ethical issues feature among these. We subsequently ask whether the salience of ethical AI leads to different behavioral intentions.\nTo shed light on these questions, we first reflect on the current state of approaches to achieving ethical AI, which is primarily connected to ethical guidelines. We highlight the shortcomings of the current approaches to ethical AI, emphasizing their conceptual weaknesses as well as the non-binding character of proposed ethical guidelines. We infer that public pressure must be exerted to achieve ethical AI for the Common Good. As the issues of emerging technologies like AI are mostly raised by the media, we then discuss the current literature on published opinion on AI. We especially reflect on the topical structure of the media reporting, i.e. the prevalence of ethical topics, as well as the presence of different actors. Afterward, we review studies about public opinion on AI and highlight that no study so far has researched citizens\u2019 salience of ethical AI without confronting respondents with ethical problems of AI beforehand.\nIn the empirical part of the paper, we first explore the extent to which German citizens are concerned with ethical issues of AI and compare that to the salience of other phenomena and consequences of AI, for instance, technical functionalities or specific applications of AI. Second, we show which factors (sociodemographic as well as interest in AI) are linked to a higher likelihood of being concerned with ethical issues. Thereby, we especially look for differences in societal groups and explore whether groups of lower social status\u2014those groups, that are arguably most endangered by contemporary AI technologies\u2014are aware of those issues. Or, in other words, whether salience of ethical issues with AI is a discourse, which primarily bears fruit with people of higher societal status. In a third step, we investigate whether the issue salience of ethical AI has an impact on behavioral intentions towards AI, namely the intention to avoid AI and the intention to engage in discussions about AI.\nWe point out theoretical and practical implications in light of the low salience of ethical AI with the German public and the differences among societal groups in their respective salience levels. We further discuss the potential of the salience of ethical AI for the articulation of ethical demands to those, who are in charge of the development and deployment of AI systems. In this paper, we show that ethical AI discourse needs to be taken into all levels of society."
        },
        {
            "heading": "2 Pathways to Ethical AI",
            "text": ""
        },
        {
            "heading": "2.1 Ethical AI Guidelines",
            "text": "Against the backdrop of ethical problems in AI development, some approaches were developed to tackle those issues that emerge with further development and implementation of AI in society. Jobin, Ienca, and Vayena (2019) collected and analyzed corresponding ethical AI guidelines from around the globe. Interestingly, many ethical guidelines were proposed by private companies or political institutions (e.g., the EU-Commission proposed a framework for the development of ethical AI (European Commission, 2019)), but also by academia and research institutions such as the Association for Computing Machinery (ACM) (Association for Computing Machinery, 2018; Jobin et al., 2019). However, many of these guidelines do not address the broad public or civil society and instead target specific stakeholder groups. Thus, they focus only on the good of a subset of society or speak to those who are in direct contact with the technology. Guidelines that address the Common Good (Berendt, 2019) rarely occur. AI for the Common Good can be defined as an approach to developing AI systems that take many different social groups into account and not only those, who \u201cdirectly pay for the development or use of this AI\u201d (Berendt, 2019, p.48). Consequently, ethical AI guidelines can\nnot necessarily be equated with AI that benefits the whole society or that may recognize and include multiple societal perspectives at all. Looking at the sources and target groups of the ethical guidelines summarized by Jobin et al. (2019), only a small subset aims for AI for the Common Good or the broad public, respectively. However, that does not imply that the other ethical guidelines exclude these aims, but they simply do not mention or prioritize them as the main goal to satisfy respective stakeholders.\nBut what about the effectiveness of ethical guidelines? Given the different aims of stakeholder groups, it is debatable to what extent these actors want a broad discussion about ethical AI or only treat guidelines primarily for communicative purposes with other stakeholders. Out of their logic of achieving or maintaining power (political perspective) or increasing monetary profit (economic perspective), one can argue that most political or economic actors are not aiming for a broad and society-including discussion about ethical AI, as this can be opposed to their primary aims.\nHagendorff (2020) analyzed 22 ethical guidelines from companies as well as research institutes and political institutions and attested the current state of AI ethics a bad condition: \u201cCurrently, AI ethics is failing in many cases. Ethics lacks a reinforcement mechanism. Deviations from the various codes of ethics have no consequences.\u201d (p.113). He argues that current ethical guidelines mainly serve as a \u201cmarketing strategy\u201d (p.113) and that ethics are seen as an add-on rather than a crucial step integrated into AI development. This is especially true in economic terms that oftentimes are in contrast to\u2014and in the end override\u2014commitment to ethical principles in AI design leading to a violation of ethical principles. Further, he highlights the non-binding character of these guidelines, which have the aim to avoid state regulation. In addition, focusing on the political approaches of the US, the EU, and the UK for AI for a \u201cGood Society\u201d, Cath, Wachter, Mittelstadt, Taddeo, and Floridi (2018) report that, although issues are adequately addressed, the reports fall short in providing clear political perspectives on how to tackle these issues. Thus, also in the context of political guidelines, the effectiveness of those guidelines is questionable. Turning towards the practical side of developers\u2019 adherence to ethical guidelines, McNamara, Smith, and Murphy-Hill (2018) surveyed software engineers and report that reading ethical guidelines did not change their reported working habits. Thus, the prevalent top-down approach to ethical guidelines appears to have only a limited impact on the ethical AI development."
        },
        {
            "heading": "2.2 The Role of the Public in Implementing Ethical AI",
            "text": "Accordingly, the expression of AI ethics guidelines is not a sufficient requirement to achieve the goal of an AI for the Common Good. From a civil society perspective, therefore, the question is how to commit relevant stakeholders to the goals of an AI for the Common Good. How can \u2018bottom-up\u2019 political pressure be created that encourages those responsible to commit themselves and each other to the observance of corresponding guidelines and to stick to their commitments? Here, especially in pluralistically organized democratic societies, the focus must inevitably turn to the public sphere as the place where the electorate informs itself about political issues and forms an opinion. Irrespective of which specific conception of democracy and its public sphere one follows\u2014for example, deliberative, participatory, and representative conceptions of democracy (Ferree, Gamson, Gerhards, & Rucht, 2002; Ferree, Gamson, Rucht, & Gerhards, 2002)\u2014the public and the media have important roles to play in the processes of political negotiation and legitimization (Beetham, 2013; Jungherr, Posegga, & An, 2019). From a normative perspective, the media is often expected to serve as a watchdog (Bennett & Serrin, 2005; Norris, 2014), while the public\u2014represented in part by advocacy groups\u2014voices support for or disapproves of the implementation of technology. This results in the widespread continuous monitoring and analysis of public opinion that assesses citizens\u2019 attitudes and understanding of technology (Besley, 2013; Miller, 2004).\nMedia coverage is especially relevant in the case of emerging technologies, since most people do not have personal experience with such technology or are not even aware of it, respectively. Thus, media present those topics to a broad public and have the potential to shape public opinion in setting a frame of reference for what and how to think about technology, or, at least, what relevant and powerful actors think about AI (Metag & Marcinkowski, 2014; Nisbet et al., 2002; Scheufele & Lewenstein, 2005). With that, the audience learns about potential opportunities and risks of a technology as well as future pathways for dealing with the emerging sociotechnical systems. Thus, citizens\u2019 opinion toward emerging technology is affected by the way the media reports on it. Accordingly, it is important to understand how the media report on AI, especially concerning the topical structure and the presence of different social groups and actors (e.g., civil society, political actors, economic actors).\nTaken together, public attention concerning ethical issues of AI thus may result in substantial pressure on the actual implementation of ethical AI, as affected groups and concerned citizens, reject sociotechnical AI systems altogether (\u2018exit\u2019) or protest their implementation (\u2018voice\u2019) (Hirschman, 1970; Kieslich, 2021). For instance, in the UK, students protested against an algorithmic decision-making (ADM) system that autonomously graded exams, which lead to a withdrawal of the respective system (Kelly, 2021). In France, a public debate about a university admission system also ended in abolition of the system (Wenzelburger & Hartmann, 2021). Concerning the use of an algorithm to allocate Covid-19 vaccination in Stanford hospital, medical staff heavily protested against the decisions of the algorithm, as front\nworkers were not given priority (Guo & Hao, 2020). Additionally, journalists and activists also raised concerns about the implementation of unethical AI systems, for example in Sweden in the use case of a social benefit application and in Germany in the use case of face recognition in public places (Algorithm Watch, 2020). Additional studies suggest that the perceptions of unfair treatment by ADM systems can lead to the rejection of such technology and of those who apply it, respectively (Marcinkowski, Kieslich, Starke, & L\u00fcnich, 2020); furthermore, L\u00fcnich and Kieslich (2022) noted that a lack of trust in ADM systems leads to them being perceived as illegitimate. However, Algorithm Watch estimates the awareness of ethical problems with AI as not being overly high: \u201cProtests in the UK and elsewhere, together with high-profile scandals based on ADM systems, have certainly raised awareness of both the risks and opportunities of automating society. But while on the rise, this awareness is still in its early stages in many countries.\u201d (Algorithm Watch, 2020, p.9)\nWhen analyzing the given examples, two reasons for reluctance towards AI systems are apparent. First, reactance towards AI is tied to a specific use case and articulated by those who are at risk of being treated negatively (e.g., students or medical staff in the examples mentioned above). These stakeholder groups organized themselves and fueled widespread protests against the use of the specific AI application. Second, media reporting led to public attention about ethical issues with some\u2014again specific\u2014AI systems. Media reporting may not lead to immediate protest behavior in the public, yet, it can damage the public image of developers or suppliers of AI technology. Hence, they may proactively work on ways to resolve ethical issues before widespread protests can emerge.\nThus, analyzing public perceptions of ethical issues of AI can indicate how strong the public sphere is concerned with AI implementation and what public demands and objections may follow from social debates around the ethics of AI. In this study, we do not focus on a specific application but on the general attitudes towards AI and its ethics."
        },
        {
            "heading": "3 Published and Public Opinion on AI",
            "text": "Focusing on the inclusion of public perceptions in the debate, we first need to address the extent of AI\u2019s ethical issues that are discussed in the sphere of the media. We then turn to the analysis of public opinion concerning AI and AI ethics to document whether issues from media discourse are also reflected within public opinion."
        },
        {
            "heading": "3.1 Published Opinion on AI",
            "text": "In recent years, a considerable amount of research has focused on the analysis of news reporting on AI (Brennen, Howard, & Nielsen, 2018; Chuan, Tsai, & Cho, 2019; Fast & Horvitz, 2017; Fischer & Puschmann, 2021; Ouchchy, Coin, & Dubljevic\u0301, 2020; Sun, Zhai, Shen, & Chen, 2020; Vergeer, 2020; Zeng, Chan, & Sch\u00e4fer, 2020). According to the studies of Fast and Horvitz (2017), Sun et al. (2020) and Chuan et al. (2019) for the US, Fischer and Puschmann (2021) for the German and Vergeer (2020) for the Dutch press, media reporting about AI sharply increased in recent years, leading to a steady presence of AI in news reporting. Thereby, several studies pointed out that media reporting about AI was more frequent by right-leaning, respectively conservative outlets (Brennen et al., 2018; Fischer & Puschmann, 2021). Further, Vergeer (2020) found for the Dutch press that national newspapers and tabloid newspapers report more on AI than the regional press.\nFocusing on the topical structure of news reporting about AI, several studies across different countries found that economic topics dominate the content of the news (Brennen et al., 2018; Chuan et al., 2019; Fischer & Puschmann, 2021; Zeng et al., 2020). Accordingly, it is economic actors that dominate the media coverage of AI across the globe; oftentimes with a special focus on so-called \u2018tech giants\u2019 (i.e., corporations that dominate the space of information and communication technology) (Brennen et al., 2018; Fischer & Puschmann, 2021; Sun et al., 2020; Vergeer, 2020; Zeng et al., 2020). On the other hand, civil society actors or NGOs are rarely featured in news reporting about AI (Brennen et al., 2018; Fischer & Puschmann, 2021; Sun et al., 2020; Vergeer, 2020; Zeng et al., 2020). For example, in German (Fischer & Puschmann, 2021) and UK media reporting (Brennen et al., 2018) only four percent of the articles analyzed cited actors from civil society. Moreover, several studies indicated a positive (Fast & Horvitz, 2017; Fischer & Puschmann, 2021; Zeng et al., 2020) or ambivalent sentiment regarding AI in news reporting (Chuan et al., 2019; Vergeer, 2020), while none of the studies reported that AI is portrayed as particularly negative. Positive sentiment is often connected to an optimistic turn towards AI (Fast & Horvitz, 2017) or based on a positive wording about AI (Vergeer, 2020; Zeng et al., 2020). Additionally, two studies explicitly focused on the occurrence of opportunities and risks in media reporting. Fischer and Puschmann (2021) report for the German context that the number of articles analyzed that outlined opportunities of AI was more than double as high as the number of articles that reported on risks of AI. Opportunities were mostly associated with economic progress and efficiency and risks, in contrast, with a lack of AI competence within the general population (i.e., not ethical AI). For the US media coverage, Chuan et al. (2019) report that there is an emphasis on opportunities compared to risks about AI as well.\nLooking at news coverage about ethical issues of AI, Chuan et al. (2019) reported that the topic of AI ethics received increased attention by the US-American press from 2018 on. However, in comparison with other perspectives on AI, it is still a niche topic. Ouchchy et al. (2020) specifically analyzed news articles about ethical issues of AI using an English-language sample of various media outlets. They found that prejudices reflected in data, privacy, and transparency were the most common issues in the news articles. Furthermore, they report that when specific applications were discussed in terms of AI ethics, autonomous driving was the most mentioned application followed by the use of AI in the military and healthcare domain. Overall, the discussion about AI ethics was neither particularly critical nor enthusiastic. For the German media landscape, Fischer and Puschmann (2021) report that, if risks of AI were mentioned in the news, especially accountability and transparency issues were featured.\nSumming up the literature on news coverage of AI, some similarities emerge on a global level. The media discourse is led by economic and political actors, and topics that highlight the positive impact of AI. Ethics is a niche topic in media reporting about AI. These findings also apply to Germany, where the present study was conducted.\nThe empirical results on published opinion on AI may be partly explained by the dominance of conservative and business-friendly publishers (McChesney, 2008; Nichols & Martinez, 2020; Wasko, Murdock, & Sousa, 2011) of news stories about AI among the press. This explains the observation that especially those actors are most present in the mass media within the public sphere who utilize ethical guidelines as a marketing device or have a vested interest in an unregulated approach to AI, while actors who actively strive for implementation of ethical AI for the Common Good are fairly underrepresented. In absence of personal experience with AI, from the perspective of communication science, the underlying assumption is \u201cthat the media crucially influence audience attitudes towards emerging technologies\u201d (Metag & Marcinkowski, 2014, p.465). For the salience of AI issues among the public this may mean the following: On the one hand, news media weigh some topics over others and the public gets to know a rather one-sided picture of the technology with a strong emphasis on economic opportunities. On the other hand, when there is no media interest in reporting on ethical AI, then salience of AI-related ethical issues among the public seems to be highly unlikely, too."
        },
        {
            "heading": "3.2 Public Opinion on AI",
            "text": "Following increasing academic interest in media reporting on AI, several studies have recently explored public opinion on AI.\nResearch on public opinion on AI found that, while many people have heard about the term, usually, only some of the surveyed people suggest to have an idea of what AI is and what AI entails, or claim to be somewhat knowledgeable about AI, respectively (Cave, Coughlan, & Dihal, 2019; Selwyn & Gallo Cordoba, 2021). Thereby, AI is mostly associated with technical terms, foremost robots or computers (Cave et al., 2019; Kelley et al., 2021; Selwyn & Gallo Cordoba, 2021). All in all, when surveyed, current AI technology is perceived rather positively in various countries (Fietta, Zecchinato, Di Stasi, Polato, & Monaro, 2021; Kelley et al., 2021; Zhang & Dafoe, 2019). In addition many studies conducted around the globe, highlight sociodemographic differences in the evaluation of AI: Male, younger, and well-educated respondents, people with higher income, and people, who consider themselves to possess much knowledge about AI are more likely to expect that opportunities materialize with the advent of AI (Carradore, 2021; Fietta et al., 2021; Zhang & Dafoe, 2019). Moreover, Kelley et al. (2021) report country differences in the strength of positive sentiment towards AI, whereby residents of non-Western countries (e.g., Brazil, and South Korea) are more enthusiastic about AI than residents of Western countries (e.g., USA, France).\nSome studies investigated public opinion towards selected ethical aspects of AI using closed-ended questions. Utilizing a representative German sample, Kieslich, Starke, Dosenovic, Keller, and Marcinkowski (2020) asked for risk perceptions of some particular ethical challenges that AI might pose in terms of injustice and discrimination. On the one hand, the authors report that uncertain accountability, as well as loss of control, were perceived as major ethical challenges of AI implementation by the public. On the other hand, potential systematic discrimination of social groups or injustice via AI were only articulated by a subset of the German population when inquired. The results also suggest that a significant portion of respondents (17% and 16%, respectively) did not answer or indicated that they did not know whether the latter issues pose a social risk in contrast to 5%-9% missing answers for other ethical issues. This indicates that fairness and discrimination issues may not be salient or even conceivable for a fair share of the German population. Another study by Kieslich et al. (2022) researched preferences for the ethical design of an AI system in the use case of tax-fraud detection. They report that accountability was perceived as the most important ethical criterion among the respondents, while machine autonomy was perceived as the least important. However, the researchers also report that the different ethical AI principles were perceived as roughly equally important, even though the respondents show different preference patterns among each other. Nearly one-quarter of the respondents cared only marginally about the ethical design of AI systems, while 32% were highly concerned about AI ethics. Moreover, the results suggest that those that were not concerned with ethical AI were predominantly older, less educated, and less interested in AI than respondents that were concerned about the ethical design of AI systems. In summary, the German public expresses\nsome concerns about AI ethics, but the ethical concerns vary between different segments of the population (Kieslich et al., 2022, 2020).\nResearching the perceived importance of ethical requirements proposed by the EU, Choung, David, and Ross (2022) showed for a US sample that, again, accountability was perceived as most important. As in the study by Kieslich et al. (2022), they also report that all ethical requirements are evaluated equally important. Ikkatai, Hartwig, Takanashi, and Yokoyama (2022) surveyed Japanese citizens to evaluate public attitudes towards AI ethics. Testing for four different scenarios, they show that public attitudes toward the importance of ethical AI design are context-dependent, with AI use in the military being the most ethically problematic one. AI use in a military context was also the scenario most citizens disagreed with. Ikkatai et al. (2022) also showed that age was significantly correlated with attitudes towards AI ethics in all scenarios; moreover, gender, interest, and AI literacy were significantly correlated to attitudes toward AI ethics in some scenarios.\nAll in all, when it comes to public opinion concerning AI ethics, there is only limited evidence towards general salience of ethical issues connected with AI. On the one hand, the research at hand often addresses these issues as a rather broad category within the topic of AI and it is not clear what specific ideas citizens have when it comes to the ethics of AI. On the other hand, research that directly gathers opinions on specific ethical aspects often does not capture the general salience of ethical issues of AI with citizens. In asking directly about opinions of ethical aspects of AI, the question itself makes ethical aspects salient; also to those, who would otherwise not indicate having thoughts about ethical AI, to begin with. Therefore, we do not know much about the actual salience of ethical AI and its consequences on intended behaviors among citizens. Thus, in this study, we bridge the literature strands of broad associations with AI and concrete opinions on ethical AI in investigating the general salience of ethical AI with the public."
        },
        {
            "heading": "4 Hypotheses and Research Questions",
            "text": "In the present study, we build on the existing literature and dig deeper into public opinion on AI with a focus on ethical AI. Our study was conducted in Germany, which-politically-follows the Western, respectively European approach to AI. That is, the German government aims for broad funding of economical and scientific progress in AI while acknowledging ethical issues (Die Bundesregierung, 2018). Thus, many AI projects were funded by the German government in recent years. However, as a small inquiry by the Left Party in the German Bundestag revealed, that only a few AI systems were checked with an AI risk analysis before their implementation (Die Bundesregierung, 2022). Regarding the state of AI usage in the economic sector in Germany, 5.8 percent of all companies used AI methods in 2019 (Bundesministerium f\u00fcr Wirtschaft und Energie, 2020), while, according to a study conducted in 2021, 30 percent of German companies indicated that they are thinking about implementing AI (Bitkom, 2021). Hence, AI plays a continually growing role-at least in politics as well as in the economy.\nWhile former public opinion studies utilized open-answered questions with a focus on knowledge about AI, we are interested in the issues that are present in the minds of citizens\u2014if any. Unlike other studies, we do not present respondents with a given set of different future perspectives or ethical issues but gather information if citizens link AI to ethical aspects on their own. As AI is still a relatively new technology and news reporting of AI is\u2014albeit increasing \u2014still a topic that is not on the top of the news, we first explore how many respondents are preoccupied with AI topics at all. In a second step, we are interested in the different topics citizens associate with AI. As discussed earlier, especially economic and political actors, which are most prominent in media reporting, are not interested in a wide discussion about the social impact of AI. Consequently, we suppose that issue salience of AI ethics is not very prominent among German citizens. Another reason for that is that in the German context, unlike in the UK, no major scandal concerning AI emerged in the past. Yet, we do not know the variety and the emphasis of different AI issues on behalf of citizens. Thus, we set up RQ1.\nRQ1: What topics are present in the German population with regard to AI?\nFurther, prior studies suggest that several individual characteristics may influence public opinion on AI such as interest in or knowledge of AI as well as sociodemographic aspects like gender, age, educational level or socioeconomic status (Carradore, 2021; Fietta et al., 2021; Ikkatai et al., 2022; Kieslich et al., 2022; Zhang & Dafoe, 2019). Given the one-sided media reporting about AI and the focus on specific stakeholders by economic and political actors, we suppose that AI can be considered as a topic of the elite. Thus, we hypothesize:\nH1a: Salience of AI issues is higher among citizens with a higher level of education.\nH1b: Salience of AI issues is higher among citizens with a higher socioeconomic status.\nH1c: Salience of AI issues is higher among citizens with a higher general interest in AI.\nAdditionally, we explore which socio-economic characteristics and interest in AI influence if someone is concerned with ethical AI in particular. In the context of ethical aspects of AI, the questions of which sociodemographic factors, as well as interest in AI, influence the issue salience of ethical AI, become particularly relevant, since AI implementation can have serious consequences for different societal groups. For example, it was found that AI systems treated women (Tambe, Cappelli, & Yakubovich, 2019) or persons with a low socioeconomic status (Pandey & Caliskan, 2021) worse than others. Additionally, Frey and Osborne (2017) assumed that the jobs of people with a lower educational degree are more at risk of being automated. Thus, we explore if those who are probably more endangered by the implementation of AI systems are aware of those ethical issues. Though, due to a lack of media reporting on ethical aspects and the dominance of economic topics, affected groups might not be concerned with ethical issues of AI. Hence, we ask:\nRQ2: Do educational level, socioeconomic status, and interest in AI have an effect on salience of ethical AI issues?\nFurther, we investigate if different topics are associated in context with each other. We suppose that ethical issues are mostly connected to an anchor example. Thus, ethical issues emerge when discussed in light of a certain usage of AI. For example, Ouchchy et al. (2020) showed that ethical aspects of AI were repeatedly reported on in the context of specific applications like autonomous driving. Also, the cases were protest was articulated by the population or by civil society actors were tied to a specific example, e.g. education admission systems (Kelly, 2021) or vaccine allocation (L\u00fcnich & Kieslich, 2022). As the literature does not suggest, which use cases are currently on the top of the head of citizens or especially suitable for a public debate about ethical issues of AI, we formulated H2 rather broad:\nH2: People who are concerned with ethical AI commonly connect AI to specific applications.\nLastly, we investigate if the salience of ethical AI influences behavioral intentions. We have outlined that ethical AI has to be demanded by the public to be put into practice by companies or politicians. For companies, considering citizens\u2019 demands is necessary in the sense of economic profit. If citizens hesitate to use AI systems because of ethical considerations or raise protests against an AI tool, it would damage the profit companies make. Hence, public pressure can presumably force economic actors to follow ethical guidelines. Likewise, political actors can be influenced by public opinion as well. If citizens demand ethical AI systems, political actors could listen to those voices and set up binding guidelines for the development and implementation of AI systems. However, for that to happen, the salience of ethical AI needs to influence citizens\u2019 behavior in a way that leads them to reluctance toward AI. Given the use cases, where ethical flaws in AI systems lead to public outrage, we presume that the salience of ethical AI among the public affects AI avoidance intention, respectively the intention to engage in public discussions about AI.\nH3a: Salience of ethical issues of AI positively affects intended AI avoidance.\nH3b: Salience of ethical issues of AI positively affects the intention to engage in public discussions about AI."
        },
        {
            "heading": "5 Method",
            "text": ""
        },
        {
            "heading": "5.1 Data Collection & Sample",
            "text": "The study is part of the long-term monitoring of public opinion on AI in Germany. In fifteen surveys, about 1,000 respondents were asked about interest, attitudes, and behavioral intentions regarding AI. The monthly surveys were conducted between May 2020 and April 2021 by the market research institute forsa GmbH as part of their Omninet omnibus panel. That is, the questions on AI were asked alongside other changing topics. The advantage of this procedure is to minimize self-selection, as respondents do not choose questionnaires by interest in specific subjects. This allows a more realistic and rather representative picture of the population.\nThe Omninet panel consists of 75,000 panelists who are recruited via random sampling of phone numbers. The panel is representative of the German population with internet access aged 14 and older for the personal respondents characteristics age, gender, and regional place of residence. Our questions were only asked those panelists who are 18 years and older.\nAll 15 samples included in this study were randomly selected from the Omninet panel. After data cleaning, the dataset contains 14,988 respondents. Of these, 51.6% are self-identifying female and the average age is 51 (SD = 15.96) years. This corresponds very well to the distribution in the German population aged 18 and older with internet access (agof e.V., 2020). 60.2 percent of our sample have at least a high school degree, higher education being slightly overrepresented (Dosenovic, Kieslich, & Keller, 2021)."
        },
        {
            "heading": "5.2 Measurement",
            "text": "Salience of AI-related issues. As for the dependent variable, we are interested in whether people are concerned with AI and, if so, what these issues are. Following agenda-setting research (McCombs & Shaw, 1972), we are looking for a\nmeasurement of the public agenda\u2014but this time precisely on the subject of AI. Two conditions were important to us: First, respondents should be able to say that no issue concerns them at all. Second, we did not want to ask about problems by default, as was often done in agenda-setting research (Smith, 1980). Thus, we asked \u201cIf you think about recent times, which issues related to artificial intelligence have been of most concern to you personally?\u201d Respondents were asked to provide up to three answers to this question in an open text box.\nAfter conducting the fourth survey, the codebook was developed in an iterative process. That is, the two coders and one of the authors randomly drew several samples of responses from the data set. They coded all the same data and discussed their results afterward. Each of the possible three responses was assigned to only one code. If two (or more) codes were deemed applicable, the coding that described AI in more detail or on which emphasis was placed was selected. For example, if an answer was \u201cjob loss due to automation\u201d, \u201cjob loss\u201d was chosen as code as it depicts the consequence of automation. Analogously, \u201cethical issues due to autonomous driving\u201d was coded as \u201cethics\u201d. After the first coding process, additional samples were drawn and previously formulated codes were used to categorize the new data until there was no new code assigned. This resulted in four major categories (technical functionality; AI in use / AI applications; ethical issues about AI; other issues about AI) with a total of 49 subcodes. A new sample of 150 responses was then coded by the two coders for a reliability test. Krippendorf\u2019s alpha (Hayes & Krippendorff, 2007) is rated as good at the value of \u03b1K = .82 [Bootstrap with 10,000 samples; CI 95%: .72; .90]. For the category ethical issues about AI, we adapted the principles outlined by Jobin et al. (2019). We added the more specific ethical problems surveillance1, manipulation as well as a general subcategory for ethics to the list of ethical criteria, as they emerged as subcategories during the coding process. The codebook with the occurrence of all subcodes in the sample can be found in Appendix A.\nInterest in AI. Interest in AI was measured with the following four items on a five-point Likert scale: \u201cI follow AI processes with great curiosity.\u201d, \u201cI am very interested in AI in general.\u201d, \u201cI read articles about AI with great attention.\u201d, \u201cI watch or listen to articles about AI with great interest.\u201d The index suggests high internal consistency (M = 2.49; SD = 0.97; \u03b1C = .94).\nBehavioral Intentions. We were interested in how the salience of specific AI-related issues is associated with the intention to avoid AI or to engage in public discussions about AI, respectively. Both were measured as single items on a five-point Likert scale: \u201cI will stay away from artificial intelligence wherever possible.\u201d (M = 2.43; SD = 1.18) and \u201cI will express my opinion in public discussions about artificial intelligence.\u201d (M = 2.67; SD = 1.26).\nControls. In the results part, we utilized age (in years), gender (0 = male, 1 = female), educational level (1 = primary degree, 2 = secondary degree, 3 = tertiary degree), and socioeconomic status (0 = household income above the median, 1 = household income below median) as control variables."
        },
        {
            "heading": "6 Results",
            "text": "All in all, 6,221 (41.5%) respondents indicated that they were recently concerned with AI and, thus, gave at least one answer to the question at hand. 2,090 (13.9%) respondents gave at least two answers and 1,030 (6.9%) respondents gave three answers. In contrast, 7699 (51.4%) respondents were not concerned with AI, while 302 (2.0%) respondents stated that they do not know the term artificial intelligence, and further 766 (5.1%) preferred not to answer this question.\nFirst, we were interested in the specific AI issues the German public was concerned with (RQ1). In general, specific application domains were the most frequent response. 4,122 (66.3%) named at least one application or application domain in the open-ended answers. In contrast to this, AI functionalities were only mentioned by 906 (14.6%), AI issues by 1,274 (20.5%), and AI ethics by 943 (15.2%) of the respondents. Thus, the German public is mostly concerned with specific use cases of AI rather than issues, functionalities, or ethical issues\nTo dig deeper into the specific topics that the German public is concerned with, we also investigated the subcodes of the categories. Figure 1 shows the most common issues with AI and groups them according to their dimensions (functionalities, applications, ethical issues, and other AI issues). For better readability of the figure, we only depicted those issues that at least 100 respondents mentioned.\nThe result shows that the most salient issue was one specific application domain, namely autonomous driving or mobility. It was mentioned by nearly one-third of the respondents who were concerned with AI at all. Also, other application domains such as medicine or smart home applications were frequently mentioned. Concerning AI issues, the most frequent mentions were connected to workplace issues such as job opportunities or job losses due to AI. Regarding\n1Surveillance was assigned to the \u201cethical issues\u201d category, since most answers referred to surveillance in the dystopian sense of mass surveillance.\ntechnical issues, AI was frequently connected to robotics, which is in line with the study of Selwyn and Gallo Cordoba (2021). Only a small portion of respondents name ethical aspects as the most salient issues.\nNevertheless, we were interested in the gradations of ethical AI issues. Thus, we also counted the mentions of all ethical issue subcodes. Figure 2 shows the result. Ethical issues are most connected with indications about control, i.e. if AI can or needs to be controlled. Another, more specific ethical concern is the possibility of surveillance through AI technology, which was the second most ethical issue raised by the respondents. Remarkably, terms that are connected with the FAccT dimensions are the ethical dimensions that are least mentioned by the respondents. Thus, there is practically almost no salience of fairness, accountability, and transparency issues among the German public.\nIn the next step, we research the hypothesized effects of educational level, socioeconomic status, and interest in AI on the salience of AI in general (H1a-H1c). We included the age and gender of respondents as control variables in the model.\nBy calculating a logistic regression model on the issue salience of AI, we find that higher educational level, being younger, and being more interested in AI in general, seeking information about AI, respectively, predict whether AI is on the personal agenda, as indicated by expressing at least one AI issue (Table 1). No significant effect could be found for socioeconomic status. The model explains 37.2 percent of variance with interest in AI being the strongest predictor. Thus, H1a and H1c are supported, while H1b needs to be rejected.\nIn the next step, we investigated the effects of educational level, socioeconomic status, and interest in AI on the salience of ethical issues. Again, we conducted a logistic regression on the dependent variable salience of ethical AI issues.\nThereby, we only calculated the model for those respondents who were concerned about at least one AI issue. Thus, we explore if specific respondents\u2019 characteristics lead to a higher salience of ethical issues. We controlled the effects for respondents\u2019 age and gender (Table 2).\nThe model reaches significance, but the independent variables only explain 0.8% of the variance of the dependent variable. Even though we find significant effects for age, socioeconomic status, educational level, and interest in AI, these effects are not substantial. The low salience of ethical principles does not allow us to look for structural differences in the population. This could change should the topic become more prominent. Then it could be precisely these characteristics that account for greater attention. At this point, however, such an assumption does not hold up. Regarding RQ2, we found no notable effect of educational level, socioeconomic status, and interest in AI on the salience of ethical AI.\nTo answer H2, we calculated the co-occurrences of the different categories of AI issue salience. Co-occurrences can only be detected if participants mentioned at least two issues they were concerned with AI. For each group we summed up the number of respondents who mentioned issues of both respective categories.\nAI ethics were mentioned 62 (6.6%) times together with AI functionalities, 218 (23.1%) times together with AI applications and 202 (21.4%) times together with other AI issues. However, 535 (56.7%) respondents mentioned AI ethics without any other category. In comparison, AI applications (3,293, 79.9%) were oftentimes named without any other category. AI functionalities (448, 49.4%) and AI issues (745, 58.5%), on the other hand, were mentioned to an equal amount with other categories. Though AI ethics are most commonly mentioned without any other categories, it is mostly connected to specific AI applications or other AI issues. Thereby, the huge number of single mentions is connected to the fact that a majority of the respondents only were concerned with AI in one issue (66.2%) and did not indicate up to three issues. Given that salience of ethical AI had the highest co-occurrence with AI applications, H2 can be supported.\nLastly, we test if the different AI-related issues people were concerned with have an impact on 1) intended AI avoidance (H3a) and 2) intention to engage in public discussions about AI (H3b). For that, we calculated two linear regression models with the dependent variables 1) intended AI avoidance and 2) intention to publicly speak about AI with the independent variable salience of ethical AI issues. We included the salience of AI applications, AI functionalities, and other AI issues as control variables. We further control the effects with the variables used in the models before\u2014age, gender, educational level, socioeconomic status, and AI interest. We calculated the model for the respondents, who were concerned with AI omitting those, who are disengaged with AI. Thus, we can investigate in detail which effects different categories of issue salience of AI have. Tables 3 and 4 show the results of the linear regression model.\nConcerning the OLS regression on avoidance intention, the independent variables explain 11.4% of the variance for the dependent variable avoidance intention. Among the issue salience variables, ethical salience had the strongest impact on avoidance intention. Avoidance of AI rather occurs if people have ethical issues of AI top of their mind, supporting H3a. Conversely, people who are concerned with specific applications show fewer intentions to avoid AI. The salience of AI functionalities and other AI issues do not have a significant effect on avoidance behavior. Concerning the control variables, AI interest is the strongest negative predictor of the dependent variable. We also find significant effects for age, socioeconomic status, and gender. Among people who are concerned with AI, those who are more interested in AI show fewer intentions to avoid AI. Additionally, older people, people with a socioeconomic status below average, and women report higher avoidance tendencies.\nRegarding the OLS regression on the intention to engage in public discussion about AI, the model explains 15.6% of the variance of the dependent variable. Hereby, only the salience of ethical aspects of AI has a significant, yet small, effect on the intention to speak about AI. Those who are concerned with ethical topics are more willing to talk about AI. Thus, H3b can be accepted. Respondents who have other AI topics in mind show no significant tendency to talk more or less about AI. Again, we find the strongest effect for interest in AI, followed by gender and age as significant predictors. Higher interest in AI leads to a higher intention to engage in public discussions about AI. This tendency is stronger for men and younger people."
        },
        {
            "heading": "7 Discussion",
            "text": "Our study sheds light on a rather understudied perspective on ethical AI\u2014public opinion. We investigated this perspective by focusing on the salience of AI issues. For that, we collected data from 14,988 German respondents."
        },
        {
            "heading": "7.1 Low Salience of Ethical Issues and its Implication for Implementing AI Ethics",
            "text": "Our results show that AI does not play a major role in the minds of German citizens with about three-fifths not been concerned with AI recently. However, when thinking about AI it is foremost thinking about specific applications, especially autonomous driving. Ethical issues only play a subordinate role among the German public, and if so, the major share does not think about issues of fairness, accountability, and transparency to which many researchers in the field of Fair Machine Learning are devoted. These results indicate that the scientific\u2014and partially political\u2014discourse does not reach the broad public. Ethical issues of AI are mainly a topic, which is discussed by researchers. However, leaving out respectively not reaching the public may interfere with the normative goal of ethical development for the\nCommon Good. Without a broad public discourse about these topics, it is very unlikely that public demands for the development of ethical AI will emerge (Kieslich, 2021). It is not plausible that political institutions or companies invest massive amounts of financial and personnel resources in the development of ethical AI that benefits the whole society and treats people equally if the public does not demand it. At this point, it seems more likely that political institutions set up guidelines but make compromises to hold their own in global competition (Hagendorff, 2020). Likewise, as long as companies make vast profits and do not provoke massive scandals, economic actors presumably will develop AI systems in a way that satisfies their stakeholders (Hagendorff, 2020). Importantly, only addressing stakeholders leaves out a significant portion of society."
        },
        {
            "heading": "7.2 How Likely is Critical Engagement with AI?",
            "text": "This leads to a reflection on the future of society\u2014and who is affected by the ethical threats of AI. Like the study of Frey and Osborne (2017) show for the context of workplace development, those who have lower income and educational levels are most susceptible to negative consequences of automation. Alarmingly, we observe that it is exactly those people who do not care much about AI. Educational level is\u2014besides interest in AI\u2014a strong predictor of being concerned with AI. Thus, those who will be negatively affected the most are not concerned with it. However, we did not find effects for gender and higher socioeconomic status for being concerned with AI. As ethical issues with AI often relate to women being discriminated against (Tambe et al., 2019) or lower incomes resulting in deprivation by algorithms (Pandey & Caliskan, 2021), it is also noteworthy that these groups of people are not particularly concerned with AI issues, which would play into the hands of economic and political actors as previously argued. Our data show that greater attention to ethical issues regarding AI is linked to both the intention to avoid AI and the intention to participate more intensively in debates. As a result, those who pay attention to ethical issues regarding AI\u2014which in most cases are potential breaches of ethical standards\u2014do not contribute to greater corporate profits and may even steer a discourse on AI toward problematic aspects. If people are more concerned with the ethical challenges AI systems pose for society, a more critical way of citizen engagement with AI will seem possible. Instead, the current discourse tends to result in a strongly product- and application-oriented way of thinking about AI. Thus, the public AI agenda reflects quite well the commerce-oriented tone in the media discourse about AI. Having the high amount of mentioned AI applications in contrast to ethical issues in mind, we describe the current state of critical engagement with AI in Germany as low. Yet, the results suggest that, if ethical issues are made prevalent, citizens are more reluctant to integrate AI in their lives or to publicly engage with AI.\nIn summary, ethical issues of AI are not very prevalent among the German public. This may be due to the lack of a critical use case, which fueled public outcry against an AI system in Germany. Different from countries such as the UK (Kelly, 2021) or France (Wenzelburger & Hartmann, 2021), there has not been a major scandal in Germany concerning AI yet. Additionally, we attribute these results to the low level of media coverage on ethical aspects of AI (Fischer & Puschmann, 2021) and the dominance of actors from the economic or political sector in news reporting. Although there are some examples where specific AI applications were blocked due to public awareness (Algorithm Watch, 2020), we are far away from a broad discourse about the ethical challenges of AI systems. Yet, we also detected in our data, that ethical issues may have an impact on citizens\u2019 behavior towards AI. We also found that ethical issues may have an impact on citizens\u2019 behavior towards AI when such issues are prevalent in the minds of citizens. Future studies should monitor the ongoing public debate about ethical AI and investigate, whether certain key events or a possible change towards more critical news reporting will affect the broad public opinion on AI. At least, for AI for the Common Good, it is relevant that the public is involved in an ethical debate about AI."
        },
        {
            "heading": "8 Limitations",
            "text": "Our study has several limitations that need to be acknowledged. First of all, we conducted our study in Germany. Citizens of other countries might have different concerns with AI, for instance, Kelley et al. (2021) showed that public opinion on AI differed largely between citizens of different countries. In the case of the focus on public concerns about AI ethics, it would be especially interesting to compare our results with those of UK or French citizens, where public debates about ethical issues of AI have taken place lately (Kelly, 2021; Wenzelburger & Hartmann, 2021). It would be interesting to investigate, whether a scandal might have an influence on public concerns with ethical issues of AI in the long term. Accordingly, we encourage researchers to replicate our survey in other countries.\nWe also point out some potential issues with our measurement of AI issues. We deliberately chose a broad approach with an open-ended question to grasp public concerns with AI. However, we had to reduce the complexity of our coding process. Thus, if two codes were applicable for one answer, we had to choose one code, which was in focus. However, some information might have gotten lost during this process. Moreover, not being concerned with AI, and especially ethical issues, does not necessarily have to mean that the public is not aware of these issues. It means, however, that\nthese issues are not of immediate concern and arguably are not on the personal agenda. As we rely on agenda-setting theory, the measurement chosen most adequately grasps our concept. Finally, our measurement is not fine-grained enough to go into detail about the concerns the public has. For example, it might very well be that respondents, who answered \u201cautonomous driving\u201d in the open-ended question think of possible ethical problems (e.g., the trolley problem). But if the answer was only \u201cautonomous driving\u201d it was coded as such (with it being value-free). Though, if respondents answered \u201cethical problems with autonomous driving\u201d, it was coded as an \u201cethics\u201d issue. As we aimed for an overview analyzing the public agenda of AI issues, we believe our measurement is satisfactory. Further research should nevertheless build on our measurement and extend it in a way that the tonality of several issues is measured as well."
        },
        {
            "heading": "9 Implications",
            "text": "We empirically showed that ethical AI plays a minor role in the minds of German citizens. Given the media reporting about AI in Western countries, which highlights economical aspects and oftentimes neglects ethical issues, this finding is not surprising. Though, media outlets oftentimes are self-committed to reporting in a multi-faceted way about issues that are of importance to citizens. Although Ouchchy et al. (2020) and Chuan et al. (2019) found that ethical issues are on the rise, news reporting on AI does not seem to be very balanced yet. Including ethical aspects in news reporting could lead to a higher salience of ethical issues on behalf of media consumers. This could, for example, be achieved by picking up ethical topics or including more perspectives of researchers or NGOs, who are actively engaging with the implementation of ethical AI. Currently, we see the dominance of economic and political goals mirrored in the public opinion towards AI in Germany. Vividly, German citizens are especially concerned with AI in the contexts of autonomous driving or advances in medicine, which are applications that the industry is vigorously pushing forward.\nThough, increasing attention on the media side is not the only way to fuel the salience of ethical AI issues. Long and Magerko (2020) define AI competencies that are needed for citizens to deal with AI reasonably. Beneath technical competencies, they argue that awareness of ethical AI should be taught to citizens. For that, literacy projects are needed that expand the current state of AI literacy programs that mostly focus on technical aspects of AI. However, for a curriculum in the sense of AI for the Common Good, societal and ethical issues should be included. Several programs already offer a wider scope on AI, for example, Elements of AI (MinnaLearn & University of Helsinki, 2022). In the long term, it also could be useful to integrate the topic of AI ethics into school or university curricula. Thus, students who study computer science would also learn about the (unintended) ethical risks of the technology and implement this knowledge in their further development of AI systems.\nAdditionally, strengthening civil society actors could be a promising way to fuel the public debate about ethical AI. Civil society actors could actively address journalists, so that media coverage includes positions of the civil society regularly. It was also shown that some civil society groups exerted public pressure in some use cases (Algorithm Watch, 2020). However, the inclusion of civil society is rather an exception. Strengthening civil society can be achieved by investing in human and material resources. This is all the more important because civil society actors or academics advocating ethical AI contrast with economic or political actors who have many times more resources at their disposal. For a more balanced public discourse, there needs to be at least a somewhat more even balance between the different stakeholder groups.\nWe conclude that the salience of ethical AI issues is very low. For AI to benefit the Common Good, the public debate about this topic needs to be strengthened and also carried out to those who are affected by negative effects of unethical AI. We find that there indeed is potential that citizens can critically engage with AI; however, it needs to be activated by media, civil society actors, researchers, and other stakeholders who aim for AI for the Common Good.\nFunding\nThis study was conducted as part of the project Meinungsmonitor K\u00fcnstliche Intelligenz (opinion monitor artificial intelligence). From January 2020 to March 2021 the project was funded by the Ministerium f\u00fcr Kultur und Wissenschaft des Landes Nordrhein-Westfalen (Ministry of Culture and Science of the State of North Rhine-Westphalia), Germany. Since April 2021 the project is funded by the Stiftung Mercator."
        }
    ],
    "year": 2022
}