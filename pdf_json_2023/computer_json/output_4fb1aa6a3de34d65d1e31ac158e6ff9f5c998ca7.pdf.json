{
    "abstractText": "Nowadays, people and organizations use social networks for allowing and facilitating the transfer of information among groups that share similar interests. Due to the wide repertoire of users that these social platforms have and the amount of information generated within them, the presence of bots has become a relevant issue, both to facilitate the sharing of true information or to disseminate false information (fake news). In the second case, bots could manipulate political opinions, be perpetrators of identity or information theft, among other possible dangers that can cause when interacting on the platform. Thus, the identification of bots in social networks can become a useful practice to evaluate credibility or to detect fake news. In this work, we extend a previously proposed credibility model for Twitter, by incorporating bot detection. The original model calculates the credibility of tweets based on three measures: text, account/user, and social impact, using different filters to analyse text (SPAM, bad words,and good spelling) and account attributes (e.g., creation date, followers, following) to calculate account/user and social credibility scores. The extended model considers in the user credibility, the bot verification. Additionally, the extended credibility model is implemented in T-CREo, a framework for real time credibility analysis. For bot detection, some machine learning algorithms for supervised learning, such as AdaBoost, Bagging, Decision Tree, Logistic Regression, and Random Forest are trained and evaluated. Results show that the best algorithm is the Random Forest for its capacity of generalization with an accuracy and F1-score values over 97% both in English and Spanish. The evaluation of the bot detection functionality in the credibility analysis shows a performance of precision=1.0, recall=0.8462, F1-score=0.9167, and accuracy=0.92 for both English and Spanish models in our validation tests. INDEX TERMS Credibility analysis, Bot detection, Social networks, Twitter.",
    "authors": [
        {
            "affiliations": [],
            "name": "ANA AGUILERA"
        },
        {
            "affiliations": [],
            "name": "PAMELA QUINTEROS"
        },
        {
            "affiliations": [],
            "name": "IRVIN DONGO"
        },
        {
            "affiliations": [],
            "name": "YUDITH CARDINALE"
        }
    ],
    "id": "SP:6bc27a9a910dfc7839ad221b11cb1aacf08ef6ca",
    "references": [
        {
            "authors": [
                "J. Clyde Mitchell"
            ],
            "title": "Social Networks on JSTOR",
            "venue": "Annual Review of Anthropology, 3:279\u2013299",
            "year": 1974
        },
        {
            "authors": [
                "Martina Draho\u0161ov\u00e1",
                "Peter Balco"
            ],
            "title": "The analysis of advantages and disadvantages of use of social media in European Union",
            "venue": "Procedia Computer Science,",
            "year": 2017
        },
        {
            "authors": [
                "Bogdan Batrinca",
                "Philip C. Treleaven"
            ],
            "title": "Social Media Analytics: A Survey of Techniques, Tools and Platforms",
            "venue": "AI & Society,",
            "year": 2015
        },
        {
            "authors": [
                "Richard J. Oentaryo",
                "Arinto Murdopo",
                "Philips K. Prasetyo",
                "Ee-Peng Lim"
            ],
            "title": "On Profiling Bots in Social Media",
            "venue": "In Social Informatics,",
            "year": 2016
        },
        {
            "authors": [
                "Chengcheng Shao",
                "Giovanni Luca Ciampaglia",
                "Onur Varol",
                "Kai-Cheng Yang",
                "Alessandro Flammini",
                "Filippo Menczer"
            ],
            "title": "The spread of lowcredibility content by social bots",
            "venue": "Nature Communications,",
            "year": 2018
        },
        {
            "authors": [
                "Stefano Cresci",
                "Roberto Di-Pietro",
                "Marianella Petrocchi",
                "Angelo Spognardi",
                "Maurizio Tesconi"
            ],
            "title": "A fake follower story: improving fake accounts detection on twitter",
            "venue": "Technical report, Consiglio Nazionale delle Ricerche,",
            "year": 2014
        },
        {
            "authors": [
                "Samara Castillo",
                "H\u00e9ctor Allende-Cid",
                "Wenceslao Palma",
                "Rodrigo Alfaro",
                "Heitor S. Ramos",
                "Cristian Gonzalez",
                "Claudio Elortegui",
                "Pedro Santander"
            ],
            "title": "Detection of bots and cyborgs in twitter: A study on the chilean presidential election in 2017",
            "venue": "In Social Computing and Social Media. Design, Human Behavior and Analytics,",
            "year": 2019
        },
        {
            "authors": [
                "Carter Yagemann",
                "Simon P. Chung",
                "Erkam Uzun",
                "Sai Ragam",
                "Wenke Lee"
            ],
            "title": "On the feasibility of automating stock market manipulation",
            "venue": "In Annual Computer Security Applications Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Serena Tardelli",
                "Marco Avvenuti",
                "Maurizio Tesconi",
                "Stefano Cresci"
            ],
            "title": "Characterizing social bots spreading financial disinformation",
            "venue": "In Social Computing and Social Media. Design, Ethics, User Behavior, and Social Network Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Srijan Kumar",
                "Neil Shah"
            ],
            "title": "False information on web and social media: A survey",
            "year": 2018
        },
        {
            "authors": [
                "McKenzie Himelein-Wachowiak",
                "Salvatore Giorgi",
                "Amanda Devoto",
                "Muhammad Rahman",
                "Lyle Ungar",
                "H. Andrew Schwartz",
                "David H. Epstein",
                "Lorenzo Leggio",
                "Brenda Curtis"
            ],
            "title": "Bots and misinformation spread on social media: Implications for covid-19",
            "venue": "Journal of Mededical Internet Research,",
            "year": 2021
        },
        {
            "authors": [
                "Wentao Xu",
                "Kazutoshi Sasahara"
            ],
            "title": "Characterizing the roles of bots on twitter during the covid-19 infodemic",
            "venue": "Journal of computational social science,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Shafahi",
                "Leon Kempers",
                "Hamideh Afsarmanesh"
            ],
            "title": "Phishing through social bots on twitter",
            "venue": "In International Conference on Big Data,",
            "year": 2016
        },
        {
            "authors": [
                "Matthew C. Benigni",
                "Kenneth Joseph",
                "Kathleen M. Carley"
            ],
            "title": "Bot-ivistm: Assessing information manipulation in social media using network analytics",
            "venue": "In Emerging Research Challenges and Opportunities in Computational Social Network Analysis and Mining,",
            "year": 2018
        },
        {
            "authors": [
                "Sourena Maroofi",
                "Maciej Korczy\u0144ski",
                "Andrzej Duda"
            ],
            "title": "Are you human? resilience of phishing detection to evasion techniques based on human verification",
            "venue": "In Internet Measurement Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Richard J. Oentaryo",
                "Arinto Murdopo",
                "Philips K. Prasetyo",
                "Ee-Peng Lim"
            ],
            "title": "On profiling bots in social media",
            "venue": "In Social Informatics,",
            "year": 2016
        },
        {
            "authors": [
                "Zi Chu",
                "Steven Gianvecchio",
                "Haining Wang",
                "Sushil Jajodia"
            ],
            "title": "Detecting automation of twitter accounts: Are you a human, bot, or cyborg",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2012
        },
        {
            "authors": [
                "Suman Kalyan Maity",
                "Aishik Chakraborty",
                "Pawan Goyal",
                "Animesh Mukherjee"
            ],
            "title": "Detection of Sockpuppets in Social Media",
            "venue": "In Companion of the ACM Conference on Computer Supported Cooperative Work and Social Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Seth A. Myers",
                "Aneesh Sharma",
                "Pankaj Gupta",
                "Jimmy Lin"
            ],
            "title": "Information network or social network? the structure of the twitter follow graph",
            "venue": "In the 23rd International Conference on World Wide Web,",
            "year": 2014
        },
        {
            "authors": [
                "Onur Varol",
                "Emilio Ferrara",
                "Clayton A. Davis",
                "Filippo Menczer",
                "Alessandro Flammini"
            ],
            "title": "Online human-bot interactions: Detection, estimation, and characterization",
            "venue": "In International AAAI Conference on Web and Social Media,",
            "year": 2017
        },
        {
            "authors": [
                "Wajiha Shahid",
                "Yiran Li",
                "Dakota Staples",
                "Gulshan Amin",
                "Saqib Hakak",
                "Ali Ghorbani"
            ],
            "title": "Are You a Cyborg, Bot or Human?\u2014A Survey on Detecting Fake News Spreaders",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Eiman Alothali",
                "Nazar Zaki",
                "Elfadil A. Mohamed",
                "Hany Alashwal"
            ],
            "title": "Detecting social bots on twitter: A literature review",
            "venue": "In International Conference on Innovations in Information Technology",
            "year": 2018
        },
        {
            "authors": [
                "V.S. Subrahmanian"
            ],
            "title": "Amos Azaria",
            "venue": "Skylar Durst, Vadim Kagan, Aram Galstyan, Kristina Lerman, Linhong Zhu, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. The DARPA Twitter Bot Challenge. Computer, 49(6):38\u201346",
            "year": 2016
        },
        {
            "authors": [
                "Suruchi Gera",
                "Adwitiya Sinha"
            ],
            "title": "T-Bot: AI-based social media bot detection model for trend-centric twitter network",
            "venue": "Social Network Analysis and Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Majed Alrubaian",
                "Muhammad Al-Qurishi",
                "Mabrook Al-Rakhami",
                "Atif Alamri"
            ],
            "title": "A Credibility Assessment Model for Online Social Network Content. In From Social Data Mining and Analysis to Prediction and Community Detection, pages 61\u201377",
            "year": 2017
        },
        {
            "authors": [
                "Fang Zhou",
                "Jianlin Jin",
                "Xiaojiang Du",
                "Bowen Zhang",
                "Xucheng Yin"
            ],
            "title": "A calculation method for social network user credibility",
            "venue": "In IEEE International Conference on Communications,",
            "year": 2017
        },
        {
            "authors": [
                "Irvin Dongo",
                "Yudith Cardinale",
                "Ana Aguilera"
            ],
            "title": "Credibility analysis for available information sources on the web: A review and a contribution",
            "venue": "In 4th International Conference on System Reliability and Safety,",
            "year": 2019
        },
        {
            "authors": [
                "Yudith Cardinale",
                "Irvin Dongo",
                "Germ\u00e1n Robayo",
                "David Cabeza",
                "Ana Aguilera",
                "Sergio Medina"
            ],
            "title": "T-CREo: A twitter credibility analysis framework",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "Stefano Cresci",
                "Roberto Di Pietro",
                "Marinella Petrocchi",
                "Angelo Spognardi",
                "Maurizio Tesconi"
            ],
            "title": "The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race",
            "venue": "In 26th International Conference on World Wide Web Companion,",
            "year": 2017
        },
        {
            "authors": [
                "Mariam Orabi",
                "Djedjiga Mouheb",
                "Zaher Al Aghbari",
                "Ibrahim Kamel"
            ],
            "title": "Detection of bots in social media: A systematic review",
            "venue": "Information Processing & Management,",
            "year": 2020
        },
        {
            "authors": [
                "Malak Aljabri",
                "Rachid Zagrouba",
                "Afrah Shaahid",
                "Fatima Alnasser",
                "Asalah Saleh",
                "Dorieh M Alomari"
            ],
            "title": "Machine learning-based social media bot detection: a comprehensive literature review",
            "venue": "Social Network Analysis and Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Oliver Beatson",
                "Rachel Gibson",
                "Marta Cantijoch Cunill",
                "Mark Elliot"
            ],
            "title": "Automation on twitter: Measuring the effectiveness of approaches to bot detection",
            "venue": "Social Science Computer Review,",
            "year": 2023
        },
        {
            "authors": [
                "Abdulrahman Alarifi",
                "Mansour Alsaleh",
                "AbdulMalik Al-Salman"
            ],
            "title": "Twitter turing test: Identifying social machines",
            "venue": "Information Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Emilio Ferrara"
            ],
            "title": "Deep neural networks for bot detection",
            "venue": "Information Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Sylvio Barbon Jr.",
                "Gabriel F.C. Campos",
                "Gabriel M. Tavares",
                "Rodrigo A. Igawa",
                "Mario L. Proen\u00e7a Jr.",
                "Rodrigo Capobianco Guido"
            ],
            "title": "Detection of human, legitimate bot, and malicious bot in online social networks based on wavelets",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Kadhim Hayawi",
                "Sujith Mathew",
                "Neethu Venugopal",
                "Mohammad Masud",
                "Pin-Han Ho"
            ],
            "title": "Deeprobot: a hybrid deep neural network model for social bot detection based on user profile data",
            "venue": "Social Network Analysis and Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Rachit Shukla",
                "Adwitiya Sinha",
                "Ankit Chaudhary"
            ],
            "title": "Tweezbot: an AI-driven online media bot identification algorithm for twitter social",
            "venue": "networks. Electronics,",
            "year": 2022
        },
        {
            "authors": [
                "Pawan Kumar Verma",
                "Prateek Agrawal",
                "Vishu Madaan",
                "Charu Gupta"
            ],
            "title": "UCred: fusion of machine learning and deep learning methods for user credibility on social media",
            "venue": "Social Network Analysis and Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Nikan Chavoshi",
                "Hossein Hamooni",
                "Abdullah Mueen"
            ],
            "title": "Debot: Twitter bot detection via warped correlation",
            "venue": "In IEEE 16th International Conference on Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Christopher Bouzy"
            ],
            "title": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3320687 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License",
            "venue": "Bot sentinel,",
            "year": 2018
        },
        {
            "authors": [
                "Kai-Cheng Yang",
                "Emilio Ferrara",
                "Filippo Menczer"
            ],
            "title": "Botometer 101: social bot practicum for computational social scientists",
            "venue": "Journal of Computational Social Science,",
            "year": 2022
        },
        {
            "authors": [
                "K. Lorek",
                "J. Suehiro-Wici\u0144ski",
                "M. Jankowski-Lorek",
                "A. Gupta"
            ],
            "title": "Automated credibility assessment on twitter",
            "venue": "Computer Science, 16(2):157\u2014 168",
            "year": 2015
        },
        {
            "authors": [
                "Carlos Castillo",
                "Marcelo Mendoza",
                "Barbara Poblete"
            ],
            "title": "Information credibility on twitter",
            "venue": "In 20th International Conference on World Wide Web,",
            "year": 2011
        },
        {
            "authors": [
                "Hend S. Al-Khalifa",
                "Rasha M. Al-Eidan"
            ],
            "title": "An experimental system for measuring the credibility of news content in twitter",
            "venue": "International Journal of Web Information Systems,",
            "year": 2011
        },
        {
            "authors": [
                "Yoshimi Namihira",
                "Naomichi Segawa",
                "Yukino Ikegami",
                "Kenta Kawai",
                "Takashi Kawabe",
                "Setsuo Tsuruta"
            ],
            "title": "High precision credibility analysis of information on twitter",
            "venue": "In International Conference on Signal-Image Technology & Internet-Based Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Majed AlRubaian",
                "Muhammad Al-Qurishi",
                "Mabrook Al-Rakhami",
                "Mohammad Mehedi Hassan",
                "Atif Alamri"
            ],
            "title": "Credfinder: A real-time tweets credibility assessing system",
            "venue": "In International Conference on Advances in Social Networks Analysis and Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Tarek Hamdi",
                "Hamda Slimi",
                "Ibrahim Bounhas",
                "Yahya Slimani"
            ],
            "title": "A hybrid approach for fake news detection in twitter based on user features and graph embedding",
            "venue": "In International Conference on Distributed Computing and Internet Technology,",
            "year": 2020
        },
        {
            "authors": [
                "Adrian Iftene",
                "Daniela G\u00eefu",
                "Andrei-Remus Miron",
                "Mihai-Stefan Dudu"
            ],
            "title": "A real-time system for credibility on twitter",
            "venue": "In 12th Language Resources and Evaluation Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Maria Hernandez-Mendoza",
                "Ana Aguilera",
                "Irvin Dongo",
                "Jose Cornejo- Lupa",
                "Yudith Cardinale"
            ],
            "title": "Credibility analysis on twitter considering topic detection",
            "venue": "Applied Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Nanda Ihwani Saputri",
                "Yuliant Sibaroni",
                "Sri Suryani Prasetiyowati"
            ],
            "title": "Covid-19 fake news detection on twitter based on author credibility using information gain and KNN MethodsCovid-19 fake news detection on twitter based on author credibility using information gain and KNN methods",
            "venue": "Jurnal Rekayasa Sistem dan Teknologi Informasi,",
            "year": 2023
        },
        {
            "authors": [
                "David M. Beskow",
                "Kathleen M. Carley"
            ],
            "title": "Its all in a name: Detecting and labeling bots by their name, 2018",
            "venue": "[Online; accessed",
            "year": 2022
        },
        {
            "authors": [
                "Rosario Gilmary",
                "Akila Venkatesan",
                "Govindasamy Vaiyapuri"
            ],
            "title": "Detection of automated behavior on twitter through approximate entropy and sample entropy",
            "venue": "Personal and Ubiquitous Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Richard J. Oentaryo",
                "Arinto Murdopo",
                "Philips K. Prasetyo",
                "Ee-Peng Lim"
            ],
            "title": "On profiling bots in social media",
            "venue": "In 8th International Conference on Social Informatics,",
            "year": 2016
        },
        {
            "authors": [
                "Buket Er\u015fahin",
                "\u00d6zlem Akta\u015f",
                "Deniz K\u0131l\u0131n\u00e7",
                "Ceyhun Akyol"
            ],
            "title": "Twitter fake account detection",
            "venue": "In International Conference on Computer Science and Engineering (UBMK),",
            "year": 2017
        },
        {
            "authors": [
                "M\u00fccahit Kantepe",
                "Murat Can Ganiz"
            ],
            "title": "Preprocessing framework for twitter bot detection",
            "venue": "In International Conference on Computer Science and Engineering (UBMK),",
            "year": 2017
        },
        {
            "authors": [
                "Chiyu Cai",
                "Linjing Li",
                "Daniel Zengi"
            ],
            "title": "Behavior enhanced deep bot detection in social media",
            "venue": "In IEEE International Conference on Intelligence and Security Informatics,",
            "year": 2017
        },
        {
            "authors": [
                "Fred Morstatter",
                "Liang Wu",
                "Tahora H. Nazer",
                "Kathleen M. Carley",
                "Huan Liu"
            ],
            "title": "A new approach to bot detection: Striking the balance between precision and recall",
            "venue": "In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Jiliang Tang",
                "Salem Alelyani",
                "Huan Liu"
            ],
            "title": "Feature selection for classification: A review. In Data Classification: Algorithms and Applications, pages 37\u201364",
            "year": 2014
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "John Platt",
                "Thomas Hofmann"
            ],
            "title": "Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation",
            "venue": "In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference,",
            "year": 2007
        },
        {
            "authors": [
                "Dimitrios Effrosynidis",
                "Avi Arampatzis"
            ],
            "title": "An evaluation of feature selection methods for environmental data",
            "venue": "Ecol. Inf.,",
            "year": 2021
        },
        {
            "authors": [
                "Jaime Lynn Speiser",
                "Michael E. Miller",
                "Janet Tooze",
                "Edward Ip"
            ],
            "title": "A Comparison of Random Forest Variable Selection Methods for Classification Prediction Modeling",
            "venue": "Expert Syst. Appl.,",
            "year": 2019
        },
        {
            "authors": [
                "Marco Sandri",
                "Paola Zuccolotto"
            ],
            "title": "Variable Selection Using Random Forests. In Data Analysis, Classification and the Forward Search, pages 263\u2013270",
            "year": 2006
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "In 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Wilson E. Marc\u00edlio",
                "Danilo M. Eler"
            ],
            "title": "From explanations to feature selection: assessing SHAP values as feature selection mechanism",
            "venue": "In 33rd SIBGRAPI Conference on Graphics, Patterns and Images,",
            "year": 2020
        },
        {
            "authors": [
                "Xin Man",
                "Ernest Chan"
            ],
            "title": "The Best Way to Select Features? Comparing MDA, LIME, and SHAP",
            "venue": "The Journal of Financial Data Science Winter,",
            "year": 2021
        },
        {
            "authors": [
                "Shilpa Bhandari",
                "Avinash K. Kukreja",
                "Alina Lazar",
                "Alex Sim",
                "Kesheng Wu"
            ],
            "title": "Feature Selection Improves Tree-based Classification for Wireless Intrusion Detection",
            "venue": "SNTA",
            "year": 2020
        },
        {
            "authors": [
                "Leo Breiman"
            ],
            "title": "Random Forests",
            "venue": "Machine Learning,",
            "year": 2001
        },
        {
            "authors": [
                "Ke Zhang",
                "Peidong Xu",
                "Jun Zhang"
            ],
            "title": "Explainable AI in deep reinforcement learning models: A shap method applied in power system emergency control",
            "venue": "In 4th Conference on Energy Internet and Energy System Integration,",
            "year": 2020
        },
        {
            "authors": [
                "James Schnebly",
                "Shamik Sengupta"
            ],
            "title": "Random forest twitter bot classifier",
            "venue": "In 9th Annual Computing and Communication Workshop and Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Nirdhum Narayan"
            ],
            "title": "Twitter bot detection using machine learning algorithms",
            "venue": "In Fourth International Conference on Electrical, Computer and Communication Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E. Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of Computer and System Sciences,",
            "year": 1997
        },
        {
            "authors": [
                "Trevor Hastie",
                "Saharon Rosset",
                "Ji Zhu",
                "Hui Zou"
            ],
            "title": "Multi-class AdaBoost",
            "venue": "Statistics and Its Interface,",
            "year": 2009
        },
        {
            "authors": [
                "Dan H. Moore"
            ],
            "title": "Classification and regression trees. Cytometry",
            "year": 1987
        },
        {
            "authors": [
                "Christopher M. Bishop"
            ],
            "title": "Pattern Recognition and Machine Learning",
            "year": 2006
        },
        {
            "authors": [
                "T. Cover",
                "P. Hart"
            ],
            "title": "Nearest neighbor pattern classification",
            "venue": "IEEE Transactions on Information Theory, 13(1):21\u201327",
            "year": 1967
        },
        {
            "authors": [
                "Frank Rosenblatt"
            ],
            "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
            "venue": "Spartan Books,",
            "year": 1962
        },
        {
            "authors": [
                "Jesse Davis",
                "Mark Goadrich"
            ],
            "title": "The relationship between Precision- Recall and ROC curves",
            "venue": "In 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Christian Herzog"
            ],
            "title": "On the risk of confusing interpretability with explicability",
            "venue": "AI Ethics,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Raschka",
                "Yuxi (Hayden) Liu",
                "Vahid Mirjalili",
                "Dmytro"
            ],
            "title": "Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python",
            "venue": "Packt Publishing,",
            "year": 2022
        },
        {
            "authors": [
                "Irvin Dongo",
                "Yudith Cadinale",
                "Ana Aguilera",
                "Fabiola Mart\u00ednez",
                "Yuni Quintero",
                "Sergio Barrios"
            ],
            "title": "Web scraping versus twitter API: a comparison for a credibility analysis",
            "venue": "In 22nd International conference on information integration and web-based applications & services,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Credibility analysis, Bot detection, Social networks, Twitter.\nI. INTRODUCTION\nCurrently, social networks are the main means of communication around the world, daily generating a huge amount of information through the interaction of the people who use these platforms. In these interactions, people share all kinds of information about themselves or other users, and express their opinions and points of view on any topic of public or private interest [1]. Social networks have different purposes depending on their main use. For instance, Facebook generates relationships among users; YouTube is for entertainment activities; and LinkedIn is used for more professional contexts [2]. Some of the most recognized social networks are Facebook and Twitter, being the latter the most used for data analysis due to the simplicity to obtain the data (through an API) and its format [3].\nBecause these social networks are the main source of information for many users, they have become a main focus of interest for the presence of \u201csocial bots\u201d. A \u201cbot\u201d is defined as an automated program for interacting on a social network, with the goal of mimicking the behavior of a human user [4]. While useful bots exist within social networks (reminders, archivers, weather reports, etc.), several of these bots are used for harmful actions within the platform. Shao et al. [5] show evidence that social bots are an effective tool to manipulate social media and they are useful to spread low-credibility content, such as false news, conspiracy theories, and junk science. Malicious bots can be used to artificially increase the popularity of a person or social movement or group [6]; to influence or manipulate people for political purposes [7]; to manipulate financial markets [8] [9]; to disseminate spam\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nor false information [10]\u2013[12]; and for conducting fraud or theft of other users\u2019 information [13]\u2013[15].\nWorks on bot detection make different classifications regarding bots, differentiating them by their behavior, usefulness, or way of operating [16], [17]. Some of these classifications include:\n\u2022 Consumer Bots: Bots whose main objective is to obtain content from various sources and provide update services, such as weather reports, horoscope, or news for consumption or personal use [16]. \u2022 Broadcast Bots: Bots with the main purpose of disseminating information to a general audience, through news articles, sites of interest, or events. They are usually used and managed by organizations or groups of people. Their content is usually benign [16]. \u2022 Spam bots: These bots are responsible for disseminating content with false information, or harmless, or irrelevant content on a repetitive basis. They also redirect to malicious sites for fraud. These bots are considered malicious [16]. \u2022 Hybrid Bots or Cyborgs: This classification corresponds to user accounts that are managed by human users with the help or support of a bot or vice versa [17]. \u2022 Crawlers and scrapers: They are bots that have been designed to index and archive webpages, for example, to make the websites available through search engines. \u2022 Chatbots: They are computer programs that converse with people in real-time using natural language. Frequently used by businesses for easier customer interaction, such as responding to frequently asked queries. \u2022 Spambots: They are software or computer systems that have been compromised by malware and are under the control of a third party. They are used to send out a large number of messages, advertising, comments, etc. \u2022 Social bots: Active on social media, these programs create content on their own and frequently pass for genuine people. \u2022 Sockpuppets: They are genuine people who assume false identities, often to advance a certain cause or product, such as business promotion, favorable book, and film reviews. The intention of these sockpuppets is to create opinion bias towards an entity and usually they are employed for online deception [18].\nConsidering this classification, it is important to highlight that not all bots found in social networks, in this case Twitter, correspond to programs designed with bad intentions. Many useful bots are used by several real users to improve the quality of experience within this platform or with beneficial purposes [11]. It is also important to take into account the existence of hybrid bots or cyborgs, since the pattern of activity that these bots have are totally different from those of a common bot or a human user, making their correct identification even more difficult [11].\nTwitter is a very popular microblogging service with structural characteristics of both an information network and a\nsocial network [19]. An evidence of this popularity is the fact that the number of Twitter accounts have doubled in the last 5 years passing of 109 millions in 2017 to 237 millions in 20221. Nevertheless, the users are exposed to fake news or being manipulated with any type of interest. A study in 2017 declares that around 15% of Twitter accounts would correspond to bots [20], possibly mirroring the situation of most other social networks on the web [21]. Many works have been interested in the issue of bot detection and there is a clear need for the research community to develop new technologies that can identify social bots [22], however this remains still a challenge [21], [23], [24]. On the other hand, there is also a latent requirement for automatic systems to help users verify the reliability/credibility of the sources they access. In the current era of information overload, restlessness, and uncertainty, it is essential to create models for confirming information from online social networks like Twitter [25]. When using the richness of social information available about almost anything, trust plays a crucial part in assisting social network users in making the right choices [26].\nIn this context, this work extends a credibility model for Twitter presented in [27], by incorporating bot detection. The original model analyses the credibility of a tweet considering tree dimensions: text, account/user, and social impact. The new extended model considers, within the user credibility, the identification of bots, in order to detect the nature of the analyzed user. Moreover, in order to evaluate the accuracy of the extended credibility model, it is incorporated into TCREo, a framework to perform credibility analysis in Twitter, in real time [28]. In summary, the main contributions of this work are summarized as follows:\n\u2022 Two datasets for bot detection (English and Spanish), extending the ones proposed in [29] with content features, such as wavelet average and entropy; \u2022 An algorithm for Twitter bots detection in real time, based on predictive models; one for tweets in English and the other one for tweets in Spanish; \u2022 A comparative study for identifying the best features involved in the models that characterize the bots; \u2022 An extended credibility model that integrates a bot detection function to calculate the level of credibility of tweets, incorporated into T-CREo to demonstrate its functionality and suitability.\nThe remainder of this paper is organized as follows. Section II corresponds to the review of works related to bot detection in Twitter and some works which consider characteristics of the user of a Twitter account, in their credibility analysis. Section III describes the methods used for Twitter bot detection including the dataset configuration, the preprocessing, and the training and selection of predictive models. In Section IV, the original credibility model proposed in [27] is described. Section V presents the addition of bot detection\n1https://www.statista.com/statistics/970920/monetizable-daily-activetwitter-users-worldwide/"
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nin the extended credibility model. Finally, Section VI discuss the main results and the future work.\nII. RELATED WORK In this section, some studies are presented and analyzed mainly according to the bot detection implementation, regardless of the prediction objective it fulfills (i.e., detection of benign bots, malicious bots, or cyborgs). Additionally, some works focused on credibility in Twitter, that consider characteristics of users are summarized.\nA. DETECTION OF BOTS ON TWITTER Bot detection in social networks is a topic that has been researched and still continues to be a challenge. Some reviews of the most recent techniques that have been published analyse the different classifiers used, the type of dataset used, and the features selected, as well as presenting comparisons among the evaluation techniques used for their validation [22], [30]\u2013[32]. Alothali et al. [22] identify three common methods, based on graph, crowdsourcing, and machine learning, for social bot detection on social networks. For the machine learning algorithms, authors select 20 features of content, profile, and behavior from accounts on social networks. Orabi et al. [30] elaborate a bibliographic review of published works between 2010 and 2019 resulting in a taxonomy of detection methods and techniques used to detect bots in social media and a comparison among them. Aljabri et al. [31] survey existing studies from 2015 to 2022, which apply supervised, semi-supervised, and unsupervised machine learning techniques for social media bot detection. Authors consider the detection of social bots, spambots, and sybil bots on Facebook, Instagram, LinkedIn, Twitter, and Weibo, and provide a novel taxonomy of social media bot detection using machine learning based techniques. Moreover, they present an analysis of the most commonly extracted features and datasets used on each social media platform. Beatson et al. [32] compare a semi-automated method based on fixed criteria against a manual model based on open source intelligence (OSINT) to detect bots in Twitter. The evaluation is performed in terms of accuracy, user friendliness, efficiency or time costs, and simplicity of operation. Authors conclude that semi-automated method removes the need for manual checking, and a manual approach relies on searching for unusual patterns of behavior.\nSome proposals consider machine learning approaches using supervised algorithms [17], [33]\u2013[38]. Chu et al. [17] focus their work on the classification of humans, bots, and cyborgs on Twitter. They consider a user\u2019s content posting time period as a feature to be analyzed, where a lower entropy value corresponds to an apparent sign of automation, while a higher entropy value would be reflecting human involvement in the account activity. Authors conclude that, due to the more defined behavioral pattern of real users and bots, both are easier to identify than cyborgs. Indeed, the classification of cyborgs can yield many false positives considering their mixed behavior as humans and bots. The accuracy of\ntheir classification model reaches 96% for binary prediction (bots and humans). Alarifi et al. [33] propose a supervised classifier considering bots, hybrids, and human users as the three prediction categories. Authors performed data acquisition through Twitter API REST and Twitter Streaming API collecting information about 1.8 million accounts. They then performed labeling of the data by a selected group of people trained in classification. After the tests performed, they obtained 96% of accuracy and a Kappa coefficient at 0.61, resulting in a good percentage of agreement among the labeling group. They use four learning algorithms for training: Random Forest, BayesNet, Support Vector Machine (SVM), and multilayer artificial neural network. After the tests carried out, the predictive models obtained by Random Forest and BayesNet proved to have an accuracy percentage of 91%. Kudugunta et al. [34] describe a work using a neural network based on contextual short-term memory (LSTM), where both tweet content and metadata are considered. Social features of the user are also used, taking into account only the detection of bots at the tweet level, obtaining an accuracy of 96%.\nBarbon et al. [35] propose an algorithm based on Discrete Wavelet Transform to obtain a pattern of writing style embedded in post contents in order to classify authors/users as being a human, a legitimate robot, or a malicious robot. Authors trained a classification model using the Random Forest algorithm over two real Twitter datasets (Super Bowl XLVIII event and Seatle Seahawks) and their model achieved an average accuracy of 94.47%. Hayawi et al. [36] propose a deep neural network designed with LSTM units and dense layers to classify human or bot accounts in Twitter. The classification is based on user profile metadata, including username, screen name, tweet count, followers count, friends count, listed count, user created date, description, location, url, verified flag, obtained through the Twitter API. The proposed model is evaluated on several datasets using two manners: testing on a hold-out set of the same dataset and training with one dataset and testing with a different one. Results show that the proposed model achieves an area under curve (AUC) as high as 0.97 with a selected set of features on the hold-out test set and 93% F1-score with a threshold=0,34.\nShukla et al. [37] propose a multi-layer condition-based model to identify fraudulent bots in Twitter. The proposed method analyzes Twitter-specific user profiles considering profile name, description, location, listed count, verification status, that are recovered with the Twitter API, and also compare the string-based features with a bag of words containing tokens that are likely to be prevalent in bots. Authors performed a comparative evaluation of the proposed model with existing benchmark classifiers, such as SVN, Categorical Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Multilayer Perceptron, Decision Trees, Random Forest, and other automation identifiers, as well as with publicly available standard automation identifier API, such as Botometer, BotSentinel, and TweetBotOrNot. The Kaggle dataset is used for the experimentation, obtaining an accuracy of 99.0049% and a\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\ntesting AUC of 0.996606. Verma et al. [38] propose UCred, a model aimed at classifying user accounts as fake or real. UCred model separates text features of the user profile (e.g., description) from numeric features (e.g., followers, friends, verified flag) to be evaluated simultaneously by several machine learning and deep learning methods. Text features are evaluated by post-trained model (i.e., CNN, LSTM, BiLSTM) and pretrained methods (i.e., BERT, RoBERT, DistilBERT), while numeric features are evaluated with Na\u00efve Bayes, SVN, Random Forest, and K-Nearest Neighbor (KNN). Outputs from all these methods are submitted to a voting classifier to finally decide if the account is real or fake. To evaluate the performance, authors performed experiments on OSN dataset, which contains 1337 fake and 1481 real profiles. Results show that UCred reaches up to 98,96% of accuracy and 98,37% of F1-score.\nCurrently, there exist some automatic tools for bot detection in tweets in English, such as Debot [39], TweetBotOrNot [40], Bot Sentinel [41], and Botometer2 [42]\nDebot [39] is a system developed at the University of New Mexico which performs a search for correlated accounts by making use of deformed correlation (correlation extended by dynamic time warping) when analyzing their activity, given the idea that humans cannot be highly synchronous over long periods of time. The approach used in this system corresponds to an unsupervised method, not needing a labeled dataset to train a model beforehand. Debot works as a near real-time system, and has an accuracy of 94%, in addition to having an API that allows using it in external projects.\nTweetBotOrNot [40] is an R package developed by Michael W. Kearney in 2018. It is based on the gradient boosted supervised model to obtain the probability of a Twitter account being a bot. To do so, the model uses user-level metadata, tweet-level, and tweet text, including friends, followers, posts, account time creation, retweets. The classifier was trained on 10,000 Twitter accounts with over 7000 bots, from which TweetBotOrNot shows an accuracy of 93.8%.\nBot Sentinel [41] is another Twitter analytics free tool founded in 2018 by Christopher Bouzy. Based on machine learning and artificial intelligence, this tool is able to identify and track trollbots and untrustworthy Twitter accounts. There is not much information available related to the implementation details of this tool, however it has been used recently by other researchers in this domain.\nBotometer [42] is a a public tool designed by a joint project of the Observatory on Social Media and the Network Science Institute at Indiana University. Botometer is a supervised machine learning classifier that distinguishes bot-like and human-like accounts considering over 1000 features categorized into six classes: user profile, friends, network, temporal, content and language, and sentiment. The tool evaluates a Twitter account\u2019s activity and assigns it a score. Greater scores indicate increased bot-like activities. This service\n2https://botometer.osome.iu.edu/\nrequires Twitter authentication and authorization in order to use. Both Botometer and Twitter API have rate limits, meaning that users can only make a certain number of queries in a given time period. Botometer has a website and API endpoints with similar functionality. Using the Botometer API requires keys associated with a Twitter app, when querying the API, users are responsible to send the required data (i.e., 200 most recent tweets by the account being checked and tweets mentioning this account) in a specified format through HTTPS requests.\nAll these works demonstrate the interest and feasibility of bot detection on Twitter. While most of these works report good results after testing, they do not fully match our requirements, regarding the consideration of English and Spanish and a prediction performance in real time, for the development of the functionality in our previous credibility model. Additionally, the datasets used are not available for general use; thus, a separate or further retraining is not possible. In the same way, although Alarifi et al. [33] perform supervised training at the end of their data collection process and design and implement a browser plug-in, none of these resources are available or enabled for use or incorporation into another project. Debot, TweetBotOrNot, and Bot Sentinel are still working today, but the teams in charge of those projects do not provide more keys for the use of their API; while Botometer has the same limitations of use as the Twitter API. Table 1 summarizes the points discussed, considering also the number of attributes used by each work as a reference.\nThe interest of this work is to be able to detect whether the user of the analyzed Twitter account is a bot or not, leaving the possibility of using additional analysis to subsequently infer whether the bot in question is a useful or dangerous bot."
        },
        {
            "heading": "B. CREDIBILITY ANALYSIS CONSIDERING THE ACCOUNTS/USERS",
            "text": "Concerning on how to do the credibility analysis, the existing works consider the analysis of some types of information to calculate different credibility measures [27], [43], [44]. This information mainly includes four dimensions or levels associated to text, user or account, topic, and social influence. Text dimension measures the level of relevance and accuracy of the text, independent of the referenced topic or with respect to a certain topic. It is mainly calculated through text analysis techniques. User dimension calculates the user account credibility based on attributes that describe it (e.g., the account creation date, if the account is verified, user gender and age). Social dimension calculates the credibility of a publication, related or not to a topic, based on the available metadata that describe the social impact of the user account and the post itself, with respect to other users. The social data, such as number of followers and friends, retweets, likes, are commonly used. Topic-level dimension measures the level of acceptance of the topic or event referenced in the text. It consists of identifying if the text refers to a specific topic or not, usually through text and sentiment analysis and topic detection techniques."
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nIn the next, we survey some studies that have considered the user level for the credibility analysis. Al-Khalifa & AlEidan [45] developed a system to measure the credibility of arabic news content published in Twitter. The system uses two approaches to assign credibility levels (low, high, and average) to each tweet. The first approach is based on the similarity between Twitter posts (tweets) and authentic (i.e., verified) news sources. The second approach is based on the similarity with verified news sources in addition to a set of proposed features. The Twitter user profile features include username, user picture, real name, and verified account.\nNamihira et al. [46] propose an expertise score by taking into account the user\u2019s knowledge (expertise). Expertise is the ratio of the number of tweets determined to be the same topic as that of the suspected tweet to the number of the past tweets of the target user. If the expertise score of the user becomes higher, then tweets of the user are treated as a more reliable opinion even if it is a minority opinion. Likewise, if an author of the tweet refers to the same topics in his other tweets in the past, his/her expert score is increased. Afterward, the credibility is calculated using the expertise score for each user.\nAlrubaian et al. [47] propose a real-time content credibility assessment system named CredFinder to measure the trustworthiness of information through user analysis and content analysis. The tweet and its metadata are sent to the extraction features process to generate a feature vectors. Afterward, all these data are used as input to the credibility score calculation algorithm. Data include the tweet content and the additional fields, such as the time of posting, the author name, number\nof followers, number of friends, hashtags, or mentions. Hamdi et al. [48] propose an hybrid approach to evaluate information sources in term of credibility in Twitter. This approach relies on node2vec to extract features from Twitter followers/friends graph and also incorporates user features provided by Twitter. Authors analyze user-based features established by them as useful information for source of fake news detection. Such features represent the profile characteristics of users who have interacted with news on social media. The extracted features concerning the users are: created at, name, status count, default profile, and default profile image.\nIftene et al. [49] trained a neural network model using a collection of tweets that were manually annotated by human users as credible and not credible. For each individual tweet, authors collect the following types of information concerning the user profile features: hasLocation (if user filled the location field), hasDescription (if user filled the description field), hasGeo (if user turned on geolocation), isVerified (if user was verified by Twitter), and creationDate (the date when account was created). Hern\u00e1ndez-Mendoza et al. [50] extend a model originally proposed by [27] to analyze the credibility of social information sources, which was instantiated for Twitter. In this instantiated model authors calculate the credibility of a user considering whether the account is verified or not and the activity time of the account since its creation.\nSaputri et al. [51] propose a method based on information gain and KNN to detect fake news related to COVID-19. The model is trained with a dataset built by authors and manually labeled as credible or not credible. Using the Twitter API, authors collected tweet with the hashtag #covid19, along\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nwith the users\u2019 profile, consisting on username, location, user description, account creation time, number of followers, as well as user verification, from which the author\u2019s credibility is determined and the manual labeling is done. From the text of tweets, the information gain is computed based on the TF-IDF method. Thus, from the user\u2019s credibility and the information gain, the KNN is able to identify tweets related to COVID-19 as fake news or valid information.\nTable 2 summarizes these works considering the different credibility levels (text, user, social, and topic), if the work can be applied in realtime or not, and the application context in which they were developed. These studies demonstrate that the credibility of the user of Twitter account is relevant for credibility analysis. In this work, besides considering the common features of users to calculate the user credibility level, we enrich the model by detecting the nature of the user (bot or not bot).\nIII. EVALUATION OF METHODS FOR BOT DETECTION IN TWITTER For the evaluation of algorithms for bot detection, we follow the methodology shown in Figure 1, which includes steps from data collection and cleaning, to training of the predictive models and their evaluation. This methodology is inspired on the classical methodology for Knowledge Discovery in Databases (KDD), which proposes five phases for data processing (i.e., data selection, preprocessing, transformation, data mining, and interpretation/evaluation). In this work, we correspond these phases such as database configuration (data selection), preproccesing (preprocessing and transformation), training phase (data mining), and testing and analysis (interpretation/evaluation), as Figure 1 illustrates. In the following, we detail each phase.\nA. DATASET CONFIGURATION The dataset configuration consists of two steps: feature engineering and data extraction from Twitter, as we explain in the following."
        },
        {
            "heading": "1) Feature engineering step",
            "text": "The first step of our methodology consists in making a feature engineering, whose result is used in the training of the predictive models. Table 2 shows works that use social features of the analyzed account [45], [47]\u2013[51], others works consider features of the user itself [45]\u2013[51], and others works use structural features associated with the text content [45], [47], [49]\u2013[51]. These features have been also considered in our credibility model. For the new extended credibility model which includes bot detection, some additional user features considering their relevance for the training of predictive models studied are selected [33]\u2013[35]. Similar to features selected in the works presented by [33], [34], in our work we also consider some social features of the user account. We further aggregate data on the text or content of the tweet, considering structural features as in [34]. We have also considered a new feature based on the Discrete Wavelet\nTransform (DWT) [35], which have been used for text analysis. The features selected and used in this work for the construction of the training datasets and subsequent use in the training process include:\n1) Followers_count: The number of followers the account has. 2) Favorite_count: The total number of times a tweet has been favourite. 3) Favourites_count: The number of tweets a user has liked. 4) Listed_count: The total number of public lists in which the user is a member. 5) Friends_count: The total number of users the user has added as friends. 6) Retweet_count: The total number of retweets of the user. 7) Statuses_count: The total number of tweets the user has or has made on their accounts. 8) Num_URLs: The number of URLs contained in the tweet to analyze. 9) Num_hashtags: The number of hashtags contained in the tweet to analyze. 10) Num_mentions: The number of mentions contained in the tweet to analyze. 11) Wavelet average: A centroid calculated from vectorization of the tweet text based on the DWT. 12) URL_ratio: The ratio of the number of URL present in the text to the total number of words in the tweet. 13) Mention_ratio: The ratio of the number of mentions present in the text to the total words in the tweet. 14) Hashtag_ratio: The ratio of the number of hashtags present in the text to the total words in the tweet. 15) Entropy: Value of the Shannon entropy formula.\nThe list of features corresponds to social features or quantitative information from the text itself. Most features can be retrieved from the Twitter account itself using the Twitter API. The features 3) Favourites_count, 6) Retweet_count, 8) Num_URLs, 9) Num_hashtags, and 10) Num_mentions were also used by Kudugunta et al. [34], while features 1) Followers_count, 2) Favorite_count, 4) Listed_count, and 5) Friends_count were used by Alarifi et al. [33]. The feature 11) Wavelet average considers the DWT to obtain spectral information according to the content of the tweet text, such as was proposed by Barbon et al. [35]. The text of the tweet is vectorized using the spaCy library, differentiating whether it is a text in Spanish or English, and then calculating the DWT of the vector. Finally, an average of the complete vector is performed and this result is used as a feature for classification. Originally, authors in [35] use the DWT to obtain spectral information according to the content of the tweet text and to characterize the writing style embedded in post contents. For the proposal of this particular work, a similar technique is used, in the sense that the content of the tweet is transformed into a vector to which the DWT is calculated and then its centroid is computed. The centroid"
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nwas not considered in the original procedure proposed by Barbon et al. [35].\nFeatures 12) URL_ratio, 13) Mention_ratio, and 14) Hashtag_ratio are structural text features based on ratios that are considered interesting to study. Some ratios provide additional information which can be useful. For example, Kudugunta et al. [34] use a feature that considers the ratio between followers and friends (following) of the account. The idea is to be able to see if the ratio between the number of words composing a tweet and the number of mentions/URLs/hashtags could be an important feature that helps to define a pattern of behavior of social bots on Twitter. Feature 15) Entropy is based on the Shannon\u2019s Entropy calculation. This feature measures the amount of information contained in the text of the tweet, lower values means less information the text contains. The entropy has been also used in similar contexts\nof bots detection [7], [52], [53]."
        },
        {
            "heading": "2) Data extraction",
            "text": "Although there are a variety of works that offer the possibility of requesting the datasets used for their studies based on Twitter, most of them only have the id of the labeled accounts and not their respective tweets. Moreover, we found that some of these datasets are obsolete with deleted accounts or they are now totally different from the period in which they were collected [54]. Other authors have created their own datasets using the Twitter API, but they are not reported as public [55]. The task of making a full dataset containing tweets and data related to them demands many resources, such as the labeling process and the data gathering from Twitter, where the API offers limitations for free accounts. For instance, authors in [56] report a time of four months to\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\ncollect data from 1800 Twitter accounts. Other studies, such as the one presented in [57], focus on the prediction models and use public datasets proposed by other works [29], [58].\nIn this work, we requested and used the datasets collected by authors in [29]. These datasets contain different user accounts and their tweets in a variety of languages, such as English, Spanish, French, Japanese. No additional requests to the Twitter API were required. Table 3 shows the total number of user accounts contained in the different datasets (genuine, social spambots, traditional spambots, and fake followers), the number of tweets associated to them, and the year which represents the average of the creation years of the accounts that belong to the dataset. This table is a simplified version of the original table presented in [29].\nB. PREPROCESSING After obtaining the datasets (Table 3), an initial review showed that they contain several encoding errors (accents, punctual letters such as \u2018\u00f1\u2019 or capital letters), emojis, and characters that do not contribute to the content of the text of the tweet. Additionally, the text associated to tweets are in a variety of languages, such as English, Spanish, French, Japanese. For these reasons, the preprocessing to prepare the data for training involves data cleaning and data filter tasks, such as:\n\u2022 Fix encoding errors: The Python library ftfy is used to correct text encoding problems in tweets. This consideration is necessary mainly when preprocessing the information that would compose the training dataset in Spanish, since several characters had encoding problems, such as accents (\u00e1,\u00e9,\u00ed,\u00f3, and \u00fa), umlauts, or the letter \u2019\u00f1\u2019. \u2022 Feature scaling: Due to different feature scales, they are standardized by removing the mean and scaling to unit variance. For this, features are scaled using the Scikit-learn library StandardScaler. This function calculates the standard score of a sample x as: z = (x \u2212 u)/s, where u is the mean and s is the standard deviation of the training samples. \u2022 Filter by text language: A function is used to identify the language in which the tweet was written with the Python library langid; thus, it is possible to make a quick classification of texts written in English or Spanish tagging them as \u2019en\u2019 or \u2019es\u2019, respectively.\nOnce the preprocessing is finished, the relevant records are collected between the group of genuine (human) accounts and the rest of the bot groups (social spambots #1, #2, and #3, traditional spambots #1 and #2), obtaining two datasets. Table 4 shows the statistics associated to each dataset distributed by language and category (human or bot).\nThe total number of rows for the Spanish training dataset is limited by the number of tweets made by bots. Although there is a larger number of tweets in Spanish from genuine (human) users, we decide to take a similar number of tweets\nfrom bots in order to keep the training dataset as balanced as possible. Although more data are available for the English training dataset, we try to do not exceed by much the size of the Spanish dataset.\nB.1. Feature selection methods In the interest of reducing the high dimensionality of datasets and to provide more parsimonious models, different methods of feature selection have been studied and classified into three groups: filters, wrappers, and embedded [59]. However, no single selection algorithm into these groups seems to be capable of ensuring optimal results in terms of predictive performance, robustness, stability, and interpretability. In this sense, we have explored two feature selection methods which have been used successfully in different domains. The first one corresponds to an embedded method, feature importance in Random Forest [60]\u2013[63]. The second one considered is the SHapley Additive exPlanations (SHAP) method, which help understanding what are the main features that affect the output of the model [64], and is also being used as a feature selection mechanism [65]\u2013[67]. In this context, the feature selection is assisted by SHAP \u2013 i.e., the features are selected and grouped according to SHAP values."
        },
        {
            "heading": "1) Feature importance in Random Forest",
            "text": "As an interesting result derived from the importance of the features in the training process of the data with the Random Forest, some features with higher variance were involved in the classification process (i.e., favourites_count, statuses_count, followers_count, friends_count, and listed_count). Figure 2 and Figure 3 show these best features. The results correspond to the permutation feature importance of the Random Forest3. This is a model inspection approach that can be used to any fitted estimator and is especially helpful for estimators that are non-linear or opaque. The permutation feature significance is defined as the reduction in model score caused by randomly rearranging a single feature value. The technique destroys the link between the feature and the goal, and the decrease in model score reflects how much the feature is relied upon. This method has the advantage of being model independent and can be calculated numerous times with various permutations of the characteristic [68]. The disadvantages of impurity-based feature importance are overcome by permutation feature importance since it does not favor high-cardinality features and may be computed on a test set with missing data."
        },
        {
            "heading": "2) SHAP method",
            "text": "SHAP (SHapley Additive exPlanations), a unified framework for interpreting predictions, assigns each feature an importance value for a particular prediction. This method has also been adopted to provide a reasonable interpretable model [64]. The SHAP value method has been used in a number of cases to calculate the importance of features in\n3https://scikit-learn.org/stable/auto_examples/ensemble/ plot_forest_importances.html"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nmachine learning models, so as to get an intuitive explanation [69]. Figure 4 and Figure 5 show the SHAP values on our two datasets.\nIn furtherance of testing the impact of applying a feature selection method, we have reduced the number of features to 7 and 5 for Spanish and English datasets, respectively. Afterwards, we have retrained our classifiers, resulting Random Forest (as classifier) with the best metrics. Selected features correspond to an importance score greater than 1 in Table 5. Table 6 shows metrics resulting from the prediction on the testing sets using Random Forest. We can observe that the metrics are good with values over 90% and the selected features are associated to social features. We could infer from these results that these features are independent of the language and the tweet content (note that the associated features to language and content were not selected by the feature selection methods). However, we prefer to remain conservative about this insight since more cases such as other languages have to be analyzed, thus we have trained our classifiers with the whole set of features \u2013 i.e., fifteen features (Section III-A1). We would like to explore, as a future work, other languages different to Spanish and English, to measure the real impact of the language and the tweet content in the bot detection; which also demands to include new datasets.\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nTABLE 5: SHAP feature values\nFeature number Feature name (Spanish) Importance Feature name (English) Importance 0 favourites_count 30.449375 favourites_count 18.714688 1 statuses_count 4.948437 statuses_count 14.826875 2 followers_count 4.694687 listed_count 8.447188 3 friends_count 2.247812 followers_count 7.021250 4 listed_count 1.743125 friends_count 1.870937 5 num_mentions 1.186562 retweet_count 0.355937 6 mention_ratio 1.128125 url_ratio 0.248437 7 retweet_count 0.239375 entropy 0.199375 8 url_ratio 0.192188 num_mentions 0.180938 9 entropy 0.097187 wavelet_avg 0.180000 10 num_urls 0.091250 mention_ratio 0.169687 11 wavelet_avg 0.063438 num_urls 0.147812 12 hashtag_ratio 0.053125 favorite_count 0.138750 13 num_hashtags 0.035937 hashtag_ratio 0.090312 14 favorite_count 0.029375 num_hashtags 0.080937\nTABLE 6: Best testing results using features selected with SHAP for Spanish and English\nClassifier Number of Features Precision Recall F1 score Accuracy AUC Score Spanish Random Forest 7 0.9133 1.0000 0.9547 0.9450 0.9347 English Random Forest 5 0.9575 0.9904 0.9737 0.9732 0.9731\nC. TRAINING AND SELECTION OF PREDICTIVE MODELS Since the writing style in English and Spanish has different structures that could hinder the analysis, we consider two separated models for training and classification, instead of one general predictive model. In the following, we present the experimental conditions and training details."
        },
        {
            "heading": "1) The experimental conditions",
            "text": "In this work, we deal with a supervised learning problem with a binary classification task considering two classes: \"bot\" and \"human\". In this context, there exist several machine learning algorithms that have shown good results for bot\ndetection [70], [71]. We have considered some of the most popular and relevant:\n\u2022 AdaBoost classifier: It is a meta-estimator that starts by fitting a classifier to the initial dataset and then fits additional copies of the classifier on the same dataset with the weights of instances that were mistakenly categorized. This is done such that future classifiers concentrate more on challenging situations [72], [73]. \u2022 Bagging classifier: It is an ensemble meta-estimator that applies basic classifiers to different random subsets of the initial dataset, and then combines each prediction (either by voting or by averaging) to get the final prediction. By adding randomization to the process of building a black-box estimator (such a decision tree), a meta-estimator of this kind can often be used to lower the variance of the estimator [74]. \u2022 Decision Tree classifier: It is a supervised learning technique that is non-parametric and is commonly used in classification tasks. The objective is to learn straightforward decision rules derived from the data features in order to build a model that predicts the value of a target variable. It is a technique capable of performing multiclass classification on a dataset [75]. \u2022 Logistic Regression: Instead of a model used in a regresion task, this one is used for classification. In the literature, logic regression, maximum-entropy classification (MaxEnt), and the log-linear classifier are also used to refer to logistic regression. In these models a logistic function is used to simulate the probabilities describing the potential outcomes of a single experiment [76]. \u2022 Random Forest classifier: It is a meta estimator that employs averaging to increase predicted accuracy and reduce overfitting after fitting numerous decision tree classifiers to different dataset subsamples [68]. \u2022 K-Neighbors classifier (KN): Neighbors-based classification is a type of instance-based learning or non-"
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\ngeneralizing learning. The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. Thus, from a simple majority vote of the nearest neighbors of each point a classification is computed. A query point is assigned the data class which has the most representatives within the nearest neighbors of the point [77]. \u2022 Support vector machine (SVM): SVM is a machine learning method widely used for solving classification and regression problems. The main idea of the SVM algorithm is to build an optimal hyperplane with the largest margin that best separates the data into classes [78]. The SVM is a popular algorithm due to its high generalization capability together with its ability to find global and non-linear classification solutions. In this work, we use Support Vector Classification (SVC), the scikit-learn implementation of this method for classification purposes. \u2022 Multi-layer Perceptron (MLP): MLP is a supervised learning algorithm that learns a function by training on a dataset. Given a set of features or input of m dimensions and a target of o dimension, it can learn a nonlinear function approximator for either classification or regression [79].\nTo determine the best parameters, each algorithm was trained several times by exploring different parameter values using a GridSearchCV method4. This method exhaustively considers all parameter combinations over specified parameter values for each classifier. The parameters of the classifier used to apply these methods are optimized by cross-validated grid-search over a parameter grid. Table 7 summarizes the parameters used for each algorithm.\nThe classification metrics5 used to quantify the predictions quality of the algorithms selected are precision, recall, F1score, accuracy, and AUC score. Considering the number of true positives (tp), false positives (fp), false negatives (fn), and true negatives (tn), these metrics reach its best value at 1 and worst score at 0, calculated as follows:\n\u2022 Precision: It is the ratio tptp+fp . The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. \u2022 Recall: It is the ratio tptp+fn . The recall is intuitively the ability of the classifier to find all the positive samples. \u2022 F1-score: It can be interpreted as a harmonic mean of the precision and recall. The relative contribution of precision and recall to the F1-score is equal, calculated as 2 \u2217 precision\u2217recallprecision+recall . \u2022 Accuracy: This function computes the proportion of correct classifications, as tp+tntp+fn+tn+fp . \u2022 AUC score: Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction\n4https://scikit-learn.org/stable/modules/generated/sklearn.model_selection. GridSearchCV.html\n5https://scikit-learn.org/stable/modules/model_evaluation.html#classificationmetrics\nscores. It is created by plotting the fraction of tp out of the positives vs. the fraction of fp out of the negatives, at various threshold settings [80]."
        },
        {
            "heading": "2) Training",
            "text": "We proceeded to train with different machine learning algorithms to analyze which of them would be the most suitable to use as classifier for bot detection. We use the library scikitlearn6 in Python to create a script to train, test, and visualize the results obtained by each algorithm. For each language, datasets are divided by 80% for training and 20% for testing. The training set is used in the cross-validation process, while the test set is subsequently used in the validation phase.\nDue to the cross-validation process, each algorithm is performed four times (i.e., cv = 4, the parameter for the crossvalidation) with the training set. This cross-validation returns stratified folds \u2013 i.e., the folds are made by preserving the percentage of samples for each class. The hyperparameter space is visited to search for the best cross validation score. In this way any parameter provided when constructing a classifier is being optimized. Table 8 shows the GridSearchCV results. All the possible combinations of parameter values are evaluated and the best combination is retained in terms of accuracy score. The mean and standard deviation of test scores are reported."
        },
        {
            "heading": "D. TESTS AND RESULTS ANALYSIS",
            "text": "In this section, we present the results obtained in the testing phase of the selected models once they were trained. Two kinds of validation are performed. The internal validation is performed with the 20% of each dataset from the splitting phase and which purpose is testing. The external validation corresponds to test performed on new cases of external sources."
        },
        {
            "heading": "1) Internal validation",
            "text": "Table 9 summarizes metrics resulting of the prediction with each best classifier (Table 7) fitted to the testing datasets (from the splitting of the training datasets). Likewise, Figure 6 and Figure 7 show the confusion matrices corresponding to these classifiers.\nIn terms of precision and accuracy in the validation phase, the worst values are for Bagging with 55.76% and 52.98%, respectively for the Spanish dataset, and Decision tree with 64.79% and 72.42% for the English dataset. While the rest of algorithms got metrics over 93% for the same metrics on both datasets (except for Adaboost with 56.39% and 54.10% for the Spanish dataset). The best results were Random Forest and SVC for Spanish, and Random Forest and MLP for English dataset.\nIn order to use the best classifier for the following tests, and include a model in further implementations, we select Random Forest. This selection corresponds to the capability to detect bots (true negative values in the confusion matrices),\n6https://scikit-learn.org/stable/\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter"
        },
        {
            "heading": "12 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nFIGURE 7: Confusion matrix for each algorithm in test (English)\nand the possibility of incorporate more explicability of our models [81]. Additionally, Random Forest only learns from a small subset of features and the random feature selection makes the trees more independent of one another than other classifiers, which frequently yields better predictive performance (due to better variance-bias trade-offs). This makes that random feature selection be the process faster than bagging [82]. The \"bagging\" methods, such as Bagging and Random Forest are designed to simplify models that overfit the training data."
        },
        {
            "heading": "2) External validation",
            "text": "In the interest of analyzing of the effectiveness of our classifiers based on Random Forest, we randomly selected 10 accounts, both in English and Spanish, and classified them as bot or human, using the Botometer program and our trained models. Table 10 and Table 11 show the results obtained. The Prediction columns correspond to the result obtained by the our models. Results of Botometer are colored darker as they are closer to 5 to indicate that the account is more likely to be an automated account (or bot); in contrast, the closer it is to 0, the more likely it is a genuine person. Botometer uses a number of characteristics associated to Twitter accounts to generate these results \u2013 e.g., volume of political tweets the accounts share (retweet), how many posts they make over time. Although we cannot be sure that the values closest to 5 in the Botometer column really correspond to a bot, we can see that there is similar values of our models with respect to the Botometer scores.\nTable 10 shows that, of the 4 values in the table close to 5 (@SpanishDict, @chile_soberano, @CNNChile, and @memewordES), the Random Forest model for Span-\nish only identified three of them as bot. Analyzing the @chile_soberano account more specifically, it is a Twitter account that is dedicated to posting and sharing tweets that have political content, mostly of them jokes or satire. This indicates the high probability that the account in question is a hybrid account being managed by a genuine person and at the same time with some degree of automation, hence the classification as \u201chuman\u201d made by our model. However, having politics as its main topic, Botometer may have considered it more likely as a bot that shares political tweets with a high volume. The rest of the results obtained for this test seems aligned with the results obtained with Botometer. Concerning the bot accounts, these have a bot behaviour. @CNNChile is a news account which usually have a hybrid account behavior, since they usually schedule their publications. @SpanishDict is an account for learning Spanish words. The structure of their tweets is basically the same for each post, being almost identical the hashtags added to each publication. @mememewordES is a memes account that shows humorous images with their Spanish translation in the text of the post. Botometer labeled it as a bot in its repository and with a spam behavior. The structure of the tweets is always the same, with content taken from other pages.\nTable 11 shows a comparison between the results obtained in Botometer and our Random Forest based model for English. Three accounts were considered as bots (@ABC7NY, @introvertsmemes, and @RickyCa09039186). @ABC7NY is a news account, usually tends to behave like a hybrid account, since they tend to schedule their posts. @introvertsmemes is a memes account that displays humorous images and texts related to the aspect of being introverted.\nVOLUME 4, 2016 13\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nWhile the structure of the posts is diverse, Botometer indicates that it has been found as a bot within their repositories, as well as possibly having been reported as a bot by user feedback. @RickyCa09039186 is an account that publishes text posts on predefined formatted topics. It is usually with political topics. Botometer indicates that it has been found in its repositories as a bot, spam behavior and possibly been reported as a bot by user feedback. In the case of the @tommyinit account, this is the account of a person who has a presence on the YouTube platform, his comments usually have images attached, with short texts that rarely have mentions, hashtags, or URLs, so that the model has considered his behavior as typical of a genuine user. The rest of the predictions made to the other accounts have a good fit with the probabilities calculated by Botometer.\nConsidering these tests, Random Forest based models for Spanish and for English, obtain good results when working with arbitrary Twitter data.\nFollowing sections describe how the bot detection can be used for credibility analysis.\nIV. A CREDIBILITY MODEL FOR TWITTER A credibility model to analyze social media, based in three levels of credibility (i.e., text, user, and social impact) was proposed in a previous work [27]. This original credibility model was instantiated in T-CREo, a credibility analysis framework for Twitter [28] and used to perform data source comparison [83], [84]. T-CREo works as a Google Chrome extension to perform an automatic credibility analysis using web scraping techniques and the Twitter API to achieve this analysis in real time. In this section, we describe the bases of the original credibility model.\nA. TEXT CREDIBILITY Text credibility analyzes syntactically the content of the post (without checking the author attributes), through SPAM, bad words, and misspelling filters, as shown in Definition 1.\nDefinition 1: Text Credibility (TextCred). Given the text of a post p, represented as p.text, Text Credibility is a function, denoted as TextCred(p.text), that returns a measure \u2208 [0, 100], defined as:\nTextCred(p.text) = wSPAM \u00d7 IsSPAM(p.text)+ wBadWords \u00d7BadWords(p.text)+ wMisspelledWords \u00d7Misspelling(p.text) where \u2022 IsSPAM(p.text) is a SPAM detector that determines\nthe probability \u2208 [0, 100] of p.text being spam; \u2022 BadWords(p.text) measures the bad words propor-\ntion \u2208 [0, 100] against the number of words in a text; \u2022 Misspelling(p.text) measures the misspelling errors\nproportion \u2208 [0, 100]; \u2022 wSPAM , wBadWords, and wMisspelledWords repre-\nsent user-defined parameters to indicate the weights that the user gives to each filter, such that wSPAM + wBadWords + wMisspelledWords = 1."
        },
        {
            "heading": "B. USER CREDIBILITY",
            "text": "User credibility analyzes only the user as a unit of the platform, without being influenced by other users, as it is described in Definition 2.\nDefinition 2: User Credibility (UserCred). Given a set of metadata of a user who published a post p, represented as p.user, User Credibility is a function, denoted as UserCred(p.user), that returns a measure \u2208 [0, 100], defined as:\nUserCred(p.user) = V erif(p.user)+Creation(p.user) where:\n\u2022 V erif(p.user) is a function that returns 50 if the user is verified and 0 otherwise; \u2022 Creation(p.user) measures the time since the user\u2019s account was created, with a value between 0 and 50, increasing with the longevity of the account, such as Creation(p.user) = AccountAge(p.user)MaxAccountAge(p.platform) \u00d7 50 where: - AccountAge(p.user) = currentY ear \u2212Y earJoined(p.user);\n- MaxAccountAge(p.platform) = currentY ear \u2212 SocialP latformCreationY ear(p.platform); - SocialP latformCreationY ear(p.platform) is the year in which the targeted social platform (p.platform) was created (e.g., 2006 for Twitter)."
        },
        {
            "heading": "C. SOCIAL CREDIBILITY",
            "text": "Social credibility is focused on the relations between a user account and the other accounts on the social media platform. It considers the number of followers and following (friends), as shown in Definition 3.\nDefinition 3: Social Credibility (SocialCred). Given a set of metadata of a user who published a post p, represented as p.user, Social Credibility is a function, denoted as SocialCred(p.user), that returns a measure \u2208 [0, 100], defined as:\nSocialCred(p.user) = FollowersImpact(p.user)+ FFProportion(p.user) where:\n\u2022 FollowersImpact(p.user) = min(p.user.followers,max_FOLLOWERS)\nMAX_FOLLOWERS \u00d7 50 measures the impact \u2208 [0, 50] on the number of followers;\n\u2022 FFProportion(p.usersocial) = p.user.followers\np.user.followers+p.user.following \u00d7 50 measures the proportion \u2208 [0, 50] between the number of followers and followings of the user. \u2022 MAX_FOLLOWERS is a user-defined parameter. The MAX_FOLLOWERS constant is supplied by the user, for example, in [27] it is considered as 2 million. FFProportion is self-explanatory\u2014a simple proportion that increases the credibility if the user has more followers than followings. The purpose of this function is to discredit"
        },
        {
            "heading": "14 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nbots, which tend to have more followings than followers. In this work, we reinforce this aspect at the User Credibility level, as we explain in Section V.\nD. GLOBAL CREDIBILITY LEVEL The credibility of a post is a weighted sum of the three credibility measures described previously. Definition 4 shows how it is calculated.\nDefinition 4: Credibility Level (Cred). Given a post, p, Credibility Level is a function, denoted as Cred(p), that returns a measure \u2208 [0, 100] of its level of credibility, defined as:\nCred(p) = wText \u00d7 TextCred(p.text) + wUser \u00d7 UserCred(p.user) + wSocial \u00d7 SocialCred(p.user),\nwhere:\n\u2022 wText, wUser, and wSocial represent the weights that the user gives to text credibility, user credibility, and social credibility, respectively, such that: wText+wUser+ wSocial = 1; \u2022 TextCred(p.text), UserCred(p.user), and SocialCred(p.user) represent the credibility measure related to the text, the user, and the social impact of p, respectively.\nFollowing section describes the use of bot detection into the credibility analysis.\nV. CREDIBOT: EXTENDED CREDIBILITY MODEL CONSIDERING BOT DETECTION Identifying bots on Twitter gives to user more tools to analyze their perceptions about the shared data (credibility). As more processed information related to the tweets can be provided to users, the sharing can be better performed and being more careful. Bot detection classifies the Twitter accounts, thus the User Credibility category is the one to be modified in order to consider bot information in the credibility model. Figure 8 shows the different levels of the credibility model described in Section IV and the bot detection functionality added to User Credibility.\nUsers mainly associate bots to fake information disseminators, which according to the literature is not always true. Due to the existence of non-malicious bots (white bots), some assumptions are made for not hard penalization when the model classifies a user as a bot. The assumptions are presented as follows:\n\u2022 If User Credibility value is higher than 50, then this credibility is penalized by 15%. \u2022 If User Credibility value is between 35 and 50, then this credibility is penalized by 25%. \u2022 If User Credibility value is less than 35, then this credibility is set to 0.\nWhen the User Credibility value is higher than 50 (a low penalization is applied), it means it is either verified or ancient account; thus, the user could be a white bot.\nWhite bots are used to share true information mainly from government institutions. Definition 5 shows the redefinition of the User Credibility (Definition 2).\nDefinition 5: User Credibility with bot detection (UserCredBot). Given a set of metadata of a user who published a post p, represented as p.user, User Credibility with bot detection is a function, denoted as UserCredBot(p.user), that returns a measure \u2208 [0, 100], defined as: UserCredBot(p.user) = UserCred(p.user) If p.user is not a bot, 0.85 \u00d7 UserCred(p.user) If p.user is a bot and UserCred(p.user) is higher than 50, 0.75 \u00d7 UserCred(p.user) If p.user is a bot and UserCred(p.user) is between 35 and 50,\n0 otherwise.\nwhere: UserCred(p.user) is the original value given in Definition 2. Text and Social Credibility levels remain the same.\nThe new credibility level is formally defined in Definition 6.\nDefinition 6: New Credibility Level (Cred\u2032). Given a post, p, New Credibility Level is a function, denoted as Cred\u2032(p), that returns a measure \u2208 [0, 100] of its level of credibility, defined as:\nCred\u2032(p) = wText \u00d7 TextCred(p.text) + wUser \u00d7 UserCredBot(p.user) + wSocial \u00d7 SocialCred(p.user)\nwhere: \u2022 wText, wUser, and wsocial represent the weights that the user gives to text credibility, user credibility, and social credibility, respectively, such as: wText + wUser + wSocial = 1; \u2022 TextCred(p.text), UserCredBot(p.user), and\nVOLUME 4, 2016 15\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nSocialCred(p.user) represent the credibility measure related to the text, the user, and the social impact of p, respectively. In the following section, the new credibility measure is\nexperimentally validated.\nA. CREDIBILITY MODEL EVALUATION To evaluate the extended credibility model, we implemented it with the Random Forest model, because of its better performance (see Section III) and integrated into T-CREo framework [28].\nFor the experiments, we designed two scenarios of tests. For the fist scenario, fourteen Twitter accounts were selected, including 10 human and 4 bot accounts for each language. The human accounts correspond to accounts that have more followers, according to Wikipedia7 and they are verified (7 verified accounts), and other accounts from genuine dataset (Genuine accounts - Table 3), which are not verified (3 not verified accounts). The bot accounts correspond to accounts known as bots from the experimentation dataset (spambot #1 - Table 3).\nFor the second scenario, 22 accounts were randomly selected from the fake followers dataset (Table 3), which were not used in the training or post-training validation process.\nThe results obtained from both scenarios are presented in the following sections.These results show the user credibility values for the original and extended model and their penalization in case that the user is qualified as a bot. Additionally, the \"Verified Weight\" and \"Creation Weight\" considered for both models, and the score obtained by Botometer for each account are also shown."
        },
        {
            "heading": "1) Results for Scenario 1",
            "text": "Table 12 shows the results for accounts that publish tweets in English. There are two accounts, @ladygaga and @TheEllenShow, that were detected as \u2019bot\u2019 despite being recognized as genuine accounts by Botometer. Doing an analysis of these accounts on Twitter, for @ladygaga, her posts tend to promote events or products that correspond to her career, such as makeup or music, in addition of using at least one hashtag, mention, or URL in each post. This behavior could be taken as a spam bot.\nTable 13 shows the accounts that publish tweets in Spanish. In this case, there are two \"bot\" accounts that do not lower their credibility, because their values for Verified Weight and Creation Weight were zero, respectively (as a consequence of the User Credibility definition \u2013 see Definition 2). Interestingly, in the case of the @RecuerdameBot account, a self-declared bot, was evaluated with a relatively high value by botometer and our model manages to classify it correctly. In this test, there are also two accounts that were detected as bot despite being labeled as genuine user (and verified as such also by Botometer). For @Rubiu5, this usually promotes events in which he participates using hashtags and some URLs, as well as promoting his own products, also\n7https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts\ncommenting or retweeting publications of official accounts of organizations that have to do with video games, which could perhaps be taken as a behavior of bot. On the other hand, although the @metrodesantiago account corresponds to an official transportation account, it presents the behavior of a hybrid account, posting at certain times to indicate the start and end of train trips, or warning with the exact time about a problem on the track or in a station."
        },
        {
            "heading": "2) Results for Scenario 2",
            "text": "For this test, the same number of accounts were selected for both Spanish and English and each account was analyzed to verify its validity (if it was active and maintained the same name), to ensure that the behavior remained the same that at the time of tagging. After verifying the validity of the 11 accounts for each language, the same analysis process was performed as in the first scenario regarding the calculation of the credibility of the original and the extended model. The results shown in Table 14 and Table 15 are consistent \u2013 i.e. considering the classification with Botometer, the credibility level according to our model, and the account label from the source.\nIn summary, the metrics of performance in the validation of bot detection included in the analysis of credibility present a precision=1.0, recall=0.8462, F1 score=0.9167, and accuracy=0.92, for both scenarios."
        },
        {
            "heading": "3) Discussion: Bot detection impact on the credibility level and execution time",
            "text": "Comparing the results between the original and extended models, the credibility values for genuine accounts remain the same, while the accounts that were classified as \u201cbot\u201d have lowered their credibility, according to the new user credibility definition. However, accounts detected as bots whose user credibility value is already zero (recently created and unverified account) keep the same credibility value in the extended model as in the original model. For example, accounts @eugeniojaque8 and @4geeksmxOfi have low credibility in the original model due to a low user credibility, and remain with the same credibility value in the extended model.\nIn this work, we assume that bots are potentially malicious and distribute false information. However, a more detailed evaluation about the type of bots is required for better credibility analysis, since some institutions use bots to disseminate periodic information about events, such as earthquakes, traffic, weather, among others. I our experiments, the highest original credibility value for bot accounts is 56.35 and 53 with the extended credibility model, which might represent an indication that the detection of bots on Twitter could be performed through the proposed credibility metrics. For instance, we could consider as a bot, accounts which have a credibility value less than 50.\nIn addition and due to the requirement of maintain a real time processing, the run times obtained by both credibility models are also shown in Table 16 and Table 17. According"
        },
        {
            "heading": "16 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nto these results, the extended credibility model increments, in average, the execution time (with respect to the original one) by 27.86 ms for scenario 1 (Table 16) and 30 ms for scenario 2 (Table 17). These averages correspond to the total sum of the time difference of the credibility calculation of the extended and original model, divided by the total number of accounts analyzed (in this case, 28 for scenario 1 and 22 for scenario 2). These results confirm that the extended credibility model computation times are maintained in real\ntime."
        },
        {
            "heading": "VI. CONCLUSIONS AND FUTURE WORK",
            "text": "In this work, we show the process carried out for the development of a real-time bot detection functionality, with the intention of extending an existing credibility model for Twitter. Thus, the extended credibility model is able to consider the existence of bots in social networks as an important factor when performing the credibility analysis process. Some machine learning algorithms, such as De-\nVOLUME 4, 2016 17\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\ncision Tree, Random Forest, Bagging, Adaboost, Logistic Regression, Support Vector Machine, K-Nearest Neighbor, and MultiLayer Perceptron are considered for evaluation and comparison. The selected classifier is Random Forest for both English and Spanish datasets with performance metrics of accuracies over 97%. This selection correspond to the capacity of the classifier for recognizing bots (true negative values), even more the possibility to provide a more interpretable result. Different tests show that the most important features for bot detection are favorites_count, statuses_count, followers_count, friends_count, and listed_count, which are social features. Validation tests of the bot detection included in the new credibility analysis model are performed, yielding a performance of precision 1.0, recall 0.8462, F1-score\n0.9167, and accuracy 0.92 for both English and Spanish model. To evaluate the performance of the extended model, it was incorporated into T-CREo, a framework for credibility analysis in Twitter in real time. In addition, the times are measured and it is found that the credibility analysis process is done in 1.26 seconds for the longest time.\nFuture work includes the expansion of the categories of recognized bots to include white bots and cyberbots. An interesting aspect to study is the behavior of bots within their network, such as expansion times, expansion patterns, activation times associated with topics, and to add these new features to the study. The exploration of hybrid methods with social network analysis in conjunction with machine learning algorithms remains to be investigated. Even though our main objective is to show the feasibility of extending our previous credibility model including bot detection, other machine learning methods such as deep learning rest to explore in the future. It could include by example, bigger datasets with different languages and additional features selected by"
        },
        {
            "heading": "18 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\ndifferent feature selection methods for bot detection. A next step of the project seeks improvements at the level of code optimization and making available the developed extension.\nREFERENCES [1] J. Clyde Mitchell. Social Networks on JSTOR. Annual Review of\nAnthropology, 3:279\u2013299, 1974. [2] Martina Draho\u0161ov\u00e1 and Peter Balco. The analysis of advantages and\ndisadvantages of use of social media in European Union. Procedia Computer Science, 109:1005\u20131009, 2017. [3] Bogdan Batrinca and Philip C. Treleaven. Social Media Analytics: A Survey of Techniques, Tools and Platforms. AI & Society, 30(1):89\u2013116, 2015. [4] Richard J. Oentaryo, Arinto Murdopo, Philips K. Prasetyo, and Ee-Peng Lim. On Profiling Bots in Social Media. In Social Informatics, pages 92\u2013109. Springer, 2016. [5] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, and Filippo Menczer. The spread of lowcredibility content by social bots. Nature Communications, 9(4787):1\u20139, 2018. [6] Stefano Cresci, Roberto Di-Pietro, Marianella Petrocchi, Angelo Spognardi, and Maurizio Tesconi. A fake follower story: improving fake accounts detection on twitter. Technical report, Consiglio Nazionale delle Ricerche, 2014. [Online; accessed 28. Feb. 2022]. [7] Samara Castillo, H\u00e9ctor Allende-Cid, Wenceslao Palma, Rodrigo Alfaro, Heitor S. Ramos, Cristian Gonzalez, Claudio Elortegui, and Pedro Santander. Detection of bots and cyborgs in twitter: A study on the chilean presidential election in 2017. In Social Computing and Social Media. Design, Human Behavior and Analytics, pages 311\u2013323. Springer, 2019. [8] Carter Yagemann, Simon P. Chung, Erkam Uzun, Sai Ragam, and Wenke Lee. On the feasibility of automating stock market manipulation. In Annual Computer Security Applications Conference, pages 277\u2013290, 2020. [9] Serena Tardelli, Marco Avvenuti, Maurizio Tesconi, and Stefano Cresci. Characterizing social bots spreading financial disinformation. In Social Computing and Social Media. Design, Ethics, User Behavior, and Social Network Analysis, pages 376\u2013392. Springer, 2020. [10] Srijan Kumar and Neil Shah. False information on web and social media: A survey. arXiv:1804.08559, 2018. [11] McKenzie Himelein-Wachowiak, Salvatore Giorgi, Amanda Devoto, Muhammad Rahman, Lyle Ungar, H. Andrew Schwartz, David H. Epstein, Lorenzo Leggio, and Brenda Curtis. Bots and misinformation spread on social media: Implications for covid-19. Journal of Mededical Internet Research, 23(5):e26933, 2021. [12] Wentao Xu and Kazutoshi Sasahara. Characterizing the roles of bots on twitter during the covid-19 infodemic. Journal of computational social science, 5(1):591\u2013609, 2022. [13] Mohammad Shafahi, Leon Kempers, and Hamideh Afsarmanesh. Phishing through social bots on twitter. In International Conference on Big Data, pages 3703\u20133712. IEEE, 2016. [14] Matthew C. Benigni, Kenneth Joseph, and Kathleen M. Carley. Bot-ivistm: Assessing information manipulation in social media using network analytics. In Emerging Research Challenges and Opportunities in Computational Social Network Analysis and Mining, pages 19\u201342. Springer, 2018. [15] Sourena Maroofi, Maciej Korczyn\u0301ski, and Andrzej Duda. Are you human? resilience of phishing detection to evasion techniques based on human verification. In Internet Measurement Conference, pages 78\u201386. ACM, 2020. [16] Richard J. Oentaryo, Arinto Murdopo, Philips K. Prasetyo, and Ee-Peng Lim. On profiling bots in social media. In Social Informatics, pages 92\u2013 109. Springer, 2016. [17] Zi Chu, Steven Gianvecchio, Haining Wang, and Sushil Jajodia. Detecting automation of twitter accounts: Are you a human, bot, or cyborg? IEEE Transactions on Dependable and Secure Computing, 9(6):811\u2013824, 2012. [18] Suman Kalyan Maity, Aishik Chakraborty, Pawan Goyal, and Animesh Mukherjee. Detection of Sockpuppets in Social Media. In Companion of the ACM Conference on Computer Supported Cooperative Work and Social Computing, pages 243\u2013246. 2017. [19] Seth A. Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. Information network or social network? the structure of the twitter follow graph. In the 23rd International Conference on World Wide Web, pages 493\u2013498. ACM, 2014.\n[20] Onur Varol, Emilio Ferrara, Clayton A. Davis, Filippo Menczer, and Alessandro Flammini. Online human-bot interactions: Detection, estimation, and characterization. In International AAAI Conference on Web and Social Media, volume 11, 2017. [21] Wajiha Shahid, Yiran Li, Dakota Staples, Gulshan Amin, Saqib Hakak, and Ali Ghorbani. Are You a Cyborg, Bot or Human?\u2014A Survey on Detecting Fake News Spreaders. IEEE Access, 10:27069\u201327083, 2022. [22] Eiman Alothali, Nazar Zaki, Elfadil A. Mohamed, and Hany Alashwal. Detecting social bots on twitter: A literature review. In International Conference on Innovations in Information Technology. IEEE, 2018. [23] V. S. Subrahmanian, Amos Azaria, Skylar Durst, Vadim Kagan, Aram Galstyan, Kristina Lerman, Linhong Zhu, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. The DARPA Twitter Bot Challenge. Computer, 49(6):38\u201346, 2016. [24] Suruchi Gera and Adwitiya Sinha. T-Bot: AI-based social media bot detection model for trend-centric twitter network. Social Network Analysis and Mining, 12(1):1\u201319, 2022. [25] Majed Alrubaian, Muhammad Al-Qurishi, Mabrook Al-Rakhami, and Atif Alamri. A Credibility Assessment Model for Online Social Network Content. In From Social Data Mining and Analysis to Prediction and Community Detection, pages 61\u201377. Springer, LNCS, 2017. [26] Fang Zhou, Jianlin Jin, Xiaojiang Du, Bowen Zhang, and Xucheng Yin. A calculation method for social network user credibility. In IEEE International Conference on Communications, pages 1\u20136. 2017. [27] Irvin Dongo, Yudith Cardinale, and Ana Aguilera. Credibility analysis for available information sources on the web: A review and a contribution. In 4th International Conference on System Reliability and Safety, pages 116\u2013125. IEEE, 2019. [28] Yudith Cardinale, Irvin Dongo, Germ\u00e1n Robayo, David Cabeza, Ana Aguilera, and Sergio Medina. T-CREo: A twitter credibility analysis framework. IEEE Access, 9:32498\u201332516, 2021. [29] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and Maurizio Tesconi. The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race. In 26th International Conference on World Wide Web Companion, pages 963\u2013972, 2017. [30] Mariam Orabi, Djedjiga Mouheb, Zaher Al Aghbari, and Ibrahim Kamel. Detection of bots in social media: A systematic review. Information Processing & Management, 57(4):102250, 2020. [31] Malak Aljabri, Rachid Zagrouba, Afrah Shaahid, Fatima Alnasser, Asalah Saleh, and Dorieh M Alomari. Machine learning-based social media bot detection: a comprehensive literature review. Social Network Analysis and Mining, 13(1):20, 2023. [32] Oliver Beatson, Rachel Gibson, Marta Cantijoch Cunill, and Mark Elliot. Automation on twitter: Measuring the effectiveness of approaches to bot detection. Social Science Computer Review, 41(1):181\u2013200, 2023. [33] Abdulrahman Alarifi, Mansour Alsaleh, and AbdulMalik Al-Salman. Twitter turing test: Identifying social machines. Information Sciences, 372:332\u2013346, 2016. [34] Sneha Kudugunta and Emilio Ferrara. Deep neural networks for bot detection. Information Sciences, 467(8), 2018. [35] Sylvio Barbon Jr, Gabriel F. C. Campos, Gabriel M. Tavares, Rodrigo A. Igawa, Mario L. Proen\u00e7a Jr, and Rodrigo Capobianco Guido. Detection of human, legitimate bot, and malicious bot in online social networks based on wavelets. ACM Transactions on Multimedia Computing, Communications, and Applications, 14(1s):1\u201317, 2018. [36] Kadhim Hayawi, Sujith Mathew, Neethu Venugopal, Mohammad Masud, and Pin-Han Ho. Deeprobot: a hybrid deep neural network model for social bot detection based on user profile data. Social Network Analysis and Mining, 12(1):43, 2022. [37] Rachit Shukla, Adwitiya Sinha, and Ankit Chaudhary. Tweezbot: an AI-driven online media bot identification algorithm for twitter social networks. Electronics, 11(5):743, 2022. [38] Pawan Kumar Verma, Prateek Agrawal, Vishu Madaan, and Charu Gupta. UCred: fusion of machine learning and deep learning methods for user credibility on social media. Social Network Analysis and Mining, 12(1):54, 2022. [39] Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. Debot: Twitter bot detection via warped correlation. In IEEE 16th International Conference on Data Mining, volume 18, pages 28\u201365, 2016. [40] Tweetbotornot: R package for detecting twitter bots via machine learning, 2018. [Software package] https://github. com/mkearney/Tweetbotornot. [41] Christopher Bouzy. Bot sentinel, 2018. [Software package] https://www.botsentinel.com.\nVOLUME 4, 2016 19\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\n[42] Kai-Cheng Yang, Emilio Ferrara, and Filippo Menczer. Botometer 101: social bot practicum for computational social scientists. Journal of Computational Social Science, 5(2):1511\u20131528, 2022. [43] K. Lorek, J. Suehiro-Wicin\u0301ski, M. Jankowski-Lorek, and A. Gupta. Automated credibility assessment on twitter. Computer Science, 16(2):157\u2014 168, 2015. [44] Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. Information credibility on twitter. In 20th International Conference on World Wide Web, pages 675\u2013684. ACM, 2011. [45] Hend S. Al-Khalifa and Rasha M. Al-Eidan. An experimental system for measuring the credibility of news content in twitter. International Journal of Web Information Systems, 7(2):130\u2013151, 2011. [46] Yoshimi Namihira, Naomichi Segawa, Yukino Ikegami, Kenta Kawai, Takashi Kawabe, and Setsuo Tsuruta. High precision credibility analysis of information on twitter. In International Conference on Signal-Image Technology & Internet-Based Systems, pages 909\u2013915, 2013. [47] Majed AlRubaian, Muhammad Al-Qurishi, Mabrook Al-Rakhami, Mohammad Mehedi Hassan, and Atif Alamri. Credfinder: A real-time tweets credibility assessing system. In International Conference on Advances in Social Networks Analysis and Mining, pages 1406\u20131409, 2016. [48] Tarek Hamdi, Hamda Slimi, Ibrahim Bounhas, and Yahya Slimani. A hybrid approach for fake news detection in twitter based on user features and graph embedding. In International Conference on Distributed Computing and Internet Technology, pages 266\u2013280, 2020. [49] Adrian Iftene, Daniela G\u00eefu, Andrei-Remus Miron, and Mihai-Stefan Dudu. A real-time system for credibility on twitter. In 12th Language Resources and Evaluation Conference, pages 6166\u20136173, 2020. [50] Maria Hernandez-Mendoza, Ana Aguilera, Irvin Dongo, Jose CornejoLupa, and Yudith Cardinale. Credibility analysis on twitter considering topic detection. Applied Sciences, 12(18), 2022. [51] Nanda Ihwani Saputri, Yuliant Sibaroni, Sri Suryani Prasetiyowati, et al. Covid-19 fake news detection on twitter based on author credibility using information gain and KNN MethodsCovid-19 fake news detection on twitter based on author credibility using information gain and KNN methods. Jurnal Rekayasa Sistem dan Teknologi Informasi, 7(1):185\u2013192, 2023. [52] David M. Beskow and Kathleen M. Carley. Its all in a name: Detecting and labeling bots by their name, 2018. [Online; accessed 2. Dec. 2022]. [53] Rosario Gilmary, Akila Venkatesan, and Govindasamy Vaiyapuri. Detection of automated behavior on twitter through approximate entropy and sample entropy. Personal and Ubiquitous Computing, 27(1):91\u2013105, 2021. [54] Richard J. Oentaryo, Arinto Murdopo, Philips K. Prasetyo, and Ee-Peng Lim. On profiling bots in social media. In 8th International Conference on Social Informatics, pages 92\u2013109, 2016. [55] Buket Ers\u0327ahin, \u00d6zlem Aktas\u0327, Deniz K\u0131l\u0131n\u00e7, and Ceyhun Akyol. Twitter fake account detection. In International Conference on Computer Science and Engineering (UBMK), pages 388\u2013392, 2017. [56] M\u00fccahit Kantepe and Murat Can Ganiz. Preprocessing framework for twitter bot detection. In International Conference on Computer Science and Engineering (UBMK), pages 630\u2013634, 2017. [57] Chiyu Cai, Linjing Li, and Daniel Zengi. Behavior enhanced deep bot detection in social media. In IEEE International Conference on Intelligence and Security Informatics, pages 128\u2013130, 2017. [58] Fred Morstatter, Liang Wu, Tahora H. Nazer, Kathleen M. Carley, and Huan Liu. A new approach to bot detection: Striking the balance between precision and recall. In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pages 533\u2013540, 2016. [59] Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review. In Data Classification: Algorithms and Applications, pages 37\u201364. CRC Press, 2014. [60] Bernhard Sch\u00f6lkopf, John Platt, and Thomas Hofmann. Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation. In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, pages 209\u2013216. MIT Press, Cambridge, MA, USA, 2007. [61] Dimitrios Effrosynidis and Avi Arampatzis. An evaluation of feature selection methods for environmental data. Ecol. Inf., 61:101224, mar 2021. [62] Jaime Lynn Speiser, Michael E. Miller, Janet Tooze, and Edward Ip. A Comparison of Random Forest Variable Selection Methods for Classification Prediction Modeling. Expert Syst. Appl., 134:93\u2013101, nov 2019. [63] Marco Sandri and Paola Zuccolotto. Variable Selection Using Random Forests. In Data Analysis, Classification and the Forward Search, pages 263\u2013270. Springer, Berlin, Germany, 2006.\n[64] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In 31st International Conference on Neural Information Processing Systems, pages 4768\u20134777. Curran Associates Inc., 2017. [65] Wilson E. Marc\u00edlio and Danilo M. Eler. From explanations to feature selection: assessing SHAP values as feature selection mechanism. In 33rd SIBGRAPI Conference on Graphics, Patterns and Images, pages 340\u2013347. IEEE, 2020. [66] Xin Man and Ernest Chan. The Best Way to Select Features? Comparing MDA, LIME, and SHAP. The Journal of Financial Data Science Winter, 3(1):127\u2013139, nov 2021. [67] Shilpa Bhandari, Avinash K. Kukreja, Alina Lazar, Alex Sim, and Kesheng Wu. Feature Selection Improves Tree-based Classification for Wireless Intrusion Detection. In SNTA \u201920: Proceedings of the 3rd International Workshop on Systems and Network Telemetry and Analytics, pages 19\u2013 26. Association for Computing Machinery, New York, NY, USA, June 2020. [68] Leo Breiman. Random Forests. Machine Learning, 45(1):5\u201332, 2001. [69] Ke Zhang, Peidong Xu, and Jun Zhang. Explainable AI in deep re-\ninforcement learning models: A shap method applied in power system emergency control. In 4th Conference on Energy Internet and Energy System Integration, pages 711\u2013716. IEEE, 2020. [70] James Schnebly and Shamik Sengupta. Random forest twitter bot classifier. In 9th Annual Computing and Communication Workshop and Conference, pages 0506\u20130512. IEEE, 2019. [71] Nirdhum Narayan. Twitter bot detection using machine learning algorithms. In Fourth International Conference on Electrical, Computer and Communication Technologies, pages 1\u20134. IEEE, 2021. [72] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997. [73] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class AdaBoost. Statistics and Its Interface, 2(3):349\u2013360, 2009. [74] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123\u2013140, 1996. [75] Dan H. Moore. Classification and regression trees. Cytometry, 8(5):534\u2013 535, 1987. [76] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [77] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21\u201327, 1967. [78] Corinna Cortes and Vladimir Vapnik. Support-Vector Networks. Machine Learning, 20(3):273\u2013297, 1995. [79] Frank Rosenblatt. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Spartan Books, 1962. [80] Jesse Davis and Mark Goadrich. The relationship between PrecisionRecall and ROC curves. In 23rd international conference on Machine learning, pages 233\u2013240. Association for Computing Machinery, 2006. [81] Christian Herzog. On the risk of confusing interpretability with explicability. AI Ethics, 2(1):219\u2013225, 2022. [82] Sebastian Raschka, Yuxi (Hayden) Liu, Vahid Mirjalili, and Dmytro Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing, 2022. [83] Irvin Dongo, Yudith Cadinale, Ana Aguilera, Fabiola Mart\u00ednez, Yuni Quintero, and Sergio Barrios. Web scraping versus twitter API: a comparison for a credibility analysis. In 22nd International conference on information integration and web-based applications & services, pages 263\u2013273, 2020. [84] Irvin Dongo, Yudith Cardinale, Ana Aguilera, Fabiola Martinez, Yuni Quintero, German Robayo, and David Cabeza. A qualitative and quantitative comparison between web scraping and API methods for twitter credibility analysis. International Journal of Web Information Systems, 17(6):580\u2013606, 2021."
        },
        {
            "heading": "20 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Aguilera, P. Quinteros, I. Dongo, Y. Cardinale: CrediBot: Applying Bot Detection for Credibility Analysis on Twitter\nANA AGUILERA received the B.S. in Computer Science Engineer from Lisandro Alvarado WestCentral University (UCLA) of Barquisimeto, Venezuela, in 1994, M.S. degree in Computer science from the Simon Bolivar University of Caracas, Venezuela, in 1998 and the Ph.D. degree in medical informatics from University of Rennes I, Rennes, France, in 2008. She is currently a Full Professor with University of Valpara\u00edso, Faculty of Engineering, Escuela de Ingenier\u00eda Inform\u00e1tica,\nValpara\u00edso, Chile. Her research interest includes the fuzzy databases, data mining, social networks and medical informatics. She obtained the BSc. honors degree in Computer Engineering, Magna Cum Laude Award from UCLA and \u201cTr\u00e8s honorable\u201d Award in the PhD thesis from Rennes I. She was accredited in Program for Researcher Promotion of Venezuela, Candidate Level (1998). Since 2011, she is member of the Program Encouragement for Research and Innovation Researcher (PEII) Level C, Venezuela. She is member of the Venezuelan Association for the Advancement of Science (AsoVAC) and Member of Venezuelan Computer Society (SVC).\nPAMELA QUINTEROS B.Sc. degree student in Informatics Engineering from Universidad de Valpara\u00edso. She is currently finishing her undergraduate dissertation. She has collaborated as a student, in the project RUTAS since 2021. Her research interests include machine learning, neural networks, and deep learning for data classification.\nIRVIN DONGO received his B.Sc. degree in Computer Science from the Catholic San Pablo University, Peru (2012); and his M.Sc. and Ph.D. degrees from the University of Pau, France (2014 and 2017, respectively). He was a postdoctoral fellow at \u00c9cole Sup\u00e9rieure des Technologies Industrielles Avanc\u00e9es (ESTIA, 2018-2020), France. He is currently associate researcher in Computer Science at \u00c9cole Sup\u00e9rieure des Technologies Industrielles Avanc\u00e9es and full-time professor at\nCatholic San Pablo University. His research interests lie in normalization and anonymization of Web resources, knowledge-bases modeling (Semantic Web); policies and management of credentials, security model and anonymization technique; and machine/deep learning techniques for an analysis and classification of data to discover patterns, gesture recognition and affective computing.\nYUDITH CARDINALE is a Full Professor and Researcher at Universidad Internacional de Valencia, Spain since 2018 and the Principal Researcher of the Research Group in Data Science (GRID). She is also Full Professor in Computer Science Department at Universidad Sim\u00f3n Bol\u00edvar (USB), Venezuela since 1996. She is the President of the Venezuelan Computer Society. Currently, she is an associate researcher at Universidad Cat\u00f3lica San Pablo, Arequipa, Per\u00fa. She graduated with\nhonors in Computer Engineering in 1990 at Universidad Centro-Occidental Lisandro Alvarado, Venezuela. She received her MSc Degree and PhD in Computer Science from USB, Venezuela, in 1993 and 2004 respectively. Her research interests include parallel processing, distributed object processing, operating systems, digital ecosystems, high performance on grid and cloud platforms, collaborative frameworks, and web services composition, including semantic web. She has written a range of scientific articles published in international journal, books, and conferences, and has participated as member of program committees of several international conferences and journals. She has been the coordinator of a variety of international research projects.\nVOLUME 4, 2016 21\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "CrediBot: Applying Bot Detection for Credibility Analysis on Twitter",
    "year": 2023
}