{
    "abstractText": "Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user\u2019s identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-thewild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to learn human 3D pose parameters and overcome the limitations of motion capture datasets. *Denotes equal contribution. 1 ar X iv :2 30 4. 07 41 0v 1 [ cs .C V ] 1 4 A pr 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Samaneh Azadi"
        },
        {
            "affiliations": [],
            "name": "Guan Pang"
        },
        {
            "affiliations": [],
            "name": "Thomas Hayes"
        },
        {
            "affiliations": [],
            "name": "Devi Parikh"
        },
        {
            "affiliations": [],
            "name": "Akbar Shah"
        },
        {
            "affiliations": [],
            "name": "Sonal Gupta"
        }
    ],
    "id": "SP:bede4ffbd7118d02503aa9e7d0c4f36770332f15",
    "references": [
        {
            "authors": [
                "Kfir Aberman",
                "Mingyi Shi",
                "Jing Liao",
                "Dani Lischinski",
                "Baoquan Chen",
                "Daniel Cohen-Or"
            ],
            "title": "Deep video-based performance cloning",
            "venue": "Computer Graphics Forum,",
            "year": 2019
        },
        {
            "authors": [
                "Chaitanya Ahuja",
                "Louis-Philippe Morency"
            ],
            "title": "Language2pose: Natural language grounded pose forecasting",
            "venue": "IEEE International Conference on 3D Vision",
            "year": 2019
        },
        {
            "authors": [
                "Samaneh Azadi",
                "Deepak Pathak",
                "Sayna Ebrahimi",
                "Trevor Darrell"
            ],
            "title": "Compositional gan: Learning imageconditional binary composition",
            "venue": "International Journal of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Rania Briq",
                "Pratika Kochar",
                "Juergen Gall"
            ],
            "title": "Towards better adversarial synthesis of human images from text",
            "venue": "arXiv preprint arXiv:2107.01869,",
            "year": 2021
        },
        {
            "authors": [
                "Pablo Cervantes",
                "Yusuke Sekikawa",
                "Ikuro Sato",
                "Koichi Shinoda"
            ],
            "title": "Implicit neural representations for variable length human motion generation",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Caroline Chan",
                "Shiry Ginosar",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Everybody dance now",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Make-a-scene: Scenebased text-to-image generation with human priors",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Anindita Ghosh",
                "Noshaba Cheema",
                "Cennet Oguz",
                "Christian Theobalt",
                "Philipp Slusallek"
            ],
            "title": "Synthesis of compositional animations from textual descriptions",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Artur Grigorev",
                "Artem Sevastopolsky",
                "Alexander Vakhitov",
                "Victor Lempitsky"
            ],
            "title": "Coordinate-based texture inpainting for pose-guided human image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Guo",
                "Shihao Zou",
                "Xinxin Zuo",
                "Sen Wang",
                "Wei Ji",
                "Xingyu Li",
                "Li Cheng"
            ],
            "title": "Generating diverse and natural 3d human motions from text",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Shihao Zou",
                "Xinxin Zuo",
                "Sen Wang",
                "Wei Ji",
                "Xingyu Li",
                "Li Cheng"
            ],
            "title": "Generating diverse and natural 3d human motions from text",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Shihao Zou",
                "Qingyao Sun",
                "Annan Deng",
                "Minglun Gong",
                "Li Cheng"
            ],
            "title": "Action2motion: Conditioned generation of 3d human motions",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-prompt image editing with cross attention control",
            "venue": "arXiv preprint arXiv:2208.01626,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Avatarclip: Zero-shot textdriven generation and animation of 3d avatars",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Kappel",
                "Vladislav Golyanik",
                "Mohamed Elgharib",
                "Jann-Ole Henningson",
                "Hans-Peter Seidel",
                "Susana Castillo",
                "Christian Theobalt",
                "Marcus Magnor"
            ],
            "title": "High-fidelity neural human motion transfer from monocular video",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Shiran Zada",
                "Oran Lang",
                "Omer Tov",
                "Huiwen Chang",
                "Tali Dekel",
                "Inbar Mosseri",
                "Michal Irani"
            ],
            "title": "Imagic: Text-based real image editing with diffusion models",
            "venue": "arXiv preprint arXiv:2210.09276,",
            "year": 2022
        },
        {
            "authors": [
                "Hsin-Ying Lee",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Ting-Chun Wang",
                "Yu-Ding Lu",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Dancing to music",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Ruilong Li",
                "Shan Yang",
                "David A. Ross",
                "Angjoo Kanazawa"
            ],
            "title": "Ai choreographer: Music conditioned 3d dance generation with aist++",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Lingjie Liu",
                "Weipeng Xu",
                "Marc Habermann",
                "Michael Zollh\u00f6fer",
                "Florian Bernard",
                "Hyeongwoo Kim",
                "Wenping Wang",
                "Christian Theobalt"
            ],
            "title": "Neural human video rendering by learning dynamic textures and rendering-to-video translation",
            "venue": "arXiv preprint arXiv:2001.04947,",
            "year": 2020
        },
        {
            "authors": [
                "Xihui Liu",
                "Dong Huk Park",
                "Samaneh Azadi",
                "Gong Zhang",
                "Arman Chopikyan",
                "Yuxiao Hu",
                "Humphrey Shi",
                "Anna Rohrbach",
                "Trevor Darrell"
            ],
            "title": "More control for free! image synthesis with semantic diffusion guidance",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Liqian Ma",
                "Xu Jia",
                "Qianru Sun",
                "Bernt Schiele",
                "Tinne Tuytelaars",
                "Luc Van Gool"
            ],
            "title": "Pose guided person image generation",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Liqian Ma",
                "Qianru Sun",
                "Stamatios Georgoulis",
                "Luc Van Gool",
                "Bernt Schiele",
                "Mario Fritz"
            ],
            "title": "Disentangled person image generation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Julieta Martinez",
                "Michael J. Black",
                "Javier Romero"
            ],
            "title": "On human motion prediction using recurrent neural networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Moustafa Meshry",
                "Dan B Goldman",
                "Sameh Khamis",
                "Hugues Hoppe",
                "Rohit Pandey",
                "Noah Snavely",
                "Ricardo Martin- Brualla"
            ],
            "title": "Neural rerendering in the wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "ArXiv, abs/1411.1784,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Timo Bolkart",
                "Ahmed A.A. Osman",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Expressive body capture: 3d hands, face, and body from a single image",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J. Black",
                "G\u00fcl Varol"
            ],
            "title": "Actionconditioned 3D human motion synthesis with transformer VAE",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J. Black",
                "G\u00fcl Varol"
            ],
            "title": "Temos: Generating diverse human motions from textual descriptions",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "ArXiv, abs/2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee"
            ],
            "title": "Generative adversarial text to image synthesis",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Prasun Roy",
                "Subhankar Ghosh",
                "Saumik Bhattacharya",
                "Umapada Pal",
                "Michael Blumenstein"
            ],
            "title": "Tips: Text-induced pose synthesis",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Alejandro Hernandez Ruiz",
                "Juergen Gall",
                "Francesc Moreno-Noguer"
            ],
            "title": "Human motion prediction via spatiotemporal inpainting",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S. Sara Mahdavi",
                "Rapha Gontijo Lopes",
                "Tim Salimans",
                "Jonathan Ho",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Style and pose control for image synthesis of humans from a single monocular view",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Kripasindhu Sarkar",
                "Vladislav Golyanik",
                "Lingjie Liu",
                "Christian Theobalt"
            ],
            "title": "Style and pose control for image synthesis of humans from a single monocular view",
            "venue": "arXiv preprint arXiv:2102.11263,",
            "year": 2021
        },
        {
            "authors": [
                "Axel Sauer",
                "Tero Karras",
                "Samuli Laine",
                "Andreas Geiger",
                "Timo Aila"
            ],
            "title": "Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis",
            "year": 2023
        },
        {
            "authors": [
                "Axel Sauer",
                "Katja Schwarz",
                "Andreas Geiger"
            ],
            "title": "Styleganxl: Scaling stylegan to large diverse datasets",
            "venue": "In ACM SIG- GRAPH 2022 Conference Proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Guy Tevet",
                "Brian Gordon",
                "Amir Hertz",
                "Amit H. Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Motionclip: Exposing human motion generation to clip space",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yonatan Shafir",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Human motion diffusion model",
            "venue": "arXiv preprint arXiv:2209.14916,",
            "year": 2022
        },
        {
            "authors": [
                "Sijie Yan",
                "Zhizhong Li",
                "Yuanjun Xiong",
                "Huahan Yan",
                "Dahua Lin"
            ],
            "title": "Convolutional sequence generation for skeletonbased action synthesis",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan",
                "Ben Hutchinson",
                "Wei Han",
                "Zarana Parekh",
                "Xin Li",
                "Han Zhang",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Human motion diffusion model",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Hongwen Zhang",
                "Yating Tian",
                "Yuxiang Zhang",
                "Mengcheng Li",
                "Liang An",
                "Zhenan Sun",
                "Yebin Liu"
            ],
            "title": "Pymaf-x: Towards well-aligned full-body model regression from monocular images",
            "venue": "arXiv preprint arXiv:2207.06400,",
            "year": 2022
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Zhongang Cai",
                "Liang Pan",
                "Fangzhou Hong",
                "Xinying Guo",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Motiondiffuse: Text-driven human motion generation with diffusion model",
            "venue": "arXiv preprint arXiv:2208.15001,",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Zhang",
                "Rania Briq",
                "Julian Tanke",
                "Juergen Gall"
            ],
            "title": "Adversarial synthesis of human pose from text",
            "venue": "42nd DAGM German Conference of Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Zhao",
                "Hui Su",
                "Qiang Ji"
            ],
            "title": "Bayesian adversarial human motion synthesis",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Xingran Zhou",
                "Siyu Huang",
                "Bin Li",
                "Yingming Li",
                "Jiachen Li",
                "Zhongfei Zhang"
            ],
            "title": "Text guided person image synthesis",
            "venue": "Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Yi Zhou",
                "Connelly Barnes",
                "Jingwan Lu",
                "Jimei Yang",
                "Hao Li"
            ],
            "title": "On the continuity of rotation representations in neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Zhu",
                "Tengteng Huang",
                "Baoguang Shi",
                "Miao Yu",
                "Bofei Wang",
                "Xiang Bai"
            ],
            "title": "Progressive pose attention transfer for 12 person image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user\u2019s identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scal-\nable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-thewild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to learn human 3D pose parameters and overcome the limitations of motion capture datasets.\n*Denotes equal contribution."
        },
        {
            "heading": "1. Introduction",
            "text": "Millions of people are connected with their family and friends through social media and chats where selfexpression is not only possible through language but more efficiently through visual features such as stickers and avatars. Avatars are a befitting method for personalization as they enable users to inject their identity into an expressive virtual self, while mitigating deepfake and privacy concerns. With current technology, however, one\u2019s expression is limited to a set of predefined stickers and avatars created by designers. Imagine you have the ability to generate images of your avatar in any context you desire \u2014 you can travel to your favorite city in the world, walk on your favorite beach, or even float in outer space.\nRecent large-scale text-to-image generation models [35] have made significant improvements in the quality, realism, and diversity of synthesized images and enable users to control content generation through language. However, the personalization aspect of these generative models is still challenging and under-explored. Dreambooth [41] enabled personalization of text-to-image generation models by finetuning on a few instances of a subject. While it can produce delightful results, it is not scalable to millions of users since fine-tuning is required for each new subject. Furthermore, it can sometimes generate images unfaithful to the subject\u2019s identity. Other image personalization methods suffer from similar limitations: poor faithfulness to user\u2019s identity, not zero-shot, and/or lacking open-world text understanding.\nIn this work, we propose a method to overcome these limitations, Personalized Avatar Scene (PAS), which enables zero-shot personalization of text-to-image generation with avatars (sample images in Fig. 1, Fig. 6). Our method is agnostic to the avatar style, requires no fine-tuning on the user\u2019s avatar (or even training on avatars at all), and remains faithful to the avatar appearance, making it truly scalable to millions of users.\nOur pipeline involves a three step approach: (1) generate human body pose from text, (2) render a user\u2019s avatar in the generated pose, (3) condition an image generation model on the input text and the rendered avatar to generate an image of the avatar contextualized in a scene. Re-posing and rendering a user\u2019s avatar enables us to maintain strict faithfulness to their appearance. Zero-shot capabilities come from the fact that our image generation model learns to faithfully place the rendered avatar in to scene, regardless of its style or degree of photo-realism.\nWe represent the avatar body pose via 3D SMPL parameters and learn text-to-3D pose generation with a Transformer-based diffusion model. We train our pose generation model on a large-scale dataset of text-pose pairs constructed by extracting 3D pseudo-pose SMPL annotations from image-text datasets filtered for images containing humans. This large-scale Image Text Pseudo-Pose (ITPP)\ndataset helps to overcome the limited diversity of existing motion capture datasets, in terms of both pose and text diversity, and demonstrates dataset preparation required for human motion generation can be significantly simplified. Our 3D human pose generation model reaches state-of-theart performance.\nBy representing avatar body poses via 3D SMPL parameters, our method is easily adaptable to a number of rendering engines to personalize with the user\u2019s avatar appearance.\nAfter rendering the avatar in a pose that aligns with the text prompt, we contextualize the avatar in a scene by leveraging large-scale text-to-image generation models to outpaint from the avatar. We use a pre-trained a text-to-image model that has been trained on the large-scale image-text datasets and fine-tune it on the corresponding human masks and pseudo-pose annotations to generate images more faithful to the avatar appearance. To summarize, our main contributions are:\n\u2022 We present Personalized Avatar Scene (PAS) \u2013 a scalable method for zero-shot personalization of image generation with a user\u2019s avatar.\n\u2022 We show, for the first time, how to leverage large-scale image datasets to learn in-the-wild human poses. Our text-to-3D pose model significantly improves on prior state-of-the-art models.\n\u2022 We present a method for zero-shot personalized image generation which improves faithfulness to the user\u2019s identity relative to baselines by injecting human body priors."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Human pose and motion synthesis",
            "text": "Prior works in human pose and motion synthesis have explored generative models either unconditionally [49, 54], or conditioned on various input signals such as a prior motion [27, 40], an action class [5, 14, 33], or music [21, 22]. Using text descriptions as a guidance in pose and motion synthesis has been a more recent research direction where many existing works use 2D keypoints as pose representations. [55] select a base pose from 8 clusters based on an input text fed to a GAN model to generate a human image. [53] use GAN to generate a set of heatmaps for body keypoints conditioned on the text input. [39] propose a coarse-to-fine approach where a refinement stage is introduced on top of the initial coarse estimate of the keypoint heatmaps. Different from the above methods, [4] use SMPL to represent a 3D body pose, generated by an LSTM GAN from a text input. Here, we similarly choose the 3D SMPL pose representation to facilitate more expressive body movements, utilize any existing 3D rendering engines, and enable a high-quality zero-shot personalization.\nText-guided motion generation can be regarded as an extension to pose generation, where motion is a sequence of poses. Many existing work approach this problem by learning to align text and pose or motion embeddings in the feature space. JL2P [2] proposes to learn the joint embedding of text and pose using an autoencoder with curriculum learning. [9] propose a two-stream model to encode upper and lower body motions separately. AvatarCLIP [18] uses a pre-trained VPoser model to generate candidate poses, which are then used to optimize a motion VAE. MotionCLIP [46] trains an auto-encoder while simultaneously reconstructing motion and aligning the motion manifold with CLIP\u2019s latent space. TEMOS [34] trains a joint latent space through separate text and motion transformer encoders, allowing non-deterministic motion sampling. T2M [12] also uses a VAE model, but encodes motion as snippets and introduces an extra sampling for motion length conditioned on the input text.\nLeveraging recent advancements in diffusion models, Motion Diffusion Model (MDM) [47] and MotionDiffuse [52] both adopt a diffusion process from text to motion with a transformer, which can significantly improve the diversity of synthesized samples. Both these methods are trained on HumanML3D [12] which partially limits their choice of representation to stick figures, while our curated large-scale dataset provides millions of SMPL pose labels. We concurrently developed our text-to-3D pose diffusion model with a more efficient choice of pose representations as discussed in sec 3.1, whereas MDM supports converting poses from stick figures to SMPL bodies only through an optimization SIMPLIFY procedure that often results in unrealistic body poses."
        },
        {
            "heading": "2.2. Text-Conditional Image Synthesis",
            "text": "Text-to-image generation studies the task of synthesizing images from natural language descriptions. Early works [38] used conditional Generative Adversarial Networks [10, 29] in constrained domains (e.g., flowers and birds). GANs have been scaled up [44, 45] using a progressive growing training strategy, which has led to success in more diverse domains.\nIn parallel with the works improving GAN-based methods, the field has found success by treating text-to-image generation as a sequence modeling problem. DALL-E [37] employed a two stage approach, whereby in the first stage a VQ-VAE is learned to compress images into discrete tokens, and in the second stage a Transformer decoder is trained to model text-to-image generation as a sequence-tosequence problem. Make-A-Scene [8] improved on this approach by implicitly modeling intermediate semantic maps and by adding human priors to improve the tokenization reconstruction quality. Recently, Parti [50] employed a Transformer-based image tokenizer, ViT-VQGAN, to fur-\nther improve reconstruction quality, and scaled an encoderdecoder Transformer up to 20B parameters.\nLately, the field has seen tremendous progress in terms of sample fidelity and alignment with text using Denoising Diffusion Probablistic Models (DDPMs) [16], a class of latent variable models which have shown superior image generation quality compared to GANs [7]. Several followup works [31, 36, 42] leverage diffusion models in the pixel space for text-to-image generation and achieve high quality results. These approaches opt for a cascade of diffusion models which progressively upsample the spatial resolution. Our model is fine-tuned based on a pre-trained diffusion model operating in an auto-encoder latent space to generate an image conditioned on a full body avatar in a given pose."
        },
        {
            "heading": "2.3. Personalization of Image Generation",
            "text": "Personalization of generative models has been a challenging problem. One way to approach personalization of image generation is to consider it as novel synthesis of a given subject. CompositionalGAN [3] proposed a model to compose two given objects in a novel and realistic way and was focused on the relation between two specific domains and objects while keeping their identity unchanged. Dreambooth [41] proposes a few-shot model that generates novel samples of a subject controlled by a text prompt. However, the primary limitation of this approach is that this model must be fine-tuned on each subject and requires 3-5 instances per subject. Fine-tuning large-scale text-to-image models is rather inaccessible for much of the community, is sensitive to hyper-parameter tuning, and is not scalable to millions of users.\nImage editing has been another approach towards personalization through local or global edits to an image or an object [15, 30]. Imagic [20] enables the ability to apply complex text-guided semantic edits to a real image such as editing the posture and composition of one or multiple objects inside an image, while preserving their original characteristics. Semantic diffusion guidance proposed in [24] enables generating variations of an input image while preserving its content.\nPersonalization has been also studied in novel view and garment synthesis of humans while preserving their identity [43] with applications in human appearance transfer, virtual try-on, and motion imitation. Some works on neural rendering [19, 23, 28] generate a 2D/3D skeleton or a heatmap as well as a coarse neural texture and then use image-to-image translation techniques to synthesize the appearance in higher resolution. Other works study human pose transfer as an image-to-image mapping problem given a reference image of a target person. In these methods body pose is represented as a rendering of a skeleton [6,57], dense mesh [11, 19], or joint position heatmaps [1, 25, 26]. How-\never, most of these learning based approaches have limits in the visual quality and add artifacts which can change the identity of the person.\nIn this work, we introduce personalization of image generation via avatars and enable their interaction with different objects and scenes controlled by a text prompt. We represent body pose through 3D SMPL parameters that make them compatible with any rendering service to generate an avatar\u2019s appearance in high quality and preserve its identity. Our model can serve all people regardless of avatar texture, style, or appearance\u2014it is zero-shot. No model retraining is required when avatar style or appearance changes."
        },
        {
            "heading": "3. Personalized Avatar Scene (PAS)",
            "text": "We represent an avatar body pose via 3D SMPL parameters, learn to predict them from text via a Transformer-based diffusion model, and then retarget to the avatar body to be rendered by our rendering tool. We train our pose generation model on a large-scale dataset of human poses, as described in section 3.2. Our image generation model is conditioned on the rendered avatar in the target pose as well as a text prompt describing the avatar\u2019s action and the scene. The schematic of our model is illustrated in Figure 2."
        },
        {
            "heading": "3.1. Background: Variational Human Pose Prior",
            "text": "Since 3D pose of human body is complex and high dimensional, we use a pre-trained VPoser [32] as a Variational Autoencoder to efficiently model the distribution of pose priors instead of learning to generate all 3D joint rotations. VPoser learns a latent representation of human pose and regularizes the distribution of the latent code to be a nor-\nmal distribution. The VPoser has been pre-trained on a large dataset of SMPL pose representations using the KullbackLeibler and reconstruction losses (following the common VAE formulation), in addition to losses which encourage encoding of valid rotation matrices. The architecture consists of a symmetric encoder-decoder, each with two dense layers, and a 32 dimensional vector as the bottleneck latent space.\nThis continuous embedding space fits well in the the gradual noising and denoising diffusion process, while the widely used human pose representations such as quaternions and Euler angles are discontinuous [56].\nFor some experiments we alternatively train our model on the 6D continuous pose representations for the 21 SMPL joints and root orient. Since joint representations are unconstrained in this setting, unnatural body and hand poses can be expected. As a post-processing inference step after generating samples with this 6D pose representation, we pass poses to the VPoser encoder and decoder to regularize them in the natural distribution of human bodies."
        },
        {
            "heading": "3.2. Large-Scale Image Text Pseudo-Pose (ITPP)",
            "text": "Dataset\nWe have collected a dataset containing 35M pairs of human poses and their text descriptions from a few large-scale image-text datasets. We processed all images by running Detectron2 keypoint detector [48] to find images with a single human, then extracted their 3D pseudo-pose SMPL annotations using a pre-trained PyMAF-X model [51]. This large-scale data overcomes the limitations of the existing mocap datasets by providing a wide variety of human poses and a huge number of (text, 3D pose) sample pairs. Due to legal concerns, we do not extract any face or hand expressions from the aforementioned datasets."
        },
        {
            "heading": "3.3. Text-to-3D Pose Synthesis",
            "text": "We design a diffusion based generative model, f\u03b8, that maps the input text encodings, y, from a pre-trained CLIP model to the concatenation of the continuous body pose representations and root orient, (xp, xr). Our text-to-3D-pose model is based on a decoder-only Transformer with a causal attention mask operating on a sequence of tokenized captions and their CLIP text embeddings, the diffusion timestep embedding, the noised body pose and root orient representations, and two final pose and orientation queries to predict the unnoised pose and root orientation, respectively. The model architecture is illustrated in Figure 3. Similar to DALL-E 2 image prior [35], we train our model to directly predict the unnoised pose and root orientation using a mean-squared error loss as:\nLMSE = Et\u223c[1,T ],xp\u223cqp,xr\u223cqr \u2016f\u03b8((x (t) p , x (t) r ), t, y)\n\u2212 (xp, xr) \u20162\nTo improve sample quality at test time, we use classifier free guidance [17] by dropping the text conditioning 10% of the time during training. After training, we pass the body pose encodings generated by our diffusion model to the pre-trained VPoser decoder to generate the 3D SMPL body rotation vectors."
        },
        {
            "heading": "3.4. Avatar Rendering",
            "text": "To perform a zero-shot personalization given a generated 3D body pose, we first render an image of the posed avatar as a reference input. In order to personalize our image generation model with high-quality user avatars, we use an internal avatar representation and rendering engine. This demonstrates how our method is not restricted to the SMPLbased avatars.\nGiven the generated 3D pose in SMPL, we first perform a retargeting process to convert it to the internal avatar pose through an optimization process which matches the corresponding joints in position and orientation. Then, we take the converted pose and the personalized avatar config, including its shape and texture, to render an avatar image in the target pose, as shown in Fig. 2. Please note the retargeting and rendering steps do not need any training or prior knowledge of users\u2019 avatar configs, i.e., they are zero-shot. This setup can work with any other avatar representation as long as a retargeting solver from SMPL and rendering engine is in place."
        },
        {
            "heading": "3.5. Text-to-Avatar Image via 3D Pose",
            "text": ""
        },
        {
            "heading": "3.5.1 High-Level Approach",
            "text": "As opposed to methods which require fine-tuning for each subject of personalization, we desire a model which can perform zero-shot personalization to generate an image of a user\u2019s avatar interacting with a scene and objects corresponding to the input text. We can perform zero-shot personalization by re-framing it as an image outpainting task. That is, given a rendered image of a posed avatar and text which describes the scene and interactions, we generate a personalized image by outpainting from the avatar to fill in the rest of the scene and objects."
        },
        {
            "heading": "3.5.2 Text-to-Image Priors",
            "text": "Our model is a latent diffusion text-to-image generation model that has been pre-trained on large-scale image-text datasets enabling vast open-world visual textual understanding. Our model is built of a UNet in a learned latent space of an image autoencoder z = E(x) conditioned on the timestep t and CLIP text encodings y making it more efficient than other large-scale text-to-image generation models."
        },
        {
            "heading": "3.5.3 Personalized Image Generation Dataset",
            "text": "To train this personalized diffusion model, we would ideally have a dataset of images of avatars interacting with open-world environments. However, no such dataset exists. Instead, we instead use our curated ITPP dataset of full body humans discussed in 3.2, and use panoptic segmentation [48] to crop out the human for conditioning. So, at train\ntime we condition the UNet on a cropped person, and at test time we condition on the image of the avatar rendered in the pose generated by the text-to-3D-Pose model."
        },
        {
            "heading": "3.5.4 Model Architecture",
            "text": "To improve the model\u2019s faithfulness to the avatar\u2019s appearance and reduce the domain gap between photo-realistic humans during train time and avatars during test time, we make two architecture modifications: (1) additionally condition the UNet on the gray body render of the 3D pose; (2) augment the personalization conditioning by downsampling the spatial dimensions and condition the UNet on the downsample factor. (1) Injects human body priors into the image generation model so it can generate more realistic avatarobject interactions. It also helps to bridge the gap between humans and avatars since gray body renders are the same for each.(2) helps the image generation model be less sensitive to the avatar\u2019s appearance when stylizing the image since downsampling removes some of the texture details in the conditioning.\nAltogether, the full set of conditioning for the UNet is the text describing the scene and avatar-object interaction (y), the rendered RGBA image of the avatar in the target pose (p), the personalization downsample rate (w), and the timestep (t). We train the model via the loss:\nL = EE(x),y,p,w\u223c[0,1], \u223cN(0,1),t\u223c[1,T ] [ \u2016\n\u2212 \u03b8(zt, t, y, p, w) \u201622 ] (1)\nTo condition on the RGBA personalization image p, we use E(p) to encode the RGB channels to zp and concatenate zp onto zt along the channels dimension. We separately downsample the \u03b1 channel to the spatial dimensions of z (64x64) and concatenate along the channels dimension. To condition on the personalization augmentation downsample rate w, we encode it using a sinusoidal embedding, project to the dimensions of the CLIP text encodings, and concatenate with text encodings for cross-attention. Given that we formulate image personalization as an outpainting problem, it is natural to fine-tune from an inpainting/outpainting model which is already conditioned on zp and a corresponding mask."
        },
        {
            "heading": "3.5.5 Post-Processing",
            "text": "Maintaining strict faithfulness to the avatar\u2019s appearance is important to respect the user\u2019s identity. Because the diffusion autoencoder reconstruction is imperfect, we chose to paste the rendered RGBA image of the posed avatar back onto the generated image."
        },
        {
            "heading": "4. Experiments",
            "text": "In addition to our ITPP dataset, we also create a train and test set from the HumanML3D motion dataset [13] to be used during training and test time, respectively. For each motion sequence, we render an avatar per frame and automatically split the corresponding long captions into smaller sentences based on their provided grammatical properties to make sure each text describes one frame. Then, we find top-10 and top-1 pairs of (rendered pose, caption) per motion sequence in terms of their image-text CLIP similarity score, resulting in 300K and 3250 samples in the train and test splits, respectively.\nWe compare the performance of our pose generation model with the existing state-of-the-art text-to-humanmotion generation models through human evaluation on 390 crowd-sourced prompts and automatic metrics on the HumanML3D test set. Given each text prompt, we generate five poses and render them through our rendering service. We then sort them based on the CLIP similarity scores between the rendered avatars as 2D images and the input text prompt to select the best generated sample. For each baseline, we generate a motion sequence given a text prompt and select the best representative frame based on their CLIP similarity scores with the text encoding. The results confirm the superiority of our pose generation system and its faithfulness to the input text.\nWe also evaluate our personalized image generation model against a simple outpainting baseline on our crowdsourced prompts and show that our fine-tuning and architecture modifications improve faithfulness to the avatar\u2019s appearance."
        },
        {
            "heading": "4.1. Evaluation of Pose Generation",
            "text": ""
        },
        {
            "heading": "4.1.1 Human Evaluation Set and Metrics",
            "text": "We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 390 prompts. We asked annotators for prompts that described an action along with some context of the scene. We filtered out prompts based on ethical concerns to remove any references to children or NSFW content. These prompts were selected without generating any poses or images for them, and were kept fixed for all of our evaluations. To evaluate faithfulness of the generated poses to the input texts, we show raters a text description and the corresponding rendered avatars from two models and ask them which output better matches the input text. For each comparison, we use the majority vote from 5 different annotators as the final result, reported in Table 1. This study confirms the superiority of our model and the impact of our large-scale ITPP dataset in human pose generation and overcoming the limitations of the MOCAP datasets. Qualitative examples shown in Figure 7."
        },
        {
            "heading": "4.1.2 Automatic Metrics",
            "text": "To evaluate the performance of each model in terms of quality and diversity of their generated samples, we propose a Fre\u0301chet Pose Distance, FPD, computed as the Fre\u0300chet distance in the Vposer embedding space. Moreover, to measure the correctness of the generated poses, we compute the CLIP similarity score between the rendered avatars corresponding to each generated pose and their textual descriptions. As reported in Table 1, our method outperforms the baseline MotionCLIP, AvatarCLIP, and MDM models significantly. Although TEMOS shows slightly better results in terms of FPD and CLIPSIM scores, it performs very poorly in our human evaluations. This could be due to overfitting to the mocap dataset and not generalizing to in-the-wild poses."
        },
        {
            "heading": "4.1.3 Ablation Studies",
            "text": "We perform another human evaluation to understand the impact of different continuous pose representations during training: 32-dimensional VPoser embedding for the body pose, 6D continuous vectors for all joints, and 6D continuous vectors for all joints regularized by the pre-trained VPoser, as explained in 3.1. Our results summarized in Table 2 confirm the benefit of VPoser representations either in the training of the diffusion model or used as a post regularizer (the last two rows in the table). We also compare the performance of our model when trained on the ITPP data vs. trained on the modified HumanML3D dataset for pose generation. This study again confirms the impact of our large-scale ITPP dataset compared with the limited existing MOCAP datasets by providing a wide range of human actions and training more generalizable motion generation models."
        },
        {
            "heading": "4.2. Evaluation of Personalized Image Generation",
            "text": "We compare our approach for avatar-personalized image generation with a simple outpainting baseline which is fine-tuned from our pre-trained text-to-image model. For this evaluation, we provide as input the posed avatars and their corresponding mask. We also compare against this inpainting/outpainting model which has been fine-tuned on our ITPP dataset to decouple the value of the dataset versus our architecture modifications (termed \u201cfine-tuned\u201d).\nWe use the same test set of human evaluation prompts described in 4.1.1, and use poses generated from our textto-3D pose generation model. We report both human evaluation and automatic metrics. For automatic metrics, we compute the mean CLIP similarity between the generated image and the input text prompt. For human evaluation, we report two metrics. To measure how well the avatar appearance corresponds to the ground truth avatar, we present raters with the input avatar as well as a generated image from our model and the baseline and ask which image is more faithful to the avatar\u2019s appearance. Note that while we blend the avatar\u2019s appearance back onto the generated image, there may still be hallucinated limbs or articles of clothing. To measure how well the avatar-scene interaction follows the text, we present raters with the input text as well as a generated image from our model and the baseline and ask which image better matches the text. We present each generation to 5 raters and report the majority vote. All human evaluation metrics are reported with respect to the inpainting/outpainting baseline.\nAs we can see in Table 3, our method outperforms the baseline in both automatic and human evaluation metrics. We also show qualitative comparisons in Figure 4. In particular, our method makes significant improvements in faithfulness to the avatar\u2019s appearance. This makes sense because the inpainting/outpainting baseline model was trained on random masks, not human masks. As such, we find that it often hallucinates new limbs and articles of clothing, disturbing the avatar\u2019s identity (see Fig. 4 for examples). Both\nthe fine-tuning on human body masks and the architecture modifications improve faithfulness to the avatar appearance. In particular, we believe that the additional conditioning of the image generator on the gray body render improves the model\u2019s understanding of the human body orientation and limb positioning to reduce hallucination of new body parts."
        },
        {
            "heading": "5. Limitations & Future Work",
            "text": "There are a number of limitations of our approach which we leave for future work. For one, we only model human body pose in our text-to-3D pose diffusion model. This means that we are unable to control hand poses and facial expressions via text. Without modeling hand poses, we are limited in the granularity of avatar-object interactions (e.g., Figure 5 left). In the future, we should be able to add hand and face pose parameters as additional targets for the textto-3D pose diffusion model to enable an even greater level of expression through avatars. There are also limitations to our approach of treating personalized image generation as an outpainting task. Namely, we are unable to generate occlusions over parts of the avatar\u2019s body and the location of the avatar in the scene must be fixed before image genera-\ntion. The inability to handle occlusions is particularly noticeable with prompts like \u201cA person is horseback riding\u201d where depending on the camera angle, one of the avatar\u2019s legs should be occluded (e.g., Figure 5 right)."
        },
        {
            "heading": "6. Conclusions",
            "text": "Motivated to facilitate self-expression of millions of users, we propose a pipeline for zero-shot personalization of image generation through avatars. In order to enable zero-shot personalization while remaining faithful to the avatar\u2019s identity, we first re-pose and render a user\u2019s avatar before generating a personalized image. Our proposed novel text-to-3D pose model beats existing baselines because of a new auto-generated dataset of 35M pairs of human poses and their text descriptions extracted from largescale image datasets. In fact, our experiments show that this dataset produces better results than adding human motion capture datasets like AMASS in our pipeline. As such, this work paves the way to leverage large-scale image and video datasets for learning human 3D pose parameters."
        }
    ],
    "title": "Text-Conditional Contextualized Avatars For Zero-Shot Personalization",
    "year": 2023
}