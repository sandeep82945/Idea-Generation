{
    "abstractText": "The traffic infrastructure of a city requires requires evaluation and improvement through a large amount of data analysis. The construction and laborious work of traditional methods make computer vision flourish in traffic analysis. Among different computer vision technologies for intelligent transportation system, one of the most important algorithms is multiple object tracking (MOT). At present, MOT used in traffic analysis has several shortcomings, such as lack of the output of vehicle speed and movement direction, and the consideration of single factor in trajectory tracking. These limitations have affected the results of traffic analysis. This research proposes an end-to-end deep learning network, called PairingNet. In addition to retaining the function and accuracy of the original detection network, PairingNet integrates the calculation of vehicle trajectory into the network through the feature fusion of consecutive images, which is introduced to predict the movement direction and speed of the vehicle. These additional features can be used to better track the trajectories of vehicles. In addition, a pipeline is designed to reduce the loading latency incurred by using the consecutive frames as the input for PairingNet. The experiment results indicate that the vehicle identification of PairingNet reaches 96% accuracy, surpassing the original YOLOv3 as the backbone structure, and reaches a near 100% accuracy rate in the predicted vehicle position. Moreover, with the pipeline process, the inference speed of PairingNet is very close to the original YOLOv3. In MOT results, the MOTA of PairingNet also has a high performance of 91%. INDEX TERMS Deep learning, multiple object tracking, object detection, traffic analysis, vehicle trajectory.",
    "authors": [
        {
            "affiliations": [],
            "name": "GUAN-WEN CHEN"
        },
        {
            "affiliations": [],
            "name": "MIN-TE SUN"
        },
        {
            "affiliations": [],
            "name": "TS\u00cc-U\u00cd"
        }
    ],
    "id": "SP:74f7b138c08ab2aae757c10b3e60f27ae8b7853a",
    "references": [
        {
            "authors": [
                "R. Ke",
                "Z. Li",
                "J. Tang",
                "Z. Pan",
                "Y. Wang"
            ],
            "title": "Real-time traffic flow parameter estimation from uav video based on ensemble classifier and optical flow",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 1, pp. 54\u201364, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhou",
                "H. Kong",
                "L. Wei",
                "D. Creighton",
                "S. Nahavandi"
            ],
            "title": "Efficient road detection and tracking for unmanned aerial vehicle",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 16, no. 1, pp. 297\u2013309, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Li",
                "J. Tan",
                "A. Liu",
                "P. Vijayakumar",
                "N. Kumar",
                "M. Alazab"
            ],
            "title": "A novel uav-enabled data collection scheme for intelligent transportation system through uav speed control",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 4, pp. 2100\u20132110, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Coifman",
                "M. McCord",
                "R. Mishalani",
                "K. Redmill"
            ],
            "title": "Surface transportation surveillance from unmanned aerial vehicles",
            "venue": "Proc. 83rd Annual Meeting of the Transportation Research Board, Washington, DC, USA, 2004, pp. 1\u20139.",
            "year": 2004
        },
        {
            "authors": [
                "H. Menouar",
                "I. Guvenc",
                "K. Akkaya",
                "A.S. Uluagac",
                "A. Kadri",
                "A. Tuncer"
            ],
            "title": "Uav-enabled intelligent transportation systems for the smart city: Applications and challenges",
            "venue": "IEEE Communications Magazine, vol. 55, no. 3, pp. 22\u201328, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Wang",
                "F. Jiang",
                "B. Zhang",
                "R. Ma",
                "Q. Hao"
            ],
            "title": "Development of uavbased target tracking and recognition systems",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 8, pp. 3409\u20133422, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhang",
                "L. Chai",
                "L. Jin"
            ],
            "title": "Vehicle detection in uav aerial images based on improved yolov3",
            "venue": "2020 IEEE International Conference on Networking, Sensing and Control (ICNSC), 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "A. Bewley",
                "Z. Ge",
                "L. Ott",
                "F. Ramos",
                "B. Upcroft"
            ],
            "title": "Simple online and realtime tracking",
            "venue": "2016 IEEE International Conference on Image Processing (ICIP), pp. 3464\u20133468, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y.-C. Huang",
                "I.-N. Liao",
                "C.-H. Chen",
                "T.-U. \u0130k",
                "W.-C. Peng"
            ],
            "title": "Tracknet: A deep learning network for tracking high-speed and tiny objects in sports applications",
            "venue": "2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2019, pp. 1\u20138.",
            "year": 2019
        },
        {
            "authors": [
                "S.S. Blackman"
            ],
            "title": "Multiple hypothesis tracking for multiple target tracking",
            "venue": "IEEE Aerospace and Electronic Systems Magazine, vol. 19, no. 1, pp. 5\u20138, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "J.-Y. Bouguet"
            ],
            "title": "Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm",
            "venue": "Intel Corporation, vol. 5, no. 1-10, p. 4, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "S. Hamid Rezatofighi",
                "A. Milan",
                "Z. Zhang",
                "Q. Shi",
                "A. Dick",
                "I. Reid"
            ],
            "title": "Joint probabilistic data association revisited",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Black",
                "T. Ellis",
                "P. Rosin"
            ],
            "title": "Multi view image surveillance and tracking",
            "venue": "Workshop on Motion and Video Computing, 2002. Proceedings, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "D.R. Magee"
            ],
            "title": "Tracking multiple vehicles using foreground, background and motion models",
            "venue": "Image and vision Computing, vol. 22, no. 2, pp. 143\u2013 155, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "H.W. Kuhn"
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83\u201397, 1955.",
            "year": 1955
        },
        {
            "authors": [
                "Z. Tang",
                "J.-N. Hwang"
            ],
            "title": "Moana: An online learned adaptive appearance model for robust multiple object tracking in 3d",
            "venue": "IEEE Access, vol. 7, pp. 31 934\u201331 945, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G. Wang",
                "Y. Wang",
                "H. Zhang",
                "R. Gu",
                "J.-N. Hwang"
            ],
            "title": "Exploit the connectivity: Multi-object tracking with trackletnet",
            "venue": "arXiv preprint arXiv:1811.07258, 2018.",
            "year": 1811
        },
        {
            "authors": [
                "W. Balid",
                "H. Tafish",
                "H.H. Refai"
            ],
            "title": "Intelligent vehicle counting and classification sensor for real-time traffic surveillance",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 19, no. 6, pp. 1784\u20131794, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.-L. Jeng",
                "W.-H. Chieng",
                "H.-P. Lu"
            ],
            "title": "Estimating speed using a sidelooking single-radar vehicle detector",
            "venue": "IEEE transactions on intelligent transportation systems, vol. 15, no. 2, pp. 607\u2013614, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Dalal",
                "Navneet",
                "Bill Triggs"
            ],
            "title": "Histograms of oriented gradients for human detection.",
            "venue": "Computer Vision and Pattern Recognition,",
            "year": 2005
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "H. Bay",
                "T. Tuytelaars",
                "L. Van Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "European conference on computer vision, pp. 404\u2013417, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "C. Cortes",
                "V. Vapnik"
            ],
            "title": "Support-vector networks",
            "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "Y. Freund",
                "R.E. Schapire"
            ],
            "title": "A decision-theoretic generalization of online learning and an application to boosting",
            "venue": "Journal of computer and system sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Y. Liu",
                "L. Zhang",
                "Z. Chen",
                "Y. Yan",
                "H. Wang"
            ],
            "title": "Multi-stream siamese and faster region-based neural network for real-time object tracking",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, pp. 1\u201314, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Tian",
                "C. Lin",
                "J. Zhou",
                "X. Duan",
                "Y. Cao",
                "D. Zhao",
                "D. Cao"
            ],
            "title": "Sayolov3: An efficient and accurate object detector using self-attention mechanism for autonomous driving",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, pp. 1\u201312, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolov3: An incremental improvement",
            "venue": "arXiv preprint arXiv:1804.02767, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2117\u20132125.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Deep learning, multiple object tracking, object detection, traffic analysis, vehicle trajectory.\nI. INTRODUCTION\nTRAFFIC data collection and analysis are key compo-nents in smart cities for efficient and real-time management of transportation networks. This motivates researchers of Intelligent Transportation Systems (ITS) to continuously develop more accurate and more cost-effective methods for traffic data collection. One of the recent methods is the Unmanned Aerial Vehicle (UAV) with video streaming capability which covers wide area, reduces installation costs, and minimizes errors [1]\u2013[4]. Also, the UAV supported by image processing techniques allows for collecting microscopic traffic information, such as vehicle type, direction, trajectory, speed, and driving behavior [1], [2], [5]. Thus, intelligent\nimage processing methods are required for detecting and tracking objects captured in the UAV videos.\nSeveral fundamental object detection methods have been used in ITS [1]\u2013[4], [6], [7]. Although these studies achieved high accuracy in object detection, the main focus was on detecting a single object to determine road parameters and traffic variables with little information about object type and trajectories. Another popular algorithm for detecting multiple objects and trajectories is Simple Online and Real-time Tracking (SORT) [8]. SORT utilizes the Kalman filter and the Hungarian algorithms to speed up the computation process. Also, SORT provides a method to generate trajectories step by step. However, the multiple steps cause more errors since\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nthese steps have a knock-on effect, and the huge amount of frame information affects the calculation of the prediction significantly. Further, the Kalman filter based methods suffer from low accuracy particularly when they deal with moving images such as drone videos [1].\nThis study focuses on detecting vehicles on roads, classifying them, and determining their trajectories. To achieve these goals, the study proposes PairingNet which is an end-to-end network with object detection and prediction algorithm. We introduce the concept of TrackNet [9] which continuously analyzes multiple frames to detect the objects and simultaneously predict their next positions. Therefore, the prediction not only considers the speed and the position, but also the vehicle features. PairingNet is divided into object detection, prediction, and pairing. Object detection is used to determine the positions of vehicles. The position of each vehicle in the next frame is predicted using the information acquired in the previous frames. Finally, the pairing algorithm is used to get the trajectory of each vehicle in the video. To evaluate the performance of PairingNet, aerial videos taken by UAV are used and analyzed. The results show that the mAP of the object detection in PairingNet is 96%, which is similar to YOLOv3, and the precision of the pairing prediction is close to 100%.\nIn summary, the contributions of this research are as follows.\n\u2022 We propose PairingNet, an end-to-end object detection and prediction, for vehicle tracking based on the topview traffic video recorded from a UAV. PairingNet is inspired by YOLOv3 and TrackNet. It takes three consecutive frames as the input to extract more information from vehicles. \u2022 PairingNet detects vehicles with not only the position of the bounding box and vehicle category but also the information including the driving direction and status. The output of PairingNet can be used to improve the performance of the vehicle tracking algorithm. \u2022 To minimize the calculation cost of PairingNet, a pipeline process is introduced to reduce the delay from loading three consecutive frames as the input. The experiment results show that the inference time of PairingNet with pipeline is extremely close to that of YOLOv3.\nThe rest of this paper is structured as follows. Section II illustrates the related works about object tracking algorithm. Section III discusses the traditional and modern methods of vehicle detection. Section IV-A and Section IV-B explain the proposed PairingNet and the multiple object tracking (MOT). Section V presents the training and testing dataset. Section VI shows the experiment results and performance analysis, and finally Section VII concludes the paper and outlines future work.\nII. RELATED WORK Traditional MOT algorithms, such as Multiple Hypothesis Tracking [10], Optical flow [11], and Joint Probabilistic\nData Association filters [12], usually consume much time, and the time increases exponentially with the number of objects being tracked [13]. Hence, these algorithms are not appropriate for application to traffic surveillance in this study. Instead, the Kalman Filter [14], [15], which has fast calculation, has become widely used in MOT. For example, the SORT [8] method proposed in 2016 a fast and stable algorithm by combining Kalman Filter and Hungarian algorithm [16]. However, SORT has certain limitations, such as using singular consideration factors and the initialization parameter, which greatly affect the performance of the algorithm. Therefore, several studies proposed advanced network based on SORT or Kalman Filter, such as Deep SORT [13] and MOANA [17], which take into account the shape and color of the objects to strengthen the capability of trajectory tracking.\nDeep SORT uses the prediction model of Kalman Filter and the Mahalanobis distance to calculate the spatial correlation of objects between two frames in the paring algorithm [13]. In addition, to eliminate a large number of paring errors when the object has high motion uncertainty, Deep SORT considers the appearance of the object as the paring information which greatly improves the object tracking accuracy. The Appearance Metric in Deep SORT is provided in Equation 1.\nd(i, j) = min(1\u2212 rTj r (i) k | r (i) k \u2208 Ri) (1)\nIn the equation, d(i, j) measures tracking association with j-th detected object in i-th trajectory. Each bounding box dj has an appearance feature descriptor rj with |rj | = 1. For the i-th trajectory, multiple trajectory appearance descriptors r(i)k are recorded in Rk = { r (i) k }Lk k=1\n, where k is the number of trajectories. The equation calculates the cosine distance of the i-th trajectory and the latest j-th bounding box, and finds the most similar appearance and spatial paring result to identify a more realistic pairing.\nTrackletNet Tracker(TNT) [18] is a tracking algorithm that has made achievements in the MOT field in recent years. It is based on the operation of Tracklet, which integrates the concept of Graphical module combined with cluster analysis to distinguish the tracklet of each different object, and finally reconstructs the complete trajectory. The whole TNT operation is explained as the Tracklet is generated by the result of IoU and appearance feature. Then, the Tracklet represents the nodes in the graph model and pushes two different Tracklets into the network to calculate the similarity Pe of the two Tracklets. Equation 2 is used to get the EdgeCost of the two Tracklets (nodes). The Pe is between zero and one, where a higher Pe means a higher similarity, and consequently a closer distance. At the end, the graph model is taken as an input of the cluster analysis. The Tracklets that are put into the same cluster are regarded as the same object, and these Tracklets are then merged into a trajectory."
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nEdgeCost = log( 1\u2212 Pe Pe ) (2)\nIII. PRELIMINARY The collection of traffic data has evolved from simple traditional tools to advanced ones such as Unmanned Aerial Vehicle (UAV). Traditional tools include inductive loop [19] in intrusive way, microwave radar [20] in non-intrusive way, and roadside camera using computer vision. The new data collection method by UAVs reduces the infrastructure cost and maximizes the accuracy. Vehicle detection by UAV uses computer vision methods for video processing and object detection.\nThe most widely used object detection methods for computer vision are feature extraction by histogram of oriented gradient (HOG) [21], scale-invariant feature transform (SIFT) [22], and speeded up robust features (SURF) [23]. To identify objects in the images, the extracted features are used as the inputs for the training process performed by machine learning methods, such as SVM [24] and Adaboost [25].\nRecently, the deep learning network has vigorously been developed due to the advancement of computer hardware. The Convolutional Neural Network (CNN), which is a classic deep learning network, has made a breakthrough in computer vision analysis, and aims to learn features in each layer using different filters [26]. The filters extract the characteristics of objects from the images as the features, then the deeper filters will elicit the more complicated features like patterns or shapes. A well-designed network with a large number of filters in different layers can extract better features to strengthen the performance of object detection. Nowadays, most of the CNN based object detection methods can be classified into\ntwo categories, which are region-based methods and regionfree methods.\nThe region-based methods generate a lot of region proposals, then detect the objects and their position with specific regions. R-CNN, Fast R-CNN, and Faster R-CNN are the most popular region-based object detection networks [27]. These networks use CNN to extract features from specific regions, which are selected from the region proposals in different ways, then they use SVM or regressor to classify and position the objects.\nThe region-free methods take the whole frame as the input without generating region proposals to detect the objects [28]. The region-free methods also divide a frame into several small regions, and detect objects from these regions. Some of the region-free object detection networks add anchor boxes in every small region before detecting objects to reduce the calculation. The You Only Look Once (YOLO) is a famous region-free object detection network [29]. YOLO is a real-time object detection network proposed in 2015 and extracts features from the entire image. The original concept of YOLO is dividing an image into S \u00d7 S grids. For the identification, the grid image is divided into two parts. The first part provides several bounding boxes in different scales to identify the target position and confidence, and the other part forms small areas to distinguish classes. YOLOv1\u2019s network architecture has a final output dimension of 7\u00d77\u00d730 which means 7\u00d7 7\u00d7 (2\u00d7 5 + 20). Each small area predicts 4 parameters and 1 credibility of 2 identification frames, and 20 class probabilities of each small area.\nYOLOv3 introduced residual network (ResNet) and feature pyramid to improve the detection performance in small objects [30], [31]. The architecture of YOLOv3 is shown in Figure 1. An image as the input is features extracted by several ResNet in different scales. Then, the feature pyramid structure is adopted to generate three feature maps. Similar to YOLOv1, input image of YOLOv3 is also divided into several grids for predicting objects. Three prior anchors in different scales are utilized for three feature maps to predict objects. In Figure 1, the size of output from three feature maps is (5+ class)\u00d7 3, where 5 represents the 4 parameters and the confidence of the predicting box, class is the probability of the object class, and 3 means the predicting boxes from the three different prior anchors. Although the detection accuracy of YOLO is not as high as that of the faster R-CNN, its ultra-fast calculation and medium-to-high accuracy rate are effective for object detection [28].\nObject detection algorithms usually take an image as the input of the network to detect and recognize objects. However, the applications in the field of smart transportation, city, and sport often require extraction of trajectories of the objects after the detection to analyze the behavior from them. In Section II, several multiple object tracking algorithms using object bounding boxes are introduced. In these works, objects occluded by obstacles and other objects will often lead to detecting errors. Their trajectories will then be interrupted in the tracking algorithm, which will consequently result\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nin tracking errors. Therefore, a concept, called detection by tracking, has been proposed to reduce the errors caused from object occlusions. TrackNet [9] is a deep learning network for tracking a tennis ball at high speed. In Figure 2, (a)-(c) are the three consecutive frames from a video where the tennis ball is occluded by the tennis player in frame (b). The concept of tracking by detection is shown in Figure 2 (d)-(f). The tennis ball can be detected and tracked in the previous and the next frames, then the position of the occluded tennis ball in the middle frame (e) can be predicted.\nFigure 3 shows the architecture of TrackNet, and the input is three consecutive frames of the video to realize detection by tracking.\nIV. METHOD A. PAIRINGNET PairingNet proposed in this research is inspired by YOLOv3 and TrackNet [9]. In TrackNet, the tennis ball is tracked with three input frames to solve the issue caused by occlusions, as shown in Figure 2. YOLOv3 integrates ResNet and feature pyramid to improve the performance of detecting small objects. The proposed PairingNet takes three consecutive images as an input and targets multiple objects in different frames for detection and tracking. For tracking detected objects, PairingNet uses the feature extraction framework ResNet in YOLOv3 to extract similar features from images, and then calculates the information for each object in the consecutive frames. The information consists of the locations\nof the object in the three frames and the algorithm uses the information to obtain the trajectory of an object. The architecture of PairingNet is shown in Figure 4. From the input, the network extracts features by ResNet, then builds feature pyramid with top-down manner and lateral connections to improve the capability of predicting small objects.\nThe feature pyramid consists of three feature maps, which are 13\u00d7 13\u00d7 1024, 26\u00d7 26\u00d7 512, and 52\u00d7 52\u00d7 256. PairingNet also predicts three bounding boxes at each scale. The output of the network contains the original five parameters of a bounding box, which are the center coordinates, width, height, confidence, and the additional four new parameters of a predicting pair, which are the coordinates of the predicting position, past confidence, and future confidence, as depicted in Figure 5. In the figure, (fx, fy) represents the coordinates of the predicting pair. The parameters Past and Future are the past and future confidence of the pair, respectively. In addition to the coordinates of the object and the pair, the network also considers the object confidence in previous frame Past and next frame Future to determine if the object is entering or leaving the section.\nIn addition to the original loss function of YOLOv3, PairingNet loss Lpair includes four new loss terms: Lfx, Lfy, LPast, andLFuture. Lfx and Lfy are the Mean Square Error (MSE) of the x and y coordinates, respectively. LPast and the LFuture are the Binary Cross-Entropy (BCE) of the past and future confidence, respectively, as the past and future confidence are binary. The used BCE is different from the original BCE Loss calculation method since a weight \u03b1 is added to make the calculation of the loss function more consistent with the results of the experiment. The modified BCE is given by Equation 3.\nWeightedBCE = \u2212 1 n n\u2211 i=1 [x\u0302ilog(xi)+\u03b1\u00d7(1\u2212x\u0302i)log(1\u2212xi)]\n(3) When analyzing the dataset and the two parameters, future and past, we can see that the probability of these parameters to be zero is low. For example, a car takes about 5 seconds from appearing on one side of the screen to driving away from the other side when there is no red light. If the experiment used eight FPS to train the network, as in the benchmark, the probability of the past or future parameter to be 0 is only about 140 . The past parameter is zero at the moment when the car has just entered the screen, and the future parameter is zero at the moment when the car is about to leave the screen. For all remaining frames, these parameters are all ones. To prevent the network from biasing towards predicting one, the error when the standard is zero is given a relatively large weight.\nThe newly added four loss terms all rely on the effectiveness of the original YOLOv3 object detection. If the object detection performance of PairingNet is not as expected, the subsequent calculation of each pair is severely limited by detection errors, which will make the network meaningless."
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nFIGURE 4. PairingNet architecture based on YOLOv3.\nTherefore, in the calculation of PairingNet loss function Lpair, a weight value \u03b2 is added to distinguish the original Lyolov3 from other newly added Loss, as shown in Equation 4.\nLpair = \u03b2\u00d7Lyolov3+[Lfx+Lfy +Lpast+Lfuture]. (4)\nIt is expected that the coordinates of each pair can be found without reducing the detection performance, as illuminated in Equation 4.\nThree consecutive images from the video are used as the inputs of PairingNet to extract additional features from the detected vehicle. Compared to the single image as the input of YOLOv3 [29], PairingNet consumes more time to load multiple images during inference. To reduce the data loading latency, a pipeline is introduced to load the images from the video. At first, PairingNet loads the first three frames of the video, then the first of the three frames will be deleted\nafter inference. The next frame right after the first three frames will be loaded for the next inference. This process is repeated so that at each inference, only one frame needs to be loaded. As will be shown in Section VI-B, this pipeline trick significantly improves the inference speed of PairingNet to be comparable to YOLOv3\u2019s."
        },
        {
            "heading": "B. MULTIPLE OBJECT TRACKING ALGORITHM",
            "text": "The vehicle information extracted from every three consecutive frames can be paired via the MOT algorithm. Hungarian algorithm, one of the most popular algorithms of MOT, is a minimum loss optimization algorithm to solve assignment problems. For example, Equation 5 shows a loss matrix of vehicle trajectories, each row in the matrix is a detected vehicle in the current frame, and each column is a trajectory extracted from previous frames. End each element of the matrix denotes the loss of the detected vehicle referring to the trajectory. The multiple vehicle tracking task is to find out the optimal solution to assign the detected vehicle to a trajectory, and each trajectory can only be paired to a single vehicle of the current frame. In this case, the optimal solution of Equation 5 using Hungarian algorithm is L11, L23, and L32. The total loss of the solution is 2 + 1 + 0 = 3, which is the lowest loss.\nL = 2 3 04 5 1 1 0 5  (5) In this paper, Hungarian algorithm is used for tracking vehicles, and the results are optimized using additional parameters, which are the threshold of distance and vehicle status. Vehicles detected from object detection algorithm contain true positive results, false positive results, and false negative results. The higher performance of the algorithm\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\ncauses smaller quantities of false positive and false negative results. To reduce the incorrect tracking results affected by the object detection, the distance between detected vehicles in consecutive frames is utilized for optimization. Note that the driving speed of the same vehicle should be in a reasonable range and the driving speed of different vehicles in the same direction are close to each other. As a consequence, a distance threshold is used as a filter, which will remove the tracking results if the vehicle distance between the last frame in the trajectory and the current frame is more than the threshold. On the other hand, parked vehicles on the roadside also may cause mismatches of the tracking results when a detected vehicle is driving through. In this paper, each detected vehicle in the video will be marked with a tag after PairingNet to determine its status, whether the vehicle is moving or not. There are two cases when a vehicle is tagged to be not moving. The first case is the vehicle is parked, and the second case is that it is waiting for a traffic light. In the case when a detected vehicle in the current frame is marked as moving and is assigned to a trajectory, but the vehicle in the latest frame is marked as stop, the assignment will be removed if the distance between the vehicle in those two frames is higher than a different threshold.\nAfter most of the detected vehicles are assigned to existing trajectories using Hungarian algorithm with optimization, some of the vehicles may not belong to any trajectory because they are just entering the road section in the video. Therefore, the values of Past and Future, part of the output of PairingNet, can be used to identify the unassigned vehicles. When Past is false and Future is true, it means the last frame of the input of PairingNet does not contain the unassigned vehicle. So the vehicle can be treated as an entering vehicle to the road section in the video. In this case, a new trajectory will be created to record the unassigned vehicle. After all detected vehicles are assigned to a trajectory, some of the existing trajectories will be removed if no updated vehicle is assigned to them for a duration longer than a threshold.\nV. DATASET The dataset used in this study is recorded from aerial videos with the versatile action camera at an altitude of 25 and 50 meters over an intersection in Hsinchu, Taiwan, as shown in Figure 6. The vehicles and pedestrians in the videos are labeled for object detection and tracking, and the labels are classified into eight categories: pedestrian, bike, scooter, sedan, bus, s_Truck for small truck, l_Truck for large truck, and trailer, as shown in Figure 7. In the s_Truck category, the length of the truck is less than 3 meters.\nGoPro Hero 7 [32], a versatile action camera, is used for recording the video in this paper. The camera has several operational modes with different points of view (PoV) in vertical, horizontal, and diagonal view, as shown in Table 1. Each video in the dataset has a resolution of 1920 \u00d7 1080 in wide mode, and its horizontal PoV is 118.2\u25e6. In most videos, the altitude of UAV is 50 meters, so the frame width in the\nvideo is about 173 meters. Two image sizes, 416 and 608 pixels as frame width, are used as the input of PairingNet, so the scales are 0.416 and 0.285 meter per pixel in the two frame sizes. However, small objects like pedestrians, bikes, and scooters are more difficult to be detected than large objects. To improve small object detection, additional videos with the altitude of UAV being 25 meters are collected. The configurations used to collect all videos are listed in Table 2.\nAn open-source online labeling tool, called Video Annotation Tool from Irvine, California (VATIC) [33], is used in this research. The design of the user interface is suitable"
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nfor labeling consecutive frames with multiple objects. In our dataset, all the objects of interest are labeled by VATIC for PairingNet. There are 9 parameters to be marked, which are TrackID, xmin, ymin, xmax, ymax, FrameID, Lost, Occluded, and Label. TrackID denotes the object in the same track, so the objects with the same TrackID in different frames can be identified as the same object. The xmin, ymin, xmax, and ymax are the upper-left and the lower-right corners of the bounding box that locates an object in the frame. FrameID marks the identity of current frame to the objects. The label Lost is 1 if the object is outside of the screen. Occluded will be marked 1 if the object is occluded. Label denotes the category that the object belongs to. The label data of the objects in each frame are exported to a different annotation file separately.\nVI. RESULT AND EVALUATION The results of object detection, pairing, and tracking are presented in this section. Section VI-A shows the experiment results using test data from the dataset. Then Section VI-B compares the results of object detection and pairing between YOLOv3 and ours, and Section VI-C exhibits the performance of the multiple object tracking algorithm.\nA. DETECTION AND TRACKING RESULT The output of PairingNet includes the vehicle location, predicted moving vector, vehicle state, and object confidence. The confidence threshold is preset to 0.8, and the NonMaximum Suppression (NMS) threshold is 0.25. As shown in Figure 8, the first number on top of the bounding box of the vehicle in the picture represents its condition. Condition 1, 2, and 3 indicate the three instantaneous situations, i.e., just enter the screen, move in the screen, and about to leave the screen. The second number is the confidence of prediction. The vector in the bounding box represents the vehicle direction and speed. For example, the scooter in Figure 8c has condition 2, and its vector points from the right to the left\nin the screen. It means that this scooter is clearly visible in the screen in last frame and also moves in the same direction as the predicted vector. In Figure 8a, the vehicle is leaving the section, so its condition is 3. In addition, because we cannot see this vehicle in the next frame, the vector will not be shown in the bounding box. Figure 9 shows tracking result in two consecutive frames. The number on top of the bounding box of the vehicle is the tracking ID, so each vehicle will have a unique tracking ID in the consecutive frames."
        },
        {
            "heading": "B. EVALUATION OF PAIRINGNET",
            "text": "To evaluate the proposed PairingNet, a total of 13312 images are used as training data, and 3328 images are used as testing data. We start with the detection evaluation, and then show the pairing evaluation. The performance of object detection result is evaluated by mean average precision (mAP), which is determined by the value of Precision and Recall. Precision and Recall are calculated by the number of true positive (TP), false positive (FP), and false negative (FN). TP means the detected vehicle is the same as the ground truth. FP means the detected vehicle does not exist in the ground truth. FN indicates the object in the ground truth is not detected by the network. Precision is defined as TP/(TP + FP ), and Recall = TP/(TP + FN). Intersection over union (IoU) is used to determine the correctness of detected object with ground truth, which is calculated by the ratio of the area of overlap and the area of union between two bounding boxes. In our experiment, a detected vehicle will be a TP instance while the IoU between the bounding boxes of detected vehicle and the ground truth is higher than 0.5, which is the same as in YOLOv3 [29]."
        },
        {
            "heading": "1) Detection Evaluation",
            "text": "Table 3 shows the detection results of the test data by PairingNet. The mAP in the table reaches 96.4% for the five categories. The accuracy and recognition rate of each category\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\nexceed 90%; the precision and recall reach 96.1% and 98.6% respectively. Comparing these results with those of YOLOv3 in Table 4, PairingNet results have higher recognition accuracy through the feature fusion of consecutive images. Also, the recognition of small objects, including pedestrians and scooters, has improved significantly. In particular, the pedestrian recognition increases by 27.7%. However, in multiple consecutive images, s_Truck has obviously more FN because the only recognition difference between s_Truck and l_Truck is the size. The probability of s_Truck to be misjudged as l_Truck increases after the feature fusion of multiple images.\nOne of the advantages of PairingNet is that it achieves tracking at different sampling rates. Table 5 shows the training results in 3 FPS. As can be seen in the table, each category exhibits a high recognition rate at both high and low sampling rates. In the data collection of this experiment, the amount of data for pedestrians is low and the frequency for pedestrians to appear in the images is low. As a consequence, the result of the low sampling rate for pedestrian detection is worse than that of the high sampling rate. If the amount of pedestrian information is increased, this issue should be solved.\nIn order to test the ability of generalization of the network on the training data set, the model is also trained by half of the training dataset, which is 6700 images, and the test data remained the same. Table 6 shows that most of the results are slightly worse than the training results of the original data. Particularly, the performance results of pedestrians and trucks have significantly reduced. The reason for this phenomenon is that the frequency that pedestrians and trucks appear in the original training data is very low. As a result, the reduction of the training data leads to insufficient training for these two categories. Therefore, a certain amount of training materials must be collected. Although the size of training data can be appropriately reduced for faster network training, it is still recommended to use more training data to achieve a better tracking performance.\nPairingNet increases the input to three consecutive images and adds four additional output parameters. This may in-\ncrease the inference time because of the additional network calculations. To evaluate the efficiency of the network under different sizes of input and output, the input of PairingNet is modified to take either one image, two consecutive images, and three consecutive images. In addition, the output of PairingNet is modified to be either with or without the four additional parameters.\nTable 7 shows the inference time of YOLOv3 and PairingNet with different input and output settings. The first number following PairingNet is the number of consecutive input images, and the second number denotes the output with or without four additional parameters. The second column of Table 7 shows the total inference consuming time, and can be split into two processes, which are the network and the data loading. It can be seen that there is not much difference in the time required for the calculation of the Networks, but the time difference in DataLoading is large. Each additional image increases the loading time by approximately 70 milliseconds, which dramatically affects the overall computation speed. This explains that the network itself has little effect on the computation time. In order to speed up data loading, the input images need to be loaded in pipeline. That is, at each timestamp, the latest two input images of the previous timestamp are kept and only one additional image is loaded. In this way, except for the first two timestamps, the data loading time can be greatly reduced, which will bring our ParingNet close to the time consumption of the original YOLOv3. The results of PairingNet after the input pipeline implementation are shown in the last row."
        },
        {
            "heading": "2) Pairing Evaluation",
            "text": "In addition to the detection results, the performance of vehicle pairing prediction is shown in Table 8. We notice that the number of TP and Pair TP are the same, while Pair FP is zero. It means that if a vehicle is detected by PairingNet, then the pair of this vehicle will also be detected. The accuracy of predicting the past and future parameters is 99.9%, and the relatively important negative situation in the evaluation of the"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network\naccess status has also reached 100% prediction. In addition, the average error of the motion vector prediction is about 1.1 pixels, a very small gap, which shows that PairingNet has high vehicle motion estimation accuracy. PairingNet also greatly improves the accuracy of the vehicle pairing as it reaches 98.1%.\nC. EXPERIMENT OF TRACKING ALGORITHM Table 9 shows the result of the trajectory tracking algorithm using CLEAR MOT as the evaluation method. The Ground Truth (GT) shows the number of objects in the test data. The Mismatch Error (MME) is the number of the vehicles in different tracking id. In the table, PairingNet has as high as 82.7% MOTP accuracy on IoU and 91.6% MOTA accuracy. The results show that PairingNet method improves the performance of MOT.\nVII. CONCLUSION AND FUTURE WORKS This research proposes PairingNet which is a new multiobject tracking method. Different from the previous tracking algorithms that are based on object identification, PairingNet uses the features of multiple consecutive images to integrate the feature extraction and regression calculations of the original object detection network. PairingNet successfully adds the object tracking function into the YOLOv3 end-toend object detection network and reduces the accumulated difference between object detection and trajectory prediction. In addition, because of the feature fusion and the extraction application, the calculation of trajectory prediction is no longer just the result of a single feature of speed and position but refers to a large number of related multiple features. The feature fusion of continuous images slightly improves the effect of object detection. The object detection mAP of PairingNet reached 96.4%, and the prediction of the pair position\nreached a success rate of nearly 100%. The overall computing time has also been improved to achieve the same real-time performance as YOLOv3. In terms of trajectory prediction, using Hungarian algorithm and the prediction of the pair position, CLEAR MOT\u2019s MOTP achieves an accuracy of 82.7% of the IoU standard, and MOTA also has an accuracy rate of 91.6%. PairingNet has demonstrated significant research results in real-time object tracking projects.\nAt present, the application environment of PairingNet uses drones to collect and analyze information on flat roads. In the future, we expect PairingNet to be applied to low angle cameras at intersections and to be transplanted to the analysis of highway monitoring cameras. In addition, although PairingNet has integrated the functions of trajectory prediction and object detection, multi-object tracking has additional operations for object pairing in addition to the aforementioned calculations. In the future, we will merge the object pairing operation with PairingNet to achieve the endto-end operation of multi-object tracking."
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "PairingNet: A Multi-frame Based Vehicle Trajectory Prediction Deep Learning Network",
    "year": 2023
}