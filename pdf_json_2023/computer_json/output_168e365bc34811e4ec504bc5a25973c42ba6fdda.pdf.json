{
    "abstractText": "Graph neural networks (GNN) have become an important class of neural network models that have gained popularity in domains such as social and financial network analysis. Different phases of GNN computations can be modeled using both dense and sparse matrix operations. There have been many frameworks and optimization techniques proposed in the literature to accelerate GNNs. However, getting consistently high performance across many input graphs with different sparsity patterns and GNN embedding sizes has remained difficult. In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of graphs and embedding sizes show that SENSEi achieves geomean speedups of 1.105\u00d7 (up to 2.959\u00d7) and 1.187\u00d7 (up to 1.99\u00d7) on graph convolutional networks and geomean speedups of 2.307\u00d7 (up to 35.866\u00d7) and 1.44\u00d7 (up to 5.69\u00d7) on graph attention networks on CPUs and GPUs respectively over the widely used Deep Graph Library. Further, we show that the compositions yield notable synergistic performance benefits on top of other established sparse optimizations such as sparse matrix tiling by evaluating against a well-tuned baseline. ACM Reference Format: Damitha Lenadora, Vimarsh Sathia, Gerasimos Gerogiannis, Serif Yesil, Josep Torrellas, and Charith Mendis. 20xx. Input-sensitive dense-sparse primitive compositions for GNN acceleration. In Proceedings of Proceedings of the Conference (Conf \u2019xx). ACM, New York, NY, USA, 12 pages. https: //doi.org/xx.xxxx/xxxxxxx.xxxxxxx \u2217Now at NVIDIA. He can be reached at syesil@nvidia.com. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conf \u2019xx, Month N-M, 20xx, Location \u00a9 20xx Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN xxx-x-xxxx-xxxx-x/xx/xx. . . $XX.xx https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx",
    "authors": [
        {
            "affiliations": [],
            "name": "Damitha Lenadora"
        },
        {
            "affiliations": [],
            "name": "Vimarsh Sathia"
        },
        {
            "affiliations": [],
            "name": "Gerasimos Gerogiannis"
        },
        {
            "affiliations": [],
            "name": "Serif Yesil"
        },
        {
            "affiliations": [],
            "name": "Josep Torrellas"
        },
        {
            "affiliations": [],
            "name": "Charith Mendis"
        }
    ],
    "id": "SP:379db0e065f5b14b57857b2480a70e867cba0de5",
    "references": [
        {
            "authors": [
                "Shaked Brody",
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "How attentive are graph attention networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Timothy A. Davis"
            ],
            "title": "Algorithm 1000: Suitesparse:graphblas: Graph algorithms in the language of sparse linear algebra",
            "venue": "ACM Trans. Math. Softw.,",
            "year": 2019
        },
        {
            "authors": [
                "Timothy A. Davis",
                "Yifan Hu"
            ],
            "title": "The university of florida sparse matrix collection",
            "venue": "ACM Trans. Math. Softw.,",
            "year": 2011
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "In ICLRWorkshop on Representation Learning on Graphs andManifolds,",
            "year": 2019
        },
        {
            "authors": [
                "Zhangxiaowen Gong",
                "Houxiang Ji",
                "Yao Yao",
                "Christopher W. Fletcher",
                "Christopher J. Hughes",
                "Josep Torrellas"
            ],
            "title": "Graphite: Optimizing graph neural networks on cpus through cooperative software-hardware techniques",
            "venue": "In Proceedings of the 49th Annual International Symposium on Computer Architecture,",
            "year": 2022
        },
        {
            "authors": [
                "Changwan Hong",
                "Aravind Sukumaran-Rajam",
                "Israt Nisa",
                "Kunal Singh",
                "P. Sadayappan"
            ],
            "title": "Adaptive sparse tiling for sparse matrix multiplication",
            "venue": "In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming,",
            "year": 2019
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "year": 2005
        },
        {
            "authors": [
                "Kezhao Huang",
                "Jidong Zhai",
                "Zhen Zheng",
                "Youngmin Yi",
                "Xipeng Shen"
            ],
            "title": "Understanding and bridging the gaps in current gnn performance optimizations",
            "venue": "In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Zhihao Jia",
                "Sina Lin",
                "Mingyu Gao",
                "Matei Zaharia",
                "Alex Aiken"
            ],
            "title": "Improving the accuracy, scalability, and performance of graph neural networks with roc",
            "venue": "In Proceedings of Machine Learning and Systems",
            "year": 2020
        },
        {
            "authors": [
                "Wengong Jin",
                "Kevin Yang",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Learning multimodal graph-to-graph translation for molecule optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Sam Kaufman",
                "Phitchaya Phothilimthana",
                "Yanqi Zhou",
                "Charith Mendis",
                "Sudip Roy",
                "Amit Sabne",
                "Mike Burrows"
            ],
            "title": "A learned performance model for tensor processing units",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "year": 2016
        },
        {
            "authors": [
                "J\u00e9r\u00f4me Kunegis",
                "Julia Preusse"
            ],
            "title": "Fairness on the web: Alternatives to the power law",
            "venue": "In Proceedings of the 4th Annual ACM Web Science Conference,",
            "year": 2012
        },
        {
            "authors": [
                "Chen Liang",
                "Ziqi Liu",
                "Bin Liu",
                "Jun Zhou",
                "Xiaolong Li",
                "Shuang Yang",
                "Yuan Qi"
            ],
            "title": "Uncovering insurance fraud conspiracy with network learning",
            "venue": "In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2019
        },
        {
            "authors": [
                "Ziqi Liu",
                "Chaochao Chen",
                "Xinxing Yang",
                "Jun Zhou",
                "Xiaolong Li",
                "Le Song"
            ],
            "title": "Heterogeneous graph neural networks for malicious account detection",
            "venue": "CIKM \u201918,",
            "year": 2018
        },
        {
            "authors": [
                "Lingxiao Ma",
                "Zhi Yang",
                "Youshan Miao",
                "Jilong Xue",
                "Ming Wu",
                "Lidong Zhou",
                "Yafei Dai"
            ],
            "title": "Neugraph: parallel deep neural network computation on large graphs",
            "venue": "{USENIX} Annual Technical Conference ({USENIX}{ATC}",
            "year": 2019
        },
        {
            "authors": [
                "KeMeng",
                "Jiajia Li",
                "Guangming Tan",
                "Ninghui Sun"
            ],
            "title": "A pattern based algorithmic autotuner for graph processing on gpus",
            "venue": "In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Shenghao Qiu",
                "Liang You",
                "Zheng Wang"
            ],
            "title": "Optimizing sparse matrix multiplications for graph neural networks",
            "venue": "Languages and Compilers for Parallel Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Md Rahman",
                "Majedul Haque Sujon",
                "Ariful Azad"
            ],
            "title": "Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks",
            "venue": "Proceedings of IEEE IPDPS,",
            "year": 2021
        },
        {
            "authors": [
                "John Thorpe",
                "Yifan Qiao",
                "Jonathan Eyolfson",
                "Shen Teng",
                "Guanzhou Hu",
                "Zhihao Jia",
                "JinliangWei",
                "Keval Vora",
                "Ravi Netravali",
                "Miryung Kim",
                "GuoqingHarry Xu"
            ],
            "title": "Dorylus: Affordable, scalable, and accurate GNN training with distributed CPU servers and serverless threads",
            "venue": "In 15th USENIX Symposium on Operating Systems Design and Implementation",
            "year": 2021
        },
        {
            "authors": [
                "Wen Torng",
                "Russ B. Altman"
            ],
            "title": "Graph convolutional neural networks for predicting drug-target interactions",
            "venue": "Journal of Chemical Information andModeling, 59(10):4131\u20134149,",
            "year": 2019
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Minjie Wang",
                "Lingfan Yu",
                "Da Zheng",
                "Quan Gan",
                "Yu Gai",
                "Zihao Ye",
                "Mufei Li",
                "Jinjing Zhou",
                "Qi Huang",
                "Chao Ma",
                "Ziyue Huang",
                "Qipeng Guo",
                "Hao Zhang",
                "Haibin Lin",
                "Junbo Zhao",
                "Jinyang Li",
                "Alexander Smola",
                "Zheng Zhang"
            ],
            "title": "Deep graph library: Towards efficient and scalable deep learning on graphs, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Yuke Wang",
                "Boyuan Feng",
                "Gushu Li",
                "Shuangchen Li",
                "Lei Deng",
                "Yuan Xie",
                "Yufei Ding"
            ],
            "title": "GNNAdvisor: An adaptive and efficient runtime system for GNN acceleration on GPUs",
            "venue": "In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI",
            "year": 2021
        },
        {
            "authors": [
                "Jaeyeon Won",
                "Charith Mendis",
                "Joel Emer",
                "Saman Amarasinghe"
            ],
            "title": "Waco: Learning workload-aware co-optimization of the format and schedule of a sparse tensor program",
            "venue": "In Proceedings of the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Yidi Wu",
                "Kaihao Ma",
                "Zhenkun Cai",
                "Tatiana Jin",
                "Boyang Li",
                "Chenguang Zheng",
                "James Cheng",
                "Fan Yu"
            ],
            "title": "Seastar: Vertex-centric programming for graph neural networks",
            "venue": "In Proceedings of the Sixteenth European Conference on Computer Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqiang Xie",
                "Minjie Wang",
                "Zihao Ye",
                "Zheng Zhang",
                "Rui Fan"
            ],
            "title": "Graphiler: Optimizing graph neural networks with message passing data flow graph",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mingyu Yan",
                "Zhaodong Chen",
                "Lei Deng",
                "Xiaochun Ye",
                "Zhimin Zhang",
                "Dongrui Fan",
                "Yuan Xie"
            ],
            "title": "Characterizing and understanding gcns on gpu",
            "venue": "IEEE Computer Architecture Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Serif Yesil",
                "Azin Heidarshenas",
                "Adam Morrison",
                "Josep Torrellas"
            ],
            "title": "Speeding up spmv for power-law graph analytics by enhancing locality and vectorization",
            "venue": "In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Serif Yesil",
                "Azin Heidarshenas",
                "Adam Morrison",
                "Josep Torrellas. Wise"
            ],
            "title": "Predicting the performance of sparse matrix vector multiplication with machine learning",
            "venue": "In Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming,",
            "year": 2023
        },
        {
            "authors": [
                "Hengrui Zhang",
                "Zhongming Yu",
                "Guohao Dai",
                "Guyue Huang",
                "Yufei Ding",
                "Yuan Xie",
                "Yu Wang"
            ],
            "title": "Understanding gnn computational graph: A coordinated computation, io, and memory perspective",
            "year": 2022
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen"
            ],
            "title": "Link prediction based on graph neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yue Zhao",
                "Jiajia Li",
                "Chunhua Liao",
                "Xipeng Shen"
            ],
            "title": "Bridging the gap between deep learning and sparse matrix format selection",
            "venue": "In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of graphs and embedding sizes show that SENSEi achieves geomean speedups of 1.105\u00d7 (up to 2.959\u00d7) and 1.187\u00d7 (up to 1.99\u00d7) on graph convolutional networks and geomean speedups of 2.307\u00d7 (up to 35.866\u00d7) and 1.44\u00d7 (up to 5.69\u00d7) on graph attention networks on CPUs and GPUs respectively over the widely used Deep Graph Library. Further, we show that the compositions yield notable synergistic performance benefits on top of other established sparse optimizations such as sparse matrix tiling by evaluating against a well-tuned baseline.\nACM Reference Format: Damitha Lenadora, Vimarsh Sathia, Gerasimos Gerogiannis, Serif Yesil, Josep Torrellas, and Charith Mendis. 20xx. Input-sensitive dense-sparse primitive compositions for GNN acceleration. In Proceedings of Proceedings of the Conference (Conf \u2019xx). ACM, New York, NY, USA, 12 pages. https: //doi.org/xx.xxxx/xxxxxxx.xxxxxxx\n\u2217Now at NVIDIA. He can be reached at syesil@nvidia.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conf \u2019xx, Month N-M, 20xx, Location \u00a9 20xx Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN xxx-x-xxxx-xxxx-x/xx/xx. . . $XX.xx https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Graph Neural Networks (GNN) have gained considerable adoption across a wide range of application domains including social media marketing [34], financial fraud detection [15, 16], drug discovery [11, 23], and systems optimization [12]. However, training GNNs is expensive and usually spans multiple hours if not days. As a result, there have been many efforts including software frameworks such as DGL [25], PyG [5], and NeuGraph [17], compilers such as Graphiler [29] and many systems optimization techniques [9, 10, 22, 33] that aim at accelerating GNN computations.\nUsually, these systems model GNN computations as individual phases consisting of sparse or dense matrix operations. In a typical GNN layer, each node collects its neighbor node states (embeddings) and aggregates them. Next, each node updates its own state using these aggregated embeddings. The first computation is modeled as a set of sparse matrix operations and the latter is a dense matrix multiplication. Apart from these phases, different GNNs perform other required computations such as normalization and edge weight (attention score) calculations that are also modeled as either sparse or dense matrix operations. The aforementioned frameworks, compilers and optimization techniques mainly optimize these computations as individual phases and do not get consistent performance across graphs with different sparsity patterns and GNN embedding sizes.\nComparatively, in this work we specifically investigate different algebraic reassociations in GNN computations. These associations lead to interesting sparse and dense matrix primitive selections and compositions that may be profitable depending on the input graph structure and GNN embedding sizes. We introduce such input-sensitive compositions to GNN computations and propose an adaptive machine learning based strategy to select the most profitable compositions depending on the input graph and embedding sizes.\nThere has been recent work that performs dynamic optimizations specialized to a given input in GNNs. GNNAdvisor [26] considers the input graph for making the decision to perform reordering and selecting the optimum set of parameters for the computations that it performs on GPUs. [20] uses machine learning to select the best data representation for the sparse computation in GNNs. Meanwhile, [30] uses the input and output embedding sizes of GNN layers to decide between sparse and dense primitive ordering. However, all of these techniques limit their adaptive decisions to\nar X\niv :2\n30 6.\n15 15\n5v 1\n[ cs\n.L G\n] 2\n7 Ju\nn 20\none aspect of input sensitivity: whether it be the input graph or the embedding size, and miss the type of dense-sparse primitive compositions explored in this paper whose profitability depends on the combination of the input graph, GNN embedding size as well as the underlying hardware.\nOur techniques are motivated by a key observation. Depending on the graph sparsity pattern and the embedding sizes of GNN layers, the bottleneck of the GNN computation can shift between the sparse matrix and the dense matrix operations. Table 1 shows the breakdown of runtimes for the sparse matrix (SpMM) and dense matrix (GEMM) operations as the input graph and embedding sizes change on graph convolution networks [13], which is a popular GNN architecture. Note that in this scenario we consider the same input and output embedding sizes for a GNN layer. For much denser graphs (e.g. Reddit), sparse operations dominate the runtime, and for much sparser graphs (e.g. asia_osm), dense computations dominate. Also, as the embedding size increases the bottleneck can shift from sparse to dense (e.g. com-Liv). Therefore, it may be beneficial to use different algebraic reassociations that lead to different compositions of dense-sparse primitives for certain GNN computations depending on the input.\nWe leverage this observation to present novel input-sensitive dense-sparse primitive compositions targeted at two popular GNN models: graph convolution networks (GCN) [13] and graph attention networks (GAT) [24]. In the GCN case, we propose to either perform normalization as a standalone phase (precompute) or as a part of the aggregate and update computations (dynamic). In the GAT case, we propose to either reuse updated embeddings from the attention calculation phase (reuse) or recompute them dynamically (recompute). The computation of each method requires a different composition of dense-sparse primitives and as such the profitability of each depends on how fast each primitive executes. As shown in Table 2 and 3, the composition selection has a direct impact on the runtime performance of these GNNs. Also note that these compositions are not limited to GCN and GAT and can be generalized to other models. Furthermore, techniques such as sampling are orthogonal to the compositions presented in this work and can work alongside them to optimize GNN performance. We bring this topic into discussion in section 7.\nTo this end, we developed SENSEi, a system to decide on which composition to use under each scenario depending on the input graph and GNN embedding sizes. Our evaluations (Section 6.6) show that selecting the best composition in every case just based on a single hard-coded metric is sub-optimal. Therefore, SENSEi uses an adaptive machine learning model that is parameterized based on the input graph features and GNN embedding size to decide which composition to use to achieve runtime speedups. We train the model using a dataset of apriori collected executions and show that SENSEi is able to predict with high accuracy the best dense-sparse primitive composition to use.\nWe extended the widely adopted GNN framework, Deep Graph Library (DGL) [25] to allow for the different primitive composition choices. We use SENSEi to select the best composition choice for both GCN and GAT under inference and training running on both CPUs and GPUs. Our evaluation reveals that SENSEi is effective in all scenarios. For example, SENSEi achieves up to 1.99\u00d7 over DGL in GCN inference and up to 5.69\u00d7 over DGL in GAT inference on\nGPUs with very little overhead over a wide variety of input graphs with different sparsity patterns. Section 6.3 gives detailed results for each setting.\nFurther, we investigated the behavior of the compositions in the presence of other established optimizations such as sparse matrix tiling. Since DGL does not support optimizations such as tiling, we implemented the compositions on top of an optimized CPU implementation that supports advanced hardware-aware optimizations. We then investigated the benefits of selecting the most profitable primitive compositions. Our results show clear synergistic performance improvements even when other optimizations are applied. Overall, SENSEi shows that selecting the best dense-sparse primitive composition adaptively based on the input leads to sizable performance improvements in GNN computations across different hardware platforms and different software implementations.\nIn this paper, we make the following specific contributions.\n\u2022 We introduce algebraic reassociations that lead to novel input-sensitive primitive compositions involving both sparse and dense matrix primitives for popular GNN models GCN and GAT. \u2022 We present SENSEi, a system that uses a machine learning based strategy to select the best composition given an input graph and the GNN embedding sizes. \u2022 We evaluate SENSEi on a wide variety of graphs with different sparsity patterns and on a wide range of embedding sizes and show that it predicts the best primitive composition to use on both CPUs and GPUs with high accuracy. \u2022 We implement the compositions on top of DGL and show that SENSEi\u2019s decisions lead to 1.105\u00d7 (up to 2.959\u00d7) and 1.187\u00d7 (up to 1.99\u00d7) geomean speedup for GCN, and 2.307\u00d7 (up to 35.866\u00d7) and 1.44\u00d7 (up to 5.69\u00d7) geomean speedup for GAT for CPU and GPU respectively. We further implement the compositions in separate optimized baselines to investigate the synergies with other optimizations such as sparse tiling. Our results show that SENSEi is able to achieve up to 1.218\u00d7 speedup for GCN, and up to 2.94\u00d7 speedup for GAT on CPU."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We first briefly introduce the sparse matrix primitives and the GNN computations of GCN and GAT to facilitate the discussion of different dense-sparse primitive compositions in Section 3."
        },
        {
            "heading": "2.1 Sparse Matrix Primitives",
            "text": "Computations used in GNN models can be modeled using both dense and sparse matrix computations [25]. Dense matrix computations mainly consist of the general matrix multiplication (GEMM) that is widely found in other neural network models. The sparse computations mainly consist of two primitives. These are the generalized forms of sparse matrix dense matrix multiplication (SpMM) and sampled dense dense matrix multiplication (SDDMM). The standard SpMM and SDDMM use + and \u00d7 as the addition and multiplication operators, whereas in the generalized form, these can come from any semi-ring [3]. We use \u2295 and \u2297 to signify these generalized addition and multiplication operators. Also, we use the notation \ud835\udc4e\ud835\udc56, \ud835\udc57 to refer to the value at row \ud835\udc56 and column \ud835\udc57 of a given matrix \ud835\udc34.\nGeneralized sparse matrix dense matrix multiplication (g-SpMM). g-SpMM takes one input sparse matrix \ud835\udc34 and multiplies (\u2297) that by an input dense matrix \ud835\udc35 to produce a dense output matrix \ud835\udc36 . Equation 1 shows how \ud835\udc50\ud835\udc56,\ud835\udc58 of \ud835\udc36 is computed.\n\ud835\udc50\ud835\udc56,\ud835\udc58 = \u2211\ufe01 \u2295, \ud835\udc57 \ud835\udc4e\ud835\udc56, \ud835\udc57 \u2297 \ud835\udc4f \ud835\udc57,\ud835\udc58 (1)\nThe computation mirrors that of GEMM. However, note that when \ud835\udc4e\ud835\udc56, \ud835\udc57 is zero, there is no contribution to the output. Since \ud835\udc34 is sparse, the majority of its entries are zeroes. Thus, SpMM leverages the sparsity of input matrix \ud835\udc34 to skip any multiplications without contributions compared to GEMM.\nGeneralized sampled dense densematrixmultiplication (g-SDDMM). g-SDDMM performs a computation between two dense input matrices \ud835\udc35 and \ud835\udc36 controlled by the non-zero (nnz) values of a sparse matrix \ud835\udc34. This results in an output sparse matrix \ud835\udc37 with a sparsity pattern matching that of \ud835\udc34. Equation 2 shows how each element \ud835\udc51\ud835\udc56, \ud835\udc57 of \ud835\udc37 is computed.\n\ud835\udc51\ud835\udc56, \ud835\udc57 =  \ud835\udc4e\ud835\udc56, \ud835\udc57 \u2297 \u2211\ufe01 \u2295,\ud835\udc58 \ud835\udc4f\ud835\udc56,\ud835\udc58 \u2297 \ud835\udc50 \ud835\udc57,\ud835\udc58 , if \ud835\udc4e\ud835\udc56, \ud835\udc57 not 0 0, otherwise (2)\nIn g-SDDMM, a multiply (\u2297) operation is performed between \ud835\udc4f\ud835\udc56,\ud835\udc58 and \ud835\udc50 \ud835\udc57,\ud835\udc58 , for all \ud835\udc58 if and only if the value of \ud835\udc4e\ud835\udc56, \ud835\udc57 is non-zero. The partial products are then summed up (\u2295) and multiplied with \ud835\udc4e\ud835\udc56, \ud835\udc57 to form the \ud835\udc51\ud835\udc56, \ud835\udc57 scalar value. This, in a sense, is again similar to GEMM with two dense input matrices. However, in g-SDDMM, the input sparse matrix is used as a mask to selectively compute values instead of performing the entire dense matrix multiplication."
        },
        {
            "heading": "2.2 Graph Neural Networks",
            "text": "Graph neural networks consist of two main stages: aggregation and update. Each node or edge has an embedding modeled as a hidden state vector. During the aggregation stage, these embeddings are passed among neighbors to form an aggregated message. In the update stage, these messages are then transformed into updated embeddings as the GNN layer\u2019s output. Usually, these computations are modeled as a collection of sparse and dense primitive operations [25]. Node-based aggregations are modeled as g-SpMM operations, while edge-based aggregations are modeled as g-SDDMM operations. The update is modeled as a GEMM computation.\nApart from these stages, there are other required computations such as normalization that open up more opportunities to consider different algebraic reassociations leading to different dense-sparse primitive compositions. In this work, we focus on two popular GNN models \u2013 GCN and GAT \u2013 to showcase such compositions that can lead to performance benefits. Note that, these compositions are not limited to GCN or GAT and can be generalized to other GNN models with normalization and edge-weight calculations. We now present both GCN and GAT computations in matrix form.\nGraph Convolutional Networks. The inference computation of a Graph Convolutional Netowk (GCN) [13] for an undirected graph on the (\ud835\udc59)\ud835\udc61\u210e layer is presented by Equation 3.\n\ud835\udc3b (\ud835\udc59 ) = \ud835\udf0e (?\u0303?\u2212 1 2 \u00b7 ?\u0303? \u00b7 ?\u0303?\u2212 1 2 \u00b7 \ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) ) (3)\nHere, ?\u0303? is the adjacency matrix of the graph augmented with selfedges. ?\u0303? is usually represented as a sparse matrix and its sparsity pattern depends on the topology of the graph. ?\u0303? is the degreematrix of this augmented graph and is represented as a diagonal matrix.\n\ud835\udc3b (\ud835\udc59 ) ,\ud835\udc4a (\ud835\udc59 ) are the node embeddings and weights for the \ud835\udc59\ud835\udc61\u210e layer in the GNN respectively and both are represented as dense matrices. After performing the necessary computations between the aforementionedmatrices, the non-linearity function \ud835\udf0e is applied to the result.\nGraph Attention Network. The primary computation of a singleheaded Graph Attention Network (GAT) [24] model can be represented as follows.\n\ud835\udefc (\ud835\udc59 ) = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc59\ud835\udc50 (?\u0303?, \ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) ,\ud835\udc4a\ud835\udc34 (\ud835\udc59 ) ) (4)\n\ud835\udc3b (\ud835\udc59 ) = \ud835\udf0e (\ud835\udefc (\ud835\udc59 ) \u00b7 \ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) ) (5) A graph edge processing function (AttenCalc) is used to produce the attention scores in GAT. For this, the updated input embeddings of the nodes (\ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) ), and the weights used for attention \ud835\udc4a\ud835\udc34\n(\ud835\udc59 ) are necessary. In addition, we also pass along ?\u0303? to avoid unnecessary computations by not computing attention scores between nodes without connecting edges, similar to the process mentioned in the original GAT [24] paper. The result is a sparse matrix \ud835\udefc containing the necessary attention scores and having the same non-zero distribution as the adjacency matrix ?\u0303?. This result is then used for the aggregation and update stages, as shown by Equation 5. An eye trained for optimization would see that \ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) is calculated multiple times (in both Equations 4 and 5). Thus, it would be obvious to reuse the computed result. However, we show in Section 3.2 that this is not always the case and that it depends on the input. For this work in particular, we focus on single-headed GAT, but we believe that the same conclusion can be extended to multi-headed GAT."
        },
        {
            "heading": "3 GNN DENSE-SPARSE PRIMITIVE COMPOSITIONS",
            "text": "As shown through the examples in Table 1, the most dominant computation in a GNN can be either sparse or dense primitives. This is determined by the embedding sizes of the GNN model as well as the input graph. Through algebraic reassociations, we were able to identify two input-sensitive primitive compositions for both\nGCN and GAT that may be profitable depending on which primitives are cheaper. In the rest of the section, we elaborate on these compositions and give evidence of their input sensitivity. We use these observations to design an input-sensitive learned decision system SENSEi as detailed in Section 4 that achieves considerable performance benefits in both GCN and GAT computations."
        },
        {
            "heading": "3.1 GCN: Normalization",
            "text": "Normalization in GCN for an un-directed graph occurs through ?\u0303?\u2212 1 2 \u00b7 ?\u0303? \u00b7 ?\u0303?\u2212 1 2 . Incorporating the aforementioned calculation into the GCN computation shown by equation 3, is possible through two different methods comprising of different dense-sparse primitives. This variation is possible due to the associative property of the computations involved.\n?\u0303? = (?\u0303?\u2212 1 2 \u00b7 ?\u0303? \u00b7 ?\u0303?\u2212 1 2 )\ufe38 \ufe37\ufe37 \ufe38\nSDDMM\n\ud835\udc3b (\ud835\udc59 ) = \ud835\udf0e (?\u0303? \u00b7 \ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 )\ufe38 \ufe37\ufe37 \ufe38 SpMM, GEMM ) (6)\nPrecomputation based primitive composition. We first precompute the normalized adjacency matrix ?\u0303? as shown in Equation 6 independently of any specific layer in the GCN, and then reuse the result during the aggregate stage of the model. The initial precomputation is possible through a g-SDDMM operation with the semirings used being the same as regular SDDMM. To elaborate, due to ?\u0303?\u2212 1 2 being a diagonal matrix, ?\u0303?\u2212 1 2 \u00b7 ?\u0303? is essentially an in-place multiplication of each value in row \ud835\udc56 in ?\u0303? with the \ud835\udc56\ud835\udc61\u210e element in the diagonal of ?\u0303?\u2212 1 2 . Similarly, ?\u0303? \u00b7 ?\u0303?\u2212 1 2 is essentially an in-place multiplication of each value in column \ud835\udc57 in ?\u0303? with the \ud835\udc57\ud835\udc61\u210e element in the diagonal of ?\u0303?\u2212 1 2 . Thus, the value of an element in ?\u0303? can be represented as in Equation 7.\n?\u0303?\ud835\udc56, \ud835\udc57 = \ud835\udc4e\ud835\udc56, \ud835\udc57 \u00d7 (\ud835\udc51 \u2212 12 \ud835\udc56 \u00d7 \ud835\udc51\u2212 1 2 \ud835\udc57 ) (7)\nThis computation in Equation 7 matches the one for g-SDDMM as presented in Equation 2 once appropriate semirings are assigned and assuming that \ud835\udc58 = 1. The normalization computation can thus be achieved through g-SDDMM, with the dense vector ?\u0303?\u2212 1 2 serving as the two dense inputs and ?\u0303? as the sparse input. We then use the precomputed result ?\u0303? during the g-SpMM used for aggregation to incorporate the normalization into the final output.\nDynamic normalization based primitive composition. Here, we perform normalization through two GEMM operations without needing to rely on a sparse computation as shown by Equation 8. We initially perform a GEMM operation to incorporate the normalization factor (?\u0303?\u2212 1 2\ud835\udc3b (\ud835\udc59 ) ) into the node embeddings. Then following the aggregation and update stages, another GEMM operation is performed to incorporate the second normalization factor (?\u0303?\u2212 1 2\ud835\udc3b (\ud835\udc59 ) ) into the node embeddings. Through these two GEMM primitives, it is possible to perform the normalization without the need for a sparse primitive. However, unlike the precomputation method, this normalization calculation must be repeated during each layer and iteration.\n\ud835\udc3b (\ud835\udc59 ) = \ud835\udf0e (?\u0303?\u2212 1 2 \u00b7 (?\u0303? \u00b7 (?\u0303?\u2212 1 2 \u00b7 \ud835\udc3b (\ud835\udc59\u22121) )\ufe38 \ufe37\ufe37 \ufe38 GEMM \u00b7\ud835\udc4a (\ud835\udc59 ) )\n\ufe38 \ufe37\ufe37 \ufe38 SpMM, GEMM\n)\n\ufe38 \ufe37\ufe37 \ufe38 GEMM\n(8)\nDecision points and insights. The two forms of primitive compositions result in different execution times based on the input graph and on the model\u2019s input and output embedding sizes. A snippet of the observations is presented in Table 2.\nConsidering unweighted graphs, in the dynamic primitive composition, one can make use of the fact that all nnz values of the original matrix ?\u0303? are equal to one. Due to this fact, during aggregation, it is possible to completely avoid reading the value of an edge as shown in Equation 9. This, naturally, reduces the memory footprint of the sparse operation. Thus, although the dynamic primitive composition requires two additional GEMM computations, the GEMM overhead might be compensated by the reduced load in one of the major sparse operations. Using this point, we can explain the observation in Table 2: denser graphs (mycielskian17) achieve higher performance with the dynamic primitive composition as their major bottleneck is the sparse computation (recall the GEMM-SpMM runtime comparison of Table 1). However, the overhead of adding additional GEMM computations outweighs any benefit for the sparse primitives when the sparse asia_osm graph is considered.\n\u210e (\ud835\udc59 ) \ud835\udc56,\ud835\udc58 = \u2211\ufe01 \u2295, \ud835\udc57 1 \u2297 \u210e (\ud835\udc59\u22121) \ud835\udc57,\ud835\udc58 = \u2211\ufe01 \u2295, \ud835\udc57 \u210e (\ud835\udc59\u22121) \ud835\udc57,\ud835\udc58\n(9)\nThere is an interesting observation when the input embedding size (\ud835\udc581), is different from the output embedding size (\ud835\udc582) of a GNN layer. We use the technique suggested in [30] to select the order of aggregation and update phases. This order is selected so that the smaller embedding size is used for aggregation. As a result, in both our precompute and dynamic primitive compositions, the g-SpMM operation is performed on an embedding size which is \ud835\udc5a\ud835\udc56\ud835\udc5b(\ud835\udc581, \ud835\udc582). However, in the dynamic composition, the additional GEMM computations operate on embedding sizes of both \ud835\udc581 and \ud835\udc582. Thus, if there is a difference in embedding size (\ud835\udc581 < \ud835\udc582, \ud835\udc581 > \ud835\udc582), the dynamic composition will need to perform a GEMM on an embedding size larger than the one used for the SpMM operation. This may be more costly than the time saved by aggregation. Because of this, when different input and output embedding sizes are used, the precompute-based primitive composition becomes more profitable in comparison to a scenario with equal embedding\nsizes (\ud835\udc581 = \ud835\udc582). This is illustrated in Table 2 where for packing500x100x100-b050, the optimal primitive composition changes from dynamic to precompute when the embedding sizes differ."
        },
        {
            "heading": "3.2 GAT: Recomputation vs. Reuse",
            "text": "In GAT models we found two different primitive compositions that are related to the reuse of updated embeddings (\ud835\udc3b \u00b7\ud835\udc4a ). Assume a GAT layer with input embedding size \ud835\udc581 and output embedding size \ud835\udc582. During the attention score calculation, we compute the multiplication of the input embedding with the relevant weights (\ud835\udc3b \u00b7\ud835\udc4a ) to produce the updated embeddings as shown by equation 10. This attention calculation step is common for both primitive compositions. The difference lies in the decision taken during aggregation.\n\ud835\udefc (\ud835\udc59 ) \ud835\udc5b\u00d7\ud835\udc5b = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc59\ud835\udc50 (?\u0303?\ud835\udc5b\u00d7\ud835\udc5b, \ud835\udc3b (\ud835\udc59\u22121) \ud835\udc5b\u00d7\ud835\udc581 \u00b7\ud835\udc4a (\ud835\udc59 ) \ud835\udc581\u00d7\ud835\udc582\ufe38 \ufe37\ufe37 \ufe38\n(\ud835\udc3b \u00b7\ud835\udc4a ) (\ud835\udc59 ) \ud835\udc5b\u00d7\ud835\udc582\n,\ud835\udc4a (\ud835\udc59 ) \ud835\udc34 \ud835\udc582\u00d71) (10)\n\ud835\udc3b (\ud835\udc59 ) = SpMM_k2\ufe37 \ufe38\ufe38 \ufe37 \ud835\udefc (\ud835\udc59 ) \ud835\udc5b\u00d7\ud835\udc5b \u00b7 (\ud835\udc3b \u00b7\ud835\udc4a )\n(\ud835\udc59 ) \ud835\udc5b\u00d7\ud835\udc582\ufe38 \ufe37\ufe37 \ufe38\nreuse\n(11)\n\ud835\udc3b (\ud835\udc59 ) =\nGEMM\ufe37 \ufe38\ufe38 \ufe37 SpMM_k1\ufe37 \ufe38\ufe38 \ufe37 (\ud835\udefc (\ud835\udc59 )\ud835\udc5b\u00d7\ud835\udc5b \u00b7 \ud835\udc3b (\ud835\udc59\u22121) \ud835\udc5b\u00d7\ud835\udc581 ) \u00b7\ud835\udc4a\n(\ud835\udc59 ) \ud835\udc581\u00d7\ud835\udc582\ufe38 \ufe37\ufe37 \ufe38\nrecomputation\n(12)\nReuse based primitive composition. Shown by Equation 11, this primitive composition focuses on reducing redundant computation. During aggregation, it simply reuses the updated embeddings that have already been computed in the attention calculation stage.\nRecomputation based primitive composition. Shown by Equation 12, this primitive composition forgoes the reuse of the updated embeddings and uses the original embeddings for aggregation. However, this requires an additional update to the embeddings through a GEMM computation.\nDecision points and insights. Which method is more profitable depends on both \ud835\udc581, \ud835\udc582 and the input graph. When \ud835\udc581 > \ud835\udc582 or \ud835\udc581 = \ud835\udc582 it is always beneficial to reuse the updated embeddings from the attention calculation stage. On the other hand, when \ud835\udc581 < \ud835\udc582, recomputation may be more beneficial. This is because the SpMM operation will be performed on the smaller embedding size (\ud835\udc581). However, recomputation will only make sense if the time it takes for the additional GEMM is less than the time saved by performing the SpMM operation at embedding size \ud835\udc581. This is often the case with graphs with a larger number of edges compared to nodes (comparatively denser). Thus, when the difference in embedding size between \ud835\udc581, \ud835\udc582 grows larger, recomputation would become more beneficial. However, if the relative difference between the embeddings stays the same but with both \ud835\udc581, \ud835\udc582 increasing, then reuse may become more beneficial.\nTable 3 presents the breakdown of the different computations involved under both methods for graphs with different sparsity patterns and conforms with the hypothesis stated prior. mycielskian17\n(comparatively denser) graph sees a considerable benefit with recomputation compared to reuse, as its SpMM operation is more expensive than the GEMM. For asia_osm (comparatively sparser), even though it initially showed benefits from recomputation, as the embedding size increases, reuse becomes better."
        },
        {
            "heading": "4 SENSEI SYSTEM",
            "text": "We present SENSEi: an input-sensitive decision-making system, that exploits the input-sensitivity of primitive compositions derived from different algebraic reassociations to achieve performance benefits in GNN computations."
        },
        {
            "heading": "4.1 Overview",
            "text": "The architecture of the proposed system, SENSEi, is shown in Figure 1(a). At a high level, SENSEi consists of two stages, the offline learning stage (Figure 1(b)) and the online execution stage (Figure 1(c)). In the learning stage, we train a ranking model based on profiling data gathered on many GNN execution configurations for the different primitive compositions suggested in Section 3. Then in the execution stage, given the characteristics of the input graph, GNN model architecture, as well as primitive compositions, the leaned model chooses the best among the compositions presented. This primitive composition is then used by the GNN execution system."
        },
        {
            "heading": "4.2 Ranking model for composition selection",
            "text": "SENSEi uses a tree-based XGBoost ranking model [2], which we name the composition selector, that predicts the best composition for a given input graph, embedding sizes, setting of auxiliary optimizations (e.g. sparse matrix tiling), and the underlying hardware architecture.\n4.2.1 Graph feature extractor. We handcraft a set of features from the input graph as listed under Table 4. We could have used a neural network such as a sparse convolutional network[35] to automatically extract features, but this would hinder the scalability of SENSEi. One of the goals of designing SENSEi was to facilitate large graphs such as ogbn-products (2M nodes and 126M non-zeros). However, NN-based automatic feature extractors do not scale or are too expensive to use in this setting [27]. As shown in Section 6.5, the overhead of the entire SENSEi system is very minimal. The graph feature extractor, given the input graph, computes the features in Table 4 and passes it along to the learned model to be used in either training or inference.\n4.2.2 Composition selector. We use an XGBoost [2] tree model as the learned composition selector. The selector expects the graph features, model architecture, auxiliary optimization configurations (if any), and primitive composition considered as input and provides a relative ordering of the profitability of each different choice. Since we consider two primitive compositions for each GNN model, the selector\u2019s decision is binary. We create a separate selector for each GNN considered by SENSEi. This allows us to easily add specialized GNN compositions for each GNN model.\n4.2.3 Profile Data Collection. We collected many profiling runs to train the composition selector. Using a predetermined set of graphs and embedding sizes, we obtain their runtimes offline for\nthe different primitive compositions. The graphs and embedding sizes chosen for the profiling must be representative of the graphs and embedding sizes SENSEi would use to later make decisions on during execution."
        },
        {
            "heading": "4.3 Execution system",
            "text": "Once a decision is made by SENSEi, it is passed to the system that will execute it. We implemented the GNN primitive compositions mentioned in Section 3, and enable the composition selection as a parameterized option. In its current state SENSEi provides decisions for two execution systems. One is implemented on top of the popular GNN framework DGL [25], and the other is a well-tuned implementation with traditional sparse optimizations.\nDGL execution system (DGL). We implemented the different primitive compositions using the node and edge primitive functions of DGL. We show the effect of SENSEi\u2019s decisions in Section 6.3.\nOptimized execution system (OPT). We implemented a tuned baseline of the GNNs augmented with other sparse optimizations in CPU, to confirm that the primitive compositions that we present see benefits not only in DGL, but can also give synergistic benefits with other optimized kernels as well. We present an evaluation for this execution system in Section 6.4.\nWe considered two sparse optimizations found in literature for this execution system: sparse tiling and reordering. For tiling, we first partition the input sparse matrix into column segments and then perform tiling based on rows. Depending on the tiling factor, the execution can lead to either excessive slowdowns or considerable speedups (3\u00d7) \u2013 whether the optimizations are beneficial or not is determined by the input graph. Similarly, the benefit of reordering is also input-sensitive. This depends on the nnz distribution of the input graph. This nnz distribution may already maximize data locality without reordering needed. Performing reordering in such cases presents the possibility that the resulting nnz distribution may be worse than the original, leading to sub-optimal performance during sparse computations.\nSince these two optimizations are input-sensitive, we auto-tuned them to achieve an optimized baseline based on the input graph. We select the optimal configuration of sparse tile size and whether to perform reordering or not using a ranking model similar to the selector used by SENSEi. We select the top 20 suggested by the ranker and perform an empirical auto-tuning to identify the best one. We then feed this selection into SENSEi as the optimization configuration."
        },
        {
            "heading": "5 IMPLEMENTATION",
            "text": "In this section, we explain our training setup and how we implemented the feature extractor and optimized execution system."
        },
        {
            "heading": "5.1 Training",
            "text": "We collect the training data necessary for the model by profiling different optimization configurations on the machines listed below. We also use the same machines for evaluation.\n\u2022 CPU - Intel Xeon Gold 6348: (L2 cache 1.28MB, LLC 86MB), RAM 1TB, No GPU \u2022 GPU - A100 GPU: (8GB), Intel Xeon Platinum 8358, RAM 256GB We use both machines to collect training data for the evaluation of the DGL execution system but focused on CPU for the optimized execution (OPT) system. We source the input graphs from the SuiteSparse matrix collection. Here, we choose all undirected graphs ranging from 10 million to 25 million non-zero values. Considering the DGL execution, we varied different input and output embedding sizes, and primitive compositions to obtain approximately 625 and 250 data points for each machine. For OPT, in addition to the parameters varied previously we also considered different tiling configurations and reordering. This led to approximately 38,000 and 19,000 data points collected for GCN and GAT respectively on CPU.\nAfter obtaining all the necessary data, we train the XGBoost model using the\ud835\udc54\ud835\udc5d\ud835\udc62_\u210e\ud835\udc56\ud835\udc60\ud835\udc61 treemethod,\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc61 booster, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 : \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 objective. This choice is common for the models of both systems. For the DGL models we used 300 estimators and 0.001 learning rate, and for the OPT models, we used 410 estimators and 0.05 learning rate. In particular for the DGL models we combined the data for both machines to create a model that is common for all machines due to the low amount of data points. We differentiate each machine by passing additional parameters such as machine architecture type, LLC size, etc, to the model."
        },
        {
            "heading": "5.2 Feature extractor and optimized system",
            "text": "The graph feature extractor and OPT were implemented in C++. The profiling data were collected utilizing code generated by #ifdef macros in C++ to easily compose optimization choices. However, the executor itself was built in a parameterizedmanner to reduce the overhead of not only preprocessing but also repeated compilation. We used the BLAS GEMM function from MKL and a third-party library for reordering1."
        },
        {
            "heading": "6 EVALUATION",
            "text": "We evaluate SENSEi by comparing against the default dense-sparse primitive composition for GNN inference and training found in DGL. For this evaluation, we test across various graphs (1 million to 126 million non-zeros with different sparsity patterns) in both CPU and GPU platforms. We used multiple combinations of input and output embedding sizes to showcase different trends in the performance of different input-sensitive compositions. We present detailed setups used for evaluation, along with performance results and various analyses in the subsequent sections."
        },
        {
            "heading": "6.1 Research questions",
            "text": "We aim to answer the following research questions through the evaluations done.\n1Rabbit reordering\n(1) Does SENSEi get performance benefits for GCN and GAT on a diverse set of input graphs and embedding sizes across multiple hardware platforms? We evaluate SENSEi on the DGL execution system to answer this question (Section 6.3). (2) Does SENSEi get synergistic benefits over some optimized baselines?We evaluate SENSEi on the OPT execution system to answer this question (Section 6.4). (3) What are the overheads of SENSEi? We compare the additional time needed for any information gathering as well as preprocessing (Section 6.5). (4) How good is the learned model of SENSEi and what features are important when making decisions? We evaluate the decisions made by the learned model and compare them against the ground truth speedups of the best primitive compositions made by an oracle (Section 6.6).\nIn Section 6.2, we describe the detailed experimental setup used to answer these questions."
        },
        {
            "heading": "6.2 Experimental Setup",
            "text": "We use the Pytorch [19] backend of DGL [25] for our comparison studies. We evaluate SENSEi on the machines listed in Section 5.1. The graphs used for evaluations are shown in Table 5. We selected the graphs to encompass multiple variations of non-zero distributions as well as scale. The evaluation set contains graphs ranging from 1 million non-zeros up to over 126 million. Among the various graph types that are included are road graphs (asia_osm), banded diagonal graphs (packing..b050), and power law graphs(Reddit). We sourced these graphs from three main locations. These were SuiteSparse (SS) [4]: which provided a plethora of graphs of varying characteristics, DGL and Open Graph Benchmark (OGB) [8]: which provided graphs commonly used in the GNN context. Similar to the training data gathered, the graphs collected for the evaluation were undirected. In contrast to the graphs used for training, those selected for the evaluation possessed non-zeros ranging from below the minimum and the maximum found in the training dataset. This allows us to evaluate the generalization capability of SENSEi\u2019s learned selector.\nWe use a single layer GNN for evaluation, and Table 6 shows the combinations of input-output embedding sizes that we tested. An extension to a multi-layered GNN is straightforward by making decisions for each individual layer separately. Also, we only evaluate\nincreasing embedding sizes for GAT as this is the scenario in which the primitive composition choice is non-trivial. We also consider all of the graphs as unweighted for the evaluations."
        },
        {
            "heading": "6.3 Performance Comparison with DGL",
            "text": "Figures 2 and 3 present the speedups observed against the default execution of primitive compositions in DGL for GCN and GAT respectively for CPUs and GPUs under both inference and training. We run each configuration for 100 iterations to calculate the speedups. The summarized results of this evaluation are shown in Table 7. In summary, the results show that SENSEi is able to speedup most configurations by selecting the best primitive composition. Also note that SENSEi\u2019s decisions vary based on the input graph and embedding sizes showing that it is important to decide the composition based on the input.\nFor denser graphs used in GCN, SENSEi found DGL\u2019s default method (dynamic) to be the most beneficial. However, SENSEi\u2019s decisions differed for sparser graphs where the selection of the precompute primitive composition gave significant speedups. An example of a denser graph in this context would be Reddit possessing a density of 2 \u00d7 10\u22123, while a sparser graph would be asia_osm possessing a density of 1.78\u00d710\u22127. This result is somewhat obvious for sparser graphs as the dense computation is much more significant. Thus, the dynamic primitive composition with its additional GEMM computations is sub-optimal, and running the precompute primitive composition would be more beneficial. SENSEi manages to correctly identify such points and make the correct decision between primitive composition choices. We observe higher speedups\nfor the precompute primitive composition as the difference between input and output embedding sizes increases. This is attributed to the GEMM computation with the larger embedding size, which has a greater impact on the overall computation.\nIn GAT, we observe that for the majority of the configurations evaluated, SENSEi decided on the recomputation composition as the optimal. However, for larger embedding sizes of sparse graphs, the reuse composition was chosen as seen with belgium_osm for the embedding size of 1024, 2048. This is due to the dense computation becoming more prominent the sparser the graph is and the larger the embedding sizes are. In such situations, performing the additional GEMM computation to recompute the updated embeddings becomes too costly. In contrast, the more significant the sparse computation is as a bottleneck (as in denser graphs), the more speedup the GAT recomputation method would achieve. The accurate identification of the appropriate composition is enabled by the input sensitivity of SENSEi, which accounts for the significance of both the graph and the embedding sizes when making a decision. Furthermore, with increasing embedding sizes the speedups observed from recomputation are larger. This is clearly seen by the high speedups observed for both Reddit and mycielskian17 graphs for the embedding size of 32, 2048. This is due to the greater time savings achieved from performing the aggregation with the smaller embedding size.\nThe slowdowns observed in the evaluations were due to SENSEi making incorrect choices. Although SENSEi is not perfect, slowdowns are observed in only 6.2% out of all the configurations evaluated. Furthermore, these slowdowns are only up to 1.36\u00d7 of the original DGL execution.\nWe observe that speedups for GAT are much greater than GCN. This is because the differences in primitive compositions for GCN are mainly based on the balancing of memory use and computation to achieve the best performance, while the primitive compositions of GAT directly lead to significant reductions in computation. The speedups observed during inference are greater than training in GCNs. The additional computations that occur during the training process that we do not optimize for could be the reason for this, as the contribution to the overall speedup is less compared to inference. However, the opposite is observed for GAT, where larger speedups for training are seen. This could point to the recomputation primitive composition affecting the backward pass as well, resulting in an overall speedup. Although the aforementioned observation highlights general trends, making the correct choice is non-trivial. We present further analysis in Section 6.6."
        },
        {
            "heading": "6.4 Synergy with other optimizations",
            "text": "We evaluated SENSEi\u2019s decision-making capabilities on the OPT execution system. Here, given an optimized execution, SENSEi decides the primitive composition to execute. We used a subset of the graphs and embedding sizes from Section 6.3 as points to show the synergy between the traditional sparse optimizations and the primitive compositions. Figure 4 shows the comparisons against both the default DGL execution, as well as the optimized implementation with SENSEi\u2019s primitive composition decisions.\nWe observe sizable speedups compared to DGL from the classical optimizations alone. These speedups range from a minimum\n1.82\u00d7 of up to 4.881\u00d7, and a minimum of 1.63\u00d7 up to 21.554\u00d7 over DGL for GCN, and GAT respectively. These speedups showcase the performance of the optimized code over DGL.\nWe see that the trend of speedups observed from the primitive compositions still hold with a similar speedup pattern to what was observed when getting decisions for DGL from SENSEi. Overall, these are speedups of upto 1.218\u00d7 and upto 2.94\u00d7 for GCN and GAT. We do not observe speedups from SENSEi\u2019s decisions in certain configurations due to the default dynamic composition being selected as the best. Also, note that, compared to the speedups observed due to the compositions in DGL, we see lower speedups\nfor GCN (asia_osm for embedding sizes of 32, 32 is observed to be 1.6\u00d7 in DGL and 1.2\u00d7 in OPT for precompute), and GAT (Reddit for embedding sizes of 32, 256 is observed to be 5.7\u00d7 in DGL and 2.93\u00d7 in OPT for recompute). The difference in primitive kernels as well as the additional optimizations applied may reduce the gains seen from changing the primitive compositions."
        },
        {
            "heading": "6.5 Overheads of SENSEi",
            "text": "SENSEi has an overhead of the additional graph inspection and model decision-making as shown by Figure 5. We run the XGBoost model of SENSEi on the evaluation architecture. Because XGBoost\ncan utilize a GPU when available, it can use it to accelerate its decision speed compared to a CPU-only system. Apart from the model decision time, which will remain the same for each input considered, the graph inspection time to identify the characteristics of the graph adds to the overhead as well. The embedding size does not influence the overhead but does affect the iteration time. Thus, we present the raw overhead in terms of time in Figure 5 for better clarity. The iteration times for GCN and GAT range from 0.001s to 10s.\nThe categorization of each overhead percentage compared to its iteration time is presented in Table 8. Here, we consider all GNN configurations evaluated (graph \u00d7 feature sizes \u00d7 model) to arrive at a total of 145 configurations. Only a small portion of the configurations result in an overhead higher than its iteration time. Even among these, themaximum overhead is limited to 5.69\u00d7 across both hardware platforms."
        },
        {
            "heading": "6.6 Learned model of SENSEi and feature importance",
            "text": "We compare the geomean speedup of the optimal configuration (Oracle Geomean), with the decisions made by SENSEi (model). In addition, we evaluate decisions made by an oracle that is capable\nof selecting the best primitive composition considering only the underlying hardware architecture (arch), embedding sizes (emb), or the input graph (graph) independently for each GCN and GAT. For example, considering the oracle for graph, if the recompute composition is most often the best for GAT on any embedding size and hardware for a given graph, then the oracle will always choose the recompute option for that graph for all architectures and embedding sizes. The result of this analysis is presented in Table 9, and shows the benefit of using a learned system in SENSEi. Although the impact of the architecture as well as embedding sizes cannot be denied through these results, the graph-based oracles come closest in terms of accuracy to SENSEi. However, the graph feature corresponding to selecting the correct primitive composition is not the most obvious, which is density. This is supported by the contrastingly different observations for ogbn-products and com-Amazon with similar density.\nConsidering the feature importance of the XGBoost models shown in Figure 6, it is possible to see that multiple features contribute to making the final decision. The top four features for both composition selectors are output embedding sizes, number of rows, number of nnzs, and graph density. However, how they contribute to the final decision does not seem straightforward to interpret.\nThese observations support that making the correct decision is non-trivial and requires a combination of multiple factors as well as a way to identify the characteristics that contribute to the correct decision. With a data-driven approach, a correct decision can be identified by training an appropriate model as that has been done with SENSEi. Instead of manually searching to find an ideal heuristic in each scenario, scripted data collection and training can decide the adaptability and accuracy of the results."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "Generalizability of compositions. Although we specialized the primitive composition choices presented to particular GNN models, it is possible to generalize them to patterns observed in GNNs in general. The choice between precomputation and dynamic compositions could emerge in a situation where re-associativity of the computations considered leads to multiple dense-sparse primitive\nchoices. Similarly, the choice between reuse and recomputation might arise at a point where an update computation on the same embedding (\ud835\udc3b (\ud835\udc59\u22121) \u00b7\ud835\udc4a (\ud835\udc59 ) ) is required multiple times. An example of a possible situation would be the GATv2 model [1].\nApproximate GNN optimization techniques. In Section 6.4, we presented how the primitive compositions give synergistic benefits for traditional sparse optimizations. However, especially when considering scaling to larger graphs, there are multiple techniques in the GNN domain to reduce the overall computation. One such method is sampling, which would alter the input graph structure in some manner which SENSEi relies on. In situations where sampling does not change the sparsity pattern of the graph, then we can apply SENSEi to a sampled graph and then reuse its decisions."
        },
        {
            "heading": "8 RELATEDWORK",
            "text": "Input Sensitive Systems. There has been limited work on systems strictly dedicated to optimizing GNNs based on input. GNNAdvisor [26] is one such system where based on the input graph, a set of handcrafted functions are used to identify optimization opportunities. Its optimizations are tailored towards GPU-based sparse executions and the thresholds are heuristically set instead of being learned. [20] is quite similar to SENSEi as it proposes a machine-learning model to predict the best data representation for sparse operations in GNNs. Similarly, WISE [32] proposes a machine-learning solution to select the best data representation for SpMV. Works such as ASpT [7] and LAV [31] present input-aware sparse optimizations that introduce new sparse representations and are coupled with specialized executions. Focusing on graph operations in general, [18] proposes a solution quite similar to SENSEi wherein it presents a learned solution that performs sparse optimizations based on characteristics of the input graph. However, this, along with the aforementioned solutions, considers optimizations primarily only on sparse computations. By not considering the input-aware interplay of sparse and dense in the context of GNNs, multiple optimization opportunities are missed as identified by SENSEi. In contrast, [30], (which we include in SENSEi), considers the interplay between sparse and dense computation, but focuses on deciding solely based on the input and output embedding size. Instead, SENSEi jointly considers the embedding sizes, the input graph and the underlying hardware architecture.\nGNN optimizations. Graphiler [29], and SeaStar [28] present compiler-based solutions that allow GNN models written in userdefined functions to be converted into highly optimized code based on the computations specified. [33] also considers recomputation as an optimization. However, this is in terms of reducing memory utilization of the end-to-end GNN training process and overlooks performance optimizations possible through recomputation as SENSEi does. FusedMM [21] and Graphite [6] also present GNN kernel fusions albeit specifically on CPU. Notably, the fusion proposed by Graphite is between sparse and dense data components. This relates to SENSEi as it considers the interplay between sparse and dense data. As SENSEi offers a method of selecting optimizations given the input, all optimizations presented in such systems would compose with SENSEi."
        },
        {
            "heading": "9 CONCLUSION",
            "text": "In this work, we propose different primitive compositions for executing two popular GNNmodels. To automatically make the correct decision between these compositions, we present SENSEi, an inputsensitive decision maker for selecting GNN primitive compositions that caters its decision based on the input graph, embedding sizes, and hardware architecture. SENSEi allows users to attain speedups over the default execution in DGL, a popular GNN framework over a wide range of graphs and embedding sizes in both CPUs and GPUs."
        },
        {
            "heading": "10 ACKNOWLEDGEMENTS",
            "text": "This work is supported byACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA."
        }
    ],
    "title": "Input-sensitive dense-sparse primitive compositions for GNN acceleration",
    "year": 2023
}