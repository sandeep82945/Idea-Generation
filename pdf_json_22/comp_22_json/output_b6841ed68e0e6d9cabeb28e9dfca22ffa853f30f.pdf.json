{
    "abstractText": "In recent years, there has been a growing interest in using machine learning to overcome the high cost of numerical simulation, with some learned models achieving impressive speed-ups over classical solvers whilst maintaining accuracy. However, these methods are usually tested at low-resolution settings, and it remains to be seen whether they can scale to the costly highresolution simulations that we ultimately want to tackle. In this work, we propose two complementary approaches to improve the framework from MeshGraphNets, which demonstrated accurate predictions in a broad range of physical systems. MeshGraphNets relies on a message passing graph neural network to propagate information, and this structure becomes a limiting factor for highresolution simulations, as equally distant points in space become further apart in graph space. First, we demonstrate that it is possible to learn accurate surrogate dynamics of a high-resolution system on a much coarser mesh, both removing the message passing bottleneck and improving performance; and second, we introduce a hierarchical approach (MultiScale MeshGraphNets) which passes messages on two different resolutions (fine and coarse), significantly improving the accuracy of MeshGraphNets while requiring less computational resources.",
    "authors": [
        {
            "affiliations": [],
            "name": "MultiScale MeshGraphNets"
        },
        {
            "affiliations": [],
            "name": "Meire Fortunato"
        },
        {
            "affiliations": [],
            "name": "Tobias Pfaff"
        },
        {
            "affiliations": [],
            "name": "Peter Wirnsberger"
        },
        {
            "affiliations": [],
            "name": "Alexander Pritzel"
        },
        {
            "affiliations": [],
            "name": "Peter Battaglia"
        }
    ],
    "id": "SP:53f26e6b51a4a47110505415bc6998cee2c30013",
    "references": [
        {
            "authors": [
                "K.R. Allen",
                "T. Lopez-Guevara",
                "K. Stachenfeld",
                "A. SanchezGonzalez",
                "P. Battaglia",
                "J. Hamrick",
                "T. Pfaff"
            ],
            "title": "Physical design using differentiable learned simulators",
            "year": 2022
        },
        {
            "authors": [
                "P.W. Battaglia",
                "J.B. Hamrick",
                "V. Bapst",
                "A. SanchezGonzalez",
                "V. Zambaldi",
                "M. Malinowski",
                "A. Tacchetti",
                "D. Raposo",
                "A. Santoro",
                "R Faulkner"
            ],
            "title": "Relational inductive biases, deep learning, and graph networks",
            "venue": "arXiv preprint arXiv:1806.01261,",
            "year": 2018
        },
        {
            "authors": [
                "Belbute-Peres",
                "F. d. A",
                "T.D. Economon",
                "J.Z. Kolter"
            ],
            "title": "Combining differentiable PDE solvers and graph neural networks for fluid flow prediction",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "A. Challapalli",
                "D. Patel",
                "G. Li"
            ],
            "title": "Inverse machine learning framework for optimizing lightweight metamaterials",
            "venue": "Materials & Design,",
            "year": 2021
        },
        {
            "authors": [
                "D. Chen",
                "Y. Lin",
                "W. Li",
                "P. Li",
                "J. Zhou",
                "X. Sun"
            ],
            "title": "Measuring and relieving the over-smoothing problem for graph neural networks from the topological view",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "J. Gilmer",
                "S.S. Schoenholz",
                "P.F. Riley",
                "O. Vinyals",
                "G.E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
            "year": 2017
        },
        {
            "authors": [
                "C.P. Goodrich",
                "E.M. King",
                "S.S. Schoenholz",
                "E.D. Cubuk",
                "M.P. Brenner"
            ],
            "title": "Designing self-assembling kinetics with differentiable statistical physics models",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2024
        },
        {
            "authors": [
                "X. Han",
                "H. Gao",
                "T. Pfaff",
                "Wang",
                "J.-X",
                "Liu",
                "L.-P"
            ],
            "title": "Predicting physics in mesh-reduced space with temporal attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "R. Hanocka",
                "A. Hertz",
                "N. Fish",
                "R. Giryes",
                "S. Fleishman",
                "D. Cohen-Or"
            ],
            "title": "Meshcnn: A network with an edge",
            "venue": "ACM Trans. Graph.,",
            "year": 2019
        },
        {
            "authors": [
                "R. Keisler"
            ],
            "title": "Forecasting global weather with graph neural networks",
            "venue": "arXiv preprint arXiv:2202.07575,",
            "year": 2022
        },
        {
            "authors": [
                "D. Kochkov",
                "T. Pfaff",
                "A. Sanchez-Gonzalez",
                "P. Battaglia",
                "B.K. Clark"
            ],
            "title": "Learning ground states of quantum hamiltonians with graph networks, 2021a",
            "year": 2021
        },
        {
            "authors": [
                "D. Kochkov",
                "J.A. Smith",
                "A. Alieva",
                "Q. Wang",
                "M.P. Brenner",
                "S. Hoyer"
            ],
            "title": "Machine learning\u2013accelerated computational fluid dynamics",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Li",
                "Z. Han",
                "Wu",
                "X.-M"
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Press,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Li",
                "J. Wu",
                "Zhu",
                "J.-Y",
                "J.B. Tenenbaum",
                "A. Torralba",
                "R. Tedrake"
            ],
            "title": "Propagation networks for model-based control under partial observation, 2018b",
            "year": 2018
        },
        {
            "authors": [
                "M. Lino",
                "C. Cantwell",
                "A.A. Bharath",
                "S. Fotiadis"
            ],
            "title": "Simulating continuum mechanics with multi-scale graph neural networks",
            "venue": "arXiv preprint arXiv:2106.04900,",
            "year": 2021
        },
        {
            "authors": [
                "I. Luz",
                "M. Galun",
                "H. Maron",
                "R. Basri",
                "I. Yavneh"
            ],
            "title": "Learning algebraic multigrid using graph neural networks, 2020",
            "year": 2020
        },
        {
            "authors": [
                "D. Mrowca",
                "C. Zhuang",
                "E. Wang",
                "N. Haber",
                "L. Fei-Fei",
                "J.B. Tenenbaum",
                "D.L. Yamins"
            ],
            "title": "Flexible neural representation for physics prediction",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "T. Pfaff",
                "M. Fortunato",
                "A. Sanchez-Gonzalez",
                "P.W. Battaglia"
            ],
            "title": "Learning mesh-based simulation with graph networks",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "S.B. Pope"
            ],
            "title": "Turbulent flows",
            "venue": "URL https://cds.cern.ch/",
            "year": 2011
        },
        {
            "authors": [
                "A. Sanchez-Gonzalez",
                "J. Godwin",
                "T. Pfaff",
                "R. Ying",
                "J. Leskovec",
                "P.W. Battaglia"
            ],
            "title": "Learning to simulate complex physics with graph networks",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "F. Scarselli",
                "M. Gori",
                "A.C. Tsoi",
                "M. Hagenbuchner",
                "G. Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2008
        },
        {
            "authors": [
                "D.I. Shuman",
                "S.K. Narang",
                "P. Frossard",
                "A. Ortega",
                "P. Vandergheynst"
            ],
            "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
            "venue": "IEEE signal processing magazine,",
            "year": 2013
        },
        {
            "authors": [
                "K. Stachenfeld",
                "J. Godwin",
                "P. Battaglia"
            ],
            "title": "Graph networks with spectral message",
            "year": 2021
        },
        {
            "authors": [
                "K. Stachenfeld",
                "D.B. Fielding",
                "D. Kochkov",
                "M. Cranmer",
                "T. Pfaff",
                "J. Godwin",
                "C. Cui",
                "S. Ho",
                "P. Battaglia",
                "A. Sanchez-Gonzalez"
            ],
            "title": "Learned coarse models for efficient turbulence simulation",
            "venue": "In 10th International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "N. Thuerey",
                "K. Wei\u00dfenow",
                "L. Prantl",
                "X. Hu"
            ],
            "title": "Deep learning methods for reynolds-averaged navier\u2013stokes simulations of airfoil flows",
            "venue": "AIAA Journal,",
            "year": 2020
        },
        {
            "authors": [
                "N. Thuerey",
                "K. Wei\u00dfenow",
                "L. Prantl",
                "X. Hu"
            ],
            "title": "Deep learning methods for reynolds-averaged navier\u2013stokes simulations of airfoil flows",
            "venue": "AIAA Journal,",
            "year": 2020
        },
        {
            "authors": [
                "R. Ying",
                "J. You",
                "C. Morris",
                "X. Ren",
                "W.L. Hamilton",
                "J. Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun"
            ],
            "title": "Multi-scale context aggregation by dilated convolutions",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [],
            "title": "MultiScale MeshGraphNets architecture details We encode the graph G = (V , E) exactly as in Pfaff et al. (2021), with the same node and edge features, as well as the latent sizes (128)",
            "year": 2021
        },
        {
            "authors": [],
            "title": "MS-MGN Processor parameter sweep with 5 seeds per processor. MSE-N is the average N-step mean squared error for the first N steps in the trajectory. processing on graphs and decompose the signal into contributions that vary across the mesh at different frequencies (see for example",
            "venue": "Shuman et al",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "In this work, we propose two complementary approaches to improve the framework from MeshGraphNets, which demonstrated accurate predictions in a broad range of physical systems. MeshGraphNets relies on a message passing graph neural network to propagate information, and this structure becomes a limiting factor for highresolution simulations, as equally distant points in space become further apart in graph space. First, we demonstrate that it is possible to learn accurate surrogate dynamics of a high-resolution system on a much coarser mesh, both removing the message passing bottleneck and improving performance; and second, we introduce a hierarchical approach (MultiScale MeshGraphNets) which passes messages on two different resolutions (fine and coarse), significantly improving the accuracy of MeshGraphNets while requiring less computational resources."
        },
        {
            "heading": "1. Introduction",
            "text": "There has been a growing interest in accelerating or replacing costly traditional numerical solvers with learned simulators, which have the potential to be much faster than classical methods (Thuerey et al., 2020a; Kochkov et al., 2021b; Pfaff et al., 2021; Keisler, 2022). Furthermore, learned simulators are generally differentiable by construction, which\n*Equal contribution 1DeepMind. Correspondence to: Meire Fortunato <meirefortunato@google.com>.\n2nd AI4Science Workshop at the 39 th International Conference on Machine Learning (ICML), 2022. Copyright 2022 by the author(s).\nopens up interesting avenues for inverse design (Challapalli et al., 2021; Goodrich et al., 2021; Allen et al., 2022). A recent approach to learning simulations discretized on unstructured meshes is MeshGraphNets (MGN, Pfaff et al. (2021)), which encodes the simulation mesh at each time step into a graph, and uses message passing Graph Neural Networks (GNNs, Gilmer et al. (2017); Scarselli et al. (2008); Battaglia et al. (2018)) to make predictions on this graph. MGN demonstrated strong generalization, and accurate predictions on a broad range of physical systems.\nThe accuracy of traditional solvers is often limited by the resolution of the simulation mesh. This is particularly true for chaotic systems like fluid dynamics: processes at very small length-scales, such as turbulent mixing, affect the overall flow and need to be resolved on very fine meshes to accurately solve the underlying partial differential equation. This leads to the characteristic spatial convergence; where simulation accuracy increases monotonically with the mesh resolution. This is an important property for the use of numerical solvers in practice, as it allows trading in compute to obtain the desired solution accuracy.\nHowever, it is unclear whether this behavior also applies to learned simulation approaches, particularly GNN-based models like MGN. There are reasons to believe it is not the case: as the mesh becomes finer, message passing GNNs have to perform more update steps to pass information along the same physical distance. This results in significantly higher computational cost, and may also cause over-smoothing (Li et al., 2018a; Chen et al., 2020).\nIn this work, we investigate MGN on highly resolved meshes, and find that message propagation speed indeed becomes a limiting factor, leading to high computational costs and reduced accuracy at high resolutions. To overcome this limitation, we propose two orthogonal approaches:\n\u2022 First, we introduce MultiScale MeshGraphNets (MSMGN), a hierarchical framework for learning meshbased simulations using GNNs, which runs message passing at two different resolutions. Namely, we have message passing on input (fine) mesh but also at a coarser mesh that facilitates the propagation of information. We demonstrate that MS-MGN restores spatial convergence, and is more accurate and computationally\nar X\niv :2\n21 0.\n00 61\n2v 1\n[ cs\n.L G\n] 2\nO ct\n2 02\n2\nefficient than MGN.\n\u2022 Second, we modify the training distribution to use highaccuracy labels that better capture the true dynamics of the physical system. As opposed to simply replicating the spatial convergence curve of traditional solvers, this allows to make better predictions than the reference simulator at a given resolution.\nTogether, these approaches are a key step forward for learned mesh-based simulations, and improve accuracy for highly resolved simulations at a lower computational cost."
        },
        {
            "heading": "2. MultiScale MeshGraphNets",
            "text": "Here we introduce MultiScale MeshGraphNets (MSMGN), a hierarchical version of MeshGraphNets (MGN). As in MGN, the model uses a message passing GNN to learn the temporal evolution of physical systems discretized on meshes. In contrast to MGN, passes are being made on both the graph defined by the fine input mesh, and in a coarser mesh. This coarse mesh is introduced only with the aim of promoting more efficient communication in latent space, to efficiently model fast-acting or non-local dynamics. All inputs and outputs are defined on the fine input mesh.\nThis architecture is inspired by both empirical findings about message propagation in graphs and by multigrid methods (Briggs et al., 2000; Bramble, 2019). First, message propagation speed in Cartesian coordinates is bounded by the length of the mesh edges multiplied by the number of message passing blocks. Refining the mesh aiming to obtain greater precision decreases the lengths of the edges, which implies a lower speed of propagation of information. This can lead to certain effects not being modeled properly on high-resolution meshes. Using an auxiliary coarse mesh, we can retain high message propagation speeds even for very fine input meshes. And second, GNNs are related to Gauss-Seidel smoothing iterations as they can only reduce errors locally. By solving the system at multiple resolutions, multigrid methods demonstrate an effective way to achieve global solutions using local updates.\nMS-MGN uses the Encode-Process-Decode GNN framework introduced in Sanchez-Gonzalez et al. (2020), and is trained for next-step predictions and applied iteratively to unroll trajectories at inference time. For training, encoding and decoding, we closely follow the MGN architecture. In this work, we focus on Eulerian dynamics, hence we only need to consider mesh edges and can omit world edges. The algorithm is described for 2D triangular meshes but it can also be applied for e.g. hexahedral or tetrahedral meshes. In departure from MGN, messages are passed independently on two graphs, the coarse graph Gl and fine graph Gh. Additionally, we define the upsampling and downsampling\ngraphs Gup, Gdown to propagate information between levels. The training loss is only placed on nodes of the fine input graph Gh. Below, we describe graph construction and message passing operators for these graphs. The four graph operators are visualized in Figure 1, a more detailed description of encoding and message passing can be found in the Appendix (A.1).\nEncoder A mesh is an undirected graph G = (V,E) specified by its nodes V and edges E. Let D \u2282 R2 be the physical domain where the problem is defined and let Gh = (V h, Eh) and Gl = (V l, El) denote high-resolution and low-resolution mesh representations of D, respectively.\nWe encode the fine input graph Gh as in Pfaff et al. (2021), with the same node and edge features, and identical latent sizes of 128. The coarse graph Gl is encoded in a similar fashion. However, we only encode geometric features in the coarse graph, i.e., relative node coordinates on edges, and a node type to distinguish between internal and boundary nodes. The input field variables such as velocity are only encoded into Gh.\nWe next construct the downsampling graph Gdown = (V l \u222a V h, Eh,l) as follows: For each fine-mesh node i \u2208 V h we find the triangle on the coarse mesh which contains this node. Then, we create three edges kh,l : i\u2192 j which connect the node i to each corner node j = j(i) \u2208 V l of the triangle.1 As the nodes in this graph are already defined above in Gh,Gl, we only need to define the edge feature encoding. The edge features are the relative node coordinates from senders and receivers, which are embedded using an MLP of the same architecture as in the MGN encoders.\n1For meshes with other element types (e.g. hexahedrons or tetrahedrons) we can do the same, by finding the containing element and connecting to all corner nodes.\nThe upsampling graph Gup = (V l \u222a V h, El,h) has the same structure. That is, for each i \u2208 V l, we create three edges kl,h : i \u2192 j connecting i \u2208 V l to the corner nodes of the triangle in the high-res input mesh that contains the node i. Edge features are encoded exactly as in the downsampling graph. Figure 1 shows a representation for both graphs.\nProcessor The processor consists of several iterated processor blocks, which compute updates on the graphs defined above by message passing, as in MGN. However, as opposed to the single graph in MGN, we now have four graphs to update. We could construct a processor block which performs updates on all four graphs simultaneously, and repeat this block n times. But this would be inefficient: For example, as inputs and outputs are defined on Gh only, the first and last updates on the other graphs would be wasted. We also note that updates on the coarse graph are significantly cheaper due to the smaller number of nodes and edges, and propagate information further. Hence, a more efficient strategy may be to perform a number of updates on Gh to aggregate local features, downsample to the coarse graph, perform updates on Gl, upsample, and perform a few updates on Gh to compute small-scale dynamics. This is a similar strategy as a V -cycle in multigrid methods; and as in multi-grid methods, we can stack several of these cycles to reduce errors even further. We will investigate a few choices for efficient processor architectures in Section 5.\nDecoder and state updater. The next-step state predictions will be produced from the updated node latents on the fine graph Gh, and exactly follow the description in MGN, including the loss function, and hyperparemeters in the training setup."
        },
        {
            "heading": "3. High-accuracy labels",
            "text": "The prediction quality of a learned model is bounded by the quality of the data it is trained on.\nIn Pfaff et al. (2021), the training examples were obtained by running a traditional reference simulator for a given initial state and space discretization, and using the simulator\u2019s predictions as training labels. However, these labels are only approximations to the actual physical phenomena we are trying to model, and their accuracy is directly linked to the spatial resolution of the simulation. To reduce the amount of computation associated with high-resolution simulations, traditional solvers often simulate on a coarser grid, and employ heuristic \u201cclosure models\u201d to approximate the effect of small-scale dynamics below the resolution of the simulation mesh (Pope, 2011).\nIn a learned model, we have another option: Instead of a handwritten heuristic, we can modify the training distribution to use \u201chigh-accuracy labels\u201d, e.g. by running a refer-\nence simulation at higher spatial resolution, and bi-linearly interpolating the solution down to the mesh to be used for prediction (Figure 7). This way, the model can implicitly learn the effect of smaller scales without any changes to the model code, and at inference time potentially achieve solutions which are more accurate than what is possible with a classical solver on a coarse scale. In Section 5 we show that learning such a subgrid-aware model for fluid dynamics is indeed tractable over a surprisingly broad resolution scale."
        },
        {
            "heading": "4. Experimental setup",
            "text": "Training set We generated a \u201cCylinderFlow\u201d dataset comprising 1000 trajectories of incompressible flow past a long cylinder in a channel, simulated with COMSOL (Comsol, 2020). Each trajectory consists of T = 200 time steps, and we vary parameters, such as radius and position of the obstacle, inflow initial velocity and mesh resolution. Notably, the mesh resolution covers a wide range from a hundred to tens of thousands of mesh nodes. Section A.3 in the Appendix shows example trajectories and dataset statistics.\nMesh generation Together with the input mesh and the associated trajectories, we also use COMSOL to generate the coarse mesh Gl used in the MS-MGN architecture. To this end, we constrain the mesh generation to a specific minimum and maximum edge length (maximum edge length is always set to 5 times the minimum length). COMSOL generates adaptive meshes based on certain conditions, for example, that the mesh contains smaller edges and more nodes close to obstacles and domain boundaries. We use the same generator to produce a high-quality mesh for both the simulation and for the coarse mesh, which conforms to the domain boundaries, and mirrors the relative node density of the simulation mesh. We use a coarse mesh of the same, fixed resolution (minimum edge length of 10\u22122) for all simulation meshes, to ensure sufficient message propagation speeds on all simulation meshes.\nMeasuring ground-truth error To evaluate the effects of the mesh resolution on the model\u2019s predictions we create a test dataset with 500 trajectories with varying mesh resolutions, but otherwise constant initial conditions. The minimum edge length (edge min) of these meshes ranges from 10\u22122 to 10\u22123. Figure 3 shows the relationship between edge min and node and edge count\u2013 we note that mesh complexity increases strongly for smaller values of edge min.\nAs analytical solutions are in general not available for nontrivial simulation setups, high-resolution simulations are commonly used as a proxy for the \u201cground-truth\u201d solution of the underlying PDE. Here, we generate such a groundtruth reference trajectory (uref) by running COMSOL at the maximum resolution in this dataset (edge min=10\u22123). In the following, we measure error by performing next-step prediction on the test set using a learned model or a classical solver at a given mesh resolution, linearly interpolating the ground-truth trajectory onto the simulation mesh, and computing the MSE."
        },
        {
            "heading": "5. Results",
            "text": "We investigated the error behavior of MGN on simulations of various resolutions on the CylinderFlow domain (Section 4). Our main finding is that message passing speed becomes a bottleneck for MGN performance for highresolution meshes; but it is possible to lift this bottleneck using multiscale methods, and even show better one-step predictions than classical solvers at a given resolution using high-accuracy labels.\nSpatial convergence of MGN We first test the spatial convergence of the baseline MGN model. For this, we trained a MGN model on a mixed-resolution dataset, and tested the performance on meshes with a minimum edge length between 10\u22123 to 10\u22122. Section 4 describes this setup, and how we measure error. Figure 4 shows the spatial convergence curve of a conventional solver (red): As expected, the error decreases when moving towards finer meshes with smaller edge lengths. MGN initially tracks this behavior, but at a certain mesh resolution, the error stagnates. This is an effect of limited message propagation speed; the smaller the edge lengths become, the more message passing steps are necessary to transport information over the same physical distance. Consequently, if we increase the number of message passing steps (mps), the point of divergence from the convergence curve shifts left. However, increasing the number of message passing blocks comes at a high performance and memory cost, and the effects are diminishing; 25mps is the maximum we could train using a single GPU for this dataset, and the error still does not converge.\nSpatial convergence of MS-MGN Next, we compare MGN and MS-MGN for different numbers of mps (15 and 25) to investigate whether multiscale message passing, as described in Section 2, can overcome the bottleneck identified for MGN in the previous paragraph. The comparison comprises two MS-MGN-variants: First, a single V-cycle consisting of a single block of Gh, downsampling, 11 blocks of Gl, upsampling, and a single block of Gh, denoted as \u2018p=1H 11L 1H (U=1,D=1)\u2019, with a total of 15 mps. Second, a variant with two V-cycles in sequence, with the architecture \u2018p=3H 6L 3H 6L 3H (U=2, D=2)\u2019, totalling 25 mps.\nThe results in Fig. 5 show a considerable reduction in the MSE for MS-MGN as compared to the MGN baseline, keeping the overall number of mps fixed. The MS-MGN model with 25 mps manages to track the reference spatial convergence curve closely. This suggests that our proposed multiscale approach with the selected processor is indeed effective at resolving the message passing bottleneck for the underlying problem, and can achieve higher accuracy with the same number of mps. In Appendix A.2, we present further results to analyze the spatial variation of the error signal within the domain, which is not captured by the MSE. In particular, we perform a Graph Fourier Transform of the error signal (see, for example, Shuman et al. (2013)) and compare the results for MGN and MS-MGN across the entire spectrum of eigenvalues. Our main finding of this analysis is that, for rollouts across 10 steps, the multiscale model is not only beneficial in terms of reducing the overall MSE, but that it also dampens the error across a range of eigenvalues that correspond to slowly varying (long-range) spatial correlations.\nComputational efficiency While the previous paragraph has shown that MS-MGN is more accurate than MGN with the same amount of message passing steps, this is not yet the full picture. A message passing step on the lowresolution level Gl is also computationally much cheaper than a step on Gh, as the number of node and edge updates to be computed is lower for Gl. Figure 6 shows the combined effect of computational cost versus accuracy in training at various mesh resolutions; the same relationship also applies at inference time. MS-MGN thus allows significantly better performance/accuracy tradeoffs than the baseline. These effects are on top of the performance benefits for MGN over traditional solvers (Pfaff et al., 2021).\nHigh-accuracy labels Next, we train both MGN and MSMGN models on a dataset with mixed mesh resolution, but with high-accuracy labels as described in Section 3. Figure 7 shows that instead of tracking the convergence curve, the error of the MGN is actually much lower than the reference simulator. This indicates that the learned model can learn an effective model of the subgrid dynamics, and can make accurate predictions even at very coarse mesh resolutions. Surprisingly, this effect extends up to edge lengths of 10\u22122 which correspond to a very coarse mesh with only around a hundred nodes.\nHowever, this method does not alleviate the message propagation bottleneck, and errors increase above the convergence curve for edge lengths below 0.0016. Thus, if a highly resolved output mesh is desired, accuracy is still limited. For a method that performs well both on low- and very highresolution meshes, we can train a MS-MGN model with high-accuracy labels (Figure 8). For the 25 mps model variant, the error stays below the reference solver curve at all resolutions, with all the performance benefits of MS-MGN.\nError accumulation So far, we have analyzed one-step prediction error, and shown that a learned model can not only be more efficient, but reach higher accuracy at a given\nmesh resolution than a classical solver. However, for transient simulation, the model needs to be rolled out over many time steps, and error may accumulate. Hence, accurate onestep predictions are a necessary, but not sufficient condition for achieving accurate simulation rollouts. While Pfaff et al. (2021) showed that MGN model rollouts remain stable and plausible, they can drift from the ground-truth solution. This effect is particularly pronounced for high-resolution meshes, as seen in Figure 9 for meshes with edge length below 0.0015. Here, MS-MGN shows reduced error compared to the baseline, but does not stop error accumulation.\nChoice of coarse mesh Our default strategy for generating the coarse-level mesh Gl is using the same mesh generator as for the simulation mesh Gh, constrained to a larger minimum edge length. However, as the method introduced in Section 2 is flexible in regards to mesh type, other options\nare possible as well. Here, we compare to a strategy similar to the one used in Lino et al. (2021), which samples the simulation domain using a uniform grid. Up- and downsampling operators now operate on grid cells (creating 4 edges for downsampling), and edges ending outside the simulation domain (e.g. if they fall inside the cylinder obstacle) are omitted. Otherwise the method stays unchanged. Figure 10 shows a comparison for both types of coarse mesh, with the grid version performing strictly worse both on 1-step (N = 1) and rollout (N > 1) error. We hypothesize that this is due to the fact that our default strategy produces meshes which conform to the domain boundaries, as well as retaining a similar relative node density as the simulation mesh, which makes it easier to learn universal local rules for transferring information."
        },
        {
            "heading": "6. Related Work",
            "text": "GNN-based simulation models Predicting physical dynamics with GNN models is an active and growing area of research in machine learning. For example, GNNs have been successfully used for predicting dynamics of liquids and soft bodies (Sanchez-Gonzalez et al., 2020; Li et al., 2018b), aerodynamics (Belbute-Peres et al., 2020), forecasting global weather (Keisler, 2022) and quantum manybody problems (Kochkov et al., 2021a). While this work specifically studies and extends MeshGraphNets (Pfaff et al.,\n2021), we expect the broad findings to transfer to other GNN-based models, particularly those operating on meshes.\nHigh-accuracy labels Grid-based learned simulator models sometimes operate on simulation data interpolated from a higher-resolution source (Thuerey et al., 2020b; Kochkov et al., 2021b). This might be unavoidable, as the simulation output might not be in grid-format, or too fine to be processed by a CNN. However, most works do not study whether and to which degree label resolution impacts performance. A notable exception is Stachenfeld et al. (2022) which predicts turbulent dynamics on grids using Dilated CNNs (Yu & Koltun, 2016), and investigates the effect of varying grid resolution. On the other hand, mesh-based GNN simulators tend to operate directly on the simulation graph. As GNN models act locally and subgrid dynamics might be harder to learn on unstructured meshes with variable local node density, it is not clear whether these models can learn effective closures and make use of high-accuracy labels. Our results suggest that this is in fact the case, and using high-accuracy labels MeshGraphNet-style models can make accurate predictions over a surprisingly broad range of mesh resolutions, allowing inference at significantly lower cost.\nHierarchical GNN models The idea of using hierarchies to structure interactions, and connect distant regions in the graph is a well-explored topic in the GNN literature, albeit in different forms and for different aims, such as representation learning (Stachenfeld et al., 2021; Ying et al., 2018) or geometry processing (Hanocka et al., 2019).\nIn GNNs for physics simulation, (Mrowca et al., 2018) use a tree hierarchy to structure particle-particle interactions in a softbody GNN physics simulation, to promote cohesion of solid objects. Han et al. (2022) uses a two-level hierarchy to reduce the number of nodes in a mesh-based fluid simulation, such that the system state can be fed into a transformer model without excessive memory overhead. Luz et al. (2020) use GNNs to learn restriction/prolongation operators which optimize performance of a (classical) AMG multigrid solver. And, most closely related to MS-MGN, the recent work of Lino et al. (2021) devises a multigridinspired hierarchy of GNNs based on (Sanchez-Gonzalez et al., 2020) to solve advection and incompressible flow. One key difference to MS-MGN is its reliance on on-the-fly graph connectivity\u2013 on the finest level, nodes are connected by proximity instead of mesh edge, and coarser levels use regular grids. We found this grid-based approach to perform worse than our conformal meshing, as it cannot leverage all the advantages of unstructured meshes (see Section 5)."
        },
        {
            "heading": "7. Conclusions and discussion",
            "text": "Message-passing GNNs can perform well and learn the dynamics of a broad range of physical problems, but for highly resolved simulations message propagation speed becomes a bottleneck, limiting the prediction accuracy. Increasing the number of message passing steps however comes with significant computational cost, and ultimately still fails to reduce error for complex meshes.\nIn this work, we studied this effect and proposed two complementary approaches to model global effects more efficiently when using GNNs to learn simulations. First, we demonstrated that GNN physics models can effectively leverage high-accuracy labels, to make accurate predictions even on extremely coarse simulation meshes. And second, we proposed the hierarchical model MS-MGN that propagates information on two different resolutions, with a coarse discretization allowing the model to exchange information more efficiently throughout the domain. We find the proposed model to be more accurate and computationally cheaper compared to the MGN baseline with the same number of message passing steps. An interesting extension to this work would be to explore more levels in the mesh hierarchy, which will likely be beneficial for exploring even higher resolutions. Furthermore, it will be very interesting to see how our proposed multiscale approach performs on other challenging problems than the one considered in this work, for example, problems that involve more complex geometries or dynamics.\nAnother important research direction is reducing error accumulation for long rollouts. While rollouts remain stable for hundreds of steps for both the MGN and MS-MGN models, and the multiscale approach substantially ameliorates error accumulation \u2013 the problem is not resolved. Currently, both models are trained on next-step prediction, and unrolled with a fixed time step. Allowing for adaptive time stepping, investigating sequence models, and the role of training noise in error accumulations are exciting topics to be explored in the future."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Alvaro Sanchez-Gonzalez, Yulia Rubanova, Charles Blundell, Michael P. Brenner, and our reviewers for valuable discussions on the work and feedback on manuscript."
        },
        {
            "heading": "A. Appendix",
            "text": "A.1. MultiScale MeshGraphNets architecture details\nWe encode the graph Gh = (V h, Eh) exactly as in Pfaff et al. (2021), with the same node and edge features, as well as the latent sizes (128). Each node i \u2208 V h has embedding vhi , and for each edge with sender i \u2208 V h and receiver j \u2208 V h, we denote its edge embedding by ehij . Similarly, v l i and e l ij denote the node and edge latents in the Gl = (V l, El) graph.\nIn the downsampling Gdown = (V l \u222a V h, Eh,l) graph, for each fine-mesh node i \u2208 V h, we first find the triangle on the coarse mesh that contains this node. We then create three edges kh,l : i \u2192 j i \u2208 V h which connect the node i to each corner node j = j(i) \u2208 V l of the triangle. The corresponding edge embeddings are denoted by eh,lij . The upsampling graph Gup = (V l \u222a V h, El,h) has the same structure. For each i \u2208 V l, we create three edges kl,h : i\u2192 j to the corner nodes of the triangle in the high-res input mesh that contains the node i, and denote by el,hij their edge embeddings.\nBelow we describe the node and edge feature update equations on the fine and coarse graphs (high- and low- resolution updates, respectively), as well as for the downsampling and upsampling graph operators.\nHigh-resolution update e\u2032\nh ij \u2190 fE,h(ehij ,vhi ,vhj ) , v\u2032 h j \u2190 fV,h(vhj , \u2211 i e\u2032 h ij) (1)\nLow-resolution update e\u2032\nl ij \u2190 fE,l(elij ,vli,vlj) , v\u2032 l j \u2190 fV,l(vlj , \u2211 i e\u2032 l ij) (2)\nDownsampling update e\u2032\nh,l ij \u2190 fE,h,l(e h,l ij ,v h i ,v l j) , v \u2032l j \u2190 fV,h,l(vlj , \u2211 i e\u2032 h,l ij ) (3)\nUpsampling update e\u2032\nl,h ij \u2190 fE,l,h(e l,h ij ,v l i,v h j ) , v \u2032h j \u2190 fV,l,h(vhj , \u2211 i e\u2032 l,h ij ) (4)\nwhere the node and edge update functions in all graphs, fE,h, fV,h, fE,l, fV,l, fE,h,l, fV,h,l, fE,l,h, fV,l,h are MLPs with residual connections, and the sum \u2211 i on the right equations is over all edges k : i\u2192 j.\nA.2. Additional results\nMS-MGN processor analysis In figure A.1, we analyse the impact of different processor choices on the MS-MGN architecture. When counting the total number of message passing steps, we consider any graph update a step, that is, we count cheaper processing steps (in the coarse mesh, or downsample and upsample graphs) in the same way as an update on the fine input mesh. We use the letters \u2018H\u2019 and \u2018L\u2019 to represent updates on the high- and low-resolution graphs (fine/coarse), respectively. We use letters \u2018U\u2019 and \u2018D\u2019 to count the total number of calls for the upsample and downsample graph operators, respectively, which happen once whenever there is a change between the fine and coarse graphs. For instance, processor \u2018p=1H 11L 1H (U=1,D=1)\u2019 has a total of 15 steps, with 1 processing step on the fine mesh, followed by downsampling, 11 updates on the coarse level, upsampling, and a final processing step on the fine graph.\nWe observe that both the 1-step errors and the accumulated errors for longer rollouts are similar for the choices analysed, except for the last processor \u2018p=1H 21L 1H (U=1,D=1)\u2019 with 25 mps, which had significantly lower accuracy. Note that its performance is similar to \u2018p=1H 11L 1H (U=1,D=1)\u2019 with 15 mps, which indicates that no further steps are needed in the coarse mesh. We have not investigated this in further detail but comparison with the other processors suggests that more than two message passing steps on the fine level are beneficial.\nGraph Fourier Analysis The convergence analysis in figure A.1 shows that our multiscale approach compares favourably to the MGN baseline in terms of the mean squared error (MSE). The MSE is a useful metric to compare the overall error computed across the entire domain but it does not reveal any insight into how the error signal varies within the domain. For a more fine-grained analysis of the latter, we can leverage techniques that are commonly employed for signal\nprocessing on graphs and decompose the signal into contributions that vary across the mesh at different frequencies (see for example Shuman et al. (2013) for details). To this end, we computed the Graph Fourier Transform (GFT) of the velocity error and analysed its power spectrum |GFT |2 as a function of the eigenvalues of the graph Laplacian, {\u03bbn}n=1,...,M , that are assumed to be ordered such that \u03bb1 < \u03bb2 \u2264 \u03bbM\u22121 \u2264 \u03bbM , where M is the number of mesh nodes. This analysis allows us to decompose the error into contributions varying with different frequencies and to compare the results for MGN and MS-MGN.\nFigure A.2 shows the power spectrum of the error signal across the entire range of eigenvalues for a mesh with M = 10117 nodes (edge min = 1.5 \u00d7 10\u22123) for next-step prediction (left panel) and for ten rollout steps (right panel). While we do not find a systematic difference between MGN and MS-MGN across a single step, we observe a clear reduction in the error of the multiscale model across the first 20\u201330 eigenvalues for ten rollout steps. This eigenvalue range corresponds to eigenvectors that vary slowly across the mesh, as illustrated in Fig. A.3. The MSE error is captured entirely by the contribution of \u03bb1, whose corresponding eigenvector is a constant on the mesh. Contributions of eigenvalues with n > 1 vary slowly across the mesh for small values of n and rapidly for large values (Shuman et al., 2013). This result therefore suggests that the MS-MGN model is not only favourable in terms of reducing the MSE but that it also captures slow-varying (long-range) correlations more accurately across long rollouts, at least for the problem investigated.\nA.3. CylinderFlow Dataset\nWe generated the 2D CylinderFlow dataset using COMSOL to simulate the temporal evolution of incompressible flow past a long cylinder in a channel. The time step is set to 0.01 seconds, and the final time on the trajectories is 2 seconds. The viscosity \u00b5 = 10\u22123 m2/s is fixed and the height and length of the channel are set to 0.4 m and 1 m, respectively. The parameters that vary for different trajectories are the radius R and center C = (cx, cy) of the cylinder obstacle, the median initial inflow velocity Umean, and the minimum edge length on the discretization mesh (edge min). The maximum edge length is set to be 5 times the minimum one. We sampled uniformly as follows: R \u223c [0.02, 0.08] (m) , C \u223c U(0.15, 0.4)\u00d7 (0.1, 0.3) (m), Umean \u223c U(0.2, 12) (m/s), edge min \u223c LU(10\u22123, 10\u22122) (m), where LU is the uniform distribution in log scale. In figure A.4 we show examples of trajectories in the training set.\nIn the CylinderFlow (Fixed Obstacle) test set, we fix Umean = 0.85 m/s, R = 0.05 m, C = (0.275, 0.25) m, and only vary the mesh resolution edge min, sampling from the same distribution as in training. Figure A.5 shows example meshes."
        }
    ],
    "year": 2022
}