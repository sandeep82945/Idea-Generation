{
    "abstractText": "Leveraging user-provided translation to constrain NMT has practical significance. Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the NMT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kai Song"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        },
        {
            "affiliations": [],
            "name": "Heng Yu"
        },
        {
            "affiliations": [],
            "name": "Weihua Luo"
        },
        {
            "affiliations": [],
            "name": "Kun Wang"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        }
    ],
    "id": "SP:c82e35777b76af88b48aefc017c2d6dc4a051c2e",
    "references": [
        {
            "authors": [
                "Tamer Alkhouli",
                "Gabriel Bretschner",
                "Hermann Ney."
            ],
            "title": "On the alignment problem in multi-head attention-based neural machine translation",
            "venue": "arXiv preprint arXiv:1809.03985.",
            "year": 2018
        },
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould."
            ],
            "title": "Guided open vocabulary image captioning with constrained beam search",
            "venue": "CoRR, abs/1612.00576.",
            "year": 2016
        },
        {
            "authors": [
                "Philip Arthur",
                "Graham Neubig",
                "Satoshi Nakamura."
            ],
            "title": "Incorporating discrete translation lexicons into neural machine translation",
            "venue": "arXiv preprint arXiv:1606.02006.",
            "year": 2016
        },
        {
            "authors": [
                "Jimmy Lei Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E Hinton."
            ],
            "title": "Layer normalization",
            "venue": "arXiv preprint arXiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "Peter F Brown",
                "Vincent J Della Pietra",
                "Stephen A Della Pietra",
                "Robert L Mercer."
            ],
            "title": "The mathematics of statistical machine translation: Parameter estimation",
            "venue": "Computational linguistics, 19(2):263\u2013311.",
            "year": 1993
        },
        {
            "authors": [
                "Yongchao Deng"
            ],
            "title": "Systran\u2019s pure neural machine translation systems. arXiv preprint arXiv:1610.05540",
            "year": 2016
        },
        {
            "authors": [
                "Anna Currey",
                "Antonio Valerio Miceli Barone",
                "Kenneth Heafield."
            ],
            "title": "Copied monolingual data improves low-resource neural machine translation",
            "venue": "Proceedings of the Second Conference on Machine Translation, pages 148\u2013156.",
            "year": 2017
        },
        {
            "authors": [
                "Chris Dyer",
                "Victor Chahuneau",
                "Noah A Smith."
            ],
            "title": "A simple, fast, and effective reparameterization of ibm model 2",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2013
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani."
            ],
            "title": "A theoretically grounded application of dropout in recurrent neural networks",
            "venue": "Advances in neural information processing systems, pages 1019\u20131027.",
            "year": 2016
        },
        {
            "authors": [
                "Jiatao Gu",
                "Zhengdong Lu",
                "Hang Li",
                "Victor OK Li."
            ],
            "title": "Incorporating copying mechanism in sequence-to-sequence learning",
            "venue": "arXiv preprint arXiv:1603.06393.",
            "year": 2016
        },
        {
            "authors": [
                "Caglar Gulcehre",
                "Sungjin Ahn",
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Yoshua Bengio."
            ],
            "title": "Pointing the unknown words",
            "venue": "arXiv preprint arXiv:1603.08148.",
            "year": 2016
        },
        {
            "authors": [
                "Eva Hasler",
                "Adri\u00e0 De Gispert",
                "Gonzalo Iglesias",
                "Bill Byrne."
            ],
            "title": "Neural machine translation decoding with terminology constraints",
            "venue": "arXiv preprint arXiv:1805.03750.",
            "year": 2018
        },
        {
            "authors": [
                "Chris Hokamp",
                "Qun Liu."
            ],
            "title": "Lexically constrained decoding for sequence generation using grid beam search",
            "venue": "arXiv preprint arXiv:1704.07138.",
            "year": 2017
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Tomasz Dwojak",
                "Rico Sennrich."
            ],
            "title": "The amu-uedin submission to the wmt16 news translation task: Attention-based nmt models as feature functions in phrase-based smt",
            "venue": "arXiv preprint arXiv:1605.04809.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Xiaoqing Li",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Neural name translation improves neural machine translation",
            "venue": "arXiv preprint arXiv:1607.01856.",
            "year": 2016
        },
        {
            "authors": [
                "Minh-Thang Luong",
                "Christopher D Manning."
            ],
            "title": "Stanford neural machine translation systems for spoken language domains",
            "venue": "Proceedings of the International Workshop on Spoken Language Translation, pages 76\u201379.",
            "year": 2015
        },
        {
            "authors": [
                "Minh-Thang Luong",
                "Ilya Sutskever",
                "Quoc V Le",
                "Oriol Vinyals",
                "Wojciech Zaremba."
            ],
            "title": "Addressing the rare word problem in neural machine translation",
            "venue": "arXiv preprint arXiv:1410.8206.",
            "year": 2014
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proc. ACL, pages 311\u2013318, Philadelphia, Pennsylvania, USA.",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post",
                "David Vilar."
            ],
            "title": "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation",
            "venue": "arXiv preprint arXiv:1804.06609.",
            "year": 2018
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V Le"
            ],
            "title": "Searching for activation functions",
            "year": 2018
        },
        {
            "authors": [
                "Abigail See",
                "Peter J Liu",
                "Christopher D Manning."
            ],
            "title": "Get to the point: Summarization with pointer-generator networks",
            "venue": "arXiv preprint arXiv:1704.04368.",
            "year": 2017
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Improving neural machine translation models with monolingual data",
            "venue": "Computer Science.",
            "year": 2015
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "arXiv preprint arXiv:1508.07909.",
            "year": 2015
        },
        {
            "authors": [
                "Yaohua Tang",
                "Fandong Meng",
                "Zhengdong Lu",
                "Hang Li",
                "Philip LH Yu."
            ],
            "title": "Neural machine translation with external phrase memory",
            "venue": "arXiv preprint arXiv:1606.01792.",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "CoRR, abs/1706.03762.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Meire Fortunato",
                "Navdeep Jaitly."
            ],
            "title": "Pointer networks",
            "venue": "Advances in Neural Information Processing Systems, pages 2692\u20132700.",
            "year": 2015
        },
        {
            "authors": [
                "Xing Wang",
                "Zhengdong Lu",
                "Zhaopeng Tu",
                "Hang Li",
                "Deyi Xiong",
                "Min Zhang."
            ],
            "title": "Neural machine translation advised by statistical machine translation",
            "venue": "AAAI, pages 3330\u20133336.",
            "year": 2017
        },
        {
            "authors": [
                "Yuguang Wang",
                "Shanbo Cheng",
                "Liyang Jiang",
                "Jiajun Yang",
                "Wei Chen",
                "Muze Li",
                "Lin Shi",
                "Yanfeng Wang",
                "Hongtao Yang."
            ],
            "title": "Sogou neural machine translation systems for wmt17",
            "venue": "Proceedings of the Second Conference on Machine Transla-",
            "year": 2017
        },
        {
            "authors": [
                "Richard Zens",
                "Daisy Stanton",
                "Peng Xu."
            ],
            "title": "A systematic comparison of phrase table pruning techniques",
            "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Proceedings of NAACL-HLT 2019, pages 449\u2013459 Minneapolis, Minnesota, June 2 - June 7, 2019. c\u00a92019 Association for Computational Linguistics\n449"
        },
        {
            "heading": "1 Introduction",
            "text": "One important research question in domainspecific machine translation (Luong and Manning, 2015) is how to impose translation constraints (Crego et al., 2016; Hokamp and Liu, 2017; Post and Vilar, 2018). As shown in Figure 1 (a), the word \u201cbreadboard\u201d can be translated into \u201c\u5207\u9762 \u5305\u677f (a wooden board that is used to cut bread on)\u201d in the food domain, but \u201c\u7535\u8def\u677f (a construction base for prototyping of electronics)\u201d in the electronic domain. To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (Arthur et al., 2016; Hasler et al., 2018). We investigate the method of leveraging pre-specified translation for NMT using such a lexicon.\nFor leveraging pre-specified translation, one existing approach uses placeholder tags to substitute named entities (Crego et al., 2016; Li et al., 2016; Wang et al., 2017b) or rare words (Luong et al.,"
        },
        {
            "heading": "I want a I want a",
            "text": "2014) on both the source and target sides during training, so that a model can translate such words by learning to translate placeholder tags. For example, the i-th named entity in the source sentence is replaced with \u201ctagi\u201d, as well as its corresponding translation in the target side. Placeholder tags in the output are replaced with pre-specified translation as a post-processing step. One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output.\nAnother approach (Hokamp and Liu, 2017; Post and Vilar, 2018) imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding. This method ensures that pre-specified translations appear in the output. A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows.\nWe investigate a novel method based on data augmentation, which combines the advantages of both methods above. The idea is to construct synthetic parallel sentences from the original paral-\nlel training data. The synthetic sentence pairs resemble code-switched source sentences and their translations, where certain source words are replaced with their corresponding target translations. The motivation is to make the model learn to \u201ctranslate\u201d embedded pre-specified translations by copying them from the modified source. During decoding, the source is similarly modified as a preprocessing step. As shown in Figure 1 (b), translation is executed over the code-switched source, without further constraints or post-processing.\nIn contrast to the placeholder method, our method keeps lexical semantic information (i.e. target words v.s. placeholder tags) in the source, which can lead to more adequate translations. Compared with the lexical constraint method, prespecified translation is learned because such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences.\nTo further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words.\nResults on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release four high-quality En-Ru e-commerce test sets translated by Russian language experts, totalling 7169 sentences with an average length of 211."
        },
        {
            "heading": "2 Related Work",
            "text": "Using placeholders. Luong et al. (2014) use annotated unk tags to present the unk symbols in\n1To best of our knowledge, this is the first public ecommerce test set.\ntraining corpora, where the correspondence between source and target unk symbols are obtained from word alignment (Brown et al., 1993). Output unk tags are replaced through a post-processing stage by looking up a pre-specified dictionary or copying the corresponding source word. Crego et al. (2016) extended unk tags symbol to specific symbols that can present name entities. Wang et al. (2017b) and Li et al. (2016) use a similar method. This method is limited when constrain NMT with pre-specified translations consisting of more general words, due to the loss of word meaning when representing them with placeholder tags. In contrast to their work, word meaning is fully kept in modified source in our work.\nLexical constraints. Hokamp and Liu (2017) propose an altered beam search algorithm, namely grid beam search, which takes target-side prespecified translations as lexical constraints during beam search. A potential problem of this method is that translation fidelity is not specifically considered, since there is no indication of a matching source of each pre-specific translation. In addition, decoding speed is significantly reduced (Post and Vilar, 2018). Hasler et al. (2018) use alignment to gain target-side constraints\u2019 corresponding source words, simultaneously use finitestate machines and multi-stack (Anderson et al., 2016) decoding to guide beam search. Post and Vilar (2018) give a fast version of Hokamp and Liu (2017), which limits the decoding complexity linearly by altering the beam search algorithm through dynamic beam allocation.\nIn contrast to their methods, our method does not make changes to the decoder, and therefore decoding speed remains unchanged. Translation fidelity of pre-specified source words is achieved through a combination of training and decoding procedure, where replaced source-side words still contain their target-side meaning. As a soft method of inserting pre-specified translation, our method does not guarantee that all lexical constraints are satisfied during decoding, but has better overall translation quality compared to their method.\nUsing probabilistic lexicons. Aiming at making use of one-to-many phrasal translations, the following work is remotely related to our work. Tang et al. (2016) use a phrase memory to provide extra information for their NMT encoder, dynamically switching between word generation and\nphrase generation during decoding. Wang et al. (2017a) use SMT to recommend prediction for NMT, which contains not only translation operations of a SMT phrase table, but also alignment information and coverage information. Arthur et al. (2016) incorporate discrete lexicons by converting lexicon probabilities into predictive probabilities and linearly interpolating them with NMT probability distributions.\nOur method is similar in the sense that external translations of source phrases are leveraged. However, their tasks are different. In particular, these methods regard one-to-many translation lexicons as a suggestion. In contrast, our task aims to constrain NMT translation through one-to-one prespecified translations. Lexical translations can be used to generate code-switched source sentences during training, but we do not modify NMT models by integrating translation lexicons. In addition, our data augmentation method is more flexible, because it is model-free.\nAlkhouli et al. (2018) simulate a dictionaryguided translation task to evaluate NMT\u2019s alignment extraction. A one-to-one word translation dictionary is used to guide NMT decoding. In their method, a dictionary entry is limited to only one word on both the source and target sides. In addition, a pre-specified translation can come into effect only if the corresponding source-side word is successfully aligned during decoding.\nOn translating named entities, Currey et al. (2017) augment the training data by copying target-side sentences to the source-side, resulting in augmented training corpora where the source and the target sides contain identical sentences. The augmented data is shown to improve translation performance, especially for proper nouns and other words that are identical in the source and target languages."
        },
        {
            "heading": "3 Data augmentation",
            "text": "Our method is based on data augmentation. During training, augmented data are generated by replacing source words or phrases directly with their corresponding target translations. The motivation is to sample as many code-switched translation pairs as possible. During decoding, given prespecified translations, the source sentence is modified by replacing phrases with their pre-specified translations, so that the trained model can directly copy embedded target translations in the output."
        },
        {
            "heading": "3.1 Training",
            "text": "Given a bilingual training corpus, we sample augmented sentence pairs by leveraging a SMT phrase table, which can be trained over the same bilingual corpus or a different large corpus. We extract source-target phrase pairs2 from the phrase table, replacing source-side phrases of source sentences using the following sampling steps:\n1. Indexing between source-target phrase pairs and training sentences: (a) For each sourcetarget phrase pair, we record all the matching bilingual sentences that contain both the source and target. Word alignment can be used to ensure the phrase pairs that are mutual translation. (b) We also sample bilingual sentences that match two source-target phrase pairs. In particular, given a combination of two phrase pairs, we index bilingual sentences that match both simultaneously.\n2. Sampling: (a) For each source-target phrase pair, we keep at most k1 randomly selected matching sentences. The source-side phrase is replaced with its target-side translation. (b) For each combination of two source-target phrase pairs, we randomly sample at most k2 matching sentences. Both source-side matching phrases are replaced with their target translations.3\nThe sampled training data is added to the original training data to form a final set of training sentences."
        },
        {
            "heading": "3.2 Decoding",
            "text": "We impose target-side pre-specified translations to the source by replacing source phrases with their translations. Lexicons are defined in the form of one-to-one source-target phrase pairs. Different from training, the number of replaced phrases in a source sentence is not necessarily restricted to one or two, which will be discussed in Section 5.5. In practice, pre-specified translations can be provided by customers or through user feedback, which contains one identified translation for specified source segment."
        },
        {
            "heading": "4 Model",
            "text": "Transformer (Vaswani et al., 2017) uses selfattention network for both encoding and decod-\n2Source-side phrase is at most trigram. 3We set k1 = 100, k2 = 30 empirically.\ning. The encoder is composed of n stacked neural layers. For time step i in layer j, the hidden state hi,j is calculated by employing self-attention over the hidden states in layer j \u2212 1, which are {h1,j\u22121, h2,j\u22121, ..., hm,j\u22121}, where m is the number of source-side words.\nIn particular, hi,j is calculated as follows: First, a self-attention sub-layer is employed to encode the context. Then attention weights are computed as scaled dot product between the current query hi,j\u22121 and all keys {h1,j\u22121, h2,j\u22121, ..., hm,j\u22121}, normalized with a softmax function. After that, the context vector is represented as weighted sum of the values projected from hidden states in the previous layer, which are {h1,j\u22121, h2,j\u22121, ..., hm,j\u22121}. The hidden state in the previous layer and the context vector are then connected by residual connection, followed by a layer normalization function (Ba et al., 2016), to produce a candidate hidden state h\n\u2032 i,j . Finally, an-\nother sub-layer including a feed-forward network (FFN) layer, followed by another residual connection and layer normalization, are used to obtain the hidden state hi,j .\nIn consideration of translation quality, multihead attention is used instead of single-head attention as mentioned above, positional encoding is also used to compensate the missing of position information in this model.\nThe decoder is also composed of n stacked layers. For time step t in layer j, a selfattention sub-layer of hidden state st,j is calculated by employing self-attention mechanism over hidden states in previous target layer, which are {s1,j\u22121, s2,j\u22121, ..., st\u22121,j\u22121}, resulting in candidate hidden state s\n\u2032 t,j . Then, a second target-to-\nsource sub-layer of hidden state st,j is inserted above the target self-attention sub-layer. In particular, the queries(Q) are projected from s\n\u2032 t,j , and\nthe keys(K) and values(V ) are projected from the source hidden states in the last layer of encoder, which are {h1,n, h2,n, ..., hm,n}. The output state is another candidate hidden state s\n\u2032\u2032 t,j . Finally, a\nlast feed-forward sub-layer of hidden state st,j is calculated by employing self-attention over s\n\u2032\u2032 t,j .\nA softmax layer based on decoder\u2019s last layer st,n is used to gain a probability distribution Ppredict over target-side vocabulary.\np(yt|y1, ..., yt\u22121, x) = softmax(st,n \u2217 W), (1)\nwhere W is the weight matrix which is learned, x\nrepresent the source sentence, {y1, y2, ..., yt} represent target words."
        },
        {
            "heading": "4.1 Shared Target Embeddings",
            "text": "Shared target embeddings enforces the correspondence between source-side and target-side expressions on the embedding level. As shown in Figure 2, during encoding, source-side target word embeddings are identical to their embeddings in the target-side vocabulary embedding matrix. This makes it easier for the model to copy source-side target words to the output."
        },
        {
            "heading": "4.2 Pointer Network",
            "text": "To strengthen copying through locating sourceside target words, we integrate pointer network (Gulcehre et al., 2016) into the decoder, as shown in Figure 2. At each decoding time step t, the target-to-source attention weights \u03b1t,1, ...,\u03b1t,m are utilized as a probability distribution Pcopy, which models the probability of copying a word from the i-th source-side position. The i-th source-side position may represent a source-side word or a source-side target word. Pcopy is added to Ppredict, the probability distribution over targetside vocabulary, to gain a new distribution over both the source and the target side vocabulary4:\nP = (1\u2212 gpred) \u2217 Pcopy + gpred \u2217 Ppredict , (2)\nwhere gpred is used to control the contribution of two probability distributions. For time step t, gpred is calculated from the context vector ct and the current hidden state of the decoder\u2019s last layer st,n:\n4For the words which belong to the source-side vocabulary but are not appeared in the source-side sentence, the probabilities are set to 0.\ngpred = \u03c3(ct \u2217Wp + st,n \u2217Wq + br), (3)\nwhere Wp, Wq, and br are parameters trained and \u03c3 is the sigmoid function. In addition, the context vector ct is calculated as ct = m i=1 \u03b1t,i \u2217 hi,n, where \u03b1t,i is attention weight mentioned earlier. {h1,n, h2,n, ..., hm,n} are the source-side hidden states of the encoder\u2019s last layer."
        },
        {
            "heading": "5 Experiments",
            "text": "We compare our method with strong baselines on large-scale En-Ru and Ch-En tasks on various test sets across different domains, using a strongly optimized Transformer (Vaswani et al., 2017). BLEU (Papineni et al., 2002) is used for evaluation."
        },
        {
            "heading": "5.1 Data",
            "text": "Our training corpora are taken from the WMT2018 news translation task.\nEn-Ru. We use 13.88M sentences as baseline training data, containing both a real bilingual corpus and a synthetic back-translation corpus (Sennrich et al., 2015a). The synthetic corpus is translated from \u201cNewsCommonCrawl\u201d, which can be obtained from the WMT task. The news domain contains four different test sets published by WMT2018 over the recent years, namely \u201cnews2015\u201d, \u201cnews2016\u201d, \u201cnews2017\u201d, and \u201cnews2018\u201d, respectively, each having one reference. The e-commerce domain contains four files totalling 7169 sentences, namely \u201csubject17\u201d, \u201cdesc17\u201d, \u201csubject18\u201d, and \u201cdesc18\u201d, respectively, each having one reference. The sentences are extracted from e-commerce websites, in which \u201csubject\u201ds are the goods names shown on a listing page. \u201cdesc\u201ds refer to information in a commodity\u2019s description page. \u201csubject17\u201d and \u201cdesc17\u201d are released5. Our development set is \u201cnews2015\u201d.\nCh-En. We use 7.42M sentences as our baseline training data, containing both real bilingual corpus and synthetic back-translation corpus (Sennrich et al., 2015a). We use seven public development and test data sets, four in the news domain, namely \u201cNIST02\u201d, \u201cNIST03\u201d, \u201cNIST04\u201d, \u201cNIST05\u201d, respectively, each with four references, and three in the spoken language domain, namely\n5https://github.com/batman2013/ e-commerce_test_sets\n\u201cCSTAR03\u201d, \u201cIWSLT2004\u201d, \u201cIWLST2005\u201d, respectively, each with 16 references. \u201cNIST03\u201d is used for development."
        },
        {
            "heading": "5.2 Experimental Settings",
            "text": "We use six self-attention layers for both the encoder and the decoder. The embedding size and the hidden size are set to 512. Eight heads are used for self-attention. A feed-forward layer with 2048 cells and Swish (Ramachandran et al., 2018) is used as the activation function. Adam (Kingma and Ba, 2014) is used for training; warmup step is 16000; the learning rate is 0.0003. We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out (Gal and Ghahramani, 2016) probabilities are set to 0.1.\nWe extract a SMT phrase table on the bilingual training corpus by using moses (Koehn et al., 2007) with default setting, which is used for matching sentence pairs to generate augmented training data. We apply count-based pruning (Zens et al., 2012) to the phrase table, the threshold is set to 10.\nDuring decoding, similar to Hasler et al. (2018), Alkhouli et al. (2018) and Post and Vilar (2018), we make use of references to obtain gold constraints. Following previous work, prespecified translations for each source sentence are sampled from references and used by all systems for fair comparison.\nIn all the baseline systems, the vocabulary size is set to 50K on both sides. For \u201cData augmentation\u201d, to allow the source-side dictionary to cover target-side words, the target- and source-side vocabularies are merged for a new source vocabulary. For \u201cShared embeddings\u201d, the source vocabulary remains the same as the baselines, where the source-side target words use embeddings from target-side vocabulary."
        },
        {
            "heading": "5.3 System Configurations",
            "text": "We use an in-house reimplementation of Transformer, similar to Google\u2019s Tensor2Tensor. For the baselines, we reimplement Crego et al. (2016), as well as Post and Vilar (2018). BPE (Sennrich et al., 2015b) is used for all experiments, the operation is set to 50K. Our test sets cover news and ecommerce domains on En-Ru, and news and spoken language domains on Ch-En.\nBaseline 1: Using Placeholder. We combine Luong et al. (2014) and Crego et al. (2016). For\ngenerating placeholder tags during training, following Crego et al. (2016), we use a named entity translation dictionary which is extracted from Wikidata6. The dictionary is released together with e-commerce test sets, which is mentioned before. For Ch-En, the dictionary contains 285K person names, 746K location names and 1.6K organization names. For En-Ru, the dictionary contains 471K person names, 254K location names and 1.5K organization names. Additionally, we manually corrected a dictionary which contains 142K brand names and product names translation for En-Ru. By further leveraging word alignment in the same way as Luong et al. (2014), the placeholder tags are annotated with indices. We use FastAlign (Dyer et al., 2013) to generate word alignment. The amount of sentences containing placeholder tags is controlled to a ratio of 5% of the corpus. During decoding, pre-specified translations described in Section 5.2 are used.\nBaseline 2: Lexical Constraints. We reimplement Post and Vilar (2018), integrating their algorithm into our Transformer. Target-side words or phrases of pre-specified translations mentioned in Section 5.2 are used as lexical constraints.\n6https://www.wikidata.org\nOur System. During training, we use the method described in Section 3.1 to obtain the augmented training data. The SMT phrase table mentioned in Section 5.2 is used for \u201cIndexing\u201d and \u201cSampling\u201d. During decoding, pre-specified translations mentioned in Section 5.2 are used. The augmented data contain sampled sentences with one or two replacements on the source side. By applying the two sampling steps described in Section 3.1, about 10M and 6M augmented Ch-En and En-Ru sentences are generated, respectively. The final training corpora consists of both the augmented training data and the original training data."
        },
        {
            "heading": "5.4 Results",
            "text": "Comparison with Baselines. Our Transformer implementation can give comparable performance with state-of-the-art NMT (Junczys-Dowmunt et al., 2018), see \u201cTransformer\u201d and \u201cMarian\u201d in Table 1, which also shows a comparison of different methods on En-Ru. The lexical constraint method gives improvements on both the news and the e-commerce domains, compared with the Transformer baseline. The placeholder method also gives an improvement on the e-commerce\ndomain. The average improvement is calculated over all the test set results in each domain. In the news domain, the average improvement of our method is 3.48 BLEU higher compared with placeholder, and 2.94 over lexical constraints. In the e-commerce domain, the average improvement of our method is 1.34 BLEU compared with placeholder, and 2.63 with lexical constraints. Both shared embedding and pointer network are effective. Table 2 shows the same comparison on ChEn. In the spoken language domain, the average improvement is 1.35 BLEU compared with placeholder, and 0.42 with lexical constraints. In the news domain, the average improvement is 1.38 BLEU compared with placeholder, and 0.74 with lexical constraints.\nWe find that the placeholder method can only bring improvements on the En-Ru e-commerce test sets, since the pre-specified translations of the four e-commerce test sets are mostly entities, such as brand names or product names. Using placeholder tags to represent these entities leads to relatively little loss of word meaning. But on many of the other test sets, pre-specified translations are mostly vocabulary words. The placeholder tags fail to keep their word meaning during translation, leading to lower results.\nThe speed contrast between unconstrained NMT, lexical constraint and our method is shown in Table 3. The decoding speed of our method is equal to unconstrained NMT, and faster than the lexical constraint method, which confirms our in-\ntuition introduced earlier. Sample Outputs. Figure 3 gives a comparison of different system\u2019s translations. Given a Chinese source sentence, the baseline system fails to translate \u201c\u8ba1\u5212\u751f\u80b2\u201d adequately, as \u201cfamily planning\u201d is not a correct translation of \u201c\u8ba1\u5212\u751f\u80b2\u201d. In the pre-specified methods, the correct translation (\u201c\u8ba1\u5212\u751f\u80b2\u201d to \u201cplanned parenthood\u201d) is achieved through different ways.\nFor the placeholder method, the source phrase \u201c\u8ba1\u5212\u751f\u80b2\u201d is replaced with the placeholder tag \u201ctag1\u201d during pre-processing. After translation, output \u201ctag1\u201d is replaced with \u201cplanned parenthood\u201d as a post-processing step. However, the underlined word \u201cprogram\u201d is generated before \u201cplanned parenthood\u201d, which has no relationship with any source-side word. The source-side word \u201c\u534f\u4f1a\u201d, which means \u201cassociation\u201d, is omitted in translation. Through deeper analysis, the specific phrase \u201cprogram tag1\u201d occurs frequently in the training data. During decoding, using the hard tag leads to the loss of the source phrase\u2019s original meaning. As a result, the word \u201cprogram\u201d is incorrectly generated along with \u201ctag1\u201d.\nThe lexical constraints method regards the tar-\nget side of the pre-specified translation as a lexical constraint. Here the altered beam search algorithm fails to predict the constraint \u201cplanned parenthood\u201d during previous decoding steps. Although the constraint finally comes into effect, over translation occurs, which is highlighted by the underlined words. This is because the method enforces hard constraints, preventing decoding to stop until all constraints are met.\nOur method makes use of pre-specified translation by replacing the source-side phrase \u201c\u8ba1\u5212\u751f \u80b2\u201d with the target-side translation \u201cplanned parenthood\u201d, copying the desired phrase to the output along with the decoding procedure. The translation \u201cassociation of planned parenthood from providing\u201d is the exact translation of the sourceside phrase \u201c\u8ba1\u5212(planned) \u751f\u80b2(parenthood) \u534f \u4f1a(association)\u63d0\u4f9b(providing)\u201d, and agrees with the reference, \u201cplanned parenthood to provide\u201d."
        },
        {
            "heading": "5.5 Analysis",
            "text": "Effect of Using More Pre-specified Translations. Even though the augmented training data have only one or two replacements on the source side, the model can translate a source sentence with up to five replacements. Figure 4 shows that compared with unconstrained Transformer, the translation quality of our method keeps increasing when the number of replacements increases, since more pre-specified translations are used.\nWe additionally measure the effect on the ChEn WMT test sets, namely \u201cnewsdev2017\u201d, \u201cnewstest2017\u201d, \u201cnewstest2018\u201d, respectively, each having only one reference instead of four. The baseline BLEU scores on these three test sets are 18.49, 20.01 and 19.05, respectively. Our method gives BLEU scores of 20.56, 22.3, 21.08, respectively, when using one or two pre-specified translations for each sentence. The increased BLEU when utilizing different number of pre-specified translations is shown in Figure 4. We found that the improvements on WMT test sets are more significant than on NIST, since pre-specified translations are sampled from one reference only, enforcing the output to match this reference. The placeholder method does not give consistent improvements on news test sets, due to the same reason as mentioned earlier.\nAs shown in Figure 5, the copy success rate of our method does not decrease significantly when the number of replacements grows. Here, a copy success refers a pre-specified target translation that can occur in the output. The placeholder method achieves a higher copy success rate than ours when the number of replacements is 1, but the copy success rate decreases when using more pre-specified translations. The copy success rate of the lexical constraint method is always 100%, since it imposes hard constraints rather than soft constraints. However, as discussed earlier, overall translation quality can be harmed as a cost of satisfying decoding constraints by their method.\nIn the presented experiment results, the highest copy success rate of our method is 90.54%, which means a number of source-side target words or phrases are not successfully copied to the translation output. This may be caused by the lack of training samples for certain target-side words or phrases. In En-Ru, we additionally train a model with augmented data that is obtained by matching\nan SMT phrase table without any pruning strategy. The copy success rate can reach 98%, even without using \u201cshared embedding\u201d and \u201cpointer network\u201d methods.\nEffect of Shared Embeddings and Pointer Network. The gains of shared embeddings and pointer network are reflected in both the copy success rate and translation quality. As shown in Table 4, when using one pre-specified translation for each source sentence, the copy success rate improves on various test sets by integrating shared embeddings and pointer network, demonstrating that more pre-specified translations come into effect. Table 1 and Table 2 earlier show the improvement of translation quality.\nTranslating non Code-Switched Sentences. Our method preserves its strength on translating non code-switched sentences. As shown in Table 5, the model trained on the augmented corpus has comparable strength on translating unreplaced sentences as the model trained on the original corpus. In addition, on some test sets, our method is slightly better than the baseline when translating non code-switched source sentences. This can be explained from two aspects: First, the augmented data make the model more robust to perturbed inputs; Second, the pointer network makes the model better by copying certain sourceside words (Gulcehre et al., 2016), such as nontransliterated named entities."
        },
        {
            "heading": "6 Conclusion",
            "text": "We investigated a data augmentation method for constraining NMT with pre-specified translations, utilizing code-switched source sentences and their translations as augmented training data. Our method allows the model to learn to translate\nsource-side target phrases by \u201ccopying\u201d them to the output, achieving consistent improvements over previous lexical constraint methods on large NMT test sets. To the best of our knowledge, we are the first to leverage code switching for NMT with pre-specified translations."
        },
        {
            "heading": "7 Future Work",
            "text": "In the future, we will study how the copy success rate and the BLEU scores interact when different sampling strategies are taken to obtain augmented training corpus and when the amount of augmented data grows. Another direction is to validate the performance when applying this approach to language pairs that contain a number of identical letters in their alphabets, such as English to French and English to Italian."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their detailed and constructed comments. Yue Zhang is the corresponding author. The research work is supported by the National Natural Science Foundation of China (61525205). Thanks for Shaohui Kuang, Qian Cao, Zhongqiang Huang and Fei Huang for their useful discussion."
        }
    ],
    "title": "Code-Switching for Enhancing NMT with Pre-Specified Translation",
    "year": 2019
}