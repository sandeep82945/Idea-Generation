{
    "abstractText": "The abundance of information in digital media, which in today\u2019s world is the main source of knowledge about current events for the masses, makes it possible to spread disinformation on a larger scale than ever before. Consequently, there is a need to develop novel fake news detection approaches capable of adapting to changing factual contexts and generalizing previously or concurrently acquired knowledge. To deal with this problem, we propose a lifelong learning-inspired approach, which allows for fake news detection in multiple languages and the mutual transfer of knowledge acquired in each of them. Both classical feature extractors, such as Term frequency-inverse document frequency or Latent Dirichlet Allocation, and integrated deep nlp (Natural Language Processing) bert (Bidirectional Encoder Representations from Transformers) models paired with mlp (Multilayer Perceptron) classifier, were employed. The results of experiments conducted on two datasets dedicated to the fake news classification task (in English and Spanish, respectively), supported by statistical analysis, confirmed that utilization of additional languages could improve performance for traditional methods. Also, in some cases supplementing the deep learning method with classical ones can positively impact obtained results. The ability of models to generalize the knowledge acquired between the analyzed languages was also observed.",
    "authors": [
        {
            "affiliations": [],
            "name": "J\u0119drzej Kozal"
        },
        {
            "affiliations": [],
            "name": "Micha\u0142 Le\u015b"
        },
        {
            "affiliations": [],
            "name": "Pawe\u0142 Zyblewski"
        },
        {
            "affiliations": [],
            "name": "Pawe\u0142 Ksieniewicz"
        },
        {
            "affiliations": [],
            "name": "Micha\u0142 Wo\u017aniak"
        }
    ],
    "id": "SP:fc8c0d3a59a435eed34ea6024c9048d9db3c7d43",
    "references": [
        {
            "authors": [
                "P. Bharadwaj",
                "Z. Shao"
            ],
            "title": "Fake news detection with semantic features and text mining",
            "venue": "International Journal on Natural Language Computing (IJNLC) Vol 8",
            "year": 2019
        },
        {
            "authors": [
                "M. Bilal",
                "H.A. Habib",
                "Z. Mehmood",
                "T. Saba",
                "M. Rashid"
            ],
            "title": "Single and multiple copy\u2013move forgery detection and localization in digital images based on the sparsely encoded distinctive features and dbscan clustering",
            "venue": "Arabian Journal for Science and Engineering 45(4), 2975\u20132992",
            "year": 2020
        },
        {
            "authors": [
                "L. Bondi",
                "S. Lameri",
                "D. Guera",
                "P. Bestagini",
                "E.J. Delp",
                "S Tubaro"
            ],
            "title": "Tampering detection and localization through clustering of camera-based cnn features",
            "venue": "CVPR Workshops. vol. 2",
            "year": 2017
        },
        {
            "authors": [
                "T.B. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell",
                "S. Agarwal",
                "A. Herbert-Voss",
                "G. Krueger",
                "T. Henighan",
                "R. Child",
                "A. Ramesh",
                "D.M. Ziegler",
                "J. Wu",
                "C. Winter",
                "C. Hesse",
                "M. Chen",
                "E. Sigler",
                "M. Litwin",
                "S. Gray",
                "B. Chess",
                "J. Clark",
                "C. Berner",
                "S. McCandlish",
                "A. Radford",
                "I. Sutskever",
                "D. Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "CoRR abs/2005.14165",
            "year": 2020
        },
        {
            "authors": [
                "J. Ca\u00f1ete",
                "G. Chaperon",
                "R. Fuentes",
                "J.H. Ho",
                "H. Kang",
                "J. P\u00e9rez"
            ],
            "title": "Spanish pretrained bert model and evaluation data",
            "venue": "PML4DC at ICLR 2020",
            "year": 2020
        },
        {
            "authors": [
                "M. Chora\u015b",
                "K. Demestichas",
                "A. Gie\u0142czyk",
                "\u00c1. Herrero",
                "P. Ksieniewicz",
                "K. Remoundou",
                "D. Urda",
                "M. Wo\u017aniak"
            ],
            "title": "Advanced machine learning techniques for fake news (online disinformation) detection: A systematic mapping study",
            "venue": "Applied Soft Computing 101, 107050",
            "year": 2021
        },
        {
            "authors": [
                "A. Conneau",
                "D. Kiela",
                "H. Schwenk",
                "L. Barrault",
                "A. Bordes"
            ],
            "title": "Supervised learning of universal sentence representations from natural language inference data",
            "venue": "arXiv preprint arXiv:1705.02364",
            "year": 2017
        },
        {
            "authors": [
                "J. Devlin",
                "M.W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805",
            "year": 2018
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "year": 2018
        },
        {
            "authors": [
                "F. Girosi",
                "M. Jones",
                "T. Poggio"
            ],
            "title": "Regularization theory and neural networks architectures",
            "venue": "Neural Computation 7(2), 219\u2013269",
            "year": 1995
        },
        {
            "authors": [
                "Z.S. Harris"
            ],
            "title": "Distributional structure",
            "venue": "Word 10(2-3), 146\u2013162",
            "year": 1954
        },
        {
            "authors": [
                "N. Hassan",
                "W. Gomaa",
                "G. Khoriba",
                "M. Haggag"
            ],
            "title": "Credibility detection in twitter using word n-gram analysis and supervised machine learning techniques",
            "venue": "International Journal of Intelligent Engineering and Systems 13(1), 291\u2013300",
            "year": 2020
        },
        {
            "authors": [
                "M. Hoffman",
                "D. Blei",
                "F. Bach"
            ],
            "title": "Online learning for latent dirichlet allocation",
            "venue": "vol. 23, pp. 856\u2013864",
            "year": 2010
        },
        {
            "authors": [
                "K.S. Jones"
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of documentation",
            "year": 1972
        },
        {
            "authors": [
                "S. Kaur",
                "P. Kumar",
                "P. Kumaraguru"
            ],
            "title": "Automating fake news detection system using multi-level voting model",
            "venue": "Soft Computing 24(12), 9049\u20139069",
            "year": 2020
        },
        {
            "authors": [
                "D. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations",
            "year": 2014
        },
        {
            "authors": [
                "P. Ksieniewicz",
                "M. Chora\u015b",
                "R. Kozik",
                "M. Wo\u017aniak"
            ],
            "title": "Machine learning methods for fake news classification",
            "venue": "International Conference on Intelligent Data Engineering and Automated Learning. pp. 332\u2013339. Springer",
            "year": 2019
        },
        {
            "authors": [
                "S. Kumar",
                "K.M. Carley"
            ],
            "title": "Tree lstms with convolution units to predict stance and rumor veracity in social media conversations",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5047\u20135058",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "Stoyanov",
                "V.: Roberta"
            ],
            "title": "A robustly optimized BERT pretraining approach",
            "venue": "CoRR abs/1907.11692",
            "year": 1907
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Fixing weight decay regularization in adam",
            "year": 2017
        },
        {
            "authors": [
                "H.P. Luhn"
            ],
            "title": "A statistical approach to mechanized encoding and searching of literary information",
            "venue": "IBM Journal of research and development 1(4), 309\u2013317",
            "year": 1957
        },
        {
            "authors": [
                "J. Posadas Dur\u00e1n",
                "H. Gomez Adorno",
                "G. Sidorov",
                "J. Moreno"
            ],
            "title": "Detection of fake news in a new corpus for the spanish language",
            "venue": "Journal of Intelligent & Fuzzy Systems 36, 4869\u20134876",
            "year": 2019
        },
        {
            "authors": [
                "J. Posetti",
                "A. Matthews"
            ],
            "title": "A short guide to the history of \u2018fake news\u2019 and disinformation",
            "venue": "International Center for Journalists 7(2018), 2018\u201307",
            "year": 2018
        },
        {
            "authors": [
                "A. Radford",
                "K. Narasimhan",
                "T. Salimans",
                "I. Sutskever"
            ],
            "title": "Improving language understanding with unsupervised learning",
            "year": 2018
        },
        {
            "authors": [
                "A. Radford",
                "J. Wu",
                "R. Child",
                "D. Luan",
                "D. Amodei",
                "I. Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "cloudfront.net/better-language-models/language-models.pdf",
            "year": 2018
        },
        {
            "authors": [
                "\u00c1.I. Rodr\u00edguez",
                "L.L. Iglesias"
            ],
            "title": "Fake news detection using deep learning",
            "venue": "arXiv preprint arXiv:1910.03496",
            "year": 2019
        },
        {
            "authors": [
                "E. Saquete",
                "D. Tom\u00e1s",
                "P. Moreda",
                "P. Mart\u00ednez-Barco",
                "M. Palomar"
            ],
            "title": "Fighting post-truth using natural language processing: A review and open challenges",
            "venue": "Expert systems with applications 141, 112943",
            "year": 2020
        },
        {
            "authors": [
                "P. Suryawanshi",
                "P. Padiya",
                "V. Mane"
            ],
            "title": "Detection of contrast enhancement forgery in previously and post compressed jpeg images",
            "venue": "2019 IEEE 5th International Conference for Convergence in Technology (I2CT). pp. 1\u20134. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "H. Telang",
                "S. More",
                "Y. Modi",
                "L. Kurup"
            ],
            "title": "Anempirical analysis of classification models for detection of fake news articles",
            "venue": "2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT). pp. 1\u20137. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "H.E. Wynne",
                "Z.Z. Wint"
            ],
            "title": "Content based fake news detection using n-gram models",
            "venue": "Proceedings of the 21st International Conference on Information Integration and Web-based Applications & Services. pp. 669\u2013673",
            "year": 2019
        },
        {
            "authors": [
                "K. Xu",
                "F. Wang",
                "H. Wang",
                "B. Yang"
            ],
            "title": "Detecting fake news over online social media via domain reputations and content understanding",
            "venue": "Tsinghua Science and Technology 25(1), 20\u201327",
            "year": 2019
        },
        {
            "authors": [
                "D. Zhang",
                "L. Zhou",
                "J.L. Kehoe",
                "I.Y. Kilic"
            ],
            "title": "What online reviewer behaviors really matter? effects of verbal and nonverbal behaviors on detection of fake online reviews",
            "venue": "Journal of Management Information Systems 33(2), 456\u2013481",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhou",
                "R. Zafarani"
            ],
            "title": "Network-based fake news detection: A pattern-driven approach",
            "venue": "ACM SIGKDD explorations newsletter 21(2), 48\u201360",
            "year": 2019
        },
        {
            "authors": [
                "A. Zubiaga",
                "G. Wong Sak Hoi",
                "M. Liakata",
                "R. Procter"
            ],
            "title": "Pheme dataset of rumours and non-rumours (Oct 2016",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords: natural language processing \u00b7 lifelong learning \u00b7 classifier ensemble \u00b7 transformers \u00b7 bidirectional encoder \u00b7 representation learning \u00b7 bert"
        },
        {
            "heading": "1 Introduction",
            "text": "It is almost a cliche to say that the modern economy and society strongly depend on information. Therefore we all expect that the information we are provided with will be reliable and credible, enabling us to make rational decisions. This is the ideal world, and the role of information and disinformation has been appreciated since ancient times and information manipulation has become a\nar X\niv :2\n20 6.\n11 86\n7v 1\n[ cs\n.C L\n] 2\n5 M\ncritical weapon used to gain political or material benefits [23]. Nowadays, the problem with the use of so-called fake news is strongly noticed because such a scale of manipulation has not been noted before. One can think of the news spread related to the COVID-19 pandemic or the Russian invasion of Ukraine.\nThis problem is particularly evident in digital media, hence almost all global Internet platforms, such as Facebook or Twitter, indicate in their terms of service the mechanisms for verifying information. Unfortunately, manual verification of the information veracity and relying on human experts as fact-checkers is relatively slow. MIT Media Lab proves, that fake news travels farther and faster than the news that is absolutely legit1. Hence, one of the challenges is to develop mechanisms that can detect fake news automatically and have the ability to improve their model continuously.\nAmong the ML-based approaches to fake news detection, the literature distinguishes methods for [6]:\n\u2013 Text analysis consists of analyzing the Natural Language Processing (nlp) data representation without the linguistic context [27], as well as the psycholinguistic analysis [32] and processing the syntactic text structure [18]. \u2013 Reputation analysis, which measures the reputation of an article and publisher based on sources such as content, reviews, domain, IP address or anonymity [31]. \u2013 Network analysis, which is related to the graph theory, and is performed to evaluate the truthfulness of the information [33]. \u2013 Image-manipulation recognition, which are dedicated for the detection of image tampering [3], copy-move forgeries [2], and other image modifications such as contrast enhancement [28].\nThis work deals with nlp, and introduces a training procedure for a fake news detection model that can classify news provided in different languages and perform knowledge transfer between them."
        },
        {
            "heading": "2 Related Works",
            "text": "Fake news detection with nlp is based on the extraction of features directly from the content of the analyzed texts, without taking into account their social context, and is currently the basis for each of the subtasks included in the detection of fake news [27]. Many of the classic nlp methods are based on bag-of-words approach [11], which creates a vector for each document containing information about the number of times each word appears in it. An example of an extension of this approach may be n-grams, which can tokenize a sequence of words of a given length, rather than focusing only on individual words. The minimum range of n-grams is one (so-called unigrams), while its upper limit strongly depends on the length of the analyzed documents. Additionally, the weights of individual 1 https://www.technologyreview.com/2018/03/08/144839/fake-news-spreads-fasterthan-the-truth-and-its-all-our-fault/\nwords can be normalized for a specific document using Term Frequency (tf) [21], or for the entire analyzed corpus, using Term Frequency-Inverse Document Frequency (tf-idf) [14]. Another noteworthy approach to extracting features from text is Latent Dirichlet Allocation (lda), which is a generative probabilistic model for topic modeling [13].\nSuch approaches, despite their simplicity, are successfully used in the literature in the task of recognizing fake news. Hassan et al classified Tweets as credible and not credible, using five different classification algorithms and wordbased n-gram analysis including tf and tf-idf [12]. The authors have proven the effectiveness of this approach on the pheme benchmark dataset [34]. Bharadwaj and Shao employed reccurent neural networks, random forest classifier and naive bayes classifier coupled with n-grams of various lengths in order to classify fake news dataset from Kaggle [1]. Similar experiments, including e.g. n-grams, tf-idf and Gradient Boosting Classifier, were conducted by Wynne and Wint [30]. Kaur et al. and Telang et al. evaluated multiple combinations of various classifier and extraction techniques in the fake news detection task [15,29]. Usage of the ensemble methods for disinformation detection was explored in [17].\nCurrently, deep neural networks Deep Neural Networks (dnn) based on pretraining general language representations are very popular in fake news detecting. [26]. A strong point of the currently used goings, which additionally fine-tune the token representations using supervised learning, is that a few parameters need to be learned from scratch only [24]. Recently, many works show the usefulness of knowledge transfer in the case of natural language inference [7]. bert (Bidirectional Encoder Representations from Transformers) is particularly recognizable, which was designed for training with the use of unlabeled text [8]. Then, bert is fine-tuned for a given problem with an additional output layer."
        },
        {
            "heading": "3 Methods",
            "text": "Each recognition system dedicated to language data is based on two basic steps. The first is vectorization, which allows for the transformation of a set of textual documents into a numerical data set interpretable by modeling procedure, and the second is an appropriate induction that allows for the construction of a model in a problem marked with bias. These procedures can be conducted separately, in a serial manner, as in the case of the classic tf-idf and mlp pair, where the basic extractor determines the vector representation and the neural network fits in the space designated by it, or jointly, in a parallel manner, as is the case in deep models, such as bert.\nRegardless of the integrated processing procedure typical of deep learning approaches, in which extraction and induction take place within the same structure, also in models such as bert we can delineate the constructive elements responsible for the extraction and \u2013 at the end of the neural network structure \u2013 fully connected layers, which in their logic are identical to typical, shallow mlp networks. This creates some potential for integrating classical and deep feature extraction methods for nlp. The deep model, getting trained on the basis of a\ngiven bias, trains weights responsible for both extraction of attributes and classification, updating them so that they are better suited to the construction of the proper problem space. It is therefore possible to separate the extraction part of the deep learning model and \u2013 using typical propagation \u2013 use it to obtain attributes for the classification model in the same manner as classical extraction methods such as tf-idf.\nTherefore, in the proposed method, the mlp model is used as a base classifier, the default in the structure for deep extractors, and at the same time, good at optimizing the decision space for classical extraction methods. Moreover, for the analysis of the integration potential of heterogeneous extractors, three methods commonly used in the literature were selected:\n\u2013 Term Frequency-Inverse Document Frequency \u2013 legacy model based on normalization to document (tf) and to corpora (idf) as the most basic method currently used in nlp applications. \u2013 Latent Dirichlet Allocation \u2013 a generative statistical model, similar to the latest unsupervised learning methods, but using approaches to identify significant thematic factors. \u2013 Bidirectional Encoder Representations from Transformers \u2013 the current stateof-the-art model, being a clear baseline in contemporary nlp experiments.\nThe bert model, unlike tf-idf and lda, is not a language-independent nlp task solver. To enable its operation in a multilingual environment, base models were selected, trained on different corpora, with a similar structure size. First was standard version pre-trained on English corpus (bert), second beto [5] - variant of bert architecture pre-trained on Spanish dataset (bertspa), and third, the standard bert pre-trained on a multilingual corpus (bertmult).\nnlp extractors, regardless of the procedure used, tend to generate many redundant attributes, significantly increasing the optimization space of the neural network. In the case of an integration architecture based on the concatenation of features obtained from a heterogeneous pool of extractors, this tendency will be further deepened. Additionally, different extraction contexts may also allow for the diversification of models, where different processing algorithms allow for alternative insight on the data. For this proposal, the following solutions will be used:\n\u2013 Mutual Information (mi) \u2013 Analysis of Variance (anova) \u2013 Principal Components Analysis (pca)\nWith the use of all the tools introduced above, it is possible to propose two alternative architectures for integrating the extractors. They are presented in Figure 1, compiling the processing schemes of sis and ers policies.\nAs part of the Simple Integration Schema policy (sis), each of the extractors \u2013 independently \u2013 builds an extraction model which, after processing the available language corpora (A, B and C corpora) for each of them, allows for the transformation from a text into a vector representation by pre-limited, constant\ndimensionality. For each transformation, a separate classifier is built, which finally goes to the pools of models dedicated to different corpora, integrated internally and externally through the accumulation of support. Internal integration is carried out classically, and external integration depends on the source languages of the corpora. In the case of a linguistically homogeneous pool, the accumulation of support is not modified. Otherwise, (a) the number of languages is a multiplier of the number of problem classes, (b) monolingual models receive zero foreign language support, and (c) multilingual models replicate class supports divided by the language multiplier to preserve the complementarity condition of the support vector.\nThe Extractor Reduction Schema (ers) strategy performs the extraction in the same way for sis. However, models are not built for each of the extractors, and the object representations obtained within the corpora are concatenated into\na wide, common space of the problem. The space dimensionality is reduced to a predetermined size, i.e. upper limit of the number of attributes per extractor. A separate classification model is trained for each corpora, constituting a narrow pool that is integrated in the same way as external sis ensemble policy.\nThe aim of the research is to obtain experimentally verified information about the appropriate integration strategy of classifier ensembles for the needs of multilingual natural language processing systems. Both the potentials of various extraction methods \u2013 in each of the selected corpora \u2013 and the capabilities of the various dimension reducers in the ers policy will be verified."
        },
        {
            "heading": "4 Experiment",
            "text": "The experiments aim to analyze how knowledge gained from different languages can positively affect the overall classification performance. During the experimental evaluation, we intend to answer the following research questions:\n1. How does fake news classification performance depend on different extraction methods? 2. How do ensemble models diversified by various extractors, training sets and training features improve fake-news classification? 3. Can we utilize models trained with different language corpora to improve other language classification? 4. What methods for feature integration can be useful in a multilingual environment?"
        },
        {
            "heading": "4.1 Setup",
            "text": "Datasets. Two datasets were utilized in this study, namely: The Spanish Fake News Corpus 2 [22] and Kaggle Fake News: Build a system to identify unreliable news articles3. These datasets were treated as a representation of fake-news distribution in Spanish and English, respectively. The third variant of a dataset was a mixed dataset that combined both Spanish and English datasets. Detailed information about the number of samples for each dataset is given in Table 1. As we can see both Spanish and English datasets are internally balanced. However, please note a significant, 1:31 imbalance in the presence of the source datasets in mixed article collection. For this reason in further parts we report balanced accuracy as the evaluation metric, which in case of mixed dataset actually shows the quality of the four-class problem \u2013 sensitizing itself to errors performed on minority language. Data preprocessing. Each word was changed to lowercase before computing tf-idf features. For this extraction method, features were computed based on unigrams. We remove words that occur in less than 20 documents or more than half of all documents during lda preprocessing. lda features were computed with 2 https://github.com/jpposadas/FakeNewsCorpusSpanish 3 https://www.kaggle.com/c/fake-news/data\nchunk size 2000, 20 passes, and 400 iterations. Both tf-idf and lda extracted 100 features from each document. Classification models. MLP used for training on extracted features had two hidden layers with 500 neurons each and was trained by Adam optimizer [16] with a learning rate 0.001 for 200 epochs with Early Stopping [10]. bert-based models were trained for 5 epochs each with AdamW [20] optimizer and learning rate 3e-5. Experimental protocol. To obtain more reliable results 5x2 cross validation was employed. However, there is a significant difference in the number of English and Spanish datasets samples. For this reason, stratification was introduced for Spanish and English labels in the case of the mixed dataset. This modification allows avoiding over-representation of learning examples from one language in some folds. Result analysis. To analyse the classification performance of the chosen algorithms we employed a 5x2cv combined F test for statistical analysis with p-value of 0.05. Implementation and reproducibility. Github repository with code for all experiments is available online 4."
        },
        {
            "heading": "4.2 Results",
            "text": "E1 Classification based on single feature extraction method Results of classification accuracy obtained for a single feature extraction methods are presented in Tables 2 and 3. We use these values as a baseline for further experiments. We state a significant difference between all deep models and tf-idf and lda for text and title attributes. Additionally, for text attribute bert was better than bertmult for the kaggle dataset, lda than tf-idf for the kaggle and mixed datasets. Solution based on bertspa was better than bertmult for esp fake dataset and better than any other model for the mixed dataset.\nE2 Classification based on multiple feature extraction methods In this part of experiments we create ensemble according to Simple Integration Schema and Extractor Reduction Schema introduced earlier. We use multiple base models 4 https://github.com/w4k2/nn-nlp\ntrained with the same language. Results are presented in tables 4 and 5. pca was significantly worse than sa for esp fake and mixed and all other methods for the mixed dataset with text attribute. For title attribute, pca was better than sa with the kaggle dataset, sa was better than all other methods for esp fake, pca was worse than other methods, and minfo and anova were better than sa for the mixed dataset.\nE3 Classification with different languages We utilize a single extraction method trained on different datasets to precompute features for one dataset. Extracted features are concatenated. Then, these features are used to train the single classifier. The impact of different language features on fake news classification performance is presented in table 6 and 7. To keep the presentation of the results consistent, the statistical analysis was carried out by comparing different models while keeping the dataset and ensemble construction method the same. No significant difference was found for text or title attributes.\nE4 Classification with different extraction methods and languages Classification performance of ensemble using a combination of all languages and extraction methods is presented in tables 8 and 9. For text attribute, we state meaningful differences between pca and the rest of the methods and between minfo and sa for the kaggle dataset and the difference between ANVOA and pca for the mixed dataset. For title attribute, we found that pca is better than minfo for kaggle, sa is better than pca for esp fake. For mixed dataset, anova is better than all other methods, sa is better than pca, and minfo is better than sa and pca.\nVisualization of learned features To provide more insight into learned representations, we visualize average feature vectors. We consider features for the dataset that the extractor was trained on and features generated by applying the trained extractor to two other datasets. Next, we plot the average feature vector as a heatmap to show the most active features for each training-evaluation dataset pair. Results are presented in Fig. 2. First, we focus on plots for tf-idf and lda. Most active features are obtained for the same dataset that the extractor was trained on. When applying the extractor to the datasets with a different language, some features have significantly higher values. This indicates that ap-"
        },
        {
            "heading": "MINFO",
            "text": ""
        },
        {
            "heading": "ANOVA",
            "text": ""
        },
        {
            "heading": "PCA",
            "text": "plying the extraction model to a different language can provide an additional source of information that can be beneficial for classification quality. Furthermore, it can explain why exploiting extractors trained on different languages improved performance for tf-idf and lda in the third experiment.\nThese findings are strictly connected to the inner workings of extraction algorithms. When extracting features with tf-idf first step is to create vocabulary from the corpus. Next, feature vectors are computed based on the frequency of each word. When processing text with words that mostly fall outside of vocabulary, tf-idf will return a sparse vector. Similar reasoning can be applied to lda."
        },
        {
            "heading": "MINFO",
            "text": ""
        },
        {
            "heading": "ANOVA",
            "text": ""
        },
        {
            "heading": "PCA",
            "text": "Transformers use a different procedure for text preprocessing. bert tokenizer can divide a single word into separate tokens if it falls outside of vocabulary. Then these tokens are used to generate word embeddings that are passed through the transformer network. As a result, when dealing with different languages model will not produce sparse output but some vector that may or may not contain useful information. This is shown in the two lowest rows of Fig 2. There is no significant difference between beto average features, regardless of whether they are computed with the same language model was trained on or not. This in\nturn, can explain why we observe a decrease in accuracy for the deep models in Experiment 3."
        },
        {
            "heading": "4.3 Lessons learned",
            "text": "When analyzing accuracy obtained for single extractors (Tab. 2 and 3) one can notice that deep learning-based methods obtain best results. This is expected, as deep models currently dominate nlp field [9,25,4]. One can also notice that models dedicated for a specific language obtained better results than MultiBERT. This can be explained by a lower number of learning examples in datasets utilized in our experiments. MultiBERT is the largest model. Therefore, it can be prone to overfitting with a smaller amount of data. Also, it is worth noting that the difference in performance between beto and MultiBERT is larger than between the English version of bert and MultiBERT. This is probably caused by an improved pretraining procedure from RoBERTa [19], that was used in beto. Better pretraining can lead to improved performance in downstream tasks [9,25]. Although MultiBERT was initially trained with a multi-language corpus and has the largest number of parameters, it is not the best model for the mixed dataset.\nUnfortunately, we found no good explanation for this phenomenon. Results obtained for the Spanish dataset are worse compared to kaggle and mixed. This can be easily explained by a larger number of samples in the kaggle dataset compared to esp fake (please refer to Tab. 1). These findings provide an answer to research question 1. Deep models obtain better or similar results when comparing text and title attributes. Unfortunately, the same cannot be stated about tf-idf and lda, where performance for the title attribute is worse in most cases.\nRegardless of the ensemble construction methodology obtained accuracy is close for all methods (Tab. 4 and 5). Comparing values of metrics to the first experiment, we can state that employing an ensemble can benefit accuracy only for the English dataset. This is probably caused by the close performance of all models for the kaggle dataset. In the case of the Spanish dataset, there is a clear gap between beto and the rest of the models, and in the case of the mixed dataset, there is a gap between deep learning models and two other. When combining strong classifiers with weaker ones in one ensemble, we can obtain worse results than for the strongest classifier. In this case, the utilization of additional models with worse performance is more harmful than helpful. These results answer research question 2.\nIn the third experiment feature extractor trained on a single dataset was used to extract features from all datasets and create a new ensemble from 3 different sets of features (Tab. 6 and 7). This approach is beneficial for tfidf and lda but detrimental for deep models. The statistical analysis results show no significant differences between tf-idf, lda, and deep models. This finding is quite interesting due to the clear dominance of deep models in the first experiment. It is important to note that deep models achieved worse results than in the first experiment; however, at the same time, there was a significant gain in performance for tf-idf and lda. We provide a possible explanation when discussing feature visualization from Fig 2. This answers research question 3.\nUtilizing both different models and features extracted for different languages (Tab. 8 and 9) does not provide an improvement over baselines, nor constructing ensembles with the utilization of different models. These findings, combined with conclusions from Experiment 2, answer research question 4."
        },
        {
            "heading": "5 Conclusions",
            "text": "This study aimed to apply nlp methods to the problem of detecting misinformation in messages produced in different languages and verify whether it is possible to transfer knowledge between models trained for different languages.\nBased on the results of the experimental studies, it was not found that different models or attributes extracted for different languages led to a noticeable, i.e., statistically significant, improvement over the baseline results. Also, no significant improvement was confirmed using methods based on the classifier ensemble concept. The limited scope of datasets contents can explain this. Articles in the Spanish dataset are written about local affairs. Therefore intersections of topics between two datasets can be small. Moreover, this indicates that preparing a fake-news detection model with a single language and utilizing learned knowledge for other languages is a difficult problem and should be further explored in research. Having this in mind, future work should focus on preparing articles and datasets for multiple languages. By utilizing the same type of article topics in multiple languages, one can obtain better results. Our work showed that some classes of models could benefit from utilizing multiple languages, so it is reasonable to expect that aligning of topics should further improve results.\nIn the calculation of the achieved results, as well as the literature analysis, it seems that the direction of further work may be related to the transfer of knowledge, but rather within a single language, while it seems attractive to transfer knowledge between tasks of fake news detection, but different subject areas."
        }
    ],
    "title": "Lifelong Learning Natural Language Processing Approach for Multilingual Data Classification",
    "year": 2022
}