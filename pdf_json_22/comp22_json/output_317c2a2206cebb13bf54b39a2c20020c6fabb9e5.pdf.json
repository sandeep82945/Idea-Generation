{
    "abstractText": "It has been shown that Recurrent Artificial Neural Networks automatically acquire some grammatical knowledge in the course of performing linguistic prediction tasks. The extent to which such networks can actually learn grammar is still an object of investigation. However, being mostly data-driven, they provide a natural testbed for usage-based theories of language acquisition. This mini-review gives an overview of the state of the field, focusing on the influence of the theoretical framework in the interpretation of results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ludovica Pannitto"
        },
        {
            "affiliations": [],
            "name": "Aurelie Herbelot"
        }
    ],
    "id": "SP:14e48528e0b6f86695ea6fc8eb3bbf3d837f78b8",
    "references": [
        {
            "authors": [
                "A. Alishahi",
                "G. Chrupa\u0142a",
                "T. Linzen"
            ],
            "title": "Analyzing and interpreting neural networks for NLP: a report on the first BlackboxNLP workshop",
            "venue": "Nat. Lang. Eng",
            "year": 2019
        },
        {
            "authors": [
                "S. Arehalli",
                "T. Linzen"
            ],
            "title": "Neural language models capture some, but not all, agreement attraction effects,",
            "venue": "CogSci",
            "year": 2020
        },
        {
            "authors": [
                "M. Baroni"
            ],
            "title": "Linguistic generalization and compositionality in modern artificial neural networks",
            "venue": "Philos. Trans. R. Soc. Lond. B Biol",
            "year": 2020
        },
        {
            "authors": [
                "L.W. Barsalou"
            ],
            "title": "The instability of graded structure: implications for the nature of concepts,\u201d in Concepts and Conceptual Development: Ecological and Intellectual Factors in Categorization, ed U. Neisser",
            "year": 1987
        },
        {
            "authors": [
                "J.K. Boyd",
                "A.E. Goldberg"
            ],
            "title": "Input effects within a constructionist framework.Mod",
            "venue": "Lang. J",
            "year": 2009
        },
        {
            "authors": [
                "C. Chelba",
                "T. Mikolov",
                "M. Schuster",
                "Q. Ge",
                "T. Brants",
                "P Koehn"
            ],
            "title": "One billion word benchmark for measuring progress in statistical language modeling",
            "year": 2013
        },
        {
            "authors": [
                "S.A. Chowdhury",
                "R. Zamparelli"
            ],
            "title": "RNN simulations of grammaticality judgments on long-distance dependencies,",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics (Association for Computational Linguistics) Santa Fe,",
            "year": 2018
        },
        {
            "authors": [
                "M.H. Christiansen"
            ],
            "title": "Implicit statistical learning: a tale of two literatures",
            "venue": "Top. Cogn. Sci.,",
            "year": 2019
        },
        {
            "authors": [
                "M.H. Christiansen",
                "N. Chater"
            ],
            "title": "The now-or-never bottleneck: a fundamental constraint on language",
            "venue": "Behav. Brain Sci",
            "year": 2015
        },
        {
            "authors": [
                "M.H. Christiansen",
                "N. Chater"
            ],
            "title": "Creating Language: Integrating Evolution, Acquisition, and Processing",
            "year": 2016
        },
        {
            "authors": [
                "H. Cornish",
                "R. Dale",
                "S. Kirby",
                "M.H. Christiansen"
            ],
            "title": "Sequence memory constraints give rise to language-like structure through iterated learning",
            "venue": "PLoS ONE",
            "year": 2017
        },
        {
            "authors": [
                "F. Davis",
                "M. van Schijndel"
            ],
            "title": "Discourse structure interacts with reference but not syntax in neural language models",
            "venue": "Proc. 24th Conf. Comput. Nat. Lang. Learn. 396\u2013407",
            "year": 2020
        },
        {
            "authors": [
                "F. Davis",
                "M. van Schijndel"
            ],
            "title": "Recurrent neural network language models always learn English-like relative clause attachment,",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "C. Dyer",
                "A. Kuncoro",
                "M. Ballesteros",
                "N.A. Smith"
            ],
            "title": "Recurrent neural network grammars,",
            "venue": "Proceedings of the Conference (Association for Computational Linguistics (ACL)),",
            "year": 2016
        },
        {
            "authors": [
                "J.L. Elman"
            ],
            "title": "On the meaning of words and dinosaur bones: lexical knowledge without a lexicon",
            "venue": "Cogn. Sci",
            "year": 2009
        },
        {
            "authors": [
                "J. Fazekas",
                "A. Jessop",
                "J. Pine",
                "C. Rowland"
            ],
            "title": "Do children learn from their prediction mistakes? a registered report evaluating error-based theories of language acquisition",
            "venue": "R. Soc. Open Sci",
            "year": 2020
        },
        {
            "authors": [
                "M. Giulianelli",
                "J. Harding",
                "F. Mohnert",
                "D. Hupkes",
                "W. Zuidema"
            ],
            "title": "Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information,",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (Brussels),",
            "year": 2018
        },
        {
            "authors": [
                "A.E. Goldberg"
            ],
            "title": "Constructions at Work: The Nature of Generalization in Language",
            "year": 2006
        },
        {
            "authors": [
                "R.L. Gomez",
                "L. Gerken"
            ],
            "title": "Artificial grammar learning by 1-year-olds leads to specific and abstract knowledge",
            "venue": "Cognition",
            "year": 1999
        },
        {
            "authors": [
                "R.L. G\u00f3mez",
                "L. Gerken"
            ],
            "title": "Infant artificial language learning and language acquisition",
            "venue": "Trends Cogn. Sci",
            "year": 2000
        },
        {
            "authors": [
                "K. Gulordava",
                "P. Bojanowski",
                "E. Grave",
                "T. Linzen",
                "M. Baroni"
            ],
            "title": "Colorless green recurrent networks dream hierarchically,",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "B. Hart",
                "T.R. Risley",
                "J.R. Kirby"
            ],
            "title": "Meaningful differences in the everyday experience of young american children",
            "venue": "Can. J. History Sport Phys. Educ",
            "year": 1997
        },
        {
            "authors": [
                "R.D. Hawkins",
                "T. Yamakoshi",
                "T.L. Griffiths",
                "A.E. Goldberg"
            ],
            "title": "Investigating representations of verb bias in neural language models,",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Association for Computational Linguistics),",
            "year": 2020
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput",
            "year": 1997
        },
        {
            "authors": [
                "J. Hu",
                "J. Gauthier",
                "P. Qian",
                "E. Wilcox",
                "R. Levy"
            ],
            "title": "A systematic assessment of syntactic generalization in neural language models,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics), 1725\u20131744",
            "year": 2020
        },
        {
            "authors": [
                "P.A. Huebner",
                "E. Sulem",
                "F. Cynthia",
                "D. Roth"
            ],
            "title": "BabyBERTa: Learning more grammar with small-scale child-directed language,",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning (Punta Cana: Association for Computational Linguistics),",
            "year": 2021
        },
        {
            "authors": [
                "R. Jackendoff"
            ],
            "title": "Foundations of Language",
            "year": 2002
        },
        {
            "authors": [
                "E. Kharitonov",
                "M. Baroni",
                "D. Hupkes"
            ],
            "title": "How bpe affects memorization in transformers. arXiv preprint arXiv:2110.02782",
            "year": 2021
        },
        {
            "authors": [
                "A. Kuncoro",
                "M. Ballesteros",
                "L. Kong",
                "C. Dyer",
                "G. Neubig",
                "N.A. Smith"
            ],
            "title": "What do recurrent neural network grammars learn about syntax?",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "A. Kuncoro",
                "C. Dyer",
                "J. Hale",
                "D. Yogatama",
                "S. Clark",
                "P. Blunsom"
            ],
            "title": "LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better,",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Lakretz",
                "G. Kruszewski",
                "T. Desbordes",
                "D. Hupkes",
                "S. Dehaene",
                "M. Baroni"
            ],
            "title": "The emergence of number and syntax units in LSTM language models,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), vol. 1 (Minneapolis, MN: Association",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Lepori",
                "T. Linzen",
                "T.R. McCoy"
            ],
            "title": "Representations of syntax mask useful: Effects of constituency and dependency structure in recursive lstms,",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics),",
            "year": 2020
        },
        {
            "authors": [
                "T. Linzen",
                "M. Baroni"
            ],
            "title": "Syntactic structure from deep learning",
            "venue": "Ann. Rev. Linguist",
            "year": 2021
        },
        {
            "authors": [
                "T. Linzen",
                "G. Chrupala",
                "A. Alishahi"
            ],
            "title": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels: Association for Computational Linguistics",
            "year": 2018
        },
        {
            "authors": [
                "T. Linzen",
                "G. Chrupala",
                "Y. Belinlov",
                "D. Hupkes"
            ],
            "title": "Proceedings of the 2019 ACLWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Florence: Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Wang",
                "J. Kasai",
                "H. Hajishirzi",
                "N.A. Smith"
            ],
            "title": "Probing across time: what does roberta know and when?",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "R. Marvin",
                "T. Linzen"
            ],
            "title": "Targeted syntactic evaluation of language models,",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (Brussels: Association for Computational Linguistics),",
            "year": 2018
        },
        {
            "authors": [
                "R.T. McCoy",
                "R. Frank",
                "T. Linzen"
            ],
            "title": "Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks,\u201d in CogSci, eds T",
            "year": 2018
        },
        {
            "authors": [
                "R.T. McCoy",
                "R. Frank",
                "T. Linzen"
            ],
            "title": "Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks",
            "venue": "Trans. Assoc. Comput. Linguist",
            "year": 2020
        },
        {
            "authors": [
                "K. McRae",
                "K. Matsuki"
            ],
            "title": "People use their knowledge of common events to understand language, and do so as quickly as possible",
            "venue": "Lang. Linguist. Compass",
            "year": 2009
        },
        {
            "authors": [
                "L. Pannitto",
                "A. Herbelot"
            ],
            "title": "Recurrent babbling: evaluating the acquisition of grammar from limited input data,",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning,",
            "year": 2020
        },
        {
            "authors": [
                "M.J. Pickering",
                "S. Garrod"
            ],
            "title": "An integrated theory of language production and comprehension",
            "venue": "Behav. Brain Sci",
            "year": 2013
        },
        {
            "authors": [
                "M. Ramscar",
                "M. Dye",
                "S.M. McCauley"
            ],
            "title": "Error and expectation in language learning: the curious absence of mouses in adult",
            "venue": "speech. Language",
            "year": 2013
        },
        {
            "authors": [
                "A.R. Romberg",
                "J.R. Saffran"
            ],
            "title": "Statistical learning and language acquisition",
            "venue": "Wiley Interdiscipl. Rev. Cogn. Sci",
            "year": 2010
        },
        {
            "authors": [
                "J.R. Saffran",
                "R.N. Aslin",
                "E.L. Newport"
            ],
            "title": "Statistical learning by 8-month-old infants",
            "venue": "Science",
            "year": 1996
        },
        {
            "authors": [
                "M. Tomasello"
            ],
            "title": "2003).Constructing a Language: A Usage-Based Theory of Language Acquisition",
            "year": 2003
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmer",
                "J. Uszkoreit",
                "L. Jones",
                "Gomez",
                "A. N"
            ],
            "title": "Attention is all you need,",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "A. Warstadt",
                "A. Parrish",
                "H. Liu",
                "A. Mohananey",
                "W. Peng",
                "Wang",
                "S.F"
            ],
            "title": "Blimp: the benchmark of linguistic minimal pairs for english",
            "venue": "Trans. Assoc. Comput. Linguist",
            "year": 2020
        },
        {
            "authors": [
                "A. Warstadt",
                "Y. Zhang",
                "X. Li",
                "H. Liu",
                "S.R. Bowman"
            ],
            "title": "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually),",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "A. Warstadt",
                "S.R. Bowman"
            ],
            "title": "Can neural networks acquire a structural bias from raw linguistic data?",
            "year": 2020
        },
        {
            "authors": [
                "E. Wilcox",
                "R. Levy",
                "T. Morita",
                "R. Futrell"
            ],
            "title": "What do RNN language models learn about filler gap dependencies?",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks",
            "year": 2018
        },
        {
            "authors": [
                "C. Yu",
                "R. Sie",
                "N. Tedeschi",
                "L. Bergen"
            ],
            "title": "Word frequency does not predict grammatical knowledge in language models,",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Association for Computational Linguistics),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "MINI REVIEW published: 23 March 2022\ndoi: 10.3389/fpsyg.2022.741321\nFrontiers in Psychology | www.frontiersin.org 1 March 2022 | Volume 13 | Article 741321\nEdited by: Valentina Cuccio,\nUniversity of Messina, Italy\nReviewed by: Alex Warstadt,\nNew York University, United States\nAlessandra Falzone, University of Messina, Italy\n*Correspondence:\nAurelie Herbelot\naurelie.herbelot@unitn.it\nSpecialty section:\nThis article was submitted to\nLanguage Sciences, a section of the journal Frontiers in Psychology\nReceived: 14 July 2021 Accepted: 25 February 2022\nPublished: 23 March 2022\nCitation:\nPannitto L and Herbelot A (2022) Can\nRecurrent Neural Networks Validate Usage-Based Theories of Grammar\nAcquisition?\nFront. Psychol. 13:741321. doi: 10.3389/fpsyg.2022.741321\nCan Recurrent Neural Networks Validate Usage-Based Theories of Grammar Acquisition? Ludovica Pannitto 1 and Aurelie Herbelot 1,2*\n1CIMeC - Centre for Mind and Brain Sciences, University of Trento, Trento, Italy, 2Department of Information Engineering and Computer Science, University of Trento, Trento, Italy\nIt has been shown that Recurrent Artificial Neural Networks automatically acquire some\ngrammatical knowledge in the course of performing linguistic prediction tasks. The extent\nto which such networks can actually learn grammar is still an object of investigation.\nHowever, being mostly data-driven, they provide a natural testbed for usage-based\ntheories of language acquisition. This mini-review gives an overview of the state of the\nfield, focusing on the influence of the theoretical framework in the interpretation of results.\nKeywords: recurrent neural networks, grammar, usage-based linguistics, language acquisition, construction grammar\n1. INTRODUCTION\nArtificial Neural Networks (ANNs), and in particular recurrent architectures such as Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997), have consistently demonstrated great capabilities in the area of language modeling, generating sentences with credible surface patterns and showing promising performance when tested on very specific grammatical abilities (Gulordava et al., 2018; Linzen and Baroni, 2021), without requiring any prior bias towards the syntactic structure of natural languages. From a theoretical point of view, however, published results sometimes appear inconsistent, and overall inconclusive. The present survey suggests however that results should be interpreted in the light of various theoretical frameworks if they are to be fully understood. To illustrate this, it approaches the literature from the point of view of usage-based theories of acquisition, which are naturally suited to the behaviorist setting implemented by language modeling techniques.\n2. USAGE-BASED THEORIES OF GRAMMAR ACQUISITION\nTaking a coarse-grained perspective on usage-based theories of language acquisition, we can pinpoint three main standpoints that are relevant to language modeling with ANNs.\nFirst and foremost, behaviorist theories argue for a systemic vision where general-purpose memory and cognitive mechanisms account for the emergence of linguistic abilities (Tomasello, 2003; Goldberg, 2006; Christiansen and Chater, 2016; Cornish et al., 2017). That is, they stand against the idea that explicit, innate biases should be required in the acquisition device.\nSecondly, usage-based theories argue for a tight relation between input and learned representations in the course of acquisition (Jackendoff, 2002; Boyd and Goldberg, 2009). This is based on results that indicate that infants understand and manipulate input signals in sophisticated ways: their ability to analyze stream-like signals like language is well explored in the statistical learning literature (G\u00f3mez and Gerken, 2000; Romberg and Saffran, 2010; Christiansen, 2019), and the shape of the input itself has been explained by its relation to basic cognitive processes (Christiansen and Chater, 2015; Cornish et al., 2017). Word segmentation for instance is accomplished by 8-month old infants, relying purely on statistical relationships between neighboring speech sounds, and with very limited exposure (Saffran et al., 1996). Such limited input is also enough for one-year-olds to acquire specific grammatical information, thus discriminating new grammatical strings from those that show string-internal violations (Gomez and Gerken, 1999).\nThirdly, gradedness of grammatical notions is a central aspect in usage-based theories. Cognitive theories tend to blur hard boundaries, e.g. when it comes to the structure of categories (Barsalou, 1987), the content of semantic knowledge (Elman, 2009; McRae and Matsuki, 2009) or the distinction between lexically filled and pattern-like instances (Goldberg, 2006).\nArtificial statistical models seem an ideal toolbox to test the above claims. They can be built without hard-coded linguistic biases and they can be fed different types of input to investigate their effect on the acquisition process. Moreover, both their behavior and internal state can be analyzed in various ways. Lakretz et al. (2019) take a physiological approach investigating how, with no explicit bias, specific neurons specialize in detecting and memorizing syntactic structures. Giulianelli et al. (2018) propose instead a diagnostic downstream classifier to evaluate representations of number agreement.\nThe rest of this survey approaches the literature in the light of the three aspects of usage-based frameworks mentioned above, discussing to what extent the theory fits both implementation and results."
        },
        {
            "heading": "3. NEURAL LANGUAGE MODELS AND LANGUAGE DEVELOPMENT",
            "text": "The comparison between artificial language models and human language development starts at a fundamental mechanism: prediction. Predictive functions are considered highly relevant to language processing (Pickering and Garrod, 2013; Ramscar et al., 2013) and have received particular attention from theories that posit a direct relation between the shape of the received input and the organization of grammar (Ramscar et al., 2013; Fazekas et al., 2020). Consequently, (artificial) predictive models should be ideally suited to test related hypotheses.\nWhile prediction is a shared mechanisms among neural architectures, different models have been specialized for different tasks, leveraging prediction in various ways. The task most relevant to this survey is known as Language Modeling (LM):\nnetworks are trained to predict the next word (or character) given the previous sequence. Language Modeling encodes language competence only partially, leaving aside aspects such as interaction, grounding or event knowledge, which are crucial to human linguistic abilities. Nevertheless, it lets us test to what extent grammar can be learned from a pure and linear linguistic signal.\nRecurrent Neural Networks (RNNs), andmore specifically the \u201cLong Short-Term Memory network\u201d or LSTM (see Figure 1 for a brief description), are among the most common architectures and the ones with the longest history in Language Modeling. In LSTMs, contextual information is maintained from one prediction step to the next. The output of the network at time t thus depends on a subset of the inputs fed to the network across a time window. The LSTM learns to regulate its attention over this time window, deciding what to remember and what to forget in the input.\nLSTMs are a useful framework to compare learning in a purely predictive setting and an innately biased model. Expectedly, LSTMs that carry explicit syntactic bias [e.g. Recurrent Neural Network Grammars, Dyer et al. (2016); Kuncoro et al. (2017)] and specifically highlight the benefits of top-down parsing as an anticipatory model (Kuncoro et al., 2018) tend to perform better in experiments. But the question asked by usage-based theories is to what extent such hard-coded biases could be learned from language exposure only. A prime example of the pure prediction approach can be found in Gulordava et al. (2018): a vanilla LSTM is trained on a Language Modeling task, under the argument that the predictive mechanism is sufficient for the network to predict long-distance number agreement. The authors conclude that \u201cLM-trained RNNs can construct abstract grammatical representations.\u201d In a more ambivalent study, Arehalli and Linzen (2020) consider how real-time human comprehension and production do not always follow the general grammatical constraint of subject-verb agreement, due to a variety of possible syntactic or semantic factors. They replicate six experiments from the agreement attraction literature using LSTMs as subjects, and find that the model, despite its relatively simple structure, captures human behavior in at least three of them. The authors argue that those phenomena can be regarded as emerging from domain-general processing mechanisms, while also conceding that additional mechanisms might be required to model others.\nNotably, LSTMs also process the linguistic signal incrementally, and can be trained on relatively small amounts of data, comparable to the quantities that children are exposed to during the acquisition years (Hart et al., 1997). While this does not make LSTMs plausible models of human cognition, it makes them good benchmarks for building and verifying a range of psycholinguistic hypotheses around incremental processing and the poverty of the stimulus. This feature is especially important to test usage-based ideas that the statistical distribution of child-directed language explains how children acquire constructions in spite of the limited input they receive (see Section 4).\nMore recently, a new class of models has emerged and shown excellent performance in generating natural language (i.e.,\nFrontiers in Psychology | www.frontiersin.org 2 March 2022 | Volume 13 | Article 741321\nFIGURE 1 | LSTM networks are capable of keeping track of long-term dependencies. As recurrent neural networks (upper layer of the figure), they present a chain-like structure: at each time step t, the network\u2019s output is computed based on both the input of time t(xt ) and the network\u2019s state at time t\u2212 1(ht\u22121). As opposed to a simple recurrent cell, an LSTM cell (lower layer of the figure) has the ability to regulate how the two kinds of information (input and previous state) are weighted towards the computation of the output. The first gate, the forget gate, evaluates Ct\u22121 (a representation of the previous state different from ht\u22121) against xt and learns what information to keep from previous steps, including it in a vector ft. Next, a candidate value for the current state C\u0302t is computed along with the input gate vector it that weighs how much of the input will contribute to the current state. Finally, the state of the cell Ct is computed by weighting Ct\u22121 with the forget gate vector ft and the at C\u0302t with the input vector it. ht is then computed from Ct. A complete and easy to read guide to LSTMs can be found at https://colah.github.io/posts/2015-08Understanding-LSTMs/.\nTransformer models Vaswani et al., 2017, TLMs) and have in fact been shown to learn structural biases from raw input data (Warstadt and Bowman, 2020). Some psycholinguistic informed approaches have emerged around the architecture. Related the question of acquisition, Warstadt et al. (2020a) and Hu et al. (2020) have compared a range of models, including LSTMs and transformers, on different sizes of corpora. While the amount of training input clearly benefits system performance, Hu et al. (2020) also conclude that the specific hard-coded architecture of a model is more important than data size in yielding correct syntactic knowledge. Their training data is however not characteristic of child-directed input. In contrast, Huebner et al. (2021) focus on training a TLM on developmentally plausible input, matched in quantity and quality to what children are exposed to. The authors also introduce a novel test suite compatible with child-directed language requirements, such as a reduced vocabulary. Their results show that both features of the input and hyperparameters setting are highly relevant for the acquisition process.\nWhile TLMs seem to be a promising new avenue for researchers, they require very large amounts of data to be trained and exhibit a real preference for linguistic generalization, as opposed to surface patterns (Warstadt et al., 2020b). It is also still unclear whether such networks truly generalize or simply memorize patterns they have encountered, leveraging their extremely large size (Kharitonov et al., 2021)."
        },
        {
            "heading": "4. THE ROLE OF INPUT",
            "text": "While widely debated in linguistic research, the effect of input on learning has received less attention in computational studies, due to the lack of availability of diverse and realistic input data. This aspect is however a pillar of usage-based theories, and can help make sense of various studies that report seemingly inconsistent results across different input data.\nStarting with the issue of input size, experiments such as McCoy et al. (2018, 2020) tackle the poverty of the stimulus by testing the acquisition of specific language abilities (i.e., auxiliary inversion). However, the setup in those studies involves no pretraining or Language Modeling phase, therefore treating the phenomenon as a free-standing task. It is difficult to analyze reported results with respect to children acquisition theories, since, as the authors note themselves, humans tend to share processing strategies across phenomena. As mentioned above, Huebner et al. (2021) propose instead an attractive framework tested on TLMs, which is however affected by the exact hyperparameter setting of the model.\nTurning to the actual shape of the input, Yu et al. (2020) investigate the grammatical judgments of NLMs in a minimal pair setting (i.e., two sentences that differ in their acceptability due to just one grammatical property). They find that performance is correlated across tasks and across models, suggesting that the learnability of an item does not depend on\nFrontiers in Psychology | www.frontiersin.org 3 March 2022 | Volume 13 | Article 741321\na specific model but seems to be rather tied to the statistical properties of the input (i.e., on the distribution of constituents).\nIn Davis and van Schijndel (2020), the authors examine biases of ANNs for ambiguous relative clause attachments. In a sentence like Andrew had dinner yesterday with the nephew of the teacher that was divorced, both nephew and teacher are available for modifications by the relative clause: from a purely grammatical perspective, both interpretations are equally plausible. English speakers however have a generic preference for attaching the relative clause to the lower nominal, while other languages such as Spanish show a preference for the higher nominal. RNNs trained on either English or Spanish do not simulate this pattern, and instead consistently prefer the low attachment (similar results are reported in Davis et al. (2020) about the influence of implicit causation on syntactic representations). The authors show this preference is an artifact of training the network on production data which, in Spanish, contains more instances of low attachments. By manually correcting this bias in the input, generating an equal proportion of high and low attachments, they find that a preference for the higher nominal is learnable by the LSTM.\nLepori et al. (2020) experiment with an artificially constructed set of simple transitive sentences (Subject-Verb-Object), containing optional adjectival or prepositional modifiers in a controlled, probabilistic setting. They show that when a BiLSTM is fine-tuned on a distribution which explicitly requires moving beyond lexical co-occurrences and creating more abstract representations, performance dramatically improves: this suggests that a simple sequential mechanism can be enough if the linguistic signal is structured in a way that abstraction is encouraged.\nFinally, Pannitto and Herbelot (2020) confirm the tendency of ANNs to reproduce the particular input they are exposed to. They train an LSTM on three different genres of childdirected data. Their results show that when asked to generate, the network accurately reproduces the distribution of the linguistic constituents in its training data, while showing much lower correlation with the distribution of the other two genres.\nOverall, there seems to be evidence across the board that the statistical properties of the language input affect learnability as a whole and are responsible for inter-speaker differences. This fits well in a usage-based framework, and it also contributes to a view of grammar that allows for partial competence, as we will now discuss."
        },
        {
            "heading": "5. GRADED VS. DISCRETE NOTION OF GRAMMAR",
            "text": "Usage-based theories take a graded view on acquisition of linguistic structures, acknowledging that partial competence can be observed, blurring the distinction between semantic and syntactic knowledge, and ultimately, allowing for a range of varied grammatical intuitions across speakers. Existing studies on the grammatical abilities of RNNs report results which tend to confirm this view, but they are interpreted in different ways, as we will presently see.\nWilcox et al. (2018) address the phenomenon of filler-gap dependencies (e.g., the dependency existing betweenwhat and its gap in I knowwhat/\u22c6that the lion devoured - at sunrise), evaluating the surprisal values assigned by the pre-trained language models of Gulordava et al. (2018) and Chelba et al. (2013). Their results show that neural language models show high peaks of surprisal in the post-gap position, irrespective of the syntactic position where the gap happens (either subject, object or prepositional phrase). When considering the whole clause, however, predictions related to the subject position are much stronger than for the other two positions, correlating with human online processing results. Overall, their results indicate that filler-gap dependencies, and the constraints on them, are acquired by language models, albeit in a graded manner, and in many cases correlate with human judgements. Similar results are reported by Chowdhury and Zamparelli (2018), but the authors commit to a stronger binary distinction between competence and performance, ultimately stating that their model \u201cis sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality.\u201d\nA call for full abstraction, as opposed to a graded view of syntactic abilities, is also expressed in Marvin and Linzen (2018): English artificial sentence pairs (i.e., a grammatical sentence with its ungrammatical counterpart) are automatically built using a non recursive context free grammar, with the intent of minimizing \u201cthe semantic or collocational cues that can be used to identify the grammatical sentence.\u201d Twomodels are evaluated: a simple RNN language model and a multi-task RNN that solves two tasks at the same time, language modeling and a tagging task that superimposes syntactic information, both trained on a Wikipedia subset. Overall, results are varied both between tasks and, for a single benchmark, between different lexical items: a result that, as the authors say \u201cwould not be expected if its syntactic representations were fully abstract.\u201d The outcome is however perfectly reasonable in a usage-based framework, if we think of abstraction as induced by the association of specific lexical items with grammatical structure and intentions.\nGradedness is instead the explicit focus of Hawkins et al. (2020), where the authors examine the performance of various pre-trained neural language models, including the LSTM of Gulordava et al. (2018), against a dataset containing human preference judgements on dative alternations in various conditions, manipulating the length and definiteness of the recipient argument. In this study aimed at modeling verb biases, human intuitions are collected and kept as graded values, which the models are tested against. Lexical bias is seen here as a proxy of syntactic abilities rather than as something that might hurt the abstraction process.\nSummarizing, we see a growing body of evidence for gradedness of linguistic judgements, both in humans and networks. Interestingly, studies such as Liu et al. (2021) also show that the acquisition of different types of linguistic knowledge proceeds in parallel, but at various rates, in both LSTMs and TLMs. This opens the door for thinking of the potential aggregation of syntactic and semantic knowledge, but also for talking of different levels of competence, as acquisition takes place over time.\nFrontiers in Psychology | www.frontiersin.org 4 March 2022 | Volume 13 | Article 741321"
        },
        {
            "heading": "6. DISCUSSION",
            "text": "The current tendency in the computational community is to give an account of the knowledge acquired at the end of the acquisition process (Linzen et al., 2018, 2019; Alishahi et al., 2019; Baroni, 2020), but the picture emerging from the analysis of NLMs linguistic abilities is variegated, both in terms of approaches and results. To some extent, the inconsistent results reported in the literature are due to differences in theoretical assumptions made by each of the mentioned studies, rather than in experimental designs. As already highlighted by Linzen and Baroni (2021), the conclusions drawn by ANNs studies largely depend on the particular notions of competence, performance, lexicon and grammar that researchers commit to. Perhaps surprisingly, very few studies explicitly link the performance of neural language models to usage-based formalisms.\nMore specifically, the evaluation of NLMs is widely performed over specialized datasets that capture some highly debated phenomena, such as auxiliary inversion or agreement in increasingly puzzling contexts. Datasets comprehending a wider range of phenomena are now emerging (Hu et al., 2020; Warstadt et al., 2020a). The mastery of such phenomena undoubtedly corresponds to important milestones in acquisition, but they only give a partial view on the learner\u2019s trajectory towards full productivity and compositionality. More careful\ninvestigations are required to show how biases in the input affect learning and grammatical performance, and how such biases are eventually overcome.\nAnother issue is that the performance of NLMs is often compared to those of adult speakers. But some usage-based theories rely on the idea that grammar is an ability that evolves throughout the human lifespan, generating different learning patterns in children and adults. To fully explore this idea, studies should increase their focus on alternative datasets, both at input and evaluation stage.\nFinally, NLMs are usually treated as an idealized average speaker, with their predictions being compared to aggregates of human judgements. While this can be regarded as a necessary simplification, it also mirrors the view that there is a universally shared grammar towards which both speakers and LMs converge, and that this convergence, rather than individual differences, is meaningful. Conceptualizing NLMs as individual speakers rather than communities would probably let different evaluation setups emerge and provide new modeling possibilities for usagebased accounts.\nAUTHOR CONTRIBUTIONS\nLP prepared the literature review. AH supervised the work. LP and AH jointly wrote the survey. Both authors contributed to the article and approved the submitted version."
        }
    ],
    "title": "Can Recurrent Neural Networks Validate Usage-Based Theories of Grammar Acquisition?",
    "year": 2022
}