{
    "abstractText": "Moving Object Segmentation (MOS) is a challenging problem in computer vision, particularly in scenarios with dynamic backgrounds, abrupt lighting changes, shadows, camouflage, and moving cameras. While graph-based methods have shown promising results in MOS, they have mainly relied on transductive learning which assumes access to the entire training and testing data for evaluation. However, this assumption is not realistic in real-world applications where the system needs to handle new data during deployment. In this paper, we propose a novel Graph Inductive Moving Object Segmentation (GraphIMOS) algorithm based on a Graph Neural Network (GNN) architecture. Our approach builds a generic model capable of performing prediction on newly added data frames using the already trained model. GraphIMOS outperforms previous inductive learning methods and is more generic than previous transductive techniques. Our proposed algorithm enables the deployment of graph-based MOS models in real-world applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wieke Prummel"
        },
        {
            "affiliations": [],
            "name": "Jhony H. Giraldo"
        },
        {
            "affiliations": [],
            "name": "Anastasia Zakharova"
        },
        {
            "affiliations": [],
            "name": "Thierry Bouwmans"
        }
    ],
    "id": "SP:417167e740e4635e757c8831c0e62d9c70ecc326",
    "references": [
        {
            "authors": [
                "B. Garcia-Garcia",
                "T. Bouwmans",
                "A. Silva"
            ],
            "title": "Background subtraction in real applications: Challenges, current models and future directions",
            "venue": "Computer Science Review, vol. 35, pp. 100204, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Bouwmans",
                "S. Javed",
                "M. Sultana",
                "S. Jung"
            ],
            "title": "Deep neural network concepts for background subtraction: A systematic review and comparative evaluation",
            "venue": "Neural Networks, vol. 117, pp. 8\u201366, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Hou",
                "Y. Liu",
                "N. Ling"
            ],
            "title": "A super-fast deep network for moving object detection",
            "venue": "IEEE ISCAS, 2020, pp. 1\u20135.",
            "year": 2020
        },
        {
            "authors": [
                "I. Osman",
                "M. Shehata"
            ],
            "title": "Few-shot learning network for moving object detection using exemplar-based attention map",
            "venue": "IEEE ICIP, 2022, pp. 1056\u20131060.",
            "year": 2022
        },
        {
            "authors": [
                "T. Minematsu",
                "A. Shimada",
                "R. Taniguchi"
            ],
            "title": "Rethinking background and foreground in deep neural networkbased background subtraction",
            "venue": "IEEE ICIP, 2020, pp. 3229\u20133233.",
            "year": 2020
        },
        {
            "authors": [
                "B. Hou",
                "Y. Liu",
                "N. Ling",
                "Y. Ren",
                "L. Liu"
            ],
            "title": "A survey of efficient deep learning models for moving object segmentation",
            "venue": "APSIPA Transactions on Signal and Information Processing, vol. 12, no. 1, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M. Braham",
                "M. Van Droogenbroeck"
            ],
            "title": "Deep background subtraction with scene-specific convolutional neural networks",
            "venue": "IEEE IWSSIP, 2016, pp. 1\u20134.",
            "year": 2016
        },
        {
            "authors": [
                "T. Akilan",
                "Q. Wu",
                "A. Safaei",
                "J. Huo",
                "Y. Yang"
            ],
            "title": "A 3D CNN-LSTM-based image-to-image foreground segmentation",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 3, pp. 959\u2013971, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I. Osman",
                "M. Abdelpakey",
                "M. Shehata"
            ],
            "title": "Transblast: Self-supervised learning using augmented subspace with transformer for background/foreground separation",
            "venue": "IEEE ICCV-W, 2021, pp. 215\u2013224.",
            "year": 2021
        },
        {
            "authors": [
                "M. Bakkay",
                "H. Rashwan",
                "H. Salmane",
                "L. Khoudour",
                "D. Puig",
                "Y. Ruichek"
            ],
            "title": "BSCGAN: Deep background subtraction with conditional generative adversarial networks",
            "venue": "IEEE ICIP, 2018, pp. 4018\u20134022.",
            "year": 2018
        },
        {
            "authors": [
                "M. Sultana",
                "A. Mahmood",
                "T. Bouwmans",
                "M. Khan",
                "S. Jung"
            ],
            "title": "Moving objects segmentation using generative adversarial modeling",
            "venue": "Neurocomputing, vol. 506, pp. 240\u2013251, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Mandal",
                "L. Kumar",
                "M. Saran"
            ],
            "title": "MotionRec: A unified deep framework for moving object recognition",
            "venue": "IEEE WACV, 2020, pp. 2734\u20132743.",
            "year": 2020
        },
        {
            "authors": [
                "A. Cioppa",
                "M. Van Droogenbroeck",
                "M. Braham"
            ],
            "title": "Real-time semantic background subtraction",
            "venue": "IEEE ICIP, 2020, pp. 3214\u20133218.",
            "year": 2020
        },
        {
            "authors": [
                "J. Giraldo",
                "S. Javed",
                "T. Bouwmans"
            ],
            "title": "Graph moving object segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2485\u20132503, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Giraldo",
                "S. Javed",
                "M. Sultana",
                "S. Jung",
                "T. Bouwmans"
            ],
            "title": "The emerging field of graph signal processing for moving object segmentation",
            "venue": "Frontiers of Computer Vision, 2021, pp. 31\u201345.",
            "year": 2021
        },
        {
            "authors": [
                "A. Ortega",
                "P. Frossard",
                "J. Kova\u010devi\u0107",
                "J. MF Moura",
                "P. Vandergheynst"
            ],
            "title": "Graph signal processing: Overview, challenges, and applications",
            "venue": "Proceedings of the IEEE, vol. 106, no. 5, pp. 808\u2013828, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Bronstein",
                "J. Bruna",
                "T. Cohen",
                "P. Veli\u010dkovi\u0107"
            ],
            "title": "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges",
            "venue": "arXiv preprint arXiv:2104.13478, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "P.M. Jodoin",
                "F. Porikli",
                "J. Konrad",
                "Y. Benezeth",
                "P. Ishwar"
            ],
            "title": "CDnet 2014: An expanded change detection benchmark dataset",
            "venue": "IEEE CVPR- W, 2014, pp. 387\u2013394.",
            "year": 2014
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Giraldo",
                "T. Bouwmans"
            ],
            "title": "GraphBGS: Background subtraction via recovery of graph signals",
            "venue": "IEEE ICPR, 2021, pp. 6881\u20136888.",
            "year": 2021
        },
        {
            "authors": [
                "L. Zhao",
                "L. Akoglu"
            ],
            "title": "PairNorm: Tackling oversmoothing in gnns",
            "venue": "ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. H Giraldo",
                "S. Javed",
                "N. Werghi",
                "T. Bouwmans"
            ],
            "title": "Graph CNN for moving object detection in complex environments from unseen videos",
            "venue": "IEEE ICCV-W, 2021, pp. 225\u2013233.",
            "year": 2021
        },
        {
            "authors": [
                "L.A. Lim",
                "H.Y. Keles"
            ],
            "title": "Learning multi-scale features for foreground segmentation",
            "venue": "Pattern Analysis and Applications, vol. 23, no. 3, pp. 1369\u20131380, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Tezcan",
                "P. Ishwar",
                "J. Konrad"
            ],
            "title": "BSUV-Net 2.0: Spatio-temporal data augmentations for video-agnostic supervised background subtraction",
            "venue": "IEEE Access, vol. 9, pp. 53849\u201353860, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Fey",
                "J.E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "ICLR-W, 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Moving object segmentation, graph neural networks, inductive learning, graph signal processing"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Moving Object Segmentation (MOS) is an important problem in computer vision, particularly in surveillance system applications [1]. The goal of MOS is to identify and separate the pixels or regions in a video that correspond to moving objects from the static background or other static objects. Deep learning models [2] have demonstrated strong performance on large-scale datasets. However, as the quality of data improves, these models become increasingly complex and computationally intensive, even with fast algorithms [3], few-shot learning methods [4], and specialized architectures [5].\nThe most common deep models are mostly supervised and can be divided into four groups [6]: 2D Convolutional Neural Networks (CNNs) [7], 3D CNNs [8], transformer neural networks [9], and generative adversarial networks [10, 11]. In addition to these, some state-of-the-art (SOTA) techniques have been combined with deep methods to create novel approaches, such as MotionRec [12], RT-SBS [13], and GraphMOS [14]. Recent graph-based algorithms like GraphMOS\n[14] and GraphMOD-Net [15] use semi-supervised learning posed as a graph signal reconstruction problem. These methods are inspired by the theory of graph signal processing [16] and have shown promising results. However, these graphbased methods are transductive in nature, meaning that the model needs to be fully retrained and the graph regenerated whenever a new video is added. To address this issue, we propose inductive techniques for graph-based MOS, where multiple graphs are built instead of a single large graph. This approach reduces the need for rebuilding the whole graph and retraining the model, making it more suitable for real-world deployments, as shown in Fig. 1.\nIn this work, we propose a novel Graph Inductive Moving Object Segmentation (GraphIMOS) algorithm based on Graph Neural Networks (GNNs) [17]. We represent each instance of the video as a node in a graph, which is generated using a Mask Region Convolutional Neural Network (Mask R-CNN) with a ResNet-50 and Feature Pyramid Network (FPN) backbone. We use optical flow, intensity, and texture features to represent the nodes. Instead of creating a single large graph, as in [14], we generate \u03c1 + \u03b1 k-Nearest Neighbors (k-NN) graphs, where each graph can have a different number of nodes. We set \u03c1 = 3 in the experiments for the training and validation graphs, which are then fed to the proposed GNN model for training and hyperparameter optimization. To evaluate the performance of our model, we test it on one single graph \u03b1 = 1, built from previously unseen videos. Our approach is evaluated on the challenging Change ar X\niv :2\n30 5.\n09 58\n5v 1\n[ cs\n.C V\n] 1\n6 M\nay 2\n02 3\nDetection 2014 (CDNet 2014) dataset [18], and we demonstrate competitive results against previous inductive methods.\nThis paper makes two main contributions. Firstly, we propose an inductive graph-based framework for Moving Object Segmentation (MOS), which is an important step toward the real-world deployment of graph-based methods in video surveillance applications. Secondly, we introduce a novel GNN architecture specifically designed for MOS, called GraphIMOS. To the best of our knowledge, GraphIMOS is the first graph-based inductive learning approach for MOS. The remainder of this paper is organized as follows. In Section 2, we introduce the preliminary concepts and describe the GraphIMOS algorithm and architecture. Section 3 presents the experimental setup and results. Finally, in Section 4, we present our conclusions."
        },
        {
            "heading": "2. GRAPH INDUCTIVE MOVING OBJECT SEGMENTATION",
            "text": "This section presents the preliminaries of this paper and the proposed inductive graph-based MOS method. Figure 2 shows the pipeline of GraphIMOS, which consists of instance segmentation, node representation, graph construction, and inductive GNN training and evaluation."
        },
        {
            "heading": "2.1. Preliminaries",
            "text": "A graph is a mathematical entity that can be represented as G = (V, E), where V = {1, 2, . . . , N} is the set of nodes and\nE \u2286 {(i, j) | i, j \u2208 V and i 6= j} is the set of edges connecting the nodes i and j. The adjacency matrix A \u2208 RN\u00d7N is a popular choice of shift operator in GNNs [19], where A(i,j) = ai,j \u2200 (i, j) \u2208 E , and 0 otherwise. For unweighted graphs we have that A \u2208 {0, 1}N\u00d7N . Similarly, we have the diagonal degree matrix given by D = diag(A1). In GNNs, we commonly associate a vector of features xi \u2208 RF to each node i. Therefore, we can represent the whole set of input features as X = [x1,x2, . . . ,xN ]T \u2208 RN\u00d7F . In this paper, we use undirected and weighted graphs.\nGraphIMOS is designed to be agnostic to the choice of the GNN used. However, in our experiments, we use the widely used Graph Convolutional Network (GCN) [19]. The propagation rule of GCN is given by:\nH(l+1) = \u03c3(D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2H(l)W(l)), (1)\nwhere A\u0303 = A + I, D\u0303 is the degree matrix of A\u0303, H(l) is the output matrix of layer l (with H(0) = X), W(l) is the matrix of trainable weights in layer l, and \u03c3(\u00b7) is an activation function such as ReLU or softmax. It is worth noting that (1) reduces to the regular multi-layer perceptron when D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2 = I."
        },
        {
            "heading": "2.2. Segmentation and Node Representation",
            "text": "For the region proposal, we use Mask R-CNN with ResNet-50 as the backbone and construct graphs using a k-NN method. The feature representation proposed in [14] is utilized, where\nthe outputs generated by Mask R-CNN are represented as nodes in the graphs. These instances are associated with meaningful representations, such as optical flow, intensity, and texture features. Finally, all the features are concatenated to form a 930-dimensional vector, which represents each instance. For further details on the feature extraction process, please refer to [14, 20]."
        },
        {
            "heading": "2.3. Graph Mini-batch",
            "text": "The proposed framework is distinct from previous graphbased MOS methods because it is an inductive architecture. For instance, in [14], the whole graph would need to be rebuilt every time a new video is fed into the algorithm. This requires the optimization problem to be solved again, making deployment in real-world scenarios more challenging. Therefore, instead of creating a single graph from the data, our proposed approach creates \u03c1 + \u03b1 separate graphs, each with a different number of nodes. This ensures that the data in the \u03c1 training-validation graphs are not connected. The adjacency matrices are arranged in a block-diagonal manner to construct a comprehensive graph that includes multiple distinct subgraphs as follows:\nA = A1 . . . A\u03c1  , (2) where A is the adjacency matrix in the mini-batch. We define \u03c1 for the number of training and validation graphs, and \u03b1 for the testing graphs as shown in Fig. 2. Our adjacency matrices are stored efficiently, using a sparse representation that only keeps track of non-zero entries: the edges. This means that there is no extra memory overhead. The node and target features are integrated into the node dimension through simple concatenation."
        },
        {
            "heading": "2.4. GNN Architecture",
            "text": "Our model consists of two GCNConv layers [19], various ReLU activation layers, and three linear layers. To reduce overfitting and improve generalization on unseen videos, we use five dropout and four pair batch normalizations (PairNorm) [21]. These techniques also enhance the training stability of the model and prevent over-smoothing. PairNorm is particularly effective in achieving a better convergence of the proposed model. Our goal is to learn meaningful representations of the graph structure and node features to classify objects as either static or moving. The final layer of our model employs a log softmax function defined as follows: log softmax = log( 1s expxi) where s = \u2211 i expxi. This GNN architecture enables GraphIMOS to be easily deployed in real-world applications."
        },
        {
            "heading": "3. EXPERIMENTS AND RESULTS",
            "text": "This section introduces the metrics, and the dataset graph partitioning used to conduct the different experiments on GraphIMOS."
        },
        {
            "heading": "3.1. Evaluation Metrics",
            "text": "The evaluation metrics F-Measure, precision, and recall are defined as follows:\nRecall = TP\nTP + FN , Precision = TP TP + FP ,\nF-measure = 2 Precision\u00d7 Recall Precision + Recall , (3)\nwhere TP, FN, and FP are the true positives, false negatives, and false positives, respectively. According to Table 1, we calculate the F-Measure in Table 2 on node level classification."
        },
        {
            "heading": "3.2. Experiments",
            "text": "We evaluate GraphIMOS against several SOTA algorithms using the large-scale CDNet 2014 dataset [18]. To build an inductive framework, we construct multiple graphs by dividing the data into four sequences: S1, S2, S3, and S4 as shown in Table 1. Therefore, we run 4 experiments to compute the FMeasure in the whole dataset. These sequences were chosen to enable unseen videos evaluation, following the approach in [24]. For each sequence, we compute the results and construct one graph per sequence : G1, G2, G3, and G4.\nIn each experiment, we use two graphs to train and evaluate GraphIMOS, and one graph for testing. For each experiment we chose to use a different graph to test the performance of the proposed algorithm on unseen videos."
        },
        {
            "heading": "3.3. Implementation Details",
            "text": "We employ the PyTorch Geometric library [25] to implement our proposed architecture. The k-nearest neighbors (k-NN) graphs are constructed with a value of k = 40. In order to prevent overfitting, we use a dropout with a coefficient of 0.5. For optimization, we use stochastic gradient descent (SGD) with a momentum of 0.9, a learning rate of 0.01, and 5e\u22124 for weight decay. We train our model for a maximum of 500 epochs, using a graph-batch size of 1. Our model architecture includes 5 hidden layers, and we opt for the negative log likelihood loss for training and evaluating the GNN model."
        },
        {
            "heading": "3.4. Results and Discussion",
            "text": "We compare GraphIMOS with FgSegNet [23], GraphMODNet [22], and GraphMOS [14]. While GraphMOD-Net is transductive in nature, we adapt it to our data partitioning and experimental framework for a fair comparison with GraphIMOS. We also include GraphMOS and GraphMOD-Net with their original performances as references, since they are transductive techniques. The numerical and visual results of the compared methods are shown in Fig. 3 and Table\n3, which demonstrate that GraphIMOS surpasses previous inductive learning methods. Our experiments also reveal that GraphMOD-Net [22] shows a performance degradation when evaluated in an inductive setting, possibly due to the challenges of real-world deployments. GraphIMOS strikes a better balance between performance and realistic deployment, making it a promising candidate for real-world applications."
        },
        {
            "heading": "4. CONCLUSION",
            "text": "This paper introduces GraphIMOS, a novel approach that uses GNNs, graph mini-batches, and inductive learning for MOS. The proposed algorithm consists of four key components: instance segmentation using Mask R-CNN, feature extraction for node representation, k-NN for graph construction, and a GNN-based inductive learning algorithm. To the best of our knowledge, GraphIMOS is the first approach that uses graph-based inductive learning for MOS, demonstrating its novelty and potential. Compared to previous works such as GraphMOD-Net, GraphIMOS offers improved performance and a better trade-off between performance and practical deployment. For future work, we plan to add skip connections and deeper GNN models."
        },
        {
            "heading": "5. REFERENCES",
            "text": "[1] B. Garcia-Garcia, T. Bouwmans, and A. Silva, \u201cBackground subtraction in real applications: Challenges, current models and future directions,\u201d Computer Science Review, vol. 35, pp. 100204, 2020.\n[2] T. Bouwmans, S. Javed, M. Sultana, and S. Jung, \u201cDeep neural network concepts for background subtraction: A systematic review and comparative evaluation,\u201d Neural Networks, vol. 117, pp. 8\u201366, 2019.\n[3] B. Hou, Y. Liu, and N. Ling, \u201cA super-fast deep network for moving object detection,\u201d in IEEE ISCAS, 2020, pp. 1\u20135.\n[4] I. Osman and M. Shehata, \u201cFew-shot learning network for moving object detection using exemplar-based attention map,\u201d in IEEE ICIP, 2022, pp. 1056\u20131060.\n[5] T. Minematsu, A. Shimada, and R. Taniguchi, \u201cRethinking background and foreground in deep neural networkbased background subtraction,\u201d in IEEE ICIP, 2020, pp. 3229\u20133233.\n[6] B. Hou, Y. Liu, N. Ling, Y. Ren, and L. Liu, \u201cA survey of efficient deep learning models for moving object segmentation,\u201d APSIPA Transactions on Signal and Information Processing, vol. 12, no. 1, 2023.\n[7] M. Braham and M. Van Droogenbroeck, \u201cDeep background subtraction with scene-specific convolutional neural networks,\u201d in IEEE IWSSIP, 2016, pp. 1\u20134.\n[8] T. Akilan, Q. Wu, A. Safaei, J. Huo, and Y. Yang, \u201cA 3D CNN-LSTM-based image-to-image foreground segmentation,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 3, pp. 959\u2013971, 2019.\n[9] I. Osman, M. Abdelpakey, and M. Shehata, \u201cTransblast: Self-supervised learning using augmented subspace with transformer for background/foreground separation,\u201d in IEEE ICCV-W, 2021, pp. 215\u2013224.\n[10] M. Bakkay, H. Rashwan, H. Salmane, L. Khoudour, D. Puig, and Y. Ruichek, \u201cBSCGAN: Deep background subtraction with conditional generative adversarial networks,\u201d in IEEE ICIP, 2018, pp. 4018\u20134022.\n[11] M. Sultana, A. Mahmood, T. Bouwmans, M. Khan, and S. Jung, \u201cMoving objects segmentation using generative adversarial modeling,\u201d Neurocomputing, vol. 506, pp. 240\u2013251, 2022.\n[12] M. Mandal, L. Kumar, and M. Saran, \u201cMotionRec: A unified deep framework for moving object recognition,\u201d in IEEE WACV, 2020, pp. 2734\u20132743.\n[13] A. Cioppa, M. Van Droogenbroeck, and M. Braham, \u201cReal-time semantic background subtraction,\u201d in IEEE ICIP, 2020, pp. 3214\u20133218.\n[14] J. Giraldo, S. Javed, and T. Bouwmans, \u201cGraph moving object segmentation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2485\u20132503, 2022.\n[15] J. Giraldo, S. Javed, M. Sultana, S. Jung, and T. Bouwmans, \u201cThe emerging field of graph signal processing for moving object segmentation,\u201d in Frontiers of Computer Vision, 2021, pp. 31\u201345.\n[16] A. Ortega, P. Frossard, J. Kovac\u030cevic\u0301, J. MF Moura, and P. Vandergheynst, \u201cGraph signal processing: Overview, challenges, and applications,\u201d Proceedings of the IEEE, vol. 106, no. 5, pp. 808\u2013828, 2018.\n[17] M. Bronstein, J. Bruna, T. Cohen, and P. Velic\u030ckovic\u0301, \u201cGeometric deep learning: Grids, groups, graphs, geodesics, and gauges,\u201d arXiv preprint arXiv:2104.13478, 2021.\n[18] Y. Wang, P. M. Jodoin, F. Porikli, J. Konrad, Y. Benezeth, and P. Ishwar, \u201cCDnet 2014: An expanded change detection benchmark dataset,\u201d in IEEE CVPRW, 2014, pp. 387\u2013394.\n[19] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in ICLR, 2017.\n[20] J. Giraldo and T. Bouwmans, \u201cGraphBGS: Background subtraction via recovery of graph signals,\u201d in IEEE ICPR, 2021, pp. 6881\u20136888.\n[21] L. Zhao and L. Akoglu, \u201cPairNorm: Tackling oversmoothing in gnns,\u201d in ICLR, 2020.\n[22] J. H Giraldo, S. Javed, N. Werghi, and T. Bouwmans, \u201cGraph CNN for moving object detection in complex environments from unseen videos,\u201d in IEEE ICCV-W, 2021, pp. 225\u2013233.\n[23] L. A. Lim and H. Y. Keles, \u201cLearning multi-scale features for foreground segmentation,\u201d Pattern Analysis and Applications, vol. 23, no. 3, pp. 1369\u20131380, 2020.\n[24] M. Tezcan, P. Ishwar, and J. Konrad, \u201cBSUV-Net 2.0: Spatio-temporal data augmentations for video-agnostic supervised background subtraction,\u201d IEEE Access, vol. 9, pp. 53849\u201353860, 2021.\n[25] M. Fey and J. E. Lenssen, \u201cFast graph representation learning with PyTorch Geometric,\u201d in ICLR-W, 2019."
        }
    ],
    "title": "INDUCTIVE GRAPH NEURAL NETWORKS FOR MOVING OBJECT SEGMENTATION",
    "year": 2023
}