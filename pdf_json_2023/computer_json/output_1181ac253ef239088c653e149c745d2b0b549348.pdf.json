{
    "abstractText": "We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra. Transformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse. Attention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras \u201dtensors with a built-in loss function gradient\u201d. This loss function gradient occurs within the single layers and no backward pass is needed. This is in stark contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adam Nemecek"
        }
    ],
    "id": "SP:447d330155c73e4ba18a2fdae04f0133715b3812",
    "references": [
        {
            "authors": [
                "Aguiar",
                "Lauve",
                "M. 2013] Aguiar",
                "A. Lauve"
            ],
            "title": "Convolution powers of the identity antipode and convolution powers of the identity in graded connected hopf algebras",
            "year": 2013
        },
        {
            "authors": [
                "Ahman",
                "Uustalu",
                "D. 2014] Ahman",
                "T. Uustalu"
            ],
            "title": "Coalgebraic update lenses",
            "venue": "Electronic Notes in Theoretical Computer Science,",
            "year": 2014
        },
        {
            "authors": [
                "Barbosa",
                "L.S. 2022] Barbosa"
            ],
            "title": "Coalgebra for the working software engineer",
            "venue": "Journal of Applied Logics-IFCoLog Journal of Logics and their Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Bonchi et al",
                "F. 2014] Bonchi",
                "P. Sobocinski",
                "F. Zanasi"
            ],
            "title": "Interacting hopf algebras",
            "venue": "Journal of Pure and Applied Algebra,",
            "year": 2014
        },
        {
            "authors": [
                "Carothers et al",
                "D.C. 2012] Carothers",
                "S.K. Lucas",
                "G.E. Parker",
                "J.D. Rudmin",
                "J.S. Sochacki",
                "R.J. Thelwell",
                "A. Tongen",
                "P.G. Warne"
            ],
            "title": "Connections between power series methods and automatic differentiation",
            "venue": "Lecture Notes in Computational Science and Engineering,",
            "year": 2012
        },
        {
            "authors": [
                "Diaconis et al",
                "P. 2012] Diaconis",
                "C.Y.A. Pang",
                "A. Ram"
            ],
            "title": "Hopf algebras and markov chains: Two examples and a theory",
            "venue": "Journal of Algebraic Combinatorics,",
            "year": 2012
        },
        {
            "authors": [
                "Dold",
                "Puppe",
                "A. 1985] Dold",
                "D. Puppe"
            ],
            "title": "Duality, trace and transfer",
            "venue": "Proceedings of the Steklov Institute of Mathematics,",
            "year": 1985
        },
        {
            "authors": [
                "Ebrahimi-Fard et al",
                "K. 2019] Ebrahimi-Fard",
                "L. Foissy",
                "J. Kock",
                "F. Patras"
            ],
            "title": "Operads of (noncrossing) partitions, interacting bialgebras, and moment-cumulant relations",
            "venue": "Advances in Mathematics,",
            "year": 2019
        },
        {
            "authors": [
                "Elhage et al",
                "N. 2021] Elhage",
                "N. Nanda",
                "C. Olsson",
                "T. Henighan",
                "N. Joseph",
                "B. Mann",
                "A. Askell",
                "Y. Bai",
                "A. Chen",
                "T. Conerly",
                "N. DasSarma",
                "D. Drain",
                "D. Ganguli",
                "Z. HatfieldDodds",
                "D. Hernandez",
                "A. Jones",
                "J. Kernion",
                "L. Lovitt",
                "K. Ndousse",
                "D. Amodei",
                "T. Brown",
                "J. Clark",
                "J. Kaplan",
                "S. McCandlish",
                "C. Olah"
            ],
            "title": "A mathematical framework for transformer circuits. Transformer Circuits Thread",
            "year": 2021
        },
        {
            "authors": [
                "Espinosa et al",
                "L.A.D. 2014] Espinosa",
                "K. Ebrahimi-Fard",
                "W.S. Gray"
            ],
            "title": "A combinatorial hopf algebra for nonlinear output feedback control systems",
            "venue": "Journal of Algebra,",
            "year": 2014
        },
        {
            "authors": [
                "Espinosa et al",
                "L.A.D. 2018] Espinosa",
                "K. Ebrahimi-Fard",
                "W.S. Gray"
            ],
            "title": "Combinatorial hopf algebra for interconnected nonlinear systems",
            "year": 2018
        },
        {
            "authors": [
                "Grinberg",
                "Reiner",
                "D. 2014] Grinberg",
                "V. Reiner"
            ],
            "title": "Hopf algebras in combinatorics",
            "year": 2014
        },
        {
            "authors": [
                "Hasegawa",
                "Lemay",
                "M. 2022] Hasegawa",
                "J.-S. P"
            ],
            "title": "Traced monads and hopf monads",
            "year": 2022
        },
        {
            "authors": [
                "Hines",
                "Scott",
                "P. 2007] Hines",
                "P. Scott"
            ],
            "title": "Categorical traces from single-photon linear optics",
            "year": 2007
        },
        {
            "authors": [
                "Hinze et al",
                "R. 2015] Hinze",
                "N. Wu",
                "J. Gibbons"
            ],
            "title": "Conjugate hylomorphisms or: The mother of all structured recursion schemes",
            "year": 2015
        },
        {
            "authors": [
                "Hur et al",
                "C.K. 2013] Hur",
                "G. Neis",
                "D. Dreyer",
                "V. Vafeiadis"
            ],
            "title": "The power of parameterization in coinductive proof",
            "year": 2013
        },
        {
            "authors": [
                "Joni",
                "Rota",
                "S.A. 1979] Joni",
                "G.-C"
            ],
            "title": "Coalgebras and bialgebras in combinatorics",
            "venue": "Studies in Applied Mathematics,",
            "year": 1979
        },
        {
            "authors": [
                "Kozen",
                "Silva",
                "D. 2017] Kozen",
                "A. Silva"
            ],
            "title": "Practical coinduction",
            "venue": "Mathematical Structures in Computer Science,",
            "year": 2017
        },
        {
            "authors": [
                "Li",
                "Zhang",
                "L. 2020] Li",
                "P. Zhang"
            ],
            "title": "Twisted hopf algebras, ringel - hall algebras, and green\u2019s categories",
            "venue": "Journal of Algebra,",
            "year": 2020
        },
        {
            "authors": [
                "Mastorou et al",
                "L. 2022] Mastorou",
                "N. Papaspyrou",
                "N. Vazou"
            ],
            "title": "Coinduction inductively: mechanizing coinductive proofs in liquid haskell",
            "year": 2022
        },
        {
            "authors": [
                "Mishna",
                "Zabrocki",
                "M. 2008] Mishna",
                "M. Zabrocki"
            ],
            "title": "Analytic aspects of the shuffle product",
            "year": 2008
        },
        {
            "authors": [
                "Nguyen",
                "Wu",
                "M. 2022] Nguyen",
                "N. Wu"
            ],
            "title": "Folding over neural networks. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture",
            "venue": "Notes in Bioinformatics),",
            "year": 2022
        },
        {
            "authors": [
                "Pang",
                "C.Y.A. 2014] Pang"
            ],
            "title": "Hopf algebras and markov chains. arXiv preprint arXiv:1412.8221",
            "year": 2014
        },
        {
            "authors": [
                "Pavlovic",
                "Escardo",
                "D. 2001] Pavlovic",
                "M. Escardo"
            ],
            "title": "Calculus in coinductive form. Esca",
            "year": 2001
        },
        {
            "authors": [
                "Peterson",
                "Taft",
                "B. 1980] Peterson",
                "E.J. Taft"
            ],
            "title": "The hopf algebra of linearly recursive sequences",
            "year": 1980
        },
        {
            "authors": [
                "Phuong",
                "Hutter",
                "M. 2022] Phuong",
                "M. Hutter"
            ],
            "title": "Formal algorithms for transformers",
            "year": 2022
        },
        {
            "authors": [
                "Redig",
                "Sau",
                "F. 2018] Redig",
                "F. Sau"
            ],
            "title": "Stochastic duality and eigenfunctions",
            "year": 2018
        },
        {
            "authors": [
                "Vajjha et al",
                "K. 2020] Vajjha",
                "A. Shinnar",
                "B. Trager",
                "V. Pestun",
                "N. Fulton"
            ],
            "title": "Certrl: Formalizing convergence proofs for value and policy iteration in coq",
            "venue": "CPP 2021 - Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs,",
            "year": 2020
        },
        {
            "authors": [
                "Yuan et al",
                "T. 2021] Yuan",
                "X. Li",
                "H. Xiong",
                "H. Cao",
                "D. Dou"
            ],
            "title": "Explaining information flow inside vision transformers using markov chain",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 2.\n01 83\n4v 1\n[ cs\n.L G\n] 3\nF eb\n2 02\n3\nCoinductive guide to inductive transformer heads\nAdam Nemecek\nadam@cofunctional.ai"
        },
        {
            "heading": "1 Abstract",
            "text": "We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra.\nTransformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse.\nAttention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras \u201dtensors with a built-in loss function gradient\u201d. This loss function gradient occurs within the single layers and no backward pass is needed. This is in stark contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring."
        },
        {
            "heading": "2 Introduction",
            "text": "With the rise of popularity of transformer models [Vaswani et al., 2017], there have been concerns about the interpretability, or lack thereof, of said models. Such concerns are warranted, transformer models exhibit behaviors that appear surprising.\nWe analyze these behaviors in the context of combinatorial Hopf algebra, which is a tensorial bialgebra. This algebra turns out to be uniquely suited for interpretation and understanding of transformer models and, as we show in future work, machine learning models in general.\nDespite the fact that this is, to our knowledge, the first paper which interprets machine learning in terms of Hopf algebras, we are confident that this venue of research will prove to be a quintessential semantic bridge between the representation of machine learning models in memory as IEEE 754 and the observable behaviors of said models.\nThe paper is structured as follows: section 3 discusses the idea of duality in general and briefly discusses how it has been used. Section 4 is an introduction to Hopf algebras, and section 5 extends that to combinatorial Hopf algebra. Section 6 discusses how these concepts apply to transformer models."
        },
        {
            "heading": "3 Duality",
            "text": ""
        },
        {
            "heading": "3.1 Induction",
            "text": "Induction, the basic idea behind algebra and recursion, is a well-understood concept in both mathematics and computer science. Fundamentally, induction is about building up objects from smaller components via the use of a product operation.\nC \u2297 C = C\nInductive data structures are defined in terms of their constructors\nnil : 1 \u2192 U\ncons : D \u00d7 U \u2192 U\nwhere U is the the data type being defined. One can think of the linked list being formalized as:\n\u03b6 : 1 + D \u00d7 U \u2192 U\n[Barbosa, 2022]"
        },
        {
            "heading": "3.2 Coinduction",
            "text": "\u201d[...] coalgebras are about observation. We can think of a coalgebra f : X \u2192 1 + AX as observing about an entity whether it contains something A-detectable or not, and if so which element of A it detects. Having observed something it modifies it. The final coalgebra has as elements all possible outcomes of the behavior you might observe. Do you still have observations to add as list elements? If ever no, we have a finite list. If always yes, we have an infinite list. And there\u2019s no other behavior that can be detected.\u201d [Corfield, 2011]\nCoinduction, a dual of induction, has been relegated to the realm of computer science esoterica. Coinduction, and the related coalgebra, is about analysis, namely specifying how things break down by defining coproduct:\nC = C \u2297 C\nTurning an algebra into a coalgebra is as simple as reversing the arrows. Using this principle, we can create destructors from linked list constructors above like so:\nhead : U \u2192 1\ntail : U \u2192 D \u00d7 U\nThe coinductive list formula is then:\n\u03b1 : U \u2192 1 + D \u00d7 U\n. [Barbosa, 2022] Coinduction is fundamentally about defining how something changes in response to observation.\nAs such, it is the underlying principle of recurrent relationships, generating functions, dynamic systems, coinductive data structures, and fundamentally anything that deals with modeling of observed, possibly infinite, behavior.\nHere are some examples of coinduction:"
        },
        {
            "heading": "3.2.1 Recurrence relations",
            "text": "Recurrence relations is a way of defining terms of a sequence in terms of combinations of previous elements of the sequence.\nThe connection between Hopf algebras and recurrence relations is discussed in [Peterson and Taft, 1980]."
        },
        {
            "heading": "3.2.2 Generating function",
            "text": "A generating function is a device somewhat similar to a bag. Instead of carrying many little objects detachedly, which could be embarrassing, we put them all in a bag, and then we have only one object to carry, the bag.\nGeorge Po\u0301lya, Mathematics and plausible reasoning\nGenerating function is a generalization of formal power series, itself a generalization of power series.\nOrdinary generating function are functions of this format:\nG(an;x) = \u221e\u2211\nn=0\nanx n\nThe nomenclature is not accurate, generating functions are not functions per se, they provide a way of expressing something in terms of a sum of lower dimensional elements with additional constraints on what combines with what via the use of \u201dpowers\u201d. However, these powers are indeterminate, one is not expected to substitute for x and evaluate. Generating functions are for handling infinite sums and establishing recurrences. As such, they have been used extensively in the context of analytic and enumerative combinatorics [Flajolet, 2009].\nThe powerful idea of generating functions is that they provide a unified interface between linearity and non-linearity due to the fact that generating functions provide a stream of semi-rings which can be further combined to a single generating function producing semi-rings.\nAs we will see, the this idea is closely related to the idea of coproduct in Hopf algebras. In the context of machine learning, automatic differentiation can be seen as an an instance of\ngenerating functions [Carothers et al., 2012]."
        },
        {
            "heading": "3.2.3 Stream algebra",
            "text": "We very briefly mention streams, or infinite lists, as they provide a good mental model for how to think about coalgebraic programming. Stream algebras are closely related to generating functions, and have been studied as a way of modeling recurrences, streaming automata, process algebras etc. One can think of them as \u201dstreaming representation-changers\u201d [Gibbons, 2004] as they change their internal state in response to external changes.\nOne way of working with defining coalgebraic object is in terms of (often self-referential) streams or infinite lists by specifying an initial value and an update function. Due to the requirement for laziness, the Haskell programming language is a natural choice for this programming paradigm.\nBy self-referential we mean that the stream we are defining appears on both sides of the assignment operator.\nConsider this Haskell definition a stream representing the natural numbers:\nnats = 1 : map(+1)nats\nNote how nats appears on both sides of the assignment operator and how it is defined in terms of an initial state and an update rule. It is for this reason that [Rutten, 2003] calls coinductive streams differential equations of programming. [Clenaghan, 2018] provides a complete Haskell implementation of such programming paradigm. Said paper praises the coinductive approach for it s \u201deconomy of statement and notation, whilst embracing variety of approach\u201d.\nThe fundamental difference between recursion and corecursion is that recursive Fibonacci function returns only a single value, while corecursive fibonacci returns a stream of all Fibonacci numbers.\nThe stream algebra has a close relationship with Hopf algebra [Bonchi et al., 2014]."
        },
        {
            "heading": "3.2.4 Dynamic programming",
            "text": "Dynamic programming can be naturally expressed in terms of recurrences. The Bellman equation\nV (s) = maxa(R(s, a) + \u03b3V (s \u2032))\nis self-referential. The relationship between coalgebras and dynamic programming is discussed for example in [Hinze et al., 2015]."
        },
        {
            "heading": "3.3 Applications",
            "text": "Coalgebras and coinduction have been slowly but surely gaining popularity. This approach of building up things from their constituent parts enables reasoning about all possible paths and interactions in a black-box dynamic system by analyzing all the possible interdependent interactions among the single atoms starting from the bottom [Barbosa, 2022]. The field of bisimulation defines equivalence in terms of observation equivalences [Sangiorgi, 2012] and thereby verifies the dynamic behaviors of systems.\n\u2022 [Vajjha et al., 2020] has applied coinduction in the context of verified reinforcement learning.\n\u2022 [Hur et al., 2013] discusses how coinduction allows for building proofs incremental by combining small proofs into larger proofs.\n\u2022 [Nguyen and Wu, 2022] discusses backpropagation as a coalgebra.\n\u2022 [Mastorou et al., 2022] extend Haskell to verify coinductive proofs.\n\u2022 [Pous, 2016] discusses how coinduction enabled validating global properties by checking only local properties.\n\u2022 [Kozen and Silva, 2017] introduced CoCaml which discusses the idea of coinductive programming as programming with a coinductive equation solver."
        },
        {
            "heading": "4 Hopf algebra",
            "text": "Hopf algebra is a tensor bialgebra A over a field C meaning it is both a tensor algebra and a tensor coalgebra at once.\nIn the diagram below, we refer to the counit-unit path as \u201dunit impulse path\u201d and the top and bottom paths as the \u201dconvolution paths\u201d.\nHopf algebra enforces coherence between the two paths by updating internal state in response to input [Ahman and Uustalu, 2014]. This is a very powerful invariant for reasoning about the behavior of a linear time-invariant system [Bonchi et al., 2014].\nA\u2297A A\u2297A\nA C A\nA\u2297A A\u2297A\nS\u2297id\nm\u2206\n\u01eb(counit/trace)\n\u2206\nu(unit/cotrace)\nid\u2297S\nm\nConsidering popularity of Hopf algebra in other scientific fields [Hazewinkel, 2004] it was only a matter of time before they are used in machine learning.\nHopf algebra is defined by the following operations.\n\u2022 unit: u: C \u2192 A\n\u2022 product: m: A\u2297A \u2192 A\n\u2022 counit: \u01eb : A \u2192 C\n\u2022 coproduct: \u2206 : A \u2192 A\u2297A\n\u2022 antipode: S : A \u2192 A"
        },
        {
            "heading": "4.1 Unit",
            "text": "The definition of unit is straightforward, it takes an element of C and construct and element of A:\nu : C \u2192 A\nand obeys the rule\nu \u00b7 e = e = e \u00b7 u\nfor all elements of A. One should however think of the unit in relationship to the equalizer."
        },
        {
            "heading": "4.2 Product",
            "text": "The product is the standard tensor product\n(a\u2297 c)(b \u2297 d) = (ab\u2297 cd)"
        },
        {
            "heading": "4.3 Counit",
            "text": "Counit is similar to the idea of trace from linear algebra and as such provides a feedback loop operator from control theory [Hasegawa and Lemay, 2022].\nIt is a dual of the unit and as such it works as a coequalizer."
        },
        {
            "heading": "4.4 Coproduct",
            "text": "The coproduct provides a way of generating all possible splittings of a subset into disjoint pieces [Majid, 1995].\n\u2206(xn) =\nn\u2211\ni=0\nxi \u2297 xn\u2212i\nwhere x0 = 1. One can think of it as converting something into its generating function [Hazewinkel, 2005] or into a sum of elements of lower graded subcoalgebra such as simplices. In the context of physics, it is a probability density function, a total probability mass that\u2019s being shared out among different spaces [Majid, 1995].\n\u2206(wz) = \u2206(w)\u2206(z)\n\u2206(1) = 1 \u2297 1\n\u2206(x) = 1 \u2297 x + x\u2297 1\n\u2206(x2) = 1 \u2297 x + x\u2297 x + x\u2297 1\nFor a more complicated example:\n(id\u2297 \u2206)(\u2206(x2)) =\n= (id\u2297 \u2206)(1 \u2297 x2 + x\u2297 x + x2 \u2297 1)\n= 1 \u2297 \u2206(x2) + x\u2297 \u2206(x) + x2 \u2297 \u2206(1)\n= 1 \u2297 (1 \u2297 x2 + x\u2297 x + x2 \u2297 1) + x\u2297 (1 \u2297 x + x\u2297 1) + x2 \u2297 (1 \u2297 1)\n= 1 \u2297 1 \u2297 x2 + 1 \u2297 x\u2297 x + 1 \u2297 x2 \u2297 1 + x\u2297 1 \u2297 x + x\u2297 x\u2297 1 + x2 \u2297 1 \u2297 1\nFuture work will discuss how superposition emerges out of an extension of a product and coproduct, namely the phased biproduct [Tull, 2018]. But fundamentally, this arises out of the fact that when multiplying a sequence of Hopf algebras, one can select which ones to multiply and as a result, the coproduct represents a combinatorial object which includes all possible ways to choose [Diaconis et al., 2012]."
        },
        {
            "heading": "4.5 Antipode",
            "text": "Since not all transformations are invertible, a weaker structure, antipode, provides a nonlocal \u201dlinearized inverse\u201d. Antipode doesn\u2019t provide an inverse for single elements but for linear combinations. One can think of the antipode as a complex conjugate. However, if the Hopf algebra is finite dimensional, then the antipode is an inverse [Majid, 1995].\nThe antipode is a defined as:\nS \u00b7 S = id\nS(hg) = S(g)S(g)\nThe antipode serves as an intertwining operator, namely a equivariant linear map between two representations which gets updated as the Hopf algebra \u201dlearns\u201d.\nIn the context of interacting particle systems, the intertwiner provides a symmetric exclusion process [Redig and Sau, 2018].\nThe importance of the antipode will become apparent in the context of Hopf convolution."
        },
        {
            "heading": "4.6 Unit impulse, Convolution & Hopf coherence",
            "text": "Given the algebraic operations of the Hopf algebra, one can define the unit impulse \u03b4, also known as Dirac delta, as:\n\u03b4 = \u01eb \u2217 u\n[Christiansen et al., 2022] discusses how traces of evolution operators can be evaluated as integrals over Dirac delta functions.\nThe purpose of the unit impulse is to serve as multiplicative identity for convolution \u2217 which is defined as:\n(f \u2217 g) = m(f \u2297 g)\u2206\nThe fundamental advantage of this convolution over the standard one is, as is the custom with bi-algebras, it provides a way of controlling how splitting-up and recombination works via the coproduct and product.\nThe unit impulse delta function then provides the multiplicative identity:\nf \u2217 \u03b4 = f\nArguably the most important property of Hopf algebra is the fact that this invariant has to hold:\nm(id\u2297 S)\u2206 = \u01eb \u2217 u\nWe refer to this invariant as the Hopf coherence. Hopf coherence enforces that the algebra updates its internal state in order for the unit impulse path to be equal the convolution path. The antipode plays an important part in enforcing this. The antipode reconstructs itself in order for this coherence to hold. This reconstruction process is a result of the \u201dTannaka\u2013Krein duality\u201d or \u201dTanaka reconstruction theorem\u201d [nca, ] [Pareigis, 1994]. The details of this reconstruction process as well as a proof thereof is provided in [Majid, 1995].\nThe exact antipode reconstruction algorithm has been a subject of much research for example in [Berlin, 2019]."
        },
        {
            "heading": "5 Combinatorial Hopf algebra",
            "text": "Combinatorial Hopf algebra is a Hopf algebra where the coproduct is defined as the shuffle product. Besides combinatorics [Grinberg and Reiner, 2014], this algebra has also been explored in the context of control theory [Espinosa et al., 2014]. For a classical introduction into the material, consider [Joni and Rota, 1979]."
        },
        {
            "heading": "5.1 Shuffle product",
            "text": "Originally, the shuffle product arose in the context of card shuffling. In the setting of non-commutative algebras setting, the shuffle product provides a way of generating all the ways in which two words can be interwoven. For another example, imagine a situation where cars from two lanes merge into one lane. The shuffle product is a combinatorial object that represents all the possible ways in which cars from the two lanes can be interleaved to merge into one. Since word composition is non-commutative, the shuffle product is a natural match for representing combinatorial objects of words.\nFor example:\nab ab = 4aabb + 2abab\nab ba = abab + 2abba + 2baab + baba\n[Lothaire, 1997] The most surprising property of the combinatorial Hopf algebra is that one can\ncalculate the eigenvectors by repeated squaring (coproduct-product) [Aguiar and Lauve, 2013].\n[Pang, 2014] provides some intuition into this coproduct-product operation and how they correspond a repeated splitting and combination of combinatorial objects.\n[Diaconis et al., 2012] uses this property in the context of diagonalization of Markov chains in natural bases.\nThe shuffle product also allows for interpreting differential equations combinatorially [Mishna and Zabrocki, 2008]."
        },
        {
            "heading": "6 Transformers",
            "text": "Since first being introduced in [Vaswani et al., 2017], the transformer architecture has become one of the most widely studied architectures and as such has been discussed extensively for example by [Phuong and Hutter, 2022], [Elhage et al., 2021]. Due to this, we only concentrate on selected parts namely the attention mechanism and the residual stream.\nIn this section, we argue that the attention-only transformer model can be understood in terms of combinatorial Hopf algebras and as such can be analyzed as a linear time-invariant system [Espinosa et al., 2018].\nThe summary of our argument is that since the residual stream has no preferred basis, it should be understood as a trace/counit/unit impulse, and since the attention mechanism is a positive definite matrix, it should be understood as a transfer function of the system. The model learns by enforcing Hopf coherence between the two paths.\nThe attention heads continuously update the residual stream (trace) and their internal state in order to enforce the invariance between the unit impulse path and Hopf coherence.\nCombinatorial Hopf algebra is a good match since it captures the non-commutativity of word composition."
        },
        {
            "heading": "6.1 Residual stream",
            "text": "The residual stream has been described as a channel used by single components of the model to communicate among themselves.\nWe argue that the residual stream is the trace/counit/unit impulse of the model. The major hint is that the residual stream has been observed to have no privileged basis\n[Elhage et al., 2021]. This is analogous to the behavior of the trace in linear algebra where the trace of a matrix is also independent of the basis. While in linear algebra the trace is a scalar, categorical trace is more akin to formal power series [Hines and Scott, 2007].\nAs such the residual stream serves a similar purpose to the evaluation trace in automatic differentiation, namely establishing recurrences. This is unsurprising considering the fact that automatic differentiation is a recurrence relationship based on formal power series [Carothers et al., 2012], in particular the Taylor series [Hoffmann, 2014].\nThe residual stream is a recurrence relationship that attention heads use for both steering (by writing into it) and being steered by it (by reading from it). The values in the stream are continuously updated in response to input in order to enforce Hopf coherence.\nThe residual stream provides the notion of a feedback analogous to feedback in control theory or linear systems while the attention mechanism is the transfer function.\nIn the context of linear time-invariant systems, the transfer function is the inverse Laplace transform of the unit impulse response and vice-versa.\nThis duality can be understood as the trace-transfer function duality [Dold and Puppe, 1985]. It can also be understood as the duality between the trace and the fixed point [Hasegawa, 2004] that\u2019s analogous to Tannaka\u2013Krein duality. By interacting with the residual stream, attention heads are steering the direction of the fixed point search algorithm that the transformer performs.\nFuture research will investigate the exact nature of this interaction."
        },
        {
            "heading": "6.2 Attention",
            "text": "The attention mechanism\u2019s role is the same as that of a transfer function in a linear time-invariant system, namely it calculates the frequency response of the transformer model, in the case of transformers, the output token.\nOur interpretation is based on the fact that attention matrices have been observed to have real positive eigenvalues. From that we conclude that they are symmetric positive definite matrices. This symmetry enables an input-output symmetry [Majid, 1995].\nAttention mechanism operates on three inputs: query, key, value and generates one output. Please reference [Vaswani et al., 2017] for explanation of these terms.\n[Elhage et al., 2021] formalizes the model as such:\nT = Id\u2297WUWE + \u2211\nh\u2208H\nAh \u2297 (WUW h OW h V WE)\nA = softmax(tT \u00b7W TEW T QWKWE \u00b7 t)\nFurthermore, they analyze the attention mechanism by splitting it into two circuits, QK (querykey), and OV (output-value) and the associated matrices WQK and WOV .\nThese two circuits roughly correspond to coproduct-product relationship of the convolutional path where the coproduct splits things and product recombines them as discussed in [Diaconis et al., 2012] or [Redig and Sau, 2018]."
        },
        {
            "heading": "6.2.1 QK circuit",
            "text": "The QK circuit assigns an attention score for a given query and key token. This score indicates how much the query wants to attend to the particular key token. We mostly agree with the interpretation of [Chen, 2021] and [Yuan et al., 2021] which understand this circuit as Markov chain transition matrices, as they are row-stochastic matrices, with each row summing to 1.\n\u03b1\u2211\nj=1\nPi,j = 1\nThis circuit fundamentally distributes the unit of attention among the potential candidates. Since it is a distributive operation, it is to be understood as a part of the coalgebraic circuit.\nThese weights can be understood as stochastic recurrence relations. They are kept around for as long as they are \u201dinteresting\u201d and after that, they are gradually pruned.\nSince the QK circuit generates the possible candidates, it can be understood as providing induced representation of the Markov chain. Induced representation is a tool in representation theory that enables building representations of large objects from representations of small objects. [Schmitt, 1993] discusses this in more detail.\n\u201dCoinjection is a partially-defined function whose restriction to where it is defined is a bijection; an example is f : 1, 2, 3, 4 \u2192 7, 8 with f(1) = 8, f(3) = 7, and f(2), f(4) undefined). Intuitively, these are combinatorial structures with a notion of restriction on a subset of their vertex set; one can restrict a graph to a subset of its vertices by considering only the edges connected to this subset (usually known as the induced subgraph)\u201d.\n[Pang, 2014]"
        },
        {
            "heading": "6.2.2 OV circuit",
            "text": "The OV circuit operates on the candidate values and combines them to produce a single output. Therefore it belongs to the product circuit.\n[Elhage et al., 2021] discusses the behavior of the OV circuit in terms of eigenvalues of the OV matrix. It was observed that there\u2019s a strong indication that if the matrix has positive eigenvalues, it is likely copying. Since the eigenvalues are real and positive we can conclude that it is a symmetric positive definite matrix.\nA symmetric positive definite matrix, since it is a self-adjoint operator, is equal to its own conjugate transpose, i.e.\nA = AT\nSelf-adjointness can be thought of as enforcing conservation of energy laws [Ibragimov, 2011]. If we interpret the QK circuit as branching, we can interpret the the OV circuit as combining in the sense of [Pang, 2014].\nAs such it can be thought of as the antipode [Li and Zhang, 2020] or intertwining operator. In the context of Markov chains, positive definite matrices can be understood as mod-\neling interactions between Markov chains [O\u2019Connell, 2019]."
        },
        {
            "heading": "6.3 Transformer learning mechanism",
            "text": "The most important property of the residual stream is its basis independence, while the most important property of the attention mechanism is its symmetry and positive definiteness.\nIn our interpretation of transformers, the transformer model learns by enforcing Hopf coherence between the residual stream (unit response path) and the attention path (convolution path).\n[Elhage et al., 2021] describes the attention mechanism as \u201dsum where every term corresponds to an end-to-end path\u201d. This is fundamentally convolution as it can be considered a sum of all impulse responses [Cheever, 2022].\nIn our model learning works as follows:\n1. attention head receives a input\n2. attention head calculates output from both the unit response path and convolution path\n3. if the outputs match, the token gets copied\n4. if the outputs differ, the transformer propagates the difference (error) backward along the convolution path and unit impulse path, distributes the error between the QK and OV mechanisms and updates the residual stream (trace)\n5. by repeating this coproduct-product process (squaring) with different inputs, the transformer learns a stationary Markov chain distribution in its natural basis [Diaconis et al., 2012]\nThis mechanism of calculating the gradient of the loss function provides a better alternative to automatic differentiation since the error is calculated within the single layer and there\u2019s no explicit backward pass.\nThe details of this mechanism will be discussed in future work, however the intuition is that it is related to Laplace transform and Tannaka\u2013Krein duality.\nLaplace transform has been observed to arise out of the interaction between convolution and shuffle product of combinatorial Hopf algebras [Rutten, 2019].\nTannaka\u2013Krein, as a non-commutative generalization of Pontryagin duality, then obviates the relationship to Laplace transform.\n[Pavlovic and Escardo, 2001] also explores the Laplace transform in the context of coinduction. Another way of looking at it is via conjectured relationship between the induced representation (QK) and the intertwining operator (OV) on one end and the trace (residual stream) on the other [Arthur, 2008].\nDynamic programming provides a simplified, if not simplistic, model of certain elements of transformer models that is nonetheless rather useful. The main difference is that most aspects of dynamic programming algorithms are, rather ironically, quite static. In comparison, the transformer prunes its recurrences when combining a solution from subsolutions.\nThe interpretation in the context of DP also explains the \u201dphase change\u201d of transformer models. When the DP algorithm first starts filling out the grid, a lot of fields will be updated over a short period of time. Gradually, the frequency of the updates will decrease and the previous subsolutions will just be copied over."
        },
        {
            "heading": "6.4 Attention head composition",
            "text": "Zero layer attention only model learns bigram statistics. Since a bigram is just a Markov chain, these can be thought of as Hopf algebras without induced representation and intertwining operators. As a result, squaring the Hopf algebra similarly to [Diaconis et al., 2012] makes the model learn a bigram. Chapter 5 of [Majid, 1995] discusses how Hopf algebras can be used to model Markov chains.\nIronically, in our model, the one layer and two layer attention models are somewhat similar. One layer attention only model then learns an ensemble of bigram and skip-gram and the two\nlayer attention head then learns induction heads.\nThe exact mechanism how this works will be explored in future work, however the intuition is that since the shuffle product allows us to generate possible interleavings and ways of combining them, the chain \u201dA ... B C\u201d can be understood as a sum over all the coproducts which start with \u201dA\u201d and end with \u201dB C\u201d.\n[Ebrahimi-Fard et al., 2019] discusses these matters in terms of gap-insertion operad of noncrossing partitions."
        },
        {
            "heading": "7 Conclusion and future work",
            "text": "To summarize this paper, combinatorial Hopf algebras provides a rich algebraic infrastructure to interpret transformer models. This algebra has the surprising property of being able to calculate eigenvalues by repeated squaring.\nBy interpreting transformers as Hopf algebras, we arrive at a new understanding of the transformer learning mechanism as enforcement of Hopf coherence.\nThis mechanism boils down to enforcing coherence between the unit impulse path and the convolution path via Tannaka\u2013Krein duality. As such, the transformer model can be understood as a linear time-invariant system.\nIn future work, we will discuss Hopf coherence in more detail."
        }
    ],
    "title": "Coinductive guide to inductive transformer heads",
    "year": 2023
}