{
    "abstractText": "In this paper, we propose a novel low-complexity hand gesture recognition framework via a multiple Frequency Modulation Continuous Wave (FMCW) radar-based sensing system. In this considered system, two radars are deployed distributively to acquire motion vectors from different observation perspectives. We first independently extract reflection points of the interested target from different radars by applying the proposed neighboring reflection points detection method after processing the traditional 2-dimensional Fast Fourier Transform (2D-FFT). The obtained sufficient corresponding information of detected reflection points, e.g., distances, velocities, and angle information, can be exploited to synthesize motion velocity vectors to achieve a high signal-to-noise ratio (SNR) performance, which does not require knowledge of the relative position of the two radars. Furthermore, we utilize a long short-term memory (LSTM) network as well as the synthesized motion velocity vectors to classify different gestures, which can achieve a significantly high accuracy of gesture recognition with a 1600-sample data set, e.g., 98.0%. The experimental results also illustrate the robustness of the proposed gesture recognition systems, e.g., changing the environment background and adding new gesture performers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yinzhe Mao"
        },
        {
            "affiliations": [],
            "name": "Minhao Ling"
        }
    ],
    "id": "SP:584fd296191e2c3e25689b229ff4f684688b87b6",
    "references": [
        {
            "authors": [
                "S. Ahmed",
                "K.D. Kallu",
                "S.H. Cho"
            ],
            "title": "Hand gestures recognition using radar sensors for human-computer-interaction: A review",
            "venue": "Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Nigam",
                "M. Shamoon",
                "S. Dhasmana",
                "T. Choudhury"
            ],
            "title": "A complete study of methodology of hand gesture recognition system for smart homes",
            "venue": "In Proceedings of the 2019 International Conference on Contemporary Computing and Informatics (IC3I), Singapore,",
            "year": 2019
        },
        {
            "authors": [
                "T. Chen",
                "L. Xu",
                "X. Xu",
                "K. Zhu"
            ],
            "title": "Gestonhmd: Enabling gesture-based interaction on low-cost VR head-mounted display",
            "venue": "IEEE Trans. Vis. Comput. Graph",
            "year": 2021
        },
        {
            "authors": [
                "T.K. Yin",
                "C.M. Manders",
                "F. Farbiz",
                "L.M. Hwan",
                "C.G. Guan",
                "C.Y. Ho"
            ],
            "title": "Intergrating a gesture interface to a commercial online dance game",
            "venue": "In Proceedings of the 2009 IEEE International Conference on Multimedia and Expo,",
            "year": 2009
        },
        {
            "authors": [
                "K. Niu",
                "F. Zhang",
                "Y. Jiang",
                "J. Xiong",
                "Q. Lv",
                "Y. Zeng",
                "D. Zhang"
            ],
            "title": "WiMorse: A contactless morse code text input system using ambient WiFi signals",
            "venue": "IEEE Internet Things J. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "K. Kudrinko",
                "E. Flavin",
                "X. Zhu",
                "Q. Li"
            ],
            "title": "Wearable sensor-based sign language recognition: A comprehensive review",
            "venue": "IEEE Rev. Biomed. Eng",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wu",
                "T.S. Huang"
            ],
            "title": "Vision-based gesture recognition: A review",
            "venue": "In International Gesture Workshop; Springer: Berlin/Heidelberg, Germany,",
            "year": 1999
        },
        {
            "authors": [
                "J. Liu",
                "H. Liu",
                "Y. Chen",
                "Y. Wang",
                "C. Wang"
            ],
            "title": "Wireless sensing for human activity: A survey",
            "venue": "IEEE Commun. Surv. Tutor",
            "year": 2019
        },
        {
            "authors": [
                "Z. Lu",
                "X. Chen",
                "Q. Li",
                "X. Zhang",
                "P. Zhou"
            ],
            "title": "A hand gesture recognition framework and wearable gesture-based interaction prototype for mobile devices",
            "venue": "IEEE Trans. Hum. Mach. Syst",
            "year": 2014
        },
        {
            "authors": [
                "Y. Ling",
                "X. Chen",
                "Y. Ruan",
                "X. Zhang"
            ],
            "title": "Comparative study of gesture recognition based on accelerometer and photoplethysmography sensor for gesture interactions in wearable devices",
            "venue": "IEEE Sens. J. 2021,",
            "year": 2023
        },
        {
            "authors": [
                "N. Tubaiz",
                "T. Shanableh",
                "K. Assaleh"
            ],
            "title": "Glove-based continuous Arabic sign language recognition in user-dependent mode",
            "venue": "IEEE Trans. Hum.-Mach. Syst",
            "year": 2015
        },
        {
            "authors": [
                "I. Laptev",
                "M. Marszalek",
                "C. Schmid",
                "B. Rozenfeld"
            ],
            "title": "Learning realistic human actions from movies",
            "venue": "In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2008
        },
        {
            "authors": [
                "C. Chen",
                "K. Liu",
                "N. Kehtarnavaz"
            ],
            "title": "Real-time human action recognition based on depth motion maps",
            "venue": "J. Real Time Image Process",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "Q. Miao",
                "K. Tian",
                "Y. Fan",
                "X. Xu",
                "R. Li",
                "J. Song"
            ],
            "title": "Large-scale gesture recognition with a fusion of RGB-D data based on saliency theory and C3D model",
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "year": 2017
        },
        {
            "authors": [
                "E. Ohn-Bar",
                "M.M. Trivedi"
            ],
            "title": "Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations",
            "venue": "IEEE trans. Intell. Transp. Syst",
            "year": 2014
        },
        {
            "authors": [
                "Z.r. Wang",
                "P. Wang",
                "L. Xing",
                "L.P. Mei",
                "J. Zhao",
                "T. Zhang"
            ],
            "title": "Leap Motion-based virtual reality training for improving motor functional recovery of upper limbs and neural reorganization in subacute stroke patients",
            "venue": "Neural Regen. Res. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "S. Al Ayubi",
                "D.W. Sudiharto",
                "E.M. Jadied",
                "E. Aryanto"
            ],
            "title": "The prototype of hand gesture recognition for elderly people to control connected home devices",
            "venue": "In Journal of Physics: Conference Series, Proceedings of the International Conference on Electronics Representation and Algorithm (ICERA 2019), Yogyakarta, Indonesia,",
            "year": 2019
        },
        {
            "authors": [
                "K. Niu",
                "F. Zhang",
                "X. Wang",
                "Q. Lv",
                "H. Luo",
                "D. Zhang"
            ],
            "title": "Understanding WiFi signal frequency features for position-independent gesture sensing",
            "venue": "IEEE Trans. Mob. Comput",
            "year": 2021
        },
        {
            "authors": [
                "W. He",
                "K. Wu",
                "Y. Zou",
                "Z. Ming"
            ],
            "title": "WiG: WiFi-based gesture recognition system",
            "venue": "In Proceedings of the 2015 24th International Conference on Computer Communication and Networks (ICCCN), Las Vegas, NV, USA,",
            "year": 2015
        },
        {
            "authors": [
                "Z. Hao",
                "Y. Duan",
                "X. Dang",
                "Y. Liu",
                "D. Zhang"
            ],
            "title": "Wi-SL: Contactless fine-grained gesture recognition uses channel state information",
            "venue": "Sensors",
            "year": 2020
        },
        {
            "authors": [
                "Q. Pu",
                "S. Gupta",
                "S. Gollakota",
                "S. Patel"
            ],
            "title": "Whole-home gesture recognition using wireless signals",
            "venue": "In Proceedings of the 19th Annual International Conference on Mobile Computing & Networking, Miami, FL, USA,",
            "year": 2013
        },
        {
            "authors": [
                "D. Zhang",
                "D. Wu",
                "K. Niu",
                "X. Wang",
                "F. Zhang",
                "J. Yao",
                "D. Jiang",
                "F. Qin"
            ],
            "title": "Practical issues and challenges in CSI-based integrated sensing and communication",
            "venue": "In Proceedings of the 2022 IEEE International Conference on Communications Workshops (ICC Workshops), Seoul, Republic of Korea,",
            "year": 2022
        },
        {
            "authors": [
                "J. Lien",
                "N. Gillian",
                "M.E. Karagozler",
                "P. Amihood",
                "C. Schwesig",
                "E. Olson",
                "H. Raja",
                "I. Poupyrev"
            ],
            "title": "Soli: Ubiquitous gesture sensing with millimeter wave radar",
            "venue": "ACM Trans. Graph. (TOG)",
            "year": 2016
        },
        {
            "authors": [
                "G. Malysa",
                "D. Wang",
                "L. Netsch",
                "M. Ali"
            ],
            "title": "Hidden Markov model-based gesture recognition with FMCW radar",
            "venue": "In Proceedings of the 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP),",
            "year": 2016
        },
        {
            "authors": [
                "Y.C. Jhaung",
                "Y.M. Lin",
                "C. Zha",
                "J.S. Leu",
                "M. K\u00f6ppen"
            ],
            "title": "Implementing a hand gesture recognition system based on Range-Doppler Map",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "C. Gu",
                "J. Mao"
            ],
            "title": "4-D gesture sensing using reconfigurable virtual array based on a 60-GHz FMCW MIMO radar sensor",
            "venue": "IEEE Trans. Microw. Theory. Tech",
            "year": 2022
        },
        {
            "authors": [
                "C. Li",
                "Z. Peng",
                "T.Y. Huang",
                "T. Fan",
                "F.K. Wang",
                "T.S. Horng",
                "J.M. Munoz-Ferreras",
                "R. Gomez-Garcia",
                "L. Ran",
                "J. Lin"
            ],
            "title": "A review on recent progress of portable short-range noncontact microwave radar systems",
            "venue": "IEEE Trans. Microw. Theory Tech",
            "year": 2017
        },
        {
            "authors": [
                "J. Hasch",
                "E. Topak",
                "R. Schnabel",
                "T. Zwick",
                "R. Weigel",
                "C. Waldschmidt"
            ],
            "title": "Millimeter-wave technology for automotive radar sensors in the 77 GHz frequency band",
            "venue": "IEEE Trans. Microw. Theory Tech",
            "year": 2012
        },
        {
            "authors": [
                "Y. Kim",
                "B. Toomajian"
            ],
            "title": "Hand gesture recognition using micro-Doppler signatures with convolutional neural network",
            "venue": "IEEE Access 2016,",
            "year": 2016
        },
        {
            "authors": [
                "S. Ahmed",
                "W. Kim",
                "J. Park",
                "S.H. Cho"
            ],
            "title": "Radar-based air-writing gesture recognition using a novel multistream CNN Approach",
            "venue": "IEEE Internet Things J. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xia",
                "Y. Luomei",
                "C. Zhou",
                "F. Xu"
            ],
            "title": "Multidimensional feature representation and learning for robust hand-gesture recognition on commercial millimeter-wave radar",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2020
        },
        {
            "authors": [
                "Z. Xia",
                "F. Xu"
            ],
            "title": "Time-space dimension reduction of millimeter-wave radar point-clouds for smart-home hand-gesture recognition",
            "venue": "IEEE Sens. J. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J.W. Choi",
                "S.J. Ryu",
                "J.H. Kim"
            ],
            "title": "Short-range radar based real-time hand gesture recognition using LSTM encoder",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yang",
                "X. Zheng"
            ],
            "title": "Hand gesture recognition based on trajectories features and computation-efficient reused LSTM network",
            "venue": "IEEE Sens. J. 2021,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Mao, Y.; Zhao, L.; Liu, C.;\nLing, M. A Low-Complexity Hand\nGesture Recognition Framework via\nDual mmWave FMCW Radar System.\nSensors 2023, 23, 8551. https://\ndoi.org/10.3390/s23208551\nAcademic Editors: Federico Alimenti\nand Roberto Vincenti Gatti\nReceived: 19 September 2023\nRevised: 8 October 2023\nAccepted: 12 October 2023\nPublished: 18 October 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: gesture recognition; mmWave FMCW radar; dual-radar system; signal processing; deep learning"
        },
        {
            "heading": "1. Introduction",
            "text": "Hand gestures are one of the most effective communication means for human beings and have been used in various applications for human\u2013computer interaction (HCI) [1], including smart homes [2], virtual reality (VR) [3], game control [4], and text input [5]. In these applications, accurate gesture recognition plays an important role in enabling sophisticated functionalities and ensuring user experience. As such, many existing works have attempted to develop gesture recognition solutions, for which there are mainly three categories, i.e., wearable device-based solutions [6], visual device-based solutions [7], and wireless sensing-based solutions [8]. Wearable device-based solutions rely on the sensors equipped on wearable devices to collect motion data in real time [9\u201311]. However, the requirement of wearing the devices can lead to annoying inconveniences and hence may limit their applications. Visual devicebased solutions rely on camera sensors for gesture recognition [12\u201314] and can be applied in a diverse range of scenarios [15\u201317] due to their exceptional experiential qualities. The main drawback of a visual device-based solution is its potential possibility of the invasion of privacy. Wireless sensing-based solutions deal with the above problems by measuring and analyzing radio signals that interact with human gestures [8]. One popular sensingbased solution relies on WiFi systems [5,18\u201320], which can classify human gestures via Doppler features extracted from the channel state information (CSI) obtained at WiFi transceivers [21]. One potential limiting factor of WiFi-based systems is that obtaining CSIs\nSensors 2023, 23, 8551. https://doi.org/10.3390/s23208551 https://www.mdpi.com/journal/sensors\nfrom commercial WiFi chips may be difficult since most commercial WiFi CSI tools are proprietary of chip manufactures [22] and are not open. As an alternative sensing-based gesture recognition solution, mmWave radar-based systems [23\u201326] have recently received significant attention due to the recent advances in mmWave radar chip manufacturing [27] and the continuing reduction in the cost and physical size of hardware [28]. Unlike WiFi-based solutions, mmWave radar-based solutions do not need to stick to the CSI measurement intervals specified by WiFi standard [22], hence allowing for more flexible measurements for hand gesture recognition. Moreover, as mmWave radar operates in a dedicated frequency band that is different from most existing communication systems, the impact of mutual interference is less severe. This works focuses on mmWave radar-based systems for hand gesture recognition. Specifically, we consider mmWave Frequency Modulation Continuous Wave (FMCW) radar for hand gesture recognition, motivated by its capability of simultaneously measuring the range and velocity of the target of interest. Unlike most FMCW-based systems that consider the use of a single FMCW-radar to extract the features of a hand gesture, in this work, we consider a dual FMCW system that utilizes two FMCW radars to measure the signals of human hand gestures. We note that as the chip size and cost are continuing to drop for mmWave FMCW radars, it is possible to deploy multiple mmWave FMCW radar sensors in the same space, e.g., in a smart home, to support diverse applications such as human activity detection, health monitoring, and hand gesture recognition. To exploit the benefit of two mmWave FMCW radars, we first develop an algorithm to extract the motion velocity vectors of human hand gestures based on the fusion of range and velocity information obtained from the multiple radars. As will be shown later in this paper, the motion velocity vector acts as an distinct feature that can be used to classify different human gestures. Based on the motion velocity vector extracted, a gesture classification model based on a long short-term memory (LSTM) network is proposed to capture the sequential dependence of the velocities. Our main contributions are highlighted below:\n\u2022 We develop a systematic framework of high-accuracy hand gesture recognition based on two mmWave FMCW radars. The proposed framework first detects the moving target and identifies the reflection points of the target from each frame of rangeDoppler (r-D) images generated from the two radars. The angular information of the target is then obtained by applying beamforming. Then, the motion velocity vector is synthesized by combining the angles and velocities of the target estimated by the two radars. \u2022 We propose a novel feature extraction algorithm to synthesize the motion velocity vector that is crucial for hand gesture recognition. The proposed algorithm can effectively estimate the true velocity of the moving target and eliminates the requirement of the knowledge on the relative position of the two radars. This allows for flexible deployment of the two radars \u2022 We propose a deep-learning gesture classification model based on the LSTM network, which effectively captures the sequential correlations between the motion velocities at different time instances. The proposed model takes only the motion velocity vector as the input feature and is shown to achieve satisfactory hand gesture recognition accuracy and robustness. In an experiment contributed by 10 volunteers with 1600 samples, the proposed model achieves 98.0% accuracy.\nThe rest of the article is organized as follows. Section 2 discusses related work with a focus on existing FMCW-based hand gesture recognition solutions. Section 3 presents some basic principles of FMCW radar processing Section 3. Our proposed hand gesture solution is detailed in Section 4. In Section 5, we present experiment results and provide a comparison with traditional approaches. Conclusions are drawn in Section 6. For ease of exposition, vectors are denoted by bold lowercase characters, whereas matrices are represented by bold uppercase characters."
        },
        {
            "heading": "2. Related Work",
            "text": "Classification algorithms for hand gesture recognition are mainly divided into two categories: machine learning-based solutions and deep learning-based solutions. At the early stage of FMCW-based hand gesture recognition, machine learning is the mainstream method for classification. The method in [23], proposed by the Google team, extracts nine low-dimensional motion features and applies a Random Forest classifier to identify gestures for HCI applications. In addition, Malysa et al. [24] proposed gathering motion feature vectors using a 77 GHz FMCW radar and training them based on Hidden Markov Models (HMM). With the development of neural networks, there are numerous hand gesture recognition systems incorporating neural networks into their designed hand gesture recognition systems, and the Convolutional Neural Network (CNN) is one of the most widely adopted algorithms. As a representative example, ref. [29] proposed generating micro-Doppler (m-D) images from the radar signals and then using a pre-trained CNN model for hand gesture classification. This approach is a straightforward application of the procedure commonly adopted in imaging processing, but for hand gesture classification, its classification accuracy is limited since the m-D images cannot distinguish between different gestures with similar image patterns, e.g., the gesture of swiping from left to right and the gesture of swiping from right to left. To resolve this issue, ref. [30] proposed a multi-stream CNN model that takes three types of figures as the input, i.e., the range-time figure, the Doppler-time figure, and the angle-time figure, and achieves better classification accuracy. The authors of [31] further improved the accuracy of gesture recognition by exploiting the range-Doppler-angular multi-dimensional gesture features space and exploring optimal multi-dimensional feature combination strategies for a multi-channel CNN. In addition, based on their previous work, the authors in [32] proposed the method of spatial position alignment and adopted a multi-channel CNN for multi-position hand gesture recognition. The LSTM network is extensively employed as another prominent method in hand recognition systems. For example, the authors of [33] extracted motion profiles from r-D images and used the LSTM encoder to complete the final recognition by extracting the global temporal features. Meanwhile, the authors of [34] obtained the range-Dopplerangle trajectories from a 77 GHz FMCW Multiple Input Multiple Output (MIMO) radar, and built a LSTM network with the reused forward propagation approach to learn the gesture features."
        },
        {
            "heading": "3. Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1. Radar Signal Model",
            "text": "In this subsection, we briefly introduce the principle of the FMCW radar. The transmitted signal of the FMCW radar consists of a series of continuous frequency-modulated waves. The signal within a frequency modulation period is called a chirp. The transmitted FMCW signal frame consists of multiple chirps; see Figure 1 for an illustration, and all notations are listed in Table 1.\nwhere t is the fast time, t \u2208 [0, Tchirp], AT denotes the complex amplitude coefficient of the transmitted signal, and K is the frequency modulation slope. The signal echoed from a moving target and received by the radar can be represented as [31]:\nsR(t) = AR exp { j2\u03c0[( f0 + fd)(t\u2212 \u2206t) + \u222b t\n\u2206t [K(\u03c4 \u2212 \u2206t)]d\u03c4]\n} , (2)\nwhere AR is the complex amplitude coefficient of the received signal, fd = 2v\u03bb = 2v fc\nc denotes the Doppler shift, \u03bb is the wavelength, fc is the carrier frequency of the transmitted signal, v is the velocity, \u2206t = 2dc is the flight time of the signal, c is the speed of light, and d denotes the range of the target. The received signal sR(t) is mixed with the transmitted signal upon receiving it to produce the intermediate frequency (IF) signal, which can be represented as [31]:\ny(t) = sT \u00d7 s\u2217R = AT AR \u00b7 exp{j2\u03c0( f0 + fd)\u2206t + (K\u2206t\u2212 fd)t\u2212 K\u2206t2/2]}. (3)\nSince K\u2206t2/2 is typically much smaller than other terms in Equation (3), the IF signal can be approximated as:\ny(t) \u2248 A \u00b7 exp{j[2\u03c0( f \u2212 fd)t + \u03d5]}, (4)\nwhere A = AT \u00b7 AR, f = K\u2206t is the frequency of the IF signal and\n\u03d5 = 2\u03c0( fc + fd) \u00b7 \u2206t = ( fc + fd) \u00b7 4\u03c0d c \u2248 4\u03c0d \u03bb , (5)\nis the time-varying phase. For different chirps in the same frame, f and \u03d5 will change from one chirp to another. This can be used to estimate of the velocity and the distance of the target of interest. Supposing the IF signal is sampled at a fixed time interval Ts, then the discrete-time samples of a frame can be stored in a two-dimensional matrix Y \u2208 CN\u00d7L, for which each column corresponds to a different chirp. Here, N is the number of samples in each chirp and L is the number of chirps in a frame. Following Equation (4), the entries of Y can be represented as:\nY[n, l] = A \u00b7 exp{j[2\u03c0( fl \u2212 fd)nTs + \u03d5l ]}, (6)\nwhere n is the fast time index, standing for the index of sampling points in the modulation time, n = 1, 2, . . . N, and l is the slow time index, corresponding to the chirp index in a frame, l = 1, 2, . . . , L. fl and \u03d5l are related to the range of target dl in the l-th chirp."
        },
        {
            "heading": "3.2. Range and Velocity Estimation",
            "text": "To extract the range information of the target, Fast Fourier Transform (FFT) is applied to each column of Y. For the l-th chirp (column), the frequency domain signal can be represented as:\ny\u0302l [\u03c9] = A exp(j\u03d5l)\u03b4[\u03c9\u2212 ( fl \u2212 fd)Ts] \u2248 A exp(j\u03d5l)\u03b4(\u03c9\u2212 flTs). (7)\nAfter the FFT operation, the range of the target can be estimated according to the peak index of the frequency response, denoted as \u03b7l , and the relationship between the fl , Tchirp, and \u03b7l can be expressed as follows:\nno\u03b7l = fl/ 1\nTs \u00b7 N = fl \u00b7 Tchirp, (8)\nwhere fl = K\u2206t = 2Kdl/c. Then, the range between the radar and the target can be calculated as:\ndl = \u03b7l \u00b7 c 2KTc = \u03b7l \u00b7 c 2B . (9)\nTo estimate the velocity of the target, FFT is applied to each row of matrix Y. For the n-th row, the IF signal in Equation (6) can be rewritten as:\nyn[l] =A \u00b7 exp{j[2\u03c0( fl \u2212 fd)nTs]} \u00b7 exp { j [ \u03d5\u2032 + (l \u2212 1)\u2206\u03d5 ]} , (10)\nwhere \u03d5\u2032 is the phase of yn[1], \u2206\u03d5 is the phase difference of two adjacent elements of yn, and \u2206\u03d5 = 4\u03c0vTchirp/\u03bb. For the n-th row, the difference between adjacent values of f is much smaller than \u2206\u03d5; it can be approximated as f1. Then, the signal after the FFT operation can be represented as:\ny\u0302n[\u03be] = A \u00b7 exp{j[2\u03c0( f1 \u2212 fd)nTs] + \u03d5\u2032} \u00b7 \u03b4[\u03be \u2212 2vTchirp\n\u03bb ]. (11)\nDenoting \u00b5n as the frequency index of the peak in the spectrum, the velocity of the target can be calculated as:\nvn = \u00b5n\u03bb\n2Tchirp . (12)"
        },
        {
            "heading": "3.3. Direction of Target Movement",
            "text": "For hand gesture recognition, it is intuitive that the direction of the hand movement is an important feature. However, such a feature is not directly available from single FMCW radar signals. To see this, let us consider a simple example, as illustrated in Figure 2, where a reflecting target P moves in the field of view (FoV) of the radar with velocity v towards direction \u03b1. We assume that the FMCW radar has multiple antennas. Thus, the instantaneous direction of the target with respect to the radar, i.e., \u03b8, can be estimated via array signal processing techniques [35]. The velocity of the motion can be decomposed into the radial velocity, vn, and the tangent velocity, v\u03c4 . The radial velocity, vn, can be expressed as [18]:\nvn = sin(\u03c0 \u2212 (\u03b1 + \u03b8)) \u00b7 v = sin(\u03b1 + \u03b8) \u00b7 v. (13)\nAs demonstrated, the azimuth angle, \u03b8, and the direction of motion, \u03b1, are the primary factors that influence vn. The velocity estimated by the principles mentioned in Section 3.2 correspond to the radial velocity vn since it is estimated based on the Doppler principle. The tangent velocity v\u03c4 , which may play an important role in gesture recognition, cannot be estimated. Motivated by this issue, we propose to utilize two mmWave FMCW radars with a MIMO antenna array."
        },
        {
            "heading": "4. The Dual FMCW Radar System for Gesture Recognition",
            "text": "In this section, we introduce the dual FMCW radar system for gesture recognition and detail the signal processing procedure. The dual-radar system consists of two mmWave FMCW radars that are placed at the same height, with their FoVs partially overlapped. The two radars are both connected to the same controller (e.g., a computer) for FMCW control and synchronization. The IF signals sampled from these two radars are both collected at the controller for further signal processing. Figure 3 illustrates the overall procedure of the hand gesture recognition system. The IF signals are first processed to remove the static part of the clutter by eliminating mean value of phase, followed by 2-dimensional Fast Fourier Transform (2D-FFT) to convert the time-domain IF signal into the r-D space. A neighboring-reflection-points detection method is then developed to identify the region of interest (ROI) in the r-D space. Upon the identification of the ROI, the information of radial velocity and angle of the target is then extracted from both the radars. The extracted information is then used to synthesize the motion velocity feature, which is fed into an LSTM network for gesture classification."
        },
        {
            "heading": "4.1. r-D Region of Interest Detection",
            "text": "As the clutter of environmental background can significantly influence the results of target detection, the proposed algorithm first removes the cluttering signals and finds\nthe ROI from the obtained movement data set to mitigate the impact of environmental background on gesture recognition. With respect to the static part of the clutter, it is easy to eliminate its impact by averaging the different chirps of IF signals. Specifically, the reference signal, yavg \u2208 CN\u00d71, can be obtained by summing and averaging the column vectors of the IF data matrix Y (see Section 3.1 for its definition):\nyavg = 1 L\nL\n\u2211 l=1 Y[:, l]. (14)\nThen, after removing signals which are contributed by the static part of the clutter, yavg, from each column of the matrix Y, the remaining signals of the IF matrix, Yremo \u2208 CN\u00d7L, containing the dynamic part of the clutter, e.g., reflection points of the moving target, can be expressed as:\nYremo[:, l] = Y[:, l]\u2212 yavg, l \u2208 {1, \u00b7 \u00b7 \u00b7 , L}. (15)\nCompared to the elimination of the static part of the clutter, the signal-processing procedure for the dynamic clutter is more complicated. To reduce the impact of dynamic clutter on target detection, we develop a new detection method, as detailed in Algorithm 1. Figure 4 presents a schematic diagram for the signal-processing procedure of removing dynamic clutter. First of all, all the reflection points detected via the constant false alarm rate (CFAR) method are considered as potential target reflection points (red circles). Among potential target reflection points, the one with the highest velocity and being closest to the radar is chosen as the anchor point (red solid circle). Algorithm 1 is applied to search all potential reflection points (see the yellow circle) around the anchor point to form an ROI, and other potential target reflection points are removed in the following processes to reduce the impact of dynamic clutter.\nAlgorithm 1 The neighboring-reflection-points detection for reducing dynamic clutter.\nInput: Yremo: the IF matrix after removing the static clutters Output: the index of reflection point Q\n1: Applying the 2D-FFT introduced in Section 3.2 on Yremo to obtain the matrix Y\u0302 2: Utilizing CFAR to calculate the dynamic threshold, \u03c2. Then, save the column vector\n[i; j] into matrix D for Y\u0302(i, j) > \u03c2 3: icentral = min(D(1, :)) 4: for k = 1 : L do 5: if Y\u0302(icentral, k) == max(Y\u0302(icentral, :)) then 6: jcentral = k 7: end if 8: end for 9: Q = [icentral; jcentral]\n10: E = [1, 0,\u22121, 0; 0, 1, 0,\u22121] 11: num1 = 0; num2 = 1 12: repeat 13: for l = num1 + 1 : num2 do 14: for q = 1 : 4 do 15: if Q(:, l) + expand(:, q) \u2208 D && Q(:, l) + E(:, q) 6\u2208 Q then 16: Save [Q(:, l) + E(:, q)] into Q 17: end if 18: end for 19: end for 20: num1 = num2; num2 = size(Q, 2) 21: until num1 == num2"
        },
        {
            "heading": "4.2. Feature Extraction",
            "text": "Upon the identification of the reflection points of the motion, the relative velocities and distances of the target, v1, d1, v2, and d2, can be acquired for the two radars using the traditional FMCW processing methods described in Section 3.2. Additionally, since both radars have multiple antennas, the angle of each reflection point, i.e., \u03b81 and \u03b82 with respect to the two radars, can also be obtained using beamforming techniques, specifically [35]:\nyBF = y\u03a8, (16)\nwhere y \u2208 C1\u00d7M denotes the IF data received at the antenna array and is presented as\ny = [Y1(n, l), Y2(n, l), \u00b7 \u00b7 \u00b7 , YM(n, l)], (17)\nand YM(n, l) denotes the n-th row and l-th column IF data for M-th receiving antennas. In addition, the beam-steering vector with a certain determined angle is used to determine the angle of arrival of the target, and \u03a8 represents the beamforming steering matrix, which is given by:\n\u03a8 = [ \u03c81 \u03c82 \u00b7 \u00b7 \u00b7 \u03c8W ] , (18)\nwhere the beamformer used for the uniform linear array, \u03c8w, can be represented as [35]:\n\u03c8w =  1 e\u2212j\u03c0\u00b7cos[\u2206\u03b8\u00b7(w\u22121)] ...\ne\u2212j\u03c0\u00b7(M\u22121)\u00b7cos[\u2206\u03b8\u00b7(w\u22121)] . (19) In Equation (19), \u2206\u03b8 is the designed angular resolution of receiving beamforming and\nW = \u03c0/\u2206\u03b8 + 1. Then, the angle of interest can be obtained by finding the peak value in yBF; i.e., supposing Wpeak is the angle index of peak value, then the angle of target can be estimated as \u03b8 = (Wpeak \u2212 1)\u00d7 \u2206\u03b8 \u2212 \u03c0/2."
        },
        {
            "heading": "4.3. Motion Velocity Vector Synthesis",
            "text": "With the features extracted from these two radars, i.e., (v1, \u03b81) and (v2, \u03b82), the full velocity of the target can be estimated. Specifically, we consider a Cartesian coordinate system with the horizontal antenna direction of radar 1 as the x-axis. Then, the radial velocity vectors of the target detected at two radars 1 and 2 can be expressed as:\n\u2212\u2192vn1 = \u2212\u2192 PF = (vn1 sin \u03b81, vn1 cos \u03b81), (20)\nand \u2212\u2192vn2 = \u2212\u2192 PE = (vn2 sin \u03b8\u20322, vn2 cos \u03b8 \u2032 2), (21)\nrespectively. It can be noted that the normal directions of the antenna arrays of radar 1 and 2 do not need to be parallel to each other. As illustrated in Figure 5, radar 2 deployed at point B is equivalent to radar 2\u2032 deployed at point C for estimating the same full velocity. Therefore, radar 2 can be equivalently considered as radar 2\u2032, which is deployed in parallel to radar 1, and the relationship between these azimuths are given as follows:\n\u03b8\u20322 = \u03b82 + \u03b3. (22)\nBased on principles presented in Section 3.2, it can be seen that vn1 and vn2 are two projection components of v in two different directions. To obtain the true velocity vector of the target, we utilize the radial velocities, vn1 and vn2, as well as the azimuth angles \u03b81 and \u03b82, detected at different radars to resolve the true velocities of the target projected onto the x-axis and the y-axis, which are given by:{\ny\u2212 vn1 cos \u03b81 = \u2212 tan \u03b81(x\u2212 vn1 sin \u03b81), y\u2212 vn2 cos \u03b8\u20322 = \u2212 tan \u03b82(x\u2212 vn2 sin \u03b8\u20322),\n(23)\nwhere the true velocity projects onto the x-axis, x, and can be expressed as:\nx = vn1 tan \u03b81 sin \u03b81 + vn1 cos \u03b81/ tan \u03b81 \u2212 tan \u03b8\u20322 + vn2 tan \u03b8\u20322 sin \u03b8 \u2032 2 + vn2 cos \u03b8 \u2032 2/ tan \u03b8 \u2032 2 \u2212 tan \u03b81. (24)\nFinally, by substituting Equation (24) into Equation (23), we can obtain the value y, which represents the projection of the true velocity onto the y-axis. The method described above is applied to each frame of the IF data to obtain the velocity of the target in the corresponding frame. After applying this process to \u03ba consecutive frames, a sequence of the velocities, i.e., the motion velocity vector, is extracted and can be presented as:\nV = [\nx1,1 x1,2 \u00b7 \u00b7 \u00b7 x1,\u03ba x2,1 x2,2 \u00b7 \u00b7 \u00b7 x2,\u03ba\n]T . (25)"
        },
        {
            "heading": "4.4. Gesture Classification via LSTM",
            "text": "To classify the gestures based on the motion velocity vector, we adopt the LSTM network, which is widely used in sequential data analysis. As the motion velocity vector contains a sequence of velocities that capture the sequential movement of hand gestures, the LSTM is a natural fit to this task. The LSTM considered in this work contains two LSTM layers and one flatten layer, as illustrated in Figure 6. Instead of making any modifications to the traditional structure of the LSTM network, we directly employ a general LSTM network which only has two LSTM layers. The input features V are passed through the flattening layer to reduce their dimensionality and subsequently subjected to the ReLU activation function."
        },
        {
            "heading": "5. Experiments and Results",
            "text": "In this section, we evaluate the performance of the dual-radar system driven by the information processing procedure detail in Section 4, using real data measured by radars."
        },
        {
            "heading": "5.1. Radar Parameters and Placement",
            "text": "In our experiments, we utilize two TI commercial mmWave FMCW radars for gesture recognition, e.g., one TI MMWCAS-DSP/RF-EVM radar and one TI IWR-6843-AOPEVM radar. The TI IWR-6843-AOP-EVM radar consists of three transmit antennas and four receive antennas and operates between 60 GHz and 64 GHz. The TI MMWCAS-DSP/RF-EVM consists of 12 transmit antennas and 16 receive antennas and operates between 76 GHz and 81 GHz. The deployment of the two mmWave FMCW radars and the environment for collecting the gesture data set are shown in Figure 7. The distance between these two radars is approximately 45 cm, as shown in Figure 7b, and the distance between radars and the hand of the gesture performers is approximately 80 cm, as shown in Figure 7c. With the aim of mitigating the influence of non-target objects on the recognition results, we ensure that the experiment encompassed predominantly wall structures within the background setting. The configuration parameters of radars used in our experiments are presented in Tables 2 and 3."
        },
        {
            "heading": "5.2. Data Set",
            "text": "To evaluate the capability of the proposed framework in gesture recognition, we consider eight different gestures, as shown in Figure 8. The gestures can be classified into three categories, namely the linear gestures, the curved gestures and the compound ones. The linear gestures consist of four different basic gestures, e.g., left to right motion, right to left motion, front to back motion, and back to front motion, and the curved gestures contain those drawing a circle and drawing a semicircle. The compound gestures can be decomposed into a combination of several basic linear and curved gestures. For example, the compound gesture of drawing a \u201cZ\u201d in the plane consists of three basic gestures: left to right \u201c\u2192\u201d, front to back \u201c\u2199\u201d and left to right \u201c\u2192\u201d.\nBased on the gestures defined, a data set with 1600 samples is collected in real experiments. To enhance the diversity of the gesture data set and to better test the robustness of the proposed framework, 10 volunteers (six males and four females) with different heights, weights, and gesture habits were invited to perform the gestures. For each volunteer, each gesture was performed 20 times. The 1600 samples in the data set are divided randomly into two groups, with the training set containing 1280 samples and the test set containing 320 samples. The training set is used to training the LSTM network, while the test set is used to assess the accuracy of gesture recognition."
        },
        {
            "heading": "5.3. Performance Evaluations",
            "text": "In this subsection, we evaluate the performance of our proposed hand gesture recognition framework. For comparisons, we consider two methods as baselines. Specifically, the first baseline adopts the conventional classification method based on the m-D image extracted from the radars and a pre-trained CNN. The m-D image is the input feature and the CNN network acts as a classifier. The second baseline converts the motion velocity vector to an image and then uses a pre-trained CNN network as a classifier. For the first baseline, we consider three types of input feature for a more comprehensive evaluation, including the m-D images obtained from IWR-6843-AOP-EVM (shown in Figure 9a\u2013h), the m-D image obtained from MMWCAS-DSP/RF-EVM (shown in Figure 9i\u2013p) and the m-D images from both radars. For the first two types of input features, a classical CNN architecture with two convolutional layers is adopted. Each layer has 16 convolutional filters and the dimensions of each filter are 3 by 3. For the training of the considered CNN network, the number of epochs is set to 100 and the learning rate is considered as a constant value of 0.0005.\nWhile both the m-D images are adopted as the input features, we train a two-channel CNN network to classify the gestures. For each channel of the CNN network, there are two convolutional layers, each with 32 filters and a kernel size of 5\u00d7 5. The outputs of these two channels of the CNN network are concatenated and flattened, which are then fed into a fully connected layer to produce the gesture label. When training the network, the number of epochs and the learning rate are set to 50 and 0.0002, respectively. For the second baseline, the image input to the CNN is an image converted from the motion velocity vector extracted from the two radars. To convert the images, we regard each column as a vector in Cartesian coordinates and plot them in chronological order. In Figure 10, we present several examples of the converted images. As can be seen from\nthe figure, some gestures cannot be distinguished from the converted images, e.g., the right to left gesture and left to right gesture cannot be accurately distinguished from the generated images since they are almost same. The parameters of the CNN are identical to those used in baseline 1 with a single image as the input, but the CNN was trained using the motion images.\nFor the proposed solution, the size of the input feature is fixed at V into 20\u00d7 2 with \u03ba = 20. The LSTM model has two LSTM layers, and each LSTM layer has 200 hidden units. ReLU is chosen as the activation function. In training process, the batch size is set to 3, while the number of epochs and learning rate are set to 100 and 0.005, respectively. Tables 4\u20137 present the confusion matrix to illustrate the recognition accuracy of the proposed framework and the baselines. As can be seen from comparing Table 4 to Table 5, the gesture recognition accuracy for the MMWCAS-DSP/RF-EVM radar is slightly higher compared to that of the IWR-6843-AOP-EVM radar, as expected, since the former has better signal quality. This is evident in Figure 9a\u2013p, which show that MMWCAS-DSP/RFEVM has lower background noise. However, for both radars, the conventional method using r-D images and CNN cannot offer satisfactory gesture recognition accuracy (from Tables 4 and 5, the accuracies are 66.3% and 71.3%, respectively). Using the m-D images from both radars increases the accuracy to 75.3%. However, this performance is still inadequate for practical applications. As a further enhancement, the second baseline, which utilizes the motion images and the CNN, achieves a significantly higher accuracy than those of baseline 1. As can be seen from Table 7, the gesture recognition accuracy is about 90%. These results show the effectiveness of the motion features extracted from the dual-radar system. The results of our proposed framework are illustrated in Table 8. As can be seen from the table, our proposed framework achieves an accuracy of about 98%, which is significantly higher than the accuracy of all the baselines. To further evaluate the robustness of our proposed gesture recognition framework against diverse gesture patterns, we perform the following experiment. In the experiment, the training set is built by using data sets extracted from eight randomly chosen volunteers, while the test set comes from the other two volunteers. In such circumstances, the LSTM network is trained only with a part of the volunteer data sets. The experimental results presented in Table 9 show a similar overall accuracy to the ones presented in Table 8, e.g., 97.5%.\nTo test the proposed framework against environmental diverseness, we introduce changes in the background environment for gesture recognition, which are demonstrated via a comparison between Figures 7a and 11. In order to keep all other testing conditions unchanged compared to the previous experiment, we ensure that the distance between the two radars remains at 45 cm and the distance between radars and the hand remains at 80 cm. To simulate a practical environment, several chairs are placed close to the performers of the gestures, as shown in Figure 11. Due to the implementation of the time division mul-\ntiplexing mode in radar transmission during the experiment, the resulting beam coverage encompassed all regions within the radars\u2019 FoVs. Consequently, non-target objects such as chairs exerted some influence on the recognition results.\nBy employing the same trained hyper-parameters of the LSTM network, as the previously conducted experiment, the results of which are presented in Table 8, 160 samples for eight gestures are inputted for recognition, and the result for the background-changed comparison is 98.1%, as illustrated in Table 10, which mirrors the gesture recognition performance given in Table 8. These comparison results validate the robustness of our proposed gesture recognition algorithm and demonstrate its resilience towards the changes in the background environment."
        },
        {
            "heading": "6. Discussion and Conclusions",
            "text": "We primarily discussed two potential sources of error in the experiments. Firstly, we control the number of LSTM layers to satisfy the requirement of low complexity, which will influence the accuracy of recognition. The accuracy can be further improved by increasing the number of LSTM layers or adopting other neural networks with higher complexity. Another factor contributing to errors is the resemblance between the gestures due to arbitrary drawing. For example, when volunteers execute the second stage of the gesture of drawing an \u201cL\u201d, in which the hand moves from left to right, arbitrary drawing leads to a short distance of motion for this specific stage. Consequently, this marginal tangential displacement fails to exhibit pronounced Doppler frequency, thereby leading to a final result that resembles the gesture of moving from front to back. Furthermore, there are three potential research directions that can be explored in the future development of this system. Regarding the application scenarios of the system, we hope to apply this system in scenarios with multiple people and hands while further improving the recognition accuracy, thus placing higher demands on the extraction of hand reflection points. Significantly, when dealing with the recognition of two hand gestures, a pressing challenge arises in effectively tracking the reflective points of the hands and synthesizing motion velocity vectors when confronted with overlapping trajectories. In terms of recognizing the types of gestures by the system, our system is currently designed for macro-gestures, and it is hoped that the ability to recognize micro-gestures can be developed by maintaining the utilization of our motion velocity vector algorithm in the recognition process. Given the inherently small magnitude of displacements exhibited by micro-gestures, the extraction of precise velocity and azimuth information is a notable challenge.\nFor the system hardware, when more than two radars are present in the same space, the synthesis algorithm for radar data necessitates addressing several challenges, including, but not limited to, minimizing placement restrictions of the radars, synchronizing and transmitting radar data, and selecting the most appropriate radar data to be utilized. In this work, we have proposed a novel gesture recognition framework based on two mmWave FMCW radars. A novel motion velocity vector synthesis algorithm has been developed to extract the gesture characteristics that are crucial for gesture recognition. The proposed motion velocity synthesis algorithm allows for flexible deployment of the mmWave radars and does not require knowledge of the relative position of the two radars. Based on the motion velocity vector, a gesture recognition model based on an LSTM network has been designed. The numerical results from real measurements demonstrate that the proposed framework can achieve satisfactory hand gesture recognition accuracy, even when there is a change in the environment and the training data and the test data are collected from different volunteers.\nAuthor Contributions: Conceptualization, Y.M., L.Z. and C.L.; methodology, Y.M., L.Z. and C.L.; software, Y.M., L.Z. and C.L.; validation, Y.M., L.Z. and C.L.; formal analysis, Y.M., L.Z. and C.L.; investigation, Y.M., L.Z. and C.L.; resources, Y.M., L.Z. and C.L.; data curation, Y.M., L.Z., C.L. and M.L.; writing\u2014original draft preparation, Y.M.; writing\u2014review and editing, L.Z. and C.L.; visualization, Y.M., L.Z. and C.L.; supervision, L.Z. and C.L.; project administration, Y.M., L.Z., C.L. and M.L.; funding acquisition, L.Z. and C.L. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was supported in part by the National Natural Science Foundation of China under Grant No. 42176190, the Zhejiang Provincial Natural Science Foundation of China under Grant No. LZ22F010001, and the Fundamental Research Funds for the Provincial Universities of Zhejiang under Grant no. GK229909299001-013.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data presented in this study are available upon request from the corresponding author.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Ahmed, S.; Kallu, K.D.; Ahmed, S.; Cho, S.H. Hand gestures recognition using radar sensors for human-computer-interaction: A review. Remote Sens. 2021, 13, 527. [CrossRef] 2. Nigam, S.; Shamoon, M.; Dhasmana, S.; Choudhury, T. A complete study of methodology of hand gesture recognition system\nfor smart homes. In Proceedings of the 2019 International Conference on Contemporary Computing and Informatics (IC3I), Singapore, 12\u201314 December 2019; pp. 289\u2013294.\n3. Chen, T.; Xu, L.; Xu, X.; Zhu, K. Gestonhmd: Enabling gesture-based interaction on low-cost VR head-mounted display. IEEE Trans. Vis. Comput. Graph. 2021, 27, 2597\u20132607. [CrossRef] [PubMed] 4. Yin, T.K.; Manders, C.M.; Farbiz, F.; Hwan, L.M.; Guan, C.G.; Ho, C.Y. Intergrating a gesture interface to a commercial online dance game. In Proceedings of the 2009 IEEE International Conference on Multimedia and Expo, New York, NY, USA, 28 June\u20133 July 2009; pp. 1688\u20131691. 5. Niu, K.; Zhang, F.; Jiang, Y.; Xiong, J.; Lv, Q.; Zeng, Y.; Zhang, D. WiMorse: A contactless morse code text input system using ambient WiFi signals. IEEE Internet Things J. 2019, 6, 9993\u201310008. [CrossRef] 6. Kudrinko, K.; Flavin, E.; Zhu, X.; Li, Q. Wearable sensor-based sign language recognition: A comprehensive review. IEEE Rev. Biomed. Eng. 2020, 14, 82\u201397. [CrossRef] [PubMed] 7. Wu, Y.; Huang, T.S. Vision-based gesture recognition: A review. In International Gesture Workshop; Springer: Berlin/Heidelberg, Germany, 1999; pp. 103\u2013115. 8. Liu, J.; Liu, H.; Chen, Y.; Wang, Y.; Wang, C. Wireless sensing for human activity: A survey. IEEE Commun. Surv. Tutor. 2019, 22, 1629\u20131645. [CrossRef] 9. Lu, Z.; Chen, X.; Li, Q.; Zhang, X.; Zhou, P. A hand gesture recognition framework and wearable gesture-based interaction prototype for mobile devices. IEEE Trans. Hum. Mach. Syst. 2014, 44, 293\u2013299. [CrossRef] 10. Ling, Y.; Chen, X.; Ruan, Y.; Zhang, X.; Chen, X. Comparative study of gesture recognition based on accelerometer and photoplethysmography sensor for gesture interactions in wearable devices. IEEE Sens. J. 2021, 21, 17107\u201317117. [CrossRef]\n11. Tubaiz, N.; Shanableh, T.; Assaleh, K. Glove-based continuous Arabic sign language recognition in user-dependent mode. IEEE Trans. Hum.-Mach. Syst. 2015, 45, 526\u2013533. [CrossRef] 12. Laptev, I.; Marszalek, M.; Schmid, C.; Rozenfeld, B. Learning realistic human actions from movies. In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, AK, USA, 23\u201328 June 2008; pp. 1\u20138. 13. Chen, C.; Liu, K.; Kehtarnavaz, N. Real-time human action recognition based on depth motion maps. J. Real Time Image Process. 2016, 12, 155\u2013163. [CrossRef] 14. Li, Y.; Miao, Q.; Tian, K.; Fan, Y.; Xu, X.; Li, R.; Song, J. Large-scale gesture recognition with a fusion of RGB-D data based on saliency theory and C3D model. IEEE Trans. Circuits Syst. Video Technol. 2017, 28, 2956\u20132964. [CrossRef] 15. Ohn-Bar, E.; Trivedi, M.M. Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations. IEEE trans. Intell. Transp. Syst. 2014, 15, 2368\u20132377. [CrossRef] 16. Wang, Z.r.; Wang, P.; Xing, L.; Mei, L.P.; Zhao, J.; Zhang, T. Leap Motion-based virtual reality training for improving motor functional recovery of upper limbs and neural reorganization in subacute stroke patients. Neural Regen. Res. 2017, 12, 1823. [PubMed] 17. Al Ayubi, S.; Sudiharto, D.W.; Jadied, E.M.; Aryanto, E. The prototype of hand gesture recognition for elderly people to control connected home devices. In Journal of Physics: Conference Series, Proceedings of the International Conference on Electronics Representation and Algorithm (ICERA 2019), Yogyakarta, Indonesia, 29\u201330 January 2019; IOP Publishing: Yogyakarta, Indonesia, 2019; Volume 1201, p. 012042. 18. Niu, K.; Zhang, F.; Wang, X.; Lv, Q.; Luo, H.; Zhang, D. Understanding WiFi signal frequency features for position-independent gesture sensing. IEEE Trans. Mob. Comput. 2021, 21, 4156\u20134171. [CrossRef] 19. He, W.; Wu, K.; Zou, Y.; Ming, Z. WiG: WiFi-based gesture recognition system. In Proceedings of the 2015 24th International Conference on Computer Communication and Networks (ICCCN), Las Vegas, NV, USA, 3\u20136 August 2015; pp. 1\u20137. 20. Hao, Z.; Duan, Y.; Dang, X.; Liu, Y.; Zhang, D. Wi-SL: Contactless fine-grained gesture recognition uses channel state information. Sensors 2020, 20, 4025. [CrossRef] [PubMed] 21. Pu, Q.; Gupta, S.; Gollakota, S.; Patel, S. Whole-home gesture recognition using wireless signals. In Proceedings of the 19th Annual International Conference on Mobile Computing & Networking, Miami, FL, USA, 30 September\u20134 October 2013; pp. 27\u201338. 22. Zhang, D.; Wu, D.; Niu, K.; Wang, X.; Zhang, F.; Yao, J.; Jiang, D.; Qin, F. Practical issues and challenges in CSI-based integrated sensing and communication. In Proceedings of the 2022 IEEE International Conference on Communications Workshops (ICC Workshops), Seoul, Republic of Korea, 16\u201320 May 2022; pp. 836\u2013841. 23. Lien, J.; Gillian, N.; Karagozler, M.E.; Amihood, P.; Schwesig, C.; Olson, E.; Raja, H.; Poupyrev, I. Soli: Ubiquitous gesture sensing with millimeter wave radar. ACM Trans. Graph. (TOG) 2016, 35, 1\u201319. [CrossRef] 24. Malysa, G.; Wang, D.; Netsch, L.; Ali, M. Hidden Markov model-based gesture recognition with FMCW radar. In Proceedings of the 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Washington, DC, USA, 7\u20139 December 2016; pp. 1017\u20131021. 25. Jhaung, Y.C.; Lin, Y.M.; Zha, C.; Leu, J.S.; K\u00f6ppen, M. Implementing a hand gesture recognition system based on Range-Doppler Map. Sensors 2022, 22, 4260. [CrossRef] 26. Li, Y.; Gu, C.; Mao, J. 4-D gesture sensing using reconfigurable virtual array based on a 60-GHz FMCW MIMO radar sensor. IEEE Trans. Microw. Theory. Tech. 2022, 70, 3652\u20133665. [CrossRef] 27. Li, C.; Peng, Z.; Huang, T.Y.; Fan, T.; Wang, F.K.; Horng, T.S.; Munoz-Ferreras, J.M.; Gomez-Garcia, R.; Ran, L.; Lin, J. A review on recent progress of portable short-range noncontact microwave radar systems. IEEE Trans. Microw. Theory Tech. 2017, 65, 1692\u20131706. [CrossRef] 28. Hasch, J.; Topak, E.; Schnabel, R.; Zwick, T.; Weigel, R.; Waldschmidt, C. Millimeter-wave technology for automotive radar sensors in the 77 GHz frequency band. IEEE Trans. Microw. Theory Tech. 2012, 60, 845\u2013860. [CrossRef] 29. Kim, Y.; Toomajian, B. Hand gesture recognition using micro-Doppler signatures with convolutional neural network. IEEE Access 2016, 4, 7125\u20137130. [CrossRef] 30. Ahmed, S.; Kim, W.; Park, J.; Cho, S.H. Radar-based air-writing gesture recognition using a novel multistream CNN Approach. IEEE Internet Things J. 2022, 9, 23869\u201323880. [CrossRef] 31. Xia, Z.; Luomei, Y.; Zhou, C.; Xu, F. Multidimensional feature representation and learning for robust hand-gesture recognition on commercial millimeter-wave radar. IEEE Trans. Geosci. Remote Sens. 2020, 59, 4749\u20134764. [CrossRef] 32. Xia, Z.; Xu, F. Time-space dimension reduction of millimeter-wave radar point-clouds for smart-home hand-gesture recognition. IEEE Sens. J. 2022, 22, 4425\u20134437. [CrossRef] 33. Choi, J.W.; Ryu, S.J.; Kim, J.H. Short-range radar based real-time hand gesture recognition using LSTM encoder. IEEE Access 2019, 7, 33610\u201333618. [CrossRef] 34. Yang, Z.; Zheng, X. Hand gesture recognition based on trajectories features and computation-efficient reused LSTM network. IEEE Sens. J. 2021, 21, 16945\u201316960. [CrossRef] 35. Van Veen, B.D.; Buckley, K.M. Beamforming: A versatile approach to spatial filtering. IEEE ASSP Mag. 1988, 5, 4\u201324. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "A Low-Complexity Hand Gesture Recognition Framework via Dual mmWave FMCW Radar System",
    "year": 2023
}