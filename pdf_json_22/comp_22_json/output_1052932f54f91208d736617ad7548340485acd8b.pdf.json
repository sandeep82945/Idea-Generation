{
    "abstractText": "Abstract \u2014 Perceiving the three-dimensional (3D) structure of the spacecraft is a prerequisite for successfully executing many on-orbit space missions, and it can provide critical input for many downstream vision algorithms. In this paper, we propose to sense the 3D structure of spacecraft using light detection and ranging sensor (LIDAR) and a monocular camera. To this end, Spacecraft Depth Completion Network (SDCNet) is proposed to recover the dense depth map based on gray image and sparse depth map. Specifically, SDCNet decomposes the object-level spacecraft depth completion task into foreground segmentation subtask and foreground depth completion subtask, which segments the spacecraft region first and then performs depth completion on the segmented foreground area. In this way, the background interference to foreground spacecraft depth completion is effectively avoided. Moreover, an attention-based feature fusion module is also proposed to aggregate the complementary information between different inputs, which deduces the correlation between different features along the channel and the spatial dimension sequentially. Besides, four metrics are also proposed to evaluate object-level depth completion performance, which can more intuitively reflect the quality of spacecraft depth completion results. Finally, a large-scale satellite depth completion dataset is constructed for training and testing spacecraft depth completion algorithms. Empirical experiments on the dataset demonstrate the effectiveness of the proposed SDCNet, which achieves 0.25m mean absolute error of interest and 0.759m mean absolute truncation error, surpassing state-of-the-art methods by a large margin. The spacecraft pose estimation experiment is also conducted based on the depth completion results, and the experimental results indicate that the predicted dense depth map could meet the needs of downstream vision tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiang Liu"
        },
        {
            "affiliations": [],
            "name": "Hongyuan Wang"
        },
        {
            "affiliations": [],
            "name": "Zhiqiang Yan"
        },
        {
            "affiliations": [],
            "name": "Yu Chen"
        },
        {
            "affiliations": [],
            "name": "Xinlong Chen"
        },
        {
            "affiliations": [],
            "name": "Weichun Chen"
        }
    ],
    "id": "SP:506b439a560300496d1d89691b6b1c2ce6db012c",
    "references": [
        {
            "authors": [
                "J J. Ventura"
            ],
            "title": "Pose tracking of a noncooperative spacecraft during docking maneuvers using a time-of-flight sensor",
            "venue": "AIAA Guidance, Navigation, and Control Conference, California, USA, 2016, pp. 0875, doi: 10.2514/6.2016-0875.",
            "year": 2016
        },
        {
            "authors": [
                "X. Liu"
            ],
            "title": "Position Awareness Network for Non-Cooperative Spacecraft Pose Estimation Based on Point Cloud,",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems, early access,",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wei"
            ],
            "title": "Robust spacecraft component detection in point clouds,",
            "venue": "Sensors, vol. 18,",
            "year": 2018
        },
        {
            "authors": [
                "R. Volpe"
            ],
            "title": "Reconstruction of the Shape of a Tumbling Target from a Chaser in Close Orbit",
            "venue": "IEEE Aerospace Conference, MT, USA, 2020, pp. 1-11, doi: 10.1109/AERO47225.2020.9172529.",
            "year": 2020
        },
        {
            "authors": [
                "W. Xu"
            ],
            "title": "A pose measurement method of a non-cooperative GEO spacecraft based on stereo vision",
            "venue": "International Conference on Control Automation Robotics & Vision, Guangzhou, China, 2012, pp. 966-971, doi: 10.1109/ICARCV.2012.6485288.",
            "year": 2012
        },
        {
            "authors": [
                "H.G. Mart\u00ednez"
            ],
            "title": "Pose estimation and tracking of non-cooperative rocket bodies using time-of-flight cameras",
            "venue": "Acta Astronautica, vol. 139, pp. 165-175, Oct. 2017, doi: 10.1016/j.actaastro.2017.07.002.",
            "year": 2017
        },
        {
            "authors": [
                "F. Ma"
            ],
            "title": "Sparse-to-dense: Depth prediction from sparse depth samples and a single image",
            "venue": "IEEE international conference on robotics and automation, Brisbane, Australia, 2018, pp. 4796-4803, doi: 10.1109/ICRA.2018.8460184.",
            "year": 2018
        },
        {
            "authors": [
                "S. Imran"
            ],
            "title": "Depth coefficients for depth completion",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, USA, 2019, pp. 12438-12447, doi: 10.1109/CVPR.2019.01273.",
            "year": 2019
        },
        {
            "authors": [
                "F. Ma"
            ],
            "title": "Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera",
            "venue": "IEEE international conference on robotics and automation, Montreal, Canada, 2019, pp. 3288-3295, doi: 10.1109/ICRA.2019.8793637.",
            "year": 2019
        },
        {
            "authors": [
                "B.U. Lee"
            ],
            "title": "Depth completion using plane-residual representation",
            "venue": "IEEE IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, USA, 2021, pp. 13911-13920, doi: 10.1109/CVPR46437.2021.01370.",
            "year": 2021
        },
        {
            "authors": [
                "P P. Hambarde"
            ],
            "title": "S2DNet: Depth estimation from single image and sparse samples",
            "venue": "IEEE Transactions on Computational Imaging, vol. 6, pp. 806-817, Mar. 2020, doi: 10.1109/TCI.2020.2981761.",
            "year": 2020
        },
        {
            "authors": [
                "L. Liu"
            ],
            "title": "Learning steering kernels for guided depth completion",
            "venue": "IEEE Transactions on Image Processing, vol. 30, pp. 2850-2861, Feb. 2021, doi: 10.1109/TIP.2021.3055629.",
            "year": 2021
        },
        {
            "authors": [
                "M. Hu"
            ],
            "title": "Penet: Towards precise and efficient image guided depth completion",
            "venue": "IEEE international conference on robotics and automation, Xi'an, China, 2021, pp. 13656- 13662, doi: 10.1109/ICRA48506.2021.9561035.",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu"
            ],
            "title": "Learning affinity via spatial propagation networks",
            "venue": "Proccedings of Neural Information Processing Systems, Long Beach, USA, 2017, pp. 1519-1529, doi: 10.48550/arXiv.1710.01020.",
            "year": 2017
        },
        {
            "authors": [
                "X. Cheng"
            ],
            "title": "Learning depth with convolutional spatial propagation network,",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "X. Cheng"
            ],
            "title": "Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, New York, USA, 2020, pp. 10615-10622, doi: 10.1609/aaai.v34i07.6635.",
            "year": 2020
        },
        {
            "authors": [
                "J. Park"
            ],
            "title": "Non-local spatial propagation network for depth completion",
            "venue": "Proceedings of the European conference on computer vision, 22020, pp. 120-136, doi: 10.1007/978-3- 030-58601-0_8.",
            "year": 1007
        },
        {
            "authors": [
                "Y. Lin"
            ],
            "title": "Dynamic spatial propagation network for depth completion,",
            "year": 2022
        },
        {
            "authors": [
                "K. He"
            ],
            "title": "Guided image filtering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 6, pp. 1397-1409, Oct. 2012, doi: 10.1109/TPAMI.2012.213.",
            "year": 2012
        },
        {
            "authors": [
                "J. Tang"
            ],
            "title": "Learning guided convolutional network for depth completion",
            "venue": "IEEE Transactions on Image Processing, vol. 30, pp. 1116-1129, Dec. 2020, doi: 10.1109/TIP.2020.3040528.",
            "year": 2020
        },
        {
            "authors": [
                "L. Liu"
            ],
            "title": "Fcfr-net: Feature fusion based coarse-to-fine residual learning for depth completion",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 2136-2144, doi: 10.48550/arXiv.2012.08270.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yan"
            ],
            "title": "RigNet: Repetitive image guided network for depth completion",
            "venue": "2021, arXiv:2107.13802.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen"
            ],
            "title": "Learning joint 2d-3d representations for depth completion",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 2019, pp. 10023-10032, doi: 10.1109/ICCV.2019.01012.",
            "year": 2019
        },
        {
            "authors": [
                "J. Uhrig"
            ],
            "title": "Sparsity invariant cnns",
            "venue": "Proceedings of the international conference on 3D Vision, Qingdao, China, 2017, pp. 11-20, doi: 10.1109/3DV.2017.00012.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, QC, Canada, 2021, pp. 9992-10002, doi: 10.1109/ICCV48922.2021.00986.",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani"
            ],
            "title": "Attention is all you need,",
            "venue": "Proccedings of Neural Information Processing Systems, Long Beach, USA,",
            "year": 2017
        },
        {
            "authors": [
                "S. Woo"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "Proceedings of the European conference on computer vision, Munich, Germany, 2018, pp. 3-19, doi: 10.48550/arXiv.1807.06521.",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "2014, arXiv:1412.6980.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "prerequisite for successfully executing many on-orbit space missions, and it can provide critical input for many downstream vision algorithms. In this paper, we propose to sense the 3D structure of spacecraft using light detection and ranging sensor (LIDAR) and a monocular camera. To this end, Spacecraft Depth Completion Network (SDCNet) is proposed to recover the dense depth map based on gray image and sparse depth map. Specifically, SDCNet decomposes the object-level spacecraft depth completion task into foreground segmentation subtask and foreground depth completion subtask, which segments the spacecraft region first and then performs depth completion on the segmented foreground area. In this way, the background interference to foreground spacecraft depth completion is effectively avoided. Moreover, an attention-based feature fusion module is also proposed to aggregate the complementary information between different inputs, which deduces the correlation between different features along the channel and the spatial dimension sequentially. Besides, four metrics are also proposed to evaluate object-level depth completion performance, which can more intuitively reflect the quality of spacecraft depth completion results. Finally, a large-scale satellite depth completion dataset is constructed for training and testing spacecraft depth completion algorithms. Empirical experiments on the dataset demonstrate the effectiveness of the proposed SDCNet, which achieves 0.25m mean absolute error of interest and 0.759m mean absolute truncation error, surpassing state-of-the-art methods by a large margin. The spacecraft pose estimation experiment is also conducted based on the depth completion results, and the experimental results indicate that the predicted dense depth map could meet the needs of downstream vision tasks.\nIndex Terms\u2014Spacecraft depth completion, 3D structure recovery, Multi-source feature\nfusion, Satellite dataset, Deep learning\n Corresponding author: Hongyuan Wang. 1 Xiang Liu, Hongyuan Wang, Zhiqiang Yan and Yu Chen are with the Research Center for Space Optical Engineering, institute of aeronautics, Harbin Institute of Technology, 150001 Harbin, China (e-mail: 19B921006@stu.hit.edu.cn; fountainhy@hit.edu.cn; 18B921006@stu.hit.edu.cn; 21S021008@stu.hit.edu.cn). Xinlong Chen and Weichun Chen are with Qian Xuesen Laboratory of Space Technology, China Academy of Space Technology, 100080, Beijing, China (e-mail: chenxinlong@qxslab.cn; chenweichun@qxslab.cn).\nI. INTRODUCTION\nWith the rapid development of aerospace technology, numerous on-orbit missions oriented\nto non-cooperative spacecraft have emerged. Among them, perceiving the three-dimensional structure of spacecraft and providing it to downstream vision algorithms (such as pose estimation [1]-[2], component detection [3], 3D reconstruction [4], etc.) are vital to ensure the successful execution of these tasks.\nAt present, stereo vision systems [5] and active time-of-flight (TOF) cameras [6] are the\nprimary options for perceiving the three-dimension structure of non-cooperative spacecraft. Unfortunately, limited by the installation baseline length and power consumption, both of them work at close distances (generally less than 20m), which brings great challenges to space onorbit tasks. Moreover, stereo vision systems generally work poorly on objects with smooth surfaces or repetitive textures due to their reliance on the quality of extracted feature points. Inspired by autonomous driving technology, this paper attempts to sense the three-dimensional structure of spacecraft at a long distance (maximum to 250m) using light detection and ranging sensor (LIDAR) and monocular camera. To this end, we propose a depth completion algorithm to recover the three-dimensional structure of spacecraft using a gray image and sparse depth map.\n(a)\n(b)\nThe depth completion task has been attracting considerable research due to its importance\nin various fields, and numerous depth completion methods for ground scenes have been proposed. Nevertheless, due to the distinct working scenarios and data characteristics, the current depth completion methods oriented to ground scenarios can\u2019t be directly applied to spacecraft depth completion. Fig.1 shows the difference between autonomous driving scene data and space on-orbit mission data. The differences in data collected from different scenarios can be embodied in the following aspects: 1) The data from ground scenarios contains rich background information. On the contrary, the on-orbit data mainly comprises the spacecraft and the simple celestial background. To some extent, the spacecraft depth completion task can be regarded as object-level depth completion. 2) Due to the different working distances, the depth map of spacecraft obtained by LIDAR is more sparse than the depth map of ground scenarios, bringing significant challenges to spacecraft depth completion. 3) Compared with the ground scene, the on-orbit lighting conditions are more complex, and some areas of the spacecraft are invisible due to lighting shadows.\nTo alleviate the problem, we propose a Spacecraft Depth Completion Network (SDCNet)\nfor the three-dimensional structure recovery of spacecraft using a gray image and sparse depth map. Specifically, we decompose the object-level depth completion task for spacecraft into two\nsubtasks: foreground segmentation subtask and foreground depth completion subtask, avoiding the interference of the starry background to the foreground satellite depth completion. Moreover, a multi-source feature fusion module is proposed to integrate the geometric features and context the gray image provides into the depth map feature, providing essential information guidance for spacecraft depth completion. The main contributions of this paper are as follows:\n(1) A novel spacecraft depth completion framework is proposed to recover spacecraft\ndense depth, which decomposes the object-level depth completion into foreground segmentation and foreground depth completion, avoiding the interference of the starry background to the satellite depth completion.\n(2) An attention-based feature fusion module is proposed for feature aggregation from\ndifferent sources, which infers the cross-channel and spatial attention maps between features and integrates the gray image's geometric features and context information into the depth map feature.\n(3) Four metrics are proposed to evaluate the quality of object-level depth completion\nresults. MAEI and RMSEI calculate the depth error in the preserved foreground region, which more intuitively reflects the depth prediction accuracy of the object itself. MATE and RMSTE use truncation error to evaluate the depth error in the predicted foreground region, comprehensively evaluating the quality of depth completion results.\n(4) We construct a satellite depth completion dataset based on 126 CAD models for\ntraining and testing spacecraft depth completion algorithms, which can promote the development of the spacecraft depth completion field.\nThe rest of the paper is organized as follows. Section II provides an overview of various\ndepth completion methods. Section III elaborates on the theories and methods of the proposed spacecraft depth completion algorithm. Then, the construction method of the satellite depth completion is introduced in detail in Section IV. Experimental results in different settings and\na comparison with other methods are presented in Section V. Finally, we conclude the article in Section VI."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "Over the last decades, numerous depth completion algorithms with the guidance of optical\nimages have been proposed due to the importance of depth completion in various applications, which can be roughly divided into two categories: the early fusion model and the late fusion model.\nThe early fusion model generally takes the concatenation of RGB-D as input and predicts\ndense depth through a U-Net-alike structure. For instance, Sparse-to-dense [7] concatenates the RGB images and sparse depth and adopts the encoder-decoder structure to regress depth for each pixel. Several methods [8]-[10] also extract low-level features from the optical image and depth map separately and concatenate them at the first layer of the encoder-decoder network. Moreover, the coarse-to-fine strategy is frequently adopted to generate more accurate depth estimation results [11]-[13]. S2DNet [11] utilizes the Sparse-to-dense [7] to predict the coarselevel depth map first. The estimated coarse depth map is then concatenated with the input image and fed into the fine network for fine-level depth map estimation. Many methods [13]-[18] also adopt the spatial diffusion process to refine the coarse depth map and achieve promising results. These methods utilize the encoder-decoder structure to simultaneously predict a blur depth map and the affinity matrix. Then the spatial diffusion process is used to generate a final refined depth map according to the predicted affinity matrix.\nThe late fusion model usually utilizes two branches to extract features from the optical\nimage and depth map, respectively, and then perform feature fusion at intermediate layers. Inspired by guided image filtering [19], GuideNet [20] introduces the guided convolution module, which generates data-driven spatially-variant kernels from the RGB image features to integrate the edge information in RGB into depth map features. FCFRNet [21] utilizes the\nchannel shuffle and the energy-based fusion operation to mix two features extracted from different inputs. On the basis of GuideNet [20], RigNet [22] adopts the repetitive hourglass network to generate more clear image guidance. Moreover, the repetitive guidance module is also proposed to progressively generate structure-detailed depth map features. FuseNet [23] replaces the 2D convolution in the depth map branch with 3D continuous convolution to extract 3D features of the depth map. The 3D features are then projected into image space to form a sparse depth feature map. The sparse depth feature map is finally fused with the RGB feature map for the dense depth prediction.\nAlthough numerous depth completion algorithms oriented to the ground scene have been\nproposed, these methods perform poorly in the spacecraft depth completion task due to the particularity of the space on-orbit environment. Therefore, according to the characteristics of on-orbit data, a novel Spacecraft Depth Completion Network (SDCNet) is proposed for spacecraft depth completion. Specifically, SDCNet decomposes the spacecraft depth completion into the foreground segmentation subnet and foreground depth completion subnet, avoiding the interference of the starry background to the foreground spacecraft depth completion. Moreover, a novel attention-based feature fusion module is introduced to integrate the geometric features and context in the gray image into the depth map features, providing essential information guidance for spacecraft depth completion. Finally, four new metrics are proposed to evaluate the quality of object-level spacecraft depth completion results."
        },
        {
            "heading": "III. SPACECRAFT DEPTH COMPLETION METHOD",
            "text": "The overall network architecture of our proposed Spacecraft Depth Completion Network\n(SDCNet) is illustrated in Fig. 2. The SDCNet comprises a foreground segmentation subnet (FSNet) and foreground depth completion subnet (FDCNet). The FSNet predicts the probability that each pixel belongs to the spacecraft and segments the foreground area for subsequent spacecraft depth completion. Then the FDCNet regresses the depth of the segmented\nforeground area by fusing gray image and sparse depth image information.\nA. Foreground Segmentation Subnet\nConsidering the on-orbit data are generally composed of spacecraft and simple celestial\nbackground, the spacecraft depth completion task can be equivalent to the object-level depth completion task. At this time, if regressing the depth of all pixels directly, the imbalance of the number of foreground and background samples will inevitably degrade network performance. Moreover, the network will also tend to learn the weight minimizing the overall pixel depth error (including the foreground and background) instead of minimizing the spacecraft depth error. To avoid these problems mentioned above, we design a simple foreground segmentation network (FSNet) to segment the foreground region, which is performed on depth regression subsequently. The detailed structure parameter of the FSNet is shown in Fig. 3.\nSince the foreground segmentation subtask belongs to the pixel\u2019s binary classification\nproblem, FSNet adopts the encoder-decoder network structure to predict the probability that each pixel belongs to the foreground. Moreover, to fully utilize the complementary information of the gray image and sparse depth map, FSNet takes their concatenation as input. Finally, the designed FSNet only contains 8 convolutional layers and 0.005M learnable parameters, which can filter out the interference of background on the subsequent spacecraft depth regression while keeping the overhead small.\nB. Foreground Depth Completion Subnet\nAfter segmenting the foreground region, the foreground depth completion subnet (FDCNet)\nis designed to regress the pixel depth in the segmented foreground region. As shown in Fig. 2, FDCNet is composed of the gray image feature extraction branch and the sparse depth map completion branch, both of which adopt the encoder-decoder structure. The gray image feature extraction branch aims to extract the geometric structure and context information from the gray image, providing critical cues for the subsequent spacecraft depth prediction. At the same time, the sparse depth map completion branch predicts the pixel depth utilizing the features extracted from the gray image and the depth map. Considering the adverse effect of sparse data on convolution operations [24], we adopt simple morphological operations to preprocess the sparse depth map, generating coarse pseudo-dense depth images and feeding the result into the sparse depth map completion branch.\nIn the process of predict the dense depth, how effectively integrating the geometric\nstructure and context information in the gray image into the depth map features plays a crucial role in the depth completion task. To this end, we propose a novel attention-based feature fusion module for aggregating the features extracted from the gray image and the depth map. Fig. 4 shows the specific structure of the feature fusion module, which can be divided into two stages: the cross-channel fusion stage and the spatial fusion stage. Specifically, the cross-channel fusion stage deduces the correlation between different features along the channel dimension. In contrast, the spatial fusion stage infers which feature is more informative along the spatial dimension.\nIn the cross-channel fusion stage, the first step is to embed the feature map along each\nchannel into a feature vector. Intuitively, we can directly expand the feature map along the row or the column. However, the dimension of the feature vector generated in this way is high, inevitably resulting in a large computational overhead. To avoid this problem, inspired by the Swin Transformer [25], we decompose the feature map along each channel into M nonoverlapping regions with size S\u00d7S. The region's features under different channels are extracted to generate the channel feature vectors of feature maps. The detailed operation of feature embedding is shown in Fig. 5.\nAs shown in Fig. 5, the max-pooling and average-pooling operations are used to extract\nthe global feature for each region. Moreover, the intra-region pixel adaptive weighting operation is also used to characterize the detail features. Finally, the extracted region features under the same channel are spliced to form the channel feature vector. Supposing the original\nfeature map size is C\u00d7H\u00d7W, the dimension of the final embedded feature vector is kC d ,\nwhere 23kd HW S .\nAfter embedding the feature maps into feature vectors along the channel dimension, the\nmulti-head co-attention mechanism is introduced to deduce the correlation between different features along the channel dimension. Assuming the feature maps of the gray image and depth map extracted by the convolutional neural network are C H W\ng\n f and C H W\ns\n f ,\nrespectively, k C dv\ng\n f and k C dv\ns  f are the embedded feature vector of gf and sf ,\nrespectively. The query of v sf and the key of v gf can be calculated as\n    , 1, , 1,\nv Q\ni s i\nv K\ni g i\ni n i n    \n \nQ f W K f W (1)\nwhere  k kd d nQ\ni\n W and\n k kd d nK i  W are the parameter matrices,\n kC d n i  Q and\n kC d n i  K are the query and the key of\nv sf and v gf , respectively. n is the number of\nattention heads, and i is the index of attention heads. Then the cross-channel attention map\nC C\ni\n\u03c9 between different features can be calculated as\nT\nsoftmax i ii kd\n    \n   \nQ K \u03c9 (2)\nThe features of the gray image are then integrated into the depth map features according\nto the cross-channel attention, which can be expressed as\n reshape v vi s i g f F \u03c9F (3)\nwhere  reshape  denotes vector reshape operation,  C H Wv s   F and\n C H Wv g   F are\nvectorized versions of sf and gf , respectively. C H W\ni\n f represents the single-head\nfused feature, aggregating the grayscale and depth image information. Similar to Transformer [26], the multi-head attention mechanism is adopted to generate advanced fused features, which perform the single-head function in parallel. The inter-head fused features adaptive weighting is then adopted to enhance the representation ability of the fused features. The multi-head attention mechanism can be expressed as\n 1 2Conv ; ; ;s n   f f f f (4)\nwhere n denotes the number of attention heads, and  ;  means concatenate operation. In\nthis work, we set n to 4.\nThe cross-channel attention layer can fully mine the correlation between heterologous\nfeature maps along channel dimensions. However, it ignores the different information richness of the gray image features and the depth map features at different spatial locations. For instance, affected by lighting conditions, the shadow region in the gray image may fail to perceive spacecraft, while the LIDAR can still provide accurate ranging information in this region. On the other hand, in the regions with satisfactory lighting conditions, due to the sparseness of LIDAR ranging information, LIDAR loses the specific edge information. At this time, the gray image can provide clear edge features to guide spacecraft structure recovery. Therefore, the spatial attention layer is designed to retain more informative features at different spatial locations, which is complementary to the cross-channel attention module.\nAs shown in Fig. 4, the spatial attention layer takes the feature C H W s  f output from\nthe cross-channel attention layer and the gray image feature C H W\ng\n f as input and outputs\nthe final fused feature C H W\nout\n f for subsequent depth prediction. Like CBAM [27], we\nfirst apply max-pooling and average-pooling operations along the channel axis on sf and gf and concatenate them to generate an efficient descriptor. Then a convolution layer and the sigmoid function are applied to generate the spatial attention map 1 H W\nspatial\n W , which\nencodes the richness of information at each location in the depth map. The final fused feature\nis the spatial-variant weighting of sf and gf , which can be expressed as\n 1out spatial s spatial g  f W f W f (5)\nwhere denotes element-wise multiplication. During multiplication, the spatial attention\nmap spatialW is broadcasted along the channel dimension.\nC. Loss Functions\nThe loss functions of SDCNet consist of two parts: the foreground prediction loss for\nFSNet and the foreground depth completion loss for FDCNet. For the foreground prediction loss, the binary cross-entropy loss is adopted to supervise the foreground region segmentation, which can be expressed as\n       1\n\u02c6 \u02c6log 1 log 1FSNet i i i i i y y y y N      (6)\nwhere iy and \u02c6iy are the foreground ground truth label and predicted foreground probability, respectively. If the pixel belongs to the foreground region, its ground truth label is set to 1. Otherwise, it is set to 0.\nThe foreground depth completion loss aims to minimize the error between the network\npredicted depth and the ground truth depth in the predicted foreground region, which can be calculated as\n  \u02c6 1 \u02c6 \u02c6 i FDCNet i i x A d d N A    (7)\nwhere A\u0302 represents the set of pixels predicted as foreground pixels in FSNet.  \u02c6N A denotes\nthe number of elements in set A\u0302 , \u02c6 id and id denote the predicted depth and ground truth\ndepth for pixel ix , respectively.\nD. Evaluation Metrics\nFor the foreground segmentation subtask, we use Intersection over Union (IOU) and\nIntersection over Interest (IOI) to evaluate the performance of FSNet, which can be calculated as\n   \n\u02c6 \u02c6 N A A IOU\nN A A  (8)\n    \u02c6N A A IOI N A  (9)\nwhere A\u0302 denotes the set of pixels predicted as foreground pixels in FSNet, A denotes the set of pixels belonging to the foreground in the ground truth label.  N  is the function that\ncounts the number of elements in the set. The IOI is mainly used to evaluate the retention rate of the ground truth foreground. The larger the IOI, the fewer foreground pixels are misclassified as background, and the more complete the retained foreground information is.\nIn the depth completion task, the mean absolute error (MAE) and the root mean squared\nerror (RMSE) are often used to evaluate the performance of depth completion. However, since MAE and RMSE are the statistics of all pixel depth prediction errors, the results are easily unstable in the object-level depth completion task due to the change in the ratio of the foreground and background. Therefore, four new metrics to evaluate the quality of object-level depth completion results are proposed, including the mean absolute error of interest (MAEI), the mean absolute truncation error (MATE), the root mean square error of interest (RMSEI), and the root mean square truncation error (RMSTE), which can be calculated as\n  \u02c6 1 \u02c6MAEI \u02c6 i i i x A A d d N A A    (10)\n  2 2\u02c6 1 \u02c6RMSEI \u02c6 i i i x A A d d N A A    (11)\n      \u02c6\n1 \u02c6MATE min , \u02c6\ni\ni i\nx A d d N A      (12)\n    2\n2\n2\u02c6\n1 \u02c6RMSTE min , \u02c6\ni\ni i\nx A\nd d N A           (13)\nwhere A\u0302 and A denote the set of pixels predicted as foreground pixels and the set of pixels belonging to the foreground in the ground truth, respectively. \u02c6 id and id denote the predicted\ndepth and ground truth depth for pixel ix .  N  is the function that counts the number of elements in the set, and the parameter  is the truncation threshold.\nMAEI, RMSEI, MATE, and RMSTE essentially calculate MAE and RMSE in different\nimage areas. MAEI and RMSEI calculate MAE and RMSE in the intersection area of the predicted foreground and the ground truth foreground, which can more intuitively reflect the depth completion accuracy of the spacecraft itself. On the other hand, MATE and RMSTE calculate MAE and RMSE in the predicted foreground area. Considering the depth error of the region misclassified as the foreground is very large and depends on the object's distance, threshold truncation is performed to prevent it from causing excessive influence on MAE and RMSE. In our work, we set the truncation threshold  to 10m."
        },
        {
            "heading": "IV. SATELLITE DEPTH COMPLETION DATASET CONSTRUCTION",
            "text": "Training a deep network for spacecraft depth completion requires an extensive collection\nof satellite images and LIDAR data. To date, there is no public dataset for spacecraft depth completion tasks. This paper constructs a large-scale satellite depth completion dataset for training and testing spacecraft depth completion algorithms.\nWe performed the imaging simulation of spacecraft using blender software based on 126\nsatellite CAD models. The specific camera and LIDAR parameters are shown in Table 1 and Table 2. Specifically, the LIDAR parameters are derived from the JAGUAR product of Innovusion company. During the simulation, we randomly set the satellite solar plane size to 3- 8m and the body size to 1-3m. The satellite's distance to the observation platform is set in the range of 50-250m, and the satellite's attitude relative to the observation platform is random. The maximum angle between the illumination and observation angles is set to 70\u00b0.\nresulting in 8064 sets of gray images and LIDAR depth maps. Fig. 6 shows some simulation results of different satellite models. To ensure the generalization performance of the algorithm to different satellite models, we divided the simulation data of 126 satellite models into three subsets: training set (simulation results of 99 models), validation set (simulation results of 9 models), and test set (simulation results of 18 models), resulting in 6336, 576 and 1152 sets of data for training, validation, and testing. In this way, we can ensure that the data used for validation and testing is invisible to the depth completion network, putting forward high requirements for the applicability of the algorithm to unknown satellite models.\nV. EXPERIMENTS A. Experiment Setup\nThe proposed SDCNet is implemented in python using the Paddle library and trained on\nan Nvidia Tesla V100 GPU. During training, we first train the FSNet using the standard binary cross-entropy loss, then freeze the weights in FSNet and train the whole network. We randomly crop or resize images to 512\u00d7512 to train effectively with limited computational resources. We train the FSNet and the whole model using the Adam [28] optimizer for 30 and 50 epochs, with an initial learning rate of 0.001 and a weight decay of 0.001. In addition, data augmentation techniques, including random flip and image jitter, are adopted.\nB. Experiment Results\nWe compare our method with state-of-the-art depth-completion methods on the\nconstructed satellite depth-completion dataset, including Sparse-to-dense [7], CSPN [15], GuideNet [20], FCFRNet [21], RigNet [22], PENet [13], and DySPN [18]. All the methods are trained on the same training set and evaluated on the same test set. It should be noted that since the spacecraft LIDAR depth maps are more sparse compared to ground scenes, most of the existing depth completion methods fail to predict reasonable dense depth results under the input of the original sparse depth map. To this end, on the basis of the original method, we additionally adopt the morphological preprocessing operation on the sparse depth map to generate the pseudo-dense depth map and feed it into the depth completion network. Table 3 The quantitative results of different depth completion methods. All the results tested on existing methods are obtained by adding the morphological preprocessing operation based on the original method to ensure reasonable completion results. Bold values represent the optimal values of different evaluation criteria among all methods.\nto the state-of-the-art depth-completion methods, the SDCNet proposed in this paper outperforms all other methods in all indicators. Fig. 7 shows some qualitative depth completion examples of SDCNet. The first and second rows in Fig. 7 are the gray images and the sparse depth maps, respectively. They are the input of the depth completion network. The third row in Fig. 7 shows the foreground segmentation results predicted by FSNet. It can be seen that thanks to the complementary information of gray images and depth maps, the designed lightweight foreground segmentation can accurately segment the foreground region even under unfavorable illumination conditions. The fourth and fifth rows in Fig. 7 are the ground truth dense depth\nmap and the predicted dense depth map, respectively. The last row in Fig. 7 also shows the point cloud generated from the predicted depth map for better visualization. It can be seen that the generated point cloud can accurately recover the three-dimensional structure of the spacecraft, which verifies the effectiveness of the proposed method in this paper.\nIn this section, we first conduct additional experiments to explore how different input\nchoices affect the performance of the FSNet. Moreover, ablation studies are also conducted to verify the effectiveness of each component proposed in our method, including the foreground prediction network (FSNet), the cross-channel attention layer (CCA), and the spatial attention layer (SA).\n1. Effects of different inputs to FSNet\nIn order to analyze the influence of different inputs on the performance of the foreground\nprediction network, we train the FSNet with different inputs, and the quantitative experimental results are listed in Table 4.\nFSNet performs worst, and the IOU and IOI of the prediction results are only 86.73% and 90.8%. That is because the on-orbit lighting condition is complex, and the regions in the shadow are generally invisible to the gray images. When the FSNet solely takes the sparse depth map as input, the IOU and IOI increase to 86.89% and 93.22%, respectively. That is because the\nLIDAR is not sensitive to illumination, and the LIDAR can still provide ranging information even if the illumination conditions are poor. However, the LIDAR can not provide the edge structure information due to its sparse ranging results, resulting in a 2.42% increase in IOI while almost no improvement in IOU. The final version takes the gray image and the sparse depth map jointly into the FSNet, and prediction results are significantly improved, with the IOU and IOI reaching 95.3% and 97.44%, respectively. On the one hand, the depth map can provide distance information for the areas where the gray image features are annihilated. On the other hand, the gray image can provide clear edge structure information in satisfactory lighting conditions. In this way, FSNet can take full advantage of the complementary information of different sensor data and accurately classify whether a pixel belongs to the foreground under different lighting conditions.\n2. Effectiveness of different component\nWe choose the state-of-the-art depth completion network GuideNet [20] as the baseline,\nwhich has the same structure as FDCNet except employs the guided convolution module to fuse the features extracted from the optical image and the depth map. On this basis, the FSNet is first introduced to enable the network to focus on recovering the foreground region depth (the second version in Table 5). It can be seen that the introduction of the FSNet significantly improves depth completion accuracy. This phenomenon reflects that the background pixels will interfere with the depth recovery of the object itself and degrade the network performance. At the same time, it verifies the effectiveness of decomposing the object-level depth completion task into the foreground segmentation subtask and foreground depth completion subtask. Further replacing the guided convolution module with the proposed cross-channel attention layer (the third version in Table 5), the depth prediction error MAEI and MATE are further reduced by 3.7cm and 5cm, respectively. Similarly, replacing the guided convolution module with the spatial attention layer can also decrease the depth completion error (the fourth version\nin Table 5). The final version (the fifth version in Table 5), which successively uses the crosschannel and spatial attention layers to aggregate features of different inputs, achieves the best performance, verifying the effectiveness of the multi-source feature fusion module.\nD. Application for Pose Estimation\nTo evaluate the predicted depth quality and explore the feasibility of utilizing the predicted\ndense depth map in downstream vision tasks, we conduct the spacecraft pose estimation experiment based on the results of SDCNet. Specifically, we simulated a sequence of data containing 36 frames for each satellite in the test set, and the Euler angle and position along each axis are randomly increased within [5\u00b0,10\u00b0] and [1m,2m] for each adjacent frame. The simulated data is then fed into SDCNet to obtain the predicted dense map, which is used as the input for the pose estimation experiments.\nWe choose the state-of-the-art spacecraft pose estimation method PANet[2] as our pose\nestimation method in this experiment. Since PANet requires the point clouds of spacecraft as input, we convert the predicted depth map into the point cloud according to the camera parameters, and the statistical outlier removal operation is adopted to improve point cloud quality. Fig. 8 shows several pose estimation visualization examples on the testing point cloud. It can be seen that the transformed source point cloud (the point clouds in red) aligns well with the target point cloud (the point clouds in blue), which implies the pose estimation results are with high accuracy. According to statistics, the average three-axis rotation and translation errors are 0.85\u00b0 and 1.47m, respectively, which verified that the predicted dense depth could be applied in downstream vision tasks.\nVI. CONCLUSIONS\nAiming at the limited work distance of the existing stereo vision system and active time-\nof-flight (TOF) camera, this paper proposes to sense the three-dimensional structure of spacecraft at a long distance (maximum to 250m) using LIDAR and a monocular camera. To this end, a novel Spacecraft Depth Completion Network (SDCNet) is proposed to recover the dense depth map using a gray image and sparse depth map. Considering that the celestial background inevitably interferes with the spacecraft depth recovery, the object-level spacecraft depth completion task is decomposed into the foreground segmentation subtask and the foreground depth completion subtask. Specifically, a lightweight foreground segmentation subnet (FSNet) is designed for foreground region segmentation first, and the pixel's depth in the segmented region is regressed using the foreground depth completion subnet (SDCNet). Moreover, we design the attention-based feature fusion module to deduce the correlation between different features along the channel and the spatial dimension sequentially, integrating the geometric features and context the gray image provides into the depth map feature. Four new metrics for the object-level depth completion task are also proposed to evaluate depth\ncompletion results, including MAEI, MATE, RMSEI, and RMSTE. Besides, we construct a large-scale satellite depth completion dataset based on 126 satellite CAD models, containing 6336, 576, and 1152 sets of data for training, validation, and testing the spacecraft depth completion algorithms. The construction of the satellite depth completion dataset solves the lack of satellite data for training and testing depth completion methods. Empirical experiments on the dataset demonstrate that our method achieves state-of-the-art depth completion performance, which achieves 0.25m mean absolute error of interest and 0.759m mean absolute truncation error. Finally, the spacecraft pose estimation experiment is also conducted based on the depth completion results, which achieves 0.85\u00b0 rotation error and 1.47m translate error, verifying that the predicted depth map could meet the needs of downstream vision tasks. The proposed spacecraft depth completion method has the potential to be integrated into space onorbit service systems, which can perceive the fine three-dimensional structure of spacecraft at a long distance using LIDAR and optical camera.\nVIIREFERENCES\n[1] J. Ventura J et al., \u201cPose tracking of a noncooperative spacecraft during docking maneuvers\nusing a time-of-flight sensor,\u201d in AIAA Guidance, Navigation, and Control Conference, California, USA, 2016, pp. 0875, doi: 10.2514/6.2016-0875.\n[2] X. Liu et al., \u201cPosition Awareness Network for Non-Cooperative Spacecraft Pose\nEstimation Based on Point Cloud,\u201d. IEEE Transactions on Aerospace and Electronic Systems, early access, 2022, doi: 10.1109/TAES.2022.3182307.\n[3] Q. Wei et al., \u201cRobust spacecraft component detection in point clouds,\u201d. Sensors, vol. 18,\nno. 4, pp. 933, Mar. 2018, doi: 10.3390/s18040933.\n[4] R. Volpe et al., \u201cReconstruction of the Shape of a Tumbling Target from a Chaser in Close\nOrbit,\u201d in IEEE Aerospace Conference, MT, USA, 2020, pp. 1-11, doi: 10.1109/AERO47225.2020.9172529.\n[5] W. Xu et al., \u201cA pose measurement method of a non-cooperative GEO spacecraft based on\nstereo vision,\u201d in International Conference on Control Automation Robotics & Vision, Guangzhou, China, 2012, pp. 966-971, doi: 10.1109/ICARCV.2012.6485288.\n[6] H. G. Mart\u00ednez et al., \u201cPose estimation and tracking of non-cooperative rocket bodies using\ntime-of-flight cameras,\u201d Acta Astronautica, vol. 139, pp. 165-175, Oct. 2017, doi: 10.1016/j.actaastro.2017.07.002.\n[7] F. Ma et al., \u201cSparse-to-dense: Depth prediction from sparse depth samples and a single\nimage,\u201d in IEEE international conference on robotics and automation, Brisbane, Australia, 2018, pp. 4796-4803, doi: 10.1109/ICRA.2018.8460184.\n[8] S. Imran et al., \u201cDepth coefficients for depth completion,\u201d in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, Long Beach, USA, 2019, pp. 12438-12447, doi: 10.1109/CVPR.2019.01273.\n[9] F. Ma et al., \u201cSelf-supervised sparse-to-dense: Self-supervised depth completion from lidar\nand monocular camera,\u201d in IEEE international conference on robotics and automation, Montreal, Canada, 2019, pp. 3288-3295, doi: 10.1109/ICRA.2019.8793637.\n[10] B. U. Lee et al., \u201cDepth completion using plane-residual representation,\u201d in IEEE\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, USA, 2021, pp. 13911-13920, doi: 10.1109/CVPR46437.2021.01370.\n[11] P. Hambarde P et al., \u201cS2DNet: Depth estimation from single image and sparse samples,\u201d\nIEEE Transactions on Computational Imaging, vol. 6, pp. 806-817, Mar. 2020, doi: 10.1109/TCI.2020.2981761.\n[12] L. Liu et al., \u201cLearning steering kernels for guided depth completion,\u201d IEEE Transactions\non Image Processing, vol. 30, pp. 2850-2861, Feb. 2021, doi: 10.1109/TIP.2021.3055629.\n[13] M. Hu et al., \u201cPenet: Towards precise and efficient image guided depth completion,\u201d in\nIEEE international conference on robotics and automation, Xi'an, China, 2021, pp. 13656-\n13662, doi: 10.1109/ICRA48506.2021.9561035.\n[14] S. Liu et al., \u201cLearning affinity via spatial propagation networks,\u201d in Proccedings of Neural\nInformation Processing Systems, Long Beach, USA, 2017, pp. 1519-1529, doi: 10.48550/arXiv.1710.01020.\n[15] X. Cheng et al., \u201cLearning depth with convolutional spatial propagation network,\u201d. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 10, pp. 2361-2379, Oct. 2019, doi: 10.1109/TPAMI.2019.2947374.\n[16] X. Cheng et al., \u201cCspn++: Learning context and resource aware convolutional spatial\npropagation networks for depth completion,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, New York, USA, 2020, pp. 10615-10622, doi: 10.1609/aaai.v34i07.6635.\n[17] J. Park et al., \u201cNon-local spatial propagation network for depth completion,\u201d in Proceedings\nof the European conference on computer vision, 22020, pp. 120-136, doi: 10.1007/978-3- 030-58601-0_8.\n[18] Y. Lin et al., \u201cDynamic spatial propagation network for depth completion,\u201d. 2022,\narXiv:2202.09769.\n[19] K. He et al., \u201cGuided image filtering,\u201d IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 35, no. 6, pp. 1397-1409, Oct. 2012, doi: 10.1109/TPAMI.2012.213.\n[20] J. Tang et al., \u201cLearning guided convolutional network for depth completion,\u201d IEEE\nTransactions on Image Processing, vol. 30, pp. 1116-1129, Dec. 2020, doi: 10.1109/TIP.2020.3040528.\n[21] L. Liu et al., \u201cFcfr-net: Feature fusion based coarse-to-fine residual learning for depth\ncompletion,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 2136-2144, doi: 10.48550/arXiv.2012.08270.\n[22] Z. Yan et al., \u201cRigNet: Repetitive image guided network for depth completion,\u201d 2021,\narXiv:2107.13802.\n[23] Y. Chen et al., \u201cLearning joint 2d-3d representations for depth completion,\u201d in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 2019, pp. 10023-10032, doi: 10.1109/ICCV.2019.01012.\n[24] J. Uhrig et al., \u201cSparsity invariant cnns,\u201d in Proceedings of the international conference on\n3D Vision, Qingdao, China, 2017, pp. 11-20, doi: 10.1109/3DV.2017.00012.\n[25] Z. Liu et al., \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer Vision, QC, Canada, 2021, pp. 9992-10002, doi: 10.1109/ICCV48922.2021.00986.\n[26] A. Vaswani et al., \u201cAttention is all you need,\u201d. in Proccedings of Neural Information\nProcessing Systems, Long Beach, USA, 2017, pp. 6000-6010. doi: arXiv:1706.03762.\n[27] S. Woo et al., \u201cCbam: Convolutional block attention module,\u201d in Proceedings of the\nEuropean conference on computer vision, Munich, Germany, 2018, pp. 3-19, doi: 10.48550/arXiv.1807.06521.\n[28] D. P. Kingma et al., \u201cAdam: A method for stochastic optimization,\u201d 2014, arXiv:1412.6980."
        }
    ],
    "title": "Spacecraft depth completion based on the gray image and the sparse depth map",
    "year": 2022
}