{
    "abstractText": "Existing theories on deep nonparametric regression have shown that when the input data lie on a low-dimensional manifold, deep neural networks can adapt to the intrinsic data structures. In real world applications, such an assumption of data lying exactly on a low dimensional manifold is stringent. This paper introduces a relaxed assumption that the input data are concentrated around a subset of Rd denoted by S , and the intrinsic dimension of S can be characterized by a new complexity notation \u2013 effective Minkowski dimension. We prove that, the sample complexity of deep nonparametric regression only depends on the effective Minkowski dimension of S denoted by p. We further illustrate our theoretical findings by considering nonparametric regression with an anisotropic Gaussian random design N (0,\u03a3), where \u03a3 is full rank. When the eigenvalues of \u03a3 have an exponential or polynomial decay, the effective Minkowski dimension of such an Gaussian random design is p = O( \u221a logn) or p = O(n\u03b3 ), respectively, where n is the sample size and \u03b3 \u2208 (0,1) is a small constant depending on the polynomial decay rate. Our theory shows that, when the manifold assumption does not hold, deep neural networks can still adapt to the effective Minkowski dimension of the data, and circumvent the curse of the ambient dimensionality for moderate sample sizes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zixuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Minshuo Chen"
        },
        {
            "affiliations": [],
            "name": "Mengdi Wang"
        },
        {
            "affiliations": [],
            "name": "Wenjing Liao"
        },
        {
            "affiliations": [],
            "name": "Tuo Zhao"
        }
    ],
    "id": "SP:6f8ab744e7b023461cebeeff96d049e3ba11c6ca",
    "references": [
        {
            "authors": [
                "N.S. Altman"
            ],
            "title": "An introduction to kernel and nearest-neighbor nonparametric regression",
            "venue": "The American Statistician.",
            "year": 1992
        },
        {
            "authors": [
                "T.W. Anderson"
            ],
            "title": "An introduction to multivariate statistical analysis",
            "year": 1962
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "P.J. Bickel",
                "B. Li"
            ],
            "title": "Local polynomial regression on unknown manifolds",
            "venue": "Lecture NotesMonograph Series, 54 177\u2013186.",
            "year": 2007
        },
        {
            "authors": [
                "C. Chatfield"
            ],
            "title": "Introduction to multivariate analysis",
            "venue": "Routledge.",
            "year": 2018
        },
        {
            "authors": [
                "M. Chen",
                "H. Jiang",
                "W. Liao",
                "T. Zhao"
            ],
            "title": "Efficient approximation of deep relu networks for functions on low dimensional manifolds",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "M. Chen",
                "H. Jiang",
                "W. Liao",
                "T. Zhao"
            ],
            "title": "Nonparametric regression on low-dimensional manifolds using deep relu networks: Function approximation and statistical recovery",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng",
                "M.-Y.",
                "Wu",
                "H.-t."
            ],
            "title": "Local linear regression on manifolds and its geometric interpretation",
            "venue": "J. Amer. Statist. Assoc., 108 1421\u20131434.",
            "year": 2013
        },
        {
            "authors": [
                "A. Cloninger",
                "T. Klock"
            ],
            "title": "Relu nets adapt to intrinsic dimensionality beyond the target domain",
            "venue": "arXiv preprint arXiv:2008.02545.",
            "year": 2020
        },
        {
            "authors": [
                "N. Ellis",
                "R. Maitra"
            ],
            "title": "Multivariate gaussian simulation outside arbitrary ellipsoids",
            "venue": "Journal of Computational and Graphical Statistics, 16 692\u2013708.",
            "year": 2007
        },
        {
            "authors": [
                "J. Fan",
                "I. Gijbels"
            ],
            "title": "Local Polynomial Modelling and Its Applications",
            "venue": "Monographs on statistics and applied probability series, Chapman & Hall.",
            "year": 1996
        },
        {
            "authors": [
                "H. Federer"
            ],
            "title": "Curvature measures",
            "venue": "Transactions of the American Mathematical Society, 93 418\u2013491.",
            "year": 1959
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets. In Advances in neural information processing systems",
            "year": 2014
        },
        {
            "authors": [
                "P. Grassberger",
                "I. Procaccia"
            ],
            "title": "Measuring the strangeness of strange attractors",
            "venue": "Physica D: nonlinear phenomena, 9 189\u2013208.",
            "year": 1983
        },
        {
            "authors": [
                "A. Graves",
                "Mohamed",
                "A.-r.",
                "G. Hinton"
            ],
            "title": "Speech recognition with deep recurrent neural networks",
            "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE.",
            "year": 2013
        },
        {
            "authors": [
                "S. Gu",
                "E. Holly",
                "T. Lillicrap",
                "S. Levine"
            ],
            "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
            "venue": "2017 IEEE international conference on robotics and automation (ICRA). IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "L. Gy\u00f6rfi",
                "M. Kohler",
                "A. Krzyzak",
                "H. Walk"
            ],
            "title": "A distribution-free theory of nonparametric regression",
            "venue": "Springer Science & Business Media.",
            "year": 2006
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "year": 2018
        },
        {
            "authors": [
                "S. Kpotufe"
            ],
            "title": "k-NN regression adapts to local intrinsic dimension",
            "venue": "Advances in Neural Information Processing Systems 24 (J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira and K. Q. Weinberger, eds.). 729\u2013737.",
            "year": 2011
        },
        {
            "authors": [
                "S. Kpotufe",
                "V.K. Garg"
            ],
            "title": "Adaptivity to local smoothness and dimension in kernel regression",
            "venue": "Advances in Neural Information Processing Systems 26 (C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani and K. Q. Weinberger, eds.). 3075\u20133083.",
            "year": 2013
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems.",
            "year": 2012
        },
        {
            "authors": [
                "E. Levina",
                "P. Bickel"
            ],
            "title": "Maximum likelihood estimation of intrinsic dimension",
            "venue": "Advances in neural information processing systems, 17.",
            "year": 2004
        },
        {
            "authors": [
                "W. Liao",
                "M. Maggioni",
                "S. Vigogna"
            ],
            "title": "Multiscale regression on unknown manifolds",
            "venue": "arXiv preprint arXiv:2101.05119.",
            "year": 2021
        },
        {
            "authors": [
                "H. Liu",
                "M. Chen",
                "T. Zhao",
                "W. Liao"
            ],
            "title": "Besov function approximation and binary classification on low-dimensional manifolds using convolutional residual networks",
            "venue": "International Conference on Machine Learning. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2015
        },
        {
            "authors": [
                "K.E. Muller",
                "P.W. Stewart"
            ],
            "title": "Linear model theory: univariate, multivariate, and mixed models",
            "venue": "John Wiley & Sons.",
            "year": 2006
        },
        {
            "authors": [
                "R. Nakada",
                "M. Imaizumi"
            ],
            "title": "Adaptive approximation and generalization of deep neural network with intrinsic dimensionality",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "A. Pakman",
                "L. Paninski"
            ],
            "title": "Exact hamiltonian monte carlo for truncated multivariate gaussians",
            "venue": "Journal of Computational and Graphical Statistics, 23 518\u2013542.",
            "year": 2014
        },
        {
            "authors": [
                "V. Panayotov",
                "G. Chen",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "P. Petersen",
                "F. Voigtlaender"
            ],
            "title": "Optimal approximation of piecewise smooth functions using deep relu neural networks",
            "venue": "Neural Networks, 108 296\u2013330.",
            "year": 2018
        },
        {
            "authors": [
                "P. Pope",
                "C. Zhu",
                "A. Abdelkader",
                "M. Goldblum",
                "T. Goldstein"
            ],
            "title": "The intrinsic dimension of images and its impact on learning",
            "venue": "arXiv preprint arXiv:2104.08894.",
            "year": 2021
        },
        {
            "authors": [
                "J. Schmidt-Hieber"
            ],
            "title": "Deep relu network approximation of functions on a manifold",
            "venue": "arXiv preprint arXiv:1908.00695.",
            "year": 2019
        },
        {
            "authors": [
                "A.B. Tsybakov"
            ],
            "title": "Introduction to Nonparametric Estimation",
            "venue": "1st ed. Springer Publishing Company, Incorporated.",
            "year": 2008
        },
        {
            "authors": [
                "G. Wahba"
            ],
            "title": "Spline models for observational data, vol",
            "venue": "59. Siam.",
            "year": 1990
        },
        {
            "authors": [
                "Y. Yang",
                "Tokdar",
                "S. T"
            ],
            "title": "Minimax-optimal nonparametric regression in high dimensions",
            "venue": "Ann. Statist., 43 652\u2013674.",
            "year": 2015
        },
        {
            "authors": [
                "D. Yarotsky"
            ],
            "title": "Error bounds for approximations with deep relu networks",
            "venue": "Neural Networks,",
            "year": 2017
        },
        {
            "authors": [
                "T. Young",
                "D. Hazarika",
                "S. Poria",
                "E. Cambria"
            ],
            "title": "Recent trends in deep learning based natural language processing",
            "venue": "ieee Computational intelligenCe magazine, 13 55\u201375.",
            "year": 2018
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2022), where we substitute l\u221e covering number with l2 covering number to deal with unbounded domain",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u221a logn) or p = O(n\u03b3 ), respectively, where n is the\nsample size and \u03b3 \u2208 (0,1) is a small constant depending on the polynomial decay rate. Our theory shows that, when the manifold assumption does not hold, deep neural networks can still adapt to the effective Minkowski dimension of the data, and circumvent the curse of the ambient dimensionality for moderate sample sizes."
        },
        {
            "heading": "1 Introduction",
            "text": "Deep learning has achieved impressive successes in various real-world applications, such as computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014; Long et al., 2015), natural language processing (Graves et al., 2013; Bahdanau et al., 2014; Young et al., 2018), and robotics (Gu et al., 2017). One notable example of this is in the field of image classification, where the winner of the 2017 ImageNet challenge achieved a top-5 error rate of just 2.25% (Hu et al., 2018) using a training dataset of 1 million labeled high resolution images in 1000 categories. Deep neural networks\n\u2217Zixuan Zhang and Tuo Zhao are affiliated with School of Industrial and Systems Engineering at Georgia Tech; Minshuo Chen and Mengdi Wang are affiliated with Electrical and Computer Engineering at Princeton University; Wenjing Liao is affiliated with School of Mathematics at Georgia Tech; Email: {zzhang3105, wliao60, tourzhao}@gatech.edu, {mc0750, mengdiw}@princeton.edu.\nar X\niv :2\n30 6.\n14 85\n9v 1\n[ cs\n.L G\n] 2\n6 Ju\nhave been shown to outperform humans in speech recognition, with a 5.15% word error rate using the LibriSpeech training corpus (Panayotov et al., 2015), which consists of approximately 1000 hours of 16kHz read English speech from 8000 audio books.\nThe remarkable successes of deep learning have challenged conventional machine learning theory, particularly when it comes to high-dimensional data. Existing literature has established a minimax lower bound of sample complexity n \u2273 \u03f5\u2212(2s+d)/s for learning s-Ho\u0308lder functions in Rd with accuracy \u03f5 (Gyo\u0308rfi et al., 2006). This minimax lower bound, however, is far beyond the practical limits. For instance, the images in the ImageNet challenge are of the resolution 224\u00d7224 = 50176, while the sample size of 1.2 million is significantly smaller than the theoretical bound.\nSeveral recent results have attempted to explain the successes of deep neural networks by taking the low-dimensional structures of data into consideration(Chen et al., 2019, 2022; Nakada and Imaizumi, 2020; Liu et al., 2021; Schmidt-Hieber, 2019). Specifically, Chen et al. (2022) shows that when the input data are supported on a p-dimensional Riemannian manifold embedded in Rd , deep neural networks can capture the low-dimensional intrinsic structures of the manifold. The sample complexity in Chen et al. (2022) depends on the intrinsic dimension p, which circumvents the curse of ambient dimension d; Nakada and Imaizumi (2020) assumes that the input data are supported on a subset of Rd with Minkowski dimension p, and establishes a sample complexity similar to Chen et al. (2022). Liu et al. (2021) considers a classification problem, and show that convolutional residual networks enjoy similar theoretical properties to Chen et al. (2022).\nConsidering the complexity of real world applications, however, the assumptions of data lying exactly on a low-dimensional manifold or a set with low Minkowski dimension are stringent. To bridge such a gap between theory and practice, we consider a relaxed assumption that the input data X are approximately supported on a subset of Rd with certain low-dimensional structures denoted by S . Roughly speaking, there exists a sufficiently small \u03c4 such that we have P(X < S) = \u03c4 , where S can be characterized by a new complexity notation \u2013 effective Minkowski dimension. We then prove that under proper conditions, the sample complexity of nonparametric regression using deep neural networks only depends on the effective Minkowski dimension of S denoted by p. Our assumption arises from practical motivations: The distributions of real-world data sets often exhibit a varying density. In practice, the low-density region can be neglected, if our goal is to minimize the L2 prediction error in expectation.\nFurthermore, we illustrate our theoretical findings by considering nonparametric regression with an anisotropic multivariate Gaussian randomly sampled from N (0,\u03a3) design in Rd . Specifically, we prove that when the eigenvalues of \u03a3 have an exponential decay, we can properly construct S with the effective Minkowski dimension p = min(O( \u221a logn),d). Moreover, when the eigenvalues of \u03a3 have a polynomial decay, we can properly construct S with the effective Minkowski dimension p = min(O(n\u03b3 ,d)), where \u03b3 \u2208 (0,1) is a small constant. Our proposed effective Minkovski dimension is a non-trivial generalization of the manifold intrinsic dimension (Chen et al., 2022) or the Minkowski dimension (Nakada and Imaizumi, 2020), as both the intrinsic dimension or Minkowski dimension of the aforementioned S \u2019s are d, which can be significantly larger than p for moderate sample size n.\nAn ingredient in our analysis is an approximation theory of deep ReLU networks for \u03b2-Ho\u0308lder functions (Yarotsky, 2017; Nakada and Imaizumi, 2020; Chen et al., 2019). Specifically, we show that, in order to uniformly approximate \u03b2-Ho\u0308lder functions on a properly selected S up to an \u03f5 error, the network consists of at most O(\u03f5\u2212p/\u03b2) neurons and weight parameters, where p is the effective Minkowski dimension of the input data distribution. The network size in our theory only weakly depends on the ambient dimension d, which circumvents the curse of dimensionality for function approximation using deep ReLU networks. Our approximation theory is established for the L2 norm instead of the L\u221e norm in Nakada and Imaizumi (2020); Chen et al. (2019). The benefit is that we only need to approximate the function accurately on the high-density region, and allow for rough approximations on the low-density region. Such flexibility is characterized by our effective Minkowski dimension.\nThe rest of this paper is organized as follows: Section 2 reviews the background; Section 3 presents our functional approximation and statistical theories; Section 4 provides an application to Gaussian random design; Section 5 presents the proof sketch of our main results; Section 6 discusses related works and draws a brief conclusion.\nNotations Given a vector v = (v1, ...,vd)\u22a4 \u2208Rd , we define \u2225v\u2225 p p = \u2211 j |vj |p for p \u2208 [1,\u221e) and \u2225v\u2225\u221e = maxj |vj |. Given a matrix W = [Wij ] \u2208 Rn\u00d7m, we define \u2225W \u2225\u221e = maxi,j |Wij |. We define the number of nonzero entries of v and W as \u2225v\u22250 and \u2225W \u22250, respectively. For a function f (x), where x \u2208 X \u2286 Rd , we define \u2225f \u2225\u221e = maxx\u2208X |f (x)|. We define \u2225f \u2225 2 L2(P ) = \u222b X f\n2(x)p(x)dx, where P is a continuous distribution defined on X with the pdf p(x)."
        },
        {
            "heading": "2 Background",
            "text": "In nonparametric regression, the aim is to estimate a ground-truth regression function f \u2217 from i.i.d. noisy observations {(xi , yi)}ni=1. The data are generated via\nyi = f \u2217(xi) + \u03bei ,\nwhere the noise \u03bei \u2019s are i.i.d. sub-Gaussian noises with E[\u03bei] = 0 and variance proxy \u03c32, which are independent of the xi \u2019s. To estimate f \u2217, we minimize the empirical quadratic loss over a concept class F , i.e.,\nf\u0302 \u2208 argmin f \u2208F 1 2n n\u2211 i=1 (f (xi)\u2212 yi)2 . (1)\nWe assess the quality of estimator f\u0302 through bounding L2 distance between f\u0302 and f \u2217, that is,\u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252 L2(Pdata) \u2264 \u03b3(n).\nHere \u03b3(n) is a function of n describing the convergence speed and Pdata is an unknown sampling distribution ofthe xi \u2019s supported on Ddata.\nExisting literature on nonparametric statistics has established an optimal rate of \u03b3(n) \u2272 n\u2212 2\u03b1\n2\u03b1+d , when f \u2217 is \u03b1-smooth with bounded functional norm, and F is properly chosen (Wahba, 1990; Altman, 1992; Fan and Gijbels, 1996; Tsybakov, 2008; Gyo\u0308rfi et al., 2006).\nThe aforementioned rate of convergence holds for any data distribution Pdata. For high-dimensional data, the convergence rate suffers from the curse of dimensionality. However, in many practical applications, Pdata exhibits important patterns. For example, data are highly clustered in certain regions, while scarce in the rest of the domain. In literature, a line of work studies when Pdata is supported on a low-dimensional manifold (Bickel and Li, 2007; Cheng and Wu, 2013; Liao et al., 2021; Kpotufe, 2011; Kpotufe and Garg, 2013; Yang et al., 2015). The statistical rate of convergence \u03b3(n) in these works depends on the intrinsic dimension of the manifold, instead of the ambient dimension. Recently, neural networks are also shown to be able to capture the lowdimensional structures of data (Schmidt-Hieber, 2019; Nakada and Imaizumi, 2020; Chen et al., 2022).\nAs mentioned, aforementioned works assume that data exactly lie on a low-dimensional set, which is stringent. Recently, Cloninger and Klock (2020) relaxes the assumption such that data are concentrated on a tube of the manifold, but the radius of this tube is limited to the reach (Federer, 1959) of the manifold. In this paper, we establish a fine-grained data dependent nonparametric regression theory, where data are approximately concentrated on a low-dimensional subset of the support.\nTo facilitate a formal description, we denoteDdata as the data support. Given r,\u03c4 > 0, we define\nN (r;\u03c4) :=inf S {Nr(S) :S\u2282Ddata withPdata(S)\u2265 1\u2212 \u03c4},\nwhere Nr(S) is the r-covering number of S with respect to L\u221e distance.\nAssumption 1. For any sufficiently small r,\u03c4 > 0, there exists a positive constant p = p(r,\u03c4) such that\nlogN (r;\u03c4) \u2212 logr \u2264 p(r,\u03c4).\nFurthermore, there exists S \u2282 Ddata such that\nNr(S) \u2264 c0N (r;\u03c4) \u2264 c0r\u2212p\nfor some constant c0 > 1, Pdata(Sc) \u2264 \u03c4 and |xi | \u2264 RS for any x = (x1, . . . ,xd) \u2208 S and some constant RS > 0.\nWe next introduce Ho\u0308lder functions and the Ho\u0308lder space.\nDefinition 1 (Ho\u0308lder Space). Let \u03b2 > 0 be a degree of smoothness. For f : X \u2192R, the Ho\u0308lder norm is defined as\n\u2225f \u2225H(\u03b2,X ) := max \u03b1:\u2225\u03b1\u22251<\u230a\u03b2\u230b sup x\u2208X |\u2202\u03b1f (x)|+ max \u03b1:\u2225\u03b1\u22251=\u230a\u03b2\u230b sup x,x\u2032\u2208X ,x,x\u2032\n|\u2202\u03b1f (x)\u2212\u2202\u03b1f (x\u2032)|\n\u2225x \u2212 x\u2032\u2225\u03b2\u2212\u230a\u03b2\u230b\u221e .\nThen the Ho\u0308lder space on X is defined as H(\u03b2,X ) = { f \u2208 C\u230a\u03b2\u230b(X ) \u2223\u2223\u2223\u2225f \u2225H(\u03b2,X ) \u2264 1}.\nWithout loss of generality, we impose the following assumption on the target function f \u2217:\nAssumption 2. The ground truth function f \u2217 :Ddata\u2192 R belongs to the Ho\u0308lder space H(\u03b2,Ddata) with \u03b2 \u2208 (0,d).\nAlthough the Ho\u0308lder norm of f \u2217 is assumed to be bounded by 1, our results can be easily extended to the case when \u2225f \u2217\u2225H(\u03b2,Ddata) is upper bounded by any positive constant. In addition, \u03b2 < d is a natural assumption. Given that ambient dimension d is always large, it is unusual for regression functions to possess a degree of smoothness larger than d.\nOur goal is to use multi-layer ReLU neural networks to estimate the function f \u2217. Given an input x, an L-layer ReLU neural network computes the output as\nf (x) = WL \u00b7ReLU(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) + bL, (2)\nwhere W1, . . . ,WL and b1, . . . , bL are weight matrices and intercepts respectively. The ReLU(\u00b7) activation function denotes the entrywise rectified linear unit, i.e. ReLU(a) = max{a,0}. The empirical risk minimization in (1) is taken over the function class F given by a network architecture.\nDefinition 2 (Function Class Given by a Network Architecture). Given a tuple (L,B,K), a functional class of ReLU neural networks is defined as follows:\nF (L,B,K) := { f |f (x) in the form of (2) with L layers,\u2225f \u2225\u221e \u2264 1,\u2225Wi\u2225\u221e \u2264 B, \u2225bi\u2225\u221e \u2264 B\nfor i = 1, . . . ,L, L\u2211 i=1 \u2225Wi\u22250 + \u2225bi\u22250 \u2264 K } ."
        },
        {
            "heading": "3 Approximation and Generalization Theory",
            "text": "In this section, we present generic approximation and generalization theory and defer detailed proofs to Section 5.1 and 5.2 respectively. Firstly, we introduce the approximation theory of utilizing deep neural networks to approximate Ho\u0308lder functions. The approximation error is determined by effective Minkowski dimension of data distribution and probability of low-density area. Furthermore, we present the generalization error when approximating regression function f \u2217. The convergence rate also depends on effective Minkowski dimension.\nTheorem 1 (Approximation of deep neural networks). Suppose Assumption 1 hold. For \u03b2 > 0 and any sufficiently small \u03f5,\u03c4 > 0, consider a tuple (L,B,K)\nL = C1, B = O(R \u03b2s S \u03f5 \u2212s), and K = C2(RSd) p\u03f5\u2212p/\u03b2 ,\nwhere RS > 0 and p = p(d\u22121\u03f51/\u03b2/2, \u03c4) are given by Assumption 1, and\nC1 = O(d), C2 = O ( d2+\u230a\u03b2\u230b ) , and s = s(\u03b2).\nThen for any f \u2217 \u2208 H(\u03b2,Ddata), we have\ninf f \u2208F (L,B,K) \u2225f \u2212 f \u2217\u22252L2(Pdata) \u2264 \u03f5 2 + 4\u03c4.\nThe novelty of Theorem 1 is summarized below: Dependence on Effective Minkowski Dimension. The approximation rate in Theorem 1 is O(K\u2212\u03b2/p), which only depends on effective Minkowski dimension p < d and function smoothness \u03b2, but not on ambient dimension d. Compared to Yarotsky (2017), our results improves the exponential dependence of neural network size on d to that on p. Moreover, unlike Nakada and Imaizumi (2020) and Chen et al. (2022), our results do not require that data distribution is exactly supported on a low-dimensional structure. Instead, our results can work for data distribution with high-dimensional support as long as its effective Minkowski dimension is relatively small.\nRelaxation to the L2-error. The approximation error in Theorem 1 is established with respect to the L2(Pdata) norm, while most of existing works focus on the L\u221e error (Yarotsky, 2017; Nakada and Imaizumi, 2020; Chen et al., 2019). Intuitively, it is not necessary for the network class to approximate the function value at each point in the domainDdata precisely when data distribution is highly concentrated at certain subset. Instead, it suffices to approximate f \u2217 where the probability density is significant, while the error for the low-density region can be easily controlled since the regression function f \u2217 and the neural network class f \u2208 F (L,B,K) are bounded.\nThe benefit of using the L2 error is that, we only need to control the approximation error of f \u2217 within some chosen region S \u2286 Ddata. Here S has an effective Minkowski dimension p, which ensures that it can be covered by O(r\u2212p) hypercubes with side length r. Then we design deep neural networks to approximate f \u2217 within each hypercube and thus the network size depends on the number of hypercubes used to cover S. This explains why network size in Theorem 1 depends on p. Meanwhile, the probability out of S is negligible since the data density is low. We further demonstrate that this probability \u03c4 is far less than the approximation error in Section 4. By this means, we succeed to reduce the network size and at the same time achieve a small L2 approximation error. We next establish the generalization result for the estimation of f \u2217 using deep neural networks.\nTheorem 2 (Generalization error of deep neural networks). Suppose Assumption 1 holds. Fix any sufficiently small r,\u03c4 > 0 satisfying r < RS and \u03c4 < r4\u03b2/4. Set a tuple (L,B,K) with C1,C2 and s appearing in Theorem 1 as\nL = C1, B = O(R \u03b2s S r \u2212\u03b2s), and K = C2R p Sr \u2212p\nwith p = p(r,\u03c4). Let f\u0302 be the global minimizer of empirical loss given in (1) with the function class F = F (L,B,K). Then we have\nE\u2225f\u0302 \u2212 f \u2217\u22252L2(Pdata) = O ( \u03c4 + \u03c3r2\u03b2 + \u03c32\nn ( RS r )p log ( (RS /r)p r4\u03b2 \u2212 4\u03c4 )) ,\nwhere O(\u00b7) hides polynomial dependence on d.\nTheorem 2 is a statistical estimation result. It implies that the generalization error also depends on effective Minkowski dimension p. To establish this result, we decompose the squared error into a squared bias term and a variance term. The bias is tackled with the approximation error in Theorem 1 and the variance depends on the network size. With the network size growing,\nthe variance term increases while the bias term decreases, since the approximation capability of neural networks is enhanced as the size of the network enlarges. Therefore, we need to trade off between the squared bias and the variance to minimize the squared generalization error.\nNotably, our analysis in Section 3 holds for any sufficiently small \u03c4 and r, and every pair of \u03c4 and r determines a p. As shown in Assumption 1, if \u03c4 and r decreases, the covering number will become larger while the approximation can be more accurate. In order to establish an explicit bound, we need to trade off \u03c4 and r for the given sample size n. Therefore the \u201coptimal\u201d p eventually becomes functions of n. We call such an \u201coptimal\u201d p effective Minkowski dimension.\nIn the next section, we give two specific classes of Gaussian random designs to illustrate how effective Minkowski dimension p(r,\u03c4) scales with r and \u03c4 . We further show that, under a proper selection of the region S and the covering accuracy r, the convergence rate for the estimation of f \u2217 using deep neural networks is O\u0303(n\u22122\u03b2/(2\u03b2+p)), where the effective Minkowski dimension p is properly chosen."
        },
        {
            "heading": "4 Application to Gaussian Random Design",
            "text": "In literature, it is common to consider random Gaussian design in nonparametric regression (Anderson, 1962; Muller and Stewart, 2006; Chatfield, 2018). In this section, we take anisotropic multivariate Gaussian design as example to justify Assumption 1 and demonstrate the effective Minkowski dimension. Here we only provide our main theorems and lemmas. The detailed proofs are given in Section 5.3.\nConsider a Gaussian distribution Pdata \u223cN (0,\u03a3) in Rd . The covariance matrix \u03a3 has the eigendecomposition form: \u03a3 = Q\u0393Q\u22a4, where Q is an orthogonal matrix and \u0393 = diag(\u03b31, . . . ,\u03b3d). For notational convenience in our analysis, we further denote eigenvalue \u03b3i = \u03bb 2 i for i = 1, . . . ,d. Without loss of generality, assume that \u03bb21 \u2265 \u03bb 2 2 \u2265 . . . \u2265 \u03bb 2 d . Furthermore, we assume that {\u03bb 2 i } d i=1 has an exponential or polynomial decay rate:\nAssumption 3 (Exponential decay rate). The eigenvalue series {\u03b3i}di=1 = {\u03bb 2 i } d i=1 satisfies \u03bbi \u2264 \u00b5exp{\u2212\u03b8i} for some constants \u00b5,\u03b8 > 0.\nAssumption 4 (Polynomial decay rate). The eigenvalue series {\u03b3i}di=1 = {\u03bb 2 i } d i=1 satisfies \u03bbi \u2264 \u03c1i \u2212\u03c9 for some constants \u03c1 > 0 and \u03c9 > 1.\nWhen the eigenvalues decay fast, the support of the data distribution Pdata has degeneracy in some directions. In this case, the majority of probability lies in some region S \u2282 Rd , which has an effective Minkowski dimension p < d. Specifically, consider a \u201cthick\u201d low-dimensional hyperellipsoid in Rd ,\nS(R,r;p) := { Qz \u2223\u2223\u2223\u2223\u2223z=(z1, . . . , zd) \u2208Rd , p\u2211 i=1 z2i \u03bb2i \u2264 R2, |zj | \u2264 r 2 for j = p+ 1, . . . ,d } , (3)\nwhere R,r > 0 and p \u2208 N+ are independent parameters. For the simplicity of notation, we first define a standard hyper-ellipsoid and then linearly transform it to align with the distribution\nN (0,\u03a3). The set S(R,r;p) can be regarded as a hyper-ellipsoid scaled by R > 0 in the first p dimensions, and with thickness r > 0 in the rest d \u2212 p dimensions. Then we construct a minimal cover as a union of nonoverlapping hypercubes with side length r for S(R,r;p). The following lemma characterizes the relationship between the probability measure outside S(R,r;p) and its covering number.\nLemma 1. Given the eigenvalue series {\u03bb2i } d i=1, for any R,r > 0, choose p > 0 such that \u03bb \u22121 p = 2R/r. If p < R2, we will have P(X < S(R,r;p)) = O ( exp(\u2212R2/3) ) ,\nNr(S(R,r;p)) \u2264 (\n2R r\n)p \u00b7 p\u220f i=1 \u03bbi = p\u220f i=1 ( \u03bbi \u03bbp ) .\nRemark 1. Since data distribution Pdata is supported on Rd , both the intrinsic dimension and the Minkowski dimension of Pdata are d. However, Lemma 1 indicates that the effective Minkowski dimension of Pdata is at most p.\nAccording to Lemma 1, if we choose scale R > \u221a p properly, the probability outside S can be sufficiently small while the covering number of S is dominated by r\u2212p, which gives that the effective Minkowski dimension of Pdata is at most p. Moreover, under fast eigenvalue decays, the product of the first p eigenvalues appearing in Nr(S(R,r;p)) is a small number dependent of p. In these cases, we specify the selection of R, r and p accordingly and show the effective Minkowski dimension is reduced to p/2 in Appendix D.\nFurthermore, we remark that the effective Minkowski dimension p is not a fixed number given data distribution Pdata, but an increasing function of sample size n. As sample size n increases, the estimation accuracy of f \u2217 is required to be higher, so that we are supposed to design more and smaller hypercubes to enable preciser estimation by neural networks. Besides, some of the d \u2212 p dimensions are not negligible anymore and thereby become effective compared to the accuracy. Therefore, we need to incorporate more dimensions to be effective to achieve higher accuracy.\nWith this observation, we construct S(R,r;p) such that its effective Minkowski dimension p(n) increases while thickness r(n) decreases as sample size n grows to enable preciser estimation. Then we develop the following sample complexity:\nTheorem 3 (Generalization error under fast eigenvalue decay). Under Assumption 2, let f\u0302 be the global minimizer of empirical loss given in (1) with function class F = F (L,B,K). Suppose Assumption 3 hold. Set a tuple (L,B,K) with the constants C1,C2 and s appearing in Theorem 1 as\nL = C1, B = O ( n\n\u03b2s\n2\u03b2+ \u221a logn/\u03b8 (logn)\u03b2s ) , and K = C2n \u221a logn/\u03b8 2\u03b2+ \u221a logn/\u03b8 .\nThen we have\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c32n \u2212 2\u03b2(1\u2212\u03b7) 2\u03b2+ \u221a logn/\u03b8 (logn)3/2 )\nfor sufficiently large n satisfying log(logn)/ \u221a \u03b8 logn \u2264 \u03b7, where \u03b7 > 0 is an arbitrarily small constant. Moreover, suppose Assumption 4 hold instead. Set a tuple (L,B,K) as\nL = C1, B = O ( n (1+1/\u03c9)\u03b2s 2\u03b2+n\u03ba ) , and K = C2 ( n (1+1/\u03c9)n\u03ba/(2\u03b2+n \u03ba ) 4\u03b2+2n\u03ba ) ,\nwhere \u03ba = (1 + 1/\u03c9)/\u03c9. Then we have\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c32n \u2212 2\u03b2 2\u03b2+n(1+1/\u03c9)/\u03c9 logn ) .\nTheorem 3 suggests the effective Minkowski dimension of Gaussian distribution is \u221a\nlogn/\u03b8 under exponential eigenvalue decay with speed \u03b8 and effective Minkowski dimension is n(1+1/\u03c9)/\u03c9 under polynomial eigenvalue decay with speed \u03c9. For moderate sample size n, i.e. effective Minkowski dimension is less that ambient dimension d, Theorem 3 achieves a faster convergence rate. When we have a vast amount of data, the effective Minkowski dimension is the same as the ambient dimension d, and then we can apply standard analysis of deep neural networks for d-dimensional inputs to obtain the convergence rate O\u0303(n\u22122\u03b2/(2\u03b2+d)). To the best of our knowledge, Theorem 3 appears to be the first result for nonparametric regression and deep learning theory, where the effective dimension varies with the sample size."
        },
        {
            "heading": "5 Proof Sketch",
            "text": "This section contains proof sketches of Theorem 1, 2 and Lemma 1."
        },
        {
            "heading": "5.1 Proof Sketch of Theorem 1",
            "text": "We provide a proof sketch of Theorem 1 in this part and defer technical details of the proof to Appendix A. The ReLU neural network in Theorem 1 is constructed in the following 5 steps:\n1. Choose region S \u2282 Ddata only on which we use ReLU neural networks to approximate f \u2217.\n2. Construct a covering of S with hypercubes and then divide these hypercubes into several groups, so that neural networks constructed with respect to each group have nonoverlapping supports.\n3. Implement ReLU neural networks to assign given input and estimated function value to corresponding hypercube.\n4. Approximate f \u2217 by a Taylor polynomial and then implement a ReLU neural network to approximate Taylor polynomial on each hypercube.\n5. Sum up all the sub-neural-networks and take maximum to approximate f \u2217.\nStep 1. Space separation. Firstly, we divide Ddata into some region S \u2282 Ddata with high probability measure and Sc = Ddata with large volume. By Assumption 1, for any sufficiently small r,\u03c4 > 0 and some constant c0 > 1, there exists S \u2282 Ddata such that Nr(S) \u2264 c0N (r;\u03c4) \u2264 c0r\u2212p for\nsome positive constant p = p(r,\u03c4) and Pdata(Sc) \u2264 \u03c4 . Intuitively, we only need to approximate f \u2217 on S while Sc is negligible due to its small probability measure. Therefore, in the following steps, we only design a covering for S and approximate f \u2217 in each hypercube of the covering.\nStep 2. Grouping hypercubes. Let C be a minimum set of hypercubes with side length r covering S. Then we partition C into C1, . . . ,CJ such that each subset Cj is composed of hypercubes separated by r from each other. Lemma 7 shows that the number of Cj \u2019s is at most a constant dependent of d.\nAs a consequence, we group hypercubes into several subsets of C so that constructed neural networks with respect to each hypercube in Cj have nonoverlapping support.\nStep 3. Hypercube Determination. This step is to assign the given input x and estimated function value y to the hypercube where they belong. To do so, we design a neural network to approximate function (x,y) 7\u2192 y1I (x) where I \u2208 C is some hypercube. To make functions positive, we firstly consider approximating f0 = f \u2217 + 2. Notice that f0 \u2208 H(\u03b2,Ddata,3) and 1 \u2264 f0(x) \u2264 3 for any x \u2208 Ddata.\nFor any fixed I \u2208 C, we define the center of I as (\u03b91, . . . , \u03b9d). Then we construct a neural network g ind,rI :Ddata \u00d7R\u2265\u2192R\u2265 with the form:\ng ind,rI (x,y) = 4ReLU ( d\u2211 i=1 1\u0302 r I,i(xi) + y 4 \u2212 d ) , (4)\nwhere 1\u0302 r I,i : R\u2192 [0,1] is the approximated indicator function given by\n1\u0302 r I,i(z) =  z\u2212(\u03b9i\u2212r) r/2 if \u03b9i \u2212 r < z \u2264 \u03b9i \u2212 r 2 , 1 if \u03b9i \u2212 r2 < z \u2264 \u03b9i + r 2 , (\u03b9i+r)\u2212z r/2 if \u03b9i + r 2 < z \u2264 \u03b9i + r,\n0 otherwise.\nWe claim that neural network g ind,rI approximates function (x,y) 7\u2192 y1I (x). Moreover, Appendix A.2 provides the explicit realization of g ind,rI by selecting specific weight matrices and intercepts.\nStep 4. Taylor Approximation. In each cube I \u2208 C, we locally approximate f \u2217 by a Taylor polynomial of degree \u230a\u03b2\u230b and then we define a neural network to approximate this Taylor polynomial. Firstly, we cite the following lemma to evaluate the difference between any \u03b2-Ho\u0308lder function and its Taylor polynomial:\nLemma 2 (Lemma A.8 in Petersen and Voigtlaender (2018)). Fix any f \u2208 H(\u03b2,Ddata) with \u2225f \u2225H(\u03b2,Ddata) \u2264 1 and x\u0304 \u2208 S. Let f\u0304 (x) be the Taylor polynomial of degree \u230a\u03b2\u230b of f around x\u0304, namely,\nf\u0304 (x) = \u2211 |\u03b1|\u2264\u230a\u03b2\u230b \u2202\u03b1f (x\u0304) \u03b1! (x \u2212 x\u0304)\u03b1 .\nThen, |f (x)\u2212 f\u0304 (x)| \u2264 d\u03b2 \u2225x \u2212 x\u0304\u2225\u03b2 holds for any x \u2208 Ddata.\nNext, we design an m-dimensional multiple output neural network gpoly\u03f5 = (g poly \u03f5,1 , . . . , g poly \u03f5,m ) to estimate multiple Taylor polynomials in each output. The existence of such neural network is\nensured in the following lemma, which is a straightforward extension of Lemma 18 in Nakada and Imaizumi (2020).\nLemma 3 (Taylor approximation on S). Fix any m \u2208 N+. Let {ck,\u03b1} \u2282 [\u22121,1] for 1 \u2264 k \u2264 m. Let {xk}mk=1 \u2282 S. Then there exist c poly 1 = c poly 1 (\u03b2,d,p), c poly 2 = c poly 2 (\u03b2,d,p) and s poly 1 = s poly 1 (\u03b2,d,p) such that for any sufficiently small \u03f5 > 0, there is a neural network gpoly\u03f5 which satisfies the followings:\n1. supx\u2208S \u2223\u2223\u2223gpoly\u03f5,k (x)\u2212\u2211|\u03b1|<\u03b2 ck,\u03b1(x \u2212 xk)\u03b1\u2223\u2223\u2223 \u2264 \u03f5 for any k = 1, . . . ,m,\n2. L(gpoly\u03f5 ) \u2264 1 + (2 + log2\u03b2)(11 + (1 + \u03b2)/p),\n3. B(gpoly\u03f5 ) \u2264 c poly 1 R\n\u03b2s poly 1\nS \u03f5 \u2212spoly1 ,\n4. K(gpoly\u03f5 ) \u2264 c poly 2 (R p S\u03f5 \u2212p/\u03b2 +m).\nFor any cube Ik \u2208 C, we take fIk (x) as a Taylor polynomial function as with setting x\u0304 \u2190 xIk and f \u2190 f0 in Lemma 2. Then we define a neural network to approximate fIk , which is an \u03f5/2accuracy Taylor polynomial of f0. Let g poly \u03f5/2 be a neural network constructed in Lemma 3 with \u03f5\u2190 \u03f5/2, m\u2190Nr(S), (xk)mk=1\u2190 (xIk ) Nr (S) k=1 , and (ck,\u03b1) m k=1\u2190 (\u2202 \u03b1f (xIk )/\u03b1!) Nr (S) k=1 appearing in Lemma 3. Then, we obtain\nsup k=1,...,Nr (S) sup x\u2208S \u2223\u2223\u2223fIk (x)\u2212 gpoly\u03f5/2,k(x)\u2223\u2223\u2223 \u2264 \u03f52 . (5) In addition, we construct a neural network to aggregate the outputs of gpoly\u03f5/2 . Define a neural\nnetwork gfilterk : R d+Nr (S)\u2192Rd+1 which picks up the first d inputs and (d + k)-th input as\ngfilterk (z) = Id e\u22a4k0d e\u22a4k z, for k = 1, . . . ,Nr(S).\nThen we design a neural network gsimul\u03f5/2 : R d \u2192 RNr (S) that simultaneously estimates Taylor polynomial at each cube. Specifically, gsimul\u03f5/2 is formulated as below\ngsimul\u03f5/2 = ( g ind,rI1 \u25e6 g filter 1 , . . . , g ind,r INr (S) \u25e6 gfilterNr (S) ) \u25e6 (gIdd,L, g poly \u03f5/2 ), (6)\nwhere gIdd,L : R d \u2192 Rd is the neural network version of the identity function whose number of layers is equal to L(gpoly\u03f5/2 ).\nStep 5. Construction of Neural Networks. In this step, we construct a neural network gf0\u03f5 to approximate f0 = f \u2217 + 2. Let gmax,5 d be the neural network version of the maximize function over 5d numbers. Besides, define\ngsum(z1, . . . , zNr (S)) = (\u2211 Ik\u2208C1 zk , . . . , \u2211 Ik\u2208CNr (S) zk ) ,\nwhich aims to sum up the output of gsimul\u03f5/2 in each subset of covering Cj .\nNow we are ready to define gf0\u03f5 . Let g f0 \u03f5 := gmax,5 d \u25e6gsum\u25e6gsimul\u03f5/2 . Equivalently, g f0 \u03f5 can be written as gf0\u03f5 = maxj\u2208[5d ] \u2211 Ik\u2208Cj g simul \u03f5/2,k . Then we come to bound the approximation error of g f0 \u03f5 . When x \u2208 S, there exists some I \u2208 C such that x \u2208 I . Based on the method to construct neural networks, we have\ng f0 \u03f5 (x) = max Ik\u2208Neig(I) gsimul\u03f5/2,k (x) \u2264 maxIk\u2208Neig(I) g poly \u03f5/2,k(x),\nwhere Neig(I) = {I \u2032 \u2208 C|(I \u2295 3r/2)\u2229 I \u2032 , \u2205} denotes the 3r/2-neighborhood of hypercube I . In other words, when computing gf0\u03f5 (x), we only need to take maximum over the estimated function value within hypercubes near x.\nGiven sufficiently small \u03f5 > 0, the error is bounded as\n|gf0\u03f5 (x)\u2212 f0(x)| \u2264 max Ik\u2208Neig(I) \u2223\u2223\u2223gpoly\u03f5/2,k(x)\u2212 f0(x)\u2223\u2223\u2223 \u2264 max\nIk\u2208Neig(I) \u2223\u2223\u2223gpoly\u03f5/2,k(x)\u2212 fIk (x)\u2223\u2223\u2223+ maxIk\u2208Neig(I)\u2223\u2223\u2223fIk (x)\u2212 f0(x)\u2223\u2223\u2223 \u2264\u03b5\n2 + d\u03b2 ( 3r 2 )\u03b2 \u2264 \u03f5,\nwhere the last inequality follows from (5) and Lemma 2. Detailed derivation of approximation error is deferred to Appendix A.3. In terms of parameter tuning, we choose r = d\u22121\u03f51/\u03b2/2.\nTo extend results of f0 to f \u2217, we implement a neural network gmod(z) = (\u2212z+1)\u25e6ReLU(\u2212z+2)\u25e6 ReLU(z\u22121) and consider gf \u2217 \u03f5 = gmod\u25e6g f0 \u03f5 to obtain the desired approximation error \u03f5 for any x \u2208 S. Then we evaluate the approximation error with respect to L2-norm:\u2225\u2225\u2225\u2225gf \u2217\u03f5 \u2212 f \u2217\u2225\u2225\u2225\u2225 L2(Pdata) = (\u222b S + \u222b Sc )( g f \u2217 \u03f5 (x)\u2212 f \u2217(x) )2 dPdata(x) \u2264 \u03f52 + 4\u03c4. This follows from the aforementioned approximation error within S, boundedness of f \u2217 and neural networks, as well as the property that out-of-S probability is upper bounded, i.e. Pdata(Sc) \u2264 \u03c4 .\nFinally, we sum up sizes of all the sub-neural-networks and thus obtain the network size of g f \u2217 \u03f5 . See Appendix A.4 for detailed calculation."
        },
        {
            "heading": "5.2 Proof Sketch of Theorem 2",
            "text": "Proof of Theorem 2 follows a standard statistical decomposition, i.e. decomposing the mean squared error of estimator f\u0302 into a squared bias term and a variance term. We bound the bias and variance separately, where the bias is tackled by the approximation results given in Theorem 1 and the variance is bounded using the metric entropy arguments. Details of the proof for Theorem2 are provided in Appendix B. At first, we decompose the L2 risk as follows:\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = 2E [ 1 n n\u2211 i=1 (f\u0302 (xi)\u2212 f \u2217(xi))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 T1\n+E \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) \u2212 2E [ 1 n n\u2211 i=1 (f\u0302 (xi)\u2212 f \u2217(xi))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 T2\n,\nwhere T1 reflects the squared bias of using neural networks to estimate f \u2217 and T2 is the variance term.\nStep 1. Bounding bias term T1. Since T1 is the empirical L2 risk of f\u0302 evaluated on the samples {xi}ni=1, we relate T1 to the empirical risk by rewriting f\n\u2217(xi) = yi \u2212 \u03bei , so that we can apply the approximation error to bound the minimal empirical risk achieved by f\u0302 . After some basic calculation, we have\nT1 \u2264 2 inf f \u2208F (L,B,K) \u2225f (x)\u2212 f \u2217(x)\u22252L2(Pdata) + 4E 1n n\u2211 i=1 \u03bei f\u0302 (xi)  . Note that the first term is the squared approximation error of neural networks, which can be controlled by Theorem 1. We bound the second term by quantifying the complexity of the network class F (L,B,K). A precise upper bound of T1 is given in the following lemma.\nLemma 4. Fix the neural network class F (L,B,K). For any \u03b4 \u2208 (0,1), there exists some constant c > 0, such that\nT1 \u2264 c inf f \u2208F (L,B,K) \u2225f (x)\u2212 f \u2217(x)\u22252L2(Pdata) + c\u03c3 2 logN2(\u03b4,F (L,B,K)) + 2 n\n+ c (\u221a\nlogN2(\u03b4,F (L,B,K)) + 2 n\n+ 1 ) \u03c3\u03b4,\nwhere N2(\u03b4,F (L,B,K)) denotes the \u03b4-covering number of F (L,B,K) with respect to the L2 norm, i.e., there exists a discretization of F (L,B,K) into N2(\u03b4,F (L,B,K)) distinct elements, such that for any f \u2208 F , there is f\u0304 in the discretization satisfying \u2225\u2225\u2225f\u0304 \u2212 f \u2225\u2225\u2225 2 \u2264 \u03f5.\nStep 2. Bounding variance term T2. We observe that T2 is the difference between the population risk of f\u0302 and its empirical risk. However, bounding this difference is distinct from traditional concentration results because of the scaling factor 2 before the empirical risk. To do this, we divide the empirical risk into two parts and use a higher-order moment (fourth moment) to bound one part. Using a Bernstein-type inequality, we are able to show that T2 converges at a rate of 1/n, and the upper bound for this is shown in the following lemma.\nLemma 5. For any \u03b4 \u2208 (0,1), there exists some constant c\u2032 > 0, such that\nT2 \u2264 c\u2032\n3n logN2(\u03b4/4H,F (L,B,K)) + c\u2032\u03b4.\nStep 3. Covering number of neural networks. The upper bounds of T1 and T2 in Lemma 4 and 5 both rely on the covering number of the network class F (R,\u03ba,L,p,K). In this step, we present an upper bound for the covering numberN2(\u03b4,F (L,B,K)) for a given a resolution \u03b4 > 0.\nLemma 6 (Covering number bound for F (Lemma 21 in Nakada and Imaizumi (2020))). Given \u03b4 > 0, the \u03b4-covering number of the neural network class F (L,B,K) satisfies\nlogN2(\u03b4,F (L,B,K)) \u2264 K log ( 2L \u221a dLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4 ) .\nStep 4. Bias-Variance Trade-off. Now we are ready to finish the proof of Theorem 2. Combining the upper bounds of T1 in Lemma 4 and T2 in Lemma 5 together and substituting the covering number in Lemma 6, we obtain\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c4 + d2\u03b2r2\u03b2+ \u03c3\u03b4+\n\u03c32K n\nlog (\u221a\ndLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4\n)) ,\nwhere we set approximation error to be d2\u03b2r2\u03b2 . Plug in our choice of (L,B,K), and choose \u03b4 = r2\u03b2 . Then we can conclude\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c4 + \u03c3r2\u03b2 + \u03c32\nn ( RS r )p log ( (RS /r)p r4\u03b2 \u2212 4\u03c4 )) ."
        },
        {
            "heading": "5.3 Proof Sketch of Lemma 1",
            "text": "In this section, we present our basic idea to construct S(R,r;p) and the proof sketch of Lemma 1. For simplicity of proof, we assume Q = I so that \u03a3 = \u039b = diag(\u03bb21, . . . ,\u03bb 2 d). The detailed proof is given in Appendix C, which can be easily extended to the case when Q is not an identity matrix. The proof of Theorem 3 is given in Appendix D.\nGiven the Gaussian sample distribution, we hope to choose some region in S \u2282 Rd with high probability measure and effective Minkowski dimension p < d. Then we can only apply neural networks to approximate f \u2217 within each cube of the small covering of S and thereby significantly reduce the network size. In literature, it is common to truncate the ambient space within a hyperellipsoid for Gaussian distribution (Ellis and Maitra, 2007; Pakman and Paninski, 2014). Similarly, we consider the \u2019thick\u2019 low-dimensional hyper-ellipsoid S(R,r;p) defined in (3). Then we construct a minimal cover of S(R,r;p) as a union of nonoverlapping hypercubes with side length r, which is equal to the thickness of S(R,r;p). In particular, this cover contains multiple layers of hypercubes to cover the first p dimensions of S(R,r;p) while only needs one layer for the rest dimensions. Intuitively, we only learn about hypercubes that cover the first p dimensions without paying extra effort to study dimensions with low probability density.\nA natural question arising from the construction of S(R,r;p) is how to select a proper dimension p. To address this problem, we first notice that each side length of the p-dimension hyper-ellipsoid is supposed to be greater than the side length of hypercubes r, i.e. 2\u03bbiR \u2265 r for i = 1, . . . ,p, so that we would not waste hypercubes to cover dimensions with too small side length. For simplicity of calculation, we choose p > 0 that satisfies \u03bb\u22121p = 2R/r for any given R,r > 0.\nNow we come to prove Lemma 1. Firstly, we compute the probability outside S(R,r;p). By union bound, this probability can be upper bounded by two parts, the probability out of hyperellipsoid for the first p dimensions and the probability out of hypercube with side length r for the rest d\u2212p dimensions. The first part is equal to the tail bound of p-dimensional standard Gaussian by the construction of S(R,r;p). The second part can be solved similarly by linearly transforming each dimension to be standard Gaussian.\nThen we calculate the covering number of S(R,r;p). Notice that the first p dimensions of S(R,r;p) is contained in a p-dimensional hyper-rectangle with side length 2\u03bbiR for i = 1, . . . ,p, while only one hypercube is required to cover the j-th dimension for j = p+1, . . . ,d. Therefore, the r-covering number can be upper bounded by \u220fp i=1(2\u03bbiR)/r p."
        },
        {
            "heading": "6 Discussion and Conclusion",
            "text": "In this paper, we have presented a generic approximation and generalization theory and applied it to Gaussian Random Design. Furthermore, our theory is applicable to scenarios where data are sampled from a mixture of distributions, denoted as Pdata = \u2211 iwiPi . Each distribution Pi is assigned a weight wi and has a low-dimensional support. In such cases, we focus on a subset of distributions with significant weights, while neglecting distributions with small weights. Then the effective Minkowski dimension depends on the data support of the selected distributions. As the sample size grows, including more distributions becomes necessary to achieve higher estimation accuracy. Another example where our theory can be applied is the case of an approximate manifold, where the data are concentrated on a low-dimensional manifold. In this scenario, the effective Minkowski dimension corresponds to the intrinsic dimension of the manifold.\nTo illustrate the concept of effective dimension in real-world examples, we refer to a study by Pope et al. (2021), which investigates the intrinsic dimension of several popular benchmark datasets for deep learning. The intrinsic dimension presented in Pope et al. (2021) can be seen as an approximate estimate of the Minkowski dimension, as demonstrated in Levina and Bickel (2004); Grassberger and Procaccia (1983). In our work, we adopt the methodology employed by Pope et al. (2021) and utilize generative adversarial networks trained on the ImageNet dataset to generate samples containing varying numbers of daisy images. To estimate the intrinsic dimension of these generated samples, we employ the Maximum Likelihood Estimation (MLE) method, which is achieved by computing the Euclidean distances between each data point and its k nearest neighbors. The obtained results are presented in Figure 1, which clearly demonstrates that the intrinsic dimension estimated from a finite sample of images increases as the sample size grows. This finding aligns with our theory that the effective Minkowski dimension is an increasing function of the sample size.\nIn conclusion, this paper studies nonparametric regression of functions supported in Rd under data distribution with effective Minkowski dimension p < d, using deep neural networks. Our results show that the L2 error for the estimation of f \u2217 \u2208 H(\u03b2,Ddata) converges in the order of n\u22122\u03b2/(2\u03b2+p). To obtain an \u03f5-error for the estimation of f \u2217, the sample complexity scales in the order of \u03f5\u2212(2\u03b2+p)/\u03b2 , which demonstrates that deep neural networks can capture the effective Minkowski dimension p of data distribution. Such results can be viewed as theoretical justifications for the empirical success of deep learning in various real-world applications where data are approximately concentrated on a low-dimensional set."
        },
        {
            "heading": "A Proof of Theorem 1",
            "text": "In this section, we provide the omitted proof in Section 5.1.\nA.1 Lemma 7\nLemma 7 (Lemma 20 in Nakada and Imaizumi (2020)). Let C = {Ik} Nr (S) k=1 be a minimum r-covering of S where Ik\u2019s are hypercubes with side length r. Then, there exists a disjoint partition {Cj}5 d j=1 \u2282 C\nsuch that C = \u22c35d\nj=1Cj and d(Ii , Il) \u2265 r hold for any Ii , Il \u2208 Cj if card(Cj ) \u2265 2, where d(A,B) := inf{ \u2225\u2225\u2225x \u2212 y\u2225\u2225\u2225 |x \u2208 A, y \u2208 B} is defined as the distance of any two sets A and B.\nA.2 Realization of hypercube determination function gind,rI Hypercube determination function g ind,rI can be realized by weight matrices and intercepts (4,0)\u2299 (W 2,\u2212d)\u2299 [(W 11 ,b 1 1), . . . , (W 1 d ,b 1 d)] where W 1 i ,b 1 i and W 2 are defined by\nW 1i := e\u22a4i e\u22a4i e\u22a4i e\u22a4i0 0 0 0 \u22a4 , b1i := (\u2212\u03b9i + r \u2212 \u03b9i + r2 \u2212 \u03b9i \u2212 r2 \u2212 \u03b9i \u2212 r ) ,\nand\nW 2 = (\n2 r ,\u22122 r ,\u22122 r , 2 r , 2 r ,\u22122 r ,\u22122 r , 2 r , . . . , 2 r ,\u22122 r ,\u22122 r , 2 r\ufe38 \ufe37\ufe37 \ufe38\n4d\n, 1 4\n) .\nThe above realization gives exactly the form in (4). Moreover, we summarize the properties of g ind,rI as following:\nProposition 1. For any x \u2208 Ddata and y \u2208R, we have\ng ind,rI (x,y)  = y, x \u2208 I and y \u2208 [0,4], \u2264 y, x \u2208 I \u2295 r2 and y \u2208 [0,4],\n= 0, otherwise.\nFurthermore, we obtain the following properties\n1. L(g ind,rI ) = 3,\n2. B(g ind,rI ) \u2264max{4,d,1 + r,2/r},\n3. K(g ind,rI ) = 24d + 6.\nA.3 Bounding the approximation error\nFirstly, we compute the approximation error of using gf0\u03f5 to estimate f0 = f \u2217 + 2. Recall that we defined gf0\u03f5 := gmax,5\nd \u25e6 gsum \u25e6 gsimul\u03f5/2 . When x \u2208 S, there exists some I \u2208 C such that x \u2208 I . Then for this x, we have\ng f0 \u03f5 (x) = max\nj\u2208[5d ] \u2211 Ik\u2208Cj gsimul\u03f5/2,k (x) = maxIk\u2208Neig(I) gsimul\u03f5/2,k (x) \u2264 maxIk\u2208Neig(I) g poly \u03f5/2,k(x),\nwhere Neig(I) = {I \u2032 \u2208 C|(I \u2295 3r/2)\u2229 I \u2032 , \u2205} denotes the 3r/2-neighborhood of hypercube I . In other words, when computing gf0\u03f5 (x), we only need to take maximum over the estimated function value within hypercubes near x. The second equality follows from the Proposition 1 that gsimul\u03f5/2,l (x) = 0 for Il < Neig(I) and d(Il , Ik) > r holds for Il , Ik \u2208 Ci for all i. The last inequality is due to the construction of gsimul\u03f5/2 in (6).\nGiven \u03f5 \u2208 (0,1), we ensure 0 \u2264 gsimul\u03f5/2,k (x) \u2264 4 for all Ik \u2208 C by Proposition 1, since g simul \u03f5/2,k approximates fIk which is an \u03f5/2-accuracy Taylor polynomial of f0 \u2208 [1,3]. When x \u2208 It, the error is bounded as\n|gf0\u03f5 (x)\u2212 f0(x)| =max {\nmax Ik\u2208Neig(It) gsimul\u03f5/2,k (x)\u2212 f0(x), f0(x)\u2212 maxIk\u2208Neig(It) gsimul\u03f5/2,k (x) } \u2264max { max\nIk\u2208Neig(It) g\npoly \u03f5/2,k(x)\u2212 f0(x), f0(x)\u2212 g poly \u03f5/2,t(x) } \u2264 max\nIk\u2208Neig(It) \u2223\u2223\u2223\u2223gpoly\u03f5/2,k(x)\u2212 f0(x)\u2223\u2223\u2223\u2223 \u2264 max\nIk\u2208Neig(It) \u2223\u2223\u2223\u2223gpoly\u03f5/2,k(x)\u2212 fIk (x)\u2223\u2223\u2223\u2223+ maxIk\u2208Neig(It)\u2223\u2223\u2223fIk (x)\u2212 f0(x)\u2223\u2223\u2223 \u2264\u03b5\n2 + d\u03b2 ( 3r 2 )\u03b2 \u2264 \u03f5,\nwhere the last inequality follows from (5) and Lemma 2. In terms of parameter tuning, we choose r = d\u22121\u03f51/\u03b2/2.\nNext, we extend approximation results of f0 to f \u2217. To do so, we firstly implement a neural network gmod(z) = (\u2212z + 1) \u25e6ReLU(\u2212z + 2) \u25e6ReLU(z \u2212 1), which has the equivalent form gmod(z) = min(max(1,x),3)\u2212 2 for any z \u2208R. In addition, gmod has the following properties:\nL(gmod) = 3, B(gmod) \u2264 2, and K(gmod) = 12.\nThen consider gf \u2217 \u03f5 = gmod \u25e6 g f0 \u03f5 to obtain the desired approximation error \u03f5 for any x \u2208 S\nsup x\u2208Ddata\n|gf \u2217\n\u03f5 (x)\u2212 f \u2217(x)| = sup x\u2208Ddata \u2223\u2223\u2223min(max(1, gf0\u03f5 (x)),3)\u2212 (f \u2217(x) + 2)\u2223\u2223\u2223 = sup\nx\u2208Ddata \u2223\u2223\u2223min(max(1, gf0\u03f5 (x)),3)\u2212 f0(x)\u2223\u2223\u2223 \u2264 sup\nx\u2208Ddata \u2223\u2223\u2223gf0\u03f5 (x)\u2212 f0(x)\u2223\u2223\u2223 \u2264\u03f5.\nA.4 Computing network sizes Recall that the ReLU neural network gf \u2217\n\u03f5 is defined as\ng f \u2217 \u03f5 =gmod \u25e6 gmax,5 d \u25e6 gsum \u25e6 gsimul\u03f5/2\n=gmod \u25e6 gmax,5 d \u25e6 gsum \u25e6 ( g ind,rI1 \u25e6 g filter 1 , . . . , g ind,r INr (S) \u25e6 gfilterNr (S) ) \u25e6 (gIdd,L, g poly \u03f5/2 ).\nNote that Nr(S) \u2264 c0r\u2212p. Combined with sub-neural network structures given in Appendix B.1.1 of Nakada and Imaizumi (2020), gf \u2217\n\u03f5 has the following properties:\nL(gf \u2217 \u03f5 (x)) = L(gmod) +L(gmax,5 d ) +L(g ind,rI1 ) +L(g filter 1 ) +L(g poly \u03f5/2 )\n\u2264 11 + 2d log2 5 + (11 + (1 + \u03b2)/d)(2 + log2\u03b2),\nB(gf \u2217 \u03f5 (x)) \u2264max { B(gmod),B(gmax,5\nd ),B(g ind,rI1 ),B(g filter 1 ),B(g Id d,L),B(g poly \u03f5/2 )\n} ,\n\u2264max{d,1 + r,2/r, cpoly1 R \u03b2s\npoly 1\nS \u03f5 \u2212spoly1 },\n\u2264max{4d\u03f5\u22121/\u03b2 , cpoly1 R \u03b2s\npoly 1\nS \u03f5 \u2212spoly1 },\nK(gf \u2217 \u03f5 (x)) \u2264 2K(gmod) + 2K(gmax,5 d ) + 2Nr(S) \u00b7K(g ind,r I1 \u25e6 gfilter1 ) + 2K(g Id d,L) + 2K(g poly \u03f5/2 )\n\u2264 2cpoly2 R p S\u03f5 \u2212p/\u03b2 + 2(50d + 17 + cpoly2 )Nr(S)\n+ 2(12 + 42\u00d7 5d + 2d + 2d(11 + (1 + \u03b2)/p)(2 + log2\u03b2))),\n\u2264 2cpoly2 R p S\u03f5 \u2212p/\u03b2 + 2(50d + 17 + cpoly2 )c0(2d) p\u03f5\u2212p/\u03b2\n+ 2(12 + 42\u00d7 5d + 2d + 2d(11 + (1 + \u03b2)/p)(2 + log2\u03b2))),\nwhere cpoly2 = O(d 2+\u230a\u03b2\u230b). By adjusting several constants, we obtain the statement."
        },
        {
            "heading": "B Proof of Theorem 2",
            "text": "The proof of Theorem 2 mainly follows Chen et al. (2022). Our Lemma 4 and 5 is a slight revision of Lemma 5 and 6 in Chen et al. (2022), where we substitute \u2113\u221e covering number with \u21132 covering number to deal with unbounded domain. In this section, we compute the \u21132 covering number of neural network class and then present the proof of Theorem 2.\nB.1 Proof of Lemma 6\nProof of Lemma 6. To construct a covering for F (H,L,B,K), we discretize each parameter by a unit grid with grid size h. Recall that we write f \u2208 F (H,L,B,K) as f (x) = WL \u00b7ReLU(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+bL\u22121)+bL in (2). Choose any f , f \u2032 \u2208 F (H,L,B,K) with parameters at most h apart from each other. Denote the weight matrices and intercepts in f , f \u2032 as WL, . . . ,W1,bL, . . . ,L1 and W \u2032L, . . . ,W \u2032 1,b \u2032 L, . . . ,L \u2032 1 respectively, where Wl \u2208Rdl\u00d7dl\u22121 and bl \u2208Rdl for l = 1, . . . ,L. Without loss of generality, we assume dl \u2264 K since all the parameters have at most K nonzero entries. If the input dimension is larger than K , we let the redundant dimensions of input equal to zeros.\nNotice that for any random variable y \u2208Rdl\u22121 which is subject to a distribution PY , we have\u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(Wly + bl)\u2212 (W \u2032l y + b\u2032l)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y) = \u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 dl\u22121\u2211 i=1 (Wl,i \u2212W \u2032l,i)yi + (bl \u2212 b \u2032 l) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y).\nBy the inequality \u2225t + s\u22252 \u2264 2\u2225t\u22252 + 2\u2225s\u22252 which holds for any s, t \u2208Rdl , we obtain\u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(Wly + bl)\u2212 (W \u2032l y + b\u2032l)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y) \u22642 \u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 dl\u22121\u2211 i=1 (Wl,i \u2212W \u2032l,i)yi \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 dPY (y) + 2\u2225\u2225\u2225bl \u2212 b\u2032l\u2225\u2225\u222522\n\u22642 sup i=1...,dl\u22121 \u2225\u2225\u2225Wl,i \u2212W \u2032l,i\u2225\u2225\u222522 \u00b7\u222b Rdl\u22121 dl\u22121\u2211 i=1 y2i dPY (y) + 2 \u2225\u2225\u2225bl \u2212 b\u2032l\u2225\u2225\u222522 .\nSince parameters Wl ,bl differ at most h from W \u2032l ,b \u2032 l with respect to each entry, we get\u222b\nRdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(Wly + bl)\u2212 (W \u2032l y + b\u2032l)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y) \u2264 2dl\u22121h2 \u2225\u2225\u2225y\u2225\u2225\u22252 L2(PY ) + 2dlh 2\n\u2264 2Kh2 \u2225\u2225\u2225y\u2225\u2225\u22252\nL2(PY ) + 2Kh2. (7)\nSimilarly, we have\u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223Wly + bl \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y) = \u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 dl\u22121\u2211 i=1 Wl,iyi + bl \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 dPY (y)\n\u22642 \u222b Rdl\u22121 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 dl\u22121\u2211 i=1 Wl,iyi \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 dPY (y) + 2\u2225bl\u222522 \u22642 sup\ni=1...,dl\u22121\n\u2225\u2225\u2225Wl,i\u2225\u2225\u222522 \u00b7\u222b Rdl\u22121 dl\u22121\u2211 i=1 y2i dPY (y) + 2\u2225bl\u2225 2 2\n\u22642dl\u22121B2 \u2225\u2225\u2225y\u2225\u2225\u22252\nL2(PY ) + 2dlB\n2\n\u22642KB2 \u2225\u2225\u2225y\u2225\u2225\u22252\nL2(PY ) + 2KB2. (8)\nSince the ReLU actiavtion function is 1-Lipschitz continuous for each coordinate, we can apply (7) and (8) repeatedly to bound \u2225f \u2212 f \u2032\u22252L2(Pdata,S):\u2225\u2225\u2225f \u2212 f \u2032\u2225\u2225\u22252\nL2(Pdata,S) = \u222b S \u2223\u2223\u2223WL \u00b7ReLU(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) + bL \u2212W \u2032L \u00b7ReLU(W \u2032 L\u22121 \u00b7 \u00b7 \u00b7ReLU(W \u2032 1x+ b \u2032 1) \u00b7 \u00b7 \u00b7+ b \u2032 L\u22121)\u2212 b \u2032 L\n\u2223\u2223\u22232 dPdata(x) \u22642\n\u222b S \u2223\u2223\u2223WL \u00b7ReLU(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) + bL \u2212W \u2032L \u00b7ReLU(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121)\u2212 b \u2032 L\n\u2223\u2223\u22232 dPdata(x) + 2\n\u2225\u2225\u2225W \u2032L\u2225\u2225\u222522 \u222b S \u2225\u2225\u2225(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) \u2212 (W \u2032L\u22121 \u00b7 \u00b7 \u00b7ReLU(W \u2032 1x+ b \u2032 1) \u00b7 \u00b7 \u00b7+ b \u2032 L\u22121) \u2225\u2225\u22252 2 dPdata(x)\n\u22644Kh2 + 4Kh2 \u222b S \u2225WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121\u22252 dPdata(x)\n+ 2KB2 \u222b S \u2225\u2225\u2225(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) \u2212 (W \u2032L\u22121 \u00b7 \u00b7 \u00b7ReLU(W \u2032 1x+ b \u2032 1) \u00b7 \u00b7 \u00b7+ b \u2032 L\u22121) \u2225\u2225\u22252 2 dPdata(x).\nBesides, we derive the following bound on \u2225WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121\u2225L2(Pdata,S):\u222b S \u2225WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121\u22252 dPdata(x)\n\u2264 2KB2 \u222b S \u2225WL\u22122 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22122\u22252 dPdata(x) + 2KB2 \u2264 (2KB2)L\u22121dR2S + (2KB 2)L\u22121 \u2264 2L(KB2)L\u22121dR2S ,\nwhere the last inequality is derived by induction and \u2225x\u22252 = \u2211d\ni=1 x 2 i \u2264 dR 2 S for any x \u2208 S. Substi-\ntuting back into the bound for \u2225f \u2212 f \u2032\u22252L2(Pdata,S) , we obtain\u2225\u2225\u2225f \u2212 f \u2032\u2225\u2225\u22252 L2(Pdata,S) \u22644Kh2 + 2L+2KLB2(L\u22121)h2dR2S\n+ 4KB2 \u222b S \u2225\u2225\u2225(WL\u22121 \u00b7 \u00b7 \u00b7ReLU(W1x+ b1) \u00b7 \u00b7 \u00b7+ bL\u22121) \u2212 (W \u2032L\u22121 \u00b7 \u00b7 \u00b7ReLU(W \u2032 1x+ b \u2032 1) \u00b7 \u00b7 \u00b7+ b \u2032 L\u22121) \u2225\u2225\u22252 2 dPdata(x)\n\u22644(L\u2212 1)(KB2)L\u22121h2 + 2L+2(L\u2212 1)KLB2(L\u22121)h2dR2S + (2KB2)L\u22121 \u222b S \u2225\u2225\u2225W1x+ b1 \u2212W \u20321x \u2212 b\u20321\u2225\u2225\u222522 dPdata(x) \u22644L\u22121LKLB2Lh2dR2S ,\nwhere the second inequality is obtained by induction. Therefore, combining the above inequality with \u2225f \u2225\u221e \u2264 1 for any f \u2208 F (L,B,K) and Pdata(Sc) \u2264 \u03c4 , we get\u2225\u2225\u2225f \u2212 f \u2032\u2225\u2225\u22252\nL2(Pdata) = \u222b S |f (x)\u2212 f \u2032(x)|2 dPdata(x) + \u222b Sc |f (x)\u2212 f \u2032(x)|2 dPdata(x)\n\u22644L\u22121LKLB2L\u22122h2dR2S + 4\u03c4. Now we choose h satisfying h = \u221a\n(\u03b42 \u2212 4\u03c4)/(4L\u22121LKLB2L\u22122dR2S ). Then discretizing each parameter uniformly into 2B/h grid points yields a \u03b4-covering on F (L,B,K). Moreover, the covering number N2(\u03b4,F (L,B,K)) satisfies\nlogN2(\u03b4,F (L,B,K)) \u2264 K log (\n2B h\n) = K log ( 2L \u221a dLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4 ) .\nB.2 Proof of Theorem 2\nProof of Theorem 2. The square error of the estimator f\u0302 can be decomposed into a squared bias term and a variance term, which can be bounded using the covering number of the function class. According to Lemmas 4 and 5, for any constant \u03b4 \u2208 (0,1), we have\nE\u2225f\u0302 \u2212 f \u2217\u22252L2(Pdata) \u2264c inff \u2208F (L,B,K) \u2225f (x)\u2212 f \u2217(x)\u22252L2(Pdata) + c\u03c3 2 logN2(\u03b4,F (L,B,K)) + 2 n\n+ c (\u221a\nlogN2(\u03b4,F (L,B,K)) + 2 n\n+ 1 ) \u03c3\u03b4+ c\u2032\n3n logN2(\u03b4/4H,F (L,B,K)) + c\u2032\u03b4.\n(9)\nChoose \u03f5 = (2dr)\u03b2 in Theorem 1. Accordingly, we set tuple (L,B,K) as\nL = C1, B = O(R \u03b2s S r \u2212s\u03b2), and K = C2R p Sr \u2212p.\nThen we have inf\nf \u2208F (L,B,K) \u2225f (x)\u2212 f \u2217(x)\u22252L2(Pdata) \u2264 (2dr) 2\u03b2 + 4\u03c4.\nInvoking the upper bound of the covering number in Lemma 6, we derive\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) \u2264(2dr)2\u03b2 + 4\u03c4 + c\u03c3\n2\nn\n( K log ( 2L \u221a dLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4 ) + 2c )\n+ c\n\u221a K log(2L \u221a dLKL/2BLRS / \u221a \u03b42 \u2212 4\u03c4) + 1\nn \u03c3\u03b4\n+ c\u2032\n3n K log ( 2L \u221a dLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4 ) + (c\u03c3 + c\u2032)\u03b4\n=O ( \u03c4 + d2\u03b2r2\u03b2 +\n\u03c32K n\nlog (\u221a\ndLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4 ) + ( K log( \u221a dLKL/2BLRS / \u221a \u03b42 \u2212 4\u03c4)\nn\n)1/2 \u03c3\u03b4+ \u03c3\u03b4+\n1 n\n) .\nBy Cauchy-Schwartz inequality, for 0 < \u03b4 < 1, we have\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) =O\n( \u03c4 + d2\u03b2r2\u03b2 +\n\u03c32K n\nlog (\u221a\ndLKL/2BLRS\u221a \u03b42 \u2212 4\u03c4\n) + \u03c3\u03b4+\n1 n\n) .\nPlugging in our choice of (L,B,K), we get\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) =O\n( \u03c4 + d2\u03b2r2\u03b2 +\n\u03c32d n ( RS r )p log ( (RS /r)p \u03b42 \u2212 4\u03c4 ) + \u03c3\u03b4+ 1 n ) Now we choose \u03b4 = r2\u03b2 . Then we deduce the desired estimation error bound\nE\u2225f\u0302 \u2212 f \u2217\u22252L2(Pdata) = O ( \u03c4 + \u03c3r2\u03b2 + \u03c32\nn ( RS r )p log ( (RS /r)p r4\u03b2 \u2212 4\u03c4 ) + 1 n ) = O ( \u03c4 + \u03c3r2\u03b2 + \u03c32\nn ( RS r )p log ( (RS /r)p r4\u03b2 \u2212 4\u03c4 )) .\nThe last equality is due to RS /r > 1."
        },
        {
            "heading": "C Proof of Lemma 1",
            "text": "Proof of Lemma 1. For simplicity of proof, set Q = I . By the construction of S(R,r;p) in (3), we notice that\nP(X < S(R,r;p)) \u2264P ( p\u2211 i=1 x2i \u03bb2i > R2 ) +P ( |xj | > r 2 for some j \u2208 [p+ 1,d] ) .\nSince X1:p = (x1, . . . ,xp) \u223cN (0,\u039bp) where \u039bp = diag(\u03bb21, . . . ,\u03bb2p), the variable Z = \u039b \u22121/2 p X1:p \u223cN (0, Ip) is a standard Gaussian. Then for any fixed R > 0, the probability P( \u2211p\ni=1 x 2 i /\u03bb 2 i > R 2) is equal to P(\u2225Z\u22252 > R2). Moreover, by Lemma 9, if we choose R2 > p, we will have\nP ( \u2225Z\u22252 > R2 ) \u2264 (\n2R2 + p p\n) p 2 exp ( \u2212 R 4\n2R2 + p ) =exp ( \u2212 p\n2 \u00b7\nR4/p2\nR2/p+ 1/2 + p 2 \u00b7 log\n( 2R2\np + 1 )) =O [exp(\u2212 2R23p + log ( 3R2 p ))] p 2 .\nBesides, by Lemma 8, for j = p+ 1, . . . ,d, we derive\nP ( |xj | >\nr 2\n) = P (\u2223\u2223\u2223\u2223\u2223 xj\u03bbj \u2223\u2223\u2223\u2223\u2223 > r2\u03bbi ) = O ( exp { \u2212 r 2\n8\u03bb2j\n}) .\nThen we can apply the union bound of probability to get\nP ( |xj | >\nr 2\nfor some j \u2208 [p+ 1,d] ) \u2264 d\u2211 j=p+1 P ( |xj | > r 2 ) = O ( d\u2211 j=p+1 exp ( \u2212 r 2 8\u03bb2j )) .\nRecall that we choose \u03bb\u22121p = 2R/r. Then we have\nP ( |xj | >\nr 2\nfor some j \u2208 [p+ 1,d] ) =O ( d\u2211 j=p+1 exp ( \u2212 \u03bb2p 2\u03bb2j R2 ))\n\u2264O ( d\u2211 j=p+1 exp ( \u2212 R 2 2 ))\n=O ( exp ( \u2212 R 2\n2 + log(d \u2212 p)\n)) ,\nwhere the inequality comes from \u03bb2j \u2264 \u03bb 2 p for j = p+ 1, . . . ,d. Therefore, we have\nP(X < S(R,r;p)) = O [exp(\u2212 2R23p + log ( 3R2 p ))] p 2 +O(exp(\u2212 R22 + log(d \u2212 p) )) .\nNext, we compute the covering number of S(R,r;p) using hypercubes with side length r > 0, which is denoted as Nr(S(R,r;p)). Notice that the first-p-dimensional hyper-ellipsoid of S(R,r;p) is contained in a p-dimensional hyper-rectangle with side length 2\u03bbiR for i = 1, . . . ,p, while only one hypercube is required to cover the j-th dimension for j = p + 1, . . . ,d. With this observation, we derive the upper bound for Nr(S(R,r;p)):\nNr(S(R,r;p)) \u2264 p\u220f\ni=1\n( 2\u03bbiR r ) = p\u220f i=1 ( \u03bbi \u03bbp ) ,\nwhere the last equality results from our choice of p."
        },
        {
            "heading": "D Proof of Theorem 3",
            "text": "D.1 Generalization error under exponential eigenvalue decay\nCombining the criteria \u03bb\u22121p = 2R/r and the exponential eigenvalue decay in Assumption 3, we have\n1 \u00b5 exp(\u03b8p) = 2R r .\nMoreover, by Lemma 1, we can compute the covering number of S(R,r,p):\nNr(S(R,r;p)) \u2264 p\u220f\ni=1\n( \u03bbi \u03bbp ) = p\u220f i=1 exp(\u03b8(p \u2212 i)) \u2264 exp ( \u03b8p2 2 ) = ( 2\u00b5R r )p/2 ,\nwhich indicates that effective Minkowski dimension of Pdata is at most p/2. Let r = n\u2212(1\u2212\u03b7)/(2\u03b2+ \u221a logn/\u03b8) and R = logn where \u03b7 \u2208 (0,1) is an arbitrarily small constant. Then we obtain\n\u03b8p = log (\n2\u00b5R r\n) =\n1 \u03b8 log(2\u00b5) + log(logn) + (1\u2212 \u03b7) logn 2\u03b2 + \u221a logn/\u03b8 \u2264 2(1\u2212 \u03b7) logn 2\u03b2 + \u221a logn/\u03b8 .\nThereby, we can compute the probability ourside S(R,r;p):\nP(X < S(R,r;p)) =O [exp(\u2212 2R23p + log ( 3R2 p ))] p 2 + exp ( \u2212R 2 2 + log(d \u2212 p) ) = O(n\u2212 logn/3). Apply Theorem 2 with our choice of R, r and p. Accordingly, the tuple (L,B,K) is set as\nL = C1, B = O ( n\n\u03b2s\n2\u03b2+ \u221a logn/\u03b8 (logn)\u03b2s ) , and K = O ( n \u221a logn/\u03b8 2\u03b2+ \u221a logn/\u03b8 ) .\nThen we can get\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( P(X < S(R,r;p)) + \u03c3r2\u03b2 + \u03c32 n \u00b7 ( RS r )p/2 log ( (RS /r)p/2 r4\u03b2 \u2212 4\u03c4 )) = O ( \u03c3n \u2212 2\u03b2(1\u2212\u03b7) 2\u03b2+ \u221a logn/\u03b8 + \u03c32\nn \u00b7n\n(1\u2212\u03b7)2 logn/\u03b8 (2\u03b2+ \u221a logn/\u03b8)2 \u00b7 (logn) (1\u2212\u03b7) logn/\u03b8 2\u03b2+ \u221a logn/\u03b8 \u00b7 logn/\u03b8 2\u03b2 + \u221a logn/\u03b8 \u00b7 logn ) = O ( \u03c3n \u2212 2\u03b2(1\u2212\u03b7) 2\u03b2+ \u221a logn/\u03b8 + \u03c32n \u22121+ (1\u2212\u03b7) 2 logn/\u03b8 (2\u03b2+ \u221a logn/\u03b8)2 + (1\u2212\u03b7) log(logn)/\u03b8 2\u03b2+ \u221a logn/\u03b8 \u00b7 (logn)3/2 ) .\nThe last equality utilizes the fact that (logn)logn = nlog(logn). Furthermore, notice that for sufficiently large n satisfying log(logn)/ \u221a \u03b8 logn \u2264 \u03b7, we have\n(1\u2212 \u03b7)2 logn/\u03b8 (2\u03b2 + \u221a logn/\u03b8)2 + (1\u2212 \u03b7) log(logn)/\u03b8 2\u03b2 + \u221a logn/\u03b8 \u2264 (1\u2212 \u03b7)2 logn/\u03b8 (2\u03b2 + \u221a logn/\u03b8)2 + (1\u2212 \u03b7)\u03b7\n\u221a logn/\u03b8\n2\u03b2 + \u221a logn/\u03b8\n\u2264 (1\u2212 \u03b7)2\n\u221a logn/\u03b8\n2\u03b2 + \u221a logn/\u03b8 +\n(1\u2212 \u03b7)\u03b7 \u221a logn/\u03b8\n2\u03b2 + \u221a logn/\u03b8\n\u2264 (1\u2212 \u03b7)\n\u221a logn/\u03b8\n2\u03b2 + \u221a logn/\u03b8 .\nTherefore, we use the above observation to derive the following upper bound for generalization error:\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c32n \u2212 2\u03b2(1\u2212\u03b7) 2\u03b2+ \u221a logn/\u03b8 (logn)3/2 ) .\nD.2 Generalization error under polynomial eigenvalue decay\nSimilarly to last section, we firstly combine the criteria \u03bb\u22121p = 2R/r and the polynomial eigenvalue decay in Assumption 4,\np = (\n2\u03c1R r\n)1/\u03c9 .\nMoreover, by Lemma 1, we can compute the covering number of S(R,r,p):\nNr(S(R,r;p)) \u2264 p\u220f\ni=1\n( \u03bbi \u03bbp ) = p\u220f i=1 ( p i )\u03c9 = ( pp p! )\u03c9 \u2264 ( pp pp/2 )\u03c9 = p\u03c9p/2 = ( 2\u03c1R r )p/2 ,\nwhich indicates that effective Minkowski dimension of Pdata is at most p/2. Let r = n\u22121/(2\u03b2+n \u03ba) and R = n1/(2\u03c9\u03b2+\u03c9n \u03ba) with \u03ba = (1 + 1/\u03c9)/\u03c9. Then we obtain\np = (\n2\u03c1R r\n)1/\u03c9 = n (1+1/\u03c9)/\u03c9 2\u03b2+n\u03ba = n \u03ba 2\u03b2+n\u03ba .\nThereby, we can compute the probability outside S(R,r;p):\nP(X < S(R,r;p)) =O [exp(\u2212 2R23p + log ( 3R2 p ))] p 2 + exp ( \u2212R 2 2 + log(d \u2212 p) ) = O(exp(\u2212n 2(2\u03c9\u03b2+\u03c9n\u03ba ) /3)). Apply Theorem 2 with our choice of R, r and p. Accordingly, the tuple (L,B,K) is set as\nL = C1, B = O ( n (1+1/\u03c9)\u03b2s 2\u03b2+n\u03ba ) , and K = O ( n (1+1/\u03c9)n\u03ba/(2\u03b2+n \u03ba ) 4\u03b2+2n\u03ba ) .\nThen we have E \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( P(X < S(R,r;p)) + \u03c3r2\u03b2 + \u03c32 n \u00b7 ( RS r )p/2 log ( (RS /r)p/2 r4\u03b2 \u2212 4\u03c4 )) = O ( \u03c3n \u2212 2\u03b22\u03b2+n\u03ba + \u03c32\nn \u00b7n\n1+1/\u03c9 2\u03b2+n\u03ba \u00b7 1 2n\n\u03ba 2\u03b2+n\u03ba\n\u00b7n \u03ba 2\u03b2+n\u03ba logn )\n= O ( \u03c3n \u2212 2\u03b22\u03b2+n\u03ba + \u03c32n\u22121+ 1+1/\u03c9 2\u03b2+n\u03ba \u00b7 1 2n \u03ba 2\u03b2+n\u03ba + \u03ba2\u03b2+n\u03ba logn ) .\n(10)\nNotice that\n1 + 1/\u03c9 2\u03b2 +n\u03ba \u00b7 1 2 n \u03ba 2\u03b2+n\u03ba + \u03ba 2\u03b2 +n\u03ba = 1 + 1/\u03c9 2\u03b2 +n\u03ba ( 1 2 n \u03ba 2\u03b2+n\u03ba + 1 \u03c9 ) \u2264 2\n2\u03b2 +n\u03ba ( 1 2 n \u03ba 2\u03b2+n\u03ba + 1 ) \u2264 n \u03ba\n2\u03b2 +n\u03ba ,\nwhere the first inequality is due to \u03c9 > 1. Therefore, plug the above inequality in (10), we derive the following upper bound for generalization error:\nE \u2225\u2225\u2225\u2225f\u0302 \u2212 f \u2217\u2225\u2225\u2225\u22252\nL2(Pdata) = O\n( \u03c32n \u2212 2\u03b22\u03b2+n\u03ba logn ) ."
        },
        {
            "heading": "E Auxiliary Lemmas",
            "text": "In this section, we investigate the probability tail bound of standard Gaussian variable, which is useful for the proof of Lemma 1. At first, we compute the tail bound for multivariate Gaussian variable.\nLemma 8. Suppose Z = (z1, . . . , zp) \u223c N (0, Ip) is a standard Gaussian variable in Rp. Then for any t > 0, we have\nP ( \u2225Z\u2225 > t ) \u2264 ( 2t2 + p\np\n) p 2 exp ( \u2212 t 4\n2t2 + p\n) .\nProof. By the Markov\u2019s inequality, for any \u00b5 \u2208 (0,1/2), we have P ( \u2225Z\u2225 > t ) =P ( exp(\u00b5\u2225Z\u22252) > exp(\u00b5t2) ) \u2264 Eexp(\u00b5\u2225Z\u22252)\nexp(\u00b5t2)\n=\n\u220fp i=1Eexp(\u00b5z 2 i )\nexp(\u00b5t2) ,\nwhere the last equality comes from the independence of zi \u2019s. To bound Eexp(\u00b5z 2 i ), we first examine the moment generating function of zi : for any t \u2208R,\nEexp(tzi) = \u222b R exp(tw)\u03c6(w)dw = exp(t2/2),\nwhere \u03c6(w) = (2\u03c0)\u2212p/2 exp(\u2212w2/2) denotes the probability density function of stardard Gaussian. Then multiply exp(\u2212t2/(2\u00b5)) on both sides,\u222b\nR exp\n( tw \u2212 t 2\n2\u00b5\n) \u03c6(w)dw = exp ( t2(\u00b5\u2212 1)\n2\u00b5\n) .\nBy integrating both sides with respect to t, we have\n\u221a 2\u03c0\u00b5 \u222b R exp ( \u00b5w2 2 ) \u03c6(w)dw = \u221a 2\u03c0\u00b5 1\u2212\u00b5 ,\nwhich indicates\nEexp(\u00b5z2i ) = Eexp ( 2\u00b5z2i 2 ) = \u221a 1 1\u2212 2\u00b5 .\nTherefore, for any \u00b5 \u2208 (0,1/2), we have P ( \u2225Z\u2225 > t ) \u2264 (1\u2212 2\u00b5)\u2212 p 2 exp(\u2212\u00b5t2).\nLet \u00b5 = t2/(2t2 + p) and thereby we can conclude the proof of the lemma.\nFor standard Gaussian in R, we derive a tighter upper bound in the following lemma.\nLemma 9. Suppose z \u223cN (0,1) is a standard Gaussian variable in R. Then for any t > 0, we have\nP ( \u2225z\u2225 > t ) \u2264 exp ( \u2212 1\n2 t2\n) .\nProof. Firstly, for any t > 0, compute the probability that z > t:\nP(z > t) = \u222b \u221e t 1 \u221a 2\u03c0 exp ( \u2212 1 2 z2 ) dz\n= \u222b \u221e\n0\n1 \u221a\n2\u03c0 exp\n( \u2212 1\n2 (u + t)2\n) du\n=exp ( \u2212 1\n2 t2 )\u222b \u221e 0 exp(\u2212tu) \u00b7 1\u221a 2\u03c0 exp ( \u2212 1 2 u2 ) du\n\u2264exp ( \u2212 1\n2 t2 )\u222b \u221e 0 1 \u221a 2\u03c0 exp ( \u2212 1 2 u2 ) du\n= 1 2\nexp ( \u2212 1\n2 t2\n) .\nThen notice that\nP ( \u2225z\u2225 > t ) = P(z > t) +P(z < \u2212t) = 2P(z > t).\nThereby, we can conclude the proof."
        }
    ],
    "title": "Effective Minkowski Dimension of Deep Nonparametric Regression: Function Approximation and Statistical Theories",
    "year": 2023
}