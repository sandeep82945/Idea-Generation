{
    "abstractText": "3D spatial perception is the problem of building and maintaining an actionable and persistent representation of the environment in real-time using sensor data and prior knowledge. Despite the fast-paced progress in robot perception, most existing methods either build purely geometric maps (as in traditional SLAM) or \u201cflat\u201d metric-semantic maps that do not scale to large environments or large dictionaries of semantic labels. The first part of this paper is concerned with representations: we show that scalable representations for spatial perception need to be hierarchical in nature. Hierarchical representations are efficient to store, and lead to layered graphs with small treewidth, which enable provably efficient inference. We then introduce an example of hierarchical representation for indoor environments, namely a 3D scene graph, and discuss its structure and properties. The second part of the paper focuses on algorithms to incrementally construct a 3D scene graph as the robot explores the environment. Our algorithms combine 3D geometry (e.g., to cluster the free space into a graph of places), topology (to cluster the places into rooms), and geometric deep learning (e.g., to classify the type of rooms the robot is moving across). The third part of the paper focuses on algorithms to maintain and correct 3D scene graphs during long-term operation. We propose hierarchical descriptors for loop closure detection and describe how to correct a scene graph in response to loop closures, by solving a 3D scene graph optimization problem. We conclude the paper by combining the proposed perception algorithms into Hydra, a real-time spatial perception system that builds a 3D scene graph from visual-inertial data in real-time. We showcase Hydra\u2019s performance in photo-realistic simulations and real data collected by a Clearpath Jackal robots and a Unitree A1 robot. We release an open-source implementation of Hydra at https://github.com/MIT-SPARK/Hydra.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nathan Hughes"
        },
        {
            "affiliations": [],
            "name": "Yun Chang"
        },
        {
            "affiliations": [],
            "name": "Siyi Hu"
        },
        {
            "affiliations": [],
            "name": "Rajat Talak"
        },
        {
            "affiliations": [],
            "name": "Rumaisa Abdulhai"
        },
        {
            "affiliations": [],
            "name": "Jared Strader"
        },
        {
            "affiliations": [],
            "name": "Luca Carlone"
        }
    ],
    "id": "SP:ae4b8e3fb311ce5f446b0c69f7fa9961e7be3886",
    "references": [
        {
            "authors": [
                "A.J. Davison"
            ],
            "title": "FutureMapping: The computational structure of spatial AI systems",
            "venue": "arXiv preprint arXiv:1803.11288, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "A. Rosinol",
                "A. Violette",
                "M. Abate",
                "N. Hughes",
                "Y. Chang",
                "J. Shi",
                "A. Gupta",
                "L. Carlone"
            ],
            "title": "Kimera: from SLAM to spatial perception with 3D dynamic scene graphs",
            "venue": "Intl. J. of Robotics Research, vol. 40, no. 12\u201314, pp. 1510\u20131546, 2021, arXiv preprint: 2101.06894, (pdf).",
            "year": 2021
        },
        {
            "authors": [
                "I. Armeni",
                "Z. He",
                "J. Gwak",
                "A. Zamir",
                "M. Fischer",
                "J. Malik",
                "S. Savarese"
            ],
            "title": "3D scene graph: A structure for unified semantics, 3D space, and camera",
            "venue": "Intl. Conf. on Computer Vision (ICCV), 2019, pp. 5664\u20135673.",
            "year": 2019
        },
        {
            "authors": [
                "A. Rosinol",
                "A. Gupta",
                "M. Abate",
                "J. Shi",
                "L. Carlone"
            ],
            "title": "3D dynamic scene graphs: Actionable spatial perception with places, objects, and humans",
            "venue": "Robotics: Science and Systems (RSS), 2020, (pdf), (media), (video). [Online]. Available: http: //news.mit.edu/2020/robots-spatial-perception-0715",
            "year": 2020
        },
        {
            "authors": [
                "U. Kim",
                "J. Park",
                "T. Song",
                "J. Kim"
            ],
            "title": "3-D scene graph: A sparse and semantic representation of physical environments for intelligent agents",
            "venue": "IEEE Trans. Cybern., vol. PP, pp. 1\u201313, Aug. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wald",
                "H. Dhamo",
                "N. Navab",
                "F. Tombari"
            ],
            "title": "Learning 3D semantic scene graphs from 3D indoor reconstructions",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 3961\u20133970.",
            "year": 2020
        },
        {
            "authors": [
                "S. Wu",
                "J. Wald",
                "K. Tateno",
                "N. Navab",
                "F. Tombari"
            ],
            "title": "SceneGraphFusion: Incremental 3D scene graph prediction from RGB-D sequences",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Izatt",
                "R. Tedrake"
            ],
            "title": "Scene understanding and distribution modeling with mixed-integer scene parsing",
            "venue": "technical report (under review), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Gothoskar",
                "M. Cusumano-Towner",
                "B. Zinberg",
                "M. Ghavamizadeh",
                "F. Pollok",
                "A. Garrett",
                "J. Tenenbaum",
                "D. Gutfreund",
                "V. Mansinghka"
            ],
            "title": "3DP3: 3D scene perception via probabilistic programming",
            "venue": "ArXiv preprint: 2111.00312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Oleynikova",
                "Z. Taylor",
                "M. Fehr",
                "R. Siegwart",
                "J. Nieto"
            ],
            "title": "Voxblox: Incremental 3d euclidean signed distance fields for on-board mav planning",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 1366\u20131373.",
            "year": 2017
        },
        {
            "authors": [
                "N. Hughes",
                "Y. Chang",
                "L. Carlone"
            ],
            "title": "Hydra: a real-time spatial perception engine for 3D scene graph construction and optimization",
            "venue": "Robotics: Science and Systems (RSS), 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "H. Bavle",
                "J.L. Sanchez-Lopez",
                "M. Shaheer",
                "J. Civera",
                "H. Voos"
            ],
            "title": "Situational graphs for robot navigation in structured indoor environments",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 9107\u2013 9114, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "S-graphs+: Real-time localization and mapping leveraging hierarchical representations",
            "venue": "arXiv preprint arXiv:2212.11770, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Grinvald",
                "F. Furrer",
                "T. Novkovic",
                "J.J. Chung",
                "C. Cadena",
                "R. Siegwart",
                "J. Nieto"
            ],
            "title": "Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery",
            "venue": "IEEE Robotics and Automation Letters, vol. 4, no. 3, pp. 3037\u20133044, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Huber"
            ],
            "title": "Persistent homology in data science",
            "venue": "Data Science\u2013 Analytics and Applications: Proceedings of the 3rd International Data Science Conference\u2013iDSC2020. Springer, 2021, pp. 81\u201388.",
            "year": 2020
        },
        {
            "authors": [
                "R. Talak",
                "S. Hu",
                "L. Peng",
                "L. Carlone"
            ],
            "title": "Neural trees for learning on graphs",
            "venue": "Conf. on Neural Information Processing Systems (NeurIPS), 2021, (pdf).",
            "year": 2021
        },
        {
            "authors": [
                "R. Sumner",
                "J. Schmid",
                "M. Pauly"
            ],
            "title": "Embedded deformation for shape manipulation",
            "venue": "ACM SIGGRAPH 2007 papers on - SIGGRAPH \u201907, 2007. 30",
            "year": 2007
        },
        {
            "authors": [
                "S. Harnad"
            ],
            "title": "The symbol grounding problem",
            "venue": "Physica D: Nonlinear Phenomena, vol. 42, no. 1, pp. 335\u2013346, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "A. Garcia-Garcia",
                "S. Orts-Escolano",
                "S. Oprea",
                "V. Villena-Martinez",
                "J. Garc\u00eda-Rodr\u00edguez"
            ],
            "title": "A review on deep learning techniques applied to semantic segmentation",
            "venue": "ArXiv Preprint: 1704.06857, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Zeng",
                "F. Zhao",
                "J. Zheng",
                "X. Liu"
            ],
            "title": "Octree-based fusion for realtime 3d reconstruction",
            "venue": "Graphical Models, vol. 75, no. 3, pp. 126\u2013 136, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "DeepSDF: Learning continuous signed distance functions for shape representation",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.L. Bodlaender"
            ],
            "title": "Treewidth: Characterizations, applications, and computations",
            "venue": "Graph-Theoretic Concepts in Computer Science. Springer Berlin Heidelberg, 2006, pp. 1\u201314.",
            "year": 2006
        },
        {
            "authors": [
                "R. Dechter",
                "R. Mateescu"
            ],
            "title": "AND/OR search spaces for graphical models",
            "venue": "Artificial Intelligence, vol. 171, no. 2-3, pp. 73\u2013106, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "T. Feder",
                "M. Vardi"
            ],
            "title": "Monotone monadic snp and constraint satisfaction",
            "venue": "ACM Symp. on Theory of Computing (STOC). New York, NY, USA: ACM Press, 1993, pp. 612\u2013622.",
            "year": 1993
        },
        {
            "authors": [
                "M. Grohe",
                "D. Neuen",
                "P. Schweitzer",
                "D. Wiebking"
            ],
            "title": "An improved isomorphism test for bounded-tree-width graphs",
            "venue": "ACM Trans. Algorithms, vol. 16, no. 3, Jun. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "V. Chandrasekaran",
                "N. Srebro",
                "P. Harsha"
            ],
            "title": "Complexity of inference in graphical models",
            "venue": "Conf. on Uncertainty in Artificial Intelligence (UAI), 2008, p. 70\u201378.",
            "year": 2008
        },
        {
            "authors": [
                "G. Cooper"
            ],
            "title": "The computational complexity of probabilistic inference using Bayesian belief networks",
            "venue": "Artificial Intelligence, vol. 42, no. 2-3, pp. 393\u2013405, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "G. Zhu",
                "L. Zhang",
                "Y. Jiang",
                "Y. Dang",
                "H. Hou",
                "P. Shen",
                "M. Feng",
                "X. Zhao",
                "Q. Miao",
                "S.A.A. Shah"
            ],
            "title": "Scene graph generation: A comprehensive survey",
            "venue": "arXiv preprint arXiv:2201.00443, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Tagliasacchi",
                "T. Delame",
                "M. Spagnuolo",
                "N. Amenta",
                "A. Telea"
            ],
            "title": "3d skeletons: A state-of-the-art report",
            "venue": "Computer Graphics Forum, vol. 35, no. 2. Wiley Online Library, 2016, pp. 573\u2013597.",
            "year": 2016
        },
        {
            "authors": [
                "C. Cadena",
                "L. Carlone",
                "H. Carrillo",
                "Y. Latif",
                "D. Scaramuzza",
                "J. Neira",
                "I. Reid",
                "J. Leonard"
            ],
            "title": "Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age",
            "venue": "IEEE Trans. Robotics, vol. 32, no. 6, pp. 1309\u20131332, 2016, arxiv preprint: 1606.05830, (pdf).",
            "year": 2016
        },
        {
            "authors": [
                "A. Chang",
                "A. Dai",
                "T. Funkhouser",
                "M. Halber",
                "M. Niessner",
                "M. Savva",
                "S. Song",
                "A. Zeng",
                "Y. Zhang"
            ],
            "title": "Matterport3d: Learning from rgb-d data in indoor environments",
            "venue": "International Conference on 3D Vision (3DV), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H.L. Bodlaender",
                "A.M. Koster"
            ],
            "title": "Treewidth computations I: Upper bounds",
            "venue": "Information and Computation, vol. 208, no. 3, pp. 259 \u2013 275, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Maniu",
                "P. Senellart",
                "S. Jog"
            ],
            "title": "An Experimental Study of the Treewidth of Real-World Graph Data",
            "venue": "Intl. Conf. Database Theory, 2019, pp. 12:1\u201312:18.",
            "year": 2019
        },
        {
            "authors": [
                "A. Rosinol",
                "M. Abate",
                "Y. Chang",
                "L. Carlone"
            ],
            "title": "Kimera: an opensource library for real-time metric-semantic localization and mapping",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 2020, arXiv preprint: 1910.02490, (video), (code), (pdf).",
            "year": 2020
        },
        {
            "authors": [
                "T. Whelan",
                "J.B. McDonald",
                "M. Kaess",
                "M.F. Fallon",
                "H. Johannsson",
                "J.J. Leonard"
            ],
            "title": "Kintinuous: Spatially extended Kinect-Fusion",
            "venue": "RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras, Sydney, Australia, July 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Nie\u00dfner",
                "M. Zollh\u00f6fer",
                "S. Izadi",
                "M. Stamminger"
            ],
            "title": "Real-time 3d reconstruction at scale using voxel hashing",
            "venue": "ACM Transactions on Graphics (ToG), vol. 32, no. 6, p. 169, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "R.B. Rusu"
            ],
            "title": "Semantic 3D object maps for everyday manipulation in human living environments",
            "venue": "Ph.D. dissertation, Computer Science department, Technische Universitaet Muenchen, Germany, October 2009.",
            "year": 2009
        },
        {
            "authors": [
                "H. Yang",
                "J. Shi",
                "L. Carlone"
            ],
            "title": "TEASER: Fast and Certifiable Point Cloud Registration",
            "venue": "IEEE Trans. Robotics, vol. 37, no. 2, pp. 314\u2013333, 2020, extended arXiv version 2001.07715 (pdf).",
            "year": 2020
        },
        {
            "authors": [
                "R. Talak",
                "J. Shi",
                "D. Maggio",
                "L. Carlone"
            ],
            "title": "A correct-and-certify approach to self-supervise object pose estimators via ensemble selftraining",
            "venue": "Robotics: Science and Systems (RSS), 2023, (pdf).",
            "year": 2023
        },
        {
            "authors": [
                "H. Oleynikova",
                "Z. Taylor",
                "R. Siegwart",
                "J. Nieto"
            ],
            "title": "Sparse 3D topological graphs for micro-aerial vehicle planning",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Lau",
                "C. Sprunk",
                "W. Burgard"
            ],
            "title": "Efficient grid-based spatial representations for robot navigation in dynamic environments",
            "venue": "Robot. Auton. Syst., vol. 61, no. 10, p. 1116\u20131130, Oct. 2013. [Online]. Available: https://doi.org/10.1016/j.robot.2012.08.010",
            "year": 2013
        },
        {
            "authors": [
                "M. Foskey",
                "M.C. Lin",
                "D. Manocha"
            ],
            "title": "Efficient computation of a simplified medial axis",
            "venue": "J. Comput. Inf. Sci. Eng., vol. 3, no. 4, pp. 274\u2013284, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "J.L. Blanco",
                "P.K. Rai"
            ],
            "title": "nanoflann: a C++ header-only fork of FLANN, a library for nearest neighbor (NN) with kd-trees",
            "venue": "https:// github.com/jlblancoc/nanoflann, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D. Ali",
                "A. Asaad",
                "M.-J. Jimenez",
                "V. Nanda",
                "E. Paluzo-Hidalgo",
                "M. Soriano-Trigueros"
            ],
            "title": "A survey of vectorization methods in topological data analysis",
            "venue": "arXiv preprint arXiv:2212.09703, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M.E. Aktas",
                "E. Akbas",
                "A.E. Fatmaoui"
            ],
            "title": "Persistence homology of networks: methods and applications",
            "venue": "Applied Network Science, vol. 4, no. 1, pp. 1\u201328, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Bormann",
                "F. Jordan",
                "W. Li",
                "J. Hampp",
                "M. H agele"
            ],
            "title": "Room segmentation: Survey, implementation, and analysis",
            "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), 2016, pp. 1019\u20131026.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kleiner",
                "R. Baravalle",
                "A. Kolling",
                "P. Pilotti",
                "M. Munich"
            ],
            "title": "A solution to room-by-room coverage for autonomous cleaning robots",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 5346\u20135352.",
            "year": 2017
        },
        {
            "authors": [
                "T. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "Intl. Conf. on Learning Representations (ICLR), Apr. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W.L. Hamilton.",
                "R. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in Neural Information Processing Systems (NIPS), Dec. 2017, p. 1025\u20131035.",
            "year": 2017
        },
        {
            "authors": [
                "M. Jordan"
            ],
            "title": "An introduction to probabilistic graphical models",
            "venue": "November 2002, unpublished Lecture Notes.",
            "year": 2002
        },
        {
            "authors": [
                "D. Koller",
                "N. Friedman"
            ],
            "title": "Probabilistic Graphical Models: Principles and Techniques",
            "year": 2009
        },
        {
            "authors": [
                "M. Henaff",
                "J. Bruna",
                "Y. LeCun"
            ],
            "title": "Deep convolutional networks on graph-structured data",
            "venue": "arXiv preprint arXiv:1506.05163, Jun. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Defferrard",
                "X. Bresson",
                "P. Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 29, Dec. 2016, pp. 3844\u20133852.",
            "year": 2016
        },
        {
            "authors": [
                "M.M. Bronstein",
                "J. Bruna",
                "Y. LeCun",
                "A. Szlam",
                "P. Vandergheynst"
            ],
            "title": "Geometric deep learning: going beyond euclidean data",
            "venue": "IEEE Signal Process. Mag., vol. 34, no. 4, pp. 18\u201342, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Li\u00f3",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "Intl. Conf. on Learning Representations (ICLR), May 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Lee",
                "R. Rossi",
                "S. Kim",
                "N. Ahmed",
                "E. Koh"
            ],
            "title": "Attention models in graphs: A survey",
            "venue": "ACM Trans. Knowl. Discov. Data, vol. 13, no. 6, Nov. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Busbridge",
                "D. Sherburn",
                "P. Cavallo",
                "N.Y. Hammerla"
            ],
            "title": "Relational graph attention networks",
            "venue": "arXiv preprint arXiv:1904.05811, Apr. 2019.",
            "year": 1904
        },
        {
            "authors": [
                "M. Fey",
                "J.E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "Intl. Conf. on Learning Representations (ICLR) Workshop on Representation Learning on Graphs and Manifolds, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. G\u00e1lvez-L\u00f3pez",
                "J.D. Tard\u00f3s"
            ],
            "title": "Bags of binary words for fast place recognition in image sequences",
            "venue": "IEEE Transactions on Robotics, vol. 28, no. 5, pp. 1188\u20131197, October 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Y. Li",
                "C. Gu",
                "T. Dullien",
                "O. Vinyals",
                "P. Kohli"
            ],
            "title": "Graph matching networks for learning the similarity of graph structured objects",
            "venue": "Intl. Conf. on Machine Learning (ICML). PMLR, 2019, pp. 3835\u20133845.",
            "year": 2019
        },
        {
            "authors": [
                "P. Antonante",
                "V. Tzoumas",
                "H. Yang",
                "L. Carlone"
            ],
            "title": "Outlier-robust estimation: Hardness, minimally tuned algorithms, and applications",
            "venue": "IEEE Trans. Robotics, vol. 38, no. 1, pp. 281\u2013301, 2021, (pdf).",
            "year": 2021
        },
        {
            "authors": [
                "M. Savva",
                "A. Kadian",
                "O. Maksymets",
                "Y. Zhao",
                "E. Wijmans",
                "B. Jain",
                "J. Straub",
                "J. Liu",
                "V. Koltun",
                "J. Malik",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Habitat: A Platform for Embodied AI Research",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Reinke",
                "M. Palieri",
                "B. Morrell",
                "Y. Chang",
                "K. Ebadi",
                "L. Carlone",
                "A. Agha-mohammadi"
            ],
            "title": "LOCUS 2.0: Robust and computationally efficient lidar odometry for real-time underground 3D mapping",
            "venue": "vol. 7, no. 4, pp. 9043\u20139050, 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "Y. Chang",
                "K. Ebadi",
                "C. Denniston",
                "M.F. Ginting",
                "A. Rosinol",
                "A. Reinke",
                "M. Palieri",
                "J. Shi",
                "C. A",
                "B. Morrell",
                "A. Agha-mohammadi",
                "L. Carlone"
            ],
            "title": "LAMP 2.0: A robust multi-robot SLAM system for operation in challenging large-scale underground environments",
            "venue": "IEEE Robotics and Automation Letters (RA-L), vol. 7, no. 4, pp. 9175\u20139182, 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "J. Jain",
                "J. Li",
                "M. Chiu",
                "A. Hassani",
                "N. Orlov",
                "H. Shi"
            ],
            "title": "OneFormer: 31 One Transformer to Rule Universal Image Segmentation",
            "venue": "2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang",
                "W. Liu",
                "B. Xiao"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 3349\u2013 3364, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Sandler",
                "A. Howard",
                "M. Zhu",
                "A. Zhmoginov",
                "L.-C. Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4510\u2013 4520.",
            "year": 2018
        },
        {
            "authors": [
                "B. Zhou",
                "H. Zhao",
                "X. Puig",
                "S. Fidler",
                "A. Barriuso",
                "A. Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprints arXiv:1301.3781, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "F. Schroff",
                "D. Kalenichenko",
                "J. Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 815\u2013823.",
            "year": 2015
        },
        {
            "authors": [
                "K. Xu",
                "W. Hu",
                "J. Leskovec",
                "S. Jegelka"
            ],
            "title": "How powerful are graph neural networks?",
            "venue": "in Intl. Conf. on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "W. Chen",
                "S. Hu",
                "R. Talak",
                "L. Carlone"
            ],
            "title": "Leveraging large language models for robot 3D scene understanding",
            "venue": "arXiv preprint: 2209.05629, 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "G. Konidaris"
            ],
            "title": "On the necessity of abstraction",
            "venue": "Current Opinion in Behavioral Sciences, vol. 29, pp. 1\u20137, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G. Konidaris",
                "L.P. Kaelbling",
                "T. Lozano-Perez"
            ],
            "title": "From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning",
            "venue": "J. of Artificial Intelligence Research, vol. 61, pp. 215\u2013289, Jan. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. James",
                "B. Rosman",
                "G. Konidaris"
            ],
            "title": "Learning portable representations for high-level planning",
            "venue": "Intl. Conf. on Machine Learning (ICML). PMLR, 2020, pp. 4682\u20134691.",
            "year": 2020
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Autonomous Learning of Object-Centric Abstractions for High- Level Planning",
            "venue": "Intl. Conf. on Learning Representations (ICLR), May 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Berg",
                "G. Konidaris",
                "S. Tellex"
            ],
            "title": "Using Language to Generate State Abstractions for Long-Range Planning in Outdoor Environments",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), May 2022, pp. 1888\u20131895.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Jinnai",
                "D. Abel",
                "D. Hershkowitz",
                "M. Littman",
                "G. Konidaris"
            ],
            "title": "Finding options that minimize planning time",
            "venue": "Intl. Conf. on Machine Learning (ICML). PMLR, 2019, pp. 3120\u20133129.",
            "year": 2019
        },
        {
            "authors": [
                "B. Smith"
            ],
            "title": "Beyond Concepts: Ontology as Reality Representation",
            "year": 2004
        },
        {
            "authors": [
                "N. Guarino",
                "D. Oberle",
                "S. Staab"
            ],
            "title": "What is an ontology?",
            "venue": "Handbook on ontologies,",
            "year": 2009
        },
        {
            "authors": [
                "T. Jepsen"
            ],
            "title": "Just what is an ontology, anyway?",
            "venue": "IT Professional Magazine,",
            "year": 2009
        },
        {
            "authors": [
                "T.R. Gruber"
            ],
            "title": "Toward principles for the design of ontologies used for knowledge sharing?",
            "venue": "International journal of human-computer studies,",
            "year": 1995
        },
        {
            "authors": [
                "W.N. Borst"
            ],
            "title": "Construction of engineering ontologies for knowledge sharing and reuse",
            "venue": "1997, phD Dissertation.",
            "year": 1997
        },
        {
            "authors": [
                "R. Studer",
                "R. Benjamins",
                "D. Fensel"
            ],
            "title": "Knowledge engineering: Principles and methods",
            "venue": "Data & Knowledge Engineering, vol. 25, no. 1-2, pp. 161\u2013197, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "D. McGuinness",
                "F. Van Harmelen"
            ],
            "title": "OWL web ontology language overview",
            "venue": "W3C recommendation, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "M.R. Genesereth",
                "N.J. Nilsson"
            ],
            "title": "Logical Foundations of Artificial Intelligence",
            "year": 2012
        },
        {
            "authors": [
                "C. Schlenoff",
                "E. Prestes",
                "R. Madhavan",
                "P. Goncalves",
                "H. Li",
                "S. Balakirsky",
                "T. Kramer",
                "E. Miguelanez"
            ],
            "title": "An IEEE standard ontology for robotics and automation",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1337\u20131342.",
            "year": 2012
        },
        {
            "authors": [
                "S. Lemaignan",
                "R. Ros",
                "L. M\u00f6senlechner",
                "R. Alami",
                "M. Beetz"
            ],
            "title": "ORO, a knowledge management platform for cognitive architectures in robotics",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). IEEE, 2010, pp. 3548\u20133553.",
            "year": 2010
        },
        {
            "authors": [
                "M. Tenorth",
                "M. Beetz"
            ],
            "title": "Knowrob: A knowledge processing infrastructure for cognition-enabled robots",
            "venue": "Intl. J. of Robotics Research, vol. 32, no. 5, pp. 566\u2013590, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Beetz",
                "D. Be\u00dfler",
                "A. Haidu",
                "M. Pomarlan",
                "A.K. Bozcuo\u011flu",
                "G. Bartels"
            ],
            "title": "KnowRob 2.0\u2014a 2nd generation knowledge processing framework for cognition-enabled robotic agents",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2018, pp. 512\u2013519.",
            "year": 2018
        },
        {
            "authors": [
                "M. Diab",
                "A. Akbari",
                "M. Ud Din",
                "J. Rosell"
            ],
            "title": "PMK\u2014a knowledge processing framework for autonomous robotics perception and manipulation",
            "venue": "Sensors, vol. 19, no. 5, p. 1166, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G.A. Miller"
            ],
            "title": "Wordnet: A lexical database for english",
            "venue": "Commun. ACM, vol. 38, no. 11, p. 39\u201341, Nov. 1995. [Online]. Available: https://doi.org/10.1145/219717.219748",
            "year": 1995
        },
        {
            "authors": [
                "D.B. Lenat"
            ],
            "title": "Cyc: A large-scale investment in knowledge infrastructure",
            "venue": "Communications of the ACM, vol. 38, no. 11, pp. 33\u201338, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "I. Niles",
                "A. Pease"
            ],
            "title": "Towards a standard upper ontology",
            "venue": "Proceedings of the International Conference on Formal Ontology in Information Systems, 2001, pp. 2\u20139.",
            "year": 2001
        },
        {
            "authors": [
                "S. Auer",
                "C. Bizer",
                "G. Kobilarov",
                "J. Lehmann",
                "R. Cyganiak",
                "Z. Ives"
            ],
            "title": "DBpedia: A nucleus for a web of open data",
            "venue": "Semantic Web. Springer, 2007, pp. 722\u2013735.",
            "year": 2007
        },
        {
            "authors": [
                "F.M. Suchanek",
                "G. Kasneci",
                "G. Weikum"
            ],
            "title": "YAGO: A large ontology from wikipedia and WordNet",
            "venue": "Journal of Web Semantics, vol. 6, no. 3, pp. 203\u2013217, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "K. Bollacker",
                "C. Evans",
                "P. Paritosh",
                "T. Sturge",
                "J. Taylor"
            ],
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "venue": "Proceedings of the ACM SIGMOD International Conference on Management of Data, 2008, pp. 1247\u20131250.",
            "year": 2008
        },
        {
            "authors": [
                "A. Carlson",
                "J. Betteridge",
                "B. Kisiel",
                "B. Settles",
                "E. Hruschka",
                "T. Mitchell"
            ],
            "title": "Toward an architecture for never-ending language learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 24, no. 1, 2010, pp. 1306\u20131313.",
            "year": 2010
        },
        {
            "authors": [
                "D. Vrande\u010di\u0107",
                "M. Kr\u00f6tzsch"
            ],
            "title": "Wikidata: A free collaborative knowledgebase",
            "venue": "Commun. ACM, vol. 57, no. 10, p. 78\u201385, sep 2014. [Online]. Available: https://doi.org/10.1145/2629489",
            "year": 2014
        },
        {
            "authors": [
                "R. Speer",
                "J. Chin",
                "C. Havasi"
            ],
            "title": "ConceptNet 5.5: an open multilingual graph of general knowledge",
            "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, ser. AAAI\u201917. AAAI Press, 2017, p. 4444\u20134451.",
            "year": 2017
        },
        {
            "authors": [
                "A. Zareian",
                "S. Karaman",
                "S.-F. Chang"
            ],
            "title": "Bridging knowledge graphs to generate scene graphs",
            "venue": "European Conf. on Computer Vision (ECCV). Springer, 2020, pp. 606\u2013623.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Guo",
                "L. Gao",
                "X. Wang",
                "Y. Hu",
                "X. Xu",
                "X. Lu",
                "H.T. Shen",
                "J. Song"
            ],
            "title": "From general to specific: Informative scene graph generation via balance adjustment",
            "venue": "Intl. Conf. on Computer Vision (ICCV), 2021, pp. 16 383\u201316 392.",
            "year": 2021
        },
        {
            "authors": [
                "F. Amodeo",
                "F. Caballero",
                "N. D\u00edaz-Rodr\u00edguez",
                "L. Merino"
            ],
            "title": "Ogsgg: Ontology-guided scene graph generation\u2014a case study in transfer learning for telepresence robotics",
            "venue": "IEEE Access, vol. 10, pp. 132 564\u2013 132 583, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Chen",
                "S. Rezayi",
                "S. Li"
            ],
            "title": "More knowledge, less bias: Unbiasing scene graph generation with explicit ontological adjustment",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 4023\u20134032.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Movshovitz-Attias",
                "Q. Yu",
                "M.C. Stumpe",
                "V. Shet",
                "S. Arnoud",
                "L. Yatziv"
            ],
            "title": "Ontological supervision for fine grained classification of street view storefronts",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1693\u20131702.",
            "year": 2015
        },
        {
            "authors": [
                "D. Porello",
                "M. Cristani",
                "R. Ferrario"
            ],
            "title": "Integrating Ontologies and Computer Vision for Classification of Objects in Images",
            "venue": "Workshop on Neural Cognitive Integration, 2015, p. 15.",
            "year": 2015
        },
        {
            "authors": [
                "S. Aditya",
                "Y. Yang",
                "C. Baral",
                "Y. Aloimonos",
                "C. Ferm\u00fcller"
            ],
            "title": "Image understanding using vision and reasoning through scene description graph",
            "venue": "Computer Vision and Image Understanding, vol. 173, pp. 33\u2013 45, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "F. Gard\u00e8res",
                "M. Ziaeefard",
                "B. Abeloos",
                "F. Lecue"
            ],
            "title": "ConceptBert: Concept-aware representation for visual question answering",
            "venue": "Conference on Empirical Methods in Natural Language Processing, 2020, pp. 489\u2013498.",
            "year": 2020
        },
        {
            "authors": [
                "K. Marino",
                "X. Chen",
                "D. Parikh",
                "A. Gupta",
                "M. Rohrbach"
            ],
            "title": "KRISP: Integrating implicit and symbolic knowledge for open-domain knowledge-based VQA",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 14 111\u201314 121.",
            "year": 2021
        },
        {
            "authors": [
                "W. Zheng",
                "L. Yin",
                "X. Chen",
                "Z. Ma",
                "S. Liu",
                "B. Yang"
            ],
            "title": "Knowledge base graph embedding module design for visual question answering model",
            "venue": "Pattern Recognition, vol. 120, p. 108153, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ding",
                "J. Yu",
                "B. Liu",
                "Y. Hu",
                "M. Cui",
                "Q. Wu"
            ],
            "title": "MuKEA: Multimodal knowledge extraction and accumulation for knowledgebased visual question answering",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 5089\u20135098.",
            "year": 2022
        },
        {
            "authors": [
                "H. Chen",
                "H. Tan",
                "A. Kuntz",
                "M. Bansal",
                "R. Alterovitz"
            ],
            "title": "Enabling robots to understand incomplete natural language instructions using commonsense reasoning",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2020, pp. 1963\u20131969.",
            "year": 2020
        },
        {
            "authors": [
                "A. Daruna",
                "L. Nair",
                "W. Liu",
                "S. Chernova"
            ],
            "title": "Towards robust one-shot 32 task execution using knowledge graph embeddings",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2021, pp. 11 118\u201311 124.",
            "year": 2021
        },
        {
            "authors": [
                "S. Tuli",
                "R. Bansal",
                "R. Paul"
            ],
            "title": "ToolTango: Common sense generalization in predicting sequential tool interactions for robot plan synthesis",
            "venue": "The Journal of Artificial Intelligence Research, vol. 75, pp. 1595\u20131631, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Hao",
                "M. Chen",
                "W. Yu",
                "Y. Sun",
                "W. Wang"
            ],
            "title": "Universal representation learning of knowledge bases by jointly embedding instances and ontological concepts",
            "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019, pp. 1709\u20131719.",
            "year": 2019
        },
        {
            "authors": [
                "J.H. Kwak",
                "J. Lee",
                "J.J. Whang",
                "S. Jo"
            ],
            "title": "Semantic grasping via a knowledge graph of robotic manipulation: A graph representation learning approach",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 9397\u20139404, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Geman",
                "D.F. Potter",
                "Z. Chi"
            ],
            "title": "Composition Systems",
            "venue": "Quarterly of Applied Mathematics, vol. 60, no. 4, pp. 707\u2013736, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "B.M. Lake",
                "T.D. Ullman",
                "J.B. Tenenbaum",
                "S.J. Gershman"
            ],
            "title": "Building machines that learn and think like people",
            "venue": "Behavioral and Brain Sciences, vol. 40, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L.L. Zhu",
                "Y. Chen",
                "A. Yuille"
            ],
            "title": "Recursive Compositional Models for Vision: Description and Review of Recent Work",
            "venue": "Journal of Mathematical Imaging and Vision, vol. 41, no. 1, pp. 122\u2013146, Sep. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "S.-C. Zhu",
                "D. Mumford"
            ],
            "title": "A Stochastic Grammar of Images",
            "venue": "Foundations and Trends in Computer Graphics and Vision, vol. 2, no. 4, pp. 259\u2013362, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S.-C. Zhu",
                "S. Huang"
            ],
            "title": "Computer Vision: Stochastic Grammars for Parsing Objects, Scenes, and Events",
            "year": 2021
        },
        {
            "authors": [
                "G. Izatt",
                "R. Tedrake"
            ],
            "title": "Generative modeling of environments with scene grammars and variational inference",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 6891\u20136897.",
            "year": 2020
        },
        {
            "authors": [
                "S. Qi",
                "Y. Zhu",
                "S. Huang",
                "C. Jiang",
                "S.-C. Zhu"
            ],
            "title": "Human-Centric Indoor Scene Synthesis Using Stochastic Grammar",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2018, pp. 5899\u20135908.",
            "year": 2018
        },
        {
            "authors": [
                "J. Chua"
            ],
            "title": "Probabilistic Scene Grammars: A General-Purpose Framework For Scene Understanding",
            "venue": "Brown University Thesis, pp. 1\u2013146, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Wang",
                "T. Zhou",
                "S. Qi",
                "J. Shen",
                "S.-C. Zhu"
            ],
            "title": "Hierarchical Human Semantic Parsing With Comprehensive Part-Relation Modeling",
            "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 44, no. 7, pp. 3508\u20133522, Jul. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2021, pp. 11 448\u201311 459.",
            "year": 2021
        },
        {
            "authors": [
                "K. Mo",
                "P. Guerrero",
                "L. Yi",
                "H. Su",
                "P. Wonka",
                "N.J. Mitra",
                "L.J. Guibas"
            ],
            "title": "StructEdit: Learning Structural Shape Variations",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 8859\u20138868.",
            "year": 2020
        },
        {
            "authors": [
                "N. Ichien",
                "Q. Liu",
                "S. Fu",
                "K. Holyoak",
                "A. Yuille",
                "H. Lu"
            ],
            "title": "Visual Analogy: Deep Learning Versus Compositional Models",
            "venue": "Annual Meeting of the Cognitive Science Society, vol. 43, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Yuan",
                "T. Chen",
                "B. Li",
                "X. Xue"
            ],
            "title": "Compositional Scene Representation Learning via Reconstruction: A Survey",
            "venue": "Feb. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J.A. Fodor",
                "Z.W. Pylyshyn"
            ],
            "title": "Connectionism and cognitive architecture: A critical analysis",
            "venue": "Cognition, vol. 28, no. 1, pp. 3\u201371, Mar. 1988.",
            "year": 1988
        },
        {
            "authors": [
                "H.N. Mhaskar"
            ],
            "title": "Neural networks for optimal approximation of smooth and analytic functions",
            "venue": "Neural Computation, vol. 8, no. 1, pp. 164\u2013 177, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "H.N. Mhaskar",
                "T. Poggio"
            ],
            "title": "Deep vs. shallow networks: An approximation theory perspective",
            "venue": "Analysis and Applications, vol. 14, no. 06, pp. 829\u2013848, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Poggio",
                "H. Mhaskar",
                "L. Rosasco",
                "B. Miranda",
                "Q. Liao"
            ],
            "title": "Why and when can deep - but not shallow - networks avoid the curse of dimensionality: A review",
            "venue": "Int. J. Autom. Comput., vol. 14, pp. 503\u2013 519, Mar. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Webb",
                "K.J. Holyoak",
                "H. Lu"
            ],
            "title": "Emergent Analogical Reasoning in Large Language Models",
            "venue": "Dec. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Xie",
                "A.S. Morcos",
                "S.-C. Zhu",
                "R. Vedantam"
            ],
            "title": "COAT: Measuring Object Compositionality in Emergent Representations",
            "venue": "Intl. Conf. on Machine Learning (ICML), Jun. 2022, pp. 24 388\u201324 413.",
            "year": 2022
        },
        {
            "authors": [
                "R. Krishna",
                "Y. Zhu",
                "O. Groth",
                "J. Johnson",
                "K. Hata",
                "J. Kravitz",
                "S. Chen",
                "Y. Kalantidis",
                "L. Li",
                "D. Shamma",
                "M. Bernstein",
                "L. Fei-Fei"
            ],
            "title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "arXiv preprints arXiv:1602.07332, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Johnson",
                "R. Krishna",
                "M. Stark",
                "L. Li",
                "D. Shamma",
                "M. Bernstein",
                "L. Fei-Fei"
            ],
            "title": "Image retrieval using scene graphs",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3668\u2013 3678.",
            "year": 2015
        },
        {
            "authors": [
                "A. Karpathy",
                "L. Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "P. Anderson",
                "B. Fernando",
                "M. Johnson",
                "S. Gould"
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "European Conf. on Computer Vision (ECCV), 2016, pp. 382\u2013398.",
            "year": 2016
        },
        {
            "authors": [
                "M. Ren",
                "R. Kiros",
                "R.S. Zemel"
            ],
            "title": "Image question answering: A visual semantic embedding model and a new dataset",
            "venue": "arXiv preprints arXiv:1505.02074, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Lu",
                "R. Krishna",
                "M. Bernstein",
                "F.-F. Li"
            ],
            "title": "Visual relationship detection with language priors",
            "venue": "European Conference on Computer Vision, 2016, pp. 852\u2013869.",
            "year": 2016
        },
        {
            "authors": [
                "D. Xu",
                "Y. Zhu",
                "C.B. Choy",
                "L. Fei-Fei"
            ],
            "title": "Scene graph generation by iterative message passing",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 3097\u20133106.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Li",
                "W. Ouyang",
                "B. Zhou",
                "K. Wang",
                "X. Wang"
            ],
            "title": "Scene graph generation from objects, phrases and region captions",
            "venue": "International Conference on Computer Vision (ICCV), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Yang",
                "J. Lu",
                "S. Lee",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Graph R-CNN for scene graph generation",
            "venue": "European Conf. on Computer Vision (ECCV), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Zellers",
                "M. Yatskar",
                "S. Thomson",
                "Y. Choi"
            ],
            "title": "Neural motifs: Scene graph parsing with global context",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Chang",
                "P. Ren",
                "P. Xu",
                "Z. Li",
                "X. Chen",
                "A. Hauptmann"
            ],
            "title": "A comprehensive survey of scene graphs: Generation and application",
            "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 45, no. 1, pp. 1\u201326, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "R.F. Salas-Moreno",
                "R.A. Newcombe",
                "H. Strasdat",
                "P.H.J. Kelly",
                "A.J. Davison"
            ],
            "title": "SLAM++: Simultaneous localisation and mapping at the level of objects",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Dong",
                "X. Fei",
                "S. Soatto"
            ],
            "title": "Visual-Inertial-Semantic scene representation for 3D object detection",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Shan",
                "Q. Feng",
                "N. Atanasov"
            ],
            "title": "Object residual constrained visual-inertial odometry",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2020, pp. 5104\u20135111.",
            "year": 2020
        },
        {
            "authors": [
                "L. Nicholson",
                "M. Milford",
                "N. S\u00fcnderhauf"
            ],
            "title": "QuadricSLAM: Dual quadrics from object detections as landmarks in object-oriented SLAM",
            "venue": "IEEE Robotics and Automation Letters, vol. 4, pp. 1\u20138, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Bowman",
                "N. Atanasov",
                "K. Daniilidis",
                "G. Pappas"
            ],
            "title": "Probabilistic data association for semantic SLAM",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 2017, pp. 1722\u20131729.",
            "year": 2017
        },
        {
            "authors": [
                "K. Ok",
                "K. Liu",
                "N. Roy"
            ],
            "title": "Hierarchical object map estimation for efficient and robust navigation",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 1132\u20131139.",
            "year": 2021
        },
        {
            "authors": [
                "J. McCormac",
                "A. Handa",
                "A.J. Davison",
                "S. Leutenegger"
            ],
            "title": "SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Narita",
                "T. Seno",
                "T. Ishikawa",
                "Y. Kaji"
            ],
            "title": "Panopticfusion: Online volumetric semantic mapping at the level of stuff and things",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Behley",
                "M. Garbade",
                "A. Milioto",
                "J. Quenzel",
                "S. Behnke",
                "C. Stachniss",
                "J. Gall"
            ],
            "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
            "venue": "Intl. Conf. on Computer Vision (ICCV), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Tateno",
                "F. Tombari",
                "N. Navab"
            ],
            "title": "Real-time and scalable incremental segmentation on dense SLAM",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2015, pp. 4465\u20134472.",
            "year": 2015
        },
        {
            "authors": [
                "K. Lianos",
                "J. Sch\u00f6nberger",
                "M. Pollefeys",
                "T. Sattler"
            ],
            "title": "Vso: Visual semantic odometry",
            "venue": "European Conf. on Computer Vision (ECCV), 2018, pp. 246\u2013263.",
            "year": 2018
        },
        {
            "authors": [
                "R. Rosu",
                "J. Quenzel",
                "S. Behnke"
            ],
            "title": "Semi-supervised semantic mapping through label propagation with semantic texture meshes",
            "venue": "Intl. J. of Computer Vision, 06 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Li",
                "H. Xiao",
                "K. Tateno",
                "F. Tombari",
                "N. Navab",
                "G.D. Hager"
            ],
            "title": "Incremental scene understanding on dense SLAM",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2016, pp. 574\u2013581.",
            "year": 2016
        },
        {
            "authors": [
                "J. McCormac",
                "R. Clark",
                "M. Bloesch",
                "A. Davison",
                "S. Leutenegger"
            ],
            "title": "Fusion++: Volumetric object-level SLAM",
            "venue": "Intl. Conf. on 3D Vision (3DV), 2018, pp. 32\u201341. 33",
            "year": 2018
        },
        {
            "authors": [
                "B. Xu",
                "W. Li",
                "D. Tzoumanikas",
                "M. Bloesch",
                "A. Davison",
                "S. Leutenegger"
            ],
            "title": "MID-Fusion: Octree-based object-level multi-instance dynamic SLAM",
            "venue": "2019, pp. 5231\u20135237.",
            "year": 2019
        },
        {
            "authors": [
                "L. Schmid",
                "J. Delmerico",
                "J. Sch\u00f6nberger",
                "J. Nieto",
                "M. Pollefeys",
                "R. Siegwart",
                "C. Cadena"
            ],
            "title": "Panoptic multi-tsdfs: a flexible representation for online multi-resolution volumetric mapping and long-term dynamic scene consistency",
            "venue": "arXiv preprint arXiv:2109.10165, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Liu",
                "J. Wu",
                "Y. Furukawa"
            ],
            "title": "FloorNet: A unified framework for floorplan reconstruction from 3d scans",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), September 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Friedman",
                "H. Pasula",
                "D. Fox"
            ],
            "title": "Voronoi random fields: Extracting the topological structure of indoor environments via place labeling",
            "venue": "Intl. Joint Conf. on AI (IJCAI). San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2007, p. 2109\u20132114.",
            "year": 2007
        },
        {
            "authors": [
                "S. Stekovic",
                "M. Rad",
                "F. Fraundorfer",
                "V. Lepetit"
            ],
            "title": "MonteFloor: Extending MCTS for reconstructing accurate large-scale floor plans",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Furukawa",
                "B. Curless",
                "S.M. Seitz",
                "R. Szeliski"
            ],
            "title": "Reconstructing building interiors from images",
            "venue": "Intl. Conf. on Computer Vision (ICCV), 2009.",
            "year": 2009
        },
        {
            "authors": [
                "R. Lukierski",
                "S. Leutenegger",
                "A.J. Davison"
            ],
            "title": "Room layout estimation from rapid omnidirectional exploration",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 2017, pp. 6315\u20136322.",
            "year": 2017
        },
        {
            "authors": [
                "T. Zheng",
                "G. Zhang",
                "L. Han",
                "L. Xu",
                "L. Fang"
            ],
            "title": "Building fusion: Semantic-aware structural building-scale 3d reconstruction",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Kuipers"
            ],
            "title": "The Spatial Semantic Hierarchy",
            "venue": "Artificial Intelligence, vol. 119, pp. 191\u2013233, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Modeling spatial knowledge",
            "venue": "Cognitive Science, vol. 2, pp. 129\u2013153, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "R. Chatila",
                "J.-P. Laumond"
            ],
            "title": "Position referencing and consistent world modeling for mobile robots",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 1985, pp. 138\u2013145.",
            "year": 1985
        },
        {
            "authors": [
                "S. Thrun"
            ],
            "title": "Robotic mapping: a survey",
            "venue": "Exploring artificial intelligence in the new millennium. Morgan Kaufmann, Inc., 2003, pp. 1\u201335.",
            "year": 2003
        },
        {
            "authors": [
                "J.-R. Ruiz-Sarmiento",
                "C. Galindo",
                "J. Gonzalez-Jimenez"
            ],
            "title": "Building multiversal semantic maps for mobile robot operation",
            "venue": "Knowledge- Based Systems, vol. 119, pp. 257\u2013272, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Galindo",
                "A. Saffiotti",
                "S. Coradeschi",
                "P. Buschka",
                "J. Fern\u00e1ndez- Madrigal",
                "J. Gonz\u00e1lez"
            ],
            "title": "Multi-hierarchical semantic maps for mobile robotics",
            "venue": "IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2005, pp. 3492\u20133497.",
            "year": 2005
        },
        {
            "authors": [
                "H. Zender",
                "O.M. Mozos",
                "P. Jensfelt",
                "G.-J. Kruijff",
                "W. Burgard"
            ],
            "title": "Conceptual spatial representations for indoor mobile robots",
            "venue": "Robotics and Autonomous Systems, vol. 56, no. 6, pp. 493\u2013502, 2008, from Sensors to Human Spatial Concepts.",
            "year": 2008
        },
        {
            "authors": [
                "H. Choset",
                "K. Nagatani"
            ],
            "title": "Topological simultaneous localization and mapping (SLAM): toward exact localization without explicit localization",
            "venue": "IEEE Trans. Robot. Automat., vol. 17, no. 2, pp. 125 \u2013 137, April 2001.",
            "year": 2001
        },
        {
            "authors": [
                "P. Beeson",
                "J. Modayil",
                "B. Kuipers"
            ],
            "title": "Factoring the mapping problem: Mobile robot map-building in the hybrid spatial semantic hierarchy",
            "venue": "Intl. J. of Robotics Research, vol. 29, no. 4, pp. 428\u2013459, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "P. Gay",
                "J. Stuart",
                "A. Del Bue"
            ],
            "title": "Visual graphs from motion (VGfM): Scene understanding with object geometry reasoning",
            "venue": "Asian Conf. on Computer Vision (ACCV). Springer International Publishing, 2018, pp. 330\u2013346.",
            "year": 2018
        },
        {
            "authors": [
                "S. Lowry",
                "N. S\u00fcnderhauf",
                "P. Newman",
                "J. Leonard",
                "D. Cox",
                "P. Corke",
                "M. Milford"
            ],
            "title": "Visual place recognition: A survey",
            "venue": "IEEE Trans. Robotics, vol. 32, no. 1, pp. 1\u201319, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Schubert",
                "P. Neubert",
                "P. Protzel"
            ],
            "title": "Fast and memory efficient graph optimization via ICM for visual place recognition",
            "venue": "Proc. of Robotics: Science and Systems (RSS), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Milford",
                "G. Wyeth"
            ],
            "title": "Seqslam: Visual route-based navigation for sunny summer days and stormy winter nights",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), may 2012, pp. 1643 \u20131649.",
            "year": 2012
        },
        {
            "authors": [
                "S. Garg",
                "M. Milford"
            ],
            "title": "Seqnet: Learning descriptors for sequencebased hierarchical place recognition",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 4305\u20134312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Arandjelovic",
                "P. Gronat",
                "A. Torii",
                "T. Pajdla",
                "J. Sivic"
            ],
            "title": "NetVLAD: CNN architecture for weakly supervised place recognition",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5297\u20135307.",
            "year": 2016
        },
        {
            "authors": [
                "A. Gawel",
                "C.D. Don",
                "R. Siegwart",
                "J. Nieto",
                "C. Cadena"
            ],
            "title": "Xview: Graph-based semantic multi-view localization",
            "venue": "IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1687\u20131694, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liu",
                "Y. Petillot",
                "D. Lane",
                "S. Wang"
            ],
            "title": "Global Localization with Object-Level Semantics and Topology",
            "venue": "2019 International Conference on Robotics and Automation (ICRA), May 2019, pp. 4909\u2013 4915, iSSN: 2577-087X.",
            "year": 2019
        },
        {
            "authors": [
                "S. Lin",
                "J. Wang",
                "M. Xu",
                "H. Zhao",
                "Z. Chen"
            ],
            "title": "Topology aware object-level semantic mapping towards more robust loop closure",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7041\u20137048, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Qin",
                "Y. Zhang",
                "Y. Liu",
                "G. Lv"
            ],
            "title": "Semantic loop closure detection based on graph matching in multi-objects scenes",
            "venue": "Journal of Visual Communication and Image Representation, vol. 76, p. 103072, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. St\u00fcckler",
                "S. Behnke"
            ],
            "title": "Multi-resolution surfel maps for efficient dense 3d modeling and tracking",
            "venue": "J. Vis. Comun. Image Represent., vol. 25, no. 1, pp. 137\u2013147, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T. Whelan",
                "R. Salas-Moreno",
                "B. Glocker",
                "A.J. Davison",
                "S. Leutenegger"
            ],
            "title": "ElasticFusion: Real-Time Dense SLAM and Light Source Estimation",
            "venue": "2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Dai",
                "M. Nie\u00dfner",
                "M. Zollh\u00f6fer",
                "S. Izadi",
                "C. Theobalt"
            ],
            "title": "Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration",
            "venue": "ACM Transactions on Graphics (ToG), vol. 36, no. 4, p. 1, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Reijgwart",
                "A. Millane",
                "H. Oleynikova",
                "R. Siegwart",
                "C. Cadena",
                "J. Nieto"
            ],
            "title": "Voxgraph: Globally consistent, volumetric mapping using signed distance function submaps",
            "venue": "IEEE Robotics and Automation Letters, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Whelan",
                "M. Kaess",
                "H. Johannsson",
                "M. Fallon",
                "J. Leonard",
                "J. Mc- Donald"
            ],
            "title": "Real-time large-scale dense RGB-D SLAM with volumetric fusion",
            "venue": "Intl. J. of Robotics Research, vol. 34, no. 4\u20135, pp. 598\u2013626, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Czarnowski",
                "T. Laidlow",
                "R. Clark",
                "A. Davison"
            ],
            "title": "DeepFactors: Real-time probabilistic dense monocular SLAM",
            "venue": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 721\u2013728, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Sucar",
                "K. Wada",
                "A. Davison"
            ],
            "title": "NodeSLAM: Neural object descriptors for multi-view shape reconstruction",
            "venue": "2020 Intl. Conference on 3D Vision (3DV), 2020, pp. 949\u2013958.",
            "year": 2020
        },
        {
            "authors": [
                "A. Rosinol",
                "J. Leonard",
                "L. Carlone"
            ],
            "title": "NeRF-SLAM: Real-time dense monocular SLAM with neural radiance fields",
            "venue": "arXiv preprint: 2210.13641, 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "X. Kong",
                "S. Liu",
                "M. Taher",
                "A. Davison"
            ],
            "title": "vMAP: Vectorised object mapping for neural field SLAM",
            "venue": "ArXiv, preprint: 2302.01838, 2023.",
            "year": 1838
        },
        {
            "authors": [
                "H. Ha",
                "S. Song"
            ],
            "title": "Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models",
            "venue": "2022.",
            "year": 2022
        },
        {
            "authors": [
                "K.M. Jatavallabhula",
                "A. Kuwajerwala",
                "Q. Gu",
                "M. Omama",
                "T. Chen",
                "S. Li",
                "G. Iyer",
                "S. Saryazdi",
                "N. Keetha",
                "A. Tewari",
                "J. Tenenbaum",
                "C.M. de Melo",
                "M. Krishna",
                "L. Paull",
                "F. Shkurti",
                "A. Torralba"
            ],
            "title": "Concept- Fusion: Open-set multimodal 3D mapping",
            "venue": "arXiv: 2302.07241, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Ravichandran",
                "L. Peng",
                "N. Hughes",
                "J. Griffith",
                "L. Carlone"
            ],
            "title": "Hierarchical representations and explicit memory: Learning effective navigation policies on 3D scene graphs using graph neural networks",
            "venue": "IEEE Intl. Conf. on Robotics and Automation (ICRA), 2022, (pdf).",
            "year": 2022
        },
        {
            "authors": [
                "C. Agia",
                "K.M. Jatavallabhula",
                "M. Khodeir",
                "O. Miksik",
                "V. Vineet",
                "M. Mukadam",
                "L. Paull",
                "F. Shkurti"
            ],
            "title": "Taskography: Evaluating robot task planning over large 3D scene graphs",
            "venue": "Conference on Robot Learning (CoRL). PMLR, Jan. 2022, pp. 46\u201358.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Chang",
                "L. Ballotta",
                "L. Carlone"
            ],
            "title": "D-Lite: Navigation-oriented compression of 3D scene graphs under communication constraints",
            "venue": "arXiv preprint: 2209.06111, 2022, (pdf),(video).",
            "year": 2022
        },
        {
            "authors": [
                "A. Becker",
                "D. Geiger"
            ],
            "title": "A sufficiently fast algorithm for finding close to optimal junction trees",
            "venue": "Conf. on Uncertainty in Artificial Intelligence (UAI), Feb. 1996, pp. 81\u201389.",
            "year": 1996
        },
        {
            "authors": [
                "A. Thomas",
                "P.J. Green"
            ],
            "title": "Enumerating the junction trees of a decomposable graph",
            "venue": "J. Comput Graph Stat., vol. 18, no. 4, pp. 930\u2013 940, Dec. 2009.",
            "year": 2009
        },
        {
            "authors": [
                "H.L. Bodlaender",
                "A.M. Koster"
            ],
            "title": "Treewidth computations II: Lower bounds",
            "venue": "Information and Computation, vol. 209, no. 7, pp. 1103 \u2013 1119, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "F.V. Jensen",
                "F. Jensen"
            ],
            "title": "Optimal junction trees",
            "venue": "Conf. on Uncertainty in Artificial Intelligence (UAI), Jul. 1994, p. 360\u2013366.",
            "year": 1994
        }
    ],
    "sections": [
        {
            "text": "1 Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems\nNathan Hughes, Yun Chang, Siyi Hu, Rajat Talak, Rumaisa Abdulhai, Jared Strader, Luca Carlone\nAbstract\u20143D spatial perception is the problem of building and maintaining an actionable and persistent representation of the environment in real-time using sensor data and prior knowledge. Despite the fast-paced progress in robot perception, most existing methods either build purely geometric maps (as in traditional SLAM) or \u201cflat\u201d metric-semantic maps that do not scale to large environments or large dictionaries of semantic labels. The first part of this paper is concerned with representations: we show that scalable representations for spatial perception need to be hierarchical in nature. Hierarchical representations are efficient to store, and lead to layered graphs with small treewidth, which enable provably efficient inference. We then introduce an example of hierarchical representation for indoor environments, namely a 3D scene graph, and discuss its structure and properties. The second part of the paper focuses on algorithms to incrementally construct a 3D scene graph as the robot explores the environment. Our algorithms combine 3D geometry (e.g., to cluster the free space into a graph of places), topology (to cluster the places into rooms), and geometric deep learning (e.g., to classify the type of rooms the robot is moving across). The third part of the paper focuses on algorithms to maintain and correct 3D scene graphs during long-term operation. We propose hierarchical descriptors for loop closure detection and describe how to correct a scene graph in response to loop closures, by solving a 3D scene graph optimization problem. We conclude the paper by combining the proposed perception algorithms into Hydra, a real-time spatial perception system that builds a 3D scene graph from visual-inertial data in real-time. We showcase Hydra\u2019s performance in photo-realistic simulations and real data collected by a Clearpath Jackal robots and a Unitree A1 robot. We release an open-source implementation of Hydra at https://github.com/MIT-SPARK/Hydra.\nI. INTRODUCTION\nThe next generation of robots and autonomous systems will need to build actionable, metric-semantic, multi-resolution, persistent representations of large-scale unknown environments in real-time. Actionable representations are required for a robot to understand and execute complex humans instructions (e.g., \u201cbring me the cup of tea I left on the dining room table\u201d). These representations include both geometric and semantic aspects of the environment (e.g., to plan a path to the dining room, and understand where the table is); moreover, they should allow reasoning over relations between objects (e.g., to understand what it means for the cup of tea to be on\nThe authors are with the Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, USA. Email: {na26933, yunchang, siyi, talak, rumaisa, jstrader, lcarlone}@mit.edu\nThis work was partially funded by the AIA CRA FA8750-19-2-1000, ARL DCIST CRA W911NF-17-2-0181, ONR RAIDER N00014-18-1-2828, MIT Lincoln Laboratory\u2019s Autonomy al Fresco program, Lockheed Martin Corporation\u2019s Neural Prediction in 3D Dynamic Scene Graphs program, and by Carlone\u2019s Amazon Research Award.\nthe table). These representations need to be multi-resolution, in that they might need to capture information at multiple levels of abstractions (e.g., from objects to rooms, buildings, and cities) to interpret human commands and enable fast planning (e.g., by allowing planning over compact abstractions rather than dense low-level geometry). Such representations must be built in real-time to support just-in-time decision-making. Finally, these representations must be persistent to support longterm autonomy: (i) they need to scale to large environments, (ii) they should allow fast inference and corrections as new evidence is collected by the robot, and (iii) their size should only grow with the size of the environment they model.\n3D spatial perception (or Spatial AI [1]) is the problem of building actionable and persistent representations from sensor data and prior knowledge in real-time. This problem is the natural evolution of Simultaneous Localization and Mapping (SLAM), which also focuses on building persistent map representations in real-time, but is typically limited to geometric understanding. In other words, if the task assigned to the robot\nar X\niv :2\n30 5.\n07 15\n4v 1\n[ cs\n.R O\n] 1\n1 M\nay 2\n02 3\n2 is purely geometric (e.g., \u201cgo to position [X, Y, Z]\u201d), then spatial perception reduces to SLAM and 3D reconstruction, but as the task specifications become more advanced (e.g., including semantics, relations, and affordances), we obtain a much richer problem space and SLAM becomes only a component of a more elaborate spatial perception system.\nThe pioneering work [1] has introduced the notion of spatial AI. Indeed the requirements that spatial AI has to build actionable and persistent representations can already be found in [1]. In this paper we take a step further by arguing that such representations must be hierarchical in nature, since a suitable hierarchical organization reduces storage during long-term operation and leads to provably efficient inference. Moreover, going beyond the vision in [1], we discuss how to combine different tools (metric-semantic SLAM, 3D geometry, topology, and geometric deep learning) to implement a realtime spatial perception system for indoor environments.\nHierarchical Representations. 3D Scene Graphs [2, 3, 4, 5, 6, 7] have recently emerged as expressive hierarchical representations of 3D environments. A 3D scene graph (e.g., Fig. 1) is a layered graph where nodes represent spatial concepts at multiple levels of abstraction (from low-level geometry to objects, places, rooms, buildings, etc.) and edges represent relations between concepts. Armeni et al. [3] pioneered the use of 3D scene graphs in computer vision and proposed the first algorithms to parse a metric-semantic 3D mesh into a 3D scene graph. Kim et al. [5] reconstruct a 3D scene graph of objects and their relations. Rosinol et al. [2, 4] propose a novel 3D scene graph model that (i) is built directly from sensor data, (ii) includes a sub-graph of places (useful for robot navigation), (iii) models objects, rooms, and buildings, and (iv) captures moving entities in the environment. Recent work [6, 7, 8, 9] infers objects and relations from point clouds, RGB-D sequences, or object detections. In this paper, we formalize the intuition behind these works that suitable representations for robot perception have to be hierarchical in nature, and discuss guiding principles behind the choice of \u201csymbols\u201d we have to include in these representations.\nReal-time Systems. While 3D scene graphs can serve as an advanced \u201cmental model\u201d for robots, methods to build such a rich representation in real-time remain largely unexplored. The works [5, 6, 7] allow real-time operation but are restricted to \u201cflat\u201d 3D scene graphs that only include objects and their relations while disregarding the top layers in Fig. 1. The works [2, 3, 4], which focus on building truly hierarchical representations, run offline and require several minutes to build a 3D scene graph ([3] even assumes the availability of a complete metric-semantic mesh of the environment built beforehand). Extending our prior works [2, 4] to operate in real-time is non-trivial. These works utilize an Euclidean Signed Distance Function (ESDF) of the entire environment to build the scene graph. Unfortunately, ESDFs memory requirements scale poorly in the size of the environment; see [10] and Section II. Moreover, the extraction of places and rooms in [2, 4] involves batch algorithms that process the entire ESDF, whose computational cost grows over time and is incompatible with real-time operation. Finally, the ESDF is reconstructed from the robot trajectory estimate which keeps\nchanging in response to loop closures. The approaches [2, 4] would therefore need to rebuild the scene graph from scratch after every loop closure, clashing with real-time operation.\nThe present paper extends our prior work [11] and proposes the first real-time system to build hierarchical 3D scene graphs of large-scale environments. Following [11], recent works has explored constructing situational graphs [12, 13], a hierarchical representation for scene geometry with layers describing free-space traversability, walls, rooms, and floors. While related to this research line, the works [12, 13] focus on LIDAR-based systems, which mostly reason over geometric features (e.g., walls, rooms, floors), but lack the rich semantics we consider in this paper (e.g., objects and room labels). We postpone a more extensive literature review to Section VII.\nContribution 1: Foundations of Hierarchical Representations (Section II). We start by observing that flat metricsemantic representations scale poorly in the size of the environment and the size of the vocabulary of semantic labels the robot has to incorporate in the map. For instance, a voxel-based metric-semantic map picturing the floor of an office building with 40 semantic labels per voxel (as the one underlying the approaches of [2] and [14]) already requires roughly 450 MiB to be stored. Envisioning future robots to operate on much large scales (e.g., an entire city) and using a much larger vocabulary (e.g., the English dictionary includes roughly 500,000 words), we argue that research should move beyond flat representations. We show that hierarchical representations allow to largely reduce the memory requirements, by enabling lossless compression of semantic information into a layered graph, as well as lossy compression of the geometric information into meshes and graph-structured representations of the free space. Additionally, we show that hierarchical representations are amenable for efficient inference. In particular, we prove that the layered graphs appearing in hierarchical map representations have small treewidth, a property that enables efficient inference; for instance, we conclude that the treewidth of the scene graph modeling an indoor environment does not scale with the number of nodes in the graph (i.e., roughly speaking, the number of nodes is related to the size of the environment), but rather with the maximum number of objects in each room. While most of the results above are general and apply to a broad class of hierarchical representations, we conclude this part by introducing a specific hierarchical representation for indoor environments, namely 3D scene graphs.\nContribution 2: Real-time Incremental 3D Scene Graph Construction (Section III). After establishing the importance of hierarchical representations, we move to developing a suite of algorithms to estimate 3D scene graphs from sensor data. In particular, we develop real-time algorithms that can incrementally estimate a 3D scene graph of an unknown building from visual-inertial sensor data. We use existing methods for geometric processing to incrementally build a metric-semantic mesh of the environment and reconstruct a sparse graph of \u201cplaces\u201d; intuitively, the mesh describes the occupied space (including objects in the environment), while the graph of places provides a succinct description of the free space. Then we propose novel algorithms to efficiently cluster the places into rooms; here we use tools from topology, and in\n3 particular the notion of persistent homology [15]. Finally, we use novel architectures for geometric deep learning, namely neural trees [16], to infer the semantic labels of each room (e.g., bedroom, kitchen, etc.) from the labels of the object within. Towards this goal, we show that our 3D scene graph representation allows to quickly and incrementally compute a tree-decomposition of the 3D scene graph, which \u2014together with our bound on the scene graph treewidth\u2014 ensures that the neural tree runs in real-time on an embedded GPU.\nContribution 3: Persistent Representations via Hierarchical Loop Closure Detection and 3D Scene Graph Optimization (Section IV). Building a persistent map representation requires the robot to recognize it is revisiting a location it has seen before, and correcting the map accordingly. We propose a novel hierarchical approach for loop closure detection: the approach involves (i) a top-down loop closure detection that uses hierarchical descriptors \u2014capturing statistics across layers in the scene graph\u2014 to find putative loop closures, and (ii) a bottom-up geometric verification that attempts estimating the loop closure pose by registering putative matches. Then, we propose the first algorithm to optimize a 3D scene graph in response to loop closures; our approach relies on embedded deformation graphs [17] to simultaneously and consistently correct all the layers of the scene graph, including the 3D mesh, objects, places, and the robot trajectory.\nContribution 4: Hydra, a Real-Time Spatial Perception System (Section V). We conclude the paper by integrating the proposed algorithms into a highly parallelized perception system, named Hydra, that combines fast early and mid-level perception processes (e.g., local mapping) with slower highlevel perception (e.g., global optimization of the scene graph). We demonstrate Hydra in challenging simulated and real datasets, across a variety of environments, including an apartment complex, an office building, and two student residences. Our experiments (Section VI) show that (i) we can reconstruct 3D scene graphs of large, real environments in real-time, (ii) our online algorithms achieve an accuracy comparable to batch offline methods and build a richer representation compared to competing approaches [7], and (iii) our loop closure detection approach outperforms standard approaches based on bag-ofwords and visual-feature matching in terms of quality and quantity of detected loop closures. The source code of Hydra is publicly available at https://github.com/MIT-SPARK/Hydra.\nNovelty with respect to [11, 16]. This paper builds on our previous conference papers [11, 16] but includes several novel findings. First, rather than postulating a 3D scene graph structure as done in [11], we formally show that hierarchical representations are crucial to achieve scalable scene understanding and fast inference. Second, we propose a novel room segmentation method based on the notion of persistent homology, as a replacement for the heuristic method in [11]. Third, we develop novel learning-based hierarchical descriptors for place recognition, that further improve performance with respect to the handcrafted hierarchical descriptors in [11]. Fourth, the real-time system described in this paper is able to also assign room labels, leveraging the neural tree architecture from [16]; while [16] uses the neural tree over small graphs corresponding to a single room, in this paper\nwe provide an efficient way to obtain a tree-decomposition of the top layers of the scene graph, hence extending [16] to simultaneously operate over all rooms and account for their relations. Finally, this paper includes further experiments and evaluations on real robots (Clearpath Jackal robots and Unitree A1 robots) and comparisons with recent scene graph construction baselines [7]."
        },
        {
            "heading": "II. SYMBOL GROUNDING AND THE NEED FOR HIERARCHICAL REPRESENTATIONS",
            "text": "The goal of this section is twofold. First, we remark that in order to support the execution of complex instructions, map representations must be metric-semantic, and hence ground symbolic knowledge (i.e., semantic aspects in the scene) into the map geometry (Section II-A). Second, we observe that \u201cflat\u201d representations, which naively store semantic attributes for each geometric entity in the map (e.g., attach semantic labels to each voxel in an ESDF) scale poorly in the size of the environment; on the other hand, hierarchical representations scale better to large environments (Section II-B) and enable efficient inference (Section II-C). We conclude the section by introducing the notion of 3D scene graph, an example of hierarchical representation for indoor environments, and discussing its structure and properties (Section II-D)."
        },
        {
            "heading": "A. Symbols and Symbol Grounding",
            "text": "High-level instructions issued by humans (e.g., \u201cgo and pick up the chair\u201d) involve symbols. A symbol is the representation of a concept that has a specific meaning for a human (e.g., the word \u201cchair\u201d is a symbol in the sense that, as humans, we understand the nature of a chair). For a symbol to be useful to guide robot action, it has to be grounded into the physical world. For instance, for the robot to correctly execute the instruction \u201cgo and pick up the chair\u201d, the robot needs to ground the symbol \u201cchair\u201d to the physical location the chair is situated in.1 In principle, we could design the perception system of our robot to ground symbols directly in the sensor data. For instance, a robot with a camera could map image pixels to appropriate symbols (e.g., \u201cchair\u201d, \u201cfurniture\u201d, \u201ckitchen\u201d, \u201capartment\u201d), as commonly done in 2D semantic segmentation [19]. However, grounding symbols directly into sensor data is not scalable: sensor data (e.g., images) is collected at high rates and is relatively expensive to store. This is neither convenient nor efficient when grounding symbols for long-term operation. Instead, we need intermediate (or subsymbolic) representations that compress the raw sensor data into a more compact format, and that can be used to ground symbols. Traditional geometric map models used in robotics (e.g., 2D occupancy grids or 3D voxel-based maps) can be understood as sub-symbolic representations: each cell in a voxel grid does not represent a semantic concept, and instead is used to ground two symbols: \u201cobstacle\u201d and \u201cfree-space\u201d. Therefore, this paper is concerned with building metricsemantic representations, which ground symbolic knowledge into geometric (i.e., sub-symbolic) representations.\n1In general, this is referred to as the symbol grounding problem [18] in artificial intelligence.\nWhich Symbols Should a Map Contain? In this paper, we directly specify the set of symbols the robot is interested in (i.e., certain classes of objects and rooms, to support navigation in indoor environments). However, it is worth discussing how to choose the symbols more generally. The choice of symbols clearly depends on the task the robot has to execute. A mobile robot may implement a path planning query by just using the \u201cfree-space\u201d symbol and the corresponding grounding, while a more complex domestic robot may instead need additional symbols (e.g., \u201ccup of coffee\u201d, \u201ctable\u201d, and \u201ckitchen\u201d) and their groundings to execute instructions such as \u201cbring me the cup of coffee on the table in the kitchen\u201d. A potential way to choose the symbols to include in the map therefore consists of inspecting the set of instructions the robot has to execute, and then extracting the list of symbols (e.g., objects and relations of interest) these instructions involve. For instance, in this paper, we mostly focus on indoor environments and assume the robot is tasked with navigating to certain objects and rooms in a building. Therefore, the symbols we include in our maps include free-space, objects, rooms, and buildings. After establishing the list of relevant symbols, the goal is for the robot to build a compact sub-symbolic representation (i.e., a geometric map), and then ground these symbols into the subsymbolic representation. A pictorial representation of this idea is given in Fig. 2a: the robot builds an occupancy map (subsymbolic representation) and attaches a number of semantic labels (symbols) to each cell. We refer to this representation as \u201cflat\u201d since each cell is assigned a list of symbols of interest. As we will see shortly, there are more clever ways to store the same information."
        },
        {
            "heading": "B. Hierarchical Representations Enable Large-Scale Mapping",
            "text": "While the flat representation of Fig. 2a may contain all the information the robot needs to complete its task, it is unnecessarily expensive to store. Here we show that the symbolic knowledge underlying such representation naturally\nlends itself to be efficiently stored using a hierarchical representation.\nConsider the case where we use a flat representation (as shown in Fig. 2a) to represent a 3D scene and assume our dictionary contains L symbols of interest. Let \u03b4 be the cell (or voxel) size and call V the volume of the scene the representation describes. Then, the corresponding metricsemantic representation would require a memory of:\nm = O ( L \u00b7 V/\u03b43 ) (1)\nto store L labels for each voxel. Note that m grows with the number of symbols L, multiplied by the size of the subsymbolic representation V/\u03b43 (i.e., the number of voxels in the volume). When mapping large environments the term V/\u03b43 quickly becomes unsustainably large; for instance, covering a 10km \u00d7 10km area with a 10cm grid resolution, even disregarding the vertical dimension, would require 1010 voxels. In addition, it would be desirable for the next generation of robots to execute a large set of tasks, hence requiring them to ground a large dictionary of symbols L.2 The size of the sub-symbolic representation, the number of symbols, and their multiplicative relation in (1) make a flat metric-semantic representation unsuitable for large-scale spatial perception.\nA key observation here is that multiple voxels encode the same grounded symbols (e.g., a chair). In addition, many symbols naturally admit a hierarchical organization where higher-level concepts (i.e., buildings or rooms for indoor environments) contain lower-level symbols (e.g., objects). This suggests that we can more efficiently store information hierarchically, where all voxels associated with a certain object (e.g., all voxels belonging to a chair) are mapped to the same symbolic node (e.g., an object node with the semantic label \u201cchair\u201d), object nodes are associated with the room they belong to, room nodes are associated to the apartment unit they belong to, and so on. This transforms the flat representation of\n2For reference, the English dictionary includes around 500, 000 words.\n5 Fig. 2a into the hierarchical model of Fig. 2b, where only the lowest level symbols are directly grounded into voxels, while the top layers are arranged hierarchically. This hierarchical representation is more parsimonious and reduces the required memory to\nm = O ( V/\u03b43 +Nobjects +Nrooms + \u00b7 \u00b7 \u00b7+Nbuildings ) , (2)\nwhere Nlayer (with layer \u2208 {objects, rooms, . . .}) is the number of symbols at the specific layer of the hierarchy. This representation is more memory efficient than (1) because it decouples the number of symbols from the size V/\u03b43 of the sub-symbolic representation. For instance, the scene in Fig. 2 has 336 voxels, and we assume L = 5.3 Then, the flat representation of Fig. 2a involves storing 1680 symbols, while Fig. 2b only requires storing 355 symbols and 354 edges describing their hierarchical relations. Crucially, the compression we performed when moving from Fig. 2a to Fig. 2b is lossless: the two representations contain exactly the same amount of information.\nWhile we \u201ccompressed\u201d the symbolic representation using a hierarchical data structure, the term V/\u03b43 in (2) that corresponds to the sub-symbolic layer is still impractically large for many applications. Therefore, our robots will also typically need some compression mechanism for the sub-symbolic layer that provides a more succinct description of the occupied and free space as compared to voxel-based maps. Fortunately, the mapping literature already offers alternatives to standard 3D voxel-based maps such as OctTree [20] or neural implicit representations [21]. In general, this compression reduces the memory requirement to\nm = O (Nsub-sym +Nobjects +Nrooms + \u00b7 \u00b7 \u00b7+Nbuildings) , (3)\nwhere Nsub-sym is the size of the compressed sub-symbolic representation. The behavior of Nsub-sym is driven by the compression approach used, but in general Nsub-sym ends up being much smaller than V/\u03b43; in the ideal case Nsub-sym would grow gracefully with respect to the complexity of the scene and the resolution required by the task. We show a notional example of a compressed sub-symbolic layer in Fig. 2c. This can represent the 336 original cells of the subsymbolic layer of Fig. 2b with roughly 23 nodes, 22 edges, and a 2D polygonal mesh with roughly 132 vertices. Note that moving from Fig. 2b to Fig. 2c may entail a lossy compression of the sub-symbolic layer, i.e., the compressed representation may only be an approximate representation of the geometry of the environment. We consider this to be a feature rather than a bug: the general idea is that we can always compress the sub-symbolic representation to fit into the available memory of our robot, although such a compression might induce some performance degradation in the execution of the task (e.g., coarser maps might lead to longer paths in motion planning)."
        },
        {
            "heading": "C. Hierarchical Representations Enable Efficient Inference",
            "text": "While above we showed that hierarchical representations are more scalable in terms of memory requirements, this section\n3The symbols are stored as building type, apartment type, room type, object type, and free-space/obstacle.\nshows that the graphs underlying hierarchical representations also enable fast inference. Specifically, we show that these hierarchical graphs have small treewidth: their treewidth does not grow with the size of the graph (i.e., proportionally to the size of the explored scene), but rather with the treewidth of each layer in the hierarchy. The treewidth is a well-known measure of complexity for many problems on graphs [22, 23, 24, 25]. Chandrasekaran et al. [26] show that the graph treewidth is the only structural parameter that influences tractability of probabilistic inference on graphical models: while inference is NP-hard in general for inference on graphical models [27], proving that a graph has small treewidth opens the door to efficient, polynomial-time inference algorithms. Additionally, in our previous work we have shown that the treewidth is also the main factor impacting the expressive power and tractability of novel graph neural tree architectures, namely neural trees [16]. The results in this section therefore open the door to the efficient use of powerful tools for learning over graphs; see Section III-D2.\nIn the rest of this section, we formalize the notion of hierarchical graph and show that the treewidth of a hierarchical graph is always bounded by the maximum treewidth of each of its layers. We do this by proving that the tree decomposition of the hierarchical graph can be obtained by a suitable concatenation of the tree decompositions of its layers. The results in this section are general and apply to arbitrary hierarchical representations (as defined below) beyond the representations of indoor environments we consider later in the paper.\nDefinition 1 (Hierarchical Graph). A graph G = (V, E) is said to be a hierarchical graph if the set of nodes V can be partitioned into ` layers, i.e., V = \u222a`i=1Vi, such that\n1) single parent: each node v \u2208 Vi at layer i shares an edge with at most one node in the layer Vi+1 above, 2) locality: each v \u2208 Vi can only share edges with nodes in the same or adjacent layers, i.e., Vi\u22121, Vi, or Vi+1, 3) disjoint children: for any u, v \u2208 Vi the children of u and v, namely C(v) and C(u) are disjoint (i.e., they share no nodes or edges), where\nC(u) , {w \u2208 Vi\u22121 | (w, u) \u2208 E} (4)\ndenotes the children of u, i.e., the set of nodes in layer Vi\u22121 sharing an edge with u. We refer to G as an `-layered hierarchical graph and denote with Vi the set of nodes in layer i. Moreover, we conventionally order the layers from the bottom to the top, i.e., the lowest layer is V1, while the top layer is V`.\nTo gain some insight into Definition 1, consider a hierarchical graph where the bottom layer V1 describes the objects in the environment, while the higher layers V2 and V3 describe room and buildings, respectively. Then, the first condition in Definition 1 requires that each object belongs to a single room, and each room belongs to a single building. The second condition restricts the inter-layer edges to connect objects to rooms, and rooms to buildings. Finally, the third condition says that objects in a room are not connected to objects in another room, and that rooms in a building do not share edges\n6 Algorithm 1: Tree-Decomposition of Hierarchical Graphs Data: `-layered hierarchical graph\nG = (V=\u222a`i=1Vi, E) Result: Tree decomposition T = (B,E) /* extract sub-graph with nodes V` */ 1 G` \u2190 G[V`]; /* tree decomposition of G` */ 2 T = (B,E)\u2190 TD [G`]; 3 for each layer i = `, . . . , 2 do 4 for each node v in Vi do 5 Tv = (Bv, Lv)\u2190 TD [G[C(v)]]; 6 T \u2032\nv \u2190 Tv + {v}; 7 Pick any bag b in T such that v \u2208 b; 8 Pick any bag b\u2032 in T \u2032\nv; 9 T \u2190 T \u2295 T \u2032 \u2295 (b, b\u2032);\nwith rooms in other buildings. These conditions are relatively mild; we note that they are easily met when the edges in the graph represent inclusion or adjacency (i.e., for the graphs considered in the rest of this paper).\nWe now show that a tree decomposition of a hierarchical graph can be constructed by concatenating the tree decomposition of each of its layers. This result will allow us to obtain the treewidth bound in Proposition 3. The resulting tree decomposition algorithm (Algorithm 1) will also be used in the neural tree approach used for room classification in Section III-D2. The interested reader can find a refresher about tree decomposition and treewidth in Appendix A4 and an example of execution of Algorithm 1 in Fig. 3.\nTheorem 2 (Tree Decomposition of Hierarchical Graph). Let G = (V = \u222a`i=1Vi, E) be an `-layered hierarchical graph. Let T be the tree decomposition of the sub-graph spanned by V` (top layer) and Tv be the tree decomposition of the subgraph spanned by the children Ci(v) of v, for v \u2208 Vi and i = `, . . . , 2. Then a tree decomposition for graph G can be constructed from {Tv | v \u2208 Vi and i = `, . . . , 2} and T , by the simple concatenation procedure described in Algorithm 1.\nProof: (i) Notation: We denote the tree decomposition of a graph G by T = (B,E) = TD [G], where B denotes the collection of bags and E denotes the set of edges in the tree decomposition graph T . For a tree decomposition graph T = (B,E) and any node w (not already included in any of the bags b \u2208 B), we denote T +{w} to be the tree decomposition T , after adding the element w to every bag in T . Finally, given two disjoint trees T, T \u2032 and an edge (b, b\u2032) with b \u2208 T and b\u2032 \u2208 T \u2032, we use T \u2190 T \u2295 T \u2032 \u2295 (b, b\u2032) to indicate updating graph T by adding graph T \u2032, along with the edge (b, b\u2032) to it.\n(ii) Intuitive Explanation of Algorithm 1: We form the tree decomposition T of the hierarchical graph sequentially. We initialize T with the tree decomposition of the top layer graph G` = G[V`]. We then augment T with the tree decomposition graphs Tv of C(v), for each v \u2208 V`. We augment bags of Tv\n4We leave the proof of Theorem 2 in the main text since it contains a description of Algorithm 1, while we postpone other proofs to the appendix.\nwith element {v} to mark the fact that v connects each node in C(v) (in graph G). This procedure is carried out for the remaining layers i = `\u2212 1, . . . , 2 and all nodes v \u2208 Vi.\nFigure 3 shows an example of execution of Algorithm 1 for the graph in Fig. 3a. Figure 3b shows the (disconnected) tree decompositions produced by line 2 (which produces the single green bag B1) and the first execution of line 5 (i.e., for i = `, which produces the two red bags). Figure 3c shows the result produced by the first execution of line 6 and line 9, which adds B1 to all the red bags, and then connects the two tree decompositions with an edge, respectively. Figures 3d and 3e show the result produced by the next iteration of the algorithm.\n(iii) Proof : Recall that a tree decomposition T = (B,E) of a graph G = (V, E) must satisfy the following conditions: C1 T must be a tree; C2 Every bag b \u2208 B must be a subset of nodes of the given\ngraph G, i.e., b \u2286 V; C3 For all edges (u, v) \u2208 E in the given graph G, there must\nexist a bag b \u2208 B such that u, v \u2208 b; C4 For every node v \u2208 V in the given graph G, the set of\nbags {b \u2208 B | v \u2208 b} must form a connected component in T ; C5 For every node v \u2208 V in the given graph G, there must exist a bag b \u2208 B that contains it, i.e., v \u2208 b. To state this in another way: \u222ab\u2208B{v \u2208 b} = V .\nAlgorithm 1 constructs a tree decomposition T of G sequentially by exploring the graph, by layers (going from layer-` to layer 1) and by nodes v in each layer. Let Get denote the subgraph of G that is explored by Algorithm 1, until iteration t. Here, we count an iteration to be the execution of Line 9, and count the initialization at Line 2 as the first iteration (i.e., the first iteration initializes the tree decomposition, while the subsequent iterations update the tree decomposition).\nWe prove that, at any iteration t, the tree decomposition graph T constructed in Algorithm 1 is a valid tree decomposition of the graph Get . We prove this by induction. At t = 1, the explored graph Ge1 is nothing but G`. Note that T , after iteration 1 (i.e., after the initialization at Line 2) by definition equals the tree decomposition Ge1 . Now we only need to prove that subsequent iterations produce a tree decomposition of Get . Each new iteration (i.e., execution of Line 9) updates T to include T \u2032\nv and the edge (b, b \u2032). The new explored graph\nis Get+1 = Get \u2295 G[C(v) + {v}]. It suffices to argue that T \u2295 T \u2032v \u2295 (b, b\u2032) (Lines 5 to 9) is a valid tree decomposition of Get+1 = Get \u2295G[C(v) + {v}]. We argue this by proving that the requirements C1-C5 are satisfied.\nFor C1, we note that T \u2295 T \u2032v \u2295 (b, b\u2032) is a tree as T and T \u2032\nv are disjoint trees and (b, b \u2032) is an edge connecting the two\ntrees, T and T \u2032 v . C2 is satisfied because T and T \u2032\nv are tree decompositions of sub-graphs of G. C3 is satisfied for all edges in Get and in G[C(v)]. The only edges that remain are those that connect node v to nodes in C(v), namely {(v, u) | u \u2208 C(v)}. We note that by adding {v} to every bag of Tv to obtain T \u2032\nv we get that all such edges are also included in the bags of T \u2032\nv . C4 is satisfied again because T and Tv are valid tree decompositions and T \u2032\nv is built from Tv by adding v to all bags of Tv . As Tv is a tree graph disjoint from T , we retain\n7 B1\nR1 R2 R3\nR4\nO1 O2 O3\nO4\nO5\nO6\nO7 O8\n(a)\nB1\nR1, R2 R2, R3, R4\nR1, O1\nR2, O2\nR3, O4\nR4, O5, O6, O7\nR4, O7, O8\nR2, O3(b)\nB1\nB1, R1, R2 B1, R2, R3, R4\nR1, O1\nR2, O2\nR3, O4\nR4, O5, O6, O7\nR4, O7, O8\nR2, O3(c)\nB1\nB1, R1, R2 B1, R2, R3, R4\nO1\nO2\nO4\nO5, O6, O7\nO7, O8\nO3\n(d)\nB1\nB1, R1, R2 B1, R2, R3, R4\nR1, O1\nR2, O2\nR3, O4\nR4, O5, O6, O7\nR4, O7, O8\nR2, O3\n(e)\nFig. 3: (a) Hierarchical object-room-building graph with objects O1, O2, . . . O8, rooms R1, R2, R3, R4, and a single building B1. (b) Tree decomposition T of the building B1 and associated Tv of the rooms (i.e., children of B1) as computed in line 2 and 5 of Algorithm 1. (c) T after adding B1 to every bag of the tree decomposition of the rooms and joining Tv to T (line 6 and line 9 in Algorithm 1). (d) T and associated Tv for the object nodes. (e) Final tree-decomposition of the objectroom-building graph formed by the concatenation procedure described in Algorithm 1.\nproperty C4. Get+1 = Get \u2295 G[C(v) + {v}] (i.e., nodes in Get and C(v)), have a bag in T \u2295 T \u2032v \u2295 (b, b\u2032).\nTheorem 2 above showed that we can easily build a tree decomposition of a hierarchical graph by cleverly \u201cgluing together\u201d tree decompositions of sub-graphs in the hierarchy. Now we want to use that result to bound the treewidth of a hierarchical graph G. Each node in the tree decomposition is typically referred to as a bag, and intuitively represents a subset of nodes in the original graph. The treewidth of a tree decomposition T is defined as the size of the largest bag minus one (see Appendix A). The treewidth of a graph G is then defined as the minimum treewidth that can be achieved among all tree decompositions of G. Since Algorithm 1 describes one such tree decompositions, Theorem 2 implies the following upper bound on the treewidth of a hierarchical graph.\nProposition 3 (Treewidth of Hierarchical Graph). The\ntreewidth of an `-layered hierarchical graph G is bounded by the treewidths of the sub-graphs G[C(v)] spanned by the children C(v) of every node v in G and the treewidth of the graph G[V`] spanned by all layer-` nodes:\ntw [G] \u2264 max {\nmax v\u2208V {tw [G[C(v)]] + 1}, tw [G[V`]]\n} . (5)\nIntuitively, the proposition states that the treewidth of a hierarchical graph does not grow with the number of nodes in the graph, but rather with the treewidth (roughly speaking, the complexity) of each layer in the graph.\nD. 3D Scene Graphs and Indoor Environments\nWhile the considerations in the previous sections apply to generic hierarchical representations, in this section we tailor the discussion to a specific hierarchical representation, namely a 3D scene graph, and discuss its structure and properties when such representation is used to model indoor environments.\nChoice of Symbols for Indoor Navigation. We focus on indoor environments and assume the robot is tasked with navigating to certain objects and rooms in a building. Therefore, we include the following groups of symbols that form the layers in our hierarchical representation: free-space, obstacles, objects and agents, rooms, and building. The details of the label spaces that we use for the object and room symbols can be found in Appendix D. The only \u201cagent\u201d in our setup is the robot mapping the environment. In terms of relations between symbols, we mostly consider two types of relations: inclusion (i.e., the chair is in the kitchen) and adjacency or traversability (i.e., the chair is near the table, the kitchen is adjacent to/reachable from the dining room).\nAs we mentioned in Section II-B, this choice of symbols and relations is dictated by the tasks we envision the robot to perform and therefore it is not universal. For instance, other tasks might require breaking down the free-space symbol into multiple symbols (e.g., to distinguish the front from the back of a room), or might require considering other agents (e.g., humans moving in the environment as in [2, 4]). This dependence on the task also justifies the different definitions of 3D scene graph found in the recent literature: for instance, the original proposal in [3] focuses on visualization and humanmachine interaction tasks, rather than robot navigation, hence the corresponding scene graphs do not include the free-space as a symbol; the proposals [7, 5] consider smaller-scale tasks and disregard the room and building symbols. Similarly, the choice of relations is task dependent and can include a much broader set of relations beyond inclusion and adjacency. For instance, relations can describe attributes of an object (e.g., \u201chas color\u201d), material properties (e.g., \u201cis made of\u201d), can be used to compare entities (e.g., \u201cis taller than\u201d), or may encode actions (e.g., a person \u201ccarries\u201d an object, a car \u201cdrives on\u201d the road). While these other relations are beyond the scope of our paper, we refer the reader to [28] for further discussion.\nChoice of Sub-symbolic Representations. We ground each symbol in compact sub-symbolic representations; intuitively, this reduces to attaching geometric attributes to each symbol observed by the robot. We ground the \u201cobstacle\u201d symbol\n8 into a 3D mesh describing the observed surfaces in the environment. We ground the \u201cfree-space\u201d symbol using a places sub-graph, which can be understood as a topological map of the environment. Specifically, each place node in the graph of places is associated with an obstacle-free 3D location (more precisely, a sphere of free space described by a centroid and radius), while edges represent traversability between places.5 We ground the \u201cagent\u201d symbol using the pose-graph describing the robot trajectory [30]. We ground\neach \u201cobject\u201d, \u201croom\u201d, and \u201cbuilding\u201d symbol using a centroid and a bounding box. Somewhat redundantly, we also store the mesh vertices corresponding to each object, and the set of places included in each room, which can be also understood as additional groundings for these symbols. As discussed in the next section, this is mostly a byproduct of our algorithms, rather than a deliberate choice of sub-symbolic representation.\n3D Scene Graphs for Indoor Environments The choices of symbolic and sub-symbolic representations outlined above lead to the 3D scene graph structure visualized in Fig. 1. In particular, Layer 1 is a metric-semantic 3D mesh. Layer 2 is a sub-graph of objects and agents; each object has a semantic label (which identifies the symbol being grounded), a centroid, and a bounding box (providing the grounding); each agent is modeled by a pose graph describing its trajectory (in our case the robot itself is the only agent). Layer 3 is a subgraph of places (i.e., a topological map) where each place is an obstacle-free location and an edge between places denotes straight-line traversability. Layer 4 is a sub-graph of rooms where each room has a centroid, and edges connect adjacent rooms. Layer 5 is a building node connected to all rooms (we assume the robot maps a single building). Edges connect nodes within each layer (e.g., to model traversability between places or rooms, or connecting nearby objects) or across layers (e.g., to model that mesh vertices belong to an object, that an object is in a certain room, or that a room belongs to a building). Note that the 3D scene graph structure forms a hierarchical graph meeting the requirements of Definition 1.\nTreewidth of Indoor 3D Scene Graphs. In Section II-C we concluded that the treewidth of a hierarchical graph is bounded by the treewidth of its layers. In this section, we particularize the treewidth bound to 3D scene graphs modeling indoor environments. We first prove bounds on the treewidth of key layers in the 3D scene graph (Lemmas 4 and 5). Then we translate these bounds into a bound on the treewidth of the object-room-building sub-graph of a 3D scene graph (Proposition 6); the bound is important in practice since we will need to perform inference over such a sub-graph to infer the room (and potentially building) labels, see Section III-D2.\nWe start by bounding the treewidth of the room layer.\nLemma 4 (Treewidth of Room Layer). Consider the subgraph of rooms, including the nodes in the room layer of a 3D scene graph and the corresponding edges. If each room connects to at most two other rooms that have doors (or more\n5While we postpone the technical details to Section III-C, the graph of places can be understood as a sparse approximation of a Generalized Voronoi Diagram (GVD) of the environment, a data structure commonly used in computer graphics and computational geometry to compress shape information [29].\n0.0 0.5 1.0 1.5 2.0 2.5 Treewidth Upper-Bound\n0\n20\n40\n60\nPe rc\nen t\n(a)\n5 10 15 20 Treewidth Upper-Bound\n0\n5\n10\n15\nPe rc\nen t\n(b)\nFig. 4: Histogram of the treewidth upper-bounds for (a) the room sub-graph and (b) the object sub-graph of the 3D scene graphs obtained using the 90 scenes of the Matterport3D dataset [31]. We use the minimum-degree and the minimumfill-in heuristic to obtain treewidth upper-bounds [32]. We then compute an upper-bound to be the lowest of the two.\ngenerally passage ways) leading to other rooms, then the room graph has treewidth bounded by two.\nThe insight behind Lemma 4 is that the room sub-graph is not very far from a tree (i.e., it has low treewidth) as long as there are not many rooms with multiple doors (or passage ways). In particular, the room sub-graph has treewidth equal to 2 if each room has at most two passage ways to other rooms with multiple entries. Note that the theorem statement allows a room to be connected to an arbitrary number of rooms with a single entry (i.e., a corridor leading to multiple rooms that are only accessible via that corridor). Figure 4a reports empirical upper-bounds of the treewidth of the room sub-graphs in the 90 scenes of the Matterport3D dataset [31] (see Section VI for more details about the experimental setup). The observed treewidth is at most 2, confirming that the conclusions of Lemma 4 hold in several apartment-like environments.\nLemma 5 (Treewidth of Object Layer). Consider the subgraph of objects, which includes the nodes in the object layer of a 3D scene graph and the corresponding edges. The treewidth of the object sub-graph is bounded by the maximum number of objects in a room.\nThe result is a simple consequence of the fact that in our 3D scene graph, there is no edge connecting objects in different rooms, therefore, the graph of objects includes disconnected components corresponding to each room, whose treewidth is bounded by the size of that connected component, i.e., the number of objects in that room. Figure 4b reports the treewidth upper-bounds for the object sub-graphs in the Matterport3D dataset. We observe that the treewidth of the object sub-graphs tends to be larger compared to the room sub-graphs, but still remains relatively small (below 20) in all tests.\nWe can now conclude with a bound on the treewidth of the object-room-building graph in a 3D scene graph.\nProposition 6 (Treewidth of the Object-Room-Building Graph). Consider the object-room-building graph of a building, including the nodes in the object, room, and building layers of a 3D scene graph and the corresponding edges. Assume the treewidth of the room graph is less than the treewidth of the object graph. Then, the treewidth tw[G] of\n9 the object-room-building graph G is bounded by\ntw[G] \u2264 1 +No, (6)\nwhere No denotes the largest number of objects in a room.\nProposition 6 indicates that the treewidth of the objectroom-building graph does not grow with the size of the scene graph, but rather depends on how cluttered each room is. This is in stark contrast with the treewidth of social network graphs [33, 16], and further motivates the proposed hierarchical organization. The treewidth bounds in this section open the door to tractable inference techniques; in particular, in Section III-D2, we show they allow applying novel graphlearning techniques, namely, the neural tree [16]."
        },
        {
            "heading": "III. REAL-TIME INCREMENTAL",
            "text": ""
        },
        {
            "heading": "3D SCENE GRAPH LAYERS CONSTRUCTION",
            "text": "This section describes how to estimate an odometric 3D scene graph directly from visual-inertial data as the robot explores an unknown environment. Section IV then shows how to correct the scene graph in response to loop closures.\nWe start by reconstructing a metric-semantic 3D mesh to populate Layer 1 of the 3D scene graph (Section III-A), and use the mesh to extract the objects in Layer 2 (Section III-B). We extract the places in Layer 3 as a byproduct of the 3D mesh computation by first computing a Generalized Voronoi Diagram (GVD) of the environment, and then approximating the GVD as a sparse graph of places (Section III-C). We then populate the rooms in Layer 4 by segmenting their geometry using persistent homology (Section III-D1) and assigning them a semantic label using the neural tree (Section III-D2). In this paper, we assume that the scene we reconstruct consists of a single building, and for Layer 5 we instantiate a single building node that we connect to all estimated room nodes."
        },
        {
            "heading": "A. Layers 1: Mesh",
            "text": "We build a metric-semantic 3D mesh (Layer 1 of the 3D scene graph) using Kimera [34] and Voxblox [10]. In particular, we maintain a metric-semantic voxel-based map within an active window around the robot. Each voxel of the map contains free-space information and a distribution over possible semantic labels. We gradually convert this voxelbased map into a 3D mesh using marching cubes, attaching a semantic label to each mesh vertex. This is now standard practice (e.g., the same idea was used in [35] without inferring the semantic labels) and the expert reader can safely skip the rest of this subsection.\nIn more detail, for each keyframe6 we use a 2D semantic segmentation network to obtain a pixel-wise semantic segmentation of the RGB image, and reconstruct a depth-map using stereo matching (when using a stereo camera) or from the depth channel of the sensor (when using an RGB-D camera). We then convert the semantic segmentation and depth into a semantically-labeled 3D point cloud and transform it\n6Keyframes are selected camera frames chosen by the visual-inertial odometry pipeline, see Section III-B and [2] for further details.\naccording to the odometric estimate of the robot pose (Section III-B). We use Voxblox [10] to integrate the semanticallylabeled point cloud into a Truncated Signed Distance Field (TSDF) using ray-casting, and Kimera [2] to perform Bayesian updates over the semantic label of each voxel during raycasting. Both Voxblox [10] and Kimera [2] operate over an active window, i.e., only reconstruct a voxel-based map within a user-specified radius ra around the robot (ra = 8 m in our tests)7 to bound the memory requirements. Within the active window, we extract the 3D metric-semantic mesh using Voxblox\u2019 marching cubes implementation, where each mesh vertex is assigned the most likely semantic label from the corresponding voxel. We then use spatial hashing [36] to integrate Ma, the mesh inside the active window, into the full mesh Mf , optionally also compressing the mesh to a given resolution. The use of the active window circumvents the need to maintain a monolithic and memory-hungry voxelbased representation of the entire environment (as done in our original proposal in [34]), and instead allows to gradually transform the voxel-based map in the active window (moving with the robot) into a lighter-weight 3D mesh. The full mesh, Mf , is uncorrected for odometric drift, and we address loop closure detection and optimization in Section IV."
        },
        {
            "heading": "B. Layers 2: Objects and Agents",
            "text": "Agents. The agent layer consists of the pose graph describing the robot trajectory (we refer the reader to [2] for extensions to multiple agents, including humans). During exploration, the odometric pose graph is obtained using stereo or RGB-D visual-inertial odometry, which is also available in Kimera [34, 2]. The poses in the pose graph correspond to the keyframes selected by Kimera, and for each pose we also store visual features and descriptors, which are used for loop closure detection in Section IV. As usual, edges in the pose graph correspond to odometry measurements between consecutive poses, while we will also add loop closures as described in Section IV.\nObjects. The object layer consists of a graph where each node corresponds to an object (with a semantic label, a centroid, and a bounding box) and edges connect nearby objects. After extracting the 3D mesh within the active window, we segment objects by performing Euclidean clustering [37] on Va, the vertices ofMa. In particular, we independently cluster the sets of vertices Via with the same semantic label i (where Via = {v \u2208 Va : label(v) = i}) for all semantic labels of interest. As in Kimera [2], each resulting cluster is then used to estimate a centroid and bounding box for each putative object. After each update, if a newly-detected object overlaps with an existing object node of the same semantic class in the scene graph, we merge them together by adding new mesh vertices to the previous object node; otherwise we add the new object as a new node. In practice, we consider two objects overlapping if the centroid of one object is contained in the other object\u2019s\n7The radius of the active window has to be larger than the maximum raycasting distance to construct the TSDF (\u22484m) and the block resolution of the spatial hashing (\u22481.5m).\n10\nbounding box, a proxy for spatial overlap measures such as Intersection over Union (IoU).8"
        },
        {
            "heading": "C. Layers 3: Places",
            "text": "We build the places layer by computing a Generalized Voronoi Diagram (GVD) of the environment, and then sparsifying it into a graph of places. While the idea of sparsifying the GVD into a graph has appeared in related work [2, 40], these works extract such representations from a monolithic ESDF of the environment, a process that is computationally expensive and confined to off-line use ([2] reports computation times in the order of tens of minutes). Instead, we combine and adapt the approaches of Voxblox [10], which presents a incremental version of the brushfire algorithm that converts a TSDF to an ESDF, and of Lau et al. [41], who present an incremental version of the brushfire algorithm that is capable of constructing a GVD during the construction of the ESDF, but takes as input a 2D occupancy map. As a result, we show how to obtain a local GVD and a graph of places as a byproduct of the 3D mesh construction.\nFrom Voxels to the GVD. The GVD is a data structure commonly used in computer graphics and computational geometry to compress shape information [29]. The GVD is the set of voxels that are equidistant to at least two closest obstacles (\u201cbasis points\u201d or \u201cparents\u201d), and intuitively forms a skeleton of the environment [29] (see Fig. 5a). Following the approach in [41], we use the fact that voxels belonging to the GVD can be easily detected from the wavefronts of the brushfire algorithm used to create and update the ESDF in the active window. Intuitively, GVD voxels have the property that multiple wavefronts \u201cmeet\u201d at a given voxel (i.e., fail to update the signed distance of the voxel), as these voxels are equidistant from multiple obstacles. The brushfire algorithm (and its incremental variant) are traditionally seeded at voxels containing obstacles (e.g., [41]), but as mentioned previously,\n8In previous work [2], we also detected objects with known shape using 3D registration [38]; in practice, we see that such approach only works well for large objects since small objects are not well-described by the 3D mesh, whose resolution is implicitly limited by the resolution of the voxel-based map in the active window (10 cm in our tests). A more promising approach is to directly ground small 3D objects (e.g., a pen) from sensor data as in [39].\nAlgorithm 2: Places Sparsification Data: GVD graph Ga, previous map of spatial hashes\nto voxel clusters C, spatial resolution \u03b4p Result: New or updated nodes N and edges E\n1 N \u2190 \u2205 2 E \u2190 \u2205 3 Vu \u2190 {voxel v \u2208 Ga : updated(v) = true} 4 for voxel v \u2208 Vu do 5 hv \u2190 hash(v, \u03b4p) 6 found \u2190 false 7 for cluster c \u2208 C[hv] do 8 if v neighbors any voxel in cluster c then 9 c\u2190 c \u222a {v}\n10 found \u2190 true 11 break\n12 if \u00acfound then 13 C[hv]\u2190 C[hv] \u222a {v}\n/* add node corresponding to cluster of v to set of returned nodes */\n14 N \u2190 N \u222a {node(v)} 15 for voxel n \u2208 neighbors(Ga, v) do 16 hn = hash(n, \u03b4p) 17 if node(v) 6= node(n) \u2227 hv = hn then 18 N \u2190 N \\ {node(n)} 19 E \u2190 E \\ {e \u2208 E : node(n) \u2208 e} 20 Merge clusters for nodes n and v 21 else if node(v) 6= node(n) then 22 E \u2190 E \u222a {(node(v), node(n))}\nwe follow the approach of [10] and use the TSDF to seed the brushfire wavefronts instead. In particular, we track all TSDF voxels that correspond to zero-crossings (i.e., voxels containing a surface) when creatingMa using marching cubes and then use these voxels as a surrogate occupancy grid (i.e., wavefronts of the brushfire algorithm are treated as if they started from one of these TSDF voxels). Like [10], we also use the TSDF voxels within the truncation distance to seed the wavefronts of the brushfire algorithm, i.e., instead of starting the wavefronts directly from occupied voxels, we start them from the outermost voxels still within the truncation distance. In our implementation, we compute a variant of the GVD called the \u03b8-simplified medial axis (\u03b8SMA) from [42] that filters out less stable and noisier portions of the GVD using a threshold on the minimum angle between basis points. Each voxel in the GVD is assigned a distance to its basis points, defining a sphere of free-space surrounding the voxel; collectively the GVD provides a compact description of the free-space and its connectivity (Fig. 5b). Indeed, up to discretization, the GVD provides an exact description of the free-space [29], meaning that one can reconstruct the full shape of the free-space from its GVD.\n11\nFrom the GVD to the Places Graph. While the GVD already provides a compressed representation of the free-space, it still typically contains a large number of voxels (e.g., more than 10, 000 in the Office dataset considered in Section VI). We could instantiate a graph of places with one node for each GVD voxel and edges between nearby voxels, but such representation would be too large to manipulate efficiently (e.g., our room segmentation approach in Section III-D1 would not scale to operating on the entire GVD). Previous attempts at sparsifying the GVD (notably [40] and our earlier paper [11]) used a subset of topological features (edges and vertices in the GVD, which correspond to GVD voxels having more than three and more than four basis points respectively) to form a sparse graph of places. However, the resulting graph does not capture the same connectivity of free-space as the full GVD, and may lead to graph of places with multiple connected components. It would also be desirable for the user to balance the level of compression with the quality of the free-space approximation instead of always picking certain GVD voxels.\nTo resolve these issues, we first form a graph Ga of the GVD where each GVD voxel corresponds to a node in Ga and edges connect any two neighboring GVD voxels. We then use voxel-hashing [36] to spatially cluster \u2014at a userspecified resolution\u2014 all updated GVD voxels with at least nb basis points (nb = 2 in our tests) inside the active window, ensuring that each cluster forms a connected component in Ga. This clustering is shown in Algorithm 2. This algorithm takes as input the graph of GVD voxels, Ga, and for every voxel that was updated, computes a spatial hash value [36], denoted hash(v, \u03b4p) in the algorithm for a given voxel v and spatial resolution \u03b4p. This hash value is the same for voxels within a cube of 3D space with side-lengths of \u03b4p and naturally clusters the voxels of the GVD into clusters of the provided spatial resolution. Lines 5 to 13 incrementally grow clusters of voxels with the same hash value. However, this results in multiple clusters on the same connected component of Ga. Lines 15 to 22 then search over the neighbors of a given voxel v in Ga and either combines clusters that share an edge in Ga and the same spatial hash value, or propose an edge between the clusters if the hash value differs. Once Algorithm 2 terminates, we obtain a set of proposed nodes (corresponding to unique clusters of voxels in Ga that form connected components) and proposed edges based on the connectivity of Ga.\nThese proposed nodes and edges are assigned information from the voxels that generated them: each new or updated node is assigned a position and distance to the nearest obstacle from the GVD voxel with the most basis points in the cluster; each new or updated edge is assigned a distance that is the minimum distance to the nearest obstacle of all the GVD voxels that the edge passes through. We then associate place nodes with the corresponding mesh-vertices of each basis point using the zero-crossing identified in the TSDF by marching cubes. We run Algorithm 2 after every update to the GVD and merge the proposed nodes N and edges E into the sparse graph of\nplaces.9\nInter-layer Edges. After building the places graph, we add inter-layer edges from each object or agent node to the nearest place node in the active window using nanoflann [43]."
        },
        {
            "heading": "D. Layer 4: Rooms",
            "text": "We segment the rooms by first geometrically clustering the graph of places into separate rooms (using persistent homology, Section III-D1), and then assigning a semantic label to each room (using the neural tree, Section III-D2). We remark that the construction of the room layer is fundamentally different from the construction of the object layer. While we can directly observe the objects (e.g., via a network trained to detect certain objects), we do not have a network trained to detect certain rooms. Instead, we have to rely on prior knowledge to infer both their geometry and semantics. For instance, we exploit the fact that rooms are typically separated by small passageways (e.g., doors), and that their semantics can be inferred from the objects they contain.\n1) Layer 4: Room Clustering via Persistent Homology: This section discusses how to geometrically cluster the environment into different rooms. Many approaches for room segmentation or detection (including [2]) require volumetric or voxel-based representations of the full environments and make assumptions on the environment geometry (i.e., a single floor or ceiling height) that do not easily extend to arbitrary buildings. To resolve these issues, we present a novel approach for constructing Layer 4 of the 3D scene graph by clustering the nodes in the graph of places Gp (Layer 3); this approach does not assume planarity of the environment and circumvents the need to process large voxel-based maps. Our room segmentation approach consists of two stages: the first stage identifies the number of rooms in the environment (as well as a subset of places associated with each room), and the second stage assigns the remaining places in Gp to rooms via flood-fill.\nIdentifying Rooms via Persistent Homology. We use the key insight that dilating the voxel-based map helps expose rooms in the environment: if we inflate obstacles, apertures in the environment (e.g., doors) will gradually close, naturally\n9Separate book-keeping is done to remove nodes and edges that no longer correspond to any GVD voxels during the brushfire update of the ESDF and GVD, as GVD voxels are removed during this update when a wavefront is able to modify the signed distance of the voxel\n12\npartitioning the voxel-based map into disconnected components (i.e., rooms); however, we apply the same insight to the graph of places Gp (rather than the voxel-based map) to enable faster computation and better scalability. As each node and edge in our place graph Gp stores a distance to its closest obstacle dp (Section III-C), dilation operations in the voxelbased map can be directly mapped into topological changes in Gp. More precisely, if we dilate the map by a distance \u03b4, every place node or edge with dp smaller than \u03b4 will disappear from the graph since it will no longer be in the free space. We call the dilated graph G\u03b4p ; see Fig. 6 for an illustration.\nDirectly specifying a single best dilation distance \u03b4\u2217 to extract putative rooms is unwieldy: door and other aperture widths vary from scene to scene (think about the small doors in an apartment versus large passage ways in a mansion). In order to avoid hard-coding a single dilation distance, we use a tool from topology, known as persistent homology [44, 15], to automatically compute the best dilation distance for a given graph.10 The basic insight is that the most natural choice of rooms is one that is more stable (or persistent) across a wide range of dilation distances. To formalize this intuition, the persistent homology literature relies on the notion of filtration. A filtration of a graph Gp is a set of graphs G\u03b4ip ordered by real-valued parameters \u03b4i such that\n\u2205 \u2286 . . . \u2286 G\u03b4i+1p \u2286 G\u03b4ip \u2286 . . . \u2286 Gp . (7)\nIn our case, each graph G\u03b4ip is the sub-graph of Gp where nodes and edges with radius smaller than \u03b4i are removed;11 hence the filtration is a set of graphs corresponding to increasing distances \u03b4i; see Figs. 7b and 7d for examples of graphs in the filtration. For each graph in the filtration, we compute the 0-homology (the number of connected components), which in our setup corresponds to the number of rooms the graph gets partitioned into; computationally, the 0-homology can be computed in one shot for all graphs in the filtration by iterating across the edges in the original graph Gp using a union set [45], leading to an extremely efficient algorithm.\nThe resulting mapping between the dilation distance and the number of connected components is an example of Betti curve, or \u03b20(\u03b4); see Fig. 7a for an example. Intuitively, for increasing distances \u03b4, the graph first splits into more and more connected components (this is the initial increasing portion of Fig. 7a), and then for large \u03b4 entire components tend to disappear (leading to a decreasing trend in the second part of Fig. 7a), eventually leading to all the nodes disappearing (i.e., zero connected components as shown in the right-most part of Fig. 7a). In practice, we restrict the set of distances to a range [d\u2212, d+], containing reasonable sizes for openings between rooms ([0.5, 1.2]m in our tests). We also do not count connected components containing less than a minimum number of vertices (15 in our tests), as we are not interested in segmenting overly small regions as rooms.\n10Persistent homology is the study of the birth and death of homologies across a filtration of a simplicial complex; see [44, 15] for an introduction on the topic. In this work, we restrict ourselves to graphs (a subset of simplicial complexes) and focus only on 0-homologies (connected components).\n11Such a filtration is known as the Vietoris-Rips filtration [45].\nChoices of rooms that are more persistent correspond to large \u201cflat\u201d horizontal regions of the Betti curve, where the number of connected components stays the same across a large sub-set of \u03b4 values. More formally, let us denote with H the set of unique values assumed by the Betti curve, i.e.,:\nH = {i \u2208 N : i = \u03b20(\u03b4),\u2200\u03b4 \u2208 [ d\u2212, d+ ] } (8)\nand \u2014for some small positive constant \u2014 denote with I the set of intervals Ij (for each j \u2208 H) defined as:\nIj = {(dmin, dmax) : \u2200\u03b4 \u2208 [dmin, dmax], \u03b20(\u03b4) = j, and \u03b20(dmin \u2212 ) 6= j, \u03b20(dmax + ) 6= j} (9)\nwhere, for each number of connected components H , the set I contains the extremes (dmin, dmax) of the corresponding flat interval in the Betti curve. Finally, we denote with L the lengths of the intervals in I . Then, the most persistent choice for the number of rooms is the one corresponding to the longest interval in L. In our approach, we choose the number of rooms (and associate an initial set of places to each room) by looking at \u201csufficiently persistent\u201d intervals (i.e., flat regions with length larger than a threshold). In more detail, we select a subset of L, denoted as L\u0304, that only contains the intervals of size greater than \u03b1 \u00b7 maxl\u2208L l, where \u03b1 is a user-specified parameter in [0, 1] that balances between oversegmentation (\u03b1 closer to 0) and under-segmentation (\u03b1 closer to 1). From these candidates, we assign \u03b4\u2217 to be \u03b4\u2217 = d\u2217min, where (d\u2217min, d \u2217 max) are the extremes of the interval in L\u0304 attaining the largest number of connected components. In other words, we choose the number of rooms to be the largest among all flat intervals that are sufficiently long (i.e., longer than \u03b1 \u00b7 maxl\u2208L l). As before, for this choice of \u03b4\u2217, we use the connected components corresponding to that dilation distance as an initial guess for the room clustering.12\nAssigning Remaining Nodes via Flood-Fill. The connected components produced by the persistent homology do not account for all places in Gp, as nodes and edges in the graph disappear depending on the dilation distance. We assign the remaining place nodes to the putative room via the floodfill algorithm, where the expansion queue is ordered by the distance of each edge to the nearest obstacle, resulting in node connections with the greatest distance to an obstacle being expanded first. This ensures that every node in the places graph Gp is assigned to a room, provided that every original connected component of Gp contains at least one room.\nRemark 7 (Novelty, Advantages, and Limitations). Many classical 2D room segmentation techniques, such as watershed or morphology approaches, are also connected to persistent homology [46]. Our room clustering method provides four key advantages. First, our approach is able to reason over arbitrary 3D environments, instead of a 2D occupancy grid (such as [47]). Second, the approach reasons over a sparse set of places and is extremely computationally and memory efficient in practice; this is in stark contrast with approaches\n12Note that for \u03b1 = 1, the proposed approach simply picks the longest (most persistent) interval. While intuitive, that choice typically ends up picking the very first (left-most interval in Fig. 7, with a single connected component) that persists for small \u03b4\u2217, which is undesirable in practice.\n13\nprocessing monolithic voxel-based maps (e.g., [2] reported runtimes of tens of minutes). Third, the approach automatically computes the dilation distance used to cluster the graph; this allows it to work across a variety of environments (in Section VI, we report experiments in both small apartments and student residences). Fourth, it provides a more solid theoretical framework, compared to the heuristics proposed in related work [11, 13]. As a downside, our room clustering approach, similarly to related work relying on geometric reasoning for room segmentation, may fail to segment rooms without a clear geometric footprint, e.g., open floor-plans.\nIntra-Layer and Inter-Layer Edges. We connect pairs of rooms, say (i, j), whenever a place in room i shares an edge with a place in room j. For each room, we add edges to all places in the room. We connect all rooms to a single building node, whose position is the mean of all room centroids.\n2) Layer 4: Room Classification via Neural Tree: While in the previous section, we (geometrically) clustered places nodes into different rooms, the approach described in this section assigns a semantic label (e.g., kitchen, bedroom) to each room. The key insight that we are going to use is that objects in a room are typically correlated with the room type (e.g., a room that contains refrigerator, oven, and toaster is likely to be a kitchen, a room with a bathtub and a toilet is likely to be a bathroom). Edges connecting rooms may also carry information about the room types (e.g., a master bedroom is more likely to be connected to a bathroom). We construct a sub-graph of the 3D scene graph that includes the object and the room layers and the corresponding intra- and interlayer edges. Given this graph, and the object semantic labels, centroids, and bounding boxes (see Section III-B), we infer the semantic labels of each room.\nWhile room inference could be attacked via standard techniques for inference in probabilistic graphical models, those techniques typically require handcrafting or estimating expressions for the factors connecting the nodes, inducing a probability distribution over the labels in the graph. We instead rely on more modern techniques that use graph neural networks (GNNs) to learn a suitable neural message passing function between the nodes to infer the missing labels. In particular, room classification can be thought of as a semisupervised node classification problem [48, 49], which has been extensively studied in machine learning. We also observe\nthat our problem has two key features that make it unique. First, the object-room graph is a heterogeneous graph and contains two kinds of nodes, namely objects and rooms, as opposed to large, homogeneous social network graphs (one of the key benchmarks applications in the semi-supervised node classification literature). Second, the object-room graph is a hierarchical graph (Definition 1), which gives more structure to the problem (e.g., Proposition 6).13 We review a recently proposed GNN architecture, the neural tree [16], that takes advantage of the hierarchical structure of the graph and leads to (provably and practically) efficient and accurate inference.\nNeural Tree Overview. While traditional GNNs perform neural message passing on the edges of the given graph G (the object-room graph in our case), the key idea behind the neural trees architecture is to construct a tree-structured graph from the input graph and perform message passing on the resulting tree instead of the input graph [16]. This tree-structured graph, the H-tree, is similar to a tree decomposition, and is such that every node in it represents either a node or a subset of nodes in the input graph. Trees are known to be more amenable for message passing (e.g., the junction tree algorithm enables exact inference for graphs with small treewidth) [50, 51]. Analogously, the neural tree has been shown to enable strong approximation results [16] and lead to better classification accuracy in practice (see [16] and Section VI). We briefly review the construction of the H-tree, the choice of message passing, and the resulting performance guarantees, and we refer the reader to [16] for an in-depth discussion.\nConstructing the H-Tree. The neural tree performs message passing on the H-tree, a tree-structured graph constructed from the input graph. Each node in the H-tree corresponds to a sub-graph of the input graph. These sub-graphs are arranged hierarchically in the H-tree such that the parent of a node in the H-tree always corresponds to a larger sub-graph in the input graph. The leaf nodes in the H-tree correspond to singleton subsets (i.e., individual nodes) of the input graph.\nThe first step to construct an H-tree is to compute a tree decomposition T of the object-room graph. Since the object-\n13Note that the result in Proposition 6 is general enough to also include the building node and perform building classification (i.e., classify an indoor environment into an office building, hospital, apartment, etc.). Here we tailor the discussion to the object-room graph for a practical reason: we lack a large enough dataset for training and testing a building classification network. The dataset in our experiments includes 90 buildings, which are mostly residential.\n14\nroom graph is a hierarchical graph, we use Algorithm 1 to efficiently compute a tree decomposition. The bags in such a tree decomposition contain either (C1) only room nodes, (C2) only object nodes, or (C3) object nodes with one room node. To form the H-tree, we need to further decompose the leaves of the tree decomposition into singleton nodes. For bags falling in the cases (C1)-(C2), we further decompose the bags using a tree decomposition of the sub-graphs formed by nodes in the bag, as described in [16]. For case (C3), we note that the sub-graph is again a hierarchical graph with one room node, hence we again use Algorithm 1 to compute a tree decomposition. We form the H-tree by concatenating these tree-decompositions hierarchically as described in [16].\nMessage Passing and Node Classification. Message passing on the H-tree generates embeddings for all the nodes and important sub-graphs of the input graph. Any of the existing message passing protocols (e.g., the ones used in Graph Convolutional Networks (GCN) [48, 52, 53, 48, 54], GraphSAGE [49], or Graph Attention Networks (GAT) [55, 56, 57]) can be re-purposed to operate on the neural tree. We provide an ablation of different choices of message passing protocols and node features in Section VI. After message passing is complete, the final label for each node is extracted by pooling embeddings from all leaf nodes in the H-tree corresponding to the same node in the input graph, as in [16].\nOne important difference between the H-tree in [16], and the H-tree constructed for the object-room graph is the heterogeneity of the latter. The H-tree of a heterogeneous graph will also be heterogeneous, i.e., the H-tree will now contain nodes that correspond to various kinds of sub-graphs in the input object-room graph. Specifically, the H-tree has nodes that correspond to sub-graphs: (i) containing only room nodes, (ii) containing one room node and multiple object nodes, (iii) containing only object nodes, and (iv) leaf nodes which correspond to either an object or a room node. Accordingly, we treat the neural tree as a heterogeneous graph when performing message passing. Message passing over heterogeneous graphs can be implemented using off-the-shelf functionalities in the PyTorch geometric library [58].\nExpressiveness of the Neural Tree and Graph Treewidth. The following result, borrowed from our previous work [16], establishes a connection between the expressive power of the neural tree and the treewidth of the corresponding graph.\nTheorem 8 (Neural Tree Expressiveness, Theorem 7 and Corollary 8 in [16]). Call F(G, N) the space of functions that can be produced by applying the neural tree architecture with N parameters to the graph G. Let f : [0, 1]n \u2192 [0, 1] be a function compatible with a graph G with n nodes, i.e., a function that can be written as f(X) = \u2211 C\u2208C(G) \u03b8C(xC), where C (G) denotes the collection of all maximal cliques in G and \u03b8C is some function that maps features associated to nodes in a clique C to a real number. Let each clique function \u03b8c in f be 1-Lipschitz and be bounded to [0, 1]. Then, for any > 0, there exists a function g \u2208 F(G, N) such that ||f\u2212g||\u221e< , while the number of parameters N is bounded by\nN = O ( n\u00d7 (tw [JG ] + 1)2tw[JG ]+3 \u00d7 \u2212(tw[JG ]+1) ) , (10)\nwhere tw [JG ] denotes the treewidth of the tree-decomposition of G, computed according to Algorithm 1.\nWhile we refer the reader to [16] for a more extensive discussion, the intuition is that graph-compatible functions can model (the logarithm of) any probability distribution over the given graph. Hence, Theorem 8 essentially states that the neural tree can learn any (sufficiently well-behaved) graphical model over G, with a number of parameters that scales exponentially in the graph treewidth, and only linearly in the number of nodes in the graph. Therefore, for graphs with small treewidth (as the ones of Proposition 6), we can approximate arbitrary relations between the nodes without requiring too many parameters. Furthermore, Proposition 6 and Proposition 3 ensure that we can compute the tree decomposition (and hence the H-tree) efficiently in practice. Beyond these theoretical results, in Section VI we show that the use of the neural tree leads to improved accuracy in practice."
        },
        {
            "heading": "IV. PERSISTENT REPRESENTATIONS: DETECTING AND ENFORCING LOOP CLOSURES IN 3D SCENE GRAPHS",
            "text": "The previous section discussed how to estimate the layers of an \u201codometric\u201d 3D scene graph as the robot explores an unknown environment. In this section, we discuss how to use the 3D scene graph to detect loop closures (Section IV-A), and how to correct the entire 3D scene graph in response to putative loop closures (Section IV-B)."
        },
        {
            "heading": "A. Loop Closure Detection and Geometric Verification",
            "text": "We augment visual loop closure detection and geometric verification by using information across multiple layers in the 3D scene graph. Standard approaches for visual place recognition rely on visual features (e.g., SIFT, SURF, ORB) and fast retrieval methods (e.g., bag of words [59]) to detect loop closures. Advantageously, the 3D scene graph not only contains visual features (included in each node of the agent layer), but also additional information about the semantics of the environment (described by the object layer) and the geometry and topology of the environment (described by the places layer). In the following we discuss how to use this additional 3D information to develop better descriptors for loop closure detection and geometric verification.\n1) Top-Down Loop Closure Detection: As mentioned in Section III-B, the agent layer stores visual features for each keyframe pose along the robot trajectory. We refer to each such poses as agent nodes. Loop closure detection then aims\n15\nat finding a past agent node that matches (i.e., observes the same portion of the scene seen by) the latest agent node, which corresponds to the current robot pose.\nTop-Down Loop Closure Detection Overview. For each agent node, we construct a hierarchy of descriptors describing statistics of the node\u2019s surroundings, from low-level appearance to semantics and geometry. At the lowest level, our hierarchical descriptors include standard DBoW2 appearance descriptors [59]. We augment the appearance descriptor with an object-based descriptor and a place-based descriptor computed from the objects and places in a sub-graph surrounding the agent node. We provide details about two choices of descriptors (hand-crafted and learning-based) below. To detect loop closures, we compare the hierarchical descriptor of the current (query) node with all the past agent node hierarchical descriptors, searching for a match. When comparing descriptors, we walk down the hierarchy of descriptors (from places, to objects, to appearance descriptors). In particular, we first compare the places descriptor and \u2014if the descriptor distance is below a threshold\u2014 we move on to comparing object descriptors and then appearance descriptors. If any of the appearance or object descriptor comparisons return a strong enough match (i.e., if two distance between two descriptors is below a threshold), we perform geometric verification; see Fig. 8 for a summary.\nHand-crafted Scene Graph Descriptors. Our top-down loop closure detection relies on having descriptors (i.e., vector embeddings) of the sub-graphs of objects and places around each agent node. In the conference version [11] of this paper, we proposed hand-crafted descriptors. In particular, for the objects, we use the histogram of the semantic labels of the object nodes in the sub-graph as an object-level descriptor. For the places, we use the histogram of the distances associated to each place node in the sub-graph as a place-level descriptor. As shown in [11] and confirmed in Section VI, the resulting hierarchical descriptors already lead to improved loop closure detection performance over traditional appearance-based loop closures. However, these descriptors fail to capture relevant information about objects and places, e.g., their spatial layout and connectivity. In the following, we describe learning-based descriptors that use graph neural networks to automatically find a suitable embedding for the object and place sub-graphs; these are observed to further improve loop closure detection performance in some cases; see Section VI.\nLearning-based Scene Graph Descriptors. Given a subgraph of objects and places around the agent node, we learn fixed-size embeddings using a Graph Neural Network (GNN). At a high level, we learn such embeddings from scene graph datasets, such that the Euclidean distance between descriptors is smaller if the corresponding agent nodes are spatially close.\nIn more detail, we learn separate embeddings for the subgraph of objects and the sub-graph of places. For every object layer sub-graph, we encode the bounding-box size and semantic label of each object as node features in a GNN. For every places layer sub-graph, we encode the distance of the place node to the nearest obstacle and the number of basis points of the node as node features. Rather than including absolute node positions in the respective node features, we assign a weight\nto each edge (i, j) between nodes i and j as wij = e\u2212\u2016xi\u2212xj\u2016, where xi and xj are the positions of nodes i and j. This results in a weight in the range [0, 1], where the closer two nodes are, the higher the edge weight. Associating intra-node distances to edges (rather than using absolute positions as node features) makes the resulting embedding pose-invariant; this is due to the fact that the node positions only enter the network in terms of their distance, which is invariant to rigid transformations. Our GNN model architecture follows the graph embedding architecture presented in [60], which consists of multi-layer perceptrons as encoders for node and edge features, message passing layers, and a graph-level multi-level perception to aggregate node embeddings into the final graph embedding. We use triplet loss [60] to train the models and defer the details of constructing triplets and other model and training parameters to the experiments in Section VI.\n2) Bottom-up Geometric Verification: After we have a putative loop closure between our query and match agent nodes (say i and j), we attempt to compute a relative pose between the two by performing bottom-up geometric verification. Whenever we have a match at a given layer (e.g., between appearance descriptors at the agent layer, or between object descriptors at the object layer), we attempt to register frames i and j. For registering visual features we use standard RANSAC-based geometric verification as in [34]. If that fails, we attempt registering objects using TEASER++ [38], discarding loop closures that also fail object registration. This bottom-up approach has the advantage that putative matches that fail appearance-based geometric verification (e.g., due to viewpoint or illumination changes) can successfully lead to valid loop closures during the object-based geometric verification. Section VI shows the proposes hierarchical descriptors improve the quality and quantity of detected loop closures.\nB. 3D Scene Graph Optimization\nThis section describes a framework to correct the entire 3D scene graph in response to putative loop closures. Assume we use the algorithms in Section III to build an \u201codometric\u201d 3D scene graph, which drifts over time as it is built from the odometric trajectory of the robot \u2014 we refer to this as the frontend (or odometric) 3D scene graph. Then, our goals here are (i) to optimize all layers in the 3D scene graph in a consistent manner while enforcing the detected loop closures (Section IV-A), and (ii) to post-processes the results to remove redundant sub-graphs corresponding to the robot visiting the same location multiple times. The resulting 3D scene graph is what we call a backend (or optimized) 3D scene graph, and we refer to the module producing such a graph as the scene graph backend. Below, we describe the two main processes implemented by the scene graph backend: a 3D scene graph optimization (which simultaneously corrects all layers of the scene graph by optimizing a sparse subset of variables), and an interpolation and reconciliation step (which recovers the dense geometry and removes redundant variables); see Fig. 9.\n3D Scene Graph Optimization. We propose an approach to simultaneously deform the 3D scene graph layers using an embedded deformation graph [17]. This approach generalizes\n16\nthe pose graph and mesh optimization approach in [2] as it also includes the graph of places in the optimization. At a highlevel, the backend optimizes a sparse graph (the embedded deformation graph) built by downsampling the nodes in the 3D scene graph, and then reconstructs the other nodes in the scene graph via interpolation as in [17].\nSpecifically, we form the deformation graph as the subgraph of the 3D scene graph that includes (i) the agent layer, consisting of a pose graph that includes both odometry and loop closures edges, (ii) the 3D mesh control points, i.e., uniformly subsampled vertices of the 3D mesh (obtained using the same spatial hashing process described in Section III-A), with edges connecting control points closer together than a distance (2.5 m in our implementation); (iii) a minimum spanning tree of the places layer.14 By construction, these three layers form a connected sub-graph (recall the presence of the inter-layer edges discussed in Section III).\nThe embedded deformation graph approach associates a local frame (i.e., a pose) to each node in the deformation graph and then solves an optimization problem to adjust the local frames in a way that minimizes deformations associated to each edge (including loop closures). Let us call Ta, Tm, Tp the set of poses associated with the agent layer, the mesh control points, and the places. The poses in Ta are initially set to be the odometric poses of the robot; the poses in Tm are initially set to have identity rotation and translation equal to the mesh control points\u2019 positions; similarly, the poses in Tp are initially set to have identity rotation, and translation equal to the position of the corresponding places. Each edge represents a relative pose or a relative position measurement between pair of poses. In particular, the set of edges is E = Eaa \u222aEmm \u222aEpp \u222aEam \u222aEap \u222aEmp, where each\n14The choice of using the minimum spanning tree of places is motivated by computational reasons: the use of the spanning tree increases sparsity of the resulting deformation graph, enabling faster optimization.\nsubset denotes intra-layer edges (e.g., Eaa contains the edges within the agent layer), or inter-layer edges (e.g., Eam contains the edges between a robot pose and a mesh control point).15 Intuitively, the proposed 3D scene graph optimization finds the set of poses that minimizes the mismatch with respect to these relative measurements: a small mismatch corresponds to small deformations of the local geometry of the scene graph and encourages small errors for the loop closure edges.\nWe can find an optimal configuration for the poses T = Ta\u222aTm\u222aTp in the deformation graph by solving the following optimization problem:\nT ? = arg min T1,T2,...\u2208T \u2211 (i,j)\u2208E \u2225\u2225T\u22121i Tj \u2212Eij\u2225\u22252\u2126ij (11) where Ti \u2208 SE(3) and Tj \u2208 SE(3) are pairs of 3D poses in T , Eij is the relative measurement associated to each edge (i, j) \u2208 E (written as a 3D pose), and for a matrix M we use the notation \u2016M\u20162\u2126 , tr ( M\u2126MT ) .16 The 4 \u00d7 4 positive semidefinite matrix \u2126ij is chosen as the inverse of the odometry (or loop closure) covariances for the edges in Eaa, while it is set to diag ([0 0 0 \u03c9t]) for the other relative position measurements, where the zeros cancel out the rotation component of the residual error T\u22121i Tj\u2212Eij , and \u03c9t is a userspecified parameter that controls how much deformation we want to allow for each edge during the optimization.17\nIn hindsight, 3D scene graph optimization transforms a subset of the 3D scene graph into a factor graph [30], where edge potentials need to be minimized. The expert reader might also realize that (11) is mathematically equivalent to standard pose graph optimization in SLAM [30], which enables the use of established off-the-shelf solvers. In particular, we solve (11) using the Graduated Non-Convexity (GNC) solver in GTSAM [61], which is also able to reject incorrect loop closures as outliers.\nInterpolation and Reconciliation. Once the optimization terminates, the agent and place nodes are updated with their new (optimized) positions and the full mesh is interpolated back from its control points according to the deformation graph approach in [17, 2]. After the 3D scene graph optimization and the interpolation step, certain portions of the scene graph \u2014corresponding to areas revisited multiple times by the robot\u2014 contain redundant information. To avoid this redundancy, we merge overlapping nodes. For places nodes, we merge nodes within a distance threshold (0.4 m in our implementation). For object nodes we merge nodes if the corresponding objects have the same semantic label and if\n15The set Eaa contains all the odometric and loop closure measurements relating poses in the robot trajectory. Emm contains all the relative positions between pairs of nearby mesh control points (expressed in the local frame attached to one of the control points). Similarly, Epp, Eam, Eap and Emp contain the relative positions between pairs of places, between robot poses and mesh control points visible from that pose, between robot poses and places near that pose, and between a place and the corresponding basis points in the mesh, respectively.\n16When the relative measurement involves only a translation, the rotation component of Eij is conventionally set to the identity \u2014 a suitable choice of the information matrix \u2126ij for those measurements ensures that such rotation component is disregarded by the optimization.\n17The interested reader might find a step-by-step derivation of (11) (but restricted to the agent layer and the mesh control points) in [2].\n17\none of nodes is contained inside the bounding box of the other node. After this process is complete, we recompute the object centroids and bounding boxes from the position of the corresponding vertices in the optimized mesh. Finally, we recompute the rooms from the graph of places using the approach in Section III-D."
        },
        {
            "heading": "V. THINKING FAST AND SLOW: THE HYDRA ARCHITECTURE",
            "text": "We integrate the algorithms described in this paper into a highly parallelized spatial perception system, named Hydra. Hydra involves a combination of processes that run at sensor rate (e.g., feature tracking for visual-inertial odometry), at sub-second rate (e.g., mesh and place reconstruction, object bounding box computation), and at slower rates (e.g., the scene graph optimization, whose complexity depends on the map size). Therefore these processes have to be organized such that slow-but-infrequent computation (e.g., scene graph optimization) does not get in the way of faster processes.\nWe visualize Hydra in Fig. 10. Each block in the figure denotes an algorithmic module, matching the discussion in the previous sections. Hydra starts with fast early perception processes (Fig. 10, left), which perform low-level perception tasks such as feature detection and tracking (required for visualinertial odometry, and executed at frame-rate), 2D semantic segmentation, and stereo-depth reconstruction (at keyframe rate). The result of early perception processes are passed to mid-level perception processes (Fig. 10, center). These include algorithms that incrementally construct (an odometric version of) the agent layer (e.g., the visual-inertial odometry backend), the mesh and places layers, and the object layer. Mid-level perception also includes the scene graph frontend, which is a module that collects the result of the other modules into an \u201cunoptimized\u201d scene graph. Finally, the high-level perception processes perform loop closure detection, execute scene graph backend optimization, and extract rooms (including both room clustering and classification).18 This results in a globally consistent, persistent 3D scene graph.\nHydra runs in real-time on a multi-core CPU. The only module that relies on GPU computing is the 2D semantic seg-\n18While room detection is fast enough to be executed at keyframe rate, it still operates on the entire graph, hence it is more suitable as a slow high-level perception process.\nmentation, which uses a standard off-the-shelf deep network. The neural tree (Section III-D2) and the GNN-based loop closure detection (Section IV-A) can be optionally executed on a GPU, but the forward pass is relatively fast even on a CPU (see Section VI-C4). The fact that most modules run on CPU has the advantage of (i) leaving the GPU to learningoriented components, and (ii) being compatible with the power limitations imposed by current mobile robots. In the next section, we will report real-time results with Hydra running on a mobile robot, a Unitree A1 quadruped, with onboard sensing and computation (an NVIDIA Xavier embedded computer)."
        },
        {
            "heading": "VI. EXPERIMENTS",
            "text": "The experiments in this section (i) qualitatively and quantitatively compare the 3D scene graph produced by Hydra to another state-of-the-art 3D scene graph construction method, SceneGraphFusion [7], (ii) examine the performance of Hydra in comparison to batch offline methods, i.e., Kimera [2], (iii) validate design choices for learned components in our method via ablation studies, and (iv) present a runtime analysis of Hydra. We also document our experimental setup, including training details for both the GNN-based loop closure descriptors and the neural-tree room classification, and datasets used. Our implementation of Hydra is available at https://github.com/MIT-SPARK/Hydra."
        },
        {
            "heading": "A. Datasets",
            "text": "We use four primary datasets for training and evaluation: two simulated datasets (Matterport3d [31] and uHumans2 [2]) and two real-world datasets (SidPac and Simmons). In addition, we use the Stanford3D dataset [3] to motivate neural tree design choices with respect to our initial proposal in [16].\nMatterport3d. We utilize the Matterport3D (MP3D) dataset [31], an RGB-D dataset consisting of 90 reconstructions of indoor building-scale scenes. We use the Habitat Simulator [62] to traverse the scenes from the MP3D dataset and render color imagery, depth, and ground-truth 2D semantic segmentation. We generate two different 3D scene graphs datasets for the 90 MP3D scenes by running Hydra on pregenerated trajectories; one for training descriptors and one for room classification. For training the GNN-based descriptors for loop closure detection (GNN-LCD), we generate a single\n18\ntrajectory for each scene through the navigable scene area such that we get coverage of the entire scene, resulting in 90 scene graphs. For training the room classification approaches, we generate 5 trajectories for each scene by randomly sampling navigable positions until a total path length of at least 100 m is reached, resulting in 450 trajectories. When running Hydra on these 450 trajectories, we save intermediate scene graphs every 100 timesteps (resulting in roughly 15 scene graphs per trajectory), giving us 6810 total scene graphs.\nuHumans2. The uH2 dataset is a Unity-based simulated dataset [2] that includes four scenes: a small apartment, an office, a subway station, and an outdoor neighborhood. For the purposes of this paper, we only use the apartment and office scenes. The dataset provides visual-inertial data, ground-truth depth, and 2D semantic segmentation. The dataset also provides ground truth trajectories that we use for benchmarking.\nSidPac. The SidPac dataset is a real dataset collected in a graduate student residence using a visual-inertial handheld device. We used a Kinect Azure camera as the primary collection device, providing color and depth imagery, with an Intel RealSense T265 rigidly attached to the Kinect to provide an external odometry source. The dataset consists of two separate recordings, both of which are used in our previous paper [11]. We only use the first recording for the purposes of this paper. This first recording covers two floors of the building (Floors 1 & 3), where we walked through a common room, a music room, and a recreation room on the first floor of the graduate residence, went up a stairwell, through a long corridor as well as a student apartment on the third floor, then finally down another stairwell to revisit the music room and the common room, ending where we started. These scenes are particularly challenging given the scale of the scenes (average traversal of around 400 m), the prevalence of glass and strong sunlight in regions of the scenes (causing partial depth estimates from the Kinect), and featurepoor regions in hallways. We obtain a proxy for the groundtruth trajectory for the Floor 1 & 3 scene via a hand-tuned pose graph optimization with additional height priors, to reduce drift and qualitatively match the building floor plans.\nSimmons. The Simmons dataset is a real dataset collected on a single floor of an undergraduate student residence with a Clearpath Jackal rover and a Unitree A1 quadruped robot (Fig. 11). The Clearpath Jackal rover uses the RealSense D455 camera as the primary collection device to provide color and depth imagery, but the rover is also equipped with a Velodyne to perform LIDAR-based odometry [63] as an external odometry source. The A1 also uses the RealSense D455 camera as the primary collection device, but is not equipped with a Velodyne; instead, it is equipped with an industrial grade Microstrain IMU to improve the performance of the visual-inertial odometry [34]. The dataset consists of two recordings, one recorded on the Jackal, and one recorded using the A1. The Jackal recording covers the rooms scattered throughout half of a single floor of the building, where the Jackal traverses a distance of around 500 m through mostly student bedrooms, but also a lounge area, a kitchen, and a laundry room; the rooms are all joined by a long hallway that spans the full floor. The A1 sequence takes place on\none end of the floor and maps 4 bedrooms, a small lounge, and a section of the hallway that connects all the rooms. The dataset is challenging due to the visual and structural similarity across student rooms that have similar layouts and furnishing. We obtain a proxy for the ground-truth trajectory for the Jackal using LIDAR-based SLAM, by running LOCUS [63] with flat ground assumption and LAMP [64] for loop closure corrections. The ground-truth trajectory of the A1 is obtained by registering individual visual keyframes with the visual keyframes in the Jackal sequence, and then corrected using the proxy ground-truth of the Jackal sequence.\nStanford3D. We use the 35 human-verified scene graphs from the Stanford 3D Scene Graph (Stanford3d) dataset [3] to compare the neural tree against standard graph neural networks for node classification and to assess new design choices against our initial proposal in [16]. These scene graphs represent individual residential units, and each consists of building, room, and object nodes with inter-layer connectivity. We use the same pre-processed graphs as in [16] where the single-type building nodes (residential) are removed and additional 4920 intra-layer object edges are added to connect nearby objects in the same room. This results in 482 room-object graphs, each containing one room and at least one object per room. The full dataset has 482 room nodes with 15 semantic labels, and 2338 objects with 35 labels."
        },
        {
            "heading": "B. Experimental Setup",
            "text": "In this section, we first discuss the implementation of Hydra, including our choice of networks for the 2D semantic segmentation. Then we provide details regarding the training of our learning-based approaches: the GNN-LCD descriptors and the neural tree for room classification.\nHydra. To provide 2D semantic segmentation for the real-world datasets, we compare three different models, all\n19\nof which use the ADE20k [68] label space. We use HRNet [66] as a \u201cnominal\u201d semantic segmentation source and MobileNetV2 [67] as a light-weight source of semantics for use on a robot. For both HRNet and MobileNetV2, we use the pre-trained model from the MIT Scene Parsing challenge [68] to export a deployable model for our inference toolchain (ONNX and TensorRT). Additionally, we use a state-of-the-art model, OneFormer [65], to provide more-accurate-but-slower 2D semantic segmentation. A comparison of the accuracy and frame-rate of these models is shown in Fig. 12.\nFor both simulated and real datasets we use KimeraVIO [34] for visual-inertial odometry. For SidPac, we fuse the Kimera-VIO estimates with the output of the RealSense T265 to improve the quality of the odometric trajectory. For Simmons, we fuse the Kimera-VIO estimates with the odometry output of [63] to improve the quality of the odometric trajectory for the sequence recorded by the Jackal platform.\nAll the remaining blocks in Fig. 10 are implemented in C++, following the approach described in this paper. In the experiments we primarily use a workstation with an AMD Ryzen9 3960X with 24 cores and two Nvidia GTX3080s. We also report timing results on an embedded computer (Nvidia Xavier NX) at the end of this section, and demonstrate the full Hydra system running on the same embedded computer onboard the A1 robot; video of the experiment is available.19\nGNN-LCD Training. We use the MP3D scene graphs to train loop closure descriptors for the object and place subgraphs. We generate a sub-graph around every agent pose which consists of all nodes and edges within a provided radius of the parent place node that the agent pose is nearest to. We adaptively determine the radius for each sub-graph; each subgraph has a minimum radius of 3 m and is grown until either\n19https://youtu.be/AEaBq2-FeY0\na minimum number of nodes (10) or a maximum radius of 5 m is reached. Each object and place sub-graph contains the node and edges features as described in Section IV-A. The semantic label of each object node is encoded either using word2vec [69] or a one-hot encoding. We explore the impact of this label encoding in Section VI-C.\nWe use triplet loss to train our model. To construct the triplets, we have an anchor (a candidate sub-graph), and need to find positive and negative examples to compute the loss. A good proxy for how similar two sub-graphs are is looking at the spatial overlap of the two sets of nodes of the sub-graphs. We compute this overlap between the sub-graphs by using IoU over the bounding boxes that encompass the positions of the nodes of each sub-graph. If the overlap between two subgraphs is at least 40 percent, we consider them to be similar (and candidates for positive examples), otherwise they are considered negative examples for that particular anchor. Subgraphs from different scenes are always considered negative examples. To train our GNN-LCD descriptors, we use online triplet mining with batch-all triplet loss [70], where we construct valid triplets for a batch of sub-graph input embeddings and average loss on the triplets that produce a positive loss.\nThe message-passing architecture that we selected is GCNConv [48]. While more expressive or performant architectures exist, few are compatible with the computational framework used for inference in Hydra (i.e., ONNX). We split the dataset by scene; we use 70% of the original 90 scene graphs to train on, 20% of the scene graphs for validation, and the last 10% for testing. Our learning rate is 5\u00d7 10\u22124, and we train the object models for 50 epochs and place models for 80 epochs, saving the model when the average validation error is at a minimum. Each model produces a descriptor of dimension 64. Other key model parameters are reported in Appendix G.\nNeural Tree Training. We train the neural tree and related baselines on two datasets: Stanford3d and MP3D.\nFor the 482 object-room scene graphs in the Stanford3D dataset, we train the neural tree and GNN baselines for the same semi-supervised node classification task examined in [16], where the architecture has to label a subset of room and object nodes. The goal of this comparison is to understand the impact of some design choices (e.g., heterogeneous graphs, edge features) with respect to our original proposal in [16] and related work. For this comparison, we implement the neural tree and baseline approaches with four different message passing functions: GCN [48], GraphSAGE [49], GAT [55], and GIN [71]. We consider both homogeneous and heterogeneous graphs; for all nodes, we use their centroid and bounding box size as node features. In some of our comparisons, we also examine the use of relative node positions as edge features, and discard the centroid from the node features. For the GNN baselines, we construct heterogeneous graphs consisting of two node types: rooms and objects. For the neural tree, we obtain graphs with four node types: room cliques, room-object cliques, room leaves, and object leaves; see Section III-D2. There are few message passing functions that are compatible with both heterogeneous graphs and edge features; therefore, we compare all heterogeneous approaches using only GAT [55]. For all tests on Stanford3d, we report average test\n20\naccuracy over 100 runs. For each run, we randomly generate a 70%, 10%, 20% split across all nodes (i.e., objects and rooms) for training, validation, and testing, respectively.\nFor the 6180 scene graphs in the MP3D dataset, we test the neural tree and baselines for room classification on objectroom graphs. The goal of this experiment is to understand the impact of the node features and the connectivity between rooms on the accuracy of room classification. We only consider heterogeneous graphs for this dataset, and as such only use GAT [55] for message passing. The heterogeneous node types are the same as for Stanford3D. We use the bounding box size as the base feature for each node, and use relative positions between centroids as edge features. We also evaluate the impact of using semantic labels as additional features for the object nodes using word2vec [69]. The MP3D dataset contains scene graphs with partially explored rooms. We discard any room nodes where the IoU between the 2D footprint of the places within the room and the 2D footprint of the ground truth room is less than a specified ratio (60% in our tests). Further details on the construction and pre-processing of the dataset are provided in Appendix E. We predict a set of 25 room labels which are provided in Appendix D. For training, we use the scene graphs from the official 61 training scenes of the MP3D dataset. For the remaining 29 scenes, we use graphs from two trajectories of the five total trajectories for validation and the other three trajectories for testing. For use with Hydra, we select the best-performing heterogeneous neural tree architecture; this architecture uses bounding box size and word2vec embeddings of the semantic labels of the object nodes as node features, as well as relative positions between nodes as edge features.\nAll training and testing is done using a single Nvidia A10G Tensor Core GPU. For both datasets, we use cross entropy loss between predicted and ground-truth labels during training, and save the models with the highest validation accuracy for testing. All models are implemented using PyTorch 1.12.1 and PyTorch Geometric 2.2.0 [58]. We base our implementation on our previous open-source version of the neural tree [16]. We provide additional training details, including model implementation, timing, and hyper-parameter tuning in Appendix E."
        },
        {
            "heading": "C. Results and Ablation Study",
            "text": "We begin this section with a comparison between Hydra and SceneGraphFusion [7] (Section VI-C1). We then analyze the accuracy and provide an ablation of the modules in Hydra (Section VI-C2), and show an example of the quality of scene graph that Hydra is able to produce while running onboard the Unitree A1 (Section VI-C3). Finally, we report a breakdown of the runtime of our system (Section VI-C4).\n1) Comparison against SceneGraphFusion: This section shows that Hydra produces better object maps compared to SceneGraphFusion [7], while also creating additional layers in the scene graph (SceneGraphFusion does not estimate rooms or places). We compare our system with SceneGraphFusion [7] for both the uHumans2 apartment and office scene. For this comparison, the only learned component used by our system is the off-the-shelf 2D semantic segmentation network, hence for a fair comparison, we do not retrain the object label prediction GNN used by SceneGraphFusion [7]. Examples of the produced objects by Hydra and SceneGraphFusion are shown in Fig. 13. Note that SceneGraphFusion tends to oversegment larger objects (e.g., the sofa in the lower right corner of Fig. 13 or the bed in the top right corner of the same figure), while Hydra tends to under-segment nearby objects (such as the dining table and chairs in the bottom left of Fig. 13). For the purposes of a fair comparison, we use OneFormer [65] to provide semantics for Hydra, and use ground-truth robot poses as input to both systems. A quantitative comparison is reported in Table I, which also includes results for Hydra using ground-truth semantics as a upper bound on performance. We report two metrics. The first, Percent Correct, is the percent of estimated objects that are within some distance threshold (0.5 m) of a ground-truth object (as provided by the simulator for uHumans2). The second, Percent Found, is the percent of ground-truth objects that are within some distance threshold (also 0.5 m) of a ground-truth object (as provided by the simulator for uHumans2). As the label space of SceneGraphFusion for objects does not line up well with the ground-truth object label space, and as SceneGraphFusion does a poor job predicting object labels for the two scenes used for comparison (see Fig. 13), we do not take the semantic labels into account when computing the metrics (as opposed to the stricter metrics used in Section VI-C2 for examining object accuracy, which do take the semantic labels of the object into account).\nTable I shows that Hydra largely outperforms SceneGraphFusion in terms of both Percent Correct and Percent Found, even after disregarding the incorrect semantic labels for the objects produced by SceneGraphFusion. In hindsight, Hydra can benefit from more powerful 2D segmentation networks and estimate better objects, while SceneGraphFusion directly attempts to extract semantics from the 3D reconstruction, which is arguably a harder task and does not benefit from the large datasets available for 2D image segmentation. The last row in the table \u2014Hydra (GT)\u2014 shows that Hydra\u2019s accuracy can further improve with a better 2D semantic segmentation. While SceneGraphFusion is less competitive in terms of object understanding, we remark that it predicts a richer set of object relationships for each edge between objects [7] (e.g., standing on, attached to) compared to Hydra, which might be also useful for certain applications.\n2) Accuracy Evaluation and Ablation: Here, we examine the accuracy of Hydra, running in real-time, as benchmarked against the accuracy attained by constructing a 3D scene graph in an offline manner (i.e., by running Kimera [2]). We also present a detailed quantitative analysis to justify key design choices in Hydra. To do this, we break down this\n21\nquantitative analysis across the objects, places, and rooms layers. We examine the impact of (i) the choice of loop closure detection and 2D semantic segmentation on the accuracy of each layer of the 3D scene graph, and (ii) the impact of model architectures and training parameters on the accuracy of the room classification network. As such, we first present the accuracy of the predicted object and place layers by Hydra. We then move to looking at room classification accuracy, including two ablation studies on the neural tree and an evaluation of our room clustering approach based on persistent homology. Finally, we examine the quality of predicted loop closures when using the proposed hierarchical descriptors.\nWhen benchmarking the accuracy of the layers of Hydra, we consider five different configurations for odometry estimation and loop closure detection. The first configuration (\u201cGTTrajectory\u201d) uses ground-truth poses to incrementally construct the scene graph and disregards loop closures. The second, third, and fourth configurations (\u201cVIO+V-LC\u201d, \u201cVIO+SGLC\u201d, and \u201cVIO+GNN-LC\u201d, respectively) use visual-inertial odometry (VIO) for odometry estimation and use vision-based loop closures (VIO+V-LC), the proposed hierarchical handcrafted descriptors for scene graph loop closures (VIO+SGLC), or the proposed hierarchical learned descriptors for scene graph loop closures (VIO+GNN-LC). The last configuration, VIO, uses visual-inertial odometry without loop closures.\nAccuracy Evaluation: Objects. Figure 14 evaluates the object layer of Hydra across the five configurations of Hydra, as well as across various 2D semantic segmentation sources: ground-truth semantics (when available), OneFormer, and HRNet. For this evaluation, we benchmark against the ground-truth objects from the underlying Unity simulation for uHumans2. As the recording for the uHumans2 scenes only explores a portion of the scene, we only compare against ground-truth objects that have a high enough number of mesh vertices inside their bounding boxes (40 vertices). For the real datasets, where we do not have ground-truth objects, we consider the object layer of the batch scene graph constructed from ground-truth poses and OneFormer-based 2D semantic segmentation using Kimera [2] as the ground-truth objects for\nthe purposes of evaluation. For the object layer, we report two metrics: the percentage of objects in the ground-truth scene graph that have an estimated object with the correct semantic label within a specified radius (\u201c% Found\u201d) and the percentage of objects in the estimated scene graph that have a groundtruth object with the correct semantic label within a specified radius (\u201c% Correct\u201d).20\nWe note some important trends in Fig. 14. First, when using GT-Trajectory, Hydra produces objects that are reasonably close to the ground-truth (80\u2013100% found and correct objects for the Office scene), or close to the objects produced by offline approaches (70\u201380% found and correct objects for SidPac and Simmons). This demonstrates that \u2014given the trajectory\u2014 the real-time scene graph from Hydra is comparable to the batch and offline approaches at the state of the art. Second, VIO+V-LC, VIO+SG-LC, and VIO+GNNLC maintain reasonable levels of accuracy for the objects and attain comparable performance in small to medium-sized scenes (i.e., the uHumans2 Apartment, Office, and Simmons A1) for the same choice of semantic segmentation method. In these scenes, the drift is small and the loop closure strategy does not radically impact performance (differences are within standard deviation). However, in larger scenes (e.g., SidPac) scene graph loop closures are more important and both VIO+GNN-LC and VIO+SG-LC largely outperform VIO+VLC and VIO in terms of object accuracy.\nFinally, as expected, the quality of the semantic segmentation impacts the accuracy; this is much more pronounced for \u201c% Found\u201d than \u201c% Correct\u201d. Note that the two metrics are somewhat coupled; a lower \u201c% Found\u201d can lead to a higher \u201c% Correct\u201d; this is one reason for the inversion in performance between OneFormer and HRNet for the uHumans2 scenes. Additionally, note that OneFormer identifies objects that exist in the ground-truth scene that are not exported by the simulator\n20As distance thresholds, we use 0.5m for the uHumans2 Apartment and Office scene, and 1m for SidPac, the Simmons Jackal scene, and the Simmons A1 scene. These thresholds were chosen to roughly correspond to the mean Absolute Trajectory Error (ATE) for each scene, in order to normalize the metrics between simulated and real scenes.\n22\n(this is another factor responsible for the lower \u201c% Correct\u201d of OneFormer). We also note a slight correlation between reduced semantic segmentation performance and reduced accuracy for VIO+GNN-LC and VIO+SG-LC, such as for the Simmons Jackal scene. The low number of objects identified by HRNet in this scene contributes to fewer scene-graph based loop closures, and as such, worse performance.\nAccuracy Evaluation: Places. Figure 14f evaluates the place layer by comparing the five configurations for Hydra. We use either the ground-truth semantics (when available) or OneFormer (when ground-truth semantics is not available), as the places are not influenced by the semantic segmentation quality. For the places evaluation, we construct a \u201cgroundtruth\u201d GVD from a 3D reconstruction of the entire scene using the ground-truth trajectory of the scene. Using this, we measure the mean distance of an estimated place node to the nearest voxel in the ground-truth GVD, which we call \u201cPosition Error\u201d. Figure 14f reports this metric, along with standard deviation across 5 trials shown as black error bars.\nWe note some important trends in Fig. 14f. Similar to\nthe case of objects, the place positions errors are almost identical for all configurations of Hydra for the uHumans2 scenes. These scenes have very low drift, and are relatively small, so it is expected that a higher quality trajectory makes less of an impact. For the larger scenes, we see a range of results. However, we see for SidPac, VIO+GNN-LC performs worse than VIO+SG-LC, though both outperform VIO+V-LC and VIO. Interestingly, we note that VIO+SG-LC performs slightly worse than VIO+V-LC for the Simmons Jackal scene, while VIO+GNN-LC does the same as VIO+V-LC (but with lower standard deviation, as indicated by the black confidence bars in the plot). Note that the Simmons Jackal scene has multiple uniform rooms with very similar objects and layouts. In general, VIO is outperformed by methods that incorporate loop closures, and the ability to correct for loop closures is important to maintaining an accurate 3D scene graph.\nNeural Tree Ablation 1: Node Classification. We first replicate the semi-supervised node classification experiment described in [16] and compare different message passing architectures on the original graphs (i.e., standard GNNs) and\n23\non the H-tree (i.e., the proposed neural tree). To construct the H-tree graphs, we apply the proposed hierarchical tree decomposition algorithm (Algorithm 1), which concatenates the tree decomposition of each layer. The results are shown in Table II. With the proposed tree decomposition approach, the neural tree achieves an advantage between 1.63% to 11.72% over standard GNN models, depending on the type of message passing architecture. In comparison to the results in [16], the results in Table II are generated using a later version of the PyTorch Geometric library, which supports heterogeneous GNN operators. Also note that the tree decomposition used in Table II differs slightly from the tree decomposition algorithm used in [16]. We provide additional results in Appendix F that examine the impact of these changes.\nWe note that the absolute position of a node centroid is not invariant to translation, and that invariance is important to generalize to new scenes regardless of the choice of coordinate frames. We therefore examine using the relative positions of node centroids as edge features, instead of including them directly as node features. For the H-tree, which contains clique nodes that are comprised of multiple objects or rooms, we use the mean room centroid as the centroid of the clique when computing relative positions. In addition, we also investigate the impact of using heterogeneous graphs (which can accommodate different node and edge types, as the ones arising in the 3D scene graph) against standard homogeneous graphs. For this comparison, we only use the GAT message passing function since it is the only one in Table II that can both handle heterogeneous message passing and incorporate edge features.\nWe present the results of this ablation study in Table III. Comparing the two position encodings, the neural tree mod-\nels achieve significantly higher accuracy when using relative positions as edge features: 7.45% on homogeneous graphs and 2.86% on heterogeneous graphs. Choosing features that are translation invariant (i.e., relative positions) has a clear advantage when using neural tree models. The standard GNNs show near-identical performance between the two position encodings. The heterogeneous graph structure has no significant impact on standard GNNs, but it degrades performance of the neural trees. The exact mechanism behind this decrease in performance in unclear; we posit that the heterogeneous neural tree variants have more parameters than the other architectures, and that the Standford3D dataset may not have enough training data for these architectures. We do not observe a significant performance decrease between the homogeneous and heterogeneous neural tree variants for the MP3D dataset.\nNeural Tree Ablation 2: Room Classification. We compare the neural tree against standard GNNs on a room classification task using the MP3D dataset, predicting room semantic labels for the object-room graphs extracted from Hydra. Compared to Stanford3d, this dataset contains both semantic labels of the object nodes and room layer connectivity from Hydra, in addition to the geometric features (i.e., centroid position and bounding box size) of each node. Therefore, we study the effect of these two additional pieces of information. We use pre-trained word2vec vectors to represent object semantic labels and concatenate them with the geometric feature vectors. As described previously, we filter out partially explored rooms, using a threshold of 60% IoU. As before, we use GAT as the message passing function when training, as all graphs are heterogeneous and have edge features. The results are shown in Table IV. Both approaches show a significant performance improvement using the semantic labels of the objects, ranging from 14% to 17%. The neural tree also shows substantial improvement when incorporating room layer edges, while the standard GNNs do not. The best-performing neural tree model is obtained when using semantic labels for the objects and accounting for room connectivity; this is the model we use in the rest of this paper.\nAccuracy Evaluation: Rooms. We evaluate the accuracy of the room segmentation in Hydra, by first evaluating the quality of the geometric room clustering described in Section III-D1 and then testing the room classification from Section III-D2 for different choices of 2D segmentation network.\nFigure 15 evaluates the room clustering performance, using the precision and recall metrics defined in [46] (here we compute precision and recall over 3D voxels instead of 2D pixels). More formally, these metrics are:\nPrecision = 1 |Re| \u2211 re\u2208Re max rg\u2208Rg |rg \u2229 re| |re|\nRecall = 1 |Rg| \u2211 rg\u2208Rg max re\u2208Re |re \u2229 rg| |rg|\n(12)\nwhere Re is the set of estimated rooms, Rg is the set of ground-truth rooms, and |\u00b7| returns the cardinality of a set; here, each room re (or rg) is defined as a set of free-space voxels. We hand-label the ground-truth rooms Rg from the ground-truth reconstruction of the environment. In particular,\n24\nwe manually define sets of bounding boxes for each room and then identify the set of free-space voxels for rg as all free-space voxels that fall within any of the defined bounding boxes for rg . For the estimated rooms Re, we derive the set of free-space voxels re from the places comprising each estimated room. In eq. (12), Precision then measures the maximum overlap in voxels with a ground-truth room for every estimated room, and Recall measures the maximum overlap in voxels with an estimated room for every groundtruth room. Intuitively, low precision corresponds to undersegmentation, i.e., fewer and larger room estimates, and low recall corresponds to over-segmentation, i.e., more and smaller room estimates. For benchmarking purposes, we also include the approach in [2] (Kimera) as a baseline for evaluation.\nFigure 15 shows that Hydra generally outperforms Kimera [2] when given the ground-truth trajectory, and that Hydra is slightly more robust to multi-floor environments. This is expected, as Kimera performs room segmentation by taking a 2D slice of the voxel-based map of the environment, which does not generalize to multi-floor scenes. For both the split-level Apartment scene and the multi-floor SidPac scene, we achieve higher precision as compared to Kimera. These differences stem from the difficulty of setting an appropriate height to attempt to segment rooms at for Kimera (this is the height at which Kimera takes a 2D slice of the ESDF). Finally, it is worth noting that our room segmentation approach is able to mostly maintain the same levels of precision and recall for VIO+GNN-LC, VIO+SG-LC and VIO+V-LC. In some cases, our approach outperforms Kimera, despite the drift inherent in VIO+GNN-LC, VIO+SG-LC and VIO+V-LC (Kimera uses ground-truth poses).\nFinally, we report room classification accuracy for uHumans2, SidPac, and Simmons across semantic segmentation sources in Table V using the proposed neural tree. We manually assign a ground-truth room category to each hand labeled ground-truth room, and compute the percentage of estimated room labels that match their corresponding ground-truth category. Label correspondences are computed by picking the ground-truth room that contains the most place nodes that comprise each estimated room; an estimated room is assigned a correspondence to unknown if too few place nodes (less than 10) fall inside the best ground-truth room.\nWe note two interesting trends. First, the richness of the object label space impacts the predicted room label accuracy, as shown by the relatively poor performance of the GT column as compared to either HRNet or OneFormer for the uHumans2 scenes (27% versus 40% or 45%): the GT semantics available in the simulator has a smaller number of semantic classes for the objects, hindering performance. This is consistent with the observations in [72]. Second, scenes such as the uHumans2 office or Simmons that are out of distribution \u2014compared to the MP3D dataset we use for training\u2014 perform poorly. An example of a resulting 3D scene graph for SidPac with room category labels is shown in Fig. 1.\nLoop Closure Ablation. Finally, we take a closer look at the quality of the loop closures candidates proposed by our hierarchical loop closure detection approach, and compare it against traditional vision-based approaches on the Office\nscene. In particular, we compare our approach against a visionbased loop closure detection that uses DBoW2 for place recognition and ORB feature matching, as described in [2].\nFigure 16 shows the number of detected loop closures against the error of the registered solution (i.e., the relative pose between query and match computed by the geometric verification) for four different loop closure configurations: (i) \u201cSG-LC\u201d: the proposed handcrafted scene graph loop closure detection, (ii) \u201cSG-GNN\u201d: the proposed learned scene graph loop closure detection, (iii) \u201cV-LC (Nominal)\u201d: a traditional vision-based loop closure detection with nominal parameters, and (iv) \u201cV-LC (Permissive)\u201d: a vision-based loop closure detection with more permissive parameters (i.e., a decreased score threshold and less restrictive geometric verification settings). We report key parameters used in this evaluation in Appendix G. As expected, making the vision-based detection parameters more permissive leads to more but lower-quality loop closures. On the other hand, the scene graph loop closure approach produces approximately twice as many loop closures within 10 cm of translation error and 1\u00b0 of rotation error as the permissive vision-based approach. The proposed approach produces quantitatively and quantitatively better loop\n25\nclosures compared to both baselines. Notably, SG-GNN also outperforms both vision baselines and the SG-LC. We present further discussion and a breakdown of additional statistics of the proposed loop closures of each method in Appendix H.\nTo further examine the relative performance of SG-LC and SG-GNN, we evaluate the top-k precision of both approaches on both the test split of the MP3D dataset used to train the descriptors, and the uHumans2 office scene. For this metric, we compute the percent of the k-highest-scored21 descriptors for each query descriptor that are valid matches; two descriptors are determined to match if the bounding box of their corresponding sub-graphs have a IoU above a specified threshold. We include two configurations of SGGNN for object sub-graphs in this analysis; one that uses a one-hot encoding and one that uses word2vec embeddings to represent the semantic labels of the object nodes. We report this metric for both SG-LC (handcrafted) and SG-GNN (learned) in Table VI for two IoU thresholds: 0.4 and 0.6.\nWe note some interesting trends in Table VI. First, the onehot encoding outperforms the word2vec encoding for both datasets, and appears to transfer better between datasets (i.e., showing 5% better performance for the MP3D dataset, but 10% better performance for uHumans2). Additionally, the original handcrafted descriptors maintain good performance\n21We map distances between descriptors to a [0, 1] range, where 1 corresponds to a match, and 0 corresponds to a non-match\ncompared to the learned descriptors, and only the learned object descriptors appear competitive to the handcrafted descriptors in terms of precision. We believe that the high performance of the handcrafted descriptors is due to the semantic and geometric diversity among the scenes of the MP3D dataset. Previous experiments (using the VIO+GNN-LC configuration of Hydra) imply that the learned descriptors offer improved performance in environments with a more uniform object distribution (i.e., the Simmons Jackal scene).\n3) Onboard Operation on a Robot: This section shows that Hydra is capable of running in real-time and is deployable to a robot. We show this by performing a qualitative experiment, running Hydra online on the Unitree A1. We run Hydra and the chosen 2D semantic segmentation network (MobilenetV2 [67]) on the Nvidia Xavier NX mounted on the back of the A1. Additionally, we run our room classification approach in the loop with Hydra on the CPU of the same Nvidia Xavier. In this test, to circumvent the computational cost of running Kimera-VIO, we use an Intel RealSense T265 to provide odometry estimates to Hydra. As a result, we use our proposed scene-graph loop closure detection method without the appearance-based descriptors (which would rely on KimeraVIO for computation of the visual descriptors and vision-based geometric verification). To maintain real-time performance, we configure the frontend of Hydra to run every fifth keyframe (instead of every keyframe), and limit the reconstruction range to 3.5 m (instead of the nominal 4.5 m). Note that this results in a nominal update rate of 1 Hz for Hydra, though we still perform dense 3D metric-semantic reconstruction at keyframe rate (5 Hz). A breakdown of the runtime of Hydra\u2019s modules during this experiment is available in Section VI-C4.\nFor this experiment, we partially explore a floor of building 31 on the MIT campus consisting of a group of cubicles, a conference room, two impromptu lounge areas, all of which are connected by a hallway. An intermediate scene graph produced by Hydra while running the experiment is shown in Fig. 17, and video is available of the experiment.22 Hydra estimates four rooms for the scene graph; of these four rooms, room 3 is centered over one of the two lounges, and room 0 covers both the conference room (located in the lower right corner of Fig. 17) and a portion of the hallway. Qualitatively, Hydra over-segments the scene, but the produced rooms and labels are still somewhat consistent with the underlying room\n22https://youtu.be/AEaBq2-FeY0\n26\nstructure and labels. In this instance, only room 0 has an incorrectly estimated label; however the room categories that Hydra estimates over the course of the experiment are not as consistent. This is likely due to the poor quality of the 2D semantics from MobilenetV2, and general lack of useful object categories for inferring room labels.\n4) Runtime Evaluation: Figure 18 reports the runtime of Hydra versus the batch approach in [2]. This plot shows that the runtime of the batch approach increases over time and takes more than five minutes to generate the entire scene graph for moderate scene sizes; as we mentioned, most processes in the batch approach [2] entail processing the entire ESDF (e.g., place extraction and room detection), inducing a linear increase in the runtime as the ESDF grows. On the other hand, our scene graph frontend (Hydra Mid-Level in Fig. 18) has a fixed computation cost. In Fig. 18, a slight upward trend is observable for Hydra High-Level, driven by room detection and scene graph optimization computation costs, though remaining much lower than batch processing. Noticeable spikes in the runtime for Hydra High-Level (e.g., at 1400 seconds) correspond to the execution of the 3D scene graph optimization when new loop closures are added.\nWhen reconstructing the scene shown in Fig. 18 (SidPac Floor1\u20133), Hydra uses a maximum of 7.2 MiB for the TSDF, 19.1 MiB for the storage of the semantic labels, and 47.8 MiB for the storage of the dense GVD inside the active window, for a total of 74.1 MiB. Kimera instead uses 79.2 MiB for the TSDF, 211 MiB for the storage of semantic labels, and 132 MiB for the storage of the ESDF when reconstructing the entire environment, for a total of 422 MiB of memory. Note that the memory storage requirement of Hydra for the active window is a fifth of the memory required for Kimera, and that the memory usage of Kimera grows with the size of the scene.\nTable VII reports timing breakdown for the incremental creation of each layer across scenes for a single trial. The object layer runtime is determined by the number of mesh vertices in the active window and the number of possible object semantic classes. The room layer runtime is determined by the number of places (a combination of how complicated and large the scene is); this is why the Office has the largest computation cost for the rooms despite being smaller than the SidPac scenes. Note that our target keyframe rate implies a limit of 200 ms for any of these processes. While the mean and standard deviation of all layers are well below this rate, we do see that the real-time rate is exceeded in some cases for extracting the objects. This specifically occurs for larger scenes (i.e., SidPac and Simmons Jackal), due to the presence of many objects. At the same time, we remark that Hydra is architected in such a way that temporarily exceeding the real-time threshold does not preclude online performance and real-time operation is restored shortly after these delays occur.\nWhile the timing results in Table VII are obtained with a relatively powerful workstation, here we restate that Hydra can run in real-time on embedded computers commonly used in robotics applications. Towards this goal, we report the timing statistics from the online experiment running Hydra onboard the Unitree A1 as shown in Fig. 17. Hydra processes the objects in 83.9\u00b1 65 ms, places in 114.8\u00b1 103 ms, and rooms in 34.7\u00b1 37.6 ms. While these numbers imply that Hydra can run faster than the 1 Hz target rate on the Xavier, note that the 1 Hz limit is chosen to not fully max out the computational resources of the Xavier. Additionally, there are other modules of Hydra and external processes that limit (sometimes significantly) the computational resources of the Xavier (e.g., the reconstruction of the TSDF and GVD). While there is still margin to optimize computation (see conclusions), these initial results stress the practicality and real-time capability of Hydra in building 3D scene graphs."
        },
        {
            "heading": "VII. RELATED WORK",
            "text": "We provide a broad literature review touching on abstractions and symbolic representations (Section VII-A), metricsemantic and hierarchical map representations and algorithms to build them from sensor data (Section VII-B), and loop closure detection and optimization (Section VII-C)."
        },
        {
            "heading": "A. Need for Abstractions and Symbolic Representations",
            "text": "State and Action Abstractions. The need to abstract sensor data into higher-level representations has been studied in the context of planning and decision-making. Konidaris [73] points out the necessity of state and action abstraction for efficient task and motion planning problems. Konidaris et al. [74] extract task-specific state abstractions. James et al. [75] show how task-independent state abstractions can also be learned. James et al. [76] show how to autonomously learn object-centric representations of a continuous and highdimensional environment, and argue that such a representation enables efficient planning for long-horizon tasks. Berg et al. [77] propose a hierarchical representation for planning in large outdoor environments. The hierarchy contains three levels:\n27\nlandmarks (e.g., forests, buildings, streets), neighborhoods, and cities. Several related works also discuss action abstractions, e.g., how to group a sequence of actions into macro-actions (usually referred to as options). Jinnai et al. [78], for instance, provide a polynomial-time approximation algorithm for learning options for a Markov decision process.\nOntologies and Knowledge Graphs. A fundamental requirement for an embodied agent to build and communicate a meaningful representation of an environment is to use a common vocabulary describing concepts of interest23 and their relations. Such a vocabulary may be represented as an ontology or the Kimera-VIO estimatesknowledge graph. The precise definition ofthe Kimera-VIO estimates an ontology varies between communities [80, 81]. A well accepted definition was proposed by Gruber [82] as an explicit specification of conceptualization and later extended to require the conceptualization must be a shared view [83] and written as a formal language [84]. In practice, the definition used is often not as strict. For example, the definition provided by W3C as part of the Web Ontology Language (OWL) [85] describes an ontology as a set of terms in a vocabulary and their inter-relationships. Generally, an embodied agent is committed to a conceptualization regardless if the commitment is explicit (e.g., represented as knowledge graph) or implicit (e.g., represented as a set of labels for a classifier) [86].\nThe need to create a standard ontology was identified by [87], which resulted in the emergence of several knowledge processing frameworks focused on robotics applications [88, 89, 90, 91]. A significant effort has been made in the creation of common-sense ontologies and knowledge graphs [92, 93, 94, 95, 96, 97, 98, 99, 100]. In recent years, there has been a surge in applying these ontologies and knowledge graphs to problems such as 2D scene graph generation [101, 102, 103, 104], image classification [105, 106], visual question answering [107, 108, 109, 110, 111], task planning [112, 113, 114], and representation learning [115, 116], to name a few.\nScene Grammars and Compositional Models. Compositionality, which is the ability of humans to perceive reality as a hierarchy of parts, has been considered fundamental to human cognition [117, 118]. Geman et al. [117] propose a mathematical formulation of compositional models, by recursively grouping or composing constituents and assigning probability factors to composition. Zhu et al. [119] represent and infer visual patters as a recursive compositional model, which is nothing but a tree-structured graphical model where the leaf nodes represent the visual patters, and the higherlayers represent complex compositions. Zhu and Mumford [120], Zhu and Huang [121] discuss how to model objects, images, and scenes using such a hierarchical tree structure, called stochastic grammar, and advocate it to be a general framework for visual representation. Inspired by these insights, recent works such as [122, 123, 124] propose probabilistic generative models to capture hierarchical relationship between entities in a scene.\n23The term \u201cconcept\u201d may be ambiguous without context as concepts may include a description of a task, a thought process, or simply the labels for a classifier. This ambiguity is discussed in more detail in [79].\nRecent works have tended to use such a hierarchical structure along with deep neural networks to provide better learning models. Wang et al. [125] show that using a hierarchical, compositional model of the human body results in better human pose estimation. The model consists of body parts (e.g., shoulder, elbow, arm) organized as a hierarchy (e.g., shoulder, elbow are children of arm). A bottom-up/top-down inference strategy is proposed that is able to correct ambiguities in perceiving the lowest-level parts. Niemeyer and Geiger [126] show that modeling a 3D scene as one composed of objects and background leads to more controllable and accurate image synthesis. Mo et al. [127] model object shape as a hierarchical assembly of individual parts, and the object shape is then generated by transforming the shape parameter of each object part. Ichien et al. [128] show that compositional model significantly outperform deep learning models on the task of analogical reasoning. Yuan et al. [129] survey work on compositional scene representation. While early work by Fodor and Pylyshyn [130] argued that neural-network-based models are not compositional in nature, recent works have suggested otherwise. The works [131, 132, 133] show that deep convolutional networks avoid the curse of dimensionality in approximating a class of compositional functions. Webb et al. [134] show that large language models such as GPT-3 show compositional generalizability as an emergent property. Such compositionality, however, is not yet evident in generative models trained on multi-object scenes [135]."
        },
        {
            "heading": "B. Metric-semantic and Hierarchical Representations for Scene Understanding and Mapping",
            "text": "2D Scene Graphs in Computer Vision. 2D scene graphs are a popular model for image understanding that describes the content of an image in terms of objects (typically grounded by bounding boxes in the image) and their attributes and relations [136, 137]. The estimation of 2D scene graphs (either from a single image or a sequence of images) has been well studied in the computer vision community and is surveyed in [28]. The seminal works [136, 137] advocated the use of 2D scene graphs to perform cognitive tasks such as image search, image captioning, and answering questions. These tasks, unlike object detection, require the model to reason about object attributes and object-to-object relations, and therefore, enable better image understanding. 2D scene graphs have been successfully used in image retrieval [137], caption generation [138, 139], visual question answering [140, 136], and relationship detection [141]. GNNs are a popular tool for joint object labels and/or relationship inference on scene graphs [142, 143, 144, 145]. Chang et al. [146] provide a comprehensive survey on the various methods that have been proposed to infer a 2D scene graph from an image.\nDespite their popularity, 2D scene graphs have several limitations. First of all, they are designed to ground concepts in the image space, hence they are not suitable to model largescale scenes (see discussion in Section II). Second, many of the annotated object-to-object relationships in [136] like \u201cbehind\u201d, \u201cnext to\u201d, \u201cnear\u201d, \u201cabove\u201d, \u201cunder\u201d are harder to assess from a 2D image due to the lack of depth information, and are\n28\nmore easily inferred in 3D. Finally, 2D representations such as 2D scene graphs are not invariant to viewpoint changes (i.e., viewing the same 3D scene from a different viewing angle may result in a different 2D scene graph), as observed in [3].\nFlat Metric-Semantic Representations. The last few years have seen a surge of interest towards metric-semantic mapping, simultaneously triggered by the maturity of traditional 3D reconstruction and SLAM techniques, and by the novel opportunities for semantic understanding afforded by deep learning. The literature has focused on both object-based maps [147, 148, 149, 150, 151, 152] and dense maps, including volumetric models [153, 14, 154], point clouds [155, 156, 157], and 3D meshes [34, 158]. Some approaches combine objects and dense map models [159, 160, 161, 162]. These approaches are not concerned with estimating higher-level semantics (e.g., rooms) and typically return dense models that might not be directly amenable for navigation [40].\nBuilding Parsing. A somewhat parallel research line investigates how to parse the layout of a building from 2D or 3D data. A large body of work focuses on parsing 2D maps [46], including rule-based [47] and learning-based methods [163]. Friedman et al. [164] compute a Voronoi graph from a 2D occupancy grid, which is then labeled using a conditional random field. Recent work focuses on 3D data. Liu et al. [163] and Stekovic et al. [165] project 3D point clouds to 2D maps, which however is not directly applicable to multi-story buildings. Furukawa et al. [166] reconstruct floor plans from images using multi-view stereo combined with a Manhattan World assumption. Lukierski et al. [167] use dense stereo from an omni-directional camera to fit cuboids to objects and rooms. Zheng et al. [168] detect rooms by performing region growing on a 3D metric-semantic model.\nHierarchical Representations and 3D Scene Graphs. Hierarchical maps have been pervasive in robotics since its inception [169, 170, 171, 172]. Early work focuses on 2D maps and investigates the use of hierarchical maps to resolve the divide between metric and topological representations [173, 174, 175, 176, 177]. These works preceded the \u201cdeep learning revolution\u201d and could not leverage the rich semantics currently accessible via deep neural networks.\nMore recently, 3D scene graphs have been proposed as expressive hierarchical models for 3D environments. Armeni et al. [3] model the environment as a graph including lowlevel geometry (i.e., a metric-semantic mesh), objects, rooms, and camera locations. Rosinol et al. [2, 4] augment the model with a topological map of places (modeling traversability), as well as a layer describing dynamic entities in the environment. The approaches in [3, 2, 4] are designed for offline use. Other papers focus on reconstructing a graph of objects and their relations [5, 6, 7]. Wu et al. [7] predict objects and relations in real-time using a graph neural network. Izatt and Tedrake [8] parse objects and relations into a scene grammar model using mixed-integer programming. Gothoskar et al. [9] use an MCMC approach. Gay et al. [178] use a quadric representation to estimate 3D ellipsoids for objects in the scene given input 2D bounding boxes, and use an RNN to infer relationships between detected objects."
        },
        {
            "heading": "C. Maintaining Persistent Representations",
            "text": "Loop Closures Detection. Established approaches for visual loop closure detection in robotics trace back to place recognition and image retrieval techniques in computer vision; these approaches are broadly adopted in SLAM pipelines but are known to suffer from appearance and viewpoint changes [179]. Alternative approaches investigate place recognition using image sequences [180, 181, 182] or deep learning [183]. More related to our proposal is the set of papers leveraging semantic information for loop closure detection. Gawel et al. [184] perform object-graph-based loop closure detection using random-walk descriptors built from 2D images. Liu et al. [185] use similar object-based descriptors but built from a 3D reconstruction. Lin et al. [186] adopt random-walk object-based descriptors and then compute loop closure poses via object registration. Qin et al. [187] propose an object-based approach based on sub-graph similarity matching. Zheng et al. [168] propose a room-level loop closure detector. None of these approaches are hierarchical in nature.\nLoop Closures Correction. After a loop closure is detected, the map needs to be corrected accordingly. While this process is easy in sparse (e.g., landmark-based) representations [30], it is non-trivial to perform in real-time when using dense representations. St\u00fcckler and Behnke [188] and Whelan et al. [189] optimize a map of surfels, to circumvent the need to correct structured representations (e.g., meshes or voxels). Dai et al. [190] propose reintegrating a volumetric map after each loop closure. Reijgwart et al. [191] correct drift in volumetric representations by breaking the map into submaps that can be rigidly re-aligned after loop closures. Whelan et al. [192] propose a 2-step optimization that first corrects the robot trajectory and then deforms the map (represented as a point cloud or a mesh) using a deformation graph approach [17]. Rosinol et al. [2] unify the two steps into a single pose graph and mesh optimization. None of these works are concerned with simultaneously correcting multiple layers in a hierarchical representation."
        },
        {
            "heading": "VIII. CONCLUSIONS",
            "text": "This paper argues that large-scale spatial perception for robotics requires hierarchical representations. In particular, we show that hierarchical representations scale better in terms of memory and are more suitable for efficient inference. Our second contribution is to introduce algorithms to build a hierarchical representation of an indoor environment, namely a 3D scene graph, in real-time as the robot explores the environment. Our algorithms combine 3D geometry (e.g., to cluster the free space into a graph of places), topology (to cluster the places into rooms), and geometric deep learning (e.g., to classify the type of rooms the robot is moving across). Our third contribution is to discuss loop closure detection and correction in 3D scene graphs. We introduce (handcrafted and learning-based) hierarchical descriptors for loop closure detection, and develop a unified optimization framework to correct drift in the 3D scene graph in response to loop closures. We integrate our algorithmic contributions into a heavily parallelized system, named Hydra, and show it can\n29\nbuild accurate 3D scene graphs in real-time across a variety of photo-realistic simulations and real datasets.\nLimitations. While we believe the proposed contributions constitute a substantial step towards high-level scene understanding and spatial perception for robotics, our current proposal has several limitations. First, the sub-graph of places captures the free-space in 3D, which is directly usable for a drone to navigate. However, traversability for ground robots is also influenced by other aspects (e.g., terrain type, steepness). These aspects are currently disregarded in the construction of the GVD and the places sub-graph, but we believe they are particularly important for outdoor extensions of 3D scene graphs. Second, our approach for room segmentation, which first clusters the rooms geometrically, and then labels each room, mainly applies to rooms with clear geometric boundaries (e.g., it would not work in an open floor-plan). We believe this limitation is surmountable (at the expense of using extra training data) by replacing the 2-stage approach with a single learning-based method (e.g., a neural tree or a standard graph neural network) that can directly classify places into rooms. Third, for computational reasons, we restricted the inference in the neural tree to operate at the level of objects, rooms, and buildings. However, it would be desirable for high-level semantic concepts (e.g., room and object labels) to propagate information downward towards the mesh geometry. While a more compact representation of the low-level geometry [193, 194] might facilitate this process, it remains unclear how to fully integrate top-down reasoning in the construction of the 3D scene graph, which is currently a bottom-up process.\nFuture Work. This work opens many avenues for current and future investigation. First of all, while this paper mostly focused on inclusion and adjacency relations, it would be interesting to label nodes and edges of the 3D scene graph with a richer set of relations and affordances, for instance building on [7]. Second, the connections between our scene graph optimization approach and pose graph optimization offer opportunities to improve the efficiency of the optimization by leveraging recent advances in pose graph sparsification as well as novel solvers. Third, it would be interesting to replace the sub-symbolic representation (currently, a 3D mesh and a graph of places) with neural models, including neural implicit representations [21] or neural radiance fields [195], which can more easily incorporate priors and better support shape completion [196]. Fourth, the current set of detectable objects is fairly limited and restricted by the use of pretrained 2D segmentation networks. However, we have noticed in Section VI and in [72] that a larger object vocabulary leads to better room classification; therefore, it would be interesting to investigate novel techniques that leverage language models for open-set segmentation, e.g., [197, 198]. Fifth, it would be desirable the extend the framework in this paper to arbitrary (i.e., mixed indoor-outdoor environments). Finally, the implications of using 3D scene graphs for prediction, planning, and decision-making are still mostly unexplored (see [199, 200, 201] for early examples).\nDISCLAIMER\nResearch was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "heading": "A. Preliminaries on Tree Decomposition and Treewidth",
            "text": "For a graph G with vertices V(G) and edges E(G), a tree decomposition is a tuple (T ,B) where T is a tree graph and B = {B\u03c4}\u03c4\u2208V(T ) is a family of bags, where B\u03c4 \u2282 V(G) for every tree node \u03c4 \u2208 V(T ), such that the tuple (T ,B) satisfies the following two properties:\n(1) Connectedness Property: for every graph node v \u2208 V(G), the sub-graph of T induced by tree nodes \u03c4 whose bag contains node v, is connected, i.e.,\nTv , T [{\u03c4 \u2208 V(T ) | v \u2208 B\u03c4}] (13)\nis a connected sub-graph of T for every v \u2208 V(G). (2) Covering Property: for every edge {u, v} \u2208 E(G) there exists a node \u03c4 \u2208 V(T ) such that u, v \u2208 B\u03c4 . The simplest tree decomposition of any graph G is a tree with a single node, whose bag contains all the nodes in G. However, in practical applications, it is desirable to obtain decompositions where the size of the largest bag is small. This is captured by the notion of treewidth. The treewidth of a tree decomposition (T ,B) is defined as the size of the largest bag minus one:\ntw [(T ,B)] , max \u03c4\u2208V(T ) |B\u03c4 |\u22121. (14)\nThe treewidth of a graph G is defined as the minimum treewidth that can be achieved among all tree decompositions of G. While finding a tree decomposition with minimum treewidth is NP-hard, many algorithms exist that generate tree decompositions with small (i.e., close to the minimum attainable for that graph) treewidth [202, 22, 203, 32, 204].\nOne of the most popular tree decompositions is the junction tree decomposition, which was introduced in [205]. In it, the graph G is first triangulated. Triangulation is done by adding a chord between any two nodes in every cycle of length 4 or more. This eliminates all the cycles of length 4 or more in the graph G to produce a chordal graph Gc. The collection of bags B = {B\u03c4}\u03c4 in the junction tree is chosen as the set of all maximal cliques in the chordal graph Gc. Then, an intersection graph I on B is built, which has a node for every bag in B and an edge between two bags B\u03c4 and B\u00b5 if they have a nonempty intersection, i.e., |B\u03c4 \u2229 B\u00b5|\u2265 1. The weight of every link {\u03c4, \u00b5} in the intersection graph I is set to |B\u03c4 \u2229 B\u00b5|. Finally, the desired junction tree is obtained by extracting a maximum-weight spanning tree on the weighted intersection graph I. It is know that this extracted tree T , with the bag B, is a valid tree decomposition of G that satisfies the connectedness and covering property."
        },
        {
            "heading": "B. Proof of Proposition 3",
            "text": "The proposition provides an upper-bound on the treewidth of a hierarchical graph. We can deduce this bound from Algorithm 1 by noting that: (i) all the bags in the tree decomposition of an `-layered hierarchical graph are nothing but the bags from the tree decomposition of either the layer-` graph G[V`] or the graphs G[C(v)], for v \u2208 V; and (ii) the size of the bags of all tree decompositions of C(v) are increased by one in Line 6 of Algorithm 1. Since the treewidth of a\ntree decompositions is defined as the size of its maximum bag minus one, and since the treewidth of the graph is the minimum attainable by any possible tree decompositions, the bound in eq. (5) follows."
        },
        {
            "heading": "C. Proof of Lemma 4",
            "text": "Let GR be the room graph, as described in the statement of the theorem. We first note that the nodes in GR that have degree one are those rooms that connect to only one other room. If we were to remove all the degree one rooms, in GR, and form another graph G\u2032R, then (i) the treewidth GR and G \u2032\nR would be the same. We can see this by noting that one can form a tree decomposition for GR by appending the tree decomposition of G\u2032R with additional bags of size 2, representing the degree-one room node v in GR\\G \u2032 R and its connecting room in G \u2032\nR. Now we note that (ii) the treewidth of G\u2032R is always bounded by two under the theorem\u2019s assumptions. This is because all nodes in G\u2032R have degree less than or equal to 2 (by assumption). And, hence, G\u2032R will never have a complete graph of four nodes as a sub-graph. This is enough to conclude that the graph G\u2032R will have a treewidth less than or equal to 2 [206]."
        },
        {
            "heading": "D. Object and Room Labels",
            "text": "Objects. For any configuration of Hydra that uses a 2D semantic segmentation network as input, we use the following set of object labels, which is derived from the mpcat40 label space [31]: chair, table, picture, cabinet, cushion, sofa, bed, curtain, chest of drawers, plant, sink, toilet, stool, towel, mirror, tv monitor, bathtub, fireplace, lighting, shelving, blinds, seating, board panel, furniture, appliances, clothes, objects. In addition, the following labels are detected, but treated as structure: wall, floor, door, window, stairs, ceiling, column, counter, railing. We manually construct a mapping from the ADE20k label space [68] that the 2D semantic segmentation networks provide output in to this subset of the mpcat40 label space [31]. The ground truth segmentation for the uHumans2 dataset has a smaller label space. For the apartment, we use the following set of object labels: chair, couch, computer, lamp, bed, table, trashcan. For the office, we use the following set of object labels: chair, bench, couch, objects, painting, plant, trashcan. Both scenes also have similar structure labels that are detected (e.g., wall, floor, ceiling).\nRooms. We predict a subset of the room category labels available in MP3D [31], as we group several room category synonyms together. First, MP3D includes several synonyms for \u201coutdoor\u201d regions in the provided ground-truth room labels (i.e., porch, balcony, outdoor). As we only focus on indoor applications, we group these outdoor labels with the original unknown label from MP3D, and ignore this label for both computing the training loss and label accuracy. Additionally, we group toilet and bathroom together into a single label (bathroom), and group other room and junk with unknown. This gives us the following room label space: bedroom, closet, dining room, lobby, family room, garage, hallway, library, laundry room, kitchen, living room, conference room, lounge, office, game room, stairwell, utility room, theater, gym, balcony, bar, classroom, dining booth, spa, bathroom. Rooms\nthat do not meet the criteria for inference are given the label unknown."
        },
        {
            "heading": "E. Graph Neural Network Training",
            "text": "We provide more details on (i) the pre-processing performed on the 3D scene graphs in the MP3D dataset, (ii) the implementation of the examined architectures, (iii) the training and testing runtime on the Stanford3d and MP3D datasets, (iv) the approach we used for hyper-parameter tuning.\nMP3D Graph Processing. Here we describe additional preprocessing steps that we take to better condition the room classification training on the MP3D dataset. For the room classification ablation in Section VI-C, we use the groundtruth room geometry in MP3D to re-segment the places layer, and add intra-layer edges between the re-segmented rooms as described in Section III-D1. This allows us to disentangle the room classification accuracy from potential errors in the room clustering. We then extract object-room graphs by connecting room and object nodes through places nodes in between. We discard all room nodes that both have no adjacent rooms and contain two or fewer objects.\nImplementation. We implement all graph learning architectures in PyTorch Geometric [58], which supports all four types of message passing functions we test on, as well as a heterogeneous data structure for learning on heterogeneous graphs. We use the ReLU activation function and dropout in between message passing operations unless otherwise specified. For semi-supervised learning on homogeneous graphs, we send the final node hidden states \u2014after performing message passing\u2014 through a linear layer to project embeddings to room and object label spaces separately, and then through softmax layers for the final prediction. For the neural tree, we add an additional mean pooling layer to combine leaf node hidden states after all message passing iterations on the Htree. For semi-supervised learning on heterogeneous graphs and room classification on both graph types, we match the hidden dimension of the last message passing iteration to the number of labels and hence remove the final linear layers before softmax.\nRuntime. Here we report the time required for preprocessing the datasets, as well as training and testing our models. We save all object-room scene graphs with at least one room and one object node for both Stanford3d and MP3D datasets. The total H-tree construction time is 2.8 s for 482 Stanford3d scene graphs with a maximum of 0.087 s for a single graph. For the MP3D dataset the total H-tree construction time is 516 s for a total of 6114 graphs, with a maximum of 0.84 s for a graph. Table VIII reports the (per epoch) train and test time for the standard GNN architectures \u2014GCN, GraphSAGE, GAT, GIN\u2014 and the corresponding neural trees on the homogeneous Stanford3d scene graphs with the training batch size set to 128. Tables IX and X report timing for the GAT architecture with various input scene graph types. We observe that the neural tree takes 1.5 to 2 times as long compared to the corresponding standard GNN for homogeneous graph inputs. This is expected as the neural tree often uses more message passing iterations, and the H-trees are larger than the original graphs. For heterogeneous graphs, the neural tree takes around 3.5 times as long, as heterogeneous H-trees are both larger than the corresponding original graphs and contain more node and edge types.\nHyper-Parameter Tuning. For replicating the experiments in [16] on Stanford3d, we tune hyper-parameters in the same order as [16], by searching over the following sets:\n\u2022 Message passing iterations: [1, 2, 3, 4, 5, 6] \u2022 Message passing hidden size: [16, 32, 64, 128, 256] \u2022 Learning rate: [0.0005, 0.001, 0.005, 0.01]\n\u2022 Dropout probability: [0.25, 0.5, 0.75] \u2022 L2 regularization strength: [0, 0.0001, 0.001, 0.01]\nFor both the standard GNN and the neural tree, the choice of the number of message passing iterations, message passing hidden dimensions, and learning rate has a significantly higher impact on accuracy than the other two hyper-parameters. Therefore, these three hyper-parameters are first tuned using a grid search while keeping dropout and L2 regularization to the lowest values. With these three hyper-parameters fixed, the dropout and L2 regularization are then tuned via another grid search. A dropout ratio of 0.25 turns out to be the optimal choice in all cases. The other tuned hyper-parameters \u2014used in the experiment in Table II\u2014 are reported in Table XI. We find the best hyper-parameters to be consistent with those reported in [16]. Any changes within the search space does not result in more than 0.5 standard deviations of improvement.\nTherefore, we use the same sets of hyper parameters as in [16] and report them in Table XI. Apart from the four listed hyper-parameters, some architectures (GAT, GraphSAGE, GIN) have architecture-specific design choices and hyper-parameters. In the case of GAT, we use 6 attention heads with concatenated output and ELU activation function (instead of ReLU) to be consistent with the original paper. For GraphSAGE, we use the GraphSAGE-mean from the original paper, which does mean-pooling after each convolution operation. In the case of GIN, we use the more general GIN- for better performance. For the additional ablation study in Table III, we keep the same hyper parameters, though we set the hidden dimension to 64 for training on heterogeneous graphs. We use the Adam optimizer and run optimization for 1000 epochs to achieve reasonable convergence during training.\nWe use a similar hyper-parameter tuning procedure for experiments on MP3D. The parameter range we explored are:\n\u2022 Message passing iterations: [2, 3, 4] \u2022 Message passing hidden size: [16, 32, 64, 128] \u2022 Number of attention heads: [1, 3, 5] \u2022 Learning rate: [0.0005, 0.001, 0.002] \u2022 Dropout probability: [0, 0.2, 0.4] \u2022 L2 regularization strength: [0, 0.0001, 0.001]\nFor MP3D, all parameters except L2 regularization have significant impact on accuracy. Therefore, we perform grid search over these hyper parameters first, and then tune L2 regularization. For all setups with more than 1 attention head, we observe averaging over the output performs consistently better than concatenation and therefore use the average when using multi-heads attention. For both approaches, we tune separate sets of hyper parameters for configurations with and without word2vec object semantic features. We reuse these hyper parameters for configurations without room edges. Following this approach, we find that 3 message passing iterations with a hidden dimension of 64 (for the first two iterations) and a L2 regularization constant of 0.001 work best for all setups. The rest of hyper parameters are summarized in Table XII. Training is run for 800 epochs of SGD using the Adam optimizer."
        },
        {
            "heading": "F. Additional Experiments on Semi-supervised Node Classification",
            "text": "The semi-supervised node classification experiment in Table II shows a significant accuracy change compared to the same experiment in [16]. Our implementation differs from [16] in two ways: the PyTorch Geometric version used and the H-tree decomposition method. Here, we provide additional experimental results with respect to these changes.\nWe use PyTorch Geometric 2.2.0 for the results in Table II, which has support for heterogeneous GNN operators. The previous results from [16] used PyTorch Geometric 1.7.0, which was the latest version at the time of publication. We keep the tree decomposition the same as in [16] and rerun the neural tree experiments using PyTorch Geometric 2.2.0. Table XIII compares the results trained using different versions. While the advantage of using the neural tree is retained, the table shows a significant accuracy penalty in moving from PyTorch Geometric 1.7.0 to 2.2.0, especially for the neural tree. The only exception is GCN, where there is a slight improvement for both networks. Using GraphSAGE, GAT, and GIN message passing architecture, standard GNNs trained on original graphs show 1 to 3% decrease in accuracy moving to PyTorch Geometric 2.2.0. For the neural tree models, there is about 4% accuracy drop using GraphSAGE and GIN, and a more significant 8.3% drop using GAT. As a side note, using the proposed tree decomposition, we are able to get comparable accuracy using GAT on the H-tree with relative position feature representations, compared to the approach in [16] (see Table III).\nThe neural tree results in Table XIII computed using PyTorch Geometric 2.2.0 and those in Table II also differ in the H-tree decomposition method. The former computes a tree decomposition of the entire graph, while the later breaks down hierarchical graphs into sub-graphs and then combines the decomposed trees according to Algorithm 1. These results are within 0.5 standard deviations when we use GraphSAGE or GAT. The proposed tree decomposition gains a 3.19% advantage with GCN, but a 4.43% disadvantage with GIN. This could be due to the difference in the H-tree construction, or to the PyTorch Geometric updates which make training certain message passing architectures much more sensitive.\nOverall, regardless of PyTorch Geometric versions and H-tree decomposition methods, the neural tree maintains a clear advantage over standard GNNs on all message passing architectures."
        },
        {
            "heading": "G. Loop Closure Ablation Parameters",
            "text": "Table XIV reports key parameters used for \u201cV-LC (Permissive)\u201d and \u201cV-LC (Nominal)\u201d in the loop closure ablation study in Fig. 16. For the meaning of these parameters we refer the reader to the open-source Kimera implementation from [2].\nTable XV reports key parameters used for \u201cSG-LC\u201d in the ablation study in Fig. 16. SG-LC does not use NSS (Normalized Similarity Scoring) to filter out matches. As such, the matching threshold for the agent visual descriptors as shown in Table XIV was chosen to produce similar numbers of matches at the visual level as \u201cV-LC (nominal)\u201d. For the meaning of each parameter (and details on other parameters), we refer the reader to our open-source implementation at https://github.com/MIT-SPARK/Hydra.\nFinally, Table XVI reports key model parameters used for the GNN architecture of \u201cSG-GNN\u201d in the ablation study in Fig. 16. In this table, we denote a multi-layer perceptron (MLP) asM (i, h1, h2, . . . , hn, o), where i is the input feature size, o is the last (i.e., output) layer size, and h1, h2 . . . , hn are the hidden layer sizes. All intermediate layers are connected via a ReLU nonlinearity activation function. In our architecture, the input node and edge features are passed through node and edge MLP encoders and then fed through multiple iterations of message passing. After message passing, the resulting node embedding are aggregated through another MLP (Graph Aggregation in Table XVI), and then the final aggregated result is passed through another MLP (Graph Transform in Table XVI). As we follow the architecture from [60], we refer the reader to that paper for more details."
        },
        {
            "heading": "H. Additional Loop Closure Experiments",
            "text": "Here we report additional results examining the properties of the loop closure detection configurations presented in\nFig. 16. In particular, we show two different pieces of data for each loop closure that was detected over the five trials: (i) the timestamp difference in seconds between the current pose of the agent and the matched pose of the agent in Fig. 19a and (ii) the rotation angle between the current pose of the agent and the matched pose of the agent in Fig. 19b.\nWe note two important trends. One is that in Fig. 19a the vision-based loop closure detection configurations (i.e., \u201cVLC (Nominal)\u201d and \u201cV-LC (Permissive)\u201d) are centered around 100 s and 500 s, which correspond to the portions of the recorded data for the uHumans2 office scene where the robot revisits the same area in the scene with a similar viewpoint. This is also supported by both vision-based configurations\n38\nhaving a rotation angle between the current and matched pose centered around 0\u00b0 in Fig. 19b. In contrast, both \u201cSG-GNN\u201d and \u201cSG-LC\u201d have a wider distribution of time differences and rotation angles in Fig. 19a and Fig. 19b, suggesting an improved viewpoint invariance.\nNote that some of the loop closures resulting from \u201cSGGNN\u201d and \u201cSG-LC\u201d have a small time difference (see leftmost part of Fig. 19a). In these cases, the sub-graphs being registered often are not disjoint (i.e., a subset of nodes is common to both), and the registration (via TEASER, as described in Section IV-A) returns an identity pose between the two sub-graphs. This relative pose already agrees with the current estimate of the relative pose between the current and the match agent node, hence it does not provide any additional information to the optimization (but does act to constrain the deformation of the scene locally). Regardless, \u201cSG-LC\u201d and \u201cSG-GNN\u201d outperform vision-only loop closures with respect to the overall system performance of Hydra.\nThe second trend to note is that \u201cSG-GNN\u201d has a wider distribution of time differences than \u201cSG-LC\u201d, especially in the range of 150 s to 250 s. While many of the loop closures detected by both of these configurations are from overlapping regions of the scene graph, \u201cSG-GNN\u201d does appear to provide loop closures over a slightly longer horizon than \u201cSG-LC\u201d."
        }
    ],
    "title": "Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems",
    "year": 2023
}