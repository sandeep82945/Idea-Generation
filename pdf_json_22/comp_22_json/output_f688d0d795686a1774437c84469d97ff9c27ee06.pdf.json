{
    "abstractText": "Te presence of these indicators undermines our confdence in the integrity of the article\u2019s content and we cannot, therefore, vouch for its reliability. Please note that this notice is intended solely to alert readers that the content of this article is unreliable. We have not investigated whether authors were aware of or involved in the systematic manipulation of the publication process. Wiley and Hindawi regrets that the usual quality checks did not identify these issues before publication and have since put additional measures in place to safeguard research integrity. We wish to credit our own Research Integrity and Research Publishing teams and anonymous and named external researchers and research integrity experts for contributing to this investigation. Te corresponding author, as the representative of all authors, has been given the opportunity to register their agreement or disagreement to this retraction. We have kept a record of any response received.",
    "authors": [
        {
            "affiliations": [],
            "name": "Li Xin"
        },
        {
            "affiliations": [],
            "name": "Hao Xiaoyan"
        }
    ],
    "id": "SP:2f21575838a34349640b300a053c3c82d553b123",
    "references": [
        {
            "authors": [
                "L.F. Rau"
            ],
            "title": "ExtractingCompany Names from Text",
            "venue": "Proceedings of the Seventh IEEE Conference on Artificial Intelligence Application, no. 1, pp. 29\u201332, IEEE, Miami Beach, FL, USA, Feburary 1991.",
            "year": 1991
        },
        {
            "authors": [
                "S. Sekine",
                "R. Grishman",
                "H. Shinnou"
            ],
            "title": "A Decision Tree Method for Finding and Classifying Names in Japanese texts",
            "venue": "Proceedings of the Sixth Workshop on Very Large Corpora, Montreal, Quebec, Canada, August 1998.",
            "year": 1998
        },
        {
            "authors": [
                "Y.H. Xia",
                "Q. Wang"
            ],
            "title": "Clinical named entity recognition: ECUST in the CCKS-2017 shared task 2",
            "venue": "CEUR Workshop Proceedings, vol. 1976, pp. 43\u201348, 2017.",
            "year": 1976
        },
        {
            "authors": [
                "U.K. Sikdar",
                "B. Barik",
                "B. Gamback"
            ],
            "title": "Flytxt_NTNU at SemEval-2018 task 8: Identifying and classifying malware text using conditional random fields and Naive Bayes classifiers",
            "venue": "Proceedings of the 12th Int Workshop on Semantic Evaluation, pp. 890\u2013893, ACL, New Orleans, Louisiana, June 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Li",
                "J. Yang",
                "B. Li"
            ],
            "title": "Named entity recognition in Chinese electronic medical records based on BERT",
            "venue": "Journal of Inner Mongolia University of Science and Technology, vol. 39, no. 1, pp. 71\u201377, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "R. Zhang",
                "C. Wu"
            ],
            "title": "Named entity recognition method based on GRU",
            "venue": "Application of Computer System, vol. 27, no. 9, pp. 18\u201324, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Chen",
                "X. Ouyang"
            ],
            "title": "Review of named entity recognition technology",
            "venue": "Radio and Communications Technology, vol. 46, no. 3, pp. 251\u2013260, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Purmar",
                "L. Jones",
                "G.N. Aidan"
            ],
            "title": "Attention Is All You need",
            "venue": "Proceedings of the Neural Information Processing Systems, pp. 5998\u20136008, Long Beach, USA, December 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A.B. Bulsari",
                "H. Sax\u00e9n"
            ],
            "title": "A Recurrent Neural Network for Time-Series modelling",
            "venue": "Artificial Neural Nets and Genetic Algorithms, pp. 285\u2013291, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "A. Borthwick"
            ],
            "title": "A Maximum Entropy Approach to Named Entity recognition",
            "venue": "pp. 4701\u20134708, New York University, New York, NY, USA, 1999, Doctor of Philosophy +esis.",
            "year": 1999
        },
        {
            "authors": [
                "M. Collins",
                "Y. Singer"
            ],
            "title": "Unsupervised models for named entity classification",
            "venue": "Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, MD, USA, June 1999.",
            "year": 1999
        },
        {
            "authors": [
                "A. McCallum",
                "W. Li"
            ],
            "title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 188\u2013191, Association for Computational Linguistics, Edmonton, Canada, May 2003.",
            "year": 2003
        },
        {
            "authors": [
                "A. Graves",
                "N. Jaitly",
                "A. Mohamed"
            ],
            "title": "Hybrid speech recognition with deep bidirectional LSTM",
            "venue": "IEEE, in Proceedings of the 2013 IEEE workshop on automatic speech recognition and understanding, pp. 273\u2013278, Olomouc, Czech Republic, December 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "J. Lafferty",
                "A. McCallum",
                "F.C.N. Pereira"
            ],
            "title": "Conditional Random fields: Probabilistic Models for Segmenting and Labeling Sequence data",
            "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, Morgan Kaufmann Publishers Inc, San Francisco, CA, USA, June 2001.",
            "year": 2001
        },
        {
            "authors": [
                "A.J. Viterbi",
                "J.K. Wolf",
                "E. Zehavi",
                "R. Padovani"
            ],
            "title": "A pragmatic approach to trellis-coded modulation",
            "venue": "IEEE Communications Magazine, vol. 27, no. 7, pp. 11\u201319, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "Google"
            ],
            "title": "Chinese BERT Model",
            "venue": "2019, https://storage. googleapis.com/bert_models/2018_11_03/chinese_L-12_H- 768_A-12.zip. 8 Computational Intelligence and Neuroscience",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Retraction",
            "text": ""
        },
        {
            "heading": "Retracted: Recognition of Unknown Entities in Specific Financial",
            "text": ""
        },
        {
            "heading": "Field Based on ERNIE-Doc-BiLSTM-CRF",
            "text": "Computational Intelligence and Neuroscience\nReceived 11 July 2023; Accepted 11 July 2023; Published 12 July 2023\nCopyright \u00a9 2023 Computational Intelligence and Neuroscience. Tis is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nTis article has been retracted by Hindawi following an investigation undertaken by the publisher [1]. Tis investigation has uncovered evidence of one or more of the following indicators of systematic manipulation of the publication process:\n(1) Discrepancies in scope (2) Discrepancies in the description of the research\nreported (3) Discrepancies between the availability of data and\nthe research described (4) Inappropriate citations (5) Incoherent, meaningless and/or irrelevant content\nincluded in the article (6) Peer-review manipulation\nTe presence of these indicators undermines our confdence in the integrity of the article\u2019s content and we cannot, therefore, vouch for its reliability. Please note that this notice is intended solely to alert readers that the content of this article is unreliable. We have not investigated whether authors were aware of or involved in the systematic manipulation of the publication process.\nWiley and Hindawi regrets that the usual quality checks did not identify these issues before publication and have since put additional measures in place to safeguard research integrity.\nWe wish to credit our own Research Integrity and Research Publishing teams and anonymous and named external researchers and research integrity experts for contributing to this investigation.\nTe corresponding author, as the representative of all authors, has been given the opportunity to register their agreement or disagreement to this retraction. We have kept a record of any response received."
        },
        {
            "heading": "1. Introduction",
            "text": "Named entity recognition (NER) refers to the recognition of text fragments belonging to predefined categories from free text. NER task was formally proposed for the first time at the sixth message understanding conference. At that time, only some general entity categories were defined, such as location, organization, and person [1]. At present, the task of named entity recognition has penetrated into various vertical fields, such as medical treatment, finance, and so on. +e named entity recognition algorithmwas first applied to NER by Sekine et al. [2] in 1998, and Borthwick et al. [3] applied themaximum entropy model to NER at the same time. +is was followed by bootstrapping learning by Collins and Singer [4] in 1999 using a small prelabeled data set (seed data). +e relatively mainstream conditional random field model was first applied to NER in 2003 by McCallum and Li et al. [5]. Because the conditional random field model is easy to implement and has certain performance, it is very popular among NLP researchers and is widely used in various types of entity recognition such as person names, place names, time, currency, and organizations. It is one of the most widely used and one of the most successful methods. In recent years, the development of deep learning has\nprogressed steadily, and the use of deep learning in named entity recognition tasks has also become a new trend. Among them, the most commonly used is a recurrent neural network (RNN) and its variants that can capture sequence information, such as long short-term memory (LSTM) network and its improved bidirectional long- and short-term memory (BiLSTM) network. Reference [6] proposed a self-trained BiLSTM-CRF model for Chinese NER tasks. +ere are also researchers using convolutional neural network (CNN) to identify entities, such as Zhu et al. [7] using CNN to encode Chinese characters. Wang Jie et al. [8\u201310] adopted the GRU computing unit in the meeting name task and proposed a GRU-based named entity recognition method.\nUnknown entity recognition in specific financial fields refers to extracting unknown financial entities from unlabeled and unstructured Internet texts. In recent years, with the development of the Internet, traditional offline facade loans and loan advertisements all over the place are gradually being transferred online, resulting in a large number of internet texts containing advertising information. Extracting these financial entity information from internet texts can help relevant institutions better build monitoring systems. However, in order to avoid supervision, these texts use\nHindawi Computational Intelligence and Neuroscience Volume 2022, Article ID 3139898, 8 pages https://doi.org/10.1155/2022/3139898\nRE TR\ntraditional Chinese, symbolic phrases, pinyin, and so on, which has caused great difficulties in extracting financial entity information. Manually extracting these financial entities from massive internet texts will consume a lot of manpower and time, and it is particularly important to use an efficient and accurate algorithm to solve this problem as shown in Figure 1.\nTo this end, a large number of scholars have begun to try to obtain prior semantic knowledge from a large amount of unlabeled text to enhance semantic representation and apply it to various NLP tasks such as named entity recognition. Google Brain proposes transformer-based pretraining models, such as BERT (bidirectional encoder representations from transformers). BERT obtains prior semantic knowledge from unlabeled text through two pretraining tasks, masked language model (MLM), and next sentence prediction (NSP), while fine-tuning downstream task parameters to apply the enhanced semantic representation containing these knowledge to downstream natural language processing tasks, such as recognizing named entities in Chinese electronic medical records [11], with the help of pretrained language models, good results can be achieved. However, the BERT model has inherent deficiencies when dealing with Chinese tasks. It adopts random strategies for\nmasking training for word granularity and does not make full use of lexical data and grammatical structures. +e word handles random masks separately, ignoring the lexical information of \u201cApple phone,\u201d so the universality of the model is affected, and it is difficult to obtain a good semantic representation for emerging mobile phone brands. At the same time, BERT is limited by the maximum input length of the model. When encountering long sentences, it has to be segmented, which is not conducive to the capture of contextual information.\nAiming at the above problems, this paper proposes an unknown entity recognition model in a specific financial domain based on ERNIE-Doc-BiLSTM-CRF (EDBC). Compared with the traditional combination of Word2VecBiLSTM-CRF and different downstream models, the experimental results show that this model can effectively improve the recognition effect of unknown entities in specific financial fields."
        },
        {
            "heading": "2. EDBC Algorithm Model",
            "text": "ERNIE-Doc-BiLSTM-CRF is an unknown financial entity recognition model proposed in this paper. Its main mechanism first obtains the semantic representation of each word\nRE TR\nAC\nand, then for each word, combined with position encoding, according to certain rules, takes the corresponding position. +e hidden layer parameters are feature-fused to obtain a unique semantic representation of each word with contextual information. At the same time, an improved selfattention mechanism is used to calculate the weight of each word in the input text synchronously, to learn the dependencies of each word in the sentence, and to obtain the internal structure of the sentence. +e closely related bidirectional LSTM network double-models the multidimensional vectors containing the internal representation of the sentence and splices it to obtain the updated semantic representation of the sentence. Finally, after being processed by the CRF decoding module, the label sequence is further optimized according to the preset rules to obtain the optimal solution.+e data collection of this studymainly comes from Baidu Encyclopedia, web pages, manual texts, and other massive databases with uniquely identified word vectors. Since each word is a uniquely identified word vector, the relevant data information is valid. +e bidirectional LSTM network further extracts the contextual information of the text and finally uses a conditional random field (CRF) to limit the sequence relationship between labels. +e EDBA model includes an input layer, a pretrained language model layer, a BiLSTM layer, and a CRF layer. +e model structure is shown in Figure 2.\n2.1. ERNIE-Doc Module. With the exploration of a large number of scholars, it has become a consensus that the semantic representation of downstream tasks using pretraining models (PTMs) trained on very large corpora in\nadvance has good results, saving the time of training models for downstream tasks. PTMs have been developed for two generations so far. Among them, the first-generation PTMs represented by the well-known Word2Vec and GloVe models have learned the embedding of a single word well, but there is no effective solution for the context relationship, such as sentence relationship, syntactic structure, polysemy, and so on. +e second-generation PTMs are designed to solve the intersentence isolation phenomenon in the firstgeneration PTMs. For example, the ELMo model using bidirectional LSTM can capture contextual information well, but there is only one layer of bidirectional LSTM network; OpenAI GPT uses a unidirectional LSTM network. +e transformer structure can only capture context information in one direction; the full name of ERNIE-Doc is Enhanced Language Representation with Informative Entities-Doc, and the structure is shown in Figure 3.\n+e core part of ERNIE-Doc is the multilayer transformer structure, which mainly includes position encoding and self-attention mechanism. Compared with the traditional recurrent neural network, the ERNIE-Doc model solves the problems of distraction and long training time in the face of long text by using a multilayer self-attention mechanism. Among them, self-attention is one of the attention mechanisms and an important part of the transformer [12].\n+e core part of ERNIE-Doc is the multilayer transformer structure, which mainly includes two parts: position encoding and self-attention. Among them, compared with the traditional recurrent neural network, the ERNIE-Doc model adopts a multilayer self-attention mechanism (selfattention), which well solves the problems of distraction and\nRE TR AC TE\nlong training time in the face of long texts. Among them, self-attention is one of the attention mechanisms and an important part of the transformer. Its operation mechanism is mainly to use multihead self-attention to connect the encoder to the decoder and multiply the word vector after word embedding with the Wq, Wk, and Wv weight matrix to obtain the query vector (Q), key vector (K), and value vector (V); then the importance of each word multiplied by the Q vector and the K vector is softmax normalized; and finally, this value is multiplied by V to obtain the processed word vector. +e correlation between words is obtained by calculating the attention between each word in the sentence, which further captures the structure of the sentence. +e attention calculation formula, such as (1), is a dimension of Q and K, and \ufffd\ufffd dk  is introduced as a penalty factor to ensure that the inner product of Q and K is within a reasonable range. Attention (Q, K, V) \ufffd softmax QKT \ufffd\ufffd dk  V. (1) Improvements have been made here when calculating attention. +e calculation time and memory usage of selfattention are square. If the sequence length becomes 2 times the original, the memory usage is 4 times the original, and the computing time is also 4 times the original. +erefore, sparse self-attention is adopted, and the attentions with the relative distances not exceeding k \u00b1 3, 2k \u00b1 3, 3k \u00b1 3. . . are set to 0, so that when calculating attention, it has the characteristics of local close correlation and long-distance sparse correlation.\nSince the calculation process of attention does not depend on the order between words in the sentence, but the information is mined by calculating the similarity between words, we can carry out multiple sets of attention training at the same time, so the speed of training is obtained. While greatly improving, it also avoids the problem of information loss caused by too long sequences. But, because of this, we need to mark the sequence of each word in the sentence through Position Embedding. At the time of word embedding calculation at time t, a position vector closely related to time t is introduced, and the two are spliced together as the input of the model. For the problem that the same word appears multiple times in a sentence, since the time t of each word appears is different, although the vector\nencoding of the same word is the same, the final vector is also unique.\nBy repeatedly inputting long text into the model twice, ERNIE-Doc learns and stores the semantic information of the whole text in the rough reading stage, and explicitly integrates the semantic information of the whole text for each text segment in the intensive reading stage, thereby realizing bidirectional modeling and avoiding the need for the problem of context fragmentation.+e calculation in the rough reading stage is shown in (2), and the calculation in the intensive reading stage is shown in (3):\nH \ufffd H 1 1: T \u2218 H 2 1: T \u00b7 \u00b7 \u00b7 \u2218 H N 1: T , (2)\nh n\u22121 \u0393+1 \ufffd SG H \u2218 h n\u22121 \u0393  \u2218 h n\u22121 \u0393+1 , (3)\nwhere H \u2208 R(L\u2217T\u2217N)\u00d7d represents the hidden state of the text T in the skimming phase, L represents the length of each segment, N represents the total number of layers, SG(\u00b7) represents the gradient descent algorithm, and H i\n1: T \ufffd [ h\ni 1 \u2218 h i 2 \u00b7 \u00b7 \u00b7 \u2218 h i\nT] represents the i-th hidden layer connection in the skimming phase. In this way, h\nn\u22121 \u0393+1 is\nguaranteed to capture bidirectional contextual information for the entire document.\nIn addition, the recurrent way of the recurrence memory structure in traditional long text models (Transformer-XL, etc.) limits the effective modeling length of the model. ERNIE-Doc improves it into a same-layer loop, obtains the output of the previous moment and the next, and supports a larger length so that the model retains the semantic information of the upper layer and has the modeling ability of superlong text. Finally, ERNIE-Doc better models the overall information of the text by letting the model learn the sequential relationship between text paragraphs at the text level.\n2.2. BiLSTMModule. Recurrent neural network (RNN) is a type of neural network for processing sequence data [13]. Due to the natural advantages of this kind of network model structure in the field of natural language processing, it has been widely used once it was proposed. For named entity recognition tasks, the forward and backward information of sentences will have a huge impact on text understanding,\nRE TR AC TE D\nand the traditional one-way recurrent neural network can only capture one-way historical information. +erefore, bidirectional RNN (BRNN) was proposed by Graves et al. [14] in 2013, and the improved model was successfully applied to the task of named entity recognition and achieved good results beyond the previous ones. +e bidirectional long short-termmemory (LSTM) network used in this paper is obtained by adjusting the structure of the traditional RNN and is a variant of the LSTM network proposed by Hochreiter and Schmidhuber in 1997 [15], which effectively solves the problem of gradient disappearance and gradient explosion in the face of long texts. Due to the addition of the forgetting gate, compared with the traditional RNN, the performance of LSTM in longer sequences is significantly improved.\nIn the following time, Graves further proposed an improved BiLSTM model, which can make good use of the forward and backward information of sentences and improve the ability of the model to use context information. As shown in Figure 2, the output of the aforementioned ERNIEDoc module will be used as the module\u2019s output.\n+e basic unit calculation method of LSTM is shown in the following formulas:\nz \ufffd tanh W x\nt\nh t\u22121 ,\nz i \ufffd \u03c3 Wi x\nt\nh t\u22121 ,\nz f \ufffd \u03c3 Wf x\nt\nh t\u22121 ,\nz o \ufffd \u03c3 Wo x\nt\nh t\u22121 .\n(4)\nAmong them, the three state calculation methods are shown in the following formulas:\nc t \ufffd z f \u2299 ct\u2212 1 + zi \u2299 z,\nh t \ufffd z o \u2299 tanh ct ,\ny t \ufffd \u03c3 W\u2032ht .\n(5)\nAmong them, zf, zi, and zo are three gated states, which are obtained by multiplying the splicing vector by the weight matrix and then converting it into a value between 0 and 1 through the sigmoid activation function. Instead, the result is converted into a value between \u20131 and 1 through a tanh activation function; W, Wi, Wf, and Wo are all trainable parameters; 88 is the memory state in the LSTM unit; and ct is the LSTM unit. +e hidden layer state of the previous layer, ht\u2212 1, is the output of the current state, and yt, \u03c3, and tanh are activation functions.\n2.3. DecodingModule. In the named entity recognition task, adjacent labels often have dependencies. Although the BiLSTM module fully considers the contextual semantic\ninformation, it lacks restrictions on the order relationship between labels. For example, the common rule is that any sentence always starts with the label \u201cB\u201d or \u201cO,\u201d the label \u201cI\u201d must appear after the label \u201cB\u201d and so on.\nConditional random field (CRF) is the most widely used serialization labeling algorithm, proposed by Lafferty et al. in 2001 [16], and the Viterbi algorithm is usually used for training and decoding linear conditional random fields [17]. +e CRF decoding module is introduced, and some constraints can be added to the predicted labels to constrain the validity of the label sequence.\nSpecifically, if for the specified sequence X(x1, x2, . . . xn), the corresponding label Y(y1, y2, . . . yn) satisfies the condition shown by\nP Yi|X, Y1, . . . , Yi\u22121, Yi+1, . . . Yn(  \ufffd P Yi|X, Yi\u22121, Yi+1( .\n(6)\nLet P (N, K) be the weight matrix output by the decoding module and obtain the evaluation score S (x, y), as shown in the following equation:\nS(x, y) \ufffd \nn\ni\ufffd0 Ayi,yi+1 + \nn\ni\ufffd1 Pi,yi, (7)\nwhere A is the transition matrix, Pi,yi represents the score of the yi-th label of the character, k is the total number of labels, and n is the sequence length.\nFinally, use softmax to get the normalized probability, as shown in the following equation:\nP(y|x) \ufffd exp(score(x, y))\n y\u2032 exp score x, y\u2032( (  . (8)\n+e probability of the label sequence Y is calculated by (8), and the set with the largest probability is selected from it, which is the final label sequence."
        },
        {
            "heading": "3. Experiments and Analysis",
            "text": "+e data set used in this paper is composed of nearly 10 million words of text provided by the national Internet Emergency Response Center, mainly from the microblog posts, microblog comments, current affairs news, Post Bar Forum, and so on captured by the crawler. In practical application, it is found that there are some label errors in the data set, so some corrections are made. Specifically, when selecting the data set, the corresponding text length distribution is mainly long text; nearly 70% of the text length is more than 500 words; the shortest text is composed of 7 words; the longest text is composed of 37,691 words, and the average length of the whole text is 1,425 words. +e training set, verification set, and test set are set according to the proportion of 8:1:1 to ensure the effectiveness of the data set as a whole.\n3.1. Data Labeling Method and Evaluation Metrics. +e data are marked with the BIO three-segment notation method: for each entity, its first word is marked as B\u2013 (entity name); B means that the word is at the beginning of an entity (begin);\nRE TR AC\nTE\nD and subsequent marks are I\u2013 (entity name), for words that have nothing to do with the current word, it is directly marked as O, and we use O to represent outside. Since the\nBIO tagging method supports word-by-word tagging, there is no need to presegment the text before the tagging, which avoids the impact of errors caused by word segmentation. +erefore, compared with the BIOES tagging method, this paper chooses the BIO tagging method to mark the data. For named entity recognition algorithms, three indicators are usually used for evaluation: precision (P), recall (R), and F value, which are defined as the number of correct entities identified by the model, the number of irrelevant entities identified by the model, and the model. +e number of related entities that were not detected. +e specific formula is shown in the following equations: P \ufffd Tp Tp + Fp \u00d7 100%, (9) R \ufffd Tp Tp + Fn \u00d7 100%, (10) F1 \ufffd 2P \u00d7 R P + R \u00d7 100%. (11) 3.2. Experimental Environment. +e environment used in all experiments is shown in Table 1.\n3.3. Data Preprocessing. According to the characteristics of the data set, such as the text contains special symbols such as expressions, various symbols and labels that are not related to the text and so on andmethods such as string substitution, regular expression filtering, and replacing noise are adopted to clean the data set. At the same time, BOI encoding is performed on the training set data.\n3.4. Experimental Setup. +e hyperparameters used in this article were found through trial and error. Table 2 lists some of the hyperparameters used in this paper. During training, in order to find the optimal parameters, multiple rounds of\niterations were carried out, and it was finally found that the fifteenth time was the best. In addition, in order to prevent overfitting, drop rate is set to 0.5 in the LSTM layer, and the activation function adopts ReLU to speed up the training speed and further prevent overfitting; the optimizer is Adam; in addition, the gradient clipping technology is used, and the clip is 0.5; set the parameter to 64 in the attention layer.\n3.5. Experimental Results and Analysis. +roughout the experiment, multiple iterations were performed, and the data of each iteration was compared, as shown in Figure 4. In the graph, the horizontal axis is the number of iterations, and the vertical axis is the percentage. If the number of iterations is too small, it will lead to underfitting, and if the number of iterations is too large, it will lead to overfitting. After experiments, it is found that the data is the best when the number of iterations is 15. At this time, the precision rate, recall rate, and F1 value reach 86.72%, 83.39%, and 85.02%, respectively. When the number of iterations in the early stage is too small, the precision rate, recall rate, and F1 values did not reach the ideal value. When the number of iterations reached 15, the model gradually fitted and became stable at the same time, so the final number of iterations was selected as 15.\nIn order to reduce the influence of randomness on the results as much as possible, fivefold cross-validation is carried out for themodels used in thismodule.+e calculationmethod is shown in (12), whereMSEi refers to themean square error of each result. In the experiment, the value of k is 5, and finally, each MSEi is averaged to get the final MSE result.\nCV(k) \ufffd 1 k \nk\ni\ufffd1 MSEi. (12)\nIn the experiment, the EDBA model and its improved model proposed in this paper are compared with the representative Word2Vec BiLSTM CRF in the field of NER, the\nTable 1: Experimental environment.\nOperating system Windows 10 CPU Intel CoreTM i7-10700F CPU@2.9\u22178 GPU Nvidia RTX 3060 (12GB) Python 3.6.5 TensorFlow 1.15 RAM 24GB\nTable 2: Hyperparameter used.\nHyperparameters Numerical value Epoch 15 Batch size 4 Heads 12 Hidden layer dimension (LSTM) 768 Learning rate 5e-6\n0\n0 5 10 Epoch\n15 20\n20\n40\n60\n80\n100\nVa l (\n% )\nRecall F1 Precision\nFigure 4: Results of different iterations.\nRE TR AC TE D famous BERTmodel in this field, and a series of variants.+e experimental results are shown in Table 3. +e overall comparison is shown in Table 3: In the experiment, the EDBA model proposed in this paper and its improved model are compared with the representative Word2Vec-BiLSTM-CRF in the NER field, the well-known BERTmodel in the field [18], and a series of variants thereof. +e experimental results are shown in Table 3. It can be seen from Table 3 that since the BERT and EDBC models are based on word embeddings, they have more advantages in capturing the semantic information of the text context. At the same time, the Word2Vec algorithm encodes the same word in exactly the same way, ignoring the possibility that the same word appears in different positions. +e semantic change occurs. For example, \u201c+e company did not take study documents seriously because everyone was busy with other things when they were asked for.\u201d \u201cLearning documents\u201d appears twice in the sentence, but the meaning is completely different. +erefore, all evaluation indicators have been significantly improved. Compared with the BERT-CRF model, the excellent information extraction capability of the BiLSTM network has been replaced by the bidirectional transformer structure within the BERTmodel itself. +erefore, the new bidirectional LSTM module in the BERT-BiLSTM-CRF model does not have the result. +e experimental results show that the training time is reduced to a certain extent and the recognition effect is not greatly compromised, so it can be used according to actual needs to make a selection. Compared with the BERT-BiLSTM-CRF model, the EDBCmodel proposed in this paper has a certain improvement in various evaluation indicators. +e experimental results show that the accuracy rate of this model is 86.72%, the recall rate is 83.39%, and the F1 value is 85.02% respectively, which is 13.36% higher than that of other models. +e recall rate increased by 13.05%; F1 value increased by 13.21%."
        },
        {
            "heading": "4. Summary and Outlook",
            "text": "+e financial entity recognition model is based on the EDBC word embedding model; the representation will be based on the left and right contexts in all layers and at the same time solve the problem of insufficient reading ability of long texts,\nso it can capture context information well, compared to traditional models and BERT.+e experimental results show that the accuracy of this model is 86.72%, the recall rate is 83.39%, and the F1 value is 85.02%, respectively. +e model has a certain improvement in precision, recall, and F1 value.\nBenefiting from the domestic environment, the development of my country\u2019s financial technology is far ahead of the world average, but it is worth noting that the rapid development of financial technology is diluting the boundaries of traditional financial business, preventing and resolving systemic financial risks, and preventing the transmission of financial risks from breaking through the limitations of time and space. In the face of new challenges, the establishment of the financial entity identification scheme will greatly improve the efficiency of financial information acquisition and then better provide information support for relevant institutions and individuals in the financial field."
        },
        {
            "heading": "Data Availability",
            "text": "+e data set can be accessed upon request."
        },
        {
            "heading": "Conflicts of Interest",
            "text": "+e authors declare that they have no conflicts of interest."
        },
        {
            "heading": "Acknowledgments",
            "text": "Key R&D Projects in Shanxi Province (rh2100005181); Key R&D projects in Shanxi Province (rh2100005178); Peking University Scientific Research and technology project (203290929-j)."
        }
    ],
    "title": "Retraction Retracted: Recognition of Unknown Entities in Specific Financial Field Based on ERNIE-Doc-BiLSTM-CRF",
    "year": 2023
}