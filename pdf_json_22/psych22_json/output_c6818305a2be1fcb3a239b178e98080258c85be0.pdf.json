{
    "abstractText": "This article aims to provide a theoretical account and corresponding paradigm for analysing how explainable artificial intelligence (XAI) influences people\u2019s behaviour and cognition. It uses insights from research on behaviour change. Two notable frameworks for thinking about behaviour change techniques are nudges aimed at influencing behaviour and boosts aimed at fostering capability. It proposes that local and concept-based explanations are more adjacent to nudges, while global and counterfactual explanations are more adjacent to boosts. It outlines a method for measuring XAI influence and argues for the benefits of understanding it for optimal, safe and ethical human-AI collaboration.",
    "authors": [
        {
            "affiliations": [],
            "name": "Matija Franklin"
        }
    ],
    "id": "SP:5501260436decc00d525d5648724dffa1a2eaf0a",
    "references": [
        {
            "authors": [
                "S. Anjomshoae",
                "A. Najjar",
                "D. Calvaresi",
                "K. Fr\u00e4mling"
            ],
            "title": "Explainable agents and robots: Results from a systematic literature review",
            "venue": "In 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019),",
            "year": 2019
        },
        {
            "authors": [
                "D. Ariely",
                "M.I. Norton"
            ],
            "title": "How actions create\u2013not just reveal\u2013preferences",
            "venue": "Trends in cognitive sciences,",
            "year": 2008
        },
        {
            "authors": [
                "H. Ashton",
                "M. Franklin"
            ],
            "title": "The problem of behaviour and preference manipulation in ai systems",
            "venue": "In The AAAI-22 Workshop on Artificial Intelligence Safety (SafeAI",
            "year": 2022
        },
        {
            "authors": [
                "U. Bhatt",
                "A. Xiang",
                "S. Sharma",
                "A. Weller",
                "A. Taly",
                "Y. Jia",
                "J. Ghosh",
                "R. Puri",
                "J.M. Moura",
                "P. Eckersley"
            ],
            "title": "Explainable machine learning in deployment",
            "venue": "In Proceedings of the 2020 conference on fairness, accountability, and transparency,",
            "year": 2020
        },
        {
            "authors": [
                "I. Donadello",
                "M. Dragoni",
                "C. Eccher"
            ],
            "title": "Explaining reasoning algorithms with persuasiveness: a case study for a behavioural change system",
            "venue": "In Proceedings of the 35th Annual ACM Symposium on Applied Computing,",
            "year": 2020
        },
        {
            "authors": [
                "M. Dragoni",
                "I. Donadello",
                "C. Eccher"
            ],
            "title": "Explainable ai meets persuasiveness: Translating reasoning results into behavioral change advice",
            "venue": "Artificial Intelligence in Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "U. Ehsan",
                "S. Passi",
                "Q.V. Liao",
                "L. Chan",
                "I. Lee",
                "M. Muller",
                "Riedl",
                "M. O"
            ],
            "title": "The who in explainable ai: how ai background shapes perceptions of ai explanations",
            "venue": "arXiv preprint arXiv:2107.13509,",
            "year": 2021
        },
        {
            "authors": [
                "D. Ensign",
                "S.A. Friedler",
                "S. Neville",
                "C. Scheidegger",
                "S. Venkatasubramanian"
            ],
            "title": "Runaway feedback loops in predictive policing",
            "venue": "In Conference on Fairness, Accountability and Transparency,",
            "year": 2018
        },
        {
            "authors": [
                "A.N. Ferguson",
                "M. Franklin",
                "D. Lagnado"
            ],
            "title": "Explanations that backfire: Explainable artificial intelligence can cause information overload",
            "venue": "In Proceedings of the Annual Meeting of the Cognitive Science Society,",
            "year": 2022
        },
        {
            "authors": [
                "M. Franklin",
                "D. Lagnado"
            ],
            "title": "Human-ai interaction paradigm for evaluating explainable artificial intelligence",
            "venue": "In International Conference on HumanComputer Interaction,",
            "year": 2022
        },
        {
            "authors": [
                "M. Franklin",
                "T. Folke",
                "K. Ruggeri"
            ],
            "title": "Optimising nudges and boosts for financial decisions under uncertainty",
            "venue": "Palgrave Communications,",
            "year": 2019
        },
        {
            "authors": [
                "M. Franklin",
                "H. Ashton",
                "R. Gorman",
                "S. Armstrong"
            ],
            "title": "Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of ai",
            "venue": "arXiv preprint arXiv:2203.10525,",
            "year": 2022
        },
        {
            "authors": [
                "G. Gigerenzer",
                "W. Gaissmaier",
                "E. Kurz-Milcke",
                "L.M. Schwartz",
                "S. Woloshin"
            ],
            "title": "Helping doctors and patients make sense of health statistics",
            "venue": "Psychological science in the public interest,",
            "year": 2007
        },
        {
            "authors": [
                "T. Gr\u00fcne-Yanoff",
                "R. Hertwig"
            ],
            "title": "Nudge versus boost: How coherent are policy and theory",
            "venue": "Minds and Machines,",
            "year": 2016
        },
        {
            "authors": [
                "R. Hertwig"
            ],
            "title": "When to consider boosting: some rules for policy-makers",
            "venue": "Behavioural Public Policy,",
            "year": 2017
        },
        {
            "authors": [
                "R. Hertwig",
                "T. Gr\u00fcne-Yanoff"
            ],
            "title": "Nudging and boosting: Steering or empowering good decisions",
            "venue": "Perspectives on Psychological Science,",
            "year": 2017
        },
        {
            "authors": [
                "R. Hertwig",
                "S.M. Herzog"
            ],
            "title": "Fast and frugal heuristics: Tools of social rationality",
            "venue": "Social Cognition,",
            "year": 2009
        },
        {
            "authors": [
                "R.R. Hoffman",
                "S.T. Mueller",
                "G. Klein",
                "J. Litman"
            ],
            "title": "Metrics for explainable ai: Challenges and prospects",
            "venue": "arXiv preprint arXiv:1812.04608,",
            "year": 2018
        },
        {
            "authors": [
                "R.M. Hogarth",
                "E. Soyer"
            ],
            "title": "Providing information for decision making: Contrasting description and simulation",
            "venue": "Journal of Applied Research in Memory and Cognition,",
            "year": 2015
        },
        {
            "authors": [
                "M.R. Islam",
                "M.U. Ahmed",
                "S. Barua",
                "S. Begum"
            ],
            "title": "A systematic review of explainable artificial intelligence in terms of different application domains and tasks",
            "venue": "Applied Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "D. Kahneman"
            ],
            "title": "A perspective on judgment and choice: mapping bounded rationality",
            "venue": "American psychologist,",
            "year": 2003
        },
        {
            "authors": [
                "D. Kazhdan",
                "B. Dimanov",
                "M. Jamnik",
                "P. Li\u00f2",
                "A. Weller"
            ],
            "title": "Now you see me (cme): concept-based model extraction",
            "venue": "arXiv preprint arXiv:2010.13233,",
            "year": 2020
        },
        {
            "authors": [
                "Kindermans",
                "P.-J",
                "S. Hooker",
                "J. Adebayo",
                "M. Alber",
                "K.T. Sch\u00fctt",
                "S. D\u00e4hne",
                "D. Erhan",
                "B. Kim"
            ],
            "title": "The (un) reliability of saliency methods",
            "venue": "In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning,",
            "year": 2019
        },
        {
            "authors": [
                "L. Kirfel",
                "T. Icard",
                "T. Gerstenberg"
            ],
            "title": "Inference from explanation",
            "venue": "Journal of Experimental Psychology: General,",
            "year": 2021
        },
        {
            "authors": [
                "M.J. Kochenderfer"
            ],
            "title": "Decision making under uncertainty: theory and application",
            "venue": "MIT press,",
            "year": 2015
        },
        {
            "authors": [
                "I. Lage",
                "E. Chen",
                "J. He",
                "M. Narayanan",
                "B. Kim",
                "S. Gershman",
                "F. Doshi-Velez"
            ],
            "title": "An evaluation of the human-interpretability of explanation",
            "year": 1902
        },
        {
            "authors": [
                "D.A. Lagnado"
            ],
            "title": "Explaining the evidence: How the mind investigates the world",
            "year": 2021
        },
        {
            "authors": [
                "H. Lakkaraju",
                "O. Bastani"
            ],
            "title": " how do i fool you?\u201d manipulating user trust via misleading black box explanations",
            "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2020
        },
        {
            "authors": [
                "D. Lubo-Robles",
                "D. Devegowda",
                "V. Jayaram",
                "H. Bedle",
                "K.J. Marfurt",
                "M.J. Pranter"
            ],
            "title": "Machine learning model interpretability using shap values: Application to a seismic facies classification task",
            "venue": "In SEG International Exposition and Annual Meeting. OnePetro,",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Lundberg",
                "G. Erion",
                "H. Chen",
                "A. DeGrave",
                "J.M. Prutkin",
                "B. Nair",
                "R. Katz",
                "J. Himmelfarb",
                "N. Bansal",
                "Lee",
                "S.-I"
            ],
            "title": "From local explanations to global understanding with explainable ai for trees",
            "venue": "Nature machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "A. Lusardi",
                "A. Samek",
                "A. Kapteyn",
                "L. Glinert",
                "A. Hung",
                "A. Heinberg"
            ],
            "title": "Visual tools and narratives: New ways to improve financial literacy",
            "venue": "Journal of Pension Economics & Finance,",
            "year": 2017
        },
        {
            "authors": [
                "T. Miller"
            ],
            "title": "why?\u201d understanding explainable artificial intelligence. XRDS: Crossroads",
            "venue": "The ACM Magazine for Students,",
            "year": 2019
        },
        {
            "authors": [
                "C. Molnar"
            ],
            "title": "Interpretable machine learning",
            "venue": "Lulu. com,",
            "year": 2020
        },
        {
            "authors": [
                "M. Narayanan",
                "E. Chen",
                "J. He",
                "B. Kim",
                "S. Gershman",
                "F. Doshi-Velez"
            ],
            "title": "How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation",
            "venue": "arXiv preprint arXiv:1802.00682,",
            "year": 2018
        },
        {
            "authors": [
                "T. Peltola",
                "M.M. \u00c7elikok",
                "P. Daee",
                "S. Kaski"
            ],
            "title": "Machine teaching of active sequential learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "P. Rogers",
                "R. Puryear",
                "J. Root"
            ],
            "title": "Infobesity: The enemy of good decisions",
            "venue": "Insights: Bain Brief,",
            "year": 2013
        },
        {
            "authors": [
                "K. Ruggeri"
            ],
            "title": "Behavioral insights for public policy: concepts and cases",
            "year": 2018
        },
        {
            "authors": [
                "S. Sagawa",
                "A. Raghunathan",
                "P.W. Koh",
                "P. Liang"
            ],
            "title": "An investigation of why overparameterization exacerbates spurious correlations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "A. Shahroudnejad"
            ],
            "title": "A survey on understanding, visualizations, and explanation of deep neural networks",
            "venue": "arXiv preprint arXiv:2102.01792,",
            "year": 2021
        },
        {
            "authors": [
                "M. Srivastava",
                "T. Hashimoto",
                "P. Liang"
            ],
            "title": "Robustness to spurious correlations via human annotations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C.R. Sunstein"
            ],
            "title": "Nudging: a very short guide",
            "venue": "Journal of Consumer Policy,",
            "year": 2014
        },
        {
            "authors": [
                "C.R. Sunstein"
            ],
            "title": "The ethics of influence: Government in the age of behavioral science",
            "year": 2016
        },
        {
            "authors": [
                "K.M. Sutcliffe",
                "K.E. Weick"
            ],
            "title": "Information overload revisited. In The Oxford handbook of organizational decision making",
            "year": 2009
        },
        {
            "authors": [
                "R. Thaler",
                "C. Sunstein"
            ],
            "title": "Nudge: The gentle power of choice architecture",
            "year": 2008
        },
        {
            "authors": [
                "R. Tomsett",
                "D. Braines",
                "D. Harborne",
                "A. Preece",
                "S. Chakraborty"
            ],
            "title": "Interpretable to whom? a role-based model for analyzing interpretable machine learning systems",
            "venue": "arXiv preprint arXiv:1806.07552,",
            "year": 2018
        },
        {
            "authors": [
                "S. Verma",
                "J. Dickerson",
                "K. Hines"
            ],
            "title": "Counterfactual explanations for machine learning: A review",
            "venue": "arXiv preprint arXiv:2010.10596,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 0.\n02 40\n7v 1\n[ cs\n.H C\n] 5\nO ct\n2 02\n2\ncount and corresponding paradigm for analysing how explainable artificial intelligence (XAI) influences people\u2019s behaviour and cognition. It uses insights from research on behaviour change. Two notable frameworks for thinking about behaviour change techniques are nudges - aimed at influencing behaviour - and boosts - aimed at fostering capability. It proposes that local and concept-based explanations are more adjacent to nudges, while global and counterfactual explanations are more adjacent to boosts. It outlines a method for measuring XAI influence and argues for the benefits of understanding it for optimal, safe and ethical human-AI collaboration."
        },
        {
            "heading": "1. Introduction",
            "text": "Deep Learning (DL) is a subfield of Machine Learning (ML) that focuses on developing Deep Neural Network (DNN) models (Shahroudnejad, 2021). DNNs are complex models that are capable of achieving high performance on a variety of tasks. Many deep neural network models are uninterpretable black-boxes, which often leads to users having less trust in them (Miller, 2019). This lack of interpretability and trust can have negative consequences \u2013 people might use an AI that makes mistakes, or not use an AI that could improve the chances of a desired result. To improve the interpretability and trustworthiness of black-box models, Explainable Artificial Intelligence (XAI) research focuses on developing methods to explain the behaviour of these models in terms that are comprehensible to humans (Molnar, 2020).\nExplanations provided by XAI can impact human behaviour and cognition (Donadello et al., 2020;\n1Department of Experimental Psychology, University College London, London, UK. Correspondence to: Matija Franklin <matija.franklin@ucl.ac.uk>.\nProceedings of the 39 th International Conference on Machine Learning, 2022. Copyright 2022 by the author(s).\nDragoni et al., 2020). This paper aims to apply frameworks from research on behaviour change within the broader field of behavioural science (Ruggeri, 2018). Two notable frameworks for thinking about and categorising behaviour change techniques are \u2018nudges\u2019 and \u2018boosts\u2019 (Gru\u0308ne-Yanoff & Hertwig, 2016). A nudge is any aspect of choice architecture \u2013 the context in which people make decisions - aimed at influencing the behaviours of individuals without limiting or forcing options (Sunstein, 2014). Boosts are interventions that foster people\u2019s competencies through changes in knowledge and skills, so that they can make their own choices more effectively (Hertwig, 2017). By doing so this paper seeks to put forward a framework and a paradigm for evaluating whether within the context of human-AI interaction an XAI is nudging performance or boosting capability. It will discuss the implications of each in relation to human-machine collaboration as well as the ethics of influence."
        },
        {
            "heading": "2. Different explanations, different outcomes",
            "text": "A great deal of XAI research has gone into developing methods to improve the explainability of DNN models (Molnar, 2020). As it stands it is not fully clear how different methods may influence human behaviour.\nFeature importance methods (such as saliency methods) generate scores that reveal how important a feature (like a word vector or pixel) is to the AI\u2019s decision-making process (Bhatt et al., 2020). Explanations generated by these methods can be either global or local in scope (Lundberg et al., 2020). Global explanations, like those provided by SHAP (SHapley Additive exPlanations) models, give a quantitative indication of the importance of each input variable on the model\u2019s output (Lubo-Robles et al., 2020). Local explanation methods, such as LIME (Local Interpretable Model Agnostic Explanations), generate a numeric score showing the importance of an input variable in relation to the outcome variable (Lee et al., 2019).\nCounterfactual explanations show what the model\u2019s output would have been if one or more of the inputs had been different (Verma et al., 2020). This can be helpful in understanding why the model arrived at a particular output.\nFinally, concept-based explanations attempt to explain a model\u2019s output by referencing pre-defined or automatically generated sets of concepts that are comprehensible to humans (Kazhdan et al., 2020).\nEven though there has been progress in the development of XAI models, it is still not completely clear which model should be used for human-machine collaboration, and for what purpose. There are still many open question. For example, do local explanations nudge performance in the short term but not necessarily provide enough information to educate a user and boost their capability in the long term? Do global explanations provide enough information to educate? Can counterfactual explanations teach people how their AI works or allow them to identify errors?\nA systematic literature review of 241 papers looked at how the validity and usefulness of explanations have been evaluated by the authors of XAI methods (Anjomshoae et al., 2019). Most studies only conducted user studies in simple scenarios or completely lacked evaluations. The results show that 32% of the research papers did not include any type of evaluation. Furthermore, 59% of the research papers conducted a user study to evaluate the usefulness of the explanation (with a small minority also evaluating user trust towards the AI system). Finally, 9% of the papers used an algorithmic evaluation, which did not involve any empirical user research. These initial findings suggest that different explanations will lead to variations in performance on a task (Lage et al., 2019; Narayanan et al., 2018), and will not always necessarily improve performance and understanding (Kindermans et al., 2019). If viewed as a behaviour change intervention, when does an XAI serve as a nudge, changing behaviour, and when does it serve as a boost, improving capability?"
        },
        {
            "heading": "3. Local and concept-based explanations as nudges",
            "text": "The common aim of nudges is to predictively change targeted behaviours. A nudge is any aspect of choice architecture aimed at influencing people\u2019s behaviour, without limiting or forcing options, or significantly changing their economic incentives (Thaler & Sunstein, 2008). All environments influence behaviour to some extent, even when people are not aware of it. Intentionally changing choice architecture is nudging. Nudges take many shapes (Sunstein, 2014). Default rules such as automatic enrollment in programs, automate decision-making for people by setting a default. Simplification nudges reduce the amount of information presented to people to avoid information overload. The use of descriptive social norms - telling people what most other people are doing - influences behaviour. As a policy tool, nudging has been used in over 80 countries worldwide, and by major supranational institutions such as\nthe World Bank and UN (OECD, 2017).\nNudges have been heavily influenced by Daniel Kahneman\u2019s dual-process account of reasoning (Kahneman, 2003). He proposed that people have \u201dtwo systems\u201d in their mind - System 1 and System 2. System 1 thinking is heuristic. It reacts intuitively and effortlessly, without analysing all available information. System 2 is an analytical and effortful, rationalising process. System 1 thinking is fast, and thus accounts for most behaviour. System 2 can re-evaluate System 1 thinking, thus using System 2 thinking leads to fewer erroneous decision. However, this is difficult, as it requires more cognitive effort. Importantly, some factors and contexts are more likely to trigger System 1 or System 2 thinking than others. Per Sunstein (2016), nudges work by targeting either System 1 thinking, thus influencing behaviour without the awareness of the decision maker, or System 2 thinking, thus promoting deliberative thinking.\nA famous example of nudging is using disclosure nudges. Disclosure nudges disclose decision-relevant information (Sunstein, 2014). They are educative, because they provide a learning experience, and target System 2, by promoting deliberative thinking. Disclosure nudges are rooted in three insights. First, as uncertainty promotes erroneous decision making (Kochenderfer, 2015), disclosure nudges seek to reduce uncertainty with decision-relevant information. Second, when too-much decision-irrelevant information is present, people find decision-making more challenging (Rogers et al., 2013); disclosing only decision-relevant information can, therefore, reduce error. Finally, disclosure nudges create an emphasis frame, making relevant information more salient (Chong & Druckman, 2007).\nViewed through this framework, local feature importance explanations and concept-based explanations can be viewed as a type of disclosure nudge. They will provide a small amount of decision-relevant information at the time a person needs to decide whether or not to trust an AIs advice or prediction. It thus could be the case that findings from the extensive body of research on disclosure nudges may generalise to local and concept-based explanations. Such comparisons can guide future practice and research directions."
        },
        {
            "heading": "4. Global and counterfactual explanations as boosts",
            "text": "Boosts are interventions that aim to promote people\u2019s competencies, so they can make better decisions (Gru\u0308ne-Yanoff & Hertwig, 2016). Proponents of boosts aim to foster skills and decision heuristics that can persist over time, throughout different decision contexts (Hertwig & Gru\u0308ne-Yanoff, 2017). An example of a boost is teaching people better decision-making skills with the use\nof decision trees (Hertwig & Herzog, 2009), or by teaching them how to calculate the expected value of a prospect (Franklin et al., 2019). Unlike nudges, a boosts effectiveness requires people\u2019s awareness of the boost and motivation to improve their competencies. Furthermore, boosts differ from educational nudges, in that they do not guide people towards a decision, but rather they assume that better-skilled people will make advantageous decisions. In the context of a decision, boosts provide people with the right capability for the task at hand.\nBoosts can either promote abilities that are specific to a single domain (e.g., decision-making under risk) or generalise across many domains (e.g., statistical literacy; Hertwig & Gru\u0308ne-Yanoff (2017)). Furthermore, boosts can change the choice architecture to foster competencies, directly teach people skills, or do both. Boosts have been used to improve people\u2019s decision-making capabilities by promoting people\u2019s understanding of statistical information. This competence has been previously achieved through: using graphical representations of statistical information (Lusardi et al., 2017), training math skills (Berkowitz et al., 2015), representing information that avoids framing effects (Gigerenzer et al., 2007), and giving people simulation based representations of statistical information (Hogarth & Soyer, 2015).\nThrough the lens of behaviour change theories, global feature importance explanations and counterfactual explanations can be viewed as a type of boost. Counterfactual explanations can be used to train people to understand when their AI tool is more likely to err. This may generalise as a more broader knowledge about how AI tools \u201dwork\u201d more broadly. Global explanations may increase people\u2019s knowledge about what factors are relevant for a given context. This can improve people\u2019s own predictions and competencies within a certain domain. As was the case with nudges, findings from the research literature on boosts may generalise to global and counterfactual explanations. These comparisons could guide both practice and future research on improving human-AI collaboration."
        },
        {
            "heading": "5. Evaluating XAI Influence",
            "text": "In light of the previous discussion on certain XAIs as performance boosting and others as capability boosting, this section will propose methods for testing whether a explanation is nudging performance or boosting capability. It will outline human-AI interaction paradigms for evaluating an XAI\u2019s influence.\nMeasures of performance will be highly context-specific (Hoffman et al., 2018). It is also possible to identify whether explanations improve performance by helping people do better, or by reducing the number of mistakes people\nmake (Franklin & Lagnado, 2022). Ideally, performance on the same task without an AI will be compared to performance on a task with an AI, and with an XAI. Multiple different XAIs can be used in this comparison. It could either between-subject - comparing the performance of people across different groups - or within-subject - tracking the performance of an individual across time.\nTo give an example, one could compare the effects of local and global explanations for improving a doctor\u2019s diagnostic ability. Specifically, the study would evaluate the benefits of an AI diagnostic tool on performance, and see how this improves when people receive explanations for how that diagnostic tool makes decisions. In this example, doctors will be tested on their ability to detect pneumonia from x-ray images. In a between-subject paradigm, doctors could be randomly placed in four separate groups: 1) doctors with no AI tools, 2) doctors with AI tools, 3) doctors with AI tools and a local explanation, 4) doctors with AI tools and a global explanation. The difference between the first and second group would be a pure measure of how the AI tool improves pneumonia detection. The difference between the second and third, or the second and fourth group would allow one to look at the benefits of the explanation on performance. Finally, the difference between the third and fourth group would serve as a measure of what XAI is more effective in improving performance.\nA similar paradigm can be used to measure capability. An AI without an explanation can allow people to understand the AI\u2019s decision-making process by detecting patterns (Peltola et al., 2019). XAI can provide a more direct learning experience for users. This learning could be procedural, for example a change in a certain ability, or semantic, for example an increase in factual knowledge (Berliner & Calfee, 2013). Procedural knowledge can be measured as people\u2019s performance in the absence of an AI tool after using an AI tool. The pause between initially using the AI tool and then measuring performance without it can be more or less longitudinal. Longer time frames would make for a more valid measure as they would reduce the confounding impact memory may have on performance. Back to the previous example, each doctor can be brought back for testing 2 weeks after the initial study. In this case they would be performing the same activity - pneumonia detection from x-ray images - but this time with no AI tools or explanations. A lack of differences between their previous and current performance would suggest that their interaction with the AI or XAI models led to an increase in capability. Measuring semantic knowledge with a similar paradigm would involve direct, context-specific questions to test a doctor\u2019s knowledge.\nFinally, measuring changes in people\u2019s mental models can provide useful information for whether an XAI has led to\nnew knowledge and capability. Mental models are representations of a person\u2019s understanding of some system or object (Lagnado, 2021). An XAI could change the person\u2019s mental models about the task, domain or the AI tool. Given that people are able to infer causal structures from explanations (Kirfel et al., 2021), explanations can both establish the presence and change the direction of perceived cause and effect relationships. Mental models can be measured using a nearest neighbor task, where participants select the explanation or diagram that best fits their beliefs, or with concept mapping, where users create a diagram which outlines their knowledge (Hoffman et al., 2018)."
        },
        {
            "heading": "6. Issues with XAI influence",
            "text": "Not understanding how an XAI\u2019s explanation influences behaviour and cognition raises three broader issues. First, there may be individual differences in the way explanations influence specific users on certain tasks (Tomsett et al., 2018). A review of 137 articles on XAI applied to different domains shows that although there is preliminary evidence for visual explanations being more acceptable to everyday users, most studies have been directed at expert users (Islam et al., 2022). More research evaluating everyday users\u2019 reactions to XAI is needed (Franklin & Lagnado, 2022). The person receiving the information matters.\nSecond, it is not evident what amount of and type of information a user should receive. Detailed explanations might conceal more information than they reveal. This phenomenon is known as information overload in psychology \u2013 when a person receives too much information, or more specifically when the amount of input to a system exceeds its processing capacity (Sutcliffe & Weick, 2009). This is also true even when all of the information is directly relevant to the task at hand. Recent research has found that information overload effects can occur in response to explanations provided by XAI methods (Ferguson et al., 2022). In general, it was found that the more detailed the explanation, the less useful and trustworthy it was considered to be. Specifically, non-numerical explanations in plain English that listed the variables involved in a model were rated as more understandable and trust-worthy than SHAP models.\nOther research has found the opposite to be true for highly technical individuals. A 2021 study found that people with backgrounds in AI have different requirements for explanations (Ehsan et al., 2021). Specifically, individuals with a background in AI get more use out of numerical explanations compared to less technical individuals. This means that it is not possible to make generalizations, such as that more information will always lead to better decisions.\nFinally, persuasive explanations will influence behaviour and preferences in ways that are currently difficult to pre-\ndict. Current research suggests that an XAI tool, as opposed to a regular AI tool, is more likely to produce a change in behaviour (Donadello et al., 2020; Dragoni et al., 2020). Researchers have also developed XAI methods which can generate misleading explanations that can both increase trust in the AI and mislead domain experts (Lakkaraju & Bastani, 2020). It is therefore essential to understand the direction of the behaviour change, whether it is desirable, and how this will differ from one XAI method to another. This is especially relevant given the fact that preference and behaviour have a bidirectional causal relationship (Ariely & Norton, 2008), which means that AI systems can influence preferences by changing behaviours (Ashton & Franklin, 2022; Franklin et al., 2022). By pushing behaviour in a certain direction, over time, people\u2019s preferences for what to do and how to do it can change."
        },
        {
            "heading": "7. Conclusions",
            "text": "This paper proposed a framework for understanding XAI influence, and a method for measuring it. It proposed that local feature importance explanations and concept-based explanations are adjacent to disclosure nudges, and global feature importance explanations and counterfactual explanations are adjacent to boosts. It is thus possible that the extensive research conducted on nudges and boosts can provide insights for how XAI will influence behaviour. Translational research using existing insights from AI, Behavioural Science and Human-Computer Interaction would be beneficial. Such research could expand our understanding of influence dynamics in human-AI interaction.\nThere are at least three reasons for why understanding XAI influence is important for better human-AI collaboration and teaming. First, the adequate use of explanations allows people to identify AI errors; namely due to spurious correlations (Sagawa et al., 2020) or runaway feedback loops (Ensign et al., 2018). This is especially relevant in light of recent work on making models robust to spurious correlations by leveraging humans\u2019 common sense knowledge of causality (Srivastava et al., 2020). Identifying XAIs for promoting this error-detecting ability would be beneficial.\nSecond, it is important to understand which XAIs result in unwanted or negative (i.e., capability and performance decreasing) influence or sludge (Thaler, 2018). Sludge can encourage self-defeating behaviours or discourage a person\u2019s best interest. A common form of sludge is friction by making something slower or unnecessarily complicated (Sunstein, 2018). Research showing that certain forms of XAI result in information overload is an example of this (Ferguson et al., 2022). Sunstein (2020) argues for sludge audits whereby one identifies the sources of sludge and eliminates them. Similar practices should be used in XAI.\nFinally, understanding the influence of XAI can allow for a more optimal distribution and selection of XAI methods. Some people performing certain tasks will want performance boosts. Others will want to learn. Understanding XAI influence would allow us to match these preferences."
        }
    ],
    "title": "The Influence of Explainable Artificial Intelligence: Nudging Behaviour or Boosting Capability?",
    "year": 2022
}