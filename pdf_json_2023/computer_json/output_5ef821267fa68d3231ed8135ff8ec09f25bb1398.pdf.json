{
    "abstractText": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs\u2019 medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM\u2019s medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sheng Wang"
        },
        {
            "affiliations": [],
            "name": "Zihao Zhao"
        },
        {
            "affiliations": [],
            "name": "Xi Ouyang"
        },
        {
            "affiliations": [],
            "name": "Qian Wang"
        },
        {
            "affiliations": [],
            "name": "Dinggang Shen"
        }
    ],
    "id": "SP:3557dd046dde2d2e46601e625cafc88342ff473d",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katie Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "arXiv preprint arXiv:2204.14198,",
            "year": 2022
        },
        {
            "authors": [
                "Emily Alsentzer",
                "John R Murphy",
                "Willie Boag",
                "Wei-Hung Weng",
                "Di Jin",
                "Tristan Naumann",
                "Matthew McDermott"
            ],
            "title": "Publicly available clinical bert embeddings",
            "venue": "arXiv preprint arXiv:1904.03323,",
            "year": 1904
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 1877
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan"
            ],
            "title": "Generating radiology reports via memory-driven transformer",
            "venue": "In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Yu Gu",
                "Robert Tinn",
                "Hao Cheng",
                "Michael Lucas",
                "Naoto Usuyama",
                "Xiaodong Liu",
                "Tristan Naumann",
                "Jianfeng Gao",
                "Hoifung Poon"
            ],
            "title": "Domain-specific language model pretraining for biomedical natural language processing",
            "venue": "ACM Transactions on Computing for Healthcare (HEALTH),",
            "year": 2021
        },
        {
            "authors": [
                "Simao Herdade",
                "Armin Kappeler",
                "Kofi Boakye",
                "Joao 7 Soares"
            ],
            "title": "Image captioning: Transforming objects into words",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Irvin",
                "Pranav Rajpurkar",
                "Michael Ko",
                "Yifan Yu",
                "Silviana Ciurea-Ilcus",
                "Chris Chute",
                "Henrik Marklund",
                "Behzad Haghgoo",
                "Robyn Ball",
                "Katie Shpanskaya"
            ],
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Seth J Berkowitz",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chih-ying Deng",
                "Roger G Mark",
                "Steven Horng"
            ],
            "title": "Mimic-cxr, a deidentified publicly available database of chest radiographs with free-text reports",
            "venue": "Scientific data,",
            "year": 2019
        },
        {
            "authors": [
                "Tiffany H Kung",
                "Morgan Cheatham",
                "Arielle Medinilla",
                "Chat- GPT",
                "Czarina Sillos",
                "Lorie De Leon",
                "Camille Elepano",
                "Marie Madriaga",
                "Rimel Aggabao",
                "Giezel Diaz-Candido"
            ],
            "title": "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang"
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text",
            "venue": "mining. Bioinformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Guanxiong Liu",
                "Tzu-Ming Harry Hsu",
                "Matthew McDermott",
                "Willie Boag",
                "Wei-Hung Weng",
                "Peter Szolovits",
                "Marzyeh Ghassemi"
            ],
            "title": "Clinically accurate chest x-ray report generation",
            "venue": "In Machine Learning for Healthcare Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Nicolson",
                "Jason Dowling",
                "Bevan Koopman"
            ],
            "title": "Improving chest x-ray report generation by leveraging warmstarting",
            "venue": "arXiv preprint arXiv:2201.09405,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Xi Ouyang",
                "Jiayu Huo",
                "Liming Xia",
                "Fei Shan",
                "Jun Liu",
                "Zhanhao Mo",
                "Fuhua Yan",
                "Zhongxiang Ding",
                "Qi Yang",
                "Bin Song"
            ],
            "title": "Dual-sampling attention network for diagnosis of covid-19 from community acquired pneumonia",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Dinggang Shen",
                "Guorong Wu",
                "Heung-Il Suk"
            ],
            "title": "Deep learning in medical image analysis",
            "venue": "Annual review of biomedical engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Karan Singhal",
                "Shekoofeh Azizi",
                "Tao Tu",
                "S Sara Mahdavi",
                "Jason Wei",
                "Hyung Won Chung",
                "Nathan Scales",
                "Ajay Tanwani",
                "Heather Cole-Lewis",
                "Stephen Pfohl"
            ],
            "title": "Large language models encode clinical knowledge",
            "venue": "arXiv preprint arXiv:2212.13138,",
            "year": 2022
        },
        {
            "authors": [
                "Akshay Smit",
                "Saahil Jain",
                "Pranav Rajpurkar",
                "Anuj Pareek",
                "Andrew Y Ng",
                "Matthew P Lungren"
            ],
            "title": "Chexbert: combining automatic labelers and expert annotations for accurate radiology report labeling using bert",
            "year": 2004
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob L Menick",
                "Serkan Cabi",
                "SM Eslami",
                "Oriol Vinyals",
                "Felix Hill"
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Wang",
                "Abhir Bhalerao",
                "Yulan He"
            ],
            "title": "Cross-modal prototype driven network for radiology report generation",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Sheng Wang",
                "Xi Ouyang",
                "Tianming Liu",
                "Qian Wang",
                "Dinggang Shen"
            ],
            "title": "Follow my eye: Using gaze to supervise computer-aided diagnosis",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903,",
            "year": 1903
        },
        {
            "authors": [
                "Wenwu Ye",
                "Jin Yao",
                "Hui Xue",
                "Yi Li"
            ],
            "title": "Weakly supervised lesion localization with probabilistic-cam pooling, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Quanzeng You",
                "Hailin Jin",
                "Zhaowen Wang",
                "Chen Fang",
                "Jiebo Luo"
            ],
            "title": "Image captioning with semantic attention",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yixiao Zhang",
                "Xiaosong Wang",
                "Ziyue Xu",
                "Qihang Yu",
                "Alan Yuille",
                "Daguang Xu"
            ],
            "title": "When radiology report generation meets knowledge graph",
            "venue": "In Proceedings of the AAAI Con- 8 ference on Artificial Intelligence,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs\u2019 medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to cre-\nate a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM\u2019s medical knowledge can be also used to improve the performance of vision-based medical-image CAD models."
        },
        {
            "heading": "1. Introduction",
            "text": "Large Language Models (LLMs) are advanced artificial intelligence systems that have been trained on vast amounts of text data [5, 22]. These models use deep learning techniques to generate human-like responses, making them useful for a variety of tasks such as language translation, question answering, and text generation. LLMs like OpenAI\u2019s GPT-3 [3] have shown remarkable results in natural language processing and have the potential to revolutionize various industries, including marketing, education, and customer service. The ability of LLMs to process and understand large amounts of data has made them highly sought after for solving complex problems. In the medical domain, LLMs have demonstrated their potential as valuable tools for providing medical knowledge and advice. For instance, a large dialog-based LLM, such as ChatGPT [17], has demonstrated remarkable results in a critical evalua-\nar X\niv :2\n30 2.\n07 25\ntion of its medical knowledge. ChatGPT has successfully passed part of the US medical licensing exams, showcasing its potential to augment medical professionals in delivering care. Inspired by their remarkable progress in natural language processing, it is an interesting topic to integrate the LLMs to understand visual information in computer vision tasks. Processing images involves understanding the spatial relationships between objects, recognizing patterns and textures, and extracting features that describe the objects in an image. These tasks require a deep understanding of visual information, which is challenging for LLMs that have been primarily trained on text data. This limitation presents a major challenge in the medical field, where images play a crucial role in supporting clinical decisions. Medical images, such as X-rays, CT scans and MRIs, are rich in information that can provide critical insights into a patient\u2019s condition. However, LLMs currently struggle to interpret and extract information from these images, limiting their ability to fully support clinical decision-making processes.\nAs the \u201cpure\u201d computer vision method, medical-image computer-aided diagnosis (CAD) networks have achieved significant success in supporting clinical decision-making processes in the medical field [24]. These networks leverage advanced deep learning algorithms to analyze medical images and provide valuable insights to support clinical decision-making. CAD networks have been designed specifically to handle the complexities of visual information in medical images, making them well-suited for tasks such as disease diagnosis [30], lesion segmentation [35], and report generation. These networks have been trained on large amounts of medical image data, allowing them to learn to recognize complex patterns and relationships in visual information that are specific to the medical field.\nThe aim of this paper is to provide a scheme that combines the strength of LLMs and CAD models. In this scheme, namely ChatCAD, the image is first fed into multiple networks, i.e., an image classification network, a lesion segmentation network, and a report generation network as depicted in Figure 1. The results produced by classifica-\ntion or segmentation are a vector or a mask, which can not be understood by LLMs. Therefore, we transform these results into the text representation form as shown in the middle panel of Figure 1. These text-form results will then be concatenated together as a prompt \u201cRevise the report based on results from Network A and Network B\u201d for the LLM. The LLM then summarizes the results from all the CAD networks. As the example in this figure, the refined report combines the findings from all three networks to provide a clear and concise summary of the patient\u2019s condition, highlighting the presence of pneumonia and the extent of the infection in the left lower lobe. In this way, the LLM could correct errors in the generated report based on the results from CAD networks. Our experiment shows that our scheme could improve the diagnosis performance score of the state-of-the-art report generation methods by 16.42%. A major benefit of our approach is the utilization of LLM\u2019s robust logical reasoning capabilities to combine various decisions from multiple models. This allows us to fine-tune each model individually. For instance, in response to an emergency outbreak such as COVID-19, we can add a pneumonia classification model (differentiating between community-acquired pneumonia and COVID-19 [19]) using very few cases without affecting the other models. Since classifiers are usually less data-hungry than other models, we mark it with \u201ctrainable\u201d (green) in Figure 1.\nAnother advantage of bootstraping LLMs to CAD models is that their extensive and robust medical knowledge can be leveraged to provide interactive explanations and medical advice as we illustrate on Figure 2. For example, based on an image and generated report, patients can inquire about appropriate treatment options (second panel) or define medical terms such as \u201cairspace consolidation\u201d (third panel). Or with patient\u2019s chief complaint (forth panel), LLMs can explain why such symptom happens. In this manner, patients can gain a deeper understanding of their symptoms, diagnosis, and treatment more efficiently. It can efficiently help patients to reduce consultation costs with clinical experts. As the performances of CAD models and LLMs become\nincreasingly improved in the future, the proposed scheme has the potential to improve the quality of radiology reports and enhance the feasibility of online healthcare services."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Large Language Models",
            "text": "Recent advances in Transformer architecture [28] and computing power have enabled the training of large language models with billions of parameters, leading to a significant improvement in their ability to summarize, translate, predict and generate human-like text [3, 23, 25].\nSeveral domain-specific LLMs have been developed using general-purpose model weight and training schemes. BioBERT [13] and PubMedBERT [8] are examples of BERT [5] models trained on PubMed for biomedical data, while ClinicalBERT [2] was further trained on the MIMIC dataset and outperformed its predecessor. Med-PaLM [25] was developed in late 2022 using curated biomedical corpora and human feedback, and showed promising results, including a 67.6% accuracy on the MedQA exam. ChatGPT, which was not given supplementary medical training, passed all three parts of the USMLE and achieved over 50% accuracy across all exams and surpassed 60% accuracy in the majority of them [12]."
        },
        {
            "heading": "2.2. Vision-Language Model",
            "text": "A popular method of converting visual information into language is through image captioning. Deep learning-based image caption models [9, 33] can generate descriptive and coherent captions using large datasets such as Microsoft COCO and Flickr 30K. In medical image analysis, image captioning methods are employed to generate exam image reports. For example, Li et al. [14] implement explicit medical abnormality graph learning for report generation. Zhang et al. [34] utilize a pre-constructed knowledge graph based on disease topics, respectively. Another line of research [4, 29] learns cross-modal patterns using selfattention architecture. The recent emergence of foundation models with more clinical knowledge holds promise as a potential future direction.\nRecently, with the increase in model size, advances in the field have shifted towards Vision-Language Pretraining (VLP) and utilizing pre-trained models. CLIP [21] merges visual and language information into a shared feature space, setting new state-of-the-art performance on various downstream tasks. Frozen [27] fine-tunes an image encoder, whose outputs serve as soft prompts for the language model. Flamingo [1] introduces cross-attention layers into the LLM to incorporate visual features, pre-training these new layers on billions of image-text pairs."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Bridge between Image and Text",
            "text": "The key idea is to utilize the powerful logical reasoning capabilities of the LLMs to make more robust disease diagnosis for medical images. Therefore, we need to build a bridge to translate medical images into texts as inputs for the LLM. Our strategy is straightforward: 1) Feed exam images (e.g., X-Ray) into trained CAD models to obtain outputs; 2) Translate these outputs (typically tensors) into natural language; 3) Use language models to summarize the results and make a final conclusion; 4) Based on the results from visual models and pre-trained medical knowledge in the language models, engage in conversation about symptoms, diagnosis, and treatment. In this section, we mainly discuss the details of our proposed scheme.\nAn example is illustrated in Figure 3, where the output of a disease classifier is a 5-value vector indicating the probabilities of five diseases, i.e., Cardiomegaly, Edema, Consolidation, Atelectasis, and Pleural Effusion. After that, we need to translate this result into a prompt sentence for the LLM. A natural way of prompting is to show all five kinds of pathology and their corresponding scores. We first tell the LLM \u201cHigher disease score means higher possibility of illness\u201d as the basic rule in order to avoid some misconception. Then, we represent the score of each disease as \u201c${disease} score: ${score}\u201d as shown in upperright panel (Prompt#1). Reports generated using Prompt#1 can be found at second column in Figure 8 and Figure 9. One may notice that the LLMs are heavily influenced by Prompt#1, usually repeating all the numbers in the output. Reports generated from Prompt#1 are very different from radiologist\u2019s reports since the concrete diagnostic scores is not frequently used in clinical settings.\nTo align with the language commonly used in clinical reports, we propose to transform the concrete scores into descriptions of disease severity as shown in lower-left panel (Prompt#2). Prompt#2 will be designed using a grading system, which will divide the scores into four categories: \u201dNo sign\u201d [0.0-0.2), \u201dSmall possibility\u201d [0.2-0.5), \u201dLikely\u201d [0.5-0.9), and \u201dDefinitely\u201d [0.9 and above). These categories will be used to describe the likelihood of each of the five observations. Prompt#3 is a concise one that reports diseases with diagnosis scores higher than 0.5 in the prompt. If no prediction is made among all five diseases, the prompt will be \u201cNo Finding\u201d. Reports generated from Prompt#2 and Prompt#3 are generally acceptable and reasonable in most cases as one can observe in Figure 8 and Figure 9. \u201cNetwork A\u201d is frequently referenced in the generated reports. Some prompt tricks, e.g., \u201cRevise the report based on results from Network A but without mentioning Network A\u201d, can be applied to removing its mention. We do not utilize these tricks in current experiments."
        },
        {
            "heading": "3.2. Dataset and Implementation",
            "text": "In this paper, we evaluate the performance of the combination of a report generation network (R2GenCMN [4]) and a classification network (PCAM [32]). The report generation networks (CvT2DistilGPT2 and R2GenCMN) are trained on the MIMIC-CXR dataset [11]. The MIMIC-CXR dataset is a large-scale public dataset of chest x-ray images with free-text radiology reports. It contains 377,110 images corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston, MA. At the same time, the classifier is trained on the CheXpert dataset [10]. CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients.\nThe reports from the LLMs are tested on the official test set of the MIMIC dataset. Due to the current limitation of ChatGPT usage (i.e., around 20 requests per hour), we are\nunable to test the entire test set of MIMIC-CXR now. Therefore, 300 cases are randomly selected, including 50 cases of Cardiomegaly, 50 cases of Edema, 50 cases of Consolidation, 50 cases of Atelectasis, 50 cases of Pleural effusion, and 50 cases with no findings. During the evaluation process, the text reports were converted to multi-class labels using cheXbert [26].\nThe LLMs are updating constantly to include more new knowledge and events, leading to the improvement of their reasoning capability. The GPT-3 model we use in this paper is text-davinci-003 which was released by OpenAI on Feb, 2023 based on IntructGPT [18]. The maxlen of the output is set to 1024 and temperature set to 0.5. The ChatGPT [17] model used is the Jan-30-2023 version. In the section \u201cInteractive and Understandable CAD\u201d, ChatGPT is used to generate the example. During our test, the GPT-3 can also provide accurate and helpful chat."
        },
        {
            "heading": "4. Report Generation",
            "text": ""
        },
        {
            "heading": "4.1. Quality Improvement of the Generated Report",
            "text": "In this section, we evaluate the performance of our proposed method with other two report-generation methods, i.e., R2GenCMN [4] and CvT2DistilGPT2 [16]. On the basis of clinical importance and prevalence, we focus on five kinds of observations. Three metrics, including precision (PR), recall (RC), and F1-score (F1), are reported in Table 1.\nThe strengths of our method are clearly shown in Table 1. It has obvious advantages in RC and F1, and is only weaker than R2GenCMN in the term of PR. Our method has a relatively high Recall and F1-score on MIMIC-CXR dataset. For all five kinds of diseases, both CvT2DistilGPT2 and R2GenCMN show inferior performance to our method concerning RC and F1. Specifically, their performances on Edema and Consolidation are rather low. Their RC values on Edema are 0.468 and 0.252, respectively, while\nF-1 Score Comparison\nour method achieves the RC value of 0.626 based on GPT3. The same phenomenon can be observed in Consolidation, where the first two methods hold the values of 0.239 and 0.121 while ours (GPT-3) drastically outperforms them, with the RC value of 0.803. The R2GenCMN has a higher PR value compared to our method on three of five diseases. However, the cost of R2GenCMN\u2019s high performance on Precision is its weakness in the other two metrics, which can lead to biased report generation, e.g., seldomly reporting any potential diseases. At the same time, our method has the highest F1 among all methods, and we believe it can be the most trustworthy report generator."
        },
        {
            "heading": "4.2. How LLMs affect Report Quality",
            "text": "In this section, we compare the performance of different LLMs for report generation. We use Prompt#3 as the default prompt. OpenAI provides four different sizes of GPT-3 models through its publicly accessible API: text-ada001, text-babbage-001, text-curie-001, and text-davinci003. The smallest text-ada-001 can not generate meaningful reports and is therefore not included in this experiment. The size of the models has not been officially disclosed. The figures listed in Table 2 are approximate estimates based on the information in [7].\nWe report the F1-score of all observations in Table 2. It is noteworthy that language models struggle to perform well in clinical tasks when their model size is limited. The diagnostic performances of text-babbage-001 and text-curie001 is subpar, as demonstrated by their low average F1scores over five observations compared with the last two models. The improvement in diagnostic performance is evident in text-davinci-003, whose model size is hundreds of times larger than that of text-babbage-001. On aver-\nage, text-davinci-003\u2019s F1-score is improved from 0.471 to 0.591. The ChatGPT is slightly better than text-davinci003, achieving the improvement of 0.014, and their diagnostic abilities are comparable. Overall, the diagnostic capability of language models is proportional to their size, highlighting the critical role of the logistic reasoning capability of LLMs.\nIn our experiments, it can be observed that more capable models generally produce longer reports as shown in Figure 6. At the same time, nearly 40% of reports generated by text-babbage-001 and nearly 15% of reports generated by text-curie-001 have no meaningful content."
        },
        {
            "heading": "5. Interactive and Understandable CAD",
            "text": "The proposed ChatCAD offers several benefits, including its ability to utilize LLM\u2019s extensive and reliable medical knowledge to provide interactive explanations and advice. As shown in Figure 7, two examples of the interactive CAD are provided, with one chat discussing pleural effusion and the other addressing edema and its relationship to swelling.\nThrough this approach, patients can gain a clearer understanding of their symptoms, diagnosis, and treatment options, leading to more efficient and cost-effective consultations with medical experts. As language models continue to advance and become more accurate with access to more trustworthy medical training data, ChatCAD has the potential to significantly enhance the quality of online healthcare services."
        },
        {
            "heading": "6. Limitations and Discussion",
            "text": "In this paper, we explore a novel framework, ChatCAD, introducing large language models in CAD. The proposed method, however, still has limitations to be solved.\nFirst, LLM-generated reports are not human like in a certain way. LLM is likely to output sentences like \u201cNetwork A\u2019s diagnosis prediction is consistent with the findings in\nthe radiological report\u201d or \u201cThe findings from Network A\u2019s diagnosis prediction are supported by the X-ray\u201d. This is reflected on natural language similarity metrics when we compare to our baseline method. ChatCAD improved the diagnosis accuracy but dropped the BLEU score [20]. A promising way to address this issue is to add a module after ChatGPT to filter generated reports. Or add prompt like \u201cplease do not mention Network A\u201d.\nAdditionally, we only design three typical kinds of prompts that are intuitive, and there is room for improvement. LLMs are capable of solving logical reasoning problems without additional computational costs [31]. In current ChatCAD, we did not give the network about patient\u2019s major complaint since there is no such dataset available. We believe the LLMs can process more complex information than what we currently provide. Better datasets and benchmarks are needed.\nOur experiments demonstrate significant impact of language model size on diagnostic accuracy. Larger, advanced, and more truthy LLMs such as the upcoming GPT-4 may improve the accuracy and report quality further. However, the role of vision classifiers has not yet been explored, and additional research is necessary to determine if models such as ViT [6] or SwinTransformer [15], which boast larger parameters, can deliver improved results. On the other hand, LLMs can also be used to help the training of vision models, such as correcting outputs of vision models using related medical knowledge learned in LLMs.\nIn our work, we have only carried out a qualitative analysis of the prompt design instead of a quantitative analysis. Further in-depth investigations will be undertaken once the API for ChatGPT becomes available for use. Moreover, the specifics of this paper have not been discussed with any\nclinical professionals, and therefore it still lacks rigor in many places. We will improve it in subsequent versions."
        }
    ],
    "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
    "year": 2023
}