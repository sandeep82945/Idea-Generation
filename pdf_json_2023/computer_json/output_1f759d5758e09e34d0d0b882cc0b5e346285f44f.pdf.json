{
    "abstractText": "Assortment optimization has received active explorations in the past few decades due to its practical importance. Despite the extensive literature dealing with optimization algorithms and latent score estimation, uncertainty quantification for the optimal assortment still needs to be explored and is of great practical significance. Instead of estimating and recovering the complete optimal offer set, decision-makers may only be interested in testing whether a given property holds true for the optimal assortment, such as whether they should include several products of interest in the optimal set, or how many categories of products the optimal set should include. This paper proposes a novel inferential framework for testing such properties. We consider the widely adopted multinomial logit (MNL) model, where we assume that each customer will purchase an item within the offered products with a probability proportional to the underlying preference score associated with the product. We reduce inferring a general optimal assortment property to quantifying the uncertainty associated with the sign change point detection of the marginal revenue gaps. We show the asymptotic normality of the marginal revenue gap estimator, and construct a maximum statistic via the gap estimators to detect the sign change point. By approximating the distribution of the maximum statistic with multiplier bootstrap techniques, we propose a valid testing procedure. We also conduct numerical experiments to assess the performance of our method. Keyword: Assortment optimization, combinatorial inference, multinomial logit model, multiplier bootstrap, hypothesis testing. \u2217Department of Biostatistics, Harvard T.H. Chan School of Public Health, shs145@g.harvard.edu \u2020Leonard N. Stern School of Business, New York University, xc13@stern.nyu.edu \u2021Department of Biostatistics & Bioinformatics, Duke University, xingyuan.fang@duke.edu \u00a7Department of Biostatistics, Harvard T.H. Chan School of Public Health, junweilu@hsph.harvard.edu 1 ar X iv :2 30 1. 12 25 4v 4 [ st at .M L ] 4 M ay 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuting Shen"
        },
        {
            "affiliations": [],
            "name": "Xi Chen"
        },
        {
            "affiliations": [],
            "name": "Ethan X. Fang"
        },
        {
            "affiliations": [],
            "name": "Junwei Lu"
        }
    ],
    "id": "SP:2dce068f159e045237d84f4842b439c8496112c5",
    "references": [
        {
            "authors": [
                "S. PMLR. Agrawal",
                "V. Avadhanula",
                "V. Goyal",
                "A. Zeevi"
            ],
            "title": "Mnl-bandit: A dynamic",
            "venue": "mnl-bandit. In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "S.D. Ahipa\u015fao\u011flu",
                "U. Ar\u0131kan",
                "K. Natarajan"
            ],
            "title": "learning approach to assortment selection",
            "venue": "Operations Research,",
            "year": 2019
        },
        {
            "authors": [
                "E.A. Akhigbe",
                "G. Worlu"
            ],
            "title": "Production planning and operational efficiency",
            "year": 2020
        },
        {
            "authors": [
                "A. Aouad",
                "V. Farias",
                "R. Levi",
                "D. Segev"
            ],
            "title": "food and beverage industry in nigeria. vvuqla/kku",
            "year": 2018
        },
        {
            "authors": [
                "A. Aouad",
                "D. Segev"
            ],
            "title": "Display optimization for vertically differentiated locations",
            "venue": "preferences. Operations Research,",
            "year": 2021
        },
        {
            "authors": [
                "D. Bertsimas",
                "M. Sim"
            ],
            "title": "The price of robustness",
            "venue": "under multinomial logit preferences. Management Science,",
            "year": 2004
        },
        {
            "authors": [
                "J. Blanchet",
                "G. Gallego",
                "V. Goyal"
            ],
            "title": "A Markov chain approximation to choice",
            "venue": "pricing with demand learning. Management Science,",
            "year": 2016
        },
        {
            "authors": [
                "S Bubeck"
            ],
            "title": "Convex optimization: Algorithms and complexity",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Burg",
                "G.J.J. v. d.",
                "C.K.I. Williams"
            ],
            "title": "An evaluation of change point detection algorithms",
            "venue": "arXiv preprint arXiv:2003.06222.",
            "year": 2020
        },
        {
            "authors": [
                "G.P. Cachon",
                "C. Terwiesch",
                "Y. Xu"
            ],
            "title": "Retail assortment planning in the presence of consumer search",
            "venue": "Manufacturing & Service Operations Management, 7(4):330\u2013346.",
            "year": 2005
        },
        {
            "authors": [
                "F. Caro",
                "J. Gallien"
            ],
            "title": "Dynamic assortment with demand learning for seasonal consumer goods",
            "venue": "Management science, 53(2):276\u2013292.",
            "year": 2007
        },
        {
            "authors": [
                "L. Chen",
                "W. Ma",
                "K. Natarajan",
                "D. Simchi-Levi",
                "Z. Yan"
            ],
            "title": "Distributionally robust linear and discrete optimization with marginals",
            "venue": "Operations Research, 70(3):1822\u2013 1834.",
            "year": 2022
        },
        {
            "authors": [
                "L. Chen",
                "M. Sim"
            ],
            "title": "Robust cara optimization",
            "venue": "Available at SSRN 3937474.",
            "year": 2021
        },
        {
            "authors": [
                "P. Chen",
                "C. Gao",
                "A.Y. Zhang"
            ],
            "title": "Partial recovery for top-K ranking: Optimality of MLE and SubOptimality of the spectral method",
            "venue": "The Annals of Statistics, 50(3):1618\u20131652.",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "A. Krishnamurthy",
                "Y. Wang"
            ],
            "title": "Robust dynamic assortment optimization in the presence of outlier customers",
            "venue": "arXiv preprint arXiv:1910.04183.",
            "year": 2019
        },
        {
            "authors": [
                "X. Chen",
                "W. Ma",
                "D. Simchi-Levi",
                "L. Xin"
            ],
            "title": "Assortment planning for recommendations at checkout under inventory constraints",
            "venue": "Available at SSRN 2853093.",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "Z. Owen",
                "C. Pixton",
                "D. Simchi-Levi"
            ],
            "title": "A statistical learning approach to personalization in revenue management",
            "venue": "Management Science, 68(3):1923\u20131937.",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "Y. Wang"
            ],
            "title": "A note on a tight lower bound for capacitated mnl-bandit assortment selection models",
            "venue": "Operations research letters, 46(5):534\u2013537.",
            "year": 2018
        },
        {
            "authors": [
                "X. Chen",
                "Y. Wang",
                "Y. Zhou"
            ],
            "title": "Dynamic assortment optimization with changing contextual information",
            "venue": "Journal of Machine Learning Research, 21(216):1\u201344.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chen",
                "J. Fan",
                "C. Ma",
                "K. Wang"
            ],
            "title": "Spectral method and regularized MLE are both optimal for top-K ranking",
            "venue": "Annals of statistics, 47(4):2204\u20132235.",
            "year": 2019
        },
        {
            "authors": [
                "V. Chernozhukov",
                "D. Chetverikov",
                "K. Kato"
            ],
            "title": "Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors",
            "venue": "The Annals of Statistics, 41(6):2786 \u2013 2819.",
            "year": 2013
        },
        {
            "authors": [
                "W.C. Cheung",
                "D. Simchi-Levi"
            ],
            "title": "Thompson sampling for online personalized assortment optimization problems with multinomial logit choice models",
            "venue": "Available at SSRN 3075658.",
            "year": 2017
        },
        {
            "authors": [
                "R.C. Curhan"
            ],
            "title": "Shelf space allocation and profit maximization in mass retailing",
            "venue": "Journal of Marketing,",
            "year": 1973
        },
        {
            "authors": [
                "J.M. Davis",
                "G. Gallego",
                "H. Topaloglu"
            ],
            "title": "Assortment optimization under variants of the nested logit model",
            "venue": "Operations Research, 62(2):250\u2013273.",
            "year": 2014
        },
        {
            "authors": [
                "G. Gallego",
                "G. Iyengar",
                "R. Phillips",
                "A. Dubey"
            ],
            "title": "Managing flexible products on a network",
            "venue": "Available at SSRN 3567371.",
            "year": 2004
        },
        {
            "authors": [
                "G. Gallego",
                "A. Li",
                "Truong",
                "V.-A.",
                "X. Wang"
            ],
            "title": "Approximation algorithms for product framing and pricing",
            "venue": "Operations Research, 68(1):134\u2013160.",
            "year": 2020
        },
        {
            "authors": [
                "G. Gallego",
                "R. Ratliff",
                "S. Shebalov"
            ],
            "title": "A general attraction model and salesbased linear program for network revenue management under customer choice",
            "venue": "Operations research, 63(1):212\u2013232.",
            "year": 2015
        },
        {
            "authors": [
                "C. Gao",
                "Y. Shen",
                "A.Y. Zhang"
            ],
            "title": "Uncertainty quantification in the bradley-terryluce model",
            "venue": "arXiv preprint arXiv:2110.03874.",
            "year": 2021
        },
        {
            "authors": [
                "N. Golrezaei",
                "H. Nazerzadeh",
                "P. Rusmevichientong"
            ],
            "title": "Real-time optimization of personalized assortments",
            "venue": "Management Science, 60(6):1532\u20131551.",
            "year": 2014
        },
        {
            "authors": [
                "P. Jaillet",
                "J. Qi",
                "M. Sim"
            ],
            "title": "Routing optimization under uncertainty",
            "venue": "Operations research, 64(1):186\u2013200.",
            "year": 2016
        },
        {
            "authors": [
                "A.G. K\u00f6k",
                "M.L. Fisher",
                "R. Vaidyanathan"
            ],
            "title": "Assortment planning: Review of literature and industry practice",
            "venue": "Retail Supply Chain Management, volume 223 of International Series in Operations Research & Management Science, pages 175\u2013236. Springer US, Boston, MA.",
            "year": 2015
        },
        {
            "authors": [
                "Lam",
                "S.-W.",
                "T.S. Ng",
                "M. Sim",
                "Song",
                "J.-H."
            ],
            "title": "Multiple objectives satisficing under uncertainty",
            "venue": "Operations Research, 61(1):214\u2013227.",
            "year": 2013
        },
        {
            "authors": [
                "M.M. Li",
                "X. Liu",
                "Y. Huang",
                "C. Shi"
            ],
            "title": "Integrating empirical estimation and assortment personalization for e-commerce: A consider-then-choose model",
            "venue": "SSRN Electronic Journal.",
            "year": 2018
        },
        {
            "authors": [
                "X. Li",
                "H. Sun",
                "C.P. Teo"
            ],
            "title": "Convex optimization for bundle size pricing problem",
            "venue": "Proceedings of the 21st ACM Conference on Economics and Computation, pages 637\u2013638.",
            "year": 2020
        },
        {
            "authors": [
                "C. Liu",
                "M. Liu",
                "H. Sun",
                "Teo",
                "C.-P."
            ],
            "title": "Product and ancillary pricing optimization: Market share analytics via perturbed utility model",
            "venue": "Available at SSRN 4095769.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "E.X. Fang",
                "J. Lu"
            ],
            "title": "Lagrangian inference for ranking problems",
            "venue": "Operations Research, Advance online publication.",
            "year": 2022
        },
        {
            "authors": [
                "S. Mahajan",
                "G. Van Ryzin"
            ],
            "title": "Stocking retail assortments under dynamic consumer substitution",
            "venue": "Operations Research, 49(3):334\u2013351.",
            "year": 2001
        },
        {
            "authors": [
                "M.K. Mantrala",
                "M. Levy",
                "B.E. Kahn",
                "E.J. Fox",
                "P. Gaidarev",
                "B. Dankworth",
                "D. Shah"
            ],
            "title": "Why is assortment planning so difficult for retailers? a framework and research agenda",
            "venue": "Journal of Retailing,",
            "year": 2009
        },
        {
            "authors": [
                "D. McFadden"
            ],
            "title": "Conditional logit analysis of qualitative choice behaviour",
            "venue": "Zarembka, P., editor, Frontiers in Econometrics, pages 105\u2013142. Academic Press New York, New York, NY, USA.",
            "year": 1973
        },
        {
            "authors": [
                "V.K. Mishra",
                "K. Natarajan",
                "H. Tao",
                "Teo",
                "C.-P."
            ],
            "title": "Choice prediction with semidefinite optimization when utilities are correlated",
            "venue": "IEEE transactions on automatic control, 57(10):2450\u20132463.",
            "year": 2012
        },
        {
            "authors": [
                "K. Natarajan",
                "M. Song",
                "Teo",
                "C.-P."
            ],
            "title": "Persistency model and its applications in choice modeling",
            "venue": "Management science, 55(3):453\u2013469.",
            "year": 2009
        },
        {
            "authors": [
                "G. Perakis",
                "M. Sim",
                "Q. Tang",
                "P. Xiong"
            ],
            "title": "Robust pricing and production with information partitioning and adaptation",
            "venue": "Management Science, Advance online publication.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ruan",
                "X. Li",
                "K. Murthy",
                "K. Natarajan"
            ],
            "title": "The limit of the marginal distribution model in consumer choice",
            "venue": "arXiv preprint arXiv:2208.06115.",
            "year": 2022
        },
        {
            "authors": [
                "P. Rusmevichientong",
                "Shen",
                "Z.-J.M.",
                "D.B. Shmoys"
            ],
            "title": "Dynamic assortment optimization with a multinomial logit choice model and capacity constraint",
            "venue": "Operations research, 58(6):1666\u20131680.",
            "year": 2010
        },
        {
            "authors": [
                "P. Rusmevichientong",
                "D. Shmoys",
                "H. Topaloglu"
            ],
            "title": "Assortment optimization with mixtures of logits",
            "venue": "Technical report, Tech. rep., School of IEOR, Cornell University.",
            "year": 2010
        },
        {
            "authors": [
                "P. Rusmevichientong",
                "H. Topaloglu"
            ],
            "title": "Robust assortment optimization in revenue management under the multinomial logit choice model",
            "venue": "Operations research, 60(4):865\u2013882.",
            "year": 2012
        },
        {
            "authors": [
                "Ryzin",
                "G. v.",
                "S. Mahajan"
            ],
            "title": "On the relationship between inventory costs and variety benefits in retail assortments",
            "venue": "Management Science, 45(11):1496\u20131509.",
            "year": 1999
        },
        {
            "authors": [
                "D. Saure",
                "A. Zeevi"
            ],
            "title": "Optimal dynamic assortment planning with demand learning",
            "venue": "Manufacturing & service operations management, 15(3):387\u2013404.",
            "year": 2013
        },
        {
            "authors": [
                "K. Talluri",
                "G. Van Ryzin"
            ],
            "title": "Revenue management under a general discrete choice model of consumer behavior",
            "venue": "Management Science, 50(1):15\u201333.",
            "year": 2004
        },
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "User-friendly tail bounds for sums of random matrices",
            "venue": "Foundations of Computational Mathematics, 12(4):389\u2013434.",
            "year": 2012
        },
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "An introduction to matrix concentration inequalities",
            "venue": "Foundations and Trends\u00ae in Machine Learning, 8(1-2):1\u2013230.",
            "year": 2015
        },
        {
            "authors": [
                "R. Wang",
                "O. Sahin"
            ],
            "title": "The impact of consumer search cost on assortment planning and pricing",
            "venue": "Management Science, 64(8):3649\u20133666.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "X. Chen",
                "Y. Zhou"
            ],
            "title": "Near-optimal policies for dynamic multinomial logit assortment selection models. In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018), volume 31 of Advances in Neural Information Processing Systems, LA JOLLA",
            "venue": "Neural Information Processing Systems (Nips)",
            "year": 2018
        },
        {
            "authors": [
                "Williams",
                "H.C.W.L."
            ],
            "title": "On the formation of travel demand models and economic evaluation measures of user benefit",
            "venue": "Environment and planning. A, 9(3):285\u2013344.",
            "year": 1977
        },
        {
            "authors": [
                "Z. Yan",
                "K. Natarajan",
                "C.P. Teo",
                "C. Cheng"
            ],
            "title": "A representative consumer model in data-driven multiproduct pricing optimization",
            "venue": "Management Science, 68(8):5798\u20135827.",
            "year": 2022
        },
        {
            "authors": [
                "E. Y\u00fccel",
                "F. Karaesmen",
                "F.S. Salman",
                "M. T\u00fcrkay"
            ],
            "title": "Optimizing product assortment under customer-driven demand substitution",
            "venue": "European Journal of Operational Research, 199(3):759\u2013768.",
            "year": 2009
        },
        {
            "authors": [
                "T. Zhu",
                "J. Xie",
                "M. Sim"
            ],
            "title": "Joint estimation and robustness optimization",
            "venue": "Management Science, 68(3):1659\u20131677.",
            "year": 2022
        },
        {
            "authors": [
                "F.S. Zufryden"
            ],
            "title": "A dynamic programming approach for product selection and supermarket shelf-space allocation",
            "venue": "Journal of the Operational Research Society, 37(4):413\u2013422.",
            "year": 1986
        },
        {
            "authors": [
                "N. \u00c7\u00f6mez Dolgan",
                "L. Moussawi-Haidar",
                "M.Y. Jaber",
                "E. Cephe"
            ],
            "title": "Capacitated assortment planning of a multi-location system under transshipments",
            "venue": "International Journal of Production Economics, 251:108550.",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "I{\u00b7} an indicator function of statements, which is equal to 1 if the statement inside {\u00b7} holds true and 0 otherwise. A Proof of Theorem 3.1 The proof basically modifies that of Theorem",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keyword: Assortment optimization, combinatorial inference, multinomial logit model, multiplier bootstrap, hypothesis testing.\n\u2217Department of Biostatistics, Harvard T.H. Chan School of Public Health, shs145@g.harvard.edu \u2020Leonard N. Stern School of Business, New York University, xc13@stern.nyu.edu \u2021Department of Biostatistics & Bioinformatics, Duke University, xingyuan.fang@duke.edu \u00a7Department of Biostatistics, Harvard T.H. Chan School of Public Health, junweilu@hsph.harvard.edu\nar X\niv :2\n30 1.\n12 25\n4v 4\n[ st\nat .M\nL ]\n4 M"
        },
        {
            "heading": "1 Introduction",
            "text": "Assortment optimization has generated extensive research interest due to its important implications in revenue management. Essentially, assortment optimization aims to study the balance between customer demand and product revenues, where the total expected revenues depend on the profits of individual products as well as customers\u2019 preference rankings over the available products. Researchers propose various merchandising strategies to align the offered assortment with the customer decision-making process to maximize the expected revenues. A multitude of applications can be found in the fields of economics (Cachon et al., 2005; C\u0327o\u0308mez Dolgan et al., 2022), marketing (Mantrala et al., 2009; Ko\u0308k et al., 2015), and operations (Blanchet et al., 2016; Aouad et al., 2018).\nTo model the customer choice behavior for assortment planning, many parametric models have been proposed, among which the multinomial logit (MNL) model (McFadden, 1973) is one of the most popular due to its efficient algorithmic solutions to optimization problems. For a complete assortment of n products, the MNL model associates each product with a preference score, and a customer\u2019s willingness to purchase each product is proportional to the underlying preference scores, while there is a non-zero probability that customers may not purchase any product. The goal of assortment optimization is to solve for the optimal assortment that maximizes the expected revenues from the offered products. In real-world scenarios, it often delivers practical benefits to assess certain properties of the optimal assortment with quantified confidence levels and build the decision-making process upon it, which will be the primary concentration of this paper.\nPrevious studies have made significant progress in multiple topics regarding the choice models, including the MNL model, among which the algorithm to find the optimal offered assortment is the main focus in the operations research community. The tractable optimization solutions under the MNL model enabled previous studies to propose efficient assortment optimization algorithms (Talluri and Van Ryzin, 2004; Gallego et al., 2004). Li et al. (2018) considered the assortment optimization for the two-stage MNL model based on an empirical estimation method using aggregated data. Other works attempted to generalize the MNL model by proposing new models to accommodate a broader range of choice behaviors, such as the nested logit model (NL) (Williams, 1977), the generalized attraction model (GAM) (Gallego et al., 2015), the marginal distribution model (MDM) (Natarajan et al., 2009), and the group marginal distribution model (G-MDM) (Ruan et al., 2022). However, assortment optimization under those more general choice models may suffer from intractability. For instance, Rusmevichientong et al. (2010b) and Davis et al. (2014) demonstrated that the assortment optimization is NP-hard for a mixture of MNL model and the nested logit model, respectively. Aouad et al. (2018) also characterized the hardness of approximation for assortment optimization under a general choice model by reducing the optimization problem to a computational problem of detecting large independent sets in graphs.\nApart from the intractable optimization solutions, the aforementioned works treated assortment planning as a deterministic process without accounting for the uncertainty induced by random input. Rusmevichientong and Topaloglu (2012) brought the robustness against perturbation into the scope of assortment optimization under the MNL model by considering\nthe robust optimization approach (Bertsimas and Sim, 2004; Ahipas\u0327aog\u0306lu et al., 2019; Chen and Sim, 2021; Chen et al., 2022a; Perakis et al., 2022; Zhu et al., 2022), where they aimed to find the optimal offer set that maximizes the worst-case expected revenue over an uncertain set of possible preference scores. However, the robust optimization is tailored to the worst-case scenario and will fail to quantify the uncertainty for general cases. Blanchet et al. (2016) proposed an alternative perspective to the same issue and proposed a Markov-chain-based model as a proper approximation to random utility-based discrete choice models, for which they provided an efficient algorithm. Nevertheless, this approach only approximated the true models without fundamentally addressing the uncertainty quantification.\nAs mentioned in the preceding discussion, despite the investigations of optimization algorithms, inferential analysis on the optimal assortment remains underexplored, especially considering the great significance of uncertainty quantification in practice (Lam et al., 2013; Jaillet et al., 2016). Taking the beverage industry (Akhigbe and Worlu, 2020), for example, beverage retailers may ponder on whether to offer a beverage product they see from an advertisement to obtain maximal revenues. Considering the uncertainty of real-world data, to quantify their confidence in making such decisions, the retailers will need information beyond the estimation of the optimal assortment. For instance, the diverse sales records might lead the retailers to be more confident in one product but less confident in another, and the optimal assortment estimation per se can not capture such a difference. Therefore, we need uncertainty quantification to provide the decision-makers with a complete picture. We provide in the following some illustrative decision-making scenarios that merchandisers might be confronted with in practice. The decision on whether to offer a given product of interest is a popular topic in marketing (Zufryden, 1986). More specifically, denote by [n] = {1, . . . , n} the full assortment of products, and by S\u2217 the optimal offered assortment, we summarize the single product inclusion problem in the following example. \u2022 Example 1. For a product i of interest, we aim to test whether i is in the optimal assortment.\nH0 : Product i is not in the optimal assortment S\u2217, H1 : Product i is in the optimal assortment S\u2217.\nApart from inferring the inclusion of a single product, research interest also lies in the shelf space allocation to different product categories (Curhan, 1973) for profit maximization. Returning to the beverage industry example, retailers may wonder about allocating what proportion to each beverage category is the most profitable, e.g., whether to make more than 50% of the offered products on shelf alcoholic and less than 50% non-alcoholic. We formulate this application into the below hypothesis testing example. \u2022 Example 2. For a given category of products A = {i1, . . . , iK} and a given ratio q%, we test if we should provide more than q% of the offered products from the category A to\nmaximize the revenues.\nH0 : More than q% of S\u2217 are from the category A, H1 : No larger than q% of S\u2217 are from the category A.\nWhen there are several competitive beverage brands, the retailers may also be interested in which brand to choose as their major supplier. Such supplier selection problem is also important in marketing (Yu\u0308cel et al., 2009), which we summarize in the example below. \u2022 Example 3. For a partition {A1, . . . , Am} of the products, we test if set A1 constitutes the largest proportion of the optimal assortment S\u2217 in comparison with all other Aj\u2019s.\nH0 : A1 contains the most products in S\u2217 compared with any other Aj\u2019s, H1 : At least one of the other Aj\u2019s contains more products in S\u2217 than A1.\nNote that the above inferential questions on partial properties of the optimal set do not require knowledge of the entire optimal assortment, and recovering the optimal offer set to answer such questions is inefficient since controlling uncertainty for the entire optimal assortment (i.e., the exact choice of all the products) is more challenging than controlling the uncertainty of only its partial properties (i.e., proportions of a given product category), which also explains why uncertainty quantification beyond estimation is of great practical importance. In summary, the aforementioned hypotheses aim to test whether the optimal assortment satisfies some given properties of interest. More generally, let S be the set of all offer sets, i.e., all non-empty subsets of [n], and let S0 \u2286 S be a subset of offer sets satisfying certain properties of interest. We are interested in the following hypothesis testing problem on S\u2217 that\nH0 : S\u2217 \u2208 S0 versus H1 : S\u2217 /\u2208 S0. (1.1)\nExamples 1 to 3 are concrete examples of the general hypothesis testing problem (1.1). In Section 2.2, we will provide an equivalent representation of (1.1) that will ease the computational difficulties caused by the combinatorial nature of assortment optimization and enable efficient inference."
        },
        {
            "heading": "1.1 Major Contributions",
            "text": "To the best of our knowledge, our paper provides the first inferential framework for performing general tests on the optimal assortment. Under the MNL choice model, our proposed method is able to test a large set of interesting properties of the optimal assortment, and we provide the theoretical guarantee for the validity of the test. We summarize our major contributions below. \u2022 As far as we know, we are the first to provide the estimator\u2019s rate of convergence under the MNL model, while previous studies mainly presented the MNL model under a deterministic framework and did not characterize the estimation error for random data. Our convergence rate is consistent with previous results under other choice models. We also\npropose a debiasing procedure under the MNL model to address the bias caused by penalized likelihood estimation, which paves the way for the follow-up inferential procedures. \u2022 By reducing the properties test on the optimal assortment to a sign change point detection problem (Burg and Williams, 2020), we provide a general inferential framework applicable to testing any arbitrary property under the MNL model. Specifically, we develop an inferential procedure based on multiplier bootstrap to construct a confidence interval for the optimal offer set, and apply the confidence interval to perform hypothesis testing on the optimal assortment. We also discuss concrete examples under the general framework, and by adapting to the case-specific settings, we may simplify the inferential procedure while maintaining the test validity. \u2022 We characterize the estimation rate under the MNL model and provide theoretical guarantees for the validity of the general inferential procedure. In comparison with the ranking problems actively studied in recent literature (Chen et al., 2019b; Gao et al., 2021; Liu et al., 2022b), where the latent preference score is the sole parameter of interest and comparing the magnitudes of the preference scores alone is sufficient for inferring general ranking properties, inference problems in assortment optimization involve new dynamics between the revenues and the customer preferences, with the subject of interest being the result of a discrete optimization procedure. Thus inference in assortment optimization is more complex in nature and requires novel theories for uncertainty quantification."
        },
        {
            "heading": "1.2 Literature Review",
            "text": "The research on choice model has a long history, as reviewed above. Regarding the optimization problem under the MNL model, apart from the classic static assortment optimization (Ryzin and Mahajan, 1999; Mahajan and Van Ryzin, 2001), many variants of the MNL model have been proposed to incorporate additional information and make the model more realistic in practical scenarios. Several tracks of works include dynamic assortment optimization with adaptation to unknown customer choice behavior (Caro and Gallien, 2007; Rusmevichientong et al., 2010a; Saure and Zeevi, 2013; Agrawal et al., 2017; Chen and Wang, 2018; Wang et al., 2018; Agrawal et al., 2019; Chen et al., 2020b), personalized assortment optimization integrating customer features (Golrezaei et al., 2014; Cheung and Simchi-Levi, 2017; Chen et al., 2020a, 2022c), robust assortment optimization allowing for model misspecification (Besbes and Zeevi, 2015; Chen et al., 2019a), and assortment optimization that restricts customer views to a subset of the offered assortment (Wang and Sahin, 2018; Gallego et al., 2020; Aouad and Segev, 2021). Specifically, Agrawal et al. (2019) proposed an efficient online algorithm that achieves simultaneous exploration and exploitation without requiring prior knowledge of instant parameters under the capacitated MNL model, and they achieved a near-optimal regret bound. Chen et al. (2020b) took into consideration the time-varying features of products and adopted a changing contextual MNL model that allows the utility to be linearly dependent on the underlying time-evolving features, upon which they designed a dynamic policy to simultaneously learn the unknown features while making adaptive decisions for the offered assortment. Cheung and Simchi-Levi (2017) studied the personalized MNL model where they assign each product with a fixed unknown coefficient and each customer\nwith a known time-varying feature vector. To allow for outlier customer behavior, Chen et al. (2019a) adopted the \u03b5-contamination model, which assumes that a small proportion of the observations might be contaminated by an arbitrary distribution of choice behavior.\nTo maximize the expected revenues, aside from optimizing the offered assortment, pricing optimization provides an alternative perspective and enjoys a wealth of literature (Li et al., 2020; Yan et al., 2022; Liu et al., 2022a). For example, Yan et al. (2022) constructed a data-driven framework for solving the multi-product pricing problem, where they considered a separable representative consumer model (SRCM) to establish mathematical relationships between pricing and product demands. Liu et al. (2022a) generalized the work of Yan et al. (2022) and studied a perturbed utility model (PUM) for modeling customers\u2019 choice over subsets of primary products and ancillary services, and they achieved an efficient solution to the pricing problem by approximating the PUM with an additive perturbed utility model (APUM).\nSince optimization problems in choice models usually entail knowledge of the latent parameters, recent choice model studies proposed various estimation methods to quantify the error rate resulting from random data. In terms of preference score estimation, the Bradley-Terry-Luce (BTL) model (Chen et al., 2019b) is among the most popular and is a special case of the MNL model with the offer set cardinality restricted to two and the no-purchase option removed. Many works in previous literature focused on estimating the preference scores and recovering the ranking of products under the BTL model. For instance, Chen et al. (2019b) showed the optimality of both the MLE and spectral method up to some constant factor for the exact recovery of the top-k ranking. Their results were further complemented by Chen et al. (2022b), who further dug into the leading constant factor of the optimal sample complexity and showed that the spectral method is sub-optimal when taking into account the leading constant, whereas the MLE is still optimal. Besides, Chen et al. (2022b) established the minimax partial recovery rate for top-k ranking problem under the BTL model. There are also works generalizing the estimation results to other choice models. For example, Mishra et al. (2012) proposed a parsimonious discrete choice model that avoids the Independence of Irrelevant Alternatives (IIA) and Invariant Proportion of Substitution (IPS) properties of the BTL model, and they estimated the choice probabilities through semidefinite optimization.\nApart from parameter estimation, uncertainty quantification also constitutes an important part of the ranking problems under the BTL model and is receiving recent research attention. For example, Gao et al. (2021) characterized the asymptotic normality of the MLE and spectral estimator under the sparse BTL model, which facilitates inferential analysis of the underlying scores. Liu et al. (2022b) applied a Lagrangian debiasing correction to the regularized MLE and performed a general inference on the ranking properties based on the resulting estimator. Compared with the rich literature on score estimation, works on inferential analysis are greatly outnumbered in choice model studies, especially for assortment optimization. In this paper, we aim to bridge the gap by proposing a general inferential procedure that quantifies the uncertainty in assortment optimization. Paper Organization. The rest of the paper is organized as follows. In Section 2, we\nprovide the preliminary setup for optimal assortment inference under the multinomial logit (MNL) choice model, and we introduce some inference-related concepts to facilitate follow-up discussions. In Section 3, we propose our general inferential procedure for testing combinatorial properties of the optimal assortment, which is based upon a debiased likelihood estimator. We also provide a theoretical guarantee for the convergence rate of the estimator and show the validity of the inferential procedure. In Section 4, we perform numerical experiments on simulated data to evaluate the performance of our method on different hypothesis testing examples, followed by brief discussions in Section 5. Notations. We denote by | \u00b7 | the cardinality of a set. For two positive sequences xn and yn, we denote xn . yn or xn = O(yn) if there exists a positive constant C > 0 independent of n such that xn \u2264 Cyn for all n sufficiently large. We say xn yn if xn . yn and yn . xn. If limn\u2192\u221e xn/yn = 0, then we say xn = o(yn). For two integers j > i \u2265 1, denote by [i] the set {1, 2, . . . , i}, by i : j the set {i, i + 1, . . . , j}, and by : the\nfull index set. For a vector v = (v1, . . . , vd) >, we use \u2016v\u2016q := (\u2211d i=1 |vi|q )1/q to denote the vector `q-norm for an integer q \u2265 1, and \u2016v\u2016\u221e := limq\u2192\u221e \u2016v\u2016q = maxi |vi| to denote the vector `\u221e-norm. For a matrix A = [Aij], we use \u2016A\u20162 to denote the matrix spectral norm, \u2016A\u2016\u221e := sup\u2016x\u2016\u221e=1 \u2016Ax\u2016\u221e = maxi \u2211 j |Aij| to denote the matrix `\u221e-norm, and \u2016A\u20162,\u221e := sup\u2016x\u20162=1 \u2016Ax\u2016\u221e = maxi \u2016Ai\u20162 to denote the 2-to-\u221e norm. We denote by 1d an d-dimensional vector with all entries equal to 1, and we omit the subscript when the dimension is clear from the context. We let {ei}di=1 be the canonical basis for Rd, where the dimension d might change from place to place. Throughout the paper, we use c, C to represent generic constants, whose values might change in different contexts."
        },
        {
            "heading": "2 Problem Setup",
            "text": "We provide the problem setup by briefly reviewing the assortment optimization problem under the multinomial logit model, followed by inferential analyses on the optimal assortment. Finally, we define the property sets corresponding to the optimal offer set to facilitate our discussions."
        },
        {
            "heading": "2.1 Multinomial Logit Choice Model",
            "text": "We consider the multinomial logit (MNL) model, in which the marginal probability for choosing one product of the offered assortment is proportional to the underlying preference scores associated with each product. More specifically, index by [n] = {1, . . . , n} the n products and by 0 the no-purchase alternative. Define [n]+ = [n] \u222a {0}. With each item i \u2208 [n]+, we assign a preference score u\u2217i > 0 and denote by u\u2217 = (u\u22170, u\u22171, . . . , u\u2217n)> the preference score vector. We let \u03b8\u2217 = (\u03b8\u22170, \u03b8 \u2217 1, . . . , \u03b8 \u2217 n) > be the log-transformation of u\u2217, where \u03b8\u2217i = log(u \u2217 i ) for i \u2208 [n]+. We set \u03b8\u22170 = 0 to ensure identifiability since the model is invariant\nup to the constant shifting of \u03b8\u2217i \u2019s. We let the condition number of the preference scores be\n\u03ba\u03b8 = maxi\u2208[n]+ u\n\u2217 i\nmini\u2208[n]+ u \u2217 i\n= maxi\u2208[n]+ e\n\u03b8\u2217i\nmini\u2208[n]+ e \u03b8\u2217i ,\nand we consider the scenario where \u03ba\u03b8 = O(1). We denote by S \u2286 [n] the offered assortment, i.e., the available set of products, and define S+ = S \u222a {0}. Under the MNL model, the probability of choosing item j \u2208 S+ is\nP(j | S+) = u\u2217j 1 + \u2211\ni\u2208S u \u2217 i\n= exp(\u03b8\u2217j ) 1 + \u2211 i\u2208S exp(\u03b8 \u2217 i ) , where \u03b8\u22170 = 0. (2.1)\nEach item i \u2208 [n]+ is associated with a corresponding revenue parameter ri, where r0 = 0 for the no-purchase alternative. Without loss of generality, assume that we label the products such that r1 \u2265 r2 \u2265 . . . \u2265 rn. We introduce the condition number for the revenues as\n\u03bar = r1/rn.\nIntuitively, a larger condition number indicates a less uniform distribution of revenues. Given an offered assortment S \u2286 [n], the total expected revenue is defined as\nr(S) = \u2211 i\u2208S u\u2217i ri 1 + \u2211 j\u2208S u \u2217 j . (2.2)\nWe are interested in finding an optimal assortment that maximizes the total expected revenue that\nS \u2032 \u2208 argmaxS\u2286[n] r(S). (2.3)\nSince there may not be a unique optimal assortment, we focus on the smallest optimal assortment, i.e., the optimal assortment with the smallest cardinality, which we denote by S\u2217.\nUnder the MNL model, the following theorem shows that the smallest optimal assortment S\u2217 can be efficiently recovered at a low computational cost.\nTheorem 2.1. [Talluri and Van Ryzin (2004)] Under the MNL model, the smallest optimal assortment S\u2217 is of the form S\u2217 = {1, 2, . . . , K\u2217} for some K\u2217 \u2208 [n]. Moreover, the following algorithm outputs the smallest optimal assortment S\u2217:\n(i) For k \u2208 [n], calculate \u2206k = \u2211k i=1 riu \u2217 i \u2212 ( \u2211k i=0 u \u2217 i )rk. If \u2206k < 0, then k \u2208 S\u2217, and move\non to k = k + 1.\n(ii) If \u2206k \u2265 0 or k > n, stop and output S\u2217 = [k \u2212 1].\nNote that the recovery of S\u2217 in Theorem 2.1 requires the knowledge of u\u2217i \u2019s, which are not known to merchandisers in practice. We will discuss in Section 3 how to obtain S\u2217 when u\u2217i \u2019s are unknown. With the concrete form of S\u2217 in place, we are now ready to introduce the inference problem on S\u2217."
        },
        {
            "heading": "2.2 Inference on the Optimal Offer Set",
            "text": "Recall that we aim to conduct inference on the smallest optimal offer set S\u2217. In particular, we aim to perform general hypothesis testings as in (1.1). By Theorem 2.1, we see that under the MNL model the optimal set S\u2217 takes the form [K\u2217], where K\u2217 \u2208 [n] is an integer. In other words, to optimize the total expected revenue, we will choose the products with the highest revenues while taking into account the customers\u2019 chances of buying the products. Since knowing S\u2217 and knowing K\u2217 are equivalent, we can see that the hypothesis testing in (1.1) in essence translates into testing whether set K\u2217 satisfies certain properties, e.g., Example 3 in Section 1 tests whether all the first K\u2217 products belong to a certain set A. Then we can reformulate the testing problem in (1.1) as that, for a given set K0 \u2286 [n], we test whether K\u2217 belongs to K0,\nH0 : K \u2217 \u2208 K0 versus H1 : K\u2217 /\u2208 K0. (2.4)\nWe call K0 the property set of K\u2217. We formally introduce below the applications provided in Section 1 along with other examples and define the corresponding property set K0 as concrete formulations of (2.4).\nExample 1. For a given product i \u2208 [n], we test if i is in the optimal assortment S\u2217, i.e.,\nH0 : i 6\u2208 S\u2217 versus H1 : i \u2208 S\u2217.\nThe corresponding property set is K0 = {k \u2208 [n] : k \u2264 i\u2212 1}.\nExample 2. For a given set A \u2286 [n], we test if A is a subset of the optimal assortment S\u2217, i.e.,\nH0 : A * S\u2217 versus H1 : A \u2286 S\u2217.\nThe corresponding property set is K0 = {k \u2208 [n] : k \u2264 (maxi\u2208A i)\u2212 1}.\nExample 3. For a given set A \u2286 [n], we test if the optimal assortment S\u2217 is a subset of A, i.e.,\nH0 : S\u2217 \u2286 A versus H1 : S\u2217 * A.\nThe corresponding property set is K0 = {k \u2208 [n] : \u2200i \u2264 k, i \u2208 A}.\nExample 4. For a given set A \u2286 [n] and a given ratio q%, we test if the proportion of S\u2217 contained in A is larger than q%, i.e.,\nH0 : |A \u2229 S\u2217|/|S\u2217| > q% versus H1 : |A \u2229 S\u2217|/|S\u2217| \u2264 q%.\nThe corresponding property set is K0 = {k \u2208 [n] : |[k] \u2229 A|/k > q%}. Example 5. For a partition {A1, . . . , Am} of the products, i.e., \u22c3m j=1Aj = [n] and Aj \u2229Ak = \u2205 for j 6= k, we test if the products in A1 constitute the largest proportion of the optimal\nassortment S\u2217 in comparison with all the other Aj\u2019s, i.e.,\nH0 : |S\u2217 \u2229 A1| = max j |S\u2217 \u2229 Aj| versus H1 : |S\u2217 \u2229 A1| < max j |S\u2217 \u2229 Aj|.\nThe corresponding property set is K0 = {k \u2208 [n] : |[k] \u2229 A1| = maxj |[k] \u2229 Aj|}. Example 6. For a partition {A1, . . . , Am} of the products, i.e., \u22c3m j=1Aj = [n] and Aj \u2229Ak = \u2205 for j 6= k, we test if no less than n0 out of the m subsets Aj\u2019s contain products that constitute the optimal assortment S\u2217, where n0 is a given count, i.e.,\nH0 : |{j \u2208 [m] : Aj \u2229 S\u2217 6= \u2205}| \u2265 n0, H1 : |{j \u2208 [m] : Aj \u2229 S\u2217 6= \u2205}| < n0.\nThe corresponding property set is K0 = {k \u2208 [n] : |{j \u2208 [m]: mini\u2208Aj i \u2264 k}| \u2265 n0}.\nWe can see that the hypothesis testing on S\u2217 is in essence testing whether the K\u2217 satisfies the properties of interest. In practice, the parameter \u03b8\u2217 is unknown, and we will approximate \u03b8\u2217 by its estimator \u03b8\u0302 obtained from the observed data and estimate u\u2217 by the plug-in u\u0302 = exp(\u03b8\u0302). Then we can estimate S\u2217 by replacing \u2206k with \u2206\u0302k = \u2211k i=1 riu\u0302i\u2212 ( \u2211k i=0 u\u0302i)rk in the algorithm proposed in Theorem 2.1. Besides, we can see from Theorem 2.1 that k \u2208 [n] belongs to S\u2217 if and only if \u2206k < 0, and thus we can construct a confidence interval [K\u0302L, K\u0302U ] of confidence level 1\u2212\u03b1 for K\u2217 using {\u2206\u0302k}k\u2208[n]. Then we will reject H0 if [K\u0302L, K\u0302U ]\u2229K0 = \u2205. We will discuss the method in more detail in the following section."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we propose the algorithm for conducting general inferences on the optimal assortment S\u2217. We begin by introducing how to estimate the latent preference scores from the observed data via a penalized optimization, and then provide a debiasing approach for the resulting estimator. Then we propose the inferential procedure based upon multiplier bootstrap to test arbitrary properties on the optimal offer set S\u2217."
        },
        {
            "heading": "3.1 Observed Data and Latent Score Estimation",
            "text": "We first discuss the mechanism for collecting the observed data. In reality, the underlying preference scores u\u2217i \u2019s are unknown, and what merchandisers observe are the customers\u2019 choice results among the offered assortments. Since there is an exponential number of possible assortments in total, in practice we may only observe the choice results for a subset of offer sets sampled from all the possible assortments. The goal is to estimate the latent preference scores from the observed data, and apply the estimators to the subsequent inference on S\u2217.\nMore specifically, recall that S is the set of all offer sets. We sample the observed offer sets from S uniformly with probability p. More specifically, for each S \u2208 S, we define ES = 1 if the offer set S is selected and ES = 0 otherwise, and we assume that ES follows a Bernoulli\ndistribution of probability p. We denote by E = {S \u2208 S : ES = 1} the set of selected offer sets. For each selected offer set S, we observe the choice results among the items in S+ for L times independently. Here we assume the sample size L to be the same for all selected sets for the simplicity of presentation. With some technical modifications, the theoretical analysis can be generalized to settings where the sample size LS is different up to a constant factor for different sets S \u2208 S. We denote by x(i,`)S the choice outcome of the `-th customer for item i in S+, and under the MNL model, {x(i,`)S }i\u2208S+ follow the following multinomial distribution\nx (i,`) S =\n{ 1, with probability\nu\u2217i 1+ \u2211 j\u2208S u \u2217 j = e \u03b8\u2217i 1+ \u2211 j\u2208S e \u03b8\u2217 j\n0, otherwise , \u03b8\u22170 = 0, for S \u2208 S. (3.1)\nThen the negative log-likelihood function can be written as\nL(\u03b8;x) = \u2212 \u2211 S\u2208E {\u2211 i\u2208S x (i) S \u03b8i \u2212 log ( 1 + \u2211 i\u2208S e\u03b8i )} , (3.2)\nwhere x (i) S = 1 L \u2211L `=1 x (i,`) S . Let x = {x (i,`) S } . We denote by \u03b8\u0302 the penalized maximum likelihood estimate (MLE), which solves the following convex problem\nmin \u03b8\u2208Rn+1,\u03b80=0\nL\u03bb(\u03b8;x) := L(\u03b8;x) + \u03bb\n2 \u2211 i\u2208[n]+ (\u03b8i \u2212 \u03b8\u0304)2, (3.3)\nwhere \u03b8\u0304 = \u2211\ni\u2208[n]+ \u03b8i/(n + 1), and \u03bb > 0 is a tuning parameter. Here we regularize over the sample variance of \u03b8i\u2019s rather than the `2-norm of \u03b8 because the regularization over the sample variance of \u03b8i\u2019s will prevent \u03b8i\u2019s from getting too far away from their mean so as to exclude ill-conditioned \u03b8. We characterize the convergence rate of \u03b8\u0302 in the following theorem.\nTheorem 3.1. Recall that \u03b8\u0302 is the regularized MLE. Under the conditions that \u03bb \u221a\n2np logn nL , 2np \u2265 Cn log n for some large enough constant C > 0 and n \u221a\nlog n/(2npL) \u2264 c for some small enough constant c > 0 , with probability at least 1\u2212O(n\u221210) we have that\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 . n\n\u221a log n\n2npL . (3.4)\nWe refer interested readers to Supplementary Materials A for the proof of Theorem 3.1. For each item i \u2208 [n]+, the observed number of purchases in each observation ` \u2208 [L] is\napproximately 2np/n in expectation, i.e., E (\u2211 S\u2208E:i\u2208S x (i,`) S ) 2np/n, which is the amount of valid information for item i for each observation. Theorem 3.1 indicates that we need to observe more than log n purchases of each item in each observation to obtain consistent estimation of \u03b8. Besides, the total number of observed purchases across all L observations for each item i \u2208 [n] needs to exceed n log n, i.e., 2npL/n & n log n. If one can sample more offer sets, the scaling condition on L will be weaker."
        },
        {
            "heading": "3.2 Newton Debiasing with Centralization",
            "text": "Recall that \u03b8\u0302 is the regularized MLE solving the problem (3.3), and we have the gradient and Hessian of L(\u03b8;x) as\n\u2207L(\u03b8;x) = \u2212 \u2211 S\u2208E \u2211 i\u2208S+ ( x (i) S \u2212 e\u03b8i\u2211 j\u2208S+ e \u03b8j ) ei  ; (3.5)\n\u22072L(\u03b8;x) = \u2211 S\u2208E  \u2211 i\u2208S+ e\u03b8i( \u2211 j\u2208S+,j 6=i e \u03b8j)(\u2211\nj\u2208S+ e \u03b8j )2 eie>i \u2212 \u2211 i,s\u2208S+ i 6=s e\u03b8ie\u03b8s(\u2211 j\u2208S+ e \u03b8j )2eie>s  . (3.6)\nSince the regularized estimation induces bias in the estimator, to perform inferential analysis based on \u03b8\u0302, we need to correct for the bias first. We debias the regularized MLE \u03b8\u0302 by a one-step Newton correction with centralization that\n\u03b8\u0302 d = \u03b8\u0302 \u2032 \u2212\u22072L(\u03b8\u0302;x)\u2020\u2207L(\u03b8\u0302;x), (3.7)\nwhere \u22072L(\u03b8\u0302;x)\u2020 is the Moore-Penrose inverse of the Hessian evaluated at \u03b8\u0302 and \u03b8\u0302 \u2032 is the centralized MLE. Namely, \u03b8\u0302\u2032i = \u03b8\u0302i \u2212 (n+ 1)\u22121 \u2211\nj\u2208[n]+ \u03b8\u0302j for i \u2208 [n]+. Here we apply centralization to \u03b8\u0302 because by (3.6), we have \u22072L(\u03b8;x)\u20201n+1 = 0 for any \u03b8 \u2208 Rn+1, which indicates that due to the singularity of the Hessian matrix, the Newton correction will only correct bias on the direction perpendicular to 1. Hence to eliminate the bias on the direction of 1, we will first project \u03b8\u0302 onto the perpendicular space of 1, i.e., apply centralization to \u03b8\u0302. Besides, from (3.5) and (3.6) we can observe that \u2207L(\u03b8;x) and \u22072L(\u03b8;x) are invariant after adding a constant to \u03b8, and hence the projection of \u03b8\u0302 does not change the likelihood evaluation.\nWith the debiased estimator in place, we are now ready to present the inferential procedures. Recall from Section 2.2 that under the MNL model, the inference on the optimal offer set S\u2217 is equivalent to the inference on its cardinality K\u2217. Furthermore, a product k \u2208 [n] is in S\u2217 if and only if \u2206k < 0, where \u2206k is defined in Theorem 2.1. Thus, we see that \u2206k\u2019s serve as pivotal intermediate quantities in the inference of S\u2217, and to provide uncertainty quantification for S\u2217, we first provide uncertainty quantification for \u2206k\u2019s.\nMore specifically, for k \u2208 [n], we define \u2206\u0302k = \u2211k i=1 ri exp(\u03b8\u0302 d i )\u2212 (\u2211k i=0 exp(\u03b8\u0302 d i ) ) rk, which is the estimator for \u2206k by plugging in the debiased estimator \u03b8\u0302 d . Note that the centralization of \u03b8\u0302 when calculating \u03b8\u0302 d\nwill scale the plug-in estimators \u2206\u0302k\u2019s by a positive factor. However, since we are only interested in the signs of \u2206\u0302k\u2019s, the scaling of \u2206\u0302k\u2019s bears no impact on the\ninference. Hence with a slight abuse of notation, we redefine \u2206k = k\u2211 i=1 ri exp(\u03b8 \u2217 i\u2212\u03b8\u0304\u2217)\u2212 ( k\u2211 i=0 exp(\u03b8\u2217i \u2212 \u03b8\u0304\u2217) ) rk = exp(\u2212\u03b8\u0304\u2217) ( k\u2211 i=1 riu \u2217 i \u2212 ( k\u2211 i=0 u\u2217i )rk ) , k \u2208 [n],\nwhere \u03b8\u0304\u2217 = (n+ 1)\u22121 \u2211\nj\u2208[n]+ \u03b8 \u2217 j . The following theorem depicts the asymptotic distribution\nof \u2206\u0302k\u2019s. Theorem 3.2. Under the same conditions as in Theorem 3.1, suppose that \u03bar = r1/rn . \u221a n\nand n2 log n/ \u221a\n2npL = o(1). Then, for all k \u2208 [n], we have that\u221a L\nv>k\u22072L(\u03b8 \u2217;x)\u2020vk\n(\u2206\u0302k \u2212\u2206k) d\u2212\u2192 N(0, 1), (3.8)\nwhere vk = exp(\u2212\u03b8\u0304\u2217) \u00b7 ( (0\u2212 rk)u\u22170, (r1 \u2212 rk)u\u22171, (r2 \u2212 rk)u\u22172, . . . , (rk\u22121 \u2212 rk)u\u2217k\u22121, 0, . . . , 0 )> .\nThe proof of Theorem 3.2 is deferred to Supplementary Materials B. Compared with Theorem 3.1, we have an extra scaling condition on the condition number \u03bar to ensure that the revenue parameters are not ill-conditioned. Besides, we have a stronger scaling condition on the sample size L to guarantee distributional convergence. Since \u03b8\u2217 is unknown in practice, we estimate the asymptotic variance by plugging in the regularized MLE \u03b8\u0302. The following corollary of Theorem 3.2 shows the validity of the plug-in estimate.\nCorollary 3.3. Under the same conditions as Theorem 3.2, we have\u221a L\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k (\u2206\u0302k \u2212\u2206k)\nd\u2212\u2192 N(0, 1), (3.9)\nwhere v\u0302k = ( (0\u2212 rk)u\u03020, (r1 \u2212 rk)u\u03021, (r2 \u2212 rk)u\u03022, . . . , (rk\u22121 \u2212 rk)u\u0302k\u22121, 0, . . . , 0 )> with u\u0302i =\nexp(\u03b8\u0302\u2032i).\nSee Supplementary Materials C for the proof of Corollary 3.3. Based upon the asymptotic normality of \u2206k\u2019s, we are ready to formally introduce the inferential procedure in the next section."
        },
        {
            "heading": "3.3 Hypothesis Testing for S\u2217",
            "text": "In this section, we will perform a general inference on S\u2217 to test whether it satisfies certain properties of interest. As established in Section 2.2, the general hypothesis testing for S\u2217 defined in (1.1) is equivalent to the following test on K\u2217,\nH0 : K \u2217 \u2208 K0 versus H1 : K\u2217 /\u2208 K0,\nwhere K0 is the property set of K\u2217 satisfying certain properties. Then to perform the inference on K\u2217, for a given level \u03b1 \u2208 (0, 1), we utilize the equivalence between the events {k \u2208 S\u2217} and {\u2206k < 0}, and construct a confidence interval of confidence level 1\u2212 \u03b1 for K\u2217. Specifically, we define the maximal statistic\nT = max k\u2208[n]\n( \u2206\u0302k \u2212\u2206k )/\u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L, (3.10)\nand we construct the confidence interval of K\u2217 by estimating the quantile of T . We consider the Gaussian multiplier bootstrap proposed in Chernozhukov et al. (2013) for estimating the quantile of T , which approximates the maximum of a sum of random vectors by the empirical quantiles of Gaussian maximum. In particular, we define the following statistic as the approximation of T ,\nW = max k\u2208[n] L\u2211 `=1  v\u0302>k\u22072L(\u03b8\u0302;x)\u2020\u221a L \u00b7 v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2211 S\u2208I ES {\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j ) ei } zS,`  , (3.11) where {zS,`}L`=1 are i.i.d. standard Gaussian, and ES = 1 if S is selected for the observed data and ES = 0 otherwise. For a given level \u03b1 \u2208 (0, 1), we define the quantile of W conditional on the data x and the selected sets E as\ncW (\u03b1,E) = inf{t \u2208 R : P(W > t |x,E) \u2264 \u03b1}. (3.12)\nWe will show that W approximates T , and cW (\u03b1,E) approximates T \u2019s \u03b1-quantile under some mild conditions. Then we can establish a (1\u2212 \u03b1)-level confidence interval for K\u2217: [K\u0302L, K\u0302U ], where\nK\u0302L = max { k : \u2206\u0302k < \u2212cW ( \u03b1\n2 ,E)\n\u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k\nL\n} ,\nK\u0302U = max { k : \u2206\u0302k \u2264 cW ( \u03b1\n2 ,E)\n\u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k\nL\n} .\nWe summarize the framework of constructing the confidence interval in Algorithm 1. Define the parameter space of \u03b8\u2217 as \u0398 = {\u03b8 : \u03ba\u03b8 \u2264 C\u03ba}, where C\u03ba is a constant independent of n. Then the following theorem shows the validity of the testing procedure based on the confidence interval for K\u2217. Theorem 3.4. Under the same conditions as Theorem 3.1, suppose \u03bar . \u221a n, and n2 \u221a (log n)3/(2npL) = o(1). Then, for any level 0 < \u03b1 < 1, we have that\nlim inf n,L\u2192\u221e inf \u03b8\u2217\u2208\u0398\nP\u03b8\u2217(K\u2217 \u2208 [K\u0302L, K\u0302U ]) \u2265 1\u2212 \u03b1. (3.13)\nMoreover, lim sup n,L\u2192\u221e sup \u03b8\u2217:S\u2217\u2208S0 P\u03b8\u2217(Reject H0) \u2264 \u03b1. (3.14)\nWe give below a proof sketch of Theorem 3.4 to facilitate understanding. The detailed proof is provided in Section E.\nProof. Proof Sketch. The proof contains three steps. First, we reduce quantifying the uncertainty of S\u2217 to quantifying the uncertainty of the sign change point detection based on the maximal scaled estimation error T . Second, we show that the multiplier bootstrap quantile cW (\u03b1,E) is valid and in turn, the confidence interval is valid. Third, based on the equivalence between the two hypothesis tests (1.1) and (2.4), we establish the asymptotic validity of our inferential procedure.\nStep 1. By previous arguments, we establish the equivalence of the following events:\n{k \u2208 S\u2217} = {k \u2264 K\u2217} = {\u2206k < 0},\nwhich indicates that making inference on S\u2217 is equivalent to inferring the signs of \u2206k\u2019s. Moreover, we have the following lower bound for the coverage probability\nP(K\u2217 \u2208 [K\u0302L, K\u0302U ]) \u2265 1\u2212P(K\u0302L>K\u2217)\u2212P(K\u0302U+1\u2264K\u2217) \u2265 1\u2212P(\u2206K\u0302L\u22650)\u2212 P ( \u2206K\u0302U+1<0 ) .\nBy our definition of K\u0302L and K\u0302U , the estimators of \u2206K\u0302L and \u2206K\u0302U+1 failing to predict the signs means that the estimation errors of both \u2206\u0302K\u0302L and \u2206\u0302K\u0302U+1 exceed the cut-off threshold, i.e.,\n\u2206\u0302K\u0302L \u2212\u2206K\u0302L < \u2212cW ( \u03b1 2 ,E)\n\u221a v\u0302> K\u0302L \u22072L(\u03b8\u0302;x)\u2020v\u0302K\u0302L\nL ,\n\u2206\u0302K\u0302U+1 \u2212\u2206K\u0302U+1 > cW ( \u03b1 2 ,E)\n\u221a v\u0302> K\u0302U+1\n\u22072L(\u03b8\u0302;x)\u2020v\u0302K\u0302U+1 L .\nSince the statistic T controls the difference (\u2206\u0302k \u2212 \u2206k)\u2019s uniformly, we can further lower bound the coverage probability by\nP(K\u2217 \u2208 [K\u0302L, K\u0302U ]) \u2265 1\u2212 P ( \u2212 T < \u2212cW (\u03b1/2,E) ) \u2212 P ( T > cW (\u03b1/2,E) ) .\nNamely, so long as we can accurately estimate the quantile of T , we will have a valid confidence interval for K\u2217.\nStep 2. The following lemma shows that the quantile cW (\u03b1,E) obtained from the multiplier bootstrap in Algorithm 1 serves as a valid quantile of T for any \u03b8\u2217 \u2208 \u0398.\nLemma 3.5. Under the same conditions as Theorem 3.4, we have that\nsup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1)\n|P\u03b8\u2217 (T > cW (\u03b1,E))\u2212 \u03b1| = o(1).\nThe proof of Lemma 3.4 is deferred to Supplementary Materials D. Then from Step 1, we have that\nsup \u03b8\u2217\u2208\u0398 P\u03b8\u2217(K\u0302L \u2264 K\u2217 \u2264 K\u0302U) \u2265 1\u2212 sup \u03b8\u2217\u2208\u0398 P\u03b8\u2217(\u2212T < \u2212cW (\u03b1/2,E))\u2212 sup \u03b8\u2217\u2208\u0398 P\u03b8\u2217(T \u2265 cW (\u03b1/2,E))\n\u2192 1\u2212 \u03b1/2\u2212 \u03b1/2 = 1\u2212 \u03b1.\nHence, (3.13) holds, and [K\u0302L, K\u0302U ] is a valid confidence interval.\nStep 3. Recall that we reject the null if and only if [K\u0302L, K\u0302U ] and K0 have no intersection. Then by the equivalence between testing S\u2217 \u2208 S0 and testing K\u2217 \u2208 K0, for large enough n and L we have that\nsup \u03b8\u2217:S\u2217\u2208S0 P\u03b8\u2217(Reject H0) = sup \u03b8\u2217:K\u2217\u2208K0 P\u03b8\u2217([K\u0302L, K\u0302U ]\u2229K0 = \u2205) \u2264 1\u2212 inf \u03b8\u2217\u2208\u0398 P\u03b8\u2217(K\u2217 \u2208 [K\u0302L, K\u0302U ]) \u2264 \u03b1,\nsuggesting that our test is asymptotically valid.\nThus, for the hypothesis test on the smallest optimal set S\u2217, we first construct the property set K0 of K\u2217 corresponding to the null hypothesis H0 as described in Section 2.2. Then, for a given level \u03b1 \u2208 (0, 1), we construct the confidence interval [K\u0302L, K\u0302U ] by Algorithm 1 and reject H0 if and only if [K\u0302L, K\u0302U ] \u2229 K0 = \u2205.\nAlgorithm 1 Construction of confidence interval for K\u2217\nInput: selected sets E \u2286 I, customer choice data {x(i,`)S }S\u2208E,`\u2208[L], regularization parameters \u03bb0, \u03bb1, revenue parameters {ri}i\u2208[n], given level \u03b1. Output: Regularized MLE \u03b8\u0302, (1\u2212 \u03b1)-level [K\u0302L, K\u0302U ].\n1: Compute the regularized MLE \u03b8\u0302 \u2190 argmin\u03b8\u2208Rn+1,\u03b80=0 L(\u03b8;x) + \u03bb 2 \u2211 i\u2208[n]+(\u03b8i \u2212 \u03b8\u0304) 2; 2: Obtain the debiased MLE \u03b8\u0302 d by (3.7); 3: Generate zS,` i.i.d.\u223c N(0, 1) for S \u2208 E , ` \u2208 [L], 4: for k \u2208 [n] do\n5: wk \u2190 \u2211L\n`=1\n{ v\u0302>k \u2207\n2L(\u03b8\u0302;x)\u2020\u221a Lv\u0302>k \u22072L(\u03b8\u0302;x)\u2020v\u0302k\n\u2211 S\u2208I ES {\u2211 i\u2208S ( x (i,`) S \u2212 e\n\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j\n) ei } zS,` } ;\n6: end for 7: W \u2190 maxk\u2208[n] wk, compute the 1\u2212 \u03b1/2 quantile of W : cW (\u03b1/2,E); 8: Compute the confidence interval of level 1\u2212 \u03b1, [K\u0302L, K\u0302U ]:\nK\u0302L = max { k : \u2206\u0302k < \u2212cW (\u03b1/2,E) \u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L } ,\nK\u0302U = max { k : \u2206\u0302k \u2264 cW (\u03b1/2,E) \u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L } .\nNote that given the specific hypothesis, the testing procedure can be simplified by taking advantage of the structure of the property set K0. For instance, Example 1 essentially boils down to testing H0 : \u2206i \u2265 0. As shown by Corollary 3.3 in Section 3.2, under certain conditions, for any i \u2208 [n], ( v\u0302>i \u22072L(\u03b8\u0302;x)\u2020v\u0302i/L )\u22121/2 (\u2206\u0302i \u2212\u2206i) converges in distribution to the standard Gaussian. Thus for a given level \u03b1 \u2208 (0, 1), we can construct the confidence interval for \u2206i as \u2206\u0302i \u2212 \u03a6\u22121(1\u2212 \u03b1\n2 )\n\u221a v\u0302>i \u22072L(\u03b8\u0302;x)\u2020v\u0302i\nL , \u2206\u0302i + \u03a6 \u22121(1\u2212 \u03b1 2 )\n\u221a v\u0302>i \u22072L(\u03b8\u0302;x)\u2020v\u0302i\nL  , i \u2208 [n]. Then for Example 1, we will reject the null hypothesis if and only if\n\u2206\u0302i + \u03a6 \u22121(1\u2212 \u03b1\n2 )\n\u221a v\u0302>i \u22072L(\u03b8\u0302;x)\u2020v\u0302i/L < 0,\nwhich is more efficient than Algorithm 1."
        },
        {
            "heading": "4 Numerical Results",
            "text": "In this section, we conduct simulation studies to evaluate the empirical performance of the proposed method (Alg. 1). We first evaluate the validity of the constructed confidence intervals by evaluating the empirical coverage probabilities, i.e., the empirical evaluation of P(K\u0302L \u2264 K\u2217 \u2264 K\u0302U). We then apply Algorithm 1 to Example 2 and Example 5 discussed in Section 2.2 as representative illustration of the method\u2019s performance. For each example, we provide the empirical evaluation of the Type I error, i.e., probability of rejecting the null when it is true, and the Power, i.e., probability of rejecting the null when it is false. Recall from Theorem 2.1 that the optimal assortment is of the form S\u2217 = [K\u2217], and we aim to conduct general inference on the properties of S\u2217, which is equivalent to testing the general null hypothesis H0 : K\n\u2217 \u2208 K0 in (2.4). Then the Type I error and the Power are defined by PK\u2217\u2208K0([K\u0302L, K\u0302U ] \u2229 K0 = \u2205) and PK\u2217 /\u2208K0([K\u0302L, K\u0302U ] \u2229 K0 = \u2205), respectively.\nNote that for the multiplier bootstrap in Algorithm 1, we generate 200 independent Gaussian samples to evaluate the quantile of W in (3.11). We generate the log preference score parameter \u03b8\u2217 \u2208 Rn by \u03b8\u2217i i.i.d.\u223c N(0, \u03c32\u03b8) for i \u2208 [n] and \u03b8\u22170 = 0. For each given K\u2217, we set \u2206k = \u22120.001 if k \u2264 K\u2217, \u2206k = 0 if k = K\u2217 + 1 and \u2206k = 0.001 if k > K\u2217 + 1, then we solve for the revenue parameters from \u2206k\u2019s through the relation-\nship \u2206k = exp(\u2212\u03b8\u0304\u2217) (\u2211k i=1 riu \u2217 i \u2212 ( \u2211k i=0 u \u2217 i )rk ) , k \u2208 [n]. Note that since rk \u2212 rk+1 =\nexp(\u03b8\u0304\u2217)( \u2211k\ni=0 u \u2217 i ) \u22121(\u2206k+1\u2212\u2206k), the corresponding revenue parameters satisfy that r1 \u2265 r2 \u2265\n. . . \u2265 rn. For the selection of observed comparison sets, we set the selection probability at p = n log n/2n. We set the penalty parameter \u03bb = c \u221a 2np log n/(nL) according to Theorem 3.1, where c is a tuning constant. For all settings, we fix n = 30 and \u03c32\u03b8 = 3. We set the customer sample size L at different\nvalues to evaluate how the performance of Algorithm 1 changes over different sample sizes. We summarize the results of 300 Monte Carlo simulations in the following sections."
        },
        {
            "heading": "4.1 Asymptotic Normality of Debiased Estimators",
            "text": "Before evaluating the performance of Algorithm 1, we demonstrate in this section the asymptotic normality of the debiased MLE \u03b8\u0302d and the plug-in estimator \u2206\u0302k. We select the tuning constant c = 0.25 for the penalty parameter \u03bb via cross-validation, and set K\u2217 = 7. We set the customer sample size at L = 1000. Figure 1 provides the Q-Q plots of \u03b8\u0302d1 and \u2206\u03021. We center \u03b8\u0302d1 by \u03b8 \u2217 1 \u2212 \u03b8\u0304\u2217 and scale \u03b8\u0302d1 by \u221a e>2\u22072L(\u03b8\u0302;x)\u2020e2/L, and we center \u2206\u03021 by \u22061 and\nscale \u2206\u03021 by \u221a v\u0302>1 \u22072L(\u03b8\u0302;x)\u2020v\u03021/L according to Corollary 3.3. We see that both \u03b8\u0302d1 and \u2206\u03021 are approximately normally distributed.\n(a) Q-Q plot for \u03b8\u0302d1 (b) Q-Q plot for \u2206\u03021"
        },
        {
            "heading": "4.2 Performance of Algorithm 1 on Hypotheses Testing",
            "text": "We apply Algorithm 1 to concrete examples provided in Section 2.2. We select the tuning constant of \u03bb through cross-validation by letting c = 1 for constructing the confidence intervals. We first evaluate the empirical coverage probability of the confidence interval [K\u0302L, K\u0302U ]. We set the nominal level at 0.95, and set the sample size at L \u2208 {5, 50, 100, 150, 250, 300} respectively. Figure 2 shows the empirical coverage probability at K\u2217 \u2208 {7, 8, 9, 10, 11, 12} for each setting of L. We can see that the empirical coverage probability of [K\u0302L, K\u0302U ] converges to the nominal level of 0.95 as L increases.\nThen we provide two concrete examples. To characterize the distance of the alternative from the null, for a given set S \u2286 [n], we define d(K\u2217, S) = mini\u2208S |i\u2212K\u2217|. Then for K\u2217 /\u2208 K0,\n1.00\nd(K\u2217,K0) characterizes the difficulty of differentiating H1 from H0. We summarize the results of Example 2 and Example 5 below.\nExample 2\nRecall that we are interested in testing whether a given set A \u2286 [n] is a subset of the optimal set S\u2217. In particular,\nH0 : A * S\u2217 versus H1 : A \u2286 S\u2217.\nWe set A = {2, 4, 6, 8} and generate K\u2217 \u2208 {7, 8, 9, 10}. It can be seen that K0 = [7]. We set the null at K\u2217 = 7 and choose different alternatives to evaluate the empirical Type I error and Power. The results are summarized in Figure 3. We see that the Type I error converges to the nominal 0.05 level as L increases, while the Power increases to 1 as L grows for all settings of d(K\u2217,K0).\nExample 5 Recall that for a partition {A1, . . . , Am} of the products, i.e., \u22c3m j=1 Aj = [n] and Aj \u2229Ak = \u2205 for j 6= k, we are interested in testing if A1 contains the most elements in S\u2217 in comparison\n(a) Type I error (b) Power\n0.00\n0.05\n0.10\n0.15\n0.20\n0 100 200 300 L\nTy pe\nI E\nrr or\n0.25\n0.50\n0.75\n1.00\n10 30 100 300 L\nPo w er\nlb\n1 2\n3d(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n30.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nlb\n1 2\n3d(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n3 0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nlb\n1 2 3\nPo w er Po w er\nd(K*, !0)\n0.25\n0.50 0.75 1.00\n0 50 100 150 200 L\nPo w er\nlb 1 2\n3\n0.4\n0.6 0.8 1.0\n3 10 30 100 300 L\nPo w er\nl 3 Po w er\nd(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n3\nPo w er\nFigure 3: Type I error and Power for Example 2 under different L and d(K\u2217,K0), where the dashed line in (a) represents the nominal 0.05 level.\nwith the other Aj\u2019s. In particular, we are interested in testing the hypothesis\nH0 : |S\u2217 \u2229 A1| = max j |S\u2217 \u2229 Aj| versus H1 : |S\u2217 \u2229 A1| < max j |S\u2217 \u2229 Aj|.\nFor the simulation, we partition [n] = [30] by A1 = [6 : 8] \u222a [14 : 15] \u222a [21 : 25], A2 = [1 : 3] \u222a {10, 12} \u222a [16 : 20] and A3 = [4 : 5] \u222a {9, 11, 13} \u222a [26 : 30], and the corresponding K0 is [8 : 9]\u222a {15} \u222a [25 : 30]. We generate K\u2217 \u2208 {9, 10, 11, 12} and set the null at K\u2217 = 9. Results are summarized in Figure 4. The Type I error converges to the nominal level of 0.05 as L increases, while the Power increases to 1 as L increases at d(K\u2217,K0) \u2208 {1, 2, 3}.\n(a) Type I error (b) Power\n0.00\n0.05\n0.10\n0.15\n0.20\n0 100 200 300 L\nTy pe\nI E\nrr or\n0.25\n0.50\n0.75\n1.00\n10 30 100 300 L\nPo w er\nlb\n1 2\n3d(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n30.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nlb\n1 2\n3d(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n3 0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nlb\n1 2 3\nPo w er Po w er\nd(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200 L\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300 L\nPo w er\nl\n3\nPo w er\nd(K*, !0)\n0.25\n0.50\n0.75\n1.00\n0 50 100 150 200\nL\nPo w er\nlb 1\n2 3\n0.4\n0.6\n0.8\n1.0\n3 10 30 100 300\nL\nPo w er\nl\n3\nPo w er\nFigure 4: Type I error and Power for Example under different L and d(K\u2217,K0), where the dashed line in (a) represents the nominal 0.05 level.\nIn summary, Algorithm 1 performs well for both examples, which shows the validity of our method."
        },
        {
            "heading": "5 Conclusion",
            "text": "To conclude, we propose a general inferential framework for testing combinatorial properties of the optimal assortment, which is the first paper making such efforts to the best of our knowledge. Under the MNL model, we first estimate the latent preference scores based on penalized likelihood optimization from customer choice data among selected offer sets. Then we apply a Newton debiasing correction to the estimator and perform the assortment optimization algorithm by plugging in the debiased latent score estimator. Finally, we implement the Gaussian multiplier bootstrap to construct confidence intervals for the optimal offer set and then perform hypothesis testing on a given property of interest upon the optimal assortment. We provide theoretical guarantees that our test is valid if the sample size is large enough under some mild conditions.\nFor future work, we plan to generalize the current results to other choice models, such as the capacitated MNL model with cardinality constraints, or the mixture of multinomial logit models (MMNL). We will also seek to develop a more computationally efficient and scalable method for constructing the confidence interval for K\u2217."
        },
        {
            "heading": "A Proof of Theorem 3.1",
            "text": "The proof basically modifies that of Theorem 6 in Chen et al. (2019b). Note that if we take \u03b8\u2032 = P\u22a51 \u03b8, then the convex problem (3.3) can be equivalently written as\nmin \u03b8\u2032\u2208Rn+1,1>\u03b8\u2032=0\nL\u03bb(\u03b8\u2032;x) := L(\u03b8\u2032;x) + \u03bb\n2 \u2016\u03b8\u2032\u201622. (A.1)\nLater we will show that the minimizer of L(\u03b8\u2032;x) + \u03bb 2 \u2016\u03b8\u2032\u201622 belongs to the subspace perpendicular to 1, and thus we can simplify (A.1) as\nmin \u03b8\u2208Rn+1\nL\u03bb(\u03b8;x) := L(\u03b8;x) + \u03bb\n2 \u2016\u03b8\u201622. (A.2)\nIt can be observed that the regularized MLE to (A.2) will be \u03b8\u0302 \u2032\n:= P\u22a51 \u03b8\u0302, where \u03b8\u0302 is the regularized MLE of problem (3.3). In the following proof, we will consider solving (A.2) instead. We will show the L2 convergence rate of the resulting MLE of (A.2), i.e., \u2016\u03b8\u0302 \u2032 \u2212 P\u22a51 \u03b8\u2217\u20162 = \u2016P\u22a51 \u03b8\u0302 \u2212 P\u22a51 \u03b8\u2217\u20162. Since \u03b8\u0302 \u2032 can be easily transformed back to \u03b8\u0302 by taking \u03b8\u0302i = \u03b8\u0302 \u2032 i\u2212 \u03b8\u0302\u20320 for i \u2208 [n]+, the L2 convergence rates \u2016\u03b8\u0302 \u2032 \u2212P\u22a51 \u03b8\u2217\u20162 and \u2016\u03b8\u0302\u2212\u03b8\u2217\u20162 will be different only by a factor of constant. Besides, since \u03b8\u2217 and P\u22a51 \u03b8 \u2217 essentially specify the same MNL model, without loss of generality, we will replace \u03b8\u2217 by P\u22a51 \u03b8 \u2217 throughout the proofs, and we will abuse the notation and let \u03b8\u2217 denote \u03b8\u2217\u2032 = P\u22a51 \u03b8 \u2217 and let \u03b8\u0302 denote \u03b8\u0302 \u2032 = P\u22a51 \u03b8\u0302, i.e., the resulting regularized MLE solving problem (A.2). Before delving into the details, we need the following lemmas to help with the proof.\nTo study the gradient and Hessian for the log-likelihood, we define the auxiliary matrix\nLE = n+1\u2211 k=2 \u2211 S\u2208Ek { 1 k2 \u2211 i,j\u2208S i<j (ei \u2212 ej)(ei \u2212 ej)> } .\nSince LE is singular with columns perpendicular to 1, we define \u03bbmin,\u22a5 to be the smallest eigenvalue restricted to the eigenvectors orthogonal to 1. More specifically, for a symmetric matrix M, we define\n\u03bbmin,\u22a5(M) = min{\u03bb : v>Mv \u2265 \u03bb, for all v such that \u2016v\u20162 = 1 and v>1 = 0}.\nThen the following lemma characterizes the range of eigenvalues of LE . Lemma A.1. We define Mn = \u2211n+1 k=2 1 k2 ( n\u22122 k\u22123 ) 2n/n2 and Nn = \u2211n+1 k=2 1 k2 ( n\u22121 k\u22122 ) 2n+1/n2. Under the condition that 2np \u2265 Cn log n for some large enough constant C > 0, we have that the event\nALE := { \u03bbmin,\u22a5(LE) \u2265 1\n2 (nMn +Nn)p, \u03bbmax(LE) \u2264\n3 2 (n+ 1)Nnp\n} (A.3)\nholds with probability at least 1\u2212O(n\u221210).\nThe proof of Lemma A.1 is deferred to Section F.1. Based upon Lemma A.1, the following lemmas depict the smoothness and convexity of the log-likelihood function,\nLemma A.2. Under the same conditions as Lemma A.1, for all \u03b8 \u2208 Rn+1 such that \u2016\u03b8 \u2212 \u03b8\u2217\u2016\u221e \u2264 C for some C \u2265 0, under the event ALE defined in (A.3) we have \u03bbmin,\u22a5 ( \u22072L\u03bb(\u03b8;x) ) \u2265\u03bb+ 1\n2(\u03ba\u03b8e2C)2 (nMn+Nn)p, \u03bbmax\n( \u22072L\u03bb(\u03b8;x) ) \u2264 \u03bb+3(\u03ba\u03b8e 2C)2\n2 (n+1)Nnp.\n(A.4)\nPlease see Section F.2 for the proof of Lemma A.2.\nLemma A.3. Under the same conditions as Lemma A.1 and the condition that \u03bb \u221a nMnp log n/L, the following event\nAg := { \u2016\u2207L\u03bb(\u03b8\u2217;x)\u20162 . n \u221a Mnp log n\nL\n} (A.5)\noccurs with probability at least 1\u2212O(n\u221210).\nPlease refer to Section F.3 for the proof of Lemma A.3.\nLemma A.4. Under the same conditions as Lemma A.1, with probability at least 1\u2212 n\u221210, we have that for any \u03b8 \u2208 Rn+1 the following event holds\nAh := { \u2016\u22072L\u03bb(\u03b8;x)\u20162 \u2264 \u03bb+ 3(n+ 1)2np/16 } . (A.6)\nPlease see Section F.4 for the proof of Lemma A.4. Now we begin with the proof. Same as Chen et al. (2019b) did in their proof, we let \u03b8T be the output of the following gradient descent\n\u2022 Initialize \u03b80 = \u03b8\u2217,\n\u2022 for t = 0, 1, . . . , T \u2212 1, \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207L\u03bb(\u03b8t;x),\nwhere \u03b7 is taken to be 1/(\u03bb+ n2np/4).\nStep I\nBy Lemma A.4, we know that with probability at least 1\u2212 n\u221210, L\u03bb(\u03b8;x) is (\u03bb+ n2np/4)smooth and \u03bb-strongly convex for any \u03b8 \u2208 Rn+1, then by Theorem 3.10 in Bubeck et al. (2015) we have\n\u2016\u03b8t \u2212 \u03b8\u0302\u20162 \u2264 \u03c1t\u2016\u03b8\u2217 \u2212 \u03b8\u0302\u20162,\nwhere \u03c1 = 1 \u2212 \u03bb \u03bb+n2np/4 . Note that since 1>\u03b8\u2217 = 0 and 1>\u2207L(\u03b8;x) = 0 for any \u03b8 \u2208 Rn+1, it follows by simple induction that 1>\u03b8t = 0 for all t \u2265 1, and hence \u03b8\u0302 > 1 = 0 due the convergence of \u03b8t to \u03b8\u0302. The following lemma provides an initial upper bound for \u2016\u03b8\u2217 \u2212 \u03b8\u0302\u20162.\nLemma A.5. On the event Ag as defined in (A.5), there exists a constant c > 0 such that\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 c \u221a n.\nThe proof is deferred to Section F.5. Recall that \u03bb \u221a\n2np logn nL , then with Lemma A.5,\nunder the event Ag, for large enough T we have\n\u2016\u03b8T \u2212 \u03b8\u0302\u20162 \u2264 \u03c1T\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 . (\n1\u2212 \u03bb \u03bb+ n2np/4\n)T \u221a n\n\u2264 exp ( \u2212 T\u03bb \u03bb+ n2np/4 )\u221a n \u2264 exp ( \u2212c3T \u221a log n n32npL ) \u221a n\n.\n\u221a log n\nn2npL .\nStep II\nNow we show the convergence rate of \u2016\u03b8T \u2212 \u03b8\u2217\u20162 by induction. More specifically, we propose that with probability at least 1\u2212O(n\u221210), the following holds true for some constant C1 > 0 for any t \u2208 [T ]+,\n\u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2264 C1\n\u221a log n\nMnpL . (A.7)\nNote that the base case t = 0 holds true trivially because of our setting of \u03b80 = \u03b8\u2217. Now suppose that (A.7) holds true for the t-th iteration, we will show that under the event\nAg \u2229ALE (occurring with probability at least 1\u2212O(n\u221210)), the following holds\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2264 C1\n\u221a log n\nMnpL .\nFirst note that if (A.7) holds for the t-th iteration, we have\n\u2016\u03b8t \u2212 \u03b8\u2217\u2016\u221e \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2264 C1\n\u221a log n\nMnpL .\nThe rest of the proof follows the proof of Lemma 14 in Chen et al. (2019b) with modifications. From the gradient descent algorithm, we know that\n\u03b8t+1 \u2212 \u03b8\u2217 = \u03b8t \u2212 \u03b7\u2207L\u03bb ( \u03b8t;x ) \u2212 \u03b8\u2217\n= \u03b8t \u2212 \u03b7\u2207L\u03bb ( \u03b8t;x ) \u2212 [\u03b8\u2217 \u2212 \u03b7\u2207L\u03bb (\u03b8\u2217;x)]\u2212 \u03b7\u2207L\u03bb (\u03b8\u2217;x)\n= { In \u2212 \u03b7 \u222b 1 0 \u22072L\u03bb(\u03b8(\u03c4);x)d\u03c4 }( \u03b8t \u2212 \u03b8\u2217 ) \u2212 \u03b7\u2207L\u03bb (\u03b8\u2217;x) ,\nwhere \u03b8(\u03c4) := \u03b8\u2217 + \u03c4(\u03b8t \u2212 \u03b8\u2217). Then we can see that if C1 \u221a\nlog n/(MnpL) \u2264 for a sufficiently small constant , we have that \u2016\u03b8(\u03c4)\u2212\u03b8\u2217\u2016\u221e \u2264 \u03c4\u2016\u03b8t\u2212\u03b8\u2217\u2016\u221e \u2264 . Then by Lemma A.2 we have that under event ALE defined in (A.3) (which occurs with probability at least 1\u2212O(n\u221210)), for all \u03c4 \u2208 [0, 1]\n\u03bb+ 1\n3\u03ba2\u03b8 nMnp \u2264 \u03bbmin,\u22a5\n( \u22072L\u03bb(\u03b8(\u03c4);x) ) \u2264 \u03bbmax ( \u22072L\u03bb(\u03b8(\u03c4);x) ) \u2264 \u03bb+ n2np/4.\nNow we denote B = \u222b 1\n0 \u22072L\u03bb(\u03b8(\u03c4);x)d\u03c4 , then we have\nB1n+1 = \u222b 1 0 \u22072L\u03bb(\u03b8(\u03c4);x)1n+1d\u03c4 = 0;\n\u03bbmin,\u22a5(B) = min v\u2208Sn,v>1=0 \u222b 1 0 v>\u22072L\u03bb(\u03b8(\u03c4);x)vd\u03c4 \u2265 \u222b 1 0 \u03bbmin,\u22a5 ( \u22072L\u03bb(\u03b8(\u03c4);x) ) d\u03c4\n\u2265 \u03bb+ 1 3\u03ba2\u03b8 nMnp;\n\u03bbmax(B) = max v\u2208Sn \u222b 1 0 v>\u22072L\u03bb(\u03b8(\u03c4);x)vd\u03c4 \u2264 \u03bb+ n2np/4.\nBesides, by the definition of \u03b7 we have that 1\u2212 \u03b7 ( \u03bb+ 1\n3\u03ba2\u03b8 nMnp\n) \u2265 1\u2212 \u03b7 (\u03bb+ n2np/4) \u2265 0.\nCombining the above results we have \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2264 (\n1\u2212 1 3\u03ba2\u03b8 \u03b7nMnp\n) \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + \u03b7\u2016\u2207L\u03bb (\u03b8\u2217;x) \u20162\n\u2264 (\n1\u2212 1 3\u03ba2\u03b8 \u03b7nMnp\n) C1 \u221a log n\nMnpL + C\u03b7n\n\u221a Mnp log n\nL\n\u2264 C1\n\u221a log n\nMnpL ,\nso long as C1 is large enough.\nStep III\nCombing the results in Step I and Step II, as T goes to infinity we have that with probability at least 1\u2212O(n\u221210),\n\u2016\u03b8T \u2212 \u03b8\u2217\u20162 .\n\u221a log n\nMnpL . n\n\u221a log n\n2npL ,\nand\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 \u2016\u03b8T \u2212 \u03b8\u0302\u20162 + \u2016\u03b8T \u2212 \u03b8\u2217\u20162 . n\n\u221a log n\n2npL ."
        },
        {
            "heading": "B Proof of Theorem 3.2",
            "text": "Before we begin with the proof, we first propose the following lemmas that provide bounds for several terms to be used in the proof.\nLemma B.1. Under the same conditions as Theorem 3.1, for any \u03b8 \u2208 Rn+1 such that \u2016\u03b8 \u2212 \u03b8\u2217\u2016\u221e < C for some constant C > 0, with probability at least 1\u2212O(n\u221210) we have the following bounds\n\u2016\u2207L(\u03b8\u2217;x)\u2016\u221e . \u221a nMnp log n\nL , (B.1)\n\u2016\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)\u2212\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)\u2016\u221e . n log n\nL , (B.2) \u2016\u22072L(\u03b8\u0302;x)\u2212\u22072L(\u03b8\u2217;x)\u2016\u221e . n \u221a Mnp log n\nL , (B.3)\u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) 1 1> 0 )\u22121 \u2212 ( \u22072L(\u03b8\u2217;x) 1 1> 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 . 1\u221a nMnp \u221a log n MnpL . (B.4)\nThe proof of Lemma B.1 is deferred to Section F.6. The following corollary of Lemma B.1 helps characterize the Moore-Penrose inverse of the Hessian matrix.\nCorollary B.2. For any \u03b8 \u2208 Rn+1, we have that( \u22072L(\u03b8;x) 1\n1> 0\n)\u22121 = ( \u22072L(\u03b8;x)\u2020 1 n+1 1\n1 n+1 1> 0\n) .\nBesides, for \u03b8 \u2208 Rn+1 such that \u2016\u03b8\u2212 \u03b8\u2217\u2016\u221e < C for some constant C > 0, with probability at least 1\u2212O(n\u221210) we have that [\u22072L(\u03b8;x)\u2020]jj 1nMnp for j \u2208 [n+ 1] and \u2223\u2223[\u22072L(\u03b8;x)\u2020]jk\u2223\u2223 . 1 nMnp for j 6= k.\nSee Section F.7 for the proof of Corollary B.2. The following lemma provides a de-\ncomposition of the error of the debiased MLE \u03b8\u0302 d\ninto the leading term and the remainder term.\nLemma B.3. Under the same conditions as Theorem 3.1 , we have the following decomposition for the debiased MLE\n\u03b8\u0302 d \u2212 \u03b8\u2217 = \u2212\u22072L(\u03b8\u2217;x)\u2020\u2207L(\u03b8\u2217;x) + R0, (B.5)\nwhere the residual term\n\u2016R0\u20162 . \u221a n log n\nMnpL\n(\u221a n log n\nMnpL + 1\n) . n5/2 log n\n2npL\n(\u221a n3 log n\n2npL + 1\n) ,\nwith probability at least 1\u2212O(n\u221210).\nSee Section F.8 for the proof of Lemma B.3. Now we can begin with the proof. We abbreviate L(\u03b8;x) to L(\u03b8) for the convenience of notation in the following proof. For k \u2208 [n], we define the mapping vk(\u00b7) : Rn+1 \u2192 Rn+1\nvk(\u03b8) = (\n(0\u2212 rk)e\u03b80 , (r1 \u2212 rk)e\u03b81 , (r2 \u2212 rk)e\u03b82 , . . . , (rk\u22121 \u2212 rk)e\u03b8k\u22121\ufe38 \ufe37\ufe37 \ufe38 first k entries\n, 0, . . . , 0 )> ,\nand for the convenience of notation we let vk = vk(\u03b8 \u2217). Define the function\ngk(\u03b8) = k\u2211 i=1 ri exp(\u03b8i)\u2212 ( k\u2211 i=0 exp(\u03b8i))rk,\nand we have \u2207g(\u03b8) = vk(\u03b8) and \u22072g(\u03b8) = diag ( vk(\u03b8) ) , where\nvk(\u03b8) = (\n(0\u2212 rk)e\u03b80 , (r1 \u2212 rk)e\u03b81 , (r2 \u2212 rk)e\u03b82 , . . . , (rk\u22121 \u2212 rk)e\u03b8k\u22121\ufe38 \ufe37\ufe37 \ufe38 first k entries\n, 0, . . . , 0 )> .\nThen we have\n\u2206\u0302k \u2212\u2206k = gk(\u03b8\u0302 d )\u2212 gk(\u03b8\u2217) = \u2207g(\u03b8\u2217)>(\u03b8\u0302 d \u2212 \u03b8\u2217) + 1\n2 (\u03b8\u0302\nd \u2212 \u03b8\u2217)>\u22072g(\u03b8\u0303)(\u03b8\u0302 d \u2212 \u03b8\u2217)\n= vk(\u03b8 \u2217)>(\u2212\u22072L(\u03b8\u2217)\u2020\u2207L(\u03b8\u2217) + R0) + r1,\nwhere \u2016\u03b8\u0303 \u2212 \u03b8\u2217\u20162 \u2264 \u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u20162, \u2016R0\u20162 . \u221a n logn MnpL (\u221a n logn MnpL + 1 ) with probability at least 1\u2212O(n\u221210) from Lemma B.3 , and\n|r1| = 1\n2 (\u03b8\u0302\nd \u2212 \u03b8\u2217)>\u22072g(\u03b8\u0303)(\u03b8\u0302 d \u2212 \u03b8\u2217) . \u2016vk\u2016\u221e\u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u201622.\nNote that from the proof of Lemma B.3 we can see that R>0 1 = 0, and hence |v>k R0| \u2264 \u2016R0\u20162\u2016P\u22a51 vk\u20162, where P\u22a51 is the projection matrix to the perpendicular space of 1. Also from Lemma B.1 and the proof of Lemma B.3, we know that under the condition that L & n3 log n/(2np), with high probability we have\n\u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u20162 .\n1\nnMnp \u00d7 n\n\u221a Mnp log n\nL +\n\u221a n log n\nMnpL .\n\u221a log n\nMnpL ,\nand thus we have |r1| . log n\u2016vk\u2016\u221e/(MnpL). Now we consider the term v>k\u22072L(\u03b8 \u2217)\u2020\u2207L(\u03b8\u2217). We have\n\u2212v>k\u22072L(\u03b8\u2217)\u2020\u2207L (\u03b8\u2217) = 1\nL v>k\u22072L(\u03b8\u2217)\u2020 L\u2211 `=1 n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 {\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei }\n= 1\nL L\u2211 `=1 n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 y (`) S ,\nwhere y (`) S := v > k\u22072L(\u03b8\n\u2217)\u2020 {\u2211\ni\u2208S\n( x\n(i,`) S \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217 j\n) ei } are independent for ` and S condi-\ntional on E . Then with high probability, we have\nVar n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 y (`) S  = v>k\u22072L(\u03b8\u2217)\u2020\u22072L(\u03b8\u2217)\u22072L(\u03b8\u2217)\u2020vk = v>k\u22072L(\u03b8\u2217)\u2020vk & \u2016P\u22a51 vk\u201622nMnp . Now consider\n\u2223\u2223\u2223y(`)S \u2223\u2223\u2223, and with high probability we have \u2223\u2223\u2223y(`)S \u2223\u2223\u2223 .\u2211\ni\u2208S\n\u2223\u2223\u2223\u2223\u2223x(i,`)S \u2212 e\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j \u2223\u2223\u2223\u2223\u2223 |v>k [\u22072L(\u03b8\u2217)\u2020]i| . |S|nMnp\u2016P\u22a51 vk\u20162,\nwhere the last inequality is due to the fact that \u22072L(\u03b8\u2217)\u20201 = 0. Then in turn we also have\nn+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 E \u2223\u2223\u2223y(`)S \u2223\u2223\u22233 . (max S \u2223\u2223\u2223y(`)S \u2223\u2223\u2223) n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 E \u2223\u2223\u2223y(`)S \u2223\u2223\u22232 . 1Mnpv>k\u22072L(\u03b8\u2217)\u2020vk\u2016P\u22a51 vk\u20162.\nThus under the condition that n/ \u221a MnpL = o(1), we have\nmax k  L\u2211 `=1 n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 Var ( y (`) S )\u22123/2 L\u2211 `=1 n+1\u2211 k\u2032=2 \u2211 S\u2208Ek\u2032 E \u2223\u2223\u2223y(`)S \u2223\u2223\u22233  = o(1). Then by Lyapunov\u2019s Central Limit Theorem, we have\n\u221a L(v>k\u22072L(\u03b8\u2217)\u2020vk)\u22121/2 ( v>k\u22072L(\u03b8\u2217)\u2020\u2207L(\u03b8\u2217) ) \u2223\u2223 E d\u2192 N(0, 1), for all k \u2208 [n]. Besides, under the condition that\nmax k { n log n\u221a MnpL + log n\u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162 \u221a n MnpL } = o(1), (B.6)\nwe have \u221a L(v>k\u22072L(\u03b8 \u2217)\u2020vk) \u22121/2(v>k R0 + r1) = oP (1), and by Slutsky\u2019s Theorem the claim follows. Now to simplify the conditions, we turn to study the norm \u2016P\u22a51 vk\u20162 for k \u2208 [n]. First note that P1vk = (n + 1) \u22121\u2206k \u00b7 1. Due to the non-decreasingness of \u2206k, we consider the scenarios of k \u2264 K\u2217 and k > K\u2217 separately.\n\u2022 k \u2264 K\u2217. From Theorem 2.1 we know that \u2206k \u2264 0 when k \u2264 K\u2217. Besides, since (ri \u2212 rk)u\u2217i \u2265 0 for i < k, we know that |rku\u22170| \u2265 |\u2206k|. Thus we have that\n\u2016P\u22a51 vk\u201622 = ( \u2212rku\u22170 \u2212\n\u2206k n+ 1\n)2 + k\u22121\u2211 i=1 { (ri \u2212 rk)u\u2217i \u2212 \u2206k n+ 1 }2 + n\u2212 k + 1 (n+ 1)2 \u22062k\n= (rku \u2217 0) 2 + k\u22121\u2211 i=1 ((ri \u2212 rk)u\u2217i ) 2 \u2212 \u2206 2 k n+ 1 \u2265 (rku\u22170)2 \u2212 \u22062k n+ 1 \u2265 (rku\u22170)2 \u2212 (rku \u2217 0) 2\nn+ 1 =\nn\nn+ 1 (rku\n\u2217 0)\n2 \u2265 (rku \u2217 0) 2\n2 .\n\u2022 k > K\u2217. When k > K\u2217, we know that \u2206K \u2265 0 from Theorem 2.1. Then we have\n\u2016P\u22a51 vk\u201622 = ( \u2212rku\u22170 \u2212\n\u2206k n+ 1\n)2 + k\u22121\u2211 i=1 { (ri \u2212 rk)u\u2217i \u2212 \u2206k n+ 1 }2 + n\u2212 k + 1 (n+ 1)2 \u22062k\n\u2265 ( \u2212rku\u22170 \u2212\n\u2206k n+ 1\n)2 \u2265 (rnu\u22170)2.\nCombining the above results we have that \u2016P\u22a51 vk\u20162 & rnu\u22170, and in turn we have that\nmax k \u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162 . r1 maxi u \u2217 i rnu\u22170 \u2264 \u03ba\u03b8 r1 rn = \u03bar\u03ba\u03b8 . \u03bar.\nTherefore, under the condition that \u03bar . \u221a n, the condition (B.6) simplifies to\nn log n\u221a MnpL + \u03bar log n\n\u221a n\nMnpL . n log n\u221a MnpL . n2 log n\u221a 2npL = o(1)."
        },
        {
            "heading": "C Proof of Corollary 3.3",
            "text": "By Slutsky\u2019s Theorem, it suffices for us to show that\nmax k\n{ |v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2212 v>k\u22072L(\u03b8\u2217;x)\u2020vk|/v>k\u22072L(\u03b8\u2217;x)\u2020vk } = oP (1).\nFrom (B.4), we know that \u2016\u22072L(\u03b8\u0302;x)\u2020\u2212\u22072L(\u03b8\u2217;x)\u2020\u20162 . 1\u221anMnp \u221a logn MnpL with high probability.\nAlso from Theorem 3.1, we know that \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 . \u221a\nlogn MnpL , and in turn\n\u2016v\u0302k\u2212vk\u20162 = OP ( \u2016\u22072g(\u03b8\u2217)(\u03b8\u0302\u2212\u03b8\u2217)\u20162 ) = OP ( \u2016 diag(vk)\u20162\u2016\u03b8\u0302\u2212\u03b8\u2217\u20162 ) = OP ( \u2016vk\u2016\u221e \u221a log n\nMnpL\n) .\nCombining the previous results, we have\n|v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2212 v>k\u22072L(\u03b8\u2217;x)\u2020vk| \u2264 |v\u0302>k (\u22072L(\u03b8\u0302;x)\u2020 \u2212\u22072L(\u03b8\u2217;x)\u2020)v\u0302k| + |(v\u0302k \u2212 vk)\u22072L(\u03b8\u2217;x)\u2020vk|+ |(v\u0302k \u2212 vk)\u22072L(\u03b8\u2217;x)\u2020v\u0302k|\n= OP ( \u2016\u22072L(\u03b8\u0302;x)\u2020 \u2212\u22072L(\u03b8\u2217;x)\u2020\u20162\u2016P\u22a51 vk\u201622 + \u2016P\u22a51 vk\u20162\u2016\u22072L(\u03b8\u2217;x)\u2020\u20162\u2016v\u0302k \u2212 vk\u20162 ) . \u2016P\u22a51 vk\u201622\u221a nMnp \u221a log n MnpL + \u2016P\u22a51 vk\u20162\u2016vk\u2016\u221e nMnp \u221a log n MnpL .\nRecall from the proof of Theorem 3.2 that v>k\u22072L(\u03b8 \u2217;x)\u2020vk & \u2016P\u22a51 vk\u201622/(nMnp), then under\nthe condition that \u03bar . \u221a n and \u221a n3 log n/(2npL) = o(1), we have that with high probability\nmax k\n{ |v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2212 v>k\u22072L(\u03b8\n\u2217;x)\u2020vk| v>k\u22072L(\u03b8 \u2217;x)\u2020vk\n}\n. max k { \u2016P\u22a51 vk\u201622\u221a nMnp \u221a log n MnpL + \u2016P\u22a51 vk\u20162\u2016vk\u2016\u221e nMnp \u221a log n MnpL } nMnp \u2016P\u22a51 vk\u201622\n= max k\n{\u221a n log n\nMnpL + \u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162\n\u221a log n\nMnpL\n} . \u221a n log n\nMnpL + \u03bar\n\u221a log n\nMnpL .\n\u221a n3 log n\n2npL = o(1),\nand by Slutsky\u2019s theorem, the claim follows."
        },
        {
            "heading": "D Proof of Lemma 3.5",
            "text": "In the following proof, we denote L(\u03b8;x) by L(\u03b8) for the convenience of notation. We will first show that with high probability with respect to E , the following event holds\nAE := {\nsup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1)\n|P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1| = o(1) } . (D.1)\nFor the convenience of notation, for two positive sequences xn and yn, we use xn . yn or xn = O(yn) to imply that there exists a constant C > 0 independent of \u03b8\n\u2217 such that xn \u2264 Cyn for large enough n.\nFor a given \u03b8\u2217 \u2208 \u0398, we define the following two auxiliary statistics,\nT0 := max k\u2208[n] L\u2211 `=1\n{ v>k\u22072L(\u03b8\n\u2217)\u2020\u221a Lv>k\u22072L(\u03b8 \u2217)\u2020vk \u2211 S\u2208I ES {\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei }} , (D.2)\nand\nW0 := max k\u2208[n] L\u2211 `=1\n{ v>k\u22072L(\u03b8\n\u2217)\u2020\u221a Lv>k\u22072L(\u03b8 \u2217)\u2020vk \u2211 S\u2208E {\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei } zS,` } . (D.3)\nWe define y (`,S) k =\n\u221a |E|\nv>k \u22072L(\u03b8 \u2217)\u2020vk\nv>k\u22072L(\u03b8 \u2217)\u2020 {\u2211 i\u2208S ( x (i,`) S \u2212 e\n\u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217 j\n) ei } , ` = 1, . . . , L, k =\n1, . . . , n. Then W0 = maxk\u2208[n] 1\u221a L|E| \u2211L `=1 \u2211 S\u2208E y (`,S) k zS,`. We will show that the following three conditions hold with high probability uniformly for all \u03b8\u2217 \u2208 \u0398.\n(1) There exist \u03b61 and \u03b62 independent of \u03b8 \u2217 with \u03b61 \u221a log n+\u03b62 = o(1) such that P (|T \u2212 T0| > \u03b61) <\n\u03b62, P (P (|W \u2212W0| > \u03b61 |x) > \u03b62) < \u03b62,\n(2) c 6 mink\u2208[n] 1L|E| \u2211L `=1 \u2211 S\u2208E E[(y (`,S) k ) 2] for some constant c > 0,\n(3) maxk\u2208[n]|y(`,S)k | . B with B2(log(nL|E|))7/(L|E|) = o(1) where B is not necessarily a constant.\nWe first verify condition (2). For any k \u2208 [n], since y(`,S)k \u2019s are independent conditional on E , we have\n1\n|E|L L\u2211 `=1 \u2211 S\u2208E E [ (y (`,S) k ) 2 ] = 1 Lv>k\u22072L(\u03b8 \u2217)\u2020vk Var [ L\u2211 `=1 { v>k\u22072L(\u03b8\u2217)\u2020 \u2211 S\u2208E \u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei }]\n= v>k\u22072L(\u03b8 \u2217)\u2020vk v>k\u22072L(\u03b8 \u2217)\u2020vk = 1,\nand thus condition (2) holds trivially. As for condition (3), with probability at least 1\u2212O(n\u221210) we have\u2223\u2223\u2223y(`,S)k \u2223\u2223\u2223 = \u221a |E|\nv>k\u22072L(\u03b8 \u2217)\u2020vk\n\u2223\u2223\u2223\u2223\u2223v>k\u22072L(\u03b8\u2217)\u2020 {\u2211\ni\u2208S\n( x\n(i,`) S \u2212\ne\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n) ei }\u2223\u2223\u2223\u2223\u2223 = \u221a |E|\nv>k\u22072L(\u03b8 \u2217)\u2020vk\n\u2223\u2223\u2223\u2223\u2223(P\u22a51 vk)>\u22072L(\u03b8\u2217)\u2020 {\u2211\ni\u2208S\nx (i,`) S ei \u2212 \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ei }\u2223\u2223\u2223\u2223\u2223 . \u221a n2Mnp\n\u2016P\u22a51 vk\u20162/ \u221a nMnp \u2016P\u22a51 vk\u20162 \u2225\u2225\u2225\u2225\u2225\u2211 i\u2208S x (i,`) S [\u2207 2L(\u03b8\u2217)\u2020]i \u2212 \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j [\u22072L(\u03b8\u2217)\u2020]i \u2225\u2225\u2225\u2225\u2225 2\n. \u221a nnMnp\u2016\u22072L(\u03b8\u2217)\u2020\u20162,\u221e \u2264 \u221a nnMnp\u2016\u22072L(\u03b8\u2217)\u2020\u20162 . \u221a n,\nwhere the last but two inequality is due to the fact that\u2225\u2225\u2225\u2225\u2225\u2211 i\u2208S x (i,`) S [\u2207 2L(\u03b8\u2217)\u2020]i \u2212 \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j [\u22072L(\u03b8\u2217)\u2020]i \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2211 i\u2208S x (i,`) S \u2016[\u2207 2L(\u03b8\u2217)\u2020]i\u20162 + \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j \u2016[\u22072L(\u03b8\u2217)\u2020]i\u20162\n\u2264 (\u2211 i\u2208S x (i,`) S + \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) \u2016\u22072L(\u03b8\u2217)\u2020\u20162,\u221e \u2264 2\u2016\u22072L(\u03b8\u2217)\u2020\u20162,\u221e,\nand from Lemma A.2, we know that sup\u03b8\u2217\u2208\u0398 \u2016\u22072L(\u03b8\u2217)\u2020\u20162 . (nMnp)\u22121 under the condition that sup\u03b8\u2217\u2208\u0398 \u03ba\u03b8\u2217 = O(1). Thus we can take B = \u221a n, and condition (3) holds uniformly for\n\u03b8\u2217 \u2208 \u0398 under the condition that n ( log(n2npL) )7 /(2npL) = o(1).\nNow we move on to verify condition (1). First recall from previous proof that the following\nhold with probability at least 1\u2212O(n\u221210),\n\u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u20162 .\n\u221a log n\nMnpL , \u2016v\u0302k \u2212 vk\u20162 . \u2016vk\u2016\u221e\n\u221a log n\nMnpL ,\n\u2016\u2207L(\u03b8\u0302)\u2212\u2207L(\u03b8\u2217)\u2016\u221e . n \u221a Mnp log n\nL , \u2016\u22072L(\u03b8\u0302)\u2020 \u2212\u22072L(\u03b8\u2217)\u2020\u20162 . 1\u221a nMnp\n\u221a log n\nMnpL , \u2223\u2223\u2223v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 v>k\u22072L(\u03b8\u2217)\u2020vk\u2223\u2223\u2223 . \u2016P\u22a51 vk\u201622\u221anMnp \u221a log n MnpL + \u2016P\u22a51 vk\u20162\u2016vk\u2016\u221e nMnp \u221a log n MnpL .\nAnd based on the above upper bounds we have\u2223\u2223\u2223\u2223 1\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 1\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk \u2223\u2223\u2223\u2223. 1\u221av>k\u22072L(\u03b8\u2217)\u2020vk \u2223\u2223\u2223\u2223v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k\u2212v>k\u22072L(\u03b8\u2217)\u2020vkv>k\u22072L(\u03b8\u2217)\u2020vk \u2223\u2223\u2223\u2223 .\n1\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk\n(\u221a n log n\nMnpL + \u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162\n\u221a log n\nMnpL\n) ,\n\u2016v\u0302>k\u22072L(\u03b8\u0302)\u2020 \u2212 v>k\u22072L(\u03b8\u2217)\u2020\u20162 . \u2016(v\u0302k \u2212 vk)>\u22072L(\u03b8\u0302)\u2020\u20162 + \u2016v>k (\u22072L(\u03b8\u0302)\u2020 \u2212\u22072L(\u03b8\u2217)\u2020)\u20162\n. \u2016vk\u2016\u221e nMnp\n\u221a log n\nMnpL + \u2016P\u22a51 vk\u20162\u221a nMnp\n\u221a log n\nMnpL ,\nand in turn, we have\u2225\u2225\u2225\u2225\u2225\u2225 v\u0302 > k\u22072L(\u03b8\u0302)\u2020\u221a\nv\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 v\n> k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk \u2225\u2225\u2225\u2225\u2225\u2225 2 . \u2016v\u0302>k\u22072L(\u03b8\u0302)\u2020 \u2212 v>k\u22072L(\u03b8 \u2217)\u2020\u20162\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk\n+ \u2223\u2223\u2223\u2223 \u2016v>k\u22072L(\u03b8\u2217)\u2020\u20162\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 \u2016v > k\u22072L(\u03b8 \u2217)\u2020\u20162\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk \u2223\u2223\u2223\u2223 .\n1\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk ( \u2016vk\u2016\u221e nMnp \u221a log n MnpL + \u2016P\u22a51 vk\u20162\u221a nMnp \u221a log n MnpL ) .\nNow we consider\nW \u2212W0 \u2264 max k\u2208[n] L\u2211 `=1 \u2211 S\u2208I ES \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a Lv\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j )\n\u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a Lv>k\u22072L(\u03b8 \u2217)\u2020vk\n( x\n(i,`) S \u2212\ne\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n)} eizS,`,\nwhere the RHS is the maximum of n centered Gaussian random variables conditioning on the data x and the comparison graph E . We have the following upper bound on the conditional variance\nmax k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208E \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j ) \u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )} ei  2\n. max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208E {\u2211 i\u2208S { v>k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk ( e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217j )} ei }2 \ufe38 \ufe37\ufe37 \ufe38\nI\n+ max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208E \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk (x(i,`)S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j )} ei  2\n\ufe38 \ufe37\ufe37 \ufe38 II\n.\nFor term I, recall from previous proof that by the mean theorem, we have that for any S \u2208 Ek and i \u2208 S we have that with probability at least 1\u2212O(n\u221210),\u2223\u2223\u2223\u2223\u2223 e\u03b8\u0302i\u2211\nj\u2208S e \u03b8\u0302j \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217j \u2223\u2223\u2223\u2223\u2223 . e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e . 1 k \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e,\nand thus with probability at least 1\u2212O(n\u221210) we have\nI = max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208I ES {\u2211 i\u2208S { v>k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk ( e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217j )} ei }2\n. max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208I ES 1 v>k\u22072L(\u03b8 \u2217)\u2020vk { (P\u22a51 vk) > \u2211 i\u2208S {( e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217j )[ \u22072L(\u03b8\u2217)\u2020 ] i }}2\n. max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208I ES 1 \u2016P\u22a51 vk\u201622/(nMnp) \u2016P\u22a51 vk\u201622\u2016\u22072L(\u03b8\u2217)\u2020\u201622\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162\u221e\n. \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162\u221e nMnp \u00d7 Ln 2Mnp L . n log n MnpL ,\nand for term II we have\nII = max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208I ES \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk (x(i,`)S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j )} ei  2\n\u2264 max k\u2208[n]\n1\nL L\u2211 `=1 \u2211 S\u2208I ES \u2225\u2225\u2225\u2225\u2225\u2225 v\u0302 > k\u22072L(\u03b8\u0302)\u2020\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n\u2225\u2225\u2225\u2225\u2225\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j ) ei \u2225\u2225\u2225\u2225\u2225 2\n2\n. max k\u2208[n]\nn2Mnp\nv>k\u22072L(\u03b8 \u2217)\u2020vk ( \u2016vk\u2016\u221e nMnp \u221a log n MnpL + \u2016P\u22a51 vk\u20162\u221a nMnp \u221a log n MnpL )2\n. nmax k\u2208[n] ( \u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162 \u221a log n MnpL + \u221a n log n MnpL )2 . n2 log n MnpL .\nDenote 0 = n2 logn MnpL , and define the event EE to be\nEE = { max k\u2208[n] 1 L L\u2211 `=1 {\u2211 S\u2208E \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a v\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j )\n\u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk\n( x\n(i,`) S \u2212\ne\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n)} ei }2 \u2264 C0 0 } ,\nwhere the constant C0 is chosen properly such that sup\u03b8\u2217\u2208\u0398 P\u03b8\u2217 ( (EE) C ) \u2264 n\u221210. By maximal inequality we have that under the event EE , there exists a large enough constant C > 0 such that\nP\u03b8\u2217 ( W \u2212W0 \u2265 C \u221a 0 log n |x, E ) \u2264 P\u03b8\u2217 ( max k\u2208[n] L\u2211 `=1 \u2211 S\u2208I ES \u2211 i\u2208S { v\u0302>k\u22072L(\u03b8\u0302)\u2020\u221a Lv\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k ( x (i,`) S \u2212 e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j )\n\u2212 v > k\u22072L(\u03b8 \u2217)\u2020\u221a Lv>k\u22072L(\u03b8 \u2217)\u2020vk\n( x\n(i,`) S \u2212\ne\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n)} eiz` \u2265 C \u221a 0 log n |x, E ) \u2264 n\u221210/2.\nVery similarly, under the event EE we can also show that P\u03b8\u2217 ( W0 \u2212W \u2265 C \u221a 0 log n |x, E ) \u2264 n\u221210/2,\nand in turn P\u03b8\u2217 ( |W \u2212W0| \u2265 C \u221a 0 log n |x, E ) \u2264 n\u221210.\nCombining the above results we have\nsup \u03b8\u2217\u2208\u0398\nP\u03b8\u2217 ( P\u03b8\u2217 ( |W \u2212W0| \u2265 C \u221a 0 log n |x, E ) > n\u221210 ) \u2264 sup \u03b8\u2217\u2208\u0398 P\u03b8\u2217 ( (EE) C ) \u2264 n\u221210.\nNow we consider |T \u2212 T0|. Under the condition that \u03bar . \u221a n, with probability at least\n1\u2212O(n\u221210) we have that\n|T \u2212 T0| . max k\u2208[n]\n{\u2223\u2223\u2223\u2223 \u221a L(\u2206\u0302k\u2212\u2206k)\u221a\nv\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212 \u221a L(\u2206\u0302k\u2212\u2206k)\u221a v>k\u22072L(\u03b8 \u2217)\u2020vk \u2223\u2223\u2223\u2223+ \u2016P\u22a51 vk\u20162\u2016R0\u20162 + \u2016vk\u2016\u221e\u2016\u03b8\u0302d\u2212\u03b8\u2217\u201622\u221av>k\u22072L(\u03b8\u2217)\u2020vk/L }\n. max k\u2208[n]\n{ \u2016P\u22a51 vk\u20162\u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u20162 \u2223\u2223\u2223\u2223 \u221a L\u221a\nv\u0302>k\u22072L(\u03b8\u0302)\u2020v\u0302k \u2212\n\u221a L\u221a\nv>k\u22072L(\u03b8 \u2217)\u2020vk \u2223\u2223\u2223\u2223 + \u2016P\u22a51 vk\u20162\u2016R0\u20162 + \u2016vk\u2016\u221e\u2016\u03b8\u0302 d \u2212 \u03b8\u2217\u201622\u221a\nv>k\u22072L(\u03b8 \u2217)\u2020vk/L } \u2264 max\nk\u2208[n] \u221a n log n\u221a L\n(\u221a n\nMnp + \u2016vk\u2016\u221e \u2016P\u22a51 vk\u20162 1\u221a Mnp\n) .\nn log n\u221a MnpL .\nThus if we take \u03b61 = C1 n logn\u221a MnpL and \u03b62 = C2n \u221210 with large enough constants C1 and C2 independent of \u03b8\u2217, under the condition that \u221a\nlog n n logn\u221a MnpL\nn 2(logn)3/2\u221a\n2npL = o(1), condition\n(1) also holds, and by Corollary 3.1 in Chernozhukov et al. (2013) we have that with high probability with respect to E the event (D.1) holds.\nThen by Jensen\u2019s inequality and the dominated convergence theorem we have\nlim n,L\u2192\u221e sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) |P\u03b8\u2217 (T >cW (\u03b1,E))\u2212\u03b1|= lim n,L\u2192\u221e sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) \u2223\u2223\u2223\u2223\u222b P\u03b8\u2217 (T >cW (\u03b1,E) |E) d\u00b5(E)\u2212\u03b1\u2223\u2223\u2223\u2223 = lim\nn,L\u2192\u221e sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) \u2223\u2223\u2223\u2223\u222b (P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1) d\u00b5(E)\u2223\u2223\u2223\u2223 \u2264 lim\nn,L\u2192\u221e \u222b sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) |P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1| d\u00b5(E)\n= lim n,L\u2192\u221e \u222b AE sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) |P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1| d\u00b5(E)\n+ lim n,L\u2192\u221e \u222b AcE sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) |P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1| d\u00b5(E)\n\u2264 \u222b AE lim n,L\u2192\u221e sup \u03b8\u2217\u2208\u0398 sup \u03b1\u2208(0,1) |P\u03b8\u2217 (T > cW (\u03b1,E) |E)\u2212 \u03b1| d\u00b5(E) + lim n,L\u2192\u221e \u222b AcE d\u00b5(E) = 0.\nThus sup\u03b8\u2217\u2208\u0398 sup\u03b1\u2208(0,1) |P\u03b8\u2217 (T > cW (\u03b1,E))\u2212 \u03b1| = o(1) as n, L\u2192\u221e."
        },
        {
            "heading": "E Proof of Theorem 3.4",
            "text": "We will first show that [K\u0302L, K\u0302U ] is a valid confidence interval for K \u2217 based on Lemma 3.5. Note that by definition, if K\u0302U < n, we have that \u2206\u0302K\u0302U+1 > cW (\u03b1/2,E) \u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L. Then we have\nP\u03b8\u2217(K\u0302L\u2264K\u2217\u2264K\u0302U) \u2265 1\u2212P\u03b8\u2217(K\u0302L>K\u2217)\u2212P\u03b8\u2217(K\u0302U<K\u2217) = 1\u2212P\u03b8\u2217(K\u0302L>K\u2217)\u2212P\u03b8\u2217(K\u0302U+1\u2264K\u2217) = 1\u2212P\u03b8\u2217(\u2206K\u0302L\u22650)\u2212 P\u03b8\u2217 ({ \u2206K\u0302U+1<0 } \u2229 {K\u0302U < n} ) = 1\u2212P\u03b8\u2217 ( {\u2206K\u0302L\u22650}\u2229 { \u2206\u0302K\u0302L<\u2212cW (\u03b1/2,E) \u221a v\u0302> K\u0302L \u22072L(\u03b8\u0302;x)\u2020v\u0302K\u0302L/L\n}) \u2212 P\u03b8\u2217 ( {\u2206K\u0302U+1<0}\u2229 { K\u0302U < n, \u2206\u0302K\u0302U+1>cW (\u03b1/2,E) \u221a v\u0302> K\u0302U+1 \u22072L(\u03b8\u0302;x)\u2020v\u0302K\u0302U+1/L })\n\u2265 1\u2212P\u03b8\u2217 (\nmin k>K\u2217\n\u221a L(\u2206\u0302k \u2212\u2206k)\u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2264\u2212cW (\n\u03b1 2 ,E) ) \u2212P\u03b8\u2217 ( max k\u2264K\u2217 \u221a L(\u2206\u0302k \u2212\u2206k)\u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2265 cW (\n\u03b1 2 ,E) )\n\u2265 1\u2212P\u03b8\u2217 (\nmin k\u2208[n]\n\u221a L(\u2206\u0302k \u2212\u2206k)\u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2264\u2212cW (\n\u03b1 2 ,E) ) \u2212P\u03b8\u2217 ( max k\u2208[n] \u221a L(\u2206\u0302k \u2212\u2206k)\u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k \u2265 cW (\n\u03b1 2 ,E) ) .\nBy Lemma 3.5, we know that\nlim n,L\u2192\u221e sup \u03b8\u2217\u2208\u0398\nP\u03b8\u2217 max k\u2208[n]\n\u2206\u0302k \u2212\u2206k\u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L \u2265 cW (\u03b1/2,E)\n = \u03b1 2 .\nAlso, following similar proof and the symmetry of Gaussian distribution we have\nlim n,L\u2192\u221e sup \u03b8\u2217\u2208\u0398\nP\u03b8\u2217 min k\u2208[n]\n\u2206\u0302k \u2212\u2206k\u221a v\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L \u2264 \u2212cW (\u03b1/2,E)  = lim\nn,L\u2192\u221e sup \u03b8\u2217\u2208\u0398\nP\u03b8\u2217 max k\u2208[n] \u2212 \u2206\u0302k \u2212\u2206k\u221a\nv\u0302>k\u22072L(\u03b8\u0302;x)\u2020v\u0302k/L \u2265 cW (\u03b1/2,E)\n = \u03b1 2 .\nand thus by taking limits and supreme on both sides of the inequality, we have that (3.13) follows.\nNow we move on to show (3.14). By the equivalence of (1.1) and (2.4), we have that\nsup S\u2217\u2208S0 P\u03b8\u2217(Reject H0) = sup \u03b8\u2217:K\u2217\u2208K0 P\u03b8\u2217(Reject H0) = sup \u03b8\u2217:K\u2217\u2208K0 P\u03b8\u2217([K\u0302L, K\u0302U ] \u2229 K0 = \u2205)\n= sup \u03b8\u2217:K\u2217\u2208K0\nP\u03b8\u2217 ( {K\u2217 \u2208 K0} \u2229 { [K\u0302L, K\u0302U ] \u2229 K0 = \u2205 }) \u2264 sup \u03b8\u2217:K\u2217\u2208K0 P\u03b8\u2217 ( {K\u2217 \u2208 K0} \u2229 {K\u2217 /\u2208 [K\u0302L, K\u0302U ]}\n) \u2264 1\u2212 inf\n\u03b8\u2217:K\u2217\u2208K0 P\u03b8\u2217(K\u2217 \u2208 [K\u0302L, K\u0302U ]) \u2264 1\u2212 inf \u03b8\u2217\u2208\u0398 P\u03b8\u2217(K\u2217 \u2208 [K\u0302L, K\u0302U ]).\nThen from (3.13), by taking limits on both sides we have that (3.14) holds."
        },
        {
            "heading": "F Proof of Technical Lemmas",
            "text": ""
        },
        {
            "heading": "F.1 Proof of Lemma A.1",
            "text": "Let AS = ES \u00b7 1|S|2 \u2211\ni,j\u2208S i<j\n(ei \u2212 ej)(ei \u2212 ej)>, then LE = \u2211 S\u2208IAS. We have that\nE(LE) = p n+1\u2211 k=2 \u2211 S\u2208Ik 1 k2 \u2211 i,j\u2208S i<j (ei \u2212 ej)(ei \u2212 ej)> = p ( nNn \u2212Nn1>n \u2212Nn1n (nMn+Nn)In\u2212Mn1n1>n )\n= p ( (n+ 1)MnIn+1 \u2212Mn1n+11>n+1 + (Nn \u2212Mn) ( n \u22121>n \u22121n In )) = p ( (n+ 1)MnIn+1 \u2212Mn1n+11>n+1 + (Nn \u2212Mn) ( In+1 + diag(n,1n1 > n )\u2212 1n+11>n+1\n)) = p ( (nMn +Nn)In+1 + (Nn \u2212Mn)diag(n,1n1>n )\u2212Nn1n+11>n+1 ) .\nConsider the matrix (Nn \u2212Mn)diag(n,1n1>n ) \u2212 Nn1n+11>n+1. Define w1 = (1,0>)>, w2 = (0,1>n / \u221a n)> and W = (w1,w2). Then it can be seen that 1n+1 = w1 + \u221a nw2 and we have that\n(Nn \u2212Mn)diag(n,1n1>n )\u2212Nn1n+11>n+1 = n(Nn \u2212Mn)WW> \u2212W ( Nn \u221a nNn\u221a\nnNn Nn\n) W>\n= W\n( (n\u2212 1)Nn \u2212 nMn \u2212 \u221a nNn\n\u2212 \u221a nNn \u2212nMn\n) W> = WGdiag (n(Nn \u2212Mn),\u2212nMn \u2212Nn) G>W>,\nwhere G = 1\u221a n+1 (\u221a n 1 \u22121 \u221a n ) and the last equality follows from the following eigen-decomposition:\n( (n\u2212 1)Nn \u2212 nMn \u2212 \u221a nNn\n\u2212 \u221a nNn \u2212nMn\n) = Gdiag (n(Nn \u2212Mn),\u2212nMn \u2212Nn) G>.\nThen combining the above results we have that E(LE) = p ( (nMn +Nn)In+1 + WGdiag (n(Nn \u2212Mn),\u2212nMn \u2212Nn) G>W> ) , (F.1)\nwhich indicates that E(LE) follows a spiked structure with \u03bb1 ( E(LE) ) = (n+1)Nn, \u03bbn ( E(LE) ) =\nnMn +Nn and \u03bbn+1 ( E(LE) ) = 0. Also since E(LE)1n+1 = 0, we have that 1n+1 corresponds to the (n+ 1)-th eigenvalue of E(LE). Let RE \u2208 R(n+1)\u00d7n denote the stacking of the top n normalized eigenvectors of E(LE), which is unique up to rotation, and we have R>E RE = In and R>E 1n+1 = 0. Then from (F.1), it can be seen that\nE(R>E LERE) = R>EE(LE)RE = diag ( (n+ 1)Nnp, (nMn +Nn)pIn\u22121 ) .\nWe have that \u03bbmin(AS) \u2265 0, \u03bbmax(AS) \u2264 |S|/|S|2 = 1/|S| \u2264 1.\nand \u03bbmin(R > E ASRE) \u2265 0, \u03bbmax(R>E ASRE) \u2264 \u03bbmax(AS) \u2264 1.\nThen by Theorem 5.1.1 in Tropp (2015), under the condition that 2np \u2265 Cn log n for some large enough constant C > 0, there exists a constant c > 0 such that for large enough n we have\nP (\u03bbmin,\u22a5(LE) \u2264 (nMn +Nn)p/2) = P ( \u03bbmin(R > E LERE) \u2264 \u03bbmin ( E(R>E LERE) ) /2 )\n\u2264 n exp(\u2212cnMnp) \u2264 n\u221210,\nand\nP (\u03bbmax(LE) \u2265 3(n+ 1)Nnp/2) = P (\u03bbmax(LE) \u2265 3\u03bbmax (E(LE)) /2) \u2264 (n+ 1) exp(\u2212cnMnp) \u2264 n\u221210."
        },
        {
            "heading": "F.2 Proof of Lemma A.2",
            "text": "First, we can rewrite\n\u22072L(\u03b8;x) = \u2211 S\u2208E  \u2211 i,l\u2208S i<l e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 (ei \u2212 el)(ei \u2212 el)>  .\nThen we have e\u03b8i\u2211 j\u2208S e \u03b8j = 1\u2211 j\u2208S e \u03b8j\u2212\u03b8i = 1\u2211 j\u2208S e (\u03b8j\u2212\u03b8\u2217j )+(\u03b8\u2217j\u2212\u03b8\u2217i )+(\u03b8\u2217i\u2212\u03b8i) \u2265 1\u2211 j\u2208S e log \u03ba\u03b8+2\u2016\u03b8\u2217\u2212\u03b8\u2016\u221e = 1 |S|\u03ba\u03b8e2C .\nSimilarly, we also have\ne\u03b8i\u2211 j\u2208S e \u03b8j = 1\u2211 j\u2208S e (\u03b8j\u2212\u03b8\u2217j )+(\u03b8\u2217j\u2212\u03b8\u2217i )+(\u03b8\u2217i\u2212\u03b8i) \u2264 1\u2211 j\u2208S e \u2212 log \u03ba\u03b8\u22122\u2016\u03b8\u2217\u2212\u03b8\u2016\u221e = \u03ba\u03b8e 2C |S| .\nSince (ei \u2212 el)(ei \u2212 el)> 0 for any i < l, we have that e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 \u2212 1|S|2(\u03ba\u03b8e2C)2  (ei \u2212 el)(ei \u2212 el)> 0,\nand  e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 \u2212 (\u03ba\u03b8e2C)2|S|2  (ei \u2212 el)(ei \u2212 el)> 0.\nHence,\n\u22072L(\u03b8;x) = \u2211 S\u2208E {\u2211 i,l\u2208S i<l e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 (ei \u2212 el)(ei \u2212 el)>}\n\u2211 S\u2208E {\u2211 i,l\u2208S i<l\n1\n|S|2(\u03ba\u03b8e2C)2 (ei \u2212 el)(ei \u2212 el)>\n}\n= 1\n(\u03ba\u03b8e2C)2 \u2211 S\u2208E {\u2211 i,l\u2208S i<l 1 |S|2 (ei \u2212 el)(ei \u2212 el)> } =\n1\n(\u03ba\u03b8e2C)2 LE ,\nand similarly \u22072L(\u03b8;x) (\u03ba\u03b8e2C)2LE . Then by Lemma A.1, the inequalities in (A.4) follow."
        },
        {
            "heading": "F.3 Proof of Lemma A.3",
            "text": "We know that\n\u2207\u03bbL(\u03b8\u2217;x) = \u03bb\u03b8\u2217 \u2212 \u2211 S\u2208E {\u2211 i\u2208S ( x (i) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei }\n= \u03bb\u03b8\u2217 \u2212 1 L \u2211 S\u2208E L\u2211 `=1 {\u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) ei } \ufe38 \ufe37\ufe37 \ufe38\n:=z (`) S\n,\nand we have E ( z\n(`) S\n) = 0 and\n\u2225\u2225\u2225z(`)S \u2225\u2225\u22252 2 = \u2211 i\u2208S ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )2 \u2264 \u2211 i\u2208S (x (i,`) S ) 2+ \u2211 i\u2208S (e\u03b8 \u2217 i )2(\u2211\nj\u2208S e \u03b8\u2217j\n)2 \u2264 1+\u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j = 2,\nE ( z\n(`) S z (`)> S ) = \u2211 i,l\u2208S i<l e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 (ei \u2212 el)(ei \u2212 el)> \u03ba2\u03b8\u2211 i,l\u2208S i<l 1 |S|2 (ei \u2212 el)(ei \u2212 el)>;\nE ( z\n(`)> S z (`) S ) = \u2211 i\u2208S e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j \u2212 \u2211 i\u2208S ( e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )2 \u2264 1,\nand thus with probability at least 1\u2212O(n\u221210) (with randomness coming from E) we have\u2225\u2225\u2225\u2225\u2225 1L2 \u2211 S\u2208E L\u2211 `=1 E ( z (`) S z (`)> S )\u2225\u2225\u2225\u2225\u2225 2 . \u03ba2\u03b8 L \u2225\u2225\u2225\u2225\u2211 S\u2208E { 1 |S|2 \u2211 i,l\u2208S i<l (ei \u2212 ej)(ei \u2212 ej)> }\u2225\u2225\u2225\u2225 2 . 1 L \u2016LE\u20162 . nMnp L ;\n\u2223\u2223\u2223 1 L2 \u2211 S\u2208E L\u2211 `=1 E ( z (`)> S z (`) S ) \u2223\u2223\u2223 = \u2223\u2223\u2223 1 L2 \u2211 S\u2208E L\u2211 `=1 Tr { E ( z (`) S z (`)> S )} \u2223\u2223\u2223 . (n+ 1) L \u2016LE\u20162 . n2Mnp L .\nThus we can take V = n 2Mnp L\nand B = 1/L, then by the matrix Bernstein inequality (Tropp, 2012), with probability at least 1\u2212O(n\u221210) we have that\n\u2016\u2207\u03bbL(\u03b8\u2217;x)\u2212 \u03bb\u03b8\u2217\u20162 . \u221a V log n+B log n . n\n\u221a Mnp log n\nL +\nlog n\nL .\nThus when \u03bb \u221a\nnMnp logn L , since we know that \u2016\u03b8\u2217\u20162 \u2264 \u221a n+ 1 log \u03ba\u03b8, with probability at\nleast 1\u2212O(n\u221210) we have\n\u2016\u2207\u03bbL(\u03b8\u2217;x)\u20162 \u2264 \u03bb\u03b8\u2217 + \u2016\u2207\u03bbL(\u03b8\u2217;x)\u2212 \u03bb\u03b8\u2217\u20162 . n \u221a Mnp log n\nL ."
        },
        {
            "heading": "F.4 Proof of Lemma A.4",
            "text": "The proof is similar to the proof of Lemma A.1. First note that for any \u03b8 \u2208 Rn+1 and any S \u2208 I, we have that\ne\u03b8ie\u03b8l/( \u2211 j\u2208S e\u03b8j)2 \u2264 e\u03b8i/( \u2211 j\u2208S e\u03b8j) ( 1\u2212 e\u03b8i/( \u2211 j\u2208S e\u03b8j) ) \u2264 1/4, \u2200i, l \u2208 S,\nand thus \u22072L(\u03b8;x) = \u2211 S\u2208E  \u2211 i,l\u2208S i<l e\u03b8ie\u03b8l(\u2211 j\u2208S e \u03b8j )2 (ei \u2212 el)(ei \u2212 el)>  14 \u2211 S\u2208E  \u2211 i,l\u2208S i<l (ei \u2212 el)(ei \u2212 el)> \ufe38 \ufe37\ufe37 \ufe38 L\u2032E .\nWe let BS = ES \u00b7 \u2211\ni,l\u2208S i<l (ei \u2212 el)(ei \u2212 el)>, then \u03bbmin(BS) \u2265 0 and \u03bbmax(BS) \u2264 |S| \u2264 n+ 1. We have E(L\u2032E) = p \u2211 S\u2208I  \u2211 i,l\u2208S i<l (ei \u2212 el)(ei \u2212 el)>  = p2n\u22122 ((n+ 2)In+1 + diag(n,1n1>n )\u2212 21n+11>n+1) = p2n\u22122 ( (n+ 2)In+1 + diag(1,1n/ \u221a n)Gdiag (n,\u2212(n+ 2)) G>diag(1,1>n / \u221a n) ) ,\nwhere G = 1\u221a n+1 (\u221a n 1 \u22121 \u221a n ) . By Theorem 5.1.1 in Tropp (2015), under the condition that 2np \u2265 C log n for some large enough constant C > 0, for large enough n we have that there exist constants c, c\u2032 > 0 such that\nP ( \u03bbmax(L \u2032 E) \u2265 3(n+ 1)2n\u22121p/2 ) = P (\u03bbmax(L\u2032E) \u2265 3\u03bbmax (E(L\u2032E)) /2)\n\u2264 (n+ 1) exp(\u2212c2np) \u2264 n\u221210.\nThus with probability at least 1\u2212 n\u221210, we have that\n\u2016\u22072L\u03bb(\u03b8;x)\u20162 . \u03bb+ \u03bbmax(L\u2032E)/4 \u2264 \u03bb+ 3(n+ 1)2np/16."
        },
        {
            "heading": "F.5 Proof of Lemma A.5",
            "text": "By the optimality of \u03b8\u0302 and Taylor\u2019s expansion, we have\nL\u03bb(\u03b8\u0302;x) = L\u03bb(\u03b8\u2217;x) + (\u03b8\u0302 \u2212 \u03b8\u2217)>\u2207L\u03bb(\u03b8\u2217;x) + 1\n2 (\u03b8\u0302 \u2212 \u03b8\u2217)>\u22072L\u03bb(\u03b8\u0303;x)(\u03b8\u0302 \u2212 \u03b8\u2217) \u2264 L\u03bb(\u03b8\u2217;x),\nwhere \u03b8\u0303 lies between \u03b8\u0302 and \u03b8\u2217. Thus\n1 2 \u03bbmin\n( \u22072L\u03bb(\u03b8\u0303;x) ) \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u201622 \u2264 1\n2 (\u03b8\u0302 \u2212 \u03b8\u2217)>\u22072L\u03bb(\u03b8\u0303;x)(\u03b8\u0302 \u2212 \u03b8\u2217)\n\u2264 \u2212(\u03b8\u0302 \u2212 \u03b8\u2217)>\u2207L\u03bb(\u03b8\u2217;x) \u2264 \u2016\u2207L\u03bb(\u03b8\u2217;x)\u20162\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162,\nand thus by Lemma A.3\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 2\u2016\u2207L\u03bb(\u03b8\u2217;x)\u20162 \u03bbmin ( \u22072L\u03bb(\u03b8\u0303;x) ) . 2\u2016\u2207L\u03bb(\u03b8\u2217;x)\u20162 \u03bb . \u221a n."
        },
        {
            "heading": "F.6 Proof of Lemma B.1",
            "text": "In this section, we provide the proof for the upper bounds given in Lemma B.1."
        },
        {
            "heading": "F.6.1 Proof of B.1",
            "text": "For each i \u2208 [n]+ and k = 2, . . . , n+ 1, define ni,k = \u2211 S\u2208Ik:i\u2208S ES. Since \u2223\u2223\u2223\u2223x(i,`)S \u2212 e\u03b8\u2217i\u2211\nj\u2208S e \u03b8\u2217 j \u2223\u2223\u2223\u2223 \u2264 1 and Var(x\n(i,`) S ) \u2264 e\u03b8\n\u2217 i / (\u2211\nj\u2208S e \u03b8\u2217j\n) . 1/|S|, by Bernstein\u2019s inequality, conditional on E , with\nprobability at least 1\u2212O(n\u221211) we have\u2223\u2223\u2223\u2223\u2223 1L n+1\u2211 k=2 \u2211 S\u2208Ek:i\u2208S L\u2211 `=1 ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )\u2223\u2223\u2223\u2223\u2223 . \u221a\u2211n+1 k=2 k \u22121ni,k log n L + log n L .\nAlso by Bernstein\u2019s inequality, under the condition that nMnp \u2265 C log n for some large enough constant C > 0, with probability at least 1\u2212O(n\u221211) we have that\u2223\u2223\u2223\u2223\u2223 n+1\u2211 k=2 k\u22121ni,k \u2212 p n+1\u2211 k=2 k\u22121 ( I(i = 0) ( n k \u2212 1 ) + I(i \u2208 [n]) ( n\u2212 1 k \u2212 2 ))\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223 n+1\u2211 k=2 \u2211 S\u2208Ik:i\u2208S k\u22121 (ES \u2212 p) \u2223\u2223\u2223\n. \u221a\u221a\u221a\u221a(p n+1\u2211 k=2 k\u22122 ( I(i = 0) ( n k \u2212 1 ) + I(i \u2208 [n]) ( n\u2212 1 k \u2212 2 ))) log n . \u221a nMnp log n . nMnp,\nand thus with probability at least 1\u2212O(n\u221210), we have\nmax i\u2208[n]+ \u2223\u2223\u2223\u2223 n+1\u2211 k=2 k\u22121ni,k \u2212 p n+1\u2211 k=2 k\u22121 ( I(i = 0) ( n k \u2212 1 ) + I(i \u2208 [n]) ( n\u2212 1 k \u2212 2 )) \u2223\u2223\u2223\u2223 . nMnp, max i\u2208[n]+ n+1\u2211 k=2 k\u22121ni,k . nMnp,\n(F.2)\nand in turn we have that\n\u2016\u2207L(\u03b8\u2217;x)\u2016\u221e = max i\u2208[n]+ \u2223\u2223\u2223\u2223\u2223 1L n+1\u2211 k=2 \u2211 S\u2208E:i\u2208S L\u2211 `=1 ( x (i,`) S \u2212 e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )\u2223\u2223\u2223\u2223\u2223 . \u221a nMnp log n L ."
        },
        {
            "heading": "F.6.2 Proof of B.2",
            "text": "We first consider \u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x). For i \u2208 [n]+, by the mean value theorem we have[ \u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x) ] i = n+1\u2211 k=2 \u2211 S\u2208Ek:i\u2208S ( e\u03b8\u0302i\u2211 j\u2208S e \u03b8\u0302j \u2212 e \u03b8\u2217i\u2211 j\u2208S e \u03b8\u2217j )\n= n+1\u2211 k=2 \u2211 S\u2208Ek:i\u2208S  e\u03b8\u0303i (\u2211 j\u2208S\\{i} e \u03b8\u0303j ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2 ei \u2212 \u2211 l\u2208S\\{i} e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2el  > (\u03b8\u0302 \u2212 \u03b8\u2217),\nwhere \u03b8\u0303 lies on the line between \u03b8\u0302 \u2212 \u03b8\u2217. Then we have\u2223\u2223\u2223[\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)\u2212\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)] i \u2223\u2223\u2223 \u2264\n\u2225\u2225\u2225\u2225\u2225\u2225\u2225 n+1\u2211 k=2 \u2211 S\u2208Ek:i\u2208S  e\u03b8\u0303i (\u2211 l\u2208S\\{i} e \u03b8\u0303l ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2 ei \u2212 \u2211 l\u2208S\\{i} e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2el \u2212 [\u22072L(\u03b8\u2217;x)]i \u2225\u2225\u2225\u2225\u2225\u2225\u2225 1 \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e.\nNow consider any S \u2208 E such that i \u2208 S. Under the condition that n \u221a\nlog n/(2npL) \u2264 c for some small enough constant c > 0, by Theorem 3.1 there exists a small enough constant > 0 such that \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e \u2264 with probability at least 1 \u2212 O(n\u221210). Then for any \u03b8\u0303 lies between \u03b8\u0302 and \u03b8\u2217 and any l \u2208 S, with probability at least 1\u2212O(n\u221210) we have\ne\u03b8\u0303l\u2211 j\u2208S e \u03b8\u0303j =\ne\u03b8 \u2217 l +(\u03b8\u0303l\u2212\u03b8 \u2217 l )\u2211\nj\u2208S e \u03b8\u2217j+(\u03b8\u0303j\u2212\u03b8\u2217j )\n\u2264 e2\u2016\u03b8\u0302\u2212\u03b8\u2217\u2016\u221e ( e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j\n) \u2264 e2 ( e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j\n) . e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j .\nThen for any l \u2208 S, by the mean value theorem one has\u2223\u2223\u2223\u2223\u2223 e\u03b8\u0303l\u2211 j\u2208S e \u03b8\u0303j \u2212 e \u03b8\u2217l\u2211 j\u2208S e \u03b8\u2217j \u2223\u2223\u2223\u2223\u2223. e\u03b8 \u2217 l\u2211 j\u2208S e \u03b8\u2217j ( 1\u2212 e \u03b8\u2217l\u2211 j\u2208S e \u03b8\u2217j ) |\u03b8\u0303l \u2212 \u03b8\u2217l |+ e\u03b8 \u2217 l\u2211 j\u2208S e \u03b8\u2217j \u2211 j\u2208S\\{l} e\u03b8 \u2217 j\u2211 j\u2208S e \u03b8\u2217j |\u03b8\u0303j \u2212 \u03b8\u2217j |\n. e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j \u2016\u03b8\u0303 \u2212 \u03b8\u2217\u2016\u221e \u2264\ne\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e.\nIn turn for any l \u2208 S\\{i}, by we have\u2223\u2223\u2223\u2223\u2223\u2223\u2223 e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2 \u2212 e\u03b8\u2217i e\u03b8\u2217l(\u2211 j\u2208S e \u03b8\u2217j )2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2 \u2212 ( e\u03b8\u0303i\u2211 j\u2208S e \u03b8\u0303j )( e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j )\u2223\u2223\u2223\u2223\u2223\u2223\u2223 +\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223 ( e\u03b8\u0303i\u2211 j\u2208S e \u03b8\u0303j )( e\u03b8 \u2217 i\u2211 j\u2208S e \u03b8\u2217j ) \u2212 e \u03b8\u2217i e\u03b8 \u2217 l(\u2211 j\u2208S e \u03b8\u2217j )2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\n. ( e\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n)( e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j\n) \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e.\nThus with probability at least 1\u2212O(n\u221210) we have that\u2225\u2225\u2225\u2225\u2225\u2225\u2225 n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S  e\u03b8\u0303i (\u2211 l\u2208S\\{i} e \u03b8\u0303l ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2 ei \u2212 \u2211 l\u2208S\\{i} e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2el \u2212 [\u22072L(\u03b8\u2217;x)]i \u2225\u2225\u2225\u2225\u2225\u2225\u2225 1\n. n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S \u2211 l\u2208S\\{i} \u2223\u2223\u2223\u2223\u2223\u2223\u2223 e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2 \u2212 e\u03b8\u2217i e\u03b8\u2217l(\u2211 j\u2208S e \u03b8\u2217j )2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\n. n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S \u2211 l\u2208S\\{i}\n( e\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n)( e\u03b8 \u2217 l\u2211\nj\u2208S e \u03b8\u2217j\n) \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e\n. \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e n+1\u2211 k=2 \u2211 S\u2208Ik,i\u2208S\n( e\u03b8 \u2217 i\u2211\nj\u2208S e \u03b8\u2217j\n) ES . nMnp\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e,\nwhere the last inequality follows from (F.2). Putting the above analysis together, with probability at least 1\u2212O(n\u221210), we have that for any i \u2208 [n]+\u2223\u2223\u2223[\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)\u2212\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)]\ni \u2223\u2223\u2223 . nMnp\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162\u221e . n log nL , and hence (B.2) holds.\nRemark F.1. Recall that [ \u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x) ] i = n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S  e\u03b8\u0303i (\u2211 j\u2208S\\{i} e \u03b8\u0303j ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2 ei \u2212\u2211 l\u2208S\\{i} e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2el  > (\u03b8\u0302\u2212\u03b8\u2217),\nwhere \u03b8\u0303 lies on the line between \u03b8\u0302 \u2212 \u03b8\u2217. Thus with probability at least 1\u2212O(n\u221211), under\nthe condition that 2np \u2265 Cn log n for some large enough constant C > 0 we have that\u2223\u2223\u2223[\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)] i \u2223\u2223\u2223 \u2264\n\u2225\u2225\u2225\u2225\u2225\u2225\u2225 n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S  e\u03b8\u0303i (\u2211 j\u2208S\\{i} e \u03b8\u0303j ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2 ei \u2212 \u2211 l\u2208S\\{i} e\u03b8\u0303ie\u03b8\u0303l(\u2211 j\u2208S e \u03b8\u0303j )2el  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n1\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e\n.  n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S e\u03b8\u0303i (\u2211 j\u2208S\\{i} e \u03b8\u0303j ) (\u2211\nj\u2208S e \u03b8\u0303j\n)2  \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e . ( n+1\u2211 k=2 \u2211 S\u2208Ek:i\u2208S k\u22121 ) \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e\n. nMnp\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e,\nwhere the last inequality follows from Bernstein\u2019s inequality. Then with probability at least 1\u2212O(n\u221210) we have\u2225\u2225\u2225\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)\u2225\u2225\u2225\n\u221e . nMnp\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e . n\n\u221a Mnp log n\nL . (F.3)"
        },
        {
            "heading": "F.6.3 Proof of B.3",
            "text": "For any i \u2208 [n]+, very similar to the proof in Section F.6.2, with probability at least 1\u2212O(n\u221210) we have\n\u2016[\u22072L(\u03b8\u0302;x)]i \u2212 [\u22072L(\u03b8\u2217;x)]i\u20161\n= \u2225\u2225\u2225\u2225\u2225\u2225\u2225 n+1\u2211 k=2 \u2211 S\u2208Ek,i\u2208S  e\u03b8\u0302i (\u2211 l\u2208S\\{i} e \u03b8\u0302l ) (\u2211\nj\u2208S e \u03b8\u0302j\n)2 ei \u2212 \u2211 l\u2208S\\{i} e\u03b8\u0302ie\u03b8\u0302l(\u2211 j\u2208S e \u03b8\u0302j )2el \u2212 [\u22072L(\u03b8\u2217;x)]i \u2225\u2225\u2225\u2225\u2225\u2225\u2225 1\n. nMnp\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e,\nand hence by Theorem 3.1 with probability at least 1\u2212O(n\u221210) we have\n\u2016\u22072L(\u03b8\u0302;x)\u2212\u22072L(\u03b8\u2217;x)\u2016\u221e = max i\u2208[n+1]\n\u2016[\u22072L(\u03b8\u0302;x)]i \u2212 [\u22072L(\u03b8\u2217;x)]i\u20161 . n \u221a Mnp log n\nL ."
        },
        {
            "heading": "F.6.4 Proof of B.4",
            "text": "For the simplicity of notation, we let \u03bb\u03031 \u2265 . . . \u2265 \u03bb\u0303n \u2265 \u03bb\u0303n+1 = 0 be the eigenvalues of \u22072L(\u03b8;x) in descending order, and let v1, . . . ,vn and 1\u221an+11 be the normalized eigenvectors corresponding to \u03bb\u03031, . . . , \u03bb\u0303n+1. From the proof of Lemma A.2, we know that for any \u03b8 \u2208 Rn+1 such that \u2016\u03b8 \u2212 \u03b8\u2217\u2016\u221e \u2264 C for some constant C > 0, we have 1(\u03ba\u03b8e2C)2 LE \u2207 2L(\u03b8;x)\n(\u03ba\u03b8e 2C)2LE . Besides, by Lemma A.1, with probability at least 1\u2212O(n\u221210), we have\n\u03bbmin,\u22a5(LE) \u2265 1\n2 (nMn +Nn)p, \u03bbmax(LE) \u2264\n3 2 (n+ 1)Nnp.\nThus we have that \u03bb\u03031 \u03bb\u0303n nMnp with probability at least 1\u2212O(n\u221210). Also recall that \u22072L(\u03b8;x)1 = 0, and hence for any constant C > 0, we have the eigen-decomposition(\n\u22072L(\u03b8;x) C1 C1> 0\n) = ( \u22072L(\u03b8;x) 0\n0 0\n) + C ( 0 1 1> 0 ) = n\u2211 i=1 \u03bb\u0303i ( vi 0 )( vi 0 )>\n+ C \u221a n+ 1\n( 1\u221a\n2(n+1) 1\n1\u221a 2\n)( 1\u221a\n2(n+1) 1\n1\u221a 2\n)> \u2212 C \u221a n+ 1 ( 1\u221a 2(n+1) 1\n\u2212 1\u221a 2\n)( 1\u221a\n2(n+1) 1 \u2212 1\u221a 2\n)> .\nIt is not hard to verify by the relationship between vi\u2019s and 1 that the above representation is\na valid eigen-decomposition of ( \u22072L(\u03b8;x) C1 C1> 0 ) . Thus we can see that the n+2 eigenvalues\nof ( \u22072L(\u03b8;x) C1 C1> 0 ) are \u03bb\u03031, . . . , \u03bb\u0303n and \u00b1C \u221a n+ 1, and ( \u22072L(\u03b8;x) C1 C1> 0 )\u22121 would take the following form( \u22072L(\u03b8;x) C1 C1> 0 )\u22121 = n\u2211 i=1 \u03bb\u0303\u22121i ( vi 0 )( vi 0 )> + C\u22121(n+ 1)\u22121/2 ( 1\u221a 2(n+1) 1 1\u221a 2 )( 1\u221a 2(n+1) 1 1\u221a 2 )>\n\u2212 C\u22121(n+ 1)\u22121/2 ( 1\u221a 2(n+1) 1\n\u2212 1\u221a 2\n)( 1\u221a\n2(n+1) 1 \u2212 1\u221a 2\n)> = ( \u22072L(\u03b8;x)\u2020 1 C(n+1) 1\n1 C(n+1) 1> 0\n) ,\nwhere \u22072L(\u03b8;x)\u2020 = \u2211n\ni=1 \u03bb\u0303 \u22121 i viv > i . Then for any \u03b8 \u2208 Rn+1 such that \u2016\u03b8\u2212 \u03b8\u2217\u2016\u221e = O(1) and\na constant Cn > 0 (dependent of n) such that nMnp . Cn \u221a n, we have\u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8;x) Cn1 Cn1 > 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 = max( 1 \u03bbn(\u22072L(\u03b8;x)) , 1 Cn \u221a n+ 1 ) . 1 nMnp .\nThus by B.3, with probability at least 1\u2212O(n\u221210) we have\u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) 1 1> 0 )\u22121 \u2212 ( \u22072L(\u03b8\u2217;x) 1 1> 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u22072L(\u03b8\u0302;x)\u2020 \u2212\u22072L(\u03b8\u2217;x)\u2020\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) Cn1 Cn1 > 0 )\u22121 \u2212 ( \u22072L(\u03b8\u2217;x) Cn1 Cn1 > 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) Cn1 Cn1 > 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u2217;x) Cn1 Cn1 > 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225(\u22072L(\u03b8\u0302;x) Cn1Cn1> 0 ) \u2212 ( \u22072L(\u03b8\u2217;x) Cn1 Cn1 > 0 )\u2225\u2225\u2225\u2225 2\n.\n\u221a n\n(nMnp)2 \u2016\u22072L(\u03b8\u0302;x)\u2212\u22072L(\u03b8\u2217;x)\u2016\u221e . 1\u221a nMnp\n\u221a log n\nMnpL ."
        },
        {
            "heading": "F.7 Proof of Corollary B.2",
            "text": "From the proof of Lemma B.1 it can be seen that( \u22072L(\u03b8;x) 1\n1> 0\n)\u22121 = ( \u22072L(\u03b8;x)\u2020 1 n+1 1\n1 n+1 1> 0 ) holds true for any \u03b8 \u2208 Rn+1, where \u22072L(\u03b8;x)\u2020 is the Moore-Penrose inverse of \u22072L(\u03b8;x). Now define \u0393 = (v1,v2, . . . ,vn,\n1\u221a n+1 1), and denote by \u03b3j = (\u03b3j1, . . . , \u03b3jn, 1\u221a n+1 ) the j-th row of \u0393, j \u2208 [n+1]. Then since \u0393 is orthonormal, it can be seen that \u2016\u03b3j\u201622 = \u2211n k=1 \u03b3 2 jk+ 1 n+1\n= 1 and \u2211n k=1 \u03b3 2 jk 1. Thus for any \u03b8 \u2208 Rn+1 such that \u2016\u03b8 \u2212 \u03b8\n\u2217\u2016\u221e < C for some constant C > 0, for j, k \u2208 [n+ 1], j 6= k, with probability at least 1\u2212O(n\u221210) we have\n[\u22072L(\u03b8;x)\u2020]jj = e>j \u22072L(\u03b8;x)\u2020ej = \u03b3>j  \u03bb\u0303\u221211 . . . 0 ... . . .\n\u03bb\u0303\u22121n 0 0\n\u03b3j = n\u2211 i=1 \u03b32ji\u03bb\u0303 \u22121 i\n1\nnMnp ,\n\u2223\u2223[\u22072L(\u03b8;x)\u2020]jk\u2223\u2223 = \u2223\u2223e>j \u22072L(\u03b8;x)\u2020ek\u2223\u2223 \u2264 \u2016\u22072L(\u03b8;x)\u2020\u20162 . 1nMnp."
        },
        {
            "heading": "F.8 Proof of Lemma B.3",
            "text": "By Corollary B.2 and the definition of \u03b8\u0302 d , we have that\n\u03b8\u0302 d = \u03b8\u0302 \u2032 \u2212\u22072L(\u03b8\u0302;x)\u2020\u2207L(\u03b8\u0302;x) = \u03b8\u0302 \u2032 \u2212 (In+1,0)\n( \u22072L(\u03b8\u0302;x) 1\n1> 0 )\u22121( In+1 0 ) \u2207L(\u03b8\u0302;x)\n= (In+1,0)\n( \u03b8\u0302 \u2032\n0\n) \u2212 (In+1,0) ( \u22072L(\u03b8\u0302;x) 1\n1> 0\n)\u22121( \u2207L(\u03b8\u0302;x)\n0\n) ,\nthen left-multiply both sides by (In+1,0) we have( \u03b8\u0302 d\n0\n) = diag(In+1, 0) ( \u03b8\u0302 \u2032\n0\n) \u2212 diag(In+1, 0) ( \u22072L(\u03b8\u0302;x) 1\n1> 0\n)\u22121( \u2207L(\u03b8\u0302;x)\n0\n)\n=\n( \u03b8\u0302 \u2032\n0\n) \u2212 ( \u22072L(\u03b8\u0302;x) 1\n1> 0\n)\u22121( \u2207L(\u03b8\u0302;x)\n0\n) ,\nwhere the second equality is due to the fact that 1>n+1\u2207L(\u03b8\u0302;x) = 0. Hence we can see that \u03b8\u0302 d can be treated as the sub-vector for the first n + 1 coordinates of an augmented Newton-debiased estimator. Recall that we state in the proof of Theorem 3.1, we will abuse the notation and let \u03b8\u0302 denote \u03b8\u0302 \u2032 = P\u22a51 \u03b8\u0302 and let \u03b8 \u2217 denote \u03b8\u2217\u2032 = P\u22a51 \u03b8 \u2217. Then we have the following decomposition ( \u03b8\u0302 d \u2212 \u03b8\u2217 0 ) = (( \u22072L(\u03b8\u0302;x) 1 1> 0 )\u22121 \u2212 ( \u22072L (\u03b8\u2217;x) 1 1> 0 )\u22121)(\u2212\u2207L(\u03b8\u0302;x)+\u22072L(\u03b8\u0302;x)(\u03b8\u0302\u2212\u03b8\u2217) 0 ) \ufe38 \ufe37\ufe37 \ufe38\nI1\n+\n( \u22072L (\u03b8\u2217;x) 1\n1> 0 )\u22121( \u2207L (\u03b8\u2217;x)\u2212\u2207L(\u03b8\u0302;x) +\u22072L(\u03b8\u0302;x)(\u03b8\u0302 \u2212 \u03b8\u2217) 0 ) \ufe38 \ufe37\ufe37 \ufe38\nI2\n+\n( \u22072L (\u03b8\u2217;x) 1\n1> 0 )\u22121( \u2212\u2207L (\u03b8\u2217;x) 0 ) .\nBy Lemma B.1, we can bound the term I1 and I2 accordingly,\n\u2016I1\u20162 \u2264 \u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) 1 1> 0 )\u22121 \u2212 ( \u22072L(\u03b8\u2217;x) 1 1> 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2212\u2207L(\u03b8\u0302;x) +\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)\u2225\u2225\u2225 2\n. \u221a n \u2225\u2225\u2225\u2225\u2225 ( \u22072L(\u03b8\u0302;x) 1 1> 0 )\u22121 \u2212 ( \u22072L(\u03b8\u2217;x) 1 1> 0 )\u22121\u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2212\u2207L(\u03b8\u0302;x) +\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)\u2225\u2225\u2225 \u221e\n. \u221a n\u00d7 1\u221a\nnMnp\n\u221a log n\nMnpL\n( n log n\nL +\n\u221a nMnp log n\nL\n) . \u221a n log n\nMnpL\n(\u221a n log n\nMnpL + 1\n) ,\nand\n\u2016I2\u20162 = \u2225\u2225\u2225\u2225\u2225 ( \u22072L (\u03b8\u2217;x)\u2020 1 n+1 1\n1 n+1 1> 0\n)( \u2207L (\u03b8\u2217;x)\u2212\u2207L(\u03b8\u0302;x) +\u22072L(\u03b8\u0302;x) ( \u03b8\u0302 \u2212 \u03b8\u2217 ) 0 )\u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u22072L (\u03b8\u2217;x)\u2020 (\u2207L (\u03b8\u2217;x)\u2212\u2207L(\u03b8\u0302;x) +\u22072L(\u03b8\u0302;x)(\u03b8\u0302 \u2212 \u03b8\u2217))\u2225\u2225\u2225\n2 . \u221a n \u2225\u2225\u2225\u22072L (\u03b8\u2217;x)\u2020\u2225\u2225\u2225\n2 \u2016\u2207L(\u03b8\u0302;x)\u2212\u2207L(\u03b8\u2217;x)\u2212\u22072L(\u03b8\u2217;x)(\u03b8\u0302 \u2212 \u03b8\u2217)\u2016\u221e\n. n log n L \u00d7 \u221a n nMnp = \u221a n log n MnpL ,\nwhere the second equality is due to the fact that 1>n+1\u2207L(\u03b8;x) = 0 and 1>n+1\u22072L(\u03b8;x) = 0 for any \u03b8 \u2208 Rn+1. Thus in turn we have that\n\u2016R0\u20162 \u2264 \u2016I1\u20162 + \u2016I2\u20162 . \u221a n log n\nMnpL\n(\u221a n log n\nMnpL + 1\n) . n5/2 log n\n2npL\n(\u221a n3 log n\n2npL + 1\n) ."
        }
    ],
    "title": "Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models",
    "year": 2023
}