{
    "abstractText": "Nowadays the trend is to acquire and share information in an immersive and natural way with new technologies such as Virtual Reality (VR) and 360\u25e6 video. However, the use of 360\u25e6 video, even more the use of VR head-mounted display, can generate general discomfort (\u201ccybersickness\u201d) and one factor is the video shaking. In this work, we developed a method to make the viewing of 360\u25e6 video smoother and more comfortable to watch. First, the rotations are obtained with an innovative technique using a Particle Swarm Optimization algorithm considering the uncertainty estimation among features. In addition, a modified Chauvenet criterion is used to find and suppress outliers features from the algorithm. Afterward, a time-weighted color filter is applied to each frame in order to handle also videos with small translational jitter, rolling shutter wobble, parallax, and lens deformation. Thanks to our complete offline stabilization process, we achieved good-quality results in terms of video stabilization. Achieving better robustness compared to other works. The method was validated using virtual and real 360\u25e6 video data of a mine environment acquired by a drone. Finally, a user study based on a subjective and standard Simulator Sickness Questionnaire was submitted to quantify simulator sickness before and after the stabilization process. The questionnaire underlined alleviation of cybersickness using stabilized videos with our approach",
    "authors": [
        {
            "affiliations": [],
            "name": "A. Luchetti"
        },
        {
            "affiliations": [],
            "name": "D. Kalkofen"
        },
        {
            "affiliations": [],
            "name": "De Cecco"
        }
    ],
    "id": "SP:f75161423524d1197ff1421d9f1b6348e69630c6",
    "references": [
        {
            "authors": [
                "Milgram",
                "Paul",
                "Kishino",
                "Fumio"
            ],
            "title": "A taxonomy of mixed reality visual displays",
            "venue": "IEICE TRANSACTIONS on Information and Systems",
            "year": 1994
        },
        {
            "authors": [
                "J.T. Bell",
                "H.S. Fogler"
            ],
            "title": "The investigation and application of virtual reality as an educational tool, In: Proceedings of the American Society for Engineering Education",
            "venue": "Annual Conference,",
            "year": 1995
        },
        {
            "authors": [
                "A. Luchetti",
                "P. Tomasin",
                "A. Fornaser",
                "P. Tallarico",
                "P. Bosetti",
                "M. De Cecco"
            ],
            "title": "The human being at the center of smart factories thanks to augmented reality",
            "venue": "IEEE5th International Forum on Research and Technology for Society and Industry (RTSI),",
            "year": 2019
        },
        {
            "authors": [
                "Butaslac",
                "I.III",
                "A. Luchetti",
                "E. Parolin",
                "Y. Fujimoto",
                "M. Kanbara",
                "M. De Cecco",
                "H. Kato"
            ],
            "title": "The feasibility of augmented reality as a support tool for motor rehabilitation, In: International Conference onAugmentedReality",
            "venue": "Virtual Reality andComputer Graphics,",
            "year": 2020
        },
        {
            "authors": [
                "G.E. Raptis",
                "C. Fidas",
                "N. Avouris"
            ],
            "title": "Effects of mixed-reality on players behaviour and immersion in a cultural tourism game: A cognitive processing perspective",
            "venue": "International Journal of HumanComputer Studies 114,",
            "year": 2018
        },
        {
            "authors": [
                "Hebbel-Seeger",
                "Andreas"
            ],
            "title": "360 degrees video and VR for training and marketing within sports",
            "venue": "Athens Journal of Sports",
            "year": 2017
        },
        {
            "authors": [
                "L. Argyriou",
                "D. Economou",
                "V. Bouki"
            ],
            "title": "360-degree interactive video application for Cultural Heritage Education, In: 3rd Annual International Conference of the Immersive Learning ResearchNetwork",
            "venue": "Verlag der Technischen Universita\u0308t Graz",
            "year": 2017
        },
        {
            "authors": [
                "E. Guerv\u00f3s",
                "J.J. Ruiz",
                "P. P\u00e9rez",
                "J.A. Mu\u00f1oz",
                "C. D\u00edaz",
                "N. Garc\u00eda"
            ],
            "title": "Using 360 VR video to improve the learning experience in veterinary medicine university degree, In: Electronic Imaging, pp. 217\u20131",
            "venue": "Society for Imaging Science and Technology",
            "year": 2019
        },
        {
            "authors": [
                "K.M. Stanney",
                "R.S. Kennedy",
                "J.M. Drexler"
            ],
            "title": "Cybersickness is not simulator sickness",
            "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting,",
            "year": 1997
        },
        {
            "authors": [
                "S. Litleskare",
                "G. Calogiuri"
            ],
            "title": "Camera stabilization in 360\u25e6 videos and its impact on cyber sickness, environmental perceptions, and psychophysiological responses to a simulated naturewalk: a singleblinded randomized trial",
            "venue": "Front. Psychol",
            "year": 2019
        },
        {
            "authors": [
                "Bonato",
                "Frederick",
                "Bubka",
                "Andrea",
                "Palmisano",
                "Stephen"
            ],
            "title": "Combined pitch and roll and cybersickness in a virtual environment",
            "venue": "Aviation, Space Environ. Med",
            "year": 2009
        },
        {
            "authors": [
                "Perozzi",
                "Gabriele: Efimov",
                "Denis",
                "Biannic",
                "Jean-Marc",
                "Planckaert",
                "Laurent"
            ],
            "title": "Trajectory tracking for a quadrotor under wind perturbations: sliding mode control with state-dependent gains",
            "venue": "Journal of the Franklin Institute 355,",
            "year": 2018
        },
        {
            "authors": [
                "Zhang",
                "Xuebo",
                "Fang",
                "Yongchun",
                "Liu",
                "Xi"
            ],
            "title": "Motion-estimationbased visual servoing of nonholonomic mobile robots",
            "venue": "IEEE Transactions on Robotics 27(6),",
            "year": 2011
        },
        {
            "authors": [
                "S. Li",
                "F. Wang",
                "T. Shi",
                "J. Kuang"
            ],
            "title": "Probably secure multi-user multi-keyword searchable encryption scheme in cloud storage, In: 2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",
            "year": 2019
        },
        {
            "authors": [
                "T.S. Huang",
                "A.N. Netravali"
            ],
            "title": "Motion and structure from feature correspondences: A review, In: Advances in Image Processing and Understanding: A Festschrift for Thomas",
            "year": 2002
        },
        {
            "authors": [
                "Nguyen",
                "Nhat-Tan",
                "Laurendeau",
                "Denis"
            ],
            "title": "Branzan-Albu,Alexandra: A robust method for camera motion estimation in movies based on optical flow",
            "venue": "International Journal of Intelligent Systems Technologies and Applications 9(3\u20134),",
            "year": 2010
        },
        {
            "authors": [
                "Kamranian",
                "Zahra",
                "Sadeghian",
                "Hamid",
                "Mehrandezh",
                "Ahmad Reza Naghsh Nilchi Mehran"
            ],
            "title": "Fast, yet robust end-to-end camera pose estimation for robotic applications",
            "venue": "Applied Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "M. Kamali",
                "A. Banno",
                "Bazin",
                "J.-C",
                "I.S. Kweon",
                "K. Ikeuchi"
            ],
            "title": "Stabilizing omnidirectional videos using 3d structure and spherical image warping",
            "venue": "IAPR MVA,",
            "year": 2011
        },
        {
            "authors": [
                "Liu",
                "Feng",
                "Gleicher",
                "Michael",
                "Jin",
                "Hailin",
                "Agarwala",
                "Aseem"
            ],
            "title": "Content-preserving warps for 3D video stabilization",
            "venue": "ACM Transactions on Graphics (TOG) 28,",
            "year": 2009
        },
        {
            "authors": [
                "C. Buehler",
                "M. Bosse",
                "L. McMillan"
            ],
            "title": "Non-metric image-based rendering for video stabilization",
            "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "year": 2001
        },
        {
            "authors": [
                "Shen",
                "L.-C",
                "Huang",
                "T.-K",
                "Chen",
                "C.-S",
                "Chuang",
                "Y.-Y"
            ],
            "title": "A 2.5 d approach to 360 panorama video stabilization",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2018
        },
        {
            "authors": [
                "Liu",
                "Shuaicheng",
                "Yuan",
                "Lu",
                "Tan",
                "Ping",
                "Sun",
                "Jian"
            ],
            "title": "Bundled camera paths for video stabilization",
            "venue": "ACM Transactions on Graphics (TOG) 32,",
            "year": 2013
        },
        {
            "authors": [
                "W. Kabsch"
            ],
            "title": "A solution for the best rotation to relate two sets of vectors",
            "venue": "Acta. Crystallograph. Sect. A Cryst. Phys. Diffract. Theor. General Crystallograph",
            "year": 1976
        },
        {
            "authors": [
                "Kopf",
                "Johannes"
            ],
            "title": "360 video stabilization",
            "venue": "ACM Transactions on Graphics (TOG) 35,",
            "year": 2016
        },
        {
            "authors": [
                "M.A. Fischler",
                "R.C. Bolles"
            ],
            "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM 24,",
            "year": 1981
        },
        {
            "authors": [
                "C. Tang",
                "O. Wang",
                "F. Liu",
                "P. Tan"
            ],
            "title": "Joint stabilization and direction of 360\u25e6 videos",
            "venue": "ACM Trans. Graph. (TOG) 38,",
            "year": 2019
        },
        {
            "authors": [
                "S. Kasahara",
                "S. Nagai",
                "J. Rekimoto"
            ],
            "title": "First person omnidirectional video: System design and implications for immersive experience",
            "venue": "Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video,",
            "year": 2015
        },
        {
            "authors": [
                "H. Bay",
                "T. Tuytelaars",
                "L. Van Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "European Conference on Computer Vision,",
            "year": 2006
        },
        {
            "authors": [
                "C. Tomasi",
                "T. Kanade"
            ],
            "title": "Detection and Tracking of Point Features",
            "venue": "Carnegie Mellon Univ. Pittsburgh, School of Computer Science",
            "year": 1991
        },
        {
            "authors": [
                "G. Bradski",
                "A. Kaehler"
            ],
            "title": "LearningOpenCV:Computer Visionwith the OpenCV Library, O\u2019Reilly Media, Inc",
            "year": 2008
        },
        {
            "authors": [
                "W.J. Blaedel",
                "V.W. Meloche",
                "J.A. Ramsay"
            ],
            "title": "A comparison of criteria for the rejection of measurements",
            "venue": "J. Chem. Edu. 28,",
            "year": 1951
        },
        {
            "authors": [
                "I.C. Trelea"
            ],
            "title": "The particle swarm optimization algorithm: convergence analysis and parameter selection",
            "venue": "Inform. Process. Lett. 85,",
            "year": 2003
        },
        {
            "authors": [
                "R.S. Kennedy",
                "N.E. Lane",
                "K.S. Berbaum",
                "M.G. Lilienthal"
            ],
            "title": "Simulator sickness questionnaire: An enhancedmethod for quantifying simulator sickness",
            "venue": "Int. J. Aviation Psychol",
            "year": 1993
        }
    ],
    "sections": [
        {
            "text": "Keywords Video stabilization \u00b7 360\u25e6 video \u00b7 Particle swarm optimization \u00b7 Chauvenet\u2019s criterion \u00b7 Uncertainty estimation \u00b7 Shaking"
        },
        {
            "heading": "1 Introduction",
            "text": "The recent spread of cheap 360\u25e6 cameras is leading to an extension of their use and therefore availability to a large number of people.\nThrough a spherical video, they give the most accessible way toVirtual Reality (VR).VRallows users to enter a digital world and fully immerse themselves in it. In a more general contextVR is part ofMixedReality (MR)[1].One of themore well-known MR uses is gaming, but in recent years with its\nB A. Luchetti alessandro.luchetti@unitn.it\nM. Zanetti matteo.zanetti@unitn.it\nD. Kalkofen kalkofen@icg.tugraz.at\nM. De Cecco mariolino.dececco@unitn.it\n1 Department of Industrial engineering, University of Trento, Sommarive, 9, 38123 Trento, Italy\n2 Institute of Computer Graphics and Vision, Graz University of Technology, RechbauerstraBe 12, 8010 Graz, Austria\nability to immerse its users in a virtual or augmented world, MR was used for education [2], training [3], healthcare [4], tourism [5].\nSome famous 360\u25e6 device examples of omnidirectional camera are the GoPro Max 1, Xiaomi Mijia Mi Sphere 360 2 and Insta360 3. After the explosion of immersive media technologies even social platforms, such as Facebook4 and Youtube5 have been providing 360\u25e6 videos in the offered services. The use of spherical videos ismainly used for action sports [6] such as biking or skiing, virtual tours [7], education [8], and adult videos.\nOmnidirectional camera videos contain more information because they capture data from the entire environment. Their videos allow the viewer to look in all directions. This technique allows creating what is better known as \u201cimmersive video\u201d because the user is completely immersed in a vir-\n1 https://gopro.com [Accessed: February 2022]. 2 https://www.mi.com [Accessed: February 2022]. 3 https://store.insta360.com [Accessed: February 2022]. 4 https://facebook360.fb.com/ [Accessed: February 2022]. 5 https://support.google.com/youtube/answer/6178631?hl=en [Accessed: February 2022].\ntual world presented to him and he can control the viewing direction during playback. Anyhow, the level of the viewer\u2019s immersive experience can change depending on the medium in which the 360\u25e6 videos are watched.\nThe viewer can watch 360\u25e6 videos on a standard display of a smartphone, tablet, or computer with the possibility to scroll the graphic interface with a mouse or touch to see different parts of the scene. This method is similar to watching a conventional video but with the benefits of having the possibility to decide the viewing angles thanks to the more images information acquired. This advantage is widely used by filmmakers during editing because, by having more options for angle choices, they can decide and fix the viewing angles generating a standard 2D video that emphasizes the video from the best parts of the scene. The medium that widens the immersive experience is given by Virtual Reality (VR) Head-Mounted Display (HMD) that isolates the user from the surrounding environment andmakes the experiencemore immersive. The user can control the viewing direction with a simple and natural interaction and an effective and hyperrealistic result. The HMD is a wearable hardware device that displays video on a small head-mounted screen that updates dynamically to show different parts of a scene as the viewer turns his head.\nWith the growing use of immersive virtual environments also the environmental psychology research based on human\u2013nature interactions started to study the connection between VR and cybersickness [9] including the impact of improved camera stability on cybersickness [10]. With the use of 360\u25e6 VR, especially with HMDs, users can undergo general discomfort, nausea, headache, vomiting, disorientation. Causes could include the user\u2019s health conditions or too much time spent in VR but mainly is due to video shaking [11]. It can be very disorienting for a VR-user, which may make the viewer fall or get sick.\nExposure to camera vibration is affected by the support to which the camera is connected, such as our body or an external object. For example, in the case of Unmanned Aerial Vehicles (UAV) the impact of the wind field and rotor dynamics conditioned its trajectory [12]. These disturbances inevitably generate shaking in panoramic shots. To make the final experience more comfortable, the most recent omnidirectional cameras have mechanical solutions and real-time optical flow algorithms to smooth out shaky movements. Sometimes this is not enough and some advanced video editing software solutions such as Adobe Premiere Pro CC6 and Cyberlink PowerDirector 3657 are used. However, commercial 360\u25e6 stabilization solutions are expensive, hard to\n6 https://www.adobe.com/products/premiere.html [Accessed: February 2022]. 7 https://www.cyberlink.com/products/powerdirector-video-editingsoftware [Accessed: February 2022].\nuse, and require intensive training before they can be used. Furthermore, it is not always clear which commercial stabilization algorithms are used in the mentioned software products, making them in many cases unsuitable for the required application.\nIn this work, we provide a robust and efficient solution to stabilize 360\u25e6 camera motion, to make the resulting panoramic video more comfortable to view. We introduce a new approach for estimating the camera orientation among frames based on the Particle Swarm Optimization algorithm. Our approach takes into account the estimation of uncertainty among features and the suppression of outliers through a modified Chauvenet criterion.\nThis paper can be divided as follows:\n\u2013 In the introduction, we presented the problem of stabilization for a more comfortable VR experience. \u2013 In Sect. 2, we discussed the state of the art on stabilizing 360\u25e6 video. \u2013 In the following section, we describe the developed method. \u2013 In the fourth section, we validate its results in a controlled environment by using simulated 360\u25e6 videos, as well as on captured 360\u25e6 videos. \u2013 In the fourth section, we present the results of a user study comparing processed and unprocessed 360\u25e6 videos. \u2013 In the final section, we expose the drawn conclusions."
        },
        {
            "heading": "2 Related work",
            "text": "The challenge of camera motion estimation is an important research topic for computer vision,widely used especially for robotics applications such as visual servoing [13] or visual simultaneous localization andmapping [14].Most of existing techniques to estimate camera motion are based on featurecorrespondences techniques [15] or on analysis of the optical flow between consecutive video frame [16]. In recent years, the approaches based on the convolutional neural networks (CNNs) have also been exploited [17].\nOn the other hand the goal of the stabilization is to remove the camera shaking from the estimated camera motion. Stabilization of 360\u25e6 video was studied in many works in the past decade. The proposed approaches can be divided into 2D, 3D, and a combination of the two (2.5D).\nThe work in [18] uses the SIFT algorithm and the 3D approach based on the Structure from Motion (SfM) process for feature extraction and obtaining the camera path, respectively. The SfM estimates a camera pose (location and orientation) and point cloud. Each output frame is generated by warping a single frame from the input video. The 3D reconstructionmakes this process computationally expensive and not robust under certain situations such as the absence of\ntranslationalmotion orwith a significant amount of pure rotations. In general, approaches based on 3D methods [19,20] depend highly on camera motion, limiting their practical application.\nIn [21], the authors through a 2.5D approach first estimate and remove all rotations; then, they employmesh-based image warping [22] in order to compensate the remaining high-frequency jitters caused by camera translation and parallax. After that, they restore proper camera rotations. They use the Kabsch algorithm [23] to find the rotation matrix between two adjacent frames, with the disadvantage that it is less robust because it does not avoid possible localminima by limiting the search. In addition, the warp stabilizer used can lead to a warping increase and sometimes an inappropriate output frame can be generated.\nMost existing 2.5D approaches start by tracking the motion of feature points in the video, such as in [24]. In [24], they convert the 360\u25e6 frames into cube map representation. This representation is less distorted than the equirectangular projection near the poles. On the other hand, if a tracked point falls outside its originating cube face that corresponding observation is dropped and the track is ended. With this method, it is not possible to select a Region of Interest (ROI) for the algorithm. Therefore, the method will fail if the video contains static overlays such as logos or text. Their algorithm [24] removes all rotation from the input video, and afterward, it adds back a smoothed version of rotations. It uses keyframes that are appropriately spaced, so it can reliably estimate the true rotations. To estimate the rotation among them it uses a five-point algorithm in a RANSAC procedure [25]. For the inner frames, it solves a 2D optimization that maximizes the smoothness of tracked feature point trajectories. Finally, it uses a flexible deformed-rotation motion model for handling residual jitter. The most recent approach [26] differs from [24] by using a 3D spherical warpingmodel which is derived from a motion estimation that handles both, rotation and translation, which allowsmore control than [24]. In both aforementioned works, the search of the keyframes slows down the tracking operation.\nOur work is similar to [27,28]. They estimate and smooth the relative rotations between consecutive equirectangular frames. Similar to [27,28], we also track feature points between adjacent frames. However, due to the similarity between each pair of frames small translations and rotations may be difficult to estimate. The limitation of their approach is that if the features tracked are always those of the first frame, less smooth results can be obtained due to the accumulation of errors.\nIn our work we developed a 2.5D method that estimates 3D rotations from the selected features motions without involving 3D structure from motion methods. We periodically detect new features with their uncertainty in frames taken in consecutive pairs. In order to remove only shaking\nwhile preserving intentional rotations, and to compensate the remaining jitters, we use a moving average filter and a timeweighted average filter in the pixel frame color, respectively. In this way, we can handle not only small translations and parallax effect but also videos with temporal artifacts, such as shearing and wobbling, otherwise impossible to manage with a mesh-warping approach."
        },
        {
            "heading": "3 Method",
            "text": "The method can be divided into three steps.\n\u2013 First, we estimate the camera rotation between each pair of consecutive frames. \u2013 Subsequently, we remove undesired rotations from each frame to generate new stabilized equirectangular images. \u2013 Finally, we produce the stabilized 360\u25e6 video."
        },
        {
            "heading": "3.1 Step 1: estimating relative camera rotations",
            "text": "First of all, the frames from the 360\u25e6 video are read and saved as equirectangular images (2880 \u00d7 5760 pixel) in the MATLAB R2019b environment using the MATLAB V ideoReader function. This function returns an object that contains information about the videofile and allows us to read data from the video. Equirectangular projection allows us to show 360\u25e6 images in a single 2D visualization since it shows 360\u25e6 \u00d7 180\u25e6 angles at once. This projection, which transforms spherical coordinates into planar coordinates, creates severe geometric distortions near the poles, i.e., away from the horizontal centerline. However, it is the most popular and most widely supported format.\nThe possible movement of the camera involves both rotations and translations. In our method we decouple rotation from the other motions and handle them separately. First we find the rotation matrix between two consecutive frames, describingonly the 3D rotations of yaw, pitch and roll, assuming zero translations. In the case of panoramic video, this is justified by the large distance of the 360\u25e6 camera image from its surroundings, which makes the relative translation between frames negligible. In addition, camera rotation has a muchmore significant impact than translation in the final stabilization process because translations can bemore tolerated. However, remaining jitters generated by the translations, not considered in this first step, is handled by a final filter added in the stabilization process.\nThe first input image is transformed into 2D grayscale. Then the SURF algorithm [29] is applied to detect blob features. The algorithm is used in a rectangular Region of Interest (ROI) to exclude the more distorted areas near the poles corresponding to high latitude and lower areas of the image.\nA feature-tracking algorithm is then used to track points using Kanade-Lucas-Tomasi (KLT) [30] applied only to the adjacent frame. KLT is based on a small motion assumption. In our case, the accuracy is high because the difference in time and space between each two consecutive equirectangular images is small. This feature-tracking algorithm is commonly used for video stabilization, camera motion estimation, and object tracking. It has the advantage of being fast and handling RGB images.\nThe matched features between two consecutive frames are shown in Fig. 1, found with SURF and KLT algorithms, respectively, for the first and adjacent frames.\nThe problems most frequently observed with the use of the KLT algorithm, in the analysis of real sequences, concern overlappingofmultiplemovements, occlusions, lighting changes, and non-rigid movements. Under these conditions, some points can be lost. For this reason, to strengthen our method, we periodically reacquired points every two consecutive frames with the SURF detector.\nWe do not use SURF feature descriptor and featurematching because they are computationally demanding and KLT is robust enough under the assumption of small transformations, e.g., rotations and scale.\nHowever, the KLTmethod does not guarantee that the corresponding points in the adjacent frame are feature points. To solve that we used the Block method [31] to extract features\u2019 descriptors in the first and consecutive frames. It consists of a simple square neighborhood extraction to identify the features. The Block method extracts only the neighborhoods fully contained within the image boundary. Therefore, only valid points in the first and the corresponding points in the adjacent frame are stored. The difference between each pair of features\u2019 descriptors is obtained by summing all the differences among the descriptors\u2019 components. This provides the uncertainty estimation values among features.\nUsing this parameter in the optimization process improves the robustness of our method without affecting its computational efficiency because the features\u2019 descriptor is simple and allows a smaller number of matching candidates to be\nused. The features are sorted according to their weights in ascending order. A high weight value means that the descriptors between a pair of features are different.\nTo find and suppress possible outliers from the list of features, a modified Chauvenet criterion [32] is used. We modified Chauvenet by changing the standard deviation with the camera angular resolution term in the expression of fractional deviation from the mean. The new expression is:\nDi = |xi \u2212 x | Res\n(1)\nwhere\n\u2013 Di is the fractional deviation from the mean; \u2013 xi is the value of suspected outlier; \u2013 x is the mean of the data set; \u2013 Res is the camera angular resolution value.\nThe set of data used to apply the modified Chauvenet criterion was obtained by taking n-times three features at a time and calculating their 3D rotations comparing them to those of the adjacent frame. The total number of previously sorted features was divided into intervals, and the calculation of 3D rotations was repeated within them. Each interval identifies a subset of features and the percentage values indicate which subset of the sorted featureswe are considering. In each interval, we obtained a histogram with the height of the baskets indicating the number of elements and the abscissa angle values. An example of the histograms obtained for two different intervals of features is shown in Fig. 2. The maximum spread value was extracted by adding the minimum and maximum absolute values of the angles obtained. In Fig. 2 the spread for each angle in the interval 0\u20135% is less than the one in the interval 95\u2013100%. In particular the spreads of yaw, pitch, roll angles in the interval 0\u20135% are 0.1037\u25e6, 0.2452\u25e6, and 0.1056\u25e6, respectively; in the interval 95\u2013100% are 0.4919\u25e6, 0.8922\u25e6, 0.5116\u25e6.\nDividing the features with 1% intervals, the results of the histogram spreads for each angle are displayed in Fig. 3. In the same figure, in red, the range selected with the modified Chauvenet criterion corresponds to the spreads of histograms with values above the Chauvenet threshold.\nIn Fig. 4 can be seen how, thanks to the use of the Res camera resolution in the modified Chauvenet expression Eq. (1), if the variations of the angle values are low, and therefore a low standard deviation occurs, no values are considered as an outlier. This is motivated also by the fact that, as shown in Figs. 3 and 4, the maximum value of the weights, when comparing their first plot, is different by one order of magnitude. Moreover, after the weights of the features are sorted in increasing order as in Figs. 3 and 4, the possibility to find outliers lies in the last intervals; for this reason, the research can\nbe focused in that area reducing by a lot the computational time.\nThe final architecture of the method used to find the valid features with their estimation of uncertainty between each pair of frames is schematically shown in Fig. 5. Each step of Fig. 5 will be validated in Sect. 4.\nThe valid points found in the previous steps are used as input to the optimization algorithm. The points selected in the first frame and those tracked in the adjacent frame are projected from the two equirectangular images with N rows and M columns, to the surface of two unitary radius spheres.\nFirst of all each selected image\u2019s pixel in 2D Cartesian coordinates pixel (n,m) is transformed in spherical coordinates, computing the corresponding azimuth a and elevation e, setting the radius r equal to 1.\nThe relations used for the conversion are:\na = \u2212 ( m M \u2212 0.5 )\n\u00b7 2\u03c0 (2) e = \u2212\n( n N \u2212 0.5 )\n\u00b7 \u03c0 (3) r = 1 (4)\nThepurpose of the value 0.5 inEqs. (2\u20133) is to have the spread of the angle between \u00b1\u03c0 and \u00b1\u03c02 for a and e, respectively.\nFinally, the 3D point cloud is obtained through the mapping from spherical coordinates to 3DCartesian coordinates:\nx = r \u00b7 cos(e) \u00b7 cos(a) (5) y = r \u00b7 cos(e) \u00b7 sin(a) (6) z = r \u00b7 sin(e) (7)\nThe two \u201cspherical\u201d point clouds centered in the same origin are rotated to try to overlap the matching points by using a Particle Swarm Optimization (PSO) algorithm [33].\nPSO algorithm is applied for each consecutive frame, trying tominimize the objective function by changing the values of the three Euler angles around the z-axis, y-axis, x-axis, which are roll (\u03b3 ), pitch (\u03b2), and yaw (\u03b1), respectively.\nThe objective function ( ) to minimize between the first image (I1) and the adjacent one (I2) is:\ny(\u03b3, \u03b2, \u03b1) = (P I1 , P I2 , R,W )\n= N\u2211 i=1 \u2223\u2223\u2223P I1i \u2212 P I2i \u00b7 R \u2223\u2223\u2223 2 \u00b7 Wi (8)\nwhere\nWi = ( max(W) \u2212 Wi\nmax(W)\n)2\nP I1 = {P I11 , P I12 , .., P I1N } P I2 = {P I21 , P I22 , .., P I2N } W = {W1,W2, ..,WN } R = Rz(\u03b3 )Ry(\u03b2)Rx (\u03b1)\nThe objective function in Eq. (8) is a function of the twopoint clouds P I1 and P I2 , of the first and adjacent frames, respectively, the rotation matrix R and the weights of the features\u2019 descriptors W . In particular, the output of Eq. (8) is given by the sum of the squared differences between the 3D Cartesian coordinates of each i th feature in I1, (P I1 ), with the corresponding one in I2, (P I2 ), multiplied by their normalized weightsW . In the optimization loop, the position of the second point clouds P I2 changes each time because it is multiplied with a rotationmatrix of design variables yaw (\u03b1), pitch (\u03b2), and roll (\u03b3 ) angles. In Eq. (8) the i th weight Wi is obtained through the distance between the descriptors vectors of each pair of features found with the Blockmethod. It gives more weight to a pair of points with a higher correspondence between their descriptors given by a smaller distance between them. The max (W) returns the maximum weight value of the array from 1 to N , with N as the total number of valid features. It is used to normalize the weight and to use the weight parameter accordingly to the minimization process.\nA lower and upper bounds on the design variables x were set so that a solution is found in the range lb \u2264 x \u2264 ub.\nOne possible operation to remove camera shake is to completely remove all camera rotations. This is possible if the positions and intentional rotations of the camera are fixed or almost fixed in time. In this case, for each frame, we could go back to the orientations of the first frame [21]. The rotation matrix applicable to each consecutive frame is a cumulative rotation matrix that sums all the contributions of the previ-\nous orientations. If the camera object has an imposed rotation movement in space, it will not be possible to relate all the frames with the first one. For this reason, a possible solution could be the use of a moving average. The length of the sliding window depends on the total number of frames and mainly on the context in which the camera is used [27]. In Fig. 6 is shown an example of the differences in yaw, pitch, and roll angles among 50 consecutive frames taken two at a time with the PSO algorithm. Figure 7 shows the cumulative rotations of angles of Fig. 6. In particular, for each angle in red, there are the cumulative rotations given by the sum of each previous rotations; in blue, there is a smooth curve found by using an average mean of length 12; in green, there is a difference between the previous two curves that allow to pass from the original to the smoothed trajectory.\nIn Fig. 8 the two original point clouds are displayed, in blue the key points related to the first frame, in red those related to the consecutive frame, plus the green points represent the new point cloud found by applying the rotation matrix with Euler angles, found through the PSO algorithm, to the consecutive point cloud. In the zoom area of Fig. 8\ncan be seen the result. The blue points that are related to the consecutive frame generate the green ones that are close to the points of the first frame."
        },
        {
            "heading": "3.2 Step 2: stabilized equirectangular images",
            "text": "After the estimations of the relative camera orientation were found, we apply the inverse rotation matrix to each image. This generates the new equirectangular images by rotating all pixels around the ZYX axis with the values found in the previous step.\nBefore applying the 3D rotation to each equirectangular image, we project all image pixels with their colors on the surface of a unitary radius sphere, see Eqs. (2\u20137). Due to the same size of each frame, the projection of all the equirectangular image\u2019s pixels to the surface of the sphere is done only the first time. For all the consecutive frames only the different pixel colors are updated. This helps to reduce the computational cost.\nRigid transformation, rotations are applied to each spherical frame keeping the position of the center of the sphere fixed. After that, the new 3D Cartesian coordinates of each pixel are converted to spherical coordinates by using the MATLAB cart2sph function.\nBetween this new set of spherical coordinates and the 2D Cartesian coordinates of the equirectangular image to be generated there could not be an exact correspondence since some rotations could be lower than the angular resolution of the camera.\nTo avoid this problem the spherical coordinates of each pixel were generated from a colorless image, Eqs. (2\u20134). The color of each colorless pixel was then reconstructed by applying aweighted average among neighboring pixels of the previous rotated image, which on a color matrix corresponds to 8 pixels if the pixels near the corners are also taken into account.\nTo increase the relation and smooth between two consecutive frames a further filter was applied over time. It generates new frames updating the pixels colors from the average between two previous and two subsequent frames\nusing an overall weight of 0.7 for the two frames nearby and 0.3 for those immediately after. These weights, although not critical values, were set with the highest value for the neighboring frames, because the farther away from the selected frame, the easier it is for the pixel color values to mismatch. In addition, to have the selected frame with the same weight between the previous and following frames, the sum of the weights was set to 1."
        },
        {
            "heading": "3.3 Step 3: 360\u25e6 video production",
            "text": "After all the new equirectangular images were saved the V ideoWriter function in MATLAB was used to create a video file with MPEG-4 container format and a frame rate of 25Hz.\nFinally, the metadata was added to the movie in *.mp4 file extension to include 360\u25e6 information with an external tool called Spatial Media Metadata injector v2.18. Adding the metadata enables online platforms, such as YouTube, to warp into a sphere when watching the video.\nThe general outline of the developed method is schematized in the flowchart of Fig. 9."
        },
        {
            "heading": "4 Validation",
            "text": "A VR environment instead of a real environment was used initially as a gold standard as far as we know the result to achieve. It has allowed us to test different algorithms and approaches by verifying their results and robustness. In this section, we will show the results obtained after applying the stabilization process to virtual and real videos with our method."
        },
        {
            "heading": "4.1 Virtual videos",
            "text": "Each step of the previous chapter was validated within a VR environment in Unity 3D platform. A script was written to simulate a 360\u25e6 camera. The 360\u25e6 capture technique\n8 https://github.com/google/spatial-media/releases [Accessed: February 2022].\nis based on Google\u2019s Omni-directional Stereo (ODS) technology using cubemap rendering [34]. After the cubemap is generated, it is possible to convert this cubemap to an equirectangular map which is a projection format used by 360\u25e6 video players. Placing the simulated camera inside the scene, Fig. 10, allows us to acquire an equirectangular image, Fig. 11.\nThe scene acquired, Fig. 11, is the one corresponding to a Wavefront 3D Object File (OBJ file extension) of the 3D high-resolution virtual environment of a mine previously imported in Unity.\nThe first validation was related to whether the uncertainty estimation of the feature\u2019s descriptors in Eq. (8) provides better results in terms of found rotations. To test it, 100 images were acquired in Unity by rotating the camera with random rotations in yaw, roll, and pitch between a \u00b10.2\u25e6 interval while keeping the surrounding environment unchanged. The results of the PSO algorithm with these images are the same whether the cost function contains weight terms related to the uncertainty estimation or not. This happens because the SURF and the KLT algorithms always find robust features for small rotations in the absence of other disturbances in the environment. To make the images more realistic in the Unity platform, lighting changes have been applied to each frame. To simulate it in theUnity scene a directional lightwas\napplied. The directional light\u2019s rays are parallel and infinite in a specific directionmaking them suitable to simulate outdoor lighting as the real sun. This light influences all objects in the scene regardless of their distance, no matter how it is positioned. As well as the lights, the shadows were approximated with the command Sof t Shadows, with softened edges. For shadows Unity uses a method called shadow mapping: in practice, it creates a depth map from the point of view of the light and uses it to decide where to cast the shadows. The color of the light was presumably fixed; instead, its intensity parameter was changed randomly, between 1 and 2, for each new camera\u2019s orientations. The intensity parameter was changed to simulate lighting changes and to force the PSO algorithm to increase variability. The swarm size of PSOwas set to 30. Usually, the iterations process of the optimization ended because the relative change in the objective value over the last options of the maximum number of stall iterations (default 20) is less than the fixed-function tolerance (default 1e-6). The values of the SURF and KLT algorithm properties are chosen by default. In the Block method, the Block size of a local square neighborhood, centered at each interest point, was set to 11 by default.\nTable 1 shows the values of the sum of the errors obtained using or not in the cost function theweights of the uncertainty estimation of the features descriptors on imageswith environmental disturbances previously discussed. How can be seen in Table 1, the results now change with the cost function; in particular, if we consider the term related to descriptors in cost function, wemake aminor error on all the final rotations. Moreover, this difference in error sum values will become greater with real environments because they are subjected to more sources of disturbance.\nThe second validation concerns how the result changes with the suppression of outliers among the features through the modified Chauvenet criterion before using the PSO algorithm. The modified Chauvenet relation used is explained in Sect. 3.1. The validation was done with the same set of 100 images acquired in Unity in the simulated case where the environment is affected by disturbances. Table 2 shows that the sum of errors is lower with outliers suppression before the PSO. This means that the presence of outliers contributes to the generation of an error in the final angles found and it has a negative influence on the weight of a normalization\nprocess because it refers to the maximum weight value. The expression of the cost function used to extract all the data in Table 1 always contains the weights of the estimated uncertainty of the feature descriptors because in this case we just want to quantify how the errors in rotations change if the outliers are suppressed using the Chauvenet criterion or not.\nFigure 12 shows the percentage difference in time and yaw, pitch, and roll errors by changing the number of features used for the optimization process. Before the selection, the features are sorted with decreasing weight in such a way the first set of 5% concerns the best and gradually all the others are addedwith theirweights.Among the sortedweights, there are no outliers discarded with the previous method. From the results of Fig. 12 it is clear how using all the features with their weights, from the point of view of angles\u2019 errors, is better than considering only a small set at the expense of increasing final optimization time. Although 15% of the features\u2019 number has a smaller error value for the pitch, the best result of all angles is at 100%. This means that for an offline stabilization process, where time can be longer, it is better to consider all the features.\nTo generate the frames used for the previous analyses, we randomly rotated the simulated 360\u25e6 camera in a controlled virtual environment and then verified the effectiveness of our method by checking the errors on the 3D rotation results. Since the real videos we had available for further analysis in\nSect. 4.2 are based on acquisitions made with a real drone equipped with a 360\u25e6 camera, we tried to estimate the drone shaking to replicate it in a virtual environment as well. We initially estimated the 3D rotations on the frames of the real video by applying our method and assumed them to be true. Then we verified their correctness and thus of our algorithm by replicating these rotations in a simulated environment and estimating the same data another time but now in a controlled environment. In Fig. 13 are shown the 3D rotations estimated from a real video acquired from a drone.\nWe generated two virtual videos: for the first video, the position of the camera was fixed and only the rotations of Fig. 13 were applied; for the second virtual video also a camera motion in the straight forward direction with a velocity of 0,5m/s was applied. From the two arrays of frames saved, 360\u25e6 virtual videos were produced by using a V ideoWriter object in MATLAB.\nThe aforementionedmethod of Sect. 3 was applied to both videos. The first step of our method is to look for camera orientations between each pair of frames. The camera orientations found from the first video where the camera position is fixed perfectly match the input 3D angles of Fig. 13 provided to the camera in Unity. It shows the goodness of the PSO algorithm.\nFigure 14, which represents the camera orientations estimated with our method for the second video, shows a similarity with the pure rotations of Fig. 13. It proves the robustness of the PSO algorithm in estimating 3D rotations even in panoramic videos where small translations are present.\nOnce the equirectangular image sets of the two input videos were stabilized, we produced the two new stabilized videos.\nIn addition, to quantify how stabilized the new videos are, we applied the PSO algorithm to find the new camera rotations. The result for the first stabilized video is shown in Fig. 16 and for the second one in Fig. 15. How it can be seen in Figs. 16 and 15, the oscillations due to the stabilization process have been reduced by an order of magnitude, so that the final video results much smoother."
        },
        {
            "heading": "4.2 Real videos",
            "text": "In a real environment compared to a virtual one, other variables come into consideration, such as the multiple movements, occlusions, lighting changes, and non-rigid movements. In this section, we want to verify the correct performance of the implemented method with real 360\u25e6 videos.\nWe tested our method on a large number of spherical panorama videos captured with the DJI Matrice 210 RTK drone9 equipped with an Insta360 camera.\nTo show the goodness and robustness of the method proposed in this paper we analyzed an example of a short real video in theworst-case undergoing high shaking. The camera yaw, pitch, and roll trajectories before and after the stabilization process can be seen, respectively, in Figs. 17 and 18.\nAs can be seen in Fig. 18 after the stabilization process, the camera rotations are consistently less. As in the case of a virtual environment, the result is smooth enough to make viewing comfortable, even with a headset. Both videos have a frame rate of 25Hz.\nGiven the short duration of this example and the type of panoramic shot with slow intentional rotations, camera rotations are removed by comparing them to the first frame.\nThe average times per frame spent in each step of the proposed method are shown in Table 3. The tests were run on aPCwith an Intel(R)Core(TM) i7-9700KFCPU@3.60GHz processor and64GBofRAM,on2880x5760 equirectangular video frames."
        },
        {
            "heading": "5 User study",
            "text": "We tested the impact of our approach to 360\u25e6 videos presented in VR. We were interested in the capability of our method to reduce symptoms of cybersickness.\n9 https://www.dji-store.it/prodotto/dji-matrice-210-rtk-g/ [Accessed: February 2022].\nTask We designed a task, in which a user watches several 360\u25e6 videos in a VR environment. In particular, each participant was asked to watch a total of five minutes of 360\u25e6 unprocessed videos, followed by watching a total of five minutes of 360\u25e6 video that is the result from the proposed stabilization process. We scheduled a one-hour break between each session to avoid compound effects between both conditions.\nApparatus We used an Oculus Quest 1 VR Headset10 in both conditions. After completing a consent form and demographics questionnaire, the participant was introduced to the system. We recruited twenty (20) participants for the experiment. Participants include 3 females and 17males: 12 people with ages between 18 and 24 years, and 8 people with ages between 25 and 35 years; 10 people reported to have experience with VR. All participants were free from any known neurological disorders, as verified by self-report.\nProcedureAfter completing each session, the participants filled out a Simulator Sickness Questionnaire (SSQ) [35]. Participants were asked to add notes wherever they feel it is necessary.\nResults Figure 19 shows the SSQ results in terms of the scores for symptoms related to their specific aspects of nausea, oculomotor, disorientation. The values range from 0 to\n10 https://www.oculus.com/quest [Accessed: February 2022].\n3 accordingly to the effect of each item: None (0), Slight (1), Moderate (2), Severe (3).\nFurthermore, we compute a total score (TS) to represent the overall severity of cybersickness experienced by the users. We follow the approach of Walter et al. [36] for calculation T S as the weighted sum of nausea, oculomotor, and disorientation. We measure a total score of 64.70 for the raw 360\u25e6 video condition and a score of 13.65 for the stabilized 360\u25e6 video.\nDiscussion From the SSQ scores it can be noticed how the trend of all the symptoms is decreased in the sessions with stabilized videos when compared to the ones with original videos. In particular, we believe that reduced camera shaking affects the oculomotor and disorientation aspects.\nVertigo\u2019s score remains almost unchanged in both sessions due to the videos showing a panoramic video shot with a drone. The remaining general discomfort score in the stabilized video is connected with the 360\u25e6 videos which slowly rotate and translate even if the participants do not move. This is confirmed by the participants\u2019 notes. Their notes also show how all these symptoms grow over time when using the VR Headset."
        },
        {
            "heading": "6 Conclusion",
            "text": "The stabilization of a 360\u25e6 video is increasingly required because viewers, especially when it comes to immersive visual environment, are dizzy or nauseous due to shaky scenes. This paper presents a new approach to stabilize the 360\u25e6 videos affected by the problem of shaking to alleviate cybersickness. Our approach is 2.5D because it estimates 3D rotations without involving 3D structure from motion methods.\nThe method\u2019s validation was achieved using real 360\u25e6 videos captured by a drone equippedwith an omnidirectional camera during a mine overflight. Instead, the virtual videos were simulated and produced in a VR environment using Unity 3D platform. Working in a simulated virtual environment allowed us to have a reference to test the goodness of each step of our approach knowing in this context the result to achieve.\nWith the method proposed in this paper, it is possible to remove shaking fromvideoswithout knowing the initial camera position and its motion. The possibility to select the ROI area where to analyze the video and to define the length of the window to interpolate differently the camera\u2019s orientations in time makes possible the use of this method in different contexts. The selection of the ROI area allows us to stabilize also videos with static overlays, logos.\nThe periodical selection of two frames at a time avoids the possibility of accumulation errors in the estimation of camera rotation angles. The detection and tracking of inter-\nest points, respectively, with the SURF and KLT algorithm, the use of a PSO algorithm, for their matching using their descriptors similarity in the cost function and the outliers suppression through the modified Chauvenet\u2019s Criterion, makes our method accurate and more robust than other works. Finally, the time-weighted color filter applied to each frame gives the possibility to handle videos with small translational jitter, rolling shutter wobble, parallax, and lens deformation.\nThrough the user study, we proved how a good stabilization can reduce all the simulator sickness symptoms that can be summarized with the final TS value. From the user study, this value was 64.70 before the stabilization and 13.65 after.\nOne main drawback of our approach is the computation time, which makes our method suitable for non-real-time applications. In addition, our algorithm is unable to stabilize videos in which there is a predominance of translations over camera rotations. Furthermore, it is based on features tracking between frames to estimate 3D camera rotations; if light is scattered or flared in the camera lens, it can generate severe lens flare that generates inaccurate estimates leading to poor stabilization.11:\nAcknowledgements This paper was developed inside the European project MiReBooks Mixed Reality Handbooks for Mining Education, a project funded by EIT Raw Materials.\nFunding Open access funding provided by Universit\u00e0 degli Studi di Trento within the CRUI-CARE Agreement.\nDeclarations\nConflict of interest The authors declare to have no potential conflict of interest. This work was enabled by a research grant of the European Institute of Innovation and Technology (EIT) Raw Materials in the project MiReBooks: Mixed Reality Handbooks for Mining Education (18060).\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, youwill need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/.\n11 https://eitrawmaterials.eu/project/mirebooks-mixed-realityhandbooks-for-mining-education/, [Accessed February 2022]."
        }
    ],
    "title": "Stabilization of spherical videos based on feature uncertainty",
    "year": 2022
}