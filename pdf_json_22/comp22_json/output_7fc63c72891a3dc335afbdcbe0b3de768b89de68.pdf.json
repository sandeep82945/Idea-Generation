{
    "abstractText": "Deep learning-based algorithms have greatly improved the performance of remote sensing image (RSI) superresolution (SR). However, increasing network depth and parameters cause a huge burden of computing and storage. Directly reducing the depth or width of existing models results in a large performance drop. We observe that the SR difficulty of different regions in an RSI varies greatly, and existing methods use the same deep network to process all regions in an image, resulting in a waste of computing resources. In addition, existing SR methods generally predefine integer scale factors and cannot perform stepless SR, i.e., a single model can deal with any potential scale factor. Retraining the model on each scale factor wastes considerable computing resources and model storage space. To address the above problems, we propose a saliencyaware dynamic routing network (SalDRN) for lightweight and stepless SR of RSIs. First, we introduce visual saliency as an indicator of region-level SR difficulty and integrate a lightweight saliency detector into the SalDRN to capture pixel-level visual characteristics. Then, we devise a saliency-aware dynamic routing strategy that employs path selection switches to adaptively select feature extraction paths of appropriate depth according to the SR difficulty of sub-image patches. Finally, we propose a novel lightweight stepless upsampling module whose core is an implicit feature function for realizing mapping from low-resolution feature space to high-resolution feature space. Comprehensive experiments verify that the SalDRN can achieve a good trade-off between performance and complexity. The code is available at https://github.com/hanlinwu/SalDRN.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanlin Wu"
        },
        {
            "affiliations": [],
            "name": "Ning Ni"
        },
        {
            "affiliations": [],
            "name": "Libao Zhang"
        }
    ],
    "id": "SP:a2ef77a1060d987cce5bc51814e060b181ed1615",
    "references": [
        {
            "authors": [
                "Q. Wang",
                "S. Liu",
                "J. Chanussot",
                "X. Li"
            ],
            "title": "Scene classification with recurrent attention of VHR remote sensing images",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 57, no. 2, pp. 1155\u20131167, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "He",
                "Da",
                "Shi",
                "Qian",
                "Liu",
                "Xiaoping",
                "Zhong",
                "Yanfei",
                "Zhang",
                "Liangpei"
            ],
            "title": "Generating 2m fine-scale urban tree cover product over 34 metropolises in China based on deep context-aware sub-pixel mapping network",
            "venue": "Int. J. Appl. Earth Obs. Geoinf., vol. 106, p. 102667, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "Y. Zhong",
                "Z. Zheng",
                "A. Ma",
                "L. Zhang"
            ],
            "title": "RSNet: The search for remote sensing deep neural networks in recognition tasks",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 59, no. 3, pp. 2520\u20132534, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Dong",
                "C.C. Loy",
                "K. He",
                "X. Tang"
            ],
            "title": "Image super-resolution using deep convolutional networks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2, pp. 295\u2013307, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B. Lim",
                "S. Son",
                "H. Kim",
                "S. Nah",
                "K. Mu Lee"
            ],
            "title": "Enhanced deep residual networks for single image super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2017, pp. 136\u2013144.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "Y. Tian",
                "Y. Kong",
                "B. Zhong",
                "Y. Fu"
            ],
            "title": "Residual dense network for image super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2472\u20132481.",
            "year": 2018
        },
        {
            "authors": [
                "H. Chen",
                "Y. Wang",
                "T. Guo",
                "C. Xu",
                "Y. Deng",
                "Z. Liu",
                "S. Ma",
                "C. Xu",
                "C. Xu",
                "W. Gao"
            ],
            "title": "Pre-trained image processing transformer",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 12 299\u201312 310.",
            "year": 2021
        },
        {
            "authors": [
                "X. Hu",
                "H. Mu",
                "X. Zhang",
                "Z. Wang",
                "T. Tan",
                "J. Sun"
            ],
            "title": "Meta-SR: A magnification-arbitrary network for super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 1575\u20131584.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chen",
                "S. Liu",
                "X. Wang"
            ],
            "title": "Learning continuous image representation with local implicit image function",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 8628\u20138638.",
            "year": 2021
        },
        {
            "authors": [
                "P. Behjati",
                "P. Rodriguez",
                "A. Mehri",
                "I. Hupont",
                "C.F. Tena",
                "J. Gonzalez"
            ],
            "title": "Overnet: Lightweight multi-scale super-resolution with overscaling network",
            "venue": "Proc. IEEE Winter Conf. Appl. Comput. Vis., January 2021, pp. 2694\u20132703.",
            "year": 2021
        },
        {
            "authors": [
                "C. Dong",
                "C.C. Loy",
                "X. Tang"
            ],
            "title": "Accelerating the super-resolution convolutional neural network",
            "venue": "Proc. Eur. Conf. Comput. Vis. Springer, 2016, pp. 391\u2013407.",
            "year": 2016
        },
        {
            "authors": [
                "W. Shi",
                "J. Caballero",
                "F. Husz\u00e1r",
                "J. Totz",
                "A.P. Aitken",
                "R. Bishop",
                "D. Rueckert",
                "Z. Wang"
            ],
            "title": "Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 1874\u20131883.",
            "year": 2016
        },
        {
            "authors": [
                "W.-S. Lai",
                "J.-B. Huang",
                "N. Ahuja",
                "M.-H. Yang"
            ],
            "title": "Deep laplacian pyramid networks for fast and accurate super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 624\u2013632.",
            "year": 2017
        },
        {
            "authors": [
                "N. Ahn",
                "B. Kang",
                "K.-A. Sohn"
            ],
            "title": "Fast, accurate, and lightweight super-resolution with cascading residual network",
            "venue": "Proc. Eur. Conf. Comput. Vis., 2018, pp. 252\u2013268.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hui",
                "X. Gao",
                "Y. Yang",
                "X. Wang"
            ],
            "title": "Lightweight image superresolution with information multi-distillation network",
            "venue": "Proc. ACM Int. Conf. Multimedia, 2019, pp. 2024\u20132032.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Hui",
                "X. Wang",
                "X. Gao"
            ],
            "title": "Fast and accurate single image superresolution via information distillation network",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., June 2018, pp. 723\u2013731.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhao",
                "X. Kong",
                "J. He",
                "Y. Qiao",
                "C. Dong"
            ],
            "title": "Efficient image super-resolution using pixel attention",
            "venue": "Proc. Eur. Conf. Comput. Vis. Springer, 2020, pp. 56\u201372.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tai",
                "J. Yang",
                "X. Liu"
            ],
            "title": "Image super-resolution via deep recursive residual network",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 3147\u20133155.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Li",
                "J. Yang",
                "Z. Liu",
                "X. Yang",
                "G. Jeon",
                "W. Wu"
            ],
            "title": "Feedback network for image super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3867\u20133876.",
            "year": 2019
        },
        {
            "authors": [
                "C. Tian",
                "Y. Xu",
                "W. Zuo",
                "B. Zhang",
                "L. Fei",
                "C.-W. Lin"
            ],
            "title": "Coarse-tofine CNN for image super-resolution",
            "venue": "IEEE Trans. Multimedia, vol. 23, pp. 1489\u20131502, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Chu",
                "B. Zhang",
                "H. Ma",
                "R. Xu",
                "Q. Li"
            ],
            "title": "Fast, accurate and lightweight super-resolution with neural architecture search",
            "venue": "Proc. Int. Conf. on Pattern Recognit., 2021, pp. 59\u201364.",
            "year": 2021
        },
        {
            "authors": [
                "X. Hu",
                "H. Mu",
                "X. Zhang",
                "Z. Wang",
                "T. Tan",
                "J. Sun"
            ],
            "title": "Meta-SR: A magnification-arbitrary network for super-resolution",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 1575\u20131584.",
            "year": 2019
        },
        {
            "authors": [
                "L. Wang",
                "Y. Wang",
                "Z. Lin",
                "J. Yang",
                "W. An",
                "Y. Guo"
            ],
            "title": "Learning a single network for scale-arbitrary super-resolution",
            "venue": "Proc. Int. Conf. Comput. Vis., 2021, pp. 4781\u20134790.",
            "year": 2021
        },
        {
            "authors": [
                "X. Xu",
                "Z. Wang",
                "H. Shi"
            ],
            "title": "UltraSR: Spatial encoding is a missing key for implicit image function-based arbitrary-scale super-resolution",
            "venue": "arXiv preprint arXiv:2103.12716, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Romano",
                "J. Isidoro",
                "P. Milanfar"
            ],
            "title": "Raisr: rapid and accurate image super resolution",
            "venue": "IEEE Trans. Comput. Imaging, vol. 3, no. 1, pp. 110\u2013 125, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "X. Wang",
                "K. Yu",
                "C. Dong",
                "C.C. Loy"
            ],
            "title": "Recovering realistic texture in image super-resolution by deep spatial feature transform",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 606\u2013615. 16",
            "year": 2018
        },
        {
            "authors": [
                "K. Yu",
                "C. Dong",
                "L. Lin",
                "C.C. Loy"
            ],
            "title": "Crafting a toolchain for image restoration by deep reinforcement learning",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2443\u20132452.",
            "year": 2018
        },
        {
            "authors": [
                "K. Yu",
                "X. Wang",
                "C. Dong",
                "X. Tang",
                "C.C. Loy"
            ],
            "title": "Pathrestore: Learning network path selection for image restoration",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2021. [Online]. Available: https://doi.org/10.1109/TPAMI.2021.3096255",
            "year": 2021
        },
        {
            "authors": [
                "C.D. Gilbert",
                "M. Sigman"
            ],
            "title": "Brain states: top-down influences in sensory processing",
            "venue": "Neuron, vol. 54, no. 5, pp. 677\u2013696, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "F. Zhang",
                "B. Du",
                "L. Zhang"
            ],
            "title": "Saliency-guided unsupervised feature learning for scene classification",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 53, no. 4, pp. 2175\u20132184, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Chen",
                "C. Fu",
                "Y. Zhao",
                "F. Zheng",
                "J. Song",
                "R. Ji",
                "Y. Yang"
            ],
            "title": "Salience-guided cascaded suppression network for person re-identification",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 3297\u20133307.",
            "year": 2020
        },
        {
            "authors": [
                "J. Ma",
                "H. Wu",
                "J. Zhang",
                "L. Zhang"
            ],
            "title": "SD-FB-GAN: Saliency-driven feedback gan for remote sensing image super-resolution reconstruction",
            "venue": "Proc. Int. Conf. Image Process., 2020, pp. 528\u2013532.",
            "year": 2020
        },
        {
            "authors": [
                "H. Wu",
                "L. Zhang",
                "J. Ma"
            ],
            "title": "Remote sensing image super-resolution via saliency-guided feedback gans",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1\u201316, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Lei",
                "Z. Shi",
                "Z. Zou"
            ],
            "title": "Super-resolution for remote sensing images via local\u2013global combined network",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 14, no. 8, pp. 1243\u20131247, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.M. Haut",
                "R. Fernandez-Beltran",
                "M.E. Paoletti",
                "J. Plaza",
                "A. Plaza",
                "F. Pla"
            ],
            "title": "A new deep generative network for unsupervised remote sensing single-image super-resolution",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 56, no. 11, pp. 6792\u20136810, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Pan",
                "W. Ma",
                "J. Guo",
                "B. Lei"
            ],
            "title": "Super-resolution of single remote sensing image based on residual dense backprojection networks",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 57, no. 10, pp. 7918\u20137933, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Lei",
                "Z. Shi",
                "Z. Zou"
            ],
            "title": "Coupled adversarial training for remote sensing image super-resolution",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 58, no. 5, pp. 3633\u20133643, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Li",
                "Q. Wang",
                "X. Li"
            ],
            "title": "Exploring the relationship between 2d/3d convolution for hyperspectral image super-resolution",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 59, no. 10, pp. 8693\u20138703, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Hou",
                "Biao",
                "Zhou",
                "Kang",
                "Jiao",
                "Licheng"
            ],
            "title": "Adaptive Super- Resolution for Remote Sensing Images Based on Sparse Representation With Global Joint Dictionary Model",
            "venue": "IEEE Geosci. Remote Sens. Lett., vol. 56, no. 4, pp. 2312\u20132327, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Zhang",
                "Haopeng",
                "Wang",
                "Pengrui",
                "Jiang",
                "Zhiguo"
            ],
            "title": "Nonpairwise- Trained Cycle Convolutional Neural Network for Single Remote Sensing Image Super-Resolution",
            "venue": "IEEE Geosci. Remote Sens. Lett., vol. 59, no. 5, pp. 4250\u20134261, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Chen",
                "Long",
                "Liu",
                "Hui",
                "Yang",
                "Minhang",
                "Qian",
                "Yurong",
                "Xiao",
                "Zhengqing",
                "Zhong",
                "Xiwu"
            ],
            "title": "Remote Sensing Image Super- Resolution via Residual Aggregation and Split Attentional Fusion Network",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 9546\u20139556, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Shao",
                "Zhenfeng",
                "Wang",
                "Lei",
                "Wang",
                "Zhongyuan",
                "Deng",
                "Juan"
            ],
            "title": "Remote Sensing Image Super-Resolution Using Sparse Representation and Coupled Sparse Autoencoder",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 12, no. 8, pp. 2663\u20132674, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Dong",
                "X. Sun",
                "X. Jia",
                "Z. Xi",
                "L. Gao",
                "B. Zhang"
            ],
            "title": "Remote sensing image super-resolution using novel dense-sampling networks",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 59, no. 2, pp. 1618\u20131633, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Zhang",
                "Q. Yuan",
                "J. Li",
                "J. Sun",
                "X. Zhang"
            ],
            "title": "Scene-adaptive remote sensing image super-resolution using a multiscale attention network",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 58, no. 7, pp. 4764\u20134779, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Zhang",
                "J. Shao",
                "X. Li",
                "H.T. Shen"
            ],
            "title": "Remote sensing image superresolution via mixed high-order attention network",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 59, no. 6, pp. 5183\u20135196, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Wang",
                "T. Zhou",
                "Y. Lu",
                "H. Di"
            ],
            "title": "Contextual transformation network for lightweight remote sensing image super-resolution",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 60, no. 5615313, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Wang",
                "Zheyuan",
                "Li",
                "Liangliang",
                "Xue",
                "Yuan",
                "Jiang",
                "Chenchen",
                "Wang",
                "Jiawen",
                "Sun",
                "Kaipeng",
                "Ma",
                "Hongbing"
            ],
            "title": "FeNet: Feature Enhancement Network for Lightweight Remote-Sensing Image Super- Resolution",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1\u201312, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Ni",
                "H. Wu",
                "L. Zhang"
            ],
            "title": "Hierarchical feature aggregation and self-learning network for remote sensing image continuous-scale superresolution",
            "venue": "IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1\u20135, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wu",
                "K. He"
            ],
            "title": "Group normalization",
            "venue": "Proc. Eur. Conf. Comput. Vis., 2018, pp. 3\u201319.",
            "year": 2018
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7132\u20137141.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Chen",
                "H. Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 5939\u20135948.",
            "year": 2019
        },
        {
            "authors": [
                "V. Sitzmann",
                "M. Zollh\u00f6fer",
                "G. Wetzstein"
            ],
            "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Proc. Eur. Conf. Comput. Vis. Springer, 2020, pp. 405\u2013 421.",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014. [Online]. Available: https: //arxiv.org/abs/1412.6980",
            "year": 2014
        },
        {
            "authors": [
                "X. Luo",
                "Y. Xie",
                "Y. Zhang",
                "Y. Qu",
                "C. Li",
                "Y. Fu"
            ],
            "title": "Latticenet: Towards lightweight image super-resolution with lattice block",
            "venue": "Proc. Eur. Conf. Comput. Vis. Springer, 2020, pp. 272\u2013289.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Remote sensing, super-resolution, saliency analysis, lightweight, stepless\nI. INTRODUCTION\nS INGLE image super-resolution (SISR) aims to recover ahigh-resolution (HR) image with clear details from its low-resolution (LR) version. SISR is a basic task of remote sensing image (RSI) processing and is helpful for subsequent interpretation tasks such as scene classification [1], fine-scale land cover classification [2], object recognition [3], and change detection [4]. With the rapid development of deep learning technology, SISR methods based on convolutional neural networks (CNNs) have made significant progress in recent years.\nEarly CNN-based SISR algorithms built shallow neural networks to learn the mapping from LR to HR images. With the progress of research, although the performance of\nThis work was supported in part by the Beijing Natural Science Foundation under Grant 4222046, in part by the National Natural Science Foundation of China under Grant 62271060, Grant 61571050 and Grant 41771407. (Corresponding author: Libao Zhang.)\nThe authors are with the School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China. (e-mail: libaozhang@bnu.edu.cn).\nmodels has been dramatically improved, the network depth has increased, as has the parameter quantity and computational complexity. For example, the super-resolution convolutional neural network (SRCNN) [5] proposed in 2015 contains only three layers and 57K parameters, the enhanced deep superresolution network (EDSR) [6] proposed in 2017 contains 68 layers and 43M parameters, the residual dense network (RDN) [7] proposed in 2018 contains 150 layers and 23M parameters, and the state-of-the-art (SOTA) transformer-based SR network [8] has more than 115M parameters. The ever-increasing computational complexity consumes an increasing number of computing resources, requiring models to be deployed on highperformance computing platforms.\nRSIs have the characteristics of large image size and massive data and face a contradiction between high-speed data acquisition and low-speed interpretation. The traditional RSI processing mode transmits massive data back to the ground and then interprets it, which requires high data transmission bandwidth and cost. Direct on-satellite image processing becomes a pressing need because of the ability to respond quickly to user needs and provide real-time feedback. However, the storage and computing capabilities of on-board devices are usually limited, which puts forward new requirements for realtime and high-efficiency algorithms. Therefore, improving the efficiency of the algorithm while maintaining performance is an essential social requirement in the application of remote sensing. Furthermore, SR is an important preprocessing step that helps improve the performance of subsequent tasks. To apply the RSI SR methods to a broader range of practical\nar X\niv :2\n21 0.\n07 59\n8v 1\n[ cs\n.C V\n] 1\n4 O\nct 2\n02 2\n2 scenarios such as on-board computing, it is necessary to build lightweight RSI SR models.\nMost existing SR algorithms were designed to deal with a single and fixed scale factor. Stepless SR aims to superresolve images with arbitrary integer/non-integer scale factors using a single model. Building a stepless SR model that can deal with multiple scale factors simultaneously has attracted researchers\u2019 interest because it can dramatically save model training and storage resources. Hu et al. [9] proposed a metalearning-based scale-arbitrary SR (Meta-SR) method, which is a pioneering work on the stepless SR task. Subsequently, Chen et al. [10] introduced local implicit image functions (LIIFs) to improve the performance of stepless SR. However, both of the above models are designed based on heavyweight networks. Behjati et al. [11] proposed an overscaling network (OverNet) to reduce the complexity of stepless SR. However, OverNet still has too many parameters, and there is a certain gap in performance compared with the previous fixed-scale SR models. Therefore, it is necessary to design a new stepless upsampling scheme to meet the needs of lightweight SR models.\nPrevious lightweight SR models [12]\u2013[16] mainly focused on designing a lightweight network structure. They use the same deep network to process all regions in the image, ignoring the fact that the SR difficulty of different regions in an image varies greatly, resulting in a waste of computing resources. We observe that in RSIs, regions with smooth textures are very easy to super-resolve, and even simple bicubic interpolation can yield satisfactory results. For regions with complex textures, deeper networks are required to obtain good SR results. Fig. 1 (a) shows an RSI from the GeoEye1 satellite, and Fig. 1 (b)-(d) are HR image patches, bicubic interpolation results, and SR results of the cascading residual network (CARN) [15], respectively. The first row of Fig. 1 shows a smooth region, and it can be seen that the result of bicubic interpolation is similar to CARN. The last row of Fig. 1 shows a textured region, where bicubic interpolation can hardly recover texture details, and it needs to rely on deep CARN to obtain sharp details. The SR difficulty of the middle row is moderate. This indicates that if different depth networks can be used for feature extraction in different regions of the image, then the computational complexity of the network will be significantly reduced. To achieve this goal, we introduce visual saliency as a guideline for the SR difficulty of image patches and propose a saliency-aware dynamic routing strategy.\nIn this article, we propose a novel saliency-aware dynamic routing network (SalDRN) for lightweight and stepless SR. The proposed SalDRN integrates a lightweight saliency detector and a dynamic routing strategy for efficient feature extraction. We employ visual saliency as a guideline of SR difficulty and choose a feature extraction path of appropriate depth for each sub-image patch, thereby avoiding the waste of computational resources. In addition, we devise a lightweight stepless upsampling module (LSUM) to achieve scale-arbitrary SR within a single model. In the LSUM, we first generate progressive resolution feature maps to approximate a continuous representation of an image via the cascaded frequency\nenhancement block (CFEB), which uses group convolution and channel splitting to reduce computational complexity. Then, we design an efficient implicit feature function (IFF) and combine it with a scale-aware pixel attention (SAPA) mechanism to learn the mapping from the LR to the HR feature space. SAPA can effectively enhance the expression capacity of IFFs and reduce the computational complexity of the upsampling module.\nThe main contributions of this article are as follows:\n1) We propose a saliency-aware dynamic routing strategy that can automatically select a feature extraction path with an appropriate depth according to the SR difficulty of the image patch. This reduces the computational complexity while maintaining the performance. 2) We propose a novel LSUM that combines CFEB and IFF to efficiently map LR features to HR features with arbitrary scale factors. We also devise an SAPA mechanism to enhance the expression capacity of the IFF. 3) We propose a novel SalDRN for the lightweight and stepless SR of RSIs. The number of FLOPs of our proposal is only 22% of the CFSRCNN with similar performance. Compared with the SOTA stepless upsampling module LIIF, our proposed LSUM has a 98% reduction of FLOPs and an 80% reduction in the number of parameters, achieving comparable performance."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": ""
        },
        {
            "heading": "A. Lightweight SR",
            "text": "SR is a typical computing-intensive task, so building a lightweight SR model to save computational resources has attracted widespread attention from researchers.\nDong et al. [12] and Shi et al. [13] proposed placing the upsampling module at the end of the network so that feature extraction is performed in the LR space, and the computational complexity can be effectively reduced. Lai et al. [14] introduced a deep Laplacian pyramid network to reconstruct HR images at multiple pyramid levels progressively. Ahn et al. [15] introduced the group convolution and proposed the CARN to reduce the computational burden of convolution layers in the feature extraction part. Hui et al. [16], [17] proposed an information distillation paradigm using a channel splitting operation to extract multilevel features and then aggregate them to improve the capacity of lightweight networks. Zhao et al. [18] proposed a pixel attention mechanism to improve the performance of lightweight SR networks. Tai et al. [19] and Li et al. [20] adopt a recurrent structure to reduce the number of parameters by weight sharing, but they cannot reduce the computational complexity in the testing phase. Tian et al. [21] proposed a coarse-to-fine super-resolution convolutional neural network (CFSRCNN) that combined long and short path features to prevent performance degradation, and proposed a feature fusion schema to improve the model efficiency. Chu et al. [22] used neural architecture search technology to automatically find an effective feature extraction structure and neuron connections for the SR task.\n3 The above algorithms have greatly reduced the model complexity, but there is still a large gap between the performance of these lightweight models and the SOTA SR models due to the significant decrease in the number of parameters and computational complexity."
        },
        {
            "heading": "B. Stepless SR",
            "text": "Most existing SR algorithms need to train and store a model for each scale factor. Considering the model flexibility and the cost of training and storage, research on stepless SR is gaining popularity.\nMeta-SR [23] is a pioneering work on stepless SR that proposes a meta-upscale module for arbitrary scale factor upsampling. The meta-upscale module projects the coordinates of the HR space into the LR space through a many-to-one mapping and then utilizes a fully connected network to dynamically predict the filter weights to upsample each scale factor. The coordinate projection operation makes a single feature vector in the LR space responsible for generating a small patch of the SR image. Two adjacent pixels in the SR result may not belong to the same image patch, so the discontinuous image representation will cause unpleasant checkerboard artifacts when the LR feature vector is switched. To solve the problem of checkerboard artifacts, Chen et al. [10] introduced LIIF and a local ensemble technique to obtain a continuous representation of an image. However, the local ensemble strategy creates an additional computational burden because repeated predictions need to be made with different latent codes. Behjati et al. [11] proposed an overscaling module (OSM) that uses overscaled feature maps for stepless upsampling. However, OSM also has considerable computational complexity due to the generation of the overscaled feature maps, and it can only perform SR with a scale factor below the predefined maximum value. Wang et al. [24] proposed a scale-aware upsampling layer that first achieves asymmetric SR. Xu et al. [25] introduced spatial encoding to address the structural distortions problem in the LIIF.\nAlthough the above methods improved the performance of stepless SR, they still face a high computational burden and are challenging to deploy and apply."
        },
        {
            "heading": "C. Region-Aware SR",
            "text": "Some researchers adopted different processing strategies for regions with different characteristics in the image. Romano et al. [26] proposed a rapid and accurate image super-resolution (RAISR) algorithm, dividing image patches into clusters and constructing appropriate filters for each cluster. RAISR adopts an efficient hashing algorithm to reduce the complexity of clustering. Wang et al. [27] introduced a spatial feature transformation layer combined with high-level semantic information to provide different priors for different regions. Yu et al. [28], [29] proposed decomposing the input image into sub-patches and implementing network path selection through reinforcement learning.\nInspired by the human visual attention mechanism [30], some researchers introduced saliency analysis to guide computer vision tasks [31], [32] . In the image, the saliency of\na region is the state or quality by which it stands out from its neighbors. Ma et al. [33] and Wu et al. [34] proposed a saliency-guided feedback generative adversarial network (SGFBGAN) that uses a saliency map as an indicator of texture complexity, so different reconstruction principles can be applied to restore areas with varying saliency levels. However, SG-FBGAN requires an external saliency detection algorithm and thus cannot achieve end-to-end model training.\nAlthough these algorithms considered the diverse reconstruction needs of different regions in the image, they did not take advantage of this feature to speed up the model."
        },
        {
            "heading": "D. SR for RSIs",
            "text": "Most of the early CNN-based SR algorithms for RSIs are improvements of SRCNN [5] or EDSR [6], and various network structures are designed to extract the features of RSIs more effectively. Lei et al. [35] proposed a local-global combined network designed with a multifork structure to learn multilevel representations of RSIs. Haut et al. [36] proposed an unsupervised convolutional generative model that learns relationships between the LR and HR domains throughout several convolutional, downsampling, batch normalization, and activation layers. Later, many researchers considered the characteristics of RSIs to design the network structure [37]\u2013[39]. Hou et al. [40] proposed an effective method for RSI SR based on sparse representation. They introduced a global selfcompatibility model for global regularization and improved the performance of the model by integrating the sparse representation and the local and nonlocal constraints. Zhang et al. [41] proposed a new cycle convolutional neural network (Cycle-CNN), which can be trained with unpaired data. The Cycle-CNN solves the problem in which paired HR and LR RSIs are actually difficult to acquire. Chen et al. [42] proposed a residual aggregation and split attentional fusion network that uses a basic split-fusion mechanism to achieve cross-channel feature group interaction, allowing the method to adapt to various land surface scene reconstructions. Shao et al. [43] proposed a coupled sparse autoencoder to effectively learn the mapping relation between LR and HR images. Dong et al. [44] developed a dense-sampling SR network (DSSR) to explore the large-scale SR of RSIs. DSSR incorporates a wide activation and attention mechanism to enhance the representation ability of the network. Recently, some studies [45], [46] introduced the attention mechanism into the RSI SR task to improve the expression ability of networks.\nAt present, few scholars have considered building a lightweight SR model for RSIs. Wang et al. [47] proposed a lightweight contextual transformation network (CTN) by replacing the classical 3\u00d73 convolutional layer with lightweight contextual transformation layers. Wang et al. [48] proposed a lightweight feature enhancement network for accurate RSI SR. They designed a lightweight lattice block as a nonlinear feature extraction function to improve the expression ability and enabled the upper and lower branches to communicate efficiently by using the attention mechanism.\nResearch on the stepless SR of RSIs started relatively late. To the best of our knowledge, only the model in [49] considers\n4 Saliency map\nFRU\nFR U\nFR U\nDynamic Routing Module\nSaliency DetectorLR\nLC SU\nM\nSR with any scale factor\nCombination\nLR\u2191\nSaliency Detection\nElement-wise addition LR\u2191 Bicubic upsampled LR Decomposition Combination\nD o\nw n\nsa m\np lin\ng\nR es\nB lo\nck\nU p\nsa m\np lin g U p sa m p lin g C o n v( 1 ) C o n v( 1 ) C o n v( 3 ) C o n v( 3 ) Si gm o id Si gm o id\nPath selection switch Skip connection\nLightweight Stepless Upsampling Module\nG ro\nu p\nC o\nn v\nG ro\nu p\nC o\nn v\nP ix\nel Sh\nu ff le P ix el Sh u ff le G ro u p C o n v P ix el Sh u ff le C h an n el S\np lit\nC h\nan n\nel S\np lit\nG ro\nu p\nC o\nn v\nG ro\nu p\nC o\nn v\nP ix\nel Sh\nu ff le P ix el Sh u ff le G ro u p C o n v P ix el Sh u ff le\nC h\nan n\nel S\np lit\nG ro\nu p\nC o\nn v\nG ro\nu p\nC o\nn v\nP ix\nel Sh\nu ff le P ix el Sh u ff le G ro u p C o n v P ix el Sh u ff\nle IF F\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nR eL U R eL U\nC o\nn v\n(1 )\nC o\nn v\n(1 )\nC o\nn v\n(3 )\nR eL\nU\nC o\nn v\n(1 )\nC o\nn v(\n3 )\nC o\nn v(\n3 )\nC o\nn v\n(1 )\nC o\nn v\n(1 )\nIM D B IM D B\nIM D B IM D B\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nCFEB\nFig. 2. Framework of the proposed SalDRN. The downsampling layer is a 3 \u00d7 3 convolutional layer with a stride of 2, and upsampling layer is a nearest interpolation followed by a 3\u00d7 3 convolutional layer; \u201cConv(k)\u201d represent a k \u00d7 k convolutional layer.\nthe stepless SR of RSIs. Ni et al. [49] constructed a selflearning network by aggregating hierarchical features but did not consider the issue of model lightweighting."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": "First, we provide an overview of our proposed SalDRN in Sec. III-A. We then introduce the saliency detector and dynamic routing strategy in Sec. III-B and Sec. III-C, respectively. Finally, in Sec. III-E, we detail the loss functions of the saliency detector and the SR network."
        },
        {
            "heading": "A. Overview",
            "text": "We aim to offer networks of different complexities for image patches with different saliency levels. To obtain an end-toend network without relying on external saliency detection algorithms, we propose integrating a saliency detector into the SR framework. The unified framework makes multi-task joint learning possible. The framework of the proposed SalDRN is shown in Fig. 2 and consists of three main modules: saliency detector, dynamic routing module (DRM) for feature learning, and LSUM for upsampling with any scale factor.\nLet ILR denote the input LR image. First, the saliency detector generates a pixel-wise saliency map Isal for ILR. Then, we decompose the input LR image ILR and the saliency map Isal into N overlapping sub-patches of the same size and denote them as {P iLR}Ni=1 and {P isal}Ni=1, respectively. After that, the sub-patches and their corresponding saliency maps are sent to the DRM for high-level feature extraction. The DRM contains K path selection switches, each of which controls the feature map flow to or bypasses the next feature extraction block according to the average saliency value. Let {F iDRM}Ni=1 denote the feature maps output by the DRM, which are then recombined into a large feature map of the same size as ILR.\nFinally, we send the recombined high-level feature maps into LSUM to obtain the scale-arbitrary SR results ISR."
        },
        {
            "heading": "B. Saliency Detector",
            "text": "In RSIs, manufactured objects such as residential areas are very different from background areas in terms of spectral information, texture richness, and boundary shape. Saliency analysis aims to simulate the human visual perception system, which can quickly select the most attractive and discriminative areas from a large-scale scene. Therefore, we propose using saliency maps to guide the path selection in the subsequent SR network. Then, we can use varying depth networks to superresolve regions with different visual features. This strategy can meet the different construction needs of different regions and save computing resources.\nThe existing algorithms proposed for the saliency detection task usually have high computational complexity due to the pursuit of high detection precision. However, our core task is saliency-aware SR, which only requires coarse region-level saliency information instead of fine saliency maps. Therefore, we designed an extremely lightweight saliency detector so that it can be integrated into the SR framework, and the additional computational burden is negligible.\nThe saliency detector uses an encoder-decoder architecture, as shown in Fig. 3. The left side of the network can be regarded as an encoder, and the right side can be regarded as a decoder. The encoder contains three residual blocks. Before each residual block, a downsampling layer is added to reduce the spatial resolution of the feature maps. The downsampling operation can effectively increase the receptive field of the shallow model and reduce the spatial dimension of the convolution operation, thus improving the computational efficiency. The downsampling layer is implemented by a 3\u00d7 3 convolutional\n5 C o n ca t\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nG N LR eL\nU\nC o\nn v\n(3 )\nSaliency mapLR\nSi gm\no id Si gm o id\nD o\nw n\nsa m\np lin\ng\nR es\nB lo ck R es B lo ck\nD o\nw n\nsa m\np lin\ng\nR es\nB lo\nck\nD o\nw n\nsa m\np lin\ng\nR es\nB lo ck R es B lo ck\nD o\nw n\nsa m\np lin\ng\nR es\nB lo\nck\nD o\nw n\nsa m\np lin\ng\nR es\nB lo ck R es B lo ck\nD o\nw n\nsa m\np lin\ng\nR es\nB lo\nck\nC o\nn v(\n1 )\nC o\nn v(\n1 )\nC o\nn v(\n3 )\nC o\nn v(\n3 )\nU p\nsa m\np lin g U p sa m p lin g\nFig. 3. The architecture of the saliency detector, where \u201cConv(k)\u201d denotes the k \u00d7 k convolutional layer.\nlayer with a stride of 2. Specifically, the residual block is implemented with six consecutive operations and a skip connection: Conv(3) \u2192 LeakyReLU \u2192 GroupNormalization \u2192 Conv(3) \u2192 LeakyReLU \u2192 GroupNormalization. We use group normalization [50] to stabilize the training. Let RB denote the operation of the residual block, and let D denote the downsampling layer. Then, the outputs of the encoder {F kenc}3k=1 can be calculated as\nF 1enc = RB(D(ILR)), F kenc = RB(D(F k\u22121enc )), k \u2208 {2, 3}.\n(1)\nSince coarse saliency maps are sufficient in our saliencyaware SR task, we design a very simple decoder to save computing resources. The decoder uses the resampling layer to adjust the size of multi-scale feature maps {Fk}3k=1 to the input image size and concatenates them. The resampling layer uses the efficient nearest interpolation. Then, we use a 1\u00d71 convolutional layer to compress the concatenated feature maps. Finally, saliency maps are obtained through a 3 \u00d7 3 convolutional layer and a sigmoid activation function. The decoder proceeds as:\nIsal = \u03c3 ( Conv3,1 ( Conv1,m([F 1\u2191 enc, F 2\u2191 enc, F 3\u2191 enc] ) ) ) , (2)\nwhere Convk,m denotes an k \u00d7 k convolutional layer with m channels, F k\u2191enc denotes the nearest upsampling result of F k enc, and [ \u00b7 ] denotes the concatenation operation."
        },
        {
            "heading": "C. Dynamic Routing Module (DRM)",
            "text": "The DRM aims to dynamically select the feature extraction path according to the saliency and reconstruction difficulty of image patches. Below, we introduce the dynamic routing architecture and then introduce the basic feature refinement unit (FRU).\n1) Dynamic Routing Strategy: The DRM takes the paired LR patches and their corresponding saliency maps {(P iLR, P isal)}Ni=1 as inputs, and contains several stacked feature refinement units (FRU) to gradually refine the coarse lowresolution feature maps. Each FRU can directly access the original shallow feature maps and further refine the output of the previous FRU. There is a path selection switch in front of\nFRU FRU FRU\nFRU FRU FRU\nFRU FRU FRU\nFRU FRU FRU\n(a)\n(b)\n(c)\n(d)\nFig. 4. Different feature extraction paths of DRM.\nFRU\nCo nv\n( 1) Co nv ( 1) IM D B IM D B IM D B IM D B IM D B IM D B\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nIMDB\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nC o\nn ca t C o n ca t\nC C\nA L\na ye r C C A L a ye r\nC o\nn v\n(1 )\nC o\nn v\n(1 )\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nC o\nn v\n(3 )\nC h\nan n\ne l S\np li\nt\nFig. 5. Illustration of the FRU.\neach FRU that decides whether to enter the next FRU or skip all subsequent FRUs according to the average saliency value of the image patch. In our experiments, all FRUs share the same structure and parameters. We discussed the parameterunshared DRM in Sec. V-D.\nSpecifically, assuming that the DRM contains K FRUs, the passing threshold of the kth path selection switch is \u03b8k. The operation after each path selection can be summarized as\nf (k)(\u00b7) = { I(\u00b7), if mean(Psal) \u2264 \u03b8k FRU([ \u00b7 , Fshallow]), else\n(3)\nwhere I(\u00b7) denotes the identity mapping, and FRU(\u00b7) represents the operation of the FRU. Suppose the sequence {\u03b8k}Kk=1 is monotonically increasing and there are K+1 possible patches in DRM, as shown in Fig. 4.\n2) FRU: The architecture of the FRU is shown in Fig. 5. We choose the information multi-distillation block (IMDB) [16] as\n6 G ro u p C o n v (3 ) G ro u p C o n v (3 ) P ix el Sh u ff le P ix el Sh u ff le G ro u p C o n v (3 ) P ix el Sh u ff le G ro u p C o n v (3 ) G ro u p C o n v (3 ) P ix el Sh u ff le P ix el Sh u ff le G ro u p C o n v (3 ) P ix el Sh u ff le\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nR eL U R eL U\nC o\nn v\n(1 )\nC o\nn v\n(1 )\nC o\nn v\n(3 )\nR eL\nU\nC o\nn v\n(1 )\nC o\nn v\n(3 )\nC o\nn v\n(3 )\nP ix\nel Sh\nu ff le P ix el Sh u ff le C o n v (3 ) P ix el Sh u ff le\nSR with any scale factor\nr\nCFEB\nSAPA\nIFF\nFig. 6. Illustration of the LSUM, where \u201cGroupConv(k)\u201d denotes the k \u00d7 k group convolutional layer.\nthe basic feature extraction block due to its superiority in previous lightweight SR tasks. The FRU contains a global residual connection from start to end and contains D multiple stacked IMDBs. The input feature maps are sequentially refined by IMDBs, and then all intermediate outputs are integrated using a 1\u00d7 1 convolutional layer followed by a 3\u00d7 3 convolutional layer. The procedure can be summarized as follows:\nHd = IMDBd(Hd\u22121), d = 1, \u00b7 \u00b7 \u00b7 , D Hout = Conv3,C ( Conv1,C([H1, H2, \u00b7 \u00b7 \u00b7 , HD]) ) +H0, (4)\nwhere H0 and Hout represent the input and output of FRU, respectively. IMDB(\u00b7) denotes the operation of IMDB.\nIn each IMDB, we progressively refine high-level features using four convolutional layers, as shown in the lower part of Fig. 5. After each convolutional layer (except the last layer), a channel-splitting operation decomposes the feature maps into two parts. One part of the refined features is kept, and the other part is processed in the next convolutional layer. Let Gin denote the input of IMDB. Then, the dth operation of IMDB can be calculated as\nGdrefined 1, G d coarse 1 = Split(Conv3,C(Gin)), Gdrefined 2, G d coarse 2 = Split(Conv3,C(G d coarse 1), Gdrefined 3, G d coarse 3 = Split(Conv3,C(G d coarse 2),\nGdrefined 4 = Conv3,C/4(G d coarse 3),\n(5)\nwhere Split(\u00b7) represents the channel-splitting operation, and Gdrefined i represents the ith refined feature maps with channel number C/4. Each convolution operation Conv3,C(\u00b7) contains a leaky rectified linear unit (LReLU) as the activation function. Then, the four refined feature maps are concatenated:\nGddistilled = [G d refined 1, G d refined 2, G d refined 3, G d refined 4], (6)\nand the result Gddistilled is fed into the contrast-aware channel attention (CCA) layer. Finally, a 1 \u00d7 1 convolutional layer is adopted to facilitate the flow of information across channels, and the result is then added to the original input feature to obtain the output of IMDB.\nThe CCA layer is an improvement of the squeezing and excitation module (SE-Block) [51], replacing the global average pooling with a global contrast information function. This\npreserves more low-level visual information such as structure, texture, and edges for SR tasks.\nLet xc \u2208 R1\u00d7H\u00d7W denote the cth channel of the feature maps X \u2208 RC\u00d7H\u00d7W , where (H,W ) represents the height and width of the image. The global contrast information function fGC is defined as follows:\nzc =fGC(xc)\n= \u221a\u221a\u221a\u221a\u221a 1 HW \u2211 (i,j)\u2208xc ( xi,jc \u2212 1 HW \u2211 (i,j)\u2208xc xi,jc )2\n+ 1\nHW \u2211 (i,j)\u2208xc xi,jc .\n(7)\nWe compress the input feature maps X into a vector Z = [z1, z2, \u00b7 \u00b7 \u00b7 , zC ] \u2208 RC\u00d71\u00d71 using (7). Then, a three-layer MLP is adopted to obtain channel-wise attention weights. The numbers of neurons in the hidden layer and output layer are C/16 and C, respectively. Finally, attention weights are normalized to [0, 1] using a sigmoid function. The input feature maps X are reweighted using the channel attention weights, thus emphasizing features that are more beneficial for the SR task."
        },
        {
            "heading": "D. Lightweight Stepless Upsampling Module (LSUM)",
            "text": "To cope with potential decimal-scale factors, a simple idea is to use interpolation algorithms such as bilinear or bicubic to resize the feature maps. However, upsampling the input LR image to the target size in advance will significantly increase the computational complexity. Additionally, Hu et al. [9] proved that interpolating the feature maps at the end of the network will provide poor results. This problem occurs mainly because the interpolation algorithms cannot effectively map the features of a single scale to other scales. To address this problem, we propose constructing a family of feature maps with increasing resolution, and designing implicit feature functions (IFFs) to obtain a continuous representation of the image in the high-level feature space.\nFig. 6 presents an overview of the LSUM. First, we design a cascade frequency enhancement block (CFEB) to obtain progressive resolution feature maps {Mt \u2208 R C 4 \u00d7tH\u00d7tW }4t=1.\n7 CFEB uses group convolutions and channel split operations to reduce computational complexity. Then, we introduce IFFs to map any 2D coordinate to a vector in the continuous feature space. We also propose a novel scale-aware pixellevel attention (SAPA) mechanism for IFF and combine it with scale factor encoding, enabling efficient feature mapping across scales. Thus, the continuous feature representation of the image is obtained with a high-resolution feature map F (r)HR for any scale factor of r. Finally, we obtain the final SR result by using two consecutive convolutional layers:\nISR = Conv1,3(ReLU(Conv3,C/4(F (r) HR))). (8)\n1) CFEB: CFEB aims to obtain a set of feature maps with progressive resolution by gradually adding high-frequency details, as shown in the left part of Fig. 6.\nFirst, we split the input feature maps Min \u2208 RC\u00d7H\u00d7W into two parts along the channel dimension, one of which contains 1/4 of the feature maps, denoted as M1 \u2208 R C 4 \u00d7H\u00d7W , and kept as the current resolution. We use group convolution and pixel-shuffling to add high-frequency details to the remaining 3/4 feature maps M\u03031 \u2208 R 3C 4 \u00d7H\u00d7W and increase the spatial resolution by a scale factor of 2. Then, we split the feature maps of the second-level resolution into two parts and keep 1/3 of the feature maps M2 \u2208 R C 4 \u00d72H\u00d72W as the current resolution. The resolution of the remaining 2/3 feature maps M\u03032 \u2208 R 2C 4 \u00d72H\u00d72W is again doubled by using group convolution and pixel-shuffling. Finally, the third-level resolution feature maps are split into two halves along the channel dimension. Half of the feature maps M3 \u2208 R C 4 \u00d74H\u00d74W are kept at the current resolution, and the other half M\u03033 \u2208 R C 4 \u00d74H\u00d74W are upscaled twice using a convolutional layer followed by a pixel-shuffle operation. The last-level resolution is 8 times the original input, and the last-level feature maps are denoted as M4 \u2208 R C 4 \u00d78H\u00d78W . Thus, we obtain feature maps {Mt}4t=1 with progressive resolution. The calculation of the CFEB can be summarized as (9)\nM1, M\u03031 = Split(Min),\nM2, M\u03032 = Split(PixShuffle(GroupConv (3) 3,3C(M\u03031)), M3, M\u03033 = Split(PixShuffle(GroupConv (2) 3,2C(M\u03032)),\nM4 = PixShuffle(Conv3,C(M\u03033)),\n(9)\nwhere GroupConv(n)k,C(\u00b7) denotes a group convolutional layer with a kernel size of k \u00d7 k, number of channels C, and number of groups n. PixShuffle(\u00b7) denotes the pixel-shuffle operation.\n2) IFF: Implicit neural representation is a continuous and differentiable function parameterized by neural networks, which has been widely used in 3D tasks [52]\u2013[54]. Our work is inspired by LIIF [10], which learns an implicit function of an image that maps any coordinate to a pixel value, enabling SR with arbitrary scale factors. Since the implicit function is shared for all images, it takes the feature vectors of pixels near the query coordinate, called latent codes, as additional input. LIIF directly uses implicit functions to predict pixel values of the HR image, resulting in a complex network structure of implicit function and high computational cost. In addition,\nz0,1 z1,1\nz0,0 z1,0\na\nb x\nMt\nFig. 7. Illustration of latent codes of a query coordinate x in Mt.\nsince the implicit functions in LIIF are constructed using only one latent code of a single scale, a local ensemble strategy is required to avoid discontinuous artifacts, which further increases the complexity of the upsampling module.\nTo address the above problems, we propose improvements in three aspects. First, we let the implicit function map from 2D coordinates to HR feature vectors instead of pixel values, which can greatly reduce the difficulty of learning the mapping. Second, we construct the implicit functions using a local set of latent codes from multi-resolution spaces, which can avoid the artifact problem at a low computational cost. Third, we introduce a scale-aware pixel attention mechanism incorporated with periodic scale encoding, which can effectively improve the performance of multi-scale factor tasks.\nFormally, let fIFF denote the IFF, which maps any 2D coordinate x to the feature vector yx of the corresponding position. Recall that we obtained progressive resolution feature maps {M1, \u00b7 \u00b7 \u00b7 ,M4} from CFEB. Then the continuous feature representation is defined as\nyx = fIFF(x;M1,M2,M3,M4)\n= SAPA(r, [h(x,Z1), h(x,Z2), h(x,Z3), h(x,Z4)]), (10)\nwhere SAPA(r, \u00b7) denotes a scale-aware pixel attention layer with a scale factor of r, and Zt = {z0,0, z0,1, z1,0, z1,1} is a set of four nearest (Euclidean distance) latent codes from x in Mt, i.e., the feature vectors of the four pixels nearest to the query coordinate x. Fig. 7 illustrates the relationship between a query coordinate and latent codes. Let z0,0, z0,1, z1,0, z1,1 \u2208 RC4 \u00d71\u00d71 (t and x are omitted) denote the lower left, upper left, lower right, and upper right latent codes of the query coordinate x, respectively. h(x,Zt) is a local bilinear function that interpolates the four latent codes in both horizontal and vertical dimensions and is defined as follows:\nh(x,Zt) = [ 1\u2212 a a ] [z0,0 z0,1 z1,0 z1,1 ] [ 1\u2212 b b ] , (11)\nwhere (a, b) represents the coordinates of x relative to z0,0 normalized to [0, 1].\n3) SAPA: Simply concatenating the feature vectors {h(x,Zt)}4t=1 from spaces of different resolutions will not provide an ideal solution because the importance of input feature vectors varies under different scale factors. It is easy to see that when the scale factor is relatively large, such as r = 4,\n8 h(x,Z3) and h(x,Z4) should be assigned higher attention weights; otherwise, h(x,Z1) and h(x,Z2) should be assigned higher attention weights. Therefore, to adaptively adjust the attention weights of the concatenated feature vectors according to r, thereby enhancing the representation ability of IFF in multi-task learning, we introduce scale-aware pixel attention.\nRecent studies [25], [54] showed that low-level information encoding is one of the keys to generating high-frequency details in 2D/3D tasks. In our scale arbitrary SR task, the scale factor r is critical input information that will affect the distribution of HR feature spaces. Therefore, we employ periodic encoding to expand 1D r into a 64D vector,\n\u03c6(r) = [sin(\u03c91r), cos(\u03c91r), sin(\u03c92r), cos(\u03c92r),\n\u00b7 \u00b7 \u00b7 , sin(\u03c932r), cos(\u03c932r)], (12)\nwhere \u03c91, \u03c92, \u00b7 \u00b7 \u00b7\u03c932 are frequency parameters, and \u03c9i = 2ei. Then, the feature vectors {h(x,Zt)}4t=1 are concatenated with the scale encoding \u03c6(r) and fed to an MLP with a single hidden layer to calculate the pixel-level attention weights. The number of hidden layer neurons in the MLP is C/2, and LReLU is adopted as the activation function. The attention weights \u03b1 are calculated as follows:\n\u03b1 = MLP([\u03c6(r), h(x,Z1), h(x,Z2), h(x,Z3), h(x,Z4)]), (13) where MLP(\u00b7) represents the operation of the three-layer MLP. Finally, we multiply the attention weights with the input feature vector to dynamically emphasize information at different scales. The output of the IFF yx can be calculated as\nyx = [h(x,Z1), h(x,Z2), h(x,Z3), h(x,Z4)]\u2297\u03b1, (14)\nwhere \u2297 is the element-wise multiplication."
        },
        {
            "heading": "E. Loss Functions",
            "text": "hlhere The loss function of the SalDRN consists of three parts: SR loss LSR, saliency loss Lsal, and reconstruction difficulty loss Ldiff . The SR loss guides the network to generate content-correct SR results, while the saliency loss enables the saliency detector to generate accurate saliency maps and guides path selection in the DRM. The reconstruction difficulty loss can provide the saliency detector with the reconstruction difficulty information of the image patches according to the error of the SR result. The total loss of the network is defined as\nL = LSR + \u03bb1Lsal + \u03bb2Ldiff , (15)\nwhere \u03bb1 and \u03bb2 are hyperparameters to balance the three sub-items.\n1) SR Loss: DRM has K + 1 possible path choices, and the kth path contains k \u2212 1 FRUs. In the testing phase, each image patch selects the most suitable feature extraction path according to the average saliency value. However, in the training phase, we pass the LR image patch PLR through all possible paths so that the parameters of all paths can be optimized simultaneously. Therefore, we obtain K + 1 SR results {P kSR} K+1 k=1 and then compute the L1 losses by\ncomparing these SR results with the HR image patch PHR. The SR loss is defined as\nLSR = K+1\u2211 k=1 \u03b2k(s)\u2016P kSR \u2212 PHR\u20161, (16)\nwhere \u2016 \u00b7 \u20161 denotes the L1 norm, and \u03b2k is the weight of the kth path loss function, which is defined according to the probability of the path being selected and will be described in detail below.\nLet s denote the mean saliency value of an image patch, i.e., s = mean(Psal). The larger the saliency value s, the higher the reconstruction difficulty of the image patch, so the path containing more FRUs should be selected with a greater probability. Recall that in DRM, \u03b8k denotes the threshold of the k path selection switch. We define \u03b8K+1 = 1. Then, the weights in (16) can be calculated as follows:\n\u03b2\u0303k(s) = \u03b3(\u03b8k+1 \u2212 s)(s\u2212 \u03b8k), k = 1, 2, \u00b7 \u00b7 \u00b7 ,K + 1 \u03b2k(s) = exp(\u03b2\u0303k)\u2211K+1\ni=1 exp(\u03b2\u0303i) , k = 1, 2, \u00b7 \u00b7 \u00b7 ,K + 1\n(17)\nwhere \u03b3 is a hyperparameter that adjusts the difference in weights. We empirically set \u03b3 = 10 to obtain satisfactory results. It can be seen from (17) that \u03b2k takes the maximum if and only if \u03b8k \u2264 s \u2264 \u03b8k+1. This property guarantees that the definition of \u03b2k is reasonable, and the path within the threshold range can be selected with the greatest probability.\n2) Saliency Loss: The saliency loss Lsal is calculated by the cross-entropy loss between the predicted saliency maps and the saliency ground truth (GT):\nLsal = \u2212 1\n|P| |P|\u2211 i=1 ( Lisal log I i sal + (1\u2212 Lisal) log(1\u2212 Iisal) ) ,\n(18) where Lisal and I i sal represent the saliency GT of the ith pixel and the predicted saliency value, respectively, and |P| is the number of pixels in the training image.\n3) Difficulty Loss: We also use the reconstruction error as a guide for saliency detection. Regions with large differences between the SR results and the GT indicate that reconstruction is difficult, and higher saliency values should be predicted to guide image patches to select deeper feature extraction paths. The error map Ierr is calculated as follows: first, calculate the mean square error between the SR result and the GT; then, apply mean filtering to the error map; finally, histogram equalization is performed on the error map so that the error distribution histogram is approximately uniform. The reconstruction difficulty loss is defined as the L1 loss of the predicted saliency map and error map,\nLdiff = \u2016Isal \u2212 Ierr\u20161. (19)"
        },
        {
            "heading": "IV. EXPERIMENTAL RESULTS",
            "text": "In this section, we introduce the datasets, metrics, and training details. Then, we compare our proposal with SOTA fixedscale/stepless SR models. Finally, we compare and analyze the computational complexity of our proposal and competitive models.\n9"
        },
        {
            "heading": "A. Datasets and Metrics",
            "text": "We use remote sensing imagery provided by the GeoEye1 satellite and Google Earth to verify the effectiveness of the proposed method. The GeoEye-1 dataset contains 130 multispectral images with a resolution of 0.41m and a size of 512 \u00d7 512, of which 115 images are used for training, and 15 images constitute a test set. The Google Earth dataset contains 239 optical RSIs with a resolution of 1m and a size of 512 \u00d7 512, of which 224 are used for training and 15 images constitute a test set. In the experiments of this study, the training set contains a total of 339 RSIs from the above two sources."
        },
        {
            "heading": "B. Training Details",
            "text": "Our proposed model is implemented based on the PyTorch framework and trained on an NVIDIA GeForce RTX 3090 GPU.\nIn the testing phase, we crop the input image into patches with a size of 48 \u00d7 48 and an overlap of 8 pixels to enforce the dynamic routing strategy. The basic number of channels C in the feature extraction part is set to 64. The number of FRUs K is set to 3, and the thresholds of the path selection switches are set to 0, 0.25, 0.5. The number of IMDBs D = 4 in each FRU, and the parameters are shared between FRUs. The weight parameters \u03bb1, \u03bb2 in the loss function are set to 0.1 and 0.15, respectively.\nIn the training phase, LR images are obtained using bicubic downsampling and randomly cropped into 32 \u00d7 32 image patches as input. We randomly flip input patches vertically, horizontally, or rotate 90\u25e6 for data augmentation. We adopt Adam [55] as the parameter optimizer, setting \u03b21 = 0.1. The model is trained with a mini-batch size of 16, and each batch of LR images has the same scale factor. The scale factor during training follows a uniform distribution of [1, 4]. We train our model for 4 \u00d7 105 iterations with an initial learning rate of 1\u00d710\u22124, and the learning rate is halved at the 2\u00d7105 iteration."
        },
        {
            "heading": "C. Comparisons for Integer Scale Factors",
            "text": "In this section, we take bicubic interpolation as the baseline and compare our proposed models with seven lightweight fixed-scale SR models: CARN [15], IMDN [16], CTN [47], LatticeNet [56], PAN [18], CFSRCNN [21], and FeNet [48]. All competitive models are retrained on each scale factor, while our proposed SalDRN is trained only once. In addition, for a fair comparison with existing fixed-scale SR models, we replace the LSUM in the proposed SalDRN with the same fixed-scale upsampling module in PAN to obtain a fixed-scale SR model named SalDRN-FS.\nTable I shows objective comparisons of SR results on three integer scale factors (\u00d72, \u00d73 and \u00d74). We also compare the number of parameters of all models in Table I. The SalDRNFS achieves the best PSNR on all datasets and almost all scale factors, with up to 0.07 dB improvement over the CFSRCNN on the GeoEye-1 dataset with a scale factor of \u00d72. For the SSIM, SalDRN-FS achieves similar or better results than CFSRCNN, while the number of parameters is only 1/3 of that\nof CFSRCNN. Compared with CTN with similar parameters, SalDRN-FS improves PSNR by up to 0.21 dB.\nOur stepless SR model SalDRN performs slightly worse than SalDRN-FS because it is more challenging to learn SR for multiple scale factors simultaneously. Nevertheless, SalDRN still outperforms the competitive algorithms CARN, CTN, and PAN, obtaining objective metrics similar to IMDN. Note that SalDRN can achieve SR with arbitrary scale factors with only one model in the testing phase, so it can significantly save model training time and storage space. Therefore, SalDRN has more prominent advantages in model lightweighting."
        },
        {
            "heading": "D. Comparisons for Arbitrary Scale Factors",
            "text": "In this section, we compare the proposed SalDRN with competitive methods on the stepless SR task. Since the existing lightweight stepless SR is only OverNet [11], to verify the effectiveness of the proposed LSUM, we construct comparison models by combining the lightweight feature extraction modules and the stepless upsampling modules. The feature extraction modules include CARN [15] and IMDN [16], and the stepless upsampling modules include LIIF [10], OSM\n10\n[11], and our proposed LSUM. Therefore, we obtain six competitive methods: CARN+LIIF, CARN+OSM, CARN+LSUM, IMDN+LIIF, IMDN+OSM, and IMDN+LSUM.\nFig. 8 shows a parameter comparison of all competitive stepless SR models. The number of parameters of the LSUM is only 72K, which is 10% of the feature extraction module in IMDN. Compared with the upsampling modules LIIF and OSM, the number of parameters is reduced by 80% and 92%, respectively. Therefore, our proposed LSUM is an extremely lightweight upsampling module.\nTable II lists the SR results of the SalDRN and seven competitive algorithms on the Google Earth and GeoEye-1 datasets. We randomly select twelve scale factors for display. Compared with the two upsampling modules proposed by\nprevious works (LIIF and OSM), our proposed LSUM can achieve similar or better results with far fewer parameters. The SalDRN achieves the best performance on most scale factors due to the powerful feature expression capability of the DRM."
        },
        {
            "heading": "E. Model Complexity Analysis",
            "text": "We evaluate the computational complexity of models from three aspects: number of parameters, floating point operations (FLOPs), and runtime. The metric of FLOPs is defined as the number of multiplication and addition operations in the test phase averaged over image patches of 48\u00d7 48 on the Google Earth dataset.\nFig. 9 shows the performance, parameters, and FLOPs of SalDRN-FS and six lightweight models: CARN [15], IMDN [16], CTN [47], LatticeNet [56], PAN [18], CFSRCNN [21], and FeNet [48]. Our proposed SalDRN-FS achieves the best PSNR, and the number of parameters is only higher than the two super lightweight algorithms, PAN and CTN, ranking third. The FLOP of SalDRN-FS is 1225M, which is only 22% of the closest performing model CFSRCNN. This performance is attributed to the dynamic routing strategy adopted by SalDRN-FS, which can reduce the computational complexity of the model while maintaining performance.\nFig. 10 shows the performance, parameters, and FLOPs of SalDRN-FS and seven stepless SR models: CARN+LIIF, CARN+OSM, CARN+LSUM, IMDN+LIIF, IMDN+OSM, IMDN+LSUM, and OverNet. It can be seen that the models using LSUM not only have fewer parameters but also have more prominent advantages in the FLOPs. Compared with the upsampling modules LIIF and OSM, the FLOPs of LSUM are reduced by 98% and 48%, respectively. Therefore, our proposed LSUM is one of the keys to constructing a lightweight stepless SR model.\nTable III shows a runtime comparison of all competitive algorithms. The results were tested on a workstation equipped\n11\nwith NVIDIA GeForce 3090 GPU, Intel 9700K CPU, and 64 GB memory. The test dataset was Google Earth, and the scale factor was \u00d74. Among the fixed-scale SR models, the proposed SalDRN-FS runs faster than CTN and slightly slower than CFSRCNN, ranking sixth. Among all stepless SR methods, the models using LSUM run as fast as OSM and are more than 20 times faster than the models using LIIF. Thanks to the LSUM and dynamic routing strategy, our proposal can better balance the contradiction between model performance and runtime.\nComparing the performance, number of parameters, FLOPs, and runtime comprehensively, our proposed SalDRN and SalDRN-FS have lower model complexity and better SR performance.\nF. Visual Comparisons\nWe compare the visual results of our proposed methods with six fixed-scale SR models: CARN [15], IMDN [16], CTN [47], LatticeNet [56], PAN [18], and FeNet [48]. Fig. 11 shows a visual comparison for scale factors of \u00d73 and \u00d74. The proposed SalDRN and SalDRN-FS can better recover edges, avoid road distortion, and generate more precise texture details.\nFig. 12 shows a visual comparison of the SalDRN with seven stepless SR models on two non-integer scale factors. Models with the LSUM obtain comparable or even better visual results than previously proposed heavyweight upsampling\nmodules. Our proposed SalDRN can obtain results in terms of texture clarity and edge sharpness."
        },
        {
            "heading": "V. DISCUSSION",
            "text": "In this section, we first discuss the effectiveness of the dynamic routing strategy and the threshold settings in the DRM. Second, we study the effect of the parameter-sharing strategy in DRM. Third, we perform ablation experiments to verify the effectiveness of the SAPA mechanism in the LSUM. Finally, we verify the effectiveness of each term in the hybrid loss function."
        },
        {
            "heading": "A. Study of Dynamic Routing Strategy",
            "text": "To verify the effectiveness of the dynamic routing strategy, we visualize the output of the saliency detector and the path selection results in DRM. Fig. 13 (a) shows the SR results from the Google Earth and GeoEye-1 datasets. Fig. 13 (b) shows saliency maps output by the saliency detector. It can be seen that regions with complex textures have higher saliency values. Fig. 13 (c) shows the path selection results for each image patch. Since the threshold of the first path selection switch is set to 0, only paths 2 \u2013 4 are possible. Fig. 13 (d) shows the error maps of the SR results. The area with large errors indicates that the reconstruction is difficult. It can be seen that the saliency maps and error maps are consistent (positive correlation), indicating that the saliency map can be used as a guide for SR difficulty. Therefore, the dynamic routing strategy is reasonable and can indeed be used to improve the computational efficiency of SR models."
        },
        {
            "heading": "B. Threshold Settings in DRM",
            "text": "We discuss the threshold settings of path select switches in DRM. Table IV shows the path selection ratio, PSNR, and FLOPs under different threshold settings. When the thresholds of the three path selection switches are set to [0, 0, 0], only the\n12\ndeepest path in DRM is used for feature extraction, so it can be regarded as a model without the dynamic routing strategy. Under this setting, the best objective evaluation index can be obtained, but with the most FLOPs. When the thresholds are set to [0, 0.25, 0.5], there is almost no reduction in PSNR, but the FLOPs on both datasets are reduced to 75% and 79%, respectively. The dynamic routing strategy dynamically selects the feature extraction path according to the texture richness and reconstruction difficulty of the image patch, which can effectively reduce the computational complexity with little performance degradation. In addition, the FLOPs decrease as the threshold increases, but the model performance deteriorates to a certain extent. Therefore, we set the threshold as [0, 0.25, 0.5] to trade off model performance and complexity."
        },
        {
            "heading": "C. Discussion of Patch Size in DRM",
            "text": "This section discusses the patch size that determines the SR difficulty. As shown in Table V, we find that as the patch size increases, the receptive field increases, and the SR reconstruction results gradually improve. However, when the patch size becomes larger, the performance sometimes degrades at certain scale factors. We believe this occurs because the patch size is too large, and the saliency area in this patch has a small proportion, resulting in a low mean saliency value. This leads to this patch selecting a path with weak learning ability for feature extraction and being unable to fully mine\nthe feature information. The accumulation of similar situations leads to a decrease in the evaluation metrics of the image SR reconstruction."
        },
        {
            "heading": "D. Discussion of Parameter Sharing in DRM",
            "text": "We discuss the parameter sharing strategy in the DRM. For this purpose, we construct a comparative model without parameter sharing named SalDRN-FS-w/o-PS. In SalDRNFS-w/o-PS, the number of FRUs in the dynamic routing module is set to 3. The number of IMDBs in the three FRUs is set to 3, 2, 2 to avoid an excessively large number of parameters. Table VI shows a comparison of the SalDRNFS and the SalDRN-FS-w/o-PS. We observe that the models achieve similar performance. SalDRN-FS-w/o-PS obtains a higher PSNR with a scale factor of \u00d72, and the SalDRN-FS performs better with scale factors of \u00d73 and \u00d74.\nIn terms of model complexity, SalDRN-FS has fewer parameters due to the parameter-sharing strategy but larger FLOPs. The result reflects a contradiction between the number of parameters, computational complexity, and model performance."
        },
        {
            "heading": "E. Discussion of SAPA Mechanism",
            "text": "This section discusses the effectiveness of SAPA mechanisms. By removing the SAPA module in LSUM, a model without the scale-aware attention mechanism is obtained,\n13\nnamed SalDRN-w/o-SAPA. Fig. 14 shows the convergence analysis of the SalDRN and SalDRN-w/o-SAPA on the GeoEye-1 dataset, where the curves represent the variation of PSNR with the number of iterations on three scale factors of \u00d72, \u00d73 and \u00d74. As we can see, the SalDRN using SAPA has a faster convergence rate, and the PSNR is better than the comparison model without SAPA. The SAPA module can dynamically adjust the pixel-level attention weights according to the scale factor, thus enhancing the generalization performance over multiple scale factors."
        },
        {
            "heading": "F. Discussion of Hybrid Loss Function",
            "text": "In this section, we discuss the effectiveness of each term in the hybrid loss function, SR loss LSR, saliency loss Lsal, and SR difficulty loss Ldiff . We add these three items one by one to obtain three comparison models. The model trained with only the SR loss is denoted Model-I; the model trained with the SR loss and the saliency loss is denoted Model-II. Table VII shows a performance comparison between SalDRN, Model-I, and Model-II on the GeoEye-1 and Google Earth datasets.\nAs seen from Table VII, the model with only SR loss achieves the highest objective metrics. This is because if no constraints are imposed on the saliency detector, it will output a saliency map with all pixels as 1 (as shown in Fig. 15 (b)), so that the most image patches can pass the deepest path. Therefore, only using the SR loss will completely invalidate\nthe dynamic routing strategy. If the SR difficulty loss is removed, then the performance of Model-II decreases slightly. This is because the saliency map only concentrates on regions of visual interest, such as residential areas, while ignoring other areas with greater SR difficulty, as shown in Fig. 15 (c) and (d). Since SalDRN adopts the SR difficulty loss, it can better guide the network for path selection and obtain better objective metrics."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "In this study, to improve the performance of SR in scenarios with limited computing resources and meet the needs of arbitrary scale factor amplification, we proposed a lightweight SalDRN for stepless SR of RSIs. We designed a superlightweight saliency detector to embed in the SR framework with a negligible additional computational burden. The proposed dynamic routing strategy crops the input image into small patches and selects the appropriate deep network branch for feature extraction according to the SR difficulty. We also proposed an LSUM for lightweight SR networks based on implicit feature functions and the pixel attention mechanism. LSUM enables the network to process multiple scale factors within a single model, significantly reducing the training and storage costs. Experimental results showed that the proposed SalDRN narrows the performance gap with heavyweight SR networks compared with SOTA lightweight SR models.\n14"
        }
    ],
    "title": "Lightweight Stepless Super-Resolution of Remote Sensing Images via Saliency-Aware Dynamic Routing Strategy",
    "year": 2022
}