{
    "abstractText": "Predicting the Stock movement attracts much attention from both industry and academia. Despite such significant efforts, the results remain unsatisfactory due to the inherently complicated nature of the stock market driven by factors including supply and demand, the state of the economy, the political climate, and even irrational human behavior. Recently, Generative Adversarial Networks (GAN) have been extended for time series data; however, robust methods are primarily for synthetic series generation, which fall short for appropriate stock prediction. This is because existing GANs for stock applications suffer from mode collapse and only consider one-step prediction, thus underutilizing the potential of GAN. Furthermore, merging news and market volatility are neglected in current GANs. To address these issues, we exploit expert domain knowledge in finance and, for the first time, attempt to formulate stock movement prediction into a Wasserstein GAN framework for multi-step prediction. We propose IndexGAN, which includes deliberate designs for the inherent characteristics of the stock market, leverages news context learning to thoroughly investigate textual information and develop an attentive seq2seq learning network that captures the temporal dependency among stock prices, news, and market sentiment. We also utilize the critic to approximate the Wasserstein distance between actual and predicted sequences and develop a rolling strategy for deployment that mitigates noise from the financial market. Extensive experiments are conducted on real-world broad-based indices, demonstrating the superior performance of our architecture over other state-of-the-art baselines, also validating all its contributing components.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingyi Gu"
        },
        {
            "affiliations": [],
            "name": "Fadi P. Deek"
        },
        {
            "affiliations": [],
            "name": "Guiling Wang"
        }
    ],
    "id": "SP:b2819aa0872051a6ed915583e4b9f086a56cc7db",
    "references": [
        {
            "authors": [
                "J. Bollen",
                "H. Mao",
                "X. Zeng"
            ],
            "title": "Twitter mood predicts the stock market",
            "venue": "Journal of computational science, vol. 2, no. 1, pp. 1\u20138, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "R. Aguilar-Rivera",
                "M. Valenzuela-Rend \u0301on",
                "J. Rodr \u0301\u0131guez-Ortiz"
            ],
            "title": "Genetic algorithms and darwinian approaches in financial applications: A survey",
            "venue": "Expert Systems with Applications, vol. 42, no. 21, pp. 7684\u20137697, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. B"
            ],
            "title": "Malkiel, A random walk down Wall Street: including a life-cycle guide to personal investing",
            "venue": "WW Norton & Company,",
            "year": 1999
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, vol. 27, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Zhou",
                "Z. Pan",
                "G. Hu",
                "S. Tang",
                "C. Zhao"
            ],
            "title": "Stock market prediction on high-frequency data using generative adversarial nets",
            "venue": "Mathematical Problems in Engineering, vol. 2018, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Faraz",
                "H. Khaloozadeh"
            ],
            "title": "Multi-step-ahead stock market prediction based on least squares generative adversarial network",
            "venue": "2020 28th Iranian Conference on Electrical Engineering (ICEE). IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "A. Srivastava",
                "L. Valkov",
                "C. Russell",
                "M.U. Gutmann",
                "C. Sutton"
            ],
            "title": "Veegan: Reducing mode collapse in gans using implicit variational learning",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 3310\u20133320.",
            "year": 2017
        },
        {
            "authors": [
                "X. Ding",
                "Y. Zhang",
                "T. Liu",
                "J. Duan"
            ],
            "title": "Deep learning for eventdriven stock prediction",
            "venue": "Twenty-fourth international joint conference on artificial intelligence, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Poterba",
                "L.H. Summers"
            ],
            "title": "The persistence of volatility and stock market fluctuations",
            "venue": "National Bureau of Economic Research, Tech. Rep., 1984.",
            "year": 1984
        },
        {
            "authors": [
                "K.R. French",
                "G.W. Schwert",
                "R.F. Stambaugh"
            ],
            "title": "Expected stock returns and volatility",
            "venue": "Journal of financial Economics, vol. 19, no. 1, pp. 3\u201329, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "G.W. Schwert"
            ],
            "title": "Stock market volatility",
            "venue": "Financial analysts journal, vol. 46, no. 3, pp. 23\u201334, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "I.K. Nti",
                "A.F. Adekoya",
                "B.A. Weyori"
            ],
            "title": "A systematic review of fundamental and technical analysis of stock market predictions",
            "venue": "Artificial Intelligence Review, pp. 1\u201351, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C.A. Assis",
                "A.C. Pereira",
                "E.G. Carrano",
                "R. Ramos",
                "W. Dias"
            ],
            "title": "Restricted boltzmann machines for the prediction of trends in financial time series",
            "venue": "2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 2018, pp. 1\u20138.",
            "year": 2018
        },
        {
            "authors": [
                "L.-C. Cheng",
                "Y.-H. Huang",
                "M.-E. Wu"
            ],
            "title": "Applied attention-based lstm neural networks in stock prediction",
            "venue": "2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 4716\u20134718.",
            "year": 2018
        },
        {
            "authors": [
                "O.B. Sezer",
                "A.M. Ozbayoglu"
            ],
            "title": "Algorithmic financial trading with deep convolutional neural networks: Time series to image conversion approach",
            "venue": "Applied Soft Computing, vol. 70, pp. 525\u2013 538, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R.J. Hyndman",
                "G. Athanasopoulos"
            ],
            "title": "Forecasting: principles and practice",
            "venue": "OTexts,",
            "year": 2018
        },
        {
            "authors": [
                "T. Bollerslev"
            ],
            "title": "Generalized autoregressive conditional heteroskedasticity",
            "venue": "Journal of econometrics, vol. 31, no. 3, pp. 307\u2013327, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "W. Yao",
                "J. Gu",
                "W. Du",
                "F.P. Deek",
                "G. Wang"
            ],
            "title": "Adpp: A novel anomaly detection and privacy-preserving framework using blockchain and neural networks in tokenomics",
            "venue": "International Journal of Artificial Intelligence & Applications, vol. 13, no. 6, pp. 17\u201332, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Y. Qin",
                "D. Song",
                "H. Chen",
                "W. Cheng",
                "G. Jiang",
                "G. Cottrell"
            ],
            "title": "A dualstage attention-based recurrent neural network for time series prediction",
            "venue": "arXiv preprint arXiv:1704.02971, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Feng",
                "H. Chen",
                "X. He",
                "J. Ding",
                "M. Sun",
                "T.-S. Chua"
            ],
            "title": "Enhancing stock movement prediction with adversarial training",
            "venue": "arXiv preprint arXiv:1810.09936, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "Q. Ding",
                "S. Wu",
                "H. Sun",
                "J. Guo"
            ],
            "title": "Hierarchical multi-scale gaussian transformer for stock movement prediction.",
            "venue": "in IJCAI,",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "Y. Li",
                "H. Yang",
                "L. Yang",
                "X.-Y. Liu"
            ],
            "title": "Dp-lstm: Differential privacy-inspired lstm for stock prediction using financial news",
            "venue": "arXiv preprint arXiv:1912.10806, 2019.",
            "year": 1912
        },
        {
            "authors": [
                "X. Zhang",
                "H. Fuehres",
                "P.A. Gloor"
            ],
            "title": "Predicting stock market indicators through twitter \u201ci hope it is not as bad as i fear",
            "venue": "Procedia-Social and Behavioral Sciences, vol. 26, pp. 55\u201362, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "E.J. De Fortuny",
                "T. De Smedt",
                "D. Martens",
                "W. Daelemans"
            ],
            "title": "Evaluating and understanding text-based stock price prediction models",
            "venue": "Information Processing & Management, vol. 50, no. 2, pp. 426\u2013441, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "B. Zhao",
                "Y. He",
                "C. Yuan",
                "Y. Huang"
            ],
            "title": "Stock market prediction exploiting microblog sentiment analysis",
            "venue": "2016 International Joint Conference on Neural Networks (IJCNN). IEEE, 2016, pp. 4482\u20134488.",
            "year": 2016
        },
        {
            "authors": [
                "J. Tang",
                "X. Chen"
            ],
            "title": "Stock market prediction based on historic prices and news titles",
            "venue": "Proceedings of the 2018 International Conference on Machine Learning Technologies, 2018, pp. 29\u201334.",
            "year": 2018
        },
        {
            "authors": [
                "J.B. Nascimento",
                "M. Cristo"
            ],
            "title": "The impact of structured event embeddings on scalable stock forecasting models",
            "venue": "Proceedings of the 21st Brazilian Symposium on Multimedia and the Web, 2015, pp. 121\u2013124.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Liu",
                "Q. Zeng",
                "H. Yang",
                "A. Carrio"
            ],
            "title": "Stock price movement prediction from financial news with deep learning and knowledge graph embedding",
            "venue": "Pacific rim Knowledge acquisition workshop. Springer, 2018, pp. 102\u2013113.",
            "year": 2018
        },
        {
            "authors": [
                "C.-Y. Lee",
                "V.-W. Soo"
            ],
            "title": "Predict stock price with financial news based on recurrent convolutional neural networks",
            "venue": "2017 conference on technologies and applications of artificial intelligence (TAAI). IEEE, 2017, pp. 160\u2013165.",
            "year": 2017
        },
        {
            "authors": [
                "D.L. Minh",
                "A. Sadeghi-Niaraki",
                "H.D. Huy",
                "K. Min",
                "H. Moon"
            ],
            "title": "Deep learning approach for short-term stock trends prediction based on twostream gated recurrent unit network",
            "venue": "Ieee Access, vol. 6, pp. 55392\u2013 55404, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Li",
                "R. Bao",
                "K. Harimoto",
                "D. Chen",
                "J. Xu",
                "Q. Su"
            ],
            "title": "Modeling the stock relation with graph network for overnight stock movement prediction",
            "venue": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, 2021, pp. 4541\u20134547.",
            "year": 2021
        },
        {
            "authors": [
                "M. Saito",
                "E. Matsumoto",
                "S. Saito"
            ],
            "title": "Temporal generative adversarial nets with singular value clipping",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 2830\u20132839.",
            "year": 2017
        },
        {
            "authors": [
                "C. Esteban",
                "S.L. Hyland",
                "G. R \u0308atsch"
            ],
            "title": "Real-valued (medical) time series generation with recurrent conditional gans",
            "venue": "arXiv preprint arXiv:1706.02633, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "Z. Gan",
                "L. Carin"
            ],
            "title": "Generating text via adversarial training",
            "venue": "NIPS workshop on Adversarial Training, vol. 21. academia. edu, 2016, pp. 21\u201332.",
            "year": 2016
        },
        {
            "authors": [
                "A. Gupta",
                "J. Zou"
            ],
            "title": "Feedback gan for dna optimizes protein functions",
            "venue": "Nature Machine Intelligence, vol. 1, no. 2, pp. 105\u2013111, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Takahashi",
                "Y. Chen",
                "K. Tanaka-Ishii"
            ],
            "title": "Modeling financial timeseries with generative adversarial networks",
            "venue": "Physica A: Statistical Mechanics and its Applications, vol. 527, p. 121261, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.-W. Dong",
                "W.-Y. Hsiao",
                "L.-C. Yang",
                "Y.-H. Yang"
            ],
            "title": "Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment",
            "venue": "Thirty- Second AAAI Conference on Artificial Intelligence, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Yu",
                "W. Zhang",
                "J. Wang",
                "Y. Yu"
            ],
            "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Yoon",
                "D. Jarrett",
                "M. Van der Schaar"
            ],
            "title": "Time-series generative adversarial networks",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Wiese",
                "R. Knobloch",
                "R. Korn",
                "P. Kretschmer"
            ],
            "title": "Quant gans: Deep generation of financial time series",
            "venue": "Quantitative Finance, vol. 20, no. 9, pp. 1419\u20131440, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xu",
                "Z. Zhang",
                "L. You",
                "J. Liu",
                "Z. Fan",
                "X. Zhou"
            ],
            "title": "scigans: single-cell rna-seq imputation using generative adversarial networks",
            "venue": "Nucleic acids research, vol. 48, no. 15, pp. e85\u2013e85, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Wu",
                "Y. Xia",
                "F. Tian",
                "L. Zhao",
                "T. Qin",
                "J. Lai",
                "T.-Y. Liu"
            ],
            "title": "Adversarial neural machine translation",
            "venue": "Asian Conference on Machine Learning. PMLR, 2018, pp. 534\u2013549.",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhang",
                "G. Zhong",
                "J. Dong",
                "S. Wang",
                "Y. Wang"
            ],
            "title": "Stock market prediction based on generative adversarial network",
            "venue": "Procedia computer science, vol. 147, pp. 400\u2013406, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Sonkiya",
                "V. Bajpai",
                "A. Bansal"
            ],
            "title": "Stock price prediction using bert and gan",
            "venue": "arXiv preprint arXiv:2107.09055, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Arjovsky",
                "S. Chintala",
                "L. Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "International conference on machine learning. PMLR, 2017, pp. 214\u2013223.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Rubner",
                "C. Tomasi",
                "L.J. Guibas"
            ],
            "title": "The earth mover\u2019s distance as a metric for image retrieval",
            "venue": "International journal of computer vision, vol. 40, no. 2, pp. 99\u2013121, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "F. Klinker"
            ],
            "title": "Exponential moving average versus moving exponential average",
            "venue": "Mathematische Semesterberichte, vol. 58, no. 1, pp. 97\u2013107, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "S. Makridakis",
                "M. Hibon"
            ],
            "title": "Arma models and the box\u2013jenkins methodology",
            "venue": "Journal of forecasting, vol. 16, no. 3, pp. 147\u2013163, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "Y. Xu",
                "S.B. Cohen"
            ],
            "title": "Stock movement prediction from tweets and historical prices",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 1970\u20131979.",
            "year": 2018
        },
        {
            "authors": [
                "A. Radford",
                "L. Metz",
                "S. Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "arXiv preprint arXiv:1511.06434, 2015.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "KEYWORDS Stock Market Prediction, Generative Adversarial Networks, Deep Learning, Finance\n1. INTRODUCTION The stock market is an essential component of a broad and intricate financial system, and stock prices reflect the dynamics of economic and financial activities. Predicting future movements of either individual stocks or overall market indices is important to investors and other market players [1], which requires significant efforts but lacks satisfactory results. Conventional approaches vary from fundamental and technical analysis to linear statistical models, such as Momentum Strategies and Autoregressive Integrated Moving Average (ARIMA), which capture simple short-term patterns from historical prices. With the tremendous power and success in exploring the nonlinear relationship and dealing with big data, machine learning and neural networks are increasingly utilized in stock movement prediction and have shown better results in prediction accuracy over traditional methods [2].\nHowever, stock prices, especially broad-based indices (such as Dow30 and S&P 500), carry inherent stochasticity caused by crowd emotional factors (such as fear and greed), economic factors (such as fiscal and financial policy and economic health), and many other unknown factors [3]. Thus, existing models suffer from a lack of robustness, and the prediction accuracy is still far from being satisfactory. Additionally, unlike many other tasks, the prediction accuracy of broadindex return cannot keep increasing, in theory, because market arbitrage opportunities will\ndegrade accuracy over time. Naturally, an effective trading strategy that is likely to generate profit will be adopted by an increasing number of investors; eventually, such a strategy may no longer perform as well considering that the market is composed of its collective participants. Thus, in practice, if a certain method is capable of achieving over 60% accuracy using a dataset of a lengthy period, encompassing a financial crisis, it is then considered powerful enough. Therefore, we aim to provide a framework that performs with accuracy, over time, and in different market conditions.\nRecently, a powerful machine learning framework named Generative Adversarial Networks (GAN) [4] achieved noteworthy success in various domains. Distinct from single neural networks, GAN trains two models contesting with each other simultaneously, a generator capturing data distribution and a discriminator estimating the probability of a sample from training data instead of a generator. Having achieved promising performance in image generation, researchers have extended GANs to the field of time series for applications in synthetic time series data generation. While aiming to simulate characteristics in sequences, these efforts have rarely been practical enough to predict future patterns. Although some advanced methods have been proposed to forecast stock pricing by building an architecture of GAN conditioned on historical prices [5, 6], they have two main deficiencies: 1) Their implemented vanilla GAN suffers from mode collapse problem [7] and creates samples with low diversity. Consequently, most predictions tend to be biased towards an up-market. 2) Existing models only predict one future step, which is then concatenated with historical prices to generate fake sequences. As a result, the only difference between the actual and generated sequence is the last element, and thus the discriminator can easily ignore it. Such methods largely underutilize GAN\u2019s potential, which can create sequences with periodical patterns of the stock market. We aim to address these issues by fully realizing the potential of GAN to solve the stock index prediction problem.\nIn addition to historical prices, other driving factors such as emerging news and market volatility should also be exploited in market prediction. An increasing amount of research results has shown that the stock market is highly affected by significant events [8]. With the popularity of social media, frequent financial reporting, targeted press releases, and news articles significantly influence investors\u2019 decisions and drive stock price movement. Natural Language Processing (NLP) techniques have been utilized to analyze textual information to forecast stock pricing. In addition to news, the volatility index, derived from options prices, contains important indicators on market sentiment [9, 10, 11]. There are some common similarity between options and insurance. For example, put options can be purchased as insurance, or a hedge, against a sudden stock price drop. A price increase in put options with the same underlying stock price indicates that the market expects a high probability of price drop, just as an increased flood insurance price imply a higher probability of flood though people may still choose to stay in their homes (i.e., to keep the stock position unsold). The volatility index, which is derived from the prices of index options with near-term expiration dates, is an important leading indicator of the stock market but is often neglected in the GAN models for predicting the stock market. We incorporate both news and volatility index in our proposed prediction model.\nBecause existing efforts predominantly rely on historical prices and neglect the inherent characteristics of the stock market, there is a need to better address the issues and yield higher prediction accuracy. Thus, we propose the following strategies which employ domain knowledge in financial systems: 1) formulating stock movement to a multiple-step prediction problem; 2) exploiting Wasserstein GAN to avoid mode collapse that tends to predict an up-market, and improve stability by prior noise; 3) leveraging volatility index to capture the market sentiment of seasoned investors; 4) designing news context learning to fully explore textual information from public news and events; and 5) adopting a rolling strategy for deployment to mitigate noise from uncertainty in the financial market, and generate the prediction with horizon-wise information.\nSpecifically, our proposed IndexGAN contains both a generator and a critic to predict the stock movement. Inside the generator, a news context learning module extracts latent word representations from GloVe embedding matrix of news for two purposes. Firstly, it fully explores\ncontext information from the news. Secondly, it strategically shrinks the size of the embedding matrix to a comparable extent with price features, considering the size of GloVe embedding matrix is several orders of magnitude larger than market-based features and prior noise, and simply concatenating them together will let the following seq2seq learning entirely concentrate on news and mask the impact of other features. An attentive seq2seq learning module captures temporal dependence from historical prices, volatility index, various technical indicators, prior noise, and latent word representations. The generator with both modules predicts the future sequence. The critic (discriminator in GAN) is designed to approximate the Wasserstein distance between real and generated sequences, which is incorporated into an adversarial loss for optimization. During the training process, besides adversarial loss, two additional losses are added to improve the similarity between real and fake distribution efficiently and avoid biased movement.\nAccurate predictions of an index return enable hedging against black swan events and thus can significantly reduce risk and help maintain market liquidity when facing extreme circumstances, thereby contributing to financial market stabilization. IndexGAN is implemented on broad indices. To understand the complexity and dynamics of the real-world market index, we select a period of high volatility that covers the 2009 financial crisis and the subsequent recovery for training and testing of IndexGAN. Experiment results show that the model boosts the accuracy from around 58% to over 60% for both indices (Dow Jones Industrial Average (DJIA) and S&P 500) compared to the previous state-of-the-art model. This improvement demonstrates the effectiveness of the proposed method. The major contributions of this work are summarized as follows:\n1. To the best of our knowledge, ours is a first attempt at formulating stock movement predictions into a Wasserstein generative adversarial networks framework for multi-step prediction. The new formulation enhances the robustness by prior noise and provides reasonable guidance on market timing to investors through the use of multi-step prediction.\n2. IndexGAN consists of particularly crafted designs that consider the inherent characteristics of the stock market. It includes a news context learning module, features of volatility index and multiple empirically valid technical indicators, and a rolling deployment strategy to mitigate uncertainty and generate prediction with horizon-wise information.\n3. We conduct extensive experiments on two stock indices that attract the most global attention. IndexGAN achieves consistent and significant improvements over multiple popular benchmarks.\n2. RELATED WORKS"
        },
        {
            "heading": "2.1. Stock Market Prediction",
            "text": "Stock movement prediction has drawn much industry attention for its potential to maximize profit, as has the intricate nature of this problem to academia. Traditional methods fall into two categories: fundamental analysis and technical analysis [12]. Fundamental analysis attempts to evaluate the fair and long-term stock price of a firm by examining its fundamental status and comparative strength within its industry as well as the overall economy such as revenues, earnings, and account payables [13, 14]. Technical analysis, also known as chart analysis, examines price trends and identifies price patterns [15]. There are numerous technical indicators, such as moving average and Bollinger Bands, that guide traders on buying and selling trends. Linear models, such as ARIMA [16] and Generalized Autoregressive Conditional Heteroskedasticity (GARCH) [17], have also been utilized in stock prediction. With the development of deep learning, many deep neural networks have been employed in the finance area, especially for stock prediction [18]. Qin et al. [19] propose a dual-stage attention-based Recurrent Neural Network (RNN) model to predict\nstock trends. Feng et al. [20] further consider employing the concept of adversarial training on Long Short-Term Memory (LSTM) [21] to predict stock market pricing. Ding et al. [22] enhance the transformer with trading gap splitter and gaussian prior for stock price prediction.\nAs the outreach of digital and social media grow enabling real-time streaming of financial news, their impact on the stock market is explored. Existing work for textual stock analysis can be classified into sentiment analysis and context-aware approaches. Sentiment analysis is leveraged to extract emotions from text and generate sentiment scores as features [23], although this often ignores useful textual information. Zhang, Fuehres, and Gloor [24] use mood words as emotional tags to classify the sentiment of tweets. De Fortuny et al. [25] select sentiment words from the news by TF-IDF and calculate sentiment polarity and subjectivity as sentiment features for Support Vector Machine (SVM) in making predictions. Zhao et al. [26] employ LDA to filter microblogs and create a financial sentiment lexicon to extract public emotions. On the other hand, context-aware approaches create embeddings from news and feed them into complicated structures to explore context as much as possible [27, 28, 29], which often suffer from the overfitting problem. Lee and Soo [30] process financial news with word2vec to create input for recurrent Convolutional Neural Network (CNN). Minh et al. [31] implement Stock2Vec and twostream Gated Recurrent Unit (GRU) models to create embeddings from financial news for stock price classification. Li et al. [32] propose using Graph Convolutional Network to represent connection given news embeddings and predict overnight movement. Both methods are widely used for leveraging news in stock market prediction.\nAlthough existing efforts have been successful in achieving their stated goals, market risk is still largely not adequately addressed. Hence, we aim to incorporate market volatility into stock prediction."
        },
        {
            "heading": "2.2. Generative Adversarial Nets on Time Series",
            "text": "More recently, there are emerging efforts in using GAN on time series data in the domain of video generation [33] in medicine [34], text generation [35], biology [36], finance [37], and music [38]. These GANs aim to generate a synthetic sequence that retains the characteristics of the original sequence. For instance, Yu et al. [39] introduce SeqGAN, which extends GANs with reinforcement learning to generate sequences of discrete tokens. Yoon et al. [40] propose TimeGAN for realistic time-series generation, combining an unsupervised paradigm with supervised autoregressive models. These methods are used for tasks of enriching limited realworld datasets [41], data imputation [42], and machine translation [43]. However, stock market predictions are attempts to discover future patterns instead of simulating original time series and thus cannot employ most existing efforts on time series GANs.\nSome recent approaches implement GANs to predict stock trends. Zhang et al. [44] employ LSTM as a generator and MLP as a discriminator to forecast stock pricing. Faraz and Khaloozadeh [6] enhance GAN with wavelet transformation and z-score method in the data preprocessing phase. Sonkiya et al. [45] generate binary sentiment score by FinBERT and employ GAN for stock price predictions. Such methods adapt Vanilla GAN which often suffers from mode collapse, and neglect possible driving factors such as complex public emotions, financial policies, and events, which limits the ability to achieve superior performance. In this work, we use GAN with Wasserstein distance to design a robust architecture enabling us to capture the characteristics of the stock market.\n3. PROBLEM FORMULATION We aim to develop a model that accurately predicts whether a stock index is up or down the next day. If the close price of the next day is higher than that of the current day, we have an up day and vice versa. We select two different types of features: market-based features read derived from the stock market data and news-based features derived from raw news headlines. Market-based\nfeatures are listed in Table 1. Specifically, we retrieve the open, high, low, and close prices of the stock index on trading days. We also deliberately select nine empirically useful technical indicators, calculate their values based on the price data, and employ them as features. In addition, we employ the market volatility data, i.e., VIX, if we predict the S&P 500 index and VXD if we predict the Dow Jones Index.\nLet \ud835\udcae = {\ud835\udc60!}!\"#$ \u2208 \u211d$\u00d7& be a matrix which represents the stock dataset with \ud835\udc5b time points, where \ud835\udc60! is a vector with \ud835\udc5d market-based features. Let \u210b = {\u210e!}!\"#$ \u2208 \u211d$\u00d7' be a matrix which denotes textual dataset, we assume that each vector \u210e! contains a number of \ud835\udc58 raw news headlines at the time step \ud835\udc61. Let \ud835\udc67 denote the prior noise which is assumed to be drawn from Gaussian distribution. We concatenate stock features and headlines together. Then we denote our training data as \ud835\udc9f = {(\ud835\udc60! , \u210e! , \ud835\udc67)}!\"#$ . To investigate the temporal dependency of sequential data, we set the length of historical sequence as \ud835\udc64 and the length of future sequence as \ud835\udc5e. This means that we use the past \ud835\udc64 days\u2019 data to predict the next \ud835\udc5e days sequence in the inference phase. Suppose the current time is day \ud835\udc47 , given the trained model, we use \ud835\udc4b = {(\ud835\udc60! , \u210e! , \ud835\udc67)}!\"()*+#( as input and generate ?\u0302? = {?\u0302?!}!\"(+#\n(+, , where ?\u0302?! is the predicted return based on close price. From the generated sequence, we can determine whether day \ud835\udc47 + 1 is up or down.\n4. METHODOLOGIES We elect to use GAN, specifically IndexGAN, for stock market prediction. The architecture of the model is shown in Figure 1. In general, GAN consists of two models: a generator that learns distribution over real data, and a discriminator that distinguishes between real data and fake output from the generator. For the purpose of controlling on modes of data being generated, conditional GAN [46] was proposed by conditioning the model to additional information. In our model, stock and news features are used as additional information. The goal is to use training data \ud835\udc9f to learn a feature map (generator) \ud835\udca2 which best generates sequences of future stock prices in the GAN framework. WGAN [47], which uses Wasserstein distance to measure the difference between the real data distribution and generated data distribution, is chosen for its stability."
        },
        {
            "heading": "4.1. Generator",
            "text": "Generator \ud835\udca2 with parameter set \u0398- takes input \ud835\udc4b and aims to build a fake sequence ?\u0302? whose distribution is as close as possible to the real sequence \ud835\udc50 = {\ud835\udc50!}!\"(+#\n(+, . Our generator has two parts: news context learning and attentive seq2seq learning."
        },
        {
            "heading": "4.1.1. News Context Learning",
            "text": "To convert the text from news into word vectors, word2vec is applied to encode the textual input as a continuous distributed representation. GloVe, one of the most popular word representation\ntechniques in word2vec, is chosen to be employed in our model. Compared with general word2vec models, GloVe not only considers words and related local context information but also incorporates aggregated global word co-occurrence statistics from the corpus to create an embedding matrix. Given a paragraph \u210e! at time step \ud835\udc61 consisting of \ud835\udc58 news headlines, each word in the headlines is mapped to a vector in \ud835\udc5a dimensions. The process is formulated as:\n\u210e.! = \ud835\udc53/0123(\u210e!) (1)\nwhere \ud835\udc53/0123 is the mapping function from GloVe to word vectors, \u210e.! \u2208 \u211d'\u00d70\u00d74 represents the word embedding matrix, and \ud835\udc59 is the maximum length of headlines in the textual dataset \u210b. Since the length of words in different headlines may be different, we add zero paddings to the end of word vectors from GloVe if the length is shorter than \ud835\udc59.\nThen a block of nonlinear layers for the embedding matrix is designed. The first component in a block is a fully connected layer. Next, batch normalization is applied to accelerate the training process by normalizing the features in the embedding matrix. In addition, since batch normalization is shown effective in preventing the mode collapse problem, we apply it to all blocks except the last one. The third layer is LeakyReLU activation which avoids saturation in the learning process and dying neurons, except that the last block uses tanh activation function for the output layer. We feed the embedding matrix \u210e.! into a number of \ud835\udc5a successive blocks formulated as follows:\n\u210e!# = \ud835\udf19A\ud835\udc5356(\ud835\udc4a7#\u210e!. + \ud835\udc4f7#)D (2)\n\u210e!8 = \ud835\udf19A\ud835\udc5356(\ud835\udc4a78\u210e!# + \ud835\udc4f78)D (3)\n\u2026\n\u210e!I = \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc4a74\u210e!4)# + \ud835\udc4f74) (4)\nwhere \u210e!9 , \ud835\udc56 = 1,\u2026 ,\ud835\udc5a is the output from \ud835\udc56!: block and input for \ud835\udc56 + 1!: block, \u210e!I \u2208 \u211d- is the latent word representation with \ud835\udc54 dimensions, \ud835\udf19 is the LeakyReLU activation function, \ud835\udc4a79 and \ud835\udc4f79 are parameters in a fully connected layer.\n4.1.2. Attentive Seq2seq Learning\nStock data shows seasonality. For example, a high open price on Monday morning frequently ends up with an up day on Friday. To adaptively capture the temporal relationship and generate sequences, we employ an encoder-decoder with the attentive mechanism as the sequence-tosequence learning. To improve the robustness of prediction, we add prior noise as an input feature to simulate the stochasticity nature of stock prices. In addition, among market-based features, Exponential Moving Average (EMA) and Simple Moving Average (SMA) over different periods of time (5, 13, 21, 50, and 200 days) are used to capture temporal information across multi-scale time spans.\nThe encoder extracts the attentive variable \ud835\udc4e. Its input is the sequence of a concatenation of features \ud835\udc65! containing prior noise \ud835\udc67, market-based feature \ud835\udc60!, and latent word representation \u210e!I :\n\ud835\udc65! = O\ud835\udc67, \ud835\udc60! , \u210e!I P (5)\nWe adopt the simple yet effective GRU to obtain dynamic changes from historical sequence {\ud835\udc65!}!\"()*+#( :\n\u210e!3 = \ud835\udc533(\ud835\udc65! , \u210e!)#3 ) (6)\nwhere \ud835\udc533 represents a GRU layer, and \u210e!)#3 denotes the hidden state at each time step.\nTo adaptively learn the different contributions of input features in the different historical time steps, we implement an attention layer to aggregate information of hidden state from GRU:\n\ud835\udc4e! = \ud835\udc63;\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc4a<\u210e!3 + \ud835\udc4f<), \ud835\udc4e!T = \ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc4e!)\n\u2211 \ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc4e!)(!\"()*+# (7)\n\ud835\udc4e = \u2211 \ud835\udc4e!T \u210e!3 (!\"()*+# (8)\nwhere \ud835\udc4e!T denotes contribution score for hidden state \u210e!3 at time \ud835\udc61, \ud835\udc63, \ud835\udc4a< and \ud835\udc4f< are parameters, and \ud835\udc4e is the attentive representation which encloses periodical information of the entire sequence.\nThe decoder aims to extract patterns from the attentive representation \ud835\udc4e and generate the fake sequence ?\u0302? \u2208 \u211d, in the future \ud835\udc5e time steps. We also adopt GRU as the decoder. The hidden state in the decoder \u210e(= is transformed from the last hidden state in encoder \u210e(3 by a fully connected layer. Then the attentive representation \ud835\udc4e is presented as input of the GRU. The process is computed as follows:\n\u210e(= = \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc4a=\u210e(3 + \ud835\udc4f=) (9)\n?\u0302? = \ud835\udc53>? Z\ud835\udc53=A\ud835\udc4e, \u210e(=D[ (10)\nwhere \ud835\udc53= denotes the GRU layer in decoder, \ud835\udc4a= and \ud835\udc4f= are weight and bias parameters, and \ud835\udc53>? represents the prediction layer to generate the fake sequence ?\u0302?.\nFor training purposes, the next step is to feed both fake and real sequences into critic for further optimization."
        },
        {
            "heading": "4.2. Critic",
            "text": "The critic is designed to approximate Wasserstein distance \ud835\udc4aA\ud835\udc43@ , \ud835\udc43-D between real data distribution \ud835\udc43@ and generated data distribution \ud835\udc43-:\n\ud835\udc4aA\ud835\udc43@ , \ud835\udc43-D = inf A\u2208CDE!,E\"G \ud835\udc38(I,J)\u223cA [||\ud835\udc65 \u2212 \ud835\udc66||] (11)\nwhere \u03a0A\ud835\udc43@ , \ud835\udc43-D denotes the set of all joint distribution \ud835\udefe(x, y) whose marginals are \ud835\udc43@ and \ud835\udc43- respectively. The joint distribution \ud835\udefe(\ud835\udc65, \ud835\udc66) represents the mass caused by transporting \ud835\udc65 to \ud835\udc66\nwhen transforming \ud835\udc43@ to \ud835\udc43-. Therefore, Wasserstein distance indicates the cost of such optimal transportation problem in intuition [47,48].\nLet ?\u0303? denote the input sequence for either real or fake data. We use GRU to temporally capture the dependency and a fully connected layer to pass the last hidden state, which is presented as:\n\ud835\udc63 = \ud835\udc53?(?\u0303?; \ud835\udee9?) = \ud835\udc53MA\ud835\udc53/NO(?\u0303?)D (12)\nwhere \ud835\udc53? denotes the critic consisting of the GRU layer \ud835\udc53/NO and the fully connected layer \ud835\udc53?, \ud835\udee9? represents the set of parameters in \ud835\udc53?, and the output \ud835\udc63 is the approximated distance value."
        },
        {
            "heading": "4.3. Optimization",
            "text": "As the infimum in \ud835\udc4aA\ud835\udc43@ , \ud835\udc43-D is highly intractable, Kantorovich-Rubinstein duality is used to reconstruct the WGAN value function in critic and solve the optimization problem in practice, as follows:\n\ud835\udc5a\ud835\udc56\ud835\udc5b \ud835\udca2 \ud835\udc5a\ud835\udc4e\ud835\udc65 >#\u2208\u2131 \ud835\udd3c?\u223cE![\ud835\udc53?(\ud835\udc50)] \u2212 \ud835\udd3c?\u0302\u223cE\"[\ud835\udc53?(?\u0302?)] (13)\nwhere \ud835\udc43- is implicitly defined by ?\u0302? , \u2131 is the set of 1-Lipschitz functions. The generator is responsible for minimizing the value function in critic which is equivalent to minimize \ud835\udc4aA\ud835\udc43@ , \ud835\udc43-D, so that the generated sequence is similar to the real sequence. The critic aims to find the optimal value function to better approximate the Wasserstein Distance.\nRelying solely on adversarial feedback is insufficient for the generator to capture the conditional distribution of data. To improve the similarity between two distributions more efficiently, we introduce two additional losses on the generator to discipline the learning. Firstly, the supervised loss \u2112\ud835\udcae is to directly describe the \ud835\udc3f1 distance between \ud835\udc50 and ?\u0302?:\n\u2112\ud835\udcae = \ud835\udd3c?\u0302\u223cE\",?\u223cE![\u2225 \ud835\udc50 \u2212 ?\u0302? \u2225] (14)\nSecondly, from the perspective of stock movement, the long-term stock market shows an increasing trend, hence the generator tends to construct sequences of higher return even with the detection of short-term fluctuations, which leads to the abnormally high false positive ratio in the classification performance. To address this problem, we design a weighted loss \u2112T which represents the error rate with more focus on False Positive (FP) samples than False Negative (FN) samples:\n\u2112T = 1 \ud835\udc5e s\n\u03bb#u\ud835\udc3c(\ud835\udc66! < 0, \ud835\udc66!x > 0) ,\n!\"#\n+ \u03bb8u\ud835\udc3c(\ud835\udc66! > 0, \ud835\udc66!x < 0) ,\n9\"#\nz (15)\n\u03bb# > \u03bb8, \u03bb# + \u03bb8 = 1, \u03bb# \u2208 (0,1), \u03bb8 \u2208 (0,1)\nwhere \ud835\udc3c(\u22c5) is the indicator function, the movement \ud835\udc66! = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc50!) is derived from the return, similar with the predicted movement \ud835\udc66|! = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(?\u0302?!), \ud835\udf06# and \ud835\udf068 are hyper-parameters to balance the error, the constraint \ud835\udf06# > \ud835\udf068 guarantees more penalty on FP ratio.\nBy integrating with the additional losses in adversarial feedback, the generator and critic networks are trained simultaneously as follows:\n\u2112\ud835\udca2 = \u2212\ud835\udd3c?\u0302\u223cE\"[\ud835\udc53?(?\u0302?)] + \ud835\udefe#\u2112\ud835\udcae + \ud835\udefe8\u2112T (16)\n\u2112># = \u2212\ud835\udd3c?\u223cE![\ud835\udc53?(\ud835\udc50)] + \ud835\udd3c?\u0302\u223cE\"[\ud835\udc53?(?\u0302?)] (17)\nwhere \u2112\ud835\udca2 and \u2112># are the objective functions for \ud835\udca2 and \ud835\udc53? respectively, \ud835\udefe# \u2265 0 and \ud835\udefe8 \u2265 0 are penalty parameters on supervised loss and weighted loss respectively.\nTo guarantee the Lipschitz constraint on the critic, it is necessary to force the weights of the critic to lie in a compact space. In the training process, we clip the weights to a fixed interval [\u2212\ud835\udefc, \ud835\udefc] after each gradient update."
        },
        {
            "heading": "4.4. Training Process",
            "text": "Figure 2 details the learning process for the generator \ud835\udca2 and critic \ud835\udc53? in the IndexGAN. Denote \ud835\udc9f!@<9$ as the training data containing input features \ud835\udc4b and real sequence \ud835\udc50, and denote \ud835\udc9f7<!?: as a subset of \ud835\udc9f!@<9$ for the training in each batch. We first initialize \ud835\udca2 and \ud835\udc53? . In each training iteration, the prior noise \ud835\udc67 sampled from Gaussian distribution and input features are fed into \ud835\udca2 to generate predicted sequence ?\u0302?, then critic estimates Wasserstein distance between real and fake sequences. We compute the gradient from critic loss \u2112># and update parameters \ud835\udee9? of \ud835\udc53? (line 8- 9), also clip all weights to accelerate them till optimality. We keep training critic for \ud835\udc5b?@9!9? times, then we focus on generator training. The reason for this iterative process is that gradient of Wasserstein becomes reliable as the number of iterations for critic increases. Usually, \ud835\udc5b?@9!9? is set to 5. For generator training, predicted sequences are generated by \ud835\udca2 and fed into \ud835\udc53?. Then \u2112\ud835\udca2 is calculated by Eq. 14-16 and parameters \u0398- of \ud835\udca2 is updated (lines 15-16). The number of epochs is set to 80. After the training in an adversarial manner, both \ud835\udca2 and \ud835\udc53? . with well-trained parameters are converged and used for deployment to real-world data.\nFigure 2 Training Process of IndexGAN"
        },
        {
            "heading": "4.5. Deployment",
            "text": "Once the training is completed, the model is ready to be deployed. Recalling that generated sequence from our proposed architecture covers \ud835\udc5e steps ahead, the predicted price can be at either location from 1 to \ud835\udc5e in the generated sequence. Thus, we propose a rolling strategy for deployment. To predict the target at time \ud835\udc61., the model is repeated for \ud835\udc5e rounds. For each round \ud835\udc57, where \ud835\udc57 = 1,\u2026 , \ud835\udc5e, the historical features in the time interval [\ud835\udc61. \u2212\ud835\udc64 + 1 \u2212 \ud835\udc57, \ud835\udc61. \u2212 \ud835\udc57] is learned to generate sequence at time [\ud835\udc61. + 1 \u2212 \ud835\udc57, \ud835\udc61. \u2212 \ud835\udc57 + \ud835\udc5e] where ?\u0302?! U is the predicted return at\n\ud835\udc57!: position. Following this method and rolling the window forward, we obtain a sequence of prediction ?\u0302?! U U\"# ,\nwith dynamic change. We take the average of the sequence to obtain the final predicted return ?\u0302?!. whose direction is the final predicted movement \ud835\udc66|!. at time \ud835\udc61\u2032. The rolling strategy considers the dynamic change of prediction since the trend of the stock market can be rapidly influenced by the various factors discussed earlier. Meanwhile, the averaged predicted return mitigates the noise caused by the uncertainty of the financial market. We expect the predicted horizon \ud835\udc5e to be 5 which can generate the best result considering every week has five trading days, and our model evaluation verifies this conjecture.\n5. MODEL EVALUATION To evaluate the performance of the proposed model, extensive experiments are carried out with real-world stock markets and news data. We will first introduce implementation details including data description, baseline models, evaluation metrics, and parameter settings. Then we will show the empirical results of compared models, forecasting horizon analysis, and ablation study to demonstrate the effectiveness and generality of the proposed model, IndexGAN. The code is publicly available1."
        },
        {
            "heading": "5.1. Datasets",
            "text": "Stock datasets contain the historical daily stock prices in Dow Jones Industrial Average (DJIA) and S&P 500 from Aug-08-2008 to Jul-01-2016, downloaded directly from Yahoo Finance. DJIA is one of the oldest equity indices, tracking merely 30 of the most highly capitalized and influential companies in major sectors of the U.S. market. S&P 500 is one of the most commonly followed indices and is composed of the 500 largest publicly traded companies. Both indices are used as reliable benchmarks of the economy. The chosen period covers a financial crisis and its following recovery period, which provides stability to our method. Volatility indices of the same period, VIX for S&P 500 and VXD for DJIA, are retrieved from Yahoo Finance. Both price and volatility features \ud835\udc60! are converted to return calculated by \ud835\udc60!/\ud835\udc60!)# \u2212 1. The employed technical indicators are calculated given the retrieved price data. The details are shown in Table 1.\nNews data is downloaded from Kaggle2 and is publicly available. It is crawled from Reddit WorldNews Channel, one of the largest social news aggregation websites. Content such as text posts, images, links, and videos are uploaded by users to the site and voted up or down by other users. The news and posts are related to popular events in various industry sectors in the world, presenting an overall attitude with a high potential to influence the market. News data contains the top 25 headlines ranked by users' votes for a single day. The headlines are processed by removing stop words, punctuation, and digits from the raw text.\nTo tune the parameters, both stock and news data are split into training, validation, and testing sets approximately as the ratio of 8:1:1 in chronological order to avoid data leakage, as shown in Table 2. To capture the dependency of time series, a lag window with a size of \ud835\udc64 time steps is moved along both stock and news data to construct sets of sequences."
        },
        {
            "heading": "5.2. Baselines",
            "text": "We compare the proposed IndexGAN with the following methods:\n\u2022 MA [49] is one of the most popular indicators used by momentum traders. 5-day EMA is chosen.\n\u2022 ARIMA [50] is a widely used statistical model. In our implementation, the parameters are determined automatically by the package 'pmdarima' in Python, regarding to the lowest AIC.\n\u2022 DA-RNN [19] integrates the attention mechanism with LSTM to extract input features and relevant hidden states.\n\u2022 StockNet [51] employs Variational Auto-Encoder (VAE) on tweets and historical prices to encode stock movements as probabilistic vectors.\n\u2022 A-LSTM [20] leverages adversarial training to add simulated perturbations on stock data to improve the robustness of stock predictions.\n\u2022 Trans [22] is the previous state-of-the-art model enhancing Transformer with trading gap splitter and gaussian prior to predicting stock movement.\n\u2022 DCGAN [52] is a powerful GAN that uses convolutional-transpose layers in the generator and convolutional layers in the discriminator.\n\u2022 GAN-FD [37] uses LSTM and convolutional layers to design a Vanilla GAN to predict stock pricing one step ahead.\n\u2022 S-GAN [45] employs FinBERT on sentiment scores and uses Vanilla GAN with convolutional layers and GRU to predict pricing.\nNote that MA and ARIMA use close prices only. DA-RNN, Adv-LSTM, Trans, DCGAN, and GAN-FD use market-based prices only, following their architecture designs. StockNet and SGAN use both market-based features and news data."
        },
        {
            "heading": "5.3. Parameter Settings",
            "text": "The proposed model IndexGAN is implemented in PyTorch and hyper-parameters are tuned based on the validation part. The length of historical sequence \ud835\udc64 is set to 35 and future steps \ud835\udc5e for prediction is set to 5. We use RMSprop as an optimizer. The details of the parameters in IndexGAN are shown in Table 3. For other baselines, we follow default hyper-parameter settings for implementation."
        },
        {
            "heading": "5.4. Evaluation Metrics",
            "text": "We evaluate the performance of all methods by three measures: Accuracy, F1 score, and Matthews Correlation Coefficient (MCC). Accuracy is the proportion of correctly predicted samples in all samples. F1 score incorporates both recall and precision. MCC takes all components of the confusion matrix into consideration, while the true negative is ignored in F1 score. Therefore, MCC is in a more balanced manner than F1 score. A low value of MCC close to 0 indicates that classes in prediction are highly imbalanced."
        },
        {
            "heading": "5.5.1. Comparison Results with Baselines",
            "text": ""
        },
        {
            "heading": "5.5. Experiment Results",
            "text": "structure of IndexGAN achieves two times of MCC increase, in general, because it implements the concept of WGAN and multi-step prediction with horizon-wise information."
        },
        {
            "heading": "5.5.2. Forecasting Horizon Analysis",
            "text": "We investigate the effects of the prediction horizon \ud835\udc5e on the proposed IndexGAN. Figure 3 shows the change of accuracy and MCC on DJIA and SPX. As \ud835\udc5e goes up from 2 to 5, the performance of the model shows a rising trend, since the critic can differentiate real and fake sequences as their length becomes longer. Both accuracy and MCC achieve the highest value when \ud835\udc5e is 5. However, as the length of the predicted sequence exceeds 5 days and continues increasing to 10 days, the quality of the model degrades. This reveals that it becomes harder to capture movement patterns if the forecasting length is too long. Hence, there exists a trade-off between the uncertainty of the stock market and the ability of GAN. Choosing the appropriate prediction horizon is critical to achieving significant performance. The experiment conducted on various lengths justifies our conjecture that 5 days forward is most effective because it is a normal and popular short trading period."
        },
        {
            "heading": "5.5.3. Ablation Study",
            "text": "We conduct experiments on ablation variants of IndexGAN to fully examine the predictive power gained from each component:\n\u2022 IndexGAN-Block: Blocks in news context learning are removed. The GloVe embedding matrix for each headline is flattened and concatenated with price features as input to be fed into seq2seq learning.\n\u2022 IndexGAN-News: News headlines are removed from input features. The noise and market-based features are concatenated together and fed into attentive seq2seq learning directly. Thus, news context learning is removed as well.\n\u2022 IndexGAN-Vol: The volatility index is removed from input features.\n\u2022 IndexGAN -WDist: Wasserstein distance is removed and Vanilla GAN is implemented.\n\u2022 IndexGAN-Attn: The attention layer in seq2seq learning is removed. The output from GRU in the encoder is directly presented as input to the decoder.\n\u2022 IndexGAN-Loss: \u2112\ud835\udcae and \u2112T are removed and only adversarial loss is used. Figure 4 exhibits the performance of variants. Firstly, IndexGAN-News, IndexGAN-Block, and IndexGAN-FC are employed to test the importance of news context learning. With market-based features only, the performance of IndexGAN-News decreases significantly. It validates that news reflects the information in the stock market and can efficiently boost performance. On the other hand, even with news, the performance of IndexGAN-Block is not ideal. It illustrates our notion that directly passing the embedding matrix into the seq2seq model is likely to mask the impact of price. When the embedding size is thousands of times greater than the price, the encoder and decoder concentrate on embedding entirely. This phenomenon reveals that merely depending on the news is challenging, and both price and news features are necessary for stock movement prediction. Compared with a single fully connected layer, the blocks containing nonlinear layers bring approximately a 3% accuracy increase on the S&P 500 and DJIA, and almost 0.05 of MCC increase on the S&P 500. This phenomenon justifies that nonlinear layers in the designed blocks can boost the predictive power efficiently. With the help of blocks that fully investigate the news context and shrink the size of the embedding matrix gradually, IndexGAN achieves the highest results.\nSecondly, IndexGAN-WDist does not perform well, especially MCC is close to 0 which is even worse than EMA, revealing low diversity of predicted movements and the mode collapse problem of IndexGAN-WDist. Simply leveraging Vanilla GAN cannot capture dynamics for movement prediction. It's necessary to employ the concept of Wasserstein distance to measure the similarity of prediction and ground truth, which improves the quality of the model significantly by accelerating the speed of convergence and avoiding mode collapse.\nThirdly, without the volatility index as a feature, the accuracy of Index-Vol on the S&P 500 decreases by 2.5% which is 1% more than the drop in accuracy on DJIA. The results indicate the necessity of a volatility index, especially for VIX which depends on more options than VDX.\nMoreover, by comparing the IndexGAN-Attn and IndexGAN-Loss with IndexGAN, the attention mechanism and two additional losses on the generator enhance the predictive power by over a 2% accuracy increase.\n6. CONCLUSION In this paper, we present a new formulation for predicting stock movement. This work is first to propose the use of Wasserstein Generative Adversarial Networks for multi-step stock movement prediction which includes particularly crafted designs for inherent characteristics of the stock market. Specifically, in the generator, IndexGAN first exploits prior noise to improve the robustness of the model, implements news context learning to explore embeddings from emerging news, and employs volatility index as one of the impacting factors. Then an attentive encoderdecoder architecture is designed for seq2seq learning to capture temporal dynamics and interactions between market-based features and word representations. The critic is used for approximating Wasserstein distance between the actual and predicted sequences. Moreover, IndexGAN adds two additional losses for the generator to constrain similarity efficiently and avoid biased movement during optimization. Also, to mitigate uncertainty, a rolling strategy for deployment in employed. Experiments conducted on DJIA and S&P 500 show the significant improvement of IndexGAN and the effectiveness of multi-step prediction over other state-of-art methods. The ablation study verifies the significant advantage of each component in the proposed IndexGAN as well. Given this, in future research, we will extend our work to other chaotic time series where it is difficult to predict future performance from past data, especially those areas driven by human factors (i.e., commerce market).\nREFERENCES [1] J. Bollen, H. Mao, and X. Zeng, \u201cTwitter mood predicts the stock market,\u201d Journal of\ncomputational science, vol. 2, no. 1, pp. 1\u20138, 2011.\n[2] R. Aguilar-Rivera, M. Valenzuela-Rend\u00b4on, and J. Rodr\u00b4\u0131guez-Ortiz, \u201cGenetic algorithms and darwinian approaches in financial applications: A survey,\u201d Expert Systems with Applications, vol. 42, no. 21, pp. 7684\u20137697, 2015.\n[3] B. G. Malkiel, A random walk down Wall Street: including a life-cycle guide to personal investing. WW Norton & Company, 1999.\n[4] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d Advances in neural information processing systems, vol. 27, 2014.\n[5] X. Zhou, Z. Pan, G. Hu, S. Tang, and C. Zhao, \u201cStock market prediction on high-frequency data using generative adversarial nets,\u201d Mathematical Problems in Engineering, vol. 2018, 2018.\n[6] M. Faraz and H. Khaloozadeh, \u201cMulti-step-ahead stock market prediction based on least squares generative adversarial network,\u201d in 2020 28th Iranian Conference on Electrical Engineering (ICEE). IEEE, 2020, pp. 1\u20136.\n[7] A. Srivastava, L. Valkov, C. Russell, M. U. Gutmann, and C. Sutton, \u201cVeegan: Reducing mode collapse in gans using implicit variational learning,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 3310\u20133320.\n[8] X. Ding, Y. Zhang, T. Liu, and J. Duan, \u201cDeep learning for eventdriven stock prediction,\u201d in Twenty-fourth international joint conference on artificial intelligence, 2015.\n[9] J. M. Poterba and L. H. Summers, \u201cThe persistence of volatility and stock market fluctuations,\u201d National Bureau of Economic Research, Tech. Rep., 1984.\n[10] K. R. French, G. W. Schwert, and R. F. Stambaugh, \u201cExpected stock returns and volatility,\u201d Journal of financial Economics, vol. 19, no. 1, pp. 3\u201329, 1987.\n[11] G. W. Schwert, \u201cStock market volatility,\u201d Financial analysts journal, vol. 46, no. 3, pp. 23\u201334, 1990.\n[12] I. K. Nti, A. F. Adekoya, and B. A. Weyori, \u201cA systematic review of fundamental and technical analysis of stock market predictions,\u201d Artificial Intelligence Review, pp. 1\u201351, 2019.\n[13] C. A. Assis, A. C. Pereira, E. G. Carrano, R. Ramos, and W. Dias, \u201cRestricted boltzmann machines for the prediction of trends in financial time series,\u201d in 2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 2018, pp. 1\u20138.\n[14] L.-C. Cheng, Y.-H. Huang, and M.-E. Wu, \u201cApplied attention-based lstm neural networks in stock prediction,\u201d in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 4716\u20134718.\n[15] O. B. Sezer and A. M. Ozbayoglu, \u201cAlgorithmic financial trading with deep convolutional neural networks: Time series to image conversion approach,\u201d Applied Soft Computing, vol. 70, pp. 525\u2013 538, 2018.\n[16] R. J. Hyndman and G. Athanasopoulos, Forecasting: principles and practice. OTexts, 2018.\n[17] T. Bollerslev, \u201cGeneralized autoregressive conditional heteroskedasticity,\u201d Journal of econometrics, vol. 31, no. 3, pp. 307\u2013327, 1986.\n[18] W. Yao, J. Gu, W. Du, F. P. Deek, and G. Wang, \u201cAdpp: A novel anomaly detection and privacy-preserving framework using blockchain and neural networks in tokenomics,\u201d International Journal of Artificial Intelligence & Applications, vol. 13, no. 6, pp. 17\u201332, 2022\n[19] Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. Cottrell, \u201cA dualstage attention-based recurrent neural network for time series prediction,\u201d arXiv preprint arXiv:1704.02971, 2017.\n[20] F. Feng, H. Chen, X. He, J. Ding, M. Sun, and T.-S. Chua, \u201cEnhancing stock movement prediction with adversarial training,\u201d arXiv preprint arXiv:1810.09936, 2018.\n[21] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[22] Q. Ding, S. Wu, H. Sun, J. Guo, and J. Guo, \u201cHierarchical multi-scale gaussian transformer for stock movement prediction.\u201d in IJCAI, 2020, pp. 4640\u20134646.\n[23] X. Li, Y. Li, H. Yang, L. Yang, and X.-Y. Liu, \u201cDp-lstm: Differential privacy-inspired lstm for stock prediction using financial news,\u201d arXiv preprint arXiv:1912.10806, 2019.\n[24] X. Zhang, H. Fuehres, and P. A. Gloor, \u201cPredicting stock market indicators through twitter \u201ci hope it is not as bad as i fear\u201d,\u201d Procedia-Social and Behavioral Sciences, vol. 26, pp. 55\u201362, 2011.\n[25] E. J. De Fortuny, T. De Smedt, D. Martens, and W. Daelemans, \u201cEvaluating and understanding text-based stock price prediction models,\u201d Information Processing & Management, vol. 50, no. 2, pp. 426\u2013441, 2014.\n[26] B. Zhao, Y. He, C. Yuan, and Y. Huang, \u201cStock market prediction exploiting microblog sentiment analysis,\u201d in 2016 International Joint Conference on Neural Networks (IJCNN). IEEE, 2016, pp. 4482\u20134488.\n[27] J. Tang and X. Chen, \u201cStock market prediction based on historic prices and news titles,\u201d in Proceedings of the 2018 International Conference on Machine Learning Technologies, 2018, pp. 29\u201334.\n[28] J. B. Nascimento and M. Cristo, \u201cThe impact of structured event embeddings on scalable stock forecasting models,\u201d in Proceedings of the 21st Brazilian Symposium on Multimedia and the Web, 2015, pp. 121\u2013124.\n[29] Y. Liu, Q. Zeng, H. Yang, and A. Carrio, \u201cStock price movement prediction from financial news with deep learning and knowledge graph embedding,\u201d in Pacific rim Knowledge acquisition workshop. Springer, 2018, pp. 102\u2013113.\n[30] C.-Y. Lee and V.-W. Soo, \u201cPredict stock price with financial news based on recurrent convolutional neural networks,\u201d in 2017 conference on technologies and applications of artificial intelligence (TAAI). IEEE, 2017, pp. 160\u2013165.\n[31] D. L. Minh, A. Sadeghi-Niaraki, H. D. Huy, K. Min, and H. Moon, \u201cDeep learning approach for short-term stock trends prediction based on twostream gated recurrent unit network,\u201d Ieee Access, vol. 6, pp. 55392\u2013 55404, 2018.\n[32] W. Li, R. Bao, K. Harimoto, D. Chen, J. Xu, and Q. Su, \u201cModeling the stock relation with graph network for overnight stock movement prediction,\u201d in Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, 2021, pp. 4541\u20134547.\n[33] M. Saito, E. Matsumoto, and S. Saito, \u201cTemporal generative adversarial nets with singular value clipping,\u201d in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2830\u20132839.\n[34] C. Esteban, S. L. Hyland, and G. R\u00a8atsch, \u201cReal-valued (medical) time series generation with recurrent conditional gans,\u201d arXiv preprint arXiv:1706.02633, 2017.\n[35] Y. Zhang, Z. Gan, and L. Carin, \u201cGenerating text via adversarial training,\u201d in NIPS workshop on Adversarial Training, vol. 21. academia. edu, 2016, pp. 21\u201332.\n[36] A. Gupta and J. Zou, \u201cFeedback gan for dna optimizes protein functions,\u201d Nature Machine Intelligence, vol. 1, no. 2, pp. 105\u2013111, 2019.\n[37] S. Takahashi, Y. Chen, and K. Tanaka-Ishii, \u201cModeling financial timeseries with generative adversarial networks,\u201d Physica A: Statistical Mechanics and its Applications, vol. 527, p. 121261, 2019.\n[38] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, \u201cMusegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment,\u201d in ThirtySecond AAAI Conference on Artificial Intelligence, 2018.\n[39] L. Yu, W. Zhang, J. Wang, and Y. Yu, \u201cSeqgan: Sequence generative adversarial nets with policy gradient,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.\n[40] J. Yoon, D. Jarrett, and M. Van der Schaar, \u201cTime-series generative adversarial networks,\u201d 2019.\n[41] M. Wiese, R. Knobloch, R. Korn, and P. Kretschmer, \u201cQuant gans: Deep generation of financial time series,\u201d Quantitative Finance, vol. 20, no. 9, pp. 1419\u20131440, 2020.\n[42] Y. Xu, Z. Zhang, L. You, J. Liu, Z. Fan, and X. Zhou, \u201cscigans: single-cell rna-seq imputation using generative adversarial networks,\u201d Nucleic acids research, vol. 48, no. 15, pp. e85\u2013e85, 2020.\n[43] L. Wu, Y. Xia, F. Tian, L. Zhao, T. Qin, J. Lai, and T.-Y. Liu, \u201cAdversarial neural machine translation,\u201d in Asian Conference on Machine Learning. PMLR, 2018, pp. 534\u2013549.\n[44] K. Zhang, G. Zhong, J. Dong, S. Wang, and Y. Wang, \u201cStock market prediction based on generative adversarial network,\u201d Procedia computer science, vol. 147, pp. 400\u2013406, 2019.\n[45] P. Sonkiya, V. Bajpai, and A. Bansal, \u201cStock price prediction using bert and gan,\u201d arXiv preprint arXiv:2107.09055, 2021.\n[46] M. Mirza and S. Osindero, \u201cConditional generative adversarial nets,\u201d arXiv preprint arXiv:1411.1784, 2014.\n[47] M. Arjovsky, S. Chintala, and L. Bottou, \u201cWasserstein generative adversarial networks,\u201d in International conference on machine learning. PMLR, 2017, pp. 214\u2013223.\n[48] Y. Rubner, C. Tomasi, and L. J. Guibas, \u201cThe earth mover\u2019s distance as a metric for image retrieval,\u201d International journal of computer vision, vol. 40, no. 2, pp. 99\u2013121, 2000.\n[49] F. Klinker, \u201cExponential moving average versus moving exponential average,\u201d Mathematische Semesterberichte, vol. 58, no. 1, pp. 97\u2013107, 2011.\n[50] S. Makridakis and M. Hibon, \u201cArma models and the box\u2013jenkins methodology,\u201d Journal of forecasting, vol. 16, no. 3, pp. 147\u2013163, 1997.\n[51] Y. Xu and S. B. Cohen, \u201cStock movement prediction from tweets and historical prices,\u201d in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 1970\u20131979.\n[52] A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d arXiv preprint arXiv:1511.06434, 2015."
        }
    ],
    "title": "STOCK BROAD-INDEX TREND PATTERNS LEARNING VIA DOMAIN KNOWLEDGE INFORMED GENERATIVE NETWORK",
    "year": 2023
}