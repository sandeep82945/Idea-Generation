{
    "abstractText": "We consider in discrete time, a general class of sequential stochastic dynamic games with asymmetric information with the following features. The underlying system has Markovian dynamics controlled by the agents\u2019 joint actions. Each agent\u2019s instantaneous utility depends on the current system state and the agents\u2019 joint actions. At each time instant each agent makes a private noisy observation of the current system state and the agents\u2019 actions in the previous time instant. In addition, at each time instant all agents have a common noisy observation of the current system state and their actions in the previous time instant. Each agent\u2019s actions are part of his private information. The objective is to determine Bayesian Nash Equilibrium (BNE) strategy profiles that are based on a compressed version of the agents\u2019 information and can be sequentially computed; such BNE strategy profiles may not always exist. We present an approach/methodology that achieves the above-stated objective, along with an instance of a game where BNE strategy profiles with the above-mentioned characteristics exist. We show that the methodology also works for the case where the agents have no common observations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi Ouyang"
        },
        {
            "affiliations": [],
            "name": "Hamidreza Tavafoghi"
        }
    ],
    "id": "SP:bfe84793c0d4df85e61ddbee4d1bb3374428a61a",
    "references": [
        {
            "authors": [
                "R Aumann",
                "M Maschler",
                "R Stearns"
            ],
            "title": "Repeated games with incomplete",
            "year": 1995
        },
        {
            "authors": [
                "P Barelli",
                "I Meneghel"
            ],
            "title": "A note on the equilibrium existence problem in discontinuous games",
            "year": 2013
        },
        {
            "authors": [
                "P Cardaliaguet",
                "C Rainer",
                "D Rosenberg"
            ],
            "title": "Markov games with frequent actions and incomplete information\u2014the limit case",
            "venue": "Mathematics of Operations Research",
            "year": 2015
        },
        {
            "authors": [
                "J Escobar",
                "J Toikka"
            ],
            "title": "Efficiency in games with Markovian private information",
            "year": 2013
        },
        {
            "authors": [
                "F Forges"
            ],
            "title": "Repeated games of incomplete information: non-zero-sum",
            "venue": "Handbook of Game Theory",
            "year": 1992
        },
        {
            "authors": [
                "F Gensbittel",
                "J Renault"
            ],
            "title": "The value of Markov chain games with incomplete information on both sides",
            "venue": "Mathematics of Operations Research",
            "year": 2015
        },
        {
            "authors": [
                "A Gupta",
                "A Nayyar",
                "C Langbort"
            ],
            "title": "Common information based Markov perfect equilibria for linear-Gaussian games with asymmetric information",
            "venue": "SIAM J Control Optim",
            "year": 2014
        },
        {
            "authors": [
                "A Gupta",
                "C Langbort",
                "T Ba\u015far"
            ],
            "title": "Dynamic games with asymmetric information and resource constrained players with applications to security of cyberphysical systems",
            "venue": "IEEE Transactions on Control of Network Systems",
            "year": 2016
        },
        {
            "authors": [
                "Y Ho"
            ],
            "title": "Team decision theory and information structures",
            "venue": "Proceedings of the IEEE",
            "year": 1980
        },
        {
            "authors": [
                "J H\u00f6rner",
                "T Sugaya",
                "S Takahashi"
            ],
            "title": "Recursive methods in discounted stochastic games: An algorithm for \u03b4 \u2192 1 and a folk theorem",
            "year": 2011
        },
        {
            "authors": [
                "D Kartik",
                "A Nayyar"
            ],
            "title": "Upper and lower values in zero-sum stochastic games with asymmetric information. Dynamic Games and Applications",
            "year": 2021
        },
        {
            "authors": [
                "P Kumar",
                "P Varaiya"
            ],
            "title": "Stochastic Systems: Estimation Identification and Adaptive Control",
            "year": 1986
        },
        {
            "authors": [
                "L Li",
                "J Shamma"
            ],
            "title": "Lp formulation of asymmetric zero-sum stochastic games",
            "venue": "IEEE conference on decision and control,",
            "year": 2014
        },
        {
            "authors": [
                "L Li",
                "J Shamma"
            ],
            "title": "Efficient strategy computation in zero-sum asymmetric repeated games",
            "year": 2017
        },
        {
            "authors": [
                "L Li",
                "C Langbort",
                "J Shamma"
            ],
            "title": "Solving two-player zero-sum repeated Bayesian games",
            "year": 2017
        },
        {
            "authors": [
                "G Mailath",
                "L Samuelson"
            ],
            "title": "Repeated Games and Reputations",
            "year": 2006
        },
        {
            "authors": [
                "A Nayyar",
                "A Mahajan",
                "D Teneketzis"
            ],
            "title": "Optimal control strategies in delayed sharing information structures",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2011
        },
        {
            "authors": [
                "A Nayyar",
                "A Gupta",
                "C Langbort"
            ],
            "title": "Common information based markov perfect equilibria for stochastic games with asymmetric information: Finite games",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2013
        },
        {
            "authors": [
                "A Nayyar",
                "A Mahajan",
                "D Teneketzis"
            ],
            "title": "Decentralized stochastic control with partial history sharing: A common information approach",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2013
        },
        {
            "authors": [
                "A Nayyar",
                "A Gupta",
                "C Langbort"
            ],
            "title": "Common information based Markov perfect equilibria for stochastic games with asymmetric information: Finite games",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2014
        },
        {
            "authors": [
                "Y Ouyang",
                "H Tavafoghi",
                "D Teneketzis"
            ],
            "title": "Dynamic oligopoly games with private Markovian dynamics",
            "venue": "IEEE Conference on Decision and Control (CDC)",
            "year": 2015
        },
        {
            "authors": [
                "Y Ouyang",
                "H Tavafoghi",
                "D Teneketzis"
            ],
            "title": "Dynamic games with asymmetric information: Common information based perfect Bayesian equilibria and sequential decomposition",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2017
        },
        {
            "authors": [
                "J Renault"
            ],
            "title": "The value of Markov chain games with lack of information on one side",
            "venue": "Math Oper Res",
            "year": 2006
        },
        {
            "authors": [
                "J Renault"
            ],
            "title": "The value of repeated games with an informed controller",
            "venue": "Mathematics of Operations Research",
            "year": 2012
        },
        {
            "authors": [
                "Sinha A",
                "Anastasopoulos"
            ],
            "title": "A (2016) Structured perfect Bayesian equilibrium in infinite horizon dynamic games with asymmetric information",
            "venue": "American Control Conference",
            "year": 2016
        },
        {
            "authors": [
                "T Sugaya"
            ],
            "title": "Efficiency in Markov games with incomplete and private information",
            "year": 2012
        },
        {
            "authors": [
                "D Tang",
                "H Tavafoghi",
                "V Subramanian"
            ],
            "title": "Dynamic games among teams with delayed intra-team information sharing",
            "venue": "Dynamic Games and Applications",
            "year": 2022
        },
        {
            "authors": [
                "H Tavafoghi",
                "Y Ouyang",
                "D Teneketzis"
            ],
            "title": "On stochastic dynamic games with delayed sharing information structure",
            "venue": "IEEE Conference on Decision and Control (CDC),",
            "year": 2016
        },
        {
            "authors": [
                "H Tavafoghi",
                "Y Ouyang",
                "D Teneketzis"
            ],
            "title": "A unified approach to dynamic decision problems with asymmetric information: Nonstrategic agents",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2022
        },
        {
            "authors": [
                "Vasal D",
                "Anastasopoulos"
            ],
            "title": "A (2016) Signaling equilibria for dynamic LQG games with asymmetric information",
            "venue": "IEEE Conference on Decision and Control (CDC),",
            "year": 2016
        },
        {
            "authors": [
                "HS Witsenhausen"
            ],
            "title": "A standard form for sequential stochastic control",
            "venue": "Mathematical Systems Theory",
            "year": 1973
        },
        {
            "authors": [
                "S Zamir"
            ],
            "title": "Repeated games of incomplete information: Zero-sum",
            "venue": "Handbook of Game Theory",
            "year": 1992
        },
        {
            "authors": [
                "J Zheng",
                "DA Casta\u00f1\u00f3n"
            ],
            "title": "Decomposition techniques for markov zero-sum games with nested information",
            "venue": "IEEE conference on decision and control,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\nKeywords: Dynamic games, asymmetric information, hidden actions, common information, information compression, sequential decomposition\n1"
        },
        {
            "heading": "1 Introduction",
            "text": "We study, in discrete time, a general class of sequential stochastic dynamic games with asymmetric information. We consider a setting where the underlying system has Markovian dynamics controlled by the agents\u2019 joint actions. Each agent\u2019s instantaneous utility depends on the agents\u2019 joint actions and the system state. At each time instant each agent makes a private noisy observation that depends on the current system state and the agents\u2019 actions in the previous time instant. In addition, at each time instant all agents may have a common noisy observation of the system state and their actions in the previous time instant. The agents\u2019 actions are hidden, that is, each agent\u2019s actions are not directly observable by the other agents. Therefore, at every time instant agents have asymmetric and imperfect information about the game\u2019s history. Dynamic games with the above features arise in engineering (cybersecurity, transportation, energy markets), in economics (industrial organization), and in socio-technological applications.\nAs pointed out in Tang et al (2022), the key challenges in the study of dynamic games with asymmetric information are: (i) The domain of agents\u2019 strategies increases with time, as the agents acquire information over time. Thus, the computational complexity of the agents\u2019 strategies increases with time. (ii) Due to signaling1 (Ho, 1980), in many instances an agent\u2019s assessment of the game\u2019s status at time t, therefore his strategy at time t, depends on the strategies of agents who acted before him. Consequently, we cannot obtain the standard sequential decomposition (that sequentially determines the components of an equilibrium strategy profile) of the kind provided by the standard dynamic programming algorithm (where the agent\u2019s optimal strategy at any time t does not depend on past strategies (Kumar and Varaiya, 1986, Chapter 6.5)).\nTo address these challenges, we can look for equilibrium strategy profiles that are based on a compressed version of the agents\u2019 information and can be sequentially computed. However, such equilibrium strategy profiles may not exist.\nIn this paper we propose an approach, described in detail in Section 3, that addresses the above-stated challenges. According to this approach, we first compress the agents\u2019 private and common information at each time instant. Then, we define strategies based on the compressed information and show that Bayesian Nash Equilibria (BNE) based on these strategies can be determined sequentially in time moving backwards, if each step of this backwards procedure has a solution. Finally, we provide an example where a BNE strategy profile based on compressed information exists.\nWe show that the proposed approach works for the case where the agents have no common observations and their actions are hidden.\n1Signaling in games is more complex than signaling in teams because the agents have diverging incentives and their strategies are their own private information."
        },
        {
            "heading": "1.1 Related Literature",
            "text": "Dynamic games with asymmetric information have been extensively investigated in the literature in the context of repeated discounted games; see Zamir (1992); Forges (1992); Aumann et al (1995); Mailath and Samuelson (2006) and the references therein. The key feature of these games is the absence of a dynamic system. Moreover, the works on repeated games study primarily their asymptotic properties when the horizon is infinite and agents are sufficiently patient (i.e. the discount factor is close one). In repeated games, agents play a stage (static) game repeatedly over time. The main objective of this strand of literature is to explore situations where agents can form self-enforcing punishment/reward mechanisms so as to create additional equilibria that improve upon the payoffs they can get by simply playing an equilibrium of the stage game over time. Recent works (see Ho\u0308rner et al (2011); Escobar and Toikka (2013); Sugaya (2012)) adopt approaches similar to those used in repeated games to study infinite horizon dynamic games with asymmetric information when there is an underlying dynamic Markovian system. Under certain conditions on the system dynamics and information structure, the authors of Ho\u0308rner et al (2011); Escobar and Toikka (2013); Sugaya (2012) characterize a set of asymptotic equilibria attained when the agents are sufficiently patient.\nThe problem we study in this paper is different from the ones in Zamir (1992); Forges (1992); Aumann et al (1995); Mailath and Samuelson (2006); Ho\u0308rner et al (2011); Escobar and Toikka (2013); Sugaya (2012) in two aspects. First, we consider a class of dynamic games where the underlying system has general Markovian dynamics and a general information structure, and we do not restrict attention to asymptotic behaviors when the horizon is infinite and the agents are sufficiently patient. Second, we study situations where the decision problem that each agent faces, in the absence of strategic interactions with other agents, is a Partially Observed Markov Decision Process (POMDP), which is a complex problem to solve by itself. Therefore, reaching (and computing) a set of equilibrium strategies, which take into account the strategic interactions among the agents, is a very challenging task. As a result, it is not very plausible for the agents to seek reaching equilibria that are generated by the formation of self-enforcing punishment/reward mechanisms similar to those used in infinitely repeated games. We believe that our results provide new insight into the behavior of strategic agents in complex and dynamic environments, and complement the existing results in the repeated games literature.\nStochastic dynamic zero-sum games with asymmetric information have been studied in Renault (2006); Cardaliaguet et al (2015); Gensbittel and Renault (2015); Li et al (2017); Kartik and Nayyar (2021); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014). The authors of Renault (2006); Cardaliaguet et al (2015); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014) study zerosum games with Markovian dynamics and lack of information on one side (i.e. one informed and one uninformed agent). The authors of Gensbittel and Renault (2015); Li et al (2017); Kartik and Nayyar (2021) study zero-sum\ngames with Markovian dynamics and lack of information on both sides. The works of Renault (2006); Cardaliaguet et al (2015); Gensbittel and Renault (2015); Li et al (2017); Kartik and Nayyar (2021); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014) consider specific information structures. Specifically: the actions of both agents are publicly observed; in Renault (2006); Cardaliaguet et al (2015); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014) the informed agent observes perfectly the state of the dynamic system, the other agent has no direct observation of the system\u2019s state; in Gensbittel and Renault (2015); Li et al (2017) each agent observes perfectly part of the system\u2019s state and the states observed by the two agents are either independent or conditionally independent (given the observed actions). The authors of Kartik and Nayyar (2021) consider a general information structure where each agent has some private information and the agents share some information about the dynamic system\u2019s state and their actions. The authors of Renault (2006); Cardaliaguet et al (2015); Gensbittel and Renault (2015); Li et al (2017); Kartik and Nayyar (2021); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014) derive their results by taking advantage of properties of zerosum games such as the interchangeability of equilibrium strategies and the unique value of the game. These properties do not extend to non-zero sum games. We study a general class of stochastic dynamic games that include zero-sum stochastic dynamic games with asymmetric information as a special case. We consider general Markovian dynamics for the underlying system in contrast to Renault (2006); Cardaliaguet et al (2015); Gensbittel and Renault (2015); Li et al (2017); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014), where the system has the special structure described above. We consider a general information structure that allows us to capture scenarios with unobservable actions and imperfect observations that are not captured by Renault (2006); Cardaliaguet et al (2015); Gensbittel and Renault (2015); Li et al (2017); Zheng and Castan\u0303o\u0301n (2013); Li and Shamma (2014).\nThe problems investigated in Tang et al (2022); Nayyar et al (2014); Gupta et al (2014); Ouyang et al (2015, 2017); Vasal and Anastasopoulos (2016); Sinha and Anastasopoulos (2016); Gupta et al (2016); Nayyar et al (2013a) are the most closely related to our problem. The authors of Nayyar et al (2014); Gupta et al (2014, 2016); Nayyar et al (2013a) study a class of dynamic games where the agents\u2019 common information based belief (defined in Nayyar et al (2014)) is independent of their strategies, that is, there is no signaling among them. This property allows them to apply ideas from the common information approach developed in Nayyar et al (2011, 2013b), and define an equivalent dynamic game with symmetric information among fictitious agents. Consequently, they characterize a class of equilibria for dynamic games called Common Information based Markov Perfect Equilibria.\nOur results are different from those in Nayyar et al (2014); Gupta et al (2014, 2016); Nayyar et al (2013a) in two aspects. First, we consider a general class of dynamic games where the agents\u2019 CIB beliefs are strategy-dependent, thus, signaling is present. Second, the proposed approach in Nayyar et al\n(2014); Gupta et al (2014, 2016); Nayyar et al (2013a) requires the agents to keep track of all of their private information over time. We propose an approach to effectively compress the agents\u2019 private information, and consequently, reduce the number of variables which the agents need to form CIB beliefs.\nThe authors of Tang et al (2022); Ouyang et al (2015, 2017); Vasal and Anastasopoulos (2016); Sinha and Anastasopoulos (2016) study a class of dynamic games with asymmetric information where signaling occurs. When the horizon in finite, the authors of Ouyang et al (2015, 2017) introduce the notion of Common Information Based Perfect Bayesian Equilibrium, and provide a sequential decomposition of the game over time. The authors of Vasal and Anastasopoulos (2016); Sinha and Anastasopoulos (2016) extend the results of Ouyang et al (2015, 2017) to finite horizon Linear-Quadratic-Gaussian (LQG) dynamic games and infinite horizon dynamic games, respectively.\nThe work of Tang et al (2022) extends the model of Ouyang et al (2017) to games among teams of agents. Each agent has his own private information which he shares with the members of his own team with delay d; teams also have common information. The authors of Tang et al (2022) consider two classes of strategies: sufficient private information based (SPIB) strategies, which only compress private information, and sufficient private and common information based (SPCIB) strategies, which compress both common and private information. They show that SPIB-strategy-based BNE exist and the set of payoff profiles of such equilibria is the same as the set of all BNE. They develop a backward inductive sequential procedure, whose solution, if it exists, provides a SPCIB BNE, and identify instances which guarantee the existence of SPCIB BNE. The class of dynamic games studied in Tang et al (2022); Ouyang et al (2015, 2017); Vasal and Anastasopoulos (2016); Sinha and Anastasopoulos (2016) satisfy the following assumptions: (i) agents\u2019 actions are observable (ii) each agent has a perfect observation of his own local states/type (iii) conditioned on the agents\u2019 actions, the evolution of the local states are independent. We relax assumptions (i)-(iii) of Tang et al (2022); Ouyang et al (2015, 2017); Vasal and Anastasopoulos (2016); Sinha and Anastasopoulos (2016), and study a general class of dynamic games with asymmetric information, hidden actions, imperfect observations, and controlled and coupled dynamics."
        },
        {
            "heading": "1.2 Contribution",
            "text": "We study/analyze, in discrete time, a general class of sequential stochastic dynamic games with asymmetric information, where the underlying system is dynamic, the information structure is non-classical, at each time instant the agents have private and common information and their actions are hidden (each agent\u2019s actions are not directly observable by the other agents). Our key contribution is a methodology for the discovery of Bayesian Nash Equilibrium (BNE) strategy profiles that are based on the agents\u2019 compressed private and\ncommon information and can be determined sequentially in time moving backwards, if each step of this backward procedure has a solution. We present an example where such a BNE strategy profile exists. We show that our methodology works also for the case where the agents have no common observations and their actions are hidden."
        },
        {
            "heading": "1.3 Organization",
            "text": "The rest of the paper is organized as follows: We present the game\u2019s model along with the equilibrium concept in Section 2. We state our objective and present the methodology that achieves it in Section 3. In Section 4 we first introduce compressed versions of the agents\u2019 private and common information that are sufficient for decision making purposes; then we define Sufficient Information Based (SIB) strategies that are based on the agents\u2019 compressed information. In Section 5 we first introduce Sufficient Information Based Bayesian Nash Equilibrium (SIB-BNE); then we present a sequential decomposition of the game, that is, a backward inductive procedure that determines SIB-BNE if each step of this procedure has a solution. In Section 6 we present an example that highlights our solution methodology and where a SIB-BNE exists. In Section 7 we show that our solution methodology works for stochastic dynamic games where the agents have no common observations and each agent\u2019s actions are part of his private information. The comparison of the definitions of compressed private information as it appears in this paper and in Tavafoghi et al (2022), along with some of the technical details related to the existence of SIB-BNE for the example of Section 6 are presented in the Appendices."
        },
        {
            "heading": "2 Model",
            "text": "We present our model for dynamic decision problems with strategic agents (dynamic games) below; this model is an analogue to the model of Tavafoghi et al (2022) for dynamic decision problems with non-strategic agents."
        },
        {
            "heading": "2.1 System Dynamics",
            "text": "There are N strategic agents who live in a dynamic Markovian world over horizon T := {1, 2, ..., T }, T <\u221e. Let Xt \u2208 Xt denote the state of the world at t\u2208 T . At time t, each agent, indexed by i\u2208N := {1, 2, ..., N}, chooses an action ait\u2208A i t, where A i t denotes the set of available actions to him at t. Given the collective action profile At := (A 1 t , ..., A N t ), the state of the world evolves according to the following stochastic dynamic equation,\nXt+1 = ft(Xt, At,W x t ), (1)\nwhereW x1:T\u22121 is a sequence of independent random variables. The initial state X1 is a random variable that has a probability distribution \u00b50 \u2208 \u2206(X1).\nAt every time t \u2208 T , before taking an action, agent i receives a noisy private observation Y it \u2208 Y i t of the current state of the world Xt and the action profile At\u22121, given by\nY it = O i t(Xt, At\u22121,W i t ), (2)\nwhereW i1:T , i \u2208 N , are sequences of independent random variables. Moreover, at every t \u2208 T , all agents receive a common observation Zt \u2208 Zt of the current state of the world Xt and the action profile At\u22121, given by\nZt = O c t (Xt, At\u22121,W c t ), (3)\nwhere W c1:T , is a sequence of independent random variables. We assume that the random variables X1, W x 1:T\u22121, W c 1:T , and W i 1:T , i \u2208 N are mutually independent. To avoid measure-theoretic technical difficulties and for clarity and convenience of exposition, we assume that all the random variables take values in finite sets.\nAssumption 1. (finite game) The sets N , Xt, Zt, Y i t , A i t, i \u2208 N , are finite."
        },
        {
            "heading": "2.2 Information Structure",
            "text": "Let Ht denote the aggregate information of all agents at time t. Assuming that agents have perfect recall, we have Ht = {Z1:t, Y 1:N1:t , A 1:N 1:t\u22121}, i.e. Ht denotes the set of all agents\u2019 past and present observations and all agents\u2019 past actions. The set of all possible realizations of the agents\u2019 aggregate information is given by Ht := \u220f \u03c4\u2264tZ\u03c4 \u00d7 \u220f i\u2208N \u220f \u03c4\u2264t Y i \u03c4 \u00d7 \u220f i\u2208N \u220f \u03c4<tA i \u03c4 .\nAt time t\u2208T , the aggregate information Ht is not fully known to all agents. Let Ct := {Z1:t} \u2208 Ct denote the agents\u2019 common information about Ht and P it := {Y i 1:t, A i 1:t\u22121}\\Ct \u2208 P i t denote agent i\u2019s private information about Ht, where P it and Ct denote the set of all possible realizations of agent i\u2019s private and common information at time t, respectively. We assume that observations Y i\u03c4 , \u03c4 \u2208 {1, 2..., t}, and actions A i \u03c4 , \u03c4 \u2208 {1, 2..., t\u22121}, are known to agent i but are not necessarily fully known to all other agents, denoted by \u2212i, at t \u2208 T . Therefore, we have P it \u2286 {Y i 1:t, A i 1:t\u22121} for all i \u2208 N , and Ht = ( \u22c3 i\u2208N P i t )\n\u222aCt for all t \u2208 T . As such, {\nCt, P i t , i \u2208 N\n}\nform a partition of Ht at every time t \u2208 T . In Section 2.5, we discuss several instances of information structures that can be captured as special cases of our model."
        },
        {
            "heading": "2.3 Strategies and Utilities:",
            "text": "Let Hit := {Ct, P i t } \u2208 H i t denote the information available to agent i at t, where Hit denote the set of all possible realizations of agent i\u2019s information at t. Agent i\u2019s behavioral strategy at t, denoted by git, is defined by\ngit : H i t \u2192 \u2206(A i t) (4)\nwhere \u2206(Ait) is the set of Probability Mass Functions (PMFs) on A i t. We denote by\ngi := (gi1, g i 2, . . . , g i T ) (5)\na strategy of agent i; gi \u2208 Gi, where Gi is the set of admissible strategies described by (4)-(5). We denote a strategy profile g by\ng := (g1, g2, . . . , gN) (6)\ng \u2208 G, where G is the set of admissible strategy profiles described by (4)-(6). We denote by\ng\u2212i := (g1, . . . , gi\u22121, gi+1, . . . , gN) (7)\nAgent i\u2019s instantaneous utility at t depends on the system state Xt and the collective action profile At, and is given by u i t(Xt,At). Agent i\u2019s total utility over horizon T , is given by,\nU i(X1:T , A1:T ) = \u2211\nt\u2208T\nuit(Xt, At). (8)"
        },
        {
            "heading": "2.4 Equilibrium Concept:",
            "text": "We consider Bayesian Nash Equilibrium (BNE) as the solution concept (Fudenberg and Tirole, 1991). A strategy profile g\u2217 = (g\u22171, g\u22172, . . . , g\u2217N) is a BNE if for all i \u2208 N\nE g\u2217{U i(X1:T , A1:T )} \u2265 E g\u2217\u2212i,g\u0302i{U i(X1:T , A1:T )}, \u2200g\u0302 i \u2208 Gi. (9)"
        },
        {
            "heading": "2.5 Special Cases",
            "text": "We discuss several instances of dynamic games with asymmetric information that are special cases of the general model described above.\n1) Nested information structure: Consider a two-player game with one informed player and one uninformed player and general Markovian dynamics. At every time t\u2208T , the informed player makes a private perfect observation of the state Xt, i.e. Y 1 t =Xt. The uninformed player does not have any observation of the state Xt. Both the informed and uninformed players observe each others\u2019 actions, i.e. Zt={At\u22121}. Therefore, we have P 1t = {X1:t}, P 2 t = \u2205, and Ct={A11:t\u22121,A 2 1:t\u22121} for all t\u2208T . The above nested information structure corresponds to dynamic games considered in Renault (2006); Cardaliaguet et al (2015); Renault (2012); Li and Shamma (2014, 2017); Zheng and Castan\u0303o\u0301n (2013), where in Renault (2012); Li and Shamma (2017) the state Xt is static.\n2) Delayed sharing information structure: Consider a N -player game with observable actions where agents observe each others\u2019 observations with d-step\ndelay. That is, P it = {Y i t\u2212d+1:t} and Ct = {Y1:t\u2212d, A1:t\u22121}. We note that in our model we assume that the agents\u2019 common observation Zt at t is only a function of Xt and and At\u22121. Therefore, to describe the game with delayed sharing information structure within the context of our model we need to augment our state space to include the agents\u2019 last d observations as part of the augmented state. Define X\u0303t := {Xt,M1t ,M 2 t , ...,M d t } as the augmented system state whereM it := {At\u2212i, Yt\u2212i} \u2208 At\u2212i\u00d7Yt\u2212i, i \u2208 N ; that is,M i t serves as a temporal memory for the agents\u2019 observation Yt\u2212i at t\u2212 i. Then, we have X\u0303t+1 = {Xt+1,M1t+1,M 2 t+1, ...,M d t+1} = {ft(Xt, At,W x t ), (Yt),M 1 t , ...,M d\u22121 t } and Zt = {Mdt , At\u22121} = {Yt\u2212d, At\u22121}. The above environment captures a connection between the symmetric information structure and asymmetric information structure. The information asymmetry among the agents increases as d increases. The above delayed sharing information structure corresponds to the dynamic game considered in Tavafoghi et al (2016).\n3) Perfectly controlled dynamics with hidden actions: Consider a N -player game where the state Xt :=(X 1 t ,X 2 t ,...,X N t ) has N components. Agent i, i\u2208N , perfectly controls X it , i.e. X i t+1 = A i t. Agent i\u2019s actions A i t, t \u2208 T , are not observable by all other agents \u2212i. Every agent i, i\u2208N , makes a noisy private observation Y ti (Xt,W i t ) of the system state at t\u2208T . Therefore, we have P i t := {A1:t, Y i1:t}, Ct=\u2205."
        },
        {
            "heading": "3 Objective and Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Objective",
            "text": "Our objective is twofold: (i) To determine BNE strategy profiles that are based on compressed versions of the agents\u2019 private and common information. (ii) To compute the above-mentioned strategy profiles by a sequential decomposition of the game, that is, by a backward inductive sequential procedure that identifies an equilibrium strategy profile when every step of the procedure has a solution."
        },
        {
            "heading": "3.2 Methodology",
            "text": "We present a methodology that achieves the above-state objective and proceeds as follows:\n\u2022 Step 1. We determine a mutually consistent compression of the agents\u2019 private information that is sufficient for decision-making purposes (such a mutually consistent compression may not be unique). Based on this compression we introduce the Sufficient Private Information Based (SPIB) belief system. \u2022 Step 2. Based on the result of Step 1, we determine a compression of the agents\u2019 common information that is sufficient for decision-making purposes by defining the Common Information Based (CIB) belief system. The CIB belief system ensures that at each time instant each agent\u2019s CIB\nbelief is consistent with his SPIB belief even when the agent deviates from his equilibrium strategy and plays an arbitrary strategy. Such a consistency implies that each agent forms his own CIB belief system, and each agent\u2019s CIB belief system is common knowledge among all agents. \u2022 Step 3. Based on the compression of the agents\u2019 private and common information we introduce Sufficient Information Based (SIB) strategies for each agent (i.e., strategies that depend at each time on the agent\u2019s sufficient private information and the CIB belief system) and SIB BNE. We show that SIB strategies satisfy a key closedness of best response property. Based on this property we provide a sequential decomposition of the game, that is, a backward inductive sequential procedure that determines a SIB BNE if each step of the procedure has a solution. \u2022 Step 4. We provide an example of a stochastic dynamic game with asymmetric information and hidden/unobservable actions where a SIB BNE exists."
        },
        {
            "heading": "4 Compression of Private and Common Information",
            "text": "In Section 4.1 we characterize/determine mutually consistent compressions of all agents\u2019 private information that are sufficient for decision-making purposes. In Section 4.2 we introduce the common information based belief, a compressed version of the agents\u2019 common information, that is sufficient for decision making purposes."
        },
        {
            "heading": "4.1 Sufficient private information (Step 1)",
            "text": "We present/consider a compression of the agents\u2019 private information that is done in a mutually consistent manner so that the compressed information is sufficient for decision making purposes.\nDefinition 1 (Sufficient private information). We say that Sit , i = 1, . . . , N , is sufficient private information for the agents if (i) Sit is a function of H i t such that S i t = \u03b6 i t(H i t) for some commonly known functions \u03b6it , i = 1, 2, . . . , N . (ii) Sit can be sequentially updated as S i t = \u03c6 i t(S i t\u22121, Y i t , Zt, A i t\u22121) using some commonly known functions \u03c6it, i = 1, 2, . . . , N . (iii) For any realization xt, p \u2212i t , p i t, ct, and the corresponding s \u2212i t = \u03b6 \u2212i t (p \u2212i t , ct)\nand sit = \u03b6 i t(p i t, ct), and any strategy profile g, where g i t : S i t \u00d7 Ct \u2192 \u2206(Ait), \u2200i, \u2200t, such that P g(pit, ct) > 0,\nP g(xt, s \u2212i t | s i t, ct) = P g(xt, s \u2212i t | p i t, ct) (10)\nRemark 1. A similar definition of sufficient private information for dynamic teams appears in (Tavafoghi et al, 2022, Definition 2). This definition is slightly\ndifferent from Definition 1 above because the objectives in Tavafoghi et al (2022) and this paper are different. In Appendix .1 we show that sufficient private information satisfying Definition 1 may violate condition (ii) of Definition 2 in Tavafoghi et al (2022). In Tavafoghi et al (2022) the compression of private (and common) information must entail no loss in performance, that is, we must be able to determine globally optimal team strategy profiles that are based on compressed private and common information. In this paper the goal is to determine BNE strategy profiles that are based on compressed information and be sequentially computed (if such BNE strategy profiles exist). We are not concerned about the equilibria we may lose when we compress information; therefore, we don\u2019t need condition (ii) of Definition 2 in Tavafoghi et al (2022).\nDefinition 1 characterizes a set of compressions for agents\u2019 private information. In the following, we show the set of sufficient private information Sit , i \u2208 N , t \u2208 N , is rich enough to form belief systems on information sets of realizations with positive or zero probability. Let g\u0303i denote the uniform strategy that assigns equal probability to every action of agent i \u2208 N . Below we show that the policy-independence property of belief (Tavafoghi et al, 2022, Theorem 1) for agent i is still true when the private information pit is replaced with the sufficient private information sit. That is, P g\u0303i,g\u2212i(xt, x \u2212i t | s i t, ct) constructed by (g\u0303i, g\u2212i) captures agent i\u2019s belief based on hit even when he plays an arbitrary strategy g\u0302i, not necessarily the same as gi or g\u0303i, provided that agents \u2212i play g\u2212i.\nLemma 1. For hit such that P g\u0302i,g\u2212i(hit) > 0, we have P g\u0303i,g\u2212i(hit) > 0 and\nP g\u0302i,g\u2212i(xt, s \u2212i t | h i t) = P g\u0303i,g\u2212i(xt, s \u2212i t | h i t) = P g\u0303i,g\u2212i(xt, s \u2212i t | s i t, ct). (11)\nProof Note that Pg\u0303 i\n(ait) = 1/|A i t|, so P g\u0303i,g\u2212i(hit) > 0 given that P g(hit) > 0. Then\nfrom part (i) of the definition of sufficient private information and part (i) of Theorem 1 in Tavafoghi et al (2022) we have\nP g\u0302i,g\u2212i(xt, s \u2212i t | h i t) =\n\u2211\nh \u2212i t :\u03b6 \u2212i t (h \u2212i t )=s \u2212i t\nP g\u0302i,g\u2212i(xt, h \u2212i t | h i t)\n= \u2211\nh \u2212i t :\u03b6 \u2212i t (h \u2212i t )=s \u2212i t\nP g\u0303i,g\u2212i(xt, h \u2212i t | h i t)\n=Pg\u0303 i,g\u2212i(xt, s \u2212i t | h i t). (12)\nFurthermore, from condition (iii) of the definition of sufficient private information we have\nP g\u0303i,g\u2212i(xt, s \u2212i t | h i t) = P g\u0303i,g\u2212i(xt, s \u2212i t | s i t, ct). (13)"
        },
        {
            "heading": "4.2 CIB Belief System (Step 2)",
            "text": "Given the compressed private information, we next compress the agents\u2019 common information in the form of a belief system. We call such a compressed belief system the Common Information Based (CIB) belief system. Similar to Tang et al (2022); Ouyang et al (2017), the CIB belief system is sufficient for decision-making if it is common knowledge among all agents, and every agent i can compute his belief about the system state and the other agents\u2019 sufficient private information using the CIB belief system and his compressed private information. More specifically, agent i should be able to compute P g\u0302i,g\u2212i(xt, st | h i t) using the CIB belief system and his sufficient private information sit whenever other agents follow the strategy profile g \u2212i and agent i plays an arbitrary strategy g\u0302i. To determine a CIB belief system that satisfies the above sufficiency requirement we proceed as follows. We first define N CIB belief systems \u03a0\u03c8 := {\u03a0\u03c8,1,\u03a0\u03c8,2, . . . ,\u03a0\u03c8,N}, one for each agent (Definition 2 below). Each belief system \u03a0\u03c8,i consists of a sequence of PMFs on Xt \u00d7 St that are sequentially updated according to an update rule \u03c8 = (\u03c81, \u03c82, . . . , \u03c8N ) that is common knowledge among the agents; for each realization ct of the common information available at t, \u03c0\u03c8,it describes the belief on Xt\u00d7St based on ct from agent i\u2019s point of view. We want \u03c0\u03c8,it , combined with s i t, to enable agent i to form his own sufficient information-based private belief (given by Pg\u0302 i,g\u2217\u2212i(xt, st | sit, ct)) about the current status of the game. Furthermore, we want the CIB belief system to capture the current status of the game when agents utilize strategies based on (St,\u03a0 \u03c8 t ). For that matter, we define the notion/concept of Sufficient Information Based (SIB) strategy profile \u03c3 := (\u03c3i, i \u2208 N ), \u03c3i := (\u03c3it, t \u2208 T ), i \u2208 N . Each component \u03c3it of \u03c3 is a function of s i t, agent i\u2019s sufficient private information at t, and \u03c0\u03c8t = (\u03c0 \u03c8,i t , i \u2208 N ) (see Definition 3 below). Using the N CIB belief systems and the SIB strategy profile \u03c3 we define update equations for each \u03c0\u03c8,it so that each \u03c0 \u03c8,i t is consistent with s i t and with agent i\u2019s sufficient private information-based belief Pg\u0302 i,g\u2217\u2212i(xt, st | s i t, ct), defined in Section 4.1 (Definition 1), and each \u03c0\u03c8,it is common knowledge among all agents (see Definition 4 below). We proceed with the (formal) definitions.\nDefinition 2 (Common information based (CIB) belief system). Given a sequence of update functions \u03c8 = {\u03c8it, i \u2208 N , t \u2208 T } that are common knowledge among the N agents, sequentially define\n\u03a0\u03c8,it = \u03c8 i t(\u03a0 \u03c8 t\u22121, Zt), i \u2208 N , t \u2208 T (14)\nwhere\n\u03a0\u03c8t :=\n\n  \u03a0\u03c8,1t ...\n\u03a0\u03c8,Nt\n\n  , t \u2208 T (15)\n\u03a0\u03c80 :=\n\n  \u00b50 ... \u00b50\n\n  (16)\nThe sequence \u03a0\u03c81:T = (\u03a0 \u03c8 1 ,\u03a0 \u03c8 2 , . . . ,\u03a0 \u03c8 T ) defines a CIB belief system; \u03a0 \u03c8,i t denotes the CIB belief over Xt \u00d7St based on Ct from agent i\u2019s point of view.\nDefinition 3 (SIB strategy). Given a CIB belief system \u03a0\u03c81:T , we define a Sufficient Information Based (SIB) strategy profile \u03c3 := (\u03c31, \u03c32, . . . , \u03c3N ), \u03c3i := (\u03c3i1, \u03c3 i 2, . . . , \u03c3 i T ) by the maps\n\u03c3it : S i t \u00d7 [\u2206(Xt \u00d7 St)] N \u2192 \u2206(At), t = 1, 2, . . . , i = 1, 2, . . . , N. (17)\nBased on Definitions 2 and 3 we present a set of conditions that an individual CIB belief system (\u03a0\u03c8,it , t \u2208 T ) must satisfy so as to ensure that each agent i can form his own (private) belief about the current status of the game, given by (Xt, St), using \u03a0 \u03c8 t and S i t when all other agents \u2212i employ SIB strategies \u03c3\u2212i. This set of conditions describe a sequential update rule of \u03a0\u03c8,it ; the update rule depends on whether or not the (new) common observation at t is feasible under the agents\u2019 strategies.\nDefinition 4 (Consistent CIB belief system). Consider a SIB strategy profile \u03c3. Let F it (xt+1, st+1, zt+1)(\u03c0 \u03c8 t ; \u03c3 \u2212i t ) denote the CIB belief about (xt+1, st+1, zt+1) constructed recursively by assuming that (i) (xt, st) is distributed according to \u03c0\u03c8,it (ii) agent i employs the uniform strategy g\u0303 i at t (i.e., the strategy that chooses every action ait \u2208 A i t with equal probability), and (iii) agent \u2212i plays according \u03c3\u2212it . That is,\nF i0(x1, s1, z1) = \u2211\ny1\n[\nP{z1, y1 | x1}\u00b50(x1)\n(\n\u220f\nj\n1{sj1 = \u03c6 j 1(z1, y j 1)}\n)]\n(18)\nat t = 1, and for t \u2265 1.\nF it (xt+1, st+1, zt+1)(\u03c0 \u03c8 t ; \u03c3 \u2212i t )\n= \u2211\nyt+1,xt,st,at\n[\nP{zt+1, yt+1, xt+1 | xt, at}\n(\n\u220f\nj\n1{sjt+1 = \u03c6 j t+1(s j t , zt+1, y j t+1, a j t )}\n)\n\n\n1\n| Ait |\n\u220f\nj 6=i\n\u03c3jt (a j t )(\u03c0 \u03c8 t , s j t )\n\n\u03c0\u03c8,it (xt, st)\n]\n(19)\nWe define the update rule \u03c8\u03c3 = (\u03c8\u03c3,it , i \u2208 N , t \u2208 T ) and the corresponding\nCIB belief system \u03a0\u03c8 \u03c3\n1:T as follows. At any t\n(i) If \u2211 x\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t ) > 0 (i.e. the new common\nobservation zt+1 is feasible from the agent i\u2019s point of view), then \u03c0 \u03c8\u03c3 ,i t+1 can be updated recursively as\n\u03c0\u03c8 \u03c3 ,i t+1 (xt+1, st+1) = F it (xt+1, st+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t )\n\u2211\nx\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t )\n, (20)\nvia Bayes rule. (ii) If \u2211\nx\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t ) = 0 (i.e. the new common\nobservation zt+1 is infeasible from the agent i\u2019s point of view), then the update rule is\n\u03c0\u03c8 \u03c3 ,i t+1 (xt+1, st+1) = 1\n|Xt+1 \u00d7 St+1| . (21)\nBased on (20) and (21) we can write\n\u03a0\u03c8 \u03c3 ,i\nt+1 = \u03c8 \u03c3,i t+1(\u03a0\n\u03c8\u03c3 t , Zt+1). (22)\n\u03a0\u03c8 \u03c3\nt+1 = \u03c8 \u03c3 t+1(\u03a0\n\u03c8\u03c3 t , Zt+1). (23)\nFurthermore, for all i \u2208 N , each agent can determine if \u2211\nx\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c3\u03c8\nt ; \u03c3 \u2212i t ) is positive or zero; thus each agent\nknows how agent i computes \u03c0\u03c8 \u03c3 ,i\nt+1 from \u03c3 i t, zt+1, \u03c3 \u2212i t and \u03c8\n\u03c3. Therefore, \u03c0\u03c8 \u03c3 ,i\nt\n(hence \u03c0\u03c8 \u03c3 t ) is common knowledge among all agents. We call \u03a0 \u03c8\u03c3\n1:T the CIB belief system consistent with the SIB strategy profile \u03c3.\nRemark 2. Since the sufficient private information is a function of the agent\u2019s available information, a SIB strategy \u03c3it corresponds to a strategy g i,\u03c3 t given by gi,\u03c3t (h i t) := \u03c3 i t(\u03b6 i t(h i t), \u03c0 \u03c8\u03c3 t ). Therefore, in the rest of the paper we use the following convention: P\u03c3(\u00b7) = Pg \u03c3 (\u00b7) and E\u03c3[\u00b7] = Eg \u03c3 [\u00b7].\nRemark 3. There are many alternative specifications of the update rule \u03c8\u03c3t , t \u2208 T defined by (22)-(23), that result in consistent CIB belief systems, that is, CIB belief systems which ensure that (i) agent i can form his private belief over (Xt, S \u2212i t ) by incorporating his private sufficient information Sit into his CIB belief \u03a0 \u03c8\u03c3 ,i t given that agents \u2212i play according to \u03c3\n\u2212i, (ii) agent i\u2019s private belief formed according to i is identical to the probability distribution over (Xt, S \u2212i t ) conditional on his complete history H i t even when he plays an arbitrary strategy g\u0302i different from \u03c3i. An example of such an alternative update rule is described by (20) (Bayes\u2019 rule) when \u2211\nx\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t ) > 0 and a arbitrary PMF \u03c0 \u03c8\u03c3 ,i t+1 (\u00b7, \u00b7)\non Xt+1 \u00d7 St+1 when \u2211 x\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t ) = 0.\nDefinition 4 ensures that agent i can form his beliefs over (Xt, S \u2212i t ) by incorporating his sufficient private information Sit into his CIB belief \u03a0 \u03c8\u03c3 ,i t given that agents \u2212i play according to \u03c3\u2212i. Moreover, this belief is sufficient to compute the probability distribution over (Xt, S \u2212i t ) conditional on his complete history Hit even when he plays an arbitrary strategy g\u0302 i different from \u03c3i. We formalize the above discussion in Lemma 2 below, by using the notation P g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (\u00b7) to indicate the belief resulting when agent i plays g\u0302i and agents \u2212i play g\u2212i,\u03c3(h\u2212it ) = \u03c3 \u2212i t (\u03b6 \u2212i t (h \u2212i t ), \u03c0 \u03c8\u03c3 t ) using the update rule \u03c8 \u03c3 .\nLemma 2. Consider a SIB strategy profile \u03c3, along with an associated consistent CIB belief system \u03a0\u03c8 \u03c3\nt . Suppose (xt, h i t, h \u2212i t is a realization with positive\nprobability under (g\u0302i, \u03c3\u2212i), where g\u0302i denotes an arbitrary strategy for agent i. Let sit = \u03b6 i t(h i t) and s \u2212i t = \u03b6 \u2212i t (h \u2212i t ) be the associated sufficient private information. Then agent i\u2019s belief at time t can be computed using \u03c0\u03c8 \u03c3\nt as\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3(xt, s \u2212i t | h i t) =\n\u03c0\u03c8 \u03c3 ,i\nt (xt, st) \u2211\ns \u2212i t ,xt\n\u03c0\u03c8 \u03c3 ,i\nt (xt, s i t, s \u2212i t )\n(24)\nProof From Lemma 1 we have\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (xt, s \u2212i t | h i t) = P g\u0303i,\u03c3\u2212i,\u03c8\u03c3 (xt, s \u2212i t | h i t). = P g\u0303i,\u03c3\u2212i,\u03c8\u03c3 (xt, s \u2212i t | ct, s i t). (25)\nBy Bayes\u2019 rule we obtain\nP g\u0303i,\u03c3\u2212i,\u03c8\u03c3 (xt, s \u2212i t | ct, s i t) =\nP g\u0303i,\u03c3\u2212i,\u03c8\u03c3 (xt, st | ct)\nP g\u0303i,\u03c3\u2212i,\u03c8\u03c3 (sit | ct)\n= \u03c0\u03c8 \u03c3,i t (xt, st)\n\u2211\ns \u2212i t ,xt\n\u03c0\u03c8 \u03c3,i\nt (xt, s i t, s \u2212i t )\n.\n(26)\nCombination of (25) and (26) establishes the assertion of Lemma 2.\nRemark 4. Suppose Xt = (X 1 t , X 2 t , . . . . , X N t ) and we have the conditional independence property, namely, that for any strategy profile g Pg(xt, st | ct) = \u220f\ni P gi(xit, s i t | ct). Then one can show for any i that\n\u03c0\u03c8 \u03c3 ,i t (xt, st) = \u220f\nj\n\u03c0\u03c8 \u03c3 ,i(xjt , s j t ) = P g\u0303it(xit, s i t | ct)\n\u220f\nj 6=i\nP \u03c3j (xjt , s j t | ct)\nTherefore, for settings with the conditional independence property as in Tang et al (2022); Ouyang et al (2017), one can use the simplified beliefs Pg\u0303 i t(xit, s i t | ct) and P \u03c3j (xjt , s j t | ct) as the compressed common information to compute the CIB belief \u03c0\u03c8 \u03c3 ,i\nt (xt, st). The conditional independence among the system components in the models of Tang et al (2022); Ouyang et al (2017) could be lost when the agents\u2019 actions are not observable."
        },
        {
            "heading": "5 Sequential decomposition (Step 3)",
            "text": "In this section we present a sequential decomposition of the game, that is, a backward inductive sequential procedure that determines a Sufficient Information Based Bayesian Nash Equilibrium (SIB-BNE), defined below, if each step of this procedure has a solution. We proceed as follows. We first establish a key closedness of best response property (Section 5.1); we use this property to provide a sequential decomposition of the game (Section 5.2)\nDefinition 5 (SIB-BNE). Consider a SIB strategy profile \u03c3\u2217 = (\u03c3\u22171, \u03c3\u22172, . . . , \u03c3\u2217n) and its corresponding consistent update rule \u03c8\u03c3 \u2217\n. The SIB strategy profile \u03c3\u2217 is a SIB-BNE if it is a BNE of the dynamic game. That is, for all i \u2208 N ,\nE g\u0302i,\u03c3\u2217\u2212i,\u03c8\u03c3\n\u2217\n{U i(X1:T , A1:T )} \u2264 E \u03c3\u2217,\u03c8\u03c3\n\u2217\n{U i(X1:T , A1:T )},\nfor all strategies (not necessarily SIB strategies) g\u0302i. (27)"
        },
        {
            "heading": "5.1 Closedness of best response",
            "text": "The key result of this subsection is presented in the following theorem.\nTheorem 1. Consider a fixed and known SIB strategy profile \u03c3 and the corresponding update rule \u03c8\u03c3. Suppose agents \u2212i use \u03c3\u2212i with \u03c8\u03c3. Then, there exists a SIB strategy \u03c3\u0302i that uses \u03c8\u03c3 and is a best response to \u03c3\u2212i with \u03c8\u03c3.\nThe proof is based on Lemmas 3, 4, and 5 that we state and prove below.\nLemma 3. Consider a SIB strategy profile \u03c3 and the corresponding update rule \u03c8\u03c3 along with the consistent CIB belief system \u03a0\u03c8 \u03c3\n1:T . If agents \u2212i play according to the SIB strategies \u03c3\u2212i and use the update rule \u03c8\u03c3, the best response problem for agent i is a POMDP with state and observation processes\nX\u0303t = (St,\u03a0 \u03c8\u03c3 t , Xt), t \u2208 T (28) Y\u0303t = (Y i t , Zt), t \u2208 T (29)\nrespectively, and instantaneous utility\nu\u0303it(X\u0303t, A i t) =\n\u2211\na \u2212i t\n(\n\u220f\nj 6=i\n\u03c3jt (a j t | S j t ,\u03a0\n\u03c8\u03c3 t ) ) uit(Xt, a \u2212i t , A i t), t \u2208 T (30)\nThe assertion of Lemma 3 is a direct consequence of Lemmas 4 and 5.\nLemma 4. Consider a SIB strategy profile \u03c3 and the corresponding update rule \u03c8\u03c3. Suppose agents \u2212i play according to the SIB strategies \u03c3\u2212i using \u03c8\u03c3\nand agent i follows an arbitrary strategy g\u0302i (not necessarily a SIB strategy). Then\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3(x\u0303t+1, y\u0303t+1 | x\u03031:t, y\u03031:t, a i 1:t) = P g\u0302i\u03c3\u2212i,\u03c8\u03c3(x\u0303t+1, y\u0303t+1 | x\u0303t, a i t) (31)\nProof The probability for the next state and observation x\u0303t+1, y\u0303t+1 can be computed by\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (x\u0303t+1, y\u0303t+1 | x\u03031:t, y\u03031:t, a i 1:t)\n=Pg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 (xt+1, \u03c0 \u03c8\u03c3 t+1, st+1, y i t+1, zt+1 | x1:t, \u03c0 \u03c8\u03c3 1:t , s1:t, y i 1:t, z1:t, a i 1:t) = \u2211\ny \u2212i t+1,a \u2212i t\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (xt+1, \u03c0 \u03c8\u03c3 t+1, st+1, yt+1, zt+1, a \u2212i t | x1:t, \u03c0 \u03c8\u03c3 1:t , s1:t, y i 1:t, z1:t, a i 1:t)\n= \u2211\ny \u2212i t+1,a \u2212i t\n(\n\u220f\nj\n1(sjt+1 = \u03c6 j t+1(s j t , y j t+1, zt+1, a j t )) ) P{zt+1, yt+1, xt+1 | xt, at}\n1(\u03c0\u03c8 \u03c3\nt+1 = \u03c8 \u03c3 t+1(\u03c0\n\u03c8\u03c3 t , zt+1)) ( \u220f\nj 6=i\n\u03c3jt (a j t | s j t , \u03c0\n\u03c8\u03c3 t ) )\n(32)\nwhere the last equality follows from the system dynamics, part (ii) of Definition 1, Definition 4, and the form of SIB strategies of agents \u2212i. Since the right hand side of (32) depends only on (x\u0303t, a i t) we conclude that\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (x\u0303t+1, y\u0303t+1 | x\u03031:t, y\u03031:t, a i 1:t) = P g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (x\u0303t+1, y\u0303t+1 | x\u0303t, a i t) (33)\nLemma 4 shows that {X\u0303t, Y\u0303t, t \u2208 T } is a Markov process conditional on {Ait, t \u2208 T }\nLemma 5. Consider a SIB strategy profile \u03c3 and the corresponding update rule \u03c8\u03c3. Suppose agents \u2212i follow the SIB strategies \u03c3\u2212i using \u03c8\u03c3 and agent i follows an arbitrary strategy g\u0302i (not necessarily a SIB strategy). Then there are utility functions u\u0303it such that E g\u0302i,\u03c3\u2212i,\u03c8\u03c3 [u\u0303it(X\u0303t, A i t)] = E\ng\u0302i,\u03c3\u2212i,\u03c8\u03c3 [uit(Xt, At)] for all t \u2208 T .\nProof Recall that X\u0303t = (St,\u03a0 \u03c8\u03c3\nt , Xt). Then\nE g\u0302i,\u03c3\u2212i,\u03c8\u03c3 [uit(Xt, At)]\n=Eg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 [uit(Xt, A \u2212i t , A i t)] =Eg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 [ E g\u0302i,\u03c3\u2212i,\u03c8\u03c3 [uit(Xt, A \u2212i t , A i t) | X\u0303t, A i t] ] =Eg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 [ \u2211\na \u2212i t\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (a\u2212it | St,\u03a0 \u03c8\u03c3 t , Xt, A i t)u i t(Xt, a \u2212i t , A i t)] ]\n=Eg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 [ \u2211\na \u2212i t\n(\n\u220f\nj 6=i\n\u03c3jt (a j t | S j t ,\u03a0\n\u03c8\u03c3 t ) ) uit(Xt, a \u2212i t , A i t)] ]\n(34)\nTherefore, we establish the claim of the lemma by defining\nu\u0303it(X\u0303t, A i t) =\n\u2211\na \u2212i t\n(\n\u220f\nj 6=i\n\u03c3jt (a j t | S j t ,\u03a0\n\u03c8\u03c3 t ) ) uit(Xt, a \u2212i t , A i t)] (35)\nProof of Theorem 1 From Lemma 3 we conclude that the best response of agent i to \u03c3\u2212i is a POMDP with state X\u0303t. From the theory of POMDP (Kumar and Varaiya, 1986, Chapter 6) we know that: (i) the belief on the state X\u0303t = (St,\u03a0 \u03c8\u03c3\nt , Xt)\nconditioned on available information hit is an information state for the agent; (ii) for each t \u2208 T there exists an optimal strategy for agent i that is a function of the information state at t. We now prove that (Sit ,\u03a0 \u03c8\u03c3\nt ) is an information state for agent i at t, t \u2208 T . We note that Sit+1 = \u03c6 i t(S i t , Y i t+1, Zt+1, A i t) from part (ii) of Definition 1, and \u03a0 \u03c8\u03c3 t+1 = \u03c8\u03c3t+1(\u03a0 \u03c8\u03c3 t , Zt+1) from (23).\nThus, we only need to show that for any strategy g\u0302i and any realization hit such\nthat Pg\u0302 i,\u03c3\u2212i,\u03c8\u03c3 (hit) > 0 the following equality is true:\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (st, \u03c0 \u03c8\u03c3 t , xt | h i t) = P g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (st, \u03c0 \u03c8\u03c3 t , xt | s i t, \u03c0 \u03c8\u03c3 t ) (36)\nFor that matter, we note that sit, \u03c0 \u03c8\u03c3\nt are perfectly known to agent i. Furthermore, from the definition of sufficient private information and Lemma 2 we have\nP g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (s\u2212it , xt | h i t) =\n\u03c0\u03c8 \u03c3,i\nt (st, xt) \u2211\ns \u2212i t ,xt\n\u03c0\u03c8 \u03c3,i\nt (s i t, s \u2212i t , xt)\n, (37)\nwhich is a function of (sit, \u03c0 \u03c8\u03c3 t ). Therefore, P g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (st, \u03c0 \u03c8\u03c3 t , xt | h i t) = 1(s i t = \u03b6 i t(h i t))1(\u03c0 \u03c8\u03c3 t = \u03b3 \u03c8\u03c3 (hit))P g\u0302i,\u03c3\u2212i,\u03c8\u03c3 (s\u2212it , xt | p i t, ct)\n(38)\nwhere \u03b3\u03c8 \u03c3\n(hit) = \u03c8 \u03c3 t (\u03c8 \u03c3 t\u22121, \u00b7 \u00b7 \u00b7 ) is the composition of \u03c8 \u03c3 from 1 to t. Then, equation\n(36) is true because of (37) and (38). Consequently, (Sit ,\u03a0 \u03c8\u03c3\nt ), t \u2208 T is an information state for the best response problem for agent i and the assertion of Theorem 1 is true.\nAs a result of Theorem 1, a definition of SIB BNE equivalent to Definition 5 is the following\nDefinition 6 (Equivalent definition of SIB BNE). Consider a SIB strategy profile \u03c3\u2217 = (\u03c3\u22171, \u03c3\u22172, . . . , \u03c3\u2217n) and its corresponding consistent update rule \u03c8\u03c3 \u2217 . The SIB strategy profile \u03c3\u2217 is a SIB BNE if for all i \u2208 N ,\nE \u03c3i,\u03c3\u2217\u2212i,\u03c8\u03c3\n\u2217\n{U i(X1:T , A1:T )} \u2264 E \u03c3\u2217,\u03c8\u03c3\n\u2217\n{U i(X1:T , A1:T )} (39)\nfor all \u03c3i \u2208 \u039bi where \u039bi is the set of SIB strategy profiles of agent i.\nA consequence of Lemmas 3-5 and Theorem 1 is the following. Consider a SIB strategy profile \u03c3, the corresponding update rule \u03c8\u03c3 along with the consistent CIB belief system \u03a0\u03c8 \u03c3\n1:T ; if agents \u2212i play according to \u03c3 \u2212i, then the\nbest response of agent i could be determined by the dynamic program\nV\u0306 iT+1(\u00b7, \u00b7) = 0 for all i (40)\nV\u0306 it (\u03c0 \u03c8\u03c3 t , s i t) = max\n\u03c3\u0303it\u2208\u039b i t\nE \u03c3\u0303it,\u03c3 \u2212i t ,\u03c8 \u03c3\n{uit(Xt, At) + V\u0306 i t+1(\u03c8 \u03c3 t+1(\u03c0\n\u03c8\u03c3 t , Zt+1), S i t+1) | s i t},\n\u2200\u03c0\u03c8 \u03c3\nt \u2208 \u2206(Xt \u00d7 St) N , \u2200sit \u2208 S i s, t \u2208 T (41)\nwhere \u039bit is the set of SIB strategies of agent i at time t."
        },
        {
            "heading": "5.2 Sequential decomposition",
            "text": "Given a set of value functions Vt+1 = {V it+1 : \u03a0t+1 \u00d7S i t+1 \u2192 R, i \u2208 N}, a SIB strategy profile \u03c3, the corresponding update rule \u03c8\u03c3t+1 defined by (23), and the consistent CIB belief \u03c0\u03c8 \u03c3 t , define the stage-game Gt(Vt+1, \u03c0 \u03c8\u03c3\nt ) as follows. (i) There are N agents. (ii) The system state is Xt. (iii) Each agent i\nobserves private information Sit and common information \u03c0 \u03c8\u03c3\nt . (iv) Agent i\u2019s belief about the state Xt and other agents\u2019 private information S \u2212i t is given by \u03c0\u03c8 \u03c3,i\nt (xt, s \u2212i t ), that is,\n\u03c0\u03c8 \u03c3 ,i\nt (xt, s \u2212i t ) \u2208 \u2206(Xt \u00d7 S \u2212i t ). (42)\n(v) Each agent i selects action Ait based on his available information; let \u03c3\u0302 i t denote agent i\u2019s strategy for this stage-game; then,\nP \u03c3\u0302t,\u03c8\n\u03c3\n(Ait = a i t | s i t, \u03c0\n\u03c8\u03c3 t ) = \u03c3\u0302 i t(a i t | s i t, \u03c0 \u03c8\u03c3 t ). (43)\n(vi) Each agent i has utility\nU i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) = uit(Xt, At) + V i t+1(\u03c8 \u03c3 t+1(\u03c0\n\u03c8\u03c3 t , Zt+1), S i t+1) (44)\nwhere (Zt+1, S i t+1) conditioned on (Xt, St, At) follows the conditional probability \u2211\nxt+1,s \u2212i t+1\nP(zt+1, xt+1, st+1 | xt, st, at) and the conditional probability\nP(zt+1, xt+1, st+1 | xt, st, at) is given by\nP(zt+1, xt+1, st+1 | xt, st, at)\n= \u2211\nyt+1\nP{xt+1 | xt, at}P{zt+1, yt+1 | xt+1, at}\n(\n\u220f\nj\n1{sjt+1 = \u03c6 j t+1(s j t , zt+1, y j t+1, a j t )}\n)\n(45)\n(vii) Given a strategy profile \u03c3\u0302t for the stage-game, the expected utility of each player i is given by\nE \u03c3\u0302t,\u03c8\n\u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit]\n= \u2211\nxt,s \u2212i t ,at,zt+1,xt+1,st+1\n\u03c0\u03c8 \u03c3 ,i t (xt, s \u2212i t ) \u220f\nj\n\u03c3\u0302jt (a i t | s i t, \u03c0\n\u03c8\u03c3 t )P(zt+1, xt+1, st+1 | xt, st, at)\n(uit(xt, at) + V i t+1(\u03c8 \u03c3 t+1(\u03c0\n\u03c8\u03c3 t , zt+1), s i t+1)) (46)\nNote that all the random variables of the stage-game Gt(Vt+1, \u03c0 \u03c8\u03c3\nt ) may not necessarily be the same as their counterparts in the original dynamic game since each agent i is allowed to choose an arbitrary SIB strategy \u03c3\u0302it which may be different from \u03c3it specified by the SIB strategy profile \u03c3. The stage-game random variables will coincide with their counterparts in the original game if all agents follow \u03c3.\nTheorem 2 (Sequential decomposition). Consider a SIB strategy profile \u03c3 = {\u03c3t, t \u2208 T } and the corresponding update rule \u03c8\u03c3 = {\u03c8\u03c3t , t \u2208 T } defined by (22)-(23). Define\nV iT+1(\u00b7, \u00b7) = 0 for all i (47)\nV it (\u03c0 \u03c8\u03c3t , sit) = E \u03c3t,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit] (48)\nwhere the right hand side of (48) is given by (46). If for all t \u2208 T , there is a SIB strategy profile \u03c3\u0302t such that \u03c3\u0302t is a BNE of the stage-game Gt(Vt+1, \u03c0 \u03c8\u03c3\nt ), that is,\nE \u03c3\u0302it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit] = max\n\u03c3\u0303it\u2208\u039b i t\nE \u03c3\u0303it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit] (49)\nfor all i \u2208 N where \u039bit is the set of SIB strategies of agent i at time t, and\n\u03c3\u0302t = \u03c3t, (50)\nthen the SIB strategy profile \u03c3 is a SIB-BNE of the original dynamic game.\nProof Suppose that for all t \u2208 T there is a SIB strategy profile \u03c3\u0302t = (\u03c3\u0302 1 t , \u03c3\u0302 2 t , . . . , \u03c3\u0302 N t ) that is a BNE of the stage game Gt(Vt+1, \u03c0 \u03c8\u03c3 t ). Then for all \u03c0 \u03c8\u03c3 t \u2208 \u2206(Xt\u00d7St) N , sit \u2208\nSis\nE \u03c3\u0302it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit]\n= max \u03c3\u0303it\u2208\u039b i t\nE \u03c3\u0303it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3 [uit(Xt, At) + V i t+1(\u03c8 \u03c3 t+1(\u03c0 \u03c8\u03c3 t , Zt+1), S i t+1) | s i t]. (51)\nEquation (51) holds for all t \u2208 T with V iT+1(\u00b7, \u00b7) = 0 and for all i \u2208 N . When \u03c3\u0302t = \u03c3t for all t \u2208 T , Equation (51) gives, for all \u03c0\u03c8 \u03c3\nt \u2208 \u2206(Xt \u00d7 St) N , sit \u2208 S i s,\nV it (\u03c0 \u03c8\u03c3 t , s i t) =E \u03c3it,\u03c3 \u2212i t ,\u03c8 \u03c3 [U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit]\n= max \u03c3\u0303it\u2208\u039b i t\nE \u03c3\u0303it,\u03c3 \u2212i t ,\u03c8 \u03c3 [uit(Xt, At) + V i t+1(\u03c8 \u03c3 t+1(\u03c0 \u03c8\u03c3 t , Zt+1), S i t+1) | s i t]\n(52)\nfor all i \u2208 N . By induction, (52), and the fact that the update rule \u03c8\u03c3 is consistent with \u03c3 we have, for all i \u2208 N and t \u2208 T ,\nE \u03c3\u0303it:T ,\u03c3 \u2212i t:T ,\u03c8 \u03c3 [\nT \u2211\n\u03c4=t\nui\u03c4 (X\u03c4 , A\u03c4 ) | s i \u03c4 ] \u2264 E\n\u03c3it:T ,\u03c3 \u2212i t:T ,\u03c8 \u03c3\n[\nT \u2211\n\u03c4=t\nui\u03c4 (X\u03c4 , A\u03c4 ) | s i \u03c4 ] (53)\nThen (53) at time t = 1 gives\nE \u03c3\u0303i,\u03c3\u2212i,\u03c8\u03c3{U i(X1:T , A1:T )} \u2264 E \u03c3,\u03c8\u03c3{U i(X1:T , A1:T )} (54)\nfor all \u03c3\u0303i \u2208 \u039bi for all i \u2208 N . Therefore, the strategy profile \u03c3 is a SIB-BNE of the original dynamic game (sf. Definition 6).\nRemark 5. Note that even when the stage-game Gt(Vt+1, \u03c0 \u03c8\u03c3\nt ) has a BNE \u03c3\u0302t, it is possible that \u03c3\u0302t 6= \u03c3t. Thus, the existence of BNE for every stagegame Gt(Vt+1, \u03c0 \u03c8\u03c3\nt ) is not sufficient to establish the existence of BNE for the original dynamic game.\nRemark 6. In the model of Tang et al (2022) when each team consists of one agent, a SIB BNE coincides with a SPCIB BNE introduced in Tang et al (2022) with an appropriate mapping of the information state as discussed in Remark 4.\nRemark 7. There may not be a solution for the set of value functions in the sequential decomposition equations described by (47)-(50) for all i \u2208 N and for all t \u2208 T .\nRemark 8. In Definition 4, (21) could be defined differently, and different (21) would lead to different choices of \u03c8. And for any choice of (21), the claim of Theorem 2 will still hold.\nRemark 9. The value functions of the sequential decomposition equations defined by Theorem 2 (Eqs. (47)-(50) for all i \u2208 N , t \u2208 T ) may not be continuous in the CIB belief \u03a0\u03c8 \u03c3\nt ."
        },
        {
            "heading": "6 An illustrative example (Step 4)",
            "text": "In Section 5 we argued (cf. Remark 7) that the sequential decomposition equations defined by (47)-(50) for all i \u2208 N , t \u2208 T may not have a solution, and that the value functions defined by (47)-(50) may not be continuous in the CIB belief \u03a0\u03c8 \u03c3\nt (cf. Remark 9). In this section we present an example that illustrates/highlights the above remarks. In the example, a two-stage stochastic dynamic game, the agents\u2019 utilities depend on a parameter c. We show that: (i) the value functions of the corresponding sequential decomposition equations are not continuous in the CIB belief \u03a0\u03c8 \u03c3\nt ; (ii) for certain values of c a SIB-BNE exists."
        },
        {
            "heading": "6.1 Model",
            "text": "We consider the following two-stage stochastic dynamic game. There are two players/agents, Alice and Bob. At stage one, t = 1, the system\u2019s state X1 is distributed on {\u22121, 1} with \u00b50(\u22121) = P(X1 = \u22121) = 0.5 and \u00b51(1) = P(X1 = 1) = 0.5. Alice observes perfectly X1, i.e., Y Alice 1 = X1, and takes action AAlice1 \u2208 {\u22121, 1}; A Alice 1 is not observable by Bob and Y Bob 1 = \u2205. Bob does not act at t = 1. At stage 2, t = 2, the system state is X2 = X1A Alice 1 . Alice and Bob have a common observation Z2 = X2A Alice 1 W1 = X1W1, where W1 \u2208 {\u22121, 1} and P(Z = i | X1 = i) = 1 \u2212 p = 0.8, i \u2208 {\u22121, 1}, and there are no private observations, i.e., Y Alice2 = Y Bob 2 = \u2205. Here p = 0.2 = P(W1 = \u22121). Bob acts at t = 2. Alice does not act at t = 2. Bob\u2019s action ABob2 \u2208 {\u22121, 1}. Alice\u2019s payoffs at t = 1 and t = 2 are\nuAlice1 (X1, A1) =\n{\nc if AAlice1 = 1 0 if AAlice1 = \u22121\n(55)\nand\nuAlice2 (X2, A2) =\n\n\n\n2 if X2 = 1, A Bob 2 = 1 1 if X2 = \u22121, A Bob 2 = \u22121\n0 otherwise (56)\nrespectively. Bob\u2019s payoffs are uBobt (Xt, At) = \u2212u Alice t (Xt, At), t = 1, 2.\nThe game\u2019s information structure is\nHAlice1 ={X1} (57) HAlice2 ={X1, A Alice 1 , X2, Z2} (58)\nHBob1 =\u2205 (59) HBob2 ={Z2} (60)\nwhere HAlicet , H Bob t , t = 1, 2, describe the information available to Alice and Bob, respectively, at stages 1 and 2.\nThis example has the same dynamics and utility functions as Example 3 in Tang et al (2022), but Bob doesn\u2019t observe Alice\u2019s action as in (Tang et al, 2022, Example 3)."
        },
        {
            "heading": "6.2 Sequential decomposition",
            "text": "Since Alice perfectly observes the state at both times, i.e., Y Alice1 = X1 and Y Alice2 = X2, and Bob doesn\u2019t have private information, S Alice 1 = X1, S Bob 1 = \u2205 are sufficient private information for Alice and Bob at stage t = 1, respectively, and SAlice2 = X2, S Bob 2 = \u2205 are sufficient private information for Alice and Bob, respectively, at stage t = 2 according to Definition 1. Suppose \u03c3 = (\u03c31, \u03c32) = (\u03c3 Alice 1 , \u03c3 Bob 2 ) is a SIB strategy and \u03c8\n\u03c3 is the corresponding update rule. Here \u03c3 is an equilibrium strategy candidate which serves as the strategy prediction for Alice and Bob. Note that \u03a0\u03c8\n\u03c3 ,Alice 1 (x1) =\n\u00b50(x1) and \u03a0 \u03c8\u03c3 ,Bob 1 (x1) = \u00b50(x1) for all x1 \u2208 X1.\nTo get a BNE using the sequential decomposition of Theorem 2, we first consider the stage-game G2(0, \u03c0 \u03c8\u03c3 2 ) at time 2. Since Bob is the only agent who acts at time 2 and SBob2 = \u2205, any BNE \u03c32 of G2(0, \u03c0 \u03c8\u03c3 2 ) must satisfy\n\u03c3\u0302Bob2 =argmax \u03c3\u0303Bob 2\nE \u03c3\u0303Bob2 ,\u03c8 \u03c3\n[uBob2 (X2, A2)]\n= argmax \u03c3\u0303Bob 2\n(\n\u2212 2P\u03c3\u0303 Bob 2 ,\n\u03c8\u03c3\n(X2 = A Bob 2 = 1)\u2212 P\n\u03c3\u0303Bob2 ,\u03c8 \u03c3\n(X2 = A Bob 2 = \u22121)\n)\n=argmax \u03c3\u0303Bob 2\n(\n\u2212 2\u03c0\u03c8 \u03c3,Bob 2 (1)\u03c3\u0303 \u03c8\u03c3 ,Bob 2 (1 | \u03c0 \u03c8\u03c3 2 )\n\u2212 (1 \u2212 \u03c0\u03c8 \u03c3,Bob 2 (1))(1 \u2212 \u03c3\u0303 \u03c8\u03c3 ,Bob 2 (1 | \u03c0 \u03c8\u03c3 2 )) )\n(61)\nFrom (61) we conclude that one of the equilibrium SIB strategies is given by\n\u03c3Bob2 (\u03c0 \u03c8\u03c3 2 ) = 1, if \u03c0 \u03c8\u03c3 ,Bob 2 (1) \u2264 1/3, \u03c3Bob2 (\u03c0 \u03c8\u03c3 2 ) = 0, if \u03c0 \u03c8\u03c3 ,Bob 2 (1) > 1/3,\nor equivalently\n\u03c3Bob2 (\u03c0 \u03c8\u03c3 2 ) = 1(\u03c0 \u03c8\u03c3 ,Bob 2 (1) \u2264 1/3) (62)\nNote that \u03c3Bob2 (\u03c0 \u03c8\u03c3 2 ) can take any value in [0, 1] if \u03c0 \u03c8\u03c3 ,Bob 2 (1) = 1/3 and \u03c32 is still a BNE of the stage-game. Alice\u2019s sufficient private information at time 2 is SAlice2 = X2. With the stage-game equilibrium SIB strategy \u03c3Bob2 (\u03c02) given by (62), the value function for Alice at t = 2 is then given, according to (48), by\nV Alice2 (\u03c0 \u03c8\u03c3 2 , x2) =E \u03c32,\u03c8 \u03c3 [uAlice2 (X2, A2) | x2]\n=\n{\n21(\u03c0\u03c8 \u03c3,Bob\n2 (1) \u2264 1/3) if x2 = 1\n1\u2212 1(\u03c0\u03c8 \u03c3 ,Bob 2 (1) \u2264 1/3) if x2 = \u22121 (63)\nGiven the above value functions at time t = 2, we now consider the stagegame G1(V2, \u03c0 \u03c8\u03c3\n1 ) at time t = 1. The utility for the stage-game for Alice is given as follows.\nUAlice G1(V2,\u03c0 \u03c8\u03c3 1 ) = uAlice1 (X1, A1) + V Alice 2 (\u03c8 \u03c3 2 (\u03c01, Z), X2) (64)\nIf Alice uses the SIB strategy \u03c3\u0303Alice1 , the expected utility of the stage-game can be calculated for X1 = \u22121 and X1 = 1, according to (46), by\nE \u03c3\u0303Alice1 ,\u03c8 \u03c3\n[UAlice G1(V2,\u03c0 \u03c8\u03c3 1 ) | X1 = \u22121]\n=c\u03c3\u0303Alice1 (1 | \u22121) + E \u03c3\u0303Alice1 ,\u03c8 \u03c3 [V A2 (\u03c8 \u03c3 2 (\u03c0 \u03c8\u03c3 1 , X1W1), X1A Alice 1 ) | X1 = \u22121] =(1 + c)(1 \u2212 \u03b1\u03031) + (3\u03b1\u03031 \u2212 1)((1\u2212 p)1(q\u22121 \u2264 1/3) + p1(q1 \u2264 1/3))\n=:rA\u22121(\u03b1\u03031, q) (65)\nE \u03c3\u0303Alice1 ,\u03c8 \u03c3\n[UAlice G1(V2,\u03c0 \u03c8\u03c3 1 ) | X1 = 1]\n=c\u03c3\u0303Alice1 (1 | 1) + E \u03c3\u0303Alice1 ,\u03c8 \u03c3 [V A2 (\u03c8 \u03c3 2 (\u03c0 \u03c8\u03c3 1 , X1W1), X1A Alice 1 ) | X1 = 1] =1 + (c\u2212 1)\u03b1\u03032 + (3\u03b1\u03032 \u2212 1)((1\u2212 p)1(q1 \u2264 1/3) + p1(q\u22121 \u2264 1/3))\n=:rA1 (\u03b1\u03032, q) (66)\nwhere q = (q\u22121, q1), q\u22121 = \u03c8 \u03c3,Bob 2 (\u03c0\n\u03c8\u03c3 1 ,\u22121)(1) and q1 = \u03c8 \u03c3,Bob 2 (\u03c0 \u03c8\u03c3 1 , 1)(1) are\nthe CIB beliefs \u03c0\u03c8 \u03c3 ,Bob\n2 (1) of {X2 = 1} when Z = \u22121 and Z = 1, respectively, and \u03b1\u0303 = (\u03b1\u03031, \u03b1\u03032), \u03b1\u03031 = \u03c3\u0303 Alice 1 (\u22121 | \u22121), \u03b1\u03032 = \u03c3\u0303 Alice 1 (1 | 1) represents Alice\u2019s SIB strategy \u03c3\u0303Alice1 . Note that from Bayes\u2019 rule in Definition 4, under the SIB strategy \u03c3Alice1 , represented by \u03b11 = \u03c3 Alice 1 (\u22121 | \u22121) and \u03b12 = \u03c3 Alice 1 (1 | 1), we have\nq\u22121 = \u03c8 \u03c8\u03c3,Bob 2 (\u03c0 \u03c8\u03c3 1 ,\u22121)(1) = P \u03b1(X2 = 1, Z = \u22121)\nP \u03b1(Z = \u22121)\n= \u03b12p+ \u03b11(1 \u2212 p) (67)\nq1 = \u03c8 \u03c8\u03c3 ,Bob 2 (\u03c0 \u03c8\u03c3 1 , 1)(1) = P \u03b1(X2 = 1, Z = 1)\nP \u03b1(Z = 1)\n= \u03b12(1\u2212 p) + \u03b11p (68)\nTherefore, a SIB strategy \u03c3\u0302Alice1 , represented by \u03b1\u03021 = \u03c3\u0302 Alice 1 (\u22121 | \u22121) and \u03b1\u03022 = \u03c3\u0302 Alice 1 (1 | 1), is a BNE of the stage-game G1(V2, \u03c0 \u03c8\u03c3 1 ) at time t = 1 if\n\u03b1\u03021 \u2208 argmax \u03b1\u03031\nrA\u22121(\u03b1\u03031, (\u03b12p+ \u03b11(1\u2212 p), \u03b12(1\u2212 p) + \u03b11p)) (69)\n\u03b1\u03022 \u2208 argmax \u03b1\u03032\nrA1 (\u03b1\u03032, (\u03b12p+ \u03b11(1 \u2212 p), \u03b12(1\u2212 p) + \u03b11p)) (70)\nConsequently, the SIB strategy \u03c3Alice1 , represented by \u03b11 = \u03c3 Alice 1 (\u22121 | \u22121) and \u03b12 = \u03c3 Alice 1 (1 | 1) will satisfy the sequential decomposition equations (49)-(50) if\n\u03b11 \u2208 argmax \u03b1\u03031\nrA\u22121(\u03b1\u03031, (\u03b12p+ \u03b11(1\u2212 p), \u03b12(1\u2212 p) + \u03b11p)) (71)\n\u03b12 \u2208 argmax \u03b1\u03032\nrA1 (\u03b1\u03032, (\u03b12p+ \u03b11(1 \u2212 p), \u03b12(1\u2212 p) + \u03b11p)) (72)\nRemark 10. Note that the functions rA\u22121(\u03b1\u03031, q) and r A 1 (\u03b1\u03032, q) are not continuous in q. Thus existence of equilibria cannot be established by the standard method relying on the continuity of the utility functions, and there may not no equilibria in the general case."
        },
        {
            "heading": "6.3 Existence of SIB-BNE under conditions on the instantaneous utility.",
            "text": "The stage-game G1(V2, \u03c0 \u03c8\u03c3 1 ) is a normal-form game with a fixed \u03c31. According to Remark 5, a BNE \u03c3\u0302 of G1(V2, \u03c0 \u03c8\u03c3 1 ) could be different from \u03c31 and the existence of a regular BNE of G1(V2, \u03c0 \u03c8\u03c3\n1 ) is not sufficient to satisfy (50) at time t = 1. In order to apply equilibrium existence results for normal-form games to the sequential decomposition at time t = 1, we introduce an agent 0 who picks the q-belief q = (q\u22121, q1) so that (50) is satisfied.\nFormally, we construct an augmented stage-game G\u03021 between Alice and agent 0. Alice chooses \u03b1\u0303 = (\u03b1\u03031, \u03b1\u03032) and agent 0 chooses q\u0303 = (q\u0303\u22121, q\u03031). Alice\u2019s utility is\nrA1 (\u03b1\u0303, q\u0303) =0.5r A \u22121(\u03b1\u03031, q\u0303) + 0.5r A 1 (\u03b1\u03032, q\u0303)\n=0.5c(1\u2212 \u03b1\u03031 + \u03b1\u03032) + 0.5(2\u2212 \u03b1\u03031 \u2212 \u03b1\u03032)\n+ 0.5(3(\u03b1\u03032p+ \u03b1\u03031(1\u2212 p))\u2212 1)1(q\u0303\u22121 \u2264 1/3)\n+ 0.5(3(\u03b1\u03032(1\u2212 p) + \u03b1\u03031p)\u2212 1)1(q\u03031 \u2264 1/3). (73)\nAgent 0\u2019s utility is\nr01(\u03b1\u0303, q\u0303) = \u2212(q\u0303\u22121 \u2212 \u03b1\u03032p\u2212 \u03b1\u03031(1 \u2212 p)) 2 \u2212 (q\u03031 \u2212 \u03b1\u03032(1\u2212 p)\u2212 \u03b1\u03031p) 2. (74)\nBoth Alice and agent 0 are utility maximizers. The game G\u03021 with utilities (74)-(73) is a normal-form game with strategies \u03b1\u0303 = (\u03b1\u03031, \u03b1\u03032) q\u0303 = (q\u0303\u22121, q\u03031). Since the utility (74) of agent 0 is a quadratic function, any best response by agent 0 must satisfy q\u0303\u22121 = \u03b1\u03032p+ \u03b1\u03031(1\u2212 p), q\u03031 = \u03b1\u03032(1 \u2212 p) + \u03b1\u03031p.\nNote that in the augmented stage-game G\u03021, the utility function r A 1 (\u03b1\u0303, q\u0303) is not continuous in q\u0303. To show the existence of a Nash equilibrium for G\u03021, we proceed to apply existence results for games with discontinuous utilities in Barelli and Meneghel (2013).\nSpecifically, Proposition 2.4 of Barelli and Meneghel (2013) guarantees the existence of a Nash equilibrium for games satisfying the generalized better reply secure property. From Definition 2.3 in Barelli and Meneghel (2013), the stage game is generalized better reply secure if for any (\u03b1\u0304, q\u0304) not an equilibrium, at least one of the followings is true\n\u2022 We can find an \u01eb > 0 and a closed correspondence \u03c60(\u03b1\u0303, q\u0303) such that\nr01(\u03b1\u0303, \u03c6 0(\u03b1\u0303, q\u0303)) \u2265 r01(\u03b1\u0304, q\u0304) + \u01eb (75)\nfor all \u03b1\u03031 \u2208 (\u03b1\u03041 \u2212 \u01eb, \u03b1\u03041+ \u01eb), \u03b1\u03032 \u2208 (\u03b1\u03042 \u2212 \u01eb, \u03b1\u03042+ \u01eb), q\u0303\u22121 \u2208 (q\u0304\u22121 \u2212 \u01eb, q\u0304\u22121+ \u01eb), q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb)\n\u2022 We can find an \u01eb > 0 and a closed correspondence \u03c6A(\u03b1\u0303, q\u0303) such that\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303) \u2265 rA1 (\u03b1\u0304, q\u0304) + \u01eb (76)\nfor all \u03b1\u03031 \u2208 (\u03b1\u03041 \u2212 \u01eb, \u03b1\u03042+ \u01eb), \u03b1\u03032 \u2208 (\u03b1\u03042 \u2212 \u01eb, \u03b1\u03042+ \u01eb), q\u0303\u22121 \u2208 (q\u0304\u22121 \u2212 \u01eb, q\u0304\u22121+ \u01eb), q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb) In Appendix .2, we show that when c > 24 the augmented stage-game G\u03021 is generalized better reply secure. Thus, there exists a Nash equilibrium of the augmented state-game G\u03021 according to (Barelli and Meneghel, 2013, Proposition 2.4).\nConsider any Nash equilibrium (\u03b1, q) of G\u03021. Since q is a best response to \u03b1 for agent 0, from agent 0\u2019s utility (74) we have\nq\u22121 = \u03b12p+ \u03b11(1 \u2212 p) (77)\nq1 = \u03b12(1\u2212 p) + \u03b11p (78)\nFurthermore, since \u03b1 is a best response to q for Alice in G\u03021,\n\u03b1 \u2208 argmax \u03b1\u0303\n(\n0.5rA\u22121(\u03b1\u03031, q) + 0.5r A 1 (\u03b1\u03032, q)\n)\n=argmax \u03b1\u0303\n(\n0.5rA\u22121(\u03b1\u03031, (\u03b12p+ \u03b11(1\u2212 p), \u03b12(1\u2212 p) + \u03b11p))\n+ 0.5rA1 (\u03b1\u03032, (\u03b12p+ \u03b11(1\u2212 p), \u03b12(1\u2212 p) + \u03b11p)) )\n= (\nargmax \u03b1\u03031\nrA\u22121(\u03b1\u03031, (\u03b12p+ \u03b11(1\u2212 p), \u03b12(1\u2212 p) + \u03b11p)),\nargmax \u03b1\u03032\nrA1 (\u03b1\u03032, (\u03b12p+ \u03b11(1 \u2212 p), \u03b12(1\u2212 p) + \u03b11p)) )\n(79)\nTherefore, (71)-(72) hold for \u03b1, and consequently the sequential decomposition requirement (49)-(50) is satisfied at t = 1 by the SIB strategy \u03c3Alice1 represented by \u03b1, and we establish the existence of a SIB equilibrium based on Theorem 2."
        },
        {
            "heading": "7 The case with no common observations",
            "text": "We consider the model of Section 2 but we assume that the agents have no common observations, that is,\nZt = \u2205 \u2200t \u2208 T . (80)\nThe system\u2019s dynamics, the agents\u2019 private observations, the functional form of the agents\u2019 strategies, their utilities, and the equilibrium concept (BNE) remain the same as in Section 2.\nEven though the agents have no common observations in this special case, we can still define SIB strategies by Definition 3, and construct the consistent CIB belief system according to Definition 4 with Zt = \u2205 \u2200t \u2208 T .\nSince there is no common observations, for any realization we always have\n\u2211\nx\u0302t+1,s\u0302t+1\nF it (x\u0302t+1, s\u0302t+1, zt+1)(\u03c0 \u03c8\u03c3 t ; \u03c3 \u2212i t )\n= \u2211\nx\u0302t+1,s\u0302t+1\nF it (x\u0302t+1, s\u0302t+1)(\u03c0 \u03c8\u03c3 t ; \u03c3 \u2212i t ) = 1 > 0 (81)\nTherefore, case (ii) in Definition 4 would never happen, and (20) can be simplified to\n\u03c0\u03c8 \u03c3,i\nt+1 (xt+1, st+1)\n= F it (xt+1, st+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t )\n\u2211\nx\u0302t+1,s\u0302t+1 F it (x\u0302t+1, s\u0302t+1)(\u03c0\n\u03c8\u03c3 t ; \u03c3 \u2212i t )\n=F it (xt+1, st+1)(\u03c0 \u03c8\u03c3 t ; \u03c3 \u2212i t )\n= \u2211\nyt+1,xt,st,at\n[\nP{yt+1, xt+1 | xt, at}\n(\n\u220f\nj\n1{sjt+1 = \u03c6 j t+1(s j t , y j t+1, a j t )}\n)\n\n\n1\n|Ait|\n\u220f\nj 6=i\n\u03c3jt (a j t )(\u03c0 \u03c8 t , s j t )\n\n\u03c0\u03c8,it (xt, st)\n]\n. (82)\nBased on (82) we can write\n\u03a0\u03c8 \u03c3,i\nt+1 = \u03c8 \u03c3,i t+1(\u03a0\n\u03c8\u03c3 t ) \u2200i \u2208 N , (83)\n\u03a0\u03c8 \u03c3\nt+1 = \u03c8 \u03c3 t+1(\u03a0\n\u03c8\u03c3 t ). (84)\nIn other words, given a SIB strategy \u03c3, the update rule \u03c8\u03c3 are deterministic functions given by (84), and the corresponding consistent CIB belief system \u03a0\u03c8 \u03c3\nt , t \u2208 T , evolves in a deterministic manner. Furthermore, since case (ii) in Definition 4 never happens without common observations, the update rule\n\u03c8\u03c3,it+1 given by (82) becomes exactly the Bayes rule. As a result, the CIB belief \u03a0\u03c8 \u03c3 ,i\nt becomes a regular PMF given by\n\u03a0\u03c8 \u03c3 ,i t (xt, st) = P g\u0303i,\u03c3\u2212i(xt, st) \u2200i \u2208 N (85)\nwhere g\u0303i denotes the uniform strategy (i.e., the strategy that chooses every action ait \u2208 A i t with equal probability for all t \u2208 T ).\nRemark 11. If the N agents have identical utilities, i.e. we have a dynamic team problem, then \u03a0\u03c8 \u03c3\nt , t \u2208 T is similar to the common knowledge that appears in Witsenhausen (1973) where a dynamic team is analyzed. The common knowledge in Witsenhausen (1973) is a sequence (over time) of PMFs on the system\u2019s history Ht, t \u2208 T . These PMFs evolve in a deterministic manner, similar to (82) for \u03a0\u03c8 \u03c3\nt , t \u2208 T , in the model of this section.\nFor this special case with no common observations, Theorem 2 becomes\nCorollary 1. Consider a SIB strategy profile \u03c3 = {\u03c3t, t \u2208 T } and the corresponding update rule \u03c8\u03c3 = {\u03c8\u03c3t , t \u2208 T } defined by (83)-(84) for the model of this section. Define\nV iT+1(\u00b7, \u00b7) = 0 for all i (86)\nV it (\u03c0 \u03c8\u03c3t , sit) = E \u03c3t,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3t ) | sit] (87)\nwhere U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) = uit(Xt, At) + V i t+1(\u03c8 \u03c3 t+1(\u03c0\n\u03c8\u03c3 t ), S i t+1), and in the\nconditional expectation E\u03c3t,\u03c8 \u03c3\n[\u00b7], the distribution of (Xt, St) conditioned\non Sit is given by \u03c0 \u03c8\u03c3 ,i t (xt, s \u2212i t ), A i t, i \u2208 N , are generated by \u03c3 i t(a i t | sit, \u03c0 \u03c8\u03c3 t ), S i t+1 conditioned on (Xt, St, At) follows the conditional probability \u2211\nxt+1,s \u2212i t+1\nP(xt+1, st+1 | xt, st, at) given by\nP(xt+1, st+1 | xt, st, at)\n= \u2211\nyt+1\nP{xt+1 | xt, at}P{yt+1 | xt+1, at}\n(\n\u220f\nj\n1{sjt+1 = \u03c6 j t+1(s j t , y j t+1, a j t )}\n)\n.\n(88)\nIf for all t \u2208 T , there is a SIB strategy profile \u03c3\u0302t such that \u03c3\u0302t is a BNE of the stage-game Gt(Vt+1, \u03c0 \u03c8\u03c3t ), that is,\nE \u03c3\u0302it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit] = max\n\u03c3\u0303it\u2208\u039b i t\nE \u03c3\u0303it,\u03c3\u0302 \u2212i t ,\u03c8 \u03c3\n[U i Gt(Vt+1,\u03c0 \u03c8\u03c3 t ) | sit] (89)\nfor all i \u2208 N , and\n\u03c3\u0302t = \u03c3t, (90)\nthen the SIB strategy profile \u03c3 is a SIB-BNE of the dynamic game without common observations defined in this section.\nRemark 12. The SIB-BNE strategy profiles {\u03c3t, t \u2208 T } determined by sequential decomposition in Corollary 1, along with the beliefs {\u03a0\u03c8 \u03c3\nt , t \u2208 T } are also Perfect Bayesian Equilibria (PBE) Fudenberg and Tirole (1991). This is true because {\u03c3t, t \u2208 T } satisfy sequential rationality (Eq. (89)) and consistency holds because the beliefs {\u03a0\u03c8 \u03c3\nt , t \u2208 T } are always updated by Bayes rule."
        },
        {
            "heading": "8 Conclusion",
            "text": "We considered stochastic dynamic games where the underlying system is dynamic, the strategic agents\u2019 actions are hidden (not observable) and their information is asymmetric. We presented an approach for the computation of BNE strategy profiles that are based on a compressed version of the agents\u2019 information and can be determined sequentially in time moving backwards, if each step of this backward procedure has a solution. The approach highlights: (i) the importance of common information/common knowledge in identifying BNE strategy profiles that can be sequentially computed; (ii) the difference between common information that is sufficient for decision-making purposes in games and common information that is sufficient for decision-making purposes in teams. The difference is due to the fact that agents have an incentive to deviate from their predicted strategies in games whereas they don\u2019t have such an incentive in teams. As a consqence of this incentive, at each time instant each agent has his own view/belief of the game\u2019s status based on the common information, but all these different views/beliefs are common knowledge among all agents. As a result the CIB belief system is described by the sequence \u03a0\u03c81:T specified by Definition 2.\nOur investigation focused on determining SIB-BNE strategy profiles for the games under consideration. We note that the SIB-BNE strategy profiles determined by our methodology are also Perfect Bayesian Equilibrium (PBE) strategy profiles when the agents have no common observations (i.e., for the model of Section 7), but this is not true when the agents have common observations (the general model of Section 2). Determining PBE strategy profiles for the general model of Section 2 is an interesting problem worthy of investigation.\n.1 Sufficient Information\nWe compare conditions (i)-(iii) of Definition 1 to the conditions of Definition 2 in Tavafoghi et al (2022); for ease of readability, we include the definition from Tavafoghi et al (2022) below.\nDefinition 7 (Sufficient private information Tavafoghi et al (2022)). We say Sit = \u03b6 i t (P i t , Ct; g1:t\u22121), i \u2208 N , t \u2208 T , is sufficient private information for the agents if, (i) it can be updated recursively as\nSit = \u03c6 i t(S i t\u22121, H i t\\H i t\u22121; g1:t\u22121) for t \u2208 T \\{1}, (91)\n(ii) for any strategy profile g and for all realizations {ct, pt, pt+1, zt+1, at} \u2208 Ct \u00d7 Pt \u00d7 Pt+1 \u00d7Zt+1 of positive probability,\nP g1:t {st+1,zt+1 | pt,ct,at}=P g1:t {st+1,zt+1 | st,ct,at}, (92)\nwhere s1:N\u03c4 = \u03b6 1:N \u03c4 (p 1:N \u03c4 , c\u03c4 ; g1:\u03c4\u22121) for \u03c4 \u2208 T ;\n(iii) for every strategy profile g\u0303 of the form g\u0303 :={g\u0303it :S i t \u00d7Ct \u2192 \u2206(A i t), i\u2208N,t\u2208\nT } and at\u2208At, t\u2208T ;\nE g\u03031:t\u22121\n{ uit(Xt,At) | ct,p i t,at } =Eg\u03031:t\u22121 { uit(Xt,At) | ct,s i t,at } , (93)\nfor all realizations {ct,pit}\u2208 Ct \u00d7P i t of positive probability where s 1:N \u03c4 = \u03b61:N\u03c4 (p 1:N \u03c4 ,c\u03c4 ; g\u03031:\u03c4\u22121) for \u03c4 \u2208 T ;\n(iv) given an arbitrary strategy profile g\u0303 of the form g\u0303 := {g\u0303it : S i t \u00d7 Ct \u2192\n\u2206(Ait), i\u2208N , t\u2208T }, i\u2208N , and t\u2208T ,\nP g\u03031:t\u22121\n{ s\u2212it | p i t,ct } =Pg\u03031:t\u22121 { s\u2212it | s i t,ct } , (94)\nfor all realizations {ct,p i t} \u2208 Ct\u00d7P i t of positive probability where s 1:N \u03c4 = \u03b61:N\u03c4 (p 1:N \u03c4 ,c\u03c4 ; g\u03031:\u03c4\u22121) for \u03c4 \u2208 T .\nCondition (i) of Definition 1 appears in the definition of Sit in Definition 7, and condition (ii) of Definition 1 on recursive update is the same as condition (i) in Definition 7. Condition (iii) of Definition 1 directly leads to (iii) and (iv) of Definition 7; the utility uit(Xt, At) in condition (iii) and the random variable s\u2212it in condition (iv) of Definition 7 are functions of (xt, st) whose distribution conditioned on (pit, ct) is the same as conditioned on (s i t, ct) under condition (iii) of Definition 1. However, condition (ii) of Definition 7 may not hold for sufficient private information satisfying Definition 1. Consider the following example. Suppose X1 = Y 1 1 XOR Y 2 1 , and Y 1 1 , Y 2 1 takes values in {0, 1} with equal probability. Z1 = \u2205 and Z2 = X1. Then S11 = S 2 1 = \u2205 satisfies Definition 1 because P(x1, s \u2212i 1 | p i 1, c1) = P(x1 | y i 1) = 0.5 = P(x1, s \u2212i 1 | s i 1, c1). However, they don\u2019t satisfy condition (ii) of Definition 7 because P(z2 | p1, c1, a1) = P(x1 | y11, y 2 1) = 1(x1 = y 1 1 XOR y 2 1) 6= P(z2 | s1, c1, a1) = P(x1) = 0.5.\n.2 Proof of the generalized better reply secure property for the augmented stage-game\nWe show that when c > 24 the augmented stage-game G\u03021 in Section 6 is generalized better reply secure. For that matter, we set \u03b2\u2217(q) = 1(q \u2264 1/3) and consider the following five cases. Case (i) r01(\u03b1\u0304, q\u0304) 6= 0. In this case Bayes\u2019 rule doesn\u2019t hold at (\u03b1\u0304, q\u0304). We focus on agent 0 and select the belief to satisfy Bayes\u2019 rule as follows:\n\u03c60(\u03b1\u0303, q\u0303) = (\u03b1\u03032p+ \u03b1\u03031(1\u2212 p), \u03b1\u03032(1 \u2212 p) + \u03b1\u03031p) (95)\nThen this \u03c60 is a closed correspondence. From this construction of \u03c60, we can pick \u01eb > 0 such that\nr01(\u03b1\u0303, \u03c6 0(\u03b1\u0303, q\u0303)) = 0 > r01(\u03b1\u0304, q\u0304) + \u01eb\nCase (ii) r01(\u03b1\u0304, q\u0304) = 0, and \u03c0\u0304\u22121 6= 1/3 and \u03c0\u03041 6= 1/3. Since \u03b2\u2217(q) = 1 if q < 1/3, \u03b2\u2217(q) = 0 if q > 1/3, \u03b2\u2217(\u00b7) is continuous\nat points where q 6= 1/3. Hence, we can find \u01eb > 0 s.t. \u03b2\u2217(q\u0303\u22121) = \u03b2\u2217(q\u0304\u22121) for all q\u0303\u22121 \u2208 (q\u0304\u22121 \u2212 \u01eb, q\u0304\u22121 + \u01eb), and \u03b2\u2217(q\u03031) = \u03b2\u2217(q\u03041) for all q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb). In this region we have\nrA1 (\u03b1, q\u0303) = r A 1 (\u03b1, q\u0304) (96)\nfor all \u03b1. Let\n\u03c6A(\u03b1\u0303, q\u0303) = argmax \u03b1 rA1 (\u03b1, q\u0303) (97)\nBecause rA1 (\u00b7) is continuous in the region under consideration, \u03c6 A(\u00b7) has a closed graph from Berge\u2019s maximum theorem. Note that for all q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb), q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb)\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303) = max \u03b1 rA1 (\u03b1, q\u0303) = max \u03b1 rA1 (\u03b1, q\u0304) (98)\nIf max\u03b1 r A 1 (\u03b1, q\u0304) > r A 1 (\u03b1\u0304, q\u0304) we can find \u01eb > 0 such that for q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb), q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb), rA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303) = max\u03b1 r A 1 (\u03b1, q\u0303) \u2265 rA1 (\u03b1\u0304, q\u0304) + \u01eb. If max\u03b1 r A 1 (\u03b1, q\u0304) = r A 1 (\u03b1\u0304, q\u0304), then Alice has no profitable deviation. Furthermore, since r01(\u03b1\u0304, q\u0304) = 0, agent 0 has no profitable deviation. Consequently, (\u03b1\u0304, q\u0304) is an equilibrium if max\u03b1r A 1 (\u03b1, q\u0304) = r A 1 (\u03b1\u0304, q\u0304).\nCase (iii) r01(\u03b1\u0304, q\u0304) = 0, \u03c0\u0304\u22121 = 1/3 and \u03c0\u03041 6= 1/3. Note that q\u0304\u22121 = 0.8\u03b1\u03041+0.2\u03b1\u03042 = 1/3 and \u03b2\n\u2217(q\u0304\u22121) = 1/3. Since \u03c0\u03041 6= 1/3, we can find \u01eb > 0 s.t. \u03b2\u2217(q\u03031) = \u03b2 \u2217(q\u03041) for all q\u03031 \u2208 (q\u03041 \u2212 \u01eb, q\u03041 + \u01eb).\nTherefore,\nrA1 (\u03b1\u0304, q\u0304) = 0.5c(1\u2212 \u03b1\u03041 + \u03b1\u03042) + 0.5(2\u2212 \u03b1\u03041 \u2212 \u03b1\u03042) + 0.5(3q\u03041 \u2212 1)\u03b2 \u2217(q\u03041) (99)\nPick for Alice\n\u03c6A(\u03b1\u0303, q\u0303) = (0, 1) (100)\nfor all \u03b1\u0303i \u2208 (\u03b1\u0304i\u2212\u01eb, \u03b1\u0304i+\u01eb), i = 1, 2, q\u0303i \u2208 (q\u0304i\u2212\u01eb, q\u0304i+\u01eb), i = \u22121, 1. We get\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303) =c+ 0.5 + 0.5(0.6\u2212 1)\u03b2\u2217(q\u0303\u22121) + 0.5(2.4\u2212 1)\u03b2 \u2217(q\u0303\u22121)\n=c+ 0.5\u2212 0.2\u03b2\u2217(q\u0303\u22121) + 0.7\u03b2 \u2217(q\u0304\u22121) (101)\nand\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb\n=0.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5(1 + \u03b1\u03041 + \u03b1\u03042)\n\u2212 0.2\u03b2\u2217(q\u0303\u22121) + 0.5(2.4\u2212 3q\u03041)\u03b2 \u2217(q\u0304\u22121)\u2212 \u01eb\n\u22650.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5 \u2217 3\u2212 0.2\u2212 0.5 \u2217 0.6\u2212 \u01eb (102)\nWhen q\u0304\u22121 = 1/3, then 0.8\u03b1\u03041 + 0.2\u03b1\u03042 = 1/3 \u21d2 \u03b1\u03041 = 5/12 \u2212 3/12\u03b1\u03042. Therefore,\n1 + \u03b1\u03041 \u2212 \u03b1\u03042 = 17/12\u2212 15/12\u03b1\u03042 \u2265 1/6 (103)\nwhere the minimum is at \u03b1\u03041 = 1/6 and \u03b1\u03042 = 1. When c > 24, then\n0.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042) \u2265 c/12 > 2 (104)\nand rA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb > 0.\nCase (iv) r01(\u03b1\u0304, q\u0304) = 0, and \u03c0\u03041 = 1/3 and \u03c0\u0304\u22121 6= 1/3. This case is similar to case (iii). Since \u03c0\u0304\u22121 6= 1/3, we can find \u01eb > 0\ns.t. \u03b2\u2217(q\u0303\u22121) = \u03b2 \u2217(q\u0304\u22121) for all q\u0303\u22121 \u2208 (q\u0304\u22121 \u2212 \u01eb, q\u0304\u22121 + \u01eb). Furthermore,\nrA1 (\u03b1\u0304, q\u0304)\n=0.5c(1\u2212 \u03b1\u03041 + \u03b1\u03042) + 0.5(2\u2212 \u03b1\u03041 \u2212 \u03b1\u03042) + 0.5(3q\u0304\u22121 \u2212 1)\u03b2 \u2217(q\u0304\u22121)\n(105)\nPick for Alice the closed correspondence (as in case (iii))\n\u03c6A(\u03b1\u0303, q\u0303) = (0, 1) (106)\nfor all \u03b1\u0303i \u2208 (\u03b1\u0304i\u2212 \u01eb, \u03b1\u0304i+ \u01eb), i = 1, 2, q\u0303i \u2208 (q\u0304i\u2212 \u01eb, q\u0304i+ \u01eb), i = \u22121, 1. Then\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\n=c+ 0.5\u2212 0.2\u03b2\u2217(q\u0304\u22121) + 0.7\u03b2 \u2217(q\u0303\u22121) (107)\nand\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb\n=0.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5(1 + \u03b1\u03041 + \u03b1\u03042)\n+ 0.5(0.6\u2212 3q\u0304\u22121)\u03b2 \u2217(q\u0304\u22121) + 0.7\u03b2 \u2217(q\u0303\u22121)\u2212 \u01eb\n\u22650.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5 \u2217 3\u2212 0.5 \u2217 2.4\u2212 \u01eb (108)\nWhen q\u03041 = 1/3, 0.2\u03b1\u03041+0.8\u03b1\u03042 = 1/3 \u21d2 \u03b1\u03042 = 5/12\u22123/12\u03b1\u03041. Therefore,\n1 + \u03b1\u03041 \u2212 \u03b1\u03042 = 7/12 + 15/12\u03b1\u03041 \u2265 7/12. (109)\nWhen c > 24, then\n0.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042) \u2265 7/24c > 2.7 (110)\nand rA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb > 0.\nCase (v) r01(\u03b1\u0304, q\u0304) = 0, and \u03c0\u03041 = 1/3 and \u03c0\u0304\u22121 = 1/3. We have\nrA1 (\u03b1\u0304, q\u0304) = 0.5c(1\u2212 \u03b1\u03041 + \u03b1\u03042) + 0.5(2\u2212 \u03b1\u03041 \u2212 \u03b1\u03042) (111)\nPick for Alice the closed correspondence (as in cases (iii) and (iv))\n\u03c6A(\u03b1\u0303, q\u0303) = (0, 1) (112)\nfor all \u03b1\u0303i \u2208 (\u03b1\u0304i\u2212 \u01eb, \u03b1\u0304i+ \u01eb), i = 1, 2, q\u0303i \u2208 (q\u0304i\u2212 \u01eb, q\u0304i+ \u01eb), i = \u22121, 1. Then\nrA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb\n=0.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5(1 + \u03b1\u03041 + \u03b1\u03042)\u2212 0.2\u03b2 \u2217(q\u0303\u22121) + 0.7\u03b2 \u2217(q\u0303\u22121)\u2212 \u01eb\n\u22650.5c(1 + \u03b1\u03041 \u2212 \u03b1\u03042)\u2212 0.5 \u2217 3\u2212 0.2\u2212 \u01eb (113)\nThen we have rA1 (\u03c6 A(\u03b1\u0303, q\u0303), q\u0303)\u2212 rA1 (\u03b1\u0304, q\u0304)\u2212 \u01eb > 0 following the steps in (iv)."
        }
    ],
    "title": "An Approach to Stochastic Dynamic Games with Asymmetric Information and Hidden Actions",
    "year": 2023
}