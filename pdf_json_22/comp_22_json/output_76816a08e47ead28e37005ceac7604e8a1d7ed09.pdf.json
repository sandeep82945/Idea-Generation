{
    "abstractText": "Electrocardiogram (ECG) is a vital diagnosis approach for the rapid explication and detection of various heart diseases, especially cardiac arrest, sinus rhythms, and heart failure. For this purpose, in this study, a different perspective based on downsampling one-dimensional-local binary pattern (1D-DS-LBP) and long short-term memory (LSTM) is presented for the categorization of Electrocardiogram (ECG) signals. A transformation method named 1DDS-LBP has been presented for Electrocardiogram signals. The 1D-DS-LBP method processes the bigness smallness relationship between neighbors. According to the proposed method, by downsampling the signal, the histograms of 1D local binary patterns (1D-LBP) calculated from the obtained signal groups are collected and included as a reference to the long short-term memory structure. The long short-term memory structure has been applied to 1D-DS-LBP conversion applied ECG signals with both unidirectional and bidirectional. To test the proposed approach, ECG signals of three (3) different states of congestive heart failure (CHF), arrhythmia (ARR), and normal sinus rhythm (NSR) consisting of 972 signals were used. Signals were taken from the MIT-BIH and BIDMC databases. Experiments were carried out in various scenarios. We observed that the success rate of the proposed approach obtained very high classification accuracies compared to other studies in the literature. The obtained ECG diagnostic performance values varied between 96.80% and 99.79%. Based on this, this approach has a high potential to have a wide field of study in medical applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "S\u00dcLEYMAN AKDA\u011e"
        },
        {
            "affiliations": [],
            "name": "FATMA KUNCAN"
        },
        {
            "affiliations": [],
            "name": "YILMAZ KAYA"
        },
        {
            "affiliations": [],
            "name": "S\u00fcleyman AKDA\u011e"
        }
    ],
    "id": "SP:8ef67c09639b226f0fd38af2063d939fd2abe658",
    "references": [
        {
            "authors": [
                "\u00d6 Y\u0131ld\u0131r\u0131m",
                "P P\u0142awiak",
                "TR San",
                "UR. Acharya"
            ],
            "title": "Arrhythmia detection using deep convolutional neural network with long-duration ECG signals. Computers in biology and medicine",
            "year": 2018
        },
        {
            "authors": [
                "M Kumar",
                "RB Pachori",
                "UR. Acharya"
            ],
            "title": "Use of accumulated entropies for automated detection of congestive heart failure in flexible analytic wavelet transform framework based on short-term HRV signals",
            "venue": "Entropy 2017;",
            "year": 2017
        },
        {
            "authors": [
                "\u00d6. Yildirim"
            ],
            "title": "A novel wavelet sequence based on a deep bidirectional LSTM network model for ECG signal classification",
            "venue": "Computers in biology and medicine 2018;",
            "year": 2018
        },
        {
            "authors": [
                "M Sharma",
                "S Singh",
                "A Kumar",
                "TR San",
                "UR. Acharya"
            ],
            "title": "Automated detection of shockable and non-shockable arrhythmia using novel wavelet-based ECG features. Computers in biology and medicine",
            "year": 2019
        },
        {
            "authors": [
                "SL Oh",
                "EY Ng",
                "TR San",
                "UR. Acharya"
            ],
            "title": "Automated diagnosis of arrhythmia using a combination of CNN and LSTM techniques with variable length heartbeats. Computers in biology and medicine",
            "year": 2018
        },
        {
            "authors": [
                "AA Bhurane",
                "M Sharma",
                "TR San",
                "UR. Acharya"
            ],
            "title": "Efficient detection of congestive heart failure using frequency localized filter banks for the diagnosis with ECG signals",
            "venue": "Cognitive Systems Research 2019;",
            "year": 2019
        },
        {
            "authors": [
                "O Yildirim",
                "UB Baloglu",
                "TR San",
                "EJ Ciaccio",
                "UR. Acharya"
            ],
            "title": "A new approach for arrhythmia classification using deep coded features and LSTM networks. Computer methods and programs in biomedicine",
            "year": 2019
        },
        {
            "authors": [
                "M Faezipour",
                "A Saeed",
                "SC Bulusu",
                "M Nourani",
                "L. Minn"
            ],
            "title": "A patient adaptive profiling scheme for ECG beat classification",
            "venue": "IEEE Transactions on Information Technology",
            "year": 2010
        },
        {
            "authors": [
                "M Thomas",
                "MK Das",
                "S. Ari"
            ],
            "title": "Automatic ECG arrhythmia classification using dual-tree complex wavelet-based features",
            "venue": "AEU\u2014International Journal of Electronics and Communications 2015;",
            "year": 2015
        },
        {
            "authors": [
                "Cornforth DJ",
                "Jelinek HF"
            ],
            "title": "Detection of congestive heart failure using Renyi entropy",
            "venue": "In Vancouver, CanadaIEEE Computing in Cardiology Conference (CinC) 2016;",
            "year": 2016
        },
        {
            "authors": [
                "UR Acharya",
                "H Fujita",
                "SL Oh",
                "Y Hagiwara",
                "JH Tan"
            ],
            "title": "Deep convolutional neural network for the automated diagnosis of congestive heart failure using ECG signals",
            "venue": "Applied Intelligence 2019;",
            "year": 2019
        },
        {
            "authors": [
                "J Revathi",
                "J Anitha",
                "DJ. Hemanth"
            ],
            "title": "An intelligent medical decision support system for diagnosis of heart abnormalities in ECG signals",
            "venue": "Intelligent Decision Technologies 2021;",
            "year": 2021
        },
        {
            "authors": [
                "V Jahmunah",
                "EYK Ng",
                "TR San",
                "UR. Acharya"
            ],
            "title": "Automated detection of coronary artery disease, myocardial infarction, and congestive heart failure using GaborCNN model with ECG signals. Computers in biology and medicine",
            "year": 2021
        },
        {
            "authors": [
                "Nguyen TN",
                "Nguyen TH"
            ],
            "title": "Deep Learning Framework with ECG Feature-Based Kernels for Heart Disease Classification",
            "venue": "Elektronika ir Elektrotechnika 2021;",
            "year": 2021
        },
        {
            "authors": [
                "V Gupta",
                "M Mittal",
                "V. Mittal"
            ],
            "title": "Chaos theory and ARTFA: Emerging tools for interpreting ECG signals to diagnose cardiac arrhythmias",
            "venue": "Wireless Personal Communications 2021;",
            "year": 2021
        },
        {
            "authors": [
                "Y Kaya",
                "F Kuncan",
                "R. Tekin"
            ],
            "title": "A New Approach for Congestive Heart Failure and Arrhythmia Classification Using Angle Transformation with LSTM",
            "venue": "Arabian Journal for Science and Engineering",
            "year": 2022
        },
        {
            "authors": [
                "AL Goldberger",
                "LA Amaral",
                "L Glass",
                "JM Hausdorff",
                "PC Ivanov"
            ],
            "title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals",
            "venue": "Circulation",
            "year": 2000
        },
        {
            "authors": [
                "DS Baim",
                "WS Colucci",
                "ES Monrad",
                "HS Smith",
                "RF Wright"
            ],
            "title": "Survival of patients with severe congestive heart failure treated with oral milrinone",
            "venue": "Journal of the American College of Cardiology",
            "year": 1986
        },
        {
            "authors": [
                "V Jahmunah",
                "SL Oh",
                "JKE Wei",
                "EJ Ciaccio",
                "K Chua"
            ],
            "title": "Computer-aided diagnosis of congestive heart failure using ECG signals\u2013a review",
            "venue": "Physica Medica 2019;",
            "year": 2019
        },
        {
            "authors": [
                "Y Kaya",
                "M Uyar",
                "R Tekin",
                "S. Y\u0131ld\u0131r\u0131m"
            ],
            "title": "1D-local binary pattern based feature extraction for classification of epileptic EEG signals",
            "venue": "Applied Mathematics and Computation",
            "year": 2014
        },
        {
            "authors": [
                "Y. Kaya"
            ],
            "title": "Hidden pattern discovery on epileptic EEG with 1-D local binary patterns and epileptic seizures detection by grey relational analysis. Australasian physical & engineering sciences in medicine",
            "year": 2015
        },
        {
            "authors": [
                "F Kuncan",
                "Y Kaya",
                "M. Kuncan"
            ],
            "title": "New approaches based on local binary patterns for gender identification from sensor signals",
            "venue": "Journal of the Faculty of Engineering and Architecture of Gazi University 2019;",
            "year": 2019
        },
        {
            "authors": [
                "Y Kaya",
                "\u00d6F Ertu\u011frul",
                "R. Tekin"
            ],
            "title": "Two novel local binary pattern descriptors for texture analysis",
            "venue": "Applied Soft Computing",
            "year": 2015
        },
        {
            "authors": [
                "Y Kaya"
            ],
            "title": "Ertu\u011frul \u00d6F. A novel feature extraction approach in SMS spam filtering for mobile communication: one\u2010dimensional ternary patterns. Security and communication networks 2016",
            "year": 2016
        },
        {
            "authors": [
                "F Kuncan",
                "Y Kaya",
                "M. Kuncan"
            ],
            "title": "A novel approach for activity recognition with down-sampling 1D local binary pattern",
            "venue": "Advances in Electrical and Computer Engineering 2019;",
            "year": 2019
        },
        {
            "authors": [
                "FA Gers",
                "E. Schmidhuber"
            ],
            "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
            "venue": "IEEE Transactions on Neural Networks",
            "year": 2001
        },
        {
            "authors": [
                "FA Gers",
                "D Eck",
                "J. Schmidhuber"
            ],
            "title": "Applying LSTM to time series predictable through time-window approaches",
            "venue": "In Neural Nets WIRN Vietri-01. London, UK Springer",
            "year": 2002
        },
        {
            "authors": [
                "S Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computing and Applications",
            "year": 1997
        },
        {
            "authors": [
                "S Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "LSTM can solve hard long time lag problems",
            "venue": "In Advances in Neural Information Processing Systems. Cambridge MA, USA: MIT Press",
            "year": 1997
        },
        {
            "authors": [
                "Abidogun OA"
            ],
            "title": "Data mining, fraud detection and mobile telecommunications: Call pattern analysis with unsupervised neural networks",
            "venue": "Ph.D. dissertation,",
            "year": 2005
        },
        {
            "authors": [
                "W Byeon",
                "M Liwicki",
                "TM. Breuel"
            ],
            "title": "Texture classication using 2D LSTM networks",
            "venue": "In 22nd International Conference on Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "F Karim",
                "S Majumdar",
                "H Darabi",
                "S. Chen"
            ],
            "title": "LSTM fully convolutional networks for time series classication",
            "venue": "IEEE Access",
            "year": 2018
        },
        {
            "authors": [
                "W Chen",
                "L Zheng",
                "K Li",
                "Q Wang",
                "G Liu"
            ],
            "title": "A novel and effective method for congestive heart failure detection and quantification using dynamic heart rate variability measurement",
            "venue": "PloS one 2016;",
            "year": 2016
        },
        {
            "authors": [
                "Y Wang",
                "S Wei",
                "S Zhang",
                "Y Zhang",
                "L Zhao"
            ],
            "title": "Comparison of time-domain, frequency-domain and nonlinear analysis for distinguishing congestive heart failure patients from normal sinus rhythm subjects",
            "venue": "Biomedical Signal Processing and Control",
            "year": 2018
        },
        {
            "authors": [
                "Y Li",
                "Y Zhang",
                "L Zhao",
                "C Liu"
            ],
            "title": "Combining convolutional neural network and distance distribution matrix for identification of congestive heart failure",
            "venue": "IEEE Access 2018;",
            "year": 2018
        },
        {
            "authors": [
                "Y Isler",
                "A Narin",
                "M Ozer",
                "M. Perc"
            ],
            "title": "Multi-stage classification of congestive heart failure based on short-term heart rate variability",
            "venue": "Chaos, Solitons & Fractals",
            "year": 2019
        },
        {
            "authors": [
                "K Kaouter",
                "T Mohamed",
                "D Sofiene",
                "D Abbas",
                "M. Fouad"
            ],
            "title": "Full training convolutional neural network for ECG signals classification",
            "venue": "In AIP conference proceedings",
            "year": 2005
        },
        {
            "authors": [
                "AS Eltrass",
                "MB Tayel",
                "AI. Ammar"
            ],
            "title": "A new automated CNN deep learning approach for identification of ECG congestive heart failure and arrhythmia using constant-Q non-stationary Gabor transform",
            "venue": "Biomedical Signal Processing and Control",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Key words: ECG, LSTM,1D-DS-LBP, arrhythmia, congestive heart failure, normal sinus rhythm"
        },
        {
            "heading": "1. Introduction",
            "text": "In recent years, signal processing operations have spread to a wide area in the literature. The priority of these processes is to focus on the specific points of the signals, to extract features from them and to make various predictions as a result. It has been observed that researchers have recently begun to pay particular attention to biomedical signals. When the literature is examined, it has been observed that medical signals are used in many fields in terms of academic and research projects. Recently, the point of applications in electronics has made all branches of science, including medical science, dependent on itself. This has had a direct impact on the prevalence of devices (sensors, measuring devices, new technological equipment, etc.) used in diagnosis and treatment studies in medicine. The data obtained from the human body by this device, which is used in the field of medicine, is called bioelectrical signal. And these signals have the potential to easily transfer over the skin to the electrodes. These electrical signals are obtained from certain parts and limbs of our body. Electroencephalogram (EEG), electromyography (EMG), and electrocardiogram (ECG) are among the signals \u2217Correspondence: fatmakuncan@siirt.edu.tr\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nmost frequently investigated by researchers. Researchers have been studying biomedical signals for many years. The core strength of computer application for biomedical signal analysis lies in signal processing and modeling techniques for quantitative or objective analysis [1\u20133]. The interpretation of a signal by an expert varies according to experience and specialized knowledge. Such an analysis is far from unbiased. Computer-based analysis has the potential to add objectivity to the expert\u2019s interpretation. In this way, the diagnostic accuracy of an experienced specialist will also increase. These methods are defined as computer aided diagnosis system. The main purpose of these systems is to simplify the burden of the expert in line with the estimation to be made. Systems working on the basis of electrocardiogram work efficiently during long monitoring processes. Reviewing a record obtained over days or weeks takes much time if done manually. This shows that these systems have a great advantage in terms of time. An electrocardiogram is a distinguishing way that evaluates also keeps the electrical activity of the heart muscles. The electrocardiogram creates a record of the electrical activity of the heart over time. Electrodes placed in specific areas are used to ensure this recording process. In this way, records are obtained without the need for an extra medical procedure and are used in the literature for the detection and criticism of many heart disease conditions [4,5]. Electrocardiographic signals are usually recorded for medical purposes such as risk analysis for diagnosing diseases such as arrhythmias. Electrical signals in the heart are recorded by an electrocardiogram device. This app is a painless, common test to detect cardiac problems and monitor our heart\u2019s health. When examining the ECG signals, doctors have the knowledge and foresight to tell which diseases have regarding the heart and circulatory system. However, a person cannot examine each data separately and in detail in the desired time. Thanks to the developing technological sensors and the successful work of researchers, computers can diagnose diseases or abnormal conditions from ECG signals faster than any doctor. Therefore, it is crucial to diagnose the disease by looking at a computer ECG database [6,7]. In that research, a new method based on 1D-DS-LBP and LSTM is proposed to classify electrocardiogram signals. The transformation way called 1D-DS-LBP has been presented for electrocardiogram signals. It is a way that utilizes the magnitude-smallness relationship resulting from each value on one-dimensional signals with its neighbors. The 1D-DS-LBP approach is presented as data to the LSTM model by combining histograms of the bigness-smallness relationship on the sign. LSTM has been utilized to electrocardiogram signals with both unidirectional and bidirectional 1D-DS-LBP conversions applied. It is predicted that this model can be applied efficiently in other smart systems with the researches and can be adapted to different working areas. The LSTM model used in this research has added originality to the classification of ECG signals. In addition, another factor that makes this study unique is the creation of histograms with the help of 1D-DS-LBP with the help of detecting the bigness-smallness relationship on the signals. As a result of the research conducted within the scope of the study, it was observed that the proposed 1D-DS-LBP+LSTM approach reached a success rate varying between 96.80% and 99.79%. Considering the studies on the same dataset in the literature, it can be interpreted that this method has acceptable success rates."
        },
        {
            "heading": "2. Related studies",
            "text": "Electrocardiogram is a distinguishing way that determines and keeps the electrical activity of the heart muscles. An electrocardiogram is generally utilized in various medical researches for the analysis of description of heart conditions. Congestive heart failure (CHF) is one of them, it is a severe cardiac condition connected to large mortality and morbidity degrees. In CHF, the heart cannot pump blood productively to provide the organs with oxygen and nutrients, and this causes easy fatigue, shortness of breath, and general swelling. According to the European Society of Cardiology (ESC), at least 26 million grown-up people worldwide are suffering from\ncongestive heart failure, while 3.6 million new cases are diagnosed each year [4]. It has been reported that 17%\u201345% of the patients diagnosed with congestive heart failure died in a year and the rest remainder died in five years [5]. There are many researchers who think that early detection of congestive heart failure will have positive effects on opportunity and success [6]. In addition, one of the disorders that cause sudden loss of life as a result of the irregular work of the heart is arrhythmia (ARR). ARR is generally divided into two classes: ventricular and supraventricular. Ventricular arrhytmias happen in the lower chambers of the heart named the ventricles. In contrast, supraventricular ARRs arise before the ventricles, ordinarily inside of the heart\u2019s upper chambers called the atria [7]. In literature studies, it is seen that ECG signals are utilized in many studies on heart diseases. For this reason, it is predicted that these studies in this field, which facilitate early diagnosis, will reduce the rate of sudden death on heart diseases. Faezipour et al. (2010) stated that ECG signals vary from person to person and may even change over time for each individual depending on the person\u2019s physical condition and environment. Therefore, having such a feature is necessary for various purposes of diagnostic (e.g., ARR and CHF). One application of such a profiling scheme automatically amplifies an early warning diagnostic experience for any individual\u2019s abnormal cardiac behavior. Using their extensive experimental results on the MIT-BIH arrhythmia database, the authors stated that they were able to detect 99.59% accuracy with their proposed method and identify abnormalities with a high classification accuracy of 97.42% [8]. Thomas et al. (2015) stated that with the computer-aided diagnosis system, early detection of heart diseases and thus the high mortality rate in heart patients is reduced. They stated that detecting cardiac arrhythmias is a challenging task as small changes in ECG signals cannot be clearly distinguished by a human eye. The authors proposed the DTCWT-based feature extraction technique for the automatic classification of cardiac arrhythmias. The authors stated that the DWT and DTCWT-based feature extraction techniques were classified with 91.23% and 94.64% success, respectively, when tested on five different ECG signals in the MIT-BIH arrhythmia database [9]. Cornforth et al. (2016) revealed that conjunctive heart failure is caused by insufficient transfer of what is needed through the blood pumped by the heart. They stated that the detection and diagnosis of CHF disease are difficult. They stated that CHF diagnosis needs tests which containing an electrocardiogram signal as a result of sensitive research. For this purpose, they proposed a KNN-based method for the detection of CHF disease. The authors stated that they achieved an overall accuracy of 87.9% with the proposed method in their study [10]. Acharya et al. (2019) defined CHF as a chronic heart disease that is difficult in the treatment process and causes a decrease in the quality of life. They stated that electrocardiogram data is a basic examination way that shows understandable differences in congestive heart failure. Nevertheless, they cautioned that manual (traditional) diagnosis of ECG signals would be error-prone due to the precision processing required. In addition, they noted that the ECG data alone was neither sensitive nor specific for CHF diagnosis. The authors observed that the proposed CNN model had 98.97% precision, 99.01% specificity, and 98.87% sensitiveness. They stated that the suggested convolutional neural network structure could perform as an examination help for cardiologists by supplying a more accurate also more rapid performance of electrocardiogram signals [11]. Revathi et al. (2021) developed an intelligent medical decision support system to diagnose cardiac abnormalities in ECG signals. The authors stated that the ECG signals they used in their studies were obtained from the PTB diagnostic ECG database. This database maintains 448 ECG records, including BBB, MI, myocarditis, cardiomyopathy, and healthy subjects. These records were obtained from 209 men and 81 women. The sampling frequency of the signal is 1000 Hz. The authors stated that the best accuracy rate obtained from their method was 96.4% using the LMNN and SCGNN classifier, respectively [12]. Jahmunah et al. (2021) developed a system to automatically classify heart diseases with a convolutional neural network (CNN) and Gabor CNN models using\nelectrocardiogram signals. The authors used the weight balancing method to balance the irregular (unbalanced) data set. The authors defined four classes: coronary artery disease, myocardial infarction, and congestive heart failure. The authors stated that they achieved a classification accuracy of over 98.5% for the four different classes they determined, thanks to the CNN and Gabor CNN models they proposed [13]. Nguyen et al. (2021) propose a core size calculation based on the P, Q, R, and S waves of a heartbeat to improve classification accuracy with deep learning. The authors stated that the electrocardiogram signals were dmey shape close to a heartbeat and were filtered using the dmey wavelet transform. With this selected dmey, each heartbeat was standardized with 300 samples to calculate core sizes, thus containing most features in each heartbeat. The authors stated that the classification accuracy is approximately 99.4% with their proposed deep learning method, especially for five types of heart disease [14]. Gupta et al. (2021) stated that it is critical the timely detection of cardiac abnormalities from the electrocardiogram signal. The authors stated that most researchers focused on linear techniques applied to filtered ECG datasets with a large area. Therefore, there is a need to complement the existing research on ECG signal interpretation using nonlinear techniques on noisy ECG data. The authors used the MIT-BIH arrhythmia database in their study. The authors stated that 99.96% SE, 99.97% PPV, and 99.93% ACC success rates were obtained, respectively; with the methods, they suggested [15]. In many studies, it has been revealed that arrhythmia and CHF examination should be evaluated accurately and consistently by experts, which is tiring and costly. Therefore, it is crucial to design modern and effective CAD systems that will improve the accuracy of Electrocardiogram data [16]. Also, thanks to the advancement in sign processing also machine learning technique, Computer-Aided Diagnosis systems have been designed that are effective in heart disease diagnosis. In this context, many researchers agree that diagnosing heart diseases using computerized systems is very important. In this study, the diagnosis of congestive heart failure and arrhythmia heart disease was performed with the 1D-DS-LBP+LSTM approach using ECG signals. It is foreseen that the described approach can be utilized to classify different signals."
        },
        {
            "heading": "3. Dataset",
            "text": "Electrocardiogram signals from ARR, CHF, NSR data were utilized to test the proposed method. ARR signals were obtained from the Massachusetts Institute of Technology\u2013Beth Israel Deaconess Medical Center (MIT-BIH) [17,18]. The second database (normal sinus rhythm) is from Massachusetts Institute of Technology\u2013Beth Israel Deaconess Medical Center Normal Sinus Rhythm and CHF was get from Beth Israel Deaconess Medical Center Congestive Heart Failure (BIDMC) [17\u201319]. There are 96, 30, 36 records in MIT-BIH arrhythmia, BIDMC CHF, MIT-BIH normal sinus rhythm databases, respectively, also there are 192 ECG records in total. First, all electrocardiogram data were resampled at a typical 128 Hz sampling rate to ensure order and uniformity in all data examined in this study. Examples of ARR, CHF, NSR signals are given in Figures 1A\u20131C, respectively. In the figure, it is seen that arrhythmia spread to different intervals. The normal sinus rhythm exhibited a regular spread in contrast to the ARR. CHF signals indicate the weakness of the pumping power [20]. In this context, a total of 972 electrocardiogram signals were obtained. The categorization of the electrocardiogram signals is presented in Table 1. The electrocardiogram recordings in the dataset belong to different ages and sexes."
        },
        {
            "heading": "4. Methods",
            "text": ""
        },
        {
            "heading": "4.1. Proposed model",
            "text": "In this study, a new method for the classification of congestive heart failure and arrhythmia based on ECG is presented. The presented method includes 3 different classes: preprocessing and segmentation of ECG, feature\nextraction and classification. Figure 2 illustrates the presented method. The transactions that take place in each block are mentioned below in a simplified way.\nBlock 1: This block segments the signals after removing the noise on the Electrocardiogram signals with the help of various preprocessing. The main purpose of noise reduction is to prevent the estimation from being made incorrectly in the diagnosis process. A multistage kernel adaptive filter is utilized to separate artifacts and noise from signals [21]. The denoised signal is then categorized into six segments of 10,000 samples (78 s) each of the 162 Electrocardiogram recordings. After the segmentation process is completed, a total of 972 Electrocardiogram recordings of Arrhythmia (576), Congestive Heart Failure (180), and Normal Sinus Rhythm (218) were created.\nBlock 2: In this block, feature extraction is performed. The features were obtained by the 1D-DSLBP method. This method has three parameters: window size (WS), the feature type to be reduced (mean, minimum, maximum, median), and the number of reductions (n), which specify the number of samplings to be used in the reduction. In each combination, 256 features are obtained from a signal. The number of features increases with the number of reductions. For example, if the reduction number is (n) = 4, a 4 \u00d7 256 matrix is obtained. These matrices are given as input data to LSTM.\nBlock 3: This block is the final stage of the process. Classification process takes place in this block. Signals subjected to 1D-DS-LBP processing are transferred to LSTM in their new form. LSTM uses these signals as inputs. Within the scope of the study, classification was carried out using 3 different Long-Short\nTerm Memory models. Information on the 3 LSTM models used is given in Table 2. It was decided to use these models as a result of after tests.\nIt is possible to use the same models as different models by changing the layer order. In these 3 models, different LSTM models were used by keeping the hyperparameters constant and changing the layer order. Batch size (27) specifies the subset size of the selected training data to be applied at each start of the loop. Therefore, data has been optimized to get the most accurate ratio. In that study, to obtain the best classification results, the most appropriate values were chosen as the batch size of 27 and the learning rate of 0.0001. The maximum value of the number of epochs varies depending on the ratio of the data to be used for training and testing. In model 3, the dropout value is 0.2. This value provides a probability of 0.2 to remove a neuron from the network. This method is used to ensure the independence of the model to certain weights of certain network neurons and only reduces the adaptation to the training data."
        },
        {
            "heading": "4.2. One-dimensional local binary patterns",
            "text": "This method has progressed as a feature extraction method from raw signals by making the Local Binary Patterns (LBP) method, which is widely used in process of the image, from dual-plane to single plane [22\u201324]. The 1D-LBP method is used in different applications in data processing for one-dimensional array signals. It provides to obtain different patterns by applying one-dimensional signals arranged in a time series [25,26]. The working principle of the 1D-LBP method is based on obtaining binary codes by making comparisons with certain neighboring values of a value determined as the central value on a one-dimensional array signal. This process is repeated for all values in the sign. Thus, the binary codes obtained are converted to decimal, and the 1D-LBP label is obtained. In Figure 3, the stages of obtaining the feature using the 1D-LBP method are described one by one.\nAs shown in Figure 3a, the 1D-LBP operator is created by comparing each point on the sign with its neighbors before and after it. The binary string obtained from the comparison is converted to decimal, and 1D-LBP labels are obtained for each point. First, p neighbors are taken to form a binary string. Then, P/2\npoints are taken before and after the Pc center signal (Figure 3b). For P = 8, they are taking the adjacent value earlier than (P0\u2013P3) and later than (P4\u2013P7) of the center point (Pc) is shown in Figure 3c. Next, all neighbors are compared with the center point. If the adjacent (Pi) value is greater than the center (Pc) value, it is labeled as \u201d1\u201d, otherwise as \u201d0\u201d. Finally, the comparison is performed for all points on the sign with Equation 1. It is obtained by 1D-LBP Equations 1 and 2, where x is a part of the natural signal.\nt = Pi \u2212 Pc (1)\n1D \u2212 LBP p\u2211\ni=0\nF (t)2i here F (t) = { 1 t \u2265 0 0 t < 0\n(2)\nHere, Pi and Pc , respectively, show the neighboring values and the center value. After creating the binary string with Equation 1, this binary value is converted to decimal and labeled as the 1D-LBP value of the Pc value (Figure 3d). These operations are performed for all points along with the signal. When the operations in Figure 3 are applied to all signals, the sign values change to values ranging from 0 to 255. These values are called local binary patterns. The frequency value of each of these values represents a model. If we take the histogram of the 1D-LBP data, we can see that there are 256 different patterns."
        },
        {
            "heading": "4.3. Downsampling one-dimensional local binary patterns (1D-DS-LBP)",
            "text": "The downsampling one-dimensional local binary patterns (1D-DS-LBP) method is an another method developed based on increasing the number of features in order for the 1D-LBP design to yield more successful results on signals with a large number of data. The 1D-DS-LBP method application is based on obtaining binary codes by comparing specific neighboring values of the value determined as the central value on the newly reduced sign obtained by making reductions after applying the 1D-LBP method over a selected center value (PC) in a one-dimensional array signal. This process is repeated for all values in the raw sign and the new reduced sign. Thus, the binary codes obtained are converted to the decimal base, and the 1D-DS-LBP signal is obtained. When applying the 1D-DS-LBP method, three important parameters are defined. These parameters are the window size (WS), which indicates the number of samplings to be used in the reduction, the feature type to be reduced (mean, minimum, maximum, median), and the number of reductions (n). The WS indicates the number of sign values to be used within each group when reducing. If the WS is kept at very large rates, significant data loss may occur. For this reason, when determining the WS value, care should be taken not to have too large values. In this study using the 1D-DS-LBP method, selecting the most suitable WS is possible by making various trials. The reduction feature gives information about the criterion for choosing the sign from the groups created according to the selected WS value. The sign value to be selected from WS is determined according to one of the min, max, mean, and median criteria among WS elements. These criteria show different effects on the data studied. Trials are required to select the most appropriate criterion according to the data studied. Another parameter used in 1D-DS-LBP method applications is the number of reductions (n). According to the n number determined by the user, the n-stage reduction can be made on the raw signal by using WS and feature criteria. The application of the 1D-DS-LBP method to a sample signal reduction is shown in Figure 4. There is the original signal in step 1. A lower step is passed in step 2 by selecting the most significant values in the window defined in step 1. Signal values in step 3 are also created by selecting them in step 2 in a similar way. Then, the 1D-LBP method is applied to the signals at each stage.\nFigure 4 (step 1) shows the original signal. The signal group in (step 2) is formed by creating groups of 4 on the original signal and reducing by taking the most significant value of the signals in each group. The signal group in (step 3) is obtained by again reducing the sign given in (step 2)."
        },
        {
            "heading": "4.4. Long short-term memory architecture (LSTM)",
            "text": "In neural networks, the data goes unidirectional from the input to the output and cannot preserve the previous information of the series. On the other hand, recurrent neural networks (RNN) have a looping feature and allow them to process data-related information over time. RNN\u2019s input values are not instantaneous but evaluated according to previous inputs. The decision for the input\u2019s current value (t-1) also affects the decision to be made at time t. In RNNs, input values produce output by combining previous and current information and using these outputs as input in the future process; this method differs from feedback networks due to these features. Thus, RNN tends to exhibit dynamic temporal behavior. It is a method used intensively in cases where the same statistical data about time comes in a particular order. The purpose of RNN is to use sequential information. In a traditional neural network, it is assumed that all inputs and outputs are independent of each other. RNNs do the same for all elements of an array. The output is subject to previous calculations, so they are called recurrent neural networks. In theory, RNNs can use data from long sequences, but they are limited to only going back a few steps in practice. This is because computation times are long when it allows the processing of any length of the input. Although the size of the working method does not depend on the increase in the number of inputs, the calculations are made by processing the previous data. It is not easy to reach the longold data for such networks. While the RNN time series can effectively model nonlinear problems, Gers et al. presented a new solution by introducing the LSTM structure [28\u201330]. Learning to process time sequences by\nusing traditional recurrent neural networks relies on delays about time, and thus, automatically finding the time-dependent window size suitable for the system is one of the challenges. The structure of recurrent neural networks is quite ordinary. That strive with volatile memory causes the majority of recurrent neural networks to not be able to work actively. The point where long short-term memories differ from the recurrent neural network group they are connected to is due to the fact that they work with the gate mechanism. With the help of this difference, the temporary memory deficit of recurrent neural networks is solved. Schmidhuber and Hochreiter are two scientists who introduced the concept of LSTM and its main purpose is to provide the appropriate time delay to solve long-term modeling and time-series issues.\nWhen Figure 5 is examined, the memory hosting time between recurrent neural networks and long shortterm memory structures can be clearly understood. It is a very important feature in terms of natural language processing as well as sequential tasks also time-series. The RNN structure is outlined in the figure below. In the standard recurrent neural network block, the data in a time step and the latent phase in the earlier time step is to penetrate an hyperbolic tangent (tanh) activation function to get a new latent phase and value. RNN working principle is shown in Figure 6.\nOther than that, the general structure of Long short-term memories is far from simple compared to that of recurrent neural networks. LSTM cell structure takes information in three parts. These are data of current\ninput, long-term memory, and short-term memory of the cell before it, respectively. The hidden state is an another name for short-term memory while long-term memory is also known as cell situation. Gateways are structures that allow cells to sort out whether or not the information at hand will be used. In short, useless information is eliminated through these doors. LSTM structure, which consists of an input layer, a repetitive hidden layer, and an output layer, uses the basic unit of the hidden layer as a memory block, unlike the ordinary neural network structure. The memory block, which has a double adaptive multiplicative switching unit, can thus easily provide the process of controlling the self-associated memory cells that save the transient mood and the knowledge it holds. Two additional gates called the entrance gate and the exit gate, respectively, control the entry and exit activations into the block. The core of the memory cell is a self-connected, linear unit-constant error cycle (CEC), and the activation of the CEC represents the cell state. It can handle the opening and closing of the slam doors with the help of CEC involvement. With the help of an unannounced gate combined with the memory block, indefinite growth is averted. At the end of the process, each memory initiates a self-reset process and leads to a change in the CEC weight through the unrecorded gate actuation. LSTM architecture is shown in Figure 7.\nThe gateway decides what kind of new information is stored in long-term memory. It only works with information from the current input and short-term memory from the previous time step. Therefore, it should filter out unhelpful information from these variables [31,32]. Mathematically, this operation is performed using two layers. The first of these creates a short-term memory and converts the stream input to sigmoid format and performs the filtering of the information. The Sigmoid format assigns the value 1 to the information that can be used while assigning the value 0 to the worthless information. With the help of this process, it is determined which data will be kept or discarded. Updates to the sigmoid process are only made to ensure positive transitions when assigning low-critical features [33,34].\nThe calculations made in sections within the LSTM block structure are shown in Equations 3\u20138 below.\nft = \u03c3(Wf [ht\u22121, xt] + bf ) (3)\nit = \u03c3(Wi[ht\u22121, xt] + bi) (4)\ngt = tanh(Wc[ht\u22121, xt] + bc) (5)\nCt = ftCt\u22121 + itgt (6)\not = \u03c3(Wo[ht\u22121, xt] + bo) (7)\nht = ot tanh(Ct) (8)"
        },
        {
            "heading": "5. Results",
            "text": "Based on the bigness-smallness relations of the ECG signals with their neighbors with the method presented within the scope of the study, a detection method on heart diseases has been proposed. While applying the 1D-DS-LBP method, three important parameters are used. These are the WS, which indicates the number of samplings to be used in the reduction, the feature type to be reduced (mean, min, max, median), and the number of reductions (n). With the help of data based on these statistics, the differences between the patterns were tried to be determined. The method presented in the study was applied to datasets obtained from Beth Israel Deaconess Medical Center (BIDMIC CHF), Massachusetts Institute of Technology\u2013Beth Israel Deaconess Medical Center normal sinus rhythm (MIT-BIH NSR) and MIT-BIH arrhythmia (MIT-BIH ARR) databases. Thus, it is possible to compare the approach with other studies in the literature. Therefore, in this study, the separation of ECG signals from each other was carried out. Five hundred and seventy-six ECG recordings were obtained owing to 47 individuals that have ARR, 180 recordings owing from 15 individuals that have congestive heart failure, and 216 ECG signals owing to 18 individuals diagnosed with normal sinus rhythm. Different models were created by playing with the parameters of the 1D-DS-LBP approach and these different models were applied to the signals. Different models were created by playing with the parameters of the 1D-DS-LBP approach and these different models were applied to the signals. As a result of this method, new forms of signals were obtained and the signals became suitable for use in the classification process of the LSTM model. The signals obtained at each stage for 4-stage WS = 4 and data selection method = min, max, and mean to an example ECG signal and the histograms obtained after applying 1D-LBP of these signals are given in Figures 8 and 9. It is seen that different histograms are formed according to different parameters of the 1D-DS-LBP method. Therefore, different patterns were captured.\nIf the 1D-LBP method is applied to each sample ECG signal, a 1 \u00d7 256 pattern is formed. This matrix changes according to the number of DS stages. For example, if the number of steps = 4, a 4 \u00d7 256 pattern matrix is formed. This matrix is given as input data to LSTM models. The success criterion was used to show the performance of the proposed approach. This criterion is calculated as (quantity of accurately categorized individuals)/(individuals amount). Three long short-term memory models were utilized in diverse compositions. The properties of long short-term memory structures in these various configurations are given in Table 2. All hyperparameters of the LSTM models are kept the same, except for the layers. The evaluation of success of the 3 LSTM models is shown in Tables 3\u20135. The success rates in Tables 3\u20135 were obtained according to the 50%\u201350% training-test ratio of the data set. Four hundred and eighty-six recordings of 972 ECG signals were applied for training and also 486 for testing objectives. Data of training is applied for the training process\nand the validation data is utilized for providing a neutral measurement of the model which is optimal for data when adjusting the hyper model parameters. The data were used for model assessment, not for training. Same records for all models were used for training and testing. In the 1D-DS-LBP method, the value of the WS parameter is applied as 3. Step number and data selection method was applied separately as min, max, mean, and median. Looking at Tables 3\u20135, it is seen that the disturbances occurring in the heart directly change with the electrocardiogram signals. In this context, it can be said that a high success has been achieved. The highest\nsuccess with LSTM model 1 was 99.18% with 1D-DS-LBP DSminimum. This success rate was observed when the number of stages (n) parameter was 6 or 8. Acceptably high success rates have been observed in other situations. For the LSTM 2 model (Table 4), the greatest value of success was achieved which is 99.79% with 1D-DS-LBPDSmaximum for 4 or 5 step numbers. Looking at the LSTM 3 model and when the data is examined, the great success rates are striking and the highest value was achieved for 1D-DS-LBP DSminimum, 1D-DS-LBP DSmaximum, and 1D-DS-LBP DSmean. As can be seen in Table 5, when the number of steps is 4, 5, and 8, high success rates such as 99.79% were observed. The confusion matrices of the LSTM models are given in Figure 10. As can be seen from Figure 10, CHF and NSR signals are classified as 100% correctly for LSTM 2 and LSTM 3 models. ARR signals are classified with 99.7% accuracy. In Figure 11, the success and loss values of the LSTM models during the training process are given. The loss values graph is used to understand the training procedures of the presented methods, how fast the network accuracy improves, and whether the network memorizes the training data. Figure 11 demonstrates success by showing the training accuracy and test accuracy of the presented methods (1D-DS-LBP+LSTM).\nDifferent ratios were determined in order to observe the effect of determining the ratio to be allocated to the training-test data on the success of the presented method. Trials were conducted at 50%\u201350%, 60%\u201340%, 70%\u201330% and 80%\u201320% training-test ratios. The success rates obtained are shown in Table 6. 1D-DS-LBP method was used as WS = 4 and step number = 4. LSTM model 3 is used.\nAccording to the table above great classification, rates have been achieved and success rates vary between 97.53% and 100%. 1D-DS-LBP DSmean + LSTM model 3 is considered the most successful model. The impact of the WS in the 1D-DS-LBP process is controlled with the help of this parameter. The success rates obtained with the LSTM 3 model for different values of WS are given in Table 7. At this stage, the number of steps of the 1D-DS-LBP method was used as 3. Looking at the table, acceptable high results were obtained. According to the results obtained, it was observed that the WS parameter had no visible effect. The most appropriate WS value should be decided as a result of trials.\nThe success of our proposed approach in diagnosing heart disease from ECG signals varies as the scenario changes. The comparison of the proposed method with the studies conducted in the same direction in the literature is shown in Table 8. When the table is examined, it is clear that the method suggested in this study has acceptable results."
        },
        {
            "heading": "6. Discussion",
            "text": "A method for diagnosing heart diseases based on the bigness-smallness relationship of ordered values in ECG signals is presented. While applying the 1D-DS-LBP method, 3 important parameters are used. These are the window size (WS), which indicates the number of samplings to be used while reducing, the feature type to be reduced (mean, minimum, maximum, median), and the number of reductions (n). With the help of these features, it is tried to obtain different pattern structures from the existing pattern structure. The success of the suggested electrocardiogram diagnostic method has been tested on 972 ECG signals segments. The data used in the study belong to the electrocardiogram type consisting of congestive heart failure, arrhythmia and normal sinus rhythm types. Five hundred and seventy-six ECG signals were obtained from 47 individuals with arrhythmia, 180 from 15 individuals with congestive heart failure, and 216 normal sinus rhythm signals from 18 individuals. Within the scope of the study, 1D-DS-LBP method was applied to these signals obtained from the database. This method applied varies according to the parameters. In the proposed approach, considering the various values of the 1D-DS-LBP method, experiments were carried out in different window sizes, according to the reduction number and reduction feature type. Our proposed approach for diagnosing heart diseases from ECG signals gives optimal success rates in various situations. Using the features received from 1D-DS-LBP, the categorizing method was applied with 3 different LSTM configurations. The success rate was observed as 99.79%. Our study, which was compared with the studies in the same field in the literature, showed that it was at an acceptable level with the results it gave. The proposed approach, the 1D-DS-LBP+LSTM model, is thought to have the potential and reliability to lead studies in different fields in the literature."
        },
        {
            "heading": "Acknowledgment",
            "text": "This study was performed in Siirt University Faculty of Engineering Machine Vision (MaVi) Laboratory. The authors would like to thank the staff of MaVi Laboratory for their support."
        }
    ],
    "title": "A new approach for congestive heart failure and arrhythmia classification using downsampling local binary patterns with LSTM",
    "year": 2023
}