{
    "abstractText": "Grading of exams is an important, labor intensive, subjective, repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while \u201cout-of-the-box\u201d LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating human oversight.",
    "authors": [
        {
            "affiliations": [],
            "name": "Johannes Schneider"
        },
        {
            "affiliations": [],
            "name": "Bernd Schenk"
        },
        {
            "affiliations": [],
            "name": "Christina Niklaus"
        },
        {
            "affiliations": [],
            "name": "Michaelis Vlachos"
        }
    ],
    "id": "SP:bf8547cf6db80736a1eab1ad7c934793eef21c1b",
    "references": [
        {
            "authors": [
                "B. Aczel",
                "Wagenmakers",
                "E.-J"
            ],
            "title": "Transparency Guidance for ChatGPT Usage in Scientific Writing. PsyArXiv",
            "year": 2023
        },
        {
            "authors": [
                "L.W. Anderson",
                "D.R. Krathwohl"
            ],
            "title": "A taxonomy for learning, teaching, and assessing: A revision of Bloom\u2019s Taxonomy of Educational Objectives",
            "year": 2001
        },
        {
            "authors": [
                "S. Baral",
                "A. Botelho",
                "J. Erickson",
                "P. Benachamardi",
                "N. Heffernan"
            ],
            "title": "Improving Automated Scoring of Student Open Responses in Mathematics",
            "venue": "Proceedings of the International Conference on Educational Data Mining",
            "year": 2021
        },
        {
            "authors": [
                "S. Basu",
                "C. Jacobs",
                "L. Vanderwende"
            ],
            "title": "Powergrading: A Clustering Approach to Amplify Human Effort for Short Answer Grading",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2013
        },
        {
            "authors": [
                "A. Borji"
            ],
            "title": "A categorical archive of chatgpt failures",
            "venue": "ArXiv Preprint ArXiv:2302.03494.",
            "year": 2023
        },
        {
            "authors": [
                "S. Burrows",
                "I. Gurevych",
                "B. Stein"
            ],
            "title": "The eras and trends of automatic short answer grading",
            "venue": "International Journal of Artificial Intelligence in Education,",
            "year": 2015
        },
        {
            "authors": [
                "J.H. Choi",
                "K.E. Hickman",
                "A. Monahan",
                "D. Schwarcz"
            ],
            "title": "Chatgpt goes to law school",
            "year": 2023
        },
        {
            "authors": [
                "A. Condor",
                "M. Litster",
                "Z. Pardos"
            ],
            "title": "Automatic short answer grading with bert on out-of-sample questions",
            "venue": "Proceedings of the 14th International Conference on Educational Data Mining",
            "year": 2021
        },
        {
            "authors": [
                "S. Dikli"
            ],
            "title": "The nature of automated essay scoring feedback",
            "venue": "Calico Journal, 28(1), 99\u2013134.",
            "year": 2010
        },
        {
            "authors": [
                "Future of Life."
            ],
            "title": "Pause Giant AI Experiments: An Open Letter",
            "venue": "https://futureoflife.org/open-letter/pause-giant-ai-experiments/",
            "year": 2023
        },
        {
            "authors": [
                "W. Geerling",
                "G.D. Mateer",
                "J. Wooten",
                "N. Damodaran"
            ],
            "title": "Is ChatGPT Smarter than a Student in Principles of Economics? Available at SSRN 4356034",
            "year": 2023
        },
        {
            "authors": [
                "G. Kortemeyer"
            ],
            "title": "Could an Artificial-Intelligence agent pass an introductory physics course? ArXiv Preprint ArXiv:2301.12127",
            "year": 2023
        },
        {
            "authors": [
                "V. Kumar",
                "D. Boulanger"
            ],
            "title": "Explainable automated essay scoring: Deep learning really has pedagogical value",
            "venue": "Frontiers in Education,",
            "year": 2020
        },
        {
            "authors": [
                "N. Madnani",
                "A. Loukina",
                "A. Von Davier",
                "J. Burstein",
                "A. Cahill"
            ],
            "title": "Building better open-source tools to support fairness in automated scoring",
            "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing",
            "year": 2017
        },
        {
            "authors": [
                "S. Saha",
                "T.I. Dhamecha",
                "S. Marvaniya",
                "R. Sindhgatta",
                "B. Sengupta"
            ],
            "title": "Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both",
            "venue": "In C. Penstein Rose\u0301 & others (Eds.), Artificial Intelligence in Education (Vol",
            "year": 2018
        },
        {
            "authors": [
                "J. Schneider",
                "R. Richner",
                "M. Riser"
            ],
            "title": "Towards trustworthy autograding of short, multi-lingual, multitype answers",
            "venue": "International Journal of Artificial Intelligence in Education,",
            "year": 2023
        },
        {
            "authors": [
                "S. Seufert",
                "C. Niklaus",
                "S. Handschuh"
            ],
            "title": "Review of AI-Enabled Assessments in Higher Education",
            "year": 2022
        },
        {
            "authors": [
                "C. Sung",
                "T. Dhamecha",
                "S. Saha",
                "T. Ma",
                "V. Reddy",
                "R. Arora"
            ],
            "title": "Pre-training BERT on domain resources for short answer grading",
            "venue": "Proceedings of the Conf. on Empirical Methods in Natural Language Processing and the Int. Joint Conf. on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "G. Tang"
            ],
            "title": "Letter to editor: Academic journals should clarify the proportion of NLP-generated content in papers",
            "venue": "Account. Res. https://doi.org/10.1080/08989621.2023.2180359",
            "year": 2023
        },
        {
            "authors": [
                "E.A. van Dis",
                "J. Bollen",
                "W. Zuidema",
                "R. van Rooij",
                "C.L. Bockting"
            ],
            "title": "ChatGPT: five priorities for research",
            "year": 2023
        },
        {
            "authors": [
                "P. Vittorini",
                "S. Menini",
                "S. Tonelli"
            ],
            "title": "An AIBased System for Formative and Summative Assessment in Data",
            "venue": "Science Courses. International Journal of Artificial Intelligence in Education,",
            "year": 2020
        },
        {
            "authors": [
                "L. Yan",
                "L. Sha",
                "L. Zhao",
                "Y. Li",
                "R. Martinez-Maldonado",
                "G. Chen",
                "X. Li",
                "Y. Jin",
                "D. Ga\u0161evi\u0107"
            ],
            "title": "Practical and ethical challenges of large language models in education: A systematic scoping review",
            "venue": "British Journal of Educational Technology",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while \u201cout-of-the-box\u201d LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating human oversight.\nKeywords: grading support, autograding, large language models, trust."
        },
        {
            "heading": "Introduction",
            "text": "Large language models like ChatGPT have already led to a widespread impact across both industry and academia. Various sectors in the industry are actively exploring all kinds of application areas and recognizing substantial potential. Certain experts also perceive substantial risks associated with LLMs and have advocated for a development moratorium on such technologies (Future of Life, 2023). In academia, LLMs are used as a tool by researchers and students to such an extent that researchers themselves have called on journals to clarify the allowable extent of AI-generated content in scholarly papers (Tang, 2023), leading to the publication of guidelines for incorporating AI in the paper-writing process(Aczel & Wagenmakers, 2023). Ethical concerns\nhave also been raised for education (Yan et al., 2023). LLMs like ChatGPT have been commonly compared against students in various disciplines \u2013 especially with respect to their capability to pass exams. While some reports have indicated inferior performance than a master\u2019s graduate in mathematics (Frieder et al., 2023), other instances showcase a successful completion of an introductory physics course (Kortemeyer, 2023), as well as the passing of numerous law school exams (Choi et al., 2023) and a standardized test of economics (principles) (Geerling et al., 2023). However, it is important to acknowledge the existence of limitations in the LLMs. These models can exhibit biases, discrimination and factual inaccuracies(Borji, 2023). Consequently, there arises doubt regarding their suitability. In particular, the necessity for human verification has been emphasized as a pressing research priority (van Dis et al., 2023) and the topic of human agency is also debated on a regulatory level (EU, 2020). Especially, high stakes decisions require careful analysis before AI can be utilized. Grading of exams is a high stakes situation, as errors in grading can cause students to fail an entire class, possibly causing a year-long delay in their education, separation of peers, etc. This, in turn, can lead to both financial and psychological strain. As such it seems natural and even necessary to assess the suitability of large language models for supporting exam grading and to reflect upon adequate ways to include them in the grading process while mitigating their risks. To this end, we seek to contribute to two intertwined research questions:\n(a) How can LLMs support educators in the grading process of exams? (b) What are issues and concerns when using LLMs to support grading? Our focus is on Automatic Short Answer Grading (ASAG), i.e., student replies are (short) textual answers (i.e., one or a few paragraphs). We use an LLM, i.e., ChatGPT to assess the instructor\u2019s answer, a student\u2019s answer in general as well as a student\u2019s answer with respect to the instructor\u2019s answer as illustrated in Figure 1. In our experimental evaluation, we use multiple exams from multiple educators.\nWhile we implicitly assess the possibility of automatic grading, our target is (i) to improve the grading process rather than simply automating it and (ii) to uncover shortcomings (and possible mitigations) of LLMs for this purpose. We seek to employ LLMs as a second opinion that might pinpoint obvious flaws in the grading, i.e., due to sloppiness, in the grading as well as provide a more general view on possible answers in order to avoid bias like accepting only correct answers that have been discussed in the course."
        },
        {
            "heading": "Methodology",
            "text": "We use an LLM, i.e., ChatGPT (GPT 3.5, June and July versions), to assess (i) answers by the educator, (ii) answers of students to exam questions in general, and (iii) answers of students compared to the instructor\u2019s answer (see Figure 1).\nThat is, to assess an instructor\u2019s answer, an instructor must be able to define an answer for each exam question that constitutes the optimal response from her/his perspective. We shall elaborate on lifting this requirement and allow for multiple possible answers per question in the discussion section.\nOur assessment is both qualitative and quantitative. That is, we highlight a few prompts that surfaced surprising issues (such as lack of robustness, i.e., sensitivity to minor variations in prompts), but we also quantify how much the LLM deviates from the educator across all graded answers. To this end, we ask the LLM to categorize its assessment, i.e., each LLM response should contain a category such as \u201cGood\u201d, \u201dOk.\u201d, or \u201cBad\u201d and an explanation of the chosen category. In turn, we also categorize the educator\u2019s responses. This allows us to compare the categorization of the LLM and the educator, e.g., to assess if both rate a student answer as \u201cgood\u201d, which we elaborate in the last subsection. We experimented with a few different prompts but we report only the ones used in the end."
        },
        {
            "heading": "Assessing the educator\u2019s answer by the LLM",
            "text": "An educator should have a good sense on different answers to exam questions. However, it is often difficult to anticipate all arguments and answers. For example, an educator might be primed to answers that are based on the material taught in class but students might include elements with different but nonetheless valid viewpoints. We utilize the LLM to provide a more general view on the question without explicit knowledge on the lecture material. We also ask the LLM for\nfeedback on the developed educator\u2019s solution. More concretely, the prompt has the following structure for a question Q and the educator\u2019s answer A.\nHere is a question: Q . Here is an answer: A. How good is the answer to the question? Start the reply with one of the following: Extremely good., Very good., Good., Ok., Bad., Very bad., Extremely bad. Explain the choice. Explain also what is missing.\nExample Prompt (1) to LLM:\nHere is a question: What is the difference between single linkage and average linkage (hierarchical) clustering? . Here is an answer: The two differ in distance metric used to cluster. Single linkage: Merge two clusters based on minimum distance between any two points; Tendency to form long chains; Average linkage: merge two clusters based on average distance between any two points; tendency to \u201cball\u201d like clusters;. How good is the answer to the question? Start the reply with one of the following: Extremely good., Very good., Good., Ok., Bad., Very bad., Extremely bad. Explain the choice. Explain also what is missing.\nResponse of LLM:\nGood. The answer provides a clear and concise explanation of the difference between single linkage and average linkage clustering. It accurately states the different distance metrics used and describes the tendencies of each clustering method. One potential improvement is to provide a more detailed explanation of the advantages and disadvantages of each method. Additionally, it would be helpful to include an example or illustration to further clarify the concepts."
        },
        {
            "heading": "Assessing the student\u2019s answer in general by the",
            "text": ""
        },
        {
            "heading": "LLM",
            "text": "We let the LLM assess the student\u2019s answer to a question. The LLM should provide a second opinion that contrasts that of the educator, possibly, adding novel arguments in the assessment of the student\u2019s answer or, weighing arguments in the reply different from the educator. More concretely, the prompt has the following structure for a question Q and a student answer A.\nHere is a question: Q . Here is an answer: A. How good is the answer to the question? Start the reply with one of the following: Extremely good., Very good., Good., Ok., Bad., Very bad., Extremely bad. Explain the choice.\nExample Prompt to LLM:\nHere is a question: What is the difference between single linkage and average linkage (hierarchical) clustering? . Here is an answer: In single linkage, we compare the two closest datapoints (the ones with minimal distance) from two separate clusters. In average linkage, we compare all the datapoints from a cluster with all the datapoints from another cluster and get an average distance. .How good is the answer to the question? Start the reply with one of the following: Extremely good., Very good., Good., Ok., Bad., Very bad., Extremely bad. Explain the choice.\nResponse of LLM:\nGood. The answer provides a clear and concise explanation of the difference between single linkage and average linkage clustering. It accurately describes how single linkage compares the closest data points from two separate clusters, while average linkage compares all data points from a cluster with all data points from another cluster and calculates the average distance. The answer effectively addresses the question and provides a solid understanding of the topic.\nComparing student answer and educator\u2019s answer\nWe compare the student answer with the educator\u2019s answer.\nWe use the LLM to assess the similarity between the student answer and the educator\u2019s answer, i.e., the optimal answer from the perspective of the educator (and student). The LLM only obtains the educator\u2019s and the student\u2019s answer as input but not the question (see also Figure 1). We found that when the question is added, the assessment of ChatGPT sometimes tended to neglect the optimal answer (although the prompt clearly stated that the task was to compare the student reply and the educator\u2019s answer).\nThus, the LLM provides a second opinion like another grader that had also knowledge on the course in the form of what is the best answer given the taught material, i.e., the educator\u2019s answer. Thus, in contrast to the prior assessments that were seeking for a more general perspective, here, we ultimately seek to assess, if an educator\u2019s grading matches that of the LLM given knowledge of the best response. That is, if both should stick to a pre-defined \u201cground truth\u201d in the form of the educator\u2019s answer. We shall also discuss how to relate the grading of the LLM and the educator quantitatively in the next section. As a prerequisite to derive quantitative statements, we ask for a category but also for an explanation to assess qualitatively. More concretely, the prompt has the following structure for a student answer A and the (assumed to be optimal) educator\u2019s answer E.\nHere is an answer: A . Here is the optimal answer: E. How close is the answer to the optimal answer? Start the reply with one of the following: Very close., Close., Somewhat close., Somewhat distant., Distant., Very distant.. Explain the choice.\nExample Prompt to LLM:\nHere is an answer: In single linkage, we compare the two closest datapoints (the ones with minimal distance) from two separate clusters. In average linkage, we compare all the datapoints from a cluster with all the datapoints from another cluster and get an average distance. .\nHere is the optimal answer: The two differ in distance metric used to cluster. Single linkage: Merge two clusters based on minimum distance between any two points; Tendency to form long chains; Average linkage: merge two clusters based on average distance between any two points; tendency to \u201cball\u201d like clusters;.How good is the answer to the question? Start the reply with one of the following: Extremely good., Very good., Good., Ok., Bad., Very bad., Extremely bad. Explain the choice. Explain also what is\nmissing. What is the difference between single linkage and average linkage (hierarchical) clustering?\nHow close is the answer to the optimal answer? Start the reply with one of the following: Very close., Close., Somewhat close., Somewhat distant., Distant., Very distant.. Explain the choice.\nResponse of LLM:\nVery close. The given answer effectively highlights the key differences between single linkage and average linkage in hierarchical clustering. It mentions that single linkage does not allow linkage between different groups, while average linkage splits the dataset into average-sized groups. Overall, the given answer accurately addresses the main distinction, making it very close to the optimal answer."
        },
        {
            "heading": "Highlighting deviations of LLM and educator",
            "text": "The prior assessments are helpful, but require a lot of effort for the educator, i.e., the educator must manually assess all responses of the LLM. It might be desirable to focus more on large differences between human grading and the reply of the LLM. Furthermore, quantifying differences allows for a quantitative assessment, which makes it more explicit how well aligned the educator\u2019s grading and the LLM\u2019s grading are.\nWe assume that an educator has provided a score in some range [0,maxPts], where maxPts gives the maximum points attainable by a student. For easier interpretability, we scale the scores to be between 0 and 1. We denote the scaled points given by the educator as ph. We also compute a score given by the LLM denoted as pL based on the LLM\u2019s responses. We compute correlations to understand the alignment between human and LLM. We sort the outcomes based on how different the LLM\u2019s ratings are from the educator\u2019s grading, which allows an educator to focus on the largest differences.\nThe deviations are computed as follows. We assume that if the LLM replies for a given student answer and the educator\u2019s answer with \u201cvery close\u201d then that student should have gotten maximal points, whereas if the response is \u201cvery distant\u201d the student should have gotten no points. Similarly, if the LLM rates a student answer in general for a given exam question as \u201cextremely good\u201d then the student should have gotten maximal points. If it is rated \u201cextremely bad\u201d then the student should have gotten no points. Inbetween, we scale accordingly, i.e., we linearly interpolate to obtain the points an LLM would assign for other categories. We also scale by maxPts to have a normalized value between 0 and 1 for the LLM. We denote the scaled points by pL. We then sort outcomes based on the gap between the human\u2019s and LLM\u2019s scores | ph - pL |. This allows to focus on responses where the educator and LLM disagree the most, i.e., we sort the LLM responses from largest to lowest differences. For easier comparison, in our implementation, we show both the educator\u2019s (or student\u2019s) response and the LLM\u2019s response. This enables us to read the answer of the human, i.e., student\nor educator, and then the reply of the LLM. If the two responses disagree significantly, and indeed the LLM had a valid point, then an educator should alter his/her grading.\nAn example output of the LLM is:\nGap: 0.9 LLM Pts p_l: 0.1 Human Pts p_h: 1.0\nAnswer Human: When using single linkage in hierarchical clustering, the clusters are made with the shortest distance between the data point the closest cluster. Average linkage on the other hand takes the maximum distance and the minium distance to each cluster, evaluates the average and then builds the clusters upon these average distances.\nAnswer LLM: Somewhat close. The answer provides a basic understanding of the differences between single linkage and average linkage in hierarchical clustering. It correctly states that single linkage considers the shortest distance between a data point and the closest cluster, whereas average linkage calculates the average distance to each cluster. However, it does not fully explain how average linkage clusters are formed based on these average distances."
        },
        {
            "heading": "Assessment Setup",
            "text": "We assess two exams administered by two different educators. One exam was in English at the master level in data science with 21 participants. It consisted of 16 questions and had a total of 90 points. It lasted 90 minutes. The other was in German at the bachelor level in Information Systems introductory course with 34 participants. There was a multiple choice part that was not considered, i.e., we were left with 3 questions each worth 10 points resembling 10 minutes of processing time per question, i.e., the replies were expected to be longer than for the questions of the first exam, i.e., the data science exam. The types of questions covered all categories according to the revised Bloom\u2019s taxonomy of educational objectives(Anderson & Krathwohl, 2001). The taxonomy ranges from simple, concrete to complex, abstract questions. Our exams contained some simple concrete questions related to understanding and remembering, such as providing definitions and examples. More complex, abstract questions consisted, for example, of evaluating different approaches for a practical problem.\nIf a question was not answered by the student, we did not send the \u201cempty\u201d response to ChatGPT for an assessment. We read through all of ChatGPT\u2019s responses."
        },
        {
            "heading": "Findings",
            "text": "We first discuss overarching findings before elaborating on each of the three questions in Figure 1. ChatGPT replies generically. ChatGPT tends to assess in a mechanistic generic manner rather than looking at content. It might respond like \u201cThere is not sufficient detail\u201d rather than pointing to specific details that are missing.\nChatGPT and human assessments differ strongly. The correlation between human and LLM\u2019s judgments is small. Generally, the LLM\u2019s judgments have a strong tendency to the middle, e.g., most are \u201cok\u201d or \u201cgood\u201d despite strong variation in the quality of student replies. ChatGPT can help to make sense of hard to understand answers. LLM\u2019s provided a more open view and less negative view on responses suffering from poor language. Thus, the assessment was particularly positive for students with poor (English) language skills, as ChatGPT tended to rate them comparatively better to an educator. That is, a human might rate them poorly because the answer is difficult to understand or remains unclear due to grammatical ambiguities or poor wording. We also found that it was sometimes easier to make sense of a student reply after reading ChatGPT\u2019s assessment. Furthermore, commonly specific concepts tied to a set of keywords are accepted or looked for. If students do not provide any of these but rather a lengthy and verbose reply, there is a higher risk that possibly correct though convoluted arguments are overlooked. We found that ChatGPT\u2019s assessment can be helpful, since it can transform hard to grasp answers into a more concise phrasing and its responses follow an expected structure, which is fast to process for an educator. ChatGPT can drastically change its assessment due to minor changes in answers. Additional content that is strikingly wrong though not related to the question (or answer) can lead to dramatic changes in judgements by the LLM. For illustration, we appended to the answer of the student used in the example prompt (1) either of the following three options: (i) 3*5=7, (ii) the cat sits on the mattress, (iii) 3*5=7, the cat sits on the mattress; ChatGPT judged two of them equivalently as the original prompt (1), i.e. as good. For (ii) and (iii) it would mention that the answers contain irrelevant information, but (ii) is still judged as good by the LLM, while the LLM judged the response (iii) as \u201cvery bad\u201d. ChatGPT favors vague content and fails to recognize contradictions. Generally, replies with vaguely related content, which might be deemed irrelevant or even incorrect by a human grader, is rated more favorably by ChatGPT than by human graders. We also found that ChatGPT can fail to distinguish contradicting statements. We appended to prompt (1) either of the following: (i) Complete linkage uses the minimum distance between any two points in clusters. Density based clustering relies on computing the number of points, possibly organizing them in a search tree or a list. (ii) Complete linkage uses the maximum distance between any two points in clusters. Density based clustering relies on computing point densities, e.g. points for a fixed volume, for the volume for a fixed set of points.\nNote, the words minimum and maximum are switched in (i) and (ii). The LLM judged (i) and (ii) equally, although they obviously contain contradicting statements and information not being asked for. ChatGPT misunderstands questions. ChatGPT can suggest to provide information that can obviously be ruled out as being asked for. For the question \u201cWhat are advantages of a decision tree?\u201d (and a student\u2019s answer) the LLM\u2019s reply included \u201cHowever, what is missing from the answer is a mention of some potential disadvantages or limitations of decision trees.\u201d ChatGPT\u2019s grading criteria are language sensitive. We applied the same prompt patterns for both exams, i.e., we utilized the English prompt pattern for the German exam. While at first, this did not seem to pose a problem, we found that ChatGPT would occasionally provide a lower rating giving as reason that (German) texts contain grammar and spelling issues, but this would not happen for answers in English.\nFindings on assessing the educator\u2019s answer by the"
        },
        {
            "heading": "LLM",
            "text": "We read through all of ChatGPT\u2019s responses. None of them led to any changes of the human crafted responses. Most suggested improvements were generic, e.g., related to giving more details, an example and sometimes visualization or limitations. ChatGPT\u2019s responses were quite sensitive to the phrasing (and potentially other factors). For example, omitting the term \u201cExplain also what is missing.\u201d changed the LLM\u2019s response for one reply from \u201cvery bad\u201d (see Figure 2) to \u201cgood\u201d, while still giving mostly the same reasoning. Overall, Figure 2 suggests that the educator provided mostly \u201cvery good\u201d answers and no answer was below \u201cgood\u201d (at least when slightly changing the prompt as mentioned before).\nFindings on assessing the student\u2019s answer in gen-\neral and relative to the educator\u2019s answer\nHere, LLM\u2019s had more impact on the grading. That is, we made minor adjustments after reading through the LLM\u2019s assessment. The adjustments were made due to two types of replies: First the LLM would rate student answers (more) positively that were not part of the lecture material and also not directly being asked for. For example, we asked \u201cWhich two of the factors \u2018data, compute and algorithms\u2019 are most important for the rise of GPT-3 (around 2020) and ChatGPT in 2022 and other generative AI models?\u201d Some students responded that one factor was media coverage and accessibility due to its public release. ChatGPT rated the responses of these students positively, although (1) the question explicitly restricts the factors to be discussed and (ii) the importance of media coverage is debatable \u2013 at least for GPT3. That is, in the lecture, it was mentioned for ChatGPT that its public release led to widespread adoption and a surge in media coverage, but not so much for GPT-3. GPT-3 was covered less in the media, and it was less accessible, i.e., only (some) scientists got access. Still, in the end we decided to provide some recognition for mentioning \u201cmedia coverage\u201d. Second, the LLM would more positively rate replies with poor English. That is, the LLM\u2019s interpretation of the answer made the student answer more understandable. For an educator, the quality of the answer needs to exceed a minimum level of proficiency to be understood. Comprehensibility is generally a factor influencing grading. Educators are not supposed to make too many assumptions about what a student wants to say (i.e. interpret) but they have to stick with the incomprehensible answer and grade accordingly. Overall, we found that any judgement of the grading after consulting the LLM was preceded by considerable reflection and debates, and it was not evident whether the differences of the LLM should really be considered. Interestingly, using an answer set of an exam conducted in German, the LLM incorporated errors in spelling and grammar in the feedback and downgraded answers of poor language quality.\nThe LLM tended to rate most student responses as \u201cgood\u201d or \u201cvery good\u201d (Figure 2), i.e., there was little differentiation. This is in stark contrast to the rating of the educator (Figure 3). The educator scored many answers with maximum or minimum points but he/she also assigned commonly points in-between the two extremes. The extremes were mostly common for short and easy answers with few points. When it comes to assessing similarity between the educator\u2019s and the students\u2019 answers the LLM gave somewhat more diverse replies. However, overall alignment was poor. The pearson correlation between the LLM\u2019s similarity assessment pL and educator\u2019s grading ph was close to 0."
        },
        {
            "heading": "Discussion",
            "text": "We set out to assess LLMs for autograding, primarily as a second opinion. Specifics of the course have not been provided to the LLM, e.g., the course material for which the exam was made for. That is, the LLM lacked any context that is lecture specific but relied more on world knowledge incorporated in its training data. Thus, discrepancies between the judgments of the LLM and lecturer are expected, e.g., as many terms are defined differently in different contexts and fields. Such contextualization of terms seems to be\nan essential part of teaching and learning and allows students to establish different perspectives on issues. However, for grading, we believe that the lack of context by the LLM can be highly valuable, as it provides a strongly complementary view that aims to avoid strong biases of a lecturer towards the lecture material. Still, this also hints that grading (or possibly even exam questions) derived by an LLM from the course material could create a more neutral and additional testing environment as the exam content would be grounded in the lecture material only. We also faced a number of practical issues when using LLMs. For example, LLM\u2019s replies would not always follow the given structure, i.e., ChatGPT would reply with any of the asked for words \u201cVery good\u201d, \u201dGood\u201d etc. but started the reply with some other sentence. This problem can often be mitigated by providing a few examples of inputs and desired outputs (few-shot learning). However, doing so means additional work for the educator, increases response time of the LLM and also costs, i.e., longer input prompts imply higher costs for commercial LLMs such as ChatGPT. We also experimented with prompting and often found that there are trade-offs. For example, when comparing the student\u2019s answer and the educator\u2019s answer, we tested prompts that included the question (as well as the answers of the student and educator) and prompts that did not. We found that without the question, ChatGPT\u2019s assessment sometimes tended to include aspects that were rather unrelated to the question. If the question was added, the assessment of ChatGPT sometimes did not consider the educator\u2019s answer. While through experimenting, this problem could be reduced to some extent, it was still prevalent. It is also tempting to use LLMs for fully automatic grading. However, from our experience this should not be undertaken at the current point in time since there is very strong disagreement between gradings of educators and LLMs. That is, they perform significantly worse in judging questions than in providing responses. This might be improved using techniques such as few shot learning, i.e., providing examples on how answers should be graded. However, first experimentation did not yield the hoped performance boost and also the number of examples that are possible to be added through prompting is limited as prompts sizes are restricted. In general, finding the best prompts for grading is non-trivial and responses could be sensitive to the slightest changes in phrasing. Grading should be robust, fair, and consistent. Accordingly, the achievement of competency levels of students should be assessed as independently as possible of individual course delivery, of lecturers and examiners, and of the performance of other students in an exam. ChatGPT did not (yet) meet these requirements in our evaluation. We employed the idea to focus on answers where the LLM and the human showed largest discrepancies. However, unfortunately, ChatGPT\u2019s rating was not too well-aligned with that of the educator. Furthermore, if not all answers are\nchecked (but only those with large differences), biases in the LLM might further impact the grading by leading to a bias in which answers are looked at (again) by a human grader. Furthermore, biases also appear as \u201cmisleading clues\u201d. That is, if for example, the LLM judges a particular argument An identical to the educator but another argument B is considered irrelevant by the LLM, students using B might be judged worse than they are by the LLM. One assessment within our work assumed that an educator provides a single answer to a question. In principle, a question might permit fairly different. However, it is not hard to allow for multiple responses, i.e., an educator could define various answers that might even be contradictory. We could then compare a student\u2019s answer with all of the educator\u2019s responses and focus on the educator\u2019s response that is deemed closest by the LLM. However, specifying answers becomes more difficult the more open-ended a question is, i.e., the more knowledge should be applied and transferred, as opposed to simply replicating knowledge. From an ethical point of view, one might also debate whether changes due to large language models should only improve grades. That is, LLMs cannot fail any student but only help them getting better, as punishing innocent people can be seen as worse than rewarding people not having deserved it. Furthermore, using an LLM as \u201ca second opinion\u201d might also provide a false sense of security."
        },
        {
            "heading": "Future Work",
            "text": "We might add a more explicit grading scheme that aims to identify specific aspects in the answer, i.e., \u201cIs this concept in the answer?\u201d (If not deduct x points). Furthermore, a fine-tuned LLM towards grading might lead to better outcomes than relaying on prompting. To this end, large number of graded exams would be needed. While graded exams already exist, sharing them is non-trivial as responses might have to be anonymized to comply with privacy regulations. LLMs might also be useful for exam development, i.e., assessing questions prior to posting an exam. One might also provide access to the lecture material to the LLM to assess gradings. This might uncover more minor issues in the grading scheme, but might not help so much in uncovering general issues. In this study, we used LLM only in grading answers on questions that have been formulated by lecturers. It would be interesting to test the end-to-end support by LLMs in designing a lecture, including the selection of topic areas, creating the lecture material, and preparing and assessing the exam."
        },
        {
            "heading": "Related Work",
            "text": "The manual grading process involves a labor-intensive evaluation of students\u2019 responses, requiring expertise and careful judgment to assign appropriate scores. Thus, to assist educators in reducing the time and effort spent on grading, there is a growing interest in leveraging AI-driven correction aids (Basu et al., 2013; Condor et al., 2021). When comparing the conventional teacher\u2019s judgement (\u201chuman scoring\u201d) to the capabilities auf automatic feedback and assessment tools (\u201cmachine scoring\u201d), we encounter distinct strengths along various quality criteria (Seufert et al., 2022), i.e., AI can support objectivity, reliability, validity and comparative values and standards. The evolution of assessment methodologies is currently exploring hybrid solutions that harness the strengths of both mechanisms. These developments, such as AI-based assistants for assessment and learner feedback, hold promise for the future education, offering more efficient and objective evaluation processes while maintaining the depth of understanding provided by human judgement (Saha et al., 2018; Schneider et al., 2023). A few works have also assessed the value of feedback through autograding, e.g., (Vittorini et al., 2020) also assesses the value of feedback provided by the autograder for students. We concentrate on the field of Automatic Short Answer Grading (ASAG) (Burrows et al., 2015). It deals with grading student answers, typically ranging from a phrase to a paragraph. ASAG also covers the grading of open-ended answers (Baral et al., 2021). The primary focus in ASAG is on content quality rather than the writing style and structure emphasized as in automatic essay scoring (AES) (Dikli, 2010). For ASAG, prior work has mostly relied on BERT as a large language model (Baral et al., 2021; Schneider et al., 2023; Sung et al., 2019). Schneider et al. (2023) investigated how LLMs such as BERT suffer from trust issues that might be partially mitigated by only automatically grading answers, if the LLM is certain about its grading. While LLMs can provide justifications for their decisions without any additional work, dedicated methods for enhancing explainability have been evaluated in the realm of automatic grading systems (Kumar & Boulanger, 2020). Efforts have also been made to tackle the limitations of AI in terms of fairness in the context of automatic grading (Madnani et al., 2017) and, more generally, ethical issues related to LLMs in education (Yan et al., 2023)."
        },
        {
            "heading": "Conclusions",
            "text": "The integration of LLMs into academic settings have become an undeniable reality. These models possess remarkable linguistic capabilities, coupled with unexpected reasoning abilities. Yet, using LLMs such as ChatGPT \u201cout-of-the-\nbox\u201d to support grading requires great care due to a multitude of issues such as sensitivity to minor changes in answers and lack of concise reasoning, which is also reflected in poor alignment with human graders. Despite these limitations, LLMs currently offer a valuable resource that provides a supplementary viewpoint with vminimal effort."
        }
    ],
    "title": "Towards LLM-based Autograding for Short Textual Answers",
    "year": 2023
}