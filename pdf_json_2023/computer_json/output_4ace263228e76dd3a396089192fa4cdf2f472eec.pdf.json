{
    "abstractText": "Automatically extracting urban buildings from remote sensing images has essential application value, such as urban planning and management. Gaofen-7 (GF-7) provides multi-perspective and multispectral satellite images, which can obtain three-dimensional spatial information. Previous studies on building extraction often ignored information outside the red\u2013green\u2013blue (RGB) bands. To utilize the multi-dimensional spatial information of GF-7, we propose a dual-stream multi-scale network (DMU-Net) for urban building extraction. DMU-Net is based on U-Net, and the encoder is designed as the dual-stream CNN structure, which inputs RGB images, near-infrared (NIR), and normalized digital surface model (nDSM) fusion images, respectively. In addition, the improved FPN (IFPN) structure is integrated into the decoder. It enables DMU-Net to fuse different band features and multi-scale features of images effectively. This new method is tested with the study area within the Fourth Ring Road in Beijing, and the conclusions are as follows: (1) Our network achieves an overall accuracy (OA) of 96.16% and an intersection-over-union (IoU) of 84.49% for the GF-7 selfannotated building dataset, outperforms other state-of-the-art (SOTA) models. (2) Three-dimensional information significantly improved the accuracy of building extraction. Compared with RGB and RGB + NIR, the IoU increased by 7.61% and 3.19% after using nDSM data, respectively. (3) DMU-Net is superior to SMU-Net, DU-Net, and IEU-Net. The IoU is improved by 0.74%, 0.55%, and 1.65%, respectively, indicating the superiority of the dual-stream CNN structure and the IFPN structure.",
    "authors": [
        {
            "affiliations": [],
            "name": "Peihang Li"
        },
        {
            "affiliations": [],
            "name": "Zhenhui Sun"
        },
        {
            "affiliations": [],
            "name": "Guangyao Duan"
        },
        {
            "affiliations": [],
            "name": "Dongchuan Wang"
        },
        {
            "affiliations": [],
            "name": "Qingyan Meng"
        },
        {
            "affiliations": [],
            "name": "Yunxiao Sun"
        }
    ],
    "id": "SP:573972db1e36a27d3db1800ff8e39dce41738252",
    "references": [
        {
            "authors": [
                "L. Cheng",
                "F. Zhang",
                "S. Li",
                "J. Mao",
                "H. Xu",
                "W. Ju",
                "X. Liu",
                "J. Wu",
                "K. Min",
                "X Zhang"
            ],
            "title": "Solar energy potential of urban buildings in 10 cities of China",
            "year": 2020
        },
        {
            "authors": [
                "M. Xu",
                "C. Cao",
                "P. Jia"
            ],
            "title": "Mapping fine-scale urban spatial population distribution based on high-resolution stereo pair images, points of interest, and land cover data",
            "venue": "Remote Sens. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shen",
                "S. Zhu",
                "T. Yang",
                "C. Chen",
                "D. Pan",
                "J. Chen",
                "L. Xiao",
                "Q. Du"
            ],
            "title": "Bdanet: Multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "G. White",
                "A. Zink",
                "L. Codec\u00e1",
                "S. Clarke"
            ],
            "title": "A digital twin smart city for citizen feedback",
            "venue": "Cities",
            "year": 2021
        },
        {
            "authors": [
                "S. Du",
                "Y. Zhang",
                "Z. Zou",
                "S. Xu",
                "X. He",
                "S. Chen"
            ],
            "title": "Automatic building extraction from LiDAR data fusion of point and grid-based features",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2017
        },
        {
            "authors": [
                "M. Shahzad",
                "M. Maurer",
                "F. Fraundorfer",
                "Y. Wang",
                "X.X. Zhu"
            ],
            "title": "Buildings detection in VHR SAR images using fully convolution neural networks",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2018
        },
        {
            "authors": [
                "D. Feng",
                "H. Chen",
                "Y. Xie",
                "Z. Liu",
                "Z. Liao",
                "J. Zhu",
                "H. Zhang"
            ],
            "title": "GCCINet: Global feature capture and cross-layer information interaction network for building extraction from remote sensing imagery",
            "venue": "Int. J. Appl. Earth Obs. Geoinf",
            "year": 2022
        },
        {
            "authors": [
                "J. Huang",
                "X. Zhang",
                "Q. Xin",
                "Y. Sun",
                "P. Zhang"
            ],
            "title": "Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2019
        },
        {
            "authors": [
                "J. Kang",
                "Z. Wang",
                "R. Zhu",
                "J. Xia",
                "X. Sun",
                "R. Fernandez-Beltran",
                "A. Plaza"
            ],
            "title": "DisOptNet: Distilling Semantic Knowledge From Optical Images for Weather-Independent Building Segmentation",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2022
        },
        {
            "authors": [
                "P. Zhang",
                "P. Du",
                "C. Lin",
                "X. Wang",
                "E. Li",
                "Z. Xue",
                "X. Bai"
            ],
            "title": "A hybrid attention-aware fusion network (HAFNET) for building extraction from high-resolution imagery and LiDAR data",
            "year": 2020
        },
        {
            "authors": [
                "F. Amjadipour",
                "H. Ghassemian",
                "M. Imani"
            ],
            "title": "Building Detection Using Very High Resolution SAR Images with Multi-Direction Based on Weighted-Morphological Indexes",
            "venue": "In Proceedings of the 2022 International Conference on Machine Vision and Image Processing (MVIP),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Sun",
                "Y. Hua",
                "L. Mou",
                "X.X. Zhu"
            ],
            "title": "CG-Net: Conditional GIS-Aware network for individual building segmentation in VHR SAR images",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "X. Huang",
                "L. Tu",
                "T. Zhang",
                "L. Wang"
            ],
            "title": "A review of building detection from very high resolution optical remote sensing images",
            "venue": "GIScience Remote Sens",
            "year": 2022
        },
        {
            "authors": [
                "X. Ji",
                "B. Yang",
                "Q. Tang",
                "W. Xu",
                "J. Li"
            ],
            "title": "Feature fusion-based registration of satellite images to airborne LiDAR bathymetry in island area",
            "venue": "Int. J. Appl. Earth Obs. Geoinf",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhu",
                "X. Tang",
                "G. Zhang",
                "B. Liu",
                "W. Hu"
            ],
            "title": "Accuracy Comparison and Assessment of DSM Derived from GFDM Satellite and GF-7 Satellite Imagery",
            "venue": "Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "H. Luo",
                "B. He",
                "R. Guo",
                "W. Wang",
                "X. Kuai",
                "B. Xia",
                "Y. Wan",
                "D. Ma",
                "L. Xie"
            ],
            "title": "Urban Building Extraction and Modeling Using GF-7 DLC and MUX Images",
            "year": 2021
        },
        {
            "authors": [
                "J. Wang",
                "X. Hu",
                "Q. Meng",
                "L. Zhang",
                "C. Wang",
                "X. Liu",
                "M. Zhao"
            ],
            "title": "Developing a Method to Extract Building 3D Information from GF-7 Data",
            "venue": "Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Gharibbafghi",
                "J. Tian",
                "P. Reinartz"
            ],
            "title": "Modified superpixel segmentation for digital surface model refinement and building extraction from satellite stereo imagery",
            "venue": "Remote Sens. 2018,",
            "year": 1991
        },
        {
            "authors": [
                "M. Kumar",
                "A. Bhardwaj"
            ],
            "title": "Building Extraction from Very High Resolution Stereo Satellite Images using OBIA and Topographic Information",
            "venue": "Environ. Sci. Proc. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "X. Jin",
                "C.H. Davis"
            ],
            "title": "Automated building extraction from high-resolution satellite imagery in urban areas using structural, contextual, and spectral information",
            "venue": "EURASIP J. Adv. Signal Process",
            "year": 2005
        },
        {
            "authors": [
                "X. Huang",
                "L. Zhang"
            ],
            "title": "A multidirectional and multiscale morphological index for automatic building extraction from multispectral GeoEye-1 imagery",
            "venue": "Photogramm. Eng. Remote Sens",
            "year": 2011
        },
        {
            "authors": [
                "D. Singh",
                "R. Maurya",
                "A.S. Shukla",
                "M.K. Sharma",
                "P. Gupta"
            ],
            "title": "Building extraction from very high resolution multispectral images using NDVI based segmentation and morphological operators",
            "venue": "In Proceedings of the 2012 Students Conference on Engineering and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "S. Dahiya",
                "P.K. Garg",
                "M.K. Jat"
            ],
            "title": "Object oriented approach for building extraction from high resolution satellite images",
            "venue": "In Proceedings of the 2013 3rd IEEE International Advance Computing Conference (IACC), Ghaziabad, India,",
            "year": 2013
        },
        {
            "authors": [
                "I. Grinias",
                "C. Panagiotakis",
                "G. Tziritas"
            ],
            "title": "MRF-based segmentation and unsupervised classification for building and road detection in peri-urban areas of high-resolution satellite images",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2016
        },
        {
            "authors": [
                "S. Sanlang",
                "S. Cao",
                "M. Du",
                "Y. Mo",
                "Q. Chen",
                "W. He"
            ],
            "title": "Integrating aerial LiDAR and very-high-resolution images for urban functional zone mapping",
            "venue": "Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "L. Zhang",
                "J. Wu",
                "Y. Fan",
                "H. Gao",
                "Y. Shao"
            ],
            "title": "An efficient building extraction method from high spatial resolution remote sensing images based on improved mask R-CNN",
            "venue": "Sensors",
            "year": 2020
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2015
        },
        {
            "authors": [
                "B. Yu",
                "L. Yang",
                "F. Chen"
            ],
            "title": "Semantic segmentation for high spatial resolution remote sensing images based on convolution neural network and pyramid pooling module",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "W. Liu",
                "M. Yang",
                "M. Xie",
                "Z. Guo",
                "E. Li",
                "L. Zhang",
                "T. Pei",
                "D. Wang"
            ],
            "title": "Accurate building extraction from fused DSM and UAV images using a chain fully convolutional neural network",
            "year": 2019
        },
        {
            "authors": [
                "W. Kang",
                "Y. Xiang",
                "F. Wang",
                "H. You"
            ],
            "title": "EU-Net: An efficient fully convolutional network for building extraction from optical remote sensing images",
            "venue": "Remote Sens. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "M. Chen",
                "J. Wu",
                "L. Liu",
                "W. Zhao",
                "F. Tian",
                "Q. Shen",
                "B. Zhao",
                "R. Du"
            ],
            "title": "DR-Net: An improved network for building extraction from high resolution remote sensing",
            "year": 2021
        },
        {
            "authors": [
                "W. Deng",
                "Q. Shi",
                "J. Li"
            ],
            "title": "Attention-gate-based encoder\u2013decoder network for automatical building extraction",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "T.Y. Lin",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "S. Wei",
                "S. Ji",
                "M. Lu"
            ],
            "title": "Toward automatic building footprint delineation from aerial images using CNN and regularization",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2019
        },
        {
            "authors": [
                "S. Ran",
                "X. Gao",
                "Y. Yang",
                "S. Li",
                "G. Zhang",
                "P. Wang"
            ],
            "title": "Building multi-feature fusion refined network for building extraction from high-resolution remote sensing images",
            "venue": "Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "P. Zhang",
                "Y. Ke",
                "Z. Zhang",
                "M. Wang",
                "P. Li",
                "S. Zhang"
            ],
            "title": "Urban land use and land cover classification using novel deep learning models based on high spatial resolution satellite imagery",
            "venue": "Sensors 2018,",
            "year": 2018
        },
        {
            "authors": [
                "R. Tamilarasi",
                "S. Prabu"
            ],
            "title": "Automated building and road classifications from hyperspectral imagery through a fully convolutional network and support vector machine",
            "venue": "J. Supercomput",
            "year": 2021
        },
        {
            "authors": [
                "N. Audebert",
                "B. Le Saux",
                "S. Lef\u00e8vre"
            ],
            "title": "Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2018
        },
        {
            "authors": [
                "S. Piramanayagam",
                "E. Saber",
                "W. Schwartzkopf",
                "F.W. Koehler"
            ],
            "title": "Supervised classification of multisensor remotely sensed images using a deep learning framework",
            "venue": "Remote Sens. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "D. Marmanis",
                "K. Schindler",
                "J.D. Wegner",
                "S. Galliani",
                "M. Datcu",
                "U. Stilla"
            ],
            "title": "Classification with an edge: Improving semantic image segmentation with boundary detection",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2018
        },
        {
            "authors": [
                "H. Hirschmuller"
            ],
            "title": "Stereo processing by semiglobal matching and mutual information",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2007
        },
        {
            "authors": [
                "W. Zhang",
                "H. Huang",
                "M. Schmitz",
                "X. Sun",
                "H. Wang",
                "H. Mayer"
            ],
            "title": "Effective fusion of multi-modal remote sensing data in a fully convolutional network for semantic labeling",
            "venue": "Remote Sens. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "S. Ji",
                "S. Wei"
            ],
            "title": "Building extraction via convolutional neural networks from an open remote sensing building dataset",
            "venue": "Acta Geod. Cartogr. Sin. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Zhou",
                "S. Wang",
                "F. Wang",
                "Z. Xu"
            ],
            "title": "House building extraction from high resolution remote sensing image based on IEU-Net",
            "venue": "J. Remote Sens. 2021,",
            "year": 1991
        },
        {
            "authors": [
                "J. Jiang",
                "F. Liu",
                "Y. Xu",
                "H. Huang"
            ],
            "title": "Multi-spectral RGB-NIR image classification using double-channel CNN",
            "venue": "IEEE Access 2019,",
            "year": 2061
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "N. Srivastava",
                "G. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "J. Mach. Learn. Res",
            "year": 1958
        },
        {
            "authors": [
                "K.A.M. Said",
                "A.B. Jambek",
                "N. Sulaiman"
            ],
            "title": "A study of image processing using morphological opening and closing processes",
            "venue": "Int. J. Control. Theory Appl. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "A. Gribov"
            ],
            "title": "Searching for a compressed polyline with a minimum number of vertices (discrete solution)",
            "venue": "In Proceedings of the International Workshop on Graphics Recognition, Kyoto, Japan,",
            "year": 2017
        },
        {
            "authors": [
                "A. Gribov"
            ],
            "title": "Optimal Compression of a Polyline While Aligning to Preferred Directions",
            "venue": "In Proceedings of the 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), Sydney, Australia,",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhao",
                "J. Shi",
                "X. Qi",
                "X. Wang",
                "J. Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "L.C. Chen",
                "Y. Zhu",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "F. Miao"
            ],
            "title": "Building extraction from remote sensing images using deep residual U-Net",
            "venue": "Eur. J. Remote Sens. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J. Avbelj",
                "R. M\u00fcller",
                "R. Bamler"
            ],
            "title": "A metric for polygon comparison and building extraction evaluation",
            "venue": "IEEE Geosci. Remote Sens. Lett. 2014,",
            "year": 2014
        },
        {
            "authors": [
                "W. Zhao",
                "C. Persello",
                "A. Stein"
            ],
            "title": "Building outline delineation: From aerial images to polygons with an improved end-to-end learning framework",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Li, P.; Sun, Z.; Duan, G.;\nWang, D.; Meng, Q.; Sun, Y.\nDMU-Net: A Dual-Stream\nMulti-Scale U-Net Network Using\nMulti-Dimensional Spatial\nInformation for Urban Building\nExtraction. Sensors 2023, 23, 1991.\nhttps://doi.org/10.3390/s23041991\nAcademic Editor: Ludovic Macaire\nReceived: 15 December 2022\nRevised: 3 February 2023\nAccepted: 7 February 2023\nPublished: 10 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: GF-7 image; building extraction; nDSM; semantic segmentation; dual-stream network"
        },
        {
            "heading": "1. Introduction",
            "text": "Buildings, as the basic geographical entity of cities, its spatial information are widely used in urban planning, disaster mitigation and prevention, population forecasting, and energy consumption [1\u20134]. With the rapid development of remote sensing technology, the spatial resolution of remote sensing images has reached the sub-meter level. The images have more spectral, texture, and spatial structure information, making it possible to refine and automatically extract buildings. Due to the density of urban buildings and the diversification of roof materials and structures, how to extract buildings accurately and quickly still face challenges. In recent decades, there has been a great deal of research on building extraction. According to the type of data used, the methods of building extraction can be roughly\nSensors 2023, 23, 1991. https://doi.org/10.3390/s23041991 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 1991 2 of 19\ndivided into two categories: one is to extract buildings based on only a single data type, such as optical image, SAR image, LiDAR data, etc. [5\u20137]; The other is to extract buildings based on the fusion of multi-source data, such as optical image + LiDAR data fusion, optical image + SAR image fusion, etc. [8,9]. Compared to a single data type, the method of multisource data fusion can obtain more features. For example, three-dimensional information can be obtained through LiDAR data, which helps to distinguish buildings and similar ground objects (such as roads, squares, etc.) [10]; Buildings with different heights, scales, and properties in SAR images have different scattering characteristics [11]. However, due to the high cost of LiDAR data acquisition and the interpretability of SAR images, it is still difficult to quickly and effectively obtain single large-scale buildings in cities [12,13]. At the same time, when the data of different sensors are fused, the registration error of different sensors and the change of ground objects caused by the gap in acquisition time have a certain impact on building extraction [14]. Very High Resolution (VHR) stereo image pairs can simultaneously acquire VHR multispectral images and digital surface model (DSM) data in a large area, which can effectively avoid the above errors and reduce the cost of LiDAR data acquisition. Gaofen-7 (GF-7), one of China\u2019s most advanced stereo observation satellites, provides DSM data with an elevation root mean square error lower than 1 m [15]. Stereo images acquired by GF-7 have been used in three-dimensional information extraction and modeling of urban buildings, showing the power to describe the vertical structure of urban features [16,17]. However, there have been studies using DSM data constructed from Pleiades or WorldView stereo satellite imagery for building extraction [18,19]. The potential of GF-7 stereo pair in building extraction needs to be further verified. There are two technical difficulties in building extraction. One is the regularity and integrity of building edge extraction, and the other is the huge difference in the size of different buildings and how to extract features about buildings of different scales efficiently. Based on the characteristics of GF-7, the fusion strategy of multimodal data should also be considered. Among them, nDSM data can enhance the detailed characteristics of buildings and improve the integrity of building segmentation, which can effectively solve the first problem. Two other problems, the multi-scale of buildings feature extraction and the fusion of multi-modal data, will be discussed in detail next. Urban buildings are of various scales, and the sizes may vary dozens of times, so it is challenging for multi-scale building feature extraction. The traditional methods of extracting buildings rely on artificial design features, such as structure, texture, spectrum, and other features of buildings, which are classified by feature matching or machine learning [20\u201325]. However, the increase in intra-class variance and the decrease of inter-class variance in high-resolution remote sensing images make it more difficult to design features manually [26]. In recent years, with the development of deep learning, convolutional neural networks (CNNs) have been widely used in remote sensing image semantic segmentation. In particular, end-to-end networks, represented by fully convolutional neural networks, achieve pixel-level classification and become a new paradigm for semantic segmentation [27]. However, due to the repeated down-sampling of the network, the spatial relationship is lost, and the result of up-sampling is blurred, which is not sensitive to the boundaries and details of buildings. Later, the encoder-decoder network structure represented by the U-Net, up-sampling through deconvolution and introducing skip layer connections to fuse shallow features and deep features, retaining certain image details. However, U-Net has limited generalization ability, which is not conducive to extracting buildings of different scales [28]. One solution is to take images of different scales as input to obtain multi-scale features. Sun et al. input three image patches of different sizes into three different CNN models and finally sent the mixed features fused by the three models into the support vector machine to obtain complete building information, but the models were large, and the operation was complicated [29]. Another solution is to use the Spatial Pyramid Pooling (SPP) module to obtain multi-scale features by fusing feature maps of different sizes of receptive fields [30\u201333]. Based on the encoder-decoder structure, Deng et al. obtained multi-scale features by adding the atrous spatial pyramid pooling module at\nSensors 2023, 23, 1991 3 of 19\nthe end of the encoder [34]. However, the amount of calculation is large, as to adding the SPP module at the end of the encoder and ignoring the shallow characteristics of the model, and the size of different receptive fields needs to be determined by multiple experiments. Lin et al. proposed the feature pyramid structure (FPN) to construct semantic features at various scales through a hierarchical structure of lateral connections [35]. FPN is simple to operate, has a minimal amount of computation, is similar to the U-Net, and can be easily integrated into the U-Net backbone network. It has been used in related semantic segmentation tasks and achieved good results [36,37]. CNNs are mainly based on RGB images, which cannot be directly applied to multimodal data, so the corresponding multi-modal fusion strategy is necessary. According to the different positions of fusion, it can be divided into three types: (1) Data-level fusion. Multi-modal data are fused before feature extraction using data superposition or dimensional reduction [38,39]. However, this strategy ignores the correlation between different modal data features. (2) Feature-level fusion. In the feature learning stage, the features of different modal data are fused [40,41]. This method fails to fully exploit the high-level features of individual modality data. (3) Decision-level fusion. The output results of different modal data are fused by averaging or voting [41,42]. This method fails to exploit individual modality data\u2019s low-level and mid-level features fully. We propose a new fusion architecture with the U-Net as the basic network structure to fully use the low-level, mid-level, and high-level features of different modal data. The dual-stream CNN structure is used to extract the features of RGB images and NIR + nDSM images. The shallow and middle-level features of different modalities are fused with the deep features of the up-sampling stage with the help of the skip-layer connection structure, to avoid the loss of varying depth features. To effectively extract the features of buildings of different scales and fully leverage both individual modal and cross-modal features, we proposed a simple and effective dualstream multi-scale building extraction network named DMU-Net. The main contributions are as follows:\n(1) The dual-stream structure of the DMU-Net can effectively extract the features of multimodal data, and the building features of different scales can be effectively integrated through the IFPN structure. (2) The fusion of three-dimensional data with two-dimensional data significantly improves the accuracy of urban building extraction. (3) Compared with different semantic segmentation networks, the DMU-Net has higher accuracy while preserving edge details."
        },
        {
            "heading": "2. Study Area and Dataset",
            "text": "GF-7 was successfully launched in November 2019, and it is China\u2019s first civilian sub-meter stereoscopic mapping satellite equipped with dual line scan cameras. GF-7 acquires 0.8 m front-view images (+26\u25e6) and 0.65 m rear-view images (\u22125\u25e6), and 2.6 m four-band multispectral images acquired by the rear-view multispectral camera. Taking the region within the Fourth Ring Road of Beijing as the study area, two adjacent GF-7 images obtained on 16 October 2020, were selected, with an area of about 302 km2, as shown in\nFigure 1. The study area covers major commercial and residential zones in Beijing, with dense buildings and diverse building structures.\nTo ensure the generalizability of the model, five distinct regions are selected from the study area, regions (a), (b), (c) as training and validation regions, and regions (d) and (e) as test regions. The ground-truth labels of the five regions are obtained by manual annotation. Regions (a), (b), and (c) cover the main types of buildings in the Fourth Ring Road, such as dense low-rise buildings, medium and high-rise buildings, and factory buildings. Region (d) contains other ground objects, such as water bodies, vegetation, squares, and roads, which can test the ability of different data to distinguish buildings from other ground objects. Region (e) contains large factory buildings, which can test the power of nDSM data and different network structures in extracting completeness and details of buildings.\nSensors 2023, 23, 1991 4 of 19\nBased on the original image and ground-truth label, 640 pairs of image slices with a size of 512 \u00d7 512 pixels are randomly cropped from regions (a), (b), and (c) as the dataset, of which 540 pairs are selected as the training set, and the remaining 100 pairs are used as the validation set. Regions (d) and (e) are taken as test sets. The details of the dataset partition are shown in Table 1.\nThere was an error in Figure 1 in the original paper. As shown in Figure (a), the places marked by red arrows are the wrong parts. We have reversed the marked positions of RGB and nDSM. Figure (b) is the corrected image. Corrected images are sent as files.\n(a). Incorrect picture in original article\nTraining and validation area Test area Study area\n(a)\n(c)\n(b) (d)\n(e)nDSM RGB\nRGB nDSM\n(b). The corrected picture Figure 1. The study area is in Beijing. (a\u2013c) as training and validation regions. (d,e) as test regions."
        },
        {
            "heading": "3. Materials and Methods",
            "text": "The flowchart of the proposed method in this study is illustrated in Figure 2. It can be summarized by the following steps: (1) Data Preprocessing. The GF-7 backward multispectral image and backward panchromatic image are fused to obtain a VHR multispectral image, and the nDSM data is constructed based on the front and backward panchromatic images. (2) DMU-Net for urban building extraction. (3) Morphological operations and vector data regularization are used for post-processing.\nSensors 2023, 23, 1991 5 of 19\nSensors 2023, 23, x FOR PEER REVIEW 5 of 19\npanchromatic images. (2) DMU-Net for urban building extraction. (3) Morphological op-\nerations and vector data regularization are used for post-processing."
        },
        {
            "heading": "3.1. Data Preprocessing",
            "text": "Geomatica. Image Sharpening mainly includes ground control point (GCP) and tie point\n(TP) collection, orthorectification, and panchromatic sharpening. The 0.5 m resolution or-\nthophoto from the Map World (TianDiTu) is used as the geographic reference image. The\nfast Fourier transform phase matching algorithm (FFTP) collects the GCPs between the\nrear-view panchromatic, multispectral image and the georeferenced image. GCPs with\nresidual values greater than three are removed, and TPs are gathered to match panchro-\nmatic and multispectral images. Afterward, the points with larger errors are removed\nbased on the residual report to complete the collection of GCPs and TPs. Finally, the back-\nward panchromatic, multispectral images are orthorectified based on the rational func-\ntional model, and then the multi-resolution analysis algorithm is used for image sharpen-\ning. nDSM generation mainly includes the collection of GCPs and TPs, creating epipolar\nimages, extracting DSM, and image filtering. The GCPs and TPs of the front-view pan-\nchromatic and forward panchromatic images are collected using FFTP. Then, the forward\npanchromatic image and the backward panchromatic image are determined as the left\nepipolar image and the right epipolar image to complete the creation of the epipolar im-\nages. The semi-global matching algorithm is used to generate DSM data [43]. Finally, a\nvariety of filtering strategies are used to obtain digital elevation model (DEM) data, DEM\ndata is subtracted from DSM data to obtain nDSM data, and the nDSM values of water\nbodies and buildings with missing height information are corrected by calculating the av-\nerage value in the region or reassigning them.\nl ti , orthorectification, and panchromatic sharpe ing. The 0.5 m resoluti n orthoph to from the Map World (TianDiTu) is used as the geogra ic r f i . f t rier transform phase matching algorithm (FFTP) collects the GCPs between the rearview panchromatic, multispectral image and the georeferenc d image. GCPs with residual values greater than three are removed, and TPs are gathered to match panchromatic and\nultispectral images. Afterward, the points with larger errors are removed based on the residual report to complete the collection of GCPs and TPs. Finally, the backward panchromatic, multispectral images are orthorectified based on the rational functional model, and then the multi-resolution analysis algorithm is used for image sharpening. nDSM generation mainly includes the collection of GCPs and TPs, creating epipolar images, extracting DSM, and image filtering. The GCPs and TPs of the front-view panchromatic and forward panchromatic images are collected using FFTP. Then, the forward panchromatic image and the backward panchromatic image are determined as the left epipolar image and the right epipolar image to complete the creation of the epipolar images. The semiglobal matching algorithm is used to generate DSM data [43]. Finally, a variety of filtering strategies are used to obtain digital elevation model (DEM) data, DEM data is subtracted from DSM data to obtain nDSM data, and the nDSM values of water bodies and buildings with missing height information are corrected by calculating the average value in the region or reassigning them."
        },
        {
            "heading": "3.2. DMU-Net Architecture",
            "text": "We design DMU-Net that can fuse multi-modal data and multi-scale features to improve the accuracy of building extraction. As shown in Figure 3, inspired by previous studies [36,44\u201346], based on U-Net, we embed the two-stream CNN structure into the encoder of U-Net to obtain comprehensive features of different modal data. In the dual-\nSensors 2023, 23, 1991 6 of 19\nstream structure, one stream is used to input the RGB image to get the feature of the two-dimensional spatial structure. The other stream is used to input the NIR + nDSM image, mainly used to obtain the feature of the three-dimensional spatial structure. In the decoder structure, it is still the up-sampling and skip connections of U-Net. Up-sampling is used to restore the dimension of the feature map, and the skip connection fuses the downsampled feature map during the up-sampling process to realize the fusion of shallow and deep features, and to reduce the loss of original data details. The design of the independent dual-stream structure can extract multi-modal data features while avoiding the mutual interference between them and make full use of the image information of R, G, B, NIR, and nDSM bands [47]. Then, the IFPN structure is introduced in the decoding structure to fuse multi-scale information to account for the features of buildings of different scales. Finally, the sigmoid function is used to obtain the building segmentation map.\nSensors 2023, 23, x FOR PEER REVIEW 6 of 19\n3.2. DMU-Net Architecture\nWe design DMU-Net that can fuse multi-modal data and multi-scale features to im-\nprove the accuracy of building extraction. As shown in Figure 3, inspired by previous\nstudies [36,44\u201346], based on U-Net, we embed the two-stream CNN structure into the en-\ncoder of U-Net to obtain comprehensive features of different modal data. In the dual-\nstream structure, one stream is used to input the RGB image to get the feature of the two-\ndimensional spatial structure. The other stream is used to input the NIR + nDSM image,\nmainly used to obtain the feature of the three-dimensional spatial structure. In the decoder\nstructure, it is still the up-sampling and skip connections of U-Net. Up-sampling is used\nto restore the dimension of the feature map, and the skip connection fuses the down-sam-\npled feature map during t e -sa pling process t realize the fusion of shallow and\nde p features, and to r duce the loss of origin l d ta details. The d sign of the independ-\nent dual-stream structure can extract ulti-modal data f tures while av iding the mu-\ntual interference between them and make full use of the image information of R, G, B,\nNIR, and nDSM bands [47]. Then, the IFPN structure is introduced in the decoding str c-\nture to fus multi-scale information to account for the featu es of buildings of different\nscales. Fi ally, the sigmoid function is used to obtain the building segmen ation map.\n3.2.1. Fusion Strategy\nAs shown in Figure 4, the dual-stream architecture in the encoder has the same network structure and is independent of each other, and each stream performs four max-pooling down-sampling operations. Before each pooling, the features of the two streams are fused by the Add method and then fused with the corresponding up-sampled features in the decoder by the Concate process. To avoid a large number of parameters and memory consumption, the number of channels of the convolution kernel in the dual-stream structure\nSensors 2023, 23, 1991 7 of 19\nis set to 32, 64, 128, 256, and 512 in turns. The up-sampling stage in the encoder adopts a method of first performing linear interpolation up-sampling, and then performing convolution. This method is equivalent to the transposed convolution operation, which is more effective than the simple interpolation up-sampling method and can effectively eliminate the aliasing effect. To speed up the training of the network, a BN layer is added after each 3 \u00d7 3 convolution for data normalization [48]. Dropout layers with a probability of 0.5 are added after the 4th and 5th groups of convolutional layers to enhance the robustness of the network and avoid overfitting [49].\nSensors 2023, 23, x FOR PEER REVIEW 7 of 19 3.2.1. Fusion Strategy As shown in Figure 4, the dual-stream architecture in the encoder has the same net-\nwork structure and is independent of each other, and each stream performs four max-\npooling down-sampling operations. Before each pooling, the features of the two streams\nare fused by the Add method and then fused with the corresponding up-sampled features\nin the decoder by the Concate process. To avoid a large number of parameters and\nmemory consumption, the number of channels of the convolution kernel in the dual-\nstream structure is set to 32, 64, 128, 256, and 512 in turns. The up-sampling stage in the\nencoder adopts a method of first performing linear interpolation up-sampling, and then\nperforming convolution. This method is equivalent to the transposed convolution opera-\ntion, which is more effective than the simple interpolation up-sampling method and can\neffectively eliminate the aliasing effect. To speed up the training of the network, a BN\nlayer is added after each 3 \u00d7 3 convolution for data normalization [48]. Dropout layers\nwith probability f 0.5 are added fter the 4t a d 5th groups of convolution l layers to\nenhance the robustness of the network and avoid verfitting [49].\nFigure 4. Dual-stream fusion structure.\n3.2.2. Improved Feature Pyramid Network\nFPN was first proposed to solve the multi-scale problem in object detection. The high-\nlevel features of low-resolution, high-semantic information and low-level features of high-\nresolution, low-semantic information are fused by top-to-bottom side connections. There-\nfore, the features of FPN at all scales have rich semantic information, which helps to ex-\ntract objects at different scales. As shown in Figure 5(a), (b), U-Net and FPN have similar\nnetwork structures. Ji et al. proposed a method to introduce the FPN module into the U-\nNet network (Figure 5(c)), and inspired by this, the IFPN is designed [45]. Different from\nthe studies of Ji et al., in the decoder part, we first up-sampled the first three up-sampled\nfeature graphs by 8\u00d7, 4\u00d7, and 2\u00d7 linear interpolation to restore the original image size.\nThen 3 \u00d7 3 convolution is used to reduce the dimension of the feature map, as shown in\nFigure 5d. The 3 \u00d7 3 convolution has a larger receptive field than the 1 \u00d7 1 convolution,\nfurther enhancing the semantic features of buildings of different scales. Finally, the Add\nmethod is used to fuse the feature map.\n3.2.2. Improved Feature Pyramid Network\nFPN was first proposed to solve the multi-scale problem in object detection. The high-level features of low-resolution, high-semantic information and low-level features of high-resolution, low-semantic information are fused by top-to-bottom side connections. Therefore, the features of FPN at all scales have rich semantic information, which helps to extract objects at different scales. As shown in Figure 5a,b, U-Net and FPN have similar network structures. Ji et al. proposed a method to introduce the FPN module into the U-Net network (Figure 5c), and inspired by this, the IFPN is designed [45]. Different from the studies of Ji et al., in the decoder part, we first up-sampled the first three up-sampled feature graphs by 8\u00d7, 4\u00d7, and 2\u00d7 linear interpolation to restore the original image size. Then 3 \u00d7 3 convolution is used to reduce the dimension of the feature map, as shown in Figure 5d. The 3 \u00d7 3 convolution has a larger receptive field than the 1 \u00d7 1 convolution, further enhancing the semantic features of buildings of different scales. Finally, the Add method is used to fuse the feature map.\nSensors 2023, 23, 1991 8 of 19e sors , , x FOR PEER REVIEW 8 of 19\nPredict\nPredict\nPredict\nPredict\nPredict\n8\u00d7\n4\u00d7\n2\u00d7\n(a) (b)\n8\u00d7\n4\u00d7\n2\u00d7\n1\n1\n1\n1\nConv1\u00d71,sigmoid Upsampling\n8\u00d7\n4\u00d7\n2\u00d7\nConv3\u00d73,sigmoid\n2\n2\n2\n2\nConcate Add\nCopy\n(c) (d)\nFigure 5. (a) U-Net, (b) FPN, (c) The FPN integration strategy proposed by Ji and Wei [45]; (d) IFPN."
        },
        {
            "heading": "3.3. Post-Processing of Buildings",
            "text": ""
        },
        {
            "heading": "3.3.1. Digital Morphological Processing",
            "text": "The binary images of buildings predicted by CNN usually have noise and voids,\nwhich significantly interfere with the accuracy of building extraction and require a series\nof optimization processes. In this paper, the opening and closing operations in morphol-\nogy are used to optimize the building prediction map, in which erosion and dilation are\nthe basis of opening and closing operations [50]. The mathematical expressions for mor-\nphological erosion and dilation are:\n z | ( ) zA B AB =  (1)\n z | ( ) zA B A AB =   (2)\nThe corrosion of B to A is expressed as BA , the expansion of B to A is defined as BA , A is a collection of building pixels, B is the structuring element, and z is the pixel\nvalue of the building.\nThe mathematical expressions for opening and closing operations are:\n( )A B A B B=   (3)\n( )A B A B B\u2022 =   (4)\nThe open operation of B on A is expressed as BA  , The closed operation of B on A is defined as BA\u2022 ."
        },
        {
            "heading": "3.3.2. Boundary Regularization",
            "text": "Although the building boundary after morphological processing has been smoothed,\nit cannot reflect the regular boundary of the buildings well. To better fit the boundary of\nthe building, we adopt the polyline compression algorithm proposed by Gribov [51,52].\nFor building vector data, the algorithm\u2019s goal is to find a point within the tolerance of all\nnodes of the vector line segment, so that the sum of the penalties of the synthetic polyline\nconnected by all the points and the source polyline is the smallest. If there is a synthetic\nFigure 5. (a) U-Net, (b) FPN, (c) The FPN integration strategy proposed by Ji and Wei [45]; (d) IFPN.\n. ost- r cessi of ildings 3.3.1. igital orphological Processing\nThe binary i ages of buildings predicted by CNN usually have noise and voids, hich significantly interfere ith the accuracy of building extraction and require a series of optimization processes. In this paper, the opening and closing operations in morphology are used to optimize the building prediction map, in which erosion and dilation are the basis of opening and closing operations [50]. The mathematical expressions for morphological erosion and dilation are:\nA\u0398B = {z|(B)z \u2286 A} (1)\nA\u2295 B = {z|(B)z \u2229 A \u2286 A} (2)\nThe corrosion of B to A is expressed as A\u0398B, the expansion of B to A is defined as A\u2295 B, A is a collection of building pixels, B is the structuring element, and z is the pixel value of the building.\nThe mathematical expressions for opening and closing operations are:\nA \u25e6 B = (A\u0398B)\u2295 B (3)\nA\u2022B = (A\u2295 B)\u0398B (4)\nThe open operation of B on A is expressed as A \u25e6 B, The closed operation of B on A is defined as A\u2022B.\n3.3.2. Boundary Regularization\nAlthough the building boundary after morphological processing has been smoothed, it cannot reflect the regular boundary of the buildings well. To better fit the boundary of the building, we adopt the polyline compression algorithm proposed by Gribov [51,52]. For buildin vector data, the algorithm\u2019s goal is t find a oint within the tolerance of all nodes f the vector line segment, so that t s m of the penalties of the synthetic pol line connected by all the oints and th s urce polyline is the smallest. If there s a synthetic p lyline with the same penalty, the polyline wi h the smallest square d viation from the s urce polyline is selected.\nSensors 2023, 23, 1991 9 of 19"
        },
        {
            "heading": "4. Experiments and Analysis",
            "text": ""
        },
        {
            "heading": "4.1. Evaluation Metric",
            "text": "To quantitatively evaluate the prediction results of the model, three evaluation metrics are calculated based on the confusion matrix: overall accuracy (OA), intersection-overunion (IoU), and F1-score (F1). The OA was used to assess the global accuracy of the extraction results, IoU was used to measure the overlap between building prediction results and real labels, and F1 took into account both the precision and recall of the model. The expression is as follows:\nOA = TP + TN\nTP + TN + FP + FN (5)\nIoU = precision\u00d7 recall\nprecison + recall \u2212 precision\u00d7 recall (6)\nF1 = 2\u00d7 precision\u00d7 recall\nprecision + recall (7)\nprecision = TP\nTP + FP (8)\nrecall = TP\nTP + FN (9)\nwhere TP (true-positive) is the number of correctly identified building pixels, FP (falsepositive) is the number of missed building pixels, TN (true-negative) is the number of correctly classified non-building pixels, and FN (false-negative) is the number of nondetected non-building pixels."
        },
        {
            "heading": "4.2. Experimental Details",
            "text": "All experiments were performed on a desktop computer with 64-bit Windows 11. It is equipped with Intel (R) Core (TM) i5-11400 F CPU @ 2.60 GHz, a GPU of NVIDIA GeForce RTX 3060 with 12 GB RAM, and 16 GB memory (DDR4 3200 MHz). The methods in this paper are based on TensorFlow (version 2.5.0) and Keras (version 2.5.0), and the programming language is Python. The hyperparameters are set as follows: cross-entropy loss function and Adam optimization algorithm were used for 100 iterations of backward propagation, with four images in each batch, and the learning rate was 0.0001. In DMU-Net, RGB images are input in one stream, and NIR + nDSM images are input in the other stream. The changes in the accuracy and loss of the dataset with the number of training times are shown in Figure 6. Sensors 2023, 23, x FOR PEER REVIEW 10 of 19\nSensors 2023, 23, 1991 10 of 19"
        },
        {
            "heading": "4.3. Results",
            "text": "4.3.1. Comparative with SOTA Methods\nTo prove the excellent performance of DMU-Net, we use the GF-7 self-annotated building dataset, and select four excellent building extraction models for comparison, namely PSPNet, DeepLab v3+, EU-Net, and RU-Net [32,53\u201355]. Among them, the feature extraction network of PSPNet and DeepLab v3+ has the same structure as the single-stream CNN of SMU-Net. As summarized in Table 2, which evaluated all three metrics on the GF-7 self-annotated building dataset, our proposed DMU-Net outperforms PSPNet, DeepLab v3+, EU-Net, and RU-Net and achieves a considerably high IoU (84.49%). Figure 7 shows the visual performances of the comparison results. As demonstrated in the Figure 7, DMU-Net shows the advantages of extracting complexly connected buildings marked in green in row 5. Other models\u2019 building extraction results are missing or misclassifying non-building areas between buildings as buildings. For other green-marked buildings, DMU-Net can accurately extract buildings of different scales. At the same time, the results of DMU-Net are purer, while the results of EU-Net and RU-Net have white noise. Sensors 2023, 23, x FOR PEER REVIEW 11 of 19\n(a) (b) (c) (d) (e) (f) (g)\nFigure 7. Building extraction results of different models. (a) Image (b) Label (c) DMU-Net (d) PSPNet (e) DeepLab V3+ (f) EU-Net (g) RU-Net."
        },
        {
            "heading": "4.3.2. Results of Building Extraction in the Study Area",
            "text": "binary results of DMU-Net, remove isolated points, and fill in holes, a mathematical mor-\nphology method is employed based on 3 \u00d7 3 rectangular structure elements; first, two\nopen operations are performed, and then three closed operations are performed. Then\nfurther eliminate the small non-building noise to obtain the final building extraction re-\nsult, and vectorize it based on the ArcGIS platform. To correct the deformation of the\nbuilding\u2019s vector boundary and better fit the building\u2019s edge, the polyline compression\nalgorithm is used to regularize the building vector data.\nFigure 8(c) shows the vector result of buildings extracted by DMU-Net in the study\narea. Results showed that most of the buildings are correctly extracted. Due to the missing\nDSM data generated by GF-7, we have no extraction result of the building in the upper\nFigure 7. Building extraction results of different models. (a) Image (b) Label (c) DMU-Net (d) PSPNet (e) DeepLab V3+ (f) EU-Net (g) RU-Net.\nSensors 2023, 23, 1991 11 of 19\n4.3.2. Results of Building Extraction in the Study Area\nWe adopt DMU-Net to extract the buildings of the whole study area. To improve the binary results of DMU-Net, remove isolated points, and fill in holes, a mathematical morphology method is employed based on 3 \u00d7 3 rectangular structure elements; first, two open operations are performed, and then three closed operations are performed. Then further eliminate the small non-building noise to obtain the final building extraction result, and vectorize it based on the ArcGIS platform. To correct the deformation of the building\u2019s vector boundary and better fit the building\u2019s edge, the polyline compression algorithm is used to regularize the building vector data. Figure 8c shows the vector result of buildings extracted by DMU-Net in the study area. Results showed that most of the buildings are correctly extracted. Due to the missing DSM data generated by GF-7, we have no extraction result of the building in the upper left area. To further analyze the results, we selected a typical local region from Figure 8b; this region contains high-rise independent buildings, contiguous low-rise buildings, and other ground objects similar to buildings. From Figure 8d, DMU-Net can completely extract large buildings with regular and complete edges. However, due to the low spatial resolution of the GF-7 image, the dense low-rise buildings (marked by the blue boxes) can only be extracted contiguously, and individual buildings cannot be distinguished. Sensors 2023, 23, x FOR PEER REVIEW 12 of 19 left area. To further analyz the results, we selected a typical local region from Figure 8(b); this regi contains high-ris independent buildings, contiguous low-rise buildi gs, and other ground objects similar to buildings. From Figure 8(d), DMU-Net can completely extract large buildings with regular and complete edges. However, due to the low spatial resolution of the GF-7 image, the dense low-rise buildings (marked by the blue boxes) can only be extracted contiguously, an indiv dual buildings cannot be distinguished.\n(a) (b)\n(c) (d)\nFigure 8. Extraction results of buildings in the study area. (a) the study area, (b) a typical local re-\ngion, (c) building extraction result using DMU-Net, (d) local region result."
        },
        {
            "heading": "5. Discussion",
            "text": ""
        },
        {
            "heading": "5.1. Comparative Analysis",
            "text": ""
        },
        {
            "heading": "5.1.1. Validity of NIR and nDSM Data",
            "text": "To explore the impact of NIR and nDSM data on building extraction, we fixed one of\nthe streams to input RGB images, and the second stream input NIR, nDSM, and NIR +\nFigure 8. Cont.\nSensors 2023, 23, 1991 12 of 19\nSensors 2023, 23, x FOR PEER REVIEW 12 of 19 left area. To further analyze the results, we selected a typical local region from Figure 8(b); this region contains high-rise independent buildings, contiguous low-rise buildings, and other ground objects similar to buildings. From Figure 8(d), DMU-Net can completely extract large buildings with regular and complete edges. However, due to the low spatial resolution of the GF-7 image, the dense low-rise buildings (marked by the blue boxes) can only be extracted contiguously, and individual buildings cannot be distinguished.\n(a) (b)\n(c) (d)\nFigure 8. Extraction results of buildings in the study area. (a) the study area, (b) a typical local re-\ngion, (c) building extraction result using DMU-Net, (d) local region result."
        },
        {
            "heading": "5. Discussion",
            "text": ""
        },
        {
            "heading": "5.1. Comparative Analysis",
            "text": ""
        },
        {
            "heading": "5.1.1. Validity of NIR and nDSM Data",
            "text": "To explore the impact of NIR and nDSM data on building extraction, we fixed one of\nthe streams to input RGB images, and the second stream input NIR, nDSM, and NIR +\nFigure 8. Extraction results of buildings in the study area. (a) the study area, (b) a typical local region, (c) building extraction result using DMU-Net, (d) local region result."
        },
        {
            "heading": "5.1. o parative nalysis",
            "text": "5.1.1. Validity of NIR and nDSM Data\nTo explore the impact of NIR and nDSM data on building extraction, we fixed one of the streams to input RGB images, and the second stream input NIR, nDSM, and NIR + nDSM images, respectively. Based on the different inputs of the second stream, four different models, M1, M2, M3, and M4, are designed. Figure 9 shows the building extraction results of different models. Among them, M1 and M3 introduce nDSM images, which can effectively distinguish similar objects (such as playgrounds, squares, etc.), improve the integrity of large buildings extraction and avoid the adhesion of adjacent buildings. To further evaluate the effectiveness of our method, we quantitatively analyze the building extraction results of different models. As shown in Table 3, for the GF-7 self-annotated building dataset, M1 has the highest building extraction accuracy. Compared with M4, the IoU of M1, M2, and M3 have increased by 8.31%, 5.12%, and 7.61%, respectively, indicating that NIR and nDSM data help improve the accuracy of building extraction. At the same time, compared with M2, the IoU of M3 has increased by 2.49%; it shows that nDSM data contribute more to improving the accuracy of building extraction.\nSensors 2023, 23, 1991 13 of 19\nSensors 2023, 23, x FOR PEER REVIEW 13 of 19 nDSM images, respectively. Based on the different inputs of the second stream, four different models, M1, M2, M3, and M4, are designed. Figure 9 shows the building extraction results of different models. Among them, M1 and M3 introduce nDSM images, which can effectively distinguish similar objects (such as playgrounds, squares, etc.), improve the integrity of large buildings extraction and avoid the adhesion of adjacent buildings. To\nfurther evaluate the effectiveness of our method, we quantitatively analyze the building\nextraction results of different models. As shown in Table 3, for the GF-7 self-annotated\nbuilding dataset, M1 has the highest building extraction accuracy. Compared with M4,\nthe IoU of M1, M2, and M3 have increased by 8.31%, 5.12%, and 7.61%, respectively, indi-\ncating that NIR and nDSM data help improve the accuracy of building extraction. At the\nsame time, compared with M2, the IoU of M3 has increased by 2.49%; it shows that nDSM\ndata contribute more to improving the accuracy of building extraction.\nFigure 9. Building extraction results of models with different inputs. (a) Image (b) Label (c) M1: NIR + nDSM + RGB (d) M2: NIR + RGB (e) M3: nDSM + RGB (f) M4: RGB.\n5.1.2. Comparison of Different Network Structures\nTo verify the contribution of the dual-stream CNN structure and the IFPN structure to DMU-Net, the dual-stream with FPN model (DMU-Net (FPN)), the single-stream with IFPN model (SMU-Net), the dual-stream without IFPN model (DU-Net) and the singlestream without IFPN model (IEU-Net) were constructed for comparison (Table 4). For a fair comparison, the input data of all models contain RGB + NIR + nDSM images. Among them, one stream of DMU-Net, DMU-Net (FPN) and DU-Net input RGB images, and the other stream input NIR + nDSM images. SMU-Net and IEU-Net directly input RGB + NIR + nDSM images. As shown in Figure 10, DMU-Net has apparent advantages in the accurate extraction of adjacent buildings and the completeness of extensive buildings extraction. According to Table 5, using the dual-stream structure and the IFPN structure can effectively improve the accuracy of building extraction. DMU-Net compared with SMU-Net, DU-Net, and IEU-Net, the IoU of buildings increased by 0.74%, 0.55%, and 1.65%, respectively. In addition, DMU-Net compared with DMU-Net (FPN), the IoU of buildings increased by\nSensors 2023, 23, 1991 14 of 19\n0.22%, and the params and floating-point operations (FLOPs) of the model changed little, showing the advantages of IFPN. Compared with the single-stream CNN structure, the Trainable params and FLOPs of the two-stream CNN structure increase by about 1.6 times. IFPN structure had little effect on the model; the Trainable params and FLOPs did not increase by more than 0.02 M.\nSensors 2023, 23, 1991 15 of 19\n5.1.3. Different Fusion Methods\nMultimodal data fusion methods are divided into data-level fusion, feature-level fusion, and decision-level fusion. We used feature-level fusion. The building extraction accuracy of different fusion methods is shown in Table 6, the fusion method in this paper is the best in three indicators: OA, IoU, and F1. The IoU increased by 0.58% and 2.08%, respectively, compared with data-level fusion and decision-level fusion.\n5.1.4. Advantages of Regularization\nTo confirm that the regularization method adopted in this paper can effectively optimize the results of building extraction, we refer to the PoLiS metric proposed by Avbelj et al. [56]. We evaluate the similarity of all building vector predictions to the ground truth building vectors by computing the overall mean of PoLiS [57]. Smaller values indicate a higher similarity between the predicted building vectors and the actual building vectors. According to Table 7, although the three indicators of OA, IoU, and F1 of the building extraction results after regularization processing are slightly reduced, PoLiS is halved, indicating that the vector boundaries of buildings after regularization processing are more similar to actual buildings. Figure 11 shows that after morphological and regularization treatment, the edge of the building is more consistent with the natural shape of the building, and the holes in the building are eliminated.\nSensors 2023, 23, 1991 16 of 19\nSensors 2023, 23, x FOR PEER REVIEW 16 of 19\nhigher similarity between the predicted building vectors and the actual building vectors.\nAccording to Table 7, although the three indicators of OA, IoU, and F1 of the building\nextraction results after regularization processing are slightly reduced, PoLiS is halved, in-\ndicating that the vector boundaries of buildings after regularization processing are more\nsimilar to actual buildings. Figure 11 shows that after morphological and regularization\ntreatment, the edge of the building is more consistent with the natural shape of the build-\ning, and the holes in the building are eliminated.\nFigure 11. The vectorization result of building extraction. The first row is the vectorized result of the original prediction of the buildings; The second row is the results of the building after morphological processing and regularization of the original building prediction results.\nTable 7. Accuracy of building prediction vector results.\nMethod OA(%) IoU(%) F1(%) PoLiS\nthe results of original prediction 96.16 84.49 91.59 16.53\nthe results after morphological processing\nand regularization 95.95 83.73 91.15 8.48"
        },
        {
            "heading": "5.2. Limitations and Future Works",
            "text": "Although the nDSM data constructed by GF-7 stereo pairs can significantly improve\nthe accuracy of building extraction, compared with LiDAR data, the quality of nDSM data\nproduced based on multi-view satellite images has shortcomings, such as insufficient pre-\ncision of nDSM, and lack of height information of some buildings due to occlusion, which\naffects the performance of multi-view satellite data in building extraction. However, the\nmain advantage of multi-view satellite imagery is that it can quickly obtain the height\ninformation of ground objects in a large area, and has strong timeliness and economic\napplicability. In the future, improving the quality of nDSM data generated from multi-\nview satellite data such as GF-7 is a promising exploration.\nThe DMU-Net performs the best effect on the GF-7 self-annotated building dataset.\nHowever, the two-stream structure has greater computational and memory overhead\nthan the single-stream, which limits the applicability on different hardware. In the future,\nhow to reduce the computational cost of the dual-stream CNN is a bottleneck. It must be\novercome to ensure its wide application. More efficient multi-modal data fusion networks\nneed to be proposed. In addition, the regularized post-processing method used in this\npaper needs to set complex calculation rules, which undoubtedly added to the task\u2019s\nFigure 11. The vectorization result of building extraction. The first row is the vectorized result of the original prediction of the buildings; The second row is the results of the building after morphological processing and regularization of the original building prediction results."
        },
        {
            "heading": "5.2. Limitations and Future Works",
            "text": "Although the nDSM data constructed by GF-7 stereo pairs can significantly improve the accuracy of building extraction, compared with LiDAR data, the quality of nDSM data produced based on multi-view satellite images has shortcomings, such as insufficient precision of nDSM, and lack of height information of some buildings due to occlusion, which affects the performance of multi-view satellite data in building extraction. However, the main advantage of multi-view satellite imagery is that it can quickly obtain the height information of ground objects in a large area, and has strong timeliness and economic applicability. In the future, improving the quality of nDSM data generated from multi-view satellite data such as GF-7 is a promising exploration. The DMU-Net performs the best effect on the GF-7 self-annotated building dataset. However, the two-stream structure has greater computational and memory overhead than the single-stream, which limits the applicability on different hardware. In the future, how to reduce the computational cost of the dual-stream CNN is a bottleneck. It must be overcome to ensure its wide application. More efficient multi-modal data fusion etworks need to be proposed. In addition, the regularized post-processing method used i this paper needs to set complex calculation rules, which undoubtedly added to the task\u2019s workload. It is an exciting direction o integrate the r gularized method into the end- o-end segmentation m d l in the future."
        },
        {
            "heading": "6. Conclusions",
            "text": "In this paper, we propose a dual-stream multi-scale U-Net network named DMU-Net, to au matically an accurately extract urb n buildings from GF-7 stereo images. For DMU-N t, the dual-stream CNN arch tecture is des gned in the encoder to learn the multidimensional features of different modal data. The decoder introduces the IFPN structure to fuse features of different scales. Compared with four SOTA models, our model achieves the best results on the GF-7 self-annotated building dataset. In addition, the nDSM data constructed from GF-7 stereo images can help improve the accuracy of building extraction. In particular, it can help distinguish buildings from similar ground objects and improve the integrity of large buildings. In the future, we will develop more effective multimodal fusion models and regularization methods.\nSensors 2023, 23, 1991 17 of 19\nAuthor Contributions: Conceptualization, Z.S.; methodology, P.L. and Z.S.; validation, Y.S. and P.L.; writing\u2014original draft preparation, P.L.; writing\u2014review and editing, Z.S., G.D., D.W. and Q.M.; funding acquisition, Z.S. and G.D. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by the Tianjin Municipal Education Commission Scientific Research Program (grant number 2021SK003); the Tianjin Educational Science Planning Project (grant number EHE210290); the Tianjin outstanding science and Technology Commissioner project (grant number 22YDTPJC00230); and the National Natural Science Foundation of China (grant number 41971310).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data are not publicly available due to [privacy restrictions].\nAcknowledgments: We appreciate the constructive comments and suggestions from the reviewers that helped improve the quality of this manuscript.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "DMU-Net: A Dual-Stream Multi-Scale U-Net Network Using Multi-Dimensional Spatial Information for Urban Building Extraction",
    "year": 2023
}