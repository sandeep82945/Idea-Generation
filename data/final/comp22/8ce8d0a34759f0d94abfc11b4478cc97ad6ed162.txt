This work presents ‘BanglaNLG,’ a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language. We aggregate six challenging conditional text generation tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue generation in the process. Furthermore, using a clean corpus of 27.5 GB of Bangla data, we pretrain ‘BanglaT5’, a sequenceto-sequence Transformer language model for Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks, outperforming several multilingual models by up to 9% absolute gain and 32% relative gain. We are making the new dialogue dataset and the BanglaT5 model publicly available at https://github. com/csebuetnlp/BanglaNLG in the hope of advancing future research on Bangla NLG. 1 introduction The emergence of pretrained language models (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) has brought about a revolutionary change in natural language processing (NLP). With little taskspecific fine-tuning, these models have achieved state-of-the-art results on many NLP tasks (Wang et al., 2018; Rajpurkar et al., 2016; Tjong Kim Sang and De Meulder, 2003). However, the focus of these models has predominantly been on natural language understanding (NLU). Even models pretrained with generative objectives (Raffel et al., 2020) concern themselves with NLU tasks more than natural language generation (NLG) tasks. Although there have been recent efforts to uplift NLG (Gehrmann et al., 2021), they are primarily geared towards high- and mid-resource languages. For example, despite being the sixth most spoken language in the world with over 230 million native speakers comprising 3% of the world’s total population,1 Bangla has remained an underrepresented
1https://w.wiki/Psq
language in the NLP literature (Joshi et al., 2020). There have been only a handful of benchmark studies on Bangla NLG (Dabre et al., 2022; Kumar et al., 2022), and that too without Bangla being the main focus. This can be attributed to the lack of diverse NLG tasks under a single benchmark and strong pretrained Bangla NLG models. To this end, we present ‘BanglaNLG,’ a comprehensive benchmark for Bangla language generation comprising six representative tasks on machine translation, text summarization, question answering, dialogue generation, headline generation, and cross-lingual summarization. To our knowledge, BanglaNLG is the first NLG benchmark exclusively for a low-resource language. To establish a strong baseline for this benchmark, we pretrain BanglaT5 – a sequence-to-sequence Transformer model (Vaswani et al., 2017) pretrained on a 27.5 GB clean Bangla text corpus covering a broad range of domains. In summary:
• We develop the BanglaNLG benchmark bringing together six NLG tasks. • We introduce a Multi-turn Dialogue dataset. • We pretrain BanglaT5 and evaluate it on the
six NLG tasks, showing strong results. BanglaT5 outperforms similar-sized multilingual models, achieving new state-of-the-art results on three tasks with a 4% gain on average. We are releasing the BanglaT5 model and a live leaderboard to promote future research on Bangla NLG. 2 the bangla natural language generation (banglanlg) benchmark There have been sporadic works on Bangla NLG, mostly catered to machine translation (Hasan et al., 2020; Mumin et al., 2019a,b) and text summarization (Bhattacharjee et al., 2021b; Dhar et al., 2021). However, Bangla NLG lacks a unified study comprising diverse and challenging tasks. Motivated by the popular benchmarks like GLUE (Wang
726
et al., 2018), XTREME (Hu et al., 2020), GEM (Gehrmann et al., 2021), that have facilitated the training/evaluation of NLP models, we establish the first-ever Bangla Natural Language Generation (BanglaNLG) Benchmark. 2.1 task selection criteria We consider the following factors while choosing the evaluation tasks:
1. Diversity: The tasks should focus on evaluating the model’s generalization capabilities. Therefore, they should vary in task nature – the input and output length, the type of generated text, the target domain, and the dataset size. 2. Practical Applicability: The choice of tasks should be driven by their practical implications. Rather than being used in abstract situations, NLG models trained on these tasks should be able to aid/reduce human effort in real-world scenarios. 3. Difficulty: The tasks should be challenging while not being unsolvable. There should be clear room for improvement to foster future research. 4. Accessibility: The selected datasets for these tasks should be openly accessible to encourage researchers to design better NLG models. 5. Evaluation: The selected tasks should have reliable automated metrics for evaluating the focused abilities of an NLG model. 2.2 selected tasks Considering the criteria mentioned above, we design BanglaNLG as an aggregation of six tasks:
1. Machine Translation (MT): MT is perhaps the most studied NLG task in Bangla and the most commonly benchmarked NLG task in general. We use the BanglaNMT parallel corpus (Hasan et al., 2020), the largest Bangla-English MT dataset curated, with 2.75 million parallel pairs for training. The sentence pairs originate from various domains such as Wikipedia, news articles, religious and law
documents, etc. We evaluate the NLG models using FLoRes-100 (Goyal et al., 2022) in both directions on this dataset, i.e., Bangla to English and English to Bangla. This task is particularly challenging since it assesses an NLG model’s bilingual generation capabilities. Following standard practice, we use detokenized SacreBLEU (Post, 2018) as the evaluation metric for this task. 2. Text Summarization (TS): This task aims to generate a short and fluent summary given a long text document. We chose the Bangla portion of XLSum (Hasan et al., 2021) for this task. XL-Sum is a large comprehensive dataset for abstractive TS where the article and summaries are written by professional editors of BBC News. The articles cover various topics such as entertainment, politics, science, sports, etc. For this task, we use ROUGE22 (Lin, 2004) as the evaluation metric. 3. Question Answering (QA): This is a fundamental NLP task that can be modeled as both an NLU and NLG task. We use the BQA (Bhattacharjee et al., 2022) dataset for this task. The training data is machine translated from SQuAD 2.0 (Rajpurkar et al., 2018), while the evaluation data come from the human-annotated question-answer pairs of the TyDi-QA (Clark et al., 2020) secondary gold passage task. Although TyDi-QA only contains answerable questions, BQA introduced unanswerable questions to make the task more challenging. Following SQuAD 2.0, we use Exact Match (EM) and F1 as the evaluation metrics. 4. Multi-turn Dialogue (MTD): Conversational AI is a crucial task for NLG (Chen et al., 2017). However, there is no public dataset for dialogue generation in Bangla. As such, we curate a new multi-turn dialogue dataset by translating the DailyDialog (Li et al., 2017) dataset using the English to Bangla translation model introduced by Hasan
2We use Bangla stemming supported ROUGE implementation from https://github.com/csebuetnlp/xl-sum/ tree/master/multilingual_rouge_scoring. et al. (2020). Unlike standard QA-style conversation datasets, DailyDialog reflects real-life conversations in various social situations rich in emotion, making it a perfect candidate for our benchmark. We automatically translate the training data following the same procedure described in Bhattacharjee et al. (2022) and have the evaluation sets manually translated by expert human translators. We use BLEU-1 as the evaluation metric for this task to properly differentiate between models since averaged BLEU scores of up to 4-gram tend to be quite low in dialogue evaluation (Zhang et al., 2020). 5. News Headline Generation (NHG): Automating headline generation can help news editors write compelling headlines to draw readers’ attention. We consider NHG as a complementary task to TS. Given an article, the objective is to generate an appropriate headline that accurately depicts the article. We repurpose the XL-Sum (Hasan et al., 2021) dataset for this task since it also includes the titles of the articles. Like TS, we use ROUGE-2 as the evaluation metric. 6. cross-lingual summarization (xls): as another task for evaluating models’ bilingual generation capabilities, we consider XLS. In this task, given a piece of text in a source language, we have to generate the corresponding summary in a target language. This is potentially harder than both MT and TS considering it combines both in a single task. We consider the English-Bengali portion of the CrossSum (Bhattacharjee et al., 2021a) dataset for this task. It is curated by aligning identical articles written in different languages from the XLSum dataset. For evaluation, we use ROUGE-2. We present detailed statistics of the BanglaNLG benchmark in Table 1. 3 banglat5 We introduce BanglaT5, a sequence-to-sequence Transformer model (Vaswani et al., 2017), to establish a strong baseline for BanglaNLG benchmark. In this section, we describe the pretraining data, objectives, and model architecture of BanglaT5. 3.1 pretraining data We chose Bangla2B+ (Bhattacharjee et al., 2022) as the pretraining corpus for BanglaT5. This is a 27.5 GB dataset containing 5.25 million documents collected from a meticulously selected list of web sources. While larger sources like CCNet (Wenzek et al., 2020) and mC4 (Xue et al., 2021) are
available, these contain a lot of noise and offensive texts that are difficult to remove. For a generative model, even small amounts of unwanted texts in pretraining could lead to potentially dangerous biases in generated text (Luccioni and Viviano, 2021). Therefore, we decided not to use them. 3.2 data pre-processing Following Hasan et al. (2020), we preprocessed the texts using their normalization pipeline3. We trained a SentencePiece (Kudo and Richardson, 2018) vocabulary of 32k subword tokens on the normalized corpus with a character coverage of 0.99995. While creating a training sample, we limited the maximum sequence length to 512 tokens for both input and output and discarded documents with a token count below 7. After tokenization, we had 4.8 million data points with an average sequence length of 402.32 tokens. 3.3 pretraining objective For generative language modeling, two standard choices are decoder-only models (Mikolov et al., 2010) and encoder-decoder models (Sutskever et al., 2014). Radford et al. (2019) trained a decoder-only Transformer (Vaswani et al., 2017) pretrained on the conditional continuation objective. However, to provide more flexibility on generation and possible usage on understanding tasks, we only consider encoder-decoder models following the original design of the Transformer. They are generally trained with different denoising objectives to increase the encoder’s and decoder’s capacity. For instance, BART (Lewis et al., 2020b), and mBART (Liu et al., 2020) use a text-infillingbased objective. In contrast, MARGE (Lewis et al., 2020a) is a multilingual encoder-decoder model trained to reconstruct a document in one language by retrieving documents in other languages. Following Raffel et al. (2020), we pretrained BanglaT5 using a "span-correction" objective, empirically shown to be an optimal choice for encoder-decoder models. In this objective, consecutive spans of input tokens are replaced with a mask token, and the model is trained to reconstruct them. 3.4 model architecture & hyperparameters We pretrained the base variant of the T5 model: 12 layers, 12 attention heads, 768 hidden size, 2048 feed-forward size with GeGLU activation (Shazeer,
3https://github.com/csebuetnlp/normalizer
2020) with a batch size of 65536 tokens for 3 million steps on a v3-8 TPU instance on GCP. We used the Adam (Kingma and Ba, 2015) optimizer with a 3e-4 learning rate, linear warmup of 10k steps, and ‘inverse square root’ learning rate decay. 4 experiments & results We compared BanglaT5 it with four multilingual models: mT5 (base) (Xue et al., 2021), mBART50 (Tang et al., 2020), XLM-ProphetNet (Qi et al., 2021), and IndicBART (both unified and separate script variants) (Dabre et al., 2022).4 All pretrained models were fine-tuned for 3-15 epochs with batch size 32 (128 for MT). We used linear warmup with a ratio of 0.1, label smoothing of 0.1 (Szegedy et al., 2016), and weight decay of 1e-6 with the Adam optimizer (Kingma and Ba, 2015). The learning rate was tuned from the set {5e-5, 1e-4, 5e-4}. The best model was evaluated based on the validation performance after each epoch. During inference, we used beam-search (HayesRoth et al., 1976) with beam size 5 (on all tasks except QA), removed duplicated trigrams during beam search (Fan et al., 2018), and used a length penalty (Wu et al., 2016) of 0.6. For QA, we used greedy decoding, i.e., picking the most probable token during each decoding step. The evaluation results are presented in Table 2. In all the tasks, BanglaT5 outperformed all multilingual models by a considerable margin, on average 4% over the second-best, mT5. In all monolingual tasks except MTD, BanglaT5 achieves a big performance gain over others (up to 9.54% in QA), which can be attributed to the quality of the pretraining data. In MD, BanglaT5 lags marginally behind XLM-ProphetNet. We hypothesize this is due to the lack of colloquial data in Bangla2B+ since Bhattacharjee et al. (2022) left out such sources to avoid
4Due to computational budget limitations, we do not benchmark on billion-parameter models like large mT5 variants. toxic and biased conversations. We find the MT results particularly interesting, where BanglaT5 outperforms larger multilingual models in both directions. This suggests that despite having very little English data in the pretraining corpus, BanglaT5 can generalize well to a new translation language, given high-quality fine-tuning data. We explore this more in the Appendix. Conspicuously, all the models achieve relatively poor scores on the XLS task. This can be attributed to the smaller amount of training data. BanglaT5 proves its superiority in compute and memory efficiency along with its performance due to its smaller size (less than half the parameters of all multilingual models except IndicBART). In practice, we observe 2-2.5x faster training and inference times with BanglaT5 than these larger multilingual models. 5 related works Pretrained models NLP has witnessed a sea of change with the advent of pretrained language models like ULMfit (Howard and Ruder, 2018), ELMo (Peters et al., 2018), and most notably BERT (Devlin et al., 2019), achieving state-of-the-art results in many NLU benchmarks. Besides these NLU models, more and more pretrained models designed for NLG tasks have been proposed. Rothe et al. (2020) adopted pretrained NLU model checkpoints for generative tasks. GPT-2 (Radford et al., 2019), and later GPT-3 (Brown et al., 2020) showed that pretrained generative language models can perform remarkably well in zero-shot transfer tasks. More recently, Qi et al. (2020) proposed ProphetNet, which introduces the future n-gram prediction mechanism for language generation. Dabre et al. (2022) introduced IndicBART, which is pretrained on 11 Indic languages, including Bangla. NLG Benchmarks Recently, many multi-task benchmarks have been proposed to drive the
progress of NLG models. Moussallem et al. (2020) proposed the BENG benchmark for NLG and knowledge extraction. GLGE (Liu et al., 2021) is a similar benchmark with a different set of tasks and difficulty levels. However, these benchmarks are limited to English only. Gehrmann et al. (2021) introduced the GEM benchmark for various tasks such as summarization (Narayan et al., 2018), datato-text generation (Nan et al., 2021) across different languages. Cahyawijaya et al. (2021) introduced different tasks and baselines for 3 Indonesian languages. More recently, Kumar et al. (2022) introduced IndicNLG, a benchmark with five tasks in 11 Indic languages, including Bangla. 6 conclusion & future works NLP research in low-resource languages is lagging behind due to the lack of reliable benchmarks and datasets. To facilitate the development, evaluation, and comparison of new NLG models, we introduced a multi-task evaluation benchmark for Bangla NLG, a widely spoken yet low-resource language. We presented BanglaT5, a pretrained NLG model in Bangla, setting new state-of-the-art results with BanglaT5. We strongly believe that our contributions in this work will help the Bangla NLP community benchmark NLG tasks more easily under a unified setup. Limitations
Although Bhattacharjee et al. (2022) claimed that Bangla2B+, the pretraining corpus for BanglaT5, had been carefully filtered for offensive or unwanted texts, they alerted that there might be small amounts of these contents may be present, which can result in bias or toxicity in the pretrained model. We, therefore, recommend using BanglaT5 with caution, especially for real-world deployment. Ethics Statement
License The TyDiQA dataset (Clark et al., 2020) is released under the Apache License 2.0, allowing modifications and distribution. All other pretraining and fine-tuning datasets are released under the
Creative Commons Attribution-NonCommercialShareAlike 4.0 International License (CC BY-NCSA 4.0), which allows modifications and distributions for non-commercial research purposes. We strictly adhere to these licenses and will release BanglaT5 and BanglaNLG benchmark resources under CC BY-NC-SA 4.0. Annotation Expert translators who provide translation services for renowned Bangla newspapers were hired to translate the evaluation sets of the dialogue dataset. Each translated sentence was further assessed for quality by another expert. It was again translated by the original translator if found to be of low quality. If the re-translation was found to be of low quality, it was then translated by the other expert. The experts were paid hourly as per standard rates in local currency. Hallucinated Text It is well-known that text generation models can hallucinate outputs that may not necessarily be faithful to the original input (Maynez et al., 2020). Though the texts may be fluent and human-like, the hallucinations may be factually inconsistent and impact the outputs negatively. BanglaT5 may be susceptible to the same kinds of hallucinations. Carbon Footprint We avoided using large models for pretraining and fine-tuning, reducing their environmental impacts. BanglaT5 was trained for about 30 days on Google v3 TPUs. Google’s TPUs are specifically designed for machine learning, which makes them up to five times more efficient than GPUs. Assuming 0.080kg carbon emission per kWh,5 the pretraining would emit fewer than 100kg carbon into the environment, far below most computationally demanding models. All fine-tuning experiments were done on a desktop machine with an 8-core Intel Core-i7 11700k CPU and NVIDIA RTX 3090 GPU, and no single run except machine translation took more than 12 hours, which amounts to fewer than 0.5kg carbon emission. On average, machine translation runs took three days each, emitting less than 3kg of carbon. acknowledgements We would like to thank the Research and Innovation Centre for Science and Engineering (RISE), BUET, for funding the project and Google TPU Research Cloud (TRC) program for providing cloud support. 5https://blog.google/technology/ai/ minimizing-carbon-footprint/ a multi-turn dialogue scores In Table 3, we mention BLEU-1, BLEU-2, BLEU3, and BLEU-4 scores for different models in the multi-turn dialogue generation task. b cross-lingual capabilities of banglat5 Despite being a monolingual model pretrained on heavily filtered Bangla data, BanglaT5 exhibits strong cross-lingual abilities, particularly in the machine translation (MT) task. In addition to the quality and size of the fine-tuning dataset, this performance can also be attributed to the presence of a significant amount of non-Bangla tokens (∼10.3%) in the BanglaT5 vocabulary. Since Bhattacharjee et al. (2022) curated the Bangla2B+ corpus by document-level language filtering, these documents preserve foreign text sequences occurring in the Bangla documents. We deliberately maintain these tokens while training the vocabulary of BanglaT5, using a relatively high character coverage. Our rationale behind doing this was to capture code-switching and allow better generalization across languages co-occurring with Bangla, as well as romanized forms of Bangla texts during fine-tuning, which is reflected in the MT results. However, it should be noted that the quality and size of fine-tuning data are essential for a strong cross-lingual performance since the mere existence of foreign tokens in the vocabulary is not enough to produce meaningful generation performance, as demonstrated by the poor performance in the cross-lingual summarization (XLS) task. This phenomenon has been studied in-depth by Blevins and Zettlemoyer (2022) in the context of pretrained language models in English, where they showed that these models develop strong
cross-lingual transfer capabilities due to the nonnegligible amount of foreign text present in the pretraining data and robustness to UNK tokens during fine-tuning.