{
    "abstractText": "Vertical federated learning (VFL) enables the collaborative training of machine learning (ML) models in settings where the data is distributed amongst multiple parties who wish to protect the privacy of their individual data. Notably, in VFL, the labels are available to a single party and the complete feature set is formed only when data from all parties is combined. Recently, Xu et al. [1] proposed a new framework called FedV for secure gradient computation for VFL using multiinput functional encryption. In this work, we explain how some of the information leakage in Xu et al. can be avoided by using Quadratic functional encryption when training generalized linear models for vertical federated learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuangyi Chen"
        },
        {
            "affiliations": [],
            "name": "Anuja Modi"
        },
        {
            "affiliations": [],
            "name": "Shweta Agrawal"
        },
        {
            "affiliations": [],
            "name": "Ashish Khisti"
        }
    ],
    "id": "SP:60f44100aab17a28615ab7c240ade6e101dcd56c",
    "references": [
        {
            "authors": [
                "R. Xu",
                "N. Baracaldo",
                "Y. Zhou",
                "A. Anwar",
                "J. Joshi",
                "H. Ludwig"
            ],
            "title": "Fedv: Privacy-preserving federated learning over vertically partitioned data",
            "venue": "03 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Paillier"
            ],
            "title": "Public-key cryptosystems based on composite degree residuosity classes",
            "venue": "Advances in Cryptology\u2014EUROCRYPT\u201999: International Conference on the Theory and Application of Cryptographic Techniques Prague, Czech Republic, May 2\u20136, 1999 Proceedings 18. Springer, 1999, pp. 223\u2013238.",
            "year": 1999
        },
        {
            "authors": [
                "A. Acar",
                "H. Aksu",
                "A.S. Uluagac",
                "M. Conti"
            ],
            "title": "A survey on homomorphic encryption schemes: Theory and implementation",
            "venue": "ACM Computing Surveys (Csur), vol. 51, no. 4, pp. 1\u201335, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Mohamad",
                "M. \u00d6nen",
                "W. Ben Jaballah",
                "M. Conti"
            ],
            "title": "Sok: Secure aggregation based on cryptographic schemes for federated learning",
            "venue": "PETS 2023, 23rd Privacy Enhancing Technologies Symposium, 10-14 July 2023, Lausanne, Switzerland (Hybrid Conference), Lausanne, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Hardy",
                "W. Henecka",
                "H. Ivey-Law",
                "R. Nock",
                "G. Patrini",
                "G. Smith",
                "B. Thorne"
            ],
            "title": "Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption",
            "venue": "11 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Yang",
                "T. Fan",
                "T. Chen",
                "Y. Shi",
                "Q. Yang"
            ],
            "title": "A quasi-newton method based vertical federated learning framework for logistic regression",
            "venue": "ArXiv, vol. abs/1912.00513, 2019.",
            "year": 1912
        },
        {
            "authors": [
                "S. Yang",
                "B. Ren",
                "X. Zhou",
                "L. Liu"
            ],
            "title": "Parallel distributed logistic regression for vertical federated learning without third-party coordinator",
            "venue": "arXiv preprint arXiv:1911.09824, 2019.",
            "year": 1911
        },
        {
            "authors": [
                "H. Sun",
                "Z. Wang",
                "Y. Huang",
                "J. Ye"
            ],
            "title": "Privacy-preserving vertical federated logistic regression without trusted third-party coordinator",
            "venue": "2022 The 6th International Conference on Machine Learning and Soft Computing, ser. ICMLSC 2022. New York, NY, USA: Association for Computing Machinery, 2022, p. 132\u2013138. [Online]. Available: https://doi.org/10.1145/3523150.3523171",
            "year": 2022
        },
        {
            "authors": [
                "D. He",
                "R. Du",
                "S. Zhu",
                "M. Zhang",
                "K. Liang",
                "S. Chan"
            ],
            "title": "Secure logistic regression for vertical federated learning",
            "venue": "IEEE Internet Computing, vol. 26, no. 2, pp. 61\u201368, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Zhao",
                "M. Yao",
                "W. Wang",
                "H. He",
                "X. Jin"
            ],
            "title": "Ntp-vfl - a new scheme for non-3rd party vertical federated learning",
            "venue": "2022 14th International Conference on Machine Learning and Computing (ICMLC), ser. ICMLC 2022. New York, NY, USA: Association for Computing Machinery, 2022, p. 134\u2013139. [Online]. Available: https://doi.org/10.1145/3529836.3529841",
            "year": 2022
        },
        {
            "authors": [
                "X. Yu",
                "W. Zhao",
                "D. Tang",
                "K. Liang",
                "J. Du"
            ],
            "title": "Privacy-preserving vertical collaborative logistic regression without trusted third-party coordinator",
            "venue": "Sec. and Commun. Netw., vol. 2022, jan 2022. [Online]. Available: https://doi.org/10.1155/2022/5094830",
            "year": 2022
        },
        {
            "authors": [
                "Q. Li",
                "Z. Huang",
                "W.-j. Lu",
                "C. Hong",
                "H. Qu",
                "H. He",
                "W. Zhang"
            ],
            "title": "Homopai: A secure collaborative machine learning platform based on homomorphic encryption",
            "venue": "2020 IEEE 36th International Conference on Data Engineering (ICDE), 2020, pp. 1713\u20131717.",
            "year": 2020
        },
        {
            "authors": [
                "S. Goldwasser",
                "S.D. Gordon",
                "V. Goyal",
                "A. Jain",
                "J. Katz",
                "F.-H. Liu",
                "A. Sahai",
                "E. Shi",
                "H.-S. Zhou"
            ],
            "title": "Multi-input functional encryption",
            "venue": "Annual International Conference on the Theory and Applications of Cryptographic Techniques. Springer, 2014, pp. 578\u2013602.",
            "year": 2014
        },
        {
            "authors": [
                "A. Sahai",
                "B. Waters"
            ],
            "title": "Fuzzy identity-based encryption",
            "venue": "Advances in Cryptology \u2013 EUROCRYPT 2005, R. Cramer, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 457\u2013473.",
            "year": 2005
        },
        {
            "authors": [
                "D. Boneh",
                "B. Waters"
            ],
            "title": "Conjunctive, subset, and range queries on encrypted data",
            "venue": "Proceedings of the 4th Conference on Theory of Cryptography, ser. TCC\u201907. Berlin, Heidelberg: Springer-Verlag, 2007, p. 535\u2013554.",
            "year": 2007
        },
        {
            "authors": [
                "D. Boneh",
                "A. Sahai",
                "B. Waters"
            ],
            "title": "Functional encryption: Definitions and challenges",
            "venue": "Theory of Cryptography, Y. Ishai, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 253\u2013273.",
            "year": 2011
        },
        {
            "authors": [
                "S. Agrawal",
                "R. Goyal",
                "J. Tomida"
            ],
            "title": "Multi-input quadratic functional encryption from pairings",
            "venue": "Crypto, 2021, https://ia.cr/2020/1285.",
            "year": 2021
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Multi-input quadratic functional encryption: Stronger security, broader functionality",
            "venue": "Theory of Cryptography, E. Kiltz and V. Vaikuntanathan, Eds. Cham: Springer Nature Switzerland, 2022, pp. 711\u2013740.",
            "year": 2022
        },
        {
            "authors": [
                "M. Abdalla",
                "F. Benhamouda",
                "R. Gay"
            ],
            "title": "From single-input to multiclient inner-product functional encryption",
            "venue": "Advances in Cryptology \u2013 ASIACRYPT 2019, S. D. Galbraith and S. Moriai, Eds. Cham: Springer International Publishing, 2019, pp. 552\u2013582.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nIn many emerging applications, a machine learning (ML) model must be trained using private data that is distributed among multiple parties. We study the setting of vertical federated learning (VFL) where each individual party has access to a subset of features and labels and must cooperate to train a ML model that makes use of all the features. When privacy of user data is required, homomorphic encryption (HE) [2], [3], which enables the computation on encrypted data, provides a natural solution. In recent years, there has been a significant interest in HE based VFL systems, see e.g., [5]\u2013 [12]. Some works such as [7], [8], [11] consider a two-party protocol without the trusted coordinator, while others [9], [10]. consider a multi-party settings. Those frameworks require a large amount of peer-to-peer communications. References [5], [6] propose frameworks comprised of one trusted coordinator, storing the global weights, and two parties, each with a subset of vertically partitioned data. However, these frameworks require the trusted coordinator to share plaintext global weights with parties, which undermines the model\u2019s confidentiality.\nIn a recent work, Xu et. al. [1] proposed a generic and efficient privacy-preserving vertical Federated Learning (VFL) framework known as FedV in the multiparty setting. FedV makes use of single-input function encryption (SIFE) and multi-input function encryption (MIFE), and makes the communication between the clients and the aggregator a one-round interaction. However, FedV still have some key drawbacks. The protocol can reveal more information to the aggregator than just the final gradient in each iteration. Moreover, the protocol reveals the respective updated weights in each iteration to clients, which additionally creates leakage. For more details, please see Section IV."
        },
        {
            "heading": "A. Our Results.",
            "text": "We observe that the leakage created in FedV is caused by choosing an multi-input functional encryption (MIFE) scheme that only supports linear functions. Due to this, the weights are required to be provided to each party for inclusion in encryption, which creates unnecessary leakage. We observe that for linear models, this leakage can be prevented by using a more powerful MIFE scheme, namely MIFE for quadratic functions which can also be constructed using standard assumptions in cryptography [17], [18]. As our main contribution in this work, we demonstrate how such a function encryption scheme can be applied in VFL training by proposing a novel construction of function vectors that serve as a basis for generating decryption keys. Our approach leads to direct computation of the gradients, without leakage of any intermediate results as is the case with FedV. We discuss our proposed protocol, SFedV, for linear model training in Section IV and the extension to logistic regression model in Appendix C. We provide a thorough analysis of both security and efficiency in Section IV."
        },
        {
            "heading": "II. SYSTEM MODEL",
            "text": ""
        },
        {
            "heading": "A. System Overview",
            "text": "Our system model involves three types of entities: aggregator, \ud835\udc41 clients, and Trusted Third Party (TTP). In the \ud835\udc61th iteration for \ud835\udc61 \u2208 [\ud835\udc47], each client holds a subset of features X \ud835\udc61\n\ud835\udc56 \u2208 R\ud835\udc46\u00d7\ud835\udc39\ud835\udc56 where \ud835\udc39\ud835\udc56 is the number of features that client party \ud835\udc56 holds and \ud835\udc46 is the batch size. A complete feature set of the current iteration is expressed as X \ud835\udc61 = [X \ud835\udc610 \u2225 ... \u2225 X \ud835\udc61\n\ud835\udc41\u22121] \u2208 R \ud835\udc46\u00d7\ud835\udc39 . One of the client parties holds the\ncorresponding labels y\ud835\udc61 \u2208 R\ud835\udc46\u00d71. The aggregator holds the entire model weights w\ud835\udc61 = [w\ud835\udc610 \u2225 w \ud835\udc61 1 \u2225 ... \u2225 w \ud835\udc61 \ud835\udc41\u22121] \u2208 R \ud835\udc39\u00d71 where w\ud835\udc61 \ud835\udc56 \u2208 R\ud835\udc39\ud835\udc56\u00d71 is the partial weights that pertains to X \ud835\udc61 \ud835\udc56 . The aggregator is responsible for computing the model weights and the TTP is responsible for the generation of keys. In this work we focus on linear models of the form: \ud835\udc53 (X \ud835\udc61 ,w\ud835\udc61 ) = X \ud835\udc61 \u00b7 w\ud835\udc61 with a squared-error loss function: \ud835\udc3f (w\ud835\udc61 ) = 1\n\ud835\udc46\n\u2211 \ud835\udc60\u2208[\ud835\udc46 ] | |y\ud835\udc61 [\ud835\udc60] \u2212 (X \ud835\udc61 \u00b7w\ud835\udc61 ) [\ud835\udc60] | |2. In our discussion,\nwe will define the prediction error as:\nu\ud835\udc61 = (y\ud835\udc61 \u2212X \ud835\udc610 \u00b7w \ud835\udc61 0 \u2212 ... \u2212X \ud835\udc61 \ud835\udc41\u22121 \u00b7w \ud835\udc61 \ud835\udc41\u22121). (1)\nThe gradient of \ud835\udc3f (w\ud835\udc61 ) with respect to w\ud835\udc61 is expressed as\n\ud835\udc54(w\ud835\udc61 ) = \u2212 2 \ud835\udc46  yt \u22a4 X \ud835\udc610 \u2212 \u2211\ud835\udc41\u22121 \ud835\udc56=0 w \ud835\udc61 \ud835\udc56 \u22a4X \ud835\udc61 \ud835\udc56 \u22a4X \ud835\udc610 ...\u2225 y\ud835\udc61\u22a4X \ud835\udc61 \ud835\udc41\u22121 \u2212 \u2211\ud835\udc41\u22121 \ud835\udc56=0 w \ud835\udc61 \ud835\udc56 \u22a4X \ud835\udc61 \ud835\udc56 \u22a4X \ud835\udc61 \ud835\udc41\u22121  \u2208 R1\u00d7\ud835\udc39 (2)\nar X\niv :2\n30 5.\n08 35\n8v 2\n[ cs\n.C R\n] 1\n9 Ju\nn 20\n23\nThe gradient is used to update the global weights in each iteration according to w\ud835\udc61+1 = w\ud835\udc61 \u2212 \ud835\udefc\ud835\udc54(w\ud835\udc61 ) where \ud835\udefc is the learning rate. We also discuss logistic regression model in Appendix C. In each iteration of the training phase, our protocol takes as input an encrypted copy of the features X \ud835\udc61\n\ud835\udc56\nand encrypted labels y\ud835\udc61 \u2208 R\ud835\udc46 from clients, and collaboratively and securely computes the gradients \ud835\udc54(w\ud835\udc61 ).\nOur threat model is defined as follows: we assume the aggregator is honest-but-curious meaning it correctly follows the algorithms and protocols but will try to infer clients\u2019 private data. Additionally, we assume that the aggregator does not collude with anyone. Similarly, the trusted third party is assumed not to collude with anyone. With respect to the clients, we assume that there are at most \ud835\udc41 \u2212 1 dishonest clients who may collude together and share their data to infer honest clients\u2019 information.\nThe protocol enables all the entities to collaboratively compute the gradient using vertically partitioned data. During the training process, we aim to achieve the following privacy requirements: 1) The client \ud835\udc56 and the aggregator should learn nothing about data X \ud835\udc57 of client \ud835\udc57 for \ud835\udc56 \u2260 \ud835\udc57 . 2) Any client should learn nothing about the trained global model weights w, intermediate results including the prediction error as in (1) and the gradient \ud835\udc54(w). Moreover, \ud835\udc56th client should not learn anything about his/her own corresponding weights w\ud835\udc56 ."
        },
        {
            "heading": "III. PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A. Functional Encryption",
            "text": "Functional Encryption [14]\u2013[16] is a public key encryption scheme that enables fine-grained access control over the encrypted data. In Single Input Functional Encryption(SIFE), the secret key is associated with a function \ud835\udc53 , and the ciphertext is associated with the vector x. The decryption of ciphertext using the secret key outputs \ud835\udc53 (x). Intuitively, the security says that the adversary learns nothing about the input x beyond what is revealed by { \ud835\udc53\ud835\udc56 (x)}\ud835\udc56 for any set of secret keys corresponding to the functions { \ud835\udc53\ud835\udc56}\ud835\udc56 that the adversary holds."
        },
        {
            "heading": "B. Multi-Input Functional Encryption",
            "text": "Goldwasser et al. [13] generalized the functional encryption to support functions with multiple inputs. Multi-Input Functional Encryption, denoted as MIFE supports functions with arity greater than one. In MIFE, the secret key is associated with a function \ud835\udc53 , and the \ud835\udc56th ciphertext is associated with the vector x\ud835\udc56 for \ud835\udc56 \u2208 [\ud835\udc41] where \ud835\udc41 is the arity of the function \ud835\udc53 . The decryption of all the ciphertexts using the secret key outputs \ud835\udc53 (x1, . . . ,x\ud835\udc41 ). We now describe this notion in more detail.\nDefinition 1 (Multi-Input Functional Encryption (MIFE) [18]). Syntax. Let \ud835\udc41 be the number of encryption slots, and F = {F\ud835\udc41 }\ud835\udc41 \u2208N be a function family such that, for all \ud835\udc53 \u2208 F\ud835\udc41 , \ud835\udc53 : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X\ud835\udc41 \u2192 Y. Here X\ud835\udc56 and Y be the input and output spaces (respectively). A multi-input functional encryption (MIFE) scheme for function family F consists of the following algorithms.\nSetup(1\ud835\udf06, 1\ud835\udc41 ) \u2192 (PP, {EK\ud835\udc56}\ud835\udc56 ,MSK). It takes a security parameter 1\ud835\udf06, number of slots 1\ud835\udc41 , and outputs a public parameter PP, \ud835\udc41 encryption keys {EK\ud835\udc56}\ud835\udc56\u2208[\ud835\udc41 ] and a master secret key MSK. (The remaining algorithms implicitly take PP as input.)\nEnc(EK\ud835\udc56 ,x) \u2192 CT\ud835\udc56 . It takes the \ud835\udc56th encryption key EK\ud835\udc56 and an input x \u2208 X\ud835\udc56 , and outputs a ciphertext CT\ud835\udc56 . KeyGen(MSK, \ud835\udc53 ) \u2192 SK. It takes the master secret key MSK and function \ud835\udc53 \u2208 F as inputs, and outputs a secret key SK.\nDec(CT1, . . . ,CT\ud835\udc41 ,SK) \u2192 \ud835\udc66. It takes \ud835\udc5b ciphertexts CT1, . . . ,CT\ud835\udc41 and secret key SK, and outputs a decryption value \ud835\udc66 \u2208 Y or a special abort symbol \u22a5. Correctness. An MIFE scheme for the function family F is correct if for all \ud835\udf06, \ud835\udc41 \u2208 N, (x1, . . . ,x\ud835\udc41 ) \u2208 X1\u00d7\u00b7 \u00b7 \u00b7\u00d7X\ud835\udc41 , \ud835\udc53 \u2208 F\ud835\udc41 , we have\nPr \ud835\udc66 = \ud835\udc53 (\ud835\udc651, . . . , \ud835\udc65\ud835\udc41 ) : (PP, {EK\ud835\udc56 }\ud835\udc56 , MSK) \u2190 Setup(1\ud835\udf06, 1\ud835\udc41 ) {CT\ud835\udc56 \u2190 Enc(EK\ud835\udc56 ,x) }\ud835\udc56\u2208 [\ud835\udc41 ] SK\u2190 KeyGen(MSK, \ud835\udc53 ) \ud835\udc66 = Dec(CT1, . . . , CT\ud835\udc41 , SK)  = 1. Security. Intuitively, security says that no information about the messages can be learned by the adversary except what is revealed by virtue of functionality \u2013 in more detail, an adversary possessing some ciphertexts and secret keys can perform decryption and learn the output of the functionality, which itself leaks something about the underlying plaintext. But besides this necessary leakage, the adversary does not learn anything. We provide the formal definition of security in Appendix A. Multi-Input FE for Quadratic Functions. Agrawal, Goyal, and Tomida [18] constructed a multi-input functional encryption for quadratic functions (qMIFE). Let us define the \ud835\udc41 input quadratic function \ud835\udc53 as \ud835\udc53 (x1, . . . ,x\ud835\udc41 ) = \u27e8c,x \u2297 x\u27e9 where x = (x1 | | . . . | |x\ud835\udc41 ). Here \u2297 denotes the Kronecker product. A \ud835\udc5b-input MIFE scheme for the function class F\ud835\udc5a,\ud835\udc5b is defined as: each \ud835\udc56th client encrypts x\ud835\udc56 \u2208 Z\ud835\udc5a using \ud835\udc56th encryption key EK\ud835\udc56 to get the \ud835\udc56th ciphertext CT\ud835\udc56 for \ud835\udc56 \u2208 [\ud835\udc5b]. The KeyGen algorithm issues the secret key SK for c \u2208 Z(\ud835\udc5a\ud835\udc5b)2 where c is the vector representation of the function \ud835\udc53 \u2208 F\ud835\udc5a,\ud835\udc5b. The Dec algorithm uses the secret key SK to decrypt CT1, . . . ,CT\ud835\udc5b to get \u27e8c,x \u2297 x\u27e9 and nothing else."
        },
        {
            "heading": "C. FedV",
            "text": "As the system model of SFedV (Section II-A), FedV involves an aggregator, N clients, and a Trusted Third Party (TTP). Each client holds a subset of features X\ud835\udc56 \u2208 R\ud835\udc46\u00d7\ud835\udc39\ud835\udc56 , and the first client also has the corresponding labels y \u2208 R\ud835\udc46 along with its subset of features. The aggregator holds the complete model weights w = [w0 | |w1 | |...| |w\ud835\udc41\u22121] where w\ud835\udc56 \u2208 R\ud835\udc39\ud835\udc56 is the partial weights that pertains to X\ud835\udc56 . In each iteration, there are two steps to compute the gradient\n\ud835\udc54(w) = \u2212 2 \ud835\udc46\n[ u\u22a4X0 \u2225 ... \u2225 u\u22a4X\ud835\udc41\u22121 ] (3)\nwhere (u)\u22a4 = (y \u2212X0w0 \u2212 ... \u2212X\ud835\udc41\u22121w\ud835\udc41\u22121)\u22a4. In the first step called Feature Dimension Secure Ag-\ngregation, FedV uses a Multi-Input Functional Encryption (MIFE) scheme for the inner product functionality [19], [20]\nto securely compute the prediction error u. In this step, the aggregator sends each \ud835\udc56th client the partial weights w\ud835\udc56 . Then each \ud835\udc56th client encrypts each sample of (\u2212X\ud835\udc56w\ud835\udc56) \u2208 R\ud835\udc46 and sends the ciphertext set CTMIFE\u2212X\ud835\udc56w\ud835\udc56 to the aggregator. The first client, holding the label y, encrypts each sample of y\u2212X1w1 and sends the ciphertext set CTMIFEy\u2212X1w1 to the aggregator. The aggregator asks the TTP for the secret key SKMIFEv corresponding to the fusion vector v. This vector v can be a binary vector where one in \ud835\udc56th position means that the aggregator has received ciphertext from client \ud835\udc56. Using the secret key SKMIFEv , the aggregator decrypts the ciphertexts {{CTMIFE\u2212X\ud835\udc56w\ud835\udc56 } \ud835\udc41\u22121 \ud835\udc56=1 ,CT MIFE y\u2212X1w1 } to get the prediction error u (Equation (1)), which is the inner product of the fusion vector v and the partial predictions from clients.\nIn the second step called Sample Dimension Secure Aggregation, FedV uses Single-Input Functional Encryption (SIFE) scheme to compute the gradient \ud835\udc54(w). In this step, each client \ud835\udc56 encrypts each element of X\ud835\udc56 and sends the ciphertext set CTSIFEX\ud835\udc56 to the aggregator. On receiving the secret key SK SIFE u corresponding to the prediction error u from TTP, the aggregator decrypts the ciphertexts to get {u\u22a4X\ud835\udc56}\ud835\udc56\u2208[\ud835\udc41 ] . The aggregator further processes the decryption results {u\u22a4X\ud835\udc56}\ud835\udc56\u2208[\ud835\udc41 ] according to Equation (3) to get the gradient \ud835\udc54(w). Using the gradients, the model weights are updated and then the training of the next epoch starts. Note the transmission of MIFE ciphertext (CTMIFEX\ud835\udc56w\ud835\udc56 or CT MIFE y\u2212X1w1 ) and SIFE ciphertext (CT SIFE X\ud835\udc56\n) can be simultaneous. Thus the communication between each client and the aggregator is a one-round interaction in each iteration.\nLeakage in FedV. While FedV preserves each client\u2019s data, it reveals the intermediate result, the prediction error u to the aggregator. Moreover, in the Feature Dimension Secure Aggregation step, the \ud835\udc56th client is required to know the respective weight w\ud835\udc56 . Additionally, the aggregator can use the secret key of \ud835\udc61th iteration to decrypt the ciphertext of some other iteration \ud835\udc61\u2032 for \ud835\udc61\u2032 \u2260 \ud835\udc61 to infer more information about client data."
        },
        {
            "heading": "IV. THE PROTOCOL",
            "text": "Now we introduce our protocol with Multi-input Quadratic Functional Encryption qMIFE [18] as a privacy enhancement technology to do training in VFL setting.\nAt the beginning of the training phase, the aggregator initializes the global weights w0 and starts training. The training phase is iterative, where in the \ud835\udc61th iteration, the TTP runs the qMIFE.Setup algorithm to get the public parameters PP\ud835\udc61 , \ud835\udc41 encryption keys {EK\ud835\udc56}\ud835\udc61\ud835\udc56\u2208[\ud835\udc41 ] and a master secret key MSK\n\ud835\udc61 , then delivers the encryption key EK\ud835\udc61\ud835\udc56 to the corresponding client \ud835\udc56. After receiving the encryption key and determining the batch X \ud835\udc61 used for training in this iteration, each client uses EK\ud835\udc61\ud835\udc56 to encrypt x \ud835\udc61 \ud835\udc56 which is the vectorized X \ud835\udc61 \ud835\udc56\n(vec(\u00b7) stacks the columns of a matrix into a vector) to get ciphertext CT\ud835\udc61\ud835\udc56 . Each client sends CT \ud835\udc61 \ud835\udc56 to the aggregator. The client that holds the labels encrypts x\ud835\udc61 \ud835\udc56\nand y\ud835\udc61 with EK\ud835\udc61\ud835\udc56 to get ciphertexts CT\ud835\udc61\ud835\udc56 and CT \ud835\udc61 y respectively and then sends (CT\ud835\udc61\ud835\udc56 ,CT\ud835\udc61y) to the aggregator. At the same time, the aggregator computes a set\nof function vectors C\ud835\udc61 according to the model weights w\ud835\udc61 of the current iteration and sends them to TTP to generate a set of decryption keys. Detailed procedure is described in Section IV-A. Then, the aggregator decrypts all the ciphertexts ({CT\ud835\udc61\ud835\udc56 }\ud835\udc41\u22121\ud835\udc56=0 ,CT \ud835\udc61 \ud835\udc66) that were received from clients using the secret keys received from TTP, to get each element of (2) respectively. By concatenating those elements and further processing the results, the aggregator gets the gradients. After this, it can update the global weights and start the training of the next iteration. Algorithm 1 shows the training procedure for linear models and also supports the training for logistic regression as discussed in Appendix C."
        },
        {
            "heading": "A. Construction of Function Vectors",
            "text": "Our goal is to compute the gradient \ud835\udc54(w) (Equation (2)). For simplicity, we drop the superscript \ud835\udc61 in our discussion. We define x = [x0 | |...| |x\ud835\udc41\u22121 | |y] where y is the label vector and x\ud835\udc56 is the vectorized X\ud835\udc56 . Recall \ud835\udc54(w) is a vector of length \ud835\udc39, where \ud835\udc39 is the total number of features. The key insight is that for the \ud835\udc53 th, \ud835\udc53 \u2208 [\ud835\udc39] element in (2), we construct a function vector c \ud835\udc53 based on weights w of current iteration, such that \ud835\udc54(w) [ \ud835\udc53 ] = \u2212 2\n\ud835\udc46\n\u2329 c \ud835\udc53 ,x \u2297 x \u232a . Then the aggregator concatenates\n\ud835\udc54(w) [ \ud835\udc53 ], \ud835\udc53 \u2208 [\ud835\udc39] to obtain the gradients. For simplicity, we define the following:\nz\ud835\udc56 = u \u22a4X\ud835\udc56 , b \ud835\udc56 \ud835\udc57 = w \u22a4 \ud835\udc57 X \u22a4 \ud835\udc57 X\ud835\udc56 , b \ud835\udc56 \ud835\udc66 = y \u22a4X\ud835\udc56 . (4)\nNow we decompose \ud835\udc54(w) and x\u2297x to reduce the assignment. Note that to compute \ud835\udc54(w) it suffices to compute z0, ..., z\ud835\udc41\u22121 as in (3). Here we define a set of function vectors C = {C\ud835\udc56}\ud835\udc41\u22121\ud835\udc56=0 and C\ud835\udc56 = {c\ud835\udc56, \ud835\udc5d}\ud835\udc39\ud835\udc56\u22121\ud835\udc5d=0 , where C\ud835\udc56 is a subset of function vectors that are used to compute elements in z\ud835\udc56 , c\ud835\udc56, \ud835\udc5d is the function vector to compute \ud835\udc5dth element of z\ud835\udc56 as in (5).\nz\ud835\udc56 [\ud835\udc5d] = \u27e8c\ud835\udc56, \ud835\udc5d ,x \u2297 x\u27e9 (5)\nWe construct c\ud835\udc56, \ud835\udc5d block by block according to the decomposition of x \u2297 x. Consider dividing x \u2297 x into \ud835\udc41 + 1 blocks as in the middle of Figure 1.\nSince in the computation of z\ud835\udc56 , only the component x\ud835\udc56 \u2297 x is required, we set 0 vector of the corresponding lengths as the coefficients of the blocks {x \ud835\udc57 \u2297 x}\ud835\udc41\ud835\udc57=0 if \ud835\udc57 \u2260 \ud835\udc56. We design a\ud835\udc56, \ud835\udc5d to make z\ud835\udc56 [\ud835\udc5d] = \u27e8a\ud835\udc56, \ud835\udc5d ,x\ud835\udc56 \u2297 x\u27e9.\nLet D \ud835\udc53 \ud835\udc56 denotes the \ud835\udc53 th column of X\ud835\udc56 . Thus x\ud835\udc56 = [D0\ud835\udc56 ; ...; D \ud835\udc39\ud835\udc56\u22121 \ud835\udc56 ]. Note that in the computation of z\ud835\udc56 [\ud835\udc5d] only the column D\ud835\udc5d \ud835\udc56\nis used, thus we set 0 vector as the coefficients of {D\ud835\udc5e\n\ud835\udc56 \u2297x}\ud835\udc39\ud835\udc56\u22121\n\ud835\udc5e=0 if \ud835\udc5e \u2260 \ud835\udc5d. Hence we can express \u27e8a\ud835\udc56, \ud835\udc5d ,x\ud835\udc56 \u2297x\u27e9 = \u27e8d,D\ud835\udc5d\n\ud835\udc56 \u2297 x\u27e9. We design d to achieve z\ud835\udc56 [\ud835\udc5d] = \u27e8d,D\ud835\udc5d\ud835\udc56 \u2297 x\u27e9.\nAlgorithm 1 Training Procedure\n1: procedure TRAINING-AGGREGATOR(w\ud835\udc61 , \ud835\udc60, {\ud835\udc39\ud835\udc56}\ud835\udc41\u22121\ud835\udc56=0 , \ud835\udc41) 2: res = 0\ud835\udc39 3: for each \ud835\udc56 \u2208 0, ..., \ud835\udc41 \u2212 1 do 4: for each \ud835\udc5d \u2208 0, ..., \ud835\udc39\ud835\udc56 \u2212 1 do 5: c\ud835\udc61\n\ud835\udc56, \ud835\udc5d := CGEN(w\ud835\udc61 , \ud835\udc46, \ud835\udc39, \ud835\udc41, \ud835\udc56, \ud835\udc5d)\n6: end for 7: end for 8: C\ud835\udc61 := {c\ud835\udc61\n\ud835\udc56, \ud835\udc5d , \ud835\udc56 \u2208 [\ud835\udc41], \ud835\udc5d \u2208 [\ud835\udc39\ud835\udc56]}\n9: {{qMIFE.SK\ud835\udc61c\ud835\udc56,\ud835\udc5d } \ud835\udc39\ud835\udc56 \ud835\udc5d=0} \ud835\udc41\u22121 \ud835\udc56=0 = obtain-dk-from-TTP(C \ud835\udc61 ) 10: for each \ud835\udc56 \u2208 0, ..., \ud835\udc41 \u2212 1 do 11: if party \ud835\udc56 has label y\ud835\udc61 then 12: (CT\ud835\udc61\ud835\udc56 ,CT \ud835\udc61 \ud835\udc66)= obtain-ct-from-client() 13: else 14: CT\ud835\udc61\ud835\udc56 = obtain-ct-from-client() 15: end if 16: end for 17: CT\ud835\udc61 = {{CT\ud835\udc61\ud835\udc56 }\ud835\udc41\u22121\ud835\udc56=0 ,CT \ud835\udc61 \ud835\udc66} 18: for each \ud835\udc5b \u2208 0, ..., \ud835\udc41 \u2212 1 do 19: for each \ud835\udc5d \u2208 0, ..., \ud835\udc39\ud835\udc5b \u2212 1 do 20: idx = \u2211\ud835\udc5b\u22121 \ud835\udc56=0 \ud835\udc39\ud835\udc56 + \ud835\udc5d 21: res[idx] = qMIFE.Dec(CT\ud835\udc61 , qMIFE.SK\ud835\udc61c\ud835\udc5b,\ud835\udc5d ) 22: end for 23: end for 24: \u2207\ud835\udc3f (w\ud835\udc61 ) = \u2212 2\n\ud835\udc46 res + \ud835\udf06\u2207\ud835\udc45(w\ud835\udc61 )\n25: w\ud835\udc61+1 = w\ud835\udc61 \u2212 \ud835\udefc\u2207\ud835\udc3f (w\ud835\udc61 ) 26: end procedure 27: procedure TRAINING-CLIENT(X \ud835\udc61\n\ud835\udc56 )\n28: function OBTAIN-CT-FROM-CLIENT() 29: qMIFE.EK\ud835\udc61\ud835\udc56= obtain-ek-from-TTP() 30: x\ud835\udc61\n\ud835\udc56 := vec(X \ud835\udc61 \ud835\udc56 )\n31: if party \ud835\udc56 has label y\ud835\udc61 then 32: CT\ud835\udc61\ud835\udc56 := qMIFE.Enc(qMIFE.EK\ud835\udc61\ud835\udc56 ,x\ud835\udc61\ud835\udc56 ) 33: CT\ud835\udc61\ud835\udc66 := qMIFE.Enc(qMIFE.EK\ud835\udc61\ud835\udc56 , y\ud835\udc61 ) 34: Return (CT\ud835\udc61\ud835\udc56 ,CT\ud835\udc61\ud835\udc66) to Aggregator 35: else 36: CT\ud835\udc61\ud835\udc56 := qMIFE.Enc(qMIFE.EK\ud835\udc61\ud835\udc56 ,x\ud835\udc61\ud835\udc56 ) 37: Return CT\ud835\udc61\ud835\udc56 to Aggregator 38: end if 39: end function 40: end procedure 41: procedure TRAINING-TTP(1\ud835\udf06, 1\ud835\udc41 ) 42: function OBTAIN-EK-FROM-TTP() 43: (PP\ud835\udc61 , {EK\ud835\udc56}\ud835\udc61\ud835\udc56 ,MSK\n\ud835\udc61 ) \u2190 qMIFE.Setup(1\ud835\udf06, 1\ud835\udc41 ) 44: Deliver qMIFE.EK\ud835\udc61\ud835\udc56 to party \ud835\udc56, \ud835\udc56 \u2208 [\ud835\udc41] 45: end function 46: function OBTAIN-DK-FROM-TTP(C\ud835\udc61 ) 47: for each \ud835\udc56 \u2208 0, ..., \ud835\udc41 \u2212 1 do 48: for each \ud835\udc5d \u2208 0, ..., \ud835\udc39\ud835\udc56 do 49: qMIFE.KeyGen(qMIFE.MSK\ud835\udc61 , c\ud835\udc61\n\ud835\udc56, \ud835\udc5d ) \u2192\nqMIFE.SK\ud835\udc61c\ud835\udc56,\ud835\udc5d 50: end for 51: end for 52: Return {{qMIFE.SK\ud835\udc61c\ud835\udc56,\ud835\udc5d } \ud835\udc39\ud835\udc56 \ud835\udc5d=0} \ud835\udc41\u22121 \ud835\udc56=0 53: end function 54: end procedure\nFrom (2), (3) and (4) note that we can express:\nz\ud835\udc56 [\ud835\udc5d] = \ud835\udc41\u22121\u2211\ufe01 \ud835\udc57=0 \u2212b\ud835\udc56\ud835\udc57 [\ud835\udc5d] + b \ud835\udc56 \ud835\udc66 [\ud835\udc5d] (6)\nwhere we have z\ud835\udc56 [\ud835\udc5d] = (u\u22a4X\ud835\udc56) [\ud835\udc5d], b\ud835\udc56\ud835\udc57 [\ud835\udc5d] = (w\u22a4\n\ud835\udc57 X\u22a4 \ud835\udc57 X\ud835\udc56) [\ud835\udc5d],b\ud835\udc56\ud835\udc66 [\ud835\udc5d] = (y\u22a4X\ud835\udc56) [\ud835\udc5d]. We expand b\ud835\udc56\ud835\udc57 [\ud835\udc5d] by first performing multiplication term-by-term and then summing products as in (7). Based on this equation, we determine the method for constructing the coefficients vector d.\nb\ud835\udc56\ud835\udc57 [\ud835\udc5d] = (w \u22a4 \ud835\udc57 X \u22a4 \ud835\udc57 X\ud835\udc56) [\ud835\udc5d] = (w \u22a4 \ud835\udc57 X \u22a4 \ud835\udc57 )D \ud835\udc5d \ud835\udc56\n=  \ud835\udc39\ud835\udc57\u22121\u2211\ufe01 \ud835\udc53 =0 w\u22a4\ud835\udc57 [ \ud835\udc53 ]D \ud835\udc53 \ud835\udc56 [0] \u2225 ... \u2225 \ud835\udc39\ud835\udc57\u22121\u2211\ufe01 \ud835\udc53 =0 w\u22a4\ud835\udc57 [ \ud835\udc53 ]D \ud835\udc53 \ud835\udc56 [\ud835\udc46 \u2212 1] D\ud835\udc5d\ud835\udc56 =\n\ud835\udc46\u22121\u2211\ufe01 \ud835\udc60=0 \ud835\udc39\ud835\udc57\u22121\u2211\ufe01 \ud835\udc53 =0 w \ud835\udc57 [ \ud835\udc53 ]D \ud835\udc53\ud835\udc57 [\ud835\udc60]D \ud835\udc5d \ud835\udc56 [\ud835\udc60] (7)\nNow the goal is to construct d such that z\ud835\udc56 [\ud835\udc5d] = \u27e8d,D\ud835\udc5d\ud835\udc56 \u2297x\u27e9. We keep decomposing d and D\ud835\udc5d\n\ud835\udc56 \u2297x to blocks as in Figure 2.\nIn Figure 2, blocks with the same color will be designed to compute the corresponding term on the right side of Equation (6). Considering d\ud835\udc60, \ud835\udc57 , \ud835\udc60 \u2208 [\ud835\udc46], we can set the following relations:\nb\ud835\udc56\ud835\udc57 [\ud835\udc5d] = \ud835\udc46\u22121\u2211\ufe01 \ud835\udc60=0 \u27e8d\ud835\udc60, \ud835\udc57 ,D\ud835\udc5d\ud835\udc56 [\ud835\udc60]x \ud835\udc57 \u27e9 (8)\nb\ud835\udc56\ud835\udc66 [\ud835\udc5d] = \ud835\udc46\u22121\u2211\ufe01 \ud835\udc60=0 \u27e8d\ud835\udc60,\ud835\udc41 ,D\ud835\udc5d\ud835\udc56 [\ud835\udc60]y\u27e9 (9)\nNext, we introduce the approach to construct d\ud835\udc60, \ud835\udc57 , \ud835\udc57 \u2208 [\ud835\udc41] according to the corresponding weight piece. We remove the outer summation in (7) and (8) to obtain:\n\ud835\udc39\ud835\udc57\u22121\u2211\ufe01 \ud835\udc53 =0 w \ud835\udc57 [ \ud835\udc53 ]D \ud835\udc53\ud835\udc57 [\ud835\udc60]D \ud835\udc5d \ud835\udc56 [\ud835\udc60] = \u27e8d\ud835\udc60, \ud835\udc57 ,D\ud835\udc5d\ud835\udc56 [\ud835\udc60]x \ud835\udc57 \u27e9 (10)\nWe design d\ud835\udc60, \ud835\udc57 to achieve (10) as Figure 3 shows. We decompose \u27e8d\ud835\udc60, \ud835\udc57 ,D\ud835\udc5d\ud835\udc56 [\ud835\udc60]x \ud835\udc57\u27e9 into blocks D \ud835\udc5d \ud835\udc56 [\ud835\udc60]D \ud835\udc53 \ud835\udc57 , \ud835\udc53 \u2208 [\ud835\udc39\ud835\udc57 ]. In each block D\ud835\udc5d \ud835\udc56 [\ud835\udc60]D \ud835\udc53 \ud835\udc57 , we take one entry D\ud835\udc5d \ud835\udc56 [\ud835\udc60]D \ud835\udc53 \ud835\udc57 [\ud835\udc60] and set its coefficient to w \ud835\udc57 [ \ud835\udc53 ] just as the left side of (10). For other unneeded terms, we set the coefficient to 0.\nThe approach to construct d\ud835\udc60, \ud835\udc57 , \ud835\udc57 \u2208 [\ud835\udc41] can be easily extended to the case for d\ud835\udc60,\ud835\udc41 . By designing the blocks of d this way, we can achieve z\ud835\udc56 [\ud835\udc5d] = \u27e8d,D\ud835\udc5d\ud835\udc56 \u2297x\u27e9. The algorithms to construct the function vectors c\ud835\udc56, \ud835\udc5d and \ud835\udc51 are provided in Appendix B."
        },
        {
            "heading": "B. Privacy Analysis",
            "text": "Recall the aim of our framework. We want the client \ud835\udc56 and the aggregator to learn nothing about data X \ud835\udc57 of client \ud835\udc57 for \ud835\udc56 \u2260 \ud835\udc57 . We also want that any client should learn nothing about the trained global model weights w, intermediate results including error between labels and feed-forward output as in (1) and the gradient \ud835\udc54(w). Moreover, \ud835\udc56th client should not learn anything about his/her own corresponding weights w\ud835\udc56 . In this section, we prove that we have achieved the abovestated goal.\nTheorem IV.1. If Quadratic MIFE (qMIFE) is secure according to definition 2, then in each training iteration \ud835\udc61, \ud835\udc56th client\u2019s data X \ud835\udc61\n\ud835\udc56 for \ud835\udc56 \u2208 [\ud835\udc41] is hidden from client \ud835\udc57 and the\naggregator, trained global model weights w\ud835\udc61 and intermediate results u\ud835\udc61 as in (1) are hidden from the clients and \ud835\udc56th client learns nothing about weight w\ud835\udc56 .\nLet us fix the iteration number to be \ud835\udc61. In each iteration, the TTP runs the qMIFE.Setup algorithm to get public parameters PP\ud835\udc61 , \ud835\udc41 encryption keys {EK\ud835\udc56}\ud835\udc61\ud835\udc56\u2208[\ud835\udc41 ] and a master secret key MSK\ud835\udc61 . The clients encrypt their respective data and send the ciphertexts to the aggregator. The aggregator asks the TTP for the secret key corresponding to the set of vectors C\ud835\udc61 . Each vector c\ud835\udc61\n\ud835\udc56, \ud835\udc5d is set in such a way that the qMIFE.Dec\nonly reveals the inner product \u27e8c\ud835\udc61 \ud835\udc56, \ud835\udc5d ,x \u2297 x\u27e9 = ((u\ud835\udc61 )\u22a4X \ud835\udc61 \ud835\udc56 ) [\ud835\udc5d]. Quadratic MIFE ensures that nothing about X \ud835\udc61 \ud835\udc56\nand u\ud835\udc61 is revealed to the aggregator. Moreover, each client encrypts their data using different encryption keys. The ciphertexts are indistinguishable; hence, clients cannot predict other clients\u2019 data.\nUnlike FedV, in our framework, the client runs the qMIFE.Enc algorithm which only takes their respective data and encryption keys as input. Hence, each client \ud835\udc56 learns nothing about their respective weight w\ud835\udc61\n\ud835\udc56 . Moreover, Quadratic\nMIFE ensures that client \ud835\udc56 learns nothing about the global weight w\ud835\udc61 . The aggregator does not share the gradients in any form with the clients. Therefore, the gradient \ud835\udc54(w\ud835\udc61 ) is also not revealed.\nImportance of using new qMIFE instance for each iteration. Let in the iteration \ud835\udc61, the ciphertext be CT\ud835\udc61 and secret key be qMIFE.SK\ud835\udc61 . Suppose the TTP uses the same MSK to generate secret keys qMIFE.SK\ud835\udc61+1 for some other iteration, say \ud835\udc61 + 1, then the aggregator may use qMIFE.SK\ud835\udc61+1 to decrypt the ciphertext CT\ud835\udc61 instead of using it to decrypt the ciphertext CT\ud835\udc61+1. Using this \"mix-andmatch\" attack by performing decryptions of secret key and ciphertexts from different iterations, he will know \ud835\udc54(w) = \u2212 2\n\ud835\udc46 [ (u\ud835\udc61+1)\u22a4X \ud835\udc610 | |...| | (u \ud835\udc61+1)\u22a4X \ud835\udc61 \ud835\udc41\u22121 ] .\nIf TTP generates different qMIFE instance for every iteration, then decryption of CT\ud835\udc61 with secret key qMIFE.SK\ud835\udc61+1\nwill give some garbage value which will be irrelevant for the aggregator. Therefore, it is important for the TTP to generate a new qMIFE instance for every iteration.\nComparison of our framework with FedV. Unlike FedV, our framework does not leak the intermediate result u to the aggregator. The global weights w are kept secret from the clients and each client also learns nothing about their respective weights. In addition to this, we also ensure that the aggregator cannot use the mix-and-match attack to learn some useful information."
        },
        {
            "heading": "C. Efficiency Analysis",
            "text": "Communication. Regarding communication complexity, SFedV requires one-way client-aggregator communication, while FedV needs one-round client-aggregator communication due to the delivery of global weights by the aggregator. Additionally, SFedV uses a new qMIFE instance in each iteration to prevent mix-and-match attacks. Thus, an increase of communication between TTP and clients becomes necessary. Note that FedV can also prevent mix-and-match attacks by using new instances of MIFE and SIFE in each iteration. In such a scenario, the client-TTP communication complexity for each iteration will be the same for both FedV and SFedV.\nComputation. Table I provides a comparison between FedV and SFedV in terms of the number of encryption and decryption processes in each iteration. The significant improvement of SFedV is attributed to the advancement of quadratic MIFE and the careful design of function vectors.\nIn terms of the number of the vector corresponding to which the secret keys are generated, FedV uses two vectors for two steps: v for feature dimension secure aggregation and u for sample dimension secure aggregation. In contrast, our SFedV framework employs \ud835\udc39 vectors c, where \ud835\udc39 is the total number of features. The increase in size can be justified by our use of a quadratic MIFE scheme instead of inner product MIFE."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "Prior \ud835\udc41-party VFL framework FedV incurs information leakage which seriously undermines individual data privacy. In this work, to address the privacy issues, We propose a leakfree protocol, called SFedV, for multiparty VFL regression model training. Our approach simplifies the VFL pipeline and preserves the privacy of client data, model weights, and intermediate results, by designing special function vectors and using a quadratic MIFE scheme to compute gradients directly."
        },
        {
            "heading": "APPENDIX A SECURITY DEFINITION FOR MIFE",
            "text": "In an indistinguishability-based security game between a challenger and an adversary, the challenger runs the Setup algorithm to generate the public parameters PP, \ud835\udc41 encryption keys EK\ud835\udc56 , and master secret key MSK. The adversary then chooses the set of encryption keys that she wants. Then the adversary chooses two messages x0 and x1 and gives them to the challenger. The challenger chooses a bit \ud835\udefd at random and encrypts the message x\ud835\udefd using the \ud835\udc56th encryption key EK\ud835\udc56\nto get challenge ciphertext CT\ud835\udc56 . The adversary then asks the challenger for the secret keys corresponding to the functions \ud835\udc53 . At last, the adversary guesses a bit \ud835\udefd\u2032 and replies to the challenger. The admissible adversary wins if \ud835\udefd\u2032 = \ud835\udefd. In security, we want the probability of the adversary winning the security game to be negligibly close to 1/2.\nThe adversary is said to be admissible if and only if she sends at least one element of the form (\ud835\udc56, \u2217, \u2217) in the message space and she queries the secret key for the function \ud835\udc53 which satisfies the constraint that \ud835\udc53 (x0) = \ud835\udc53 (x1). We formally define MIFE security in Definition 2.\nDefinition 2 (MIFE Security [18]). An MIFE scheme is INDsecure if for any stateful admissible PPT adversary A, there exists a negligible function negl(\u00b7) such that for all \ud835\udf06, \ud835\udc41 \u2208 N, the following probability is negligibly close to 1/2 in \ud835\udf06:\nPr  \ud835\udefd\u2032 = \ud835\udefd : \ud835\udefd\u2190 {0, 1} (PP, {EK\ud835\udc56}\ud835\udc56\u2208[\ud835\udc41 ] ,MSK) \u2190 Setup(1\ud835\udf06, 1\ud835\udc41 ) (CS,MS, FS) \u2190 A(1\ud835\udf06,PP) s.t. CS \u2286 [\ud835\udc41] MS = {\ud835\udc56\ud835\udf07,x\ud835\udf07,0,x\ud835\udf07,1}\ud835\udf07\u2208[\ud835\udc5e\ud835\udc50 ] FS = { \ud835\udc53 \ud835\udc63}\ud835\udc63\u2208[\ud835\udc5e\ud835\udc58 ] {CT\ud835\udf07 \u2190 Enc(EK\ud835\udc56\ud835\udf07 ,x\ud835\udf07,\ud835\udefd)}\ud835\udf07 {SK\ud835\udc63 \u2190 KeyGen(MSK, \ud835\udc53 \ud835\udc63)}\ud835\udc63 \ud835\udefd\u2032 \u2190 A ( {EK\ud835\udc56}\ud835\udc56\u2208CS , {CT\ud835\udf07}\ud835\udf07, {SK\ud835\udc63}\ud835\udc63 )  where the adversary A is said to be admissible if and only if \u2022 \ud835\udc5e\ud835\udc50 [\ud835\udc56] > 0 for all \ud835\udc56 \u2208 [\ud835\udc41], where \ud835\udc5e\ud835\udc50 [\ud835\udc56] denotes the number\nof elements of the form (\ud835\udc56, \u2217, \u2217) in MS. \u2022 \ud835\udc53 (x01, . . . ,x 0 \ud835\udc5b) = \ud835\udc53 (x11, . . . ,x 1 \ud835\udc5b) for all sequences\n(x01, . . . ,x 0 \ud835\udc5b,x 1 1, . . . ,x 1 \ud835\udc5b, \ud835\udc53 ) such that: \u2013 For all \ud835\udc56 \u2208 [\ud835\udc5b], [(\ud835\udc56,x0 \ud835\udc56 ,x1 \ud835\udc56 ) \u2208 MS] or [\ud835\udc56 \u2208\nCS and x0 \ud835\udc56 = x1 \ud835\udc56 ],\n\u2013 \ud835\udc53 \u2208 FS."
        },
        {
            "heading": "APPENDIX B PSEUDOCODE",
            "text": "In this section, we give reference to the algorithms used for constructing the function vectors. The algorithm 2 shows the construction of c\ud835\udc56, \ud835\udc5d and the algorithm 3 shows the construction of vector \ud835\udc51."
        },
        {
            "heading": "APPENDIX C EXTENSION TO LOGISTIC REGRESSION MODEL",
            "text": "In this section, we extend the protocol to work for logistic regression with the help of Taylor approximation. The prediction function of logistic models is as follows:\n\ud835\udc53 (x,w) = 1 1 \u2212 \ud835\udc52\u2212xw (11)\nWe use Cross-Entropy as the loss function for logistic regression. The loss function in the vectorized form is\n\ud835\udc3f (w) = 1 \ud835\udc60\n[ \u2212y\u22a4 log( \ud835\udc53 (X ,w)) \u2212 (1 \u2212 y)\u22a4 log(1 \u2212 \ud835\udc53 (X ,w)) ]\n(12)\nHere we use Taylor approximation to make the loss function polynomial. In [5], it takes a Taylor Series expansion of log(1+ \ud835\udc52\u2212\ud835\udc67) around \ud835\udc67 = 0.\nlog(1 + \ud835\udc52\u2212\ud835\udc67) = log 2 \u2212 1 2 \ud835\udc67 + 1 8 \ud835\udc672 \u2212 1 192 \ud835\udc674 +\ud835\udc42 (\ud835\udc676) (13)\nWe apply (13) to (12) and get the gradients of vector-format expression as given below.\n\ud835\udc54(w) \u2248 1 \ud835\udc46 ( 1 4 Xw \u2212 y + 1 2 )\u22a4 X (14)\nThen we decompose Xw = X0w0 +X1w1 + ...+X\ud835\udc41\u22121w\ud835\udc41\u22121 and X = [X0 | |X1 | |...| |X\ud835\udc41\u22121] and substitute the decomposi-\ntion into (14) to get\n\ud835\udc54(w) \u2248 1 \ud835\udc46  \u2212(y \u2212 12 ) \u22a4X0 + 14 \u2211\ud835\udc41\u22121 \ud835\udc57=0 w \u22a4 \ud835\udc57 X\u22a4 \ud835\udc57 X0 | | ...| | \u2212(y \u2212 12 ) \u22a4X\ud835\udc41\u22121 + 14 \u2211\ud835\udc41\u22121 \ud835\udc57=0 w \u22a4 \ud835\udc57 X\u22a4 \ud835\udc57 X\ud835\udc41\u22121  (15)\nEach term in (15) has the similar format to the term in (2) except for w\u22a4\n\ud835\udc57 X\u22a4 \ud835\udc57 X\ud835\udc56 in (15) has coefficient 14 and y \u22a4X\ud835\udc56 in (2) becomes (y\u2212 12 )\n\u22a4X\ud835\udc56 in (15). We can modify the protocol to compute the gradients for non-linear models without exposing labels y and X\ud835\udc56 . First, when the aggregator constructs function vector c, instead of using the original weights, we multiply the weights with 14 element-wise and use the modified weights to construct c. Moreover, the active party, instead of sending the ciphertext of y, now sends the ciphertext of y \u2212 12 . After decrypting and concatenating all the elements in (15), the aggregator multiplies the concatenated results with 1\n\ud835\udc46 to obtain\nthe gradients. Other procedures remain same as the procedure for the linear regression models."
        }
    ],
    "title": "Quadratic Functional Encryption for Secure Training in Vertical Federated Learning",
    "year": 2023
}