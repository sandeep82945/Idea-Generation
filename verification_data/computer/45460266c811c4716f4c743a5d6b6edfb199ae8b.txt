"The calculation complexity is too high, which is mainly caused by a large amount of data redundancy in the selection of sampling points (M) and random plaintext (N) on the fault probability trace: First, there are too many sampling points in the fault probability trace. The subkey only affects the fault probability in a few moments, so not all sampling points are essential in calculating correlation. Combined with the data dependence of fault probability, we need to locate the output of the S-box more accurately and reduce the search range of sampling points by selecting the sampling points with the prominent peak of fault probability trace as POIs. Second, there is much redundancy in selecting random plaintexts. All the sampling points in the fault probability traces are required to calculate the hypothetical intermediate value. Hence, the number of iterations in the calculation is enormous. The HW of single-byte data obeys the binomial distribution. Tab. I shows the probability distribution of HW for uniformly distributed 8-bit data. The data with HW of 0 and 8 have the slightest probability of occurrence, and those with HW of 4 have the highest probability of occurrence. Therefore, there must be much repetition in the randomly selected plaintexts. The experiments show that the difference in fault probability corresponding to the S-box output with the same HW is almost the same, as shown in Tab. II. If we select the plaintexts whose S-box outputs are of the same HW, column vectors of the hypothetical intermediate value mapped by the HW model are the same, and the corresponding fault probability traces are almost the same. In this case, it is impossible to recover the key. Therefore, we consider selecting plaintexts whose S-box outputs are of different HW, which can reduce the computational redundancy significantly."