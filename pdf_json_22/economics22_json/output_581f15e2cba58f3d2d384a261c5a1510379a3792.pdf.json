{
    "abstractText": "Can a principal still offer optimal dynamic contracts that are linear in end-of-period outcomes when the agent controls a process that exhibits memory? We provide a positive answer by considering a general Gaussian setting where the output dynamics are not necessarily semi-martingales or Markov processes. We introduce a rich class of principal-agent models that encompasses dynamic agency models with memory. From the mathematical point of view, we develop a methodology to deal with the possible non-Markovianity and non-semimartingality of the control problem, which can no longer be directly solved by means of the usual Hamilton-Jacobi-Bellman equation. Our main contribution is to show that, for one-dimensional models, this setting always allows for optimal linear contracts in end-of-period observable outcomes with a deterministic optimal level of effort. In higher dimension, we show that linear contracts are still optimal when the effort cost function is radial and we quantify the gap between linear contracts and optimal contracts for more general quadratic costs of efforts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Eduardo Abi Jaber"
        },
        {
            "affiliations": [],
            "name": "St\u00e9phane Villeneuve"
        }
    ],
    "id": "SP:bf0e0806433082355bdbe9640886ad743e89015e",
    "references": [
        {
            "authors": [
                "E. Abi Jaber",
                "M. Larsson",
                "S. Pulido"
            ],
            "title": "Affine Volterra processes",
            "venue": "The Annals of Applied Probability, 29(5):3155\u20133200.",
            "year": 2019
        },
        {
            "authors": [
                "J. Beran"
            ],
            "title": "Statistics for long-memory processes, volume 61 of",
            "venue": "Monographs on Statistics and Applied Probability, pages 38\u201372.",
            "year": 1994
        },
        {
            "authors": [
                "B. Biais",
                "T. Mariotti",
                "G. Plantin",
                "Rochet",
                "J.-C."
            ],
            "title": "Dynamic security design: Convergence to continuous time and asset pricing implications",
            "venue": "The Review of Economic Studies, 74(2):345\u2013390.",
            "year": 2007
        },
        {
            "authors": [
                "B. Biais",
                "T. Mariotti",
                "Rochet",
                "J.-C.",
                "S. Villeneuve"
            ],
            "title": "Large risks, limited liability, and dynamic moral hazard",
            "venue": "Econometrica, 78(1):73\u2013118.",
            "year": 2010
        },
        {
            "authors": [
                "P. Bolton",
                "M. Dewatripont"
            ],
            "title": "Contract theory",
            "venue": "MIT press.",
            "year": 2005
        },
        {
            "authors": [
                "G. Carroll"
            ],
            "title": "Robustness and linear contracts",
            "venue": "American Economic Review, 105(2):536\u201363.",
            "year": 2015
        },
        {
            "authors": [
                "G. Cisternas"
            ],
            "title": "Carrer concerns and the nature of skills",
            "venue": "American Economic Journal: Microeconomics, 10(2):152\u2013189.",
            "year": 2018
        },
        {
            "authors": [
                "G.L. Clementi",
                "H.A. Hopenhayn"
            ],
            "title": "A theory of financing constraints and firm dynamics",
            "venue": "The Quarterly Journal of Economics, 121(1):229\u2013265.",
            "year": 2006
        },
        {
            "authors": [
                "J. Cvitani\u0107",
                "D. Possam\u00e4\u0131",
                "N. Touzi"
            ],
            "title": "Dynamic programming approach to principal\u2013agent problems",
            "venue": "Finance and Stochastics, 22(1):1\u201337.",
            "year": 2018
        },
        {
            "authors": [
                "L. Decreusefond",
                "A.S. Ustunel"
            ],
            "title": "Stochastic analysis of the fractional Brownian motion",
            "venue": "Potential analysis, 10(2):177\u2013214.",
            "year": 1999
        },
        {
            "authors": [
                "P.M. DeMarzo",
                "M.J. Fishman"
            ],
            "title": "Optimal long-term financial contracting",
            "venue": "The Review of Financial Studies, 20(6):2079\u20132128.",
            "year": 2007
        },
        {
            "authors": [
                "P.M. DeMarzo",
                "M.J. Fishman",
                "Z. He",
                "N. Wang"
            ],
            "title": "Dynamic agency and the q theory of investment",
            "venue": "The Journal of Finance, 67(6):2295\u20132340.",
            "year": 2012
        },
        {
            "authors": [
                "P.M. DeMarzo",
                "Y. Sannikov"
            ],
            "title": "Optimal security design and dynamic capital structure in a continuous-time agency model",
            "year": 2006
        },
        {
            "authors": [
                "A. Edmans",
                "X. Gabaix"
            ],
            "title": "Tractability in incentive contracting",
            "venue": "The Review of Financial Studies, 24(9):2865\u20132894.",
            "year": 2011
        },
        {
            "authors": [
                "A. Edmans",
                "X. Gabaix",
                "T. Sadzik",
                "Y. Sannikov"
            ],
            "title": "Dynamic ceo compensation",
            "venue": "The Journal of Finance, 67(5):1603\u20131647.",
            "year": 2012
        },
        {
            "authors": [
                "J. Gatheral",
                "T. Jaisson",
                "M. Rosenbaum"
            ],
            "title": "Volatility is rough",
            "venue": "Quantitative Finance, 18(6):933\u2013949.",
            "year": 2018
        },
        {
            "authors": [
                "E. Green"
            ],
            "title": "Lending and the smoothing of uninsurable income",
            "venue": "Prescott, E. and Wallace, N., editors, Contractual arrangements for intertemporal trade, volume 1 of Minnesota studies in macroeconomics, pages 3\u201325. University of Minnesota Press, Minneapolis.",
            "year": 1987
        },
        {
            "authors": [
                "G. Gripenberg",
                "Londen",
                "S.-O.",
                "O. Staffans"
            ],
            "title": "Volterra integral and functional equations, volume 34 of Encyclopedia of Mathematics and its Applications",
            "venue": "Cambridge University Press, Cambridge.",
            "year": 1990
        },
        {
            "authors": [
                "M.F. Hellwig",
                "K.M. Schmidt"
            ],
            "title": "Discrete-time approximations of the holmstr\u00f6mmilgrom brownian-motion model of intertemporal incentive provision",
            "venue": "Econometrica, 70(6):2225\u20132264.",
            "year": 2002
        },
        {
            "authors": [
                "B. Holmstrom",
                "P. Milgrom"
            ],
            "title": "Aggregation and linearity in the provision of intertemporal incentives",
            "venue": "Econometrica: Journal of the Econometric Society, pages 303\u2013 328.",
            "year": 1987
        },
        {
            "authors": [
                "Y. Hu",
                "P. Imkeller",
                "M. M\u00fcller"
            ],
            "title": "Utility maximization in incomplete markets",
            "venue": "The Annals of Applied Probability, 15(3):1691\u20131712.",
            "year": 2005
        },
        {
            "authors": [
                "I. Karatzas",
                "S. Shreve"
            ],
            "title": "Brownian motion and stochastic calculus, volume 113",
            "venue": "Springer.",
            "year": 1991
        },
        {
            "authors": [
                "A.N. Kolmogorov"
            ],
            "title": "Wienersche spiralen und einige andere interessante kurven in hilbertscen raum, cr (doklady)",
            "venue": "Acad. Sci. URSS (NS), 26:115\u2013118.",
            "year": 1940
        },
        {
            "authors": [
                "Laffont",
                "J.-J.",
                "D. Martimort"
            ],
            "title": "The theory of incentives",
            "venue": "Princeton university press.",
            "year": 2009
        },
        {
            "authors": [
                "B.B. Mandelbrot",
                "J.W. Van Ness"
            ],
            "title": "Fractional brownian motions, fractional noises and applications",
            "venue": "SIAM review, 10(4):422\u2013437.",
            "year": 1968
        },
        {
            "authors": [
                "W.P. Rogerson"
            ],
            "title": "Repeated moral hazard",
            "venue": "Econometrica: Journal of the Econometric Society, pages 69\u201376.",
            "year": 1985
        },
        {
            "authors": [
                "H. Sch\u00e4ttler",
                "J. Sung"
            ],
            "title": "The first-order approach to the continuoustime principal-agent problem with exponential utility",
            "venue": "Journal of Economic Theory, 61(2):331\u2013371.",
            "year": 1993
        },
        {
            "authors": [
                "S.E. Spear",
                "S. Srivastava"
            ],
            "title": "On repeated moral hazard with discounting",
            "venue": "The Review of Economic Studies, 54(4):599\u2013617.",
            "year": 1987
        },
        {
            "authors": [
                "J. Sung"
            ],
            "title": "Linearity with project selection and controllable diffusion rate in continuous-time principal-agent problems",
            "venue": "The RAND Journal of Economics, pages 720\u2013743.",
            "year": 1995
        },
        {
            "authors": [
                "J. Thomas",
                "T. Worrall"
            ],
            "title": "Income fluctuation and asymmetric information: An example of a repeated principal-agent problem",
            "venue": "Journal of Economic Theory, 51(2):367\u2013 390.",
            "year": 1990
        },
        {
            "authors": [
                "N. Williams"
            ],
            "title": "Persistent private information",
            "venue": "Econometrica: Journal of the Econometric Society, 79(4):1233\u20131275.",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Keywords Principal-Agent models, Continuous-time control problems."
        },
        {
            "heading": "1 Introduction",
            "text": "The extensive literature analyzing the dynamic principal-agent problem has shown that it is important but difficult to design the optimal shape of contracts in a tractable way. Indeed,\n\u2217CMAP, Ecole Polytechnique Paris, route de Saclay 91128 Palaiseau Cedex \u2020Universite\u0301 Toulouse 1 Capitole (TSE-TSM-R), 1 esplanade de l\u2019universite\u0301, 31000 Toulouse, stephane.villeneuve@tse-fr.eu. The author acknowledges funding from ANR PACMAN and from ANR under grant ANR-17-EUR-0010 (Investissements d\u2019Avenir program) and gratefully thanks the FdR-SCOR \u201cChaire Marche\u0301 des risques et cre\u0301ation de valeurs\u201d. We are grateful to Thomas Mariotti and Takuro Yamashita for their comments and supports. We thank participants of Financial and Actuarial Mathematics Seminar at Liverpool, Financial and Insurance Mathematics at ETH,Math. Finance Seminar at Bielefeld, 10th AMaMeF Conference at Padova, Annual TSE-SCOR conference at Toulouse for their helpful comments and suggestions.\noptimal contracts in dynamic agency problems are generally defined as complex functionals of a stream of contractible variables, such as revenues. Moreover, as first identified by Rogerson (1985) (see also Laffont and Martimort (2009)) theoretical contracts exhibit memory, even in the most commonly used models that assume uncorrelated shocks, which unfortunately prevents them from matching real-world practices (see Bolton and Dewatripont (2005)). In addition, firms\u2019 revenues empirically show long memory1 and we lack a theoretical framework that justifies the signing of simple tractable contracts in an environment with inter-temporal links across time periods. In a Brownian setting, the breakthrough paper by Holmstrom and Milgrom (1987) (HM) shows that the optimal contract is linear in profits under some specific assumptions: the agent exerts effort continuously, principal and agent have CARA utilities, the agent bears a pecuniary cost of effort and finally the outcomes generated in the absence of effort are modeled by a fully observable Brownian motion. Since then, several attempts have been made to obtain closed-form contracts in environments that relax at least one of the assumptions of the HM model. Sung (1995) showed that the optimal contract is still linear when the agent also controls the variance of the output. Hellwig and Schmidt (2002) showed that linear contracts are nearly optimal in a discrete-time version of the HM model. Edmans and Gabaix (2011) and Edmans et al. (2012) obtain striking general results in a discrete-time model where none of the four hypotheses is retained but where the agent makes its decision in each period after having observed the noise. However, they focus primarily on the cheapest implementation of a particular action, rather than on the objective of maximizing the principal\u2019s preference. Another important recent contribution beyond the Holmstrom and Milgrom setting has been made by Carroll (2015) who showed that optimal contracts are linear in a general one-period model with uncertainty.\nIn this article, we enrich the Holmstrom and Milgrom modeling framework by going beyond the assumption that the revenues are driven by a Brownian motion.\nWe will instead consider Volterra Gaussian processes that are a generalization of the standard Brownian motion to study time-dependent effects. More precisely, a Volterra Gaussian process is a Wiener integral process with respect to a standard Brownian motion involving a deterministic integrand called -kernel. Thus, at every point in time, it is an infinite linear combination of i.i.d. Gaussian random variables with time-dependent coefficients. Although we are aware of the shortcomings of the three remaining HM assumptions2, our targeted choice is primarily motivated by the fact that, by allowing arbitrary integrand kernel functions in the Wiener integral, our Volterra Gaussian agency model encompasses agency models with short and long run autocorrelations. In particular, one of the main examples of Volterra Gaussian processes is the mean-reverting process which\n1In this paper, we use indifferently the terms long (short) memory and long (short)-range dependence see definition 2.1. page 42 in Beran (1994).\n2For instance, Edmans and Gabaix (2011) clearly argue there is ample evidence of decreasing absolute risk aversion, and many effort decisions do not involve a monetary expenditure\nallows us to get closer of recent models of dynamic contracts with persistence such as those developed in Williams (2011) or career concerns as in Cisternas (2018).\nFor a long time, Volterra Gaussian processes have been considered as a natural tool for modeling continuous phenomena with memory. In particular, the fractional Brownian motion (FBM), a Volterra Gaussian process with short and long range dependence, initially introduced by Kolmogorov (1940), was popularized by Mandelbrot and Van Ness (1968) in finance to model the empirically-validated long-term dependence of stock returns. More recently, a stream of literature suggested the use of variants of the fractional Brownian motion in stochastic volatility modeling to capture the roughness of the time series of the volatility of an asset which has been observed empirically in the market, see Gatheral et al. (2018); Abi Jaber et al. (2019) and the references therein. In general, such processes are non-Markovian and non-semimartingales, which make their study more intricate, both theoretically and practically, and prevents the use of standard stochastic calculus tools. Within the framework of optimal dynamic contracting theory, the continuous-time semimartingale models have received a lot of attention the past thirty years, when a significant progress has been made by relying on the recursive approach pioneered by Green (1987), Spear and Srivastava (1987), Thomas and Worrall (1990). Discrete-time models were first developed (see Clementi and Hopenhayn (2006); DeMarzo and Fishman (2007), followed by Biais et al. (2007)), while the breakthrough paper by Sannikov (2008) resulted in the recent study of dynamic contracting in continuous-time models (see also DeMarzo and Sannikov (2006); Biais et al. (2010); DeMarzo et al. (2012)). The main advantage of continuous-time semi-martingale models lies in the fact the procedure to find the optimal contract can be embedded in the standard theory of Markovian stochastic control using the theory of martingales and stochastic calculus, see Scha\u0308ttler and Sung (1993) for a general presentation and Cvitanic\u0301 et al. (2018) for a rigorous mathematical justification. Hence, under a fairly general set of assumptions, the contract can be characterized unambiguously by solving an Hamilton-Jacobi-Bellman equation where the so-called agent promised value plays the role of a state variable.\nIn allowing a non-semimartingale and non-Markovian setting, this paper makes an additional methodological contribution to solve for the optimal contract. The main idea is to use the so-called martingale optimality principle to study the agent and the principal problem sequentially as a Stackelberg game. The first step is to offer a class of incentivecompatible contracts by revisiting the martingale approach of Scha\u0308ttler and Sung (1993) and Sannikov (2008). The second step and our main contribution is to explicitly solve the principal problem which becomes a controlled stochastic Volterra problem. This requires the introduction of an auxiliary state variable - the effort-corrected forward output - which captures all the non-Markovianity and allows the application of the martingale optimality principle for the principal problem. In a one-dimensional setting, our key result is that the optimal contract is linear in the terminal value of the output, although the principal has in\ngeneral a coarser information than the agent. The optimal contract has interesting features. The slope or marginal value of the contract is independent of the output dynamics, only the intercept depends on the latter as a function of the optimal effort which is proportional to the kernel. Therefore, random parts of contracts signed in different one-dimensional Gaussian environments are identical although the required effort levels are environment specific, deterministic and exhibit interesting features in relation with the properties of the kernel.\nWe extend the paper with a discussion of the optimality of linear contracts in higher dimension. We address the multitask principal-agent problem in which a principal with CARA preferences hires a single agent with CARA preferences to perform different tasks. The outcome of each task is assumed to follow a Volterra Gaussian process whose evolution depends on the agent\u2019s continuous effort in each task while the profit is the sum of these different outcomes. Our main result is that there is no value in observing the agent\u2019s activities separately when the cost of effort is assumed to be a radial quadratic function. Under this assumption, the optimal contract only uses aggregate information and is still linear in the end-of-period outcome. When we consider a general effort cost function, we characterize the contract that would be optimal if the principal were able to observe the Brownian filtration and measure the utility gap when a less-informed principal forces herself to sign the best linear contract and identify factors that reduce the loss of utility associated with the use of linear contracts. This paper thus shows that linear contracts can closely achieve maximum principal utility in Gaussian environments."
        },
        {
            "heading": "2 The one-dimensional model",
            "text": "In this section, we present the economic model which is essentially an extension of the Holmstrom and Milgrom (1987) framework.\nGeneral Description: We consider a risk-averse investor, who owns a project and signs a fixed-term contract with a risk-averse manager, the latter being necessary to operate a project. Time is continuous and the time horizon is T > 0. In the absence of effort, the stochastic output process (Xt)t\u2264T of the project evolves up to time T as\nXt = g0(t) + \u222b t 0 K(t, s)dBs, (2.1)\nwhere B is a standard one-dimensional Brownian motion, g0 : [0, T ] \u2192 R is a measurable deterministic input function, K : [0, T ]2 \u2192 R is a measurable Volterra Kernel, i.e. K(t, s) = 0 for s \u2265 t such that\nsup t\u2264T \u222b T 0 K2(t, s)ds <\u221e.\nWe first observe that our model encompasses a large class of output dynamics that offers great modeling flexibility to model agency relationships in different sectors of the economy. Obviously, this setting contains the Holmstrom and Milgrom Brownian model by choosing g0(t) = x0 and K(t, s) = \u03c3 for any pair s < t for some constant \u03c3. Even more generally, the case of a time-dependent volatility can be recovered by setting K(t, s) = \u03c3(s)11s<t for some square-integrable function \u03c3. More interestingly, this framework also contains mean-reverting processes which are widely used to model output in the energy and mining sectors, if for instance, we choose g0(t) = e \u2212\u03bbtx0 + \u00b50 \u03bb (1\u2212e\n\u2212\u03bbt) and K(t, s) = e\u2212\u03bb(t\u2212s)11s<t, the output then follows the Ornstein-Ulhenbeck dynamics\ndXt = (\u00b50 \u2212 \u03bbXt) dt+ dBt.\nAnother example is the Brownian bridge pinned for instance in 0 at some time T0 > T which falls into this category with a kernel given by\nK(t, s) = T0 \u2212 t T0 \u2212 s 11s<t.\nThe Brownian bridge has the semi-martingale decomposition\ndXt = \u2212 Xt\nT0 \u2212 t dt+ dBt,\nand may be used in any situation where the agent has access to information about the future output. For example, it can be used to model the output of a seasonal crop which will end up being zero after the harvest season. A more striking example is given by the family of fractional Gaussian processes such as:\n\u2022 the Riemann-Liouville fractional Brownian motion where for s < t,\nK(t, s) = cH(t\u2212 s)H\u22121/2, H \u2208 (0, 1), for some constant cH .\n\u2022 the fractional Brownian motion whose covariance function is \u03a30(s, u) = 12(s 2H +\nu2H \u2212 |s \u2212 u|2H), for H \u2208 (0, 1), admits the Volterra representation (2.1) with the kernel\nK(t, s) = 1 s<t (t\u2212 s)H\u22121/2\n\u0393(H + 12) 2F1\n( H \u2212 1\n2 ; 1 2 \u2212H;H + 1 2 ; 1\u2212 t s\n) ,\nwhere 2F1 is the Gauss hypergeometric function, see Decreusefond and Ustunel (1999).\nBoth types of fractional Brownian motion do not fall into the semi-martingale and Markovian frameworks when the so-called Hurst parameter H is different than 1/2 (which corresponds to the case of the standard Brownian motion): they exhibit long range dependence\nwhen H > 1/2 and short run dependence when H < 1/2 and prove to be statistically very good models for industries related to power generation. Even more remarkably, equations with delay in the drift in the form of linear integrodifferential convolution equations:\ndXt =\n( h(t) + \u222b [0,t] \u00b5(ds)Xt\u2212s ) dt+ \u03c3dBt, (2.2)\nwith initial condition X0 \u2208 R, where h : [0, T ] \u2192 R, \u00b5 : B([0, T ]) \u2192 R of bounded variation, X0 \u2208 R and \u03c3 \u2208 R admit a unique solution in the form of a Gaussian Volterra process (2.1) for some specific choice of input curve g0 : [0, T ]\u2192 R and convolution kernel K : [0, T ]2 \u2192 R, see Appendix 7.2 for a detailed presentation of this observation. For instance, setting \u00b5(dt) = \u2211m k=1 ak\u03b4tk , we recover equations with delay. Such equations fall into the semi-martingale framework, but are clearly not Markovian. While very different in nature and in modeling objective, all these dynamics have in common to be Gaussian processes.\nAnother important observation about this framework relates to assumptions about the asymmetry in information, which has been interpreted by Holmstrom and Milgrom (1987) as a distinction between linear optimal contracts in outcomes X and those in accounts B. We denote by FB the augmented filtration generated by (Bt)t\u2264T and FX the one generated by the output process (Xt)t\u2264T . It readily follows from (2.1) that FX \u2282 FB. Hereafter, we assume that the agent has better information than the principal about the project in the sense that he has access to the full information FB while the principal observes only some aggregated information generated by the output FX . In general, these two filtrations do not coincide even in one-dimensional models as shown in the following example corresponding to a situation where the principal observes the output in a discretionary way.\nDiscrete observations of a Brownian motion: Assume Xt = f(t)Bt where f is a bounded function on [0, T ]. Observe that X is a Volterra process with K(t, s) = f(t)11s\u2264t. Consider a subdivision 0 < t1 < . . . < tn = T of the interval [0, T ] and let f be the function defined as a linear combination of unit impulses\nf(t) = N\u2211 i=1 11ti(t). (2.3)\nThe output process is purely discontinuous with Xti = Bti and Xt = 0 for t 6= ti and may correspond to a situation where the principal performs audits at regular intervals. Therefore, FX is strictly included in FB. We deduce that, even in a situation where the principal knows the agent is not exerting effort, the principal has a coarser information than the agent. In Volterra Gaussian models, we must therefore be careful that there may be asymmetric information between the principal and the agent regardless of the agency\nproblem we introduce below.\nAgency problem: We assume that the agent can exert a continuous effort (at)t\u2264T that modifies the probability distribution of X as follows\nXt = g0(t) + \u222b t 0 K(t, s)(as ds+ dB a s ), (2.4)\nwhere Ba is also a Brownian motion. As is customary in the agency theory literature, while the output process X is observable by both players, the effort is the agent\u2019s private information. The agent\u2019s cost for exercising some effort level a is modeled through a strictly convex C2 function k(a) satisfying k(0) = 0. To alleviate the exposition, we will assume hereafter that the effort cost function is quadratic,\nk(a) = \u03ba a2\n2 , for some \u03ba > 0. (2.5)\nHereafter and in accordance with the paper of Holmstrom and Milgrom (1987), we model the preferences of the principal and the agent with CARA utility functions that are given respectively by\nUP (x) := \u2212exp(\u2212\u03b3Px) and UA(x) := \u2212exp(\u2212\u03b3Ax), \u2200x \u2208 R.\nIn the beginning of the relationship, principal and agent agree on a contract of maturity T . To foster incentives, the contrat specifies a payment at time T which is modeled by a random variable \u03be that is supposed to be FXT measurable. We assume that both players can fully commit to the contract and that the agent has a reservation utility level R0 = UA(y0) < 0 below which he will refuse the contract. The latter inequality is referred to the participation constraint of the agent who has the option to reject a contract and enjoy a utility of autarky R0.\nDescription of the probabilistic background: For completeness, we recall the rigorous formulation of the agency problem in order to make understandable the first-order conditions that we will give in the next section. Let (\u2126,F ,F := (Ft)t\u2264T ,P0) be a filtered probability space on which a F\u2013Brownian motion B := (Bt)t\u2264T is defined with natural (completed) filtration FB := (FBt )t\u2264T .\nThe firm\u2019s output or cash-flows observed by the principal are given by a stochastic process X with dynamics under P0,\nXt = g0(t) + \u222b t 0 K(t, s)dBs,\nThe impact of the agent\u2019s effort is modeled as a change of probability measure which changes the drift of the driving Brownian process. More precisely, agent\u2019s admissible actions are given by the following set\nA = { (at)t\u2264T F-progressively measurable: there exists A > 0 s.t. \u222b T\n0 a2sds \u2264 A, P0 \u2212 a.s.\n} .\nObserve that the set of admissible actions A is not empty because it contains bounded actions. Clearly, any admissible process a \u2208 A satisfies the Novikov\u2019s criterion\nE [ exp ( 1\n2 \u222b T 0 a2sds )] < +\u221e,\nwhich ensures that the process ( exp (\u222b T\n0 as dBs \u2212 1 2 \u222b T 0 a 2 s ds )) 0\u2264t\u2264T is a martingale, see\nKaratzas and Shreve (1991, Proposition 5.12 p. 198). We can therefore define a family of equivalent probability measures Pa by\ndPa dP0 = exp (\u222b T 0 as dBs \u2212 1 2 \u222b T 0 a2s ds ) ,\nwhere a ranges trough A. Under Pa, the process Ba = B\u2212 \u222b \u00b7\n0 as ds is a F\u2212Brownian motion by Girsanov theorem and X evolves as\nXt = g0(t) + \u222b t 0 K(t, s)(as ds+ dB a s ). (2.6)\nBecause, the effort is unobservable, the principal only observes the trajectory of the output process X, the deterministic curve g0 but not the last two terms of the decomposition (2.6) separately. Interestingly, in the case of general Volterra processes, this model leads to a novel simple setting where we have persistence of past efforts on the output variation. To understand this, let us imagine that the agent makes a constant effort a on the interval [0, t] and then stops exerting effort after t. Then, we have for h > 0, that\nE[(Xt+h \u2212Xt)|Ft] = g0(t+ h)\u2212 g0(t) + \u222b t\n0 (K(t+ h, s)\u2212K(t, s))(dBas + a ds),\nwhich induces persistence of past efforts on the future output increments whenever the functions K(t+h, .) and K(t, .) are not identical. Notice that in the HM model, the kernel is constant, so we recover that past efforts have no influence on future variations of the output.\nThe Principal-agent problem: It is well known that principal-agent relationships can be viewed as a Stackelberg game. The principal moves first by offering a contract that consists\nin a compensation \u03be, which belongs to the set of FXT measurable random variables, to the agent. The latter then reacts by choosing an effort policy based on the information available at each date inducing a probability measure Pa. For any given contract \u03be, let V A0 (\u03be) denote the agent\u2019s utility at time 0 which is defined as\nV A0 (\u03be) := sup a\nEa ( UA ( \u03be \u2212 \u222b T 0 k(as) ds )) (2.7)\nrecall the definition of k in (2.5). As common in agency problems, we define the concept of incentive-compatible contracts.\nDefinition 2.1. A contract \u03be is said to be incentive compatible if V A0 is finite and if there exists an effort policy a\u2217(\u03be) \u2208 A that maximizes (2.7), i.e.\nV A0 (\u03be) = Ea \u2217(\u03be) ( UA ( \u03be \u2212 \u222b T 0 k(a\u2217s(\u03be)) ds )) .\nIt is critical to understand what incentive-compatible contracts are, as these are the ones for which the principal can enforce desirable efforts. As common in the literature, we will focus on a class \u039e of contracts \u03be that are incentive-compatible (IC). Before defining rigorously the class of IC contracts \u039e we will focus on, we clarify the principal\u2019s problem. By offering an incentive-compatible contract \u03be \u2208 \u039e, the principal will be able to anticipate the optimal effort level a\u2217(\u03be). Hence, she will propose an incentive-compatible contract that maximizes the expected value of her CARA preference. Then, her aim is to solve\nV P0 := sup \u03be\u2208\u039e\nEa \u2217(\u03be) [UP (XT \u2212 \u03be)] , (2.8)\nunder the participation constraint Ea\u2217(\u03be) ( UA ( \u03be \u2212 \u222b T 0 k(a \u2217 s(\u03be)) ds )) \u2265 R0. The first result of this paper is given by the following theorem which shows that the problem (2.8) admits an optimal contract which is linear in end-of-period outcomes. The result of the Holmstrom-Milgrom model thus extends to all Gaussian Volterra processes, even though these may exhibit very different statistical properties. Following Scha\u0308ttler and Sung (1993), we introduce the class of contracts we will focus on. Let us define\nf\u2217(z) := \u03b3A 2 |z|2 + inf a\u2208R {k(a)\u2212 az} = \u03ba\u03b3A \u2212 1 2\u03ba z2, (2.9)\nand consider the following class \u039e of Incentive Compatible contracts, see Proposition 3.2 below,\n\u039e = {\u03be = Y (y,\u03b2)T \u2208 F X T , where y \u2265 y0, \u03b2 = (\u03b2t)t\u2264T \u2208 A and Y (y,\u03b2) T = y+ \u222b T 0 f\u2217(\u03b2s)ds+ \u222b T 0 \u03b2sdBs}.\nWe have:\nTheorem 2.1. The optimal contract \u03be\u2217 that maximizes the principal problem (2.8) is linear in end-of-period profits XT and is given by\n\u03be\u2217 = y0 \u2212 \u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba g0(T ) + \u03ba\u03b3A \u2212 1 2\u03ba \u222b T 0 (\u03b2\u2217s ) 2 ds+\n\u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba XT , (2.10)\nand the optimal level of recommended effort a\u2217 that maximizes the agent\u2019s problem (2.7) is deterministic and given by a\u2217 = \u03b2 \u2217\n\u03ba with\n\u03b2\u2217t = \u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba K(T, t), t \u2264 T. (2.11)\nProof. See Section 4.\nSimilarly to HM, the optimal compensation is made up of a deterministic base salary y0 \u2212 \u03b3P+1/\u03ba\u03b3A+\u03b3P+1/\u03bag0(T ) + \u03ba\u03b3A+1 2\u03ba \u222b T 0 (\u03b2 \u2217 s )\n2 ds and a random compensation to foster incentives \u03b3P+1/\u03ba \u03b3A+\u03b3P+1/\u03ba XT . One of the striking results is, when agents have CARA preferences, the incentive part of the optimal contract, through the performance-based bonus coefficient \u03b3P+1/\u03ba \u03b3A+\u03b3P+1/\u03ba , is common to all one-dimensional Volterra Gaussian models and thus independent of the output dynamics, even though they have very different statistical properties. Only the base salary is industry-specific depending on the output dynamic through the Volterra kernel K. The optimal effort level is deterministic and firm-specific and can, depending on the choice of the Volterra kernel, exhibit interesting behaviors. For instance, for the mean-reverting dynamics, i.e. K(t, s) = e\u2212\u03bb(t\u2212s)11s<t, the optimal effort is increasing if the mean-reverting intensity \u03bb is positive. The closer one gets to contract maturity, the more work the agent has to do. The intuition is that the optimal effort should compensate for the natural tendency of the process to revert to its long-term average. The closer the contract is to maturity, the greater the effort should be to allow X to deviate from its long-term average and thus allow the principal to benefit from a greater profit. When the mean-reverting intensity is negative, the effort must be greater at the beginning of the contract in order to give the necessary impetus to the process to diverge towards large positive values. Once this momentum is established, it is less effective to ask the agent to work.\nThe following two sections are dedicated to proving Theorem 2.1. Section 3 solves the agent problem, while section 4 solves the principal problem. An extension of Theorem 2.1 to the multi-dimensional set-up is considered in Section 5."
        },
        {
            "heading": "3 The one-dimensional agent problem",
            "text": "This section aims at completely solving the problem of the Agent in (2.7). The ideas developed here are not new, they rely on the martingale approach to stochastic control already used in Scha\u0308ttler and Sung (1993) which we adapt to develop the first-order approach to\nprincipal-agent problems in a continuous-time Gaussian setting with exponential utilities. We will show that the class \u039e of contracts are incentive compatible contracts and design the optimal response of the agent for a given contract in \u039e. Our construction relies on the following martingale optimality principle that brings a clear intuition of the stochastic maximum principle used in the context of dynamic contracting by Williams (2011)."
        },
        {
            "heading": "3.1 The Martingale optimality principle",
            "text": "The Martingale optimality principle must be seen as a sufficient condition for a contract to be incentive-compatible. The following lemma, which is due to Hu et al. (2005) and proved in Appendix 7.1 for completeness, states this principle.\nLemma 3.1. Given a contract \u03be, suppose the existence of a family of stochastic processes Ra(\u03be) := (Rat )t\u2264T indexed by a \u2208 A such that the following four assertions hold\ni) RaT = UA\n( \u03be \u2212 \u222b T 0 k(as)ds ) , \u2200a \u2208 A,\nii) Ra\u00b7 is a ((Ft)t\u2208[0,T ],Pa)-supermartingale for every a in A,\niii) Ra0 is independent of a,\niv) there exists a\u2217 in A, such that Ra\u2217 is a (Ft)t\u2208[0,T ]-martingale.\nThen, \u03be is incentive compatible for the Agent problem (2.7) and a\u2217 is the agent best reply.\nIn the dynamic agency literature, the process (Ra \u2217 t )t\u2264T describes the Agent\u2019s expected utility given the contract \u03be. A contract \u03be is thus incentive compatible if we are able to build such a family Ra(\u03be). This will be done in the next section."
        },
        {
            "heading": "3.2 Enlarging the class of Incentive Compatible Contracts",
            "text": "In accordance with the result of Scha\u0308ttler and Sung (1993), we expect that the contracts belonging to \u039e are incentive compatible. It is at this point that a difficulty arises in our setting compared to the Brownian model of Holmstrom and Milgrom (1987) and more generally to the standard literature where the information sets of the two players coincide in the absence of moral hazard. Because the principal has a coarser information (recall that the paths of B are not always observable by the principal), she cannot in general implement the process (Y y,\u03b2t )t\u2264T given by\nY y,\u03b2t = y + \u222b t 0 f\u2217(\u03b2s)ds+ \u222b t 0 \u03b2sdBs, (3.1)\nfor y \u2265 y0 and \u03b2 \u2208 A, because Y y,\u03b2t fails to be FXt -measurable. In other words, the contracts in \u039e, that are the most natural to be incentive compatible are a priori inaccessible,\nunless we are able to characterize the controls \u03b2 \u2208 A that induce Y (y,\u03b2)T \u2208 FXT . Putting aside for a while this problem of information asymmetry between the two players, we consider a larger game where the principal is supposed to have the same information as the agent. We will forget for a while the constraint \u03be \u2208 FXT and introduce the enlarged set of contracts\n\u039e\u0302 = {\u03be = Y (y,\u03b2)T where y \u2265 y0, \u03b2 \u2208 A, Y (y,\u03b2) T = y + \u222b T 0 f\u2217(\u03b2s)ds+ \u222b T 0 \u03b2sdBs}\nand naturally extend Definition 2.1 of incentive-compatibility for FBT -measurable contracts. We have the following result that we prove for sake of completeness using the Martingale optimality principle.\nProposition 3.2. Let \u03be\u0302 \u2208 \u039e\u0302 be of the form\n\u03be\u0302 = y + \u222b T 0 f\u2217(\u03b2s)ds+ \u222b T 0 \u03b2sdBs,\nwith y \u2265 y0 and \u03b2 \u2208 A. Then, \u03be\u0302 is incentive compatible for the Agent problem in (2.7) and satisfies the participation constraint. Furthermore, the agent best reply is given by the effort a\u2217(\u03be\u0302) = \u03b2\u03ba and the utility of the agent at 0 is given by V A 0 (\u03be\u0302) = \u2212exp(\u2212\u03b3Ay).\nProof. Fix \u03be\u0302 \u2208 \u039e\u0302. Let y \u2265 y0 and \u03b2 \u2208 A such that \u03be\u0302 = y + \u222b T 0 f \u2217(\u03b2s)ds + \u222b T 0 \u03b2sdBs and define the process Y (y,\u03b2) by (3.1). For an admissible effort policy a = (at)t\u2264T \u2208 A, we define Ra as\nRat := \u2212exp ( \u2212\u03b3A ( Y (y,\u03b2) t \u2212 \u222b t 0 k(as)ds )) , t \u2208 [0, T ].\nWe will show that the family (Rat )t\u2264T satisfies condition i)\u2013iv) of the Martingale optimality principle of Lemma 3.1. Observe that Y (y,\u03b2) T = \u03be\u0302 so that Lemma 3.1-i) is satisfied. Also, Ra0 = \u2212exp(\u2212\u03b3Ay) is independent of a as needed in Lemma 3.1-iii). Furthermore, recalling that Ba = B \u2212 \u222b \u00b7 0 as ds, we note that\nY (y,\u03b2) t \u2212 \u222b t 0 k(as)ds = y + \u222b t 0 (f\u2217(\u03b2s)\u2212 k(as)) ds+ \u222b t 0 \u03b2sdBs\n= y + \u222b t 0 (f\u2217(\u03b2s)\u2212 k(as) + as\u03b2s) ds+ \u222b t 0 \u03b2sdB a s .\nUsing the definition of f\u2217 in (2.9), a completion of the squares in as yields the expression\nf\u2217(\u03b2s)\u2212 k(as) + as\u03b2s = \u2212 \u03ba 2 (as \u2212 a\u2217s) 2 + \u03b3A 2 \u03b22s\nwith a\u2217s := \u03b2s \u03ba so that combining the above leads to\nRat = \u2212exp (\u2212\u03b3Ay) exp (\u03b3A\u03ba\n2 (as \u2212 a\u2217s)\n2 )\nexp ( \u2212 \u03b32A 2 \u222b t 0 \u03b22sds\u2212 \u03b3A \u222b t 0 \u03b2sdB a s )\nIt remains to argue that the process Ma := exp ( \u2212\u03b3A \u222b \u00b7 0 \u03b2sdB a s \u2212 \u03b32A 2 \u222b \u00b7 0 \u03b2 2 s ds ) is a martingale under Pa. Indeed, if this is the case, then, since \u2212exp(\u03b3A\u03ba2 (as \u2212 a \u2217 s) 2 \u2264 \u22121,\nEa[RaT ] \u2264 \u2212exp(\u2212\u03b3Ay)Ea[MaT ] = \u2212exp(\u2212\u03b3Ay) = Ra0,\nwhich shows thatRa is a Pa-supermartingale for each a \u2208 A, which corresponds to condition Lemma 3.1-ii). Furthermore, for a = a\u2217, we have that Ra \u2217 is a Pa\u2217-martingale which gives Lemma 3.1-iv). Obtaining that Ma is a martingale under Pa is equivalent to proving that\nMt = exp (\u222b t 0 as dBs \u2212 1 2 \u222b t 0 a2s ds ) exp ( \u2212\u03b3A \u222b t 0 \u03b2s dB a s \u2212 \u03b32A 2 \u03b22s ds ) is a martingale under P0. But, observe that\nMt = exp (\u222b t 0 (as \u2212 \u03b3A\u03b2s) dBs \u2212 1 2 \u222b t 0 (as \u2212 \u03b3A\u03b2s)2 ds )\nwhich is a martingale for (at)t and (\u03b2t)t in A. An application of Lemma 3.1 shows that \u03be\u0302 is incentive compatible such that the agent best reply is given by the effort a\u2217s(\u03be\u0302) = \u03b2s \u03ba . Finally, since y \u2265 y0, the identity\nV A0 (\u03be) = Ea \u2217 [Ra \u2217 ] = \u2212exp(\u2212\u03b3Ay) \u2265 \u2212exp(\u2212\u03b3Ay0) = R0,\nshows that \u03be\u0302 satisfies the participation constraint by giving the required utility of the agent at 0, which concludes the proof.\nNotably, when the principal offers a contract parametrized by the pair (y, \u03b2), the agent best reply is \u03b2\u03ba and thus independent of y. This is due to the no wealth effect of CARA preferences. The agent utility is \u2212exp(\u2212\u03b3Ay) and thus independent of \u03b2. This is due to the agent\u2019s full commitment allowing the principal to choose the best incentive contract that binds the participation constraint. To sum up, restricting our attention to contracts in \u039e\u0302 transforms the puzzling principal\u2019s problem to a stochastic Volterra control problem, namely3\nVSB = sup y\u2265y0,\u03b2\u2208A\nE\u03b2 [ UP ( XT \u2212 Y y,\u03b2T )] = sup\n\u039e\u0302\nE [ UP ( XT \u2212 \u03be\u0302 )] . (3.2)\nwhere (Y y,\u03b2t )t\u2264T is given by (3.1). The principal problem (3.2) corresponds to the enlarged stochastic control problem where the principal would have access to the information generated by the Brownian motion. Clearly, the principal value (2.8) satisfies V P0 \u2264 VSB because of the inclusion \u039e \u2282 \u039e\u0302. In the one-dimensional Brownian model, Holmstrom and Milgrom (1987) show that the\n3 To alleviate notations, we will denote hereafter P\u03b2 , the probability corresponding to the agent\u2019s effort choice a = \u03b2\n\u03ba .\ntwo values coincide because the sets of information FB and FX are identical (\u039e = \u039e\u0302) and thus there is no need to introduce the enlarged control problem. Our contribution will be to show that the two values always coincide for one-dimensional Gaussian Volterra models, even if FX is strictly included in FB. This is the object of the next section."
        },
        {
            "heading": "4 The one-dimensional principal Gaussian problem",
            "text": "This section is devoted to the explicit resolution of the principal problem (3.2) and to the proof of Theorem 2.1. Contrary to the standard literature, the problem (3.2) is not a Markovian stochastic control problem because the process Xt is not necessarily Markov. More precisely, it corresponds to a stochastic Volterra control problem with the following controlled processes\nXt = g0(t) + 1\n\u03ba \u222b t 0 K(t, s)\u03b2sds+ \u222b t 0 K(t, s)dB\u03b2s ,\nY y,\u03b2t = y + \u03ba\u03b3A + 1\n2\u03ba\n\u222b t 0 \u03b22sds+ \u222b t 0 \u03b2sdB \u03b2 s .\nWe will show that the optimal second-best contract exists and is furthermore FXT -measurable. As a consequence, the second-best principal value VSB will coincide with the principal value V P0 . In other words, our main message is that there is no gain to the principal in acquiring more information than that generated by the observed output process X in one-dimensional Gaussian Volterra models, regardless of the definition of the kernel K. For instance, in the example of discrete observations of Brownian motion, i.e. K(t, s) = f(t)11s\u2264t with f as in (2.3), there is no gain to the principal in increasing the frequency of the discrete observations of the Brownian output.\nFor y \u2265 y0 and a control policy \u03b2 \u2208 A, we define J(y, \u03b2) = E\u03b2 [ exp ( \u2212\u03b3P ( XT \u2212 Y y,\u03b2T ))] ,\nin order to write the second-best principal problem\nVSB = inf y\u2265y0 VSB(y), with VSB(y) = inf \u03b2\u2208A J(y, \u03b2).\nThe rest of the section is dedicated to the proof of Theorem 2.1 that characterizes the optimal control for the principal problem (3.2). The idea of the proof of Theorem 2.1 is to apply again the martingale optimality principle. To do this, we need to introduce a good family of processes indexed by \u03b2. Inspired by the agent problem, one possibility would be to consider the following family\nexp ( \u2212\u03b3P ( Xt \u2212 Y y,\u03b2t )) .\nUnfortunately, it may be impossible to apply Ito\u0302\u2019s formula since the process X may not be a semi-martingale, as in the fractional Gaussian processes case. To get around this problem,\nwe introduce a new state variable that can be interpreted as a forward price which is a semi-martingale that coincides with X at date T . Let us define the effort-corrected forward output process by\ng\u03b2t (T ) = E \u03b2 [ XT \u2212 1\n\u03ba \u222b T t K(T, u)\u03b2udu | Ft ] .\nUsing the output dynamics (2.4) with effort \u03b2 \u2208 A, we have\ng\u03b2t (T ) = g0(T ) + 1\n\u03ba \u222b t 0 K(T, u)\u03b2udu+ \u222b t 0 K(T, u)dB\u03b2u .\nThen, we observe that the process (g\u03b2t (T ))t\u2264T is a semi-martingale on [0, T ) with dynamics\ndg\u03b2t (T ) = 1\n\u03ba K(T, t)\u03b2tdt+K(T, t)dB\n\u03b2 t (4.1)\nand terminal value g\u03b2T (T ) = XT . To apply the martingale optimality principle, we will consider the family of processes\nM\u03b2t = exp ( \u2212\u03b3P ( g\u03b2t (T )\u2212 Y y,\u03b2 t ) + \u03c6t ) ,\nwhere \u03c6 is the deterministic function given by\n\u03c6t = \u03b3P 2\n( \u03b32P \u2212 (\u03b3P + 1/\u03ba) 2\n(\u03b3A + \u03b3P + 1/\u03ba) )\u222b T t K(T, s)2ds.\nLemma 4.1, which is proved in Appendix 7.1, provides the dynamics of M\u03b2 that plays a key role in the determination of the optimal contract.\nLemma 4.1. For each \u03b2 \u2208 A, we have\ndM\u03b2t\nM\u03b2t = \u03b3P 2 (\u03b3A + \u03b3P + 1/\u03ba)(\u03b2t \u2212 \u03b2\u2217t )2dt+ (\u03b3P\u03b2t \u2212 \u03b3PK(T, t)) dB \u03b2 t , P \u03b2 \u2212 a.s. (4.2)\nwith \u03b2\u2217 given by (2.11).\nProof. See Appendix 7.1.\nWe can now complete the proof of Theorem 2.1.\nProof of Theorem 2.1. \u2022 The Principal\u2019s problem is solved by an application of the martingale optimality principle on the process M\u03b2. Fix \u03b2 \u2208 A and y \u2265 y0. We show that the family M\u03b2 satisfies the four assertions of the martingale optimality principle in Lemma 3.1. We have,\ni) For all \u03b2 \u2208 A, it follows from (4.2) that M\u03b2 is a P\u03b2-sub-martingale and thus M\u03b20 \u2264 E \u03b2 [ M\u03b2T ] = E\u03b2 [ exp ( \u2212\u03b3P ( XT \u2212 Y y,\u03b2T ))] = J(y, \u03b2),\nwhere we used that g\u03b2T (T ) = XT .\nii) Observe that M\u03b20 = exp (\u2212\u03b3P (g0(T )\u2212 y) + \u03c60) = M0 is independent of \u03b2.\niii) Finally, for \u03b2\u2217 given by (2.11), M\u03b2 \u2217 is a P\u03b2\u2217-martingale by (4.2) and thus, we have J(y, \u03b2\u2217) = E\u03b2 [ M\u03b2 \u2217\nT\n] = M\u03b2 \u2217\n0 = M0 \u2264 J(y, \u03b2), \u03b2 \u2208 A,\nwhich shows that \u03b2\u2217 is the optimal control for the second-best principal problem and the principal value is given by VSB(y) = M0.\nOptimizing on y \u2265 y0, yields VSB = exp (\u2212\u03b3P (g0(T )\u2212 y0) + \u03c60). Furthermore, since \u03b2\u2217t is proportional to the Volterra Kernel K(T, t), it is straightforward to obtain the linear form of the contract \u03be\u2217 = Y y0,\u03b2 \u2217\nT as in (2.10). In particular, \u03be \u2217 is FXT -measurable as an affine\nfunction of XT . Therefore, the optimal control for the enlarged principal problem (3.2) induces an optimal contract in \u039e, so that VSB = V P 0 . \u2022The Agent\u2019s problem. An application of Proposition 3.2 yields that the optimal level of recommended effort a\u2217 that maximizes the agent\u2019s problem (2.7) is given by a\u2217 = \u03b2 \u2217\n\u03ba .\nTo sum up, this study shows that the transition from Brownian to Volterra models preserves the optimality of linear in end-of-period profit contracts. Moreover, in onedimensional models, the principal does not suffer from having a coarser information than the agent about the dynamics of the production process. Aggregating production over time is sufficient for optimal compensation in Volterra Gaussian environments and there is no need to use all the available information - the brownian path in our setting- to design an optimal contract. The next section deals with the robustness of this result in the multidimensional set-up."
        },
        {
            "heading": "5 The multi-dimensional model",
            "text": "So far, we have assumed that the shocks are modeled by a one-dimensional Brownian motion. In this section, we present a tractable class of multitask principal-agent problems, such as the one faced by a firm with a manager that supervises several projects. This model amounts to study the principal-agent problem in the case where the shocks are modeled by a standard Brownian motion of dimension d, that we also denote by (Bt)t\u2264T . As in Holmstrom and Milgrom, the i-th component Bit of Bt is interpreted as the outcome of the i-th account of the firm. We model the aggregate output or profit of the firm as follows\nXt = g0(t) + \u222b t 0 < K(t, s), dBs >,\nwhere < \u00b7, \u00b7 > denotes the canonical inner product in Rd, g0 is a deterministic function and K : [0, T ]2 \u2192 Rd is a measurable Volterra Kernel, i.e. K(t, s) = 0 for s \u2265 t such that\nsup t\u2264T \u222b T 0 ||K(t, s)||2ds <\u221e.\nFor instance, the caseK(t, s) = \u03c311s\u2264t, \u03c3 \u2208 Rd corresponds to the Brownian model in Holmstrom and Milgrom (1987, Section 4). We can also consider a mining company that exploits two different types of minerals in two different mines. Each component of X represents the revenue of a mine. This may correspond to the choice K(t, s) = (e\u2212\u03bb1(t\u2212s), e\u2212\u03bb2(t\u2212s)) where each separate outcomes follows a mean-reverting process with two different mean-reverting intensity \u03bbi, i = 1, 2. Even more importantly than in the one-dimensional case, the filtration generated by the output FX is strictly included in the filtration generated by the multi-dimensional Brownian motion FB and thus, the principal has always a coarsest information than the agent even when the latter does not exert any hidden effort. This observation is central to Holmstrom and Milgrom\u2019s distinction between the optimal contracts that are linear in profits or in accounts and more generally to understand when it is useless to use all the information generated by the Brownian motion. It is useful to recall here that we focus in this paper on contracts that are FXT measurable. In a similar way to the one-dimensional case, we assume that the agent can exert a continuous vector of effort (at)t\u2264T \u2208 Rd, ait being the effort made by the manager to improve the account i, that modifies the probability distribution of X as follows:\nXt = g0(t) + \u222b t 0 < K(t, s), dBs + as ds >,\nSimilarly to the one-dimensional case, we say that an agent\u2019s action a = (at)t is admissible if a = (at)t is F-progressively measurable and such that there exists A > 0 such that\u222b T\n0 ||as||2 ds < A, P0 \u2212 a.s.\nStill denoting by A the set of admissible actions, we define for any a \u2208 A a family of equivalent probability measures Pa by\ndPa dP0 = exp (\u222b T 0 < as, dBs > \u2212 1 2 \u222b T 0 ||as||2 ds ) .\nUnder Pa, the process Ba = B\u2212 \u222b \u00b7\n0 as ds is a F\u2212Brownian motion and the output dynamics is\nXt = g0(t) + \u222b t 0 < K(t, s), dBas + as ds > .\nWe also assume that the agent incurs an instantaneous cost k(a) where k is a convex function on Rd with k(0Rd) = 0. When the kernel is a constant vector, Holmstrom and Milgrom have considered the case where the effort cost function is k(a) = g( \u2211d i=1 ai) with g strictly convex and have showed that the optimal compensation is linear in profit in that case. Nevertheless, this specification does not allow us to determine the optimal effort that the agent must make in each of his tasks. In this section, we will rather consider a quadratic effort cost function\nk(a) = 1\n2 < a,\u0393a >,\nwhere \u0393 is a symmetric positive-definite matrix. When \u0393 is proportional to the identity matrix, i.e. \u0393 = \u03baId for some \u03ba > 0, we say that the effort cost function is radial, because, in this case, the effort cost function is proportional to the norm of the vector a. A radial cost is to assume that the effort costs are not specific to the different tasks that define the accounts.\nIn the sequel, we will highlight the interplay between the choice of the matrix \u0393 and the optimality of linear contracts. In a nutshell, our main results of this section in the multi-dimensional framework can be summarized as follows:\n\u2022 If \u0393 is proportional to the identity matrix, then the optimal contract \u03be\u2217 is linear in the end-of-period profit XT . As in the one-dimensional model, the principal does not have to worry about her lack of information to sign an optimal contract.\n\u2022 For more general matrices \u0393, the optimal contract \u03be\u2217 is no longer linear in the end-ofperiod profit XT . More importantly, \u03be\n\u2217 is not necessarily FXT measurable, meaning that the less-informed principal cannot implement/sign the contract \u03be\u2217. In this situation, we quantify the gap between such contract \u03be\u2217 and the best linear contract that can be implemented by the principal. The gap can be interpreted as the value of information."
        },
        {
            "heading": "5.1 The agent\u2019s problem",
            "text": "From a methodological viewpoint, there is no hurdle to adapt the techniques developed in Section 3. As a consequence, we will roughly repeat the approach detailed in Section 3 to apply the martingale optimality principle and consider a class of incentive-compatible contracts. To do this, we assume for a while that the principal has access to the Brownian filtration generated by (Bt)t and can implement the controlled process\nY y,\u03b2t = y + \u222b t 0 f\u2217(\u03b2s)dt+ \u222b t 0 < \u03b2s, dBs >, (5.1)\nwith y \u2265 y0 and \u03b2 \u2208 A, and for z \u2208 Rd,\nf\u2217(z) = \u03b3A 2 ||z||2 + inf\na\n( 1\n2 < a,\u0393a > \u2212 < a, z >\n) = 1\n2 < z,\n( \u03b3AId \u2212 \u0393\u22121 ) z >\nto offer the wage \u03be = Y y,\u03b2T which is FBT measurable. For a given contract \u03be defined by (y, \u03b2) \u2208 [y0,\u221e) \u00d7 A, the agent has to determine his best response a\u2217(\u03be). To apply the martingale optimality principle, we introduce the family of processes indexed by a given by\nRat = \u2212exp ( \u2212\u03b3A ( Y y,\u03b2t \u2212 \u222b t 0 1 2 < as,\u0393as > ds )) The first-order condition gives the agent\u2019s best effort, a\u2217(\u03be)t = \u0393\n\u22121\u03b2t. Furthermore, the agent utility at time 0 is given by V A0 (\u03be) = R a 0 = \u2212exp(\u2212\u03b3Ay). We collect the result in the following proposition, which is the analogue of Proposition 3.2 in the multi-dimensional framework.\nProposition 5.1. Let \u03be\u0302 be a contract of the form\n\u03be\u0302 = y + \u222b T 0 f\u2217(\u03b2s)ds+ \u222b T 0 < \u03b2s, dBs >,\nwith y \u2265 y0 and \u03b2 \u2208 A. Then, \u03be\u0302 is incentive compatible for the Agent problem (2.7) and satisfies the participation constraint. In particular, the agent best reply is given by the effort a\u2217(\u03be\u0302) = \u0393\u22121\u03b2 and the agent utility at time 0 is given by V A0 (\u03be\u0302) = \u2212exp(\u2212\u03b3Ay).\nWe can again re-write the Principal\u2019s problem as Volterra stochastic optimal control problem on the process\nY y,\u03b2T = y + 1\n2 \u222b T 0 < \u03b2t, ( \u03b3AId+ \u0393 \u22121)\u03b2t > dt+ \u222b T 0 < \u03b2t, dB \u03b2 t >,\nwhere the stochastic process B\u03b2 = (B\u03b2t )t is a d-dimensional Brownian motion under the probability measure indexed by a\u2217t = \u0393\n\u22121\u03b2t that we will denote hereafter P\u2217. Then, without asymmetry of information, the enlarged principal problem is given by\nVSB = sup y\u2265y0 VSB(y),\nwith\nVSB(y) = sup \u03b2\u2208A\nE\u2217 [ UP ( XT \u2212 Y y,\u03b2T )] (5.2)\nand is an upper bound for the principal problem (2.8) with the constraint \u03be \u2208 FXT ."
        },
        {
            "heading": "5.2 The Enlarged Principal problem",
            "text": "The idea is to mimic the methodology developed in details in the one-dimensional case. For this purpose, we reintroduce the effort-corrected forward output\ng\u03b2t (T ) = E [ XT \u2212 \u222b T t < K(T, s),\u0393\u22121\u03b2s > ds|Ft ]\nand apply the martingale optimality principle to the process M\u03b2t = exp ( \u2212\u03b3P ( g\u03b2t (T )\u2212 Y y,\u03b2 t ) + \u03c6t ) ,\nwhere \u03c6 is the deterministic function to determine.\nThe following theorem gives the optimal contract in a multi-dimensional setting for the enlarged principal problem.\nTheorem 5.1. Let \u0393 be symmetric positive-definite. The optimal level of effort \u0393\u22121\u03b2\u2217 that maximizes the enlarged principal\u2019s problem (5.2) is deterministic and \u03b2\u2217 is given by\n\u03b2\u2217t = ( (\u03b3A + \u03b3P ) Id + \u0393 \u22121)\u22121 (\u03b3P Id + \u0393\u22121)K(T, t), t \u2264 T. (5.3) The utility of the principal at time 0 is given by\nVSB = VSB(y0)\nwith\nVSB(y) = \u2212exp (\u2212\u03b3P (g0(T )\u2212 y) + \u03c60) (5.4)\nand\n\u03c60 = \u03b3P 2 \u3008KT ,\n( \u03b3P Id \u2212 ( \u03b3P Id + \u0393 \u22121) ((\u03b3A + \u03b3P ) Id + \u0393\u22121)\u22121 (\u03b3P Id + \u0393\u22121))KT \u3009L2 , where KT (s) := K(T, s) and \u3008f, g\u3009L2 := \u222b T 0 < f(s), g(s) > ds. The optimal contract \u03be \u2217 that maximizes the principal problem is given by\n\u03be\u2217 = y0 + \u222b T 0 f\u2217(\u03b2\u2217s )ds+ \u222b T 0 < \u03b2\u2217s , dBs > . (5.5)\nProof. See Appendix 7.1.\nWe now make two important observations.\nRemark 5.2. \u2022 We note that in general, the optimal contract (5.5) is not linear in XT , indeed the term \u222b T 0 < \u03b2 \u2217 s , dBs\u3009 with \u03b2\u2217 given by (5.3) cannot be expressed in\nterms of the integral \u222b T\n0 < K(T, s), dBs >.\n\u2022 More importantly, \u03be\u2217 is measurable with respect to FBT but not necessarily with respect to the smaller filtration FXT , which means that such contract cannot be implemented by the less-informed principal.\nThe following corollary shows that if the cost is radial then the optimal contract is linear in XT and it can therefore be implemented by the principal. In Section 5.3 below, we study a class of optimal linear implementable contracts for the principal in case the cost is not radial.\nCorollary 5.2. Assume that the effort cost function is radial, i.e. \u0393 = \u03baId for some \u03ba > 0. Then, the optimal level of effort that maximizes the enlarged principal\u2019s problem (5.2) is deterministic and given by\n\u03b2\u2217t = \u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba K(T, t)."
        },
        {
            "heading": "In particular, the optimal contract \u03be\u2217 is linear in profits and given by",
            "text": "\u03be\u2217 = y0 \u2212 \u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba g0(T ) +\n\u03b3A + 1/\u03ba\n2\n\u222b T 0 < \u03b2\u2217s , \u03b2 \u2217 s > ds+\n\u03b3P + 1/\u03ba\n\u03b3A + \u03b3P + 1/\u03ba XT .\nFurthermore, VSB = V P 0 . Proof. The expression for \u03b2\u2217 and \u03be\u2217 follow directly from Theorem 5.1. In particular, \u03be\u2217 is FXT -measurable as an affine function of XT . Therefore, the optimal control for the enlarged principal problem (5.2) induces a contract which is implementable by the principal, so that VSB = V P 0\nThe message of Corollary 5.2 is very simple. When an agent has to allocate his time on several tasks and when his effort cost function is not specific to tasks and thus measured by the norm of the vector a, it is not necessary for the principal to scrutinize the revenues of each activity. It is sufficient to sign a linear contract in the final value of the aggregate profits to give the optimal incentives without regard to the optimal level of information. When the effort cost function is radial, the principal need not observe the paths of individual accounts to offer an optimal compensation that is linear in profits. With this specification, we can disentangle the optimal efforts to allocate to the different tasks because the ith component to the optimal effort is proportional to the ith component of the kernel. In order to illustrate how an agent should optimally allocate his time to the different tasks he has to perform, let us consider the following toy example. A salesperson must visit two clients in two different geographical areas. We assume that the first geographical area generates Brownian outcomes and the second area generates mean-reverting outcomes. The firm\u2019s aggregate output is given by\nXt = B 1 t + \u222b t 0 e\u2212\u03bb(t\u2212s) dB2s , with \u03bb > 0.\nWhile the share of the output that goes to the agent is independent of the parameter \u03bb, the saleperson must differentiate his customers visit. The first customer\u2019s visit must be on a constant basis and the second customer\u2019s visit must be accelerated as the maturity of the contract approaches."
        },
        {
            "heading": "5.3 The subclass of linear contracts and the value of information",
            "text": "In this section, we no longer assume that the cost is radial, we study the subclass of linear contracts, and we quantify the incurred loss on the utility of the principal. Beyond their simplicity, the main advantage of linear contracts is that they are FXT measurable and can therefore be implemented by the less-informed principal. We will define the value of information as the premium the principal would have to pay to access the agent\u2019s information and implement the optimal contract. We will consider contracts as in (5.1) with y \u2265 y0 but only for controls \u03b2 in the form\n\u03b2t = bK(T, t), for some b \u2208 R.\nNote that in this case, Proposition 5.1 still apply to ensure that such contracts are Incentive Compatible and the agent best reply is still \u0393\u22121\u03b2. Furthermore, for any b \u2208 R the contract Y y,bT is by construction linear in XT and given by\nY y,bT = y + \u222b T 0 f\u2217(bK(T, s))ds+ b(XT \u2212 g0(T )),\nso that b is the share of the output that goes to the agent. In this case, the principal will optimize on (y, b) to find the optimal linear contract\nVlin = sup y\u2265y0 Vlin(y),\nwith\nVlin(y) = sup b\u2208R\nE\u2217 [ UP ( XT \u2212 Y y,bT )] (5.6)\nA direct computation and optimization of the expectation leads to the following result for the optimal linear contract.\nTheorem 5.3. Let \u0393 be symmetric positive-definite. The optimal level of effort that maximizes the linear principal\u2019s problem (5.6) is given by \u0393\u22121\u03b2\u2217 with\n\u03b2\u2217t = b \u2217K(T, t), t \u2264 T, (5.7)\nb\u2217 = \u3008KT ,\n( \u03b3P Id + \u0393 \u22121)KT \u3009L2 \u3008KT , ((\u03b3A + \u03b3P ) Id + \u0393\u22121)KT \u3009L2 .\nThe utility of the principal at time 0 is given by\nVlin = Vlin(y0)\nwith\nVlin(y) = \u2212exp (\u2212\u03b3P (g0(T )\u2212 y) + \u03c70) (5.8)\nand\n\u03c70 = \u03b3P 2 \u3008KT ,\n( \u03b3P Id \u2212 b\u2217 ( \u03b3P Id + \u0393 \u22121))KT \u3009L2 . The optimal linear contract \u03be\u2217 that maximizes the linear principal\u2019s problem is given by\n\u03be\u2217 = y0 \u2212 b\u2217g0(T ) + \u222b T\n0 f\u2217(\u03b2\u2217s )ds+ b \u2217XT .\nProof. Fix y \u2265 y0, b \u2208 R and \u03b2t = bK(T, t). Then the random variable XT \u2212 Y y,bT reads\nXT \u2212 Y y,bT = g0(T )\u2212 y+ < KT , ( b\u0393\u22121 \u2212 b 2\n2\n( \u03b3AId + \u0393 \u22121))KT >L2 + (1\u2212 b)\n\u222b T 0 < K(T, s), dB\u03b2s >\nand is therefore Gaussian under P\u03b2. So that a direct computation of the Laplace transform of a Gaussian random variable yields\nE\u03b2 [ UP ( XT \u2212 Y y,bT )] = \u2212exp (\u2212\u03b3P (g0(T )\u2212 y) + F (b))\nwith\nF (b) =< KT , ( \u03b32P 2 Id \u2212 b ( \u03b32P Id + \u03b3P\u0393 \u22121)+ \u03b3P b2 2 ( (\u03b3A + \u03b3P )Id + \u0393 \u22121))KT >L2 . A direct maximization of F on b yields that the optimum is achieved for b\u2217 given by (5.7) and F (b\u2217) = \u03c70. Maximizing on y \u2265 y0 then gives Vlin = Vlin(y0).\nObviously, when the principal restricts to linear contracts, her utility at 0 satisfies Vlin(y) \u2264 VSB(y). It follows from (5.4) and (5.8) that \u03c70 \u2265 \u03c60. More precisely, one has\nVlin(y0) = VSB ( y0 +\n\u03c70 \u2212 \u03c60 \u03b3P\n) = VSB(y0)exp (\u03c70 \u2212 \u03c60) . (5.9)\nThe term exp (\u03c60 \u2212 \u03c70) = VSB(y0)Vlin(y0) lies in [0, 1] and can be interpreted as the value of information in the following way: since in general, the principal cannot implement the optimal contract in the enlarged filtration, recall Remark 5.2, she has to restrict to suboptimal, more simple but implementable contracts. The price to pay when she restricts to linear contracts, is the decrease of her utility by the factor exp (\u03c60 \u2212 \u03c70), which would correspond to the price to pay to access the optimal contract. Note that the agent utility at time 0 is unchanged compared to the previous section, and is still equal to exp(\u03b3Ay0) by Proposition 5.1 when the principal proposes the contract (y0, b \u2217).\nRemark 5.4. We note that contrary to the one dimensional setting, the coefficient b\u2217 in (5.7) depends in general on the kernel K. For the case of radial costs, i.e. \u0393 = \u03baId for some \u03ba > 0, one recovers from (5.7) that b\u2217 = (\u03b3P +1/\u03ba)/(\u03b3A+\u03b3P +1/\u03ba) which is independent of K. Note also that in this context, \u03c70 = \u03c60, so that the value of information vanishes in this case, that is linear contracts are optimal for the Principal\u2019s problem, recall Corollary 5.2.\nHaving characterized both the fully optimal contract and the optimal linear end-ofperiod contract, it remains to compare the performances of the two types of contracts. We implement this comparison by studying the sensitivity of the nonnegative difference \u03c70\u2212\u03c60 with respect to the input kernel K and the cost matrix \u0393. The smaller the quantity (\u03c70\u2212\u03c60), the more efficient the implementation of a linear contract, recall (5.9). The next proposition provides an upper bound for the value of information in terms of two quantities: the condition number4 of the matrix \u0393, denoted Cond(\u0393) and the L2-norm of the kernel K. The condition number Cond(\u0393) measures how sensitive is the effort cost function to changes in efforts.\nProposition 5.3. There exists a positive constant C independent of the dimension d, kernel K and terminal time T such that\n0 \u2264 \u03c70 \u2212 \u03c60 \u2264 C(Cond(\u0393)\u2212 1) \u222b T\n0 ||K(T, t)||2 dt. (5.10)\nProof. See Appendix 7.1.\nWhen the cost is radial, then Cond(\u0393) = 1 so that one recovers \u03c70 = \u03c60, meaning that linear contracts are optimal, recall Corollary 5.2. When Cond(\u0393) is close to one, it means that the agent\u2019s best reply in terms of effort, solution to the linear equation \u0393a\u2217 = \u03b2, is not very sensitive to errors in the principal control \u03b2. In that case, it is noticeable that the optimal linear contract is nearly optimal regardless of the Volterra process that drives the output. For convolution kernels of the formK(T, t) = 1 t<Tk(T\u2212t), \u222b T 0 \u2016K(T, t)\u2016 2dt = \u222b T 0 \u2016k(t)\u2016 2dt, so that the upper bound in (5.10) shrinks as the horizon of the contract T decreases, suggesting that linear contract seem more performant for short-term relationships compared to long-term relationships. Furthermore, for the exponential kernel k(t) = e\u2212\u03bbt with \u03bb \u2208 R, we have \u222b T 0 \u2016K(T, t)\u2016\n2dt = (1\u2212e\u22122\u03bbT )/2\u03bb, thus the higher is the mean-reverting intensity, the smaller the upper bound. For the fractional kernel k(t) = \u221a 2HtH\u22121/2 with H \u2208 (0, 1),\nwe have \u222b T\n0 \u2016K(T, t)\u2016 2dt = T 2H .\nWe now illustrate numerically the value of information exp(\u03c60 \u2212 \u03c70) using Equation (7.3) for exponential and fractional kernels with d = 2 and with a diagonal matrix for the cost efforts \u0393 = diag(\u03bb1, \u03bb2). First, we look at the case of two exponential kernels ki(t) = e\n\u2212\u03c1it with different mean reversions5 \u03c1i \u2208 R, i = 1, 2. 4The condition number of symmetric positive definite matrix S is the ratio \u03bbmax\n\u03bbmin where \u03bbmax (resp.\n\u03bbmin) is the largest (resp. smallest) eigenvalue of S. 5Note the change in notation to avoid confusion with the eigenvalues of \u0393.\nFigure 1 describes a situation where providing one unit of effort for Task 1 is more costly than for Task 2, \u03bb1 > \u03bb2, and when the mean-reverting parameters \u03c1i vary. The figure shows that linear contracts are more performant for negative and smaller mean reversions, which is the case that is usually of interest in practice. When the intensity of the most expensive task is fixed and positive, the variation of the intensity parameter of the least expensive task has very little effect on the value of the information. More generally, the value of information increases when \u03c12 increases. The linear contracts are very efficient (more than 90%) when the mean-reverting parameter of the most expensive task is one.\nFor our second example, we consider two fractional kernels ki(t) = \u221a 2Hit Hi\u22121/2 with Hi \u2208 (0, 1), we see on Figure 2 that for short maturities linear contracts are more performant for Hurst indices larger than 1/2 (long-memory processes), while for larger maturities they are more performant for values of H < 1/2, (short memory processes). The inflexion point at T = 1 is explained by the behavior of the variance of the fractional Brownian motion that reads \u222b T 0 \u2016K(T, t)\u2016 2dt = T 2H , see Figure 3."
        },
        {
            "heading": "6 Conclusion",
            "text": "The principal-agent framework generally leads to complex optimal contracts that do not align with real-world practices. Also, the proposal of a theoretical framework justifying the signing of simple (linear) optimal contracts deserves attention. In this paper, we have shown that the remarkable results of the Holmstrom and Milgrom model extend surprisingly to a large class of Gaussian models that exhibit memory: the Volterra processes. In particular,\nwe prove that optimal contracts are linear in one-dimensional models and that the principal has no incentives to expand its information set. In multi-dimensional models, this is no longer generally the case except when the effort cost function is radial. Nevertheless, we are able measure the utility gap when the principal proposes a linear contract in the case of a general effort cost function. Thus, we can examine the key features that make the performance of linear contracts very close to optimality."
        },
        {
            "heading": "7 Appendix",
            "text": ""
        },
        {
            "heading": "7.1 Proofs",
            "text": "Proof of Lemma 3.1. Let us consider any admissible effort policy a in A. Conditions i)-iv) immediately imply that\nEa [ UA ( \u03be \u2212 \u222b T 0 k(as)ds )] i) = Ea[RaT ] ii)\n\u2264 Ra0 iii) = Ra \u2217 0 iv) = Ea \u2217 [Ra \u2217 T ]\ni) = Ea \u2217 [ UA ( \u03be \u2212 \u222b T 0 k(a\u2217s)ds )] which concludes the proof.\nProof of Lemma 4.1. Denoting by U\u03b2t = \u2212\u03b3P ( g\u03b2t (T )\u2212 Y \u03b2 t ) + \u03c6t, an application of Ito\u0302\u2019s formula yields\ndM\u03b2t = M \u03b2 t ( dU\u03b2t + 1\n2 d\u3008U\u03b2\u3009t\n) .\nUsing (4.1), we obtain that\ndU\u03b2t = ( \u03c6\u0307t \u2212 \u03b3PK(T, t)\u03ba\u03b2t + \u03b3P \u03b3A + 1/\u03ba\n2 \u03b22t\n) dt+ (\u03b3P\u03b2t \u2212 \u03b3PK(T, t)) dB\u03b2t , P\u03b2 \u2212 a.s.\nso that\ndM\u03b2t\nM\u03b2t =\n( \u03c6\u0307t + 1\n2 \u03b32PK(T, t) 2 + \u03b3P 2 (\u03b3A + \u03b3P + 1/\u03ba)\u03b2 2 t \u2212 \u03b3P (\u03b3P + 1/\u03ba)K(T, t)\u03b2t\n) dt\n+ (\u03b3P\u03b2t \u2212 \u03b3PK(T, t)) dB\u03b2t , P\u03b2 \u2212 a.s.\nCompleting the squares in \u03b2 yields\n\u03b3P 2 (\u03b3A + \u03b3P + 1/\u03ba)\u03b2 2 t \u2212 \u03b3P (\u03b3P + 1/\u03ba)K(T, t)\u03b2t = \u03b3P 2 (\u03b32A + \u03b3P + 1/\u03ba) (\u03b2t \u2212 \u03b2\u2217t ) 2\n\u2212 \u03b3P 2\n(\u03b3P + 1/\u03ba) 2\n(\u03b3A + \u03b3P + 1/\u03ba) K(T, t)2,\nwith \u03b2\u2217 given by (2.11). Combining the above and using that\n\u03c6\u0307t = \u03b3P 2\n( (\u03b3P + 1/\u03ba) 2\n(\u03b3A + \u03b3P + 1/\u03ba) \u2212 \u03b32P\n) K(T, t)2\nyields (4.2).\nProof of Theorem 5.1. The incentive-compatible contract parametrized by (y, \u03b2) takes the form\ndY y,\u03b2t = 1\n2 < \u03b2t, D\u03b2t > dt+ < \u03b2t, dB\n\u03b2 t >, Y \u03b2 0 = y,\nwith D := \u03b3AId + \u0393 \u22121. Under the probability P\u03b2, the output process evolves as\nX\u03b2t = g0(t) + \u222b t 0 < K(t, s),\u0393\u22121\u03b2s > ds+ \u222b t 0 < K(t, s), dB\u03b2s > .\nFor s \u2265 t, define the effort-corrected forward output by\ng\u03b2t (s) = E \u03b2 [ X\u03b2s \u2212 \u222b s t < K(s, u),\u0393\u22121\u03b2u > du | Ft ] = g0(s) + \u222b t 0 < K(s, u),\u0393\u22121\u03b2u > du+ \u222b t 0 < K(s, u), dB\u03b2u > .\nNote that for fixed s \u2264 T , t 7\u2192 g\u03b2t (s) is a semimartingale on [0, s) with dynamics\ndg\u03b2t (s) =< K(s, t),\u0393 \u22121\u03b2t > dt+ < K(s, t), dB \u03b2 t > . (7.1)\nTo mimic the proof of the one-dimensional case, we will apply the martingale optimality principle to the process\nM\u03b2t = exp ( \u2212\u03b3P ( g\u03b2t (T )\u2212 Y \u03b2 t ) + \u03c6t ) ,\nwith\n\u03c6t = \u03b3P 2 \u222b T t < K(T, s), ( \u03b3P Id \u2212 ( \u03b3P Id + \u0393 \u22121) (\u03b3P Id +D)\u22121 (\u03b3P Id + \u0393\u22121))K(T, s) > ds. (7.2)\nDenoting by\nU\u03b2t = \u2212\u03b3P ( g\u03b2t (T )\u2212 Y \u03b2 t ) + \u03c6t,\nan application of Ito\u0302\u2019s formula yields\ndM\u03b2t = M \u03b2 t ( dU\u03b2t + 1\n2 d\u3008U\u03b2\u3009t\n) .\nUsing (7.1), we obtain that dU\u03b2t = ( \u03c6\u0307t \u2212 \u03b3P < K(T, t),\u0393\u22121\u03b2t > +\n\u03b3P 2 < \u03b2t, D\u03b2t >\n) dt\n+ \u03b3P < \u03b2t \u2212K(T, t), dB\u03b2t >, P\u03b2 \u2212 a.s.\nso that\ndM\u03b2t\nM\u03b2t =\n[ \u03c6\u0307t + 1\n2 \u03b32P \u2016K(T, t)\u20162 + \u03b3P 2 < \u03b2t, (\u03b3P Id +D)\u03b2t >\n\u2212 \u03b3P < K(T, t), (\u03b3P Id + \u0393\u22121)\u03b2t > ] dt\n+ \u03b3P < \u03b2t \u2212K(T, t), dB\u03b2t >, P\u03b2 \u2212 a.s.\nCompleting the squares in \u03b2 yields that\n\u03b3P 2 < \u03b2t, (\u03b3P Id +D)\u03b2t > \u2212\u03b3P < K(T, t), (\u03b3P Id + \u0393\u22121)\u03b2t >\nis equal to\n\u03b3P 2 < (\u03b2t \u2212 \u03b2\u2217t ) , (\u03b3P Id +D) (\u03b2t \u2212 \u03b2\u2217t ) >\n\u2212 \u03b3P 2 < K(T, t),\n( \u03b3P Id + \u0393 \u22121) (\u03b3P Id +D)\u22121 (\u03b3P Id + \u0393\u22121)K(T, t) >\nwith \u03b2\u2217 given by (5.3). Combining the above computations and using (7.2) yield\ndM\u03b2t\nM\u03b2t = \u03b3P 2 < (\u03b2t \u2212 \u03b2\u2217t ) , (\u03b3P Id +D) (\u03b2t \u2212 \u03b2\u2217t ) > dt\n+ \u03b3P < (\u03b2t \u2212K(T, t)) , dB\u03b2t >, P\u03b2 \u2212 a.s.\nWe then conclude the proof by proceeding analogously to the proof of Theorem 2.1 in Section 4 in the one-dimensional case by applying the Martingale optimality principle on M\u03b2.\nProof of Proposition 5.3. Let us define the symmetric positive definite matrix\nA = \u03b3P Id + \u0393 \u22121.\nThe matrix A is diagonalizable and thus, there exists an orthogonal matrix P such that\ntPAP = D = diag(\u03b71, . . . , \u03b7d)\nwith \u03b7i = \u03b3P + 1 \u03bbi where \u03bb1, . . . , \u03bbd are the eigenvalues of \u0393. We have\n\u03c70 \u2212 \u03c60 =< KT , A(\u03b3AId +A)\u22121AK > \u2212 < KT , AKT >\n2\n< KT , (\u03b3AId +A)KT > .\nSetting KT (t) = PK\u0302T (t) for every t \u2264 T and denoting by k\u0302i(t) the components of the vector K\u0302t(t), we obtain\n\u03c70 \u2212 \u03c60 =< K\u0302T (t), D(\u03b3AId +D)\u22121DK\u0302T (t) > \u2212 < K\u0302T (t), DK\u0302T (t) >\n2\n< K\u0302T (t), (\u03b3AId +D)K\u0302T (t) > ,\nor equivalently\n\u03c70 \u2212 \u03c60 = d\u2211 i=1 \u03b72i \u03b3A + \u03b7i \u222b T 0 k\u0302i(t) 2dt\u2212\n(\u2211d i=1 \u03b7i \u222b T 0 k\u0302i(t) 2dt )2\n\u2211d i=1(\u03b3A + \u03b7i) \u222b T 0 k\u0302i(t) 2dt . (7.3)\nThen, observing that\nd\u2211 i=1 \u222b T 0 k\u0302i(t) 2dt = \u222b T 0 \u2016tPK(T, t)\u20162dt = \u222b T 0 \u2016K(T, t)\u20162dt,\nsince P is orthogonal, we deduce that \u03c70 \u2212 \u03c60 \u2264 (\n\u03b72max \u03b3A + \u03b7min \u2212 \u03b7 2 min \u03b3A + \u03b7max )\u222b T 0 \u2016K(T, t)\u20162dt\nRearranging terms and using a3 \u2212 b3 = (a\u2212 b)(a2 + ab+ b2), we finally obtain \u03c70 \u2212 \u03c60 \u2264 C(\u03b7max \u2212 \u03b7min) \u222b T\n0 \u2016K(T, t)\u20162dt,\nfrom which we deduce the result by noting that\n\u03b7max \u2212 \u03b7min = 1\n\u03bbmax (Cond(\u0393)\u2212 1)."
        },
        {
            "heading": "7.2 Stochastic linear integro-differential convolution equations",
            "text": "In this section, we consider the stochastic linear integro-differential convolution equations of the form\ndXt =\n( h(t) + \u222b [0,t] \u00b5(ds)Xt\u2212s ) dt+ \u03c3dBt, (7.4)\nwith initial condition X0 \u2208 Rd, where h : [0, T ] \u2192 Rd, \u00b5 : B([0, T ]) \u2192 Rd\u00d7d of bounded variation, X0 \u2208 Rd and \u03c3 \u2208 Rd\u00d7d. We show that (7.4) admits a unique solution given by the Volterra Gaussian process\nXt = g0(t) + \u222b t 0 K(t, s)dBs\nfor some specific choice of input curve g0 : [0, T ]\u2192 Rd and convolution kernel K : [0, T ]2 \u2192 Rd\u00d7d.\nExample 7.1. Setting \u00b5(dt) = \u2211m\nk=1 ak\u03b4tk , we recover equations with delay.\nFor a Lebesgue measurable matrix-valued function f and a matrix-valued measure \u00b5 of locally bounded variation we define the convolutions f \u2217 \u00b5 and \u00b5 \u2217 f by\n(f \u2217 \u00b5)(t) = \u222b\n[0,t] f(t\u2212 s)\u00b5(ds), (\u00b5 \u2217 f)(t) = \u222b [0,t] \u00b5(ds)f(t\u2212 s).\nWe note that f \u2217 \u00b5 = \u00b5 \u2217 f when d = 1. We need the notion of the differential resolvent of \u00b5. For any bounded measure with locally finite variation \u00b5, the differential resolvent R : [0, T ] \u2192 Rd\u00d7d is the unique locally absolutely continuous function R such that\nR\u2032 = \u00b5 \u2217R = R \u2217 \u00b5, R(0) = Id. (7.5)\nWe refer to (Gripenberg et al., 1990, Theorem 3.1) for the existence and uniqueness statement regarding R. One observes that R is continuous and therefore bounded on [0, T ] so that the stochastic convolution \u222b \u00b7 0 R(\u00b7 \u2212 s)\u03c3dBs is well-dedined on [0, T ].\nTheorem 7.2. Let h : [0, T ] \u2192 Rd, \u00b5 : B([0, T ]) \u2192 Rd\u00d7d of bounded variation and X0 \u2208 Rd. The stochastic linear integrodifferential equation (7.4) admits a unique (continuous) solution on [0, T ] given by\nXt = g0(t) + \u222b t 0 R(t\u2212 s)\u03c3dBs, (7.6)\nwith\ng0(t) = R(t)X0 + \u222b t 0 R(t\u2212 s)h(s)ds,\nand R the differential resolvent of \u00b5.\nSketch of proof. \u2022 Uniqueness: we show that any solution X to (7.4) is given by (7.6). We first write (7.4) in compact integral form\nX = X0 + 1 \u2217 h+ 1 \u2217 (\u00b5 \u2217X) + 1 \u2217 \u03c3dB.\nThen, convolving both sides with R, and applying stochastic Fubini\u2019s theorem combined with the resolvent equation (7.5), leads to\nR \u2217X = R \u2217X0 +R \u2217 (1 \u2217 h) +R \u2217 (1 \u2217 \u00b5 \u2217X) +R \u2217 (1 \u2217 \u03c3dB) = 1 \u2217 (RX0) + 1 \u2217 (R \u2217 h) + (1 \u2217R \u2217 \u00b5) \u2217X + 1 \u2217 (R \u2217 \u03c3dB) = 1 \u2217 (RX0) + 1 \u2217 (R \u2217 h) +R \u2217X \u2212R(0)(1 \u2217X) + 1 \u2217 (R \u2217 \u03c3dB)\nSimplifying R\u2217X on both sides, recalling that R(0) = Id, and inspecting the densities lead to\nXt = R(t)X0 + \u222b t 0 R(t\u2212 s)h(s)ds+ \u222b t 0 R(t\u2212 s)\u03c3dBs, dt\u00d7 dP\u2212 a.e.\nThe conclusion follows from the continuity of the sample paths. \u2022 Existence: we verify that X given by (7.6) solves (7.4). We use the resolvent equation to write R = Id + 1 \u2217 (\u00b5 \u2217R) so that (7.6) reads\nX = X0 + 1 \u2217 ((\u00b5 \u2217R)X0 + h+ (\u00b5 \u2217R) \u2217 h+ (\u00b5 \u2217R) \u2217 \u03c3dB) + 1 \u2217 \u03c3dB,\nwhich shows that X is a continuous semimartingale with the following dynamics\ndXt = (h(t) + (\u00b5 \u2217 (RX0 +R \u2217 h+R \u2217 \u03c3dB)) (t)) dt+ \u03c3dBt = (h(t) + (\u00b5 \u2217X) (t)) dt+ \u03c3dBt\nwhere we made use of stochastic Fubini\u2019s theorem. The proof is complete."
        }
    ],
    "title": "Gaussian Agency problems with memory and Linear Contracts",
    "year": 2024
}