{
    "abstractText": "We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal\u2019s signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximatelybest-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal\u2019s optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal\u2019s obtainable utility in the learning model and the nonlearning model is bounded by the agent\u2019s regret (swap-regret). If the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can do significantly better than the non-learning model. These conclusions hold not only for Bayesian persuasion, but also for any generalized principal-agent problem with complete information, including Stackelberg games and contract design.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tao Lin"
        },
        {
            "affiliations": [],
            "name": "Yiling Chen"
        }
    ],
    "id": "SP:b2e1657946c0b54de9dc48845a7b3bb8a4be5b28",
    "references": [
        {
            "authors": [
                "Jerry Anunrojwong",
                "Krishnamurthy Iyer",
                "David Lingenbrink."
            ],
            "title": "Persuading RiskConscious Agents: A Geometric Approach",
            "venue": "Operations Research (March 2023), opre.2023.2438. https://doi.org/10.1287/opre.2023.2438",
            "year": 2023
        },
        {
            "authors": [
                "Eshwar Ram Arunachaleswaran",
                "Natalie Collina",
                "Jon Schneider."
            ],
            "title": "Pareto-Optimal Algorithms for Learning in Games",
            "venue": "arXiv:2402.09549 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Jean-Yves Audibert",
                "S\u00e9bastien Bubeck."
            ],
            "title": "Regret Bounds and Minimax Policies under Partial Monitoring",
            "venue": "Journal of Machine Learning Research 11, 94 (2010), 2785\u20132836. http://jmlr.org/papers/v11/audibert10a.html",
            "year": 2010
        },
        {
            "authors": [
                "Peter Auer",
                "Nicol\u00f2 Cesa-Bianchi",
                "Yoav Freund",
                "Robert E. Schapire."
            ],
            "title": "The Nonstochastic Multiarmed Bandit Problem",
            "venue": "SIAM J. Comput. 32, 1 (Jan. 2002), 48\u201377. https://doi.org/10.1137/S0097539701398375",
            "year": 2002
        },
        {
            "authors": [
                "Yakov Babichenko",
                "Inbal Talgam-Cohen",
                "Haifeng Xu",
                "Konstantin Zabarnyi."
            ],
            "title": "Regret-Minimizing Bayesian Persuasion",
            "venue": "Proceedings of the 22nd ACM Conference on Economics and Computation. ACM, Budapest Hungary, 128\u2013128. https://doi.org/10.1145/3465456.3467574",
            "year": 2021
        },
        {
            "authors": [
                "Daniel J Benjamin."
            ],
            "title": "Errors in probabilistic reasoning and judgment biases",
            "venue": "Handbook of Behavioral Economics: Applications and Foundations 1 2 (2019), 69\u2013186.",
            "year": 2019
        },
        {
            "authors": [
                "Avrim Blum",
                "Yishay Mansour."
            ],
            "title": "From External to Internal Regret",
            "venue": "Journal of Machine Learning Research 8, 47 (2007), 1307\u20131324. http://jmlr.org/papers/v8/blum07a.html",
            "year": 2007
        },
        {
            "authors": [
                "Mark Braverman",
                "Jieming Mao",
                "Jon Schneider",
                "Matt Weinberg."
            ],
            "title": "Selling to a No-Regret Buyer",
            "venue": "Proceedings of the 2018 ACM Conference on Economics and Computation. ACM, Ithaca NY USA, 523\u2013538. https://doi.org/10.1145/3219166.3219233",
            "year": 2018
        },
        {
            "authors": [
                "George W. Brown."
            ],
            "title": "Iterative Solution of Games by Fictitious Play",
            "venue": "Activity Analysis of Production and Allocation, T. C. Koopmans (Ed.). Wiley, New York.",
            "year": 1951
        },
        {
            "authors": [
                "Linda Cai",
                "S Matthew Weinberg",
                "Evan Wildenhain",
                "Shirley Zhang."
            ],
            "title": "Selling to Multiple No-Regret Buyers",
            "venue": "arXiv preprint arXiv:2307.04175 (2023).",
            "year": 2023
        },
        {
            "authors": [
                "Modibo K. Camara",
                "Jason D. Hartline",
                "Aleck Johnsen."
            ],
            "title": "Mechanisms for a No-Regret Agent: Beyond the Common Prior",
            "venue": "2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS). IEEE, Durham, NC, USA, 259\u2013270. https://doi.org/10.1109/FOCS46700.2020.00033",
            "year": 2020
        },
        {
            "authors": [
                "Colin Camerer."
            ],
            "title": "Bounded rationality in individual decision making",
            "venue": "Experimental economics 1, 2 (1998), 163\u2013183.",
            "year": 1998
        },
        {
            "authors": [
                "Joel Sobel"
            ],
            "title": "Strategic Information Transmission",
            "year": 1982
        },
        {
            "authors": [
                "Geoffroy de Clippel",
                "Xu Zhang"
            ],
            "title": "Non-Bayesian Persuasion",
            "venue": "Journal of Political Economy 130,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Deng",
                "Jon Schneider",
                "Balasubramanian Sivan."
            ],
            "title": "Strategizing against No-regret Learners",
            "venue": "Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
            "year": 2019
        },
        {
            "authors": [
                "Piotr Dworczak",
                "Alessandro Pavan."
            ],
            "title": "Preparing for the worst but hoping for the best: Robust (bayesian) persuasion",
            "venue": "(2020).",
            "year": 2020
        },
        {
            "authors": [
                "Yiding Feng",
                "Chien-Ju Ho",
                "Wei Tang."
            ],
            "title": "Rationality-robust information design: Bayesian persuasion under quantal response",
            "venue": "Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 501\u2013546.",
            "year": 2024
        },
        {
            "authors": [
                "Yiding Feng",
                "Wei Tang",
                "Haifeng Xu."
            ],
            "title": "Online Bayesian Recommendation with No Regret",
            "venue": "Proceedings of the 23rd ACM Conference on Economics and Computation. ACM, Boulder CO USA, 818\u2013819. https://doi.org/10.1145/3490486.3538327",
            "year": 2022
        },
        {
            "authors": [
                "Drew Fudenberg",
                "David K. Levine."
            ],
            "title": "The theory of learning in games",
            "venue": "Number 2 in MIT Press series on economic learning and social evolution. MIT Press, Cambridge, Mass.",
            "year": 1998
        },
        {
            "authors": [
                "Jiarui Gan",
                "Minbiao Han",
                "Jibang Wu",
                "Haifeng Xu."
            ],
            "title": "Robust Stackelberg Equilibria",
            "venue": "Proceedings of the 24th ACM Conference on Economics and Computation. ACM, London United Kingdom, 735\u2013735. https://doi.org/10.1145/3580507.3597680",
            "year": 2023
        },
        {
            "authors": [
                "Jiarui Gan",
                "Minbiao Han",
                "Jibang Wu",
                "Haifeng Xu."
            ],
            "title": "Generalized Principal-Agency: Contracts, Information, Games and Beyond",
            "venue": "arXiv:2209.01146 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Guru Guruganesh",
                "Yoav Kolumbus",
                "Jon Schneider",
                "Inbal Talgam-Cohen",
                "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
                "Joshua R. Wang",
                "S. Matthew Weinberg."
            ],
            "title": "Contracting with a Learning Agent",
            "venue": "arXiv:2401.16198 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Sergiu Hart",
                "Andreu Mas-Colell."
            ],
            "title": "A Simple Adaptive Procedure Leading to Correlated Equilibrium",
            "venue": "Econometrica 68, 5 (Sept. 2000), 1127\u20131150. https://doi.org/10.1111/1468-0262.00153",
            "year": 2000
        },
        {
            "authors": [
                "Shinji Ito."
            ],
            "title": "A Tight Lower Bound and Efficient Reduction for Swap Regret",
            "venue": "Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 18550\u201318559. https://proceedings.neurips.cc/paper_files/paper/2020/file/d79c8788088c2193f0244d8f1f36d2db-Pa",
            "year": 2020
        },
        {
            "authors": [
                "Emir Kamenica",
                "Matthew Gentzkow."
            ],
            "title": "Bayesian Persuasion",
            "venue": "American Economic Review 101, 6 (Oct. 2011), 2590\u20132615. https://doi.org/10.1257/aer.101.6.2590",
            "year": 2011
        },
        {
            "authors": [
                "B.H. Korte",
                "Jens Vygen"
            ],
            "title": "Combinatorial optimization: theory and algorithms (5th ed ed.). Number v. 21 in Algorithms and combinatorics",
            "year": 2012
        },
        {
            "authors": [
                "Svetlana Kosterina."
            ],
            "title": "Persuasion with unknown beliefs",
            "venue": "Theoretical Economics 17, 3 (2022), 1075\u20131107. https://doi.org/10.3982/TE4742",
            "year": 2022
        },
        {
            "authors": [
                "Rachitesh Kumar",
                "Jon Schneider",
                "Balasubramanian Sivan."
            ],
            "title": "Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions",
            "venue": "arXiv:2402.07363 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Yishay Mansour",
                "Mehryar Mohri",
                "Jon Schneider",
                "Balasubramanian Sivan"
            ],
            "title": "Strategizing against Learners in Bayesian Games",
            "venue": "In Proceedings of Thirty Fifth Conference on Learning Theory (Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Richard D. McKelvey",
                "Thomas R. Palfrey."
            ],
            "title": "Quantal Response Equilibria for Normal Form Games",
            "venue": "Games and Economic Behavior 10, 1 (July 1995), 6\u201338. https://doi.org/10.1006/game.1995.1023",
            "year": 1995
        },
        {
            "authors": [
                "Roger B Myerson."
            ],
            "title": "Optimal coordination mechanisms in generalized principal\u2013agent problems",
            "venue": "Journal of Mathematical Economics 10, 1 (June 1982), 67\u201381. https://doi.org/10.1016/0304-4068(82)90006-4",
            "year": 1982
        },
        {
            "authors": [
                "Denis Nekipelov",
                "Vasilis Syrgkanis",
                "Eva Tardos."
            ],
            "title": "Econometrics for Learning Agents",
            "venue": "Proceedings of the Sixteenth ACM Conference on Economics and Computation - EC \u201915. ACM Press, Portland, Oregon, USA, 1\u201318. https://doi.org/10.1145/2764468.2764522",
            "year": 2015
        },
        {
            "authors": [
                "James Renegar."
            ],
            "title": "Some perturbation theory for linear programming",
            "venue": "Mathematical Programming 65, 1-3 (Feb. 1994), 73\u201391. https://doi.org/10.1007/BF01581690",
            "year": 1994
        },
        {
            "authors": [
                "Aviad Rubinstein",
                "Junyao Zhao."
            ],
            "title": "Strategizing against No-Regret Learners in First-Price Auctions",
            "venue": "arXiv:2402.08637 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Tyler Lu",
                "David Pal",
                "Martin Pal."
            ],
            "title": "Contextual Multi-Armed Bandits",
            "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Vol. 9. PMLR, 485\u2013492. https://proceedings.mlr.press/v9/lu10a.html",
            "year": 2010
        },
        {
            "authors": [
                "Bernhard Von Stengel",
                "Shmuel Zamir."
            ],
            "title": "Leadership with commitment to mixed strategies",
            "venue": "Technical Report. Citeseer.",
            "year": 2004
        },
        {
            "authors": [
                "JibangWu",
                "Zixuan Zhang",
                "Zhe Feng",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Michael I. Jordan",
                "Haifeng Xu."
            ],
            "title": "Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning",
            "venue": "Proceedings of the 23rd ACM Conference on Economics and Computation. ACM, Boulder CO USA, 471\u2013472. https://doi.org/10.1145/3490486.3538313",
            "year": 2022
        },
        {
            "authors": [
                "Kunhe Yang",
                "Hanrui Zhang."
            ],
            "title": "Computational Aspects of Bayesian Persuasion under Approximate Best Response",
            "venue": "arXiv:2402.07426 [cs.GT]",
            "year": 2024
        },
        {
            "authors": [
                "Gabriel Ziegler."
            ],
            "title": "Adversarial bilateral information design",
            "venue": "Technical Report.",
            "year": 2020
        },
        {
            "authors": [
                "You Zu",
                "Krishnamurthy Iyer",
                "Haifeng Xu."
            ],
            "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
            "venue": "Proceedings of the 22nd ACM Conference on Economics and Computation. ACM, Budapest Hungary, 927\u2013928. https://doi.org/10.1145/3465456.3467593",
            "year": 2021
        },
        {
            "authors": [],
            "title": "|A|) (doubling trick + polyINF (Audibert and Bubeck, 2010)) for any time horizon T > 0. By swap-to-external regret reductions, they can be converted to multi-armed bandit algorithms with swap regret O",
            "year": 2010
        },
        {
            "authors": [],
            "title": "We then convert A into a contextual no-regret (contextual no-swap-regret) algorithm, in the following way: Algorithm 1: Convert any MAB algorithm to a contextual MAB algorithm Input: MAB algortihm A. Arm set A. Context set S",
            "venue": "(Ito,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n40 2.\n09 72\n1v 2\n[ cs\n.G T\n] 2\n2 Fe\nb 20"
        },
        {
            "heading": "1 Introduction",
            "text": "How well can one persuade through strategic information revelation when the receiver of the information does not act with textbook rationality? The equilibria of classic models of information design, such as Bayesian persuasion (Kamenica and Gentzkow, 2011) and cheap talk (Crawford and Sobel, 1982), depend on the receiver forming an accurate belief about the state of the world after receiving the sender\u2019s information (which requires the receiver accurately knowing the prior as well as the signaling scheme of the sender) and acting optimally based on his belief. These strong knowledge and rationality assumptions, often observed to be violated in practice (Camerer, 1998; Benjamin, 2019), are also key to many other principal-agent problems, including contract design and Stackelberg games.\nIn this work, using information design (Bayesian persuasion) as the main example, we study general principal-agent problems under an alternative behavioral model for the agent: learning. The use of learning as a behavioral model dates back to early economic literature on learning in games (Brown, 1951; Fudenberg and Levine, 1998) and has been actively studied recently in the computer science community (Nekipelov et al., 2015; Braverman et al., 2018; Deng et al., 2019; Mansour et al., 2022; Cai et al., 2023; Rubinstein and Zhao, 2024). A learning agent no longer has perfect knowledge of the prior or the principle\u2019s strategy. Instead of best responding, which is no\n\u2217Harvard University, tlin@g.harvard.edu \u2020Harvard University, yiling@seas.harvard.edu\nlonger possible or well-defined, the agent chooses his action based on past interactions with the principal. We focus on no-regret learning, which requires the agent to not feel a large regret due to not taking the optimal action in the end of repeated interactions with the principal. This is a mild requirement satisfied by many natural learning algorithms (e.g., \u03b5-greedy, MWU, UCB, Exp3) and can reasonably serve as a possible behavioral assumption for real-world agents.\nMain results Assuming a no-regret learning agent, we aim to characterize the principal\u2019s optimal strategy and payoff in principal-agent games. Our first result shows that, against a no-regret learning agent, the principal can always guarantee a payoff that is arbitrarily close to the principal\u2019s optimal payoff in the original principal-agent problem where the agent best responds. The difference between the principal\u2019s payoff in the learning model and the best-response model is at most O ( \u221a\nReg(T ) T\n) where Reg(T ) is the regret of the agent and T is the time horizon:\nTheorem 1.1 (Informal). Playing against a no-regret learning agent with regret Reg(T ) in T periods, the principal can always obtain an average payoff of at least U\u2217 \u2212O ( \u221a\nReg(T ) T\n) , where U\u2217\nis the principal\u2019s optimal payoff in the principal-agent problem with best response. To achieve this, the principal does not need to know the exact learning algorithm of the agent (knowing the regret bound Reg(T ) is sufficient) and the principal can simply use a fixed strategy across all periods.\nOur second result shows that, if the agent does a stronger version of no-regret learning, noswap-regret learning (Hart and Mas-Colell, 2000; Blum and Mansour, 2007), then the principal can never obtain any payoff that is significantly higher than the optimal payoff in the best-response model.\nTheorem 1.2 (Informal). Playing against a no-swap-regret learning agent with swap-regret SReg(T ) in T periods, the principal cannot obtain any payoff larger than U\u2217 +O (SReg(T )\nT\n) . This holds even\nif the principal knows the learning algorithm of the agent and uses time-varying strategies.\nFinally, we show that, if the agent uses some learning algorithm satisfying no-regret but not no-swap-regret, then there exist instances where the principal can do significantly better than the best-response optimal objective U\u2217. We show this in the Bayesian persuasion problem specifically:\nTheorem 1.3 (Informal). There exist Bayesian persuasion instances where, against a no-regret but not no-swap-regret learning agent, the principal can do significantly better than U\u2217.\nReduction from learning to approximate best response The main intuition behind our first two results is: principal-agent problems with a no-regret learning agent is closely related principalagent problems with an agent who approximately best respond to the principal\u2019s strategy. The no-regret learning behavior of the agent can be seen as a form of approximate best response, where the agent makes sub-optimal decisions with the sub-optimality measured by the regret. When the sub-optimality/regret is small, the principal-agent problem with approximate best response returns to the problem with exact best response. This explains why the principal against a noregret learning agent can obtain a payoff that is close to the payoff with a best-responding agent, and cannot obtain any payoff better than that.\nHowever, formalizing the exact connection between learning and approximate best response is tricky. In particular, since no-regret (no-swap-regret) learning algorithms are randomized, they correspond to approximately-best-responding agents who use randomized strategies. We find that, against an agent using randomized \u03b4-approximately-best-responding strategies, the principal can\nand can only obtain payoff in [U\u2217 \u2212 O( \u221a \u03b4), U\u2217 + O(\u03b4)]. If the agent only uses deterministic \u03b4approximately-best-responding strategies, then the range of principal payoff is [U\u2217 \u2212 O(\u03b4), U\u2217 + O(\u03b4)]. Here, we see an interesting asymmetry between the lower bound (with a squared-root dependency on \u03b4) and the upper bound (with a linear dependency on \u03b4) with randomized strategies, but not with deterministic strategies.\nStructure of the paper Since information design is the main motivation of our work, we define the model of persuasion (cheap talk) with a learning agent in Section 2. With a learning agent, Bayesian persuasion and cheap talk turn out to be equivalent. We then define a general model, generalized principal-agent problem with a learning agent, in Section 3. We develop our main results in Section 4 and Section 5, by first reducing the generalized principal-agent problem with a learning agent to the problem with approximate best response, and then characterizing the problem with approximate best response. Finally, Section 6 applies our general results to the three specific principal-agent problems mentioned above: Bayesian persuasion, contract design, and Stackelberg games.\nRelated works The literature on information design has investigated various relaxations of the strong rationality assumptions in the classic models. For the sender, known prior (Dworczak and Pavan, 2020; Camara et al., 2020; Kosterina, 2022; Zu et al., 2021; Ziegler, 2020; Wu et al., 2022) and known utility (Babichenko et al., 2021; Castiglioni et al., 2020; Feng et al., 2022) are relaxed. For the receiver, the receiver may make mistakes in Bayesian updates (de Clippel and Zhang, 2022), be risk-conscious (Anunrojwong et al., 2023), do quantal response (Feng et al., 2024) or approximate best response (Yang and Zhang, 2024). We focus on a learning receiver and show that it can be reduced to approximate best response. While Yang and Zhang (2024) show that Bayesian persuasion with approximate best response is computationally hard as the instance size increases, our results imply that this problem is not hard with a fixed instance and a diminishing degree of approximate best response.\nLearning agents have been studied in principal-agent problems like auctions (Braverman et al., 2018; Cai et al., 2023; Rubinstein and Zhao, 2024; Kumar et al., 2024), bimatrix Stackelberg games (Deng et al., 2019; Mansour et al., 2022; Arunachaleswaran et al., 2024), and contract design (Guruganesh et al., 2024), but not in Bayesian persuasion. These problems belong to the class of generalized principalagent problems (Myerson, 1982; Gan et al., 2024). We thus propose a framework of generalized principal-agent problem with a learning agent, which not only encompasses several previous models but also provides new results for Bayesian persuasion with a learning agent.\nCamara et al. (2020) also propose a general framework of principal-agent problems with learning players, but has two key differences with ours: (1) They drop the common prior assumption while we still keep it. This assumption allows us to compare the principal\u2019s utility in the learning model with the classic model with common prior. (2) Their principal has commitment power, which is reasonable in, e.g., auction design, but less realistic in information design where the principal\u2019s strategy is a signaling scheme. Our principal does not commit.\nDeng et al. (2019) show that the follower\u2019s no-swap-learning can cap the leader\u2019s utility in Stackelberg games. We find that this is a general conclusion that holds for all generalized principalagent problems with complete information. Nevertheless, this conclusion does not hold when the agent has private information, as shown by Mansour et al. (2022)\u2019s work on Bayesian Stackelberg games. We believe that generalized principal-agent problems with private information with learning players is a viable direction for future research."
        },
        {
            "heading": "2 Special Model: Persuasion (Cheap Talk) with a Learning Re-",
            "text": "ceiver\nWe define a repeated persuasion model where the receiver learns to respond to the sender\u2019s signals using some algorithms. There are a finite set \u2126 of states of the world, a signal set S, an action set A, a prior distribution \u00b50 \u2208 \u2206(\u2126) over the states, a sender utility function u : \u2126 \u00d7 A \u2192 R, and a receiver utility function v : \u2126 \u00d7 A \u2192 R. When the state is \u03c9 \u2208 \u2126 and the receiver takes action a \u2208 A, the sender and the receiver obtain utility u(\u03c9, a), v(\u03c9, a), respectively. The sender knows the prior \u00b50. The receiver does not know the prior \u00b50 or the signaling scheme of the sender. The sender and the receiver interact for T rounds.\nPersuasion with a Learning Receiver\nIn each round t = 1, . . . , T , the following events happen:\n(1) The sender chooses a signaling scheme \u03c0t : \u2126 \u2192 \u2206(S), which is a mapping from each state to a distribution over signals. The receiver does not know \u03c0t.\n(2) A state of the world \u03c9t \u223c \u00b50 is drawn, observed by the sender but not the receiver. The sender then sends signal st \u223c \u03c0t(\u03c9t) to the receiver.\n(3) Upon receiving st, the receiver (randomly) chooses some action at \u2208 A to take, using some learning algorithm (described in details later).\n(4) The sender obtains utility ut = u(\u03c9t, at) and the receiver obtains utility vt = v(\u03c9t, at).\nWith some knowledge of the receiver\u2019s learning algorithm, the sender aims to maximize its expected average utility 1\nT E [\u2211T t=1 u(\u03c9 t, at) ] .\nSince the receiver does not know the sender\u2019s signaling scheme when making decisions, we can flip the order of the decision making processes of the two players:\nAlternative Equivalent Definition of \u201cPersuasion with a Learning Receiver\u201d\nIn each round t = 1, . . . , T , the following events happen:\n(1\u2019) The receiver first chooses, using some learning algorithm, a strategy \u03c1t : S \u2192 \u2206(A) that maps each signal s \u2208 S to a probability distribution over actions \u03c1t(s) \u2208 \u2206(A).\n(2\u2019) The sender chooses a signaling scheme \u03c0t : \u2126 \u2192 \u2206(S).\n(3\u2019) A state of the world \u03c9t \u223c \u00b50 is drawn. Signal st \u223c \u03c0t(\u03c9t) is sent to the receiver, who then chooses an action at according to the distribution \u03c1t(st).\n(4) The sender obtains utility ut = u(\u03c9t, at) and the receiver obtains utility vt = v(\u03c9t, at).\nOne can see from the alternative definition that the sender does not need to commit to the signaling scheme. The receiver\u2019s strategy \u03c1t is chosen before the sender\u2019s signaling scheme and the sender has an incentive to best respond to it. This in fact corresponds to the well-known cheap talk model (Crawford and Sobel, 1982) where the sender does not have commitment power. So, our model can also be called \u201ccheap talk with a learning receiver\u201d.\nReceiver\u2019s learning algorithm: no-regret and no-swap-regret The receiver\u2019s learning problem can be regarded as a contextual multi-armed bandit problem (Tyler Lu et al., 2010) where A is the set of arms, and the signal st \u2208 S serves as a context that affects the utility of each\narm a \u2208 A. The receiver picks an arm to pull based on the current context st and the historical information about each arm under different contexts, adjusting its strategy over time based on the feedback collected after each round.\nWhat feedback can the receiver observe after each round? One may assume that the receiver sees the state \u03c9t after each round (this is call full-information feedback in the multi-armed bandit literature), or not the state but just the utility vt = v(\u03c9t, at) obtained in that round (this is called bandit feedback). We do not make specific assumptions on the feedback. All we need is that the feedback is sufficient for the receiver to achieve contextual no-regret or contextual no-swap-regret, which are defined below.\nDefinition 2.1. The receiver\u2019s learning algorithm satisfies:\n\u2022 contextual no-regret if: there exists a function CReg(T ) = o(T ) such that, for any deviation function d : S \u2192 A, the regret of the receiver not deviating according to d is at most CReg(T ):\nE\n[ T\u2211\nt=1\n( v(\u03c9t, d(st))\u2212 v(\u03c9t, at) )] \u2264 CReg(T ) = o(T ), \u2200d : S \u2192 A. (1)\n\u2022 contextual no-swap-regret if: there exists a function CSReg(T ) = o(T ) such that, for any deviation function d : S \u00d7A \u2192 A, the regret of the receiver not deviating according to d is at most CSReg(T ):\nE\n[ T\u2211\nt=1\n( v(\u03c9t, d(st, at))\u2212 v(\u03c9t, at) )] \u2264 CSReg(T ) = o(T ), \u2200d : S \u00d7A \u2192 A. (2)\nWe call CReg(T ) and CSReg(T ) the contextual regret and contextual swap-regret of the receiver.\nClearly, contextual no-swap-regret implies contextual no-regret because the former has a larger set of deviation functions. Contextual no-regret (no-swap-regret) algorithms can be constructed easily by running an ordinary no-regret (no-swap-regret) algorithm for each context independently. Since algorithms with O( \u221a T ) regret (swap-regret) with bandit feedback are known to exist (Audibert and Bubeck, 2010; Ito, 2020), they can be converted into algorithms with O( \u221a\n|S|T ) contextual regret (swap-regret). The following proposition formalizes this; the proof is in Appendix A.\nProposition 2.1. There exist learning algorithms with contextual regret CReg(T ) = O( \u221a\n|A||S|T ) and contextual swap-regret CSReg(T ) = O(|A| \u221a\n|S|T ). They can be constructed by running an ordinary no-regret (no-swap-regret) multi-armed bandit algorithm for each context independently."
        },
        {
            "heading": "3 General Model: Generalized Principal-Agent Problems",
            "text": "This section defines a more general model, generalized principal-agent problem (with a learning agent), that includes Bayesian persuasion as a special case. Introducing this general model actually simplifies the analysis for the problem of persuasion with a learning agent, and allows us to apply our results to other principal-agent problems, such as contract design and Stackelberg games, with learning agents (see Section 6 for details)."
        },
        {
            "heading": "3.1 Generalized Principal-Agent Problem with a Learning Agent",
            "text": "Generalized principal-agent problem, originally proposed by (Myerson, 1982), is a general model that includes auction design, contract design, and Stackelberg games. Gan et al. (2024) further generalize it to include Bayesian persuasion. We take the model of (Gan et al., 2024).1\nThere are two players in a generalized principal-agent problem: a principal and an agent. The principal has a convex, compact decision space X and the agent has a finite action set A. The principal and the agent have utility functions u, v : X \u00d7A \u2192 R. We assume that u(x, a), v(x, a) are linear in x \u2208 X , which is satisfied by all the examples of generalized principal-agent problems we will consider (Bayesian persuasion, Stackelberg games, contract design). There is a signal/message set S. Signals are usually interpreted as action recommendations, where S = A, but we allow any general finite signal set. A strategy of the principal is a distribution \u03c0 \u2208 \u2206(X \u00d7 S) over pairs of decision and signal {(xi, si)}i with probability \u03c0i = \u03c0(xi, si) \u2265 0 for the pair (xi, si), \u2211\ni \u03c0i = 1. When the utility functions are linear, it is without loss of generality to assume that the principal does not randomize over multiple decisions for one signal, namely, the principal chooses a distribution over signals and a unique decision xs associated with each signal s \u2208 S (Gan et al., 2024). So, we can write a principal strategy as \u03c0 = {(\u03c0s, xs)}s\u2208S where \u03c0s \u2265 0 is the probability of signal s \u2208 S, \u2211\ns\u2208S \u03c0s = 1, and xs \u2208 X . There are two variants of generalized principal-agent problems:\n\u2022 Unconstrained (Myerson, 1982): there is no restriction on the principal\u2019s strategy \u03c0.\n\u2022 Constrained (Gan et al., 2024): the principal\u2019s strategy \u03c0 has to satisfy the constraint:\n\u2211 s\u2208S \u03c0sxs \u2208 C (3)\nwhere C \u2286 X is some convex set.\nUnconstrained generalized principal-agent problems include contract design and Stackelberg games (see Section 6 for details). Constrained generalized principal-agent problems further include Bayesian persuasion (discussed later).\nIn a one-shot generalized principal-agent problem where the principal has commitment power, the principal first commits to a strategy \u03c0 = {(\u03c0s, xs)}s\u2208S , then nature draws a signal s \u2208 S according to the distribution {\u03c0s}s\u2208S and sends s to the agent (note: due to the commitment assumption, this is equivalent to revealing the pair (s, xs) to the agent), then the agent takes an action as = argmaxa\u2208A v(xs, a) to maximize its utility, and the principal obtains utility u(xs, as). The principal aims to maximize its expected utility Es\u223c\u03c0[u(xs, as)] by choosing the strategy \u03c0.\nBayesian persuasion as a generalized principal-agent problem We show why Bayesian persuasion is a special case of constrained generalized principal-agent problems. The sender is the principal; the receiver is the agent. It is well-known (Kamenica and Gentzkow, 2011) that a signaling scheme \u03c0 : \u2126 \u2192 \u2206(S) in the Bayesian persuasion problem decomposes the prior \u00b50 into a distribution over posteriors: Let \u00b5s \u2208 \u2206(\u2126) be the posterior belief induced by signal s: \u2200\u03c9 \u2208 \u2126,\n\u00b5s(\u03c9) = \u00b50(\u03c9)\u03c0(s|\u03c9)\n\u03c0s , where \u03c0s =\n\u2211 \u03c9\u2208\u2126 \u00b50(\u03c9)\u03c0(s|\u03c9) is the probability that signal s is sent.\n1The models of Myerson (1982) and Gan et al. (2024) allow the agent to have private information. In this work we study the complete-information version of their models. Our conclusions do not hold for the private-information case.\nThen\n\u2211 s\u2208S \u03c0s\u00b5s = \u00b50 \u2208 {\u00b50} =: C, \u2211 s\u2208S \u03c0s = 1. (4)\nEquation (4) is called the Bayes plausibility condition. Conversely, any distribution over posteriors {(ps, \u00b5s)}s\u2208S satisfying Bayes plausibility \u2211\ns\u2208S ps\u00b5s = \u00b50 can be converted into a signaling scheme that sends signal s with probability ps. Thus, we can use a distribution over posteriors {(\u03c0s, \u00b5s)}s\u2208S satisfying Bayes plausibility to represent a signaling scheme. Then, let\u2019s equate the posterior belief \u00b5s to the principal\u2019s decision xs in the generalized principal-agent problem, so the principal/sender\u2019s decision space is X = \u2206(\u2126). When the agent/receiver takes action a, the principal/sender\u2019s (expected) utility under decision/posterior xs = \u00b5s is:\nu(xs, a) = E\u03c9\u223c\u00b5su(\u03c9, a) = \u2211\n\u03c9\u2208\u2126 \u00b5s(\u03c9)u(\u03c9, a).\nSuppose the agent takes action as given signal s \u2208 S. Then we see that the sender\u2019s utility of using signaling scheme \u03c0 in Bayesian persuasion (left) is equal to the principal\u2019s utility of using strategy \u03c0 in the generalized principal-agent problem (right):\n\u2211 \u03c9\u2208\u2126 \u00b50(\u03c9) \u2211 s\u2208S \u03c0(s|\u03c9)u(\u03c9, as) = \u2211 s\u2208S \u03c0s \u2211 \u03c9\u2208\u2126 \u00b5s(\u03c9)u(\u03c9, as) = \u2211 s\u2208S \u03c0su(xs, as) = Es\u223c\u03c0[u(xs, a)].\nSimilarly, the agent/receiver\u2019s utilities in the two problems are equal. The utility functions u(x, a), v(x, a) are linear in the principal\u2019s decision x \u2208 X , satisfying our assumption.\nGeneralized principal-agent problem with a learning agent Now we define the model of generalized principal-agent problem with a learning agent, which generalizes persuasion with a learning agent. The principal has no commitment power and the agent learns to respond to signals. In each round t = 1, . . . , T :\n(1) The principal chooses a strategy \u03c0t = {(\u03c0ts, xts)}s\u2208S , which is a distribution over signals S and a decision xts \u2208 X associated with each signal. The agent does not know \u03c0t.\n(2) Nature draws a signal st \u223c \u03c0t and reveals to the agent. The principal makes decision xt = xt st .\n(3) Based on st and history (and not knowing xt), the agent (randomly) chooses an action at \u2208 A, using some learning algorithm.\n(4) The principal and the agent obtain utility ut = u(xt, at) and vt = v(xt, at).2\nThe definition of utility in (4) is slightly different from the persuasion model. The utility in the persuasion model is the ex post utility u(\u03c9t, at), v(\u03c9t, at) while the utility here is the expected utility conditioning on decision/posterior xt, u(xt, at), v(xt, at). Since we are concerned with the expected utility of the sender and the expected regret of the receiver, this difference does not matter.\n2The agent does not necessarily observe the utility v(xt, at) as the feedback. He can observe, e.g., v(\u03c9t, at) in persuasion."
        },
        {
            "heading": "3.2 Approximate Best Response",
            "text": "As we will prove in Section 4, the generalized principal-agent problem with a learning agent can be reduced to the generalized principal-agent problem with an approximately-best-responding agent. So we define \u201capproximate best response\u201d here.\nClassic works on Bayesian persuasion and generalized principal-agent problems assume that, after receiving a signal s \u2208 S (and observing the principal\u2019s decision xs \u2208 X ), the agent will take an action \u2208 argmaxa\u2208A v(xs, a) to maximize its utility with respect to xs. This means that the agent uses a strategy \u03c1\u2217 that best responds to the principal\u2019s strategy \u03c0:\n\u03c1\u2217(s) \u2208 argmax a\u2208A v(xs, a), \u2200s \u2208 S =\u21d2 \u03c1\u2217 \u2208 argmax \u03c1:S\u2192\u2206(A) V (\u03c0, \u03c1). (5)\nHere, V (\u03c0, \u03c1) denotes the expected utility of the receiver when the principal uses strategy \u03c0 and the receiver uses (randomized) strategy \u03c1 : S \u2192 \u2206(A):\nV (\u03c0, \u03c1) = \u2211\ns\u2208S \u03c0s\n\u2211 a\u2208A \u03c1(a|s)v(xs, a). (6)\nIn this work, we allow the agent to approximately best respond. Let \u03b4 \u2265 0 be a parameter. We define two types of \u03b4-best-responding strategies for the agent: deterministic and randomized.\n\u2022 A deterministic strategy \u03c1: for each signal s \u2208 S, the agent takes an action a that is \u03b4-optimal for xs. Denote this set of strategies by D\u03b4(\u03c0):\nD\u03b4(\u03c0) = { \u03c1 : S \u2192 A | v(xs, \u03c1(s)) \u2265 v(xs, a\u2032)\u2212 \u03b4,\u2200a\u2032 \u2208 A } . (7)\n\u2022 A randomized strategy \u03c1: for each signals s, the agent can take a randomized action. The expected utility of \u03c1 is at most \u03b4-worst than the best strategy \u03c1\u2217.\nR\u03b4(\u03c0) = { \u03c1 : S \u2192 \u2206(A) | V (\u03c0, \u03c1) \u2265 V (\u03c0, \u03c1\u2217)\u2212 \u03b4 } . (8)\nEquivalently, R\u03b4(\u03c0) = { \u03c1 : S \u2192 \u2206(A) | V (\u03c0, \u03c1) \u2265 V (\u03c0, \u03c1\u2032)\u2212 \u03b4, \u2200\u03c1\u2032 : S \u2192 A } .\nClearly, D\u03b4(\u03c0) \u2286 R\u03b4(\u03c0). Our model of approximately-best-responding agent includes, for example, two other models in the Bayesian persuasion literature that also relax the agent\u2019s Bayesian rationality assumption: the quantal response model (proposed by McKelvey and Palfrey (1995) in normal-form games and studied by Feng et al. (2024) in Bayesian persuasion) and a model where the agent makes mistakes in Bayesian update (de Clippel and Zhang, 2022).\nExample 3.1. Assume that the receiver\u2019s utility is in [0, 1]. In Bayesian persuasion, the following receiver strategies are \u03b4-best-responding:\n\u2022 Quantal response: given signal s \u2208 S, the agent chooses action a \u2208 A with probability exp(\u03bbv(\u00b5s ,a))\u2211\na\u2032\u2208A exp(\u03bbv(\u00b5s ,a \u2032)) , with \u03bb > 0. This strategy belongs to R\u03b4(\u03c0) with \u03b4 = 1+log(|A|\u03bb) \u03bb .\n\u2022 Inaccurate belief: given signal s \u2208 S, the agent forms some posterior \u00b5\u2032s that is different yet close to the true posterior \u00b5s in total variation distance dTV(\u00b5 \u2032 s, \u00b5s) \u2264 \u03b5. The agent picks an\noptimal action for \u00b5\u2032s. This strategy belongs to D2\u03b5(\u03c0).\nSee Appendix B for a proof of this example.\nPrincipal\u2019s objectives With an approximately-best-responding agent, we will study two types of objectives for the principal. The first type is the maximal utility that the principal can obtain if the agent approximately best responds in the worst way for the principal: for X \u2208 {D,R}, define\nOBJX(\u03b4) = sup \u03c0 min \u03c1\u2208X\u03b4(\u03c0) U(\u03c0, \u03c1), (9)\nwhere U(\u03c0, \u03c1) is the principal\u2019s expected utility when the principal uses strategy \u03c0 and the agent uses strategy \u03c1:\nU(\u03c0, \u03c1) = \u2211\ns\u2208S \u03c0s\n\u2211 a\u2208A \u03c1(a|s)u(xs, a). (10)\nWe used \u201csup\u201d in (9) because the maximizer does not necessarily exist. OBJX(\u03b4) is a \u201cmaximin\u201d objective and can be regarded as the objective of a \u201crobust generalized principal-agent problem\u201d.\nThe second type of objectives is the maximal utility that the principal can obtain if the agent approximately best responds in the best way:\nOBJ X (\u03b4) = max\n\u03c0 max \u03c1\u2208X\u03b4(\u03c0) U(\u03c0, \u03c1). (11)\nThis is a \u201cmaximax\u201d objective that quantifies the maximal extent to which the principal can exploit the agent\u2019s irrational behavior.\nClearly, OBJX(\u03b4) \u2264 OBJX(0) \u2264 OBJX(0) \u2264 OBJX(\u03b4). And we note that OBJX(0) = OBJ(0) is independent of X and equal to the optimal principal utility in the classic generalized principalagent problem, which we denote by U\u2217:\nU\u2217 = OBJ(0) = max \u03c0 max \u03c1: best-response to \u03c0 U(\u03c0, \u03c1). (12)\nFinally, we note that, because D0(\u03c0) \u2286 D\u03b4(\u03c0) \u2286 R\u03b4(\u03c0), the following series of inequalities hold:\nOBJR(\u03b4) \u2264 OBJD(\u03b4) \u2264 U\u2217 \u2264 OBJD(\u03b4) \u2264 OBJR(\u03b4). (13)"
        },
        {
            "heading": "4 Reduction from Learning to Approximate Best Response",
            "text": "Our first main result is a reduction from the generalized principal-agent problem with a learning agent to the one-shot problem with an approximately-best-responding agent. We show that, if the agent uses contextual no-regret learning algorithms, then the principal can always obtain an average utility that is at least the \u201cmaximin\u201d approximate-best-response objective OBJR ( CReg(T )/T ) . On the other hand, if the agent does contextual no-swap-regret learning, then the principal cannot do better than the \u201cmaximax\u201d approximate-best-response objective OBJ R( CSReg(T )/T ) . In addition, we show that the principal can do better than the \u201cmaximax\u201d objective OBJ R( CSReg(T )/T ) if the agent uses some natural learning algorithms that are no-regret but not no-swap-regret (in particular, mean-based algorithms (Braverman et al., 2018))."
        },
        {
            "heading": "4.1 Agent\u2019s No-Regret Learning: Lower Bound on Principal\u2019s Utility",
            "text": "Theorem 4.1. Suppose the agent uses a contextual no-regret learning algorithm. By using some fixed strategy \u03c0t = \u03c0 in all T rounds, the principal can obtain an average utility 1\nT E [\u2211T t=1 u(x t, at) ]\nthat is arbitrarily close to OBJR ( CReg(T )/T ) .\nTo prove Theorem 4.1, we first prove a lemma to characterize the agent\u2019s regret and the principal\u2019s expected utility. We define some notations. Let the principal use some fixed strategy \u03c0t = \u03c0 and the agent use some learning algorithm. Let pt\na|s = Pr[a t = a | st = s] be the probability that the\nagent\u2019s algorithm chooses action a conditioning on signal s being sent in round t. Let \u03c1 : S \u2192 \u2206(A) be a randomized agent strategy that, given signal s, chooses each action a \u2208 A with probability\n\u03c1(a|s) = \u2211T t=1 p t a|s\nT . (14)\nLemma 4.2. When the principal uses a fixed strategy \u03c0t = \u03c0 in all T rounds, the regret of the agent not deviating according to d : S \u2192 A is equal to 1\nT E [\u2211T t=1 ( v(xt, d(st)) \u2212 v(xt, at) )] =\nV (\u03c0, d) \u2212 V (\u03c0, \u03c1), and the average utility of the sender 1 T E [\u2211T t=1 u(x t, at) ] is equal to U(\u03c0, \u03c1).\nProof. Since \u03c0t = \u03c0 is fixed, we have \u03c0ts = \u03c0s and x t s = xs, \u2200s \u2208 S. The regret of the receiver not deviating according to d is:\n1 T E\n[ T\u2211\nt=1\n( v(xt, d(st))\u2212 v(xt, at) )] = 1\nT\nT\u2211\nt=1\n\u2211 s\u2208S \u03c0ts \u2211 a\u2208A pta|s\n( v(xts, d(s))\u2212 v(xts, a) )\n= \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\n\u2211T t=1 p t a|s\nT\n( v(xs, d(s)) \u2212 v(xs, a) )\n= \u2211\ns\u2208S \u03c0sv(xs, d(s)) \u2212\n\u2211 s\u2208S \u03c0s \u2211 a\u2208A \u03c1(a|s)v(xs, a) = V (\u03c0, d)\u2212 V (\u03c0, \u03c1).\nHere, d is interpreted as an agent strategy that deterministically takes action d(s) for signal s. By a similar derivation, we see that the principal\u2019s expected utility is equal to\n1 T E\n[ T\u2211\nt=1\nu(xt, at) ] = \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\n\u2211T t=1 p t a|s\nT u(xs, a) = U(\u03c0, \u03c1),\nwhich proves the lemma.\nProof of Theorem 4.1. By Lemma 4.2 and the no-regret condition that the agent\u2019s regret E [\u2211T\nt=1\n( v(xt, d(st))\u2212 v(xt, at) )] \u2264 CReg(T ), we have\nV (\u03c0, d) \u2212 V (\u03c0, \u03c1) = 1 T E\n[ T\u2211\nt=1\n( v(xt, d(st))\u2212 v(xt, at) )] \u2264 CReg(T ) T , \u2200d : S \u2192 A\nThis means that the agent\u2019s randomized strategy \u03c1 is a \u03b4 = CReg(T ) T\n-best-response to the principal\u2019s fixed signaling scheme \u03c0:\n\u03c1 \u2208 R \u03b4=CReg(T )\nT\n(\u03c0).\nThis holds for any \u03c0. In particular, if for any \u03b5 > 0 the principal uses a signaling scheme \u03c0\u03b5 that obtains an objective that is \u03b5-close to OBJR(\u03b4) = sup\u03c0 min\u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1), then the principal obtains an expected utility of, by Lemma 4.2:\n1 T E\n[ T\u2211\nt=1\nu(at, \u03c9t) ]\n= U(\u03c0\u03b5, \u03c1) \u2265 min \u03c1\u2208R\u03b4(\u03c0\u03b5)\nU(\u03c0\u03b5, \u03c1) \u2265 OBJR ( \u03b4 = CReg(T )\nT\n)\n\u2212 \u03b5\nin the learning model. Letting \u03b5 \u2192 0 proves the theorem."
        },
        {
            "heading": "4.2 Agent\u2019s No-Swap-Regret Learning: Upper Bound on Principal\u2019s Utility",
            "text": "Theorem 4.3. If the agent uses a contextual no-swap-regret learning algorithm, then the principal\u2019s obtainable utility, even using time-varying strategies, is at most:\n1 T E\n[ T\u2211\nt=1\nu(xt, at) ] \u2264 OBJR (CSReg(T )\nT\n) .\nProof. Let pts = Pr[s t = s] = E\n[ 1[st = s] ] = E[\u03c0ts] be the probability that signal s \u2208 S is sent in\nround t. Let pt a|s = Pr[a t = a | st = s] be the probability that the agent takes action a conditioning on signal st = s is sent in round t. Let d : S\u00d7A \u2192 A be any deviation function for the agent. The utility gain by deviation for the agent is upper bounded by the contextual swap-regret:\nCSReg(T ) T \u2265 1 T E\n[ T\u2211\nt=1\n( v(xt, d(st, at))\u2212 v(xt, at) )]\n(15)\n= 1\nT\nT\u2211\nt=1\n\u2211 s\u2208S pts \u2211 a\u2208A pta|sExts|st=s\n[ v(xts, d(s, a)) \u2212 v(xts, a) ]\n= 1\nT\nT\u2211\nt=1\n\u2211 s\u2208S pts \u2211 a\u2208A pta|s\n( v(E[xts|st = s], d(s, a)) \u2212 v(E[xts|st = s], a) )\nby linearity of v(\u00b7, a)\n= \u2211\ns\u2208S\n\u2211\na\u2208A\n\u2211T j=1 p j sp j a|s\nT\n1 \u2211T\nj=1 p j sp j a|s\nT\u2211\nt=1\nptsp t a|s\n( v(E[xts|st = s], d(s, a)) \u2212 v(E[xts|st = s], a) )\n= \u2211\ns\u2208S\n\u2211\na\u2208A\n\u2211T j=1 p j sp j\na|s T\n[\nv (\u2211T t=1 p t sp t a|s E[xts|st=s] \u2211T\nj=1 p j sp j\na|s\n, d(s, a) ) \u2212 v (\u2211T t=1 p t sp t a|s E[xts|st=s] \u2211T\nj=1 p j sp j\na|s\n, a )] .\nDefine qs,a =\n\u2211T j=1 p j sp j\na|s\nT and ys,a =\n\u2211T t=1 p t sp t a|s\nE[xts|st=s] \u2211T\nj=1 p j sp j\na|s\n\u2208 X . Then the above is equal to\n= \u2211\ns\u2208S\n\u2211 a\u2208A qs,a\n[ v(ys,a, d(s, a)) \u2212 v(ys,a, a) ] . (16)\nWe note that \u2211 s\u2208S \u2211 a\u2208A qs,a = \u2211T j=1 \u2211 s\u2208S \u2211 a\u2208A p j sp j a|s T = 1, so q is a probability distribution over S \u00d7A. And note that\n\u2211\ns,a\u2208S\u00d7A qs,ays,a =\n\u2211\ns,a\u2208S\u00d7A\n1\nT\nT\u2211\nt=1\nptsp t a|sE[x t s|st = s] =\n1\nT\nT\u2211\nt=1\n\u2211 s\u2208S ptsE[x t s|st = s]\n= 1\nT\nT\u2211\nt=1\n\u2211 s\u2208S E [ 1[st = s]xts ] = 1 T\nT\u2211\nt=1\nE [\u2211\ns\u2208S 1[st = s]xts\n] = 1\nT\nT\u2211\nt=1\nE [ xt ]\n= 1\nT\nT\u2211\nt=1\nE [\u2211\ns\u2208S \u03c0tsx t s\n] \u2208 C because\n\u2211 s\u2208S \u03c0tsx t s \u2208 C.\nThis means that \u03c0\u2032 = {(qs,a, ys,a)}(s,a)\u2208S\u00d7A defines a valid principal strategy with the larger signal space S \u00d7A. Then, we note that (16) is the difference between the agent\u2019s expected utility under\nprincipal strategy \u03c0\u2032 when responding using strategy d : S \u00d7 A \u2192 A and using the strategy that maps signal (s, a) to action a. And (16) is upper bounded by CSReg(T )\nT by (15):\n(16) = V (\u03c0\u2032, d) \u2212 V (\u03c0\u2032, (s, a) 7\u2192 a) \u2264 CSReg(T ) T , \u2200d : S \u00d7A \u2192 A. (17)\nIn particular, this holds when d is the agent\u2019s best-responding strategy. This means that the agent strategy (s, a) 7\u2192 a is a (CSReg(T )\nT )-best-response to \u03c0\u2032. So, the principal\u2019s expected utility is upper\nbounded by the utility in the approximate-best-response model:\n1 T E\n[ T\u2211\nt=1\nu(xt, at) ] = 1\nT\nT\u2211\nt=1\n\u2211 s\u2208S pts \u2211 a\u2208A pta|sv(E[x t s|st = s], a)\n= \u2211\ns\u2208S\n\u2211 a\u2208A qs,au(ys,a, a) = U(\u03c0 \u2032, (s, a) \u2192 a) \u2264 OBJR (CSReg(T ) T ) ."
        },
        {
            "heading": "4.3 Agent\u2019s Mean-Based Learning: Exploitable by the Principal",
            "text": "Many no-regret (but not no-swap-regret) learning algorithms (e.g., MWU, FTPL, EXP-3) satisfy the following contextual mean-based property:\nDefinition 4.1 (Braverman et al. (2018)). Let \u03c3ts(a) = \u2211 j\u2208[t]:sj=s v(\u03c9 j , a) be the sum of historical utility of the receiver in the first t rounds if he takes action a when the signal/context is s. An algorithm is called \u03b3-mean-based if: whenever \u2203a\u2032 such that \u03c3t\u22121s (a) < \u03c3t\u22121s (a\u2032)\u2212\u03b3T , the probability that the algorithm chooses action a at round t if the context is s is Pr[at = a|st = s] < \u03b3, with \u03b3 = o(1).\nTheorem 4.4. There exists a Bayesian persuasion instance where, as long as the receiver does \u03b3-mean-based learning, the sender can obtain a utility significantly larger than OBJ R (\u03b3) and U\u2217.\nProof The instance has 2 states (A, B), 3 actions (L, M, R), uniform prior \u00b50(A) = \u00b50(B) = 0.5, with the following utility matrices (left for sender\u2019s, right for receiver\u2019s):\nu(\u03c9, a) L M R v(\u03c9, a) L M R A 0 \u22122 \u22122 A \u221a\u03b3 \u22121 0 B 0 0 2 B \u22121 1 0\nClaim 4.5. In this instance, the optimal sender utility U\u2217 in the classic BP model is 0, and the approximate-best-response objective OBJ R (\u03b3) = O(\u03b3).\nProof. Recall that any signaling scheme decomposes the prior \u00b50 into multiple posteriors {\u00b5s}s\u2208S . If a posterior \u00b5s puts probability > 0.5 to state B, then the receiver will take action M, which gives the sender a utility \u2264 0; if the posterior \u00b5s puts probability \u2264 0.5 to state B, then no matter what action the receiver takes, the sender\u2019s expected utility on \u00b5s cannot be greater than 0. So, the sender\u2019s expected utility is \u2264 0 under any signaling scheme. An optimal signaling scheme is to reveal no information (keep \u00b5s = \u00b50); the receiver takes R and the sender gets utility 0.\nThis instance satisfies the assumptions of Theorem 5.3, so OBJ R (\u03b3) \u2264 U\u2217 +O(\u03b3) = O(\u03b3).\nClaim 4.6. By doing the following, the sender can obtain utility \u2248 12 \u2212 O( \u221a \u03b3) if the receiver is \u03b3-mean-based learning:\n\u2022 in the first T/2 rounds: if the state is A, send signal 1; if the state is B, send 2.\n\u2022 in the remaining T/2 rounds, switch the scheme: if the state is A, send 2; if state is B, send 1.\nProof Sketch. In the first T/2 rounds, the receiver finds that signal 1 corresponds to state A so he will take action L with high probability when signal 1 is sent; signal 2 corresponds to B so he will take action M with high probability. In this phase, the sender obtains utility \u2248 0 per round. At the end of this phase, for signal 1, the receiver accumulates utility \u2248 T2 12 \u221a \u03b3 = T4 \u221a \u03b3 for action L. For signal 2, the receiver accumulates utility \u2248 T2 12 \u00b7 1 = T4 for action M. In the remaining T/2 rounds, the following will happen:\n\u2022 For signal 1, the receiver finds that the state is now B, so the utility of action L decreases by 1 every time signal 1 is sent. Because the utility of L accumulated in the first phase was \u2248 T4 \u221a \u03b3, after \u2248 T4 \u221a \u03b3 rounds in second phase the utility of L should decrease to below 0,\nand the receiver will no longer play L (with high probability) at signal 1. The receiver will not play M at signal 1 in most of the second phase either, because there are more A states than B states at signal 1 historically. So, the receiver will play action R most times, roughly T 4 \u2212 T4 \u221a \u03b3 rounds. This gives the sender a total utility of \u2248 (T4 \u2212 T4 \u221a \u03b3) \u00b7 2 = T2 \u2212O(T \u221a \u03b3).\n\u2022 For signal 2, the state is now A. But the receiver will continue to play action M in most times. This because: R has utility 0; L accumulated \u2248 \u2212T4 utility in the first phase, and only increases by \u221a \u03b3 per round in the second phase, so its accumulated utility is always negative;\ninstead, M has accumulated T4 utility in the first phase, and decreases by 1 every time signal 2 is sent in the second phase, so its utility is positive until near the end. So, the receiver will play M. This gives the sender utility 0. Summing up, the sender obtains total utility \u2248 T2 \u2212 O(T \u221a \u03b3) in these two phases, which is\n1 2 \u2212O(\n\u221a \u03b3) > 0 per round in average."
        },
        {
            "heading": "5 Generalized Principal-Agent Problems with Approximate Best",
            "text": "Response\nHaving presented the reduction from learning to approximate best response in Section 4, we now turn to study generalized principal-agent problems with approximate best response. We derive lower bounds on the maximin objectives OBJD(\u03b4), OBJR(\u03b4), and upper bounds on the maximax objectives OBJ D (\u03b4), OBJ R (\u03b4). We will show that, when the degree \u03b4 of agent\u2019s approximate best response is small, all the four objectives will be close to the optimal principal objective U\u2217 in the best-response model.\nAssumptions and notations We make some innocuous assumptions. First, the agent has no weakly dominated action:\nAssumption 5.1 (No Dominated Action). An action a0 \u2208 A of the agent is weakly dominated if there exists a mixed action \u03b1\u2032 \u2208 \u2206(A \\ {a0}) such that v(x, \u03b1\u2032) = Ea\u223c\u03b1\u2032 [v(x, a)] \u2265 v(x, a0) for all x \u2208 X . We assume that the agent has no weakly dominated action.\nClaim 5.1. Assumption 5.1 implies: there exists a constant G > 0 such that, for any agent action a \u2208 A, there exists a principal decision x \u2208 X such that v(x, a)\u2212v(x, a\u2032) \u2265 G for every a\u2032 \u2208 A\\{a}.\nThe proof of this claim is in Appendix C.1. The constant G > 0 in Claim 5.1 is analogous to the concept of \u201cinducibility gap\u201d in Stackelberg games (Von Stengel and Zamir, 2004; Gan et al., 2023). In fact, Gan et al. (2023) show that, if the inducibility gap G > \u03b4, then the maximin approximate-best-response objective satisfies OBJD(\u03b4) \u2265 U\u2217 \u2212 \u03b4\nG in Stackelberg games. Our results will significantly generalize theirs to any generalized\nprincipal-agent problem, to randomized agent strategies, and to the maximax objectives OBJ D (\u03b4), OBJ R (\u03b4).\nTo present our results, we need to introduce a few more notions and assumptions. Let\ndiam(X ; \u2016 \u00b7 \u2016) = max x1,x2\u2208X \u2016x1 \u2212 x2\u2016 (18)\nbe the diameter of the space X , where \u2016 \u00b7 \u2016 is some norm. For convenience we assume X \u2286 Rd and use the \u21131-norm \u2016x\u20161 = \u2211d i=1 |x(i)| or the \u2113\u221e-norm \u2016x\u2016\u221e = maxdi=1 |x(i)|. For a generalized principal-agent problem with the constraint \u2211\ns\u2208S \u03c0sxs \u2208 C, let \u2202X be the boundary of X and let\ndist(C, \u2202X ) = min c\u2208C,x\u2208\u2202X \u2016c\u2212 x\u2016 (19)\nbe the distance from C to the boundary of X . We assume that C is away from the boundary of X :\nAssumption 5.2 (C is in the interior of X ). dist(C, \u2202X ) > 0.\nAssumption 5.3 (Bounded and Lipschitz utility). The principal\u2019s utility function is bounded: |u(x, a)| \u2264 B, and L-Lipschitz in x \u2208 X : |u(x1, a)\u2212 u(x2, a)| \u2264 L\u2016x1 \u2212 x2\u2016.\nMain results We now present the main results of this section: lower bounds on OBJX(\u03b4) and upper bounds on OBJ X (\u03b4) in generalized principal-agent problems without and with constraints.\nTheorem 5.2 (Without constraint). For an unconstrained generalized principal-agent problem, under Assumptions 5.1 and 5.3, for 0 \u2264 \u03b4 < G, we have\n\u2022 OBJD(\u03b4) \u2265 U\u2217 \u2212 diam(X )L \u03b4 G . \u2022 OBJR(\u03b4) \u2265 U\u2217 \u2212 2 \u221a\n2BL G diam(X )\u03b4 for \u03b4 < diam(X )GL2B .\n\u2022 OBJ D (\u03b4) \u2264 OBJR(\u03b4) \u2264 U\u2217 + diam(X )L \u03b4\nG .\nTheorem 5.3 (With constraint). For a generalized principal-agent problem with the constraint \u2211\ns\u2208S \u03c0sxs \u2208 C, under Assumptions 5.1, 5.2 and 5.3, for 0 \u2264 \u03b4 < Gdist(C,\u2202X ) diam(X ) , we have\n\u2022 OBJD(\u03b4) \u2265 U\u2217 \u2212 ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4 G . \u2022 OBJR(\u03b4) \u2265 U\u2217 \u2212 2 \u221a\n2B G ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4.\n\u2022 OBJ D (\u03b4) \u2264 OBJR(\u03b4) \u2264 U\u2217 + ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nThe expression \u201c diam(X )dist(C,\u2202X )\u03b4\u201d suggests that 1 dist(C,\u2202X ) is similar to a \u201ccondition number\u201d (Renegar, 1994) that quantifies the \u201cstability\u201d of the principal-agent problem against the agent\u2019s approximatebest-responding behavior. When dist(C, \u2202X ) is larger (C is further away from the boundary of X ), the condition number is smaller, the problem is more stable, and the \u03b4-best-response objectives OBJX(\u03b4), and OBJ X (\u03b4) are closer to the best-response objective U\u2217.\nLower bounds for OBJD(\u03b4) imply lower bounds for OBJR(\u03b4) The lower bounds for OBJD(\u03b4) in Theorem 5.2 and 5.3 actually imply the lower bounds for OBJR(\u03b4). This is one step in the proofs of the two theorems, so we present it here. We use the following lemma (proved in Appendix C.2):\nLemma 5.4. For any \u03b4 \u2265 0,\u2206 > 0, OBJR(\u03b4) \u2265 OBJD(\u2206)\u2212 2B\u03b4\u2206 .\nUsing Lemma 5.4 with \u2206 = \u221a\n2BG\u03b4 diam(X )L and the bound for OBJ D(\u2206) in Theorem 5.2, we obtain:\nOBJR(\u03b4) \u2265 OBJD(\u2206)\u2212 2B\u03b4\u2206 \u2265 U \u2217 \u2212 diam(X )L\u2206 G \u2212 2B\u03b4\u2206 = U\n\u2217 \u2212 2 \u221a\n2BL G diam(X )\u03b4,\nwhich gives the lower bound for OBJR(\u03b4) in Theorem 5.2.\nUsing Lemma 5.4 with \u2206 = \u221a\n2BG\u03b4 Ldiam(X )+2B diam(X )dist(C,\u2202X ) and the bound for OBJD(\u2206) in Theorem\n5.3,\nOBJR(\u03b4) \u2265 OBJD(\u2206)\u2212 2B\u03b4\u2206 \u2265 U \u2217 \u2212 ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u2206 G \u2212 2B\u03b4\u2206\n= U\u2217 \u2212 2 \u221a\n2B G ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4.\nThis proves the lower bound for OBJR(\u03b4) in Theorem 5.3.\nThe bound OBJR(\u03b4) \u2265 U\u2217 \u2212 O( \u221a \u03b4) is tight. We note that, in Theorem 5.2 and 5.3, the maximin objective with randomized agent strategies is bounded by OBJR(\u03b4) \u2265 U\u2217 \u2212O( \u221a \u03b4) while the objective with deterministic agent strategies is bounded by OBJD(\u03b4) \u2265 U\u2217 \u2212O(\u03b4). This is not because our analysis is not tight. In fact, the tight bound for OBJR(\u03b4) is U\u2217 \u2212 \u0398( \u221a \u03b4), with a squared root dependency on \u03b4. We prove this by giving an example where OBJR(\u03b4) \u2264 U\u2217\u2212\u2126( \u221a \u03b4). Consider the following classical Bayesian persuasion example with two states and two actions:\nExample 5.1. There are two states \u2126 = {Good,Bad}, two actions A = {a, b}. The sender wants the receiver to take action a regardless of the state. Action a gives the receiver utility 1 at Good state and \u22121 at Bad state; action b gives utility 0. The prior is represented by a real number \u00b50 \u2208 [0, 1] denoting the probability of Good state. Assume \u00b50 < 12 , so the receiver will take action b by default.\nsender a b receiver a b Good 1 0 Good 1 0 Bad 1 0 Bad \u22121 0\nIn this example, for \u03b4 < \u00b502 , OBJ R(\u03b4) \u2264 U\u2217 \u2212 2 \u221a 2\u00b50\u03b4 + \u03b4 = U \u2217 \u2212 \u2126( \u221a \u03b4). (The proof is in Appendix C.3.)"
        },
        {
            "heading": "5.1 Proof of Theorem 5.2 and 5.3",
            "text": "Since we have shown that the lower bounds for OBJD(\u03b4) imply the lower bounds for OBJR(\u03b4), we only need to prove the lower bounds for OBJD(\u03b4) and the upper bounds for OBJ R (\u03b4).\nLemma 5.5. In an unconstrained generalized principal-agent problem, OBJD(\u03b4) \u2265 U\u2217\u2212diam(X )L \u03b4 G .\nWith the constraint \u2211 s\u2208S \u03c0sxs \u2208 C, OBJD(\u03b4) \u2265 U\u2217 \u2212 ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nLemma 5.6. In an unconstrained generalized principal-agent problem, OBJ R (\u03b4) \u2264 U\u2217+diam(X )L \u03b4\nG .\nWith the constraint \u2211 s\u2208S \u03c0sxs \u2208 C, OBJ R (\u03b4) \u2264 U\u2217 + ( diam(X )L+ 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nLemma 5.5 and 5.6 immediately prove Theorem 5.2 and 5.3. We prove Lemma 5.6 here and Lemma 5.5 in Appendix C.4. The main idea is the following: let (\u03c0, \u03c1) be a pair of principal\u2019s strategy and agent\u2019s \u03b4-best-responding strategy that gives the principal a high utility. We perturb the principal\u2019s strategy \u03c0 slightly to be a strategy \u03c0\u2032 for which \u03c1 is best-responding (such a perturbation is possible due to Assumption 5.1). Since \u03c1 is bestresponding to \u03c0\u2032, the pair (\u03c0\u2032, \u03c1) cannot give the principal a higher utility than U\u2217 (which is the optimal principal utility under the best-response model). This means that the original pair (\u03c0, \u03c1) cannot give the principal a utility much higher than U\u2217.\nProof of Lemma 5.6. Let \u03c0 be a principal strategy and \u03c1 \u2208 R\u03b4(\u03c0) be a \u03b4-best-responding randomized strategy of the agent. The principal strategy \u03c0 consists of pairs {(\u03c0s, xs)}s\u2208S with\n\u2211 s\u2208S \u03c0sxs =: \u00b50 \u2208 C. (20)\nAt signal s, the agent takes action a with probability \u03c1(a|s). Let \u03b4s,a be the \u201csuboptimality\u201d of action a with respect to xs:\n\u03b4s,a = max a\u2032\u2208A\n{ v(xs, a \u2032)\u2212 v(xs, a) } . (21)\nBy Claim 5.1, for action a there exists ya \u2208 X such that v(ya, a)\u2212 v(ya, a\u2032) \u2265 G for any a\u2032 6= a. Let \u03b8s,a =\n\u03b4s,a G+\u03b4s,a \u2208 [0, 1] and let x\u0303s,a be the convex combination of xs and ya with weights 1\u2212 \u03b8s,a, \u03b8s,a:\nx\u0303s,a = (1\u2212 \u03b8s,a)xs + \u03b8s,aya. (22)\nClaim 5.7. We have two useful claims regarding x\u0303s,a and \u03b8s,a:\n(1) a is an optimal action for the agent with respect to x\u0303s,a: v(x\u0303s,a, a)\u2212 v(x\u0303s,a, a\u2032) \u2265 0,\u2200a\u2032 \u2208 A. (2) \u2211\ns\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,a \u2264 \u03b4G .\nProof. (1) For any a\u2032 6= a, by the definition of x\u0303s,a and \u03b8s,a,\nv(x\u0303s,a, a)\u2212 v(x\u0303s,a, a\u2032) = (1\u2212 \u03b8s,a) [ v(xs, a)\u2212 v(xs, a\u2032) ] + \u03b8s,a [ v(ya, a)\u2212 v(ya, a\u2032) ]\n\u2265 (1\u2212 \u03b8s,a)(\u2212\u03b4s,a) + \u03b8s,aG = GG+\u03b4s,a (\u2212\u03b4s,a) + \u03b4s,a G+\u03b4s,a G = 0.\n(2) By the condition that \u03c1 is a \u03b4-best-response to \u03c0, we have\n\u03b4 \u2265 max \u03c1\u2217:S\u2192A\nV (\u03c0, \u03c1\u2217)\u2212 V (\u03c0, \u03c1) = \u2211\ns\u2208S \u03c0s\n(\nmax a\u2032\u2208A\n{ v(xs, a \u2032) } \u2212\n\u2211 a\u2208A \u03c1(a|s)v(xs, a)\n)\n= \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)max a\u2032\u2208A { v(xs, a \u2032)\u2212 v(xs, a) } = \u2211 s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b4s,a.\nSo, \u2211 s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,a = \u2211 s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s) \u03b4s,a G+\u03b4s,a \u2264 \u2211s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s) \u03b4s,a G \u2264 \u03b4 G .\nWe let \u00b5\u2032 be the convex combination of {x\u0303s,a}s,a\u2208S\u00d7A with weights {\u03c0s\u03c1(a|s)}s,a\u2208S\u00d7A:\n\u00b5\u2032 = \u2211\ns,a\u2208S\u00d7A \u03c0s\u03c1(a|s)x\u0303s,a. (23)\nNote that \u00b5\u2032 might not satisfy the constraint \u00b5\u2032 \u2208 C. So, we want to find another vector z \u2208 X and a coefficient \u03b7 \u2208 [0, 1] such that\n(1\u2212 \u03b7)\u00b5\u2032 + \u03b7z \u2208 C. (24)\n(If \u00b5\u2032 already satisfies \u00b5\u2032 \u2208 C, then let \u03b7 = 0.) To do this, we consider the ray pointing from \u00b5\u2032 to \u00b50: {\u00b5\u2032 + t(\u00b50 \u2212 \u00b5\u2032) | t \u2265 0}. Let z be the intersection of the ray with the boundary of X :\nz = \u00b5\u2032 + t\u2217(\u00b50 \u2212 \u00b5\u2032), t\u2217 = argmax{t \u2265 0 | \u00b5\u2032 + t(\u00b50 \u2212 \u00b5\u2032) \u2208 X}.\nThen, rearranging z = \u00b5\u2032 + t\u2217(\u00b50 \u2212 \u00b5\u2032), we get 1 t\u2217 (z \u2212 \u00b5\u2032) = \u00b50 \u2212 \u00b5\u2032 \u21d0\u21d2 (1\u2212 1t\u2217 )\u00b5 \u2032 + 1 t\u2217 z = \u00b50 \u2208 C,\nwhich satisfies (24) with \u03b7 = 1 t\u2217 . We then give an upper bound on \u03b7 = 1 t\u2217 :\nClaim 5.8. \u03b7 = 1 t\u2217 \u2264 diam(X )dist(C,\u2202X ) \u03b4 G . (See proof in Appendix C.5.)\nThe convex combinations (24) (23) define a new principal strategy \u03c0\u2032 (with |S|\u00d7 |A|+1 signals) consisting of x\u0303s,a with probability (1\u2212 \u03b7)\u03c0s\u03c1(a|s) and z with probability \u03b7. Consider the following deterministic agent strategy \u03c1\u2032 in response to \u03c0\u2032: for x\u0303s,a, take action \u03c1\u2032(x\u0303s,a) = a; for z, take any action that is optimal for z. We note that \u03c1\u2032 is a best-response to \u03c0\u2032, \u03c1\u2032 \u2208 R0(\u03c0\u2032), because, according to Claim 5.7, a is an optimal action with respect to x\u0303s,a.\nThen, consider the principal\u2019s utility under \u03c0\u2032 and \u03c1\u2032:\nU(\u03c0\u2032, \u03c1\u2032) (24),(23) = (1\u2212 \u03b7) \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)u(x\u0303s,a, \u03c1\u2032(x\u0303s,a)) + \u03b7u(z, \u03c1\u2032(z))\n\u2265 (1\u2212 \u03b7) \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)u(x\u0303s,a, a) \u2212 \u03b7B\n\u2265 (1\u2212 \u03b7) \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)\n(\nu(xs, a)\u2212 L \u2016x\u0303s \u2212 xs\u2016 \ufe38 \ufe37\ufe37 \ufe38\n=\u2016\u03b8s,a(ya\u2212xs)\u2016\u2264\u03b8s,adiam(X )\n)\n\u2212 \u03b7B\n\u2265 (1\u2212 \u03b7)U(\u03c0, \u03c1) \u2212 Ldiam(X ) \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,a \u2212 \u03b7B\n(Claim 5.7) \u2265 U(\u03c0, \u03c1)\u2212 Ldiam(X ) \u03b4 G \u2212 2\u03b7B (Claim 5.8) \u2265 U(\u03c0, \u03c1)\u2212 ( Ldiam(X ) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nRearranging, U(\u03c0, \u03c1) \u2264 U(\u03c0\u2032, \u03c1\u2032)+ ( Ldiam(X )+2B diam(X )dist(C,\u2202X ) ) \u03b4 G . Note that this argument holds for any pair (\u03c0, \u03c1) that satisfies \u03c1 \u2208 R\u03b4(\u03c0). And recall that \u03c1\u2032 \u2208 R0(\u03c0\u2032). So, we conclude that\nOBJ R (\u03b4) = max\n\u03c0 max \u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1) \u2264 max \u03c0\u2032 max \u03c1\u2032\u2208R0(\u03c0) U(\u03c0\u2032, \u03c1\u2032) + ( Ldiam(X ; \u21131) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G\n= U\u2217 + ( Ldiam(X ; \u21131) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nThis proves the case with the constraint \u2211\ns\u2208S \u03c0sxs \u2208 C. The case without \u2211\ns\u2208S \u03c0sxs \u2208 C is proved by letting \u03b7 = 0 in the above argument."
        },
        {
            "heading": "6 Applications to Specific Principal-Agent Problems",
            "text": "In this section, we apply the general results in Section 4 and 5 to specific principal-agent problems to derive some concrete results for Bayesian persuasion, Stackelberg games, and contract design with learning agents. Our result for Bayesian persuasion is new, while our results for Stackelberg games and contract design refine Deng et al. (2019) and Guruganesh et al. (2024)."
        },
        {
            "heading": "6.1 Bayesian Persuasion",
            "text": "As noted in Section 3, Bayesian persuasion is a generalized principal-agent problem with constraint\n\u2211 s\u2208S \u03c0sxs \u2208 C = {\u00b50}\nwhere each xs = \u00b5s = (\u00b5s(\u03c9))\u03c9\u2208\u2126 \u2208 X = \u2206(\u2126) is a posterior belief. Suppose the principal\u2019s utility is bounded: |u(\u03c9, a)| \u2264 B. Then, the principal\u2019s utility function u(\u00b5s, a) = \u2211\n\u03c9\u2208\u2126 \u00b5s(\u03c9)u(\u03c9, a) is (L = B)-Lipschitz (under \u21131-norm):\n|u(\u00b51, a)\u2212 u(\u00b52, a)| \u2264 \u2211\n\u03c9\u2208\u2126 |\u00b51(\u03c9)\u2212 \u00b52(\u03c9)| \u00b7 |u(\u03c9, a)\n\u2223 \u2223 \u2264 \u2016\u00b51 \u2212 \u00b52\u20161B, (25)\nso Assumption 5.3 is satisfied. Suppose the prior \u00b50 has positive probability for every \u03c9 \u2208 \u2126, and let\np0 = min \u03c9\u2208\u2126 \u00b50(\u03c9) > 0.\nThen, we have the distance\ndist(C, \u2202X) = min { \u2016\u00b50 \u2212 \u00b5\u20161 : \u00b5 \u2208 \u2206(\u2126) s.t. \u00b5(\u03c9) = 0 for some \u03c9 \u2208 \u2126 } \u2265 p0 > 0,\nso Assumption 5.2 is satisfied. The diameter satisfies, because \u2016\u00b5\u20161 = 1 for \u00b5 \u2208 \u2206(\u2126),\ndiam(X ; \u21131) = max \u00b51,\u00b52\u2208\u2206(\u2126) \u2016\u00b51 \u2212 \u00b52\u20161 \u2264 2.\nFinally, we assume Assumption 5.1 (no dominated action for the agent). Then, Theorem 5.3 gives bounds on the approximate-best-response objectives in Bayesian persuasion:\nCorollary 6.1 (Bayesian persuasion with approximate best response). For 0 \u2264 \u03b4 < Gp02 ,\n\u2022 OBJD(\u03b4) \u2265 U\u2217 \u2212 2B(1 + 2 p0 ) \u03b4 G . \u2022 OBJR(\u03b4) \u2265 U\u2217 \u2212 4B \u221a\n(1 + 2 p0 ) \u03b4 G .\n\u2022 OBJ D (\u03b4) \u2264 OBJR(\u03b4) \u2264 U\u2217 + 2B(1 + 2\np0 ) \u03b4 G .\nFurther applying Theorem 4.1 and 4.3, we obtain the central result of our main problem, persuasion with a learning agent:\nCorollary 6.2 (Persuasion with a learning agent). Suppose T is sufficiently large such that CReg(T ) T\n< Gp0 2 and CSReg(T ) T < Gp02 , then\n\u2022 with a contextual no-regret learning agent, the principal can obtain utility at least\n1 T E [\nT\u2211\nt=1\nu(xt, at) ] \u2265 OBJR (CReg(T ) T ) \u2265 U\u2217 \u2212 4B\n\u221a\n(1 + 2 p0 ) 1 G\n\u221a CReg(T )\nT (26)\nusing a fixed signaling schemes in all rounds.\n\u2022 with a contextual no-swap-regret learning agent, the principal\u2019s obtainable utility is at most\n1 T E [\nT\u2211\nt=1\nu(xt, at) ] \u2264 OBJD (CSReg(T ) T ) \u2264 U\u2217 + 2B(1 + 2 p0 ) 1 G CSReg(T ) T\n(27)\neven using time-varying signaling schemes.\nThe result (27) is particularly interesting. Recall (from Section 2) that the learning agent actually chooses strategy before the principal, so the principal can best respond to the agent\u2019s strategy. Nevertheless, (27) shows that the principal cannot do much better than the classic model where the principal moves first. This is a special property of no-swap-regret learning. If the agent does no-regret but not no-swap-regret learning, then the principal can do significantly better than U\u2217 in some instances, as we showed in Section 4.3."
        },
        {
            "heading": "6.2 Stackelberg Games",
            "text": "In a Stackelberg game, the principal (leader), having a finite action set B, first commits to a mixed strategy x = (x(b))b\u2208B \u2208 \u2206(B), which is a distribution over actions. So the principal\u2019s decision space X is \u2206(B). The agent (follower) then takes an action a \u2208 A in response to x. The (expected) utilities for the two players are u(x, a) = \u2211\nb\u2208B x(b)u(b, a) and v(x, a) = \u2211\nb\u2208B x(b)u(b, a). The signal s can (but not necessarily) be an action that the principal recommends the agent to take.\nAssume bounded utility |u(b, a)| \u2264 B. Then, the principal\u2019s utility function is bounded in [\u2212B,B] and (L = B)-Lipschitz (by a similar argument as (25)). The diameter diam(X ) = maxx1,x2\u2208\u2206(B) \u2016x1 \u2212 x2\u20161 \u2264 2. Applying the theorem for unconstrained generalized principal-agent problems (Theorem 5.2) and the theorems for learning agent (Theorem 4.1 and 4.3), we obtain:\nCorollary 6.3 (Stackelberg game with a learning agent). Suppose T is sufficiently large such that CReg(T )\nT < G and CSReg(T ) T < G, then: \u2022 with a contextual no-regret learning agent, the principal can obtain utility 1 T E [\u2211T t=1 u(x t, at) ] \u2265\nOBJR (CReg(T )\nT\n) \u2265 U\u2217 \u2212 4B\u221a\nG\n\u221a CReg(T )\nT using a fixed strategy in all rounds.\n\u2022 with a contextual no-swap-regret learning agent, the principal cannot obtain utility more than 1 T E [\u2211T t=1 u(x t, at) ] \u2264 OBJD (CSReg(T ) T ) \u2264 U\u2217 + 2B G CSReg(T ) T even using time-varying strate-\ngies.\nThe conclusion that the principal can obtain utility at least U\u2217\u2212o(1) against a no-regret learning agent and no more than U\u2217+ o(1) against a no-swap-regret agent in Stackelberg games was proved by Deng et al. (2019). Our Corollary 6.3 reproduces this conclusion and moreover provides bounds on the o(1) terms, namely, U\u2217 \u2212 O( \u221a\nCReg(T ) T ) and U\u2217 + O(CSReg(T ) T\n). This demonstrates the generality and usefulness of our framework."
        },
        {
            "heading": "6.3 Contract Design",
            "text": "In contract design, there is a finite outcome space O = {r1, . . . , rd} where each ri \u2208 R is a monetary reward to the principal. When the agent takes action a \u2208 A, outcome ri will happen with probability pai \u2265 0, \u2211d i=1 pai = 1. The principal cannot observe the action taken by the agent but can observe the realized outcome. The principal\u2019s decision space X is the set of contracts, where a contract\nx = (x(i)) d i=1 \u2208 [0,+\u221e]d is a vector that specifies the payment to the agent for each possible outcome. So, if the agent takes action a under contract x, the principal obtains expected utility\nu(x, a) = d\u2211\ni=1\npai(ri \u2212 x(i))\nand the agent obtains v(x, a) = \u2211d\ni=1 paix(i). The signal s can (but not necessarily) be an action\nthat the principal recommends the agent to take. The principal\u2019s decision space X \u2286 [0,+\u221e]d in contract design, however, may be unbounded and violate the requirement of bounded diameter diam(X ) that we need. We have two remedies for this.\nThe first remedy is to require the principal\u2019s payment to the agent be upper bounded by some constant P < +\u221e, so 0 \u2264 x(i) \u2264 P and X = [0, P ]d. Under this requirement and the assumption of bounded reward |ri| \u2264 R, the principal\u2019s utility becomes bounded by |u(x, a)| \u2264 \u2211d i=1 pai(R+P ) = R+ P = B and (L = 1)-Lipschitz under \u2113\u221e-norm:\n|u(x1, a)\u2212 u(x2, a)| = \u2223 \u2223\nd\u2211\ni=1\npai(x1(i) \u2212 x2(i)) \u2223 \u2223 \u2264 dmax\ni=1 |x1(i) \u2212 x2(i)|\nd\u2211\ni=1\npai = \u2016x1 \u2212 x2\u2016\u221e. (28)\nAnd the diameter of X is bounded by (under \u2113\u221e-norm)\ndiam(X ; \u2113\u221e) = max x1,x2\u2208X \u2016x1 \u2212 x2\u2016\u221e = max x1,x2\u2208[0,P ]d d max i=1 |x1(i) \u2212 x2(i)| \u2264 P. (29)\nNow, we can apply the theorem for unconstrained generalized principal-agent problems (Theorem 5.2) and the theorems for learning agent (Theorem 4.1 and Theorem 4.3) to obtain:\nCorollary 6.4 (Contract design (with bounded payment) with a learning agent). Suppose T is sufficiently large such that CReg(T ) T < PG2(R+P ) and CSReg(T ) T < G, then:\n\u2022 with a contextual no-regret learning agent, the principal can obtain utility at least 1 T E [\u2211T t=1 u(x t, at) ]\n\u2265 OBJR (CReg(T )\nT\n) \u2265 U\u2217 \u2212 2\n\u221a 2(R+P )P\nG\n\u221a CReg(T )\nT using a fixed contract in all rounds.\n\u2022 with contextual a no-swap-regret learning agent, the principal cannot obtain utility more than 1 T E [\u2211T t=1 u(x t, at) ] \u2264 OBJD (CSReg(T ) T ) \u2264 U\u2217+ P G CSReg(T ) T even using time-varying contracts.\nThe second remedy is to write contract design as a generalized principal-agent problem in another way. Let x\u0303 = (x\u0303(a))a\u2208A \u2208 [0,+\u221e]|A| be a vector recording the expected payment from the principal to the agent for each action a \u2208 A:\nx\u0303(a) = d\u2211\ni=1\npaix(i). (30)\nAnd let r\u0303(a) be the expected reward of action a, r\u0303(a) = \u2211d\ni=1 pairi. Then, the principal and the agent\u2019s utility can be rewritten as functions of x\u0303 and a:\nu(x\u0303, a) = r\u0303(a) \u2212 x\u0303(a), v(x\u0303, a) = x\u0303(a), (31)\nwhich are linear in x\u0303 \u2208 X\u0303 . Assuming bounded reward |r\u0303(a)| \u2264 R, we can without loss of generality assume that the expected payment x\u0303(a) is bounded by R as well, because otherwise the principal will get negative utility. So, the principal\u2019s decision space can be restricted to\nX\u0303 = { x\u0303 | \u2203 x \u2208 [0,+\u221e]d such that x\u0303(a) =\nd\u2211\ni=1\npaix(i) for every a \u2208 A } \u2229 [0, R]|A|, (32)\nwhich is convex and has bounded diameter (under \u2113\u221e norm)\ndiam(X\u0303 ; \u2113\u221e) \u2264 diam([0, R]|A|; \u2113\u221e) = R. (33)\nThe utility function u(x\u0303, a) is bounded by 2R and (L = 1)-Lipschitz (under \u2113\u221e norm):\n|u(x\u03031, a)\u2212 u(x\u03032, a)| = |x\u03031(a) \u2212 x\u03032(a)| \u2264 max a\u2208A |x\u03031(a) \u2212 x\u03032(a)| = \u2016x\u03031 \u2212 x\u03032\u2016\u221e. (34)\nThus, we can apply the theorem for unconstrained generalized principal-agent problems (Theorem 5.2) and the theorems for learning agent (Theorem 4.1 and Theorem 4.3) to obtain:\nCorollary 6.5 (Contract design with a learning agent). Suppose T is sufficiently large such that CReg(T )\nT < G2 and CSReg(T ) T < G, then: \u2022 with a contextual no-regret learning agent, the principal can obtain utility at least 1 T E [\u2211T t=1 u(x t, at) ]\n\u2265 OBJR (CReg(T )\nT\n) \u2265 U\u2217 \u2212 4R\u221a\nG\n\u221a CReg(T )\nT using a fixed contract in all rounds.\n\u2022 with a contextual no-swap-regret learning agent, the principal cannot obtain utility more than 1 T E [\u2211T t=1 u(x t, at) ] \u2264 OBJD (CSReg(T ) T ) \u2264 U\u2217+ R G CSReg(T ) T even using time-varying contracts.\nProviding the quantitative lower and upper bounds, the above results refine the result in Guruganesh et al. (2024) that the principal can obtain utility at least U\u2217 \u2212 o(1) against a noregret learning agent and no more than U\u2217 + o(1) against a no-swap-regret agent. This again demonstrates the versatility of our general framework."
        },
        {
            "heading": "A Proof of Proposition 2.1",
            "text": "Let A be an arbitrary no-regret (no-swap-regret) learning algorithm for a multi-armed bandit (MAB) problem with |A| arms. There exist such algorithms with regret O( \u221a\nT |A| log |A|) (variants of Exp3 (Auer et al., 2002)) and even O( \u221a\nT |A|) (doubling trick + polyINF (Audibert and Bubeck, 2010)) for any time horizon T > 0. By swap-to-external regret reductions, they can be converted to multi-armed bandit algorithms with swap regret O( \u221a\nT |A|3 log |A|) (Blum and Mansour, 2007) and O(|A| \u221a T ) (Ito, 2020). We then convert A into a contextual no-regret (contextual no-swap-regret) algorithm, in the following way:\nAlgorithm 1: Convert any MAB algorithm to a contextual MAB algorithm\nInput: MAB algortihm A. Arm set A. Context set S. Instantiate |S| copies A1, . . . ,A|S| of A, and initialize their round number by t1 = \u00b7 \u00b7 \u00b7 = t|S| = 0. for round t = 1, 2, . . . do Receive context st. Call Ast to obtain an action at. Play at and obtain feedback (which includes the reward vt(at) of action at). Feed the feedback to Ast. Increase its round number tst by 1. end\nProposition A.1. The contextual regret of Algorithm 1 is at most\nCReg(T ) \u2264 max { |S|\u2211\ns=1\nReg(Ts) \u2223 \u2223 \u2223 T1 + \u00b7 \u00b7 \u00b7 + T|S| = T } ,\nwhere Reg(Ts) is the regret of A for time horizon Ts. The contextual swap-regret of Algorithm 1 is at most\nCSReg(T ) \u2264 max { |S|\u2211\ns=1\nSReg(Ts) \u2223 \u2223 \u2223 T1 + \u00b7 \u00b7 \u00b7+ T|S| = T } ,\nwhere SReg(Ts) is the swap-regret of A for time horizon Ts. When plugging in Reg(Ts) = O( \u221a |A|Ts), we obtain CReg(T ) \u2264 O( \u221a\n|A||S|T ). When plugging in SReg(Ts) = O(|A| \u221a Ts), we obtain CSReg(T ) \u2264 O(|A| \u221a\n|S|T ). Proof. The contextual regret of Algorithm 1 is\nCReg(T ) = max d:S\u2192A E\n[ T\u2211\nt=1\n( vt(d(st))\u2212 vt(at)\n)]\n= max d:S\u2192A E\n[ |S|\u2211\ns=1\n\u2211\nt:st=s\n( vt(d(s))\u2212 vt(at)\n)]\n\u2264 |S| \u2211\ns=1\nmax a\u2032\u2208A E\n[ \u2211\nt:st=s\n( vt(a\u2032)\u2212 vt(at)\n)]\n\u2264 |S| \u2211\ns=1\nETs [ Reg(Ts) ] where Ts is the number of rounds where s t = s\n\u2264 max { |S|\u2211\ns=1\nReg(Ts) \u2223 \u2223 \u2223 T1 + \u00b7 \u00b7 \u00b7 + T|S| = T } .\nWhen Reg(Ts) = O( \u221a |A|Ts), by Jensen\u2019s inequality we obtain\nCReg(T ) \u2264 |S| \u2211\ns=1\nO( \u221a |A|Ts) \u2264 O( \u221a |A|) \u221a |S|\n\u221a \u221a \u221a \u221a |S| \u2211\ns=1\nTs = O( \u221a |A||S|T ).\nThe argument for contextual swap-regret is similar:\nCSReg(T ) = max d:S\u00d7A\u2192A E\n[ T\u2211\nt=1\n( vt(d(st, at))\u2212 vt(at)\n)]\n= max d:S\u00d7A\u2192A E\n[ |S|\u2211\ns=1\n\u2211\nt:st=s\n( vt(d(s, at))\u2212 vt(at)\n)]\n\u2264 |S| \u2211\ns=1\nmax d\u2032:A\u2192A E\n[ \u2211\nt:st=s\n( vt(d\u2032(at))\u2212 vt(at)\n)]\n\u2264 |S| \u2211\ns=1\nETs [ SReg(Ts) ] where Ts is the number of rounds where s t = s\n\u2264 max { |S|\u2211\ns=1\nSReg(Ts) \u2223 \u2223 \u2223 T1 + \u00b7 \u00b7 \u00b7+ T|S| = T } .\nWhen SReg(Ts) = O(|A| \u221a Ts), by Jensen\u2019s inequality we obtain\nCSReg(T ) \u2264 |S| \u2211\ns=1\nO(|A| \u221a Ts) \u2264 O(|A|) \u221a |S|\n\u221a \u221a \u221a \u221a |S| \u2211\ns=1\nTs = O(|A| \u221a |S|T )."
        },
        {
            "heading": "B Proof of Example 3.1",
            "text": "Consider the quantal response model. Let \u03b3 = log(|A|\u03bb) \u03bb\n. Given signal s, with posterior \u00b5s, we say an action a \u2208 A is not \u03b3-optimal for posterior \u00b5s if\nv(\u00b5s, a \u2217 s)\u2212 v(\u00b5s, a) \u2265 \u03b3\nwhere a\u2217s is an optimal action for \u00b5s. The probability that the receiver chooses not \u03b3-optimal action a is at most:\nexp(\u03bbv(\u00b5s, a)) \u2211 a\u2208A exp(\u03bbv(\u00b5s, a)) \u2264 exp(\u03bbv(\u00b5s, a)) exp(\u03bbv(\u00b5s, a\u2217s)) = exp\n(\n\u2212 \u03bb [ v(\u00b5s, a \u2217 s)\u2212 v(\u00b5s, a)\n])\n\u2264 exp(\u2212\u03bb\u03b3) = 1|A|\u03bb.\nBy a union bound, the probability that the receiver chooses any not \u03b3-approxiamtely optimal action is at most 1\n\u03bb . So, the expected loss of utility of the receiver due to not taking the optimal action is\nat most\n(1\u2212 1 \u03bb ) \u00b7 \u03b3 + 1 \u03bb \u00b7 1 \u2264 log(|A|\u03bb) + 1 \u03bb\nThis means that the quantal response strategy is a log(|A|\u03bb)+1 \u03bb\n-best-responding randomized strategy. Consider inaccurate belief. Given signal s, the receiver has belief \u00b5\u2032s with total variation distance dTV(\u00b5 \u2032 s, \u00b5s) \u2264 \u03b5 to the true posterior \u00b5s. For any action a \u2208 A, the difference of expected utility of action a under beliefs \u00b5\u2032s and \u00b5s is at most \u03b5: \u2223 \u2223E\u03c9\u223c\u00b5\u2032s [v(\u03c9, a)] \u2212 E\u03c9\u223c\u00b5s [v(\u03c9, a)] \u2223 \u2223 \u2264 dTV(\u00b5\u2032s, \u00b5s) \u2264 \u03b5. So, the optimal action for \u00b5\u2032s is a 2\u03b5-optimal action for \u00b5s. This means that the receiver strategy is a deterministic 2\u03b5-best-responding strategy."
        },
        {
            "heading": "C Missing Proofs from Section 5",
            "text": "C.1 Proof of Claim 5.1\nIf no G > 0 satisfies the claim, then there must exist an a0 \u2208 A such that for all x \u2208 X , v(a0, \u00b5)\u2212 v(a\u2032, \u00b5) \u2264 0 for some a\u2032 \u2208 A \\ {a0}. Namely,\nmax x\u2208X min a\u2032\u2208A\\{a0}\n{ v(x, a0)\u2212 v(x, a\u2032) } \u2264 0.\nThen, by the minimax theorem, we have\nmin \u03b1\u2032\u2208\u2206(A\\{a0}) max x\u2208X\n{ v(x, a0)\u2212 v(x, \u03b1\u2032) } = max\nx\u2208X min\na\u2032\u2208A\\{a0}\n{ v(x, a0)\u2212 v(x, a\u2032) } \u2264 0.\nThis means that a0 is weakly dominated by some mixed action \u03b1 \u2032 \u2208 \u2206(A \\ {a0}), violating Assumption 5.1.\nC.2 Proof of Lemma 5.4 Let A\u2206(x) = { a \u2208 A | v(x, a) \u2265 v(x, a\u2032) \u2212 \u2206,\u2200a\u2032 \u2208 A } be the set of \u2206-optimal actions of the agent in response to principal decision x \u2208 X . The proof of Lemma 5.4 uses another lemma that relates the principal utility under a randomized \u03b4-best-responding agent strategy \u03c1 \u2208 R\u03b4(\u03c0) and that under an agent strategy \u03c1\u2032 that only randomizes over A\u2206(xs).\nLemma C.1. Let \u03c0 = {(\u03c0s, xs)}s\u2208S be a principal strategy and \u03c1 \u2208 R\u03b4(\u03c0) be a randomized \u03b4-bestresponse to \u03c0. For any \u2206 > 0, there exists an agent strategy \u03c1\u2032 : s 7\u2192 \u2206(A\u2206(xs)) that randomizes over \u2206-optimal actions only for each xs, such that the principal\u2019s utility under \u03c1\n\u2032 and \u03c1 satisfies: \u2223 \u2223U(\u03c0, \u03c1\u2032)\u2212 U(\u03c0, \u03c1)\n\u2223 \u2223 \u2264 2B\u03b4\u2206 .\nProof. Let a\u2217s = maxa\u2208A v(xs, a) be the agent\u2019s optimal action for xs. Let A\u2206(xs) = A \\ A\u2206(xs) be the set of actions that are not \u2206-optimal for xs. By the definition that \u03c1 \u2208 R\u03b4(\u03c0) is a \u03b4-bestresponse to \u03c0, we have\n\u03b4 \u2265 \u2211\ns\u2208S \u03c0s\n[\nv(xs, a \u2217 s)\u2212\n\u2211 a\u2208A \u03c1(a|s)v(xs, a)\n]\n= \u2211\ns\u2208S \u03c0s\n( \u2211\na\u2208A\u2206(xs) \u03c1(a|s)\n[ v(xs, a \u2217 s)\u2212 v(xs, a)\n\ufe38 \ufe37\ufe37 \ufe38\n\u22650\n] +\n\u2211\na\u2208A\u2206(xs)\n\u03c1(a|s) [ v(xs, a \u2217 s)\u2212 v(xs, a)\n\ufe38 \ufe37\ufe37 \ufe38\n>\u2206\n])\n\u2265 0 + \u2206 \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\u2206(xs)\n\u03c1(a|s)\n= \u2206 \u2211\ns\u2208S \u03c0s\u03c1(A\u2206(xs) | s).\nRearranging, \u2211\ns\u2208S \u03c0s\u03c1(A\u2206(xs) | s) \u2264\n\u03b4 \u2206 . (35)\nThen, we consider the randomized strategy \u03c1\u2032 that, for each s, chooses each action a \u2208 A\u2206(xs) with the conditional probability that \u03c1 chooses a given a \u2208 A\u2206(xs):\n\u03c1\u2032(a | s) = \u03c1(a | s) \u03c1(A\u2206(xs) | s) .\nThe sender\u2019s utility under \u03c1\u2032 is:\nU(\u03c0, \u03c1\u2032) = \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\u2206(xs)\n\u03c1(a | s) \u03c1(A\u2206(xs) | s) u(xs, a).\nThe sender\u2019s utility under \u03c1 is\nU(\u03c0, \u03c1) = \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\u2206(xs) \u03c1(a | s)u(xs, a) +\n\u2211 s\u2208S \u03c0s \u2211\na\u2208A\u2206(xs)\n\u03c1(a | s)u(xs, a)\nTaking the difference between the two utilities, we get \u2223 \u2223U(\u03c0, \u03c1\u2032)\u2212 U(\u03c0, \u03c1) \u2223 \u2223\n\u2264 \u2223 \u2223 \u2223 \u2211\ns\u2208S \u03c0s\n( 1\n\u03c1(A\u2206(xs) | s) \u2212 1\n) \u2211\na\u2208A\u2206(xs) \u03c1(a | s)u(xs, a)\n\u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\u2206(xs)\n\u03c1(a | s)u(xs, a) \u2223 \u2223 \u2223\n= \u2223 \u2223 \u2223 \u2211\ns\u2208S \u03c0s\n1\u2212 \u03c1(A\u2206(xs) | s) \u03c1(A\u2206(xs) | s) \u2211\na\u2208A\u2206(xs) \u03c1(a | s)u(xs, a)\n\u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2211\ns\u2208S \u03c0s\n\u2211\na\u2208A\u2206(xs)\n\u03c1(a | s)u(xs, a) \u2223 \u2223 \u2223\n\u2264 \u2211\ns\u2208S \u03c0s\n1\u2212 \u03c1(A\u2206(xs) | s) \u03c1(A\u2206(xs) | s) \u2211\na\u2208A\u2206(xs) \u03c1(a | s) \u00b7 B +\n\u2211 s\u2208S \u03c0s \u2211\na\u2208A\u2206(\u00b5s)\n\u03c1(a | s) \u00b7B\n= B \u2211\ns\u2208S \u03c0s\n\u03c1(A\u2206(xs) | s) \u03c1(A\u2206(xs) | s) \u03c1(A\u2206(xs) | s) + B \u2211 s\u2208S \u03c0s\u03c1(A\u2206(xs) | s)\n= 2B \u2211\ns\u2208S \u03c0s\u03c1(A\u2206(xs) | s)\n(35)\n\u2264 2B\u03b4 \u2206 .\nThis proves the lemma.\nWe now prove Lemma 5.4.\nProof of Lemma 5.4. Consider the objective OBJR(\u03b4) = sup\u03c0 min\u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1). By Lemma C.1, for any (\u03c0, \u03c1) there exists an agent strategy \u03c1\u2032 : s 7\u2192 \u2206(A\u2206(xs)) that only randomizes over \u2206- optimal actions such that \u2223 \u2223U(\u03c0, \u03c1\u2032)\u2212U(\u03c0, \u03c1)\n\u2223 \u2223 \u2264 2B\u03b4\u2206 . Because minimizing over \u2206(A\u2206(xs)) is equiv-\nalent to minimizing over A\u2206(xs), which corresponds to deterministic \u2206-best-responding strategies, we get:\nOBJR(\u03b4) = sup \u03c0 min \u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1) \u2265 sup \u03c0 min \u03c1\u2032:s 7\u2192\u2206(A\u2206(xs)) U(\u03c0, \u03c1\u2032)\u2212 2B\u03b4\u2206\n= sup \u03c0 min \u03c1\u2032:s 7\u2192A\u2206(xs)\nU(\u03c0, \u03c1\u2032)\u2212 2B\u03b4\u2206\n= OBJD(\u2206)\u2212 2B\u03b4\u2206 .\nC.3 Proof of Example 5.1\nFirst, the sender\u2019s optimal utility when the receiver exactly best responds is 2\u00b50:\nU\u2217 = 2\u00b50.\nThis is achieved by decomposing the prior \u00b50 into two posteriors \u00b5a = 1 2 and \u00b5b = 0 with probability 2\u00b50 and 1\u2212 2\u00b50 respectively, with the receiver taking action a under posterior \u00b5a and b under \u00b5b. Then, consider any signaling scheme of the sender, \u03c0 = {(\u03c0s, \u00b5s)}s\u2208S , which is a decomposition of the prior \u00b50 into |S| posteriors \u00b5s \u2208 [0, 1] such that \u2211\ns\u2208S \u03c0s\u00b5s = \u00b50. Let \u03c1 : S \u2192 \u2206(A) be a randomized strategy of the receiver, where \u03c1(a|s) (and \u03c1(b|s)) denotes the probability that the receiver takes action a (and b) under signal s. The sender\u2019s expected utility under \u03c0 and \u03c1 is:\nU(\u03c0, \u03c1) = \u2211 s\u2208S \u03c0s [ \u03c1(a|s) \u00b7 1 + \u03c1(b|s) \u00b7 0 ] = \u2211 s\u2208S \u03c0s\u03c1(a|s). (36)\nThe receiver\u2019s utility when taking action a at posterior \u00b5s is \u00b5s \u00b7 1 + (1\u2212 \u00b5s) \u00b7 (\u22121) = 2\u00b5s \u2212 1. So, the receiver\u2019s expected utility under \u03c0 and \u03c1 is\nV (\u03c0, \u03c1) = \u2211 s\u2208S \u03c0s [ \u03c1(a|s) \u00b7 (2\u00b5s \u2212 1) + \u03c1(b|s) \u00b7 0 ] = \u2211 s\u2208S \u03c0s\u03c1(a|s)(2\u00b5s \u2212 1). (37)\nClearly, the receiver\u2019s best response \u03c1\u2217 is to take action a with certainty if and only if \u00b5s > 1 2 , with expected utility\nV (\u03c0, \u03c1\u2217) = \u2211\ns:\u00b5s> 1 2\n\u03c0s(2\u00b5s \u2212 1). (38)\nTo find OBJR(\u03b4) = sup\u03c0 min\u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1), we fix any \u03c0 and solve the inner optimization problem (minimizing the sender\u2019s utility) regarding \u03c1:\nmin \u03c1\nU(\u03c0, \u03c1) = \u2211\ns\u2208S \u03c0s\u03c1(a|s)\ns.t. \u03c1 \u2208 R\u03b4(\u03c0) \u21d0\u21d2 \u03b4 \u2265 V (\u03c0, \u03c1\u2217)\u2212 V (\u03c0, \u03c1) = \u2211\ns:\u00b5s> 1 2\n\u03c0s(2\u00b5s \u2212 1)\u2212 \u2211\ns\u2208S \u03c0s\u03c1(a|s)(2\u00b5s \u2212 1).\nWithout loss of generality, we can assume that the solution \u03c1 satisfies \u03c1(a|s) = 0 whenever \u00b5s \u2264 12 (if \u03c1(a|s) > 0 for some \u00b5s \u2264 12 , then making \u03c1(a|s) to be 0 can decrease the objective \u2211\ns\u2208S \u03c0s\u03c1(a|s) while still satisfying the constraint). So, the optimization problem can be simplified to:\nmin \u03c1\nU(\u03c0, \u03c1) = \u2211\ns:\u00b5s> 1 2\n\u03c0s\u03c1(a|s)\ns.t. \u03b4 \u2265 \u2211\ns:\u00b5s> 1 2\n\u03c0s(2\u00b5s \u2212 1)\u2212 \u2211\ns:\u00b5s> 1 2\n\u03c0s\u03c1(a|s)(2\u00b5s \u2212 1)\n= \u2211\ns:\u00b5s> 1 2\n\u03c0s(2\u00b5s \u2212 1)(1 \u2212 \u03c1(a|s)),\n\u03c1(a|s) \u2208 [0, 1], \u2200s \u2208 S : \u00b5s > 12 .\nWe note that this is a fractional knapsack linear program, which has a greedy solution (e.g., (Korte and Vygen, 2012)): sort the signals with \u00b5s > 1 2 in increasing order of 2\u00b5s\u22121 (equivalently, increasing order of \u00b5s); label those signals by s = 1, . . . , n; find the first position k for which \u2211k\ns=1 \u03c0s(2\u00b5s \u2212 1) > \u03b4:\nk = min { j :\nj \u2211\ns=1\n\u03c0s(2\u00b5s \u2212 1) > \u03b4 } ;\nthen, an optimal solution \u03c1 is given by: \n\n\n\u03c1(a|s) = 0 for s = 1, . . . , k \u2212 1; \u03c1(a|k) = 1\u2212 \u03b4\u2212\n\u2211k\u22121 s=1 \u03c0s(2\u00b5s\u22121) \u03c0k(2\u00b5k\u22121) for s = k;\n\u03c1(a|s) = 1 for s = k + 1, . . . , n.\nThe objective value (sender\u2019s expected utility) of the above solution \u03c1 is\nU(\u03c0, \u03c1) = \u2211\ns:\u00b5s> 1 2\n\u03c0s\u03c1(a|s)\n= \u03c0k\n( 1\u2212 \u03b4 \u2212 \u2211k\u22121 s=1 \u03c0s(2\u00b5s \u2212 1) \u03c0k(2\u00b5k \u2212 1) ) + n\u2211\ns=k+1\n\u03c0s\n= n\u2211\ns=k\n\u03c0s \u2212 \u03b4\n2\u00b5k \u2212 1 +\nk\u22121\u2211\ns=1\n\u03c0s(2\u00b5s \u2212 1) 2\u00b5k \u2212 1 .\nSince the signaling scheme \u03c0 must satisfy \u2211\ns\u2208S \u03c0s\u00b5s = \u00b50, we have\n\u00b50 = \u2211\ns\u2208S \u03c0s\u00b5s \u2265\nn\u2211\ns=1\n\u03c0s\u00b5s = k\u22121\u2211\ns=1\n\u03c0s\u00b5s + n\u2211\ns=k\n\u03c0s\u00b5s \u2265 k\u22121\u2211\ns=1\n\u03c0s\u00b5s + n\u2211\ns=k\n\u03c0s\u00b5k\n=\u21d2 n\u2211\ns=k\n\u03c0s \u2264 \u00b50 \u2212 \u2211k\u22121 s=1 \u03c0s\u00b5s \u00b5k .\nSo,\nU(\u03c0, \u03c1) \u2264 \u00b50 \u2212 \u2211k\u22121\ns=1 \u03c0s\u00b5s \u00b5k \u2212 \u03b4 2\u00b5k \u2212 1 +\nk\u22121\u2211\ns=1\n\u03c0s(2\u00b5s \u2212 1) 2\u00b5k \u2212 1\n= \u00b50 \u00b5k \u2212 \u03b4 2\u00b5k \u2212 1 +\nk\u22121\u2211\ns=1\n\u03c0s (2\u00b5s \u2212 1 2\u00b5k \u2212 1 \u2212 \u00b5s \u00b5k ) .\nSince 2\u00b5s\u221212\u00b5k\u22121 \u2212 \u00b5s \u00b5k = \u00b5s\u2212\u00b5k(2\u00b5s\u22121)\u00b5k \u2264 0 for any s \u2264 k \u2212 1, we get\nU(\u03c0, \u03c1) \u2264 \u00b50 \u00b5k \u2212 \u03b4 2\u00b5k \u2212 1 = f(\u00b5k).\nWe find the maximal value of f(\u00b5k) = \u00b50 \u00b5k \u2212 \u03b42\u00b5k\u22121 . Take its derivative:\nf \u2032(\u00b5k) = \u2212 \u00b50 \u00b52k + 2\u03b4 (2\u00b5k \u2212 1)2 =\n[ ( \u221a 2\u03b4 + 2 \u221a \u00b50)\u00b5k \u2212 \u221a \u00b5 0 ] \u00b7 [ ( \u221a 2\u03b4 \u2212 2\u221a\u00b50)\u00b5k + \u221a \u00b5 0 ]\n\u00b52k(2\u00b5k \u2212 1)2 ,\nwhich has two roots \u221a \u00b5 0\u221a\n2\u03b4+2 \u221a \u00b50\n< 12 and \u221a \u00b5 0\n2 \u221a \u00b50\u2212 \u221a 2\u03b4 \u2208 (12 , 1) when 0 < \u03b4 < \u00b50 2 . So, f(x) is increasing in\n[12 , \u221a \u00b5 0\n2 \u221a \u00b50\u2212 \u221a 2\u03b4 ) and decreasing in (\n\u221a \u00b5 0 2 \u221a \u00b50\u2212 \u221a 2\u03b4 , 1]. Since \u00b5k > 1 2 , f(\u00b5k) is maximized at \u00b5k =\n\u221a \u00b5 0 2 \u221a \u00b50\u2212 \u221a 2\u03b4 .\nThis implies\nU(\u03c0, \u03c1) \u2264 f ( \u221a\u00b5 0\n2 \u221a \u00b50\u2212 \u221a 2\u03b4\n) = \u00b50\u221a \u00b5 0 (2 \u221a \u00b50 \u2212 \u221a 2\u03b4)\u2212 \u03b4 2 \u221a \u00b5 0\n2 \u221a \u00b50\u2212 \u221a 2\u03b4\n\u2212 1 = 2\u00b50 \u2212 2\n\u221a\n2\u00b50\u03b4 + \u03b4.\nThis holds for any \u03c0. So, OBJR(\u03b4) = sup\u03c0 min\u03c1\u2208R\u03b4(\u03c0) U(\u03c0, \u03c1) \u2264 U\u2217 \u2212 2 \u221a 2\u00b50\u03b4 + \u03b4 = U \u2217 \u2212 \u2126( \u221a \u03b4).\nC.4 Proof of Lemma 5.5\nLet (\u03c0, \u03c1) be a pair of principal strategy and agent strategy that achieves the optimal principal utility with an exactly-best-responding agent, namely, U(\u03c0, \u03c1) = U\u2217. Without loss of generality \u03c1 can be assumed to be deterministic, \u03c1 : S \u2192 A. The strategy \u03c0 consists of pairs {(\u03c0s, xs)}s\u2208S that satisfy\n\u2211 s\u2208S \u03c0sxs =: \u00b50 \u2208 C, (39)\nand the action a = \u03c1(s) is optimal for the agent with respect to xs. We will construct another principal strategy \u03c0\u2032 such that, even if the agent chooses the worst \u03b4-best-responding strategy to \u03c0\u2032, the principal can still obtain utility arbitrarily close to U\u2217 \u2212\n( Ldiam(X ; \u21131) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G .\nTo construct \u03c0\u2032 we do the following: For each signal s \u2208 S, with corresponding action a = \u03c1(s), by Claim 5.1 there exists ya \u2208 X such that v(ya, a)\u2212 v(ya, a\u2032) \u2265 G for any a\u2032 6= a. Let \u03b8 = \u03b4G + \u03b5 \u2208 [0, 1] for arbitrarily small \u03b5 > 0, and let x\u0303s be the convex combination of xs and y\u03c1(s) with weights 1\u2212 \u03b8, \u03b8: x\u0303s = (1\u2212 \u03b8)xs + \u03b8y\u03c1(s). (40) We note that a = \u03c1(s) is the agent\u2019s optimal action for x\u0303s and moreover it is better than any other action a\u2032 6= a by more than \u03b4:\nv(x\u0303s, a)\u2212 v(x\u0303s, a\u2032) = (1\u2212 \u03b8) [\nv(xs, a) \u2212 v(xs, a\u2032) \ufe38 \ufe37\ufe37 \ufe38\n\u22650 because a = \u03c1(s) is optimal for xs\n] + \u03b8 [ v(ya, a)\u2212 v(ya, a\u2032) \ufe38 \ufe37\ufe37 \ufe38\n\u2265G by our choice of ya\n]\n\u2265 0 + \u03b8G > \u03b4 G G = \u03b4. (41)\nLet \u00b5\u2032 be the convex combination of {x\u0303s}s\u2208S with weights {\u03c0s}s\u2208S :\n\u00b5\u2032 = \u2211\ns\u2208S \u03c0sx\u0303s. (42)\nNote that \u00b5\u2032 might not satisfy the constraint \u00b5\u2032 \u2208 C. So, we want to find another vector z \u2208 X and a coefficient \u03b7 \u2208 [0, 1] such that (1\u2212 \u03b7)\u00b5\u2032 + \u03b7z \u2208 C. (43) (If \u00b5\u2032 already satisfies \u00b5\u2032 \u2208 C, then let \u03b7 = 0.) To do this, we consider the ray starting from \u00b5\u2032 pointing towards \u00b50: {\u00b5\u2032+t(\u00b50\u2212\u00b5\u2032) | t \u2265 0}. Let z be the intersection of the ray with the boundary of X :\nz = \u00b5\u2032 + t\u2217(\u00b50 \u2212 \u00b5\u2032), t\u2217 = argmax{t \u2265 0 | \u00b5\u2032 + t(\u00b50 \u2212 \u00b5\u2032) \u2208 X}. Then, rearranging z = \u00b5\u2032 + t\u2217(\u00b50 \u2212 \u00b5\u2032), we get\n1 t\u2217 (z \u2212 \u00b5\u2032) = \u00b50 \u2212 \u00b5\u2032 \u21d0\u21d2 (1\u2212 1t\u2217 )\u00b5\u2032 + 1t\u2217 z = \u00b50 \u2208 C,\nwhich satisfies (43) with \u03b7 = 1 t\u2217 . We then give an upper bound on \u03b7 = 1 t\u2217 :\nClaim C.2. \u03b7 = 1 t\u2217 \u2264 diam(X )dist(C,\u2202X )\u03b8.\nProof. On the one hand,\n\u2016\u00b50 \u2212 \u00b5\u2032\u2016 = \u2225 \u2225 \u2211\ns\u2208S \u03c0sxs \u2212\n\u2211 s\u2208S \u03c0sx\u0303s \u2225 \u2225 = \u2225 \u2225 \u2211 s\u2208S \u03c0s\u03b8(y\u03c1(s) \u2212 xs) \u2225 \u2225\n\u2264 \u03b8 \u2211 s\u2208S \u03c0s \u2225 \u2225y\u03c1(s) \u2212 xs \u2225 \u2225 \u2264 \u03b8 \u2211 s\u2208S \u03c0s \u00b7 diam(X ) = \u03b8 \u00b7 diam(X ).\nOn the other hand, because z \u2212 \u00b5\u2032 and \u00b50 \u2212 \u00b5\u2032 are in the same direction, we have\n\u2016z \u2212 \u00b5\u2032\u2016 = \u2016z \u2212 \u00b50\u2016+ \u2016\u00b50 \u2212 \u00b5\u2032\u2016 \u2265 \u2016z \u2212 \u00b50\u2016 \u2265 dist(C, \u2202X )\nbecause \u00b50 is in C and z is on the boundary of X . Therefore, \u03b7 = 1t\u2217 = \u2016\u00b50\u2212\u00b5\u2032\u2016 \u2016z\u2212\u00b5\u2032\u2016 \u2264 diam(X ) dist(C,\u2202X )\u03b8.\nThe convex combinations (43) (42) define a new principal strategy \u03c0\u2032 with |S| + 1 signals, consisting of x\u0303s with probability (1\u2212\u03b7)\u03c0s and z with probability \u03b7, satisfying \u2211\ns\u2208S(1\u2212\u03b7)\u03c0sx\u0303s+\u03b7z = \u00b50 \u2208 C. Consider the agent\u2019s worst (for the principal) \u03b4-best-responding strategies \u03c1\u2032 to \u03c0\u2032:\n\u03c1\u2032 \u2208 argmin \u03c1\u2208D\u03b4(\u03c0\u2032) U(\u03c0\u2032, \u03c1).\nWe note that \u03c1\u2032(x\u0303s) must be equal to \u03c1(s) for each s \u2208 S. This is because a = \u03c1(s) is strictly better than any other action a\u2032 6= a by a margin of \u03b4 (41), so a is the only \u03b4-optimal action for x\u0303s.\nThen, the principal\u2019s expected utility under \u03c0\u2032 and \u03c1\u2032 is\nU(\u03c0\u2032, \u03c1\u2032) (43),(42) = (1\u2212 \u03b7) \u2211\ns\u2208S \u03c0su(x\u0303s, \u03c1\n\u2032(x\u0303s)) + \u03b7u(z, \u03c1 \u2032(z))\n\u2265 (1\u2212 \u03b7) \u2211\ns\u2208S \u03c0su(x\u0303s, \u03c1(s)) \u2212 \u03b7B\n\u2265 (1\u2212 \u03b7) \u2211\ns\u2208S \u03c0s\n(\nu(xs, \u03c1(s))\u2212 L \u2016x\u0303s \u2212 xs\u2016 \ufe38 \ufe37\ufe37 \ufe38\n=\u2016\u03b8(y\u03c1(s)\u2212xs)\u2016\u2264\u03b8diam(X )\n)\n\u2212 \u03b7B\n\u2265 (1\u2212 \u03b7)U(\u03c0, \u03c1) \u2212 L\u03b8diam(X )\u2212 \u03b7B \u2265 U(\u03c0, \u03c1) \u2212 L\u03b8diam(X )\u2212 2\u03b7B\n(Claim C.2) \u2265 U(\u03c0, \u03c1) \u2212 L\u03b8diam(X )\u2212 2B diam(X )dist(C,\u2202X )\u03b8\n= U(\u03c0, \u03c1) \u2212 ( Ldiam(X ) + 2B diam(X )dist(C,\u2202X ) ) ( \u03b4 G + \u03b5) = U\u2217 \u2212 ( Ldiam(X ) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G \u2212O(\u03b5).\nSo, we conclude that\nOBJD(\u03b4) = sup \u03c0 min \u03c1\u2208D\u03b4(\u03c0) U(\u03c0, \u03c1) \u2265 min \u03c1\u2208D\u03b4(\u03c0\u2032) U(\u03c0\u2032, \u03c1)\n= U(\u03c0\u2032, \u03c1\u2032) \u2265 U\u2217 \u2212 ( Ldiam(X ) + 2B diam(X )dist(C,\u2202X ) ) \u03b4 G \u2212O(\u03b5).\nLetting \u03b5 \u2192 0 finishes the proof for the case with the constraint \u2211s\u2208S \u03c0sxs \u2208 C. The case without \u2211\ns\u2208S \u03c0sxs \u2208 C is proved by letting \u03b7 = 0 in the above argument.\nC.5 Proof of Claim 5.8\nOn the one hand,\n\u2016\u00b50 \u2212 \u00b5\u2032\u2016 = \u2225 \u2225 \u2211\ns\u2208S \u03c0sxs \u2212\n\u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)x\u0303s,a \u2225 \u2225 = \u2225 \u2225 \u2211 s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,a(ya \u2212 xs) \u2225 \u2225\n\u2264 \u2211\ns\u2208S\n\u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,a \u2225 \u2225ya \u2212 xs \u2225 \u2225 \u2264 \u2211 s\u2208S \u2211 a\u2208A \u03c0s\u03c1(a|s)\u03b8s,adiam(X ) Claim 5.7 \u2264 diam(X ) \u03b4 G .\nOn the other hand, because z \u2212 \u00b5\u2032 and \u00b50 \u2212 \u00b5\u2032 are in the same direction, we have\n\u2016z \u2212 \u00b5\u2032\u2016 = \u2016z \u2212 \u00b50\u2016+ \u2016\u00b50 \u2212 \u00b5\u2032\u2016 \u2265 \u2016z \u2212 \u00b50\u2016 \u2265 dist(C, \u2202X )\nbecause \u00b50 is in C and z is on the boundary of X . Therefore, \u03b7 = 1t\u2217 = \u2016\u00b50\u2212\u00b5\u2032\u2016 \u2016z\u2212\u00b5\u2032\u2016 \u2264 diam(X ) dist(C,\u2202X ) \u03b4 G ."
        }
    ],
    "title": "Persuading a Learning Agent",
    "year": 2024
}