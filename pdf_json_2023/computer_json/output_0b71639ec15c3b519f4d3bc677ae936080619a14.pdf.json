{
    "abstractText": "The core idea of semantic recommendation is to incorporate semantic knowledge into the recommendation process. The semantic recommendation algorithm, based on knowledge graph, ignores the deep implicit semantics of the evaluation data. The semantic recommendation algorithm based on the deep matrix decomposition model is limited to the implicit semantics of the evaluation data. The semantic recommendation algorithm based on the collaborative filtering algorithm performs only the selection of the nearest neighbors of the user or the item unilaterally and ignores the influence of other aspects, which naturally leads to a decrease in the recommendation accuracy. To solve the above problems, this paper introduces Formal Concept Analysis (FCA) based on collaborative filtering. Using the property that the formal concept in FCA can cluster objects (users) and attributes (items) simultaneously, we propose a semantic recommendation algorithm (SRKGFCA) based on the knowledge graph and formal concept analysis to solve the problem of ignoring user or item factors. Finally, the proposed semantic recommendation algorithm is validated on two public datasets in this work. By using traditional algorithms and current semantic recommendation algorithms as benchmarks, extensive experiments show that our proposed algorithm consistently outperforms state-of-the-art methods. INDEX TERMS Semantic Recommendation, Knowledge Graphs, Representation Learning, Formal Concept Analysis, Deep Matrix Decomposition",
    "authors": [
        {
            "affiliations": [],
            "name": "Lina Wei"
        }
    ],
    "id": "SP:ae0b74c0015ee4ec99c7c29ed2025a26cf852ed4",
    "references": [
        {
            "authors": [
                "Y Cao",
                "X Wang",
                "X He"
            ],
            "title": "Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences[C]//The world wide web conference",
            "year": 2019
        },
        {
            "authors": [
                "G Linden",
                "B Smith",
                "J. York"
            ],
            "title": "Amazon. com recommendations: Item-to-item collaborative filtering[J",
            "venue": "IEEE Internet computing,",
            "year": 2003
        },
        {
            "authors": [
                "F Zhang",
                "J Yuan N",
                "D Lian"
            ],
            "title": "Collaborative knowledge base embedding for recommender systems[C",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
            "year": 2016
        },
        {
            "authors": [
                "Q. Shambour"
            ],
            "title": "A deep learning based algorithm for multicriteria recommender systems[J",
            "venue": "Knowledge-Based Systems,",
            "year": 2021
        },
        {
            "authors": [
                "C Bizer",
                "J Lehmann",
                "G Kobilarov"
            ],
            "title": "DBpedia-A crystallization point for the Web of Data[J",
            "venue": "Journal of web semantics,",
            "year": 2009
        },
        {
            "authors": [
                "J Hoffart",
                "M Suchanek F",
                "K Berberich"
            ],
            "title": "YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia[J",
            "venue": "Artificial Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "D Vrande\u010di\u0107",
                "M. Kr\u00f6tzsch"
            ],
            "title": "Wikidata: a free collaborative knowledgebase[J",
            "venue": "Communications of the ACM,",
            "year": 2014
        },
        {
            "authors": [
                "A. Miller G"
            ],
            "title": "WordNet: a lexical database for English[J",
            "venue": "Communications of the ACM,",
            "year": 1995
        },
        {
            "authors": [
                "B Xu",
                "Y Xu",
                "J Liang"
            ],
            "title": "CN-DBpedia: A neverending Chinese knowledge extraction system[C",
            "venue": "International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",
            "year": 2017
        },
        {
            "authors": [
                "A Bordes",
                "N Usunier",
                "A Garcia-Duran"
            ],
            "title": "Translating embeddings for modeling multi-relational data[C",
            "venue": "Neural Information Processing Systems",
            "year": 2013
        },
        {
            "authors": [
                "Z Wang",
                "J Zhang",
                "J Feng"
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes[C",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, Que\u0301bec City, Que\u0301bec, Canada",
            "year": 2014
        },
        {
            "authors": [
                "Y Lin",
                "Z Liu",
                "M Sun"
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion[C",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "G Ji",
                "S He",
                "L Xu"
            ],
            "title": "Knowledge graph embedding via dynamic mapping matrix[C]. Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing",
            "year": 2015
        },
        {
            "authors": [
                "H Wang",
                "M Zhao",
                "X Xie"
            ],
            "title": "Knowledge graph convolutional networks for recommender systems[C",
            "venue": "The world wide web conference",
            "year": 2019
        },
        {
            "authors": [
                "Y Wu",
                "Y ZHao",
                "S. Wei"
            ],
            "title": "Collaborative filtering recommendation algorithm based on interval-valued fuzzy numbers[J",
            "venue": "Applied Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "G Xu",
                "G Jia",
                "L Shi"
            ],
            "title": "Personalized course recommendation system fusing with knowledge graph and collaborative filtering[J",
            "venue": "Computational Intelligence and Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "X He",
                "L Liao",
                "H Zhang"
            ],
            "title": "Neural collaborative filtering[C",
            "venue": "Proceedings of the 26th international conference on world wide web",
            "year": 2017
        },
        {
            "authors": [
                "J Xue H",
                "X Dai",
                "J Zhang"
            ],
            "title": "Deep Matrix Factorization Models for Recommender Systems[C",
            "venue": "Proceedings of the 26th International joint Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Shi Jiarong",
                "Li Jinhong"
            ],
            "title": "Recommendation algorithm via fusing matrix completion and deep matrix factorization[J",
            "venue": "Application Research",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Quan",
                "Cheng Zhenhua",
                "Jiang Yang"
            ],
            "title": "A movie recommendation algorithm based on knowledge graph and collaborative filtering[J",
            "venue": "Computer Engineering & Science,",
            "year": 2020
        },
        {
            "authors": [
                "C Zou",
                "D Zhang",
                "J Wan"
            ],
            "title": "Using concept lattice for personalized recommendation system design[J",
            "venue": "IEEE Systems Journal,",
            "year": 2015
        },
        {
            "authors": [
                "H Mezni",
                "T. Abdeljaoued"
            ],
            "title": "A cloud services recommendation system based on Fuzzy Formal Concept Analysis[J",
            "venue": "Data & Knowledge Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Jiang"
            ],
            "title": "Semantifying formal concept analysis using description logics[J",
            "venue": "Knowledge-Based Systems,",
            "year": 2019
        },
        {
            "authors": [
                "D Boucher-Ryan P",
                "D. Bridge"
            ],
            "title": "Collaborative Recommending using Formal Concept Analysis[J",
            "venue": "Knowledge- Based Systems,",
            "year": 2006
        },
        {
            "authors": [
                "B Ganter",
                "R. Wille"
            ],
            "title": "Formal concept analysis: mathematical foundations[M",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Y Jiang",
                "M. Yang"
            ],
            "title": "Semantic search exploiting formal concept analysis, rough sets, and Wikipedia[J",
            "venue": "International Journal on Semantic Web and Information Systems,",
            "year": 2018
        },
        {
            "authors": [
                "N Mendes P",
                "M Jakob",
                "A Garc\u00eda-Silva"
            ],
            "title": "DBpedia spotlight: shedding light on the web of documents[C",
            "venue": "Proceedings of the 7th international conference on semantic systems",
            "year": 2011
        },
        {
            "authors": [
                "Y. Koren"
            ],
            "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model[C",
            "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nrecommendation process. The semantic recommendation algorithm, based on knowledge graph, ignores the deep implicit semantics of the evaluation data. The semantic recommendation algorithm based on the deep matrix decomposition model is limited to the implicit semantics of the evaluation data. The semantic recommendation algorithm based on the collaborative filtering algorithm performs only the selection of the nearest neighbors of the user or the item unilaterally and ignores the influence of other aspects, which naturally leads to a decrease in the recommendation accuracy. To solve the above problems, this paper introduces Formal Concept Analysis (FCA) based on collaborative filtering. Using the property that the formal concept in FCA can cluster objects (users) and attributes (items) simultaneously, we propose a semantic recommendation algorithm (SRKGFCA) based on the knowledge graph and formal concept analysis to solve the problem of ignoring user or item factors. Finally, the proposed semantic recommendation algorithm is validated on two public datasets in this work. By using traditional algorithms and current semantic recommendation algorithms as benchmarks, extensive experiments show that our proposed algorithm consistently outperforms state-of-the-art methods.\nINDEX TERMS Semantic Recommendation, Knowledge Graphs, Representation Learning, Formal Concept Analysis, Deep Matrix Decomposition\nI. INTRODUCTION In recent years, researchers have introduced semantic information into traditional recommendation algorithms to alleviate problems in various types of algorithms. The most classical application is the personalized recommendation for Amazon online shopping mall [1]. Although semantic recommendation algorithms have facilitated the development of recommender systems [2], the main drawback is the cold-start and data sparsity problems. Zhang F et al [3] achieved high quality movie recommendation by introducing movie synopsis, poster and ontology as movie knowledge embedded in a lowdimensional vector space, which largely alleviated the cold-start and data sparsity problems. Semantic recommendation algorithms recommend items based on user preferences, but it is difficult to perform feature extraction when the items are multimedia data.\nDeep learning- based recommendation algorithms [4] can extract semantic information about users or items from a large amount of text data, but make recommendation systems overly dependent on external text data and face the problem of data source and reliability. The emergence of knowledge graph that contain rich semantic information has brought semantic recommendation research to a new stage of development. The emergence of a large number of public knowledge graphs (e.g., DBpedia [5], YAGO [6], Wikidata [7], WordNet [8], and CN-DBpedia [9], etc.) has enabled researchers to more easily access and utilize semantic information and obtain the final recommendation list by merging semantic top-k nearest neighbors and collaborative top-k nearest neighbors to form the final recommendation list and improve recommendation accuracy. Translation modelbased (Trans) map embedding technology is the most widely used, including TransE[10], TransH[11], TransR[12] and TransD[13]. They embed knowledge graph triples into low-\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\ndimensional vector space through gradient update training, which is more interpretable and simple than the method based on graph neural network [14].Wu et al [15] learned the representation of movies by constructing a knowledge graph in the movie domain, and then combined the semantic similarity of movies and the similarity of elements of collaborative filtering to achieve higher accuracy and coverage of recommendations. [16] the knowledge graph representation learning method is used to embed the semantic information of the items into a low-dimensional semantic space; then, the semantic similarity between the recommended items is calculated, and then, this item semantic information is fused into the collaborative filtering recommendation algorithm. This algorithm increases the performance of recommendation at the semantic level. Although the above knowledge graph-based recommendation methods reflect user preferences through rating data, the high sparsity of the rating matrix is the major drawback of the above algorithms. The key is to unlock the hidden semantics of the rating matrix.\nFor the past few years, with the development of deep learning, He et al. [17] proposed NeuMF model (Neural Matrix Factorization), which combines probabilistic computation and multi-layer perceptron.Xue et al [18] proposed DMF (Deep Matrix Factorization) model to address the shortcomings of NeuMF in processing user ratings and obtained better results. Although matrix factorization can effectively determine the latent semantics in user-item interaction data, it ignores the external semantic knowledge. Because it is limited to the cryptic meaning mining of the items scored by users, and deep learning itself applies the idea of heuristic algorithm, when there are too many missing values in the scoring matrix, the deep matrix factorization model is easy to fall into the dilemma of local optimum in the training. Therefore, the input of matrix factorization model is processed accordingly, so that the input matrix or vector can describe the characteristics of users and items more accurately, which can often achieve better results. Shi Jiarong et al. [19] performed DMF model training after scoring matrix completion in the way of mathematical matrix completion, and achieved good recommendation accuracy. Yuan et al [20] achieved better results by performing graph embedding through a modified TransHR model and then populating the scoring matrix with semantic similarity mixed with collaborative filtering similarity. However, Shi Jiarong et al. [19] simply used mathematical formulas to complete the scoring matrix, while ignoring external semantic information. Yuan et al [20] used only a superficial model for hidden semantic mining and ignored the deeper semantic information of the evaluation matrix. In recent years, researchers have found that FCA can cluster both objects (users) and attributes (items) and is able to find the nearest neighbors of users or items more efficiently than the knearest neighbor algorithm. Both user-based K-nearest neighbor and item-based K-nearest neighbor consider only\none-sided neighbor relationship. By introducing FCA into collaborative filtering recommendation and considering the neighbor relationship between users and items at the same time, they achieved good recommendation effects [21][22]. Yuncheng Jiang [23] proposed an ontology semantic extension-based FCA to improve the computation of formal concept similarity by incorporating semantic information from knowledge graph. In FCA-based recommendation, P Du Boucher-Ryan et al. [24] first introduced the concept lattice of FCA into the collaborative filtering recommendation algorithm to find the nearest neighbor users based on the concept lattice, so as to make product recommendation Zou et al [21] used a concept lattice in cosine similarity computation to improve the prediction accuracy of a recommendation system with collaborative filtering. Mezni et al [22] proposed a cloud server recommendation system based on fuzzy FCA to determine the relationship between users and items at a deeper level, alleviating the problems of data sparsity and cold start and achieving better results. The approach described above can only make a recommendation for small amounts of data, since a concept grid must be constructed. At present, the recommendation algorithm based on FCA is far from popular recommendation algorithms based on matrix factorization and knowledge graph in terms of recommendation accuracy [21][22][24]. However, how to effectively use the related theories of FCA and its semantic extension and integrate them into the existing recommendation algorithms to improve the recommendation effect of the existing algorithms is a problem worth studying.\nIn summary, the recommendation model based on knowledge graph ignores the deeper implicit semantics of the evaluation data, while the recommendation model based on the deep matrix decomposition is limited to the implicit semantics of the evaluation data FCA can effectively solve the problem of selecting the nearest neighbors of users or items by traditional recommendation algorithms of collaborative filtering, and has the theoretical basis for combining with related semantic techniques. However, there are also problems in creating concept lattices in large-scale datasets or the accuracy of heuristic concept algorithms is not high. In this paper, we combine knowledge graph and FCA to propose a new and better semantic recommendation algorithm.\n1) In collaborative filtering, combined with the depth matrix decomposition model, a semantic recommendation algorithm based on knowledge graph is proposed in this paper.(Semantics based Recommendation using Knowledge Graph, SRKG)\n2)A fuzzy FCA-based concept area calculation method and an attribute (item) based heuristic concept construction algorithm are proposed to solve the problems of lack of consideration of user ratings and difficulties in operating with large-scale datasets.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\n3)FCA is integrated into the SRKG algorithm. A semantics-based recommendation algorithm using knowledge graph and formal concept analysis (Semantics based Recommendation using Knowledge Graph and Formal concept analysis, SRKGFCA) is proposed to solve the SRKG in PR1 evaluation prediction, which considers only the item factor and ignores the user factor.\nThe remainder of this paper is organized as follows. In Section II, we briefly introduce the related work. Section III presents semantic recommendation algorithms based on knowledge graph. Section IV introduces our SRKGFCA algorithm in detail. Section V evaluates our implementation based on experimental results. Section VI concludes this paper with future work."
        },
        {
            "heading": "II. Related Research",
            "text": ""
        },
        {
            "heading": "A. Knowledge Graph",
            "text": "Knowledge Graph(KG) is a practical approach to model factual information in the form of entity-to-entity relationships for representing extensive information from different domains. DBpedia Knowledge Graph [5] is a specific example of a Semantic Web application that automatically extracts semi-structured data from Wikipedia and transforms it into structured data that can be linked via Linked Data to other knowledge graphs such as Wikidata [7] , YAGO[6], Freebase, etc. DBpedia's Semantic Web technology facilitates the application of Wikipedia data and was awarded Best Application Service at the 2009 Semantic Web Awards. The atlas contains more than 6 million entities, and in particular all the entities of the Movielen movie recommendation dataset required for use in this work are includeded."
        },
        {
            "heading": "B. Formal Concept Analysis",
            "text": "Formal Concept Analysis (FCA) [25] is a theoretical framework that provides the basis for data analysis and knowledge processing and can represent the relationships between objects and attributes in a given domain. In real life, many relationships between objects and attributes are ambiguous, for example, even if someone has seen a certain movie, there are often a variety of levels of personal preferences. For example, not bad, average, not so good, etc. Fuzzy information like this cannot be defined by a simple 0 or 1. Therefore, fuzzy set theory was introduced to define Fuzzy Formal Concept Analysis (FFCA) [25].\nDefinition 1 (Fuzzy Formal Background)[25] A fuzzy formal background is a quadruple ))(,,,( AORVAOK   , where is the set of objects, A is the set of attributes, R denotes O the fuzzy relation between the set of objects O and the set of attributes A , and ]1,0[V denotes the affiliation taken in the fuzzy relation. Each object and attribute in FFCA has an affiliation ]1,0[,1 Vao \uff09\uff08 between them. Given a grid of concept pairs AOao \uff09\uff08 , and the affiliation\n1 , that is, an\nindication of the extent to which the object o has ( 1 ) the attribute a . Corresponding to the user rating relationship for the movie, how much (\n1 ) the user o likes the movie a , or how much the user o rates the movie a as\n1 . The fuzzy formal background is shown in Table I."
        },
        {
            "heading": "1 0.8 0.8 0 0.4 0.8",
            "text": ""
        },
        {
            "heading": "2 1 0.4 0.6 1 0",
            "text": "Wher ),(1 ao is the affiliation degree of o corresponding to a , and when the binary \uff09\uff08 IEC ),( satisfies EIIE  *,* , then C is said to be a fuzzy formal concept, where E is called the fuzzy concept externality and I is called the fuzzy concept implication.\nAlso in the fuzzy concept C , the affiliation o  of each\nobject Oo for the fuzzy set )(E is computed publicly\nas\n),(min 1 aoIao   (2-3) Taking the fuzzy formal background of Table I as an example, let the affiliation threshold  be 0.5. It is necessary to state that since the only rating of user 2 for movie b is 0.4 , which indicates that this user 2 does not like movie b, this correspondence will not be considered when generating fuzzy concepts. One of the fuzzy concepts can be obtained from the above definition is )},,(),2/0.6{( dca , where \u2018 6.0/2 \u2019 indicates that user 2 has an affiliation of 0.6 for the movie set ),,( dca .\nIn FFCA, the concept lattice is defined similarly to singlevalued FCA, where two fuzzy concepts are given and then the corresponding fuzzy concept lattice can be obtained by connecting the concepts through the partial order relation \u2018  \u2019 between them. Figure 1 is the fuzzy concept lattice corresponding to Table I.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nFIGURE 1. User-Film Fuzzy Concept Grid"
        },
        {
            "heading": "III. semantic recommendation algorithms based on knowledge graph",
            "text": "A. Similarity calculation method based on knowledge graph\nFor semantic recommendation algorithms, similarity computation is the core work, including semantic similarity of items based on knowledge graph, and similarity of items based on ratings."
        },
        {
            "heading": "1) ITEM SIMILARITY BASED ON SEMANTICS OF KNOWLEDGE GRAPH",
            "text": "In the semantic recommendation algorithm, the items Ii are represented by vectors as T\ndiiii eeeI ),,,( ,21 \uff0c\uff0c (3-1)\nwhere d is denoted as the embedding of items into a vector\nspace of dimension d and qie , is the value corresponding to\nthe qth dimensional vector.\nThe Euclidean distance formula is used to calculate the\nentity similarity of the recommended items.\n \n d\nk kjkiji eeIIdis 1\n2\n,, )(),(\n(3-2)\nThe Euclidean distance formula leads to a distance-based\nsemantic similarity formula for the two items graphsim ,\n),(1\n1 ,\nji\njigraph IIdis IIsim  \uff09\uff08\n(3-3)"
        },
        {
            "heading": "2) SCORING-BASED ITEM SIMILARITY",
            "text": "We use the user-item interaction data, i.e., the user-item rating data, to construct a rating matrix nmR  of user set U to item set I\n   \n\n   \n\n\nnmm\nn\nnm\nrr\nrr\nR\n,1,\n,11,1\n\n\n\n(3-4)\nThe user set's rating of an item is used as a rating vector, e.g. the vector for item is denoted as T miiii rrrI },,,{ 21  . Then the similarity simcos of two items Ii and I is denoted as,\n\n\n\n\n\n\n m\nu\nju\nm\nu\niu\nm\nu\njuiu\nji\nrr\nrr\nIIsim\n1\n2 ,\n1\n2 ,\n1\n,,\ncos\n)(\n),( (3-5)\nwhere can be known from the vector Ii and ru,i is denoted as user u's rating of item i.\nThe cosine similarity takes values in the range of [-1,1], so normalization is performed. The normalized cosine similarity formula for item similarity based on ratings simrate is.\n\n\n\n\n\n\n \nji\nji\nUu\nuju\nUu\nuiu\nUu\nujuuiu\njirate\nrrrr\nrrrr\nIIsim 2\n,\n2\n,\n,,\n)()(\n))((\n0.50.5),( (3-6)\nwhere, ur denotes the average rating of user u, and jiU \ndenotes the set of users who rated both items i and j.\nB. Semantic recommendation algorithms based on knowledge graph\nIn this paper, we propose a semantic recommendation algorithm based on knowledge graph for similarity (SRKG), which mainly includes the fusion of two similarities and makes initial score prediction, then improves and trains the DMF model by including semantic information in the predicted scores, and finally results in the recommendation."
        },
        {
            "heading": "1) SIMILARITY FUSION AND SCORE PREDICTION",
            "text": "The semantic information of the knowledge graph and the rating information of each user are incorporated into the algorithm, i.e., the two item similarities mentioned above are fused. According to (3-3) and (3-6), we can obtain the fusion formula sim for the similarity of the two items.\n \n \n\n\n\n\n\n),()1(),(\n0),( iff , ),(\n0),( iff , ),(\n),(\njigraphjirate\njiratejigraph\njigraphjirate\nji\nIIsimaIIsima\nIIsimIIsim\nIIsimIIsim\nIIsim\n(3-7)\nwhere, 1a is the fusion factor and when 0a , the proposed SRKG algorithm considers only the item similarity calculated based on the knowledge graph.\nWe segmented the function, mainly to alleviate the cold start (simrate = 0) or cases where the knowledge graph cannot be the similarity (e.g., problems such as movie entities not existing, simgraph = 0). By introducing the item similarity computed by the knowledge graph, we can more accurately predict the user's rating of unrated items. The predicted rating PR1 of user u for item i is,\nu\nkiSuNj\nji\nkiSuNj\nujuji r IIsim\nrrIIsim\niuPR \n\n \n\n\n\n)),()((\n)),()((\n,\n),(\n))(),((\n),(1\n(3-8)\nwhere N(u) denotes the items rated by user u, ),( kiS denotes the top k items with the greatest similarity to item i, and the intersection of the two is the reference object for rating prediction."
        },
        {
            "heading": "2) DMF MODEL TRAINING",
            "text": "The purpose of the DMF depth matrix decomposition model is actually to represent item and user features with lowdimensional vectors, while accurately measuring the similarity of users and items. For this purpose, its objective function is as follows\n)))((( *12233  ii i i i i i ni YWfWfWfp \n(3-9)\n)))((( **12233  Tjj\nj\nj\nj\nj\nj\nnj YWfWfWfq \n(3-10)\njjiijijiDMFij YvYuqpsimvuFY **cos ,),,()|,( \u02c6   (3-11)\nThen, we measure the loss of training by the cosine\nsimilarity, and the specific loss function is shown below\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\n))\u02c61log() )max( 1(\u02c6log )max( ( ),( ij\nij\nYYji\nij\nij Y\nrate\nY Y\nrate\nY L  \n\n(3-12)\nwhere,  YY , denote the non-zero and zero values in the rating matrix, respectively; max(rate) denotes the highest rating of the recommendation system, which is mainly used to normalize the ratings; ijY and ijY\u0302 denote the true and predicted values of the ratings, respectively.\nwhere both R and RPR1 are m*n matrices, and the missing values of the R matrix are filled with 0,the RPR1 matrix in Algorithm 1 represents the prediction rating matrix, which is derived from the above computed PR1 prediction ratings, and is filled with 0 values for items that have been selected by the user. Compared to the original DMF training approach [18], the improvement in this paper is mainly reflected in the 8th row of the Algorithm 1, which is populated into the input vectors pi,qj by the predicted ratings PR1 containing the external semantics. It is worth noting that the positive and negative samples of the model, as well as the true ratings Yij required by the loss function, are still derived from the original rating matrix R in order to guarantee the accuracy of the model. We mainly populate the input vectors pi,qj with the main purpose of making the input vectors pi,qj more accurate in characterizing the user and the item. Because the training positive and negative samples of the original model are not changed, the training method of the DMF model proposed in this paper is not much different in time\ncomplexity compared with the training method of the original model.\nAfter the DMF model is trained, we can get the u and i\ncosine similarity by inputting the target user u and the target item i into the model, i.e., the normalized prediction score\nDMFsim ,\n),(),( iuDMFiusimDMF  (3-13)\nSince the normalized cross-entropy loss function of the model is normalized considering the ratings as well as the bias term, the predicted rating PR2 of the target user u for item i is\n),()max(),2( iusimrateiuPR DMF (3-14) Where )max(rate indicates the maximum value of the rating in the recommender system, e.g., if the highest user rating in MovieLens is 5, then 5)max( rate . Rating predictions are made for each target user, and by ranking the predicted ratings, the SRKG algorithm can be summarized as the following process,\n1) Retrieve the corresponding entity and its triples from\nDBpedia by the name of the item.\n2) The triad is trained to learn the TransE mapping representation, and is given a low-dimensional vector containing items.\n3) Knowledge graph-based similarity of items and evaluation-based similarity of items are computed from the low-dimensional vector of items and the evaluation information, respectively, and the similarity fusion is performed.\n4) The fused item similarity of the item is used to calculate\nthe prediction score PR1using k-nearest neighbors;\n5) Fill the predicted scores PR1 into the input vector pi,qj\nand train the DMF depth matrix decomposition model.\n6) After the model is trained, the prediction results PR2 and recommendation results are obtained by inputting the target users and items into the model."
        },
        {
            "heading": "IV. semantic recommendation algorithms for formal concept analysis",
            "text": "A. Strong concept calculation methods\nFollowing the FCA of Yuncheng Jiang et al. in semantic retrieval [26] and the semantized FCA [23] ,for fuzzy formal concepts )),(( IEC  , the main purpose is to determine the closest set of items, and we will also include semantic\nAlgorithm 1: DMF model training algorithm Train_DMF\nijY\u0302 \u2190 input pi ,qj to (3-11); // ijY\u0302 cosine\nsimilarity, i.e. normalized prediction score\nL \u2190 Enter ijY\u0302 and T to (3-12); // Loss\ncalculation\nReverse learning update Wui ,Wvi ; //\nGradient update model parameters end for end for return Wui ,Wvi , ijY\u0302 ;\nAlgorithm 1: DMF model training algorithm Train_DMF Input: number of model iterations iter,negative\nsample rate nratio,original scoring matrix R,predicted scoring matrix RPR1\nOutput: user and item weight matrix Wui ,Wvi , where\n(i=1...N-1), the similarity of users and items\nijY\u0302\nInitialization :\nRandomly generate Wui ,Wvi ; // where\nWui ,Wvi is the weight matrix\nY = the values in the matrix R, = the non-zero values in Y, = all zero values in Y,\n= randomly select nratio*|Y| values\nfrom ; // where is the positive sample, and is the negative sample\n; //T is the training set\nfor it from 1 to iter do\nfor each user i and item j in T do\nYi* ,Y*j \u2190 R+RPR1 ; //Yi* ,Y*j are the\nrow and column vectors representing user and item characteristics in R+R PR1\npi, qj \u2190 Input Yi* ,Y*j to (3-9), (3-10) ;\n// pi, qj denotes the low-dimensional vector obtained after training\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\ninformation in the computation of the fuzzy concept area calculation. For item i, the formula for the fuzzy concept area formula as follows\n   )( ),(),),(( E i Ii concept\ni\niisimiIES  \n(4-1)\nwhere i denotes the affiliation of each object in the extents )(E , which follows from (2-3), and sim(i,i') denotes the similarity of the two items, which follows from (3-7).\nTo find the set of nearest neighbors of an object or item, we need to subtract concepts that contain only one item or object from the concept lattice. For example, in the concept lattice of Figure 2, there are three fuzzy concepts that contain item b. For example, to find the nearest neighbors of item b, we first need to subtract the concept (b)}6/0.8), 4/1.0, 3/0.7, {(1/1.0,C1  . Among the remaining 2 concepts, for the sake of example, let the similarity between attributes be both 1. Concept b)} (a, ),6/0.8, {(4/1.0,C2  is a strong concept of item b because it has the largest concept area 6.311*)8.01(  \uff09\uff08S , so concept 2C is a strong concept of item b.\nB. Strong concept-based scoring prediction methods\nBy the above work we can calculate a strong concept for each item. The implication I of a strong concept is the set of nearest items of the item. For example, if the concept 2C is a strong concept of item b, then the implication (a,b) is the set of closest users of item b, and a is the closest user of item b.\nWith the set of nearest neighbor users, we can improve (3-\n8), so that the improved PR1FCA is\nu\nkiSCIuNj\nji\nkiSCIuNj\nujuji r IIsim\nrrIIsim\nPR\ni\ni \n\n \n\n\n\n))),()(()((\n))),()(()((\n,\nFCA ),(\n))(),((\ni)(u,1\n(4-2)\nwhere I(Ci) denotes the set of implication of the formal concept Ci.\nCompared to (3-8), this equation differs mainly in the selection of nearest neighbors by replacing S(k,i) with I(Ci) S(k,i), which solves the problem that only item factors are\nconsidered in the SRKG semantic recommendation algorithm. Of course, we will also perform an experimental comparative analysis of the effectiveness of the substitution in the experiment.\nC. Heuristic concept construction algorithm\nIntegrating FCA into semantic recommendation solves the problem that the SRKG algorithm lacks consideration of user factors. However, to implement FCA-based rating prediction, we need to construct concept lattices whose construction requires extremely high time complexity.We propose a heuristic concept construction algorithm to heuristically construct strong concepts for each item.\nFor a fuzzy form background ))(,,,( AORVAOK   processed by affiliation thresholding, the Algorithm 2 to heuristically find the strong concept of any attribute (item) Aa is as follows: Algorithm 2: Heuristic Concept Construction Algorithm HCC Input: Attribute a , fuzzy formal background K  ; Output: Strong concept C (E,I) of the attribute a ;\nInitialization. ; /// Initialize extents and connotations, maximum conceptual area while (true) : , me=0; //initialize this epoch and concept area\nfor do // iterate through all ephemeral objects of a that\nare not in E and add them to the ephemeral in turn\nif ))*,(,' aoEoESconcept \uff08 is max then\n//find the epitope with the largest conceptual area\n(E );\n//save this epoch and area\nend if end for if e > ME or ( |I| + 1 ) < 2 then\n// Determine if the current maximum area is\nlarger than the global maximum area, and the number of inner attributes cannot be less than 2 E = e, ME = me , I = ; // stored as global extents and intents, and maximum concept area else then Break; end if end while return (E,I);\nTaking the processed fuzzy background and item b in\nTable II as an example, we describe an example of the operation of the Algorithm 2. We first obtain the extents }6,4,3,1{*b of b, where b does the * operation when the\naffiliation of user 5 is filtered because it is lower than the\naffiliation threshold ( 5.0a ), and also for the convenience\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nof demonstration, we set the inter-attribute similarity in the inner set to 1. After combining the objects in the extents with b in turn, we can get the users 1 and 4 that make the largest concept area, and the alternative area 2; randomly select user 1 or 4 as the alternative user, and continue adding the remaining objects to the epitaxy. If user 1 is chosen as the alternative user, we end up with a strong concept )},b ( 3/0.5), {(1/1.0, c with an area of 3. If user 4 is chosen,\nwe get the above concept b)} (a, ),6/0.8, {(4/1.0,C2  with an area of 3.6. Obviously the heuristic algorithm does not always give the optimal result, but it ensures that the product of the number of extents and connotations is the largest.\nFinally, we apply the HCC algorithm to the FCA-based rating prediction to replace the concept lattice generation and thus reduce the complexity of the algorithm.\nD. SRKGFCA semantic recommendation algorithm We achieve the purpose of dynamically setting the number of nearest neighbors through the cluster property of formal concepts by using the strong concept connotation of items as the nearest neighbors of items. For the semantic recommendation algorithm, the number of nearest neighbors for each item is obviously not fixed, so dynamically setting the number of nearest neighbors by FCA can improve the scoring prediction efficiency to some extent. As for the accuracy, we will also conduct comparative experiments in the experiment. To solve the problem that the concept lattice is difficult to generate in large-scale datasets, we propose a heuristic method to construct concepts and a method to measure the range of fuzzy concepts, which can reduce the complexity of the algorithm to a great extent. By incorporating the heuristic concept construction algorithm into SRKGFCA, our algorithm can be executed in large-scale datasets.The SRKGFCA algorithm can be summarized as the following process. 1) preprocessing and correlation as in SRKG algorithm and computation of two similarities followed by similarity fusion.\n2) construction of a fuzzy background using the scoring\nmatrix R.\n3) Generation of a strong fuzzy concept for each item\nusing the heuristic concept construction algorithm HCC.\n4) The set of inclusions by the strong concept of an item as the nearest neighbour set for that item and PR1FCA score prediction.\n5) replacing PR1FCA in SRKG for the same input vector filling operation, DMF model training and finally the prediction score PR2 and recommendation results.\nObviously, the problem of ignoring the user factor in the SRKG algorithm is solved by FCA, using heuristic ideas to generate strong concepts that allow the algorithm to run in large-scale datasets. The improved SRKGFCA algorithm, whose recommendation accuracy changes, is demonstrated in the experiment."
        },
        {
            "heading": "V. Experiment and Result Analysis",
            "text": "A. Experimental data\nIn our experiments, we use two movie standard datasets in Table III (MovieLens 100K (ML100K) and MovieLens 1M (ML1M)) to test the evaluation prediction error and recommendation effect of the proposed algorithm. ML100K is a dataset with relatively small data volume and ML1M is a dataset with relatively large data volume.\nof the experiment, and the version number of DBpedia is 2019-8-30. We used DBpedia Spotlight[27] to map the movie name data contained in the ML100K and ML1M datasets into DBpedia, respectively. Although all movie name information has been integrated into DBpedia knowledge graph [5], there are cases where movie names cannot match the knowledge graph. The main reasons are as follows,1)The movie noun in ML100K and ML1M data sets is missing, and the movie marked as 'unknown'. 2)The movie name exists in the special characters, such as '\u2153' . 3)Movies are released in different years. Finally, for ML100K we get 1621 movie entities corresponding to DBpedia and for ML1M we get 3892 movie entities corresponding to DBpedia. After obtaining the corresponding movie entities in DBpedia, we extract and filter triples (head entity, relation, tail entity). The triples corresponding to 1621 and 3892 movie entities in ML100K and ML1M data sets were obtained. All the relations which appear less than 10 times in the triplet set have been cleaned out. Finally, 'dbo:wikiPageWikiLink' and 'dbo:starring' to be some 20 semantic relations of knowledge graph which indicate the relations among categories, actors and actors. The triplet related information that we finally use for knowledge graph representation learning is shown in Table IV.\nTABLE IV\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nTriplet information table used for knowledge graph\nembedding\nDataset\nNumber of\nentities\nNumber of\nrelations\nNumber of\ntriples\nML100K 111729 27 394100\nML1M 297112 29 1037098\nB. Benchmark and evaluation criteria We used the evaluation method used in the literature\n[17][29][30] with the following formula.\n \n N\ni\nihit n HR 1\n)( 1\n(5-1)\n  \n n\ni ipn NDCG 1 2 )1(log\n11 (5-2)\nwhere, hit(i) of (5-1) indicates whether the target item appears in the Top-k recommendation list, 1 if it exists and 0 if it does not exist, pi of (5-2) indicates the position of the target item appearing in the recommendation list, and ip if the target item does not exist in the list; both take values in the range [0,1].\nC. Experimental setup and analysis of results"
        },
        {
            "heading": "1) SEMANTIC VALIDATION EXPERIMENTS FOR FUSED KNOWLEDGE GRAPHS",
            "text": "We fuse the rating-based item similarity with the semantic similarity of the knowledge graph by a fusion factor  ,which increases from 0 to 1 in steps of 0.1, i.e., from using only the semantics of the knowledge graph for recommendations to using only collaborative filtering for recommendations. The graph embedding dimension is 200 and default values are used for all other parameters. We run the SRKG algorithm on the ML100K and ML1M datasets (with a nearest neighbor k-value of 20) and obtain the experimental results in Figure 3 and Figure 4 .\nAs seen in Figure 3 and Figure 4, both MAE and RMSE error values decrease as the fusion factor increases. For ML100K, the lowest error value is reached at  =0.6, while for ML1M, the lowest error value is reached at  =0.5. The reason is that ML1M has a larger amount of evaluation data to measure the similarity of items based on evaluations only, and has a higher accuracy than ML100K, so it does not need more knowledge graph similarity to be optimized relatively. It is worth noting that the two errors are actually relatively larger when  = 0 or 1. This shows the effectiveness of merging two similarities. In the following experiments, we choose 0.5 as the fusion factor for ML1M and 0.6 as the fusion factor for ML100K."
        },
        {
            "heading": "2) INTEGRATED FCA VALIDATION EXPERIMENTS",
            "text": "To calculate MAE and RMSE, we use two prediction\nscores, PR1 and PR1FCA. To find the best value for nearest neighbor k, for the ML100K dataset, k is set from 1 to 100 with a step size of 10. For the ML1M dataset, k is set from 1 to 200 with a step size of 20. Finally, we obtain the experimental results in Figure 5 and Figure 6 below.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nAs seen in Figure 5, the smallest MAE and RMSE are\nobtained for k=30 in the ML100K dataset. It can be seen in the figure that SRKGFCA has lower error values than the SRKG algorithm in all cases for the same k. In the extreme case of k (k=1), the error of SRKGFCA can still be at a low level, which indicates that SRKGFCA has a better recommendation for \u2018cold start\u2019 ; at the same time, the extreme cases of k value often occur in practical applications, which means that the algorithm has a stronger ability to deal with extreme cases after integrating FCA. In this case, we can basically conclude that the integrated FCA effectively solves the problem of the SRKG algorithm ignoring user factors. In particular, the significant improvement in the effectiveness of the SRKGFCA algorithm in extreme cases (k=1) in terms of RMSE error is due to the fact that the algorithm is able to find the set of nearest neighbors of each item more accurately instead of generalizing to k nearest neighbors.\nFor the ML1M dataset with a large amount of data (e.g.,\nFigure 6), the two algorithms achieve the lowest error at k = 40, and the SRKGFCA algorithm has a lower error deviation, confirming the conclusions drawn in Figure 5. In general, the SRKGFCA algorithm shows better recommendation results and confirms the effectiveness of the integrated FCA."
        },
        {
            "heading": "3) COMPARISON EXPERIMENTS AND ANALYSIS OF RESULTS",
            "text": "To verify the effectiveness and rationality of SRKG and SRKGFCA, we experimentally compared them with the current recommendation algorithms, which performed well on the MovieLens dataset.Benchmark methods are as follows\nItemKNN[28] : Item-based collaborative filtering\nrecommendation algorithm proposed by Rosing et al. By integrating the score similarity into the classic Amazon item collaborative filtering recommendation algorithm [17], this algorithm achieves good recommendation effect and is one of the benchmark comparison algorithms for most current recommendation algorithms.\nUserKNN+FCA[21] : A user-based collaborative filtering recommendation algorithm based on FCA and concept lattice proposed by Zou C et al in 2015. By integrating FCA into the coordinated filtering recommendation algorithm, the authors can more effectively mine the neighbor relationship between users. It is a good algorithm in the field of FCA\nrecommendation in recent years, but it cannot be run in largescale data sets. It is worth noting that compared with recommendation algorithms based on knowledge graph or deep matrix factorization, the recommendation effect based on FCA is relatively poor. Therefore, only the representative UserKNN+FCA[21] algorithm is selected in this section, instead of other FCA-based algorithms [22].\nNeuMF[17] : He et al. proposed the deep matrix\nfactorization NeuMF model combining probabilistic computation and multi-layer perceptron in 2017. This method performs deep decomposition of the rating matrix through deep learning, so as to achieve high-quality semantic recommendation. It is the reference model for most deep matrix factorization models at present.\nDMF[18] : Xue et al. proposed an improved deep matrix\nfactorization model through normalized cross-entropy loss to deal with the shortcomings of NeuMF in dealing with user ratings. It is a good algorithm in cryptic meaning recommendation algorithms in recent years, and the DMF model proposed by Xue et al. is also a model used in this paper to mine cryptic meaning.\nTransHR+SVD[20] : A semantic recommendation\nalgorithm implemented by Yuan Quan et al. in the past two years through TransHR model and SVD matrix factorization model. They introduce external semantic knowledge through knowledge graph representation learning, and then use matrix factorization to mine cryptic meaning, and achieve good recommendation results.\nMC+DMF[19] : implicit recommendation algorithm\nimplemented by Shi Jiarong et al., through matrix completion method and DMF model in recent two years. The sparse user rating matrix is filled to reduce the sparsity of the matrix, and then the deep matrix factorization is carried out to realize the implicit recommendation algorithm with better recommendation effect.\nFor training the DMF model, the number of hidden layers\nis set to 2 for the ML100K dataset and 3 for the ML1M dataset, the maximum number of training iterations is 500 in each case, and the other hyperparameters are the default parameters of the DMF model. The experimental results can be found in Table V.\nTABLE V Experimental results for each type of recommendation algorithm\nDataset Algorithm MAE RMSE HR@10 NDCG@10\nML100K\nItemKNN 0.778 0.985 0.606 0.334\nUserKNN+FC\nA 0.771 0.980 0.615 0.341 NeuMF 0.740 0.936 0.670 0.395\nDMF 0.704 0.895 0.686 0.409\nTransHR+SV\nD 0.678 0.864 0.683 0.389 MC+DMF 0.685 0.875 0.702 0.422\nSRKG 0.680 0.873 0.701 0.423 SRKGFCA 0.673 0.864 0.710 0.431\nML1M\nItemKNN 0.728 0.924 0.637 0.372\nUserKNN+FC A\n- - - -\nNeuMF 0.613 0.891 0.722 0.440\nDMF 0.613 0.877 0.732 0.451\nTransHR+SV D 0.595 0.851 0.738 0.447\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nMC+DMF 0.603 0.866 0.745 0.469\nSRKG 0.607 0.862 0.749 0.472 SRKGFCA 0.592 0.854 0.758 0.479\nAs shown in Table V, the SRKG and SRKGFCA\nalgorithms proposed in this paper, which combine external knowledge graph semantics and internal hidden semantics, achieve excellent recommendation performance. It is worth noting that MAE and RMSE are error indicators and their corresponding lower values indicate better recommendation results, while HR and NDCG are recommendation accuracy indicators and their corresponding higher values indicate better recommendation results. In particular, compared to several existing semantic recommendation algorithms (NeuMF, DMF, TransHR+SVD, MC +DMF), SRKGFCA achieves the best results in both datasets, except for the RMSE index, which does not show much advantage. The reason for the lack of advantage in RMSE metric is that some movie titles are not matched with DBpedia and some movie entities correspond to too few triads.\nOur experimental results with UserKNN+FCA and\nItemKNN on the ML100K dataset confirm the effectiveness of previous work on improved collaborative filter recommendation algorithms based on FCA. The main improvement lies in the fact that the k-nearest neighbor algorithm only considers the item factor in nearest neighbor selection and simply determines the nearest neighbor set of items through item similarity ranking. In contrast, the FCAbased nearest neighbor determination performs clustering of both users and items, improving both the prediction rating error (MAE, RMSE) and the accuracy of recommendation results (HR, HDCG). Since UserKNN+FCA on the ML1M dataset is not able to create a concept lattice in a short time, we did not test the four categories of metrics for this algorithm on this dataset.\nMoreover, by comparing the experimental results of\ngrouping implicit semantic-based recommendation algorithms (NeuMF, DMF) and knowledge graph-based algorithms (TransHR+SVD), we can conclude that the implicit semantic-based approaches are better at making sequential recommendations, since their HR and NDCG values are relatively high, which means that the content they recommend to users is more consistent with their preferences, while knowledge graph-based methods are better able to perform the rating prediction task since their rating error values (MAE and RMSE values) are relatively low. Comparing the DMF and NeuMF algorithms, the MC +DMF algorithm, and the SRKG and SRKGFCA algorithms, it is found that effectively padding the model input vectors increases the accuracy of the recommendations, i.e., the problem of deep neural networks falling into local optima during the training process is mitigated. Combining the advantages of the two types of algorithms, it can be said that the combined SRKG and SRKGFCA algorithms have excellent representation in prediction evaluation and also perform well in sequential recommendation tasks.\nIn Table V, the obvious shortcomings of our proposed\nsemantic recommendation algorithm can be found by comparing the differences in MAE and RMSE between our\nproposed semantic recommendation algorithm and the TransHR+SVD algorithm in the two datasets. In both datasets, the MAE values are not significantly improved, and the RMSE values are even lower than those of the TransHR+SVD algorithm in the ML1M dataset. When analyzing the ternary data from DBpedia, we found that the main reason is that there is little or no ternary data for some of the movies in the data. To check whether the MAE and RMSE values are not significantly improved due to the small number of triad data of some movies, we deleted the records of movies with less than 10 occurrences in the DBpedia triad in the ML100K and ML1M datasets, i.e., 1544 movie records were retained in ML100K and 3460 movie records were retained in ML1M. We reran the SRKG and SRKGFCA algorithms and found that both MAE and RMSE were reduced, as shown in Table VI.\nvalues are further reduced, and it can be verified that some movie entities are missing or the corresponding triad data are too few to affect the recommendation effect of the SRKG algorithm and the SRKGFCA algorithm in the above experiments. In addition, we find that the main reason for the missing movie entities or the too few corresponding triad data is that this work uses the public knowledge map of encyclopedias instead of the domain knowledge map constructed by the authors of the TransHR+SVD algorithm. However, exploring more accurate knowledge graph representation methods and constructing domain knowledge graphs is our main research direction for the future. In the meantime, we find that the semantic recommendation algorithm proposed in this paper still dominates the recommendation accuracy in the comparison between HR and NDCG."
        },
        {
            "heading": "VI. Conclusion",
            "text": "In this paper, we propose a semantic recommendation algorithm based on a knowledge graph and formal concept analysis (FCA) to address the shortcomings of the semantic recommendation algorithm based on a collaborative filtering algorithm. We address the problem that the collaborative filtering algorithm does not consider user or item factors when selecting nearest neighbors by combining formal concepts that can simultaneously cluster objects (users) and attributes (items), further reducing the error in predicting\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 7\nratings. At the same time, we address the problem that the conceptual grid of FCA cannot be generated in large-scale datasets. Based on previous work, we propose a method to calculate the fuzzy concept area and a heuristic to construct formal concepts, so that the semantic recommendation algorithm integrating FCA in this paper can run on different datasets. We also selected several representative recommendation algorithms as benchmark methods for comparison and verified that our proposed semantic recommendation algorithm based on a knowledge graph and FCA achieves the best results on various metrics. Our next work will address how to effectively use FCA for better nearest neighbor discovery, how to effectively construct an intra-domain knowledge graph, e.g., a knowledge graph for the movie domain, and how to effectively use deep neural networks for representation learning of large-scale knowledge graphs to further improve the accuracy of semantic recommendations."
        }
    ],
    "title": "Semantic Recommendation Model via Fusing Knowledge Graph and Formal Concept Analysis",
    "year": 2023
}