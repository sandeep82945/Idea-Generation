{
    "abstractText": "We study single-player extensive-form games with imperfect recall, such as the Sleeping Beauty problem or the Absentminded Driver game. For such games, two natural equilibrium concepts have been proposed as alternative solution concepts to ex-ante optimality. One equilibrium concept uses generalized double halving (GDH) as a belief system and evidential decision theory (EDT), and another one uses generalized thirding (GT) as a belief system and causal decision theory (CDT). Our findings relate those three solution concepts of a game to solution concepts of a polynomial maximization problem: global optima, optimal points with respect to subsets of variables and Karush\u2013Kuhn\u2013Tucker (KKT) points. Based on these correspondences, we are able to settle various complexity-theoretic questions on the computation of such strategies. For ex-ante optimality and (EDT,GDH)-equilibria, we obtain NP-hardness and inapproximability, and for (CDT,GT)-equilibria we obtain CLS-completeness results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Emanuel Tewolde"
        },
        {
            "affiliations": [],
            "name": "Caspar Oesterheld"
        },
        {
            "affiliations": [],
            "name": "Paul W. Goldberg"
        }
    ],
    "id": "SP:20b2b02d9d7f3aeb4a45294885023ef5033f966d",
    "references": [
        {
            "authors": [
                "Yakov Babichenko",
                "Aviad Rubinstein. Settling the complexity of nash equilibrium in congestion games"
            ],
            "title": "In STOC \u201921: 53rd Annual ACM SIGACT Symposium on Theory of Computing",
            "venue": "pages 1426\u20131437. ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Rachael Briggs. Putting a value on Beauty. In Tamar Szab\u00f3 Gendler",
                "John Hawthorne"
            ],
            "title": "editor",
            "venue": "Oxford Studies in Epistemology: Volume 3, pages 3\u201334. Oxford University Press,",
            "year": 2010
        },
        {
            "authors": [
                "Colin F. Camerer"
            ],
            "title": "Behavioral Game Theory: Experiments in Strategic Interaction",
            "venue": "Princeton University Press,",
            "year": 2003
        },
        {
            "authors": [
                "John Canny. Some algebraic",
                "geometric computations in pspace. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing"
            ],
            "title": "STOC \u201988",
            "venue": "page 460\u2013467, New York, NY, USA,",
            "year": 1988
        },
        {
            "authors": [
                "Xi Chen",
                "Xiaotie Deng",
                "Shang-hua Teng"
            ],
            "title": "Computing nash equilibria: Approximation and smoothed complexity",
            "venue": "2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pages 603\u2013612,",
            "year": 2006
        },
        {
            "authors": [
                "Xi Chen",
                "Xiaotie Deng",
                "Shang-Hua Teng. Settling the complexity of computing two-player Nash equilibria. J"
            ],
            "title": "ACM",
            "venue": "56(3):14:1\u201314:57,",
            "year": 2009
        },
        {
            "authors": [
                "Francis C. Chu",
                "Joseph Y. Halpern. On the np-completeness of finding an optimal strategy in games with common payoffs. Int. J"
            ],
            "title": "Game Theory",
            "venue": "30(1):99\u2013106,",
            "year": 2001
        },
        {
            "authors": [
                "Conitzer",
                "Oesterheld",
                "2023] Conitzer"
            ],
            "title": "Foundations of cooperative ai",
            "year": 2023
        },
        {
            "authors": [
                "Vincent Conitzer. A Dutch book against sleeping beauties who are evidential decision theorists"
            ],
            "title": "Synthese",
            "venue": "192(9):2887\u20132899,",
            "year": 2015
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Christos Papadimitriou. Continuous local search. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms"
            ],
            "title": "SODA \u201911",
            "venue": "page 790\u2013804, USA,",
            "year": 2011
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Paul W. Goldberg",
                "Christos H. Papadimitriou. The complexity of computing a Nash equilibrium"
            ],
            "title": "SIAM J",
            "venue": "Comput., 39(1):195\u2013259,",
            "year": 2009
        },
        {
            "authors": [
                "Etienne de Klerk. The complexity of optimizing over a simplex"
            ],
            "title": "hypercube or sphere: a short survey",
            "venue": "Central Eur. J. Oper. Res., 16(2):111\u2013125,",
            "year": 2008
        },
        {
            "authors": [
                "Kai Draper",
                "Joel Pust. Diachronic Dutch Books",
                "Sleeping Beauty"
            ],
            "title": "Synthese",
            "venue": "164(2):281\u2013287,",
            "year": 2008
        },
        {
            "authors": [
                "Adam Elga. Self-locating belief",
                "the Sleeping Beauty problem"
            ],
            "title": "Analysis",
            "venue": "60(2):143\u2013147,",
            "year": 2000
        },
        {
            "authors": [
                "Elkind et al",
                "2022] Edith Elkind",
                "Abheek Ghosh",
                "Paul W. Goldberg"
            ],
            "title": "Simultaneous contests with equal sharing allocation of prizes: Computational complexity and price of anarchy",
            "venue": "In Algorithmic Game Theory - 15th International Symposium, SAGT,",
            "year": 2022
        },
        {
            "authors": [
                "Fearnley et al",
                "2021] John Fearnley",
                "Paul W. Goldberg",
                "Alexandros Hollender",
                "Rahul Savani"
            ],
            "title": "The complexity of gradient descent: CLS = PPAD \u2229 PLS",
            "venue": "STOC \u201921: 53rd Annual ACM SIGACT Symposium on Theory",
            "year": 2021
        },
        {
            "authors": [
                "John Fearnley",
                "Paul Goldberg",
                "Alexandros Hollender",
                "Rahul Savani"
            ],
            "title": "The complexity of gradient descent: CLS = PPAD \u2229 PLS",
            "venue": "J. ACM, 70(1):7:1\u20137:74,",
            "year": 2023
        },
        {
            "authors": [
                "Drew Fudenberg",
                "Jean Tirole. Game Theory"
            ],
            "title": "MIT Press",
            "venue": "October",
            "year": 1991
        },
        {
            "authors": [
                "Gimbert et al",
                "2020] Hugo Gimbert",
                "Soumyajit Paul",
                "B. Srivathsan"
            ],
            "title": "A bridge between polynomial optimization and games with imperfect recall",
            "venue": "In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Johan H\u00e5stad"
            ],
            "title": "Clique is hard to approximate within n1-epsilon",
            "venue": "37th Annual Symposium on Foundations of Computer Science, FOCS, pages 627\u2013636. IEEE Computer Society,",
            "year": 1996
        },
        {
            "authors": [
                "Christopher Hitchcock. Beauty",
                "the bets"
            ],
            "title": "Synthese",
            "venue": "139(3):405\u2013420,",
            "year": 2004
        },
        {
            "authors": [
                "Hub\u00e1cek",
                "Yogev",
                "2017] Pavel Hub\u00e1cek",
                "Eylon Yogev"
            ],
            "title": "Hardness of continuous local search: Query complexity and cryptographic lower bounds",
            "venue": "Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2017
        },
        {
            "authors": [
                "David S. Johnson",
                "Christos H. Papadimitriou",
                "Mihalis Yannakakis"
            ],
            "title": "How easy is local search? Journal of Computer and System Sciences",
            "venue": "37(1):79\u2013100,",
            "year": 1988
        },
        {
            "authors": [
                "Daphne Koller",
                "Nimrod Megiddo. The complexity of two-person zero-sum games in extensive form"
            ],
            "title": "Games and Economic Behavior",
            "venue": "4(4):528\u2013552,",
            "year": 1992
        },
        {
            "authors": [
                "Vojtech Kovarik",
                "Caspar Oesterheld",
                "Vincent ConitzerYuta. Game theory with simulation of other players. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence"
            ],
            "title": "IJCAI 2023",
            "venue": "Macao, 9-25 August 2023. ijcai.org,",
            "year": 2023
        },
        {
            "authors": [
                "Christian Kroer",
                "Tuomas Sandholm. Imperfect-recall abstractions with bounds in games"
            ],
            "title": "In Proceedings of the Seventeenth ACM Conference on Economics and Computation (EC)",
            "venue": "pages 459\u2013 476, Maastricht, the Netherlands,",
            "year": 2016
        },
        {
            "authors": [
                "Marc Lanctot",
                "Richard G. Gibson",
                "Neil Burch",
                "Michael Bowling. No-regret learning in extensive-form games with imperfect recall"
            ],
            "title": "In Proceedings of the 29th International Conference on Machine Learning (ICML-12)",
            "venue": "Edinburgh, Scotland, UK,",
            "year": 2012
        },
        {
            "authors": [
                "S. Lang"
            ],
            "title": "Algebraic Number Theory",
            "venue": "Graduate Texts in Mathematics. Springer",
            "year": 1994
        },
        {
            "authors": [
                "Nimrod Megiddo",
                "Christos H. Papadimitriou. On total functions"
            ],
            "title": "existence theorems and computational complexity",
            "venue": "Theor. Comput. Sci., 81(2):317\u2013324, apr",
            "year": 1991
        },
        {
            "authors": [
                "Noam Nisan",
                "Tim Roughgarden",
                "\u00c9va Tardos",
                "Vijay V. Vazirani"
            ],
            "title": "editors",
            "venue": "Algorithmic Game Theory. Cambridge University Press,",
            "year": 2007
        },
        {
            "authors": [
                "Oesterheld",
                "Conitzer",
                "2022] Caspar Oesterheld",
                "Vincent Conitzer"
            ],
            "title": "Can de se choice be ex ante reasonable in games of imperfect recall? https://www.andrew.cmu.edu/ user/coesterh/DeSeVsExAnte.pdf, 2022",
            "venue": "Working paper",
            "year": 2022
        },
        {
            "authors": [
                "Christos H. Papadimitriou. On the complexity of the parity argument",
                "other inefficient proofs of existence"
            ],
            "title": "Journal of Computer and System Sciences",
            "venue": "48(3):498\u2013532,",
            "year": 1994
        },
        {
            "authors": [
                "Michele Piccione",
                "Ariel Rubinstein. On the interpretation of decision problems with imperfect recall"
            ],
            "title": "Games and Economic Behavior",
            "venue": "20(1):3\u201324,",
            "year": 1997
        },
        {
            "authors": [
                "Marcus Schaefer",
                "Daniel Stefankovic. Fixed points"
            ],
            "title": "nash equilibria",
            "venue": "and the existential theory of the reals. Theory Comput. Syst., 60(2):172\u2013193,",
            "year": 2017
        },
        {
            "authors": [
                "Waugh et al",
                "2009] Kevin Waugh",
                "Martin Zinkevich",
                "Michael Johanson",
                "Morgan Kan",
                "David Schnizlein",
                "Michael H. Bowling"
            ],
            "title": "A practical use of imperfect recall",
            "venue": "In Eighth Symposium on Abstraction,",
            "year": 2009
        },
        {
            "authors": [
                "Fearnley"
            ],
            "title": "2021] note that any norm can be used",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Most formalisms that are used for reasoning under uncertainty and for decision making in AI \u2013 for example, HMMs, (dynamic) Bayesian networks, influence diagrams, MDPs, POMDPs, multiagent versions of these \u2013 assume what is known as perfect recall: the agent does not forget anything it knew before. This may seem to be a very natural assumption: in the design of AI agents, generally we have plenty of reliable memory available. Moreover, the property of perfect recall ensures various desirable properties in the context of extensive-form games, including polynomial-time solvability of two-player zero-sum games [Koller and Megiddo, 1992] (and hence, a fortiori, single-player games). Finally, even when modeling humans \u2013 as in, for example, behavioral game theory [Camerer, 2003] \u2013 in spite of our clearly imperfect memory, usually perfect-recall models are used. So why use models with imperfect recall in AI?\n\u2217Published in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23), Macao, 2023. (http://www.ijcai.org)\nIt turns out there are a number of reasons why imperfect recall is relevant for AI agents; moreover, in cases where it is relevant, it is clear what the agent will and will not remember \u2013 unlike in the case of human memory, which is harder to predict and consequently to model in standard representations of imperfect recall. Imperfect-recall games already appear in the AI literature in the context of solving very large games such as poker: one technique for solving such games is abstraction \u2013 i.e., reducing the game to a smaller, simplified one to solve instead \u2013 and this process can give rise to imperfect recall in the abstracted game [Waugh et al., 2009; Lanctot et al., 2012; Kroer and Sandholm, 2016]. But imperfect recall is also of interest for other reasons. First, we may deliberately choose to have our agents forget: for example, the agent may temporarily need access to data that is sensitive from a privacy perspective, and therefore best forgotten afterwards. Conitzer and Oesterheld [2023] give the example of an AI driver assistant that can take over whenever the human car driver makes a major error. When that happens, the AI needs to reason about how good the human driver is in general, about whom it is not allowed to store information. An AI agent could also take the form of a highly distributed system operating across many nodes, where not all the nodes have access to the same information; hence, it may act at one node without having access to information that it did have available when acting at another node. Relatedly, the same agent (in the sense of being based on the same source code) may be instantiated multiple times, for example by human users deploying it in multiple contexts. In such cases it can still be useful to consider this family of instantiations as a single agent, but again these instantiations will not all have access to the same information. Finally, again building on the previous case, an agent may be acting not only in the real world, but also in simulations; for example, it may be simulated by another agent that wants to ensure that another instantiation of the same agent will act in a trustworthy fashion in the real world later [Kovarik et al., 2023]. In this case, the real-world instantiation of the agent will generally not have access to the information that the simulation had access to earlier.\nNotably, we need to model this phenomenon as imperfect recall rather than merely as imperfect information. In single-agent perfect-recall imperfect-information games (say, a POMDP), there is never a (strict) reason to randomize, whereas in imperfect-recall games the agent might have to\nar X\niv :2\n30 5.\n17 80\n5v 1\n[ cs\n.G T\n] 2\n8 M\nay 2\n02 3\nrandomize in order to perform well overall; cf. the Absentminded Driver example [Piccione and Rubinstein, 1997] (Appendix C). For example, suppose we deploy a content recommendation system to many people\u2019s phones, in an edgecomputing sort of setup: We are not in constant communication with the phones, so the nodes of our system have to act independently each day before getting back in touch with us. Over the next day, we would like to experiment (in an optimal way) what kind of content to recommend. With a pure strategy, we would show all users the same content and learn very little from it. Instead, we would prefer to randomize the content shown on each phone, that is, use a mixed strategy. Therefore, this situation cannot be a perfect-recall game (even of imperfect information).\nBeing able to make decisions with imperfect recall also represents a technical frontier. Many existing techniques inherently rely on perfect recall. Solving two-player zerosum games becomes NP-hard as soon as one player has imperfect recall [Koller and Megiddo, 1992]. Moreover, in these contexts, there remains controversy at the very foundations of how to do probabilistic reasoning and decision making. For example, the Sleeping Beauty problem [Elga, 2000] (Appendix C) asks one to give the probability of a state of the world in an imperfect-recall setting; some (Thirders) believe that the correct answer is 1/3, and others (Halvers) believe it is 1/2, see Section 3.2. Only recently has a clear picture started to emerge regarding how each of these positions can be combined with a corresponding form of decision theory to make good decisions [Hitchcock, 2004; Draper and Pust, 2008; Briggs, 2010; Conitzer, 2015; Oesterheld and Conitzer, 2022]; here we build on that recent conceptual work to define and study several foundational computational problems.\nIn this paper, we use extensive-form games to represent settings with imperfect recall. Even though we are considering a single-agent setting, the extensive form is still especially natural to use to model imperfect-recall settings, specifically with the use of information sets. Indeed, as we will discuss, game-theoretic phenomena such as notions of equilibrium naturally come up in the presence of imperfect recall even when there is just a single agent. Intuitively that is because it is more challenging for that agent to coordinate its actions with those it takes at other times. Moreover, randomization is in general necessary. We consider behavior strategies, which map each information set to a probability distribution over actions. Based on recent literature, we study three distinct solution concepts: (1) ex ante optimality, where the behavior strategy is one that maximizes expected utility at the outset; (2) equilibria based on causal decision theory and generalized thirding, in which an agent would not want to change its action at any information set, under the assumption that at all other game tree nodes (including ones in the same information set) the agent would follow the original strategy; and (3) equilibria based on evidential decision theory and generalized double halving, in which an agent would not want to switch to a different distribution over actions at a given information set, assuming that the agent would also use the new distribution at other nodes in that information set (but would use the original strategy at all other information sets).\nSection 2 and 3 define those solution concepts and cover previously known characterizations and hardness results. Section 4 presents our novel results: First we show that the equilibria based on causal decision theory and generalized thirding are exactly the Karush\u2013Kuhn\u2013Tucker points of a corresponding utility maximization problem. This makes gradient descent methods applicable to the computation of such an equilibrium, and, relatedly, we derive that problems of finding such an equilibrium \u2013 up to an inverse exponential precision \u2013 are complete for the class CLS (Continuous Local Search). Finally, we derive various NP-hardness results for maximizing over the set of equilibria and for finding an equilibrium based on evidential decision theory and generalized double halving. Naturally, all these complexity results also have implications for learning or dynamics that converge to these solutions."
        },
        {
            "heading": "2 Background for Imperfect-Recall Games",
            "text": ""
        },
        {
            "heading": "2.1 Single-Player Extensive-Form Games with Imperfect Recall",
            "text": "We first define single-player extensive-form games, allowing for imperfect recall. The concepts we use in doing so are standard; for more detail and background, see, e.g., Fudenberg and Tirole [1991], Nisan et al. [2007] and Piccione and Rubinstein [1997].\nDefinition 1. A single-player extensive-form game with imperfect recall (denoted \u0393), sometimes also called an extensive decision problem with imperfect recall, consists of:\n1. A rooted tree, with nodes H and where the edges are labeled with actions. The game starts at the root node h0 and finishes at a leaf node, also called terminal node. The terminal nodes in H will be denoted as Z . The set Ah refers to the set of actions available at a nonterminal node h \u2208 H \\ Z . 2. A utility function u : Z \u2192 R, where u(z) represents the payoff that the player receives from finishing the game at terminal node z. 3. A partition H \\ Z = H\u2217 \u2294 Hc of nonterminal nodes into a set of the player\u2019s decision nodes H\u2217 and a set of chance nodes Hc. This partition indicates whether the single player or exogenous stochasticity determines the action at any given node. 4. For each chance node h \u2208 Hc, a fixed distribution Pc(\u00b7 | h) over Ah according to which chance determines an action at h. 5. A partition H\u2217 = \u2294I\u2208I\u2217I of the player\u2019s decision nodes into information sets (\u201cinfo sets\u201d for short). We require Ah = Ah\u2032 for any nodes h, h\u2032 in the same info set I .\nThroughout this paper, we let \u2113 := |I\u2217| denote the number of info sets. For computational purposes, we assume that a game \u0393 is represented by its game tree structure of size \u0398(|H|) (which includes the info set partition), and by a binary encoding of its chance node probabilities and its utility payoffs. The last two shall take on rational values only.\nAny node h \u2208 H uniquely corresponds to a (node,action)-history hist(h) from root h0 to h in the game tree. Define functions d, \u03bd, a such that the\nnode history and action history from h0 to h consist of the sequences ( \u03bd(h, 0), \u03bd(h, 1), ... , \u03bd(h, d(h)) ) and(\na(h, 0), a(h, 1), ... , a(h, d(h) \u2212 1) )\nrespectively. In other words, function d : H \u2192 N0 identifies the tree depth of a node, \u03bd : H\u00d7N0 \u2192 H the node ancestor at a specified depth, and a : H\u00d7N0 \u2192 \u2294h\u2208HAh the action ancestor at a specified depth. In particular, for all h \u2208 H, we have \u03bd(h, 0) = h0 and \u03bd(h, d(h)) = h. We restrict the domain of functions \u03bd and a to inputs (h, k) with k \u2264 d(h) and k \u2264 d(h)\u22121 respectively, and note that a maps (h, k) into A\u03bd(h,k).\nThe depth of \u0393 is defined to be the maximal depth of the leaf nodes. For notational convenience, we add a singleton info set to \u0393 for each chance node in \u0393. The collection of these info sets, each consisting of a single element in Hc, shall be denoted by Ic. For each nonterminal node h \u2208 H\\Z , let Ih \u2208 I\u2217 \u2294 Ic denote its info set. For each info set I \u2208 I\u2217 \u2294 Ic, let AI denote its action set.\nNodes of the same info set are assumed to be indistinguishable to the player during the game (even though the player is always aware of the full game structure). There may be information about the history of play that the player holds at some node, and that the player forgets somewhere further down its subtree. For instance, consider the game in Figure 1. Once the player arrives at node h3, she cannot distinguish it from possibly being at node h2. Thus she has already forgotten that she has only taken one action (action C) so far. In contrast to that, games with perfect recall have every info set reflect that the player remembers all her earlier actions. In particular, the player does not forget which info sets she entered in which order in the history of play.\nWith imperfect recall, it could furthermore be the case that multiple nodes of the same history (of some terminal node) belong to the same info set, as in info set I1 in the game of Figure 1. The inability of a player to distinguish between two nodes on the same history is a property that we will refer to as absentmindedness; cf. the Absentminded Driver from Piccione and Rubinstein [1997] (Appendix C).\nLet \u2206(AI) denote the set of probability distributions over\nthe actions in AI . A (behavioural) strategy \u00b5 : I\u2217 \u2192 \u2294I\u2208I\u2217\u2206(AI) of the player assigns to each info set I a probability distribution \u00b5(\u00b7 | I) \u2208 \u2206(AI). At info set I , the player will then randomly draw an action according to \u00b5(\u00b7 | I). By abuse of notation, we extend any strategy \u00b5 of the player to info sets Ih \u2208 Ic of chance nodes h \u2208 Hc by setting \u00b5(\u00b7 | Ih) := Pc(\u00b7 | h) there.\nGiven that the player is currently at node h\u0304 \u2208 H \\ Z and that she plays according to strategy \u00b5, we can calculate the probability of reaching node h \u2208 H by multiplying the probabilities of the actions on the path from h\u0304 to h:\nP(h | \u00b5, h\u0304) = d(h)\u22121\u220f k=d(h\u0304) \u00b5 ( a(h, k) | I\u03bd(h,k) ) if h\u0304 \u2208 hist(h)\nand P(h | \u00b5, h\u0304) = 0 otherwise. As a special case, we define the reach probability P(h | \u00b5) := P(h | \u00b5, h0) of a node h \u2208 H to be its reach probability from the root h0 of \u0393. Naturally, the reach probability of the root is 1.\nThe expected utility payoff for being at node h \u2208 H \\ Z and using strategy \u00b5 from then on can be determined by U(\u00b5 | h) := \u2211 z\u2208Z P(z | \u00b5, h) \u00b7 u(z). Furthermore, let U : \u00b5 7\u2192 U(\u00b5) := U(\u00b5 | h0) be the function that takes a strategy \u00b5 of \u0393 and returns the expected utility payoff of the player from following \u00b5 from the game start to termination. U(\u00b5) is also called the ex-ante (expected) utility of \u00b5."
        },
        {
            "heading": "2.2 Utility as a polynomial function",
            "text": "Fix an ordering I1, . . . , I\u2113 of the info sets in I\u2217 and denote mi := |AIi | for all i \u2208 [\u2113]. Moreover, fix an ordering a1, . . . , ami of the actions in AIi for all i \u2208 [\u2113].\nWe can uniquely describe a strategy \u00b5 of \u0393 by the probability values that it assigns to each action aj at info set Ii, for i \u2208 [\u2113] and j \u2208 [mi]. A strategy \u00b5 is a vector \u00b5 = (\u00b5ij)i,j \u2208\u00d7\u2113i=1 Rmi such that each subvector \u00b5i\u00b7 = (\u00b5ij)j lies in the simplex \u2206(AIi) \u2261 \u2206mi\u22121 := {y \u2208 Rmi : yj \u2265 0 \u2200j , \u2211mi j=1 yj = 1}. Therefore, the strategy space of \u0393 is \u00d7\u2113i=1 \u2206(AIi) \u2261\u00d7\u2113i=1 \u2206mi\u22121. The expected utility function U of a strategy \u00b5 can be fully written out as\nU(\u00b5) = \u2211 z\u2208Z ( u(z) \u00b7 d(z)\u22121\u220f k=0 \u00b5 ( a(z, k) | I\u03bd(z,k) )) .\nAs noted by Piccione and Rubinstein [1997], this is a polynomial function in the variables (\u00b5ij)i,j . Recall that at chance nodes, the probabilities are exogenously fixed constants. Thus, the degree of the polynomial function U is upper-bounded by the maximum number of times the player of \u0393 might have to take a decision in order to reach a terminal node. Note that polynomial U can be constructed in polynomial time in the encoding size of \u0393.\nExample 2. In the game of Figure 1, we get \u2113 = 2, m1 = 3, m2 = 2. Let the actions be ordered as (L,C,R) and (X,Y ). Then, for any point \u00b5 \u2208 R3 \u00d7 R2, we have U(\u00b5) = 5\u00b511\u00b513\u00b521 + \u00b513\u00b522.\nWe show in Appendix A that one can also reduce any multivariate polynomial p :\u00d7\u2113i=1 Rmi \u2192 R to a single-player extensive-form game \u0393 with imperfect recall such that its expected utility function satisfies U(\u00b5) = p(\u00b5) on\u00d7\u2113i=1 Rmi ."
        },
        {
            "heading": "2.3 (Computing) Ex-ante Optimal Strategies",
            "text": "Suppose we want to solve a given game \u0393. From a planning perspective, one would naturally search for a strategy that promises the highest payoff at a time before the player enters the game. Definition 3. We say a strategy \u00b5\u2217 is ex-ante optimal for \u0393 if it solves\nmax \u00b5\nU(\u00b5) s.t. \u00b5 \u2208 \u2113\n\u00d7 i=1 \u2206(AIi) . (1)\nDue to Koller and Megiddo [1992], we can find an ex-ante optimal strategy for a single-player game with perfect recall in polynomial time. This will not be the case anymore in the presence of imperfect recall, as we will show next.\nThe class ZPP contains all those decision problems that can be solved in expected polynomial time by a randomized (Las Vegas) algorithm. Let OPT be an optimization problem with max and min values q\u0304 and q. Then, a fully polynomial-time approximation scheme (FPTAS) for OPT computes a solution to an instance of OPT with an objective value that is at most \u03f5 \u00b7 (q\u0304 \u2212 q) away from the optimal value. This computation must take polynomial time in 1/\u03f5 and the encoding size of the instance. For a more precise definition, see de Klerk [2008]. Proposition 4. Consider the problem that takes a game \u0393 and target value t \u2208 Q (encoded in binary) as inputs and asks whether there is a strategy \u00b5 for \u0393 with ex-ante expected utility U(\u00b5) \u2265 t. This problem is NP-hard. Moreover: (1.) Unless NP = ZPP, there is no FPTAS for this prob-\nlem. NP-hardness and conditional inapproximability hold even if the game instance \u0393 has a tree depth of 3 and only one info set. (2.) NP-hardness holds even if \u0393 has no absentmindedness, a tree depth of 4 and the player has 2 actions per info set. (3.) NP-hardness holds even if \u0393 has no absentmindedness, a tree depth of 3 and the player has 3 actions per info set.\nGimbert et al. [2020] shows that this problem of deciding whether a given target value can be achieved in \u0393 is in fact \u2203R-complete (a complexity class described in the last paragraph of this section).\nAn early NP-hardness result of that kind was given by Koller and Megiddo [1992]. Note that finding the ex-ante optimal strategy of \u0393 is at least as hard as this NP-hard decision problem of whether a target value can be achieved in \u0393. On the other hand, with an efficient solver of the decision version, one can recover the optimal ex-ante utilityU\u2217 := max\u00b5 U(\u00b5) through binary search.\nA proof of Proposition 4 can be found in Appendix B. Result (1.) is based on our reduction from polynomials to games and with the known hardness of maximizing a polynomial function over a simplex [de Klerk, 2008]. Result (3.) reduces\nfrom the hard problem of finding an optimal joint strategy in multiplayer common payoff games [Chu and Halpern, 2001]. The proofs reveal that NP-hardness remains even if the encoding size of the chance node probabilities and utility values are in O(|H|).\nAs for complexity upper bounds, consider the complexity class \u2203R called the existential theory of the reals [Renegar, 1992; Schaefer and Stefankovic, 2017]. It consists of all those problems that reduce to deciding whether a sentence of the following form is true: \u2203x1 . . . \u2203xnF (x1, . . . xn), where the xi are real-valued variables and where F is a quantifierfree formula that may contain equalities and inequalities of real polynomials. \u2203R lies in between NP and PSPACE [Shor, 1990; Canny, 1988]. By Section 2.2, it is straight-forward to see that the decision problem of Proposition 4 is contained in \u2203R. As of now, it is unclear whether NP membership can be shown; in part because easy-to-encode games may only admit ex-ante optimal strategies that take on irrational numbers (see Appendix G). But we can decide an approximate version of the problem in Proposition 4 in NP time; namely, when it is allowed to incorrectly return \u201cyes\u201d to the problem instance (\u0393, t, \u03f5) if there exists a strategy profile \u00b5 with U(\u00b5) \u2265 t\u2212 \u03f5. Here, \u03f5 > 0 represents an inverse-exponential precision parameter."
        },
        {
            "heading": "3 Equilibria in Imperfect-Recall Games",
            "text": "Proposition 4 shows a strong obstacle to finding or approximating ex-ante optimal strategies for single-player extensiveform games with imperfect recall. In light of these limitations, we will relax the space of solutions to equilibrium strategies. This solution concept argues that, whenever the player finds herself in an info set, she has no influence over which actions she chooses at other info sets. Therefore, at an equilibrium strategy \u00b5, the player will play the best action at each info set, assuming that she has been playing according to \u00b5 up to the current decision point and that she will continue to do so at future decision points. Prior work has given a detailed description of viable equilibrium concepts in single-player games with imperfect recall [Piccione and Rubinstein, 1997; Briggs, 2010; Oesterheld and Conitzer, 2022]. We will consider two well-motivated equilibrium concepts that have been proposed and where an ex-ante optimal strategy also constitutes an equilibrium. In games without absentmindedness, these two equilibrium concepts coincide. In games with absentmindedness, the concepts differ in how expected utilities are evaluated for an action a at a current info set I , given that the player plays according to strategy \u00b5 anywhere \u201celse\u201d. Computing such expected utilities requires\n1. A Belief System: A method to form beliefs (i.e., a probability distribution) over being at a specific node/history of \u0393 given that the player is at info set I; and 2. A Decision Theory: An understanding of how an action choice at the current node affects the freedom to choose an action at other nodes of the same info set.\nIn the sequel, let I be the player\u2019s current info set at which she finds herself during play while playing \u00b5 in game \u0393."
        },
        {
            "heading": "3.1 Decision Theories",
            "text": "Causal Decision Theory (CDT) postulates that the player can take an action \u03b1 \u2208 \u2206(AI) at the current node without violating that the player has been playing according to \u00b5 at past arrivals at I , or that she will be playing according to \u00b5 at future arrivals at I . The intuition behind CDT is that the player\u2019s choice to deviate away from \u00b5 at the current node does not cause any change in behaviour at any other node of the same info set I .\nIn contrast to that, Evidential Decision Theory (EDT) postulates that if the player takes an action \u03b1 \u2208 \u2206(AI) at the current node, then she will have also deviated to \u03b1 whenever she arrived in I in past play, and she will be deviating to \u03b1 whenever she arrives in I again in future play. Indeed, EDT argues that the choice to deviate to \u03b1 now is evidence for the player taking the same deviation choice in the past and future.\nDenote with \u00b5I 7\u2192\u03b1 an EDT deviation, i.e., the strategy of \u0393 that plays according to \u00b5 at every info set except at the info set I \u2208 I\u2217 where it plays according to \u03b1 \u2208 AI . By contrast, a CDT deviation may result in different actions taken at the same info set. This might not constitute a valid strategy that the player could have picked before the game started.\nExample 5. Consider the game in Figure 1 and suppose the player enters the game with the strategy \u00b5 = (R,X). Say, upon visiting info set I1, the player plans to deviate from the \u00b5-prescribed action R to the action L this one time only. Then, CDT argues that the player will stick to her \u00b5prescribed action R at the other node of I1, leading to one of the two action histories (L,R,X) or (R,X). EDT, on the other hand, argues that such a deviation will then happen at both nodes of I , leading to the action histories (L,L)."
        },
        {
            "heading": "3.2 Self-locating Belief Systems",
            "text": "Let I1st \u2286 I refer to those nodes h \u2208 I that are the first node of their history to enter info set I . Define the reach probability and (expected) visit frequency of I under \u00b5 as P(I | \u00b5) := \u2211 h\u2208I1st P(h | \u00b5) and Fr(I | \u00b5) := \u2211 h\u2208I P(h | \u00b5). Note that the reach probability and the visit frequency can only differ in games with absentmindedness, and that the visit frequency can be greater than 1. However, we have in general that P(I | \u00b5) > 0 if and only if Fr(I | \u00b5) > 0. Finally, denote with \u03c7 : P \u2192 {0, 1} the function that takes a Boolean property P as input and evaluate 1 if and only if P is true.\nThe first belief system argues that one should focus on the visit frequencies:\nDefinition 6. Let I be an info set with Fr(I | \u00b5) > 0 under \u00b5, and let h \u2208 H\u2217 be a player node. Then, Generalized Thirding (GT) determines the probability of the player to be at h, given that she uses \u00b5 and is currently in I , through\nPGT(h | \u00b5, I) := \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) Fr(I | \u00b5) .\nThe second belief system argues that one should rather focus on the reach probabilities. Note that the statement I \u2229 hist(z) \u0338= \u2205 evaluates as true if and only if I occurs in history hist(z) of terminal node z \u2208 Z at least once.\nDefinition 7. Let I be an info set with P(I | \u00b5) > 0 under \u00b5, and let z \u2208 Z be a terminal node. Then, Generalized Double Halving (GDH) determines the probability of the player being on the path hist(z) to terminal node z, given that she uses \u00b5 and is currently in I , through\nPGDH( hist(z) | \u00b5, I ) := \u03c7(I \u2229 hist(z) \u0338= \u2205) \u00b7 P(z | \u00b5) P(I | \u00b5) .\nGT and GDH were introduced as \u201cconsistency\u201d and \u201czconsistency\u201d by [Piccione and Rubinstein, 1997].\nWith the current definitions, GT and GDH assign probabilities to different type of events (to be at player node versus to be in the history of a terminal node). In Appendix D, we phrase GT and GDH in each other\u2019s language. In the language of GDH, GT assigns the event of being in history hist(z) of a terminal node z a higher probability if the reach probability of z under \u00b5 is higher (same as GDH) and if hist(z) visits info set I very often (whereas GDH only cares about I being visited at least once by hist(z)). Example 8. Consider the game in Figure 1 again and suppose the player enters the game with the strategy \u00b5 = ( 12L+ 1 2R,X). Say, the player observes to be in info set I1. Then a GT player believes to be at the node h0 in the history (R,X) with probability 13 whereas a GDH player believes to be at h0 in (R,X) with probability 12 . The names \u201cHalving\u201d and \u201cThirding\u201d originate from this contrast but for a different example called Sleeping Beauty [Elga, 2000] (Appendix C)."
        },
        {
            "heading": "3.3 Two equilibrium concepts",
            "text": "Any claims made in this section are proven in Appendix E.\nWe start with the equilibrium concept that uses Causal Decision Theory and Generalized Thirding. Denote with h \u25e6 a the child node reached in \u0393 by following action a \u2208 Ah from player node h \u2208 H\u2217. Then U(\u00b5 | h \u25e6 a) is the expected utility the player receives from being at h, playing a now, and playing according to \u00b5 afterwards. Definition 9. Let the player currently be at an info set I with Fr(I | \u00b5) > 0, and let \u03b1 \u2208 \u2206(AI) be a mixed action. Then, the (CDT,GT)-expected utility of playing \u03b1 now and according to \u00b5 otherwise is\nEUCDT,GT(\u03b1 | \u00b5, I) := \u2211 h\u2208I PGT(h | \u00b5, I)\n\u00b7 ( \u2211\na\u2208AI\n\u03b1(a) \u00b7 U(\u00b5 | h \u25e6 a) ) .\nNote that the inner sum collapses to U(\u00b5 | h \u25e6 a) if the considered mixed action \u03b1 is a pure action a. Definition 10. We say a strategy \u00b5\u2217 of \u0393 is a (CDT,GT)equilibrium if for all info sets I \u2208 I\u2217 with Fr(I | \u00b5\u2217) > 0 under \u00b5\u2217, we have\n\u00b5\u2217(\u00b7 | I) \u2208 argmax \u03b1\u2208\u2206(AI) EUCDT,GT(\u03b1 | \u00b5\u2217, I) .\nAlternatively, we can use the easier-to-check condition that for all info sets I \u2208 I\u2217 with Fr(I | \u00b5\u2217) > 0 and all pure actions a \u2208 AI with \u00b5\u2217(a | I) > 0, we have\na \u2208 argmax a\u2032\u2208AI EUCDT,GT(a\u2032 | \u00b5\u2217, I) . (2)\nNext, we introduce the equilibrium concept that uses Evidential Decision Theory and Generalized Double Halving. Definition 11. Let the player currently be at an info set I with P(I | \u00b5) > 0, and let \u03b1 \u2208 \u2206(AI) be a mixed action. Then, the (EDT,GDH)-expected utility of playing \u03b1 now and according to \u00b5 otherwise is\nEUEDT,GDH(\u03b1 | \u00b5, I) := \u2211 z\u2208Z PGDH( hist(z) | \u00b5I 7\u2192\u03b1, I ) \u00b7 u(z) .\nThe GDH belief probabilities in Definition 11 are welldefined due to P(I | \u00b5) = P(I | \u00b5I 7\u2192\u03b1). Definition 12. We say a strategy \u00b5\u2217 of \u0393 is a (EDT,GDH)equilibrium if for all info sets I \u2208 I\u2217 with P(I | \u00b5\u2217) > 0 under \u00b5\u2217, we have\n\u00b5\u2217(\u00b7 | I) \u2208 argmax \u03b1\u2208\u2206(AI) EUEDT,GDH(\u03b1 | \u00b5\u2217, I) . (3)\nFor (EDT,GDH), it is not sufficient to only check for optimality of pure actions that are in the support of \u00b5\u2217. For instance, take the game in Figure 1 and suppose the player enters the game with the strategy \u00b5 = (C,X). Say, the player observes to be in info set I1. Then action C is optimal among pure actions {L, C, R}. But the player would strictly benefit from deviating to mixed action 12L + 1 2R.\nFinally, observe that in games without absentmindedness, the following notions coincide: CDT and EDT, GT and GDH, and Definitions 9 and 11. In particular, both equilibrium concepts coincide: Lemma 13. In games without absentmindedness, a strategy \u00b5 is a (CDT,GT)-equilibrium if and only if it is an (EDT,GDH)-equilibrium."
        },
        {
            "heading": "3.4 Equilibria from the Ex-Ante Perspective",
            "text": "Recall from Section 2.2 that the (ex-ante) strategy utility function U of \u0393 is a polynomial function from\u00d7\u2113i=1 Rmi to R. In this section, we give characterizations for (CDT,GT)and (EDT,GDH)-equilibria in terms of U , as presented by Oesterheld and Conitzer [2022] and Piccione and Rubinstein [1997]. We reprove these results in the appendix since our setup and end goal differs slightly.\nPolynomial U is continuously differentiable in \u00b5 \u2208 \u00d7\u2113i=1 Rmi . For i \u2208 [\u2113] and j \u2208 [mi], let \u2207ij U stand for the partial derivative in direction (i, j), that is, the linear change of U at a point \u00b5 if you infinitesimally increase its \u00b5(aj | Ii) value. Lemma 14. Let Ii be an info set, aj \u2208 AIi an action, and \u00b5 \u2208\u00d7\u2113i=1 \u2206(AIi) a strategy. Then:\n1. \u2207ij U(\u00b5) = 0 if Fr(Ii | \u00b5) = 0, and 2. \u2207ij U(\u00b5) = Fr(Ii | \u00b5) \u00b7EUCDT,GT(aj | \u00b5, Ii) otherwise. Note that an infinitesimal increase of \u00b5(aj | Ii) means in a game-theoretic sense that the decision for action aj is made slightly more probable at every node of info set Ii. This resembles an EDT type of deviation power but restricted to small deviations that stay close to the current action profile \u00b5. Then, Lemma 14 says that a CDT deviation \u2013 rescaled by Fr(Ii | \u00b5) \u2013 accurately captures the linear (=dominant) effect of such a \u201clocal EDT deviation\u201d.\nLemma 15. Strategy \u00b5 \u2208 \u00d7\u2113i=1 \u2206(AIi) of \u0393 is an (EDT,GDH)-equilibrium if and only if for all i \u2208 [\u2113]:\n\u00b5i\u00b7 \u2208 argmax y\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, y, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nOne possible interpretation of Lemma 15 is that (EDT,GDH)-equilibria of \u0393 are exactly the Nash equilibria of an \u2113-player simultaneous and identical-interest game G: Each player i shall have the continuous action space \u2206(AIi) and the (single) utility payoff, a function of the chosen action profile (\u00b5i\u00b7)\u2113i=1 \u2208\u00d7\u2113i=1 \u2206(AIi), shall be the polynomial U ."
        },
        {
            "heading": "3.5 Computational Considerations",
            "text": "One of our main results addresses the complexity of finding a (CDT,GT)-equilibrium. There are problem instances where all (CDT,GT)-equilibria take on irrational numbers even though the game is easy to encode (Appendix G). Therefore, we relax our search to \u03f5-approximate (CDT,GT)equilibria where \u03f5 is an inverse-exponential numerical precision parameter. Definition 16. An instance of the problem (CDT,GT)EQUILIBRIUM consists of a single-player extensive-form game \u0393 with imperfect recall and a precision parameter \u03f5 > 0 encoded in binary. A solution consists of a strategy \u00b5 for \u0393 that satisfies for all I \u2208 I\u2217 with Fr(I | \u00b5) > 0: EUCDT,GT ( \u00b5(\u00b7 | I) | \u00b5, I ) \u2265 max\na\u2032\u2208AI EUCDT,GT(a\u2032 | \u00b5, I)\u2212 \u03f5 .\nThere is also an alternative notion of being close to an equilibrium, called \u03f5-well-supported (CDT,GT)-equilibrium. It instead requires condition (2) to be satisfied up to \u03f5 precision. An analysis of when both approximation concepts are polynomial-time related can be found in Appendix H.\nWe will give hardness results and restricted membership results for (CDT,GT)-EQUILIBRIUM for the class CLS (Continuous Local Search). CLS was introduced by Daskalakis and Papadimitriou [2011] who noted that it contains various important problems of continuous local optimization that belong both to PPAD and PLS. PPAD [Papadimitriou, 1994] is well-known as the class that captures the complexity of many problems of Nash equilibrium computation ([Daskalakis et al., 2009; Chen et al., 2009] and much subsequent work), while PLS (Polynomial Local Search [Johnson et al., 1988]) represents the complexity of many problems of discrete local optimization. Recently, Fearnley et al. [2023] showed that CLS is equal to the intersection of PPAD and PLS, indicating that CLS-hardness is quite a reliable notion of computational difficulty. In addition, the hardness of CLS can also be based on the cryptographic assumption of indistinguishability obfuscation [Huba\u0301cek and Yogev, 2017]. Fearnley et al. [2023] also showed that a version of the KKT point search problem is CLS-complete. Subsequent work has established CLScompleteness of mixed Nash equilibria of congestion games [Babichenko and Rubinstein, 2021] and solutions to a certain class of contests [Elkind et al., 2022]. We can characterize CLS through any of its complete problems.\nWe will mainly be interested in KKT points. Consider a general non-linear maximization problem\nmax x\u2208Rn\nf(x) s.t. Bx+ b \u2264 0 , Cx+ c = 0 (4)\nwhere f : Rn \u2192 R is continuously differentiable, B \u2208 Rm\u00d7n, b \u2208 Rm, C \u2208 R\u2113\u00d7n, and c \u2208 R\u2113, and the domain is bounded. A point x \u2208 Rn is then said to be a KKT point for (4) if there exist KKT multipliers \u03c41, . . . , \u03c4m, \u03ba1, . . . , \u03ba\u2113 \u2208 R such that Bx + b \u2264 0 and Cx + c = 0, and \u2200j \u2208 [m] : \u03c4j \u2265 0, \u2200j \u2208 [m] : \u03c4j = 0 or Bj\u00b7x+ bj = 0, and\n\u2207 f(x) = m\u2211 j=1 \u03c4j \u00b7 (Bj\u00b7)T + \u2113\u2211 i=1 \u03bai \u00b7 (Ci\u00b7)T = 0 .\nThe KKT conditions are necessary first-order conditions for a point to be a local optimum of (4). Furthermore, feasible stationary points satisfy the KKT conditions."
        },
        {
            "heading": "4 Main Results",
            "text": "To our knowledge, the results of this section are all novel unless explicitly stated otherwise. All proofs can be found in Appendix H."
        },
        {
            "heading": "4.1 Complexity of the Search Problems",
            "text": "First, we use Lemma 14 to give a characterization of (CDT,GT)-equilibria in terms of ex-ante utility. For that, recall the ex-ante maximization problem (1).\nTheorem 1. Strategy \u00b5 \u2208\u00d7\u2113i=1 \u2206(AIi) of \u0393 is a (CDT,GT)equilibrium if and only if \u00b5 is a KKT point of (1).\nWe visualize Theorem 1 in Figure 2. This result also reveals a method to find (CDT,GT)-equilibria, namely by applying Gradient Descent on U . Note that in continuous optimization, there can be KKT points that are not locally optimal. An analogous effect can also happen in games with imperfect recall: In the game of Figure 1, strategy \u00b5 = (C,X) is a (CDT,GT)-equilibrium. But it is not a local optimum because for any \u03f5 > 0 a shift from \u00b5(\u00b7 | I1) = C to \u03f5 \u00b7 ( 12L + 1 2R) + (1 \u2212 \u03f5) \u00b7 C would yield the player ex-ante utility 5 \u00b7 \u03f52 \u00b7 \u03f5 2 > 0. However, from a (CDT,GT) standpoint, the player should be satisfied with her choice at I1. In terms of the original definition of CDT, this is because deviating exactly once in I1 never suffices to attain a utility of 5. In terms of Lemma 14 and Theorem 1, the issue is that the first order effect of increasing the probabilities of R and L is 0.\nThe three solution concepts considered in this paper form an inclusion hierarchy, a result shown by Oesterheld and Conitzer [2022] [cf. Piccione and Rubinstein, 1997]: Lemma 17. An ex-ante optimal strategy of a game \u0393 is also an (EDT,GDH)-equilibrium. An (EDT,GDH)-equilibrium is also a (CDT,GT)-equilibrium.\nIn particular, any single-player extensive-form game \u0393 with imperfect recall admits an (EDT,GDH)-equilibrium and a (CDT,GT)-equilibrium.\nThe implication chain of Lemma 17 does not hold in the reverse direction: Consider the game in Figure 1. Then strategy \u00b5 = (C,X) is a (CDT,GT)-equilibrium, but not an (EDT,GDH)-equilibrium. Moreover, strategy \u00b5\u2032 = (R, Y ) is an (EDT,GDH)-equilibrium with ex-ante utility 1. This is not ex-ante optimal because strategy \u00b5\u2032\u2032 = ( 12L + 1 2R,X) achieves the (optimal) ex-ante utility 5/4.\nThe second part of Lemma 17 holds because ex-ante optimal strategies always exist. This is in contrast to the multiplayer setting where Nash equilibria may not exist in the presence of imperfect recall. Moreover, Lemma 17 implies that finding an ex-ante optimal strategy must be at least as hard as finding an (EDT,GDH)-equilibrium which must be at least as hard as finding a (CDT,GT)-equilibria. For the latter, we get the following classification: Theorem 2. (CDT,GT)-EQUILIBRIUM is CLS-hard. CLShardness holds even for games restricted to: (1.) a tree depth of 6 and the player has 2 actions per info\nset, (2.) no absentmindedness and a tree depth of 6, and (3.) no chance nodes, a tree depth of 5, and only one info set. The problem is in CLS for the subclass of problem instances of (CDT,GT)-EQUILIBRIUM where a lower bound on positive visit frequencies in \u0393 is easily obtainable.\nAll CLS results of Theorem 2 also hold analogously for the search problem of an approximate well-supported (CDT,GT)equilibrium. We prove (1.) by a reduction from finding a KKT point of a polynomial function over the hypercube. For (2.) and (3.), we reduce from finding a Nash equilibrium of a polytensor identical interest game. Both search problems we reduce from were shown to be CLS-complete by Babichenko\nand Rubinstein [2021]. The CLS membership in Theorem 2 implies that, unless NP = co-NP, the considered problem cannot be hard for the class NP [Megiddo and Papadimitriou, 1991]. We only prove CLS membership for those games \u0393 where we can construct a lower bound value \u03bb > 0 that satisfies Fr(I | \u00b5) = 0 or Fr(I | \u00b5) \u2265 \u03bb for all strategies \u00b5 and info sets I in \u0393. That is because if Fr(I | \u00b5) > 0 is too small, the approximation error may explode when transitioning from the ex-ante perspective \u2207ij U(\u00b5) to the de se perspective EUCDT,GT(aj | \u00b5, Ii). Fortunately, such a lower bound exists and is polynomial-time computable for many well-known imperfect-recall games, such as the Absentminded Driver, game variants of the Sleepy Beauty problem, and all the games used in the CLS-hardness results of Theorem 2. Thus, the computation of an approximate (CDT,GT)-equilibrium is CLS-complete in those games that admit such a lower bound on positive visit frequencies. Statement (2.) shows in particular that absentmindedness is not the reason for CLS hardness. With Lemma 13, this implies Corollary 18. In games without absentmindedness where a lower bound on positive visit frequencies is easily obtainable, it is CLS-complete to find an \u03f5-(EDT,GDH)-equilibrium.\nThe authors are not aware of any complexity classification for the problem of finding an approximate (EDT,GDH)equilibrium in games that may have absentmindedness \u2013 even though Lemma 15 gives a nice optimization characterization of (EDT,GDH)-equilibria. Nonetheless, we are able to give conditional inapproximability results for (EDT,GDH)equilibria with the next theorem."
        },
        {
            "heading": "4.2 Complexity of the Decision Problems",
            "text": "Next, we show that maximizing expected utility in an info set or maximizing over the space of equilibria is NP-hard. In the following problem formulations, any target value t \u2208 Q shall be encoded in binary. Theorem 3. The following problems are all NP-hard. Unless NP = ZPP, there is also no FPTAS for these problems. (1a.) Given \u0393 and t \u2208 Q, is there a (CDT,GT)-equilibrium of \u0393 with ex-ante utility \u2265 t? (1b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a (CDT,GT)-equilibrium \u00b5 with Fr(I | \u00b5) > 0, and such that the player has a (CDT,GT)-expected utility \u2265 t upon reaching I? (1c.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a strategy \u00b5 of \u0393 with Fr(I | \u00b5) > 0, and such that the player has a (CDT,GT)-expected utility \u2265 t upon reaching I? (2a.) Given \u0393 and t \u2208 Q, is there an (EDT,GDH)-equilibrium of \u0393 with ex-ante utility \u2265 t? (2b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there an (EDT,GDH)-equilibrium \u00b5 with P(I | \u00b5) > 0, and such that the player has an (EDT,GDH)-expected utility \u2265 t upon reaching I? (2c.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a strategy \u00b5 of \u0393 with P(I | \u00b5) > 0, and such that the player has an (EDT,GDH)-expected utility \u2265 t upon reaching I? (3a.) Given \u0393 and t \u2208 Q, do all (EDT,GDH)-equilibria of \u0393 have ex-ante utility \u2265 t?\n(3b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, do all (EDT,GDH)-equilibria \u00b5 with P(I | \u00b5) > 0 yield the player an (EDT,GDH)-expected utility \u2265 t upon reaching I?\nAll the results of Theorem 3 follow from Proposition 4. Therefore, NP-hardness and conditional inapproximability remain for problems of the form (-a.) even if we restrict the game instances as described in Proposition 4. The same holds for problems of the form (-b.) and (-c.) except that we have to add one information set and one tree depth level to the game instances. Hardness of decision problem (3a.) also relies on the observation that in games with one info set only, any (EDT,GDH)-equilibrium is also ex-ante optimal (cf. Lemma 15).\nFrom (3a.) we obtain in particular that, unless NP = ZPP, there is no FPTAS for the search problem of an (EDT,GDH)equilibrium in games with imperfect recall1. To compare this to Theorem 2, we remark that this conditional inapproximability result for (EDT,GDH)-equilibria (and ex-ante optimal strategies) is obtained even for games where a lower bound on positive visit frequencies is easily obtainable.\nThe decision problems of the form (1-.), (2a.), and (2c.) are all members of the complexity class \u2203R, and therefore, in particular, in PSPACE. On one hand, this is because (CDT,GT)expected utilities and (EDT,GDH)-expected utilities can be described as rational functions (fractions of polynomial functions). Furthermore, this is because the alternative definition (2) of a (CDT,GT)-equilibrium gives rise to polynomially many comparisons of polynomial functions, and, for (2a.), because ex-ante optimal strategies are (EDT,GDH)equilibria."
        },
        {
            "heading": "5 Conclusion",
            "text": "Games of imperfect recall have traditionally often been considered a theoretical curiosity; it is hard to model settings with human actors as imperfect-recall games, because, while most of us frequently forget things, we do not reliably forget things according to well-specified rules. For AI agents, however, this is no longer true; moreover, because they can be instantiated many times, sometimes in simulation, one instantiation will generally not know what another knew earlier. All this motivates the computational study of games of imperfect recall, which we initiated here for the single-player case. We are aided in this endeavor by recent conceptual work that specifies and motivates several natural solution concepts, and we based our work on these. Standard polynomial-time algorithms such as ones based on the sequence form are known to no longer work in the presence of imperfect recall. In this paper we found various complexity-theoretic evidence that indeed, single-player imperfect-recall games are hard to solve. Some of this evidence is, intriguingly, based on the complexity class CLS whose careful study is only very recent. On the positive side, we also provided insights into solving such games by drawing close connections to several problems about maximizing polynomial functions.\n1Note that FPTAS even allow for an approximation up to an inverse-polynomial precision \u03f5.\nThere remain many avenues for future work. What can be said about these computational problems for representation schemes other than the extensive form? Are there special cases of imperfect-recall games that can be solved more efficiently, whether they are single-player or multi-player? One may also ask whether our results give insight into the more conceptual questions. For example, to the extent that (CDT,GT)-equilibria are (under reasonable complexity assumptions) easier to compute than (EDT,GDH)-equilibria, does that provide support for using the former solution concept, at least for certain purposes? We hope that the work we have done in this paper can serve as a springboard for further research into this fascinating and important topic."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to Manolis Zampetakis, Vojte\u030cch Kovar\u030c\u0131\u0301k and the anonymous reviewers for their valuable feedback on this project. Emanuel Tewolde, Caspar Oesterheld and Vincent Conitzer thank the Cooperative AI Foundation, Polaris Ventures (formerly the Center for Emerging Risk Research) and Jaan Tallinn\u2019s donor-advised fund at Founders Pledge for financial support. Paul Goldberg is currently supported by a JP Morgan faculty award."
        },
        {
            "heading": "A On Section 2.2: Reductions from Polynomial Maximization to Games",
            "text": "We consider problems of optimizing polynomials p : \u00d7\u2113i=1 Rmi \u2192 R, where each subset of mi variables are constrained to lie in the standard (mi \u2212 1)-simplex.\nAt the end of Section 2.2, we mention that we can reduce a polynomial p :\u00d7\u2113i=1 Rmi \u2192 R to a single-player extensiveform game \u0393 with imperfect recall such that p = U\u0393 on \u00d7\u2113i=1 Rmi . Let us show this by giving two variants of the same reduction idea.\nA general polynomial function p : \u00d7\u2113i=1 Rmi \u2192 R of degree d can be uniquely represented in terms of the standard monomial basis {\u220f\u2113,mi i,j=1 x Dij ij } D\u2208MB(d,m). Here, we summarized (mi)\u2113i=1 to the vector m, and we denote with MB(d,m) the set\n{D = (Dij)ij \u2208 \u2113\n\u00d7 i=1 Nmi0 : \u2113,mi\u2211 i,j=1 Dij \u2264 d}\nof all variations to draw up to d \u2208 N elements out of the set {(i, j)}\u2113,mii,j=1, with replacement and without regard to draw order. Each element D \u2208 MB(d,m) captures the variable degrees of the monomial basis element \u220f\u2113,mi i,j=1 x Dij ij that it represents. Throughout this paper (specifically, this appendix), if the instance to a problem contains a polynomial function p : \u00d7\u2113i=1 Rmi \u2192 R, then we assume it to be represented as a binary encoding of \u2113, (mi)\u2113i=1, and the polynomials coefficients (\u03bbD)D\u2208MB(d,m), which need to be rational. This is also called the Turing (bit) model. Denote supp(p) := {D \u2208 MB(d,m) : \u03bbD \u0338= 0}) and, for each D \u2208 MB(d,m), supp(D) := {(i, j) with i \u2208 [\u2113], j \u2208 [mi] : Dij > 0}) as well as |D| := \u2211\u2113,mi i,j=1Dij . Let supp(D)\nms be the multiset that contains Dij many copies of the element (i, j) in it for each (i, j) \u2208 supp(D) (in multisets, duplicate elements are allowed). Then | supp(D)ms| = |D|.\nGiven such a polynomial function, let us construct a corresponding single-player extensive-form game \u0393 with imperfect recall. It shall have info sets Ii for i \u2208 [\u2113] with action sets AIi := {a1, . . . , ami}, and a tree depth of up to d+1. Let the root h0 be a chance node that has one outgoing edge to depth 1-node hD for each monomial index D \u2208 supp(p). An outgoing edge is drawn uniformly at random. First, handle the special case of D being the zero vector. There, hD will be a terminal node with a utility payoff of | supp(p)|. So consider D \u0338= 0 from now on, where hD will be a nonterminal node. Denote the subtree rooted at hD with TD. Keep in mind that it is associated with monomial \u220f\u2113,mi i,j=1 x Dij ij . We will now build TD depth layer by depth layer, until a tree of depth |D|, by lexicographically going through the set supp(D)ms. Depth k \u2212 1 of TD corresponds to the k-th element of supp(D)ms, referred to as ( i(k), j(k) ) . We present two variants with how to continue building \u0393: Variant 1: There will be at most one nonterminal node h of depth k \u2212 1 of TD (it would be hD on depth 0). Assign h to info set Ii(k). Create mi(k) outgoing edges out\nof h into a new node of depth k, and the edges shall be labeled with {a1, . . . , ami(k)} of AIi(k) . The created node h \u25e6 aj(k) shall be non-terminal and the created nodes h \u25e6 aj for j \u0338= j(k) shall be terminal with utility payoff 0. With this procedure, subtree TD will have depth \u2211 (i,j)\u2208supp(D)Dij . The nonterminal node of the last depth layer has action history ( aj(k) \u2208 AIi(k) ) k\u2208[|D|] in TD. Now reverse the fact that it is nonterminal, and make it terminal instead. Denote it as zD and assign it a utility of \u03bbD \u00b7 | supp(p)|.\nVariant 2: This variant does not have terminal nodes in depth layers 0, . . . , |D| \u2212 1. Each node h of depth k \u2212 1 of TD shall belong to info set Ii(k) (Depth 0 only has one node, namely hD). Create mi(k) outgoing edges out of each h into a new node of depth k. The edges shall be labeled with {a1, . . . , ami(k)} of \u2208 AIi(k) . The nodes of the last depth layer (depth |D|) shall be terminal nodes. One of those nodes has the action history ( aj(k) \u2208 AIi(k) ) k\u2208[|D|] in TD, which we will refer to as zD. Assign it a utility of \u03bbD \u00b7 | supp(p)|. Assign all other terminal nodes of TD a utility of 0.\nIn both variants, we have that any point x \u2208\u00d7\u2113i=1 Rmi of p that is also in\u00d7\u2113i=1 \u2206mi\u22121 naturally comprises a strategy \u00b5 in \u0393 with probabilities \u00b5(aj | Ii) = xij . Moreover, the strategy utility function U of both variants of \u0393 satisfies\nU(\u00b5) = \u2211 z\u2208Z P(z | \u00b5) \u00b7 u(z) =\n= \u2211\nD\u2208supp(p)\nP(zD | \u00b5) \u00b7 \u03bbD \u00b7 | supp(p)|\n= \u2211\nD\u2208supp(p)\n[( 1 | supp(p)| \u00b7 \u220f\nDij>0\nx Dij ij ) \u00b7 \u03bbD \u00b7 | supp(p)| ] =\n\u2211 D\u2208MB(d,m) \u03bbD \u00b7 \u220f i,j x Dij ij\n= p(x)\nfor corresponding x and \u00b5. This extends to U = p on all \u00d7\u2113i=1 Rmi .\nThe construction of the first variant of \u0393 takes polynomial time in the encoding size of p :\u00d7\u2113i=1 Rmi \u2192 R. That is, because the game tree has size \u2264 | supp(p)|\u00b7deg(p)\u00b7(maximi). The second variant of \u0393 can have exponential game tree size in general. But, if we know that polynomial instances p have fixed degree d for example, then the tree size is \u2264 | supp(p)| \u00b7 (maximi)d which makes the whole construction of \u0393 polynomial time in the size of the input instances again. An advantage of the second variant is that in that game, the reach probability P(I | \u00b5) and visit frequency Fr(I | \u00b5) of an info set I \u2208 I are independent of the used strategy \u00b5. That is because a subtree TD is chosen by nature at the beginning, and within TD, any outcome path visits the exact same info sets with the same multiplicity. The reach probability of info set Ii becomes\nP(Ii) = P(Ii | \u00b5) = \u2211\nD\u2208supp(p)\n1\n| supp(p)| \u00b7\u03c7( \u2211 j\u2208[mi] Dij \u2265 1)\nand its visit frequency becomes\nFr(Ii) = Fr(Ii | \u00b5) = \u2211\nD\u2208supp(p)\n1 | supp(p)| \u00b7 \u2211\nj\u2208[mi]\nDij .\n(5)\nWe will make use of this observation later when it comes to games that admit an easy-to-compute lower bound on positive visit frequencies."
        },
        {
            "heading": "B On Section 2.3: Proof of Proposition 4",
            "text": "We recall and prove Proposition 4:\nProposition. Consider the problem that takes a game \u0393 and target value t \u2208 Q (encoded in binary) as inputs and asks whether there is a strategy \u00b5 for \u0393 with ex-ante expected utility U(\u00b5) \u2265 t. This problem is NP-hard. Moreover: (1.) Unless NP = ZPP, there is no FPTAS for this prob-\nlem. NP-hardness and conditional inapproximability hold even if the game instance \u0393 has a tree depth of 3 and only one info set. (2.) NP-hardness holds even if \u0393 has no absentmindedness, a tree depth of 4 and the player has 2 actions per info set. (3.) NP-hardness holds even if \u0393 has no absentmindedness, a tree depth of 3 and the player has 3 actions per info set.\nProof. (1.) Appendix A gives a reduction from maximizing polynomial p :\u00d7\u2113i=1 Rmi \u2192 R over the product of simplices to maximizing ex-ante utility in a single-player extensive-form game \u0393 with imperfect recall (take the first variant of the reduction). de Klerk [2008] gives a survey on maximizing polynomials over popular compact domains. Consider the decision problem that takes as an instance a polynomial function p : \u2206m\u22121 \u2192 R and a target value t \u2208 Q, and that has to decide whether there exists a point x \u2208 \u2206m\u22121 with p(x) \u2265 t. de Klerk [2008] note that this problem is NP-hard and has no FPTAS unless NP=ZPP, even if p is known to be quadratic functions only. They derive the conditional inapproximability from Ha\u030astad [1996]. In terms of our reduction, such polynomials reduce to a game of depth 3 and one info set. Moreover, a instance of the polynomial problem is a yes instance if and only if the reduced instance of ex-ante utility game problem of this proposition is a yes instance.\n(2.) Reduce from 3SAT. Let x1, . . . , x\u2113 be the variables of a 3CNF formula \u03d5 having n clauses. We construct the corresponding game instance \u0393 as follows. Each variable xi has corresponding info set Ii whose nodes have 2 outgoing edges with action labels T, F . The root of the tree is a chance node that selects amongst n subtrees, corresponding to the clauses, with equal probability 1/n. Each subtree is a binary tree of depth 3. If the clause C associated with the subtree contains variables xi, xj , xk, then the root node of the subtree shall belong to info set Ii, its two children belong to info set Ij , and their four children belong to info set Ik. Each leaf has a path that is associated by truth assignments of xi, xj , xj . A leaf shall\nyield a utility of 1 if the associated truth assignment satisfies the clause C, else value 0. The utility target value for the game instance shall be 1.\nThen, \u03d5 is satisfiable if and only if there is a strategy in the corresponding game with ex-ante utility 1. Note that every clause subtree is reached with positive probability and that an ex-ante utility of 1 means that any realization of the candidate strategy must only reach terminal node with utility payoff 1. For the backward implication, say \u00b5 is a behavioural (=randomizing) strategy with an ex-ante utility of 1. Take any pure-action strategy \u00b5\u2032 that only uses actions that are in the support of \u00b5. Then \u00b5\u2032 makes a satisfying truth assignment for \u03d5.\n(3.) Consider the following type of 2-player games G: A game G consists of a family (Gs)s\u2208S of 2-player simultaneous games where |S| \u2208 N is finite. There is a probability distribution over S determining which game the two players will play. There is a partition S = \u2294I\u2208I\u2217I for player 1 and S = \u2294J\u2208J\u2217J for player 2 representing the info sets of each player within which the respective player cannot differentiate. For any I \u2208 I\u2217, games Gs with s \u2208 I have the same action set AI for player 1. Analogously, for any J \u2208 J\u2217, games Gs with s \u2208 J have the same action set AJ for player 2. First, nature draws s \u2208 S according to the probability distribution, then each player simultaneously takes an action in Gs. Such an outcome (s, a \u2208 AIs , a\u2032 \u2208 AJs) yields each player the same payoff u(s, a, a\u2032). A strategy for player 1 (resp. player 2) returns a mixed action \u03b1 \u2208 \u2206(AI) to each set I \u2208 I\u2217 (resp. an \u03b1 \u2208 \u2206(AJ) to each set J \u2208 J\u2217.\nChu and Halpern [2001] show that given a game G as described above, it is NP-hard to decide whether there is a strategy profile (\u00b51, \u00b52) that yields a payoff \u2265 1 in G. After a closer inspection of their reduction from 3SAT, we can see that their NP-hardness result holds even for game instances where each player only has up to three actions in eachGs and where all the payoffs are as simple as 0 or 3. They also show that such a gameG can be transformed into an extensive-form game in polynomial time. We will use almost the same transformation in our upcoming reduction from their problem to our problem of interest.\nTake an instance G as described above. Assume further that G has up to three actions in each Gs. Create a singleplayer extensive-form game \u0393 with imperfect recall in the following way: \u0393 has depth 3 and information sets I\u2217 \u222a J\u2217. The root of \u0393 is a chance node that chooses a subtree Ts associated with s \u2208 S according to the given probability distribution over S. The root of a subtree Ts shall be assigned to info set Is \u2208 I\u2217 and it shall have AIs outgoing edges. Each node of subtree Ts of subtree depth level 1 shall be assigned to info set Js \u2208 J\u2217 and they shall have AJs outgoing edges. Each node of the whole game \u0393 of depth level 3 shall be a terminal node, and if its associated action history is (s, a \u2208 AIs , a\u2032 \u2208 AJs), then it shall yield the single player a payoff of u(s, a, a\u2032). Also, let the target value t for \u0393 be 1. A strategy (\u00b51, \u00b52) of G shall correspond to the strategy \u00b5 of \u0393 that returns \u00b51(I) \u2208 \u2206(AI) for info sets I \u2208 I\u2217 \u2282 I\u2217 \u222a J\u2217 and returns \u00b51(J) \u2208 \u2206(AJ) for info sets J \u2208 J\u2217 \u2282 I\u2217 \u222a J\u2217\nThen, there is a strategy profile (\u00b51, \u00b52) of G that yields\na payoff \u2265 1 if and only if its corresponding strategy \u00b5 in \u0393 yields an ex-ante utility \u2265 t.\nNote that in this reduction, \u0393 has no absentmindedness, a tree of depth 3, and at most three actions for each player info set."
        },
        {
            "heading": "C Popular Game Examples of Imperfect Recall",
            "text": "In Figure 3, we describe the Absentminded Driver game. In Figure 4, we describe the Sleeping Beauty problem, which is less of a decision problem, but rather a probability puzzle."
        },
        {
            "heading": "D On Section 3.2: Comparing Generalized Thirding and Generalized Double Halving",
            "text": "This section stays close to the exposition of Oesterheld and Conitzer [2022].\nLet \u0393 be a single-player extensive-form game with imperfect recall, \u00b5 be the strategy with which the player entered the game, and let the player find herself at an info set I \u2208 I\u2217 with P(I | \u00b5) > 0 \u21d0\u21d2 Fr(I | \u00b5) > 0.\nRecall that then, GT determines the probability of being at player node h \u2208 H\u2217 specifically as\nPGT(h | \u00b5, I) = \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) Fr(I | \u00b5) .\nThe number of times info set I occurs in the history of a terminal node z can be described with |I \u2229 hist(z)|. Let \u201cat h in hist(z)\u201d be the event that the player is currently at node h and on the root-to-end path hist(z) for some given terminal node z. Then we get the following GT probabilities for being in\nthe history hist(z) of a specified terminal node z \u2208 Z: PGT( hist(z) | \u00b5, I ) = \u2211 h\u2208H\u2217 PGT( ath in hist(z) | \u00b5, I )\n:= \u2211\nh\u2208H\u2217\u2229hist(z)\nPGT(h | \u00b5, I ) \u00b7 P(z | \u00b5, h)\n= \u2211\nh\u2208I\u2229hist(z)\nP(h | \u00b5) Fr(I | \u00b5) \u00b7 P(z | \u00b5, h)\n= |I \u2229 hist(z)| \u00b7 P(z | \u00b5) Fr(I | \u00b5)\nCompare this to the definition of GDH:\nPGDH( hist(z) | \u00b5, I ) = \u03c7(I \u2229 hist(z) \u0338= \u2205) \u00b7 P(z | \u00b5) P(I | \u00b5)\nSo GT assigns a history hist(z) of a terminal node z a higher probability if the reach probability of z under \u00b5 is higher (same as GDH) and if hist(z) visits info set I very often (GDH only cares about I being visited at least once).\nAs we can see, GT assigns being in the history of a terminal node z a higher probability if the reach probability P(z | \u00b5) under \u00b5 is higher and/or if the history hist(z) enters info set I very often.\nWhen it comes to events of the form \u201cat h in hist(z)\u201d, then GDH will just uniformly distribute the probability of being in hist(z) among all those nodes in hist(z) that are also in I . With this, GDH can also assign probabilities to the event of\nbeing at a specified node h \u2208 H\u2217, namely, as\nPGDH(h | \u00b5, I ) = \u2211 z\u2208Z PGDH( ath in hist(z) | \u00b5, I )\n= \u2211 z\u2208Z\nh\u2208I\u2229hist(z)\nPGDH( ath in hist(z) | \u00b5, I )\n:= \u2211 z\u2208Z\nh\u2208I\u2229hist(z)\nPGDH( hist(z) | \u00b5, I ) |I \u2229 hist(z)|\n= \u03c7(h \u2208 I) \u00b7 \u2211 z\u2208Z\nh\u2208hist(z)\nP(z | \u00b5) P(I | \u00b5) \u00b7 1 |I \u2229 hist(z)|\n= \u03c7(h \u2208 I) \u00b7 \u2211 z\u2208Z\nh\u2208hist(z)\nP(h | \u00b5) \u00b7 P(z | \u00b5, h) P(I | \u00b5) \u00b7 |I \u2229 hist(z)|\n= \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) P(I | \u00b5) \u00b7 \u2211 z\u2208Z\nh\u2208hist(z)\nP(z | \u00b5, h) |I \u2229 hist(z)|"
        },
        {
            "heading": "E On Section 3.3: Proofs",
            "text": "This section stays close to the ideas of Oesterheld and Conitzer [2022].\nMixed vs Pure Action Deviations in (CDT,GT). We have\nEUCDT,GT(\u03b1 | \u00b5, I) = \u2211 h\u2208I PGT(h | \u00b5, I) \u00b7 ( \u2211 a\u2208AI \u03b1(a) \u00b7 U(\u00b5 | h \u25e6 a) )\n= \u2211 a\u2208AI \u03b1(a) \u00b7 (\u2211 h\u2208I PGT(h | \u00b5, I) \u00b7 U(\u00b5 | h \u25e6 a) )\n= \u2211 a\u2208AI \u03b1(a) \u00b7 EUCDT,GT(a | \u00b5, I) .\n(6)\nTherefore, \u03b1\u2217 \u2208 argmax\u03b1\u2208\u2206(AI) EUCDT,GT(\u03b1 | \u00b5, I) if and only if it mixes only over optimal pure actions, i.e., \u03b1\u2217(a\u2217) > 0 =\u21d2 a\u2217 \u2208 argmaxa\u2208AI EUCDT,GT(a | \u00b5, I).\nEDT Deviation does not affect Reach Probabilities. Observe that a node h \u2208 I1st in \u0393 satisfies P(h | \u00b5) = P(h | \u00b5I 7\u2192\u03b1) because info set I - in which \u00b5 and \u00b5I 7\u2192\u03b1 differ - never appeared in the history of h. Therefore,\nP(I | \u00b5) = \u2211 h\u2208I1st P(h | \u00b5) = \u2211 h\u2208I1st P(h | \u00b5I 7\u2192\u03b1)\n= P(I | \u00b5I 7\u2192\u03b1) . (7)\nProof of Lemma 13: Without absentmindedness, (CDT,GT) equals (EDT,GDH). First, we shall show two facts that hold in games with imperfect recall (with or without absentmindedness). For any strategy \u00b5\u2032 of \u0393, node h \u2208 H\\Z ,\nand terminal node z \u2208 Z , we have\nP(z | \u00b5\u2032, h)\n= \u03c7(h \u2208 hist(z)) \u00b7 d(z)\u22121\u220f k=d(h) \u00b5\u2032 ( a(z, k) | I\u03bd(z,k) ) = \u03c7(h \u2208 hist(z)) \u00b7 \u00b5\u2032 ( a(z, d(h)) | I\u03bd(z,d(h)) ) \u00b7\nd(z)\u22121\u220f k=d(h)+1 \u00b5\u2032 ( a(z, k) | I\u03bd(z,k) ) =\n\u2211 a\u2032\u2208AIh \u03c7(h \u2208 hist(z)) \u00b7 \u03c7 ( a\u2032 = a(z, d(h)) ) \u00b7 \u00b5\u2032 ( a\u2032 | Ih ) \u00b7\nd(z)\u22121\u220f k=d(h)+1 \u00b5\u2032 ( a(z, k) | I\u03bd(z,k) ) =\n\u2211 a\u2032\u2208AIh \u00b5\u2032 ( a\u2032 | Ih ) \u00b7 \u03c7(h \u25e6 a\u2032 \u2208 hist(z))\u00b7\nd(z)\u22121\u220f k=d(h\u25e6a) \u00b5\u2032 ( a(z, k) | I\u03bd(z,k) ) =\n\u2211 a\u2032\u2208AIh \u00b5\u2032 ( a\u2032 | Ih ) \u00b7 P(z | \u00b5\u2032, h \u25e6 a\u2032) .\nMoreover, we can obtain\nU(\u00b5\u2032 | h) = \u2211 z\u2208Z P(z | \u00b5\u2032, h) \u00b7 u(z)\n= \u2211 z\u2208Z \u2211 a\u2032\u2208AIh \u00b5\u2032 ( a\u2032 | Ih ) \u00b7 P(z | \u00b5\u2032, h \u25e6 a\u2032) \u00b7 u(z)\n= \u2211\na\u2032\u2208AIh\n\u00b5\u2032 ( a\u2032 | Ih ) \u00b7 \u2211 z\u2208Z P(z | \u00b5\u2032, h \u25e6 a\u2032) \u00b7 u(z)\n= \u2211\na\u2032\u2208AIh\n\u00b5\u2032 ( a\u2032 | Ih ) \u00b7 U(\u00b5\u2032 | h \u25e6 a\u2032) .\nNow let \u0393 be a single-player extensive-form game with imperfect recall and without absentmindedness. Let \u00b5 be the strategy with which the player entered the game, and let the player find herself at info set I \u2208 I\u2217 with P(I | \u00b5) > 0 \u21d0\u21d2 Fr(I | \u00b5) > 0.\nCDT and EDT address how a players choices at the current node affect the players choice at other nodes of the same path and of the same info set I . Without absentmindedness, this consideration becomes obsolete because for any root-to-end path in \u0393, the player can arrive in I at most once on that path. Thus, EDT deviations and CDT deviations have the same effect.\nMoreover, that lack of absentmindedness means I1st = I . Hence, P(I | \u00b5) = Fr(I | \u00b5) and \u03c7(I \u2229 hist(z) \u0338= \u2205) = |I \u2229 hist(z)| for any terminal node z \u2208 Z . Therefore, by\nAppendix D, we have for any terminal node z:\nPGT( hist(z) | \u00b5, I ) = |I \u2229 hist(z)| \u00b7 P(z | \u00b5) Fr(I | \u00b5)\n= \u03c7(I \u2229 hist(z) \u0338= \u2205) \u00b7 P(z | \u00b5) P(I | \u00b5)\n= PGDH( hist(z) | \u00b5, I )\nand for any node h \u2208 H\u2217:\nPGDH(h | \u00b5, I )\n= \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) P(I | \u00b5) \u00b7 \u2211 z\u2208Z\nh\u2208hist(z)\nP(z | \u00b5, h) |I \u2229 hist(z)|\n= \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) Fr(I | \u00b5) \u00b7 \u2211 z\u2208Z\nh\u2208hist(z)\nP(z | \u00b5, h)\n= \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) Fr(I | \u00b5) \u00b7 1\n= PGT(h | \u00b5, I )\nFinally, we show that (CDT,GT) equals (EDT,GDH) compute the same expected utilities from a deviation to a mixed action \u03b1 \u2208 \u2206(AI). Since I1st = I , we have P(h | \u00b5) = P(h | \u00b5I 7\u2192\u03b1) for all h \u2208 I . With the general facts showed in the beginning, we can derive for games without absentmindedness:\nEUCDT,GT(\u03b1 | \u00b5, I) = \u2211 h\u2208I PGT(h | \u00b5, I) \u00b7 ( \u2211 a\u2208AI \u03b1(a) \u00b7 U(\u00b5 | h \u25e6 a) )\n= \u2211 h\u2208I \u03c7(h \u2208 I) \u00b7 P(h | \u00b5) Fr(I | \u00b5) \u00b7 \u2211 a\u2208AI \u03b1(a) \u00b7 U(\u00b5 | h \u25e6 a)\n= \u2211 h\u2208I 1 \u00b7 P(h | \u00b5) P(I | \u00b5) \u00b7 \u2211 a\u2208AI \u00b5I 7\u2192\u03b1(a | I) \u00b7 U(\u00b5I 7\u2192\u03b1 | h \u25e6 a)\n= \u2211 h\u2208I P(h | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1) \u00b7 U(\u00b5I 7\u2192\u03b1 | h)\n= \u2211 h\u2208I P(h | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1)\n\u00b7\u2211 z\u2208Z P(z | \u00b5I 7\u2192\u03b1, h) \u00b7 \u03c7(h \u2208 hist(z)) \u00b7 u(z)\n= \u2211 z\u2208Z u(z) \u00b7 \u2211 h\u2208I\u2229hist(z) P(h | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1) \u00b7 P(z | \u00b5I 7\u2192\u03b1, h)\n= \u2211 z\u2208Z u(z) \u00b7 \u2211 h\u2208I\u2229hist(z) P(z | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1)\n= \u2211 z\u2208Z u(z) \u00b7 |I \u2229 hist(z)| \u00b7 P(z | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1)\n= \u2211 z\u2208Z u(z) \u00b7 \u03c7(I \u2229 hist(z) \u0338= \u2205) \u00b7 P(z | \u00b5I 7\u2192\u03b1) P(I | \u00b5I 7\u2192\u03b1)\n= \u2211 z\u2208Z u(z) \u00b7 PGDH( hist(z) | \u00b5I 7\u2192\u03b1, I )\n= EUEDT,GDH(\u03b1 | \u00b5, I)\nAs a consequence, we get Lemma 13:\nLemma. In games without absentmindedness, a strategy \u00b5 is a (CDT,GT)-equilibrium if and only if it is an (EDT,GDH)equilibrium."
        },
        {
            "heading": "F On Section 3.4: Proofs",
            "text": "The proofs of this section stay conceptually close to Piccione and Rubinstein [1997] and Oesterheld and Conitzer [2022].\nF.1 Proof of Lemma 14 We identified the strategy space\u00d7\u2113i=1 \u2206(AIi) of a game \u0393 with \u00d7\u2113i=1 \u2206mi\u22121 and established that the strategy utility function U extends to all\u00d7\u2113i=1 Rmi . In order to discuss differentiabity, we view elements \u00b5 \u2208 \u00d7\u2113i=1 Rmi as flattened vectors in R \u2211\u2113 i=1 mi . However, for notational convenience, we keep the vector description \u00b5 = (\u00b5ij) \u2113,mi i,j=1 \u2208\u00d7\u2113i=1 Rmi because it is indexed by a (info set, action)-pair for our gametheoretic perspective. Therefore, the basis vectors that span \u00d7\u2113i=1 Rmi are the vectors eij , for i \u2208 [\u2113] and j \u2208 [mi], with (i\u2032, j\u2032) entries\n(eij)i\u2032j\u2032 := { 1 if i\u2032 = i and j\u2032 = j 0 otherwise.\nThen, polynomial U has one partial derivative \u2207ij U for each coordinate direction eij which is defined as\n\u2207ij U(\u00b5) := lim \u03f5\u21920\n1 \u03f5 \u00b7 ( U(\u00b5+ \u03f5 \u00b7 eij)\u2212 U(\u00b5) ) . (8)\nWe are ready to prove Lemma 14.\nLemma. Let Ii be an info set, aj \u2208 AIi an action, and \u00b5 \u2208 \u00d7\u2113i=1 \u2206(AIi) a strategy of the game. Then:\n1. \u2207ij U(\u00b5) = 0 if Fr(Ii | \u00b5) = 0, and 2. \u2207ij U(\u00b5) = Fr(Ii | \u00b5) \u00b7EUCDT,GT(aj | \u00b5, Ii) otherwise.\nProof. Using (8), let us first get an expression for U(\u00b5 + \u03f5 \u00b7 eij). By Section 2.2, we have\nU(\u00b5+ \u03f5 \u00b7 eij) = \u2211 z\u2208Z u(z)\u00b7 (9)\nd(z)\u22121\u220f k=0 ( \u00b5 ( a(z, k) | I\u03bd(z,k) ) + \u03f5 \u00b7 eij ( a(z, k) | I\u03bd(z,k) )) where we use eij ( a(z, k) | I\u03bd(z,k) ) to indicate the entry of coordinate direction eij at the info set index of I\u03bd(z,k) and the\naction index of a(z, k). We continue the equation chain of (9) by sorting the product of sums by their order in \u03f5:\n= \u2211 z\u2208Z\n( u(z)\nd(z)\u22121\u220f k=0 \u00b5 ( a(z, k) | I\u03bd(z,k)\n)) +\n\u2211 z\u2208Z\n( u(z)\n\u2211 h\u2208hist(z) [ \u03f5 \u00b7 eij ( a(z, d(h)) | Ih ) \u00b7\nd(z)\u22121\u220f k=0 k \u0338=d(h) \u00b5 ( a(z, k) | I\u03bd(z,k)\n)]) +O(\u03f52)\n= U(\u00b5) + \u03f5 \u00b7 \u2211 z\u2208Z [ u(z) \u00b7 \u2211 h\u2208hist(z) eij ( a(z, d(h)) | Ih ) \u00b7\nP(h | \u00b5) \u00b7 P ( z | \u00b5, h \u25e6 a(z, d(h)) )] +O(\u03f52)\n(\u2217) = U(\u00b5) +O(\u03f52)+ \u03f5 \u00b7 \u2211 z\u2208Z \u2211 h\u2208hist(z)\u2229Ii \u03c7 ( aj \u0338= a(z, d(h)) ) \u00b7 u(z)\u00b7\nP(h | \u00b5) \u00b7 P ( z | \u00b5, h \u25e6 a(z, d(h)) ) (\u2217\u2217) = U(\u00b5) +O(\u03f52)+\n\u03f5 \u00b7 \u2211 z\u2208Z \u2211 h\u2208Ii u(z) \u00b7 P(h | \u00b5) \u00b7 P ( z | \u00b5, h \u25e6 aj ) \ufe38 \ufe37\ufe37 \ufe38\nDenote this term as (\u2020)\nIn equation line (\u2217), we use that eij is zero in any info set \u0338= Ii or any action \u0338= aj \u2208 AIi . In equation line (\u2217), we use that P ( z | \u00b5, h\u25e6aj) is zero if h \u0338= hist(z) or aj \u0338= a(z, d(h)).\nTerm (\u2020) is constant in \u03f5. Once we derived a better expression for (\u2020), we can obtain the statement of the lemma from\n\u2207ij U(\u00b5) = lim \u03f5\u21920\n1\n\u03f5\n( U(\u00b5+ \u03f5 \u00b7 eij)\u2212 U(\u00b5) ) = lim\n\u03f5\u21920\n1\n\u03f5\n( U(\u00b5) + \u03f5 \u00b7 (\u2020) +O(\u03f52)\u2212 U(\u00b5) ) = (\u2020) .\nConsider the case where 0 = Fr(Ii | \u00b5) = \u2211\nh\u2208Ii P(h | \u00b5). Recall that \u00b5 \u2208\u00d7\u2113i=1 \u2206mi\u22121. Therefore, all reach probabilities are non-negative. In particular, we obtain P(h | \u00b5) = 0 for all h \u2208 Ii. Hence, (\u2020) = 0.\nConsider the other case, namely, where Fr(Ii | \u00b5) > 0.\nThen, we can simplify: (\u2020) = \u2211 h\u2208Ii \u2211 z\u2208Z u(z) \u00b7 P(h | \u00b5) \u00b7 P ( z | \u00b5, h \u25e6 aj) ) =\nFr(Ii | \u00b5) Fr(Ii | \u00b5) \u00b7 \u2211 h\u2208Ii P(h | \u00b5) \u00b7 \u2211 z\u2208Z u(z) \u00b7 P ( z | \u00b5, h \u25e6 aj) ) = Fr(Ii | \u00b5) \u00b7\n\u2211 h\u2208Ii \u03c7(h \u2208 Ii) \u00b7 P(h | \u00b5) Fr(Ii | \u00b5) \u00b7 U(\u00b5 | h \u25e6 aj)\n= Fr(Ii | \u00b5) \u00b7 \u2211 h\u2208Ii PGT(h | \u00b5, Ii) \u00b7 U(\u00b5 | h \u25e6 aj)\n= Fr(Ii | \u00b5) \u00b7 EUCDT,GT(aj | \u00b5, Ii)\nF.2 Proof of Lemma 15\nLemma. Strategy \u00b5 \u2208 \u00d7\u2113i=1 \u2206(AIi) of a game \u0393 is an (EDT,GDH)-equilibrium if and only if for all i \u2208 [\u2113]:\n\u00b5i\u00b7 \u2208 argmax y\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, y, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nProof. We start with the definition (EDT,GDH)-expected utilities (Definition 11). Say, the player entered the game with strategy \u00b5 \u2208\u00d7\u2113i=1 \u2206(AIi), and arrived at an info set Ii with P(Ii | \u00b5) > 0. Let \u03b1 \u2208 \u2206(AIi). Then\nEUEDT,GDH(\u03b1 | \u00b5, Ii) = \u2211 z\u2208Z PGDH( hist(z) | \u00b5Ii 7\u2192\u03b1, Ii ) \u00b7 u(z)\n= \u2211 z\u2208Z \u03c7(I \u2229 hist(z) \u0338= \u2205) \u00b7 P(z | \u00b5Ii 7\u2192\u03b1) P(Ii | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\n= 1\nP(Ii | \u00b5Ii 7\u2192\u03b1) \u00b7\n\u2211 z\u2208Z\nIi\u2229hist(z)\u0338=\u2205\nP(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z) (10)\nContinue with the definition of an (EDT,GDH)-equilibrium (Definition 12). When taking the argmax of (10) over \u03b1 \u2208 \u2206(AIi), we can rescale (10) by strictly positive factors and add terms to it without changing the solution set to the argmax (as long as the factors and terms are independent of \u03b1). First, multiply (10) by \u03b1-independent factor\nP(Ii | \u00b5Ii 7\u2192\u03b1) (7) = P(Ii | \u00b5) > 0. Second, observe that for any z \u2208 Z for which info set I does not occur in hist(z), we have P(z | \u00b5Ii 7\u2192\u03b1) = P(z | \u00b5). Hence, we can add \u03b1-independent term\u2211\nz\u2208Z (I not in hist(z))\nP(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\nto (10) afterwards. These two steps yield\nargmax \u03b1\u2208\u2206(AIi )\nEUEDT,GDH(\u03b1 | \u00b5, Ii)\n(10) = argmax\n\u03b1\u2208\u2206(AIi )\n1\nP(Ii | \u00b5Ii 7\u2192\u03b1) \u00b7 \u2211 z\u2208Z\nI\u2229hist(z)\u0338=\u2205\nP(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\n= argmax \u03b1\u2208\u2206(AIi ) \u2211 z\u2208Z\nI\u2229hist(z) \u0338=\u2205\nP(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\n= argmax \u03b1\u2208\u2206(AIi ) \u2211 z\u2208Z P(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z) = argmax \u03b1\u2208\u2206(AIi ) U(\u00b5Ii 7\u2192\u03b1)\n= argmax \u03b1\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nwhere we use that strategy \u00b5Ii 7\u2192\u03b1 has vector description (\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7).\nAll in all, we obtain that \u00b5 is an (EDT,GDH)-equilibrium\n\u21d0\u21d2 \u2200i \u2208 [\u2113] with P(Ii | \u00b5) > 0 : \u00b5(\u00b7 | Ii) \u2208 argmax\n\u03b1\u2208\u2206(AIi ) EUEDT,GDH(\u03b1 | \u00b5, Ii)\n\u21d0\u21d2 \u2200i \u2208 [\u2113] with P(Ii | \u00b5) > 0 : \u00b5i\u00b7 \u2208 argmax\n\u03b1\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7)\n(\u2217)\u21d0\u21d2 \u2200i \u2208 [\u2113] : \u00b5i\u00b7 \u2208 argmax\n\u03b1\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7)\nThe last equivalence is based on the following argument: If info set Ii has reach probability P(Ii | \u00b5) = 0, then function U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) is constant in the action choice \u03b1 at info set Ii. Therefore, any \u03b1\u2032 \u2208 \u2206(AIi) satisfies\n\u03b1\u2032 \u2208 argmax \u03b1\u2208\u2206(AIi ) U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, \u03b1, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nThis is why we can add or remove this generally true statement as a condition requirement."
        },
        {
            "heading": "G Solutions to Simple Games can be Irrational",
            "text": "We here show that the solutions (ex ante optimal policy, and (EDT,GDH)- and (CDT,GT)-equilibria) to simple games can be irrational. In fact, we will show that they are sometimes are not even expressible in radicals.\nAs a starting point, consider the polynomial equation\nx5 \u2212 x\u2212 1 = 0.\nThis equation has a unique real-valued solution x\u2217 \u2248 1.1673 that cannot be expressed in radicals, i.e., that cannot be expressed using sums, products, divisions and roots [Lang, 1994][P.121]. In particular, it is irrational.\nFrom the above polynomial equation we will construct a polynomial whose unique KKT point on [0, 1] is x\u2217/2. First, note that the equation 32x5 \u2212 2x\u2212 1 = (2x)5 \u2212 2x\u2212 1 = 0, obtained by substituting 2x for x in the above, has only one\nsolution, x\u0303 = x\u2217/2 \u2248 0.58365. Now consider the polynomial\np(x) = \u221216 3 x6 + x2 + x.\nNotice that the derivative of this is exactly \u221232x5 + 2x + 1. Thus p\u2019s only critical point is x\u0303. The function is plotted in Figure 5. This point is a local maximum, and the function has no other local maximum in the compact interval [0, 1] (or elsewhere).\nNow consider the game in Figure 6. Let U : R2 \u2192 R : (x, y) 7\u2192 \u2212 163 x\n6+x2+x = p(x) be the ex ante expected utility function extended to R2. Clearly, the unique optimal policy is x = x\u0303, y = 1\u2212 x\u0303. Next we will show that the unique (CDT,GT)-equilibrium is also x = x\u0303. It is easy to see that neither x = 0 nor x = 1 induces a (CDT,GT)-equilibrium. So for any (CDT,GT)-equilibrium it has to be the case that EUCDT,GT(a1 | x) = EUCDT,GT(a2 | x). By Lemma 14, this means that we must have ddxU(x, y) = d dyU(x, y) = 0. Therefore, the only (CDT,GT)-equilibrium is at x = x\u0303. By Lemma 17, this means that the only (EDT,GDH)-equilibrium is also at x = x\u0303.\nAll in all, we have given an easy to represent game for which the only solution (x\u0303, 1\u2212 x\u0303) has entries which are irrational (or even more, cannot be expressed in radicals)."
        },
        {
            "heading": "H Proofs of the Main Results",
            "text": "H.1 Proof of Theorem 1 Recall the problem of maximizing the ex-ante utility in a game \u0393:\nmax \u00b5\u2208\u00d7\u2113i=1 Rmi U(\u00b5)\ns.t. \u00b5ij \u2265 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] mi\u2211 j=1 \u00b5ij = 1 \u2200i \u2208 [\u2113]\n(11)\nThen, the KKT conditions for (11) and a point \u00b5 \u2208 \u00d7\u2113i=1 Rmi are: There exist KKT multipliers {\u03c4ij \u2208 R}\u2113,mii,j=1\nand {\u03bai \u2208 R}\u2113i=1 such that\n\u00b5ij \u2265 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] mi\u2211 j=1 \u00b5ij = 1 \u2200i \u2208 [\u2113]\n\u03c4ij \u2265 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] \u03c4ij = 0 or \u00b5ij = 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] \u2207ij U(\u00b5) = \u2212\u03c4ij + \u03bai \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] .\n(12)\nWe are ready to prove Theorem 1.\nTheorem. Strategy \u00b5 \u2208\u00d7\u2113i=1 \u2206(AIi) of \u0393 is a (CDT,GT)equilibrium if and only if \u00b5 is a KKT point of (11).\nProof. \u201c =\u21d2 \u201d: Suppose \u00b5 \u2208\u00d7\u2113i=1 \u2206(AIi) is a (CDT,GT)-equilibrium of \u0393. Then the first two KKT conditions of (12) are satisfied by assumption. Let i \u2208 [\u2113] and \u2200j \u2208 [mi].\nIf Fr(Ii | \u00b5) = 0, then by Lemma 14 we have \u2207ij U(\u00b5) = 0. Therefore, choose \u03c4ij = 0 and \u03bai = 0 to satisfy the last three KKT conditions for the respective i and j.\nSuppose Fr(Ii | \u00b5) > 0. Choose\n\u03bai = max j\u2032\u2208[mi]\n\u2207ij\u2032 U(\u00b5) .\nThe choice of \u03c4ij depends on \u00b5ij . If \u00b5ij > 0, then by Lemma 14 and by the equivalent characterization of a (CDT,GT)equilibrium right below Definition 10, we have\n\u2207ij U(\u00b5) = Fr(Ii | \u00b5) \u00b7 EUCDT,GT(aj | \u00b5, Ii) = max\nj\u2032\u2208[mi]\n{ Fr(Ii | \u00b5) \u00b7 EUCDT,GT(aj\u2032 | \u00b5, Ii) } = max\nj\u2032\u2208[mi] \u2207ij\u2032 U(\u00b5) = \u03bai .\nTherefore choose \u03c4ij = 0. If \u00b5ij = 0, on the other hand, then the above equation chain would yield inequality \u2207ij U(\u00b5) \u2264 \u03bai instead (when transitioning to the max). Thus, choose \u03c4ij = \u03bai \u2212\u2207ij U(\u00b5). These choices of \u03bai and \u03c4ij satisfy the last three KKT conditions for the respective i and j.\n\u201c \u21d0= \u201d: Suppose \u00b5 is a KKT point of problem (11), that is, it satisfies the KKT conditions (12) for some KKT multipliers {\u03c4ij \u2208 R}\u2113,mii,j=1 and {\u03bai \u2208 R}\u2113i=1. Then, the first two KKT conditions ensure that \u00b5 makes a valid strategy for \u0393 (recall that we always identify \u00b5(aj | Ii) := \u00b5ij for info set Ii \u2208 I\u2217 and action aj \u2208 Ii). So let us check equivalent characterization of a (CDT,GT)-equilibrium right below Definition 10. Let i \u2208 [\u2113] and j \u2208 [mi] be such that Fr(Ii | \u00b5) > 0 and \u00b5ij > 0. Then, Lemma 14 and the last three KKT conditions give us for all j\u2032 \u2208 [mi]:\nEUCDT,GT(aj | \u00b5, Ii) = Fr(Ii | \u00b5) Fr(Ii | \u00b5) \u00b7 EUCDT,GT(aj | \u00b5, Ii)\n= 1\nFr(Ii | \u00b5) \u00b7 \u2207ij U(\u00b5) =\n1\nFr(Ii | \u00b5) \u00b7 (\u2212\u03c4ij + \u03bai)\n\u03c4ij=0 =\n1\nFr(Ii | \u00b5) \u00b7 \u03bai \u2265\n1\nFr(Ii | \u00b5) \u00b7 (\u2212\u03c4ij\u2032 + \u03bai)\n= 1\nFr(Ii | \u00b5) \u00b7 \u2207ij\u2032 U(\u00b5) = EUCDT,GT(aj\u2032 | \u00b5, Ii)\nThis implies EUCDT,GT(aj | \u00b5, Ii) = maxa\u2208AIi EUCDT,GT(a | \u00b5, Ii). Overall, we get that \u00b5 is a (CDT,GT)-equilibrium for \u0393\nH.2 Reproof of Lemma 17 The next lemma is due to Oesterheld and Conitzer [2022] [cf. Piccione and Rubinstein, 1997]. Lemma. An ex-ante optimal strategy of a game \u0393 is also an (EDT,GDH)-equilibrium. An (EDT,GDH)-equilibrium is also a (CDT,GT)-equilibrium.\nIn particular, any single-player extensive-form game \u0393 with imperfect recall admits an (EDT,GDH)-equilibrium and a (CDT,GT)-equilibrium.\nProof. An ex-ante optimal strategy is defined as a maximum to the maximization problem (1). Therefore, it will in particular satisfy the characterization of an (EDT,GDH)-equilibrium according to Lemma 15.\nLet us show that an (EDT,GDH)-equilibrium is a (CDT,GT)-equilibrium by using their respective ex-ante characterizations from Lemma 15 and Theorem 1. So let \u00b5 =\u2208\n\u00d7\u2113i=1 \u2206(AIi) satisfy for all i \u2208 [\u2113] \u00b5\u2217i\u00b7 \u2208 argmax\ny\u2208\u2206(AIi ) U(\u00b5\u22171\u00b7, . . . , \u00b5 \u2217 i\u22121\u00b7, y, \u00b5 \u2217 i+1\u00b7, . . . , \u00b5 \u2217 \u2113\u00b7) .\nFor i \u2208 [\u2113] denote the function\nqi,\u00b5 : \u2206 mi\u22121 \u2192 R\ny 7\u2192 U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, y, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nThen, for j \u2208 [mi], we have\n\u2207j qi,\u00b5(y) = \u2207ij U(\u00b51\u00b7, . . . , \u00b5i\u22121\u00b7, y, \u00b5i+1\u00b7, . . . , \u00b5\u2113\u00b7) .\nBy assumption on \u00b5, each \u00b5i\u00b7 is a solution to the maximum problem\nmax y\u2208\u2206mi\u22121 qi,\u00b5(y) .\nGlobal optimum \u00b5i\u00b7 is therefore in particular a KKT point of that problem. So there exist KKT multipliers {\u03c4ij \u2208 R}mij=1 and \u03bai such that\n\u00b5ij \u2265 0 \u2200j \u2208 [mi] mi\u2211 j=1 \u00b5ij = 1\n\u03c4ij \u2265 0 \u2200j \u2208 [mi] \u03c4ij = 0 or \u00b5ij = 0 \u2200j \u2208 [mi] \u2207ij U(\u00b5) = \u2207j qi,\u00b5(\u00b5i\u00b7) = \u2212\u03c4ij + \u03bai \u2200j \u2208 [mi] .\nSince i \u2208 [\u2113] was arbitrary, we can use these multipliers to obtain that \u00b5 satisfies KKT conditions (12).\nLastly, since there always exists an ex-ante optimal strategy (as a maximum (1) of a continuous polynomial function over a compact domain), there also always exists an (EDT,GDH)equilibrium and (CDT,GT)-equilibrium.\nH.3 Approximate and Well-Supported (CDT,GT)-equilibria\nBefore we get to the proof of Theorem 2, we should discuss two definitions of being close to a (CDT,GT)-equilibrium that are useful to computatational considerations:\nDefinition 19. Let \u0393 be a single-player extensive-form game with imperfect recall, \u00b5 be a strategy for \u0393, and \u03f5 > 0 be a precision parameter. Then:\n\u2022 \u00b5 is called an \u03f5-approximate (CDT,GT)-equilibrium if for all I \u2208 I\u2217 with Fr(I | \u00b5) > 0:\nEUCDT,GT ( \u00b5(\u00b7 | I) | \u00b5, I ) \u2265 max\na\u2032\u2208AI EUCDT,GT(a\u2032 | \u00b5, I)\u2212 \u03f5 .\n\u2022 \u00b5 is called an \u03f5-well-supported (CDT,GT)-equilibrium if for all I \u2208 I\u2217 with Fr(I | \u00b5) > 0 and all a \u2208 AI with \u00b5(a | I) > 0:\nEUCDT,GT(a | \u00b5, I) \u2265 max a\u2032\u2208AI EUCDT,GT(a\u2032 | \u00b5, I)\u2212 \u03f5 .\nIdentity (6) directly implies the following relationship:\nLemma 20. Let \u0393 be a single-player extensive-form game with imperfect recall and \u03f5 > 0. Then any \u03f5-well-supported (CDT,GT)-equilibrium \u00b5 is also an \u03f5-approximate (CDT,GT)equilibrium.\nWe can also prove a polynomial-time reduction in the reverse direction for all those games that have admit the following characteristic:\nDefinition 21. Ia single-player extensive-form game \u0393 (with imperfect recall), a value \u03bb > 0 is said to be a lower bound on positive visit frequencies in \u0393 if it satisfies for all info sets I and all strategies \u00b5 in \u0393\nFr(I | \u00b5) = 0 or Fr(I | \u00b5) \u2265 \u03bb .\nSince Fr(I | \u00b5) as a function in \u00b5 is a polynomial function, and therefore continuous over the connected strategy set \u00d7\u2113i=1 \u2206(AIi), this definition becomes equivalent to\n\u2200 I \u2208 I\u2217 : Fr(I | \u00b5) = 0\u2200\u00b5 or Fr(I | \u00b5) \u2265 \u03bb \u2200\u00b5 . (13)\nWhile the next lemma seems to be novel for imperfectrecall games, the proof borrows its main ideas from the equivalency of \u03f5-approximate Nash equilibria and \u03f5-well-supported Nash equilibria, shown by Chen et al. [2006]. In the below formulation and proof, any roots \u221a x taken shall always refer to the positive valued root.\nLemma 22. Let \u0393 be a single-player extensive-form game with imperfect recall with a lower bound \u03bb \u2208 Q on its positive visit frequencies. Then we can compute a Lipschitz constant L \u2265 1 w.r.t. the infinity norm || \u00b7 ||\u221e for all functions( EUCDT,GT(a | \u00b7, I) ) I\u2208I,Fr(I|\u00b7)\u2265\u03bb,a\u2208AI in \u00b5 over the strategy space\u00d7\u2113\u0131\u0302=1 \u2206(AI\u0131\u0302). Let \u03f5 > 0 such that \u03f5 < 1mi for all info sets Ii \u2208 I\u2217.\nThen, given an \u03f5-approximate (CDT,GT)-equilibrium \u00b5 of \u0393, we can compute a (3L|H| \u221a \u03f5)-well-supported (CDT,GT)equilibrium of \u0393 in polynomial time.\nProof. Let us observe how to compute such a Lipschitz constant L in polynomial time. We first describe the general process of computing a Lipschitz constant Lq of any polynomial function q : Rn \u2192 Rm w.r.t. the infinity norm and over the hypercube [0, 1]n. One possible Lipschitz constant is\nmax x\u2208[0,1]n {||\u2207q(x)||\u221e} = max x\u2208[0,1]n max i\u2208[n] |\u2207iq(x)|\n= max i\u2208[n] max x\u2208[0,1]n\n|\u2207iq(x)| .\nNote that \u2207iq(x) is a polynomial function itself and that the factors xki in its monomials are < 1. Thus, maxx\u2208[0,1]n |\u2207iq(x)| can be upper bounded by the absolute value sum \u2211 \u03bbD\u2208\u2207iq |\u03bbD| of all coefficients present in \u2207iq. Taking the maximum over index i over those upper bound sums gives us a Lipschitz value Lq that can be computed in polynomial time in the description size of q.\nGoing back to the setup of this lemma, take an info set Ii of \u0393 with Fr(Ii | \u00b7) \u2265 \u03bb and an action aj \u2208 Ai. Then we can\nderive as possible Lipschitz constant for EUCDT,GT(aj | \u00b7, Ii): max\n\u00b5\u2208\u00d7\u2113i=1 \u2206(AIi ) {||\u2207\u00b5 EUCDT,GT(aj | \u00b5, Ii)||\u221e}\n\u2264 max \u00b5\u2208[0,1] \u2211 i mi {||\u2207\u00b5 EUCDT,GT(aj | \u00b5, Ii)||}\n= max \u00b5\u2208[0,1] \u2211 i mi {\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u00b5 (\u2207ij U(\u00b5)Fr(Ii | \u00b5) )\u2223\u2223\u2223\u2223\u2223\u2223}\n= max \u00b5\u2208[0,1] \u2211 i mi{\u2223\u2223\u2223\u2223\u2223\u2223Fr(Ii | \u00b5) \u00b7 \u2207\u00b5 \u2207ij U(\u00b5)\u2212\u2207ij U(\u00b5) \u00b7 \u2207\u00b5 Fr(Ii | \u00b5) Fr(Ii | \u00b5)2 \u2223\u2223\u2223\u2223\u2223\u2223}\n\u2264 max \u00b5\n1 Fr(Ii | \u00b5)2 \u00b7 ( max \u00b5 Fr(Ii | \u00b5) \u00b7max \u00b5 ||\u2207\u00b5 \u2207ij U(\u00b5)||\n+max \u00b5 |\u2207ij U(\u00b5)| \u00b7max \u00b5\n||\u2207\u00b5 Fr(Ii | \u00b5)|| )\n\u2264 1 \u03bb2\n\u00b7 ( |H| \u00b7max\n\u00b5 ||\u2207\u00b5 \u2207ij U(\u00b5)||\n+max \u00b5 |\u2207ij U(\u00b5)| \u00b7max \u00b5\n||\u2207\u00b5 Fr(Ii | \u00b5)|| ) .\nThe terms max\u00b5 ||\u2207\u00b5 \u2207ij U(\u00b5)|| and max\u00b5 ||\u2207\u00b5 Fr(Ii | \u00b5)|| as well max\u00b5 |\u2207ij U(\u00b5)| can be bounded above with the method described in the previous paragraph. The final value can be chosen as our Lipschitz constant Lij for EUCDT,GT(aj | \u00b7, Ii). Finally, we can choose\nL = max Ii\u2208I\u2217,Fr(Ii|\u00b7)\u2265\u03bb,aj\u2208AI {1,max i,j Lij}\nas the desired mutual Lipschitz constant. This construction takes polynomial time in the encoding of \u0393 and \u03bb.\nIt remains to prove the first result. Denote with Uij(\u00b5) := EUCDT,GT(aj | \u00b5, Ii) the (CDT,GT)-expected utility function in \u00b5 from being in info set Ii and using action aj . Let \u00b5 be an \u03f5-approximate (CDT,GT)-equilibrium of \u0393, and take an info set Ii with Fr(Ii | \u00b7) > 0, thus Fr(Ii | \u00b7) \u2265 \u03bb. As the first step, let us show that any action that sufficiently suboptimal will also be played with only very low probability. Assume aj\u2032 \u2208 AIi is a suboptimal action, that is, there exists some aj \u2208 AIi such that Uij(\u00b5) \u2265 Uij\u2032(\u00b5) + \u221a \u03f5. Consider the mixed action \u03b1 \u2208 \u2206(AIi) defined as \u03b1j\u2032 = 0, \u03b1j = \u00b5ij + \u00b5ij\u2032 , and \u03b1\u0237\u0302 = \u00b5i\u0237\u0302 for all \u0237\u0302 \u0338= j, j\u2032. Since \u00b5 is assumed to be an \u03f5-approximate (CDT,GT)-equilibrium, we get\u2211 \u0237\u0302\u2208[mi] \u00b5i\u0237\u0302 \u00b7 Ui\u0237\u0302(\u00b5) (6) = EUCDT,GT(\u00b5(\u00b7 | Ii) | \u00b5, Ii)\n\u2265 max k\u2208[mi] EUCDT,GT(ak | \u00b5, Ii)\u2212 \u03f5 = \u2212\u03f5+ \u2211\n\u0237\u0302\u2208[mi]\n\u03b1\u0237\u0302 \u00b7 max k\u2208[mi] Uik(\u00b5)\n\u2265 \u2212\u03f5+ \u2211\n\u0237\u0302\u2208[mi]\n\u03b1\u0237\u0302 \u00b7 Ui\u0237\u0302(\u00b5)\n= \u2212\u03f5+ (\u00b5ij + \u00b5ij\u2032) \u00b7 Uij(\u00b5) + \u2211 \u0237\u0338\u0302=j,j\u2032 \u00b5i\u0237\u0302 \u00b7 Ui\u0237\u0302(\u00b5) .\nRearranging yields\n\u00b5ij\u2032 \u00b7 Uij\u2032(\u00b5) \u2265 \u00b5ij\u2032 \u00b7 Uij(\u00b5)\u2212 \u03f5 .\nUsing Uij(\u00b5) \u2265 Uij\u2032(\u00b5) + \u221a \u03f5, we therefore get\n\u00b5ij\u2032 \u2264 \u03f5 Uij(\u00b5)\u2212 Uij\u2032(\u00b5) \u2264 \u03f5\u221a \u03f5 = \u221a \u03f5 .\nIn other words, a suboptimal action aj\u2032 \u2208 AIi in an \u03f5approximate (CDT,GT)-equilibrium will be played with low probability. Denote with lowi \u2286 [mi] the set of all those action indices j\u2032 such that aj\u2032 is played with \u2264 \u221a \u03f5 probability in \u00b5. Since we assumed \u221a \u03f5 < 1mi , there will be at least one action index j \u2208 [mi]\\lowi. We can therefore create a new candidate strategy \u03c0 from \u00b5 by redistributing the probability of the lowi actions to the others and achieve well-supportedness for \u03c0. More precisely, define \u03c0 \u2208\u00d7\u2113i=1 \u2206mi\u22121 as follows:\n\u2022 for all Ii with Fr(Ii | \u00b7) = 0 and all aj \u2208 AIi set \u03c0ij := \u00b5ij ,\n\u2022 for all Ii with Fr(Ii | \u00b7) \u2265 \u03bb and all aj \u2208 AIi with \u00b5ij \u2264 \u221a \u03f5 set \u03c0ij = 0, and\n\u2022 for all Ii with Fr(Ii | \u00b7) \u2265 \u03bb and all aj \u2208 AIi with \u00b5ij > \u221a \u03f5 set\n\u03c0ij = \u00b5ij +\n\u2211 \u0237\u0302\u2208lowi \u00b5i\u0237\u0302 mi \u2212 |lowi| .\nThis is easily computable. Let us prove that \u03c0 is a (3L|H| \u221a \u03f5)-well-supported (CDT,GT)-equilibrium: First note ||\u03c0 \u2212 \u00b5||\u221e \u2264 \u221a \u03f5 \u00b7 |H|. Let Ii \u2208 I\u2217 again be an info set with Fr(Ii | \u00b7) > 0, hence Fr(Ii | \u00b7) \u2265 \u03bb, and let an action aj \u2208 AIi have \u03c0(aj | Ii) > 0. Recall that L serves as a Lipschitz constant on Uij . By construction of \u03c0, we get \u00b5ij > \u221a \u03f5 which implies that aj could not have been suboptimal for \u00b5, that is,\nUij(\u00b5) \u2265 max \u0237\u0302\u2208[mi]\nUi\u0237\u0302(\u00b5)\u2212 \u221a \u03f5 . (14)\nTherefore,\nEUCDT,GT(aj | \u03c0, Ii) = Uij(\u03c0) = Uij(\u03c0)\u2212 Uij(\u00b5) + Uij(\u00b5) \u2265 \u2212L \u00b7 ||\u03c0 \u2212 \u00b5||+ Uij(\u00b5) (14)\n\u2265 \u2212L \u00b7 \u221a \u03f5 \u00b7 |H|+ max\n\u0237\u0302\u2208[mi] Ui\u0237\u0302(\u00b5)\u2212\n\u221a \u03f5\n\u2265 \u22122L|H| \u221a \u03f5+ max\n\u0237\u0302\u2208[mi] {Ui\u0237\u0302(\u00b5)\u2212 Ui\u0237\u0302(\u03c0) + Ui\u0237\u0302(\u03c0)}\n\u2265 \u22122L|H| \u221a \u03f5+ max\n\u0237\u0302\u2208[mi] {\u2212L \u00b7 ||\u03c0 \u2212 \u00b5||+ Ui\u0237\u0302(\u03c0)}\n\u2265 \u22123L|H| \u221a \u03f5+ max\n\u0237\u0302\u2208[mi] Ui\u0237\u0302(\u03c0)\n= max a\u0237\u0302\u2208AIi\nEUCDT,GT(a\u0237\u0302 | \u03c0, Ii)\u2212 3L|H| \u221a \u03f5\nH.4 Proof of Theorem 2 Theorem. (CDT,GT)-EQUILIBRIUM is CLS-hard. CLShardness holds even for games restricted to: (1.) a tree depth of 6 and the player has 2 actions per info\nset,\n(2.) no absentmindedness and a tree depth of 6, and (3.) no chance nodes, a tree depth of 5, and only one info set. The problem is in CLS for the subclass of problem instances of (CDT,GT)-EQUILIBRIUM where a lower bound on positive visit frequencies in \u0393 is easily obtainable.\nWe prove this result in four parts.\nCLS Membership of restricted (CDT,GT)-EQUILIBRIUM By Theorem 1, (CDT,GT)-equilibria of \u0393 coincide with the KKT points of the strategy utility function U of \u0393. Finding an approximate KKT point of a continuously differentiable function over a compact domain was shown to be CLS-complete by Fearnley et al. [2023]: Definition 23. An instance of the problem KKT consists of (1) a precision parameter \u03f5 > 0 encoded in binary, (2) a matrix A \u2208 Rm\u00d7n and a vector b \u2208 Rm defining a bounded non-empty domain D = {x \u2208 Rn : Ax \u2264 b}, (3) two wellbehaved arithmetic circuits f : Rn \u2192 R and \u2207f : Rn \u2192 Rn, and (4) a Lipschitz constant L > 0.\nA solution consists of a point x \u2208 D such that there exist \u00b51, . . . , \u00b5m \u2265 0 such that ||\u2207f(x) + AT\u00b5|| \u2264 \u03f5 and \u00b5T (Ax \u2212 b) = 0. Alternatively, we also accept points x, y \u2208 D as a solution that show that one of the following is true: (i) f or \u2207f is not L-Lipschitz, or (ii) \u2207f is not the gradient of f . Lemma 24 (Fearnley et al. [2021]). KKT is CLS-complete.\nFearnley et al. [2021] note that any norm can be used in Definition 23. We will henceforth use the infinity norm ||\u00b7||\u221e.\nLet us outline the reduction from the restricted (CDT,GT)EQUILIBRIUM to KKT. Let (\u0393, \u03f5) be an instance of (CDT,GT)-EQUILIBRIUM for which a lower bound \u03bb on positive visit frequencies in \u0393 can be computed in polynomial time. Construct the strategy utility function U of \u0393 and its gradient \u2207U . Then \u2212U and \u2212\u2207U as polynomial functions will make well-behaved arithmetic circuits f and \u2207f (the negative sign comes in because KKT is about minimizing f whereas (CDT,GT)-equilibrium is about utility maximization). The domain D will be the Cartesian product of simplices\u00d7li=1 \u2206(AIi) described with inequalities as in (11). Next, we can construct a Lipschitz constant L for the polynomial functions \u2212U and \u2212\u2207U over\u00d7li=1 \u2206(AIi) as described in the proof of Lemma 22. Finally, set the precision parameter \u03b4 of the KKT instance to \u03b4 = 12\u03bb\u03f5, which is polynomial time computable in the encoding size of (\u0393, \u03f5).\nGet a solution to the KKT instance. We have by construction that L is a valid Lipschitz constant for f and \u2207f , and that \u2207f is indeed the gradient of f . Thus, the solution has to be a \u03b4-KKT point \u00b5, i.e. have KKT multipliers {\u03c4ij \u2208 R}\u2113,mii,j=1 and {\u03bai \u2208 R}\u2113i=1 such that\n\u00b5ij \u2265 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] mi\u2211 j=1 \u00b5ij = 1 \u2200i \u2208 [\u2113]\n\u03c4ij \u2265 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] \u03c4ij = 0 or \u00b5ij = 0 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] |\u2207ij U(\u00b5)\u2212 (\u2212\u03c4ij + \u03bai)| \u2264 \u03b4 \u2200i \u2208 [\u2113],\u2200j \u2208 [mi] .\nThe point \u00b5 forms a valid strategy of \u0393. Let us show that it is also an \u03f5-well-supported (CDT,GT)-equilibrium of \u0393. Let Ii be an info set with Fr(Ii | \u00b5) > 0, hence Fr(Ii | \u00b5) \u2265 \u03bb, and let aj \u2208 AIi be an action with \u00b5ij = \u00b5(aj | Ii) > 0. Then, for any other action aj\u2032 \u2208 AIi , we have by Lemma 14 and the KKT conditions:\nEUCDT,GT(aj | \u00b5, Ii) = \u2207ij U(\u00b5) Fr(Ii | \u00b5) \u2265 \u2212\u03c4ij + \u03bai \u2212 \u03b4 Fr(Ii | \u00b5)\n= \u03bai \u2212 \u03b4 Fr(Ii | \u00b5) \u2265 \u2212\u03c4ij \u2032 + \u03bai \u2212 \u03b4 Fr(Ii | \u00b5) \u2265 \u2207ij \u2032 U(\u00b5)\u2212 2\u03b4 Fr(Ii | \u00b5)\n= EUCDT,GT(aj\u2032 | \u00b5, Ii)\u2212 2\nFr(Ii | \u00b5) \u00b7 \u03b4\n\u2265 EUCDT,GT(aj\u2032 | \u00b5, Ii)\u2212 2\n\u03bb \u00b7 \u03b4\n= EUCDT,GT(aj\u2032 | \u00b5, Ii)\u2212 \u03f5 Thus \u00b5 forms an \u03f5-well-supported (CDT,GT)-equilibrium for \u0393, and therefore, also an \u03f5-(CDT,GT)-equilibrium for \u0393.\nFirst CLS Hardness Result of Theorem 2 We will derive our first CLS hardness result from a KKT problem studied by Babichenko and Rubinstein [2021]. Consider the maximization of a polynomial function p : R\u2113 \u2192 R over the hypercube:\nmax x\u2208R\u2113 p(x) s.t. x \u2208 [0, 1]\u2113 . (15)\nHere, p is again assumed to be represented in the Turing (bit) model p(x) = \u2211 D\u2208MB(d,\u2113) \u03bbD \u00b7 \u220f i\u2208[\u2113] x Di i .\nDefinition 25. An instance of the problem KKTPOLYOVERCUBE consists of a polynomial function p : R\u2113 \u2192 R together with a precision value \u03f5 > 0. A solution consists of a point x \u2208 [0, 1]\u2113 that is an \u03f5-KKT point of the maximization problem (15):\n\u2200i \u2208 [\u2113] : xi > 0 =\u21d2 \u2207i p(x) \u2265 \u2212\u03f5 \u2200i \u2208 [\u2113] : xi < 1 =\u21d2 \u2207i p(x) \u2264 \u03f5\n(16)\nBabichenko and Rubinstein [2021] call this problem GDFIXEDPOINT. Lemma 26 (Babichenko and Rubinstein [2021]). KKTPOLYOVERCUBE is CLS-complete. Hardness holds even2 for polynomials in which every monomial has degree 5.\nWe can now derive CLS-hardness of (CDT,GT)EQUILIBRIUM by reducing from KKTPOLYOVERCUBE.\nTake an instance (p : R\u2113 \u2192 R, \u03f5) of KKTPOLYOVERCUBE where p is known to only have degree 5 monomials. There are at most ( \u2113+5\u22121\n5\n) = O(\u21135) many such monomials.\nConsider the modified polynomial\np\u0302 : \u2113\n\u00d7 i=1 R2 \u2192 R( (xi1, xi2) )\u2113 i=1 7\u2192 p(x11, x21, . . . , x\u21131) . (17)\n2Hardness even holds if each summand \u03bbD \u00b7 \u220f\ni\u2208[\u2113] x Di i for\u2211\nD\u2208MB(5,\u2113) has degree 5 and is component-wise concave (and therefore the polynomial is also component-wise concave).\nCreate a game \u0393 out of p\u0302 as described in Appendix A with the second variant, which is constructed in polynomial time in the encoding of p since its degree is known to be 5. Let |H| be the number of nodes in \u0393. Recall that for any info set Ii of \u0393 its visit frequency Fr(Ii | \u00b5) is constant in the used strategy \u00b5 and easily computable by (5). Thus, we are able to get a lower bound on the positive visit frequencies, which allows us to use Lemma 22 later on. Note moreover that the visit frequencies are always bounded above by |H|. Set\n\u03b4 := min{1 3 ,\n\u03f52\n(3L|H|2)2 }\nwhich is computable in polynomial time in the encoding of p and \u03f5.\nRecall from Appendix A that the strategy utility function U of \u0393 has the property U(\u00b5) = p\u0302(\u00b5) = p(\u00b511, . . . , \u00b5\u21131) for any point \u00b5 \u2208 \u00d7\u2113i=1 R2. Therefore, \u2207i1 U(\u00b5) = \u2207i p(\u00b511, . . . , \u00b5\u21131) and \u2207i2 U(\u00b5) = 0 for all \u00b5 \u2208\u00d7\u2113i=1 R2 and i \u2208 [\u2113].\nLet \u03c0\u2217 \u2208 \u00d7\u2113i=1 \u22061 be a a solution to the (CDT,GT)EQUILIBRIUM-instance (\u0393, \u03b4). By Lemma 22, we can compute from it a (3L|H| \u221a \u03b4)-well-supported (CDT,GT)equilibrium \u00b5\u2217 = (\u00b5\u2217ij) \u2113,2 i,j=1 \u2208 \u00d7\u2113i=1 \u22061 of \u0393 in polynomial time. For us \u00b5\u2217 is, in particular, a \u03f5|H| -wellsupported (CDT,GT)-equilibrium because we set \u03b4 such that 3L|H| \u221a \u03b4 \u2264 \u03f5|H| . We can now show that the point x \u2217 := (\u00b5\u2217i1) \u2113 i=1 \u2208 [0, 1]\u2113 is a solution to the KKTPOLYOVERCUBEinstance (p : R\u2113 \u2192 R, \u03f5). Let us first consider any index i \u2208 [\u2113] with Fr(Ii | \u00b5\u2217) = 0. Then, by Lemma 14, we get 0 = \u2207i1 U(\u00b5\u2217) = \u2207i p(x\u2217). Thus, such indices i always satisfy the conditions (16) independent of the value x\u2217i .\nNow consider an index i \u2208 [\u2113] with Fr(Ii | \u00b5\u2217) > 0 and 0 < x\u2217i = \u00b5 \u2217 i1. Then, due to \u00b5\n\u2217 being a \u03f5|H| -well-supported (CDT,GT)-equilibrium, we get\nEUCDT,GT(a1 | \u00b5\u2217, Ii) \u2265 max j=1,2\nEUCDT,GT(aj | \u00b5\u2217, Ii)\u2212 \u03f5\n|H|\n\u21d0\u21d2 EUCDT,GT(a1 | \u00b5\u2217, Ii) \u2265 EUCDT,GT(a2 | \u00b5\u2217, Ii)\u2212 \u03f5\n|H|\nWith Lemma 14, we can therefore derive\n\u2207i p(x\u2217) = \u2207i1 U(\u00b5\u2217) = Fr(Ii | \u00b5\u2217) \u00b7 EUCDT,GT(a1 | \u00b5\u2217, Ii)\n\u2265 Fr(Ii | \u00b5\u2217) \u00b7 ( EUCDT,GT(a2 | \u00b5\u2217, Ii)\u2212 \u03f5\n|H| ) = Fr(Ii | \u00b5\u2217) \u00b7 EUCDT,GT(a2 | \u00b5\u2217, Ii)\u2212 Fr(Ii | \u00b5\u2217) \u00b7 \u03f5\n|H|\n= \u2207i2 U(\u00b5)\u2212 Fr(Ii | \u00b5\u2217) \u00b7 \u03f5\n|H| = \u2212Fr(Ii | \u00b5\u2217) \u00b7\n\u03f5\n|H|\n\u2265 \u2212|H| \u00b7 \u03f5 |H| = \u2212\u03f5\nNow consider an index i \u2208 [\u2113] with Fr(Ii | \u00b5\u2217) > 0 and 1 > x\u2217i = \u00b5 \u2217 i1 = 1\u2212 \u00b5\u2217i2, i.e., \u00b5\u2217i2 > 0. Then with analogous\narguments, we get\nEUCDT,GT(a2 | \u00b5\u2217, Ii) \u2265 max j=1,2\nEUCDT,GT(aj | \u00b5\u2217, Ii)\u2212 \u03f5\n|H|\n\u21d0\u21d2 EUCDT,GT(a1 | \u00b5\u2217, Ii) \u2264 EUCDT,GT(a2 | \u00b5\u2217, Ii) + \u03f5\n|H| Similarly to the other case, we can therefore derive \u2207i p(x\u2217) = \u2207i1 U(\u00b5\u2217) = Fr(Ii | \u00b5\u2217) \u00b7 EUCDT,GT(a1 | \u00b5\u2217, Ii)\n\u2264 Fr(Ii | \u00b5\u2217) \u00b7 ( EUCDT,GT(a2 | \u00b5\u2217, Ii) + \u03f5\n|H| ) = \u2207i2 U(\u00b5) + Fr(Ii | \u00b5\u2217) \u00b7 \u03f5\n|H| = Fr(Ii | \u00b5\u2217) \u00b7\n\u03f5\n|H|\n\u2264 |H| \u00b7 \u03f5 |H| = \u03f5\nTherefore, all in all, point x\u2217 makes a solution to the KKTPOLYOVERCUBE-instance (p : R\u2113 \u2192 R, \u03f5). This finishes the polynomial time reduction from the search problem of DEGREE-5-KKTPOLYOVERCUBE to the search problem (CDT,GT)-EQUILIBRIUM.\nNote that in the above reduction, the constructed game \u0393 has a tree depth of 6. The root is a chance node with a number of outgoing edges that equals the number of monomials in p. Any other node of \u0393 will have two outgoing actions. Therefore, CLS-hardness of (CDT,GT)-EQUILIBRIUM remains even if the game instance has a tree depth of 6 and the player only has 2 actions per info set.\nSecond CLS Hardness Result of Theorem 2 The second and third CLS hardness results will both rely on the following game-theoretic problem studied by Babichenko and Rubinstein [2021] (again).\nLet G be a n-player simultaneous game with action sets {Ai}i\u2208[n] of size mi := |Ai| and with utility functions Vi : \u00d7n\u0131\u0302=1 \u2206(A\u0131\u0302) \u2192 R for each player i. A mixed action profile x = (xij) n,mi i,j=1 \u2208 \u00d7ni=1 \u2206(AIi) is called an \u03f5-Nash equilibrium of G if for every player i \u2208 [n] and every action aj \u2208 Ai of player i, we have Vi(x) \u2265 Vi(x1\u00b7, . . . , xi\u22121\u00b7, aj , xi+1\u00b7, . . . , xn\u00b7) \u2212 \u03f5. In standard game theory notation, this would be phrased as Vi(x) \u2265 Vi(aj , x\u2212i\u00b7) \u2212 \u03f5. For us, it will be helpful to use the notation of a EDT deviation, in which this condition becomes Vi(x) \u2265 Vi(xIi 7\u2192aj )\u2212 \u03f5.\nA c-polytensor game is a multiplayer simultaneous game presented in terms of payoff tables, one for each subset of c players. Given a pure-strategy action profile of all players, a player\u2019s payoff consists of the sum of payoffs they get from the payoff tables of the c-subsets they belong to. For any constant c, such games with n \u2265 c players and up to m pure actions per player have a polynomial-sized representation in n andm because there are \u2264 n\u00b7 ( n c ) \u00b7mc = O(nc+1\u00b7m5) many payoff entries overall. A c-polytensor identical interest game is a c-polytensor game in which every subgame associated with a c-subset yields the participating player the same utility. Definition 27. An instance of the problem c-POLYTENSORIDENTICALINTEREST consists of a c-polytensor identical interest game together with a precision value \u03f5 > 0. A solution consist of an \u03f5-Nash equilibrium of the game.\nLemma 28 (Babichenko and Rubinstein [2021]). 5- POLYTENSOR-IDENTICALINTEREST is CLS-complete.\nWithout changing the complexity, we may assume that in instances of 5-POLYTENSOR-IDENTICALINTEREST every player has exactly m actions (by possibly copying actions) and that the payoffs lie in [0, 1] (by shifting and rescaling utilities them).\nWe can now prove the second CLS hardness of (CDT,GT)-EQUILIBRIUM by reducing from 5- POLYTENSOR-IDENTICALINTEREST.\nTake an instance (G, \u03f5) of 5-POLYTENSORIDENTICALINTEREST. Let n be the number of players of G, and m be the number of pure strategies of player i. Then, each subset of 5 players of G has a table of m5 payoffs in [0, 1] one for each of their possible pure profiles. Moreover, there will be ( n 5 ) many subsets of 5 players among n players. Collect all those subsets to \u039b := {i[5] \u2282 [n] : |i[5]| = 5}. If i[5] \u2208 \u039b is a 5-subset and the players of i[5] play strategy profile \u00b5i[5] \u2208 \u00d7i\u2208i[5] \u2206(AIi), then denote the payoff that these players get in that subgame by ui[5](\u00b5i[5]). Then, the overall utility function Vi of a player i \u2208 [n] takes as input a strategy profile \u00b5 for the game \u0393 and returns the payoff:\nVi(\u00b5) = \u2211\ni[5]\u2208\u039b: i\u2208i[5]\nui[5](\u00b5i[5])\nConstruct an instance (\u0393, \u03b4) of (CDT,GT)-EQUILIBRIUM as follows. There will be n info sets I1, . . . , In. The root of the tree of \u0393 is a chance node having ( n 5 ) subtrees, one for each set i[5] of 5 players of G. The actions towards these subtrees are chosen uniform randomly, that is, with the same probability 1/ ( n 5 ) . Consider a subtree Ti[5] associated with a set i[5] = {i1, . . . , i5} \u2208 \u039b. Then Ti[5] shall have depth 5 with depth layer k \u2212 1 for k \u2208 [5] corresponding to player ik. Every node of Ti[5] of depth k1 shall be assigned to info set Iik and havem outgoing edges labelled by the pure strategies of player ik. Nodes of depth 5 shall be terminal nodes. If terminal node z of \u0393 has action history (i[5], j1, . . . , j5), then it shall yield a payoff equal to ui[5](j1, . . . , j5), that is, the payoff of the subgame associated with i[5] from the pure action profile (j1, . . . , j5). Define the precision parameter as \u03b4 := \u03f5/ ( n\u22121 4 ) . The instance (\u0393, \u03b4) can be constructed in polynomial time in the encoding of (G, \u03f5). Note that the game \u0393 has tree depth 6 and no absentmindedness.\nTake an info set Ii of \u0393. The player reaches info set Ii with probability one in every subtree Ti[5] with i \u2208 i[5], independent of her strategy choice \u00b5. Thus, the overall reach probability of each info set Ii of \u0393 is exactly the number subsets i[5] \u2208 \u039b that contain i, divided by the number of 5- subsets overall, which is ( n\u22121 4 ) / ( n 5 ) = 5n .\n3 In particular, this reach probability is non-zero, so an approximate (CDT,GT)equilibrium must be approximately optimal in every info set Ii of \u0393.\nSince there is no absentmindedness in \u0393, we get by Appendix E for all info sets Ii, all strategies \u00b5, and all mixed\n3Note that this value would also serve as a lower bound on positive frequencies.\nactions \u03b1 \u2208 \u2206(AIi):\nEUCDT,GT(\u03b1 | \u00b5, Ii) = EUEDT,GDH(\u03b1 | \u00b5, Ii) (10) = 1\nP(Ii | \u00b5Ii 7\u2192\u03b1) \u00b7\n\u2211 z\u2208Z\nIi\u2229hist(z)\u0338=\u2205\nP(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\n= n\n5 \u00b7 \u2211 i[5]\u2208\u039b: i\u2208i[5] \u2211 z\u2208Ti[5] P(z | \u00b5Ii 7\u2192\u03b1) \u00b7 u(z)\n= n\n5 \u00b7 \u2211 i[5]\u2208\u039b: i\u2208i[5] \u2211 j[5]\u2208\u00d7\u0302\u0131\u2208i[5] AI\u0131\u0302\nP(zi[5], j[5] | \u00b5Ii 7\u2192\u03b1) \u00b7 u i[5](j[5])\n(\u2217) = n\n5 \u00b7 \u2211 i[5]\u2208\u039b: i\u2208i[5] 1( n 5 ) \u00b7 ui[5]((\u00b5Ii 7\u2192\u03b1)i[5]) = n\n5 \u00b7 1(n\n5 ) \u00b7 Vi(\u00b5Ii 7\u2192\u03b1) =\n1( n\u22121 4 ) \u00b7 Vi(\u00b5Ii 7\u2192\u03b1) where in (\u2217) we used that for any i[5] = {i1, . . . , i5} \u2208 \u039b\nand strategy \u00b5 that\nui[5](\u00b5i[5])\n= \u2211\n(jk)5k=1\u2208\u00d75k=1 AIik\nui[5](j1, . . . , j5) \u00b7 5\u220f\nk=1\n\u00b5i[5](jk | Iik)\n= \u2211\nj[5]\u2208\u00d7i\u2032\u2208i[5] AIi\u2032\n( n\n5\n) \u00b7 P(zi[5], j[5] | \u00b5Ii 7\u2192\u03b1) \u00b7 u i[5](j[5]) .\nLet \u00b5\u2217 \u2208\u00d7\u2113i=1 \u2206(AIi) be a solution to the (CDT,GT)EQUILIBRIUM-instance (\u0393, \u03b4). Then we can show that the mixed action profile (\u00b5\u2217i\u00b7) n i=1 is a solution to 5- POLYTENSOR-IDENTICALINTEREST instance (G, \u03f5), that is, an \u03f5-Nash equilibrium of G. Note that \u00b5\u2217Ii 7\u2192\u00b5\u2217i\u00b7 = \u00b5\n\u2217. We obtain for all player i \u2208 [n] and all pure actions j \u2208 [mi] of player i:\nVi(\u00b5 \u2217) =\n=\n( n\u22121 4 )( n\u22121 4\n) \u00b7 Vi(\u00b5\u2217Ii 7\u2192\u00b5\u2217i\u00b7) = ( n\u2212 1 4 ) \u00b7 EUCDT,GT(\u00b5\u2217i\u00b7 | \u00b5\u2217, Ii)\n\u2265 ( n\u2212 1 4 ) \u00b7 [ EUCDT,GT(aj | \u00b5\u2217, Ii)\u2212 \u03b4 ] = ( n\u22121 4\n)( n\u22121 4 ) \u00b7 Vi(\u00b5\u2217Ii 7\u2192aj )\u2212 (n\u2212 14 ) \u00b7 \u03b4\n= Vi(\u00b5 \u2217 Ii 7\u2192aj ) + \u03f5 .\nTherefore, a \u03b4-(CDT,GT)-equilibrium in \u0393 makes an \u03f5Nash equilibrium in G.\nThird CLS Hardness Result of Theorem 2 We prove the third CLS hardness of Theorem 2 by again reducing from 5-POLYTENSOR-IDENTICALINTEREST.\nTake an instance (G, \u03f5) of 5-POLYTENSORIDENTICALINTEREST. Use the same notation as in the last reduction proof from 5-POLYTENSORIDENTICALINTEREST. In particular, G has n players with m actions, and payoffs lie in [0, 1]. Moreover, we can assume \u03f5 < 1.\nCorresponding Game Construct an instance (\u0393, \u03b4) of (CDT,GT)-EQUILIBRIUM as follows. \u0393 is a game tree of depth 5 and all nodes of \u0393 will be player nodes that belong to the same info set I . The action set AI is defined as {(i, j) : i \u2208 [n], j \u2208 [m]}. Nodes of depth \u2264 4 all have out-degree n \u00b7 m. At depth 5, we have (n \u00b7 m)5 many nodes that shall be terminal nodes. If the path to a terminal node z is (i[5], j[5]) = ((i1, j1), . . . , (i5, j5)), and if \u03b7(i[5]) \u2208 {1, . . . , 5} is the number of distinct values present4 in i[5] of z, then z shall yield the player a payoff of\n1. M2 \u00b7 \u03b7(i[5]) if \u03b7(i[5]) \u2264 4, and\n2. M2 \u00b7 \u03b7(i[5]) + ui[5](j[5]) if \u03b7(i[5]) = 5.\nNote that we have constant visit frequency Fr(I | \u00b5) = Fr(I) = 5 independent of the strategy \u00b5 used in \u0393. This makes Lemma 22 applicable to \u0393. Let L be the Lipschitz constant L from Lemma 22. Then we can define the values\nM1 := 2 \u00b7 100 \u00b7 n9m4 \u00b7 1\n\u03f5\n\u03b41 := 1\n5 ( 1 n \u2212 1 M1 )4 \u00b7 \u03f5 2\n\u03b42 := ( \u03b41 3L \u22115\nk=0(nm) k )2 M2 := (\u03b41 + n 4) \u00b7M41\nwhere the sum \u22115\nk=0(nm) k represents the number of nodes\n|H| in \u0393. To give some intuition, \u03b42 is chosen such that Lemma 22 can be applied to get a \u03b41-well-supported (CDT,GT)-equilibrium. M2 is chosen large enough (in comparison to the payoffs in [0, 1]) such that for equilibrium play, the player of \u0393 mostly cares about mixing up the players i from which she chooses an action in an approximate uniform fashion. M1 is chosen large enough to counterbalance the errors in this approximate uniform mixing, and \u03b41 is chosen small enough to recover an \u03f5-Nash equilibrium at the end.\nLet us show that a \u03b42-(CDT,GT)-equilibrium of \u0393 gives rise to an \u03f5-Nash equilibrium of G. For that, we first have to collect some further observations about \u0393.\nEx-ante Utility in \u0393 Let us describe the expected utility U(\u00b5) of a strategy \u00b5 = (\u00b5ij) n,m i,j=1 \u2208 \u2206(AI) of \u0393. Split it into U(\u00b5) =M2\u03d5(\u00b5)+\u03c8(\u00b5) where the first part shall come from the M2 \u00b7 \u03b7(i[5]) and the second part from the ui[5](j[5]). The\n4Note our shift in notation from the previous reduction proof. Now, tuple i[5] may contain duplicates from the set of players [n].\nlatter is simply\n\u03c8(\u00b5) = \u2211\n(i[5],j[5]):\u03b7(i[5])=5\nui[5](j[5]) 5\u220f k=1 \u00b5ikjk . (18)\nFor the former, denote with pi = \u2211\nj \u00b5ij the total weight on actions that came from player i of the original game. Consider the stochastic experiment of drawing 5 items out of the set of players [n] according to the probability distribution p = (pi)i. Let \u03c7i \u2208 {0, 1} be the random variable that denotes whether player i was drawn at least once in the stochastic experiment. Then we can calculate the expected number of distinct players that are drawn in the experiment as\n\u03d5(\u00b5) = E[ \u2211 i\u2208[n] \u03c7i] = \u2211 i\u2208[n] E[\u03c7i]\n= \u2211 i\u2208[n] P(\u03c7i = 1) = \u2211 i\u2208[n] ( 1\u2212 P(\u03c7i = 0) ) = n\u2212\n\u2211 i\u2208[n] (1\u2212 pi)5 .\nDe Se Utility in \u0393 Next, we describe EUCDT,GT(aij | \u00b5, I) for an action aij at I with the help of Lemma 14.\nEUCDT,GT(aij | \u00b5, I) = 1\nFr(I) \u2207ij U(\u00b5) =\n1 5 \u2207ij U(\u00b5)\n= M2 5 \u00b7 \u2207ij \u03d5(\u00b5) + 1 5 \u2207ij \u03c8(\u00b5)\n= M2 5 \u00b7 5(1\u2212 pi)4\n+ 1\n5 \u2211 (i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j, j[4]) 5\u220f k=2 \u00b5ikjk\n(\u2217) \u2208 [ M2(1\u2212 pi)4,M2(1\u2212 pi)4 + 1\n5 ( n\u2212 1 4 )] \u2286 [ M2(1\u2212 pi)4,M2(1\u2212 pi)4 + n4 ] .\nIn (\u2217), we used that all factors in the indexed sum lie in [0, 1]. Solution Recovery Let \u00b5\u2032 be a \u03b42-(CDT,GT)-equilibrium of \u0393. By Lemma 22, we can then compute a \u03b41-wellsupported (CDT,GT)-equilibrium \u00b5 of \u0393 from \u00b5\u2032. In the remaining part of the proof, we will only work with \u00b5. In equilibrium, every player gets to play similarly often Denote pi = \u2211 j \u00b5ij again. Take the two players i\u2217 \u2208 argmaxi pi and i\u2217 \u2208 argmaxi pi that are played the most and least often under \u00b5. Choose any actions j\u2217, j\u2217 \u2208 [m] of players i\u2217 and i\u2217 respectively. Then we have\n\u03b41 \u2265 EUCDT,GT(ai\u2217j\u2217 | \u00b5, I)\u2212 EUCDT,GT(ai\u2217j\u2217 | \u00b5, I) \u2265M2(1\u2212 pi\u2217)4 \u2212M2(1\u2212 pi\u2217)4 \u2212 n4\n\u2265M2 \u00b7 [ (1\u2212 pi\u2217 + pi\u2217 \u2212 pi\u2217)4 \u2212 (1\u2212 pi\u2217)4 ] \u2212 n4\n(\u2020) \u2265 M2 \u00b7 (pi\u2217 \u2212 pi\u2217)4 \u2212 n4 .\nIn (\u2020) we first used the binomial theorem for the components 1 \u2212 pi\u2217 \u2265 0 and pi\u2217 \u2212 pi\u2217 \u2265 0 to split up the first exponent term. Next, the term (1\u2212 pi\u2217)4 canceled out and we dropped three other positive terms. From this, we conclude\n0 \u2264 pi\u2217 \u2212 pi\u2217 \u2264 \u2223\u2223\u2223\u2223 4 \u221a \u03b41 + n4\nM2 \u2223\u2223\u2223\u2223 = 1M1 . All in all we get that in a \u03b41-well-supported (CDT,GT)equilibrium \u2013 such as \u00b5 \u2013 the actions aij of each player i \u2208 [n] get played with summed probability\npi \u2208 [ pi\u2217 , pi\u2217 ] \u2286 [ 1 n \u2212 1 M1 , 1 n + 1 M1 ] .\nRecovering an \u03f5-Nash equilibrium We now have everything needed to complete the proof. Define the corresponding strategy profile \u03c0 = (\u03c0i\u00b7)ni=1 \u2208\u00d7ni=1 \u2206m\u22121 in G to \u00b5 as \u03c0ij :=\n\u00b5ij pi for each player i \u2208 [n] and action j \u2208 [m]. This is well-defined because pi \u2265 1n\u2212 1 M1\n> 0. We show that \u03c0 is an \u03f5-Nash equilibrium of G by contradiction. Assume \u03c0 is not an \u03f5-Nash equilibrium. Then, by Chen et al. [2006], it is also not an \u03f5-well-supported Nash equilibrium , that is, there exists a player i \u2208 [n] with actions j, j\u2032 \u2208 [m] such that action j\u2032 is played with positive probability \u03c0ij\u2032 > 0 and such that playing j yields her more than \u03f5 more utility than playing j\u2032, given that the other players play \u03c0\u2212i\u00b7. Formally, this means\n\u2211 (i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j, j[4]) 5\u220f k=2 \u03c0ikjk\n> \u03f5+ \u2211\n(i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j\u2032, j[4]) 5\u220f k=2 \u03c0ikjk . (19)\nSince 1n \u2212 1 M1 \u2264 p\u0131\u0302 \u2264 1n + 1 M1 for all \u0131\u0302 \u2208 [n], we can derive\n\u2211 (i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j, j[4]) 5\u220f k=2 \u03c0ikjk\n= \u2211 u(i,i[4])(j, j[4]) 5\u220f k=2 \u00b5ikjk pik\n\u2264 \u2211 u(i,i[4])(j, j[4]) 5\u220f k=2 \u00b5ikjk 1 n \u2212 1 M1\n= ( 1\n1 n \u2212 1 M1\n)4 \u00b7 \u2211 u(i,i[4])(j, j[4]) 5\u220f k=2 \u00b5ikjk ,\nand analogously\n\u2211 (i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j\u2032, j[4]) 5\u220f k=2 \u03c0ikjk\n\u2265 \u2211 u(i,i[4])(j\u2032, j[4]) 5\u220f k=2 \u00b5ikjk 1 n + 1 M1\n= ( 1\n1 n + 1 M1\n)4 \u00b7 \u2211 u(i,i[4])(j\u2032, j[4]) 5\u220f k=2 \u00b5ikjk .\nObserve the connection to the previously defined function \u03c8 here: \u2211\n(i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j, j[4]) 5\u220f k=2 \u00b5ikjk = \u2207ij \u03c8(\u00b5) ,\nand \u2211 (i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\nu(i,i[4])(j\u2032, j[4]) 5\u220f k=2 \u00b5ikjk = \u2207ij\u2032 \u03c8(\u00b5) .\nInserting the bounds into (19) yields( 1 1 n \u2212 1 M1 )4 \u2207ij \u03c8(\u00b5) > \u03f5+ ( 1 1 n + 1 M1 )4 \u2207ij\u2032 \u03c8(\u00b5)\n= \u03f5+ ( 1\n1 n \u2212 1 M1\n)4 \u2207ij\u2032 \u03c8(\u00b5)\n\u2212 [( 1\n1 n \u2212 1 M1\n)4 \u2212 ( 1\n1 n + 1 M1\n)4] \u00b7 \u2207ij\u2032 \u03c8(\u00b5)\n= \u03f5+ ( 1\n1 n \u2212 1 M1\n)4 \u2207ij\u2032 \u03c8(\u00b5)\n\u2212 n4M41 ( 1 (M1 \u2212 n)4 \u2212 1 (M1 + n)4 ) \ufe38 \ufe37\ufe37 \ufe38\nDenote this term as (\u2020)\n\u00b7\u2207ij\u2032 \u03c8(\u00b5)\n= . . . to be continued under (\u25e6) .\nWe have (\u2020) \u2265 0 as well as\n(\u2020) = (M1 + n) 4 \u2212 (M1 \u2212 n)4\n(M1 \u2212 n)4(M1 + n)4\n= 8M31n+ 8M1n 3\n(M1 \u2212 n)4(M1 + n)4\n\u2264 16M 3 1n\n(M1 \u2212 n)4(M1 + n)4\n\u2264 16M 3 1n\n(M1 \u2212 M12 )4(M1 + M1 2 ) 4\n= 16 34\n28\nM31n\nM81 \u2264 100 n M51 .\nMoreover, we have 0 \u2264 \u2207ij\u2032 \u03c8(\u00b5) \u2264 \u2211\n(i[4],j[4]):\n\u03b7(i[4])=4 ,i/\u2208i[4]\n1 \u00b7 1 \u2264 (nm)4 .\nTherefore, we can continue the inequality chain at (\u25e6) with (\u25e6) \u2265 \u03f5+ ( 1\n1 n \u2212 1 M1\n)4 \u2207ij\u2032 \u03c8(\u00b5)\u2212 n4M41 \u00b7 100 n\nM51 \u00b7 (nm)4\n= \u03f5+ ( 1\n1 n \u2212 1 M1\n)4 \u2207ij\u2032 \u03c8(\u00b5)\u2212 100 n9m4\nM1 = \u03f5+ ( 1\n1 n \u2212 1 M1\n)4 \u2207ij\u2032 \u03c8(\u00b5)\u2212 \u03f5\n2 ,\nwhere we inserted for the value of M1. All in all, we derived( 1 1 n \u2212 1 M1 )4 \u2207ij \u03c8(\u00b5) > ( 1 1 n \u2212 1 M1 )4 \u2207ij\u2032 \u03c8(\u00b5) + \u03f5 2 .\nThus, we can conclude with our previous observations about the de se utility in \u0393 that\nEUCDT,GT(aij | \u00b5, I) = M2 5 \u00b7 \u2207ij \u03d5(\u00b5) + 1 5 \u2207ij \u03c8(\u00b5)\n= M2 5 \u00b7 5(1\u2212 pi)4 + 1 5 \u2207ij \u03c8(\u00b5)\n= M2 5 \u00b7 \u2207ij\u2032 \u03d5(\u00b5) + 1 5 \u2207ij \u03c8(\u00b5)\n> M2 5 \u00b7 \u2207ij\u2032 \u03d5(\u00b5) + 1 5 \u00b7 [ \u2207ij\u2032 \u03c8(\u00b5) + ( 1 n \u2212 1 M1 )4 \u00b7 \u03f5 2 ] = M2 5 \u00b7 \u2207ij\u2032 \u03d5(\u00b5) + 1 5 \u2207ij\u2032 \u03c8(\u00b5) + \u03b41 = EUCDT,GT(aij\u2032 | \u00b5, I) + \u03b41 .\nHence, we derived that an action (i, j\u2032) of \u0393 has positive play probability\n\u00b5ij\u2032 = \u03c0ij\u2032 \u00b7 pi \u2265 \u03c0ij\u2032 \u00b7 ( 1 n \u2212 1 M2 ) > 0 ,\nunder \u00b5 while simultaneously being more than \u03b41 dominated by another action (i, j). This contradicts \u00b5 being a \u03b41-wellsupported (CDT,GT)-equilibrium. The contradiction completes our proof that \u03c0 is an \u03f5-Nash equilibrium.\nH.5 Proof of Theorem 3 Theorem. The following problems are all NP-hard. Unless NP = ZPP, there is also no FPTAS for these problems. (1a.) Given \u0393 and t \u2208 Q, is there a (CDT,GT)-equilibrium of \u0393 with ex-ante utility \u2265 t? (1b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a (CDT,GT)-equilibrium \u00b5 such that Fr(I | \u00b5) > 0, and such that the player has a (CDT,GT)-expected utility \u2265 t upon reaching I? (1c.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a strategy \u00b5 of \u0393 such that Fr(I | \u00b5) > 0, and such that the player has a (CDT,GT)-expected utility \u2265 t upon reaching I? (2a.) Given \u0393 and t \u2208 Q, is there an (EDT,GDH)-equilibrium of \u0393 with ex-ante utility \u2265 t?\n(2b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there an (EDT,GDH)-equilibrium \u00b5 such that P(I | \u00b5) > 0, and such that the player has an (EDT,GDH)-expected utility \u2265 t upon reaching I? (2c.) Given \u0393, an info set I of \u0393 and t \u2208 Q, is there a strategy \u00b5 of \u0393 such that P(I | \u00b5) > 0, and such that the player has an (EDT,GDH)-expected utility \u2265 t upon reaching I? (3a.) Given \u0393 and t \u2208 Q, do all (EDT,GDH)-equilibria of \u0393 have ex-ante utility \u2265 t? (3b.) Given \u0393, an info set I of \u0393 and t \u2208 Q, do all (EDT,GDH)-equilibria \u00b5 with P(I | \u00b5) > 0 yield the player an (EDT,GDH)-expected utility \u2265 t upon reaching I?\nWe reduce all those decision problems from the problem in Proposition 4, which we will henceforth call EXANTEOPTD.\nFirst note that any single-player extensive-form game with imperfect recall \u0393 has an ex-ante optimal strategy since the maximization problem (1) is about a continuous polynomial function over a compact domain. Moreover, observe that (\u0393, t) is a yes instance for EXANTEOPT-D (by definition) if and only if there is a strategy \u00b5 in \u0393 with U(\u00b5) \u2265 t if and only if there is an ex-ante optimal strategy \u00b5\u2217 in \u0393 with U(\u00b5\u2217) \u2265 t.\n(1a.) and (2a.): Let (\u0393, t) be an instance for EXANTEOPT-D. Without transforming, we can choose (\u0393, t) as the instance to the problems (1a.) and (2a.). Then, it will be a yes instance of (- a.) if and only if there is a (CDT,GT)-equilbrium or, resp., (EDT,GDH)-equilibrium in \u0393 with ex-ante utility \u2265 t if and only if (by Lemma 17) there is an ex-ante optimal strategy in \u0393 with with ex-ante utility \u2265 t if and only if (by the comment above) (\u0393, t) is a yes instance for EXANTEOPT-D.\n(1b.), (1c.), (2b.), and (2c.): Let (\u0393, t) be an instance for EXANTEOPT-D. Let us refer to the root of \u0393 with h0. We construct a new game \u0393\u2032 by adding a new artificial game start: Copy the game tree of \u0393. Add a new node h\u22121 to it which shall represent root of \u0393\u2032. Connect h\u22121 to h0 by one edge, called action a\u22121, and assign h\u22121 to a new info set I\u22121 that is added to I\u2217. The corresponding instance shall be (\u0393\u2032, I\u22121, t).\nObserve that any strategy \u00b5 for \u0393 corresponds to a strategy \u00b5\u2032 in \u0393\u2032 that behaves like \u00b5 at info sets I that were inherited from \u0393 and that takes the only viable action a\u22121 at info set I\u22121 with a 100% certainty. Moreover, info set I\u22121 has a visit frequency and reach probability of 1 for any strategy \u00b5\u2032 of \u0393\u2032 because they occur in the history of any terminal node exactly once in the beginning. Most crucially, we also get that the (CDT,GT)-expected utility (resp. (EDT,GDH)-expected utility) of \u00b5\u2032 upon reaching info set I\u22121 is equal to the exante expected utility of \u00b5\u2032 in \u0393\u2032 and of corresponding strategy \u00b5 in \u0393.\nWe obtain: \u00b7 (\u0393\u2032, I\u22121, t) is a yes instance for problem (1b.) (resp. (2b.)) =\u21d2 (\u0393\u2032, I\u22121, t) it is a yes instance for problem (1c.) (resp.\n(2c.)) =\u21d2 there is a strategy \u00b5\u2032 in \u0393\u2032 with (CDT,GT)-expected\nutility (resp. (EDT,GDH)-expected utility) \u2265 t upon reaching I\u22121\n=\u21d2 corresponding strategy \u00b5 in \u0393 has ex-ante utility \u2265 t =\u21d2 (\u0393, t) is a yes instance for EXANTEOPT-D.\nand for the other direction:\n\u00b7 (\u0393, t) is a yes instance for EXANTEOPT-D =\u21d2 there exists an ex-ante optimal strategy \u00b5\u2217 of (\u0393, t) with ex-ante utility \u2265 t =\u21d2 corresponding strategy (\u00b5\u2217)\u2032 in \u0393\u2032 is ex-ante optimal\nfor \u0393\u2032, and it has (CDT,GT)-expected utility (resp. (EDT,GDH)-expected utility) \u2265 t upon reaching I\u22121\n=\u21d2 there exists a (CDT,GT)-equilibrium in \u0393\u2032 (resp. (EDT,GDH)-equilibrium) with (CDT,GT)-expected utility (resp. (EDT,GDH)-expected utility) \u2265 t upon reaching I\u22121 =\u21d2 (\u0393\u2032, I\u22121, t) is a yes instance for problem (1b.) (resp. (2b.))\n=\u21d2 (\u0393\u2032, I\u22121, t) is a yes instance for problem (1c.) (resp. (2c.)).\nwhich completes the reduction. Note that G\u2032 has one additional info set and tree depth in comparison to G.\n(3a.) and (3b.): For (3a.), consider the reductions for (2a.) again and assume the starting instance (\u0393, t) of EXANTEOPT-D has one info set only. By Proposition 4 we know that even with such instances EXANTEOPT-D is NP-hard and conditionally inapproximable. But an (EDT,GDH)-equilibrium of such an instance must be an ex-ante optimal strategy (seen by Lemma 15). Therefore, each (EDT,GDH)-equilibrium of \u0393 must have the same (ex-ante optimal) value in ex-ante utility. Thus, deciding whether all (EDT,GDH)-equilibrium exceeds a target ex-ante utility (problem (3a.)) coincides with deciding whether one (EDT,GDH)-equilibrium does that (problem (2a.)).\nAn analogous argument holds for (3b.) by considering the reductions for (2b.) for starting instances (\u0393, t) with only one info set. There, we observe that (EDT,GDH)-equilibria of \u0393\u2032 correspond exactly to the ex-ante optimal strategies in \u0393, and must therefore all promise the same (EDT,GDH)-expected utility upon reaching I\u22121."
        }
    ],
    "title": "The Computational Complexity of Single-Player Imperfect-Recall Games\u2217",
    "year": 2023
}