{
    "abstractText": "Machine learning has made significant advances in the field of image processing. The foundation of this success is supervised learning, which necessitates annotated labels generated by humans and hence learns from labelled data, whereas unsupervised learning learns from unlabeled data. Self-supervised learning (SSL) is a type of un-supervised learning that helps in the performance of downstream computer vision tasks such as object detection, image comprehension, image segmentation, and so on. It can develop generic artificial intelligence systems at a low cost using unstructured and unlabeled data. The authors of this review article have presented detailed literature on self-supervised learning as well as its applications in different domains. The primary goal of this review article is to demonstrate how images learn from their visual features using self-supervised approaches. The authors have also discussed various terms used in self-supervised learning as well as different types of learning, such as contrastive learning, transfer learning, and so on. This review article describes in detail the pipeline of self-supervised learning, including its two main phases: pretext and downstream tasks. The authors have shed light on various challenges encountered while working on self-supervised learning at the end of the article.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ajay Mittal2 \u00b7 Krishan Kumar"
        }
    ],
    "id": "SP:99b6d99e6826f55260b5fb1f82eeaf341f613485",
    "references": [
        {
            "authors": [
                "S Albelwi"
            ],
            "title": "Survey on self-supervised learning: auxiliary pretext tasks and contrastive learning methods in imaging. Entropy 24(4):551",
            "venue": "https:// doi. org/ 10. 3390/",
            "year": 2022
        },
        {
            "authors": [
                "K Ohri",
                "M Kumar"
            ],
            "title": "Review on self-supervised image recognition using deep neural networks. Knowl-Based Syst 224:7090",
            "venue": "https:// doi. org/ 10. 1016/j. knosys",
            "year": 2021
        },
        {
            "authors": [
                "Orhan AE",
                "VV Gupta",
                "BM Lake"
            ],
            "title": "Self-supervised learning through the eues of a child 2020",
            "venue": "arXiv e-prints,",
            "year": 2007
        },
        {
            "authors": [
                "L Tao",
                "X Wang",
                "T Yamasaki"
            ],
            "title": "An improved inter-intra contrastive learning framework on self-supervised video representation",
            "venue": "IEEE Trans Circ Syst Video Technol. https:// doi",
            "year": 2022
        },
        {
            "authors": [
                "A Jaiswal",
                "AR Babu",
                "MZ Zadeh",
                "D Banerjee",
                "F Makedon"
            ],
            "title": "A survey on contrastive self-supervised learning. Technologies 9(1):2",
            "venue": "https:// doi. org/",
            "year": 2020
        },
        {
            "authors": [
                "D Pathak",
                "P Kr\u00e4henb\u00fchl",
                "J Donahue",
                "T Darrell",
                "AA Efros"
            ],
            "title": "Context encoders: feature learning by inpainting",
            "venue": "IEEE Conf Comput Vis Pattern Recogn 2016:2536\u20132544. https:// doi. org/",
            "year": 2016
        },
        {
            "authors": [
                "G Larsson",
                "M Maire",
                "G Shakhnarovich"
            ],
            "title": "Learning representations for automatic colorization",
            "venue": "Vision\u2014ECCV",
            "year": 2016
        },
        {
            "authors": [
                "M Alloghani",
                "D Al-Jumeily",
                "J Mustafina",
                "A Hussain",
                "AJ Aljaaf"
            ],
            "title": "A systematic review on supervised and unsupervised machine learning algorithms for Data Science. In: Unsupervised and Semi-Supervised Learning, pp 3\u201321",
            "venue": "https:// doi",
            "year": 2019
        },
        {
            "authors": [
                "JEV Engelen",
                "HH Hoos"
            ],
            "title": "A survey on semi-supervised learning. Mach Learn 109(2):373\u2013440",
            "venue": "https:// doi. org/ 10",
            "year": 2019
        },
        {
            "authors": [
                "ZH Zhou"
            ],
            "title": "A brief introduction to weakly supervised learning",
            "venue": "Natl Sci Rev 5(1):44\u201353. https:// doi. org/",
            "year": 2017
        },
        {
            "authors": [
                "IZ Yalniz",
                "H J\u00e9gou",
                "K Chen",
                "M Paluri",
                "D Mahajan"
            ],
            "title": "Billionscale semi-supervised learning for image classification",
            "venue": "Comput Vis Pattern Recogn. arXivpreprint arXiv:",
            "year": 2019
        },
        {
            "authors": [
                "MA Samsuden",
                "NM Diah",
                "NA Rahman"
            ],
            "title": "A review paper on implementing reinforcement learning technique in optimising games performance",
            "venue": "https:// doi. org/",
            "year": 2019
        },
        {
            "authors": [
                "X Xin",
                "A Karatzoglou",
                "I Arapakis",
                "JM Jose"
            ],
            "title": "Self-supervised reinforcement learning for recommender systems",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Y Luo",
                "L Yin",
                "W Bai",
                "K Mao"
            ],
            "title": "An appraisal of incremental learning methods. Entropy (Basel, Switzerland) 22(11):1190",
            "venue": "https:// doi. org/ 10. 3390/",
            "year": 2020
        },
        {
            "authors": [
                "K Weiss",
                "TM Khoshgoftaar",
                "D Wang"
            ],
            "title": "2016) A survey of transfer learning",
            "venue": "J Big Data 3:9. https:// doi. org/ 10",
            "year": 2016
        },
        {
            "authors": [
                "SJ Pan",
                "Q Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Trans Knowl Data Eng 22(10):1345\u20131359. https:// doi. org/",
            "year": 2010
        },
        {
            "authors": [
                "R Zhang",
                "Isola P",
                "AA Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "M (eds) Computer Vision\u2013 ECCV 2016. ECCV. Lecture Notes in Computer Science,",
            "year": 2016
        },
        {
            "authors": [
                "C Doersch",
                "A Gupta",
                "AA Efros"
            ],
            "title": "Unsupervised visual representation learning by context prediction",
            "venue": "IEEE International Conference on Computer Vision (ICCV). https:// doi",
            "year": 2015
        },
        {
            "authors": [
                "S Treneska",
                "E Zdravevski",
                "IM Pires",
                "P Lameski",
                "S Gievska"
            ],
            "title": "Gan-based image colorization for self-supervised visual feature learning. Sensors 22(4):1599",
            "venue": "https:// doi. org/",
            "year": 2022
        },
        {
            "authors": [
                "Z Feng",
                "C Xu",
                "D Tao"
            ],
            "title": "Self-supervised representation learning by rotation feature decoupling",
            "year": 2019
        },
        {
            "authors": [
                "S Devgon",
                "J Ichnowski",
                "A Balakrishna",
                "H Zhang",
                "K Goldberg"
            ],
            "title": "Orienting novel 3D objects using self-supervised learning of rotation transforms",
            "venue": "IEEE 16th International Conference on Automation Science and Engineering (CASE),",
            "year": 2020
        },
        {
            "authors": [
                "M Noroozi",
                "P Favaro"
            ],
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "venue": "European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "FA Breiki",
                "M Ridzuan",
                "R Grandhe"
            ],
            "title": "Self-supervised learning for fine-grained image classification",
            "venue": "arXiv preprint arXiv:",
            "year": 2021
        },
        {
            "authors": [
                "R Li",
                "S Liu",
                "G Wang",
                "G Liu",
                "B Zeng"
            ],
            "title": "Jigsawgan: auxiliary learning for solving jigsaw puzzles with generative adversarial networks. IEEE Trans Image Process 31:513\u2013524",
            "venue": "https:// doi",
            "year": 2022
        },
        {
            "authors": [
                "DP Isravel",
                "S Silas",
                "EB Rajsingh"
            ],
            "title": "Self-supervised learning approaches for traffic engineering in software-defined networks. In: Advances in Intelligent Systems and Computing, pp 511\u2013522",
            "venue": "https:// doi",
            "year": 2021
        },
        {
            "authors": [
                "A Krizhevsky",
                "I Sutskever",
                "GE Hinton"
            ],
            "title": "ImageNet classification with deep convolutional Neural Networks",
            "venue": "Commun ACM 60(6):84\u201390. https:// doi",
            "year": 2017
        },
        {
            "authors": [
                "L Jing",
                "Y Tian"
            ],
            "title": "Self-supervised visual feature learning with deep neural networks: a survey",
            "venue": "IEEE Trans Pattern Anal Mach Intell 43(11):4037\u20134058. https:// doi",
            "year": 2021
        },
        {
            "authors": [
                "R Girshick",
                "J Donahue",
                "T Darrell",
                "J Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition. https:// doi. org/",
            "year": 2014
        },
        {
            "authors": [
                "J Redmon",
                "S Divvala",
                "R Girshick",
                "Farhadi"
            ],
            "title": "A (2016) You only look once: unified, real-time object detection",
            "venue": "IEEE Conf Comput Vis Pattern Recogn 2016:779\u2013788. https:// doi. org/",
            "year": 2016
        },
        {
            "authors": [
                "H Haresamudram",
                "I Essa",
                "T Pl\u00f6tz"
            ],
            "title": "Assessing the state of self-supervised human activity recognition using wearables",
            "venue": "arXiv preprint arXiv:",
            "year": 2022
        },
        {
            "authors": [
                "W You",
                "X Wang"
            ],
            "title": "View enhanced jigsaw puzzle for selfsupervised feature learning in 3D human action recognition. IEEE Access 10:36385\u201336396",
            "venue": "https:// doi. org/ 10",
            "year": 2022
        },
        {
            "authors": [
                "A Bhattacharjee",
                "M Karami",
                "H Liu"
            ],
            "title": "Text transformations in contrastive self-supervised learning: a review",
            "venue": "arXiv preprint arXiv:",
            "year": 2022
        },
        {
            "authors": [
                "K He",
                "H Fan",
                "F Wu",
                "S Xie",
                "R Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "IEEE/CVF Conf Comput Vis Pattern Recogn 2020:9726\u20139735. https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "M Zhou",
                "Z Li",
                "P Xie"
            ],
            "title": "Self-supervised regularization for text classification",
            "venue": "Trans Assoc Comput Linguist 9:641\u2013656. https:// doi. org/",
            "year": 2021
        },
        {
            "authors": [
                "T Chen",
                "S Liu",
                "S Chang",
                "Y Cheng",
                "L Amini",
                "Z Wang"
            ],
            "title": "Adversarial robustness: from self-supervised pre-training to fine-tuning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "T Chen",
                "Kornblith",
                "M Norouzi",
                "G Hinton"
            ],
            "title": "2020b) A simple framework for contrastive learning of visual representations",
            "venue": "arXiv preprint arXiv:2002",
            "year": 2002
        },
        {
            "authors": [
                "S Gururangan",
                "A Marasovi\u0107",
                "S Swayamdipta",
                "K Lo",
                "I Beltagy",
                "D Downey",
                "NA Smith"
            ],
            "title": "Don\u2019t stop pretraining: adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https:// doi. org/",
            "year": 2020
        },
        {
            "authors": [
                "Y Sun",
                "S Wang",
                "Y Li",
                "S Feng",
                "H Tian",
                "H Wu",
                "H Wang"
            ],
            "title": "Ernie 2.0: a continual pre-training framework for language understanding",
            "venue": "Proc AAAI Conf Artifi Intell 34(05):8968\u20138975. https:// doi. org/",
            "year": 2020
        },
        {
            "authors": [
                "Z Lan",
                "M Chen",
                "S Goodman",
                "K Gimpel",
                "P Sharma",
                "R Soricut"
            ],
            "title": "Albert: a lite bert for self-supervised learning of language representations",
            "year": 2019
        },
        {
            "authors": [
                "A Chowdhury",
                "J Rosenthal",
                "J Waring",
                "R Umeton"
            ],
            "title": "Applying self-supervised learning to medicine: review of the State of the art and medical implementations. Informatics 8(3):59",
            "venue": "https:// doi. org/",
            "year": 2021
        },
        {
            "authors": [
                "A Jamaludin",
                "T Kadir",
                "Zisserman"
            ],
            "title": "A (2017) Self-supervised learning for Spinal Mris. In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp 294\u2013302",
            "venue": "https:// doi",
            "year": 2017
        },
        {
            "authors": [
                "L Chen",
                "P Bentley",
                "K Mori",
                "K Misawa",
                "M Fujiwara",
                "D Rueckert"
            ],
            "title": "Self-supervised learning for medical image analysis using image context restoration. Med Image Anal 58:101539",
            "venue": "https:// doi. org/",
            "year": 2019
        },
        {
            "authors": [
                "A Kwasigroch",
                "M Grochowski"
            ],
            "title": "Miko\u0142ajczyk A (2020) Self-supervised learning to increase the performance of skin lesion classification. Electronics 9(11):1930",
            "venue": "https:// doi. org/",
            "year": 1930
        },
        {
            "authors": [
                "FC Ghesu",
                "B Georgescu",
                "A Mansoor",
                "Y Yoo",
                "D Neumann",
                "P Patel",
                "D Comaniciu"
            ],
            "title": "Self-supervised Learning from 100 Million Medical Images",
            "venue": "arXiv preprint arXiv:",
            "year": 2022
        },
        {
            "authors": [
                "D Spathis",
                "I Perez-Pozuelo",
                "L Marques-Fernandez",
                "C Mascolo"
            ],
            "title": "Breaking away from labels: the promise of self-supervised machine learning in intelligent health. Patterns 3(2):1410",
            "venue": "https:// doi. org/ 10. 1016/j. patter",
            "year": 2022
        },
        {
            "authors": [
                "XB Nguyen",
                "GS Lee",
                "SH Kim",
                "HJ Yang"
            ],
            "title": "Self-supervised learning based on spatial awareness for medical image analysis",
            "venue": "IEEE Access 8:162973\u2013162981. https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "S Gidaris",
                "P Singh",
                "N Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:",
            "year": 2018
        },
        {
            "authors": [
                "G Huang",
                "I Laradji",
                "D Vazquez",
                "S Lacoste-Julien",
                "P Rodriguez"
            ],
            "title": "A survey of self-supervised and few-shot object detection",
            "venue": "arXiv preprint arXiv:",
            "year": 2021
        },
        {
            "authors": [
                "S Liu",
                "Z Li",
                "J Sun"
            ],
            "title": "Self-EMD: self-supervised object detection without imagenet",
            "venue": "arXiv preprint arXiv:",
            "year": 2020
        },
        {
            "authors": [
                "E Amrani",
                "R Ben-Ari",
                "I Shapira",
                "T Hakim",
                "Bronstein"
            ],
            "title": "A (2020) Self-supervised object detection and retrieval using unlabeled videos",
            "year": 2020
        },
        {
            "authors": [
                "W Lee",
                "J Na",
                "G Kim"
            ],
            "title": "Multi-task self-supervised object detection via recycling of bounding box annotations",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https:// doi",
            "year": 2019
        },
        {
            "authors": [
                "K Yun",
                "J Park",
                "J Cho"
            ],
            "title": "Robust human pose estimation for rotation via self-supervised learning",
            "venue": "IEEE Access",
            "year": 2020
        },
        {
            "authors": [
                "D Pototzky",
                "A Sultan",
                "M Kirschner",
                "L Schmidt-Thieme"
            ],
            "title": "Self-supervised learning for object detection in autonomous driving",
            "venue": "Lecture Notes in Computer Science,",
            "year": 2021
        },
        {
            "authors": [
                "Y Jain",
                "CI Tang",
                "C Min",
                "F Kawsar",
                "Mathur"
            ],
            "title": "A (2022) ColloSSL: collaborative self-supervised learning for human activity recognition",
            "venue": "Proc ACM Interact Mob Wearable Ubiquitous Technol",
            "year": 2022
        },
        {
            "authors": [
                "A Ziegler",
                "YM Asano"
            ],
            "title": "Self-supervised learning of object parts for semantic segmentation",
            "venue": "arXiv preprint arXiv:",
            "year": 2022
        },
        {
            "authors": [
                "J Ding",
                "E Xie",
                "H Xu",
                "C Jiang",
                "Z Li",
                "P Luo",
                "G-S Xia"
            ],
            "title": "Deeply unsupervised patch re-identification for pre-training object detectors",
            "venue": "IEEE Trans Pattern Anal Mach Intell. https:// doi",
            "year": 2022
        },
        {
            "authors": [
                "F Taherkhani",
                "A Dabouei",
                "S Soleymani",
                "J Dawson",
                "NM Nasrabadi"
            ],
            "title": "Self-supervised Wasserstein pseudo-labeling for semi-supervised image classification",
            "venue": "https:// doi",
            "year": 2021
        },
        {
            "authors": [
                "J Ramapuram",
                "D Busbridge",
                "R Webb"
            ],
            "title": "Evaluating the fairness of fine-tuning strategies in self-supervised learning. arXiv preprint arXiv: 2110",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\nKeywords Contrastive learning\u00a0\u00b7 Machine learning\u00a0\u00b7 Self-supervised\u00a0\u00b7 Supervised learning\u00a0\u00b7 Un-supervised learning"
        },
        {
            "heading": "1 Introduction",
            "text": "Machine learning (ML) and deep learning (DL) algorithms have revealed incredible performance in computer vision applications such as image recognition, object detection, image segmentation, and so on. These models are trained using either labeled data or un-labeled data. Supervised learning works on labeled data and unsupervised learning works on un-labeled data. Manually labeling data is a time-consuming and labor-intensive process\n[1]. Self-Supervised Learning (SSL) is a solution to the aforementioned issues that has emerged as one of the most promising techniques that does not necessitate any expensive manual annotations. The term \"self-supervised learning\" was first used in robotics, where labels were automatically assigned to training data to exploit the relationships between input signals and sensors. The basic idea behind SSL's operation is that while providing input, some parts are hidden, and the visible parts are used to predict the hidden parts. SSL differs from unsupervised learning in that it requires labels but does not require human labeling [2]. The fundamental process of SSL is depicted in Fig.\u00a01, with a large unlabeled corpus of images as input. The ConvNet model is trained to predict the image's hidden portion based on the visible portion. The concept of self-supervised learning is inspired by the way infants teach [3]. Infants always learn through observation, common sense, their surroundings, and little interaction. All these factors contribute to his or her ability to self-learn. An infant's surroundings become a source of supervision for him, which helps in his understanding of how things work without constant supervision. The same idea is replicated in the machine via SSL, where the data supervises itself for training the model rather than having annotated labels, instructing the network on what is right or wrong. SSL employs two main concepts: auxiliary pretext tasks and contrastive learning [4]. In pretext tasks, * Munish Kumar munishcse@gmail.com Veenu Rani veenugoyal7@gmail.com Syed Tufael Nabi ertufail32@gmail.com Ajay Mittal ajaymittal@pu.ac.in Krishan Kumar k.saluja@pu.ac.in 1 Department of\u00a0Computational Sciences, Maharaja Ranjit Singh Punjab Technical University, Bathinda, Punjab, India\n2 University Institute of\u00a0Engineering and\u00a0Technology, Panjab University, Chandigarh, India\n1 3\npseudo labels are used for representations that were generated automatically by taking into account the attributes of the dataset. These pseudo labels are then used for classification, detection, and segmentation [5]. The auxiliary pretext is primarily used to fill in missing parts of an image [6], convert it to gray scale [7], predict hidden parts, and many other tasks. Contrastive learning, on the other hand, distinguishes between augmented image features. For example, in one image, a close-up view is captured, while in another, a distant view is captured. Considering the differences in perspectives between the two helps the model in learning. Before being integrated into computer vision, SSL had its origins in the NLP (natural language processing domain. Bidirectional Encoder Representations from Transformers (BERT is the most widely used SSL method in NLP. Neighbour sentence prediction, Neighbor word prediction, auto-regressive language modeling, and other NLP methods are used for SSL. As we all know, shuffling a single word in NLP can change the semantics of a sentence,the method described above can help with this."
        },
        {
            "heading": "2 Motivation Behind the\u00a0Work",
            "text": "One important distinction between humans and machines is that humans can learn faster than machines by observing and sensing their surroundings. Machines, on the other hand, can take hours or even days to simulate. A large dataset is required to train machines to predict. Most artificial intelligence techniques require labelled datasets to make predictions. Labeling each data point manually or with data labelling software is a costly and time-consuming task. Unlabeled data, on the other hand, is available in plenty and easily. Thus, the motivation behind SSL is to learn useful depictions of the data from unlabeled data by making use of the self-supervision concept\nand then fine-tune these depictions with some labels for the supervised downstream task. These downstream tasks may range from simple to complex, like from image classification to semantic segmentation and object detection, etc. Therefore, there was a need for an hour to thoroughly discuss the SSL in detail. This article aims to provide a detailed illustration of SSL with applications in various areas, along with the literature review. The latter section also discusses the benefits and drawbacks of SSL. Also, the critical analysis along with future directions has been provided to make researchers familiar with the future research scope of SSL.\nThe article is organized as follows: Sect.\u00a01 gives a brief introduction to SSL; Sect.\u00a02 discusses the motivation for this work; and Sect.\u00a03 discusses various learning algorithms. Section\u00a04 discusses in detail the SSL algorithm, it\u2019s types, its learning tasks, the SSL applications and related work and the various terms used in the SSL paradigm. Section\u00a05 discusses about various SSL datasets used in the medical domain and computer vision. Section\u00a06 summarizes the critical analysis of the literature review, and at the end, Sect.\u00a07 concludes the article by discussing the domain's future prospects."
        },
        {
            "heading": "3 Learning Algorithms",
            "text": ""
        },
        {
            "heading": "3.1 Supervised Learning",
            "text": "Supervised learning necessitates manual labelling of data, which slows the system's performance [8]. Supervised learning attempts to map input variables to output variables. Supervised learning is similar to a classroom setting in which a teacher is present to guide the students [2]. These models are created for a specific task and contain a large\namount of manually annotated data. This data is divided into three categories at random: training data, testing data, and validation data. The success of computer vision programs is dependent on this annotated data, which is a time-consuming and expensive process to acquire. Figure\u00a02 depicts the operation of supervised learning."
        },
        {
            "heading": "3.2 Semi\u2011supervised Learning",
            "text": "Semi-supervised learning uses both labelled and unlabeled data to perform specific tasks [9]. First, some of the systems are trained using manually labelled data, and then the system is used to predict the remaining portion using unlabeled data, as shown in Fig.\u00a03. Finally, a full dataset containing both labelled and pseudo-labeled datasets is used to train the network.\nThe goal of semi-supervised learning is to combine the benefits of both supervised and unsupervised learning techniques. Semi-supervised learning algorithms' main goal is to use unlabeled data to build a reliable model. These algorithms do not guarantee that including only unlabeled data will improve prediction performance because unlabeled data is only useful when it contains information useful for label prediction."
        },
        {
            "heading": "3.3 Weakly\u2011Supervised Learning",
            "text": "Weakly-supervised learning is the process of learning from noisy or poorly labelled data. Because of the high cost of manually labelling data, it is difficult to obtain strong supervision information. In this scenario, it is necessary to build machines that can operate with minimal supervision. Weak supervision is classified into three types: incomplete supervision, inexact supervision, and inaccurate supervision [10]. Incomplete supervision is one of the, in which only a portion of the training data is labeled, and the rest is left unlabeled. The second type of supervision is inexact supervision, which contains only coarse-grained labels, and the last type is inaccurate supervision, in which the given labels do not depict\nthe truth. Instagram is an example of weakly-supervised learning, in which users' hash tags are used to label data [2]."
        },
        {
            "heading": "3.4 Semi\u2011weak Supervised Learning",
            "text": "The combination of semi-supervised and weakly-supervised learning techniques is known as semi-weak supervised learning [11]. It adheres to the \"student\u2013teacher\" framework, in which a weakly supervised dataset is first trained with noisy hash tags, referred to as the teacher model. This teacher model is further refined using an ImageNet labelled dataset, and the refined labels are used to train the target student model."
        },
        {
            "heading": "3.5 Unsupervised Learning",
            "text": "Unsupervised learning, on the other hand, seeks to discover implicit patterns in data that has not been labeled. Manual annotations are not required for unsupervised learning. Unsupervised learning is done through clustering. Unsupervised learning is the process of teaching a computer to operate on unlabeled data and allowing the algorithm to act on it without supervision. The machine's task is to sort unsorted data into groups based on similarities, patterns, and differences, as illustrated in Fig.\u00a04.\n1 3"
        },
        {
            "heading": "3.6 Reinforcement Learning (RL)",
            "text": "Reinforcement learning is a subset of machine learning in which an agent learns from trial-and-error feedback [12]. Feedback can be either a punishment or a reward. A variety of software and computers use it to determine the best possible solution in a given situation. The agent decides what to do with the given task in reinforcement learning. It is based on previous experience. Xin et\u00a0al. [13] proposed selfsupervised and reinforcement learning for sequential recommendation tasks. Their proposed model includes two output layers: one for self-supervised learning and one for reinforcement learning. The RL component acts as a regularize,\ndirecting the supervised layer's attention to specific rewards. Figure\u00a05 depicts the operation of RL."
        },
        {
            "heading": "3.7 Increment Learning (IL)",
            "text": "As illustrated in Fig.\u00a06, the goal of incremental learning is to learn new knowledge from new samples [14] and solve new tasks on a continuous basis without forgetting previous tasks by using new data. IL is a subset of machine learning technology that can handle more consistent applications with human behavior and thought. When learning with new knowledge, a back propagation method adjusts parameter weights based on losses on available sequential data. The model's performance on previously learned knowledge will suffer as a result. This is referred to as catastrophic forgetting (CF), and it is the primary issue with incremental learning."
        },
        {
            "heading": "3.8 Transfer Learning",
            "text": "Transfer learning is used to help learners improve their knowledge. Transfer learning refers to the transfer of knowledge from one domain to another [15]. For example, two people want to learn how to play the guitar. One has no musical knowledge, while the other has extensive musical knowledge. A person with a musical background\n1 3\nwill be able to learn guitar more quickly by applying prior knowledge to a new domain [16]. The basic idea behind using transfer learning is to transfer the information contained in a trained model on a task with a large amount of data to an objective task with less data.\nOne of the most important learning algorithms is selfsupervised learning (SSL) which has been discussed in the upcoming section."
        },
        {
            "heading": "4 Self\u2011Supervised Learning (SSL)",
            "text": "SSL is considered to be the bridge between supervised learning and unsupervised learning. The SSL model trains itself using one part of the input data to learn the other part of the input data. This is also called predictive or pretext learning. The SSL algorithm has the ability to auto-generate the labels for un-labelled data, which converts the un-supervised model to a supervised model. Figure\u00a07 shows the block diagram of the SSL algorithm."
        },
        {
            "heading": "4.1 Self\u2011Supervised Learning (SSL) Tasks",
            "text": "SSL is a new approach that differs from other techniques. The main distinction between them is that SSL does not\nrequire manual labeling. SSL tasks are divided into two categories: pretext and downstream tasks. The former employs supervised learning to learn representations, with labels generated from the data itself. When this learning is complete, the model applies the previously learned representations to the subsequent tasks. Figure\u00a08a, b depict various tasks performed by pretext and downstream tasks, respectively."
        },
        {
            "heading": "4.1.1 Pretext Task Learning Framework",
            "text": "In pretext tasks, the hidden portion of data is predicted using the visible portion. The pretext task can be applied to any type of data, such as images, audio, video, and so on [5]. Figure\u00a09a\u2013d show examples of pretext tasks such as colorizing an image [17], predicting a missing patch [6], estimating the rotation angel [2], jigsaw puzzle [18], and so on. This task allows machines to learn automatically by obtaining supervision directly from the data, without the use of annotations. Designing an appropriate pretext task necessitates domain knowledge.\n4.1.1.1 Image Colorization Image colorization, as shown in Fig.\u00a09a refers to the process of converting a colour image to a black-and-white image. Each pixel's full-color information is stored by the trained model. It is a pretext for learning vis-\nFig. 7 Block diagram of SSL\nFig. 8 Various tasks performed by a pretext and b downstream\n1 3\nual features. Treneska et\u00a0al. [19] used an image colorization model based on a Generative Adversarial Network (GAN). The GAN model can produce the most realistic results. The extracted knowledge was transferred to two downstream tasks after applying the GAN model: multi-label image classification and semantic segmentation. In multi-label classification, a single image is assigned to multiple classes. The authors used 11,987 training images and 1713 testing images in their study. Semantic labels are assigned to each pixel of an image in semantic segmentation. They conducted experiments using the PascalVOC 2012 dataset.\n4.1.1.2 Predicting a\u00a0Missing Patch A pretext task that can predict the position of an image patch is predicting a missing patch from an image. To predict the relative position of a patch within an image, models should be trained. Doersch et\u00a0al. [18] proposed using SSL to predict a missing patch. They chose a random patch and predicted the relative position of an image's second and central patches, as shown in Fig.\u00a09b. The patches are numbered 1 through 8. Following the selection of a patch, each patch is fed into a CNN that adheres to the AlexNet architecture. Both architectures are completely interconnected and share weights between layers. A gap was added between patches to prevent over-fitting. Softmax was used as a final layer to predict the relative position of each patch configuration.\n4.1.1.3 Estimating the\u00a0Rotation Angle Estimating the rotation is a task requiring instance discrimination [20]. Predicting picture rotations is a simple but effective method for identifying rotation discriminative features. If an image is rotated at any angle (0, 90, 180, or 270 degrees) and fed into a CNN model, the network model is pre-trained on pairs of rotated images from an unlabeled dataset. The network must understand the location, type, and pose of an object\nin an image in order to estimate the rotation angle of the original image. In Fig.\u00a09c, an image is rotated three times, and the CNN model recovers the original image by learning the position of the object. Devgon et\u00a0 al. [21] developed a self-supervised learning model to estimate an object's rotation between the desired rotation and the current rotation. They used a trained model to estimate the rotation between two depth images.\n4.1.1.4 Jigsaw Puzzle Solving a jigsaw puzzle requires not only knowledge of a single patch, but also knowledge of the relationships between different patches of the same image, as shown in Fig.\u00a0 9d [22]. Understanding the patches discriminatory features helps in solving the puzzle. There is no shortcut method for predicting the right position of a patch, so multiple permutation functions are generated for each patch to find its right position [23]. Li et\u00a0al. [24] solved Jigsaw puzzles using the GAN model. They created a multitask pipeline for solving unpaired image jigsaw puzzles. They classified jigsaw permutations into a separate branch and then used a GAN model to recover features from images in the correct order. The classification branch concentrated on the pseudo labels generated by shuffling the image pieces, whereas the GAN model concentrated on the semantic information of the pieces."
        },
        {
            "heading": "4.1.2 Downstream Task",
            "text": "Downstream tasks are primary tasks that define the model's purpose. Pretext tasks, also known as secondary tasks, allow the model to learn useful feature representation information that is used to complete downstream tasks. To ensure quality in downstream tasks, feature representation learned in pretext tasks should be calculated. The primary task downstream is to perform classification or object detection\n1 3\nwith insufficient data labels, semantic segmentation, and action recognition. Down streaming can be accomplished in two ways: fine-tuning or using a linear classifier [25]. To achieve good performance, a small amount of data labeling is required in the downstream task. When the domain gap between self-supervised pre-training and the downstream task is smaller, performance on the downstream task is usually better.\n4.1.2.1 Image Classification Image classification is the process of recognizing the category of each object in an image. Many networks are used for image classification, including AlexNet, ConvNet, ResNet, DenseNet, GoogLeNet, VGG, and others [26]. An image may contain multiple objects of different classes, but only one class label is used for each image. Image classification is used as a downstream task to estimate the quality of an image's features. To extract features from each image, a self-supervised learning model is used, which is then used to train a classifier.\nLiu Object detection is a downstream task that recognizes the category of an object as well as its relative position in an image. This task is extremely important in computer vision applications such as robotics, autonomous driving, scene text detection, and so on [27]. The two most popular datasets for object detection are MOSOCO and OpenImage. Many ConvNet models, such as CNN, R-CNN [28], Fast-RCNN, Fast YOLO, and others, have been proposed to achieve high performance [29].\n4.1.2.2 Semantic Segmentation Semantic segmentation is the process of assigning semantic labels to each pixel in an image. This task is critical in a variety of applications, such as human\u2013machine interaction, robotics, and autonomous driving. Many networks have been used in downstream tasks, and semantic segmentation is no exception. These networks include VGG, ResNet, AlexNet, CNN, and FCN (fully connected network). FCN is a watershed moment in semantic segmentation because it employs a fully convolutional network to solve the problem.\n4.1.2.3 Human Action Recognition (HAR) Human action recognition is the task of recognizing what people are doing in videos for a set of pre-defined action classes. This task necessitates the use of both spatial and temporal features. HAR has enabled the field to address one of its most pressing issues: using unlabeled data to build reliable recognition systems with only a few labeled training samples [30]. HAR is frequently used to evaluate the quality of video features learned using self-supervised methods. You and Wang [31] proposed using a view-enhanced jigsaw puzzle (VEJP) to recognize 3D human actions. VEJP captures multi-view data and forces the encoder to obtain view independent high-\nlevel features from the human skeleton. VEJP-extracted features are more robust and distinguishable."
        },
        {
            "heading": "4.2 Types of\u00a0SSL Learning Techniques",
            "text": "The ConvNet model serves as the foundation of SSL, and its architecture influences the quality of visual representations learned through the pretext task. There are numerous pre-trained convolutional neural network models available, including ResNet50, ResNet50 v1, and upgraded versions. When a low-capacity model, such as AlexNet, is used for pre-training, little improvement can be obtained when compared to the Resnet50 family [26]. The major categories of SSL learning are Contrastive learning and non-contrastive learning."
        },
        {
            "heading": "4.2.1 Contrastive Learning",
            "text": "Contrastive learning is used to learn a representation or feature space that attracts and repels representations from similar images. Contrastive learning has a wide range of applications in computer vision and natural language processing. For example, in NLP, changing the position of a single word can change the semantics of a sentence. The goal of contrastive learning is to bring semantically related samples closer together while keeping dissimilar samples separate [32]. In contrastive learning one sample from the training data is used as an anchor, its augmented form is labeled as a positive sample, and the remaining examples in the training batch are labeled as negative samples. Because people can distinguish items without remembering every detail about them, their cognitive learning patterns give rise to the concept of contrastive learning. The contrastive learning framework is depicted in Fig.\u00a010, and it consists of a large set of unlabeled data, accurate data augmentation, an encoder (ResNet-50, ConvNet, CNN, etc.), and a handcrafted pretext task. Data augmentation is the use of stochastic transformations to map an image into different perspectives, such as resizing, random flipping, color distortion, Gaussian blurring, and so on. Data augmentation can alter an image's visual appearance without changing its semantics. An encoder is used to extract features from images. The quality of features increases as the number of layers in a network increases. In contrastive learning, three types of encoders are used: image encoders, momentum encoders, and dictionaries [33]. To predict the unlabeled data, pretext tasks generate pseudo labels from these extracted features."
        },
        {
            "heading": "4.2.2 Non\u2011contrastive Learning",
            "text": "Non-contrastive learning techniques only depends on positive sample pairs which means that the training data contains only related representations, for example the data may\ncontain the two versions of same image of a dog like color and black and white but not the image of some unrelated data or negative samples like a picture of a building. This may be thought that the contrastive learning model being trained on positive samples only may be prone to collapse, but FAIR found that the model has the ability to learn good representations despite learning from only positive samples. The non-contrastive SSL model uses stop-gradient and extra-predictor operations to achieve better learning results. BYOL and SimSiam proved that using these operations, the non-contrastive learning model does not suffer representation collapse. The Table\u00a01 given below mentions some differences between contrastive and non-contrastive learning."
        },
        {
            "heading": "4.3 Applications of\u00a0SSL and\u00a0Its State of\u00a0the\u00a0Art Work",
            "text": ""
        },
        {
            "heading": "4.3.1 Self\u2011supervised Learning and\u00a0NLP",
            "text": "SSL has become a hot research topic for researchers since 2018, when Google introduced the natural language processing (NLP) model. SSL's most fruitful results in the field of NLP are BERT and T5. A substantial amount of research has been conducted in this area. This section goes over a few of them. Zhou et\u00a0al. [34] used the SSL regularization technique in NLP for text classification. They defined text classification as a key concept in NLP. Authors provided training texts that were encoded using a text encoder as input. In the encoded\ntext, the authors defined two tasks: prediction and self-supervision. Both tasks use the same encoded text. They tested their model on 17 text classification datasets, attempting to minimize classification and regularization losses. Chen et\u00a0al. [35, 36] used a hybrid SSL approach to regularize the training of a text classification task. Task adaptive pre-training (TAPT) and domain adaptive pre-training (DAPT) have been proposed by Gururangan et\u00a0al. [37]. TAPT continues to train it on the target task's training dataset, after pre-training RoBERTa on large-scale corpora. DAPT is still pre-training RoBERTa on datasets with minor domain differences from datasets in target tasks. The difference between the SSL-Reg technique and TAPT and DAPT is that SSL-Reg uses a selfsupervised task (for example, mask token prediction) to regularize RoBERTa fine-tuning, whereas TAPT and DAPT use separate tasks. TAPT and DAPT, on the other hand, use a self-supervised task for pre-training, with the text classification task coming first, followed by the self-supervised task. SSL-Reg method and TAPT are similar in that they both use texts in target tasks to conduct self-supervised learning. Sun et\u00a0al. [38] proposed ERNIE 2.0, a framework for learning multiple tasks in incremental increments. They created several tasks and trained the ERNIE 2.0 model to extract lexical, syntactic, and semantic information from the training dataset. They used General Language Understanding Evaluation (GLUE) to assess the model's performance. As the size of the model in natural language processing grows, so\n1 3\ndoes the need for large memory. To address this issue, Lan et\u00a0al. [39] proposed a two-parameters reduction technique called ALBERT (A lite BERT) that use fewer parameters than traditional BERT. ALBERT has two parameters: factorized embedding parameterization and cross-layer parameter sharing. One large vocabulary matrix is decomposed into two small matrices in the first step. The size of the vocabulary embedding is kept separate from the hidden layers. If the number of parameters is increased, this separation will not increase the size of the memory. Multiple techniques are used to share parameters in cross-layer parameter sharing. When the network's depth increases, cross-layer parameter sharing prevents the parameters from growing."
        },
        {
            "heading": "4.3.2 Self\u2011supervised Learning in\u00a0Healthcare",
            "text": "Chowdhury et\u00a0al. [40] discussed SSL implementation in healthcare as well as four other major areas: pixel to scalar pretext task, pixel to pixel pretext task, adversarial learning, and contrastive learning. The findings explain in their article that SSL has the ability to solve the problems caused by supervised learning. Jamaludin et\u00a0al. [41] used longitudinal MRI scan data to train a Siamese CNN to learn embedding in which pairs of images from the same patient at different times in time are pushed further apart in the latent space and vice versa. These two pretext tasks' loss functions are combined, and a third pretext task is used to forecast vertebral body levels. The authors collected data from 1016 subjects for their experimental work. To address the issue of high traffic at network stations, Isravel et\u00a0al. [25] incorporating SSL techniques into the software-defined networking domain. By sensing the channel, SSL can estimate the traffic behavior. Chen et\u00a0al. [42] used SSL for context restoration in medical image analysis. They validated context restoration in three medical imaging problems: classification, localization, and segmentation. They used 2D ultrasound images for classification and CT images of abdominal organs for localization. They considered MRI of brain tumors to segment the imaging problem. The semantic features of images are taken into account by using SSL-based restoration. Kwasigroch et\u00a0al. [43] proposed a fusion method that combines transfer and self-supervised learning. They used this method in skin treatment, which is the most delicate application in the medical domain. They divided the dermoscopic images into two categories: benign and malignant. The dataset used for this experiment includes 2000 training images, 150 validation images, and 600 testing images. Because this dataset is unbalanced, they encountered various problems while conducting experiments. They demonstrated in their proposed method that SSL can perform better even when only a small number of images are labeled. Ghesu et\u00a0al. [44] proposed a contrastive learning and online feature clustering method. They collected 100 million 2D and 3D medical\nimages from a variety of modalities, including radiography, computed tomography (CT), magnetic resonance imaging (MR), and ultrasonography. They extracted features from these images and using these extracted features to train the model in both supervised and unsupervised modes. They validate three medical problems: chest radiography abnormality assessment, brain metastasis detection in MR, and brain hemorrhage detection in CT image data. Spathis et\u00a0al. [45] discussed the role of self-supervised learning in the medical domain. They collected ECG signal data and applied prominent SSL methods to it. They demonstrated that self-supervised learning can perform well with large amounts of data without the need for annotations. Nguyen et\u00a0al. [46] proposed a method using SSL to solve a variety of medical problems. They have worked with image data containing spatial 3D information, such as CT and MR images. The system has been divided into two stages: classification and segmentation. They used Resnet 34 to predict the value of spatial information. This stage's output was used as the initial parameters for the second segmentation stage. The authors considered two medical domain problems for experiment purposes. The first issue is organ risk, and the second is detecting intracranial hemorrhage. The StructSeg dataset, which contains CT images of 60 lung cancer patients, was used for the Organ at Risk problem, and the RSNA dataset was used for Intracranial Hemorrhage detection. This dataset contains 17,079 images of patients."
        },
        {
            "heading": "4.3.3 Self\u2011supervised Learning in\u00a0Computer Vision",
            "text": "SSL has been widely used in computer vision applications such as object detection, image classification, graph classification, visual question answering, and so on. For object detection, two approaches are used: a one-stage detector and a two-stage detector. In a two-stage detector, the object proposal is generated first, and then the object is located and classified in the second stage, whereas in a one-stage detector, classification and bounding box location are done in the same stage. Self-supervised learning methods are rapidly gaining popularity. The basic idea behind SSL is to develop a model that can solve problems in the field of computer vision. Gidaris et\u00a0al. [47] proposed a ConvNets-based method for recognizing 2D rotations in images. According to Huang et\u00a0al. [48], SSL should be used for few shot object detection (FSOD) and instance segmentation. Object detection necessitates dense labeling, which is a time-consuming process,the FSOD method attempts to recognize previously unseen object classes based on a few labels. They discussed various object detection benchmark datasets and their evaluation matrices also in this work. Self-EMD is a method for learning spatial-visual representation proposed by Liu et\u00a0al. [49]. They used more than one set of images from various perspectives. The image is then cropped to 224 \u00d7 224 and\n1 3\nresized using various methods such as color distortion, random Gaussian blur, random horizontal flip, and so on. The COCO image dataset was used for experimentation. Amrani et\u00a0al. [50] proposed a self-supervised learning method for detecting and retrieving an object from an unlabeled video. By listening to the video, this model captures similar frames with a common theme. For background rejection, contrastive learning was used, and new clusters were formed by a high level of label noise. Table\u00a02 lists the major contributions of researchers in various fields, such as health care, natural language processing, image classification, object detection, and so on."
        },
        {
            "heading": "4.4 Taxonomy of\u00a0SSL",
            "text": "Self-supervised learning has attracted many researchers for its excellent data efficiency. In this method, fewer labels and smaller samples are used to learn more by incorporating a neural network. In this section, we will go over some fundamental SSL terms."
        },
        {
            "heading": "4.4.1 Pseudo Labels",
            "text": "The data curation process must be automated to reduce manual input, and the number of labels required for good performance must be reduced [57]. Pseudo labels are labels that are assigned automatically by the network based on pretext tasks. For example, an image is provided as input, and the system performs some type of transformation on it, such as rotation, colorization, and so on. The transformed image is fed into ConvNet, which predicts the transformation via a pseudo labeling process. Figure\u00a011 depicts this process."
        },
        {
            "heading": "4.4.2 Linear Classification",
            "text": "A typical assessment protocol involves training a linear classifier on top of (frozen) representations learned through self-supervised methods. It is evaluated for the classification accuracy of the learned classifier model on the ImageNet Val/Test set."
        },
        {
            "heading": "4.4.3 Pre\u2011trained Models",
            "text": "A common assessment protocol involves training a linear classifier on top of (frozen) representations learned through self-supervised methods. The classification accuracy of the learned classifier model is evaluated on the ImageNet Val/ Test set."
        },
        {
            "heading": "4.4.4 Fine\u2011tuning the\u00a0Model",
            "text": "The fairness of self-supervised learning is affected by model fine-tuning [58]. The design of all models, including their\nparameters, is copied into the target model, except for the output layer when fine-tuning models. These parameters are adjusted based on the target datasets. To achieve the highest level of performance, downstream tasks in SSL use finetuned models."
        },
        {
            "heading": "5 Sources of\u00a0Datasets",
            "text": "This section will go over various datasets used for training and evaluating self-supervised visual features, natural language processing, image classification, the medical domain, and so on. These datasets are collected for self-supervised training without the use of human-annotated labels. Table\u00a03 discusses various datasets used for computer vision, medical imaging, and natural language processing, along with their source links."
        },
        {
            "heading": "6 Critical analysis",
            "text": "When compared to supervised models, SSL has achieved remarkable success in the computer vision, NLP, and healthcare domains. SSL has some issues that researchers encounter, just like every coin has two sides. We have discussed a few of the pros and cons of SSL in this section.\n\u2022 The vast majority of self-supervised pre-trained models, such as those in the ImageNet dataset, are trained on images with a single dominant object. The scene in applications such as self-driving cars contains several items, making it difficult to distinguish between two similar scenes. \u2022 Finding context in satellite and medical images is extremely difficult due to their lack of structure. As a result, approaches like relative patch prediction and jigsaw puzzles are useless when dealing with such images. \u2022 When dealing with less structured datasets containing medical and satellite images, a different augmentation method is required than when dealing with datasets for natural language processing. \u2022 Creating a useful pretext assignment that allows a network to learn meaningful images/text is the most difficult aspect of self-supervised learning. \u2022 After reviewing a large amount of literature, we discovered that as the size of the dataset grows, so does the system's performance. Larger datasets should thus be used whenever possible. \u2022 Because SSL can process large data sets without relying on labels, an incomplete sentence in the NLP domain can be completed with a few words. Later words can be completed by understanding the semantics of the previous sentence.\n1 3\nTa bl\ne 2\nM aj\nor c\non tri\nbu tio\nn by\nre se\nar ch\ner s\nA ut\nho rs\nn am\ne Jo\nur na\nl/c on\nfe re\nnc e\nH ig\nhl ig\nht s o\nf t he\na rti\ncl e\nD at\nas et\n/n um\nbe r o\nf s ub\nje ct\ns\nG irs\nhi ck\ne t\u00a0a\nl. [2\n8] IE\nEE c\non fe\nre nc\ne O\nbj ec\nt d et\nec tio\nn an\nd se\nm an\ntic se\ngm en\nta tio\nn PA\nSC A\nL V\nO C\n2 01 2 Ja m al ud in e t\u00a0a l. [4 1] D ee p Le ar ni ng in M ed ic al Im ag e A na ly si s a nd M ul tim od al L ea rn in g fo r C lin ic al D ec is io n Su pp or t Sp in al M R I 10 16 su bj ec ts C he n et \u00a0a l. [4 2] M ed ic al Im ag e A na ly si s U se d C on te xt re sto ra tio n str at eg y in m ed ic al im ag in g \u2013 Le e et \u00a0a l. [5 1] IE EE e xp lo re c on fe re nc e on c om pu te r v is io n an d pa tte rn re co gn iti on O bj ec t d et ec tio n vi a re cy cl in g of b ou nd in g bo x an no ta tio n PA SC A L V O C a nd\nC O\nCO\nFe ng\ne t\u00a0a\nl. [2\n0] IE\nEE C\non fe\nre nc\ne C\nV F\nEs tim\nat in\ng th\ne ro\nta tio\nn an\ngl e\nin c\nom pu\nte r v\nis io\nn PA\nSC A\nL V\nO C\n2 00\n7( 20\n,0 44\nte sti\nng im\nag es\na nd\n1 9,\n80 8\nte sti\nng im\nag es ) Y un e t\u00a0a l. [5 2] IE EE A cc es s Pr op os ed a h um an p os e es tim at io n m et ho d by c re at in g a pa th fo r l ea rn in g ne w ro ta tio na l c ha ng es b as ed o n a se lfsu pe rv is ed m et ho d CO CO k ey -p oi\nnt d\net ec\ntio n\nda ta\nse t\nA m\nra ni\ne t\u00a0a\nl. [5\n0] IE\nEE /C\nV F\nC on\nfe re\nnc e\non C\nom pu\nte r V\nis io\nn an\nd Pa tte rn R ec og ni tio n W or ks ho ps\nO bj\nec t d\net ec\ntio n\nan d\nre tre\niv al\nfr om\nv id\neo s\nH O\nW 2\nda ta\nse t\nLi u\net \u00a0a\nl. [4\n9] C\nom pu\nte r v\nis io\nn an\nd Pa\ntte rn\nre co\ngn iti\non Se\nlfEM\nD m\net ho\nd fo\nr O bj\nec t d\net ec\ntio n\nCO CO\nSu n\net \u00a0a\nl.[ 38\n] C\nom pu\nta tio\nn an\nd la\nng ua\nge D\nev el\nop ed\na n\nER N\nIE 2\n.0 fr\nam ew\nor k\nfo r n\nat ur\nal\nla ng\nua ge\np ro\nce ss\nin g\nG en\ner al\nL an\ngu ag\ne U\nnd er\nst an\ndi ng\nE va\nlu at\nio n\n(G LU\nE)\nC he\nn et\n\u00a0a l.\n[3 5,\n3 6]\nM ac\nhi ne\nle ar\nni ng\nPr op\nse d\nC on\ntra sti\nve le\nar ni\nng fr\nam ew\nor k\nfo r v\nis ua l re pr es en ta tio n\nIm ag\neN et\nN gu\nye n\net \u00a0a\nl. [4\n6] IE\nEE A\ncc es\ns U\nse d\nSe lf-\nsu pe\nrv is\ned le\nar ni\nng fo\nr m ed\nic al\nim ag e an al ys is Tw\no da\nta se\nt: St\nru ct\nSe g\nco nt\nai ns\nim ag\nes o\nf 6 0\nlu ng\nca\nnc er\np at\nie nt\ns, R\nSN A\nd at\nas et\nc on\nta in\ns 1 7,\n07 9\nim ag\nes . F\nur th\ner m\nor e,\nim ag\nes w\ner e\nad de\nd at\nth e\ntim e\nof p\nro ce\nss in g K w as ig ro ch e t\u00a0a l. [4 3] El ec tro ni c Em pl oy ed S SL to in cr ea se th e pe rfo rm na ce in sk in ca re d om ai n 20 00 tr ai ni ng , 1\n50 v\nal id\nat io\nn, a\nnd 6\n00 te\nsti ng\nim ag\nes\nH ua\nng e\nt\u00a0a l.[\n48 ]\nC om\npu te\nr v is\nio n\nan d\nPa tte\nrn re\nco gn\niti on\nFe w\n-s ho\nt o bj\nec t d\net ec\ntio n \u2013 C ho w dh ur y et \u00a0a l. [4 0] In fo rm at ic s Re vi ew o n ap pl ic at io n of S SL in H ea lth ca re \u2013 Zh ou e t\u00a0a l.[ 34 ] Tr an sa ct io ns o f t he A ss oc ia tio n fo r C om pu ta tio na l Li ng ui sti cs Te xt c la ss ifi ca tio n in N LP\n17 d\nat as\net s\nPo to\ntz ky\net a\nl. [5\n3] D\nA G\nM c\non fe\nre nc\ne on\np at\nte rn\nre co\ngn iti\non O\nbj ec\nt d et\nec tio\nn in\nA ut\non om\nou s d\nriv in\ng 1,\n20 0,\n00 0\nim ag\nes ,a\nnd 2\n0, 00\n0 un\nla be\nle d\nim ag es Ja in e t\u00a0a l.[ 54 ] Pr oc ee di ng s o f t he A C M o n In te ra ct iv e SS L fo r h um an a ct io n re co gn iti on b y us in g in er tia l se ns or s p la ce d on h um an b od y \u2013 B re ik i e t\u00a0a l. [2 3] C om pu te r v is io n an d Pa tte rn re co gn iti on Im ag e cl as si fic at io n in c om pu te r v is io n C as sa va p la nt d is ea se d at as et . T ha t c on ta in 1 2,\n00 0\nun la\nbe le\nd im\nag es\na nd\n6 00\n0 la\nbe le d Zi eg le r & A sa no [5 5] C om pu te r v is io n an d Pa tte rn re co gn iti on Se m an tic se gm en ta tio n in o bj ec t d et ec tio n CO CO Tr en es ka e t\u00a0a l. [1 9] Se ns or s Im ag e co lo riz at io n fo r v is ua l f ea tu re le ar ni ng CO CO a nd P as ca lV O C 2 01 2 D in g et \u00a0a l.[ 56 ] IE EE T ra ns ac tio ns o n Pa tte rn A na ly si s a nd M ac hi ne In te lli ge nc e Pr op os ed D ee pl y U ns up er vi se d Pa tc h Re -I D (D U PR ) fo r u ns up er vi se d vi su al re pr es en at io n le ar ni ng PA SC A LV O C G he su e t\u00a0a l. [4 4] C om pu te r v is io n an d Pa tte rn re co gn iti on So lv e im ag e as se ss em ne t p ro bl em s i n m ed ic al d om ai n 10 0 m ill io n im ag es (r ad io gr ap hy\n, C T,\nM R\nim ag\nin g,\nul\ntra so\nno gr\nap hy\ne tc\n.)\n1 3"
        },
        {
            "heading": "7 Conclusion and\u00a0Future Directions",
            "text": "Self-supervised approaches have dominated supervised learning. They use the vast amounts of unlabeled data that are freely available. Self-supervised learning approaches have been shown to be effective in difficult downstream tasks such as image classification, object detection, image segmentation, and other tasks with little labeled input. The authors of this review article investigated the SSL application area as well as various types of learning. When combined with other learning methods, SSL can achieve greater success. Various pretext tasks generate different supervision signals, which can help the network learn more typical characteristics. In most existing self-supervised visual feature learning algorithms, ConvNet is trained to solve one pretext task. This review article provided a comprehensive overview of various learning schemes, the SSL pipeline, and recent research in this domain. Only a few studies have examined learning multiple pretext tasks for self-supervised feature learning. Self-supervised feature learning on multiple pretext tasks can be investigated further. The majority of self-supervised visual feature learning approaches currently available are focused on learning features for a single modality. If multiple data modalities from other sensors are available, the constraints between them can be used to train networks to learn features. As everyone is busy these days and wants to do most of the work automatically, researchers have a lot of room to explore many new techniques in this domain. SSL provides this level of security without the need for human intervention.\n[1] https:// resea rch. aimul tiple. com/ self- super vised- learn ing/ [2] https:// neptu ne. ai/ blog/ self- super vised- learn ing [3] https:// atcold. github. io/ pytor ch- Deep- Learn ing/ en/\nweek10/ 10-1/ [4] https:// engin eering. purdue. edu/ ECE/ News/ 2020/ incre\nmental- learn ing- in- online- scena rio\nAuthors Contributions VR and STN: database collection, methodology, original draft. MK and AM: supervision, review and editing. KK: review and final draft.\nFunding Nil.\nDeclarations\nConflict of interest The authors declare that they have no conflict of interest in this work."
        }
    ],
    "title": "Self\u2010supervised Learning: A Succinct Review",
    "year": 2023
}