{
    "abstractText": "At present, deep learning has been well applied in many fields. However, due to the high complexity of hypothesis space, numerous training samples are usually required to ensure the reliability of minimizing experience risk. Therefore, training a classifier with a small number of training examples is a challenging task. From a biological point of view, based on the assumption that rich prior knowledge and analogical association should enable human beings to quickly distinguish novel things from a few or even one example, we proposed a dynamic analogical association algorithm to make the model use only a few labeled samples for classification. To be specific, the algorithm search for knowledge structures similar to existing tasks in prior knowledge based on manifold matching, and combine sampling distributions to generate offsets instead of two sample points, thereby ensuring high confidence and significant contribution to the classification. The comparative results on two common benchmark datasets substantiate the superiority of the proposed method compared to existing data generation approaches for few-shot learning, and the effectiveness of the algorithm has been proved through ablation experiments.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuncong Peng"
        },
        {
            "affiliations": [],
            "name": "Xiaolin Qin"
        },
        {
            "affiliations": [],
            "name": "Qianlei Wang"
        },
        {
            "affiliations": [],
            "name": "Boyi Fu"
        },
        {
            "affiliations": [],
            "name": "Yongxiang Gu"
        }
    ],
    "id": "SP:844d190d38ea8580cfe3e6edf1f10f35e3f9e978",
    "references": [
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Proc. of Int. Conf. on Neural Information Processing Systems, Carson, Nevada, USA, pp. 1097\u20131105, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "Int. Conf. on Machine Learning, Long Beach, USA, pp. 6105\u20136114, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, pp. 770\u2013778, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P. -L. Hong",
                "J. -Y. Hsiao",
                "C. -H. Chung",
                "Y. -M. Feng",
                "S. -C. Wu"
            ],
            "title": "ECG biometric recognition: Template-free approaches based on deep learning",
            "venue": "Proc. of Int. Conf. of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, Germany, pp. 2633\u20132636, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R.D. Labati",
                "E. Muoz",
                "V. Piuri",
                "R. Sassi",
                "F. Scotti"
            ],
            "title": "Deep-ECG: Convolutional neural networks for ECG biometric recognition",
            "venue": "Pattern Recognition Letters, vol. 126, no. 6, pp. 78\u201385, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wang",
                "Z. Mao",
                "B. Wang",
                "L. Guo"
            ],
            "title": "Knowledge graph embedding: A survey of approaches and applications",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 12, pp. 2724\u20132743, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Sun",
                "R. Grishman"
            ],
            "title": "Employing lexicalized dependency paths for active learning of relation extraction",
            "venue": "Intelligent Automation & Soft Computing, vol. 34, no. 3, pp. 1415\u20131423, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Sun",
                "R. Grishman"
            ],
            "title": "Lexicalized dependency paths based supervised learning for relation extraction",
            "venue": "Computer Systems Science and Engineering, vol. 43, no. 3, pp. 861\u2013870, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.R. Jeyaraj",
                "E.R.S. Nadar"
            ],
            "title": "Computer-assisted medical image classification for early diagnosis of oral cancer employing deep learning algorithm",
            "venue": "Journal of Cancer Research and Clinical Oncology, vol. 145, no. 4, pp. 829\u2013837, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Guo",
                "S. Ren",
                "M.Z.A. Bhuiyan",
                "T. Li",
                "D. Liu"
            ],
            "title": "Mdmaas: Medical-assisted diagnosis model as a service with artificial intelligence and trust",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 3, pp. 2102\u20132114, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Bachman",
                "R.D. Hjelm andW. Buchwalter"
            ],
            "title": "Learning representations by maximizing mutual information across views",
            "venue": "arXiv preprint arXiv:1906.00910, 2019.",
            "year": 1906
        },
        {
            "authors": [
                "D. Chen",
                "Y. Chen",
                "Y. Li",
                "F. Mao",
                "Y. He"
            ],
            "title": "Self-supervised learning for few-shot image classification",
            "venue": "Proc. of IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Toronto, Ontario, Canada, pp. 1745\u2013 1749, 2021. CSSE, 2023, vol.46, no.1 1245",
            "year": 2021
        },
        {
            "authors": [
                "P. Mangla",
                "N. Kumari",
                "A. Sinha",
                "M. Singh",
                "B. Krishnamurthy"
            ],
            "title": "Charting the right manifold: Manifold mixup for few-shot learning",
            "venue": "Proc. of the IEEE/CVF Winter Conf. on Applications of Computer Vision, Snowmass Village, USA, pp. 2218\u20132227, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. -X. Wang",
                "R. Girshick",
                "M. Hebert",
                "B. Hariharan"
            ],
            "title": "Low-shot learning from imaginary data",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, pp. 7278\u20137286, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "I.A. Weinshall",
                "Daphna"
            ],
            "title": "Generative latent implicit conditional optimization when learning from small sample",
            "venue": "Int. Conf. on Pattern Recognition, Milano, Lombardia, Italy, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Schwartz",
                "L. Karlinsky",
                "J. Shtok",
                "S. Harary",
                "M. Marder"
            ],
            "title": "Delta-encoder: An effective sample synthesis method for few-shot object recognition",
            "venue": "Proc. of the 32nd Int. Conf. on Neural Information Processing Systems, Montreal, Canada, pp. 2850\u20132860, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Chen",
                "Y. Fang",
                "X. Wang",
                "H. Luo",
                "Y. Geng"
            ],
            "title": "Diversity transfer network for few-shot learning",
            "venue": "Proc. of the AAAI Conf. on Artificial Intelligence, New York, USA, pp. 10559\u201310566, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Li",
                "Y. Zhang",
                "K. Li",
                "Y. Fu"
            ],
            "title": "Adversarial feature hallucination networks for few-shot learning",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, Seattle, WA, USA, pp. 13470\u201313479, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "Y. Fu",
                "Y. Zhang",
                "Y. -G. Jiang",
                "X. Xue"
            ],
            "title": "Multi-level semantic feature augmentation for one-shot learning",
            "venue": "IEEE Transactions on Image Processing, vol. 28, no. 9, pp. 4594\u20134605, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Santoro",
                "S. Bartunov",
                "M. Botvinick",
                "D. Wierstra",
                "T. Lillicrap"
            ],
            "title": "Meta-learning with memory-augmented neural networks",
            "venue": "Int. Conf. on Machine Learning, Vienna, Austria, pp. 1842\u20131850, 2016.",
            "year": 1842
        },
        {
            "authors": [
                "N. Mishra",
                "M. Rohaninejad",
                "X. Chen",
                "P. Abbeel"
            ],
            "title": "A simple neural attentive meta-learner",
            "venue": "Int. Conf. on Learning Representations, Vancouver, BC, Canada, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A.A. Rusu",
                "D. Rao",
                "J. Sygnowski",
                "O. Vinyals",
                "R. Pascanu"
            ],
            "title": "Meta-learning with latent embedding optimization",
            "venue": "Int. Conf. on Learning Representations, Vancouver, BC, Canada, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Bertinetto",
                "J. Henriques",
                "P. Torr",
                "A. Vedaldi"
            ],
            "title": "Meta-learning with differentiable closed-form solvers",
            "venue": "Int. Conf. on Learning Representations, New Orleans, Louisiana, United States, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Jamal",
                "G. -J. Qi"
            ],
            "title": "Task agnostic meta-learning for few-shot learning",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, Long Beach, CA, USA, pp. 11719\u201311727, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. -Y. Chen",
                "Y. -C. Liu",
                "Z. Kira",
                "Y. -C.F. Wang",
                "J. -B. Huang"
            ],
            "title": "A closer look at few-shot classification",
            "venue": "Int. Conf. on Learning Representations, New Orleans, Louisiana, United States, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "Int. Conf. on Machine Learning, Stockholm, Sweden, pp. 1126\u20131135, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Sung",
                "Y. Yang",
                "L. Zhang",
                "T. Xiang",
                "P.H. Torr"
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, pp. 1199\u20131208, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "V.G. Satorras",
                "J.B. Estrach"
            ],
            "title": "Few-shot learning with graph neural networks",
            "venue": "Int. Conf. on Learning Representations, Vancouver, BC, Canada, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Sun",
                "Y. Liu",
                "T. -S. Chua",
                "B. Schiele"
            ],
            "title": "Meta-transfer learning for few-shot learning",
            "venue": "Proc. of the IEEE/ CVF Conf. on Computer Vision and Pattern Recognition, Long Beach, CA, USA, pp. 403\u2013412, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Kim",
                "T. Kim",
                "S. Kim",
                "C.D. Yoo"
            ],
            "title": "Edge-labeling graph neural network for few-shot learning",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, Long Beach, CA, USA, pp. 11\u201320, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I. Ziko",
                "J. Dolz",
                "E. Granger",
                "I.B. Ayed"
            ],
            "title": "Laplacian regularized few-shot learning",
            "venue": "Int. Conf. on Machine Learning, Austria, Venna, pp. 11660\u201311670, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Hu",
                "V. Gripon",
                "S. Pateux"
            ],
            "title": "Leveraging the feature distribution in transfer-based few-shot learning",
            "venue": "Int. Conf. on Artificial Neural Networks, Bratislava, Slovakia, pp. 487\u2013499, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Xie",
                "F. Long",
                "J. Lv",
                "Q. Wang",
                "P. Li"
            ],
            "title": "Joint distribution matters: Deep brownian distance covariance for fewshot classification",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, New Orleans, LA, USA, pp. 7972\u20137981, 2022. 1246 CSSE, 2023, vol.46, no.1",
            "year": 2022
        },
        {
            "authors": [
                "P. Chikontwe",
                "S. Kim",
                "S.H. Park"
            ],
            "title": "CAD: Co-Adapting discriminative features for improved few-shot classification",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, New Orleans, LA, USA, pp. 14554\u201314563, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Koltchinskii",
                "Vladimir"
            ],
            "title": "Rademacher penalties and structural risk minimization",
            "venue": "IEEE Transactions on Information Theory, vol. 47, no. 5, pp. 1902\u20131914, 2001.",
            "year": 1902
        },
        {
            "authors": [
                "L. Bottou",
                "O. Bousquet"
            ],
            "title": "The tradeoffs of large scale learning",
            "venue": "Proc. of the Int. Conf. on Neural Information Processing Systems, Vancouver, BC, Canada, pp. 161\u2013168, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "L. Bottou",
                "F.E. Curtis",
                "J. Nocedal"
            ],
            "title": "Optimization methods for large-scale machine learning",
            "venue": "Siam Review, vol. 60, no. 2, pp. 223\u2013311, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "Q. Yao",
                "J.T. Kwok",
                "L.M. Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM Computing Surveys, vol. 53, no. 3, pp. 1\u201334, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "I. Misra",
                "L.V.D. Maaten"
            ],
            "title": "Self-supervised learning of pretext-invariant representations",
            "venue": "Proc. of the IEEE/ CVF Conf. on Computer Vision and Pattern Recognition, Seattle, WA, USA, pp. 6707\u20136717, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "European Conf. on Computer Vision, Amsterdam, The Netherlands, pp. 649\u2013666, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R. Moradi",
                "R. Berangi",
                "B. Minaei"
            ],
            "title": "A survey of regularization strategies for deep models",
            "venue": "Artificial Intelligence Review, vol. 53, no. 6, pp. 3947\u20133986, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Genevay",
                "G. Peyre",
                "M. Cuturi"
            ],
            "title": "Learning generative models with sinkhorn divergences",
            "venue": "Int. Conf. on Artificial Intelligence and Statistics, Playa Blanca, Lanzarote, pp. 1608\u20131617, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Dai",
                "H. Hang"
            ],
            "title": "Manifold matching via deep metric learning for generative modeling",
            "venue": "Proc. of the IEEE/ CVF Int. Conf. on Computer Vision, Montreal, Quebec, Canada, pp. 6587\u20136597, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C.E. Priebe",
                "D.J. Marchette",
                "Z. Ma",
                "S. Adali"
            ],
            "title": "Manifold matching: Joint optimization of fidelity and commensurability",
            "venue": "Brazilian Journal of Probability and Statistics, vol. 27, no. 3, pp. 377\u2013400, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Harandi",
                "M. Salzmann",
                "M. Baktashmotlagh"
            ],
            "title": "Beyond gauss: Image-set matching on the riemannian manifold of pdfs",
            "venue": "Proc. of the IEEE Int. Conf. on Computer Vision, Santiago, Chile, pp. 4112\u20134120, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "O. Arandjelovic",
                "G. Shakhnarovich",
                "J. Fisher",
                "R. Cipolla",
                "T. Darrell"
            ],
            "title": "Face recognition with image sets using manifold density divergence",
            "venue": "IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, San Diego, CA, USA, pp. 581\u2013588, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "M. Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Proc. of Int. Conf. on Neural Information Processing Systems, Carson, Nevada, USA, pp. 2292\u20132300, 2013. CSSE, 2023, vol.46, no.1 1247",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Keywords: Few-shot learning; manifold matching; analogical association; data generation"
        },
        {
            "heading": "1 Introduction",
            "text": "Artificial intelligence algorithms represented by deep learning have achieved advanced performance in image classification [1\u20133], biometric recognition [4,5], relation extraction [6\u20138] and medical assisted diagnosis [9\u201310] by virtue of ultra-large-scale datasets and powerful computing resources. It is worth noting that although the complex hypothesis space easily contains the real mapping, it is also more difficult to find the target mapping. Therefore, deep neural networks usually require a large number of supervised samples for training.\nUnfortunately, it is hard to obtain large-scale trainable data in most real scenarios because of the high cost of data labeling and the inability to obtain large amounts of data in some specific areas. In order to be able to learn in the case of limited supervised information, the research of few-shot learning has\nThis work is licensed under a Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nsprung up. In few-shot classification, the model is trained on a set of classes with sufficient samples, which are called base classes. When evaluating performance, further training and testing are carried out on another set of novel classes with small samples. It is worth mentioning that testing on novel classes that have not been seen before in training is called zero-shot learning.\nAs is known to all, human beings have rich prior knowledge and superb ability of association and analogy, so human beings can distinguish novel things from just a few or even one example. For example, as shown in Fig. 1, when people need to distinguish killer whales, doves and cats that they have never seen before, but if they have seen sharks, sparrows and dogs, people can make analogical associations and make full use of prior knowledge for classification. In other words, people can use the knowledge structure in the familiar category to make analogical associations, since some elements of the latent semantic structure already exist in other already familiar categories. Specifically, fins, wings and ears are the most obvious distinguishing features in the classification of sharks, sparrows and dogs, considering killer whales, doves and cats also have the similar structures, so people need to pay attention to these characteristics as well.\nThe Fig. 1 shows the importance of analogical association in humans\u2019 rapid recognition of novel things. The positive represents the knowledge structure similar to the anchor, on the contrary, the negative represents the knowledge structure not similar to the anchor.\nAt present, the existing researches for few-shot learning mainly focus on representation learning [11\u201313], data generation [14\u201319] and learning strategies [20\u201334]. These methods alleviate the problem of insufficient training samples, but only consider the use of rich priors and ignore the importance of analogy and association.\nTherefore, in order to make rational use of analogies and associations, the dynamic analogical association algorithm is proposed to search for knowledge structures that are similar to the current task and exist in prior knowledge. The in-depth exploration of the knowledge structure combined with the observation distribution of the current task can generate a sample that not only has high confidence but also can make the significant contribution to the classification. Our main contributions in this paper are as follows:\n1. The data generation framework which ensures the high confidence of the generated samples and significant contribution to the classification is proposed.\n2. The comparative results substantiate the superiority of the proposed method to existing data generation approaches for few-shot learning, and the effectiveness of the algorithm has been proved through ablation experiments and synthetic experiments.\n3. More importantly, we explained the importance of analogical association based on prior knowledge. Researchers need to re-examine how to make better use of prior knowledge."
        },
        {
            "heading": "2 Related Work",
            "text": "2.1 The Difficulty of Few-shot Learning\nSuppose a problem to be learned, it has a from X to Y optimal mapping h?.D is the joint distribution on X Y. The D is the train dataset which contains the observation samples in D. The expected risk on D and the empirical risk on D are as follow [35].\nR\u00f0h;D\u00de \u00bc E\u00f0x;y\u00de D\u00f0\u2018\u00f0h\u00f0x\u00de; y\u00de\u00de (1)\nR\u0302m\u00f0h;D\u00de \u00bc 1\nm Xm i\u00bc1 \u2018 h xi\u00f0 \u00de; yi\u00f0 \u00de (2)\nAssuming hm \u00bc argminh2H R\u0302m\u00f0h\u00de and h0 \u00bc argminh2H R\u00f0h\u00de, whereH is the given hypothetical space. The error can be decomposed [36\u201338] according to the following formula, as visualized in Fig. 2."
        },
        {
            "heading": "E R hm\u00f0 \u00de R h?\u00f0 \u00de\u00bd \u00bc E R hm\u00f0 \u00de R h0\u00f0 \u00de\u00bd \u00fe E R h0\u00f0 \u00de R h?\u00f0 \u00de\u00bd (3)",
            "text": "E R hm\u00f0 \u00de R h0\u00f0 \u00de\u00bd is called generalization error. The upper bound of generalization error is determined by model complexity and sample size. In general, it can be reduced by having a larger number of examples. Therefore, the difficulty of few-shot learning is that minimizing empirical risk becomes unreliable.\nE R h \u00f0 \u00de R h0\u00f0 \u00de\u00bd is called approximation error, which is mainly determined by h and hypothesis space H. It is worth noting that if the hypothesis space is sufficiently complex, such as h? 2 H, then the generalization error will increase while reducing the approximation error.\nThe above analysis explains the reason why the algorithm is difficult to generalize with small samples, and explains the design motivation of the few-shot learning algorithm. Based on the design motivation, the existing algorithms can be divided into three categories: representation learning, data generation and learning strategy."
        },
        {
            "heading": "2.2 Representation Learning",
            "text": "The motivation of representation learning is to change the original data into embedding which has lower dimensions and semantic information obtained according to a priori knowledge to reduce the difficulty of learning in the latent semantic space, which can reduce the approximation error and generalization error at the same time.\nThe simplest idea is to learn a feature extractor through a large number of base-class dataset, so that it can adapt to the limited differences between base-class dataset and novel-class dataset, and then recognize it through a classifier. Although the pretraining and fine-tuning method [25] is intuitive and concise, it is hard to learn general features.\nWith the continuous development of self-supervised technology [39,40], the backbone network based on self-supervised learning can learn better representation, so as to improve the performance of few-shot\nlearning. The augmented multiscale deep infomax algorithm (AMDIM) [11] can learn more generalized representations of images based on maximizing mutual information and achieve advanced results, which proves the importance of self-supervised learning in few-shot learning [12].\nIt is worth noting that robust representation is beneficial for few-shot learning. Therefore, the use of regularization technology in representation learning can improve the performance of few-shot learning. Puneet Mangla et al. [13] proposed self-supervised manifold mixup method (S2M2) that uses regularization technology based on manifold mixing, which significantly improves the performance of few-shot learning. In addition, there are some regularization techniques [41] which are beneficial for fewshot learning, such as adding penalty items, stopping early, etc."
        },
        {
            "heading": "2.3 Data Generation",
            "text": "The motivation of data generation is to generate non-trivial and diverse samples to increase the sample size, which can reduce the upper bound of generalization error and make the empirical risk more reliable.\nWang et al. [14] proposed a general generation algorithm based on generation network. Its motivation is to generate samples that is useful for learning classifiers, which is different from the traditional image reconstruction. Weinshall et al. [15] proposed the generation hidden condition optimization algorithm (GLICO), which generates new samples by hyperspherical interpolation of any two intra-class samples and restores them to images.\nTaking into account the difficulty of image reconstruction, it is also a good choice to generate samples directly from the latent semantic space. Schwartz et al. [16] proposed Delta Encoder to generate samples through offset learning. It is worth noting that generating samples from the semantic space is dependent on the performance of the representation model.\nMost of the existing data generation methods usually only consider the high confidence of the generated samples and ignore its weak contribution to the classification, or consider the significant contribution to the classification and ignore its low confidence. Therefore, we have the motivation to propose a data generation framework which ensures the high confidence of the generated samples and significant contribution to the classification."
        },
        {
            "heading": "2.4 Manifold Matching",
            "text": "Manifold matching usually refers to getting a distribution closest to a given distribution through optimization or selection.\nHow to measure the difference between distributions is very important, which can usually help model training, such as cross entropy, Kullback-Leibler divergence (KL divergence), Wasserstein distance and so on. As a special case of the optimal transport cost, the Wasserstein distance has the advantage over KL divergence in that even if the two distributions do not overlap, the Wasserstein distance can still reflect their distance, which can be used as a very suitable loss in the generative model.\nGenevay et al. [42] introduced the sinkhorn loss which is the optimal transport cost with entropy regularization into the generative model and achieved better results.\nDai et al. [43] proposed a generative model based on metric learning and manifold matching. Different from the traditional method which only considers the optimal transmission distance, it performs matching based on geometric descriptors.\nIn addition, manifold matching is not exclusive to generative tasks, which is also commonly used in document matching [44], image-set matching [45] and other tasks. For example, Arandjelovic et al. [46] proposed an image-set matching method based on the similarity between Gaussian mixture distributions.\nIn this work, based on the assumption that the feature distribution of latent semantic space can reflect the knowledge structure after reasonable representation learning, manifold matching is used to select the knowledge structure in prior knowledge closest to the current task, rather than generate samples directly.\n3 Dynamic Analogical Association Algorithm\n3.1 Problem Definition\nIn the few-shot learning classification benchmark, the classes in the dataset are usually divided into two non-overlapping class sets. One class set with rich sample size is called base-class set Cbase, while the other class set with only a small number of samples is called novel-class set Cnovel, Cbase \\ Cnovel \u00bc [. Then the dataset D can be divided according to the class set Dbase \u00bc f\u00f0x; y\u00dej\u00f0x; y\u00de 2 D; y 2 Cbaseg, Dnovel \u00bc f\u00f0x; y\u00dej\u00f0x; y\u00de 2 D; y 2 Cnovelg.\nIn few-shot classification, Firstly, the model is trained on the base-class dataset Dbase with abundant examples to obtain appropriate prior knowledge. Then the few-shot learning methods are evaluated using N-way K-shot classification framework.\nFor each task instance of N-way K-shot T i, including support set and query set. The labeled support set contains N classes randomly sampled from the novel-class datasetDnovel with K examples for each class. The query set contains unseen samples similar to support set. The evaluation method of few-shot learning is mainly to accurately classify unlabeled unseen query sample set through the learning of support set."
        },
        {
            "heading": "3.2 Overview",
            "text": "Biologically speaking, the ability of human beings to quickly understand new things mainly comes from the fact that human beings have rich prior knowledge and superb ability of association and analogy. Therefore, they can distinguish new things from only a few or even one examples.\nBased on this assumption, the dynamic analogical association algorithm in the way of sample generation for few-shot learning is proposed. Most of the existing sample generation methods usually only consider the high confidence of the generated samples and ignore its weak contribution to the classification, or consider the significant contribution to the classification and ignore its low confidence.\nDifferent from these methods, our method tends to search for knowledge structures similar to existing tasks in prior knowledge based on manifold matching, and combine sampling distributions to generate offsets instead of two sample points, thereby ensuring the high confidence of the generated samples and significant contribution to the classification. The overall structure of algorithm is shown in Fig. 3.\nThe algorithm includes two core parts: manifold matching module and data generation module. The manifold matching module is responsible for finding the knowledge structure similar to the current task in the prior knowledge. The data generation module is responsible for generating the offset with the help of the knowledge structure."
        },
        {
            "heading": "3.3 Manifold Matching Based on Optimal Transportation",
            "text": "We regard the distribution of labeled samples in the latent semantic space in each task support set as a sampling of the knowledge structure of N-way entities. The q refers to the corresponding latent overall distribution of the data in T , which is usually assumed to be the Gaussian mixture model with unknown parameters.\nThe distribution with unknown parameters is difficult to sample directly. However, samples of support set can be regarded as observation data sampled from latent distribution, which is also called the observed manifold M , and M \u00bc sample\u00f0q\u00de.\nTheM is a matrix, each row of which represents the feature of a sample in the support set. Therefore, for the N-way K-shot task, its dimension is NK d, where d represents the dimension of the feature.\nThe optimal transportation cost between the two distributions l; v is used to define the distance between the two distributions in the latent semantic space, so as to approximate the similarity of knowledge structure.\nC\u00f0l; v\u00de :\u00bc min c2 \u00f0l;v\u00de Z X Y c\u00f0x; y\u00dedc\u00f0x; y\u00de (4)\nIn the above formula, c\u00f0x; y\u00de represents the transportation cost function and c\u00f0x; y\u00de represents the joint distribution.\nFurthermore, the sinkhorn algorithm [47] is used to solve the optimal transport problem. Specifically, sinkhorn algorithm uses L2 distance as the cost function and takes the manifold of the current task and the reference manifold as the input to obtain the distance, which reflects the similarity of their knowledge structure. The specific algorithm process is shown by Algorithm 1.\nAlgorithm 1 Calculate the distance between two knowledge structures\nInput: current manifold MNK d \u00bc \u00bdm1;\u2026;mNK T , reference manifold M 0NK d \u00bc \u00bdm01;\u2026;m0NK T , parameter e; L\nOutput: distance dist, transport probability matrix P 1: 8\u00f0i; j\u00de;Ci; j \u00bc mi m0j\n2\n2: K \u00bc e Ce 3: b 1n 4: for \u2018 \u00bc 1; 2; . . . ;L do 5: a 1n\nKb ; b 1n\nK>a"
        },
        {
            "heading": "6: end for",
            "text": ""
        },
        {
            "heading": "7: dist \u00bc h\u00f0K C\u00deb; ai",
            "text": "8: P \u00bc hKb; ai 9: return dist;P\nAfter the definition of knowledge structure similarity is completed, we hope to seek similar knowledge structure from prior knowledge to assist in the learning of current tasks. Therefore, T basei from base-class dataset Dbase and corresponding manifold set fMbasei jMbasei \u00bc sample\u00f0qbasei \u00deg are generated.\nMbase? \u00bc arg minMibase C\u00f0M ;M base i \u00de (5)\nAccording to the optimal transport distance between manifold sets, we can find the T base? closest to the current T , and trace back to the corresponding category with rich samples in the base-class dataset, which can assist in sample generation. In other words, each time learning from novel-class information, the similar knowledge structure in the base-class can be referred to for knowledge transfer. For example, samples of the novel task are generated by learning the offset.\nIt is worth mentioning that the proposed method usually chooses Topk closest manifolds instead of only considering the closest one, which is similar to ensemble approach to make the algorithm more stable."
        },
        {
            "heading": "3.4 Data Generation Method",
            "text": "Based on manifold matching, for each new task T , one or more matching tasks T base? and its corresponding feature manifold of a large number of samples in the base-class dataset are obtained.\nThe traditional data generation methods in semantic space mainly utilize a pair of intra-class sample points to generate offsets. For example, methods such as Delta-Encoder [16] and DTN (Diversity Transfer Network) [17] do not consider the distribution of samples. If an offset is added to the sample points at the boundary of the distribution, it is easy to generate wrong samples, so it only considers the significant contribution to the classification and ignores its low confidence. In contrast, another type of method, such as GLICO [15], uses interpolation between two sample points to generate the new samples. The new samples exist in the convex area, with high confidence, but it is difficult to improve the classification performance. The shortcomings of the two types of existing methods are shown in Figs. 4a and 4b.\nFig. 4a shows that the interpolation of the convex area is difficult to affect the interface, and Fig. 4b shows that simply considering the offset learning is easy to produce sample points that do not belong to the current distribution.\nTherefore, in order to ensure high confidence of the generated samples and significant contribution to the classification at the same time, new samples are generated based on intra-class distribution. In Fig. 4c, the advantages of the proposed method are clearly demonstrated, ~x is located in the convex region, while xnew\nadds an offset to ~x to make it possible to rush out of the convex area and close to the latent real sample that has not been seen.\nThe data generation method is divided into two steps.\n(1) The first step is to find the maximum variation direction within each class in base-class dataset corresponding to T base? and other offsets.\nTo be specific, assuming that there is the centralized intra-class data x, the corresponding w which is the maximum variation direction can be obtained by solving the following problem.\nmax w Var\u00f0x\u00de \u00bc 1 n Xn i\u00bc1 xTi w 2 \u00bc wT 1 n Xn i\u00bc1 xix T i\n! w\ns:t:jjwjj22 \u00bc 1 (6)\nThe above problems can be solved by Lagrange multiplier method and transformed into solving the eigenvector corresponding to the largest eigenvalue of covariance matrix. The w is the intra-class maximum variation direction of prior knowledge structure.\nTherefore, increasing training samples along this direction can effectively increase the diversity of datasets. For example, in Fig. 5, adding an offset to a silver cat can generate a golden cat. In addition, global manifold offset D1 and central attraction offset D2 are also noteworthy.\nSpecifically, the D1 refers to the offset between the center of the current manifold and the matched manifold, and the D2 refers to the offset between the center of each category of the current manifold and the matched manifold.\n(2) In the second step, based on the Dirichlet distribution, the basic new sample is determined by the weighted sum of intra-class sample points in current novel task to ensure that it must be inside the convex area. Then, the generated data is generated by adding an offset to the basic new sample.\nTo be specific, assuming h obeys Dirichlet distribution. h Dirichlet a1; a2; . . . ; am\u00f0 \u00de, and Pm i\u00bc1\nhi \u00bc 1. The basic new sample ~x is determined by the weighted sum of intra-class sample points in current novel\ntask to ensure that it must be inside the convex area, ~x \u00bc P hixi; ~x 2 conv\u00f0x\u00de.\nAfter adding the offsets, the samples generated by this method ensure high confidence and significant contribution to the classification, as shown in Fig. 4c. Therefore, the formula for our final sample generation is as follows: xnew \u00bc X\nhixi \u00fe X2 j\u00bc1 ajDj \u00fe bw (7)\n3.5 Classifier\nHere is a brief introduction to the PT-MAP (Power Transform-Maximum A Posteriori) algorithm [32], which will be combined with the dynamic analogical association algorithm to verify performance.\nThe algorithm assumes that each class distribution is a Gaussian distribution with different means, and the mean value is a prototype vector.\nTherefore, the following problem needs to be solved, where f represents the representation vector of the image and lk represents the prototype vector.\ny\u0302if g; lkf g \u00bc arg max yif g lkf g Y i P fijyi\u00f0 \u00de\n\u00bc arg max yif g; lkf g Y i e fi lk\u00f0 \u00deT k\u00f0 \u00de 1 fi lk\u00f0 \u00de 2\n\u00bc arg min yif g lkf g\nX fi lk\u00f0 \u00deT k\u00f0 \u00de 1 fi lk\u00f0 \u00de\n(8)\nIf it is assumed that the covariance matrices are equal, then the above formula is transformed into the following formula.\narg min \u2018 f i\u00f0 \u00def g2C; lkf g X i;k \u00f0jjfi lk jj2\u00deP\u00f0\u2018 fi\u00f0 \u00de \u00bc k\u00de (9)\nFor labeled data f\u00f0fi; yi\u00deg, P\u00f0\u2018 fi\u00f0 \u00de \u00bc yi\u00de \u00bc 1, and its value is fixed, for unlabeled data f\u00f0fj\u00deg, P\u00f0\u2018 fj \u00de can be learned. Therefore, this method is also a transductive inference. The above problem is transformed into an optimal transportation problem, so it can be solved by the sinkhorn algorithm.\nP \u00bc Sinkhorn\u00f0C; u; v; k\u00de \u00bc arg min\nP2U\u00f0u;v\u00de X ij PijCij \u00fe kH\u00f0P\u00de (10)\nWhen a new prototype is obtained, the original prototype vector can be updated. Then the algorithm iterates many times to obtain a reasonable prototype."
        },
        {
            "heading": "4 Experiments",
            "text": "The standardized few-shot classification benchmark is used to evaluate the performance of the proposed method. The effectiveness of the proposed method has been proved based on comparison with existing methods and ablation experiments. It should be noted that the proposed method emphasizes the importance of manifolds, so only experiments are conducted on 5-shot."
        },
        {
            "heading": "4.1 Implementation Details",
            "text": "The 1000 5-way 5-shot classification tasks on miniImageNet and CUB (Caltech-UCSD Birds) are evaluated. It should be noted that our method emphasizes the importance of manifolds, so experiments\nare conducted only on 5-shot setting. Query set in task contains 15 images per class. The average accuracy of these few shot tasks is reported along with the 95% confidence interval based on Gaussian distribution hypothesis.\nThe WRN (Wide Residual Network), which is a wide residual network of 28 layers and width factor 10, is used as backbone to obtain the features in experiments. The WRN is trained following the same settings as S2M2 [13]. For each dataset, the feature extractor is trained on base-class dataset to learn the representation of images and test the performance on novel-class dataset.\nThe tuned hyperparameters with validation classes for miniImageNet and CUB are shown in Table 1."
        },
        {
            "heading": "4.2 Quantitative Comparison",
            "text": "Following the standard setting, Table 2 provides the comparison results on the miniImagenet and CUB with the 95% confidence interval. The comparative existing methods are categorized into two groups, NonDataGen (few-shot learning algorithm without data generation) and DataGen (few-shot learning algorithm based on data generation). It can be clearly observed that the proposed method outperforms existing methods in the 5-way 5-shot setting, with gains that are consistent across different datasets.\nFor instance, compared to well-known MAML (Model-Agnostic Meta-Learning) [26] and DeltaEncoder [16], the proposed method brings improvements of nearly 26% and 20% respectively, under the same standard setting. In addition, the WRN used in the proposed method is trained according to S2M2 [13], and good representation can be obtained without complex meta learning. Combined with dynamic analogical association, its performance is significantly better than LEO (Latent Embedding Optimization) [32] and S2M2 [13], which also use WRN as the backbone network.\nMore importantly, because the core of our algorithm is data generation, we need to pay more attention to the combination with state-of-the-art methods and whether it can further improve the performance. Our method surpasses the PT-MAP algorithm [32] with the same representation and the same classifier, which proves that the usefulness of dynamic analogical association. It also shows that it can be combined with the state-of-the-art methods to further improve the performance in different few-shot learning scenarios without relying on any additional information from other datasets."
        },
        {
            "heading": "4.3 Synthetic Experiments",
            "text": "Synthesis experiments show whether the proposed method can generate more meaningful data points. For the 5-way 5-shot task, based on t-SNE (t-distributed Stochastic Neighbor Embedding), the samples of the support set, the samples generated by the proposed method and the samples of the query set are drawn on a 2D view, as shown in the Fig. 6, in which different colors represent different categories; forks represent the samples of the support set; circles represent the synthetic samples; light colored pentagons represent the samples of the query set.\nAs can be seen from the Fig. 6, the visualization of the synthesized samples reveals the following points:\n1. The synthesized samples are not concentrated in the center of the support set samples and have a certain tendency, and some synthesized sample points rush out of the convex region composed of the support set samples. This shows that non-trivial samples are generated. In addition, the results of the synthesis experiment are consistent with the expected results speculated in Fig. 4c.\n2. More importantly, the green and purple synthesized samples which rush out of the convex region are close to the samples of the query set (real latent samples) that are invisible to the proposed method, which shows that manifold matching plays a positive role and the proposed dynamic analogical association algorithm can generate meaningful samples."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "Considering that the proposed method is the simplest way to use prior knowledge by analogical association, the performance improvement of existing algorithms combined with dynamic analogical association should be paid attention to illustrate the importance of using prior knowledge more flexibly.\nTherefore, in this subsection, the ablation studies are conducted to evaluate the performance of the proposed algorithm with quantitative results.\nBy comparing the accuracy of no sample generation and sample generation based on dynamic analogical association in Table 3, it can be clearly found that the proposed method improves the performance of two common classifiers, which proves the generality and effectiveness of the algorithm."
        },
        {
            "heading": "4.5 Manifold Matching Does Work",
            "text": "In order to verify whether the manifold matching works, the most intuitive method is to see whether the distance between our manifold sampling and the current task is significantly different.\nFig. 7 shows the maximum and minimum distances, indicating that the current task is only similar to some knowledge structures, but is significantly different from other knowledge structures.\nAs we all know, traditional methods do not treat or use prior knowledge differently, but for a certain task, the part of the knowledge structure of prior knowledge is only needed. Therefore, it is difficult to achieve better results if only considering the use of all prior information without careful analysis."
        },
        {
            "heading": "5 Conclusion",
            "text": "It can be observed that our method can lead to consistent improvement of few-shot learning tasks on different image classification datasets. More importantly, we explained the importance of analogical association based on prior knowledge. Relevant researchers need to re-examine how to make better use of prior knowledge.\nIn the future, we believe that there will be other or more advanced methods combined with manifold matching to further improve algorithm performance, such as meta-learning. This work opens up a path for further exploration.\nFunding Statement: This work was supported by The National Natural Science Foundation of China (No. 61402537), Sichuan Science and Technology Program (Nos. 2019ZDZX0006, 2020YFQ0056), the West Light Foundation of Chinese Academy of Sciences (201899) and the Talents by Sichuan provincial Party Committee Organization Department, and Science and Technology Service Network Initiative (KFJ-STSQYZD-2021-21-001).\nConflicts of Interest: The authors declare that they have no conflicts of interest to report regarding the present study."
        }
    ],
    "title": "Dynamic Analogical Association Algorithm Based on Manifold Matching for Few-Shot Learning",
    "year": 2023
}