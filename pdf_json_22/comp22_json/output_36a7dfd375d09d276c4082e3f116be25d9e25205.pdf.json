{
    "abstractText": "Despite the impressive results achieved by deep learning based 3D reconstruction, the techniques of directly learning to model 4D human captures with detailed geometry have been less studied. This work presents a novel framework that can effectively learn a compact and compositional representation for dynamic human by exploiting the human body prior from the widely used SMPL parametric model. Particularly, our representation, named H4D, represents a dynamic 3D human over a temporal span with the SMPL parameters of shape and initial pose, and latent codes encoding motion and auxiliary information. A simple yet effective linear motion model is proposed to provide a rough and regularized motion estimation, followed by per-frame compensation for pose and geometry details with the residual encoded in the auxiliary code. Technically, we introduce novel GRU-based architectures to facilitate learning and improve the representation capability. Extensive experiments demonstrate our method is not only efficacy in recovering dynamic human with accurate motion and detailed geometry, but also amenable to various 4D human related tasks, including motion retargeting, motion completion and future prediction. Please check out the project page for video and code: https://boyanjiang.github.io/H4D/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Boyan Jiang"
        },
        {
            "affiliations": [],
            "name": "Yinda Zhang"
        },
        {
            "affiliations": [],
            "name": "Xingkui Wei"
        },
        {
            "affiliations": [],
            "name": "Xiangyang Xue"
        },
        {
            "affiliations": [],
            "name": "Yanwei Fu"
        }
    ],
    "id": "SP:2359c211f9ad6febbd944565a41bfa7b4b92b52d",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas Guibas"
            ],
            "title": "Representation learning and adversarial generation of 3d point clouds",
            "venue": "arXiv preprint arXiv:1707.02392,",
            "year": 2017
        },
        {
            "authors": [
                "Emre Aksan",
                "Peng Cao",
                "Manuel Kaufmann",
                "Otmar Hilliges"
            ],
            "title": "A spatio-temporal transformer for 3d human motion prediction",
            "venue": "arXiv e-prints, pages arXiv\u20132004,",
            "year": 2020
        },
        {
            "authors": [
                "Emre Aksan",
                "Manuel Kaufmann",
                "Otmar Hilliges"
            ],
            "title": "Structured prediction helps 3d human motion modelling",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Thiemo Alldieck",
                "Marcus Magnor",
                "Bharat Lal Bhatnagar",
                "Christian Theobalt",
                "Gerard Pons-Moll"
            ],
            "title": "Learning to reconstruct people in clothing from a single rgb camera",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Thiemo Alldieck",
                "Marcus Magnor",
                "Weipeng Xu",
                "Christian Theobalt",
                "Gerard Pons-Moll"
            ],
            "title": "Detailed human avatars from monocular video",
            "venue": "In 2018 International Conference on 3D Vision (3DV),",
            "year": 2018
        },
        {
            "authors": [
                "Thiemo Alldieck",
                "Gerard Pons-Moll",
                "Christian Theobalt",
                "Marcus Magnor"
            ],
            "title": "Tex2shape: Detailed full human body geometry from a single image",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Emad Barsoum",
                "John Kender",
                "Zicheng Liu"
            ],
            "title": "Hp-gan: Probabilistic 3d human motion prediction via gan",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Bharat Lal Bhatnagar",
                "Cristian Sminchisescu",
                "Christian Theobalt",
                "Gerard Pons-Moll"
            ],
            "title": "Combining implicit function learning and parametric models for 3d human reconstruction",
            "venue": "In European Conference on Computer Vision (ECCV). Springer, aug 2020",
            "year": 2020
        },
        {
            "authors": [
                "Federica Bogo",
                "Angjoo Kanazawa",
                "Christoph Lassner",
                "Peter Gehler",
                "Javier Romero",
                "Michael J Black"
            ],
            "title": "Keep it smpl: Automatic estimation of 3d human pose and shape from a single image",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Yujun Cai",
                "Lin Huang",
                "Yiwei Wang",
                "Tat-Jen Cham",
                "Jianfei Cai",
                "Junsong Yuan",
                "Jun Liu",
                "Xu Yang",
                "Yiheng Zhu",
                "Xiaohui Shen"
            ],
            "title": "Learning progressive joint propagation for human motion prediction",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Chabra",
                "Jan Eric Lenssen",
                "Eddy Ilg",
                "Tanner Schmidt",
                "Julian Straub",
                "Steven Lovegrove",
                "Richard Newcombe"
            ],
            "title": "Deep local shapes: Learning local sdf priors for detailed 3d reconstruction",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "ChaoWen",
                "Yinda Zhang",
                "Zhuwen Li",
                "Yanwei Fu"
            ],
            "title": "Pixel2mesh++: Multi-view 3d mesh generation via deformation",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Julian Chibane",
                "Thiemo Alldieck",
                "Gerard Pons-Moll"
            ],
            "title": "Implicit functions in feature space for 3d shape reconstruction and completion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078,",
            "year": 2014
        },
        {
            "authors": [
                "Hongsuk Choi",
                "Gyeongsik Moon",
                "Kyoung Mu Lee"
            ],
            "title": "Beyond static features for temporally consistent 3d human pose and shape from a video",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Christopher B Choy",
                "Danfei Xu",
                "JunYoung Gwak",
                "Kevin Chen",
                "Silvio Savarese"
            ],
            "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Enric Corona",
                "Albert Pumarola",
                "Guillem Aleny\u00e0",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "Smplicit: Topology-aware generative model for clothed people",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Philipp Erler",
                "Paul Guerrero",
                "Stefan Ohrhallinger",
                "Niloy J. Mitra",
                "Michael Wimmer"
            ],
            "title": "Points2surf: Learning implicit surfaces from point clouds",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Haoqiang Fan",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "A point set generation network for 3d object reconstruction from a single image",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "P Thomas Fletcher",
                "Conglin Lu",
                "Stephen M Pizer",
                "Sarang Joshi"
            ],
            "title": "Principal geodesic analysis for the study of nonlinear statistics of shape",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2004
        },
        {
            "authors": [
                "Rohit Girdhar",
                "David F. Fouhey",
                "Mikel Rodriguez",
                "Abhinav Gupta"
            ],
            "title": "Learning a predictable and generative vector representation for objects",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Thibault Groueix",
                "Matthew Fisher",
                "Vladimir G Kim",
                "Bryan C Russell",
                "Mathieu Aubry"
            ],
            "title": "Atlasnet: A papierm\u00e2ch\u00e9 approach to learning 3d surface generation",
            "venue": "arXiv preprint arXiv:1802.05384,",
            "year": 2018
        },
        {
            "authors": [
                "Riza Alp Guler",
                "Iasonas Kokkinos"
            ],
            "title": "Holopose: Holistic 3d human reconstruction in-the-wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Boyi Jiang",
                "Juyong Zhang",
                "Yang Hong",
                "Jinhao Luo",
                "Ligang Liu",
                "Hujun Bao"
            ],
            "title": "Bcnet: Learning body and cloth shape from a single image",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Boyan Jiang",
                "Yinda Zhang",
                "Xingkui Wei",
                "Xiangyang Xue",
                "Yanwei Fu"
            ],
            "title": "Learning compositional representation for 4d captures with neural ode",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Chiyu Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "venue": "In Proceedings 9 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ian T Jolliffe"
            ],
            "title": "Principal component analysis",
            "venue": "Technometrics, 45(3):276,",
            "year": 2003
        },
        {
            "authors": [
                "Angjoo Kanazawa",
                "Michael J Black",
                "David W Jacobs",
                "Jitendra Malik"
            ],
            "title": "End-to-end recovery of human shape and pose",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Angjoo Kanazawa",
                "Shubham Tulsiani",
                "Alexei A Efros",
                "Jitendra Malik"
            ],
            "title": "Learning category-specific mesh reconstruction from image collections",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Angjoo Kanazawa",
                "Jason Y Zhang",
                "Panna Felsen",
                "Jitendra Malik"
            ],
            "title": "Learning 3d human dynamics from video",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Muhammed Kocabas",
                "Nikos Athanasiou",
                "Michael J Black"
            ],
            "title": "Vibe: Video inference for human body pose and shape estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nikos Kolotouros",
                "Georgios Pavlakos",
                "Michael J Black",
                "Kostas Daniilidis"
            ],
            "title": "Learning to reconstruct 3d human pose and shape via model-fitting in the loop",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Christoph Lassner",
                "Javier Romero",
                "Martin Kiefel",
                "Federica Bogo",
                "Michael J Black",
                "Peter V Gehler"
            ],
            "title": "Unite the people: Closing the loop between 3d and 2d human representations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Verica Lazova",
                "Eldar Insafutdinov",
                "Gerard Pons-Moll"
            ],
            "title": "360-degree textures of people in clothing from a single image",
            "venue": "In 2019 International Conference on 3D Vision",
            "year": 2019
        },
        {
            "authors": [
                "Yiyi Liao",
                "Simon Donne",
                "Andreas Geiger"
            ],
            "title": "Deep marching cubes: Learning explicit surface representations",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Oliver Wang",
                "Bryan C Russell",
                "Eli Shechtman",
                "Vladimir G Kim",
                "Matthew Fisher",
                "Simon Lucey"
            ],
            "title": "Photometric mesh optimization for video-aligned 3d object reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Shichen Liu",
                "Tianye Li",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "venue": "The IEEE International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Shaohui Liu",
                "Yinda Zhang",
                "Songyou Peng",
                "Boxin Shi",
                "Marc Pollefeys",
                "Zhaopeng Cui"
            ],
            "title": "Dist: Rendering deep implicit signed distance function with differentiable sphere tracing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Smpl: A skinned multiperson linear model",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2015
        },
        {
            "authors": [
                "Naureen Mahmood",
                "Nima Ghorbani",
                "Nikolaus F Troje",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Amass: Archive of motion capture as surface shapes",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Wei Mao",
                "Miaomiao Liu",
                "Mathieu Salzmann",
                "Hongdong Li"
            ],
            "title": "Learning trajectory dependencies for human motion prediction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Julieta Martinez",
                "Michael J Black",
                "Javier Romero"
            ],
            "title": "On human motion prediction using recurrent neural networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Dushyant Mehta",
                "Oleksandr Sotnychenko",
                "Franziska Mueller",
                "Weipeng Xu",
                "Mohamed Elgharib",
                "Pascal Fua",
                "Hans-Peter Seidel",
                "Helge Rhodin",
                "Gerard Pons-Moll",
                "Christian Theobalt"
            ],
            "title": "Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera",
            "year": 1907
        },
        {
            "authors": [
                "Dushyant Mehta",
                "Oleksandr Sotnychenko",
                "Franziska Mueller",
                "Weipeng Xu",
                "Srinath Sridhar",
                "Gerard Pons-Moll",
                "Christian Theobalt"
            ],
            "title": "Single-shot multi-person 3d pose estimation from monocular rgb",
            "venue": "In 2018 International Conference on 3D Vision (3DV),",
            "year": 2018
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Lars Mescheder",
                "Michael Oechsle",
                "Andreas Geiger"
            ],
            "title": "Occupancy flow: 4d reconstruction by learning particle dynamics",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Pablo Palafox",
                "Alja\u017e Bo\u017ei\u010d",
                "Justus Thies",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "Npms: Neural parametric models for 3d deformable shapes",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Timo Bolkart",
                "Ahmed A.A. Osman",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Expressive body capture: 3D hands, face, and body from a single image",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Luyang Zhu",
                "Xiaowei Zhou",
                "Kostas Daniilidis"
            ],
            "title": "Learning to estimate 3d human pose and shape from a single color image",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Gerard Pons-Moll",
                "Sergi Pujades",
                "Sonny Hu",
                "Michael J Black"
            ],
            "title": "Clothcap: Seamless 4d clothing capture and retargeting",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Charles R. Qi",
                "Wei Liu",
                "Chenxia Wu",
                "Hao Su",
                "Leonidas J. Guibas"
            ],
            "title": "Frustum pointnets for 3d object detection from RGB-D data",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Nikhila Ravi",
                "Jeremy Reizenstein",
                "David Novotny",
                "Taylor Gordon",
                "Wan-Yen Lo",
                "Justin Johnson",
                "Georgia Gkioxari"
            ],
            "title": "Accelerating 3d deep learning with pytorch3d",
            "year": 2007
        },
        {
            "authors": [
                "Davis Rempe",
                "Tolga Birdal",
                "Aaron Hertzmann",
                "Jimei Yang",
                "Srinath Sridhar",
                "Leonidas J Guibas"
            ],
            "title": "Humor: 3d human motion model for robust pose estimation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Davis Rempe",
                "Tolga Birdal",
                "Yongheng Zhao",
                "Zan Gojcic",
                "Srinath Sridhar",
                "Leonidas J Guibas"
            ],
            "title": "Caspr: Learning canonical spatiotemporal point cloud representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Javier Romero",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Embodied hands: Modeling and capturing hands and bodies together",
            "venue": "ACM Transactions on Graphics, (Proc. SIG- GRAPH Asia),",
            "year": 2017
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Zeng Huang",
                "Ryota Natsume",
                "Shigeo Morishima",
                "Angjoo Kanazawa",
                "Hao Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Tomas Simon",
                "Jason Saragih",
                "Hanbyul Joo"
            ],
            "title": "Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Jinlong Yang",
                "Qianli Ma",
                "Michael J Black"
            ],
            "title": "Scanimate: Weakly supervised learning of skinned clothed avatar networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Akash Sengupta",
                "Ignas Budvytis",
                "Roberto Cipolla"
            ],
            "title": "Probabilistic 3d human shape and pose estimation from multiple unconstrained images in the wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Robert W Sumner",
                "Jovan Popovi\u0107"
            ],
            "title": "Deformation transfer for triangle meshes",
            "venue": "ACM Transactions on graphics (TOG),",
            "year": 2004
        },
        {
            "authors": [
                "Pavel Tokmakov",
                "Yu-Xiong Wang",
                "Martial Hebert"
            ],
            "title": "Learning compositional representations for few-shot recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Raquel Urtasun",
                "David J Fleet",
                "Pascal Fua"
            ],
            "title": "Temporal motion models for monocular and multiview 3d human body tracking",
            "venue": "Computer vision and image understanding,",
            "year": 2006
        },
        {
            "authors": [
                "Nanyang Wang",
                "Yinda Zhang",
                "Zhuwen Li",
                "Yanwei Fu",
                "Wei Liu",
                "Yu-Gang Jiang"
            ],
            "title": "Pixel2mesh: Generating 3d mesh models from single rgb images",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Peng-Shuai Wang",
                "Yang Liu",
                "Yu-Xiao Guo",
                "Chun-Yu Sun",
                "Xin Tong"
            ],
            "title": "O-cnn: Octree-based convolutional neural networks for 3d shape analysis",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Shaofei Wang",
                "Andreas Geiger",
                "Siyu Tang"
            ],
            "title": "Locally aware piecewise transformation fields for 3d human mesh registration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Ira Kemelmacher- Shlizerman"
            ],
            "title": "Photo wake-up: 3d character animation from a single photo",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Y Zhang",
                "Panna Felsen",
                "Angjoo Kanazawa",
                "Jitendra Malik"
            ],
            "title": "Predicting 3d human dynamics from video",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The vanilla SMPL based parametric representations have been extensively studied and widely utilized for modeling 3D human shapes, and thus shown critical impacts to many human-centric tasks, such as pose estimation [16,24,31,33, 35, 47] and body shape fitting [9, 18, 34, 54, 65]. However, these representations are arguably insufficient for applications involving dynamic signals, e.g. 3D moving humans (Fig.1 top), since the temporal information is not captured.\nAs solutions, 4D representations are proposed and can\n\u2217 indicates equal contributions. Boyan Jiang, Xingkui Wei and Xiangyang Xue are with the School of Computer Science, Fudan University. Yanwei Fu is with the School of Data Science, Fudan University, and Fudan ISTBI\u2014ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China.\nbe in general categorized into free-form and prior-based methods depending on the 3D representation of the output shape (Fig. 1). The free-form methods leveraging Neural ODE [13] and deep implicit function [26, 50] often rely on computational expensive architectures to learn the compact latent spaces and reconstruct 4D sequences. Unfortunately, since the human body prior is not explicitly modeled, the reconstruction results of these methods may contain obvious geometry artifacts such as missing hands, and their modeling errors accumulate rapidly over time. On the other hand, prior-based methods [31,33,73] are mostly derived from the SMPL parametric model [41], which typically employs one shape parameter and a series of pose parameters to model dynamic sequences. Although they produce plausible results, their motion representations are not compact or only\nsupport a small time span, e.g. \u00b1 5 frames [31]. In this paper, we propose H4D, which is a novel neural representation for Human 4D modeling that combines the merits of both the prior-based and free-form solutions. To reflect the compositional natures [67], we encode each dynamic human sequence with SMPL parameters representing shape and initial pose, and a compact latent code representing temporal motion, which can then be used to reconstruct the input sequence through a decoder. At the core of this decoder, a simple yet effective prior model extended from SMPL [41] is designed to provide coarse but long-term estimation of the 3D human geometry and motion. This can ensure more complete and plausible outputs compared to the prior arts of free-form reconstructions [26, 50], but potentially be inclined to suffer from the limited representation capability. To this end, we add an auxiliary latent code to compensate the inaccurate motion and enrich the geometry details. Such a representation takes full advantage of parametric models by exploiting strong prior based regularization for plausible initialization and complement them with powerful deep networks to facilitate the human 4D modeling with impressive motion and geometry accuracy.\nOur representation is learned via an auto-encoding framework. The encoder predicts the SMPL parameters and latent codes for each aspect from densely sampled point clouds, which are fed into the decoder to reconstruct the identical input dynamic human sequence. Once trained, the encoder and decoder are both fixed to support various applications, such as motion retargeting, completion, and prediction, through either forward propagation (feed-forward) or backward optimization (auto-decoding) depending on the inputs. We design novel Gated Recurrent Unit (GRU) [15] based architectures for both encoder and decoder to benefit the model performance while working in either mode. In feed-forward mode, we do not require the input point clouds to be temporarily tracked, i.e. the point trajectories like in previous work [26, 50]. This simplifies the training requirements and enhances the applicability of high-level applications. In auto-decoding mode, our model leverages the temporal information for optimization, which is critical for robustness to recover detailed motion and geometry. Contributions We propose H4D, a compact and compositional representation for 4D human captures, which combines a linear prior model with the residual encoded in a learned auxiliary code. The framework is learned via 4D reconstruction, and the latent representation can be extracted from either nonregistered point clouds in a feed-forward fashion or auto-decoding through optimization. Extensive experiments show that our representation and GRU-based architecture are effective in recovering accurate dynamic human sequences and providing robust performance for a variety of 4D human related applications, including motion retargeting/completion and future prediction."
        },
        {
            "heading": "2. Related Work",
            "text": "4D Representation There has been a lot of work aiming to reconstruct 3D objects based on various representations, such as 3D voxels [17, 22, 70], point clouds [1, 20, 56, 57], meshes [12, 23, 30, 37, 69] and implicit functions [11, 14, 19, 27, 48, 52]. However, the deep representation for 4D data, i.e. a time-varying 3D object, has received less attention mostly due to the challenge of encoding the temporal dimension. Pioneer work mostly relies on Neural ODE [13] and combines with occupancy network [48, 50], point clouds [60], and compositional property [26]. Despite state-of-the-art performance in various motion-related tasks, Neural ODE tends to accumulate errors over time that causes incomplete geometry, and slows down training convergence and inference run-time. In contrast, our model relies on the prior model for comprehensive geometry and motion and the recurrent network for efficient inference. Human Body Estimation For human shape and pose estimation [9, 16, 24, 33\u201335, 47, 54] or motion prediction [2, 3, 7, 10, 44, 45], most of works are based on SMPL or its extension [41, 53, 61]. Specifically, HMMR [31] learns to encode temporal information by reconstructing a small number of past and future frames. Zhang et al. [73] propose the first autoregressive model for predicting 3D human motion from image sequences. VIBE [33] leverages GRU to regress SMPL parameters, and designs an adversarial learning framework to predict temporal transitions. Though produce plausible motion, the motion representations in these methods are either implicit [33], coupled with geometry [31, 33, 73], or limited to a short temporal range [31]. In contrast, we formulate the motion with a prior model based on PCA [28, 68] (its non-linear extension PGA [21] is also applicable) for long-range context followed by per-frame adjustments controlled by a learned latent code, which is compact, compositional, and tolerant to error accumulation. Fine-grained Human Reconstruction Many human reconstruction methods [9, 30, 31, 33, 34, 47] are limited to the unclothed body as SMPL-based models may suffer from limited expressive power in the shape space. To capture fine-grained geometry, such as clothing or hair, the neural implicit function has been used to reconstruct a free-form surface [8, 14, 18, 62\u201364, 71, 74], but it is still challenging to recover detailed structure like fingers, face, or wrinkles on clothes. Another family of approaches extends the parametric model by predicting per-vertex displacements upon the canonical body mesh [4\u20136,36,42,72], which achieves a good balance between the expressiveness and prior regularization. Most related to us, CAPE [42] trains a generator to synthesize fine-grained geometry from a latent space, and can run in the auto-decoding mode for fitting. However, it is empirically not robust to work with temporal frames and sensitive to the errors in imperfect poses, which is not ideal to plug and play in our scenario."
        },
        {
            "heading": "3. Method",
            "text": "This section introduces our H4D representation, which is learned through reconstruction task (Fig. 2). Given a 3D human model performing motion in a time span (mesh sequence of 30 frames), we sample a point cloud of 8192 points from each as the input sequence to the network. Note that we do not assume the temporal correspondences among frames (e.g. point trajectories) are available, which are critically required by previous 4D representations [26, 50].\nThe input sequence is fed into a compositional encoder to extract SMPL parameters representing shape and initial body pose, together with latent codes representing temporal motion and auxiliary of additional compensation on motion and geometry (Sec. 3.1). To reconstruct the input temporal sequence, the shape, initial pose, and motion codes are combined through a pre-learned Linear Motion Model (LMM) to generate a rough estimation of per-frame 3D shapes represented as SMPL [41] (Sec. 3.2). Due to the limited capacity of LMM, the output, though plausible, demands additional refinements for accurate reconstruction. To this end, we feed the motion code, auxiliary code, and initial estimation to the GRU based Motion-Comp network (Sec. 3.3) and Shape-Comp network (Sec. 3.4) to predict the residual on temporal motion and shape in the canonical pose, respectively. The final sequence is obtained by deforming the refined canonical shape using the linear blending weight according to the refined per-frame poses."
        },
        {
            "heading": "3.1. Compositional Encoder",
            "text": "To keep the representation compositional, we train four separate encoders to extract SMPL parameters representing\nthe shape cs and initial pose cp, and latent codes representing motion cm and auxiliary information ca. The shape and pose encoders are implemented as PointNet-based [57] network with ResNet blocks, which take only the starting frame as input since it is sufficient to tell the canonical body shape and initial pose. On the other hand, the motion and auxiliary encoders take all frames as input since temporal information is needed. To achieve that, we firstly encode the point cloud of each frame into a feature vector using a shallow PointNet, and then further aggregate per-frame feature with a GRU layer. The feature extractor is shared between motion and auxiliary encoders, and only GRUs are trained respectively. Note that our temporal encoders can process sequences of unordered point clouds without temporal correspondences."
        },
        {
            "heading": "3.2. Linear Motion Model",
            "text": "We take the predicted cp and cm to reconstruct a coarse estimation of motion. Inspired by Urtasun et al. [68], we employ the parameter space of SMPL model and prelearn a linear model for the motion to ensure robustness. Each input temporal sequence can be represented as \u03a6 = [\u03b81, . . . \u03b8L], L = 30, where \u03b8i \u2208 R72 is the SMPL pose parameter for frame i. We then represent motion as the per-frame difference of the pose parameter from the first frame, i.e. \u03a8 = [\u03b82 \u2212 \u03b81, . . . , \u03b8L \u2212 \u03b81] \u2208 R72(L\u22121), and run a Principal Component Analysis (PCA) [28] to reduce the dimension. The input motion now can be reconstructed through the linear model: \u03a6\u0302 = [\u03b81, \u03b1T \u00b7 M + \u00b5\u03a8 + \u03b81], where \u03b1 \u2208 RK is the coefficient of principal components, M = [M1, . . . ,MK ] \u2208 R72(L\u22121)\u00d7K and \u00b5\u03a8 are the top K principal components and mean of \u03a8.\nIn practice, we found it more robust to run PCA separately for the global orientation (i.e. pelvis) and body joint rotation. We pick 4 bases for global orientation and 86 basis for body joint rotation, which explains 90% of the variance1. Finally, we plug the linear motion model into our pipeline, amenable to the output of the compositional encoder: {cpt} L\u22121 t=0 = [cp, c T m \u00b7 M + \u00b5\u03a8 + cp], where cpt is the pose parameter for frame t."
        },
        {
            "heading": "3.3. Motion Compensation Network",
            "text": "The LMM is effective in representing motion with a relatively large number of temporal frames; unfortunately, it lacks the capacity to represent motion details. As a result, the predicted pose sequences are not accurate enough.\nTo improve the motion accuracy, we build a motion compensation network (Motion-Comp) to adjust the pose parameter of each frame. Specifically, we adopt a GRU-based network [15] as it was demonstrated to be effective for processing temporal information. We concatenate the motion code cm and the auxiliary code ca to the pose parameters of each frame from LMM prediction {cpt} L\u22121 t=0 , and then feed them sequentially into GRU to produce the residual of each frame. Once the per-frame pose parameters are updated with the output of Motion-Comp network, we combine them with the shape parameter cs from the encoder into the standard SMPL decoder to reconstruct the per-frame mesh. Overall, our motion model benefits from both the strong prior in the linear motion model and the impressive capacity of the motion compensation network."
        },
        {
            "heading": "3.4. Shape Compensation Network",
            "text": "So far, we are able to reconstruct the correct motion sequences, which can be further converted to body mesh sequences via SMPL decoder. However, the predicted shapes are still inferior, as many details such as hairs or clothes are missing. This is mostly due to the constrained capacity of SMPL shape space. To enhance the geometry, the shape representation presented in CAPE [42] is introduced: a pervertex offset is estimated for the body mesh in the canonical space via a graph-based neural network conditioned on target pose. The added details would be then transferred to the target body pose via the pre-defined linear blending weight in SMPL. When combined with our framework, one straightforward way is to have the auxiliary code ca encode shape details and feed it through the CAPE decoder for per-vertex offsets. We found this works reasonably well in feed-forward mode but not the back-propagation. We suspect this might because of the inconsistent gradients from different temporal frames, especially when the pose estimation is not perfectly accurate. As a result, the compensated geometry is vaguely correct (e.g. bump on the head for some hairstyles) but not precise. To improve the stability,\n1Please refer to Supp. Mat. for visualization of principal components\nwe propose a shape compensation network (Shape-Comp), in which a GRU takes the auxiliary code ca as input and predicts a new latent vector for each temporal frame conditioned on the predicted pose. The latent vector is then fed into the graph network to predict per-vertex offset, which is similar to the CAPE decoder. We remove the VAE and adversarial loss as they empirically hurt the performance. The GRU enables information exchanging across temporal frames, which is critical for robust back-propagation when running applications like motion completion and prediction."
        },
        {
            "heading": "3.5. Training Strategy",
            "text": "Neural networks with multiple stages are highly nonlinear and could easily fall in the local minimum. We advocate a stage-wise training strategy to enhance the training stability. Specifically, we first train the shape encoder, pose encoder, point feature extractor and motion encoder jointly with the pre-learned linear motion model. Once the model converges, we enable the Motion-Comp and Shape-Comp networks for the end-to-end joint training. Similar training strategy has been commonly used by other works, e.g. BCNet [25], XNect [46] and Predicting Human Dynamics [73].\nLoss Functions Since our reconstructions are registered with SMPL topology, we use per-vertex L1-loss with the ground truth mesh as the objective function. To further alleviate the ambiguity between body shape and clothing, we add an L2-loss on the shape code cs with regard to the ground truth. During the first training stage, we use the mesh reconstructed with the motion from LMM for supervision. In the second stage, we use added loss on both meshes before and after the Shape-Comp network. The detailed formulation of the loss functions can be found in Supp. Mat."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we perform extensive experiments to verify the efficacy of our method. First, we evaluate the capacity of our representation for encoding accurate shape and motion on the tasks of 4D reconstruction and human shape and motion recovery. We then demonstrate that a large variety of 4D related applications, including motion retargeting, completion, and prediction can be achieved with high quality with our representation. Finally, we provide an ablation study to test the impact of each component in our framework on the reconstruction quality.\nDataset We use the CAPE dataset [42, 55] for training and evaluating, which is a dataset of 3D dynamic clothed humans containing 10 male and 5 female subjects wearing different types of outfits. More than 600 motion sequences of large pose variations are provided. In each sequence, the clothed body shapes are captured at 60 FPS along with corresponding meshes in the canonical pose and pose parameter of each frame. Overall, the dataset provides good di-\nversity on both 3D geometry and motion. Following OFlow [50], we divide all sequences in CAPE into subsequences of 30 frames. We use subsequences from 488 motion sequences for training, and randomly sampled 2000 subsequences from the other 123 motion sequences for testing.\nImplementation We use PyTorch to implement the model, and train with the Adam optimizer [32]. In the first stage, the learning rate is 10\u22124 with batch size 16. In the second stage, the initial learning rate is set to 10\u22124 and dropped to 10\u22125 after 200K iterations with batch size 4 due to the limitation of GPU memory. We use 4 NVIDIA GeForce RTX 2080Ti GPU cards.\nEvaluation To measure the difference between the prediction and ground truth 3D shape, we use Chamfer Distance (CD) and Volumetric IoU (IoU) [48] for free-form geometry and Per Vertex Error (PVE) for SMPL registered shape. To measure the accuracy of motion, we use Procrustes-aligned mean per joint position error (PA-MPJPE), mean per joint position error (MPJPE), and acceleration error (mm/s2) computed on 45 keypoints which include 24 joints and 21 keypoints on the face, feet and hands. Please refer to [31, 33, 48] for more details about these metrics. For temporal sequences, we take the mean score of all frames."
        },
        {
            "heading": "4.1. Representation Capability",
            "text": "We first show that our representation is capable of encoding and reconstructing human sequences with correct motion and geometry. 4D Reconstruction We compare to state-of-the-art 4D representation, Occupancy Flow (OFlow) [50] and 4D-CR [26], on mesh reconstruction from sampled point cloud inputs. As shown in Tab. 1 (I), our method significantly outperforms other methods on the 4D reconstruction accuracy. Qualitative results are shown in Fig. 3 for two temporal sequences. OFlow tends to produce incomplete geometry with missing hands, and the results from 4D-CR are overly smoothed, e.g. around face and hands. On the contrary, thanks to the human prior, our results are significantly better than others with complete geometry, correct motion, and rich details such as fingers, clothes, and hair. It is also worth noting that both OFlow and 4D-CR require point clouds with temporal correspondence as input, whereas our method can take unregistered point cloud sequences which is more convenient for many applications. Shape and Motion Recovery We then study the performance of our motion model, which consists of a linear motion model and per-frame compensations recovered\nfrom the auxiliary code. As baselines, we compare to SoTA video-based human shape and pose estimation methods HMMR [31] and VIBE [33]. Originally designed for color images input, we replace their image encoder with the point cloud encoder in our setup. Moreover, as an additional baseline, we extend 4D-CR [26] to an SMPL-based version by replacing their implicit occupancy decoder with the SMPL decoder so that it also benefits from the human prior. Since all of these methods produce only unclothed SMPL defined shapes, we disable our Shape-Comp network and use the output from the SMPL decoder for fair comparisons. All the baseline methods are retrained on our dataset. For extending 4D-CR, we train two models with registered point clouds as in their work (4D-CR-SMPL\u2217) and unregistered point clouds like us (4D-CR-SMPL) respectively. The quantitative comparisons are shown in Tab. 1 (II). Our method achieves more accurate motion estimation (as measured at body keypoints by PA-MPJPE, MPJPE, acceleration error) and SMPL shape (as measured by PVE) than HMMR and VIBE. 4D-CR-SMPL performs relatively poorly when the input point cloud is unordered and gets much better once given tracked point clouds (4DCR-SMPL\u2217) but still performs worse than our method. We provide additional comparisons to the recent motion-based human body estimation method HuMoR [59] with the task of shape and motion recovery from point clouds via autodecoding in Supp. Material."
        },
        {
            "heading": "4.2. Applications",
            "text": "Our representation can support various applications. Note that for all applications, the encoder and decoder are both fixed after training. Motion Retargeting The goal of motion retargeting is to transfer the motion sequence from one subject to another. Traditional methods typically require manual works, e.g. provide correspondences between source and target identities [66], to fulfill such a task.\nWe achieve motion retargeting without any human intervention. Taking two point cloud sequences, one as the identity (I) and the other as the motion (M ), we feed both into our compositional encoder to get SMPL parameters and latent codes for each (cIs, c I p, c I m, c I a) and (cMs , c M p , c M m , c M a ). We then conduct the motion retargeting by using (cIs, c M p , c M m ) for linear motion model, c M a for Motion-Comp network, and cIa for Shape-Comp network. Note that two ca are used for Motion-Comp and ShapeComp networks separately as they encode motion and shape information respectively.\nFor evaluation purpose, we randomly sampled 100 pairs of identity and motion sequences with L = 30 frames. We use the provided ground truth shape in the canonical pose and pose parameters provided by CAPE [42] dataset to generate motion retargeted ground truth sequences. We compare with free-form geometry based approach (OFlow and\n4D-CR) on the full geometry (in Tab. 1 (I)) and SMPL based approach (HMMR, VIBE, 4D-CR-SMPL) on the unclothed body mesh from SMPL (in Tab. 1 (II)). Our method significantly outperforms OFlow and 4D-CR. As shown in a qualitative example in Fig. 4, our method produces much more complete motion retargeting results. Note how clothing details are successfully transferred, e.g. long trousers in the identity sequence compared to shorts in the motion sequence. Our method also outperforms all the human body estimation methods, showing that the compositional encoder is more effective in extracting correct information from inputs and facilitating motion recovering.\nMotion Completion Our representation can also fulfill fitting tasks in the auto-decoding fashion, in which the SMPL parameters and latent codes are optimized to produce output similar to the observation. With this, our representation can perform motion completion, where the goal is to predict the missing data in a dynamic human sequence. For evaluation, we randomly choose 100 sequences with 30 frames from our test set. For each sequence, we randomly pick 15 frames as the observation, optimize the SMPL parameters and latent codes, reconstruct the full sequence, and then measure the geometry accuracy on the other 15 frames. Note that we use Chamfer loss with the additional prior terms borrowed from IPNet [8] on uniformly sampled points instead of PVE to simulate the case in real applications, where the observed meshes may not be registered.\nComparisons to free-form based methods and SMPL based methods are shown in Tab. 1 (I) and (III) respectively, and the qualitative results are in Supp. Material. Zhang et al. [73] uses a similar motion model to HMMR [31], so we only evaluate one of them. Overall, our method consistently outperforms all the other methods.\nMoreover, we compare the robustness of auto-decoding based fitting using our Shape-Comp network against the naive CAPE decoder, and show the error of completion w.r.t the amount of random noise added to the observed frames in Fig. 6 (b). The error of our model is consistently lower than the naive CAPE decoder and deteriorates less with increasing noise. This is presumably because CAPE performs per-frame optimization, which may confuse the latent space if the gradients are not consistent from temporal frames, whereas we use GRU to model the temporal sequence for more robustness.\nLast but not least, our model can also complete the temporal sequences from partial spatial observation. To show this, we generate one depth image per-frame from a camera rotating concurrently with the motion of the 3D shape, and run auto-decoding based fitting to complete the sequence. This can be also considered as a typical non-rigid fusion with known camera poses. We show the qualitative and quantitative comparisons to NPMs [51] in Supp. Material.\nFuture Prediction Our representation also supports future\nprediction. Specifically, we run a fitting algorithm on the first 20 frames, generate the SMPL parameters and latent codes, and then reconstruct the full sequence to predict the 10 frames in the future. Tab. 1 (I, III) and Fig. 5 show the comparison to the previous methods. Again, we obtain significantly better performance than other 4D representation methods (OFlow and 4D-CR). When comparing only the motion accuracy using SMPL mesh with previous work on motion prediction (HMMR [31], Zhang et al. [73] and 4D-CR-SMPL [26]), our method still achieves better performance. Moreover, we empirically found that, though given\npose prior terms during backward optimization, these baseline methods are more easily to produce unnatural poses than us, and predict unreasonable motions as shown in Fig. 5, possibly because our PCA-based motion model provides regularization and global context for output motions."
        },
        {
            "heading": "4.3. Ablation Study",
            "text": "In this section, we perform an ablation study and show the quantitative results to demonstrate the effect of the major designs in our method. The visualization example can be found in Supp. Material. Motion Model We first study the effect of the linear motion model and auxiliary code for motion recovery. We compare the ablation cases on the output of the SMPL decoder with the registered SMPL model on the ground truth mesh, which removes the free-form deformation and allows us to focus on the motion quality. In Fig. 6 (a), we show the performance of our model removing the linear motion model (\u201c-LMM\u201d), which directly updates the initial pose code, or removing Motion-Comp network (\u201c-Motion ca\u201d), which only relies on the linear motion model (LMM) for motion\nrecovery. In either case, the motion accuracy drops consistently as measured by all metrics, indicating the necessity of combining the prior model with learned compensations. Shape Model We then verify if the auxiliary code helps to recover the detailed geometry. In Fig. 6 (a), we show the performance of the final mesh without and with auxiliary code-driven shape compensation (the last two rows), which is measured by PVE between the output mesh with the ground truth clothed mesh. The advantage of the shape compensation can also be found in Fig. 4 and 5, which show that the auxiliary code helps to improve the geometry details when comparing our results with the SMPL outputs from VIBE, HMMR or 4D-CR-SMPL. Encoder Last but not least, we verify the effectiveness of our GRU-based temporal encoder. We replace our temporal encoder with the PointNet adopted in OFlow [50] and 4DCR [26], and see a significant performance drop (\u201c-GRU Enc.\u201d), which shows our GRU-based encoder helps to extract temporal information from the point cloud sequences without temporal correspondence."
        },
        {
            "heading": "5. Conclusion",
            "text": "This paper introduces H4D, a compact and compositional neural representation for 4D human captures, which combines the merits of both the prior-based and free-form solutions. A novel framework is designed for learning our representation, which encodes the input point cloud sequences into the SMPL parameters of shape and initial pose, and latent codes of motion and auxiliary information. Extensive experiments on 4D reconstruction, shape and motion recovery, motion retargeting, completion and prediction validate the efficacy of the proposed approach."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by NSFC Project (62176061), Shanghai Municipal Science and Technology Major Project (2018SHZDZX01). The corresponding authors are Xiangyang Xue, and Yanwei Fu."
        },
        {
            "heading": "1. Implementation Details",
            "text": "In this section, we first provide network architectures used for the compositional encoder, Motion-Comp and Shape-Comp networks in our framework. Next, we explain the strategy of choosing the number of principal components for our linear motion model. Finally, we discuss more details in our experiments."
        },
        {
            "heading": "1.1. Network Architecture",
            "text": "Compositional Encoder Both the shape encoder and the initial pose encoder take point cloud of the first time step as input, and we adopted the same architecture as the spatial encoder in Occupancy Flow (OFlow) [50]. The network is a variation of PointNet [57] which has five residual blocks as show in Fig. 7a. Each of the first four blocks has an additional max-pooling operation to obtain the aggregated feature of size (B, 1, C) where C denotes the dimension of hidden layers, and an expansion operation (repeat the pooled feature to the size (B,N,C)) to make it suitable for concatenation. The output of the fifth block is passed through a max-pooling layer and a fully connected layer to get the final outputs of dimension 10 for shape parameter and 72 for initial pose parameter.\nOur temporal encoder, for the purpose of learning motion and auxiliary codes, is composed of a point feature extractor (shallow PointNet) and a double layers GRU [15], as shown in Fig. 7b. The shallow PointNet extracts spatial features for each input point cloud, which has 3 hidden layers with hidden sizes equal to 128. We use the same maxpooling and concatenating operations as the spatial encoder. Then the per-frame features are processed sequentially by the GRU layer to provide the latent vector of dimension 90 for motion code and 128 for auxiliary code. Motion-Comp Network We design a conditional GRU for our Motion-Comp network to learn the compensation of the input motion sequence. Specifically, we use the motion code cm and auxiliary code ca as conditions, copy and concatenate them with the pose parameter of each time step estimated by our linear motion model. The detailed architecture is shown in Fig. 7c. The output of the conditional GRU is the motion compensation, and we apply a residual connection to obtain the refined motion sequence, i.e. perframe poses. We can recover body mesh sequences with the predicted shape and per-frame pose codes by using SMPL\ndecoder, here we use the neutral shape model as in previous work [29, 33, 34]. Shape-Comp Network We propose a Shape-Comp network, in which a conditional GRU takes the auxiliary code ca as input and predicts a new latent vector for each temporal frame conditioned on the predicted pose (we follow CAPE to represent each joint with the flattened rotational matrix and filter the joints that are not related to clothing). The latent vector of each frame is then fed into the graph network to predict per-vertex offsets, which is similar to the CAPE decoder. We remove the one-hot vector of clothing type and only use the predicted pose as condition since we do not focus on the generative task. The architecture is shown in Fig. 7d. Implementation of GRUs We use the standard API of GRU provided by PyTorch. All the GRUs in our framework share the same architecture, which has 2 layers with the hidden size of 512, except we apply an additional linear layer for each GRU to transform the output dimensions for different modules."
        },
        {
            "heading": "1.2. Linear Motion Model",
            "text": "For the linear motion model (Section 3.2 in the main paper), we employ the Principal Component Analysis (PCA) to model the per-frame difference of the pose parameter regarding the first frame in a sequence. As stated in Sec. 3.2 of the main paper, we run PCA separately for the global orientation (i.e. pelvis) and the remaining body joint rotations. Inspired by Urtasun et al. [68], we choose the number of PCA components depending on the fraction of the total variance of the training data that is captured by the subspace, denoted by Q(m):\nQ(m) = \u2211m i=1 \u03bbi\u2211M i=1 \u03bbi\n(1)\nwhere m controls the number of principal components, \u03bbi are ordered eigenvalues of the data covariance matrix such that \u03bbi \u2265 \u03bbi+1, and M is the total number of eigenvalues. In our experiments, we choose m = 4 for the global rotation and m = 86 for the remaining body joints rotation, which satisfy Q(m) > 0.9. We visualize some principal components in Fig. 11 and 12 (Sec. 4)."
        },
        {
            "heading": "1.3. Experiment Details",
            "text": "Loss Functions Given an input point cloud sequence, our model generates one shape code cs and three mesh sequences Xlinear, Xmotion and Xshape, which correspond to the outputs of LMM, Motion-Comp network and Shape-Comp network respectively. Each sequence has L = 30 mesh frames and each mesh has K = 6890 vertices. We also have the ground truth shape parameter c\u2217s and posed SMPL body mesh sequence Ybody. Furthermore, we compute ground\ntruth offsets sequence by Yoffset = Mclothed \u2212 MSMPL, where Mclothed and MSMPL stand for the vertices of the clothed human mesh and corresponding SMPL body mesh in the canonical pose respectively.\nThen we define the reconstruction loss as the per-vertex L1 error with the ground truth mesh\nLr (X,Y) = 1\nLK L\u2211 l=1 K\u2211 k=1 \u2225Xl,k \u2212Yl,k\u22251 . (2)\nFurthermore, we apply L2 penalization on the predicted shape code to further alleviate the ambiguity between body shape and clothing, given by\nLs (c, c\u2217) = \u2225c\u2212 c\u2217\u222522 . (3)\nFinally, the total loss for training can be formulated as\nL = \u03bbsLs (cs, c\u2217s) + \u03bbr1Lr (Xlinear,Ybody) + \u03bbr2Lr (Xmotion,Ybody) + \u03bbr3Lr (Xshape,Yoffset) ,\n(4)\nwe set \u03bbs = \u03bbr1 = 1, \u03bbr2 = \u03bbr3 = 0 for the first training stage and \u03bbs = \u03bbr2 = 1, \u03bbr3 = 30 and \u03bbr1 = 0 for the second stage. Backward Experiments For our auto-decoding based experiments, i.e. completion and prediction, we use the trained model to perform a backward fitting algorithm. Specifically, we remove the encoder, freeze the parameters of the remaining modules and optimize the SMPL parameters and latent codes with back-propagation to produce the outputs as similar to the observations as possible.\nWe initialize the SMPL parameters and latent codes with the random vector sampled from a Gaussian distribution N(0, 0.01) and use the Adam optimizer [32] with learning rate 3e\u22122 to perform back-propagation for 500 iterations. In each iteration, we uniformly sample 8192 points on the surface of the predicted meshes, and compute Chamfer loss [48,58] w.r.t the observed points for penalizing. Additionally, we follow IPNet [8] to add pose and shape prior terms, which penalize unnatural output bodies during optimization.\nCompletion We conduct two different types of motion completion experiments, i.e. temporal completion and spatial completion. Given a temporal sequence of L = 30 frames, for temporal completion, we randomly select 15 frames as observation and optimize the SMPL parameters and latent codes to complete the missing frames. We choose HMMR [31] and 4D-CR-SMPL (an extension we implement for 4D-CR [26]) as baselines. To implement 4D-CRSMPL, we replace their implicit decoder with the SMPL decoder, and set the dimensions of their identity code and initial pose code to 10 and 72 respectively. Then we can obtain the pose code for each time step with the Neural ODE conditioned on the motion code, and input it to the SMPL decoder with the identity code to produce the reconstructed mesh frame. For 4D-CR-SMPL, we use the model trained on our dataset, and for HMMR, we use the official pretrained model.\nThe goal of spatial completion is to complete the temporal sequence with partial spatial observation. To this end, we use the raw scanned mesh sequences of CAPE [42], and render the depth images of resolution 512 \u00d7 512 with the\napproach illustrated in the main paper (Sec. 4.2) to simulate the real world scenario. We assume the camera poses are known and back project the depth images to obtain partial point clouds. We initialize our codes with the random vectors sampled from a Gaussian distribution N(0, 0.01) with no requirement for an additional initialization step like NPMs [51], and use the Adam optimizer with learning rate 3e\u22122 to perform back-propagation for 500 iterations. Note that in this experiment, we adopt one-directional point-tosurface loss instead of two-directional Chamfer loss due to the partial geometry. We show some qualitative examples in Fig. 18."
        },
        {
            "heading": "2. Generalization of Our Motion Model",
            "text": "In this section, our goal is to investigate the capacity of our motion model for representing novel motions from another dataset. To this end, we choose some motion sequences from AMASS [43], a large 3D MoCap dataset. And then we use our model trained on the CAPE dataset [42], perform the similar backward algorithm in completion and prediction experiments to fit the whole sequence of L = 30. Instead of dense SMPL, we randomly sample 8192 points from the SMPL mesh of each frame as observations, then use the Chamfer loss to the points sampled from the predicted mesh. Prior terms [8] are also used to penalize the unnatural output. Since AMASS only provides SMPL parameters, we disable the Shape-Comp network and use the results from Motion-Comp network for visualization. Fig. 8 shows that the proposed method successfully reconstructs the full sequence from such sparse input, which demonstrates the generalization capability of our model to represent novel motions from another data source."
        },
        {
            "heading": "3. Extended Comparisons to Related Works",
            "text": "Comparison to HuMoR We compare with a SoTA human body estimation method HuMoR [59] on the task of fitting point cloud sequences. Specifically, we choose 100 mesh sequences of 30 frames from our test set and randomly sample 8192 points from each frame. Then we use the pretrained model of both methods to conduct backward optimization with 2 choices of loss functions, i.e. Chamfer loss (HuMoR, Ours) or 3D keypoint loss (HuMoR\u2217, Ours\u2217). For both losses, we also enabled prior losses to regularize the predicted shape and motion for H4D. As shown in Tab. 2, our method beats HuMoR in both cases. The qualitative comparisons are shown in Fig. 9, HuMoR can only recover the global movement trend without accurate limbs by using Chamfer loss and gains a significant improvement when using 3D keypoints as supervision. In contrast, our method obtains more accurate results in both cases. Comparison to NPMs We provide the comparisons to NPMs [51] on depth completion task. We choose 100 se-\nquences and use the pretrained model of NPMs to perform completion from partial depth. Specifically, given a depth image sequence of 30 frames, we project the depth values into a 2563-SDF grid to generate the inputs for NPMs, and then optimize the latent codes frame-by-frame with the default setup. Note that NPMs runs 10 times slower than H4D and uses twice of the GPU memory. Tab. 3 shows that our model outperforms NPMs, either w/ or w/o (NPMs\u2217) encoders for code initialization, on both metrics. As can be seen from the qualitative results shown in Fig. 10, NPMs produces accurate motion but fails to recover fine-grained geometry, while our results are plausible on both shape and motion."
        },
        {
            "heading": "4. Visualization of Principal Components",
            "text": "The linear motion model in our framework totally has 90 principal components, the first 4 components are for global rotation (pelvis joint) and the rest 86 for other body joints. We start from the same rest pose and visualize some principal components in Fig. 11 and 12. Specifically, for each shown component, we select different scaling factors (before each row) and multiply them with this component to show the motion results. As shown, PC0 roughly controls the global rotation around the vertical axis; PC4 and PC6 affect the opening and closing of the upper arm and forearm, respectively; PC7 is related to the bending of the legs; and PC9 tells the movement of arms and legs at the same time. In general, the positive and negative scaling factors of components correspond to opposite directions of motion, and the absolute value affects the magnitude of the motion."
        },
        {
            "heading": "5. Additional Qualitative Results",
            "text": "We show additional qualitative examples on 4D reconstruction in Fig. 13, shape and motion recovery in Fig. 15,\ntemporal completion in Fig. 16 and 17, spatial completion in Fig. 18, future prediction in Fig. 14, motion retargeting in Fig. 19 and ablation study in Fig. 20."
        },
        {
            "heading": "6. Run-time",
            "text": "In Tab. 4, we show the per sequence run-time of our method and previous 4D representation methods on forward inference for 4D reconstruction and backward optimization for temporal completion. Note that we report the time cost to run a complete backward optimization process for a sequence (500 iterations). The length of the full sequence is L = 30, and all models run on a single NVIDIA 2080Ti GPU. Instead of the Neural ODE [13], we model the human motion using the linear model and GRU-based compensation networks. As can be seen, our model runs faster in both cases, especially in backward optimization."
        },
        {
            "heading": "7. Limitations and Future work",
            "text": "We now discuss a few limitations of our approach that point to future work. First, our motion model can reconstruct the discretized frame w.r.t each input time step, but not the arbitrary time in the continuous whole time span like 4D-CR [26] or OFlow [50], which will be useful in some scenarios requesting higher temporal resolution from inputs. Incorporating a network that takes a time value scalar as input, e.g. Neural ODE [13], temporal MLP, would be a solution. Second, we currently conduct all experiments on the sequences of 3D data, e.g. point clouds or meshes. On the one hand, this is due to the lack of 4D human datasets with color images, e.g. pairs of video and 3D human sequences (with clothing and hair). And on the other hand, the focus of this work is to propose a compositional representation and effectively power various 4D human-related applications based on point cloud. Combining our representation with techniques such as neural rendering [39, 49] or photometric-based optimization [38,40] for image-based full human 4D reconstruction would be a promising future direction. Third, we adopt the same clothing representation used in previous work [6,36,42], i.e. per-vertex offsets upon the body in the canonical pose, and extend it to apply to temporal sequences. However, as discussed in CAPE [42], some loose garments such as skirts and coats are difficult to represent with offsets due to the limited capacity. Modeling clothes and hair as separate layers on the body with meshes or implicit surface is a feasible way, and we leave\nit to future work. Forth, since we have a compact motion representation that uses one single motion code to provide global control upon the whole sequence, future works also include high-level inference applications such as using the motion code learned in an unsupervised fashion to perform action classification with a simple linear classifier."
        },
        {
            "heading": "8. Broader Impact and Social Impact",
            "text": "Learning a compact representation for 3D data is a widely interested problem. However, less attention has focused on the 4D cases, though it is important for various applications to understand time-varying objects, e.g. Robotics, VR/AR. This work focuses on 4D human modeling and proposes H4D, a compact and compositional representation, which uses low-dimensional SMPL parameters and latent codes to encode key factors of dynamic humans. We make some attempts and demonstrate our representation has rich capacity and is amenable to many applications. We hope these explorations could provide insights for future research directions. For instance, using our representation for videobased full human reconstruction; exploiting the compositional property to control the outputs for generative tasks; and improving the 4D human representation and make up for the discussed limitations of our method. Broadly, our approach can serve as an important core tech in achieving the Metaverse. It may enable everyone to produce their own Avatar with their motions, potentially benefiting the Social Welfare.\nOFlow\n4D-CR\n4D-CRSMPL\nVIBE\nOurs\nGT\nTime\nMotion Sequence\nIdentity Sequence\nHMMR\nFigure 19. Motion Retargeting. Our goal is to transfer the human movements of the motion sequence (Row 1) to the people in the identity sequence (Row 2).\n- GRU Enc.\n- Motion \ud835\udc84\ud835\udc82\n- LMM\nFull Model\nTime\n- Shape \ud835\udc84\ud835\udc82\nGT\nFigure 20. Ablation Study."
        }
    ],
    "title": "H4D: Human 4D Modeling by Learning Neural Compositional Representation",
    "year": 2022
}