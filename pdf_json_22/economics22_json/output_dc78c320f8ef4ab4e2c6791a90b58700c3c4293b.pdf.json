{
    "abstractText": "Estimation and inference on causal parameters is typically reduced to a generalized method of moments problem, which involves auxiliary functions that correspond to solutions to a regression or classification problem. Recent line of work on debiased machine learning shows how one can use generic machine learning estimators for these auxiliary problems, while maintaining asymptotic normality and root-n consistency of the target parameter of interest, while only requiring mean-squared-error guarantees from the auxiliary estimation algorithms. The literature typically requires that these auxiliary problems are fitted on a separate sample or in a cross-fitting manner. We show that when these auxiliary estimation algorithms satisfy natural leave-one-out stability properties, then sample splitting is not required. This allows for sample re-use, which can be beneficial in moderately sized sample regimes. For instance, we show that the stability properties that we propose are satisfied for ensemble bagged estimators, built via sub-sampling without replacement, a popular technique in machine learning practice.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qizhao Chen"
        },
        {
            "affiliations": [],
            "name": "Vasilis Syrgkanis"
        },
        {
            "affiliations": [],
            "name": "Morgane Austern"
        }
    ],
    "id": "SP:dba8a001e000ca68ddaf2c4abb3915e91a3bdf58",
    "references": [
        {
            "authors": [
                "Chunrong Ai",
                "Xiaohong Chen"
            ],
            "title": "Efficient estimation of models with conditional moment restrictions containing unknown functions",
            "year": 2003
        },
        {
            "authors": [
                "Chunrong Ai",
                "Xiaohong Chen"
            ],
            "title": "Estimation of possibly misspecified semiparametric conditional moment restriction models with different conditioning variables",
            "venue": "Journal of Econometrics,",
            "year": 2007
        },
        {
            "authors": [
                "Chunrong Ai",
                "Xiaohong Chen"
            ],
            "title": "The semiparametric efficiency bound for models of sequential moment restrictions containing unknown functions",
            "venue": "Journal of Econometrics,",
            "year": 2012
        },
        {
            "authors": [
                "Morgane Austern",
                "Wenda Zhou"
            ],
            "title": "Asymptotics of cross-validation",
            "venue": "arXiv preprint arXiv:2001.11111,",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Bayle",
                "Alexandre Bayle",
                "Lucas Janson",
                "Lester Mackey"
            ],
            "title": "Cross-validation confidence intervals for test error",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Ivan Fern\u00e1ndez-Val",
                "Christian Hansen"
            ],
            "title": "Program evaluation and causal inference with high-dimensional data",
            "year": 2017
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Christian Hansen"
            ],
            "title": "Inference for high-dimensional sparse econometric models",
            "year": 2011
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Christian Hansen"
            ],
            "title": "Inference on treatment effects after selection among high-dimensional controls",
            "venue": "The Review of Economic Studies,",
            "year": 2014
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Kengo Kato"
            ],
            "title": "Uniform post-selection inference for least absolute deviation regression and other Z-estimation problems",
            "year": 2014
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Lie Wang"
            ],
            "title": "Pivotal estimation via square-root lasso in nonparametric regression",
            "venue": "The Annals of Statistics,",
            "year": 2014
        },
        {
            "authors": [
                "Peter J Bickel"
            ],
            "title": "On adaptive estimation",
            "venue": "The Annals of Statistics,",
            "year": 1982
        },
        {
            "authors": [
                "Peter J Bickel",
                "Chris AJ Klaassen",
                "Ya\u2019acov Ritov",
                "Jon A Wellner"
            ],
            "title": "Efficient and adaptive estimation for semiparametric models, volume 4",
            "year": 1993
        },
        {
            "authors": [
                "Peter J Bickel",
                "Yaacov Ritov"
            ],
            "title": "Estimating integrated squared density derivatives: Sharp best order of convergence estimates",
            "venue": "Sankhya\u0304: The Indian Journal of Statistics,",
            "year": 1988
        },
        {
            "authors": [
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi",
                "Olivier Bousquet"
            ],
            "title": "Concentration inequalities",
            "venue": "In Summer school on machine learning,",
            "year": 2003
        },
        {
            "authors": [
                "Olivier Bousquet",
                "Andr\u00e9 Elisseeff"
            ],
            "title": "Stability and generalization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Jelena Bradic",
                "Mladen Kolar"
            ],
            "title": "Uniform inference for high-dimensional quantile regression: Linear functionals and regression rank scores",
            "year": 2017
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Denis Chetverikov",
                "Mert Demirer",
                "Esther Duflo",
                "Christian Hansen",
                "Whitney Newey",
                "James Robins"
            ],
            "title": "Double/debiased machine learning for treatment and structural parameters: Double/debiased machine learning",
            "venue": "The Econometrics Journal,",
            "year": 2018
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Juan Carlos Escanciano",
                "Hidehiko Ichimura",
                "Whitney K Newey",
                "James M Robins"
            ],
            "title": "Locally robust semiparametric estimation",
            "venue": "arXiv preprint arXiv:1608.00033,",
            "year": 2016
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Christian Hansen",
                "Martin Spindler"
            ],
            "title": "Valid post-selection and postregularization inference: An elementary, general approach",
            "venue": "Annual Review of Economics,",
            "year": 2015
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Whitney Newey",
                "Rahul Singh"
            ],
            "title": "Double/de-biased machine learning of global and local parameters using regularized Riesz representers",
            "venue": "arXiv preprint arXiv:1802.08667,",
            "year": 2018
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Whitney Newey",
                "Rahul Singh",
                "Vasilis Syrgkanis"
            ],
            "title": "Adversarial estimation of riesz representers",
            "venue": "arXiv preprint arXiv:2101.00009,",
            "year": 2020
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Whitney K Newey",
                "Rahul Singh"
            ],
            "title": "A simple and general debiased machine learning theorem with finite sample guarantees",
            "venue": "arXiv preprint arXiv:2105.15197,",
            "year": 2021
        },
        {
            "authors": [
                "Andr\u00e9 Elisseeff",
                "Massimiliano Pontil"
            ],
            "title": "Leave-one-out error and stability of learning algorithms with applications. NATO science series sub series iii computer and systems",
            "year": 2003
        },
        {
            "authors": [
                "Moritz Hardt",
                "Ben Recht",
                "Yoram Singer"
            ],
            "title": "Train faster, generalize better: Stability of stochastic gradient descent",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Moritz Hardt",
                "Benjamin Recht",
                "Yoram Singer"
            ],
            "title": "Train faster, generalize better: Stability of stochastic gradient descent",
            "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,",
            "year": 2016
        },
        {
            "authors": [
                "Rafail Z Hasminskii",
                "Ildar A Ibragimov"
            ],
            "title": "On the nonparametric estimation of functionals",
            "venue": "In Proceedings of the Second Prague Symposium on Asymptotic Statistics,",
            "year": 1979
        },
        {
            "authors": [
                "Rustam Ibragimov",
                "Sh Sharakhmetov"
            ],
            "title": "On an exact constant for the rosenthal inequality",
            "venue": "Theory of Probability & Its Applications,",
            "year": 1998
        },
        {
            "authors": [
                "Jana Jankova",
                "Sara Van De Geer"
            ],
            "title": "Confidence intervals for high-dimensional inverse covariance estimation",
            "venue": "Electronic Journal of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Jana Jankova",
                "Sara Van De Geer"
            ],
            "title": "Confidence regions for high-dimensional generalized linear models under sparsity",
            "year": 2016
        },
        {
            "authors": [
                "Jana Jankova",
                "Sara Van De Geer"
            ],
            "title": "Semiparametric efficiency bounds for high-dimensional models",
            "venue": "The Annals of Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "title": "Confidence intervals and hypothesis testing for highdimensional regression",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "title": "Hypothesis testing in high-dimensional regression under the Gaussian random design model: Asymptotic theory",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2014
        },
        {
            "authors": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "title": "Debiasing the lasso: Optimal sample size for Gaussian designs",
            "venue": "The Annals of Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Satyen Kale",
                "Ravi Kumar",
                "Sergei Vassilvitskii"
            ],
            "title": "Cross-validation and mean-square stability",
            "venue": "Proceedings of the Second Symposium on Innovations in Computer Science (ICS2011,",
            "year": 2011
        },
        {
            "authors": [
                "Khashayar Khosravi",
                "Greg Lewis",
                "Vasilis Syrgkanis"
            ],
            "title": "Non-parametric inference adaptive to intrinsic dimension",
            "venue": "arXiv preprint arXiv:1901.03719,",
            "year": 2019
        },
        {
            "authors": [
                "Chris AJ Klaassen"
            ],
            "title": "Consistent estimation of the influence function of locally asymptotically linear estimators",
            "venue": "The Annals of Statistics,",
            "year": 1987
        },
        {
            "authors": [
                "Michael R Kosorok"
            ],
            "title": "Introduction to empirical processes and semiparametric inference",
            "venue": "Springer Science & Business Media,",
            "year": 2007
        },
        {
            "authors": [
                "B Ya Levit"
            ],
            "title": "On the efficiency of a class of non-parametric estimates",
            "venue": "Theory of Probability & Its Applications,",
            "year": 1976
        },
        {
            "authors": [
                "Alexander R Luedtke",
                "Mark J Van Der Laan"
            ],
            "title": "Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy",
            "venue": "Annals of Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "Whitney K Newey"
            ],
            "title": "The asymptotic variance of semiparametric estimators",
            "venue": "Econometrica, pages 1349\u20131382,",
            "year": 1994
        },
        {
            "authors": [
                "Whitney K Newey",
                "Fushing Hsieh",
                "James M Robins"
            ],
            "title": "Undersmoothing and bias corrected functional estimation",
            "venue": "Technical report, MIT Department of Economics,",
            "year": 1998
        },
        {
            "authors": [
                "Whitney K Newey",
                "Fushing Hsieh",
                "James M Robins"
            ],
            "title": "Twicing kernels and a small bias property of semiparametric estimators",
            "year": 2004
        },
        {
            "authors": [
                "Matey Neykov",
                "Yang Ning",
                "Jun S Liu",
                "Han Liu"
            ],
            "title": "A unified theory of confidence regions and testing for high-dimensional estimating equations",
            "venue": "Statistical Science,",
            "year": 2018
        },
        {
            "authors": [
                "Jerzy Neyman"
            ],
            "title": "Optimal asymptotic tests of composite statistical hypotheses",
            "venue": "In Probability and Statistics,",
            "year": 1959
        },
        {
            "authors": [
                "Yang Ning",
                "Han Liu"
            ],
            "title": "A general theory of hypothesis tests and confidence regions for sparse high dimensional models",
            "venue": "The Annals of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Johann Pfanzagl"
            ],
            "title": "Lecture notes in statistics. Contributions to a general asymptotic statistical theory",
            "year": 1982
        },
        {
            "authors": [
                "Zhao Ren",
                "Tingni Sun",
                "Cun-Hui Zhang",
                "Harrison H Zhou"
            ],
            "title": "Asymptotic normality and optimalities in estimation of large Gaussian graphical models",
            "venue": "The Annals of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Emmanuel Rio"
            ],
            "title": "Moment inequalities for sums of dependent random variables under projective conditions",
            "venue": "Journal of Theoretical Probability,",
            "year": 2009
        },
        {
            "authors": [
                "James M Robins",
                "Andrea Rotnitzky"
            ],
            "title": "Semiparametric efficiency in multivariate regression models with missing data",
            "venue": "Journal of the American Statistical Association,",
            "year": 1995
        },
        {
            "authors": [
                "James M Robins",
                "Andrea Rotnitzky",
                "Lue Ping Zhao"
            ],
            "title": "Analysis of semiparametric regression models for repeated outcomes in the presence of missing data",
            "venue": "Journal of the American Statistical Association,",
            "year": 1995
        },
        {
            "authors": [
                "Peter M Robinson"
            ],
            "title": "Root-n-consistent semiparametric regression",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1988
        },
        {
            "authors": [
                "B Toth",
                "MJ van der Laan"
            ],
            "title": "TMLE for marginal structural models based on an instrument",
            "venue": "Technical report, UC Berkeley Division of Biostatistics,",
            "year": 2016
        },
        {
            "authors": [
                "Anastasios Tsiatis"
            ],
            "title": "Semiparametric theory and missing data",
            "venue": "Springer Science & Business Media,",
            "year": 2007
        },
        {
            "authors": [
                "Sara Van de Geer",
                "Peter B\u00fchlmann",
                "Ya\u2019acov Ritov",
                "Ruben Dezeure"
            ],
            "title": "On asymptotically optimal confidence regions and tests for high-dimensional models",
            "venue": "The Annals of Statistics,",
            "year": 2014
        },
        {
            "authors": [
                "Mark J Van der Laan",
                "Sherri Rose"
            ],
            "title": "Targeted Learning: Causal Inference for Observational and Experimental Data",
            "venue": "Springer Science & Business Media,",
            "year": 2011
        },
        {
            "authors": [
                "Mark J Van Der Laan",
                "Daniel Rubin"
            ],
            "title": "Targeted maximum likelihood learning",
            "venue": "The International Journal of Biostatistics,",
            "year": 2006
        },
        {
            "authors": [
                "Aad Van Der Vaart"
            ],
            "title": "On differentiable functionals",
            "venue": "The Annals of Statistics,",
            "year": 1991
        },
        {
            "authors": [
                "Aad W Van der Vaart"
            ],
            "title": "Asymptotic Statistics, volume 3",
            "year": 2000
        },
        {
            "authors": [
                "Cun-Hui Zhang",
                "Stephanie S Zhang"
            ],
            "title": "Confidence intervals for low dimensional parameters in high dimensional linear models",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2014
        },
        {
            "authors": [
                "Wenjing Zheng",
                "Mark J Van Der Laan"
            ],
            "title": "Asymptotic theory for cross-validated targeted maximum likelihood estimation",
            "venue": "U.C. Berkeley Division of Biostatistics Working Paper Series. Working Paper 273,",
            "year": 2010
        },
        {
            "authors": [
                "Yinchu Zhu",
                "Jelena Bradic"
            ],
            "title": "Breaking the curse of dimensionality in regression",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A large variety of problems in causal inference and more generally semi-parametric inference can be framed as finding a solution to a moment condition:\nM(\u03b8, g) , EZ [m(Z; \u03b8, g)] M(\u03b80, g0) = 0\nwhere Z \u2208 Z is a vector of random variables that, apart from the target parameter \u03b80 \u2208 \u0398 \u2282 Rp of interest, also depends on unknown nuisance functions g0 \u2208 G, which need to be estimated in a flexible manner from the data. This framework has a long history in the literature on semiparametric inference [39, 26, 27, 48, 37, 53, 59, 12, 41, 51, 60, 13, 42, 1, 43, 2, 55, 38, 3], which analyzes the following two-stage estimation process with sample re-use, when having access to n iid samples {Z1, . . . , Zn} and as n grows, treating the target parameter dimension p as a constant:\n1. Obtain an estimate g\u0302 \u2208 G of the nuisance function g0 based on all the samples.\nar X\niv :2\n20 6.\n01 82\n5v 2\n[ ec\non .E\nM ]\n1 4\n2. Return any estimate \u03b8\u0302 \u2208 \u0398 that satisfies:\nMn(\u03b8\u0302, g\u0302) = op\n( n\u22121/2 ) with Mn(\u03b8, g) , 1\nn n\u2211 i=1 m(Zi; \u03b8, g).\nGiven that the estimation of function g\u0302 is a complex non-parametric problem, it will typically only satisfy slower than parametric error rates. Moreover, in high dimensional settings, when machine learning techniques are used to estimate g\u0302, then the regularization bias of g\u0302 will propagate to the final estimate \u03b8\u0302, leading to non-regular estimates and the inability to construct confidence intervals.\nThe literature on efficient semi-parametric estimation provides conditions on the moment function so that the influence of the estimation error of the nuisance function g\u0302 is of second order importance and does not alter the distributional properties of the second stage [26, 13, 61, 7, 8, 9, 10, 32, 33, 34, 56, 47, 19, 44, 49, 29, 30, 31, 16, 63, 64]. This approach dates back to the classical work on doubly robust estimation and targeted maximum likelihood [52, 51, 58, 57, 40, 54] as well as the more recent work on locally robust or Neyman orthogonal conditions on the moment function [45, 46, 18, 6, 17], typically referred to as double or debiased machine learning. At a high-level, the earlier literature on semi-parametric inference shows that if the moment function satisfies some form of robustness to nuisance perturbations and, importantly, as long as the space G used in the first stage estimation is a relatively simple function class, in terms of statistical complexity, typically referred to as a Donsker function class, then the second stage estimate is root-n consistent and asymptotically normal. Hence, one can easily construct confidence intervals for the target parameter of interest.\nCrucially the recent literature on debiased machine learning alters the standard two-stage estimation algorithm to introduce the idea of sample-splitting, [11, 62]. In particular, instead of estimating g\u0302 on all the samples, the recent work on debiased machine learning [17, 18, 22] estimates g\u0302 on a separate sample, or for better sample efficiency invokes \u201ccross-fitting,\u201d where we train a nuisance model on half the data and evaluate it in the second stage on the other half and vice versa. Sample splitting avoids the Donsker conditions that were prevalent in the classic semi-parametric inference literature and only requires a root-mean-squared-error (RMSE) guarantee of the estimate g\u0302 of op(n\u22121/4).\nHowever, sample splitting or cross-fitting still leads to poorer sample usage, as we can lose half of our data when training complex non-parametric or machine learning models, which can be problematic in small and moderate sample regimes. Our main result is to show that root-n consistency and asymptotic normality of the standard algorithm, without sample splitting, can be achieved without the Donsker property, but solely if one assumes that the first stage estimation algorithm is o(n\u22121/2) leave-one-out stable, a relatively widely studied property in the statistical machine learning and generalization theory literature [15, 35, 23, 25]. As a leading example we show that our stability conditions are satisfied by bagging estimators formed with sub-sampling without replacement.\nRecent prior work of [21] also considered asymptotic normality based on stability conditions, but as we expand in the main text, their requirement on the stability property is much harsher than the one we derive here. Moreover, [21] analyzes only a special case of the class of moment problems that we consider here. For instance, in our leading example of bagging estimators, the prior result of [21] would require that the bias of the base estimator decays faster than 1/n, where n is the sample size, which is typically not the case. In contrast, our stability condition does not impose any explicit assumption on the bias and solely requires that the sub-sample size m is o( \u221a n).\nTo simplify the regularity assumptions required for asymptotic normality, we focus on the case where m(Z; \u03b8, g) is linear in \u03b8, i.e.\nm(Z; \u03b8, g) = a(Z; g) \u03b8 + \u03bd(Z; g)\nwhere a(Z; g) \u2208 Rp\u00d7p is a p\u00d7 p matrix and \u03bd(Z; g) \u2208 Rp is a p-vector, and we denote with:\nA(g) := EZ [a(Z; g)] An(g) := En[a(Z; g)] V (g) := EZ [\u03bd(Z; g)] Vn(g) := En[\u03bd(Z; g)].\nMany of the leading examples in semi-parametric problems that arise in causal inference correspond to linear moment problems. We present below a representative set of problems that are widely used in the practice of causal inference.\nExample 1 (Partially Linear Treatment Effect [53]). If one assumes that the outcome of interest Y is linear in the treatment, i.e. Y = \u03b8\u20320T + f0(X) + , with E[ | T,X] = 0, then estimating the treatment effect \u03b80 boils down to solving the following linear moment:\nm(Z; \u03b8, g) = (Y \u2212 q(X)\u2212 \u03b8\u2032(T \u2212 p(X))) (T \u2212 p(X))\nwhere the corresponding true values of (q, p) are q0(X) = E[Y |X], p0(X) = E[T |X].\nExample 2 (Partially Linear IV [17]). If one assumes that the outcome of interest Y is linear in the treatment, i.e. Y = \u03b8\u20320T + f0(X) + , but the treatment is endogenous (i.e. there are unobserved confounders) and one has access to a random variable Z, that is referred to as an instrument, which correlates with the treatment but is un-correlated with the residual in the outcome equation, i.e. satisfies that E[ | Z,X] = 0, then estimating \u03b80 boils down to solving the following linear moment:\nm(Z; \u03b8, g) = (Y \u2212 q(X)\u2212 \u03b8\u2032(T \u2212 p(X))) (Z \u2212 r(X))\nwhere the true values of (q, p, r) are q0(X) = E[Y |X], p0(X) = E[T |X], r0(X) = E[Z|X].\nExample 3 (Average linear functionals of regression functions). Consider a class of moment functions of the form:\nm(Z; \u03b8, g) = \u03b8 \u2212mb(Z; q)\u2212 \u00b5(T,X) (Y \u2212 q(T,X))\nwhere g = (q, \u00b5), mb is a linear functional of q and the corresponding true value q is a regression function q0(T,X) = E[Y | T,X] and \u00b50 is Riesz representer of the functional E[mb(Z; q)] (see [20] for examples). For instance, in the case of a binary treatment T , where we have that Y = q(T,X)+ and E[ | T,X] = 0, then the average treatment effect \u03b80 = E[q(1, X) \u2212 q(0, X)] is identified by a moment of the latter type, with:\nmb(Z; q) = q(1, X)\u2212 q(0, X) (Average Treatment Effect (ATE))\nWhile if we have a target treatment policy \u03c0 : X \u2192 {0, 1}, and we want to estimate its average value \u03b80, we can identify do so with a moment of the aforementioned type, with:\nmb(Z; q) = \u03c0(X) (q(1, X)\u2212 q(0, X)) (Average Policy Effect)\nFor completeness, we also include in Appendix G an extension of our results to nonlinear moment problems."
        },
        {
            "heading": "2 Asymptotic Normality without Sample Splitting",
            "text": "We start by providing an asymptotic normality theorem for semi-parametric moment estimators without sample splitting and where the moment satisfies the well-studied property of Neyman orthogonality. Our theorem requires four main conditions: i) root-mean-squared-error (RMSE) rates for the nuisance function estimates of op(n\u22121/4), ii) Neyman orthogonality of the moment with respect to the nuisances, iii) second-order smoothness of the moment with respect to the nuisance functions and, iv) stochastic equicontinuity of the Jacobian and the offset part of the linear moment function as the nuisance estimate g\u0302 converges to g0. For a vector x \u2208 Rp we denote with \u2016x\u20162 the `2 norm and for a matrix X \u2208 Rp\u00d7p we denote with \u2016X\u2016op the operator norm with respect to the `2 norm.\nTheorem 1. Suppose that the nuisance estimate g\u0302 \u2208 G satisfies:\n\u2016g\u0302 \u2212 g0\u201622 , EX [ \u2016g\u0302(X)\u2212 g0(X)\u201622 ] = op ( n\u22121/2 ) . (Consistency Rate)\nSuppose that the moment satisfies the Neyman orthogonality condition: for all g \u2208 G\nDgM(\u03b80, g0)[g \u2212 g0] , \u2202\n\u2202t M(\u03b80, g0 + t (g \u2212 g0))\n\u2223\u2223 t=0 = 0 (Neyman Orthogonality)\nand a second-order smoothness condition: for all g \u2208 G\nDggM(\u03b80, g0)[g \u2212 g0] , \u22022\n\u2202t2 M(\u03b80, g0 + t (g \u2212 g0))\n\u2223\u2223 t=0 = O ( \u2016g \u2212 g0\u201622 ) (Smoothness)\nand that the moment m satisfy the stochastic equicontinuity conditions: \u221a n \u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op = op(1)\u221a n \u2016V (g\u0302)\u2212 V (g0)\u2212 (Vn(g\u0302)\u2212 Vn(g0))\u20162 = op(1). (Stochastic Equicontinuity)\nAssume that A(g0)\u22121 exists and that for any g, g\u2032 \u2208 G:\n\u2016A(g)\u2212A(g\u2032)\u2016op = O (\u2016g \u2212 g\u2032\u20162) .\nMoreover, assume that for any i, j \u2208 [p]\u00d7 [p], the random variable ai,j(Z; g0) has bounded variance. Then \u03b8\u0302 is asymptotically normal:\n\u221a n ( \u03b8\u0302 \u2212 \u03b80 ) n\u2192\u221e,d\u2212\u2212\u2212\u2212\u2212\u2192 N ( 0, A(g0) \u22121E [ m(Z; \u03b80, g0)m(Z; \u03b80, g0) >]A(g0)\u22121) . The first three conditions are standard assumptions in the literature on debiased machine learning. The final condition (stochastic equicontinuity) is exactly where sample splitting comes very handy in the literature. To illustrate the reason why sample splitting helps with the stochastic equicontinuity condition, let us consider the first part of the condition (the reasoning is analogous for the second part). It asks that the difference of two centered empirical processes, namely An(g\u0302) \u2212 A(g\u0302) and An(g0) \u2212 A(g0), goes to zero faster than n\u22121/2. We expect each empirical process to go down to zero at exactly n\u22121/2 and so this condition asks, since g\u0302 converges to g0 is the empirical process continuous in its argument and for that reason does the difference converge to zero faster than each\nindividual component. If the estimate g\u0302 was fitted on a separate sample, then conditional on g\u0302, we have that each element t = (i, j) \u2208 [p] \u00d7 [p] of An(g\u0302) \u2212 An(g0) is an empirical average of iid random variables with mean A(g\u0302) \u2212 A(g0). Thus a simple Bernstein inequality would show that the difference of the two empirical processes would converge to zero at the order of:\nOp\n(\u221a E[(at(Z; g\u0302)\u2212 at(Z; g0))2]\nn +\n1\nn\n) = Op (\u221a \u2016g\u0302 \u2212 g0\u201622\nn +\n1\nn\n) = op(n \u22121/2)\nwhere we also invoked a mean-squared-continuity property of at(Z; g) and the fact that \u2016g\u0302\u2212g0\u20162 = op(1). Thus, with sample splitting, no further constraint is required from g\u0302, other than a convergence rate on \u2016g\u0302\u2212g0\u20162. In fact, as was noted in recent work of [22], in the above step it suffices to assume that E[(at(Z; g\u0302) \u2212 at(Z; g0))2] = O (\u2016g\u0302 \u2212 g0\u2016q2) for any q < \u221e, which is a much weaker meansquared-continuity assumption, and the property would still hold, since \u2016g\u0302\u2212g0\u2016q/22 n\u22121/2 = o(n\u22121/2), whenever \u2016g\u0302 \u2212 g0\u20162 = op(1).\nWithout sample splitting, note that g\u0302 is now correlated with the samples in the empirical averages and hence An(g\u0302) \u2212 An(g0) is no longer an average of i.i.d. random variables. Typical approaches would try to prove a uniform stochastic equicontinuity property over the function space G, typically referred to as a Donsker property of the function space G. In particular, if we could show that with high probability:\n\u2200g \u2208 G : \u2016A(g)\u2212A(g0)\u2212 (An(g)\u2212An(g0))\u2016op = O ( \u03b4n\u2016g \u2212 g0\u20162 + \u03b42n ) = O ( \u03b42n + \u2016g \u2212 g0\u201622 ) then the above property would also hold for g\u0302. Subsequently, since we know that \u2016g\u0302 \u2212 g0\u201622 = op(n\n\u22121/2), by our convergence rate assumptions on g\u0302, then it would suffice that \u03b42n = o(n\u22121/2). Such localized concentration inequalities have been known to hold for Donsker classes, which are typically defined via entropy integrals, and more recently it was also noted that such inequalities are satisfied with \u03b4n being the critical radius of the space G, defined via localized Rademacher complexities.\nHowever, the latter approach is conservative as it requires a uniform control over the function space G and does not utilize at all the properties of the estimation algorithm itself. In particular, as we will show in the next section, the main result of our work is that this stochastic equicontinuity condition follows from o(n\u22121/2) leave-one-out stability conditions on our estimation algorithm, which are typical in the machine learning literature and in the excess risk and generalization bounds literature."
        },
        {
            "heading": "3 Stochastic Equicontinuity via Stability",
            "text": "We will show that the Condition (Stochastic Equicontinuity) is satisfied, whenever the estimate g\u0302 satisfies leave-one-out stability properties and the moment satisfies the weak mean-squaredcontinuity property of [22]. We start by some preliminary definitions required to state our stability conditions. Define Z(\u2212l) as the data Z1, . . . , Zn with the l-th data point Zl replaced with an independent copy Z\u0303l. Define Z(\u2212l1,\u2212l2) as the data Z1, . . . , Zn with both the l1-th and the l2-th data points Zl1 , Zl2 replaced with independent copies Z\u0303l1 , Z\u0303l2 . Define similarly for Z(\u2212l1,\u2212l2,\u2212l3) and so on. Let g\u0302(\u2212l) be the estimator trained on Z(\u2212l) instead of Z1, . . . , Zn. Similarly, let g\u0302(\u2212l1,\u2212l2)\nbe trained on Z(\u2212l1,\u2212l2) and so on. Moreover, we will always denote with Z a fresh random variable drawn from the distribution of the samples, but which is not part of any training sample. For any random variable X, we denote with \u2016X\u20161 := E[|X|], with \u2016X\u20162 := \u221a E[X2], and with \u2016X\u2016p := (E[|X|p])1/p for any p \u2265 1 in general.\nLemma 2 (Main Lemma). If the estimation algorithm satisfies the stability conditions: for all i, j \u2208 [p]\nmax l\u2208[n] \u2225\u2225\u2225ai,j(Zl, g\u0302)\u2212 ai,j(Zl, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(n\u22121/2) max l\u2208[n] \u2225\u2225\u2225ai,j(Z, g\u0302)\u2212 ai,j(Z, g\u0302(\u2212l))\u2225\u2225\u2225 2 = o(n\u22121/2)\nmax l\u2208[n] \u2225\u2225\u2225\u03bdi(Zl, g\u0302)\u2212 \u03bdi(Zl, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(n\u22121/2) max l\u2208[n] \u2225\u2225\u2225\u03bdi(Z, g\u0302)\u2212 \u03bdi(Z, g\u0302(\u2212l))\u2225\u2225\u2225 2 = o(n\u22121/2)\nand the moment satisfies the mean-squared-continuity condition: for all i, j \u2208 [p]\n\u2200g, g\u2032 : E[(ai,j(Z; g)\u2212 ai,j(Z; g\u2032))2] \u2264 L\u2016g \u2212 g\u2032\u2016q2 E[(\u03bdi(Z; g)\u2212 \u03bdi(Z; g\u2032))2] \u2264 L\u2016g \u2212 g\u2032\u2016 q 2\nfor some q <\u221e and some L > 0, then the Condition (Stochastic Equicontinuity) is satisfied.\nRemark 1. We show in section 3.1 that the stability conditions are tight. We present a counter example for which \u2225\u2225\u2225\u03bdi(Z, g\u0302)\u2212 \u03bdi(Z, g\u0302(\u2212l))\u2225\u2225\u2225 2 is exactly of order n\u22121/2 and for which the Stochastic Equicontinuity condition is not satisfied.\nRemark 2. We note that prior work of [21] that established asymptotic normality without sample splitting via stability, required significantly stronger conditions than what we invoke here. In particular, if we let \u03b2n be the stability of the estimator g\u0302 as measured by the quantities in Lemma 2, then the prior work of [21], would require that n\u03b2n\u2016g\u0302\u2212g0\u20162 \u2192 0. If we only know that \u2016g\u0302\u2212g0\u20162 = o(n\u22121/4), then the above would require \u03b2n = o(n\u22123/4), which is much slower than o(n\u22121/2). Moreover, for bagged kernel estimators that we analyze in section 4, the prior work would require that if we use bags of size m, then the bias of the base estimator with m samples, denoted as bias(m) satisfies that mbias(m) \u2192 0. This would rarely be satisfied and in prior work, the only concrete case that was given was forest estimators with binary variables under strong sparsity conditions, in which case the bias decays exponentially with the sample size. For more general estimators, we expect bias(m) = 1/m\u03b1, for some \u03b1. For such settings, our work still applies and, as we show in section 4, gives results for bagged 1-nearest neighbor estimation algorithms, which do not satisfy any entropy or critical radius bound, but are stable. The key innovation that enables our improved results is a \u201cdouble centering\u201d approach that derives intuition from techniques invoked in the analysis of crossvalidation via stability and the proof of the Efron-Stein inequality[14]. This idea has already been used in the study of the cross validated risk [4, 5].\nProof of Main Lemma. We will show the first part of the lemma, i.e. that if for all i, j \u2208 [p]\nmax l\u2208[n] \u2225\u2225\u2225ai,j(Zl, g\u0302)\u2212 ai,j(Zl, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(n\u22121/2) max l\u2208[n] \u2225\u2225\u2225ai,j(Z, g\u0302)\u2212 ai,j(Z, g\u0302(\u2212l))\u2225\u2225\u2225 2 = o(n\u22121/2)\nthen \u221a n \u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op = op(1)\nThe analogous statement for \u03bd and V , follows in an identical manner.\nSince A(g) and An(g) are p \u00d7 p matrices and p = O(1), it suffices to show the above property for every element (i, j) \u2208 [p]\u00d7 [p], i.e. that\n\u221a n |Ai,j(g\u0302)\u2212Ai,j(g0)\u2212 (An,i,j(g\u0302)\u2212An,i,j(g0))| = op(1).\nFor this it suffices to show that:\nJn := \u221a n \u2016Ai,j(g\u0302)\u2212Ai,j(g0)\u2212 (An,i,j(g\u0302)\u2212An,i,j(g0))\u20161 = o(1).\nIn the remainder of the proof we look at a particular (i, j) and hence for simplicity we overload notation and we let a := ai,j and A := Ai,j.\nBy triangle inequality and monotonicity of Lp norms we have\nJn = \u2225\u2225\u2225\u2225\u2225 1\u221an n\u2211 l=1 { a(Zl, g\u0302)\u2212A(g\u0302)\u2212 [ a(Zl, g0)\u2212A(g0) ]}\u2225\u2225\u2225\u2225\u2225 1\n\u2264 \u2225\u2225\u2225\u2225\u2225 1\u221an n\u2211 l=1 { a(Zl, g\u0302)\u2212 a(Zl, g\u0302(\u2212l)) }\u2225\u2225\u2225\u2225\u2225 1 + \u221a nmax l\u2208[n] \u2225\u2225\u2225A(g\u0302)\u2212A(g\u0302(\u2212l))\u2225\u2225\u2225 1\n+ \u2225\u2225\u2225\u2225\u2225 1\u221an n\u2211 l=1 { a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0)\u2212 ( A(g\u0302(\u2212l))\u2212A(g0) )}\u2225\u2225\u2225\u2225\u2225 2 .\nTo ease notations, we denote\nJ1,n := \u2225\u2225\u2225\u2225\u2225 1\u221an n\u2211 l=1 { a(Zl, g\u0302)\u2212 a(Zl, g\u0302(\u2212l)) }\u2225\u2225\u2225\u2225\u2225 1 , J2,n := \u221a nmax l\u2208[n] \u2225\u2225\u2225A(g\u0302)\u2212A(g\u0302(\u2212l))\u2225\u2225\u2225 1 ,\nJ3,n := \u2225\u2225\u2225\u2225\u2225 1\u221an\u2211 l { a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0)\u2212 ( A(g\u0302(\u2212l))\u2212A(g0) )}\u2225\u2225\u2225\u2225\u2225 2 .\nNow we have that by triangle inequality\nJ1,n \u2264 1\u221a n \u2211 l \u2225\u2225\u2225a(Zl, g\u0302)\u2212 a(Zl, g\u0302(\u2212l))\u2225\u2225\u2225 1 \u2264 \u221a nmax `\u2208[n] \u2225\u2225\u2225a(Zl, g\u0302)\u2212 a(Zl, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(1).\nSimilarly we can handle J2,n:\nJ2,n \u2264 \u221a nmax l\u2208[n] \u2225\u2225\u2225a(Z, g\u0302)\u2212 a(Z, g\u0302(\u2212l))\u2225\u2225\u2225 1\n\u2264 \u221a nmax l\u2208[n] \u2225\u2225\u2225a(Z, g\u0302)\u2212 a(Z, g\u0302(\u2212l))\u2225\u2225\u2225 2 = o(1).\nWe now aim to show that J3,n = o(1). Write for simplicity\nKl := a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0)\u2212 ( A(g\u0302(\u2212l))\u2212A(g0) ) Now by expanding the square we obtain\nJ23,n = E ( 1\u221a n n\u2211 l=1 Kl )2 \u2264 max l\u2208[n] E [ K2l ] + (n\u2212 1) max l1 6=l2\u2208[n] E[Kl1Kl2 ]\nTo bound the first term, we have by mean-squared continuity: E[K2l ] = E [( a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0)\u2212 ( A(g\u0302(\u2212l))\u2212A(g0) ))2] \u2264 2E [( a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0) )2] + 2E [( A(g\u0302(\u2212l))\u2212A(g0)\n)2] = 2E [ E [( a(Zl, g\u0302 (\u2212l))\u2212 a(Zl, g0) )2 \u2223\u2223\u2223Z(\u2212l)]]+ 2E [E [(A(g\u0302(\u2212l))\u2212A(g0))2 \u2223\u2223\u2223Z(\u2212l)]]\n\u2264 2L \u00b7 E [\u2225\u2225\u2225g\u0302(\u2212l) \u2212 g0\u2225\u2225\u2225q\n2\n] + 2L \u00b7 E [\u2225\u2225\u2225g\u0302(\u2212l) \u2212 g0\u2225\u2225\u2225q 2 ] = 4L \u00b7 E [\u2016g\u0302 \u2212 g0\u2016q2] = o(1).\nwhere we invoked the property that \u2016g\u0302 \u2212 g0\u20162 = op(1), and the second to last equality exploited the tower law. Thus maxl\u2208[n] E [ K2l ] = o(1).\nDouble centering. We now bound the term, (n\u22121) maxl1 6=l2\u2208[n] E[Kl1Kl2 ]. Define, for simplicity, for l1 6= l2\nK (l2) l1 := a(Zl1 , g\u0302 (\u2212l1,\u2212l2))\u2212 a(Zl1 , g0)\u2212 ( A(g\u0302(\u2212l1,\u2212l2))\u2212A(g0) ) .\nNote that K(l2)l1 does not depend on the l2-th data point and is only a function of Z (\u2212l2), Z\u0303l1 . Moreover, by the definition of A, noting that Zl is independent of Z(\u2212l) and Zl1 is independent of Z(\u2212l1,\u2212l2):\nE [ Kl \u2223\u2223\u2223Z(\u2212l)] = E [a(Zl, g\u0302(\u2212l))\u2212A(g\u0302(\u2212l))\u2223\u2223\u2223Z(\u2212l)]\u2212 E [a(Zl, g0)\u2212A(g0)\u2223\u2223\u2223Z(\u2212l)] = E [ a(Zl, g\u0302 (\u2212l))\u2212A(g\u0302(\u2212l)) \u2223\u2223\u2223Z(\u2212l)]\u2212 E [a(Zl, g0)\u2212A(g0)] = 0\nand\nE [ K\n(l2) l1 \u2223\u2223\u2223Z(\u2212l1,\u2212l2)] = E [ a(Zl1 , g\u0302 (\u2212l1,\u2212l2))\u2212A(g\u0302(\u2212l1,\u2212l2)) \u2223\u2223\u2223Z(\u2212l1,\u2212l2)]+ E [a(Zl1 , g0)\u2212A(g0)\u2223\u2223\u2223Z(\u2212l1,\u2212l2)]\n= E [ a(Zl1 , g\u0302 (\u2212l1,\u2212l2))\u2212A(g\u0302(\u2212l1,\u2212l2)) \u2223\u2223\u2223Z(\u2212l1,\u2212l2)]+ E [a(Zl1 , g0)\u2212A(g0)] = 0\nFor simplicity, we show that (n\u2212 1)E[K1K2] = o(1), i.e. we show that the second term vanishes for l1 = 1 and l2 = 2. The same exact arguments generalize to arbitrary l1, l2. We begin by writing:\n(n\u2212 1)E[K1K2] = (n\u2212 1)E [( K1 \u2212K(2)1 ) K2 ] ,\nsince by tower law: E [ K\n(2) 1 K2\n] = E [ E [ K2 \u2223\u2223\u2223Z(\u22122), Z\u03031]K(2)1 ] = E [E [K2\u2223\u2223\u2223Z(\u22122)]K(2)1 ] = 0 Similarly by conditioning on Z(\u22121), Z\u03032 and using tower law, we can show that\nE [( K1 \u2212K(2)1 ) K (1) 2 ] = E [ E [ K1 \u2212K(2)1 \u2223\u2223\u2223Z(\u22121), Z\u03032]K(1)2 ] = E [{ E [ K1\n\u2223\u2223\u2223Z(\u22121), Z\u03032]\u2212 E [K(2)1 \u2223\u2223\u2223Z(\u22121), Z\u03032]}K(1)2 ] = E [{ E [ K1\n\u2223\u2223\u2223Z(\u22121)]\u2212 E [K(2)1 \u2223\u2223\u2223Z(\u22121,\u22122)]}K(1)2 ] = 0. Hence, we have\n(n\u2212 1)E[K1K2] = (n\u2212 1)E [( K1 \u2212K(2)1 ) K2 ] = (n\u2212 1)E [( K1 \u2212K(2)1 )( K2 \u2212K(1)2 )] With identical arguments, the same equality holds for any indices l1, l2. By Cauchy-Schwarz:\nmax l1 6=l2 (n\u2212 1)E[Kl1Kl2 ] = max l1 6=l2\n(n\u2212 1)E [( Kl1 \u2212K (l2) l1 )( Kl2 \u2212K (l1) l2 )] \u2264 max\nl1 6=l2 (n\u2212 1) \u2225\u2225\u2225Kl1 \u2212K(l2)l1 \u2225\u2225\u22252 \u2225\u2225\u2225Kl2 \u2212K(l1)l2 \u2225\u2225\u22252 Thus for maxl1 6=l2(n \u2212 1)E[Kl1Kl2 ] = o(1) it suffices that: maxl1 6=l2\n\u2225\u2225\u2225Kl1 \u2212K(l2)l1 \u2225\u2225\u22252 = o(n\u22121/2). Expanding the definitions Kl1 and K (l2) l1\n, the above simplifies to:\u2225\u2225\u2225Kl1 \u2212K(l2)l1 \u2225\u2225\u22252 = \u2225\u2225\u2225a(Zl1 , g\u0302(\u2212l1))\u2212A(g\u0302(\u2212l1))\u2212 (a(Zl1 , g\u0302(\u2212l1,\u2212l2))\u2212A(g\u0302(\u2212l1,\u2212l2)))\u2225\u2225\u22252 . The latter is upper bounded by a triangle inequality and a Jensen\u2019s inequality by:\u2225\u2225\u2225Kl1 \u2212K(l2)l1 \u2225\u2225\u22252 \u2264 2 \u2225\u2225\u2225a(Zl1 , g\u0302(\u2212l1))\u2212 a(Zl1 , g\u0302(\u2212l1,\u2212l2))\u2225\u2225\u22252 . If we denote with Z a fresh random sample not part of the training sets, then since Zl1 is not part of Z(\u2212l1), we have:\u2225\u2225\u2225a(Zl1 , g\u0302(\u2212l1))\u2212 a(Zl1 , g\u0302(\u2212l1,\u2212l2))\u2225\u2225\u2225 2 = \u2225\u2225\u2225a(Z, g\u0302(\u2212l1))\u2212 a(Z, g\u0302(\u2212l1,\u2212l2))\u2225\u2225\u2225 2\n= \u2225\u2225\u2225a(Z, g\u0302)\u2212 a(Z, g\u0302(\u2212l2))\u2225\u2225\u2225\n2\nwhere we also used the fact that Z\u0303l1 only appears in the training sets of g\u0302(\u2212l1) and g\u0302(\u2212l1,\u2212l2) and we can simply rename it to Zl1 , as they are identically distributed and both independent from all the other data points. Invoking the second of our stability conditions for a, we have that: maxl1 6=l2 \u2225\u2225\u2225Kl1 \u2212K(l2)l1 \u2225\u2225\u22252 = o(n\u22121/2). Hence, J3,n = o(1), which completes the proof."
        },
        {
            "heading": "3.1 Tightness of Stability Condition",
            "text": "We present here an example that shows that, without further structural constraints (on the moment or the function space G), the stability condition we impose is required for stochastic equicontinuity condition to hold. Let (Xi)\ni.i.d\u223c unif[0, 1] be i.i.d uniform random variables. We set Yi := I(Xi \u2264 0.5) and set Zi := (Xi, Yi). For any x \u2208 [0, 1], we define c(Z1:n, x) := arg mini\u2264n |Xi \u2212 x| the function that returns the index of the nearest example to x in {X1, . . . , Xn} and note that the quantity Yc(Z1:n,x) is its nearest neighbour estimator. Let\ng\u0302(Z1:n)(x, y) := n 1/6I(y 6= Yc(Z1:n,x)), \u03bd(Z, g) := g(Z) 3.\nWe remark that \u03bd(\u00b7, g\u0302) does not satisfy our stability conditions as \u2016\u03bd(Z, g\u0302)\u2212\u03bd(Z, g\u0302(\u22121))\u20162 is exactly of order 1/ \u221a n, neither does it respect the stochastic equicontinuity property.\nLemma 3. Let (Xi) i.i.d\u223c unif[0, 1] be i.i.d uniform random variables. We set Yi := I(Xi \u2264 0.5) and set Zi := (Xi, Yi). There are constants C, c > 0 such that\n\u2022 Set g0(Z) := 0 then we have \u2016g\u0302(Z)\u2212 g0(Z)\u201622 \u2192 0\n\u2022 C\u221a n \u2265 \u2016\u03bd(Z, g\u0302)\u2212 \u03bd(Z, g\u0302(\u22121))\u20162 \u2265 c\u221an\n\u2022 \u221a n|V (g\u0302)\u2212 V (g0)\u2212 ( Vn(g\u0302)\u2212 Vn(g0) ) | 6 P\u2212\u2192 0."
        },
        {
            "heading": "4 Application: Bagging Estimators",
            "text": "We remark that if the functions a and \u03bd satisfy certain Lp-Lipchitz conditions then the stability conditions of lemma 2 are implied by the algorithmic stability of the estimator g\u0302. Those conditions are only marginally stronger than the condition of mean-squared-continuity found in lemma 2\nCorollary 4. Fix any constant r > 1. Suppose that there is L < \u221e such that the estimation algorithm satisfies the following uniform L2r-continuity condition: for all i, j \u2208 [p] and l \u2208 [n]\nE[(ai,j(Zl; g\u0302)\u2212 ai,j(Zl; g\u0302(\u2212l)))2] \u2264 L \u00b7 E [\nsup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162r2\n]1/r (1)\nE[(\u03bdi(Zl; g\u0302)\u2212 \u03bdi(Zl; g\u0302(\u2212l)))2] \u2264 L \u00b7 E [\nsup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162r2\n]1/r (L2r-Continuity)\nE[(ai,j(Z; g\u0302)\u2212 ai,j(Z; g\u0302(\u2212l)))2] \u2264 L \u00b7 E [\nsup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162r2 ]1/r E[(\u03bdi(Z; g\u0302)\u2212 \u03bdi(Z; g\u0302(\u2212l)))2] \u2264 L \u00b7 E [ sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162r2 ]1/r .\nSuppose in addition that\nmax l\u2264n\nEZ1:n [\nsup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162r2\n]1/2r = o ( n\u22121/2 ) (Algorithmic Stability)\nand if in addition mean-squared-continuity condition is satisfied, then the conditions of lemma 2 are satisfied.\nThe uniform L2r-continuity conditions are going to be satisfied by most moment functions m(\u00b7; \u00b7, \u00b7). We notably show in the appendix that example 1, example 2 and example 3 with general moment conditions satisfy our conditions.\nThe condition of (Algorithmic Stability) is a commonly assumed condition in recent literature [4, 5] and is satisfied by various regularized empirical risk minimization estimators and stochastic gradient descent estimators [15, 24]. Notably, it is satisfied by bagged estimators. We show in theorem 5 that under very general conditions a bagged ensemble of any machine learning estimator is stable. Let Z1, . . . , Zn \u2208 Z be an independent and identically distributed (i.i.d.) sample of size n. We sample uniformly randomly without replacement from these observations repeatedly and independently, each time taking a sample of size m, for a number of B times. We denote the resulting samples as\nZb1:m := { Zb1, . . . , Z b m } , b \u2208 [B].\nLet h\u0302m : Zm \u2192 RP be a base machine learning estimator trained on m observations. This base estimator can be tree, a CNN, a nearest-neighbour classifier, or any other type of machine learning estimator. The corresponding bagged estimator is\ng\u0302(\u00b7) = 1 B B\u2211 b=1 h\u0302m(Z b 1:m)(\u00b7).\nWe will show that subject to B and m being sufficiently big and mild moment conditions on the base estimator h\u0302m, the bagged estimator satisfies condition (Algorithmic Stability).\nTheorem 5. Fix any constants s, k \u2265 2r such that 1s + 1 k = 1 2r . Assume B,m satisfy\nm = o( \u221a n) B >> m2/k \u00b7 n1\u2212 2k ,\nand assume the base estimator h\u0302 has bounded moments:\u2225\u2225\u2225\u2225sup x \u2225\u2225\u2225h\u0302(Z11:m)(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 s \u2264 C\nfor some constant C > 0. Then (Algorithmic Stability) is achieved:\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = o(n\u22121/2).\nTherefore if a and \u03bd satisfy the condition (1) then the condition (2) is satisfied.\nIn particular, we can take the base machine learning estimator to be the 1-nearest neighbor estimator. This specific choice leads to a bagged estimator that can be proved to satisfy both our stability conditions and our consistency rate condition in Theorem 1, in regression settings where covariates are of small intrinsic dimension.\nLet {Zi = (Xi, Yi)}ni=1 be a sample of size n, drawn independently and identically distributed from Z = (X,Y ). Here X \u2208 X \u2282 RD are known covariates and Y \u2208 Y \u2282 RP is the response variable. As in our bagging setting, let Z11:m, . . . , ZB1:m be B independent samples of size m drawn without\nreplacement from observations {Zi}ni=1. A bagged 1-nearest neighbor (1-NN) estimator takes the following form:\ng\u0302(x) := 1\nB B\u2211 b=1 n\u2211 i=1 1{Xi=Sb(x)}Yi,\nwhere Sb(x) is the 1-NN of x in the set Zb1:m. The estimator is used to estimate the conditional expectation g0(x) := E[Y | X = x].\nLemma 6 (Special Case of Theorem 3 of [36]). Assume that\n\u2022 The marginal distribution \u00b5 of X1 satisfies (C, d)-homogeneity on the ball B(x, r):\n\u00b5(B(x, r)) \u2264 C\u03b1\u2212d\u00b5(B(x, \u03b1r)) \u2200\u03b1 \u2208 (0, 1)\nfor some C, r > 0. Here d is referred to as the intrinsic dimension of the distribution.\n\u2022 The conditional expectation E[Y | X = x] is a Lipschitz function in the coordinates x.\n\u2022 The response variable Y is bounded in L\u221e-norm: \u2016Y \u2016\u221e <\u221e.\nThen the bagged 1-NN estimator g\u0302 with B \u2265 nm satisfies the following convergence condition:\u221a E [ \u2016g\u0302(X)\u2212 g0(X)\u201622 ] \u2264 O ( m\u22121/d ) +O (\u221a mP log log(Pn/m)\nn\n) .\nIn particular, we note that when 0 < d < 2 and m = O(n 1 2\u2212 ) for some 0 < \u2264 12 \u2212 1 4d, we\nimmediately have that \u221a E [ \u2016g\u0302(X)\u2212 g0(X)\u201622 ] = o(n\u22121/4),\nachieving the convergence rate assumed in Theorem 1. Moreover, provided that we choose B to be sufficiently large such that\nB >> n + 1 2 B \u2265 n1\u2212 2 +1 k ,\nthe required stability conditions can also be satisfied."
        },
        {
            "heading": "5 Experimental Evaluation",
            "text": "We consider a synthetic experimental evaluation of our main theoretical findings. We focus on the partially linear model with a scalar outcome Y \u2208 R, a scalar continuous treatment T \u2208 R and many controls X \u2208 Rnx , where:\nT = p0(X) + \u03b7, \u03b7 \u223c N(0, 1) Y = \u03b80T + f0(X) + , \u223c N(0, 1)\nOur goal is the estimation of the treatment effect \u03b80, while estimating the nuisance functions p0(X) and q0(X) = \u03b80p0(X) + f0(X) in a flexible manner. We will consider estimation based on the orthogonal moment presented in Example 1.\nWe considered sub-sampled 1-nearest neighbor regression (NN) and sub-sampled fully grown (with only 1 sample minimum leaf size) random forest (RF) regression for the estimation of p0 and q0, regressing T on X and Y on X correspondingly.\nThe true functions p0 and f0 are actually linear in our data generating process, with p0(X) = \u03b2\u20320X and f0(X) = \u03b3\u20320X and where \u03b20 and \u03b30 have only one non-zero coefficient (1-sparse), and which is the same coefficient for both \u03b20 and \u03b30. In other words, only one of the nx potential confounding variables X is actually a confounder.\nWe evaluate the performance of the estimate for \u03b80 for a range of values of the sample size n and the dimension of the controls nx and with or without cross-fitting. For the cross-fitted estimates we used 2 splits. For each specification we draw 1000 experimental samples to evaluate the distributional properties of the estimate.\nWe considered sub-sample sizes for the nuisance regressions based on our theoretical n0.49 specification and for larger specifications too. We find that the estimate without cross-fitting typically has almost equal bias and smaller (and always comparable) variance (due most probably to the smaller mean squared error of the nuisances, since they are trained on larger sizes), especially in smaller sample sizes, and has better coverage properties, when the sub-sample size is m = o(n1/2). Moreover, the estimate is approximately normally distributed, even without cross-fitting as is verified qualitatively via quantile-quantile (Q-Q) plots.\nWe also report the mean of the estimate of the standard error across the 1000 experiments, to evaluate the bias in the estimation of the standard error. We find that the estimate of the standard error is more accurate without cross-fitting, potentially because the estimate of the standard error does not incorporate the extra variance that stems from the sample-splitting process. This inaccuracy of the standard error estimate is most probably the reason for the worst coverage properties of the confidence intervals with cross-fitting.\nIn summary, we verify experimentally that for stable estimators, with the theoretically required level of stability, sample splitting or cross-fitting is not needed to maintain asymptotic normality, small bias and nominal coverage.\nAs expected however, we do verify that as the sub-sample size becomes m = O(n), where the estimate is both not stable and also does not optimize over hypothesis spaces with small critical radius or that satisfy the Donsker property, then the estimate without cross-fitting breaks down, while the estimate with cross-fitting maintains a decent performance, despite the high-variance of the nuisance estimate.\nThe experiments were run on a normal PC laptop with Processor Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz, 2112 Mhz, 4 Core(s), 8 Logical Processor(s), 16GB RAM. It took around 1.5 hours to run all the experiments."
        },
        {
            "heading": "A Proof of Theorem 1",
            "text": "Proof. For any g \u2208 G, by the linearity of the moment with respect to \u03b8:\nA(g) ( \u03b8\u0302 \u2212 \u03b80 ) = M(\u03b8\u0302, g)\u2212M(\u03b80, g)\n= M(\u03b8\u0302, g)\u2212Mn(\u03b8\u0302, g) +M(\u03b80, g0)\u2212M(\u03b80, g) +Mn(\u03b8\u0302, g).\nMoreover, for any g, with \u2016g \u2212 g0\u20162 = op(1):\nA(g) ( \u03b8\u0302 \u2212 \u03b80 ) = A(g0) ( \u03b8\u0302 \u2212 \u03b80 ) + (A(g)\u2212A(g0)) ( \u03b8\u0302 \u2212 \u03b80 ) = A(g0) ( \u03b8\u0302 \u2212 \u03b80 ) +O ( \u2016g \u2212 g0\u20162 \u2016\u03b8\u0302 \u2212 \u03b80\u20162\n) = A(g0) ( \u03b8\u0302 \u2212 \u03b80 ) + op ( \u2016\u03b8\u0302 \u2212 \u03b80\u20162 ) .\nThus for any g, with \u2016g \u2212 g0\u20162 = op(1):\nA(g0) ( \u03b8\u0302 \u2212 \u03b80 ) = M(\u03b8\u0302, g)\u2212Mn(\u03b8\u0302, g) +M(\u03b80, g0)\u2212M(\u03b80, g) +Mn(\u03b8\u0302, g) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nLet Gn(\u03b8, g) := M(\u03b8, g)\u2212Mn(\u03b8, g), then we have:\nA(g0) ( \u03b8\u0302 \u2212 \u03b80 ) = Gn(\u03b8\u0302, g) +M(\u03b80, g0)\u2212M(\u03b80, g) +Mn(\u03b8\u0302, g) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nApplying the above for g = g\u0302 and by the definition of \u03b8\u0302, we have: A(g0) ( \u03b8\u0302 \u2212 \u03b80 ) = Gn(\u03b8\u0302, g\u0302) +M(\u03b80, g0)\u2212M(\u03b80, g\u0302) +Mn(\u03b8\u0302, g\u0302) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162)\n= Gn(\u03b8\u0302, g\u0302) +M(\u03b80, g0)\u2212M(\u03b80, g\u0302) + op(n\u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nApplying Neyman orthogonality and bounded second derivative of the moment with respect to g:\nM(\u03b80, g0)\u2212M(\u03b80, g) = DgM(\u03b80, g0)[g0 \u2212 g] +O ( \u2016g \u2212 g0\u201622 ) = O ( \u2016g \u2212 g0\u201622 ) = op(n \u22121/2).\nThus we have that:\nA(g0) ( \u03b8\u0302 \u2212 \u03b80 ) = Gn(\u03b8\u0302, g\u0302) + op(n \u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nNow we decompose the empirical process part into an asymptotically normal component and asymptotically equicontinuous parts that converge to zero in probability:\nGn(\u03b8\u0302, g\u0302) = Gn(\u03b80, g0) + ( Gn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) ) + (Gn(\u03b80, g\u0302)\u2212Gn(\u03b80, g0)) .\nBy the linearity of the moment, the middle term can be written as:\nGn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) = (A(g\u0302)\u2212An(g\u0302))\u2032 (\u03b8\u0302 \u2212 \u03b80).\nNote that by a triangle inequality:\n\u2016A(g\u0302)\u2212An(g\u0302)\u2016op \u2264 \u2016A(g0)\u2212An(g0)\u2016op + \u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op\nNote that the first quantity is a simple centered empirical process and hence assuming that ai,j(Z; g0) has bounded variance, by classical results in empirical process theory we have that:\n\u2016A(g0)\u2212An(g0)\u2016op = op(1)\nMoreover, by our stochastic equicontinuity condition we have that:\n\u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op = op(n \u22121/2) = op(1).\nThus we get that \u2016A(g\u0302)\u2212An(g\u0302)\u2016op = op(1), and therefore: Gn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) = op ( \u2016\u03b8\u0302 \u2212 \u03b80\u20162 ) .\nMoreover, since by our stochastic equicontinuity conditions: \u221a n \u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op = op(1)\u221a n \u2016V (g\u0302)\u2212 V (g0)\u2212 (Vn(g\u0302)\u2212 Vn(g0))\u20162 = op(1)\nwe have by triangle inequality, the definition of the operator norm, and the fact that \u2016\u03b80\u20162 = O(1) that:\n\u2016Gn(\u03b80, g\u0302)\u2212Gn(\u03b80, g0)\u20162 \u2264 \u2016A(g\u0302)\u2212A(g0)\u2212 (An(g\u0302)\u2212An(g0))\u2016op \u2016\u03b80\u20162 + \u2016V (g\u0302)\u2212 V (g0)\u2212 (Vn(g\u0302)\u2212 Vn(g0))\u20162\n= op(n \u22121/2).\nThus we can conclude that: A(g0) ( \u03b8\u0302 \u2212 \u03b80 ) = Gn(\u03b80, g0) + op(n \u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nAssuming that the inverse A(g0)\u22121 exists, we can re-arrange to:\n\u03b8\u0302 \u2212 \u03b80 = A(g0)\u22121Gn(\u03b80, g0) + op ( n\u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162 ) .\nSince Gn(\u03b80, g0) is a mean-zero empirical process, we have that \u2016Gn(\u03b80, g0)\u20162 = Op(n\u22121/2). Thus the above equation implies that \u2016\u03b8\u0302 \u2212 \u03b80\u20162 = Op(n\u22121/2). Thus we get:\n\u03b8\u0302 \u2212 \u03b80 = A(g0)\u22121Gn(\u03b80, g0) + op ( n\u22121/2 ) or equivalently that: \u221a\nn ( \u03b8\u0302 \u2212 \u03b80 ) = \u221a nA(g0) \u22121Gn(\u03b80, g0) + op (1) .\nThe first term converges in distribution to the claimed normal limit by invoking the Central Limit Theorem. Thus the theorem follows by Slutsky\u2019s theorem."
        },
        {
            "heading": "B Proof of Lemma 3",
            "text": "Proof. Before diving into the proof, recall that c(Z1:n, x) = arg mini\u2264n|Xi \u2212 x|, and define:\nc\u22171 := c(Z1:n, 1\n2 ),\nc\u22172 :=  arg min i\u2264n s.t Xi\u2264 12 |1 2 \u2212Xi| if Xc\u22171 > 1 2 ,\narg min i\u2264n s.t Xi> 12\n|1 2 \u2212Xi| if Xc\u22171 \u2264 1 2 . .\nThat is, we let c\u22171 be the index of the nearest example in {X1, . . . , Xn} to 12 , and let c \u2217 2 be the index of the nearest example to 12 on the other side of 1 2 from.\nA new observation Z = (X,Y ), where X \u223c unif[0, 1] and Y = I(X \u2264 0.5), will be misclassified if Yc(Z1:n,X) is different from Y . Therefore it is mislabeled if it falls in the following set:\nE ( Xc\u22171 , Xc\u22172 ) := {[ 1 2 , 1 2 ( Xc\u22171 +Xc\u22172 )] if Xc\u22171 \u2264 1 2 ,[\n1 2 ( Xc\u22171 +Xc\u22172 ) , 12 ] otherwise.\nFor a given pair of random variables Xi1 , Xi2 , we write: \u03bbXi1,i2 := nP ( X \u2208 E(Xi1 , Xi2) | Xi1 , Xi2 ) .\nWe note B1 := {i \u2264 n|Xi \u2264 1/2}. Now we remark that if Xc\u22171 \u2264 0.5 then \u2223\u2223\u22231\u2212 [Xc\u22171 +Xc\u22172]\u2223\u2223\u2223 = Xc\u22171 +Xc\u22172 \u2212 1 and\nXc\u22172 \u2212 (1\u2212Xc\u22171 )|Xc\u22171 \u223c mini\u2208[n]\\B1 Ui(Xc\u22171 )\nwhere Ui(Xc\u22171 )|Xc\u22171 \u223ci.i.d unif[0,Xc\u22171 ]. Here \u223c means \"has the same distribution as.\"\nTherefore we have P ( Xc\u22171 +Xc\u22172 \u2212 1 \u2264 2t\nn \u2223\u2223\u2223Xc\u22171 \u2264 0.5, Xc\u22171) = P( mini\u2208[n]\\B1 Ui(Xc\u22171 ) \u2264 2tn \u2223\u2223\u2223Xc\u22171 \u2264 0.5, Xc\u22171)\n= 1\u2212 (\n1\u2212 2t nXc\u22171\n)n\u2212|B1| ,\nwhere |B1| denotes the cardinality of set B1.\nTherefore as Xc\u22171 \u2192 0.5 and |B1| \u2192 n/2 we have\nP ( Xc\u22171 +Xc\u22172 \u2212 1 \u2264 2t\nn \u2223\u2223\u2223Xc\u22171 \u2264 0.5, Xc\u22171)\u2192 1\u2212 e\u22122t. Similarly we remark that if Xc\u22171 > 0.5 then\n\u2223\u2223\u22231\u2212 [Xc\u22171 +Xc\u22172]\u2223\u2223\u2223 = 1\u2212Xc\u22171 \u2212Xc\u22172 and 1\u2212Xc\u22171 \u2212Xc\u22172 |Xc\u22171 \u223c mini\u2208B1 U bisi (Xc\u22171 )\nwhere U bisi (Xc\u22171 )|Xc\u22171 \u223ci.i.d unif[0, 1\u2212Xc\u22171 ].\nTherefore we have P (\n1\u2212Xc\u22171 \u2212Xc\u22172 \u2264 2t\nn \u2223\u2223\u2223Xc\u22171 > 0.5, Xc\u22171) = P(mini\u2208B1 U bisi (Xc\u22171 ) \u2264 2tn \u2223\u2223\u2223Xc\u22171 > 0.5, Xc\u22171)\n= 1\u2212 (\n1\u2212 2t n(1\u2212Xc\u22171 )\n)|B1| .\nTherefore as Xc\u22171 \u2192 0.5 and |B1| \u2192 n/2 we have\nP (\n1\u2212Xc\u22171 \u2212Xc\u22172 \u2264 2t\nn \u2223\u2223\u2223Xc\u22171 > 0.5, Xc\u22171)\u2192 1\u2212 e\u22122t. This directly implies that\n\u03bbXc\u22171 ,c\u22172 = n\n2 \u2223\u2223\u22231\u2212 [Xc\u22171 +Xc\u22172]\u2223\u2223\u2223 d\u2212\u2192 Exp(2), where Exp(2) denotes an exponential distribution with rate parameter 2.\nMoreover, we now also show that the expectation E [ \u03bbXc\u22171 ,c\u22172 ] = O(1).\nAs a first step, we note that we can write E [ \u03bbXc\u22171 ,c\u22172 ] = nP ( X \u2208 E ( Xc\u22171 , Xc\u22172 )) \u2264 nP ( X \u2208 E ( Xc\u22171 , Xc\u22172 ) , \u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 ) + nP (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 > 14 ) .\nBy Azuma\u2019s concentration inequality, we know that nP (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 > 14 ) \u2264 2ne\u2212 32n \u2192 0 as n\u2192\u221e.\nTo treat the other term in the sum, we have that nP ( X \u2208 E ( Xc\u22171 , Xc\u22172 ) , \u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 ) = nE [ P ( X \u2208 E ( Xc\u22171 , Xc\u22172 ) , \u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 \u2223\u2223\u2223\u2223\u2223X1:n )]\n= nE [ P ( X \u2208 E ( Xc\u22171 , Xc\u22172 )\u2223\u2223\u2223\u2223\u2223X1:n ) I (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )]\n= nE [\u2223\u2223\u2223E(Xc\u22171 , Xc\u22172)\u2223\u2223\u2223 I(\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )]\nwhere X1:n := (X1, . . . , Xn) and \u2223\u2223\u2223E(Xc\u22171 , Xc\u22172)\u2223\u2223\u2223 denotes the length of the interval E(Xc\u22171 , Xc\u22172).\nNow by triangle inequality nE [\u2223\u2223\u2223E(Xc\u22171 , Xc\u22172)\u2223\u2223\u2223 I(\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )]\n= nE [ 1\n2 \u2223\u22231\u2212 (Xc\u22171 +Xc\u22172)\u2223\u2223 I(\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )] \u2264 n 2 E [\u2223\u2223\u2223\u222312 \u2212Xc\u22171 \u2223\u2223\u2223\u2223 I(\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )] + n 2 E [\u2223\u2223\u2223\u222312 \u2212Xc\u22172 \u2223\u2223\u2223\u2223 I(\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14\n)] = n 2 E [ min i\u2208B1 Ui \u00b7 I (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )] + n 2 E [ min i\u2208[n]\\B1 Ui \u00b7 I (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )]\n= nE [\nmin i\u2208B1 Ui \u00b7 I (\u2223\u2223\u2223\u2223 |B1|n \u2212 12 \u2223\u2223\u2223\u2223 \u2264 14 )]\n\u2264 nE [\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4\n)] .\nwhere Ui := \u2223\u2223 1 2 \u2212Xi \u2223\u2223 \u223ci.i.d. unif[0, 0.5], and the penultimate line follows from symmetry.\nWe can express the expectation in terms of integrals of tail probabilities as nE [\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4\n)] = nE [ E [\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4 ) \u2223\u2223\u2223 |B1|]] = nE\n[\u02c6 \u221e 0 P ( min i\u2208B1 Ui \u00b7 I ( |B1| \u2265 n 4 ) \u2265 t \u2223\u2223\u2223 |B1|) dt] = nE [ 1\n|B1| \u02c6 \u221e 0\nP (\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4 ) \u2265 t |B1| \u2223\u2223\u2223 |B1|) dt] . Here\nP (\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4 ) \u2265 t |B1| \u2223\u2223\u2223 |B1|) =\n( 1\u2212 2t |B1| )|B1| \u00b7 I ( |B1| \u2265 n 4 ) \u2264 e\u22122t \u00b7 I ( |B1| \u2265 n 4 ) where we have used the inequality that 1\u2212 x \u2264 e\u2212x for all x.\nHence, we have nE [ 1\n|B1| \u02c6 \u221e 0\nP (\nmin i\u2208B1\nUi \u00b7 I ( |B1| \u2265 n\n4 ) \u2265 t |B1| \u2223\u2223\u2223 |B1|) dt] \u2264 nE [ 1\n|B1| \u02c6 \u221e 0 e\u22122t \u00b7 I ( |B1| \u2265 n 4 ) dt ] = n 2 E [ 1 |B1| \u00b7 I ( |B1| \u2265 n 4 )] \u2264 n 2 E [ 4 n \u00b7 I ( |B1| \u2265 n 4 )] = 2P ( |B1| \u2265 n 4 ) \u2264 2.\nAltogether, we have shown that E [ \u03bbXc\u22171 ,c\u22172 ] = O(1). (2)\nThe key point is to note that\n\u2016g\u0302(Z1:n)(Z)\u201622 = n1/3P(Y 6= Yc(Z1:n,X))\n\u2264 n1/3P(X \u2208 E(Xc\u22171 , Xc\u22172 )) (a)\u2212\u2212\u2192 0.\nwhere (a) comes from realizing that\nP(X \u2208 E(Xc\u22171 , Xc\u22172 )) = 1 n E [ \u03bbXc\u22171 ,c\u22172 ] .\nTherefore we proved the first point. Moreover if we denote\nc \u2217(\u22121) 1 := c(Z (\u22121) 1:n ,\n1 2 ),\nc \u2217(\u22121) 2 :=  arg min i\u2264n s.t X(\u22121)i \u2264 1 2 |1 2 \u2212X(\u22121)i | if X (\u22121) c \u2217(\u22121) 1 > 12 ,\narg min i\u2264n s.t X(\u22121)i > 1 2\n|1 2 \u2212X(\u22121)i | if X (\u22121) c \u2217(\u22121) 1\n\u2264 12 ,\nwhere Z(\u22121)1:n is Z1:n with the first observation replaced with an independent copy Z\u03031 = (X\u03031, Y\u03031), then we remark that\nP ( c \u2217(\u22121) 1 6= c\u22171 or c \u2217(\u22121) 2 6= c\u22172)\n\u2264 P ( c \u2217(\u22121) 1 6= c\u22171) + P(c \u2217(\u22121) 2 6= c\u22172)\n\u2264 P(1 = c\u22171) + P(1 = c \u2217(\u22121) 1 ) + P(1 = c \u2217 2) + P(1 = c \u2217(\u22121) 2 )\n(b) \u2264 4 n\nwhere (b) comes from symmetry (each observation has an equal chance of being c\u22171, for example). Moreover we note that if neither c\u2217(\u22121)1 6= c\u22171 nor c \u2217(\u22121) 2 6= c\u22172 then we have E(X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2 ) =\nE(Xc\u22171 , Xc\u22172 ).\nNow for ease of notation denote Xc\u22171 ,c\u22172 := ( Xc\u22171 , Xc\u22172 ) ,\nX c \u2217(\u22121) 1 ,c \u2217(\u22121) 2 :=\n( X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2\n) ,\nX c\u22171 ,c \u2217 2 ,c \u2217(\u22121) 1 ,c \u2217(\u22121) 2 :=\n( Xc\u22171 , Xc\u22172 , X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2\n) ,\nand E ( X c\u22171 ,c \u2217 2 ,c \u2217(\u22121) 1 ,c \u2217(\u22121) 2 ) := E(Xc\u22171 , Xc\u22172 )4E ( X (\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2 ) .\nThen we have that there is C <\u221e such that\n\u2016\u03bd(Z, g\u0302)\u2212 \u03bd(Z, g\u0302(\u22121))\u20162 = \u221a n \u221a P ( X \u2208 E ( X c\u22171 ,c \u2217 2 ,c \u2217(\u22121) 1 ,c \u2217(\u22121) 2 )) = \u221a nE [ P ( X \u2208 E ( X c\u22171 ,c \u2217 2 ,c \u2217(\u22121) 1 ,c \u2217(\u22121) 2\n) \u2223\u2223\u2223Xc\u22171 ,c\u22172 ,c\u2217(\u22121)1 ,c\u2217(\u22121)2 ) I(c\u2217(\u22121)1 6= c\u22171 or c\u2217(\u22121)2 6= c\u22172)] \u2264 \u221a nE [(\u2223\u2223E(Xc\u22171 ,c\u22172 )\u2223\u2223+ \u2223\u2223\u2223E(Xc\u2217(\u22121)1 ,c\u2217(\u22121)2 )\u2223\u2223\u2223) I(c\u2217(\u22121)1 6= c\u22171 or c\u2217(\u22121)2 6= c\u22172)]\n\u2264 \u221a 2nE [\u2223\u2223E(Xc\u22171 ,c\u22172 )\u2223\u2223 I(c\u2217(\u22121)1 6= c\u22171 or c\u2217(\u22121)2 6= c\u22172)] by symmetry\n\u2264 \u221a 2nE [\u2223\u2223E(Xc\u22171 ,c\u22172 )\u2223\u2223 (I (c\u22171 = 1) + I(c\u2217(\u22121)1 = 1)+ I (c\u22172 = 1) + I(c\u2217(\u22121)2 = 1))]\n(c) = \u221a 2nP ( X \u2208 E(Xc\u22171 ,c\u22172 ) ) ( P (c\u22171 = 1) + P ( c \u2217(\u22121) 1 = 1 ) + P (c\u22172 = 1) + P ( c \u2217(\u22121) 2 = 1 )) =\n\u221a 8\u221a n \u221a E [ \u03bbXc\u22171 ,c\u22172 ] (d) \u2264 C\u221a n\nwhere to get (c) we exploited independence of (Xc\u22171 , Xc\u22172 ) and the events {c \u2217 1 = 1}, {c\u22172 = 1}, {c\u2217(\u22121)1 = 1}, {c \u2217(\u22121) 2 = 1} and where to get (d) we exploited (2).\nMoreover we also notice that P ( c \u2217(\u22121) 1 6= c\u22171 ) \u2265 P ( c\u22171 = 1, X\u03031 6\u2208 [ 1 2 \u2212 \u2223\u2223\u2223\u222312 \u2212Xc\u22173 \u2223\u2223\u2223\u2223 , 12 + \u2223\u2223\u2223\u222312 \u2212Xc\u22173\n\u2223\u2223\u2223\u2223]) = P (c\u22171 = 1)P ( X\u03031 6\u2208 [ 1 2 \u2212 \u2223\u2223\u2223\u222312 \u2212Xc\u22173 \u2223\u2223\u2223\u2223 , 12 + \u2223\u2223\u2223\u222312 \u2212Xc\u22173\n\u2223\u2223\u2223\u2223]) by independence = 1 n E [ 1\u2212 2 \u2223\u2223\u2223\u222312 \u2212Xc\u22173 \u2223\u2223\u2223\u2223] = 1n \u2212 2nE [\u2223\u2223\u2223\u222312 \u2212Xc\u22173 \u2223\u2223\u2223\u2223]\nwhere c\u22173 := arg mini\u2208[n]\\{c\u22171} \u2223\u2223Xi \u2212 12 \u2223\u2223 is the index of the second nearest neighbor of 12 among X1:n. Note that c\u22173 is not necessarily equal to c\u22172, since the definition of c\u22172 requires Xc\u22172 to be on the other side of 12 from Xc\u22171 while that of c \u2217 3 does not.\nBy our knowledge of the expectation of the second order statistic among i.i.d. uniform random variables, we obtain that\n1 n \u2212 2 n E [\u2223\u2223\u2223\u222312 \u2212Xc\u22173 \u2223\u2223\u2223\u2223] = 1n \u2212 2n \u00b7 12 \u00b7 2n+ 1 = 1n \u2212 2n(n+ 1) .\nTherefore, similarly to before, we also have that there is a constant c\u0303, c > 0 such that\n\u2016\u03bd(Z, g\u0302)\u2212 \u03bd(Z, g\u0302(\u22121)\u20162 = \u221a n \u221a P ( X \u2208 E(Xc\u22171 , Xc\u22172 )4E ( X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2 )) \u2265 \u221a n \u221a P ( X \u2208 E(Xc\u22171 , Xc\u22172 )4E ( X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2 )\u2223\u2223\u2223c\u2217(\u22121)1 6= c\u22171)P(c\u2217(\u22121)1 6= c\u22171) \u2265 \u221a n \u221a P ( X \u2208 E(Xc\u22171 , Xc\u22172 )4E ( X\n(\u22121) c \u2217(\u22121) 1 , X (\u22121) c \u2217(\u22121) 2 )\u2223\u2223\u2223c\u2217(\u22121)1 6= c\u22171, c\u2217(\u22121)2 = c\u22172)P(c\u2217(\u22121)1 6= c\u22171) = \u221a n \u221a E [1\n2 \u2223\u2223\u2223\u2223Xc\u22171 \u2212X(\u22121)c\u2217(\u22121)1 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223c\u2217(\u22121)1 6= c\u22171, c\u2217(\u22121)2 = c\u22172]P(c\u2217(\u22121)1 6= c\u22171)\n\u2265 c\u0303 \u221a P ( c \u2217(\u22121) 1 6= c\u22171 ) \u2265 c\u221a\nn .\nTherefore we proved the second point. The third point follows because \u221a n [ Vn(g\u0302)\u2212 Vn(g0)\u2212 ( V (g\u0302)\u2212 V (g0) )] =\u2212 \u221a nV (g\u0302)=\u2212\n\u221a nV (g\u0302) = \u03bbXc\u22171 ,c\u22172 d\u2212\u2192 Exp(0.5)\nwhere Vn(g0) = V (g0) = 0 by definition, and Vn(g\u0302) since the nearest neighbor estimator evaluated at a training data point never misclassifies the point."
        },
        {
            "heading": "C Proof of Corollary 4",
            "text": "Proof. We note that by monotonicity of Lp norms, plugging the bound in (Algorithmic Stability) into the right hand side terms of (1) gives the stability conditions in lemma 2. Corollary 4 then immediately follows."
        },
        {
            "heading": "D Proof of Theorem 5",
            "text": "Proof. Denote Zb1:m,(\u2212l), b \u2208 {1, . . . , B} as the corresponding bagged samples when the l-th data point Zl is replaced with an independent copy Z\u0303l. We have that for l \u2208 [n] :\u2225\u2225\u2225\u2225sup\nx \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r\n= \u2225\u2225\u2225\u2225\u2225supx \u2225\u2225\u2225\u2225\u2225 1B B\u2211 b=1 ( h\u0302(Zb1:m)(x)\u2212 h\u0302(Zb1:m,(\u2212l))(x) )\u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r\n= \u2225\u2225\u2225\u2225\u2225supx \u2225\u2225\u2225\u2225\u2225 1B B\u2211 b=1 ( h\u0302(Zb1:m)(x)\u2212 h\u0302(Zb1:m,(\u2212l))(x) ) 1{\u2203t\u2264m s.t. Zbt=Zl} \u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r .\nThe last equality was because\nh\u0302(Zb1:m)(x)\u2212 \u03b8\u0302(Zb1:m,(\u2212l))(x) 6= 0\nonly if \u2203t \u2264 m s.t. Zbt = Zl.\nFix any l \u2208 [n]. To simplify notations, let\n\u2207h\u0302(Zb)(x) := h\u0302(Zb1:m)(x)\u2212 h\u0302(Zb1:m,(\u2212l))(x),\nand let Ab be the event {\u2203t \u2264 m s.t. Zbt = Zl}. Then \u2225\u2225\u2225\u2225sup\nx \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = \u2225\u2225\u2225\u2225\u2225supx \u2225\u2225\u2225\u2225\u2225 1B B\u2211 b=1 \u2207h\u0302(Zb)(x)1Ab \u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r .\nBy triangle inequality and symmetry of distributions\u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = \u2225\u2225\u2225\u2225\u2225supx \u2225\u2225\u2225\u2225\u2225 1B B\u2211 b=1 \u2207h\u0302(Zb)(x)1Ab \u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r\n\u2264 1 B \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r\n\u2264 1 B \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 { sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2212 E [ sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l]} \u2225\u2225\u2225\u2225\u2225\n2r\n+ 1\nB B\u2211 b=1 \u2225\u2225\u2225\u2225E [sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l]\u2225\u2225\u2225\u2225 2r\n\u2264 1 B \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 { sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2212 E [ sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l]} \u2225\u2225\u2225\u2225\u2225\n2r\n+ \u2225\u2225\u2225\u2225E [sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l]\u2225\u2225\u2225\u2225 2r .\nFor ease of notations, denote Z(l) := ( Z1, . . . , Zn, Z\u0303l ) and denote\nRb := sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2212 E [ sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z(l)] . For the first term, we have by tower law that\n1\nB \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 { sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2212 E [ sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z(l)]} \u2225\u2225\u2225\u2225\u2225\n2r\n= 1\nB \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 Rb \u2225\u2225\u2225\u2225\u2225 2r\n= 1\nB E ( B\u2211 b=1 Rb )2r 12r = 1 B E E ( B\u2211 b=1 Rb )2r \u2223\u2223\u2223\u2223\u2223Z(l)  12r .\nTo further simplify, we use the following lemma:\nLemma 7 (Marcinkiewicz-Zygmund inequality [50]). Let p \u2265 1. If X1, . . . , Xn are i.i.d. random variables such that E[X1] = 0, then there exists a constant Cp such that\n\u2016 1\u221a n n\u2211 i=1 Xi\u2016p \u2264 Cp\u2016Xi\u2016p.\nSince Rb, b \u2208 [B] are i.i.d. conditional on Z(l), we then have that\n1 B E\nE ( B\u2211\nb=1\nRb )2r \u2223\u2223\u2223\u2223\u2223Z(l)  12r\n\u2264 1 B \u00b7 E\n[(\u221a BC2r )2r E [ R2rb \u2223\u2223\u2223\u2223\u2223Z(l) ]]1/2r = C2r\u221a B \u00b7 \u2016Rb\u20162r by tower law.\nNow also \u2225\u2225\u2225\u2225E [sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l]\u2225\u2225\u2225\u2225 2r\n= \u2225\u2225\u2225\u2225P(A1\u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l)E [sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l, A1]\u2225\u2225\u2225\u2225 2r\nsince \u2207h\u0302(Z1)(x) = 0 for all x on Ac1\n= \u2225\u2225\u2225\u2225P (A1)E [sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l, A1]\u2225\u2225\u2225\u2225 2r\nsince A1 is independent of Z1, . . . , Zn, Z\u0303l = P (A1) \u2225\u2225\u2225\u2225E [sup\nx\n\u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l, A1]\u2225\u2225\u2225\u2225 2r\n\u2264 P (A1) \u2225\u2225\u2225\u2225sup\nx\n\u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r by Jensen\u2019s inequality and tower law\n\u2264 P (A1) \u00b7 2C by moment condition, since 2r \u2264 s.\nBy union bound,\nP (A1) \u2264 m\u2211 t=1 P ( Z1t = Zl ) = m n .\nTherefore, altogether we obtain\u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r\n\u2264 C2r\u221a B \u00b7 \u2016Rb\u20162r + 2C \u00b7 m n\n\u2264 C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r + C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225E [sup x \u2225\u2225\u2225\u2207h\u0302(Zb)(x)1Ab\u2225\u2225\u2225 2 \u2223\u2223\u2223Z(l)]\u2225\u2225\u2225\u2225 2r + 2C \u00b7 m n\n\u2264 C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r + C2r\u221a B \u00b7 2C \u00b7 m n + 2C \u00b7 m n \u2264 2C \u00b7 C2r\u221a B \u00b7 \u20161A1\u2016k + C2r\u221a B \u00b7 2C \u00b7 m n + 2C \u00b7 m n = 2C \u00b7 C2r\u221a\nB \u00b7 (P(A1))1/k + C2r\u221a B \u00b7 2C \u00b7 m n + 2C \u00b7 m n\n\u2264 2C \u00b7 C2r\u221a B \u00b7 (m n )1/k + C2r\u221a B \u00b7 2C \u00b7 m n + 2C \u00b7 m n .\nHence, we also have\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r \u2264 2C \u00b7 C2r\u221a B \u00b7 (m n )1/k + C2r\u221a B \u00b7 2C \u00b7 m n + 2C \u00b7 m n .\nAssuming m = o( \u221a n)\nand B >> m2/k \u00b7 n1\u2212 2k ,\nthis upper bound is of order o(n\u22121/2):\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = o(n\u22121/2).\nRemark 3. We could in fact relax the conditions in Theorem 5 by using Rosenthal\u2019s inequality instead of Marcinkiewicz-Zygmund inequality in the proof. Moreover, we can relax the bounded moments condition to restrict on L2r norm instead of on Ls norm. This gives the following theorem.\nTheorem 8. Assume B,m satisfy\nm = o( \u221a n) B >> m 1 2r\u22121 \u00b7 n r\u22121 2r\u22121 ,\nand assume the base estimator h\u0302 has bounded moments:\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2225\u2225\u2225h\u0302(Z11:m)(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r \u2264 C\nfor some constant C > 0. Then (Algorithmic Stability) is achieved:\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = o(n\u22121/2).\nTherefore if a and \u03bd satisfy the condition (1) then the condition (2) is satisfied.\nProof of Theorem 8. We follow the proof of Theorem 5 and obtain\u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = \u2225\u2225\u2225\u2225\u2225supx \u2225\u2225\u2225\u2225\u2225 1B B\u2211 b=1 \u2207h\u0302(Zb)(x)1Ab \u2225\u2225\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225\u2225 2r\n\u2264 1 B \u2225\u2225\u2225\u2225\u2225 B\u2211 b=1 Rb \u2225\u2225\u2225\u2225\u2225 2r + 2C \u00b7 m n\n= 1\nB \u00b7 E\nE ( B\u2211\nb=1\nRb )2r \u2223\u2223\u2223\u2223\u2223Z(l) 1/2r + 2C \u00b7 m n by tower law.\nWe then use the following lemma.\nLemma 9 (Rosenthal\u2019s inequality [28]). Let p \u2265 1. If X1, . . . , Xn are i.i.d. random variables such that E[X1] = 0, then there exists a constant Cp such that\n\u2016 1\u221a n n\u2211 i=1 Xi\u2016p \u2264 Cp ( \u2016Xi\u20162 + n 1 p\u2212 1 2 \u2016Xi\u2016p ) .\nSince Rb, b \u2208 [B] are i.i.d. conditional on Z(l), we then have that by symmetry of distributions\n1 B \u00b7 E\nE ( B\u2211\nb=1\nRb )2r \u2223\u2223\u2223\u2223\u2223Z(l) 1/2r\n\u2264 1 B \u00b7 E\n[(\u221a BC2r )2r (( E [ R21 \u2223\u2223\u2223Z(l)])1/2 +B 12r\u2212 12 (E [R2r1 \u2223\u2223\u2223Z(l)])1/2r)2r ]1/2r\n= C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21\u2223\u2223\u2223Z(l)])1/2 +B 12r\u2212 12 (E [R2r1 \u2223\u2223\u2223Z(l)])1/2r\u2225\u2225\u2225\u2225\n2r\n.\nBy triangle inequality, we have C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21\u2223\u2223\u2223Z(l)])1/2 +B 12r\u2212 12 (E [R2r1 \u2223\u2223\u2223Z(l)])1/2r\u2225\u2225\u2225\u2225\n2r \u2264 C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21\u2223\u2223\u2223Z(l)])1/2\u2225\u2225\u2225\u2225\n2r\n+ C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225B 12r\u2212 12 (E [R2r1 \u2223\u2223\u2223Z(l)])1/2r\u2225\u2225\u2225\u2225\n2r\n.\nFor further ease of notations let\nR1,(1) := sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 ,\nR1,(2) := E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 \u2223\u2223\u2223Z1, . . . , Zn, Z\u0303l] . Note that\nR1 = R1,(1) \u2212R1,(2).\nThen also by triangle inequality and Jensen\u2019s inequality C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21\u2223\u2223\u2223Z(l)])1/2\u2225\u2225\u2225\u2225\n2r \u2264 C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21,(1)\u2223\u2223\u2223Z(l)])1/2\u2225\u2225\u2225\u2225\n2r\n+ C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21,(2)\u2223\u2223\u2223Z(l)])1/2\u2225\u2225\u2225\u2225\n2r \u2264 2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225(E [R21,(1)\u2223\u2223\u2223Z(l)])1/2\u2225\u2225\u2225\u2225\n2r\n= 2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225\u2225\u2225 ( E [( sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)1A1\u2225\u2225\u2225 2 )2 \u2223\u2223\u2223Z(l) ])1/2\u2225\u2225\u2225\u2225\u2225\u2225\n2r\n= 2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225\u2225 ( E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u00b7 1A1 \u2223\u2223\u2223Z(l)])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\n.\nFurther, we rewrite this term as\n2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225\u2225 ( E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u00b7 1A1 \u2223\u2223\u2223Z(l)])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\n= 2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225\u2225 ( P ( A1 \u2223\u2223\u2223Z(l)) \u00b7 E [sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u2223\u2223\u2223Z(l), A1])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\nsince \u2207h\u0302(Z1)(x) = 0 for all x on Ac1\n= 2C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225\u2225 ( P (A1) \u00b7 E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u2223\u2223\u2223Z(l), A1])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\nsince A1 is independent of Z(l)\n= 2C2r\u221a B \u00b7 (P (A1))1/2 \u2225\u2225\u2225\u2225\u2225 ( E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u2223\u2223\u2223Z(l), A1])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\n.\nBy Jensen\u2019s inequality and tower law, we have\n2C2r\u221a B \u00b7 (P (A1))1/2 \u2225\u2225\u2225\u2225\u2225 ( E [ sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u22252 2 \u2223\u2223\u2223Z(l), A1])1/2 \u2225\u2225\u2225\u2225\u2225\n2r\n\u2264 2C2r\u221a B \u00b7 (P (A1))1/2 \u2225\u2225\u2225\u2225sup x \u2225\u2225\u2225\u2207h\u0302(Z1)(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r\n\u2264 2C2r\u221a B \u00b7 (m n )1/2 \u00b7 2C.\nSimilarly, by replacing the powers of 2 and 1/2 with 2r and 1/2r, we can show that C2r\u221a B \u00b7 \u2225\u2225\u2225\u2225B 12r\u2212 12 (E [R2r1 \u2223\u2223\u2223Z(l)])1/2r\u2225\u2225\u2225\u2225\n2r\n\u2264 2C2r \u00b7B 1 2r\u22121 (m n )1/2r \u00b7 2C.\nAltogether, we have\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r\n\u2264 2C2r\u221a B \u00b7 (m n )1/2 \u00b7 2C + 2C2r \u00b7B 1 2r\u22121 (m n )1/2r \u00b7 2C + 2C \u00b7 m n .\nAssuming m = o( \u221a n)\nand B >> m 1 2r\u22121 \u00b7 n r\u22121 2r\u22121 ,\nthis upper bound is of order o(n\u22121/2):\nmax l\u2264n \u2225\u2225\u2225\u2225sup x \u2016g\u0302(x)\u2212 g\u0302(\u2212l)(x)\u20162 \u2225\u2225\u2225\u2225 2r = o(n\u22121/2)."
        },
        {
            "heading": "E Proof of Lemma 6",
            "text": "Proof. Lemma 6 follows from Theorem 3 of [36] by taking \u03c8(Z; \u03b8) := Y \u2212 \u03b8(x).\nF Establishing L2r-Continuity and Mean-Squared-Continuity for Examples 1, 2, and 3\nRecall that\nm(Z; \u03b8, g) = a(Z; g)\u03b8 + \u03bd(Z; g).\nWe will establish L2r-continuity and mean-squared-continuity for Examples 1, 2, and 3 in this section.\nF.1 Establishing for Example 1\nFor this example, we have a(Z; g) = (T \u2212 p(X))(T \u2212 p(X))\u2032\n\u03bd(Z; g) = (Y \u2212 q(X))(T \u2212 p(X))\u2032.\nLet v \u2265 1 be the constant such that 1 = 1r + 1 v . Denote Ti, i \u2208 [p] as the i-th coordinate of T . Denote Yi, i \u2208 [p] as the i-th coordinate of Y . For any function p denote pi(X), i \u2208 [p] as the i-th coordinate of p(X). We will show that subject to\n\u2016Ti\u20162v , \u2016p\u0302i(X)\u20162v , \u2016p\u0302i(Xl)\u20162v , \u2016Y \u20162v , \u2016q\u0302(X)\u20162v\nbeing finite for all i \u2208 [p] then L2r-continuity conditions hold.\nMoreover, we will show that if further\n\u2016Ti\u2016\u221e , \u2016pi(X)\u2016\u221e , \u2016p \u2032 i(X)\u2016\u221e , \u2016Y \u2016\u221e , \u2016q \u2032(X)\u2016\u221e\nare finite for all i \u2208 [p] then mean-squared-continuity conditions hold with q = 2.\nWe will illustrate for function a and for function \u03bd separately.\nFor function a: We first verify L2r-continuity for function a. We have that for any i, j \u2208 [p]\nai,j(Z; g\u0302)\u2212 ai,j(Z; g\u0302(\u2212l)) = ( p\u0302\n(\u2212l) i (X)\u2212 p\u0302i(X) ) Tj + Ti ( p\u0302 (\u2212l) j (X)\u2212 p\u0302j(X) ) + 1\n2\n( p\u0302\n(\u2212l) i (X)\u2212 p\u0302i(X)\n)( p\u0302\n(\u2212l) j (X) + p\u0302j(X) ) + 1\n2\n( p\u0302\n(\u2212l) i (X) + p\u0302i(X)\n)( p\u0302\n(\u2212l) j (X)\u2212 p\u0302j(X)\n) .\nThen by triangle inequality and H\u00f6lder\u2019s inequality, we obtain\u2225\u2225\u2225ai,j(Z; g\u0302)\u2212 ai,j(Z; g\u0302(\u2212l))\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225sup\nx \u2223\u2223\u2223p\u0302(\u2212l)i (x)\u2212 p\u0302i(x)\u2223\u2223\u2223 \u00b7 |Tj |\u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)j (x)\u2212 p\u0302j(x)\u2223\u2223\u2223 \u00b7 |Ti|\u2225\u2225\u2225\u2225 2\n+ 1\n2 \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)i (x)\u2212 p\u0302i(x)\u2223\u2223\u2223 \u00b7 (p\u0302(\u2212l)j (X) + p\u0302j(X))\u2225\u2225\u2225\u2225 2\n+ 1\n2 \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)j (x)\u2212 p\u0302j(x)\u2223\u2223\u2223 \u00b7 (p\u0302(\u2212l)i (X) + p\u0302i(X))\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225sup\nx \u2223\u2223\u2223p\u0302(\u2212l)i (x)\u2212 p\u0302i(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 \u2016Tj\u20162v + \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)j (x)\u2212 p\u0302j(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 \u2016Ti\u20162v\n+ 1\n2 \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)i (x)\u2212 p\u0302i(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 \u2225\u2225\u2225p\u0302(\u2212l)j (X) + p\u0302j(X)\u2225\u2225\u2225 2v\n+ 1\n2 \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)j (x)\u2212 p\u0302j(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 \u2225\u2225\u2225p\u0302(\u2212l)i (X) + p\u0302i(X)\u2225\u2225\u2225 2v\n(a) \u2264 L1 \u00b7 \u2225\u2225\u2225\u2225sup\nx\n\u2225\u2225\u2225g\u0302(\u2212l)(x)\u2212 g\u0302(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r\nwhere\nL1 := \u2016Tj\u20162v + \u2016Ti\u20162v + 1\n2 \u2225\u2225\u2225p\u0302(\u2212l)i (X)\u2225\u2225\u2225 2v + 1 2 \u2016p\u0302i(X)\u20162v + 1 2 \u2225\u2225\u2225p\u0302(\u2212l)j (X)\u2225\u2225\u2225 2v + 1 2 \u2016p\u0302j(X)\u20162v\n(b) = \u2016Tj\u20162v + \u2016Ti\u20162v + \u2016p\u0302i(X)\u20162v + \u2016p\u0302j(X)\u20162v .\nHere for (a) we have used the triangle inequality, and for (b) we have used the fact that Zl and Z\u0303l have the same distribution.\nBy replacing Z with Zl, we can similarly show that\u2225\u2225\u2225ai,j(Zl; g\u0302)\u2212 ai,j(Zl; g\u0302(\u2212l))\u2225\u2225\u2225 2\n\u2264 L2 \u00b7 \u2225\u2225\u2225\u2225sup\nx\n\u2225\u2225\u2225g\u0302(\u2212l)(x)\u2212 g\u0302(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r\nwhere\nL2 := \u2016Tj\u20162v + \u2016Ti\u20162v + 1\n2 \u2016p\u0302i(X)\u20162v +\n1 2 \u2016p\u0302i(Xl)\u20162v + 1 2 \u2016p\u0302j(X)\u20162v + 1 2 \u2016p\u0302j(Xl)\u20162v .\nHence, the L2r-continuity conditions hold for function a provided that all the aforementioned L2vnorm quantities are finite.\nNow we check the mean-squared-continuity conditions for function a.\nFor any g, g\u2032, any i, j \u2208 [p], we have\nai,j(Z; g)\u2212 ai,j(Z; g\u2032) = (p\u2032i(X)\u2212 pi(X))Tj + Ti ( p\u2032j(X)\u2212 pj(X) ) + 1\n2 (p\u2032i(X)\u2212 pi(X))\n( p\u2032j(X) + pj(X) ) + 1\n2 (p\u2032i(X) + pi(X))\n( p\u2032j(X)\u2212 pj(X) ) .\nThen by triangle inequality and H\u00f6lder\u2019s inequality, we obtain\n\u2016ai,j(Z; g)\u2212 ai,j(Z; g\u2032)\u20162 \u2264 \u2016p\u2032i(X)\u2212 pi(X)\u20162 \u00b7 \u2016Tj\u2016\u221e + \u2225\u2225p\u2032j(X)\u2212 pj(X)\u2225\u22252 \u00b7 \u2016Ti\u2016\u221e + 1\n2 \u2016p\u2032i(X)\u2212 pi(X)\u20162 \u00b7 \u2016p \u2032 j(X) + pj(X)\u2016\u221e +\n1\n2 \u2225\u2225p\u2032j(X)\u2212 pj(X)\u2225\u22252 \u00b7 \u2016p\u2032i(X) + pi(X)\u2016\u221e \u2264 L3 \u00b7 \u2016g \u2212 g\u2032\u20162\nwhere\nL3 := \u2016Tj\u2016\u221e + \u2016Ti\u2016\u221e + 1\n2 \u2016p\u2032j(X)\u2016\u221e +\n1 2 \u2016pj(X)\u2016\u221e + 1 2 \u2016p\u2032i(X)\u2016\u221e + 1 2 \u2016pi(X)\u2016\u221e.\nProvided all these L\u221e-norm quantities are finite, mean-squared-continuity conditions hold for function a with q = 2.\nFor function \u03bd: We have\n\u03bdi(Z; g\u0302)\u2212 \u03bdi(Z; g\u0302(\u2212l)) = ( q\u0302(\u2212l)(X)\u2212 q\u0302(X) ) Ti + Y ( p\u0302 (\u2212l) i (X)\u2212 p\u0302i(X) ) \u2212 ( q\u0302(\u2212l)(X)\u2212 q\u0302(X) ) p\u0302i(X)\u2212 q\u0302(\u2212l)(X) ( p\u0302 (\u2212l) i (X)\u2212 p\u0302i(X) ) .\nHence, by triangle inequality and H\u00f6lder\u2019s inequality, we similarly obtain that\n\u2225\u2225\u2225\u03bdi(Z; g\u0302)\u2212 \u03bdi(Z; g\u0302(\u2212l))\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225sup\nx \u2223\u2223\u2223q\u0302(\u2212l)(x)\u2212 q\u0302(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 {\u2016Ti\u20162v + \u2016p\u0302i(X)\u20162v}\n+ \u2225\u2225\u2225\u2225sup x \u2223\u2223\u2223p\u0302(\u2212l)i (x)\u2212 p\u0302i(x)\u2223\u2223\u2223\u2225\u2225\u2225\u2225 2r \u00b7 { \u2016Y \u20162v + \u2225\u2225\u2225q\u0302(\u2212l)(X)\u2225\u2225\u2225 2v } \u2264 \u2225\u2225\u2225\u2225sup\nx\n\u2225\u2225\u2225g\u0302(\u2212l)(x)\u2212 g\u0302(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r \u00b7 {\u2016Ti\u20162v + \u2016p\u0302i(X)\u20162v + \u2016Y \u20162v + \u2016q\u0302(X)\u20162v} .\nBy replacing Z with Zl, we can similarly show that\n\u2225\u2225\u2225\u03bdi(Zl; g\u0302)\u2212 \u03bdi(Zl; g\u0302(\u2212l))\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225sup\nx\n\u2225\u2225\u2225g\u0302(\u2212l)(x)\u2212 g\u0302(x)\u2225\u2225\u2225 2 \u2225\u2225\u2225\u2225 2r \u00b7 {\u2016Ti\u20162v + \u2016p\u0302i(Xl)\u20162v + \u2016Y \u20162v + \u2016q\u0302(X)\u20162v} .\nTherefore, the L2r-continuity conditions hold for function \u03bd provided that all these L2v-norm quantities are finite.\nAs for mean-squared-continuity, note that we have that\n\u03bdi(Z; g)\u2212 \u03bdi(Z; g\u2032) = (q\u2032(X)\u2212 q(X))Ti + Y (p\u2032i(X)\u2212 pi(X)) \u2212 (q\u2032(X)\u2212 q(X)) pi(X)\u2212 q\u2032(X) (p\u2032i(X)\u2212 pi(X)) .\nHence, by triangle inequality and H\u00f6lder\u2019s inequality, we derive that\n\u2016\u03bdi(Z; g)\u2212 \u03bdi(Z; g\u2032)\u20162 \u2264 \u2016q\u2032(X)\u2212 q(X)\u20162 \u00b7 \u2016Ti\u2016\u221e + \u2016p\u2032i(X)\u2212 pi(X)\u20162 \u00b7 \u2016Y \u2016\u221e + \u2016q\u2032(X)\u2212 q(X)\u20162 \u00b7 \u2016pi(X)\u2016\u221e + \u2016p\u2032i(X)\u2212 pi(X)\u20162 \u00b7 \u2016q\u2032(X)\u2016\u221e \u2264 \u2016g \u2212 g\u2032\u20162 \u00b7 {\u2016Ti\u2016\u221e + \u2016Y \u2016\u221e + \u2016pi(X)\u2016\u221e + \u2016q\u2032(X)\u2016\u221e} .\nProvided all these L\u221e-norm quantities are finite, mean-squared-continuity conditions hold for function \u03bd with q = 2.\nF.2 Establishing for Example 2\nFor this example, we have a(Z; g) = (Z \u2212 r(X))(T \u2212 p(X))\u2032\n\u03bd(Z; g) = (Y \u2212 q(X))(Z \u2212 r(X))\u2032.\nDenote Zi, i \u2208 [p] as the i-th coordinate of Z. Denote ri(X), i \u2208 [p] as the i-th coordinate of r(X). Analogously to Example 1, replacing all functions pi and their estimates with ri and their corresponding estimates, replacing all Ti with Zi in the analysis of ai,j and \u03bdi, we can show that subject to\n\u2016Ti\u20162v , \u2016Zi\u20162v , \u2016p\u0302i(X)\u20162v , \u2016p\u0302i(Xl)\u20162v , \u2016r\u0302i(X)\u20162v , \u2016r\u0302i(Xl)\u20162v , \u2016Y \u20162v , \u2016q\u0302(X)\u20162v\nbeing finite for all i \u2208 [p] then L2r-continuity conditions hold. Moreover, if further\n\u2016Ti\u2016\u221e , \u2016Zi\u2016\u221e , \u2016pi(X)\u2016\u221e , \u2016p \u2032 i(X)\u2016\u221e , \u2016ri(X)\u2016\u221e , \u2016r \u2032 i(X)\u2016\u221e , \u2016Y \u2016\u221e , \u2016q \u2032(X)\u2016\u221e\nare finite for all i \u2208 [p] then mean-squared-continuity conditions hold with q = 2.\nF.3 Establishing for Example 3\nFor this example, we have a(Z; g) \u2261 1\n\u03bd(Z; g) = \u2212mb(Z; q)\u2212 \u00b5(T,X)(Y \u2212 q(T,X)).\nThe L2r-continuity and mean-squared-continuity conditions trivially hold for function a. For function \u03bd, we have\n\u03bd(Z; g\u0302)\u2212 \u03bd(Z; g\u0302(\u2212l)) = (mb(Z; q\u0302(\u2212l))\u2212mb(Z; q\u0302)) + \u00b5\u0302(\u2212l)(T,X)(q\u0302(T,X)\u2212 q\u0302(\u2212l)(T,X))\u2212 (Y \u2212 q\u0302(T,X))(\u00b5\u0302(T,X)\u2212 \u00b5\u0302(\u2212l)(T,X)).\nHence, by triangle inequality and H\u00f6lder\u2019s inequality we obtain\n\u2016\u03bd(Z; g\u0302)\u2212 \u03bd(Z; g\u0302(\u2212l))\u20162 \u2264 \u2016mb(Z; q\u0302(\u2212l))\u2212mb(Z; q\u0302)\u20162 + \u2016\u00b5\u0302(\u2212l)(T,X)(q\u0302(T,X)\u2212 q\u0302(\u2212l)(T,X))\u20162 + \u2016(Y \u2212 q\u0302(T,X))(\u00b5\u0302(T,X)\u2212 \u00b5\u0302(\u2212l)(T,X))\u20162 \u2264 \u2016mb(Z; q\u0302(\u2212l))\u2212mb(Z; q\u0302)\u20162 + \u2016\u00b5\u0302(\u2212l)(T,X)\u20162v \u00b7 \u2016q\u0302(T,X)\u2212 q\u0302(\u2212l)(T,X)\u20162r + \u2016Y \u2212 q\u0302(T,X)\u20162v \u00b7 \u2016\u00b5\u0302(T,X)\u2212 \u00b5\u0302(\u2212l)(T,X)\u20162r \u2264 \u2016mb(Z; q\u0302(\u2212l))\u2212mb(Z; q\u0302)\u20162 + \u2016\u00b5\u0302(T,X)\u20162v \u00b7 \u2016 sup\nt,x |q\u0302(t, x)\u2212 q\u0302(\u2212l)(t, x)|\u20162r\n+ (\u2016Y \u20162v + \u2016q\u0302(T,X)\u20162v)\u2016 sup t,x |\u00b5\u0302(t, x)\u2212 \u00b5\u0302(\u2212l)(t, x)|\u20162r.\nSince mb is a linear functional, there exists Lm > 0 such that\n\u2016mb(Z; q\u0302(\u2212l))\u2212mb(Z; q\u0302)\u20162 \u2264 Lm \u00b7 \u2016 sup t,x |q\u0302(t, x)\u2212 q\u0302(\u2212l)(t, x)|\u20162r.\nHence, we have\n\u2016\u03bd(Z; g\u0302)\u2212 \u03bd(Z; g\u0302(\u2212l))\u20162 \u2264 \u2016 sup\nt,x |g\u0302(t, x)\u2212 g\u0302(\u2212l)(t, x)|\u20162r \u00b7 {Lm + \u2016\u00b5\u0302(T,X)\u20162v + \u2016Y \u20162v + \u2016q\u0302(T,X)\u20162v} .\nAnalogously, by replacing Z with Zl, we can show that\n\u2016\u03bd(Zl; g\u0302)\u2212 \u03bd(Zl; g\u0302(\u2212l))\u20162\n\u2264 \u2016 sup t,x |g\u0302(t, x)\u2212 g\u0302(\u2212l)(t, x)|\u20162r \u00b7\n{ L\u0303m + \u2016\u00b5\u0302(T,X)\u20162v + \u2016Y \u20162v + \u2016q\u0302(Tl, Xl)\u20162v }\nfor some constant L\u0303m > 0.\nTherefore, subject to\nLm, L\u0303m, \u2016\u00b5\u0302(T,X)\u20162v, \u2016Y \u20162v, \u2016q\u0302(T,X)\u20162v, \u2016q\u0302(Tl, Xl)\u20162v\nbeing finite for all i \u2208 [p] then L2r-continuity conditions also hold for function \u03bd.\nMoreover, we have\n\u2016\u03bd(Z; g)\u2212 \u03bd(Z; g\u2032)\u20162 \u2264 \u2016mb(Z; q\u2032)\u2212mb(Z; q)\u20162 + \u2016\u00b5\u2032(T,X)(q(T,X)\u2212 q\u2032(T,X))\u20162 + \u2016(Y \u2212 q(T,X))(\u00b5(T,X)\u2212 \u00b5\u2032(T,X))\u20162 \u2264 \u2016g \u2212 g\u2032\u20162 \u00b7 {Lb + \u2016\u00b5\u2032(T,X)\u2016\u221e + \u2016Y \u2016\u221e + \u2016q(T,X)\u2016\u221e}\nwhere since mb is a linear functional, there exists Lb > 0 such that\n\u2016mb(Z; q\u2032)\u2212mb(Z; q)\u20162 \u2264 Lb \u00b7 \u2016q(t, x)\u2212 q\u2032(t, x)\u20162.\nProvided that Lb, \u2016\u00b5\u2032(T,X)\u2016\u221e, \u2016Y \u2016\u221e, \u2016q(T,X)\u2016\u221e\nare finite, mean-squared-continuity conditions hold for function \u03bd with q = 2."
        },
        {
            "heading": "G Extension to Nonlinear Moments",
            "text": "In this section, we extend our results to the case where the moment function m(Z; \u03b8, g) is not necessarily linear in the target parameter \u03b8. For simplicity, we assume that the nuisance estimator g\u0302 is symmetric in each of the training data points Z1, . . . , Zn. Moreover, we will denote with Z a fresh random draw from the distribution.\nWe introduce some notation. We denote with \u2016 \u00b7 \u20162,2 the norm of a random vector defined as: \u2016Z\u20162,2 = \u221a E [ \u2211 i Z 2 i ], which can also be thought as taking the L2 norm of each coordinate and and then taking the `2 vector norm of this vector, or equivalently taking the L2 norm of the random variable defined as the `2 norm of the random vector. For clarity, for any random vector Z we will denote with \u2016Z\u2016v,2 the random variable that corresponds to the `2 vector norm of the random vector, i.e. \u2016Z\u2016v,2 = \u221a\u2211 i Z 2 i . For any random variable V , denote with \u2016V \u20162 its `2 norm \u221a E[V 2]. Note that for any random vector Z, we have \u2016Z\u20162,2 = \u2016\u2016Z\u2016v,2\u20162.\nFirstly, we establish a consistency lemma for \u03b8\u0302. We note that in the linear moment case, such a separate proof of consistency was not required and a single step proof of asymptotic normality was feasible due to linearity. In the non-linear case, as is typical for moment based estimators, we first need to show that the estimate will eventually lie in a small ball around \u03b80, and then argue normality. This is what the consistency lemma achieves.\nLemma 10 (Consistency). Assume that\n1. The parameter space \u0398 \u2282 Rp for target parameter \u03b8 is compact.\n2. \u03b80 is the unique solution of \u03b8 to the equation M(\u03b8, g0) = 0.\n3. The moment function m(z; \u03b8, g) is uniformly continuous in \u03b8 over all \u0398 and a suficciently small `2 ball B2(g0) \u2286 G around g0. That is, \u2200 > 0, \u2203\u03b4 > 0 such that for any \u03b8\u03031, \u03b8\u03032 \u2208 \u0398 with \u2016\u03b8\u03031 \u2212 \u03b8\u03032\u20162 < \u03b4, \u2200g \u2208 B2(g0), \u2200z, we have\n\u2016m(z; \u03b8\u03031, g)\u2212m(z; \u03b8\u03032, g)\u20162 < .\n4. The moment function m(Z; \u03b8, g) is mean-squared-continuous in g, uniformly in \u03b8, i.e. \u2203L > 0 and q > 0 such that:\nmax \u03b8\u2208\u0398 \u2016m(Z; \u03b8, g1)\u2212m(Z; \u03b8, g2)\u20162,2 \u2264 L \u00b7 \u2016g1(Z)\u2212 g2(Z)\u2016q2,2.\n5. Estimator g\u0302 of the nuisance function is consistent: as n\u2192\u221e\n\u2016g\u0302(Z)\u2212 g0(Z)\u20162,2 = o(1).\n6. The moment function m and estimator g\u0302 satisfies the following o(1) leave-one-out stability condition:\nmax \u03b8\u2208\u0398 \u2016m(Z1; \u03b8, g\u0302)\u2212m(Z1; \u03b8, g\u0302(\u22121))\u20162,2 = o(1)\nas n\u2192\u221e.\nThen any estimator \u03b8\u0302 that satisfies that Mn(\u03b8\u0302, g\u0302) = op(1), also satisfies that \u03b8\u0302 p\u2192 \u03b80.\nProof. Fix any > 0. Since m(z; \u03b8, g) is uniformly continuous in \u03b8 \u2208 \u0398 for a sufficiently small `2-ball B2(g0) around g0, we have that, \u2203\u03b4 > 0 such that for any \u03b8\u03031, \u03b8\u03032 \u2208 \u0398 with \u2016\u03b8\u03031 \u2212 \u03b8\u03032\u20162 < \u03b4, \u2200g \u2208 B2(g0), \u2200z, we have \u2016m(z; \u03b8\u03031, g)\u2212m(z; \u03b8\u03032, g)\u20162 < /6. Then\n\u2016M(\u03b8\u03031, g)\u2212M(\u03b8\u03032, g)\u20162 \u2264 E [ \u2016m(Z; \u03b8\u03031, g)\u2212m(Z; \u03b8\u03032, g)\u2016v,2 ] \u2264 /6.\n(that is, M(\u00b7, g0) is uniformly continuous) and\n\u2016Mn(\u03b8\u03031, g)\u2212Mn(\u03b8\u03032, g)\u2016v,2 \u2264 1\nn n\u2211 i=1 \u2225\u2225\u2225m(Zi; \u03b8\u03031, g)\u2212m(Zi; \u03b8\u03032, g)\u2225\u2225\u2225 v,2 \u2264 /6.\nSince the parameter space \u0398 is compact, there exist \u03b8j , j = 1, . . . , J such that\n\u0398 \u2282 \u222aJj=1B(\u03b8j , \u03b4).\nBy Law of Large Numbers, we have \u2200j, as n\u2192\u221e\nMn(\u03b8j , g0)\u2212M(\u03b8j , g0) p\u2192 0.\nHence, \u2200\u03b7 > 0, for every j there exists nj such that \u2200n > nj P ( \u2016Mn(\u03b8j , g0)\u2212M(\u03b8j , g0)\u2016v,2 >\n3\n) < \u03b7\n3J .\nThen \u2200n > maxj nj , we have P (\nmax j \u2016Mn(\u03b8j , g0)\u2212M(\u03b8j , g0)\u2016v,2 > 3\n) < \u03b7\n3 .\nMoreover, for any j \u2208 J , we have that:\n\u2016Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0)\u20162,2 \u2264 max\n\u03b8 \u2016Mn(\u03b8, g\u0302)\u2212Mn(\u03b8, g0)\u20162,2\n= max \u03b8 \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 {m(Zi; \u03b8, g\u0302)\u2212m(Zi; \u03b8, g0)} \u2225\u2225\u2225\u2225\u2225 2,2\n\u2264 1 n n\u2211 i=1 max \u03b8 \u2016m(Zi; \u03b8, g\u0302)\u2212m(Zi; \u03b8, g0)\u20162,2 (triangle inequality)\n= max \u03b8 \u2016m(Z1; \u03b8, g\u0302)\u2212m(Z1; \u03b8, g0)\u20162,2 (symmetry of estimator)\n\u2264 max \u03b8 \u2225\u2225\u2225m(Z1; \u03b8, g\u0302)\u2212m(Z1; \u03b8, g\u0302(\u22121))\u2225\u2225\u2225 2,2 + max \u03b8 \u2225\u2225\u2225m(Z1; \u03b8, g\u0302(\u22121))\u2212m(Z1; \u03b8, g0)\u2225\u2225\u2225 2,2\n\u2264 max \u03b8 \u2225\u2225\u2225m(Z1; \u03b8, g\u0302)\u2212m(Z1; \u03b8, g\u0302(\u22121))\u2225\u2225\u2225 2,2 + max \u03b8 \u2225\u2225\u2225m(Z; \u03b8, g\u0302(\u22121))\u2212m(Z; \u03b8, g0)\u2225\u2225\u2225 2,2\n\u2264 max \u03b8 \u2225\u2225\u2225m(Z1; \u03b8, g\u0302)\u2212m(Z1; \u03b8, g\u0302(\u22121))\u2225\u2225\u2225 2,2 + L \u00b7 \u2225\u2225\u2225g\u0302(\u22121)(Z)\u2212 g0(Z)\u2225\u2225\u2225q 2,2\n= o(1).\nThus we have that Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0) = op(1). Which means that \u2200\u03b7 > 0, there exists nj such that for every n > nj :\nP ( \u2016Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0)\u2016v,2 >\n3\n) < \u03b7\n3J .\nThen \u2200n > maxj\u2208J nj : P (\nmax j\u2208[J] \u2016Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0)\u2016v,2 > 3\n) < \u03b7\n3 .\nNow \u2200\u03b8 \u2208 \u0398, since \u0398 \u2282 \u222aJj=1B(\u03b8j , \u03b4), there exists k \u2208 {1, . . . , J} such that \u2016\u03b8\u2212 \u03b8k\u20162 < \u03b4. Then for n sufficiently large, such that g\u0302 \u2208 B2(g0):\n\u2016Mn(\u03b8, g\u0302)\u2212M(\u03b8, g0)\u2016v,2 \u2264 \u2016Mn(\u03b8k, g\u0302)\u2212M(\u03b8k, g0)\u2016v,2 + \u2016Mn(\u03b8, g\u0302)\u2212Mn(\u03b8k, g\u0302)\u2016v,2 + \u2016M(\u03b8, g0)\u2212M(\u03b8k, g0)\u2016v,2 \u2264 max\nj \u2016Mn(\u03b8j , g\u0302)\u2212M(\u03b8j , g0)\u2016v,2 + 2 /6.\nHence, we obtain P (\nmax \u03b8 \u2016Mn(\u03b8, g\u0302)\u2212M(\u03b8, g0)\u2016v,2 >\n) \u2264 P ( max j \u2016Mn(\u03b8j , g\u0302)\u2212M(\u03b8j , g0)\u2016v,2 > 2 3 ) .\nMoreover, note that by the triangle inequality:\nmax j \u2016Mn(\u03b8j , g\u0302)\u2212M(\u03b8j , g0)\u2016v,2\n\u2264 max j \u2016Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0)\u2016v,2 + max j \u2016Mn(\u03b8j , g0)\u2212M(\u03b8j , g0)\u2016v,2.\nThus: P (\nmax j \u2016Mn(\u03b8j , g\u0302)\u2212M(\u03b8j , g0)\u2016v,2 >\n2\n3 ) \u2264 P ( max j \u2016Mn(\u03b8j , g\u0302)\u2212Mn(\u03b8j , g0)\u2016v,2 > 3 ) + P ( max j \u2016Mn(\u03b8j , g0)\u2212M(\u03b8j , g0)\u2016v,2 > 3\n) \u2264 2\u03b7\n3 \u2264 \u03b7.\nAnd we conclude that:\nP (\nmax \u03b8 \u2016Mn(\u03b8, g\u0302)\u2212M(\u03b8, g0)\u2016v,2 >\n) \u2264 \u03b7.\nThis shows that max \u03b8\u2208\u0398 \u2016Mn(\u03b8, g\u0302)\u2212M(\u03b8, g0)\u2016v,2 p\u2192 0 as n\u2192\u221e.\nIn particular, this implies that\n\u2016Mn(\u03b8\u0302, g\u0302)\u2212M(\u03b8\u0302, g0)\u2016v,2 p\u2192 0\nas n\u2192\u221e.\nHence, by triangle inequality and the fact that Mn(\u03b8\u0302, g\u0302) = op(1), we obtain\n\u2016M(\u03b8\u0302, g0)\u2016v,2 \u2264 \u2016Mn(\u03b8\u0302, g\u0302)\u2016v,2 + \u2016Mn(\u03b8\u0302, g\u0302)\u2212M(\u03b8\u0302, g0)\u2016v,2 = op(1).\nHence, M(\u03b8\u0302, g0) = op(1).\nIt remains to show that \u03b8\u0302\np\u2192 \u03b80 as n\u2192\u221e.\nTo achieve this, again fix any > 0. Then since \u0398 is compact, B(\u03b80, )c is also compact as a closed subset of \u0398. By continuity of \u03b8 7\u2192 \u2016M(\u03b8, g0)\u20162 and the fact that \u03b80 is the unique solution to M(\u03b80, g0) = 0, we must have that \u2016M(\u03b8, g0)\u2016v,2 is bounded away from zero on B(\u03b80, )c. That is, \u2203\u03b7 > 0 such that for any \u03b8 with \u2016\u03b8 \u2212 \u03b80\u2016v,2 \u2265 ,\n\u2016M(\u03b8, g0)\u2016v,2 > \u03b7.\nThen since M(\u03b8\u0302, g0) = op(1), there exists N \u2208 N such that \u2200n > N P ( \u2016M(\u03b8\u0302, g0)\u2016v,2 > \u03b7 ) < .\nThen \u2200n > N P ( \u2016\u03b8\u0302 \u2212 \u03b80\u2016v,2 \u2265 ) \u2264 P ( \u2016M(\u03b8\u0302, g0)\u2016v,2 > \u03b7 ) < .\nThis establishes consistency of \u03b8\u0302.\nNow we extend Theorem 1 to nonlinear moments.\nTheorem 11. Let A(\u03b8, g) := \u2202\u03b8M(\u03b8, g) denote the Jacobian of the moment vector, with respect to \u03b8 and Hi(\u03b8, g) := \u22022\u03b8Mi(\u03b8, g) denote the Hessian of the i-th moment coordinate. Suppose that the moment m is twice differentiable with a(z; \u03b8, g) := \u2202\u03b8m(z; \u03b8, g) and hi(z; \u03b8, g) := \u22022\u03b8mi(z; \u03b8, g). Let An(\u03b8, g) := 1 n \u2211n i=1 a(Zi; \u03b8, g) and Hi,n := 1 n \u2211n i=1 h(Zi; \u03b8, g) denote the empirical counterparts of A,Hi.\nSuppose that the nuisance estimate g\u0302 \u2208 G satisfies:\n\u2016g\u0302 \u2212 g0\u201622 , EX [ \u2016g\u0302(X)\u2212 g0(X)\u201622 ] = op ( n\u22121/2 ) . (Consistency Rate)\nSuppose that the moment satisfies the Neyman orthogonality condition: for all g \u2208 G\nDgM(\u03b80, g0)[g \u2212 g0] , \u2202\n\u2202t M(\u03b80, g0 + t (g \u2212 g0))\n\u2223\u2223 t=0 = 0 (Neyman Orthogonality)\nand a second-order smoothness condition: for all g \u2208 G\nDggM(\u03b80, g0)[g \u2212 g0] , \u22022\n\u2202t2 M(\u03b80, g0 + t (g \u2212 g0))\n\u2223\u2223 t=0 = O ( \u2016g \u2212 g0\u201622 ) (Smoothness)\nAssume that A(\u03b80, g0)\u22121 exists and that for any g, g\u2032 \u2208 G:\n\u2016A(\u03b80, g)\u2212A(\u03b80, g\u2032)\u2016op = O (\u2016g \u2212 g\u2032\u20162) . (Lipschitz)\nSuppose that the moment m and estimator g\u0302 satisfy the stochastic equicontinuity conditions:\n\u2016A(\u03b80, g\u0302)\u2212A(\u03b80, g0)\u2212 (An(\u03b80, g\u0302)\u2212An(\u03b80, g0))\u2016op = op(1)\u221a n \u2016M(\u03b80, g\u0302)\u2212M(\u03b80, g0)\u2212 (Mn(\u03b80, g\u0302)\u2212Mn(\u03b80, g0))\u20162,2 = op(1)\n(NonLin. Stoc. Equi.)\nSuppose that the conditions in Lemma 10 are true. Moreover, assume that for any i, j \u2208 [p] \u00d7 [p], the random variable ai,j(Z; \u03b80, g0) has bounded variance and that \u2016\u03b80\u20162 = O(1). Suppose that \u2203 open neighborhood W of g0 such that\nsup \u03b8\u2208\u0398,g\u2208W,i\u2208[p] \u2016Hi(\u03b8, g)\u2016op , \u2016Hi,n(\u03b8, g)\u2016op <\u221e (Bounded Hessian)\nLet \u03b8\u0302 denote any approximate solution to the plug-in empirical moment equation that satisfies Mn(\u03b8\u0302, g\u0302) = op(n \u22121/2). Then \u03b8\u0302 is asymptotically normal:\n\u221a n ( \u03b8\u0302 \u2212 \u03b80 ) n\u2192\u221e,d\u2212\u2212\u2212\u2212\u2212\u2192 N ( 0, A(\u03b80, g0) \u22121E [ m(Z; \u03b80, g0)m(Z; \u03b80, g0) >]A(\u03b80, g0)\u22121) .\nProof. Let Ai(\u03b8, g) denote the i-th column of the Jacobian matrix. By Taylor\u2019s expansion, we have \u2200g and i \u2208 [p]\nMi(\u03b8\u0302, g)\u2212Mi(\u03b80, g) = Ai(\u03b80, g)\u2032(\u03b8\u0302 \u2212 \u03b80) + (\u03b8\u0302 \u2212 \u03b80)\u2032Hi(\u03b8\u0303i, g)(\u03b8\u0302 \u2212 \u03b80)\nfor some \u03b8\u0303i between \u03b8\u0302 and \u03b80.\nBy Lemma 10, we know that \u2016\u03b8\u0302 \u2212 \u03b80\u20162 = op(1).\nFurther, by the bounded Hessian condition and consistency of \u03b8\u0302, we know that uniformly for g \u2208W\n(\u03b8\u0302 \u2212 \u03b80)\u2032Hi(\u03b8\u0303i, g)(\u03b8\u0302 \u2212 \u03b80) = Op ( \u2016\u03b8\u0302 \u2212 \u03b80\u201622 ) = op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nMoreover, for any g with \u2016g \u2212 g0\u20162 = op(1) we have\nA(\u03b80, g) \u00b7 (\u03b8\u0302 \u2212 \u03b80) = A(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) + (A(\u03b80, g)\u2212A(\u03b80, g0)) \u00b7 (\u03b8\u0302 \u2212 \u03b80)\n= A(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) +O(\u2016g \u2212 g0\u20162 \u2016\u03b8\u0302 \u2212 \u03b80\u20162)\n= A(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162),\nwhere the second to last equality uses the Lipschitz condition.\nBy consistency, we have \u2016g\u0302 \u2212 g0\u20162 = op(1) and that g\u0302 \u2208W for n large enough.\nHence, we obtain that for large enough n,\nA(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) = M(\u03b8\u0302, g\u0302)\u2212M(\u03b80, g\u0302) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162)\n= M(\u03b8\u0302, g\u0302)\u2212Mn(\u03b8\u0302, g\u0302) +M(\u03b80, g0)\u2212M(\u03b80, g\u0302) +Mn(\u03b8\u0302, g\u0302) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162)\n= M(\u03b8\u0302, g\u0302)\u2212Mn(\u03b8\u0302, g\u0302) +M(\u03b80, g0)\u2212M(\u03b80, g\u0302) + op(n\u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162),\nwhere the last line follows because by definition Mn(\u03b8\u0302, g\u0302) = op(n\u22121/2).\nBy Neyman orthogonality and the boundness condition on the second derivative of M with respect to g, for any g we have\nM(\u03b80, g0)\u2212M(\u03b80, g) = DgM(\u03b80, g0)[g0 \u2212 g] +O ( \u2016g \u2212 g0\u201622 ) = O ( \u2016g \u2212 g0\u201622 ) .\nPlugging in g = g\u0302, noting that \u2016g \u2212 g0\u201622 = op(n\u22121/2) we obtain\nM(\u03b80, g0)\u2212M(\u03b80, g\u0302) = op(n\u22121/2).\nLet Gn(\u03b8, g) := M(\u03b8, g)\u2212Mn(\u03b8, g). Thus we have that\nA(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) = Gn(\u03b8\u0302, g\u0302) + op(n\u22121/2 + \u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nWe decompose Gn(\u03b8\u0302, g\u0302) into the following sum:\nGn(\u03b8\u0302, g\u0302) = Gn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) + (Gn(\u03b80, g\u0302)\u2212Gn(\u03b80, g0)) +Gn(\u03b80, g0).\nBy Taylor\u2019s expansion, we have Gn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) = M(\u03b8\u0302, g\u0302)\u2212M(\u03b80, g\u0302)\u2212 ( Mn(\u03b8\u0302, g\u0302)\u2212Mn(\u03b80, g\u0302) ) = (A(\u03b80, g\u0302)\u2212An(\u03b80, g\u0302))\u2032 (\u03b8\u0302 \u2212 \u03b80) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162)\nNow\n\u2016A(\u03b80, g\u0302)\u2212An(\u03b80, g\u0302)\u2016op \u2264 \u2016A(\u03b80, g0)\u2212An(\u03b80, g0)\u2016op + \u2016A(\u03b80, g\u0302)\u2212A(\u03b80, g0)\u2212 (An(\u03b80, g\u0302)\u2212An(\u03b80, g0))\u2016op = \u2016A(\u03b80, g0)\u2212An(\u03b80, g0)\u2016op + op(1),\nwhere the last equality follows from the stochastic equicontinuity condition on the Jacobian. Since A(\u03b80, g0)\u2212An(\u03b80, g0) is a mean zero empirical process with \u2202\u03b8m(Z; \u03b80, g0) having bounded variance, we have \u2016A(\u03b80, g0)\u2212An(\u03b80, g0)\u2016op = op(1). Hence,\nGn(\u03b8\u0302, g\u0302)\u2212Gn(\u03b80, g\u0302) = op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162).\nMoreover,\nGn(\u03b80, g\u0302)\u2212Gn(\u03b80, g0) = M(\u03b80, g\u0302)\u2212Mn(\u03b80, g\u0302)\u2212 (M(\u03b80, g0)\u2212Mn(\u03b80, g0)) = op(n\u22121/2)\nby stochastic equicontinuity.\nIn summary, we have Gn(\u03b8\u0302, g\u0302) = Gn(\u03b80, g0) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162 + n\u22121/2),\nand thus A(\u03b80, g0) \u00b7 (\u03b8\u0302 \u2212 \u03b80) = Gn(\u03b80, g0) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162 + n\u22121/2).\nThat is, we have\n(\u03b8\u0302 \u2212 \u03b80) = A(\u03b80, g0)\u22121Gn(\u03b80, g0) + op(\u2016\u03b8\u0302 \u2212 \u03b80\u20162 + n\u22121/2).\nSince Gn(\u03b80, g0) is a mean-zero empirical process, we have that \u2016Gn(\u03b80, g0)\u20162 = Op(n\u22121/2).\nBy consistency of \u03b8\u0302, above implies that\n\u03b8\u0302 \u2212 \u03b80 = Op(n\u22121/2).\nThus we get (\u03b8\u0302 \u2212 \u03b80) = A(\u03b80, g0)\u22121Gn(\u03b80, g0) + op(n\u22121/2).\nBy Slutsky\u2019s Theorem, we conclude that \u221a n(\u03b8\u0302 \u2212 \u03b80) d\u2192 N ( 0, A(\u03b80, g0) \u22121E[m(Z; \u03b80, g0)m(Z; \u03b80, g0)\u2032]A(\u03b80, g0)\u22121 ) .\nWe can also directly extend Lemma 2 to general nonlinear moments.\nLemma 12 (Non-Linear Main Lemma). If the estimation algorithm satisfies the stability conditions: for all i, j \u2208 [p]\nmax l\u2208[n] \u2225\u2225\u2225ai,j(Zl; \u03b80, g\u0302)\u2212 ai,j(Zl; \u03b80, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(n\u22121/2)\nmax l\u2208[n] \u2225\u2225\u2225ai,j(Z; \u03b80, g\u0302)\u2212 ai,j(Z; \u03b80, g\u0302(\u2212l))\u2225\u2225\u2225 2 = o(n\u22121/2)\nmax l\u2208[n] \u2225\u2225\u2225mi(Zl; \u03b80, g\u0302)\u2212mi(Zl; \u03b80, g\u0302(\u2212l))\u2225\u2225\u2225 1 = o(n\u22121/2)\nmax l\u2208[n] \u2225\u2225\u2225mi(Z; \u03b80, g\u0302)\u2212mi(Z; \u03b80, g(\u2212l))\u2225\u2225\u2225 2 = o(n\u22121/2)\nand the moment satisfies the mean-squared-continuity condition:\n\u2200g, g\u2032 : E[(ai,j(Z; \u03b80, g)\u2212 ai,j(Z; \u03b80, g\u2032))2] \u2264 L\u2016g \u2212 g\u2032\u2016q2 \u2200g, g\u2032 : E[(mi(Z; \u03b80, g)\u2212mi(Z; \u03b80, g\u2032))2] \u2264 L\u2016g \u2212 g\u2032\u2016q2\nfor some 0 < q <\u221e and some L > 0, then the Condition (NonLin. Stoc. Equi.) is satisfied.\nProof. The proof follows by replacing all functions a(z, g), \u03bd(z, g) in the proof of Lemma 2 correspondingly with the functions a(z; \u03b80, g) and m(z; \u03b80, g).\nApplication to bagging estimators. We finally note that if the moment satisfies Lipschitz conditions of the form:\nE [( ai,j(Zl; \u03b80, g\u0302)\u2212 ai,j(Zl; \u03b80, g\u0302(\u2212l)) )2] \u2264 L \u00b7 E [ sup z \u2016g\u0302(z)\u2212 g\u0302(\u2212l)(z)\u20162r2 ]1/r max \u03b8\u2208\u0398 E [( mi(Zl; \u03b8, g\u0302)\u2212mi(Zl; \u03b8, g\u0302(\u2212l)) )2] \u2264 L \u00b7 E [ sup z \u2016g\u0302(z)\u2212 g\u0302(\u2212l)(z)\u20162r2\n]1/r Then Theorem 5 can be applied to upper bound the right hand side of these inequalities by op(n\u22121/2) for bagging estimators. This would then imply the stability conditions invoked in both the consistency and the normality theorem. Thus the main application for bagging estimators carries over to non-linear moments."
        }
    ],
    "title": "Debiased Machine Learning without Sample-Splitting for Stable Estimators",
    "year": 2022
}