{
    "abstractText": "We apply a number of atomic decomposition schemes across the standard QM7 dataset\u2014a small model set of organic molecules at equilibrium geometry\u2014to inspect the possible emergence of trends among contributions to atomization energies from distinct elements embedded within molecules. Specifically, a recent decomposition scheme of ours based on spatially localized molecular orbitals is compared to alternatives that instead partition molecular energies on account of which nuclei individual atomic orbitals are centred on. We find these partitioning schemes to expose the composition of chemical compound space in very dissimilar ways in terms of the grouping, binning, and heterogeneity of discrete atomic contributions, e.g., those associated with hydrogens bonded to different heavy atoms. Furthermore, unphysical dependencies on the oneelectron basis set are found for some, but not all of these schemes. The relevance and importance of these compositional factors for training tailored neural network models based on atomic energies are next assessed. We identify both limitations and possible advantages with respect to contemporary machine learning models and discuss the design of potential counterparts based on atoms and the intrinsic energies of these as the principal decomposition units. 1 ar X iv :2 21 2. 09 48 9v 3 [ ph ys ic s. ch em -p h] 1 8 A pr 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Frederik \u00d8. Kjeldal"
        },
        {
            "affiliations": [],
            "name": "Janus J. Eriksen"
        }
    ],
    "id": "SP:5e686c6cc4e96e9e91187f54d7fb48081cb6a497",
    "references": [
        {
            "authors": [
                "W. Kohn",
                "L.J. Sham"
            ],
            "title": "Self-Consistent Equations Including Exchange and Correlation Effects",
            "venue": "Phys. Rev. 1965,",
            "year": 1965
        },
        {
            "authors": [
                "R.G. Parr",
                "W. Yang"
            ],
            "title": "Density-Functional Theory of Atoms and Molecules",
            "year": 1994
        },
        {
            "authors": [
                "J.J. Eriksen"
            ],
            "title": "Mean-Field Density Matrix Decompositions",
            "venue": "J. Chem. Phys. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J.J. Eriksen"
            ],
            "title": "Decomposed Mean-Field Simulations of Local Properties in Condensed Phases",
            "venue": "J. Phys. Chem. Lett. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J.J. Eriksen"
            ],
            "title": "Electronic Excitations Through the Prism of Mean-Field Decomposition Techniques",
            "venue": "J. Chem. Phys. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Chmiela",
                "H.E. Sauceda",
                "K.-R. M\u00fcller",
                "A. Tkatchenko"
            ],
            "title": "Towards Exact Molecular Dynamics Simulations with Machine-Learned",
            "venue": "Force Fields. Nat. Commun. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H.E. Sauceda",
                "L.E. G\u00e1lvez-Gonz\u00e1lez",
                "S. Chmiela",
                "L.O. Paz-Borb\u00f3n",
                "K.-R. M\u00fcller",
                "A. Tkatchenko"
            ],
            "title": "BIGDML \u2013 Towards Accurate Quantum Machine Learning Force Fields for Materials",
            "venue": "Nat. Commun. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A.S. Christensen",
                "S.K. Sirumalla",
                "Z. Qiao",
                "M.B. O\u2019Connor",
                "D.G. Smith",
                "F. Ding",
                "P.J. Bygrave",
                "A. Anandkumar",
                "M. Welborn",
                "F.R. Manby",
                "T.F. Miller III"
            ],
            "title": "OrbNet Denali: A Machine Learning Potential for Biological and Organic Chemistry with Semi-Empirical Cost and DFT Accuracy",
            "venue": "J. Chem. Phys. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J. Behler",
                "M. Parrinello"
            ],
            "title": "Generalized Neural-Network Representation of HighDimensional Potential-Energy Surfaces",
            "venue": "Phys. Rev. Lett. 2007,",
            "year": 2007
        },
        {
            "authors": [
                "S. Grimme",
                "J. Antony",
                "S. Ehrlich",
                "H. Krieg"
            ],
            "title": "A Consistent and Accurate Ab Initio Parametrization of Density Functional Dispersion Correction (DFT-D) for the 94 Elements H-Pu",
            "venue": "J. Chem. Phys. 2010,",
            "year": 2010
        },
        {
            "authors": [
                "S. Grimme",
                "A. Hansen",
                "J.G. Brandenburg",
                "C. Bannwarth"
            ],
            "title": "Dispersion-Corrected Mean-Field Electronic Structure Methods",
            "venue": "Chem. Rev. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "B. Parsaeifard",
                "D.S. De",
                "A.S. Christensen",
                "F.A. Faber",
                "E. Kocer",
                "S. De",
                "J. Behler",
                "A. von Lilienfeld",
                "S. Goedecker"
            ],
            "title": "An Assessment of the Structural Resolution of Various Fingerprints Commonly Used in Machine Learning",
            "venue": "Mach. Learn.: Sci. Technol. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J. Behler"
            ],
            "title": "Atom-Centered Symmetry Functions for Constructing High-Dimensional Neural Networks Potentials",
            "venue": "J. Chem. Phys. 2011,",
            "year": 2011
        },
        {
            "authors": [
                "K.T. Sch\u00fctt",
                "H.E. Sauceda",
                "P.-J. Kindermans",
                "A. Tkatchenko",
                "K.-R. M\u00fcller"
            ],
            "title": "SchNet \u2013 A Deep Learning Architecture for Molecules and Materials",
            "venue": "J. Chem. Phys. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "A.S. Christensen",
                "O.A. Von Lilienfeld"
            ],
            "title": "On the Role of Gradients for Machine Learning of Molecular Energies and Forces",
            "venue": "Mach. Learn.: Sci. Technol. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Huang",
                "J. Kang",
                "W.A. Goddard III",
                "L.-W. Wang"
            ],
            "title": "Density Functional Theory Based Neural Network Force Fields from Energy Decompositions",
            "venue": "Phys. Rev. B 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Hu",
                "M. Weng",
                "X. Chen",
                "S. Li",
                "F. Pan",
                "L.-W. Wang"
            ],
            "title": "Neural Network Force Fields for Metal Growth Based on Energy Decompositions",
            "venue": "J. Phys. Chem. Lett. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "X. Song",
                "C. Deng"
            ],
            "title": "Atomic Energy in Grain Boundaries Studied by Machine Learning",
            "venue": "Phys. Rev. Mater. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "L.C. Blum",
                "J.-L. Reymond"
            ],
            "title": "Million Druglike Small Molecules for Virtual Screening in the Chemical Universe Database GDB-13",
            "venue": "J. Am. Chem. Soc",
            "year": 2009
        },
        {
            "authors": [
                "M. Rupp",
                "A. Tkatchenko",
                "K.-R. M\u00fcller",
                "O.A. Von Lilienfeld"
            ],
            "title": "Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning",
            "venue": "Phys. Rev. Lett. 2012,",
            "year": 2012
        },
        {
            "authors": [
                "R.S. Mulliken"
            ],
            "title": "Electronic Population Analysis on LCAO-MO",
            "venue": "Molecular Wave Functions. I. J. Chem. Phys. 1955,",
            "year": 1955
        },
        {
            "authors": [
                "H. Nakai"
            ],
            "title": "Energy Density Analysis with Kohn-Sham Orbitals",
            "venue": "Chem. Phys. Lett. 2002,",
            "year": 2002
        },
        {
            "authors": [
                "Y. Kikuchi",
                "Y. Imamura",
                "H. Nakai"
            ],
            "title": "One-Body Energy Decomposition Schemes Revisited: Assessment of Mulliken-, Grid-, and Conventional Energy Density Analyses",
            "venue": "Int. J. Quantum Chem",
            "year": 2009
        },
        {
            "authors": [
                "T. Baba",
                "M. Takeuchi",
                "H. Nakai"
            ],
            "title": "Natural Atomic Orbital Based Energy Density Analysis: Implementation and Applications",
            "venue": "Chem. Phys. Lett",
            "year": 2006
        },
        {
            "authors": [
                "Y. Imamura",
                "A. Takahashi",
                "H. Nakai"
            ],
            "title": "Grid-Based Energy Density Analysis: Implementation and Assessment",
            "venue": "J. Chem. Phys. 2007,",
            "year": 2007
        },
        {
            "authors": [
                "Q. Sun",
                "T.C. Berkelbach",
                "N.S. Blunt",
                "G.H. Booth",
                "S. Guo",
                "Z. Li",
                "J. Liu",
                "J.D. McClain",
                "E.R. Sayfutyarova",
                "S. Sharma",
                "S. Wouters",
                "G.K.-L. Chan"
            ],
            "title": "PySCF: The Python-Based Simulations of Chemistry Framework",
            "venue": "WIREs Comput. Mol. Sci. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "E. T",
                "C. Sun",
                "S.-N. Sun",
                "S. Upadhyay",
                "L.K. Wagner",
                "X. Wang",
                "A. White",
                "J.D. Whitfield",
                "M.J. Williamson",
                "S. Wouters",
                "J. Yang",
                "J.M. Yu",
                "T. Zhu",
                "T.C. Berkelbach",
                "S. Sharma",
                "A.Y. Sokolov",
                "G.K.-L. Chan"
            ],
            "title": "Recent Developments in the PySCF Program Package",
            "venue": "J. Chem. Phys. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J.P. Perdew",
                "K. Burke",
                "M. Ernzerhof"
            ],
            "title": "Generalized Gradient Approximation Made Simple",
            "venue": "Phys. Rev. Lett. 1996,",
            "year": 1996
        },
        {
            "authors": [
                "A.D. Becke"
            ],
            "title": "Density-Functional Thermochemistry. III. The Role of Exact Exchange",
            "venue": "J. Chem. Phys. 1993,",
            "year": 1993
        },
        {
            "authors": [
                "P.J. Stephens",
                "F.J. Devlin",
                "C.F. Chabalowski",
                "M.J. Frisch"
            ],
            "title": "Ab Initio Calculation of Vibrational Absorption and Circular Dichroism Spectra",
            "venue": "Using Density Functional Force Fields. J. Phys. Chem",
            "year": 1994
        },
        {
            "authors": [
                "Y. Zhao",
                "D.G. Truhlar"
            ],
            "title": "The M06 Suite of Density Functionals for Main Group Thermochemistry, Thermochemical Kinetics, Noncovalent Interactions, Excited States, and Transition Elements: Two New Functionals and Systematic Testing of Four M06-Class Functionals and 12 Other Functionals",
            "year": 2008
        },
        {
            "authors": [
                "F. Jensen"
            ],
            "title": "Polarization Consistent Basis Sets: Principles",
            "venue": "J. Chem. Phys. 2001,",
            "year": 2001
        },
        {
            "authors": [
                "J.S. Smith",
                "O. Isayev",
                "A.E. Roitberg"
            ],
            "title": "ANI-1: An Extensible Neural Network Potential with DFT Accuracy at Force Field Computational Cost",
            "venue": "Chem. Sci. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "X. Gao",
                "F. Ramezanghorbani",
                "O. Isayev",
                "J.S. Smith",
                "A.E. Roitberg"
            ],
            "title": "TorchANI: A Free and Open Source PyTorch-Based Deep Learning Implementation of the ANI Neural Network Potentials",
            "venue": "J. Chem. Inf. Model. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "O.T. Unke",
                "M. Meuwly"
            ],
            "title": "PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges",
            "venue": "J. Chem. Theory Comput. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Pipek",
                "P.G. Mezey"
            ],
            "title": "A Fast Intrinsic Localization Procedure Applicable for Ab Initio and Semiempirical Linear Combination of Atomic Orbital Wave Functions",
            "venue": "J. Chem. Phys. 1989,",
            "year": 1989
        },
        {
            "authors": [
                "G. Knizia"
            ],
            "title": "Intrinsic Atomic Orbitals: An Unbiased Bridge Between Quantum Theory and Chemical Concepts",
            "venue": "J. Chem. Theory Comput. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "B. Senjean",
                "S. Sen",
                "M. Repisky",
                "G. Knizia",
                "L. Visscher"
            ],
            "title": "Generalization of Intrinsic Orbitals to Kramers-Paired Quaternion Spinors, Molecular Fragments and Valence Virtual Spinors",
            "venue": "J. Chem. Theory Comput. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "R. Zubatyuk",
                "J.S. Smith",
                "J. Leszczynski",
                "O. Isayev"
            ],
            "title": "Accurate and Transferable Multitask Prediction of Chemical Properties with an Atoms-in-Molecules",
            "venue": "Neural Network. Sci. Adv. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "O.T. Unke",
                "M. Meuwly"
            ],
            "title": "A Reactive, Scalable, and Transferable Model for Molecular Energies from a Neural Network Approach Based on Local Information",
            "venue": "J. Chem. Phys. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "N. Bernstein",
                "B. Bhattarai",
                "G. Cs\u00e1nyi",
                "D.A. Drabold",
                "S.R. Elliott",
                "V.L. Deringer"
            ],
            "title": "Quantifying Chemical Structure and Machine-Learned Atomic Energies in Amorphous and Liquid Silicon",
            "year": 2019
        },
        {
            "authors": [
                "T. Schnake",
                "O. Eberle",
                "J. Lederer",
                "S. Nakajima",
                "K.T. Sch\u00fctt",
                "K.-R. M\u00fcller",
                "G. Montavon"
            ],
            "title": "Higher-Order Explanations of Graph Neural Networks via Relevant Walks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "M.S. J\u00f8rgensen",
                "J. Li",
                "B. Hammer"
            ],
            "title": "Atomic Energies from a Convolutional Neural Network",
            "venue": "J. Chem. Theory Comput. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "J.H. Jensen"
            ],
            "title": "A Graph-Based Genetic Algorithm and Generative Model/Monte Carlo Tree Search for the Exploration of Chemical Space",
            "venue": "Chem. Sci. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C. Coley",
                "W. Jin",
                "L. Rogers",
                "T.F. Jamison",
                "T.S. Jaakkola",
                "W.H. Green",
                "R. Barzilay",
                "K.F. Jensen"
            ],
            "title": "A Graph-Convolutional Neural Network Model for the Prediction of Chemical Reactivity",
            "venue": "Chem. Sci. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "K. Yang",
                "K. Swanson",
                "W. Jin",
                "C. Coley",
                "P. Eiden",
                "H. Gao",
                "A. Guzman-Perez",
                "T. Hopper",
                "B.P. Kelley",
                "M. Mathea",
                "A. Palmer",
                "V. Settels",
                "T.S. Jaakkola",
                "K.F. Jensen",
                "R. Barzilay"
            ],
            "title": "Analyzing Learned Molecular Representations for Property Prediction",
            "venue": "J. Chem. Inf. Model. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C.A. Grambow",
                "L. Pattanaik",
                "W.H. Green"
            ],
            "title": "Deep Learning of Activation Energies",
            "venue": "J. Phys. Chem. Lett. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "T. Stuyver",
                "C. Coley"
            ],
            "title": "Quantum Chemistry-Augmented Neural Networks for Reactivity Prediction: Performance, Generalizability, and Explainability",
            "venue": "J. Chem. Phys. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "K. Yao",
                "J.E. Herr",
                "S.N. Brown",
                "J. Parkhill"
            ],
            "title": "Intrinsic Bond Energies from a Bondsin-Molecules",
            "venue": "Neural Network. J. Phys. Chem. Lett. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "M. Wen",
                "S.M. Blau",
                "E.W.C. Spotte-Smith",
                "S. Dwaraknath",
                "K.A. Persson"
            ],
            "title": "BonDNet: A Graph Neural Network for the Prediction of Bond Dissociation Energies for Charged Molecules",
            "venue": "Chem. Sci. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "B. Huang",
                "O.A. von Lilienfeld"
            ],
            "title": "Communication: Understanding Molecular Representations in Machine Learning: The Role of Uniqueness and Target Similarity",
            "venue": "J. Chem. Phys. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "B. Huang",
                "O.A. von Lilienfeld"
            ],
            "title": "Quantum Machine Learning Using Atom-in-MoleculeBased Fragments",
            "venue": "Selected on the Fly. Nat. Chem. 2020,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "We apply a number of atomic decomposition schemes across the standard QM7 dataset\u2014a small model set of organic molecules at equilibrium geometry\u2014to inspect the possible emergence of trends among contributions to atomization energies from distinct elements embedded within molecules. Specifically, a recent decomposition scheme of ours based on spatially localized molecular orbitals is compared to alternatives that instead partition molecular energies on account of which nuclei individual atomic orbitals are centred on. We find these partitioning schemes to expose the composition of chemical compound space in very dissimilar ways in terms of the grouping, binning, and heterogeneity of discrete atomic contributions, e.g., those associated with hydrogens bonded to different heavy atoms. Furthermore, unphysical dependencies on the oneelectron basis set are found for some, but not all of these schemes. The relevance and importance of these compositional factors for training tailored neural network models based on atomic energies are next assessed. We identify both limitations and possible advantages with respect to contemporary machine learning models and discuss the design of potential counterparts based on atoms and the intrinsic energies of these as the principal decomposition units.\nar X\niv :2\n21 2.\n09 48\n9v 3\n[ ph\nys ic\ns. ch"
        },
        {
            "heading": "1 Introduction",
            "text": "In standard formulations of electronic-structure theory, e.g., Kohn-Sham density functional theory (KS-DFT),1\u20133 total properties are computed globally for chemical systems in their entirety on the basis of sets of nuclear coordinates and the finer details of the simulations, e.g., the one-electron basis set of atomic orbitals (AOs) and the exchange-correlation (xc) functional of choice. However, if one desires to decompose said simulations into local entities, for instance, amongst either the functional moieties, bonds, or atoms of a system at hand, this necessarily becomes a non-trivial task as there will be a strong element of ambiguity surrounding the infinitely many ways by which to partition a quantum-mechanical observable into the constituting parts of a quantum many-body system. Bar none, the most important principle any such partitioning scheme will need to satisfy is formal exactness in the decomposition of a chemical simulation (at a given level of theory), that is, the involved transformation from global to local properties must not be subject to any kind of loss. Other fundamental features include rapid and robust convergence of partitioned contributions upon an increase of the underlying AO basis set and a natural alignment of results with chemical intuition (with regards to electronegativities, molecular graphs, substituent effects, etc.).\nIn a recent study, we proposed a novel atomic partitioning scheme completely agnostic with respect to heuristics or supposed rules on the bond network of a chemical system.4 Instead, the canonical molecular orbitals (MOs) of a standard mean-field simulation first undergo a unitary transformation into a spatially localized representation, and the contributions to total first-order properties, e.g., energies or dipole moments, from individual MOs next get weighted amongst the atoms of the molecule in question. While at least two degrees of freedom exist following this protocol, a set of optimal choices for both the localized MO basis and its atomic populations has been firmly established by now, effectively eliminating any arbitrariness (vide infra). By avoiding explicit references to structural or bonding motifs, we have since shown how these decompositions can allow for probing local, atomic\nproperties within condensed (bulk-like) phases,5 but also the perturbations to a system\u2019s electronic structure that occur in optical transitions between different electronic states.6\nThe present study is concerned with the ability of decomposition schemes like our own to expose diversities among elements of the same type when these are embedded within different spatial environments across a small model subsection of chemical compound space. Besides comparing our own scheme to alternatives from the scientific literature on the basis of how systematic distributions of their results will be for different basis sets and density functional approximations (DFAs), we will also discuss the overall alignment of all results with chemical intuition. Finally, we are interested in assessing the accuracy by which atomic energies can be learned from tailored, custom neural network models trained on data from said partitioning schemes, but also the extent to which an augmentation of standard training sets by explicit atomic contributions will influence the prediction of total molecular energies.\nMachine learning (ML), particularly when based on neural network (NN) architectures, is fast becoming a key component of quantum chemistry, given how data-driven models promise to allow for the treatment of much larger problem sizes than standard simulations formulated around electronic wave functions or densities.7\u201311 Arguably the most successful of such architectures is the high-dimensional variant (HDNN) of Behler and Parrinello.12 In HDNNs, total properties of a molecule consisting of M atoms are obtained as a sum of intrinsic contributions associated with each of these. Either a single NN is used to predict all atomic contributions for a given system or separate NNs are implemented for each element. Predicted atomic properties are thus assumed to depend chiefly on local chemical environments, although corrections that account for long-range and dispersion effects may also be included.13,14 There are multiple benefits to such a decomposition into atomic contributions since the prediction of total, molecular properties will scale as O(M) and the evaluation can be trivially parallelized. Furthermore, it is, at least in principle, possible to train models on\nsmall molecules before applying them to more extended problems, thereby allowing for some degree of transferability across chemical compound space. However, the vast majority of all such existing models have been trained exclusively on global data, e.g., molecular energies, which correlate with molecular composition through exceedingly intricate scaling relations.\nTwo main classes of HDNN models have emerged over the years; those based on descriptors and those formulated around message passing, differing from one another in how local chemical environments are represented.15,16 Descriptor-based NNs operate in terms of fixed sets of such representations, encoded into so-called atomic environment vectors (AEVs) used as the input for a standard feed-forward NN.17 In contrast, message-passing models iteratively encode these local environments, whereby representations are granted the flexibility to automatically adapt to reference data by capturing more complex chemical interactions.18\nAs the production of quantum-chemical training data typically involves numerous, expensive simulations, it is beneficial to be able to extract the maximum amount of information from each of these, at the lowest possible overhead. To that end, the inclusion of nuclear forces for each atom as a training label allows for more fine-grained data in comparison to the case of training on total energies only. The extra cost of obtaining forces varies, but is generally of the same order of magnitude as the preceding energy calculation. The addition of gradient data has been demonstrated to improve upon the prediction of both forces and energies in applications to potential energy surfaces of single molecules, but when considering datasets whose entries vary in conformation and size, improvements in energy evaluations tend not to materialize.19 Also, the inclusion of forces is useful only whenever these do not vanish, i.e., for model applications to molecular structures distorted away from equilibrium.\nWe will here consider an alternative approach, namely, one in which atomic energies, rather than forces, are included as training labels, whereby existing datasets of total ener-\ngies get augmented by an additionalM data points for each entry (as opposed to thrice this number in the case of forces). Again, the overhead of performing a decomposition of total properties into their atomic constituents will depend on the exact scheme employed, but the cost may generally be assumed to be of the order of a single mean-field iteration. Such a strategy has previously been considered for solid-state applications, e.g., in the design of force fields20,21 and in the study of grain boundaries.22 In here, we will instead restrict our focus to chemical Hamiltonians only, seeking to evaluate the usefulness of training standard NNs on atomic, rather than solely total energies. For this purpose, we will decompose the standard QM7 dataset,23,24 which consists of 7165 constitutional and structural molecular isomers each containing up to 7 heavy atoms (C, N, O, and S), with the largest molecule built from a mere total of 23 atoms. Despite its small extent, the dataset encompasses a number of fundamentally distinct local atomic environments. Sulfur atoms are the most scarce, nitrogens and oxygens exist in an approximately equal amount, while carbons and hydrogens are the most numerous, the latter being the far most common element in the dataset.\nWe will begin our study by comparing how various decomposition schemes distribute atomic energies for the entire QM7 suite, with a focus on if and how partitioned results align with expectation. We will discuss energies in terms of polarization and accepted trends in tendencies of electrons to be shared unevenly between atoms, but also any possible variances under a change of basis set or DFA. Next, we will proceed to study how both the aggregate and diversity in the representation of each element from these decompositions will influence the rate by which atomic energies may be learned using different kinds of NNs. Finally, we will end by discussing how these results can impact the ability to predict total energies as a sum of atomic contributions, touching upon the extent to which such constraints on NNs to yield specific contributions may inflict penalties on molecular property predictions, but also what opportunities such new local models may offer going forward, e.g., in relation to the prediction of properties associated with independent atoms, bonds, or functional moieties."
        },
        {
            "heading": "2 Theory",
            "text": "As outlined in Ref. 4, a KS-DFT molecular energy may be partitioned amongst the atomic nuclei of a chemical system by recasting the energy functional into the following form\nE = M\u2211 K Eelec,K(D, \u03b4K) + Exc,K(\u03c1,%K) + Enuc,K . (1)\nIn Eq. 1, the nuclear and electronic contributions are defined as\nEnuc,K = ZK 2 M\u2211 K 6=L ZL |rK \u2212 rL|\n(2a)\nEelec,K = Tr[Tkin\u03b4K ] + 12(Tr[VKD] + Tr[Vnuc\u03b4K ]) + 1 2 \u2211 \u03c3 Tr[G\u03c3(D)\u03b4K,\u03c3] (2b)\nExc,K = Tr[ xc(\u03c1)%K ] . (2c)\nIn Eq. 2a, ZK and rK denote the nuclear charge and position of atom K, while the kinetic energy and nuclear attraction operators in Eq. 2b are denoted by Tkin and Vnuc, respectively, alongside the attractive potential associated with atom K, VK , and an effective Fock potential, G\u03c3 (\u03c3 = \u03b1, \u03b2 is an electronic spin index). In Eq. 2b, D denotes the full, spin-summed 1-electron reduced density matrix (1-RDM), while the objects that principally define this decomposition\u2014the atom-specific 1-RDMs, {\u03b4}\u2014are constructed as follows:\n\u03b4K = \u2211 \u03c3 \u03b4K,\u03c3 = \u2211 \u03c3 N\u03c3\u2211 i di,\u03c3p K i,\u03c3 . (3)\nIn turn, these are formulated via a set of 1-RDMs, di,\u03c3 = Ci,\u03c3CTi,\u03c3, unique to the individual occupied spin-\u03c3 MOs of the system, Ci,\u03c3, and a set of weights of all N\u03c3 MOs of \u03b1-/\u03b2-spin on a given atom K, {pK}. Our earlier investigations in Refs. 4\u20136 have convincingly indicated how the atomic weights used to assign {d} should ideally not be drawn from regular Mulliken population analyses,25 but rather recast into a basis of reduced dimension. The xc energy in Eq. 2c is expressed in terms of the computed energy density, xc, as derived from the\ntotal electronic density, \u03c1, and possibly its derivatives, which are all quantities that may be trivially defined in an atom-specific manner, {%}, by proceeding through the 1-RDMs, {\u03b4}.\nIn most other alternative decomposition schemes, particularly those that are similarly exact (i.e., lossless), properties are instead partitioned amongst the constituent atoms on the basis of the full 1-RDM. In the arguably most intuitive example of such a decomposition, the energy density analysis (EDA) by Nakai,26,27 D is partitioned on account of which atoms the individual AOs are localized on (that is, irrespective of further population measures) by limiting trace operations in Eqs. 2 to only those AOs that are spatially assigned to atom K. In here, we will compare the use of Eqs. 1 and 2 to the original EDA scheme. However, it should be noted how this scheme has since been extended by employing either natural atomic orbitals or real-space grids,28,29 both of which ameliorate the excessive basis-set dependence of the theory somewhat (albeit at the potential expense of numerical exactness in Ref. 29)."
        },
        {
            "heading": "3 Computational Details",
            "text": "For the QM7 dataset, PySCF30,31 was used to compute molecular energies using the PBE,32 B3LYP,33,34 and M06-2X35 xc functionals in Jensen\u2019s segmented polarization-consistent basis sets36 (pcseg-x, x = 0\u20132), possibly augmented by appropriate diffuse functions (aug-). All decompositions were subsequently performed using the decodense package,37 yielding atomization energies by subtracting gas-phase, free-atom energies from the decomposed results.\nBoth NN architectures based on fixed descriptors and message-passing techniques were investigated due to their different processing of local chemical environments. The acclaimed ANI model,38 as implemented in the TorchAni Python package,39 was used to train an example of the former kind, whereas the PhysNet code was used to train a message-passing analogue.40 Throughout Section 4, we will restrict our focus to results from the PhysNet-\nbased model, since the standard ANI model was found to yield slightly worse results on the whole, while being insensitive to training on a combination of both atomic and molecular energies as opposed to training on the latter only (as is standard practice, cf. Sect. 4). Instead, these additional results have all been collected in the supporting information (SI).\nWe will collectively refer to the combined set of training and validation data as our training set, and the ratio between the two components was fixed to 4:1 throughout, with all remaining molecules constituting the accompanying test set.41 Each model was trained five times, i.e., on sets of five seeds with x random training and validation molecules, and default hyperparameters were used throughout (i.e., no cross-validations were performed).42"
        },
        {
            "heading": "4 Results and Discussion",
            "text": "In Fig. 1, we present distributions of atomization energies across all of QM7 for two different models formulated around Eqs. 1 and 2\u2014using a combination of either Pipek-Mezey43 (PM) localized MOs and standard Mulliken charges or intrinsic bond and atomic orbitals44,45 (IBO/IAO)\u2014alongside the original EDA scheme. As is obvious, these three schemes all yield vastly different results, both in the way they expose any underlying structure in the data, but also in their sometimes highly skewed responses to an augmentation of the double-\u03b6 pcseg-1 basis set with additional diffuse functions. The PM/Mulliken distributions for the different elements are found to greatly overlap, as are the corresponding EDA distributions, while this trend is much less pronounced in the case of the IBO/IAO results, which appear much more structured in general, i.e., binned into distinct groups of contributions, than the other two. In the pcseg-1 basis set, the EDA results are observed to be characteristically uniform, spanning only a restricted energy interval across all different elements, while this is all but the case upon moving to the corresponding aug-pcseg-1 basis set. The same observation holds true for the PM/Mulliken results, as these also change dramatically upon\nadding more diffuse AOs. In the case of the IBO/IAO results, however, these are practically mirror images of one another in the two basis sets, which is satisfactory on its own, but also works to corroborate previous conclusions from Refs. 4\u20136 on the pronounced stability under a change of basis set of this particular decomposition scheme. In the SI, Fig. S1 presents corresponding atomization energies computed in corresponding single-\u03b6 and triple-\u03b6 basis sets (pcseg-0 and pcseg-2, respectively), and these results further support the observations drawn from Fig. 1, namely, the stability of the IBO/IAO scheme and the striking lack of this\nproperty for the other two schemes. The corresponding variances of results under a change of DFA, on the other hand, are significantly less severe in the case of all three schemes, bar a near-constant shift in all atomization energies, cf. the results in Fig. S2 of the SI.\nNext, in Fig. 2, we zoom in on the hydrogen results from Fig. 1, which may be grouped based on their neighbouring heavy atom (either C, N, or O). For this subset of hydrogen atoms, the atomization energies are all predicted to be negative in the EDA model in the pcseg-1 basis, and predominantly so in the aug-pcseg-1 basis, while all corresponding results of the two other schemes are positive. Recalling that positive and negative results in this frame imply destabilization and stabilization of an atom with respect to its resting state\nin vacuum, respectively, these variances indicate major differences in how the individual schemes represent the local electronic structures around the hydrogens in question. In the specific case of a hydrogen atom, it is arguably fair to assume results to be correlated with the electronegativity of its bonded neighbour. In Fig. 2, the MO-based models of Ref. 4 predict exactly this, whereas the EDA scheme generally predicts a lowering of the energy of a hydrogen atom upon its embedding in a molecule. A clear separation between distinct types of hydrogens is observed for the IBO/IAO scheme, in particular, as these energies are clearly binned for the three classes. For the EDA partitioning, on the other hand, it appears as if results are principally determined by (while scaling with) the composition and extent of the underlying basis set due to the dependence on the full 1-RDM. The EDA results in the aug-pcseg-1 basis, in particular, are testament to this, unlike the decompositions of Ref. 4, which allow for distinguishing different, unique atomic environments in any basis set.\nThese observations are further supported by the results in Fig. S3 of the SI, in which atomization energies of individual oxygen atoms are grouped by some of the numerous functional moieties that contain these. In here, the PM/Mulliken energies span the largest range, followed by the IBO/IAO and EDA energies. As for the hydrogen energies in Fig. 2, the EDA distributions in Fig. S3 are once again observed to be notably uniform in the pcseg-1 basis, while any such structure in the data appears to vanish upon adding diffuse functions. For all three schemes, results for oxygens bonded to hydrogen and either of carbon or nitrogen have little overlap in the smaller of the two basis sets (true only for the IBO/IAO results in both of these), but how well these align with chemical intuition is seen to differ. The oxygen atoms in sulfone groups clearly have very peculiar energies compared to all other oxygens in both of the MO-based models of Ref. 4, while the EDA scheme yields energies largely on par with the other oxygens. The exact reasons as to why local electronic structures around oxygens bonded to sulfur are so different, resulting in a characteristically singular shift upwards towards less stabilization in moving from, say, carbonyl to sulfone oxygens, is clearly\nworthy of further investigation, but unfortunately fall outside the scope of the present work.\nNow, how well can we learn to predict atomic energies from each of the three models discussed so far, and how accurate are the total energies derived from these in comparison with standard NN implementations that only learn total, molecular energies? Comparing the total energy training curves in Fig. 3, it is observed that standard training on total energies yields lower root-mean-square errors (RMSEs) than training on a combination of molecular and atomic energies from Fig. 1, albeit only for limited training sets. Upon increasing the amount of data available to the models, their performances align with one another. For smaller training sets, the best atomic partitioning scheme is observed to be EDA, but only so in the pcseg-1 basis set. When training on EDA/aug-pcseg-1 data instead, the NN model\nis notably less capable of learning the underlying physics. The use of IBO/IAO data, on the other hand, is seen to result in somewhat larger errors for sparse training sets, but this NN model clearly learns more rapidly upon enlarging the combined training and validation set, yielding similar RMSE values as the model based on EDA data for the largest sets (and with identical results in both basis sets). The worst partitioning scheme for this purpose appears to be PM/Mulliken, with derived models yielding the largest RMSE values for all datasets, although the learning rate is still better in the pcseg-1 basis set than the EDA-based model.\nAll of these observations are further supported by the results in Fig. S4 of the SI, in which training curves similar to those in Fig. 3 are presented for PhysNet-based models trained exclusively on atomic energies. Here, the model trained on EDA data in the augmented basis set performs as poorly as that trained on PM/Mulliken, learning hardly anything at all upon being offered more training data. From Fig. S4, it is also worth noting how the RMSEs of all models exceed those reported in Fig. 3, a point which is further substantiated in Fig. S5 where training curves are compared for PhysNet-based models that weigh (emphasize) atomic and molecular energies differently. While errors in predictions of individual atomic contributions are approximately alike in-between models, the availability of total energies still manages to regularize atomic contributions to such an extent that they accumulate to yield fair molecular energies (cf. Fig. 3). For models based on fixed descriptors, on the other hand, no such improvements are observed from augmenting traditional datasets of total energies by additional atomic counterparts, cf. the ANI-based results in Figs. S12\u2013S14 of the SI. We ascribe this insensitivity to the fact that the representations associated with individual atomic environments cannot relax in these models (by design), unlike more flexible, message-passing analogues like the custom PhysNet-based models of the present study.\nThe decline in performance of the models trained on a combination of atomic and total energies, rather than just the latter, for limited training sets can be traced back to two main\nattributes, namely, a lack of layers in the training data and an increase in the scarcity of the data. First, by design, the standard PhysNet and ANI models are free to vary their predicted atomic energies, and optimal values of these can thus be readily obtained for any given set of training data. How and whether these align with chemical intuition is somewhat irrelevant in this context, given that only their accumulated sum is evaluated. Furthermore, both types of NN models may be thought of as performing an atomic decomposition of molecular data themselves as, upon training, an implicit type of linear fit of all total energies as a function of the different elements provides a set of reference values for these, the sum of which gets subtracted from the property to be learned (typically accounting for more than 95% this). The so-called self-interaction atomic energies (SIAEs) of the standard ANI model are prime examples of these references. Such a procedure will inevitably lead to a decrease in the standard deviation across a given training set, resulting in total quantities to be predicted that fall within a much more reasonable numerical range, thereby lowering the RMSE. In contrast, models trained on atomic energies are constrained to learn specific values of these from a partitioning scheme of choice, and these contributions hence cannot be decomposed further through the introduction of an intermediate, SIAE-like basis. Be that as it may, Fig. 3 still demonstrates how total energies\u2014derived as a sum of atomic contributions\u2014can be successfully learned whenever NN models are trained on physically sound datasets.\nOf much greater importance, however, is the issue of data scarcity in relation to all models trained to learn atomic energies. To illustrate the detrimental effect the heterogeneity of the different datasets for different atoms can have on model performance, Fig. 4 splits the total energy training curves from Fig. 3 into its compositional constituents. For each training curve, the combined size of the training and validation sets now depends explicitly on the number of atoms of a specific kind in these. As an example, only three sulfur atoms are on average contained in the smallest datasets consisting of 100 molecules, and the RMSEs for this element are thus enormous and of the same order across all three partitioning\nschemes. For the PM/Mulliken data, the training curves are all exceedingly flat, regardless of the element in question, particularly so when the size of the training data is increased, and this partitioning yields the largest overall errors at each sampling point. For hydrogen, in particular, hardly anything is learned upon offering more data to the model, although, at the same time, the element show the lowest RMSE for this partitioning. In the case of the IBO/IAO data, much steeper slopes are observed. Importantly, all training curves have approximately the same slope\u2014and the rate of learning is the same in either of the two tested basis sets\u2014while hydrogen once again has a significantly lower RMSE over the other elements for training data of the same size. Finally, in the pcseg-1 basis set, the EDA data reveal similar patterns as for IBO/IAO, with similar slopes for each atom. However, in this case, the training curves for all but carbon coincide, with similar RMSEs at corresponding training sizes. The prediction of carbon atomization energies, however, is far more inaccurate in comparison. Upon augmentation of the basis set, NN-based ML models are practically incapable of learning any physics on the basis of the EDA atomic contributions, cf. also the ANI version in Fig. S15 of the SI, which show much the same picture as in Fig. 4.\nWe will proceed to interpret these results in the following way. For the EDA/pcseg-1 data, atomization energies for the various elements all fall within relatively narrow distributions, with the exception of carbon, for which the distribution of energies is notably broader. This is the principal reason as to why the energies of this element are more difficult for the model to learn in this basis. As for the model trained on IBO/IAO data, it learns how to predict the energies of all heavy elements at a remarkably similar rate. In this case, however, the RMSEs for hydrogen differ from the other elements, and these data are also more distinctly binned for this partitioning, cf. Fig. 2. This is only true to much less extent for the PM/Mulliken counterpart, which is generally found to be unsuitable for the present purpose on account of its unsystematic decomposition of molecular energies into atomic contributions.\nThese observations may be supported by untangling the training curves in Fig. 3, not on the basis of individual elements, as in Fig. 4, but rather individual functional groups. As the simplest example of this, Fig. 5 presents training curves for the different hydrogen types in\nFig. 2. As is clear, a model based on PM/Mulliken data is incapable of learning the atomic energy of a hydrogen atom in either of the two basis sets, regardless of its neighbouring heavy atom. Here, the least perturbed hydrogens, that is, those bonded to carbons, have the lowest RMSEs, but variances in-between these obviously cannot be learned from the model. Due to the evident difficulties in learning these atomic energies, the NN appears to resort to focusing only on H-C energies in minimizing the loss function, as these are the most abundant. In contrast, the IBO/IAO and EDA hydrogen atomic energies can both be learned, albeit only in the unaugmented basis set in the case of the latter. Again, the training curves basically overlap for the IBO/IAO model, indicating that these are individually distinct from one another, while the curve for the H-C energies differs somewhat from the other two in the case of the EDA model in the pcseg-1 basis set. In the aug-pcseg-1 basis, on the other hand, none of the training curves overlap for the NN model trained on EDA data, rendering this partitioning scheme useless for the present purposes, on par with the PM/Mulliken scheme. The exact same conclusions may be drawn for the oxygen atoms of the most common functional groups in the QM7 dataset. In the SI, Figs. S6 and S17 present training curves for each of these (based on PhysNet and ANI, respectively), showing how the learning rates for the NN models trained on IBO/IAO data in either of the (aug-)pcseg-1 basis sets are mirror images of one another, but not so for the models trained on PM/Mulliken nor EDA data.\nWe are now in a position to probe the origin of the errors from the NN models based on atomic energies. As alluded to above in the discussion around Fig. 4, the uncertainty in the sulfur predictions dominates the molecular RMSEs for low training sizes, on account of the overall scarcity of these atoms across the full QM7 dataset. In contrast, the uncertainty in the hydrogen predictions will be the lowest in this regime, but what about the accumulated errors from these atoms, given that they are the far most numerous? In Fig. 6, cumulative contributions to molecular errors from each type of element are presented for three representative training sets of IBO/IAO atomization energies (PM/Mulliken and EDA versions\nin Figs. S7 and S8). From the results in Fig. 6, we note how the S atoms indeed do make up the largest source of error, followed next by C, O, N, and H, but also that accumulation of errors may pose an issue, as evidenced, e.g., from the carbon results, at least for limited training sets. For this reason, one might speculate whether or not atoms per se identify the best possible decomposition units for the present type of partitioned machine-learned chemistry. Instead, contributions associated with unique functional moieties within molecules might exhibit a degree of physical binning on par with or even surpassing that of atomic contributions, as may energies inherent to individual chemical bonds. That being said, it should still prove instructive to closer inspect the exact manners by which different types of NN models predict local electronic structures, for instance, by comparing standard PhysNet and ANI models with the partitioned counterparts of the present work. Although atomic contributions yielded by routine NN models are often claimed to bear little to no significance on their own,46 some studies have made claims to the contrary.47\u201349 Figs. S9\u2013S11 and S21\u2013S23 of the SI compare the atomic contributions predicted by models trained on total energies with those yielded by custom models trained also on the data in Fig. 1, but how exactly different ML models manage to predict finer details relating to polarization and tendencies\nof electrons to be shared unevenly in molecules will motivate forthcoming work from us."
        },
        {
            "heading": "5 Summary and Conclusion",
            "text": "The present study has compared a number of different ways by which to decompose the molecular energies of all members of the standard QM7 model dataset into atomic contributions. Emphasis has been placed on the emergence of trends among energies from distinctly embedded atom types, but also the stability of different schemes in their response to a change of one-electron basis set. A recent decomposition scheme of ours based on spatially localized molecular orbitals was unanimously found to expose the composition of this small chemical compound space in the most stable and physically motivated manner,4 when measured in terms of its grouping and binning of discrete atomic contributions. The possibility of training tailored high-dimensional neural networks directly on atom-partitioned, rather than merely total molecular energies was further examined, and both the relevance and importance of different compositional factors for training such ML models were assessed in the process.\nWhile only marginally worse in comparison with standard NN models when trained on substantial cross-sections of the full QM7 dataset, the fact that ML models built upon atomic contributions will necessarily accumulate a distribution of errors in their predictions of total molecular properties likely renders these practically unsuitable, or at least non-competitive with routine models, for this particular purpose. However, how exactly these fare in comparison with standard models for more important tasks that necessitate models to generalize well beyond (limited) training set remains to be evaluated. That being said, we still envision a number of possible practical applications for physically well-founded ML models centred around atomic contributions, such as, those based on IBO/IAO data. For instance, the availability of robust and chemically meaningful atomic contributions to total molecular energies may find use in evolutionary algorithms for the optimization of global structures, on par\nwith previous efforts by Hammer and co-workers concerned with the spawning of molecular structures on the basis of local stability evaluations.50 On that note, genetic algorithms for the exploration of chemical compound space and their uses in the rational design of materials may likely also benefit from more local partitionings of global electronic structures.51 While the physical insight that one may lift from models like these is valuable on its own, the potential use of structurally unique local properties as fingerprints of regioselectivity and more general reactivity proxies is also an obvious application for ML models capable of efficiently exposing the heterogeneity of local electronic structures within molecules.52\u201355\nFinally, we remain highly interested in theoretical generalizations of our decomposition schemes from Ref. 4 that will allow for bond-wise partitionings and the machine learning of these.56,57 While contributions associated with individual atoms are at risk of being intangible to most chemists, in particular, synthetic chemists who tend to make use of an altogether different language based on chemical bonds and the energies and classes of these, the present study marks an initial demonstration of the fact that physically motivated partitionings of electronic properties can indeed be efficiently learned by means of contemporary NN architectures. To that end, it remains a long-term goal of ours to further link decomposed features of chemical compound space to objects of broader interest in more practically oriented areas of chemistry. As an example, the access to contributions to molecular energies from the individual entities of a complete bond network may potentially enable partitioned models that can operate on the basis of unique functional constituents of a chemical system, e.g., in the spirit of the fragment-based ML theories of von Lilienfeld based on dictionary groupings of embedded atoms.58,59 Whether simple atomic energies and the relative ranking of these in a molecular setting can prove useful for such applications, or one will ultimately need to rely on more elaborate decomposition schemes, is an area of ongoing research in our lab."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by a research grant (no. 37411) from VILLUM FONDEN (a part of THE VELUX FOUNDATIONS).\nData Availability\nData in support of the findings of this study are available within the article and its SI. In addition, all raw data files and metadata have been deposited in a dedicated Zenodo repository: https://doi.org/10.5281/zenodo.7646088.\nSupporting Information\nThe supporting information (SI) collects a number of additional results in support of the present study. In Figs. S1 and S2, dependencies on one-electron basis set and DFA for all three of the tested partitioning schemes are evaluated, respectively. On par with Fig. 2, Fig. S3 presents atomization energies for oxygen atoms of the most common functional groups in the QM7 dataset, while Fig. S6 presents corresponding training curves for these (as for the hydrogens in Fig. 5). Figs. S4 and S5 show the sensitivity of PhysNet-based models to an augmentation of standard data by atomic energies. Figs. S7 and S8 present the same results as in Fig. 6, but instead trained on PM/Mulliken and EDA atomization energies, respectively, while Figs. S9\u2013S11 report predicted atomic energies from PhysNet NNs trained on sets of 5 seeds with either 100, 1000, or 5732 random molecules in each, respectively, using either exclusively total energies or a combination of these and data from any of the three atomic partitioning schemes as input. Finally, Figs. S12\u2013S23 present corresponding versions of Figs. 3\u20136 and S4\u2013S11, but based on fixed-descriptor ANI NN architectures."
        }
    ],
    "title": "Decomposing Chemical Space: Applications to the Machine Learning of Atomic Energies",
    "year": 2023
}