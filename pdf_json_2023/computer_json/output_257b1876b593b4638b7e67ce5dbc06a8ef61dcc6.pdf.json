{
    "abstractText": "We present Neural Pixel Composition (NPC), a novel approach for continuous 3D-4D view synthesis given only a discrete set of multi-view observations as input. Existing state-of-the-art approaches require dense multi-view supervision and an extensive computational budget. The proposed formulation reliably operates on sparse and widebaseline multi-view imagery and can be trained efficiently within a few seconds to 10 minutes for hi-res (12MP) content, i.e., 200-400\u00d7 faster convergence than existing methods. Crucial to our approach are two core novelties: 1) a representation of a pixel that contains color and depth information accumulated from multi-views for a particular location and time along a line of sight, and 2) a multi-layer perceptron (MLP) that enables the composition of this rich information provided for a pixel location to obtain the final color output. We experiment with a large variety of multi-view sequences, compare to existing approaches, and achieve better results in diverse and challenging settings. Finally, our approach enables dense 3D reconstruction from sparse multi-views, where COLMAP, a state-of-the-art 3D reconstruction approach, struggles.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aayush Bansal"
        },
        {
            "affiliations": [],
            "name": "Michael Zollhoefer"
        }
    ],
    "id": "SP:410d8f1641ffb2962e932482c32069957e59ea9e",
    "references": [
        {
            "authors": [
                "E.H. Adelson",
                "Bergen",
                "J.R"
            ],
            "title": "The plenoptic function and the elements of early vision, vol",
            "venue": "2. Vision and Modeling Group, Media Laboratory, Massachusetts Institute of Technology",
            "year": 1991
        },
        {
            "authors": [
                "S. Agarwal",
                "N. Snavely",
                "I. Simon",
                "S.M. Seitz",
                "R. Szeliski"
            ],
            "title": "Building rome in a day",
            "venue": "ICCV",
            "year": 2009
        },
        {
            "authors": [
                "A. Bansal",
                "M. Vo",
                "Y. Sheikh",
                "D. Ramanan",
                "S. Narasimhan"
            ],
            "title": "4d visualization of dynamic events from unconstrained multi-view videos",
            "venue": "CVPR",
            "year": 2020
        },
        {
            "authors": [
                "J.T. Barron",
                "B. Mildenhall",
                "M. Tancik",
                "P. Hedman",
                "R. Martin-Brualla",
                "P.P. Srinivasan"
            ],
            "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "J. Chibane",
                "A. Bansal",
                "V. Lazova",
                "G. Pons-Moll"
            ],
            "title": "Stereo radiance fields (srf): Learning view synthesis from sparse views of novel scenes",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "K. Deng",
                "A. Liu",
                "J.Y. Zhu",
                "D. Ramanan"
            ],
            "title": "Depth-supervised nerf: Fewer views and faster training for free",
            "venue": "CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Y. Du",
                "Y. Zhang",
                "H.X. Yu",
                "J.B. Tenenbaum",
                "J. Wu"
            ],
            "title": "Neural radiance flow for 4d view synthesis and video processing",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Y. Furukawa",
                "C Hernandez"
            ],
            "title": "Multi-view stereo: A tutorial",
            "venue": "Foundation and Trends in Computer Graphics and Vision",
            "year": 2015
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "NeurIPS",
            "year": 2014
        },
        {
            "authors": [
                "S.J. Gortler",
                "R. Grzeszczuk",
                "R. Szeliski",
                "M.F. Cohen"
            ],
            "title": "The lumigraph",
            "venue": "SIGGRAPH",
            "year": 1996
        },
        {
            "authors": [
                "R. Hartley",
                "A. Zisserman"
            ],
            "title": "Multiple view geometry in computer vision",
            "venue": "Cambridge university press",
            "year": 2003
        },
        {
            "authors": [
                "J. Heinly"
            ],
            "title": "Toward Efficient and Robust Large-Scale Structure-from-Motion Systems",
            "venue": "Ph.D. thesis, The University of North Carolina at Chapel Hill",
            "year": 2015
        },
        {
            "authors": [
                "P.H. Huang",
                "K. Matzen",
                "J. Kopf",
                "N. Ahuja",
                "J.B. Huang"
            ],
            "title": "Deepmvs: Learning multi-view stereopsis",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "Y. Jeong",
                "S. Ahn",
                "C. Choy",
                "A. Anandkumar",
                "M. Cho",
                "J. Park"
            ],
            "title": "Self-calibrating neural radiance fields",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "T. Kanade",
                "P. Narayanan"
            ],
            "title": "Historical perspectives on 4d virtualized reality",
            "venue": "CVPR Workshops",
            "year": 2006
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980",
            "year": 2014
        },
        {
            "authors": [
                "A. Knapitsch",
                "J. Park",
                "Q.Y. Zhou",
                "V. Koltun"
            ],
            "title": "Tanks and temples: Benchmarking large-scale scene reconstruction",
            "venue": "ACM Trans. Graph.",
            "year": 2017
        },
        {
            "authors": [
                "M. Levoy",
                "P. Hanrahan"
            ],
            "title": "Light field rendering",
            "venue": "Annual Conference on Computer Graphics and Interactive Techniques. ACM",
            "year": 1996
        },
        {
            "authors": [
                "C.H. Lin",
                "W.C. Ma",
                "A. Torralba",
                "S. Lucey"
            ],
            "title": "Barf: Bundle-adjusting neural radiance fields",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "S. Lombardi",
                "T. Simon",
                "J. Saragih",
                "G. Schwartz",
                "A. Lehrmann",
                "Y. Sheikh"
            ],
            "title": "Neural volumes: Learning dynamic renderable volumes from images",
            "venue": "ACM Trans. Graph.",
            "year": 2019
        },
        {
            "authors": [
                "S. Lombardi",
                "T. Simon",
                "G. Schwartz",
                "M. Zollhoefer",
                "Y. Sheikh",
                "J. Saragih"
            ],
            "title": "Mixture of volumetric primitives for efficient neural rendering",
            "venue": "ACM Trans. Graph.",
            "year": 2021
        },
        {
            "authors": [
                "L. McMillan"
            ],
            "title": "An image-based approach to three-dimensional computer graphics",
            "venue": "Ph. D. Dissertation, UNC Computer Science",
            "year": 1999
        },
        {
            "authors": [
                "L. McMillan",
                "G. Bishop"
            ],
            "title": "Plenoptic modeling: An image-based rendering system",
            "venue": "SIGGRAPH",
            "year": 1995
        },
        {
            "authors": [
                "A. Meyer",
                "F. Neyret"
            ],
            "title": "Interactive volumetric textures",
            "venue": "Eurographics Workshop on Rendering Techniques. pp. 157\u2013168. Springer",
            "year": 1998
        },
        {
            "authors": [
                "M. Mihajlovic",
                "A. Bansal",
                "M. Zollhoefer",
                "S. Tang",
                "S. Saito"
            ],
            "title": "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints",
            "venue": "ECCV",
            "year": 2022
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "R. Ortiz-Cayon",
                "N.K. Kalantari",
                "R. Ramamoorthi",
                "R. Ng",
                "A. Kar"
            ],
            "title": "Local light field fusion: Practical view synthesis with prescriptive sampling guidelines",
            "venue": "ACM Trans. Graph.",
            "year": 2019
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "ECCV",
            "year": 2020
        },
        {
            "authors": [
                "R. Ng"
            ],
            "title": "Light field photography with a hand-held plenoptic camera",
            "venue": "Ph.D. thesis, Stanford University",
            "year": 2005
        },
        {
            "authors": [
                "M.M. Oliveira",
                "G. Bishop"
            ],
            "title": "Image-based objects",
            "venue": "Proceedings of the 1999 symposium on Interactive 3D graphics. pp. 191\u2013198",
            "year": 1999
        },
        {
            "authors": [
                "E. Penner",
                "L. Zhang"
            ],
            "title": "Soft 3d reconstruction for view synthesis",
            "venue": "ACM Trans. Graph.",
            "year": 2017
        },
        {
            "authors": [
                "A. Pumarola",
                "E. Corona",
                "G. Pons-Moll",
                "F. Moreno-Noguer"
            ],
            "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "G. Riegler",
                "V. Koltun"
            ],
            "title": "Free view synthesis",
            "venue": "ECCV",
            "year": 2020
        },
        {
            "authors": [
                "G. Riegler",
                "V. Koltun"
            ],
            "title": "Stable view synthesis",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "International Conference on Medical image computing and computer-assisted intervention. Springer",
            "year": 2015
        },
        {
            "authors": [
                "J.L. Sch\u00f6nberger",
                "J.M. Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "J.L. Sch\u00f6nberger",
                "E. Zheng",
                "M. Pollefeys",
                "J.M. Frahm"
            ],
            "title": "Pixelwise view selection for unstructured multi-view stereo",
            "venue": "ECCV",
            "year": 2016
        },
        {
            "authors": [
                "J. Shade",
                "S. Gortler",
                "He",
                "L.w.",
                "R. Szeliski"
            ],
            "title": "Layered depth images",
            "venue": "Proceedings of the 25th annual conference on Computer graphics and interactive techniques",
            "year": 1998
        },
        {
            "authors": [
                "H. Shum",
                "S.B. Kang"
            ],
            "title": "Review of image-based rendering techniques",
            "venue": "Visual Communications and Image Processing",
            "year": 2000
        },
        {
            "authors": [
                "V. Sitzmann",
                "M. Zollh\u00f6fer",
                "G. Wetzstein"
            ],
            "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations",
            "venue": "Neurips",
            "year": 2019
        },
        {
            "authors": [
                "N. Snavely",
                "S.M. Seitz",
                "R. Szeliski"
            ],
            "title": "Photo tourism: Exploring photo collections in 3d",
            "venue": "ACM Trans. Graph.",
            "year": 2006
        },
        {
            "authors": [
                "A. Tewari",
                "O. Fried",
                "J. Thies",
                "V. Sitzmann",
                "S. Lombardi",
                "K. Sunkavalli",
                "R. MartinBrualla",
                "T. Simon",
                "J. Saragih",
                "M. Nie\u00dfner",
                "R. Pandey",
                "S. Fanello",
                "G. Wetzstein",
                "J.Y. Zhu",
                "C. Theobalt",
                "M. Agrawala",
                "E. Shechtman",
                "D.B. Goldman",
                "M. Zollh\u00f6fer"
            ],
            "title": "State of the Art on Neural Rendering",
            "venue": "Computer Graphics Forum (EG STAR 2020)",
            "year": 2020
        },
        {
            "authors": [
                "A. Tewari",
                "J. Thies",
                "B. Mildenhall",
                "P. Srinivasan",
                "E. Tretschk",
                "Y. Wang",
                "C. Lassner",
                "V. Sitzmann",
                "R. Martin-Brualla",
                "S. Lombardi",
                "T. Simon",
                "C. Theobalt",
                "M. Niessner",
                "J.T. Barron",
                "G. Wetzstein",
                "M. Zollhoefer",
                "V. Golyanik"
            ],
            "title": "Advances in neural rendering",
            "venue": "Neural Pixel Composition (NPC)",
            "year": 2021
        },
        {
            "authors": [
                "Q. Wang",
                "Z. Wang",
                "K. Genova",
                "P. Srinivasan",
                "H. Zhou",
                "J.T. Barron",
                "R. MartinBrualla",
                "N. Snavely",
                "T. Funkhouser"
            ],
            "title": "Ibrnet: Learning multi-view image-based rendering",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "S. Wizadwongsa",
                "P. Phongthawee",
                "J. Yenphraphai",
                "S. Suwajanakorn"
            ],
            "title": "Nex: Realtime view synthesis with neural basis expansion",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "L. Wu",
                "J.Y. Lee",
                "A. Bhattad",
                "Y. Wang",
                "D. Forsyth"
            ],
            "title": "Diver: Real-time and accurate neural radiance fields with deterministic integration for volume rendering",
            "venue": "CVPR",
            "year": 2022
        },
        {
            "authors": [
                "G. Yang",
                "J. Manela",
                "M. Happold",
                "D. Ramanan"
            ],
            "title": "Hierarchical deep stereo matching on high-resolution images",
            "venue": "CVPR",
            "year": 2019
        },
        {
            "authors": [
                "A. Yu",
                "V. Ye",
                "M. Tancik",
                "A. Kanazawa"
            ],
            "title": "Pixelnerf: Neural radiance fields from one or few images",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "G. Riegler",
                "N. Snavely",
                "V. Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros",
                "E. Shechtman",
                "O. Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "T. Zhou",
                "R. Tucker",
                "J. Flynn",
                "G. Fyffe",
                "N. Snavely"
            ],
            "title": "Stereo magnification: learning view synthesis using multiplane images",
            "venue": "ACM Trans. Graph.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: 3D View Synthesis, 4D Visualization, 3D Reconstruction."
        },
        {
            "heading": "1 Introduction",
            "text": "Novel views can be readily generated if we have access to the underlying 6D plenoptic function R(\u03b8,d, \u03c4) [1,23] of the scene that models the radiance incident from direction \u03b8 \u2208 R2 to a camera placed at position d \u2208 R3 at time \u03c4 . Currently, no approach exists that can automatically reconstruct an efficient space- andtime representation of the plenoptic function given only a (potentially sparse) set of multi-view measurements of the scene as input. The core idea of image-based rendering [22,37] is to generate novel views based on re-projected information from a set of calibrated source views. This re-projection requires a high-quality estimate of the scene\u2019s geometry and is only correct for Lambertian materials, since the appearance of specular surfaces is highly view-dependent. Building a dense 3D volume from multi-view inputs that provides correct 3D information for each pixel location is a non-trivial task.\nRecent approaches such as Neural Radiance Fields (NeRF) [27] and Neural Volumes (NV) [20] attempt to create rich 3D information along a ray of\nar X\niv :2\n20 7.\n10 66\n3v 1\n[ cs\n.C V\n] 2\n1 Ju\nl 2 02\nlight by sampling 3D points at regular intervals given a min-max bound. Radiance fields are highly flexible 3D scene representations that enables them to represent a large variety of scenes including semi-transparent objects. The price to be paid for this flexibility is that current approaches are restricted to datasets that provide dense 3D observations [20,27,31,32,33,48], can only model bounded scenes [5,20,25,27,43,47], and require intensive computational resources [20,27,48]. In contrast, we introduce a multi-view composition approach that combines the insights from image-based rendering [38] with the power of neural rendering [41] by learning how to best aggregate information from different views given only imperfect depth estimates as input. Figure 1 shows novel views synthesized using our approach for different multi-view sequences. Our approach can operate on sparse and wide-baseline multi-view imagery (assuming known camera parameters) and requires limited computational resources for operation. The model learned on a single time-instant for one subject (Fig 1-(e)) generalizes to unseen time instances and unseen subjects without any fine-tuning (Fig 1-(f)).\nWe accumulate rich 3D information (color and depth) for a pixel location using an off-the-shelf disparity estimation approach [46] given multiple stereo pairs as input. We then learn a small multi-layer perceptron (MLP) for a given multi-view sequence that inputs the per-pixel information at a given camera position and outputs color at the location. Figure 2 illustrates the components of our approach. We train an MLP for a sequence by sampling random pixels given multi-views. In our experiments, we observe that a simple 5-layer perceptron is sufficient to generate high-quality results. Our model roughly requires 1 GB of GPU memory and can be trained within a few seconds to 10 minutes from scratch for a hi-res multi-view sequence. The trained model allows us to perform a single forward-pass at test time for each pixel location in a target camera view. A single forward pass per pixel is more efficient than radiance field based approaches that require hundreds of samples along each ray. Finally, the alpha values (\u03b1i) allow us to perform dense 3D reconstruction of the scene by selecting appropriate depth values at a given location.\nIn summary, our contributions are:\n\u2013 A surprisingly simple, yet effective approach for view synthesis from calibrated multi-view images that works with limited computational resources on diverse multi-view sequences.\n\u2013 Our approach offers a natural extension to the 4D view synthesis problem. Our approach can also generalize to unseen time instances.\n\u2013 Our approach is able to obtain dense 3D reconstruction on challenging inthe-wild scenes."
        },
        {
            "heading": "2 Related Work",
            "text": "Our novel view synthesis work is closely related to several research domains, such as classical 3D reconstruction and plenoptic modeling, as well as neural rendering for static and dynamic scenes. In the following, we cover the most related approaches. For a detailed discussion of neural rendering approaches, we refer to the surveys [38,41,42]. Plenoptic Modeling and NeRF: Plenoptic function [1,23] does not require geometric modeling. A plenoptic or a light-field camera [10,18,28] captures all possible rays of light (in a bounded scene), which in turns enables the synthesis of a new view via a per-ray look-up. Recent approaches such as NeRF [27] and follow-up work [4,45,48] employ a multi-layer perceptron (MLP) that infers color and opacity values at 3D locations along each camera ray. These color and opacity values along the ray are then being integrated to obtain the final pixel color. This requires: 1) dense multi-view inputs [5,47]; 2) perfect camera parameters [14,19]; and 3) a min-max bound to sample 3D points along a ray of light [32,48]. We observe degenerate outputs if all three conditions are not met (as shown in Figure 3). Different approaches either use prior knowledge or a large number of multi-view sequences [5,43,47], additional geometric optimization [14,19], or large capacity models to separately capture foreground and background [48]. In this work, we use an off-the-shelf disparity estimation module [46] that allows us to accumulate 3D information for a given pixel location. A simple MLP provides us with blending parameters that enable the composition of color information. This allows us to overcome the above-mentioned three challenges albeit using limited computational resources to train/test the model. 3D Reconstruction and View Synthesis: Another approach to solve the problem is to obtain dense 3D reconstruction from the input images [11] and project 3D points to the target view. There has been immense progress in densely reconstructing the static world from multi-view imagery [8,15], internet scale photos [2,12,35,40], and videos [36]. Synthesizing a novel view from accumulated 3D point clouds may not be consistent due to varying illumination, specular material, and different cameras used for the capture of the various viewpoints. Riegler et al. [32,33] use a neural network to obtain consistent visuals given a dense 3D reconstruction. This works well for dense multi-view observations [17]. However, 3D reconstruction is sparse given wide-baseline views or scenes with specular surfaces. This is highlighted in Figure 3, which shows 3D reconstruction results of COLMAP [35,36] using one of the sequences. Recently, DS-NeRF [6] use sparse 3D points from COLMAP along with NeRF to learn better and faster view synthesis. As shown in Figure 3, adding explicit depth information enables DS-NeRF to capture scene structure but still struggles with details. Layered Depth and Multi-Plane Images: Closely related to our work are layered depth images [24,26,29,30,37,50] that learn an alpha composition for multi-plane images at discrete depth positions. In this work, we did not restrict our approach to 2D planes or specific depth locations. Instead, we learn a representation for a pixel at arbitrary depth locations. A pixel-wise representation not only allows us to interpolate, but also to extrapolate, and obtain dense 3D\nreconstruction results. Since we have a pixel-wise representation, we are able to generate 12MP resolution images without any modifications of our approach. Prior work has demonstrated results on a maximum of 2MP resolution content.\n4D View Synthesis:Most approaches are restricted to 3D view synthesis [26,27] and would require drastic modifications [7,31] to be applied to the 4D viewsynthesis problem. Lombardi et al. [21] employ a mixture of animated volumetric primitives to model the dynamic appearance of human heads from dense multiview observations. Open4D [3] requires foreground and background modeling for 4D visualization. Our work does not require major modifications to extend to 4D view-synthesis. In addition, we do not require explicit foreground-background modeling for 4D view synthesis. We demonstrate our approach on the challenging Open4D dataset [3] where the minimum distance between two cameras is 50cm. Our composition model trained on a single time instant also enables us to do 4D visualization for unseen time instances. Finally, the model learned for view synthesis enable dense 3D reconstruction on multi-view content. To our knowledge, no prior work has demonstrated these results for 3D-4D multi-view view synthesis."
        },
        {
            "heading": "3 Method",
            "text": "We are given M multi-view images with camera parameters (intrinsics and extrinsics) as input. Our goal is to learn a function, f , that inputs pixel information (p), p \u2208 RNp , and outputs color (c\u0304 \u2208 R3) at that location, i.e., f : p \u2192 c\u0304. Learning such a function is challenging since we live in a 3D-4D world and images provide only 2D measurements. We present two crucial components: 1) a representation of a pixel that contains relevant multi-view information for highfidelity view synthesis; and 2) a multi-layer perceptron (MLP) that inputs the pixel information and outputs the color. Overview: We input a pixel location (x, y) given corresponding camera parameters (rx, ry, rz, tx, ty, tz) at time, \u03c4 , along with an array of possible 3D points along the line of sight. The ith location of this array contains depth (di) and color (ci). The MLP outputs alpha (\u03b1i) values for the i\nth location that allow us to obtain the final color at (x, y). The MLP also outputs gamma, \u03b3 \u2208 R3, which is a correction term learned by the model. We get the final color at pixel location (x, y) as: c\u0304 = \u03b3 + \u2211N i=1 \u03b1ici, where N is the number of points in the array.\nWe describe our representation of a pixel in Sec. 3.1 and the MLP in Sec. 3.2."
        },
        {
            "heading": "3.1 Representation of a Pixel",
            "text": "Given a pixel location (x, y) for a camera position (rx, ry, rz, tx, ty, tz), our goal is to collect dense 3D information that contains depth and color information at"
        },
        {
            "heading": "OursNeRF",
            "text": "all possible 3D points along a line of sight. We obtain 3D points via two-view geometry [11] by forming ( M 2 ) stereo-pairs. The estimated disparity between a stereo pair provides the depth for the 3D point locations. Multiple stereo pairs allow us to densely populate 3D points along the rays.\nColor and Depth Array: We use multiple stereo pairs to build an array of depth (d) and color (c) for a pixel. We store the values in order of increasing depth, i.e., di+1 \u2265 di. The array is similar to a ray of light that travels in a particular direction connecting the 3D points. We limit the number of 3D points to be N . If there are less than N depth observations, we set di = 0 and ci = (0, 0, 0). If there are more than N observation, we clip to the closest N 3D points.\nUncertainty Array: In this work, we use an off-the-shelf disparity estimation module from Yang et al. [46]. This approach provides an estimate of uncertainty (entropy) for each prediction. We also keep an array of uncertainty values (H) of equal size as the depth array (obtained from disparity and camera parameters), s.t., Hi \u2208 [0, 1], where a higher value represents higher uncertainty. The uncertainty allow us to suppress noise or uncertain 3D points.\nEncoding Spatial Information: For each pixel, we concatenate its spatial location, i.e., (x, y) location and camera position (rx, ry, rz, tx, ty, tz). We employ highfrequency positional encoding [26] to represent spatial information of a pixel for a given camera position. We normalize the pixel coordinates, s.t., x \u2208 [\u22121, 1] and y \u2208 [\u22121, 1].\nIncorporating Temporal Information: Our approach enables a natural extension to incorporate temporal information. Given a temporal sequence with T frames, we represent each time instant as a Gaussian distribution with peak at the frame \u03c4 . We concatenate the color, depth, and uncertainty array alongside the spatial and temporal information in a single Np-dimensional array, where Np is sum of the dimensions of each term. We input this array to the MLP to compute the color output at the pixel location."
        },
        {
            "heading": "3.2 Neural Composition via Multi-Layer Perceptron (MLP)",
            "text": "Our goal is to output blending values \u03b1 that enable us to take the appropriate linear combination of color values in the color-array. A naive way is to directly use the output of the last layer of the MLP as an alpha array and compute a dot product with the color-array:\nf(x, y, \u03c4, rx, ry, rz, tx, ty, tz, c,d,H) = w. (1)\nWhile this is reasonable, it assumes that the MLP will implicitly understand the relationship between color (c), depth (d), and uncertainty (H). This is challenging to learn. In this work, we observe that explicitly using the depth and uncertainty with the output of the MLP (w) enables better view synthesis. We, therefore, define:\n\u03b1i = (1\u2212 Hi)e\u2212(widi\u2212\u00b5)\n2\n\u2211N j=1(1\u2212 Hj)e\u2212(wjdj\u2212\u00b5) 2 , (2)\nwhere \u00b5 = 1N \u2211N\nj=1 wjdj . The Gaussian distribution forces the model to select color values belonging to depth location that are: 1) closest to the average depth value; and 2) are confident and less noisy. We employ these alpha values together with the original color array to predict the final values (c\u0304):\nc\u0304 =\nN\u2211\ni=1\n\u03b1ici + \u03b3, (3)\nwhere \u03b3 is an additional correction term that helps us to obtain sharp outputs. Note that the fifth layer of the MLP outputs \u03b1 and \u03b3 values.\nMulti-Layer Perceptron: We employ a 5-layer perceptron. Each linear function has 256 activations followed by a non-linear ReLU activation function. We train the MLP in a self-supervised manner using a photometric \u21131-loss:\nmin f\nL = m\u2211\nk=1\n\u2223\u2223\u2223\u2223ck \u2212 c\u0304k \u2223\u2223\u2223\u2223 1 , (4)\nwhere ck and c\u0304k are the ground truth color and predicted color respectively for the kth pixel, and m is the number of randomly sampled pixels from the\nM images. We train the MLP from scratch using the Adam optimizer [16]. We randomly sample 4 images, and sample 256 pixels from each image for every forward/backward pass. The learning rate is kept constant at 0.0002 for the first 5 epochs and is then linearly decayed to zero over next 5 epochs. We observe that composition model converges around in a few seconds of training on a single GPU with 1 GB GPU memory. Figure 5 contrasts our results with NeRF on one such dense multi-view sequence. Naive Composition: One can also naively use the pixel representation to generate the final output by selecting the color value for the closest depth location. A slightly nuanced version is to take average of color values for three closest depth location (Naive Composition++). We use this naive composition for comparisons in our work. Figure 4 shows the importance of using neural composition via MLP over naive composition. We believe it is an importance baseline for view synthesis as this simple nearest-neighbor based method generates results without any training."
        },
        {
            "heading": "4 3D Multi-View View Synthesis",
            "text": "We study various aspects of 3D view synthesis using our approach: (1) synthesizing novel views given sparse and unconstrained multi-views (Sec. 4.1); (2) synthesizing hi-res 12MP content (Sec. 4.2); and (3) scenes with unbounded depth and influence of the number of views (Sec. 4.3). We then demonstrate our approach on hi-resolution studio capture in Sec. 4.4 and show that our approach can generalize to unseen subjects and unknown time instances from a single time-instant. We study convergence in Sec. 4.5 where we observe that our model gets close to convergence within a few seconds of learning. Finally, there are more analysis in Appendix C."
        },
        {
            "heading": "4.1 Sparse and Unconstrained Multi-Views",
            "text": "We use 24 sequences of sparse and unconstrained real-world samples from the Open4D dataset [3]. Open4D consists of temporal sequences. We use certain time instants for 3D view synthesis. The minimum distance between two adjacent cameras is 50cm in these sequences. We contrast our approach with NeRF [27]. We also study DS-NeRF [6], which additionally employs sparse 3D point clouds from COLMAP for training NeRF. DS-NeRF has shown promising results given the sparse views from LLFF dataset [26]. Both approaches are trained for 200, 000 iterations (roughly 420 minutes) per sequence on a NVIDIA V100 GPU. Table 1 compares the performance of different methods on held-out views from these sequences using PSNR, SSIM, and LPIPS (AlexNet) [49]. In this work, we observe that these three evaluation criteria are not self-sufficient in determining the relative ranking of different methods. While PSNR and SSIM may favor smooth or blurry results [49], LPIPS may ignore the structural consistency in images. We posit that it is important to look at all three criteria and not one. Figure 3 shows the qualitative performance of our approach on\nthese challenging sequences. We observe degenerate results using NeRF on these sequences. DS-NeRF also results in degenerate outputs most of the times except for the scenes with bounded depth. Our approach is able to generate high-quality results (with details such as faces, hair, dress, etc.) in this setting both qualitatively and quantitatively. Total time taken to process (pre-processing multi-view content and training the composition model) a sequence is less than 10 minutes. We also observe that a naive pixel composition can also yield meaningful results better than prior work. However, we obtain better pixel composition using MLPs. The details of these sequences are available in Appendix A.1."
        },
        {
            "heading": "4.2 High-Resolution (12MP) View Synthesis",
            "text": "We use twelve high-resolution (4032\u00d73024) multi-view sequences from the LLFF dataset [26] that contain challenging specular surfaces. In this setting, we train NeRF [27] on these sequences for 2, 000, 000 iterations which take approximately 64 hours on a single NVIDIA V100 GPU (10, 000 iterations take 20 minutes). Performance saturates at 1M iterations after 32 hours of training. We also show the performance for vanilla NeRF that is trained for 200, 000 iterations and takes 400 \u2212 420 minutes to train. We train our model for 10 epochs, which takes around 10 minutes on a single GPU and only 1GB GPU of memory. We estimate disparity [46] for multiple stereo pairs at one-fourth resolution for these sequences. Disparity estimation using the off-the-shelf model takes less than 5 minutes per sequence on a single GPU. Table 2 contrasts the performance of NeRF models at different intervals of training using PSNR, SSIM, and LPIPS\n(AlexNet). We compute the average of per-frame statistics as the number of samples in the test set for these 12 sequences are roughly the same. We once again observe that it is crucial to include all three evaluation criteria. Figure 6 shows the results of NeRF at different intervals of time. We observe that the NeRF model improves over time and captures sharp results as suggested by LPIPS. Our method enables sharper outputs as compared to NeRF. Interestingly, NeRF does not capture details even for training samples when trained sufficiently long (64 hours) which suggests that it is non-trivial to capture details using NeRF on held-out samples. The qualitative and quantitative analysis suggest that we can efficiently generate results on 12MP images without drastically increasing the computational resources. We also show the performance of naive composition to generate the final outputs. We observe that MLPs allow us to obtain better results. The details of these sequences are available in Appendix A.2.\nWe also vary the number of stereo pairs (K) to synthesize the target view. We observe that we can get better results with a few stereo pairs than using all pairs. Synthesizing a new view for a dense multi-view sequence can be achieved by looking at the local neighborhood of the target location instead of using all the views. Local neighborhood is determined based on position in world space, i.e., we use stereo-pairs corresponding to the closest camera and then next and so on, unless we have K samples. This allows us to speed-up training and testing.\nShiny Dataset: We use 8 multi-view sequences from the Shiny Dataset [44] that consists of multi-views captured for specular surfaces. The resolution of 6 sequences (less than 60 samples in each) in this dataset is 4032 \u00d7 3024, and the remaining two (cd and labs have more than 300 samples) have resolution 1920\u00d71080. We train NeRF on the original resolution of these sequences for 2M iterations (64 hours per GPU). We contrast the performance with our approach that is trained for 10 epochs and 50 epochs. Table 3 shows the performance of different methods. We follow the evaluation criteria (average of per-sequence PSNR, multi-channel SSIM, LPIPS1) from NeX [44]. We also add the results generated by NeX [44] that synthesizes on one-fourth resolution for these sequences. We do a simple 4\u00d7-upsampling of their results to target resolution for an apples-to-apples comparison. Our model trained for 10 minutes achieves results close to the best performance. Figure 7 contrasts our method with NeX. We observe small details are better captured by our method.\nStandard LLFF Sequences: We quantitatively evaluate our approach on 8 forward-facing real-world multi-view sequences [27] in Table 4. We use the original hi-res (4032 \u00d7 3024) undistorted images provided by Wizadwongsa et al. [44]. We once again train NeRF models for these hi-res sequences for 2M iterations (64 hours per GPU). Training the model for long allows us to get better performing NeRF models for these sequences. We follow the evaluation criteria (average of per-sequence PSNR, multi-channel SSIM, LPIPS) from NeX [44]. We also add the results reported by NeX [44]. These results were generated on one-fourth resolution. We upsample them to the desired resolution. We report\n1 We, however, use LPIPS via AlexNet (alex) instead of VGG-Net (vgg) to fit 12MP images on a single GPU.\nthe performance of our approach (without any modification for these sequences) trained for 10 and 50 epochs. Our approach underperform both PSNR and SSIM but achieves a competitive LPIPS score. However, we can generate novel hi-res views (12MP) in a few minutes with limited computational resources."
        },
        {
            "heading": "4.3 Unbounded Scenes and Varying Number of Views",
            "text": "We study the influence of the number of views on the quality of synthesized views. We use challenging synthetic multi-view sequences from MVS-Synth dataset [13] that consist of different unbounded scenes. We use the first 13 sequences with unbounded depth from this dataset for our analysis. Each sequence consists of 100 frames. We use 50 frames (1920 \u00d7 1080 resolution) for evaluation, and train models by varying the number of views between {10, 20, 30, 40, 50}. The details about train-test splits are available in Appendix A.3. The ground truth camera parameters are provided for these sequences. For this analysis, we train 65 NeRF [27] models (each for 200, 000 iterations taking roughly 420 minutes per model) and 65 models for our approach. Our approach takes 10\u221220 minutes per sequence depending on the number of views. We contrast the performance of two methods in Table 5. Without any modification, our approach can generate better results. We can also generate better results with fewer views. For e.g., our method can get better results with 10 views than NeRF with 50 views on these sequences. Figure 8 and Figure 9 shows the comparison of our approach with NeRF when using 10 and 50 views respectively. We show the improvement in performance\nwhen using more views in Figure 11. Consistent with the quantitative analysis (Table 5), we see better results visually when increasing the number of views. Estimating camera parameters via SfM: We repeat the above experiment with different camera parameters. We estimate camera parameters (intrinsics and extrinsics) from multi-views using Agisoft Metashape (a professional software used widely in industry). Table 6 shows the performance of two approaches. We observe similar trends. The performance of NeRF improved drastically with camera parameters estimated using SfM. However, it still underperforms in comparison to our method by a large margin. We show the best performing result of NeRF on a held-out view from one of these sequences in Figure 10. We observe that our approach captures details better than NeRF. Note, we also tried COLMAP to obtain camera poses and point clouds. However, COLMAP struggles on some of these sequences."
        },
        {
            "heading": "4.4 Hi-Res Studio Capture",
            "text": "Multi-View Facial Capture: We employ multi-view hi-res facial captures. We can synthesize hi-resolution novel views with a few minutes of training without any modification and without using any expert knowledge such as facial details, foreground-background etc. Figure 12 shows novel views synthesized and facial details (such as hair, eyes, wrinkles, teeth, etc.) captured using a model trained for a specific subject. Multi-View Full Body Capture: Our approach also enables us to synthesize full-bodies from hi-res multi-view captures. Once again, we did not use any human-body specific information. Figure 14 shows novel views synthesized and body details captured using a model trained for a specific subject.\nAbility to Generalize: An important aspect of our approach is to enable generalization to unseen time instants and unknown subjects. We train a model for one time instant of one subject and can use it to synthesize new views for unknown time instants. We show extreme facial expressions and unseen subjects in Figure 13. We also contrast the results of generalization with a subject-specific model in Figure 15. We observe that the learned model generalizes well except for the clothing in the bottom part of the images. We posit that there isn\u2019t sufficient coverage from multi-views in that area. However, an exemplar model learned for a specific subject is able to capture the details. We leave the reader with an open philosophical question as to whether we should think about generalization if we can learn an exemplar model for a given data distribution in a few seconds?"
        },
        {
            "heading": "4.5 Convergence Analysis",
            "text": "We study the convergence properties of our approach using 12 LLFF sequences [26] (Appendix A.2) and Shiny Dataset [44]. We show the plots in Figure 16 for model training in the first 10 epochs, i.e. from 60 seconds to 600 seconds. We observe that our model gets close to convergence in the first few seconds. Crucially, our approach obtains competitive results to prior work on the Shiny dataset within 60 seconds of training as compared to 64 hours for NeRF [27] on fullresolution and 24 \u2212 30 hours of training of NeX [44] on one-fourth resolution. We also study convergence using 24 sparse and unconstrained multi-view sequences (Appendix A.1). Training an epoch on these sequences roughly take 10 seconds because these are sparse. We observe that model gets close to the best performance in the first 10 seconds of training. The raw values for the plots are available in Appendix A.4.\nWe contrast the performance of naive composition using depth ordering with neural pixel composition for unseen temporal sequences. We observe that neural composition allows us to generate more realistic views in contrast to the naive composition."
        },
        {
            "heading": "5 4D Multi-View View Synthesis",
            "text": "We study the ability of our approach to perform 4D view synthesis. We train our model on the temporal sequences (1920\u00d71080 resolution) from the Open4D dataset and contrast our approach with their method [3]. Figure 17 shows different things that we can do using our approach without any modification.\nOpen4D computes foreground and background images, and trains a modified U-Net model [34] for composition. The foreground image is computed by a naive composition of pixels from multi-views using depth ordering (as shown on the left side in Fig 18). The background image is computed by averaging foreground images for various time instances. We conduct two experiments: (1) held-out temporal sequences; and (2) held-out camera views. Held-out temporal sequences: In the first experiment, we study the performance of the trained model on unseen temporal sequences. We train the model without temporal constraint. Our goal is to study the compositional ability of our model in contrast to the more explicit Open4D. The model is trained with multi-views available for 300\u2212 400 time instances and evaluated on unseen 100\ntime instances. Table 7 contrasts the performance of our approach with Open4D. Quantitatively, we observe similar performance of our approach as compared to Open4D on unseen temporal sequences. We observe better qualitative results as shown in Figure 19. Our approach is able to capture details such as human faces consistently better than Open4D. Crucially, our approach does not require\nexplicit foreground-background modeling and can work with arbitrary temporal sequences. The details of the sequences are in Sec B.1.\nHeld-out camera views: In the second experiment, we study the performance on unseen camera views but a known temporal sequence. We train the model for 500 time instances with and without temporal constraint to understand its importance. Table 8 contrasts the performance of our approach with Open4D. Without any heuristics and foreground-background estimation, we are able to learn a representation that allows 4D view synthesis. Our approach use a simple reconstruction loss whereas Open4D use an additional adversarial loss [9]. Using the adversarial loss enables Open4D to generate overall sharp results that leads to lower LPIPS score. We contrast our approach with Open4D in Figure 20. Once again, we observe that our approach is able to capture details (facial and body details) better than Open4D. Finally, incorporating temporal constraint as the input to the model further improves performance. The details of the sequences are in Sec B.2."
        },
        {
            "heading": "6 3D reconstruction and Depth from Multi-Views",
            "text": "We use the learned MLPs to construct depth map for a given view. Given an array of depth values for a pixel, we select the depth value corresponding to the maximum \u03b1i value. Figure 21 shows the depth map for images from various sequences. We do not have ground truth depth values for these sequences.\nMultiple stereo pairs also provide us with dense 3D point clouds. However, correspondences can still be noisy, and using them with noisy camera parameters leads to poor 3D estimates. We observe that the learned MLP enables us to select good 3D points per view that can be accumulated across multi-views to obtain a dense 3D reconstruction. For each pixel, we take the top-3 \u03b1i values and check if the corresponding di values are in the vicinity of each other (this is done by empirically selecting a distance threshold). If they are, then we select the 3D point from a stereo pair corresponding to the maximum \u03b1i value. The process is repeated for all the pixels in the available multi-views. We show the results of 3D reconstruction using our approach in Fig 22 for sparse multi-view sequences. COLMAP [35,36] struggle to achieve dense 3D reconstruction on these sequences."
        },
        {
            "heading": "7 Discussion",
            "text": "We propose a novel approach for continuous 3D-4D view synthesis from sparse and wide-baseline multi-view observations. Leveraging a rich pixel representation that consists of color, depth, and uncertainty information leads to a high performing view-synthesis approach that generalizes well to novel views and unseen time instances. Our approach can be trained within few minutes from scratch utilizing as few as 1GB of GPU memory. In this work, we strive to provide an extensive analysis of our approach in contrast to existing methods on a wide\nvariety of settings. Importantly, our method works well on numerous settings without incorporating any task-specific or sequence-specific knowledge. We see our approach as a first step towards more efficient and general neural rendering techniques via the explicit use of geometric information and hope that it will inspire follow-up work in this exciting field.\nNote to reader: We suggest the reader to see our project page and attached videos for more results and analysis.\nAcknowledgements: AB would like to thank David Forsyth, Deva Ramanan, Minh Vo, and Srinivasa Narasimhan for many wonderful discussions on 3D-4D view synthesis. Many comments and insights from David Forsyth were extremely helpful in designing this work.\nDisclaimer: This academic article may contain images and/or data from sources that are not affiliated with the article submitter. Inclusion should not be construed as approval, endorsement or sponsorship of the submitter, article or its content by any such party."
        },
        {
            "heading": "A.1 Sparse and Unconstrained Multi-Views",
            "text": "We use 24 time instants from multi-view temporal sequences from the Open4D dataset [3]. The dynamic scenes are captured by a varying number of cameras in these sequences. The number of views vary from 7 to 11. We use one held-out view (or camera) for evaluation. Following is the setup for this analysis:"
        },
        {
            "heading": "Sequences",
            "text": "WFD-01: 6 time-stamps - {2000, 2500, 3000, 3500, 4000, 4500}. Test CAM-ID: {2, 9, 2, 2, 6, 4}. WFD-02: 5 time-stamps - {1900, 3000, 3500, 4000, 4500}. Test CAM-ID: {3, 6, 4, 2, 3}. JiuJitsu: 7 time-stamps - {3000, 3500, 4000, 4500, 5000, 5500, 6000}. Test CAM-ID: {5, 4, 9, 5, 7, 11, 1}. Gangnam: 3 time-stamps - {0200, 0300, 0900}. Test CAM-ID: {4, 4, 4}. Jumping: 3 time-stamps - {0200, 0300, 0400}. Test CAM-ID: {0, 0, 0}."
        },
        {
            "heading": "A.2 Hi-Resolution View Synthesis",
            "text": "We use the following 12 sequences from LLFF dataset [26] for this analysis:\nSequences: airplants, data2 apeskeleton, data2 benchflower, data2 bridgecar, data2 chesstable, data2 colorfountain, data2 colorspout, data2 redtoyota, data3 ninjabike, data4 colinepiano, data5 piano, pond. Test IDs: For each sequence, we held-out every 8th frame for evaluation."
        },
        {
            "heading": "A.3 Unbounded Views and Varying Number of Views",
            "text": "We use the following 13 synthetic multi-view sequences for this analysis from the MVS-Synth dataset [13]:\nSequence IDs: {0000, 0001, 0002, 0003, 0004, 0005, 0006, 0007, 0008, 0009, 0010, 0011, 0012}. For each sequence, we held-out every other frame for evaluation: Test IDs: {000:002:098}. Train IDs: 10 views: {003, 013, 023, 033, 043, 053, 063, 073, 083, 093}. 20 views: {003, 009, 013, 019, 023, 029, 033, 039, 043, 049, 053, 059, 063, 069, 073, 079, 083, 089, 093, 099}. 30 views: {003, 007, 009, 013, 017, 019, 023, 027, 029, 033, 037, 039, 043, 047, 049, 053, 057, 059, 063, 067, 069, 073, 077, 079, 083, 087, 089, 093, 097, 099}.\n40 views: {001, 003, 007, 009, 011, 013, 017, 019, 021, 023, 027, 029, 031, 033, 037, 039, 041, 043, 047, 049, 051, 053, 057, 059, 061, 063, 067, 069, 071, 073, 077, 079, 081, 083, 087, 089, 091, 093, 097, 099}. 50 views: {001:002:099}."
        },
        {
            "heading": "A.4 Convergence Analysis",
            "text": "In this section, we provide the raw data used in Sec 4.5. We use 24 sparse and unconstrained multi-view sequences (Sec A.1) from Open4D [3]. Training an epoch on these sequences roughly take 10 seconds because these are sparse. Table 9 shows the performance of our model for 10 epochs (from 10 seconds to roughly 2 minutes). We also use two hi-res (12 MP) datasets for these analysis: (1) 12 sequences (Sec A.2) from LLFF dataset [26]; and (2) 8 sequences from Shiny dataset [44]. We compute the performance of the models for the first 10 epochs, i.e., from 60 to 600 seconds of training. We follow the three settings (as in Sec 4.2) where we vary the number of stereo-pairs (K) and number of 3D points (N): (1) (K = 50, N = 50); (2) (K = 100, N = 100); and (3) (K = 200, N = 200). Table 10, Table 11, and Table 12 shows the performance for 12 sequences from LLFF. Table 13, Table 14, and Table 15 shows the performance for 8 sequences from the Shiny dataset. We observe that our approach gets close to convergence within the first 60 seconds of training in all the settings.\nB 4D View Synthesis\nWe use temporal sequences from Open4D dataset [3] for these analysis."
        },
        {
            "heading": "B.1 Unseeen Temporal Sequences",
            "text": "We use all the available views of the following 5 publicly available temporal sequences."
        },
        {
            "heading": "Sequences",
            "text": "WFD-01: Training - {0011:0411}. Testing - {0412:0511}. WFD-02: Training - {0400:0800}. Testing - {0801:0900}. JiuJitsu: Training - {0001:0400}. Testing - {0401:0500}. Gangnam: Training - {0100:0400}. Testing - {0401:0500}. Birds: Training - {0309:0709}. Testing - {0710:0809}."
        },
        {
            "heading": "B.2 Held-out Camera Views",
            "text": "We held-out one camera view from the following 5 publicly available temporal sequences."
        },
        {
            "heading": "Sequences",
            "text": "WFD-01: time - {0011:0511}. Test CAM-ID: {4}. WFD-02: time - {0400:0900}. Test CAM-ID: {4}. JiuJitsu: time - {0001:0500}. Test CAM-ID: {0}. Gangnam: time - {0100:0500}. Test CAM-ID: {4}. Birds: time - {0309:0809}. Test CAM-ID: {7}."
        },
        {
            "heading": "C More Analysis",
            "text": "We run more analysis on our model for various settings and study their impact on performance of our approach. In these experiments, we train the model for 10 epochs using LLFF-12 sequences (Sec A.2) and Shiny Dataset [44], and we use K = 50 stereo-pairs and N = 50 3D points. We also use 24 sparse and unconstrained sequences from Open4D (Sec A.1). Number of Filters: We vary the number of filters in our MLP model, nf = {16, 32.64, 128, 256, 512}. Our default setting is nf = 256. Table 17 shows the performance for Open4D-24 sequences, LLFF-12 sequences and Shiny dataset.\nThe performance improves as we increase the number of filters. The use of nf = 256 is a good balance between performance and size of model. We also observe that we can make extremely compact model at the loss of slight performance. Number of Layers: We vary the number of layers in our MLP model, nl = {1, 2, 3, 4, 5, 6}. Our default setting is nl = 5. Table 18 shows the performance for Open4D-24 sequences, LLFF-12 sequences and Shiny dataset respectively. Influence of Gamma: We use \u03b3 as a correction term that helps us to obtain sharp outputs. Table 16 (first row)) shows the performance for Open4D-24 sequences, LLFF-12 sequences and Shiny dataset. We observe that the additional \u03b3 term helps in inpainting the missing information. Influence of Spatial Information: The second row in Table 16 shows the performance of our approach without using spatial information as an input to MLP. We observe that using spatial information enables us to provide smooth outputs and better inpaints missing information. Influence of Uncertainty/Entropy: The third row in Table 16 shows the performance of our approach without using the uncertainty of the depth estimates (H). Using uncertainty provides slightly better performance. Direct MLP: Finally, we observe the benefits of using depth explicitly in computing \u03b1 to do a proper color composition. The fourth row in Table 16 shows the performance for Open4D-24 sequences, LLFF-12 sequences and Shiny dataset. We observe that using depth explicitly allows to do better view synthesis."
        }
    ],
    "title": "Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views",
    "year": 2022
}