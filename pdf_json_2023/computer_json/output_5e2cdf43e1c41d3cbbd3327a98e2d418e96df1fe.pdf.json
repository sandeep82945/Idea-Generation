{
    "abstractText": "Reliable multi-agent trajectory prediction is crucial for the safe planning and control of autonomous systems. Compared with single-agent cases, the major challenge in simultaneously processing multiple agents lies in modeling complex social interactions caused by various driving intentions and road conditions. Previous methods typically leverage graph-based message propagation or attention mechanism to encapsulate such interactions in the format of marginal probabilistic distributions. However, it is inherently sub-optimal. In this paper, we propose IPCCTP, a novel relevance-aware module based on Incremental Pearson Correlation Coefficient to improve multi-agent interaction modeling. IPCC-TP learns pairwise joint Gaussian Distributions through the tightly-coupled estimation of the means and covariances according to interactive incremental movements. Our module can be conveniently embedded into existing multi-agent prediction methods to extend original motion distribution decoders. Extensive experiments on nuScenes and Argoverse 2 datasets demonstrate that IPCC-TP improves the performance of baselines by a large margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dekai Zhu"
        },
        {
            "affiliations": [],
            "name": "Guangyao Zhai"
        },
        {
            "affiliations": [],
            "name": "Yan Di"
        },
        {
            "affiliations": [],
            "name": "Fabian Manhardt"
        },
        {
            "affiliations": [],
            "name": "Hendrik Berkemeyer"
        },
        {
            "affiliations": [],
            "name": "Tuan Tran"
        },
        {
            "affiliations": [],
            "name": "Nassir Navab"
        },
        {
            "affiliations": [],
            "name": "Federico Tombari"
        },
        {
            "affiliations": [],
            "name": "Benjamin Busam"
        }
    ],
    "id": "SP:98caac1eac5136e7c057078e8f2fbe537c8d1522",
    "references": [
        {
            "authors": [
                "Alexandre Alahi",
                "Kratarth Goel",
                "Vignesh Ramanathan",
                "Alexandre Robicquet",
                "Li Fei-Fei",
                "Silvio Savarese"
            ],
            "title": "Social lstm: Human trajectory prediction in crowded spaces",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Luke Vilnis",
                "Oriol Vinyals",
                "Andrew M Dai",
                "Rafal Jozefowicz",
                "Samy Bengio"
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "In 20th SIGNLL Conference on Computational Natural Language Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Sergio Casas",
                "Cole Gulino",
                "Renjie Liao",
                "Raquel Urtasun"
            ],
            "title": "Spagnn: Spatially-aware graph neural networks for relational behavior forecasting from sensor data",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Yuning Chai",
                "Benjamin Sapp",
                "Mayank Bansal",
                "Dragomir Anguelov"
            ],
            "title": "Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "INTERPRET Challenge"
            ],
            "title": "http : / / challenge",
            "venue": "interaction-dataset.com",
            "year": 2021
        },
        {
            "authors": [
                "Ming-Fang Chang",
                "John Lambert",
                "Patsorn Sangkloy",
                "Jagjeet Singh",
                "Slawomir Bak",
                "Andrew Hartnett",
                "De Wang",
                "Peter Carr",
                "Simon Lucey",
                "Deva Ramanan"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "In NIPS 2014 Workshop on Deep Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Henggang Cui",
                "Vladan Radosavljevic",
                "Fang-Chieh Chou",
                "Tsung-Han Lin",
                "Thi Nguyen",
                "Tzu-Kuo Huang",
                "Jeff Schneider",
                "Nemanja Djuric"
            ],
            "title": "Multimodal trajectory predictions for autonomous driving using deep convolutional networks",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Proceedings of NAACL-HLT,",
            "year": 2019
        },
        {
            "authors": [
                "Yan Di",
                "Ruida Zhang",
                "Zhiqiang Lou",
                "Fabian Manhardt",
                "Xiangyang Ji",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "title": "Gpv-pose: Category-level object pose estimation via geometry-guided point-wise voting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Liangji Fang",
                "Qinhong Jiang",
                "Jianping Shi",
                "Bolei Zhou"
            ],
            "title": "Tpnet: Trajectory proposal network for motion prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jiyang Gao",
                "Chen Sun",
                "Hang Zhao",
                "Yi Shen",
                "Dragomir Anguelov",
                "Congcong Li",
                "Cordelia Schmid"
            ],
            "title": "Vectornet: Encoding hd maps and agent dynamics from vectorized representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Roger Girgis",
                "Florian Golemo",
                "Felipe Codevilla",
                "Martin Weiss",
                "Jim Aldon D\u2019Souza",
                "Samira Ebrahimi Kahou",
                "Felix Heide",
                "Christopher Pal"
            ],
            "title": "Latent variable sequential set transformers for joint multi-agent motion prediction",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Junru Gu",
                "Chen Sun",
                "Hang Zhao"
            ],
            "title": "Densetnt: End-to-end trajectory prediction from dense goal sets",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "DE Hilt",
                "DE Seegrist"
            ],
            "title": "Ridge: a computer program for calculating ridge regression estimates",
            "venue": "USDA Forest Service Research Note NE (USA). no. 236.",
            "year": 1977
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Nikolay Jetchev",
                "Marc Toussaint"
            ],
            "title": "Trajectory prediction: learning to map situations to robot trajectories",
            "venue": "In Proceedings of the 26th annual international conference on machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Xin Kong",
                "Xuemeng Yang",
                "Guangyao Zhai",
                "Xiangrui Zhao",
                "Xianfang Zeng",
                "Mengmeng Wang",
                "Yong Liu",
                "Wanlong Li",
                "Feng Wen"
            ],
            "title": "Semantic graph based place recognition for 3d point clouds",
            "venue": "In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Parth Kothari",
                "Sven Kreiss",
                "Alexandre Alahi"
            ],
            "title": "Human trajectory forecasting in crowds: A deep learning perspective",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alon Lerner",
                "Yiorgos Chrysanthou",
                "Dani Lischinski"
            ],
            "title": "Crowds by example",
            "venue": "In Computer graphics forum,",
            "year": 2007
        },
        {
            "authors": [
                "Yujia Li",
                "Richard Zemel",
                "Marc Brockschmidt",
                "Daniel Tarlow"
            ],
            "title": "Gated graph sequence neural networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Ming Liang",
                "Bin Yang",
                "Rui Hu",
                "Yun Chen",
                "Renjie Liao",
                "Song Feng",
                "Raquel Urtasun"
            ],
            "title": "Learning lane graph representations for motion forecasting",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Liang Liu",
                "Guangyao Zhai",
                "Wenlong Ye",
                "Yong Liu"
            ],
            "title": "Unsupervised learning of scene flow estimation fusing with local rigidity",
            "venue": "In IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jean Mercat",
                "Thomas Gilles",
                "Nicole El Zoghby",
                "Guillaume Sandou",
                "Dominique Beauvois",
                "Guillermo Pita Gil"
            ],
            "title": "Multi-head attention for multi-modal joint vehicle motion forecasting",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Jiquan Ngiam",
                "Vijay Vasudevan",
                "Benjamin Caine",
                "Zhengdong Zhang",
                "Hao-Tien Lewis Chiang",
                "Jeffrey Ling",
                "Rebecca Roelofs",
                "Alex Bewley",
                "Chenxi Liu",
                "Ashish Venugopal"
            ],
            "title": "Scene transformer: A unified architecture for predicting future trajectories of multiple agents",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Stefano Pellegrini",
                "Andreas Ess",
                "Konrad Schindler",
                "Luc Van Gool"
            ],
            "title": "You\u2019ll never walk alone: Modeling social behavior for multi-target tracking",
            "venue": "IEEE 12th international conference on computer vision,",
            "year": 2009
        },
        {
            "authors": [
                "Tung Phan-Minh",
                "Elena Corina Grigore",
                "Freddy A Boulton",
                "Oscar Beijbom",
                "Eric M Wolff"
            ],
            "title": "Covernet: Multimodal behavior prediction using trajectory sets",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Christoph R\u00f6smann",
                "Malte Oeljeklaus",
                "Frank Hoffmann",
                "Torsten Bertram"
            ],
            "title": "Online trajectory prediction and planning for social robot navigation",
            "venue": "IEEE International Conference on Advanced Intelligent Mechatronics (AIM),",
            "year": 2017
        },
        {
            "authors": [
                "Tim Salzmann",
                "Boris Ivanovic",
                "Punarjay Chakravarty",
                "Marco Pavone"
            ],
            "title": "Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Yongzhi Su",
                "Yan Di",
                "Guangyao Zhai",
                "Fabian Manhardt",
                "Jason Rambach",
                "Benjamin Busam",
                "Didier Stricker",
                "Federico Tombari"
            ],
            "title": "Opa-3d: Occlusion-aware pixel-wise aggregation for monocular 3d object detection",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Qiao Sun",
                "Xin Huang",
                "Junru Gu",
                "Brian C Williams",
                "Hang Zhao"
            ],
            "title": "M2i: From factored marginal trajectory prediction to interactive prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Bohan Tang",
                "Yiqi Zhong",
                "Ulrich Neumann",
                "Gang Wang",
                "Siheng Chen",
                "Ya Zhang"
            ],
            "title": "Collaborative uncertainty in multi-agent trajectory forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Benjamin Wilson",
                "William Qi",
                "Tanmay Agarwal",
                "John Lambert",
                "Jagjeet Singh",
                "Siddhesh Khandelwal",
                "Bowen Pan",
                "Ratnesh Kumar",
                "Andrew Hartnett",
                "Jhony Kaesemodel Pontes",
                "Deva Ramanan",
                "Peter Carr",
                "James Hays"
            ],
            "title": "Argoverse 2: Next generation datasets for self-driving perception and forecasting",
            "venue": "In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Ya Wu",
                "Ariyan Bighashdel",
                "Guang Chen",
                "Gijs Dubbelman",
                "Pavol Jancura"
            ],
            "title": "Continual pedestrian trajectory learning with social generative replay",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Ye Yuan",
                "Xinshuo Weng",
                "Yanglan Ou",
                "Kris M Kitani"
            ],
            "title": "Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Guangyao Zhai",
                "Dianye Huang",
                "Shun-Cheng Wu",
                "HyunJun Jung",
                "Yan Di",
                "Fabian Manhardt",
                "Federico Tombari",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "title": "Monograspnet: 6-dof grasping with a single rgb image",
            "venue": "In IEEE International Conference on Robotics and Automation",
            "year": 2023
        },
        {
            "authors": [
                "Guangyao Zhai",
                "Xin Kong",
                "Jinhao Cui",
                "Yong Liu",
                "Zhen Yang"
            ],
            "title": "Flowmot: 3d multi-object tracking by scene flow association",
            "venue": "arXiv preprint arXiv:2012.07541,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Zhan",
                "Liting Sun",
                "Di Wang",
                "Haojie Shi",
                "Aubrey Clausse",
                "Maximilian Naumann",
                "Julius K\u00fcmmerle",
                "Hendrik K\u00f6nigshof",
                "Christoph Stiller",
                "Arnaud de La Fortelle",
                "Masayoshi Tomizuka"
            ],
            "title": "INTERACTION Dataset: An IN- TERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps",
            "year": 2019
        },
        {
            "authors": [
                "Chenyangguang Zhang",
                "Zhiqiang Lou",
                "Yan Di",
                "Federico Tombari",
                "Xiangyang Ji"
            ],
            "title": "Sst: Real-time end-to-end monocular 3d reconstruction via sparse spatial-temporal guidance",
            "venue": "arXiv preprint arXiv:2212.06524,",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Zhang",
                "Jiaqing Yan",
                "Xin Kong",
                "Guangyao Zhai",
                "Yong Liu"
            ],
            "title": "Efficient motion planning based on kinodynamic model for quadruped robots following persons in confined spaces",
            "venue": "IEEE/ASME Transactions on Mechatronics,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Trajectory prediction refers to predicting the future trajectories of one or several target agents based on past trajectories and road conditions. It is an essential and emerging subtask in autonomous driving [23,28,37,45] and industrial robotics [21, 35, 48].\nPrevious methods [7, 15, 18, 33, 49] concentrate on Single-agent Trajectory Prediction (STP), which leverages past trajectories of other agents and surrounding road conditions as additional cues to assist ego-motion estimation. De-\n\u2217 Equal contribution. \u2020 Corresponding author."
        },
        {
            "heading": "Means",
            "text": "spite the significant progress made in recent years, its application is limited. The reason is that simultaneously processing all traffic participants remains unreachable, although it is a common requirement for safe autonomous driving.\nA straightforward idea to deal with this issue is to directly utilize STP methods on each agent respectively and finally attain a joint collision-free prediction via pairwise collision detection. Such methods are far from reliably handling Multi-agent Trajectory Prediction (MTP) since the search space for collision-free trajectories grows exponentially as the number of agents increases, i.e., N agents with M modes yield MN possible combinations, making such search strategy infeasible with numerous agents. By in-\nar X\niv :2\n30 3.\n00 57\n5v 4\n[ cs\n.C V\n] 3\n0 A\ncorporating graph-based message propagation or attention mechanism into the framework to model the future interactions among multiple agents, [17,31,43] aggregate all available cues to simultaneously predict the trajectories for multiple agents in the format of a set of marginal probability distributions. While these frameworks improve the original STP baseline to some extent, the estimation based on a set of marginal distributions is inherently sub-optimal [34]. To address this issue, a predictor that can predict a joint probability distribution for all agents in the scene is necessary. An intuitive strategy for designing such a predictor is to directly predict all parameters for means and covariance matrices, which define the joint probability distributions [39]. To predict a trajectory of T time-steps with N agents and M modes up to 4N2TM covariance parameters are needed. However, such redundant parameterization fails to analyze the physical meaning of covariance parameters and also makes it hard to control the invertibility of the covariance matrix.\nIn this paper, we take a further step to explicitly interpret the physical meanings of the individual variances and Pearson Correlation Coefficients (PCC) inside the covariance matrix. Instead of directly modeling the locations of agents at each sampled time, we design a novel movementbased method which we term Incremental PCC (IPCC). It helps to simulate the interactions among agents during driving. IPCC models the increment from the current time to a future state at a specific step. Thereby individual variances capture the local uncertainty of each agent in the Bird-EyeView without considering social interactions (Figure 1.a), while IPCC indicates the implicit driving policies of two agents, e.g., one agent follows or yields to another agent or the two agents\u2019 motions are irrelevant (Figure 1.b). Compared to position-based modeling, IPCC has two advantages. First, IPCC models social interactions during driving in a compact manner. Second, since position-based modeling requires respective processes along the two axes, IPCC can directly model the motion vector, which reduces memory cost for covariance matrix storage by a factor of four and facilitates deployment on mobile platforms, such as autonomous vehicles and service robots.\nBased on IPCC, we design and implement IPCC-TP, a module for the MTP task. IPCC-TP can be conveniently embedded into existing methods by extending the original motion decoders to boost their performance. Experiments show that IPCC-TP effectively captures the pairwise relevance of agents and predicts scene-compliant trajectories.\nIn summary, the main contributions of this work are threefold:\n\u2022 We introduce a novel movement-based method, namely IPCC, that can intuitively reveal the physical meaning of pairwise motion relevance and facilitate deployment by reducing memory cost.\n\u2022 Based on IPCC, we propose IPCC-TP, which is compatible with state-of-the-art MTP methods and possess the ability to model future interactions with joint Gaussian distributions among multiple agents.\n\u2022 Experiment results show that methods enhanced with IPCC-TP outperform the original methods by a large margin and become the new state-of-the-art methods."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we first briefly review the general frameworks of learning-based trajectory prediction and then introduce the literature closely related to our approach. General trajectory prediction architecture. Trajectory prediction for moving agents is important for the safe planning of autonomous driving [11, 16, 43]. One branch of trajectory prediction consists of the approaches [2, 6, 7, 15, 18, 27, 30, 33, 36, 49] based on typical neural networks (CNN, GNN [13, 22, 26], RNN [10, 20]). Cui et al. [11] propose to use agents\u2019 states and features extracted by CNN from Bird-Eye-View images to generate a multimodal prediction. Similarly, CoverNet [33] uses CNN to encode the scene context while it proposes dozens of trajectory anchors based on the bicycle model and converts prediction from position regression to anchor classification. A hierarchical GNN named VectorNet [16] encapsulates the sequential features of map elements and past trajectories with instancewise subgraphs and models interactions with a global graph. TNT [49] and DenseTNT [18] are extensions of [16] that focus on predicting reasonable destinations. Social LSTM [2] models the trajectories of individual agents from separate LSTM networks and aggregates the LSTM hidden cues to model their interactions. CL-SGR [42] considers the sample replay model in a continuous trajectory prediction scenario setting to avoid catastrophic forgetting. The other branch [17,31,43] models the interaction among the agents based on the attention mechanism. They work with the help of Transformer [40], which achieves huge success in the fields of natural language processing [12] and computer vision [5, 14, 29, 44, 47]. Scene Transformer [31] mainly consists of attention layers, including self-attention layers that encode sequential features on the temporal dimension, self-attention layers that capture interactions on the social dimension between traffic participants, and cross-attention layers that learn compliance with traffic rules. Prediction of multi-agent interaction. Most aforementioned methods are proposed for the STP task. Recently, more work has been concentrated on the MTP task. M2I [38] uses heuristic-based labeled data to train a module for classifying the interacting agents as pairs of influencers and reactors. M2I first predicts a marginal trajectory for the influencer and then a conditional trajectory for the reactor. But M2I can only deal with interactive scenes consisting\nof two interacting agents. Models such as [17, 31, 43] are proposed for general multi-agent prediction. Scene Transformer [31] and AutoBots [17] share a similar architecture, while the latter employs learnable seed parameters in the decoder to accelerate inference. AgentFormer [43] leverages the sequential representation of multi-agent trajectories by flattening the state features across agents and time. Although these methods utilize attention layers intensively in their decoders to model future interactions and achieve state-of-the-art performance, their predictions are suboptimal since the final prediction is not a joint probability distribution between agents but only consists of marginal probability distributions. To the best of our knowledge, [39] is the only work that describes the future movement of all agents with a joint probability distribution. This work investigates the collaborative uncertainty of future interactions, mathematically speaking, the covariance matrix per time step. They use MLP decoders to predict mean values and the inverse of covariance matrices directly. We refer to [39] as CUM (Collaborative Uncertainty based Module) for convenience and provide methodological and quantitative comparisons between CUM and our module in Sec. 3 and 4."
        },
        {
            "heading": "3. Methodology",
            "text": "In this section, we first introduce the background knowledge of MTP. Then we propose the IPCC method, which models the relevance of the increments of target agents. Based on this, we design IPCC-TP and illustrate how this module can be plugged into the state-of-the-art models [17,43]. In the end, we compare our module with CUM."
        },
        {
            "heading": "3.1. Preliminary",
            "text": "Problem Statement. Given the road map G and N agents with trajectories of past Tobs steps, MTP is formulated as predicting future trajectories of all agents for T steps. For observed time steps t \u2208 (\u2212Tobs, 0], the past states at t step are Ht = {Hti|i = 1, ..., N}, where Hti contains the state information, i.e. {x, y} position of agent i at this time step. And the past trajectories are denoted as H = {Ht|\u2212Tobs < t \u2264 0}. Similarly, for future time steps t \u2208 (0, T ], the future states at step t are Ft = {F ti |i = 1, ..., N}, where F ti contains {x, y} position of agent i at this step. And the future trajectories are F = {Ft|0 < t \u2264 T}. Considering that a specific current state could develop to multiple interaction patterns, most MTP methods predict M interaction modes in the future. For simplicity, we only analyze the cases of M = 1 in this section and our method can be conveniently extended to predict multiple modes. Marginal Probability Modeling. In learning-based MTP methods, the training objective is to find the optimal parameter \u03b8\u2217 to maximize the Gaussian likelihood P :\n\u03b8\u2217 = argmax \u03b8 P (F | H,G, \u03b8) . (1)\nThereby the core of MTP lies in constructing an effective joint probability distribution P to model the generative future trajectories of all agents. For simplicity, existing methods such as AutoBots [17] and AgentFormer [43] approximate the high-dimensional multivariate distribution P = {P ti } in Eq. (1) by aggregating multiple marginal probability distributions. Each distribution P ti describes ith agent\u2019s position at a specific time t. The objective function Eq. (1) is then transformed as:\n\u03b8\u2217 = argmax \u03b8 T\u2211 t=1 N\u2211 i=1 P ti ( F ti | H,G, \u03b8 ) , (2)\nwhere the marginal prediction P ti \u223c N (\u00b5ti,\u03a3ti) is a 2D Gaussian distribution defined by:\n\u00b5ti = [\u00b5 tx i , \u00b5 ty i ]\n\u03a3ti =  (\u03c3txi )2 \u03c1txyi \u03c3txi \u03c3tyi \u03c1tyxi \u03c3 ty i \u03c3 tx i (\u03c3 ty i ) 2  , (3)\nwhere \u00b5ti denotes the mean position of the i-th agent at time step t, \u03a3ti is its corresponding covariance matrix. \u03c1 txy i , \u03c1 tyx i in \u03a3ti are the PCCs of agent i on x, y axis respectively.\nHowever, such marginal approximation is inherently sub-optimal [34], resulting in unsatisfactory results on realworld benchmarks [4,9,24,25,32]. Although existing methods typically employ attention-based mechanism to help each agent attends to its interactive counterparts, social interactions in motion between different pairs of agents is in fact not explicitly analyzed and modeled, which limits the capability of networks to predict scene compliant and collision-free trajectories."
        },
        {
            "heading": "3.2. PCC: Position Correlation Modeling",
            "text": "The joint probability distribution among multiple agents in the scene contains correlations between the motions of different agents, which are essential parts of precisely modeling future interactions. Theoretically, using such a scenelevel modeling method would lead to a more optimal estimation of future interactions [34] than stacking a set of marginal probability distributions to approximate the joint distribution.\nFor time step t, the joint prediction among agents in the scene F\u0302t = Pt (\u00b7 | H,M, \u03b8) \u223c N (Mt,\u03a3t) is defined by:\nMt = [ \u00b5t1, . . . , \u00b5 t N ] , \u00b5ti = [ \u00b5txi , \u00b5 ty i ] , (4)\n\u03a3t =  \u03c3 t 11 \u00b7 \u00b7 \u00b7 \u03c3t1N ... . . . ...\n\u03c3tN1 \u00b7 \u00b7 \u00b7 \u03c3tNN\n ,\n\u03c3tij =  \u03c1txxij \u03c3txi \u03c3txj \u03c1txyij \u03c3txi \u03c3tyj \u03c1tyxij \u03c3 ty i \u03c3 tx j \u03c1 tyy ij \u03c3 ty i \u03c3 ty j  , (5)\nwhere Mt \u2208 R2N contains the mean positions of all agents, and \u03a3t \u2208 R2N\u00d72N is the corresponding covariance matrix, and \u03c1txxij = \u03c1 tyy ij = 1 when i = j. The objective function Eq. (1) then turns into:\n\u03b8\u2217 = argmax \u03b8 T\u2211 t=1 Pt (Ft | H,G, \u03b8) . (6)\nThereby, a straightforward strategy for designing a predictor based on such joint probability modeling is to directly predict all parameters in means and covariance matrices. However, such redundant parameterization still fails to capture the interaction patterns in motion between agents since PCCs inside the covariance matrix reflect the agent-to-agent position correlations rather than motion correlations. Moreover, such strategy also makes it hard to control the invertibility of the covariance matrix."
        },
        {
            "heading": "3.3. IPCC: Motion Interaction Modeling.",
            "text": "Since the joint Gaussian distribution defined by Eq. (4) and Eq. (5) is not feasible in real applications, we propose a novel motion interaction modeling method, IPCC. For Eq. (5), we observe that each pair of agents requires estimations of PCCs: {\u03c1txxij , \u03c1 txy ij , \u03c1 tyx ij , \u03c1 tyy ij }. These representations are redundant since they respectively model the\npairwise position correlation between agent i and j along the 2D axis from the bird-eye view. In the following part, we demonstrate that this correlation can be merged into a single motion-aware parameter IPCC by modeling the onedimensional increments from current time step (t=0) to any future time step. From PCC to IPCC. For future time step t > 0, we predict the increments F\u0302 t\u2206 as shown in Fig. 2, which is the onedimensional displacements with respect to the current positions M0, for all agents in the scene. F\u0302 t\u2206 \u223c N (M t\u2206,\u03a3t\u2206) is defined by:\nM t\u2206 = [ \u00b5t\u03b41 , . . . , \u00b5 t\u03b4 N ] , \u00b5t\u03b4i \u2208 R+0 (7)\n\u03a3t\u2206 = P t \u2206\u2299  ( \u03c3t\u03b41 )2 . . . \u03c3t\u03b41 \u03c3 t\u03b4 N ... . . .\n... \u03c3t\u03b4N\u03c3 t\u03b4 1 \u00b7 \u00b7 \u00b7 ( \u03c3t\u03b4N\n)2  ,\nP t\u2206 =  1 . . . \u03c1 t\u03b4 1N ... . . .\n... \u03c1t\u03b4N1 . . . 1\n , (8)\nwhere P t\u2206,\u03a3 t \u2206 \u2208 RN\u00d7N . \u03a3t\u2206 denotes the covariance matrix and IPCC matrix P t\u2206 can be extracted from \u03a3 t \u2206 as in Eq. (8). P t\u2206 preserves all pairwise correlation parameters for the increments. \u2299 denotes element-wise multiplication. Thereby \u03c1t\u03b4ij \u2208 [\u22121,+1] naturally encapsulates the motion relevance between i-th and j-th agents, which conveys various interaction patterns: (1) The more it approaches +1, the more similar behaviours the two agents share, e.g., two vehicles drive on the same lane towards the same direction. (2) The more it approaches \u22121, the more one vehicle is likely to yield if the other one is driving aggressively, e.g., two vehicles are at a merge. (3) The closer it is to 0 the less two agents impact each other. Such explicit modeling of motion interactions enforces the network to effectively predict the trajectory of each agent by attending to different interaction patterns from other agents.\nSince we aim to build a plug-in module that can be conveniently embedded into other existing methods, we project the predicted IPCC parameters back to the original x, y axis. Therefore, the training losses of baselines can be directly inherited and extended for supervision. IPCC Projection. Suppose that for agent i, the projections of mean displacement \u00b5t\u03b4i on x- and y-axis, namely \u00b5 t\u03b4x i and \u00b5 t\u03b4y i , are already obtained, we can estimate the approximate yaw angle for agent i at step t via\n\u03b8ti = arctan(\u00b5 t\u03b4y i , \u00b5 t\u03b4x i ). (9)\nSimilarly, \u0398t = [\u03b8t1, . . . , \u03b8 t N ] at time t is available. The reconstructed future positions F\u0302\u2217t in the global coordinate\ncan be expressed as a sum of the current states Mt0 and increments. F\u0302\u2217t \u223c N (M\u2217t ,\u03a3\u2217t ) is defined by\nM\u2217t = C \u00b7M t\u2206r +Mt0 ,\nM t\u2206r = [ \u00b5t\u03b41 , \u00b5 t\u03b4 1 , . . . , \u00b5 t\u03b4 N , \u00b5 t\u03b4 N ] ,\nC = diag ([ cos \u03b8t1, sin \u03b8 t 1, . . . , cos \u03b8 t N , sin \u03b8 t N ]) , (10)\nand\n\u03a3\u2217t = C T\u03a3t\u2206rC =  \u03c3 t\u03b4 11 \u00b7 \u00b7 \u00b7 \u03c3t\u03b41N ... . . . ...\n\u03c3t\u03b4N1 \u00b7 \u00b7 \u00b7 \u03c3t\u03b4NN\n ,\n\u03a3t\u2206r =  ( \u03c3t\u03b41 )2 ( \u03c3t\u03b41 )2 . . . \u03c3t\u03b41 \u03c3 t\u03b4 N \u03c3 t\u03b4 1 \u03c3 t\u03b4 N( \u03c3t\u03b41 )2 ( \u03c3t\u03b41 )2 . . . \u03c3t\u03b41 \u03c3 t\u03b4 N \u03c3 t\u03b4 1 \u03c3 t\u03b4 N ... ... . . . ... ... \u03c3t\u03b4N\u03c3 t\u03b4 1 \u03c3 t\u03b4 N\u03c3 t\u03b4 1 \u00b7 \u00b7 \u00b7 ( \u03c3t\u03b4N )2 ( \u03c3t\u03b4N\n)2 \u03c3t\u03b4N\u03c3 t\u03b4 1 \u03c3 t\u03b4 N\u03c3 t\u03b4 1 \u00b7 \u00b7 \u00b7 ( \u03c3t\u03b4N )2 ( \u03c3t\u03b4N )2\n ,\n\u03c3t\u03b4ij = \u03c1 t\u03b4 ij\u03c3 t\u03b4 i \u03c3 t\u03b4 j  cos \u03b8ti cos \u03b8tj cos \u03b8ti sin \u03b8tj sin \u03b8ti cos \u03b8 t j sin \u03b8 t i sin \u03b8 t j  ,\n(11)\nwhere M t\u2206r and \u03a3 t \u2206r are the replicated augments of M t\u2206 and \u03a3t\u2206 respectively, where M t \u2206r\n\u2208 R2N and \u03a3t\u2206r , C \u2208 R2N\u00d72N . Note that when the approximate yaw angles \u0398t = [\u03b8 t 1, . . . , \u03b8 t N ] are equal to the actual yaw angles \u03a6t = [\u03d5 t 1, . . . , \u03d5 t N ], F\u0302\u2217t and F\u0302t are identical distributions, which leads to:\nM\u2217t = Mt, \u03a3 \u2217 t = \u03a3t, (12)\nand \u03c3tij = \u03c3 t\u03b4 ij . (13)\nAccording to Eq. (5), (11), (12), and (13), we obtain \u03c1txxij , \u03c1 txy ij , \u03c1 tyx ij , \u03c1\ntyy ij with the signum function sgn(\u00b7), \u03c1txxij \u03c1txyij\n\u03c1tyxij \u03c1 tyy ij  = \u03c1t\u03b4ij \u00b7sgn(  cos \u03b8ti cos \u03b8tj cos \u03b8ti sin \u03b8tj sin \u03b8ti cos \u03b8 t j sin\u03b8 t i sin \u03b8 t j ) (14) So far, we have shown that when the approximate yaw angles are equal to the actual yaw angles, \u03c1txxij , \u03c1 txy ij , \u03c1 tyx ij , \u03c1 tyy ij can be obtained via Eq. (14). For short-term trajectory prediction (no longer than 6s), the vehicles are unlikely to have a sharp turn in such a short time, thus the angle \u03b8ti based on the incremental movement is\nclose to the actual yaw angle \u03d5ti, and F\u0302\u2217t is a suitable approximation to F\u0302t."
        },
        {
            "heading": "3.4. IPCC-TP: a Plugin Module",
            "text": "Based on the IPCC method, we design and implement a plugin module named IPCC-TP, which can be easily inserted into existing models. The pipeline of IPCC-TP is shown in Figure. 3. Previous methods use spatial-temporal encoders to extract latent features L \u2208 RN\u00d7T\u00d7d, which are subsequently sent to MLP decoders to generate deterministic predictions [43] or marginal distributions [17] (marked with the sad face). In contrast, our IPCC-TP first sends the latent features Lt \u2208 RN\u00d7d at time step t to the Attention Block. This block is designated for modeling the future interactions via which the latent features Lt are mapped to the relevance-aware latent features Lrelt \u2208 RN\u00d7d. Considering PCC is in the range of [-1, 1], we compute pairwise motion relevance based on cosine similarity. In this way, if agent i and agent j share a higher motion relevance, their relevance-aware feature, Lrelit ,Lreljt \u2208 Rd, will have a higher cosine similarity. For future time step t > 0, IPCCTP computes the social cosine similarities as P t\u2206 in Eq. (8) for each pair of agents. Then with the approximated yaw angles \u0398t, P t\u2206 is used to predict the \u03c1 parameters in \u03a3 t according to Eq. (14). Once the \u03c1 parameters in \u03a3t are obtained, together with {\u00b5txi , \u00b5 ty i , \u03c3 tx i , \u03c3 ty i , \u03c1 txy i }, which are already available in the marginal probability distributions, we can extend the marginal probability distributions to a joint Gaussian distribution F\u0302t defined in Eq. (4) and Eq. (5). As shown in Figure. 2, the approximated yaw angles are computed via Eq. (9). In order to stabilize the training, we use Tikhonov Regularization [19] in IPCC-TP by adding a small value \u2206reg to the main diagonal of covariance matrices \u03a3t to obtain \u03a3 reg t ,\n\u03a3regt = \u03a3t +\u2206reg \u00b7 I, (15)\nwhere I \u2208 R2N\u00d72N , an identity matrix. Thereby we adopted the following loss functions for training, following [17, 43]. In the training of the IPCC-TP embedded models, we replace the negative log-likelihood (NLL) loss of [17, 43] with the scene-level NLL loss. The scene-level NLL loss is calculated as\nLNLL\u2212scene = T\u2211\nt=1\n0.5[ln(|\u03a3t|)+\n(Ft \u2212Mt)T\u03a3\u22121t (Ft \u2212Mt) + 2Nln(2\u03c0)].\n(16)\nThus the loss function for training the enhanced AgentFormer is\nLAg = Lkld + Lsample + LNLL\u2212scene, (17)\nwhere Lkld term is the Kullback-Leibler divergence for the CVAE [3] in AgentFormer, Lsample term is the mean square\nerror between the ground truth and the sample from the latent z distribution. And the loss function for training the enhanced AutoBots is\nLAb = LME + L\u2217kld + LNLL\u2212scene, (18)\nwhere LME is a mode entropy regularization term to penalize large entropy of the predicted distribution, L\u2217kld is the Kullback-Leibler divergence between the approximating posterior and the actual posterior."
        },
        {
            "heading": "3.5. Methodology Comparison to CUM",
            "text": "CUM [39] is another work that tries to predict the joint probabilistic distribution for the MTP task. Compared with our IPCC-TP, the main difference lies in the interpretation and implementation of the covariance matrix. In order to avoid the inverse calculation for covariance matrices \u03a3t, CUM uses MLP decoders to predict all the parameters in \u03a3\u22121t . In contrast, our module instead predicts \u03c1 parameters in P t\u2206, which can intuitively encapsulate the physical meaning of motion relevance in future interactions, and then predicts \u03a3t based on Eq. (14). Experiments demonstrate the superiority of IPCC-TP."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Expermental Settings",
            "text": "Datasets. We apply IPCC-TP to AgentFormer [43] and AutoBots [17] and evaluate the performance on two public datasets, nuScenes [4] and Argoverse 2 [41]. nuScenes collects 1K scenes with abundant annotations for the research of autonomous driving. For the prediction task, observable past trajectories are 2s long, and predicted future trajectories are 6s long. The sample rate is 2 Hz. Argoverse 2 contains 250K scenes in total. In each scene, observable past trajectories are 5s long, and predicted future trajectories are 6s long. The sample rate is 10 Hz. Considering the vast number of scenes in Argoverse 2, we randomly pick 8K scenes for training and 1K for testing. Implementation Details. For both AgentFormer and AutoBots, we use the official implementations as the baselines in our experiments. For IPCC-TP, we implement the attention block inside it with a self-attention layer followed by a 2-layer MLP. We set the Tikhonov regularization parameter \u2206reg as 1e-4 in the experiments. More details of the training are provided in the supplementary material. Metrics. For evaluation, considering that the widely used metrics minADE and minFDE are designed for the egomotion prediction in STP task, we choose to use Minimum Joint Average Displacement Error (minJointADE) and Minimum Joint Final Displacement Error (minJointFDE) in the multi-agent prediction task of INTERPRET Challenge [8, 46]. These two metrics are proposed specifically\nfor the MTP task; their calculation formula is listed in the supplementary material. According to the rule of challenges on the nuScenes and Argoverse 2 benchmarks, we set the number of modes M as 5 in the evaluation on nuScenes and 6 in the evaluation on Argoverse 2."
        },
        {
            "heading": "4.2. Training Details",
            "text": "AgentFormer. In the evaluation on nuScenes, we use the official pre-trained model as the baseline and the default hyperparameters for training the enhanced model. In the evaluation on Argoverse 2 dataset, since there are much more future steps and agents, we set the dimension as 24 and the number of heads as 4 in the attention layers to reduce VRAM cost. In both evaluations, we use Adam optimizer with a learning rate of 1.0e-4.\nAutoBots. In the evaluation on nuScenes, we use the default hyperparameters for AutoBots baseline and the enhanced AutoBots. In the evaluation on Argoverse 2 dataset, we set the dimension as 32 in the attention layers also for reducing the VRAM cost. In both evaluations, we use Adam optimizer with a learning rate of 7.5e-4."
        },
        {
            "heading": "4.3. Experimential Results",
            "text": "We report and analyze the qualitative and quantitative results against AgentFormer and AutoBots on nuScenes and Argoverse 2. After that, we compare the performance of CUM and IPCC-TP, and illustrate why our module can surpass CUM on the baseline. More results can be found in the supplementary material.\nEvaluation on nuScenes. We enhance AgentFormer and AutoBots with IPCC-TP for the evaluation on nuScenes. We use the official pre-trained AgentFormer as a baseline, which is reported to perform minADE(5) of 1.86m and minFDE(5) of 3.89m on nuScenes. The experiment results are listed in Table. 1 from which we can observe that IPCC-TP improves the original AgentFormer and AutoBots by 0.33m (4.22%) and 0.23m (3.54%) regarding minJointFDE(5) respectively. The bottom part of Figure. 4 shows a case of comparisons between the original AgentFormer and the AgentFormer embedded with IPCCTP, which illustrates that AgentFormer boosted by IPCC-TP can predict more compliant and logical future interactions with fewer collisions than the original one can do.\nIn Sec. 3.3, we have illustrated how \u03c1 in P t\u2206 (8) represents pairwise motion relevance. Here we show visualizations of P t\u2206 in different interaction patterns in Figure. 5. In Scene 1, Vehicle \u2460 is entering the intersection while Vehicle \u2461\u2212\u2463 have almost left the intersection, and they are driving with a similar speed towards the same direction. IPCCTP predicts P t\u2206 consisting of all positive values, and it also predicts a larger correlation between two vehicles that are closer to each other, e.g., \u2460 has a larger correlation with \u2461\nthan the correlation with \u2463. In Scene 2, IPCC-TP predicts a negative correlation between two vehicles, which describes the interaction mode that \u2461 is yielding to \u2460.\nEvaluation on Argoverse 2. We repeat the experiment on Argoverse 2 dataset with AgentFormer and AutoBots. The\nexperiment results are listed in Table. 2. In this evaluation, IPCC-TP improves the original AgentFormer and AutoBots by 1.24m (21.09%) and 0.97m (16.84%) regarding minJointFDE(6) respectively. One case for comparison between the original AutoBots and the AutoBots embedded with IPCC-TP is illustrated at the top part of Figure. 4. In this instance, AutoBots plus IPCC-TP predicts reasonable and collision-free interactions. In contrast, the predicted trajectories from the original AutoBots deviate significantly from the center lines, resulting in two vehicles crashing into each other. The prediction of collisions also reveals that the initial baseline is inefficient in modeling future interactions. Quantitative Comparison to CUM. We have compared CUM with our IPCC-TP methodologically in Sec. 3.5. In this subsection, we conduct a quantitative comparison between these two plugin modules on both Argoverse 2 and nuScenes using AutoBots and AgentFormer as baselines respectively. The experiment results are listed in Table. 3 and Table. 4. In the evaluation, IPCC-TP is superior to CUM by 1.24% on AutoBots and by 3.35% on AgentFormer regarding minJointFDE. The reason is that, compared with CUM, IPCC-TP not only models the future interaction more intuitively but also boosts the performance of MTP models more efficiently with less memory cost."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "Training the enhanced models involves the inverse computation of \u03a3t in (16). In order to improve the robustness of the inverse computation, we use Tikhonov Regularization (15) on \u03a3t during training the models in the aforementioned experiments. In this subsection, we conduct an ablation experiment on the magnitude of Tikhonov Regularization parameter \u2206reg (15). Based on AutoBots, we repeat training the enhanced models on Argoverse 2 with \u2206reg in different magnitudes. The experiment results are listed in Table. 5. Without any regulation, the training fails at the first iteration. When the regulation parameter is 1e-5, the training can run for the first epoch but still fails soon. In this experiment, 1e-4 is the most appropriate regulation we found for IPCC-TP. When we set the regulation to a rela-\nScene 1 Scene 2\ntively large value such as 1e-3, the performance of the enhanced model is even worse than the original model. We additionally conduct an ablation study on the Attention Block. More details can be found in the supplementary."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a new interaction modeling method named IPCC, which is based on one-dimensional motion increments relative to the current positions of the agents. Compared with the traditional methods modeling the future interactions based on the x-y coordinate system, the IPCC method can intuitively reveal the pairwise motion relevance, and furthermore, with fewer parameters need to be predicted. Based on the IPCC method, we design and implement IPCC-TP. This module can be conveniently embedded into state-of-the-art MTP models to model future interactions more precisely by extending the original predictions in the format of marginal probability distributions to a joint prediction represented by a multivariate Gaussian distribution. Experiments demonstrate that IPCC-TP can boost the performance of MTP models on two real-world autonomous driving datasets."
        },
        {
            "heading": "Supplementary Material of IPCC-TP",
            "text": ""
        },
        {
            "heading": "A. Metrics",
            "text": "In our experiments, we leverage the Minimum Joint Average Displacement Error (MinJointADE) and Minimum Joint Final Displacement Error (MinJointFDE) as the evaluation metrics, which are specifically proposed for multi-agent trajectory prediction task in INTERPRET Challenge [8]. They are different from Minimum Average Displacement Error (minADE) and Minimum Final Displacement Error (minFDE), which are frequently used in the benchmarking of ego-motion prediction. Since we focus on predicting a scene-compliant future interaction among the agents, metrics that consider all agents in the scene, such as minJointADE and minJointFDE, are more suitable options.\nMinJointADE represents the minimum value of the Euclidean Distance averaged by time and all agents between the ground truth and the mode with the lowest value [1]. Note that in the problem statement in subsection 3.1, we only analyze the case when the number of modes M = 1. In this case, the future states and the corresponding estimations at step t are Ft = {F ti |i = 1, ..., N} and F\u0302t = {F\u0302 ti |i = 1, ..., N} respectively, where F ti = {F txi ,F ty i } and F\u0302 ti = {F\u0302 txi , F\u0302 ty i } are the position ground truth and estimation for agent i at this step. Since a specific current situation could develop into multiple possible future interactions, most MTP model predicts multiple interaction modes, where M > 1, thus F\u0302 ti = {F tim|m = 1, ...,M} and \u02c6F tim = { \u02c6F txim, \u02c6F tyim}. The minJointADE is calculated as\nminJ-ADE = min 1\u2264m\u2264M\n1\nNT \u2211 i,t \u221a ( \u02c6Ftxim \u2212Ftxi )2 + ( \u02c6Ftyim \u2212F ty i ) 2.\n(19)\nMinJointFDE represents the minimum value of the euclidean distance at the last predicted timestamps averaged by all agents between the ground truth and the mode with the lowest value [1]. The minJointFDE is defined as\nminJ-FDE = min 1\u2264m\u2264M\n1\nN \u2211 i \u221a ( \u02c6FTxim \u2212FTxi )2 + ( \u02c6FTyim \u2212F Ty i ) 2.\n(20)"
        },
        {
            "heading": "B. Experiment Details",
            "text": ""
        },
        {
            "heading": "B.1 Data Preprocessing",
            "text": "NuScenes. In the nuScenes [4] dataset, the observable histories and predicted futures respectively last for 2s and 6s, with a sample rate of 2Hz. Thus the histories contain 4 timesteps, and the futures contain 12 timesteps. In the data preprocessing for Agentformer [43] and AutoBots [17], we remove agents with recorded past trajectories less than 2 steps or with incomplete future trajectories.\n1\n2\n3\n1\n2\n3\nScene 9 Scene 10\nScene 11 Scene 12\nArgoverse 2. In the Argoverse 2 [41] dataset, the observable histories, and predicted futures are 5s and 6s with a sample rate of 10Hz. Henceforth, the histories contain 50 timesteps, and the futures contain 60 timesteps. Compared with nuScenes, Argoverse 2 has significantly more agents in most scenes, and most agents have complete future trajectories. Thus, we only select agents with complete trajectories in the data preprocessing for Agentformer and AutoBots."
        },
        {
            "heading": "B.2 Training Details",
            "text": "For IPCC-TP and the backbones of Agentformer and AutoBots, we set a dropout rate of 0.1. We train the Agentformer for 50 epochs with an initial learning rate of 1e-4 in the evaluation on both nuScenes and Argoverse 2. We further decay the learning rate by 0.5 every 10 epochs. As for the training of AutoBots, we set the initial learning rate as 7.5e-4. In the evaluation on nuScenes, we train the model for 100 epochs. Thereby, in the first 50 epochs, we decay the learning rate by 0.5 every 10 epochs. In the evaluation on Argoverse 2, we train the model for 100 epochs, reducing the learning\nrate by 0.5x every 20 epochs. We train the models on a single NVIDIA Titan Xp."
        },
        {
            "heading": "C. More Experiment Results",
            "text": "In this supplementary material, we first provide more results about the qualitative comparisons between the baseline models and the enhanced models on nuScenes and Argoverse 2. Then, to support our statement about yaw angle \u03b8 estimation mentioned in Sec.3.3. IPCC Projection, we also study the error caused by the approximation in a quantitative way. Next, we provide the ablation study results on the Attention Block introduced in the main paper. Finally, we provide the result of AutoBots enhanced by our module compared with the original version on the INTERACTION dataset [46]."
        },
        {
            "heading": "C.1 Qualitative Results on nuScenes and Argo 2",
            "text": "We visualize IPCC matrices in several scenes in nuScenes. The result is shown in Figure. 6. In addition, we show more comparisons in Figure. 8 and Figure. 9."
        },
        {
            "heading": "C.2 Yaw Angle Analysis",
            "text": "As described in Sec.3.3 in the main paper, for short-term trajectory prediction (no longer than 6s), vehicles are unlikely to have sharp turns in such a short time, thus the angle \u03b8ti based on the incremental movement is close to the actual yaw angle \u03d5ti, and F\u0302\u2217t is a suitable approximation to F\u0302t. We counted the distribution of the error \u03b4\u03b8 between the real and estimated yaw angles from 378k samples. It turns out that \u03b4\u03b8 \u223c N (\u22120.4, 14.92) (degree), which means that 95.4% of the estimated angles have an error less than 30\u25e6 (2\u03c3 rule), as depicted in Figure 7. Thus, our approximation of \u03b8 is fairly reasonable."
        },
        {
            "heading": "C.3 Ablation Study",
            "text": "The proposed Attention Block is designed to assign the weight of others\u2019 influences to each agent. In this experiment, we substitute a Multi-Layer Perception (MLP) Block for it and summarize the result in Table 6. Results demonstrate the superiority of the Attention Block, which captures the cross-agent relevance."
        },
        {
            "heading": "C.4 Quatitative results on INTERACTION",
            "text": "The INTERACTION dataset requires 3s predicted future trajectories, which is less challenging compared to nuScenes and Argoverse 2 (6s long). Thus, the demonstration of IPCC-TP\u2019s ability to model multi-agent relevance over long periods of time is limited when evaluated on INTERACTION. Regardless, we still provide our results in Table 7 below for completeness. Although the baseline\nmethod AutoBots already demonstrates satisfactory results, IPCC-TP can still exceed its performance."
        }
    ],
    "title": "IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction",
    "year": 2023
}