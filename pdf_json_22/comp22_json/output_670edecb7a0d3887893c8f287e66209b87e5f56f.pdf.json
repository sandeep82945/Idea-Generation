{
    "abstractText": "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pei Ke"
        },
        {
            "affiliations": [],
            "name": "Hao Zhou"
        },
        {
            "affiliations": [],
            "name": "Yankai Lin"
        },
        {
            "affiliations": [],
            "name": "Peng Li"
        },
        {
            "affiliations": [],
            "name": "Jie Zhou"
        },
        {
            "affiliations": [],
            "name": "Xiaoyan Zhu"
        },
        {
            "affiliations": [],
            "name": "Minlie Huang"
        }
    ],
    "id": "SP:3320940915f856cdb71ad8a18caba2bf396d5fee",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation",
            "year": 2005
        },
        {
            "authors": [
                "Peter F. Brown",
                "Stephen Della Pietra",
                "Vincent J. Della Pietra",
                "Jennifer C. Lai",
                "Robert L. Mercer."
            ],
            "title": "An estimate of an upper bound for the entropy of english",
            "venue": "Comput. Linguistics, 18(1):31\u201340.",
            "year": 1992
        },
        {
            "authors": [
                "Asli Celikyilmaz",
                "Elizabeth Clark",
                "Jianfeng Gao."
            ],
            "title": "Evaluation of text generation: A survey",
            "venue": "arXiv preprint arXiv:2006.14799.",
            "year": 2020
        },
        {
            "authors": [
                "Alvin Chan",
                "Yew-Soon Ong",
                "Bill Pung",
                "Aston Zhang",
                "Jie Fu."
            ],
            "title": "Cocon: A self-supervised approach for controlled text generation",
            "venue": "9th International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Andrea Madotto",
                "Janice Lan",
                "Jane Hung",
                "Eric Frank",
                "Piero Molino",
                "Jason Yosinski",
                "Rosanne Liu."
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "8th International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Mingkai Deng",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jessica Ficler",
                "Yoav Goldberg."
            ],
            "title": "Controlling linguistic style aspects in neural language generation",
            "venue": "Proceedings of the Workshop on Stylistic Variation, pages 94\u2013104.",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Cristina Garbacea",
                "Samuel Carton",
                "Shiyan Yan",
                "Qiaozhu Mei."
            ],
            "title": "Judge the judges: A large-scale evaluation study of neural language models for online review generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Sarik Ghazarian",
                "Johnny Wei",
                "Aram Galstyan",
                "Nanyun Peng."
            ],
            "title": "Better automatic evaluation of open-domain dialogue systems with contextualized embeddings",
            "venue": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Yuxian Gu",
                "Xu Han",
                "Zhiyuan Liu",
                "Minlie Huang."
            ],
            "title": "Ppt: Pre-trained prompt tuning for few-shot learning",
            "venue": "arXiv preprint arXiv:2109.04332.",
            "year": 2021
        },
        {
            "authors": [
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "UNION: an unreferenced metric for evaluating open-ended story generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9157\u20139166.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Zhexin Zhang",
                "Zhuoer Feng",
                "Zitao Liu",
                "Wenbiao Ding",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Minlie Huang."
            ],
            "title": "Openmeva: A benchmark for evaluating open-ended story generation metrics",
            "venue": "Proceedings of the 59th Annual Meeting of the Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Tatsunori B. Hashimoto",
                "Hugh Zhang",
                "Percy Liang."
            ],
            "title": "Unifying human and statistical evaluation for natural language generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Zhiting Hu",
                "Zichao Yang",
                "Xiaodan Liang",
                "Ruslan Salakhutdinov",
                "Eric P. Xing."
            ],
            "title": "Toward controlled generation of text",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1587\u20131596.",
            "year": 2017
        },
        {
            "authors": [
                "Anjuli Kannan",
                "Oriol Vinyals."
            ],
            "title": "Adversarial evaluation of dialogue models",
            "venue": "arXiv preprint arXiv:1701.08198.",
            "year": 2017
        },
        {
            "authors": [
                "Pei Ke",
                "Jian Guan",
                "Minlie Huang",
                "Xiaoyan Zhu."
            ],
            "title": "Generating informative responses with controlled sentence function",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1499\u20131508.",
            "year": 2018
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R Varshney",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Ctrl: A conditional transformer language model for controllable generation",
            "venue": "arXiv preprint arXiv:1909.05858.",
            "year": 2019
        },
        {
            "authors": [
                "Ben Krause",
                "Akhilesh Deepak Gotmare",
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Shafiq Joty",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "Gedi: Generative discriminator guided sequence generation",
            "venue": "arXiv preprint arXiv:2009.06367.",
            "year": 2020
        },
        {
            "authors": [
                "Klaus Krippendorff."
            ],
            "title": "Content analysis: An introduction to its methodology",
            "venue": "Sage publications.",
            "year": 2018
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2016
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dexperts: Decodingtime controlled text generation with experts and antiexperts",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Ruibo Liu",
                "Jason Wei",
                "Soroush Vosoughi."
            ],
            "title": "Language model augmented relevance score",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "The 49th Annual Meeting of the Association for",
            "year": 2011
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever."
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "OpenAI Blog.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Asli Celikyilmaz",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Plotmachines: Outlineconditioned generation with dynamic plot state tracking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Few-shot text generation with natural language instructions",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 390\u2013402.",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur P. Parikh."
            ],
            "title": "BLEURT: learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892.",
            "year": 2020
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi S. Jaakkola."
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "Advances in Neural Information Processing Systems, pages 6830\u20136841.",
            "year": 2017
        },
        {
            "authors": [
                "Chongyang Tao",
                "Lili Mou",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "RUBER: an unsupervised method for automatic evaluation of open-domain dialog systems",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 722\u2013729.",
            "year": 2018
        },
        {
            "authors": [
                "Guy Tevet",
                "Jonathan Berant."
            ],
            "title": "Evaluating the evaluation of diversity in natural language generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 326\u2013346.",
            "year": 2021
        },
        {
            "authors": [
                "Svitlana Vakulenko",
                "Maarten de Rijke",
                "Michael Cochez",
                "Vadim Savenkov",
                "Axel Polleres."
            ],
            "title": "Measuring semantic coherence of a conversation",
            "venue": "The Semantic Web - ISWC 2018 - 17th International Semantic Web Conference, volume 11136, pages",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein."
            ],
            "title": "FUDGE: controlled text generation with future discriminators",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2021
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter J. Liu."
            ],
            "title": "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 11328\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ruqing Zhang",
                "Jiafeng Guo",
                "Yixing Fan",
                "Yanyan Lan",
                "Jun Xu",
                "Xueqi Cheng."
            ],
            "title": "Learning to control the specificity in neural response generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1108\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Wei Zhao",
                "Maxime Peyrard",
                "Fei Liu",
                "Yang Gao",
                "Christian M. Meyer",
                "Steffen Eger."
            ],
            "title": "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in",
            "year": 2019
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 12697\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhou",
                "Minlie Huang",
                "Tianyang Zhang",
                "Xiaoyan Zhu",
                "Bing Liu."
            ],
            "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelli-",
            "year": 2018
        },
        {
            "authors": [
                "Xianda Zhou",
                "William Yang Wang."
            ],
            "title": "Mojitalk: Generating emotional responses at scale",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1128\u2013 1137.",
            "year": 2018
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Richard S. Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "IEEE International",
            "year": 2015
        },
        {
            "authors": [
                "Raffel"
            ],
            "title": "2020) as our base models respectively, and present the results in Table 7. The results show that larger numbers of parameters can benefit the model performance while degrading the computation efficiency",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Controlled text generation aims to generate texts under some control variables, including prespecified content prefixes and attribute labels (such as sentiments and topics). Controlled text generation has been significantly advanced by large-scale pre-trained models with respect to generation quality and various control variables (Keskar et al., 2019; Dathathri et al., 2020; Yang and Klein, 2021; Liu et al., 2021a; Chan et al., 2021).\nDespite the great success of these generation models, it becomes critical to evaluate the quality of generated texts accurately. Most of the existing studies adopt unsupervised and supervised metrics to measure the quality of generated texts\n\u2217 Part of the work was done while Peng Li was working at Tencent.\n\u2020 Corresponding author 1The data and codes are available at https://github.\ncom/thu-coai/CTRLEval.\nunder different combinations of control variables (Dathathri et al., 2020; Chan et al., 2021). The evaluation is commonly conducted in a reference-free setting because it is challenging to collect sufficient high-quality references for each input of control variables in this open-ended text generation task (Dathathri et al., 2020).\nHowever, both unsupervised and supervised metrics have shown limitations in the evaluation of controlled text generation: 1) Unsupervised metrics such as perplexity (Brown et al., 1992) can only provide task-agnostic evaluation regarding the overall quality of generated texts. However, controlled text generation tasks typically involve multiple evaluation aspects (Deng et al., 2021), including the quality of generated texts themselves and the relationship between generated texts and control variables. It is thus not surprising that existing unsupervised metrics without multi-aspect interpretability have low correlations with human judgments (Hashimoto et al., 2019). 2) Supervised metrics are commonly trained on the datasets of specific tasks to measure the corresponding aspects of generated texts (e.g., evaluating whether a generated text is accordant with the sentiment label) (Dathathri et al., 2020; Chan et al., 2021). This may cause over-fitting to task-specific data and degrade the generalization ability of metrics (Garbacea et al., 2019), thereby giving unstable evaluation of generated texts from different models or with different qualities (Guan and Huang, 2020).\nTo deal with the above issues, we propose an unsupervised reference-free metric called CTRLEval for evaluating controlled text generation models. This metric performs evaluation from different aspects without any training on task-specific data. Specifically, we formulate the evaluation of each aspect into \u201cfill-in-the-blank\u201d tasks whose input and output patterns can be designed based on the definition of the aspect. Then, we utilize a pretrained model whose pre-training task is text in-\nar X\niv :2\n20 4.\n00 86\n2v 2\n[ cs\n.C L\n] 5\nD ec\n2 02\n2\nfilling (such as PEGASUS (Zhang et al., 2020a)) as our base model, and fuse the generation probabilities from these \u201cfill-in-the-blank\u201d tasks as the evaluation result. To alleviate the potential bias caused by the task design (Zhao et al., 2021), we devise multiple text infilling tasks for each aspect and use the weighted sum of all the results as the final score. In this paper, we consider three aspects which are commonly used to measure the performance of controlled text generation models, including coherence (Yuan et al., 2021), consistency (Rashkin et al., 2020), and attribute relevance (Dathathri et al., 2020). These evaluation aspects cover both the quality of generated texts and the relationship between generated texts and different control variables, which can provide a comprehensive evaluation result for controlled text generation. Experimental results show that our metric can maintain the generalization ability and achieve stable performance faced with model drift and quality drift.\nOur main contributions are as follows:\n\u2022 We propose an unsupervised reference-free metric called CTRLEval for evaluating controlled text generation. This metric formulates three evaluation aspects (i.e., coherence, consistency, and attribute relevance) into multiple text infilling tasks, and utilizes the ensemble of generation probabilities from a pre-trained language model as the evaluation results.\n\u2022 We conduct experiments on two benchmark tasks including sentiment-controlled and topic-controlled text generation based on our collected evaluation set. Experimental results show that our proposed metric has higher correlations with human judgments, while obtaining better generalization of evaluating generated texts from different models and with different qualities."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Controlled Text Generation",
            "text": "Early studies on controlled text generation adopt attribute label embeddings (Ficler and Goldberg, 2017; Zhou et al., 2018) or latent variables (Hu et al., 2017; Ke et al., 2018; Zhou and Wang, 2018) to learn the complex relationship between control variables and generated texts. With the development of large-scale generative pre-trained\nmodels, it is costly to re-train or fine-tune pretrained models on the corpora with attribute annotations (Keskar et al., 2019). Recent works resort to decoding-time methods and directly make pre-trained models generate texts towards desired attributes during inference, including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2020), FUDGE (Yang and Klein, 2021) and DEXPERTS (Liu et al., 2021a). These works rely heavily on human evaluation because existing reference-free metrics including unsupervised and supervised ones are shown to have evident limitations for evaluating controlled text generation (Dathathri et al., 2020)."
        },
        {
            "heading": "2.2 Evaluation Metric for Text Generation",
            "text": "Automatic evaluation metrics are important for natural language generation tasks, which can be simply divided into referenced, reference-free (also known as unreferenced) and hybrid metrics: 1) Referenced metrics usually measure the relevance between generated texts and reference texts via lexicon overlap (such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004)) or embedding similarity (such as MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) and MARS (Liu et al., 2021b)). 2) Reference-free metrics directly evaluate the quality of generated texts without references. Since unsupervised metrics like perplexity (Brown et al., 1992) and distinct n-grams (Li et al., 2016) can only provide a task-agnostic result which correlates weakly with human judgments (Hashimoto et al., 2019; Tevet and Berant, 2021), most of the reference-free metrics resort to supervised models. Specifically, they are trained to fit human-annotated ratings / labels (such as discriminator scores (Shen et al., 2017)) or distinguish human-written texts from negative samples (such as UNION (Guan and Huang, 2020)). 3) Hybrid metrics contain both referenced and reference-free scores, such as RUBER (Tao et al., 2018; Ghazarian et al., 2019), BLEURT (Sellam et al., 2020) and BARTScore (Yuan et al., 2021).\nCompared with existing reference-free metrics which are unsupervised, our metric can support the evaluation of generated texts from different aspects via the full utilization of pre-trained models and the formulation of text infilling tasks, which fits the evaluation protocol of controlled text generation well. Also, in contrast with supervised reference-free metrics, our metric can avoid over-\nfitting task-specific data and maintain better generalization ability to evaluate generated texts from different models and with different qualities."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition and Method Overview",
            "text": "Given the input I = (X, a, Y ) which consists of a content prefix X , an attribute label a, and a generated text Y , our goal is to acquire three evaluation results for coherence, consistency and attribute relevance, respectively.\nAs shown in Figure 1, our main idea is to formulate each evaluation aspect into multiple text infilling tasks and utilize the ensemble of the scores from each task as the final evaluation results. We denote each text infilling task as a pattern evaluator , which means evaluation with different input and output patterns. Inspired by the recent works on pattern-exploiting training (Schick and Sch\u00fctze, 2021a,b) and prompt tuning (Gu et al., 2021), we define each pattern evaluator as E = (f, g), which consists of two pattern functions to build the input and output sequence of text infilling tasks, respectively. The score of each pattern evaluator is acquired from the generation probability of the encoder-decoder pre-trained language model whose pre-training task is to generate the masked part from the remaining texts of the input. For each aspect, we devise multiple pattern evaluators to\nalleviate the potential bias caused by the pattern design (Zhao et al., 2021), and weight the scores of all the evaluators to obtain the final result:\nS(I) = NE\u2211 j=1 \u03b2j(I) \u00b7 sj(I) (1)\nwhereNE is the number of pattern evaluators, S(I) denotes the overall score for each aspect, \u03b2j(I) is a factor to weight the pattern evaluators of the corresponding aspect and sj(I) indicates the score of each pattern evaluator based on the generation probability of the pre-trained model."
        },
        {
            "heading": "3.2 Evaluation Aspect",
            "text": ""
        },
        {
            "heading": "3.2.1 Coherence",
            "text": "Coherence aims to measure whether the sentences in the generated text are semantically relevant to compose a coherent body (Vakulenko et al., 2018; Yuan et al., 2021), which reflects the quality of the generated text itself. Assume that the generated text Y consists of M sentences, i.e., Y = (Y1, Y2, \u00b7 \u00b7 \u00b7 , YM ), we devise M pattern evaluators Ej = (fj , gj)(1 \u2264 j \u2264 M) to measure the relevance between each sentence and all the remaining sentences:\nfj(I) = Y\\j = Y1 \u00b7 \u00b7 \u00b7Yj\u22121[M]Yj+1 \u00b7 \u00b7 \u00b7YM (2) gj(I) = Yj (3)\nwhere Y\\j indicates the generated text Y with the j-th sentence replaced by a mask token [M]. The score of each pattern evaluatorEj can be computed via the log probability of the pre-trained model P\u03b8:\nsj(I) = logP\u03b8(gj(I)|fj(I)) = logP\u03b8(Yj |Y\\j) (4)\nSince specific and informative sentences are more likely to impact the quality of the whole text, we adopt normalized inverse sentence frequency (NISF) (Zhang et al., 2018) of the output sentence which can reflect its specificity to weight each pattern evaluator:\n\u03b2j(I) = NISF(Yj) = ISF(Yj)\u2211M k=1 ISF(Yk)\n(5)\nISF(Yj) = max w\u2208Yj IWF(w) (6)\nwhere the inverse sentence frequency (ISF) of Yj is computed by the maximum inverse word frequency (IWF) of the words in Yj . We estimate IWF on a general corpus BookCorpus (Zhu et al., 2015), which is commonly adopted as the pre-training dataset in the existing works (Devlin et al., 2019):\nIWF(w) = log(1 + |C|)\nfw (7)\nwhere |C| indicates the total number of sentences in BookCorpus and fw denotes the number of sentences containing the word w. Thus, the evaluation result of coherence can be obtained by the ensemble of the scores from all the pattern evaluators:\nScoh(I) = M\u2211 j=1 NISF(Yj) \u00b7 logP\u03b8(Yj |Y\\j) (8)"
        },
        {
            "heading": "3.2.2 Consistency",
            "text": "Consistency aims to evaluate whether the generated text is consistent to the content prefix (Celikyilmaz et al., 2020; Rashkin et al., 2020). We devise two symmetric pattern evaluatorsEX\u2192Y andEY\u2192X to evaluate the consistency between the content prefix and the generated text as follows:\nfX\u2192Y (I) = X[M], gX\u2192Y (I) = Y\\X (9)\nfY\u2192X(I) = [M]Y\\X , gY\u2192X(I) = X (10)\nwhere Y\\X denotes the remaining part of the generated text without the prefix. Similar to coherence, we still adopt the log probability of the pre-trained model as the pattern evaluator\u2019s score and weight\nthem with normalized inverse sentence frequency to obtain the final result of consistency:\nScons(I) = NISF(Y\\X) \u00b7 logP\u03b8(Y\\X |X[M]) + NISF(X) \u00b7 logP\u03b8(X|[M]Y\\X) (11)"
        },
        {
            "heading": "3.2.3 Attribute Relevance",
            "text": "Attribute relevance aims to measure whether the generated text satisfies the attribute label (Dathathri et al., 2020). To probe the relevance between generated texts and attribute labels, we first introduce a verbalizer v(\u00b7) which maps all the attribute labels a in the attribute set A to the corresponding words (Schick and Sch\u00fctze, 2021a). Then, we design the pattern evaluators Ej = (fj , gj)(1 \u2264 j \u2264 NAR) where fj(\u00b7) adds prompts and a mask token to the generated text, and gj(\u00b7) is set to be a verbalizer:\nfj(I) = Concat(Promptj ,[M], Y ) (12)\ngj(I) = vj(a) (13)\nwhere Concat(\u00b7) indicates the concatenation of the prompt, the mask token, and the generated text in some order. We give an example for the pattern design of attribute relevance which is also shown in Figure 1. In this example, the attribute is set to be the sentiment A = {Positive,Negative}, while the patterns are designed as f(I) = \u201cY It was [M].\u201d and g(I) = v(Positive/Negative) = good/bad.\nInspired by the existing works (Schick and Sch\u00fctze, 2021a), we use the generation probability of the corresponding label word over all the label words as the score of the pattern evaluator:\nsj(I) = P\u03b8(vj(a)|fj(I))\u2211\na\u2032\u2208A P\u03b8(vj(a \u2032)|fj(I))\n(14)\nBased on the assumption that the pattern evaluator is adequate to measure the data sample if the words of all the attribute labels are easily generated, we devise the unnormalized weighted score of each evaluator as the sum of generation probabilities over all the attribute labels:\nwj(I) = \u2211 a\u2032\u2208A P\u03b8(vj(a \u2032 )|fj(I)) (15) \u03b2j(I) = wj(I)\u2211NAR k=1 wk(I) (16)\nSimilarly, the evaluation result of attribute relevance can be acquired by the weighted sum of all the pattern evaluators\u2019 scores:\nSAR(I) = NAR\u2211 j=1 \u03b2j(I) \u00b7 sj(I) (17)"
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Since there is no standard benchmark dataset for evaluating controlled text generation, we construct an evaluation set to measure the correlation between automatic metrics and human judgments. Task: We choose sentiment-controlled and topiccontrolled text generation as the benchmark tasks, which are widely used in the existing works (Dathathri et al., 2020; Chan et al., 2021). These two tasks require the models to generate texts conditioned on the given prefixes and sentiment / topic labels, respectively. In the task of sentiment-controlled text generation, we follow PPLM (Dathathri et al., 2020) and CoCon (Chan et al., 2021) to adopt 15 prefixes and 2 sentiment labels (i.e., positive and negative). As for topiccontrolled text generation, we follow CoCon (Chan et al., 2021) to adopt 20 prefixes and 4 topic labels (i.e., computers, politics, religion, and science). Generation Models: We consider various generation models including CTRL (Keskar et al., 2019), PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2020), and CoCon (Chan et al., 2021). These representative models support both the sentimentcontrolled and topic-controlled text generation tasks, and cover different levels of generation abilities. We make these models generate 3 different samples for each unique pair of prefixes and attribute labels. We set the maximum length of generated texts to be 80 and remove the last sentence if it is not complete. We directly use the generation results if they have been released by the original papers. Otherwise, we run the original codes to obtain the generation results. Human Annotation: We collect human ratings on the generated texts from Amazon Mechanical Turk (AMT). Each survey of AMT contains a prefix, an attribute label, and five generated texts including (a) four generated texts from the above four models respectively, and (b) one negative sample which is constructed by perturbing (e.g. sentence shuffling and dropping) another sample from the evaluation set (Guan et al., 2021). We ask annotators to rate\nthese texts with a 1-5 Likert scale for each aspect. To control the annotation quality, we discard the submissions if the annotator assigns a higher rating to the negative sample than other texts. We ensure that each generated text contains 5 valid ratings for each aspect, where the average value of valid ratings is used as the human judgments. We also calculate Krippendorff\u2019s \u03b1 (Krippendorff, 2018) to show the agreement of human ratings, which is 0.626 / 0.622 for sentiment-controlled / topiccontrolled text generation tasks, respectively.\nThe statistics of the evaluation set are shown in Table 1."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We choose PEGASUS (Zhang et al., 2020a) as our base model in the overall result and also explore other pre-trained models in \u00a74.8. The hyperparameters of Transformer blocks are the same as PEGASUS-large with 568M parameters. As for the pattern evaluators in attribute relevance involving prompts and verbalizers which need to be additionally designed, we follow BARTScore (Yuan et al., 2021) to first adopt manually devised seed prompts and verbalizers in the existing works (Schick and Sch\u00fctze, 2021a,b), and then collect paraphrases to automatically expand our evaluator set. The statistics of pattern evaluators in attribute relevance are presented in Table 2. More details about the specific design of prompts and verbalizers are included in Appendix A."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We choose several state-of-the-art reference-free metrics as our baselines: Perplexity (PPL) (Brown et al., 1992): This method calculates the perplexity of generated texts\nwith a language model. We use GPT (Radford et al., 2018) and PEGASUS (Zhang et al., 2020a) as the base models since GPT is commonly used in the existing works (Dathathri et al., 2020) and PEGASUS is our base model. They are denoted as PPL-GPT and PPL-PEGASUS, respectively. Discriminator Score (DisScore) (Kannan and Vinyals, 2017; Chan et al., 2021): This method trains a discriminator with different objectives. We adopt the IMDB movie review dataset (Maas et al., 2011) / HuffPost News category dataset2 (Misra, 2018) for sentiment-controlled / topic-controlled text generation tasks, respectively. For coherence and consistency, the discriminator is trained to distinguish human-written texts from manually constructed negative samples, where the ratio of positive and negative samples is 1:1. For attribute\n2https://www.kaggle.com/rmisra/ news-category-dataset\nrelevance, it is trained based on the sentiment / topic classification task, respectively (Chan et al., 2021). Both the sentiment and topic discriminators are implemented based on BERT (Devlin et al., 2019) and they achieve 94.15% / 91.54% on the corresponding test set, respectively. UNION (Guan and Huang, 2020): This method is a self-supervised metric which is trained to distinguish human-written texts from the automatically perturbed negative samples with well-designed negative sampling strategies and multi-task learning. We use the same datasets as the discriminator score to train UNION. BLEURT (Sellam et al., 2020): This method is a supervised metric which is pre-trained on synthetic examples and then fine-tuned to fit human ratings. We used the same instruction in \u00a74.1 to additionally annotate the generated texts to construct the training set for BLEURT, whose amount is the same as the evaluation set. There is no overlap between BLEURT\u2019s training set and the evaluation set. BARTScore (Yuan et al., 2021): This method utilizes the generation probabilities of BART (Lewis et al., 2020) to measure the relationship among sources, hypotheses, and references. Since this metric simultaneously contains referenced and reference-free parts, we only use the referencefree score in our experiments. We also use PEGASUS (Zhang et al., 2020a) as the base model for a fair comparison, which is denoted as BARTScorePEGASUS."
        },
        {
            "heading": "4.4 Overall Result",
            "text": "We follow the existing work (Guan and Huang, 2020; Yuan et al., 2021) to adopt Pearson (r), Spearman (\u03c1), and Kendall (\u03c4 ) correlation coefficients between automatic metrics and human judgments\nto measure the performance of different metrics. The overall results on sentiment-controlled and topic-controlled text generation are shown in Table 3 and 4. We can observe that CTRLEval outperforms other baselines with a large margin, indicating the effectiveness of our metric on different evaluation aspects. In Table 4, unsupervised baselines can hardly measure the relevance between generated texts and attribute labels because they only provide a task-agnostic score which is weakly relevant to this specific aspect. For comparison, our metric, which supports the evaluation for different aspects of generated texts via the design of text infilling tasks, can obtain much better performance and even outperform the supervised baselines."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "To further investigate the effect of each module, we conduct ablation studies on the weight of pattern evaluators and the design of pattern functions. For the weight of evaluators, we use the mean, maximum and minimum values of all the evaluators as the final result rather than the weighted sum based on the factor \u03b2. As for the design of pattern functions, we fix the base model and replace our input and output patterns (f&g) with those of PPL-GPT (Radford et al., 2018) and BARTScore (Yuan et al., 2021). The pattern functions of these ablation models are not designed for text infilling tasks. Both of them remove the mask token in the input pattern, and PPL-GPT additionally places the input pattern at the beginning of the output pattern.\nThe results in Table 5 show that each module in our metric contributes to the final performance. As for the weight of evaluators, we can observe that our weight factor performs better than common aggregation functions especially in consistency, indi-\ncating the necessity of the well-designed ensemble method when the number of pattern evaluators is small. Also, our pattern functions outperform those of other baselines, thereby showing the effectiveness of text infilling tasks which can fully utilize pre-trained models in an unsupervised setting."
        },
        {
            "heading": "4.6 Analysis on Generalization Ability",
            "text": "Generalization ability is essential for automatic metrics to evaluate open-ended text generation models. In this section, we will test whether our metric can be generalizable to measure the generated texts faced with model drift and quality drift."
        },
        {
            "heading": "4.6.1 Model Drift",
            "text": "To measure whether CTRLEval is reliable to assess the generated results of different models, we split the evaluation set into four subsets based on the generation model and calculate Pearson correlation between each metric and human judgments.\nThe results in Figure 2 show that our metric can outperform other baselines on the generated texts of all the generation models. Simultaneously, CTRLEval can achieve stable performance with smaller variances when evaluating different generation models, indicating that our metric can generalize to the model drift better."
        },
        {
            "heading": "4.6.2 Quality Drift",
            "text": "To evaluate the generalization ability of CTRLEval on the generated texts with different qualities, we follow the existing work (Sellam et al., 2020; Guan\nand Huang, 2020) to construct four biased subsets based on the coherence score of topic-controlled text generation. We first sort all the samples in the evaluation set and use the quartiles to split them into four subsets with the index from 0 to 3. Then, we create four biased subsets. For the jth subset, we sampled the generated texts which belong to the original ith subset with a probability of 1|j\u2212i|+1 where i, j = 0, 1, 2, 3. Thus, the four biased subsets have different distributions of generated texts with different qualities, as shown in Figure 3.\nWe then calculate the Pearson correlation between each metric and human judgments. The results in Figure 3 show that CTRLEval has higher correlations than the baselines on the evaluation subsets with different qualities. Also, our metric can achieve more stable performance on different subsets, which shows our better generalization ability to deal with quality drift."
        },
        {
            "heading": "4.7 Analysis on the Number of Evaluators",
            "text": "To investigate how the number of pattern evaluators affects the performance, we randomly sample the evaluators 20 times when evaluating attribute relevance in topic-controlled text generation, and illustrate mean values and standard deviations of each number of evaluators in Figure 4.\nFigure 4 shows that as the number of evaluators increases, the mean value of our performance can be persistently improved while the standard deviation is gradually reduced. This demonstrates the necessity of devising multiple pattern evaluators for each aspect, which can alleviate the bias brought by the pattern design. The comparison between the pattern functions of CTRLEval and other base-\nlines indicates our superior performance on all the numbers of evaluators."
        },
        {
            "heading": "4.8 Analysis on Base Model",
            "text": "Since our method can adapt to different pretrained models whose pre-training task is text infilling, we additionally choose BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) as our base model, and present the results in Table 6.\nTable 6 shows that PEGASUS and T5 obtain comparable performance on all the evaluation aspects, which indicates that our well-designed text infilling tasks can be transferable to T5 without considerable modification. As for BART which performs worse on consistency and attribute relevance, we conjecture that the fewer parameters and the form of pre-training tasks may limit the performance. Since the pre-training task of BART is to generate the complete text rather than only the masked part of the input text, it may not be good at the evaluation involving a short span of texts, such as the prefix in the evaluation of consistency and the label word in attribute relevance.\nWe also provide the analysis on the number of parameters in Appendix B and the case study in Appendix C."
        },
        {
            "heading": "5 Discussion",
            "text": "Extension to More Control Variables: In this paper, we evaluate the relationship between generated texts and two control variables (including content prefixes and attribute labels) via consistency and attribute relevance, respectively. We can also extend our metric to other control variables by designing additional pattern evaluators to measure the relationship between generated texts and each variable, respectively. We will further investigate the extensibility of our metric in the future work. Design of Pattern Evaluators: With the rapid development of prompt tuning, recent works have proposed new methods on the design of prompts and verbalizers (Gao et al., 2021; Lester et al., 2021), which provide alternatives to our metric in attribute relevance. Also, the weight factor of each evaluator can be set as diversity metrics (Hashimoto et al., 2019) besides NISF in coherence and consistency. We will leave the exploration of more settings on pattern evaluators as the future work."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present an unsupervised reference-free metric called CTRLEval for evaluating controlled text generation. This metric formulates the evaluation of different aspects into multiple text infilling tasks, and utilizes the ensemble of generation probabilities from a pre-trained model in different tasks as the evaluation result. Experimental results indicate that CTRLEval obtains higher correlations with human judgments and shows better generalization ability for addressing model drift and quality drift."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005.\nEthics Statement\nWe construct an evaluation set for evaluating controlled text generation. The data samples in this set are all from the existing works with open-source codes, model checkpoints, and generated results. We directly use the generated results if the authors have released them. Otherwise, we adopt the same\nsetting as the original papers to make these models generate texts. We do not apply extra selection strategies to the generated results.\nWe resort to Amazon Mechanical Turk (AMT) for the annotation of this evaluation set. We do not invade the privacy or collect personal information of annotators. We pay each annotator $0.06 for each survey including four generated texts and one negative sample. The payment is determined based on the length of data samples. We additionally ask annotators to check whether there is a potential ethical problem in the data, and remove these problematic data in the evaluation set. After annotation on AMT, we manually review all the annotated samples from an ethical perspective. However, we admit that there may still exist unpredictable bias in this evaluation set."
        },
        {
            "heading": "A Pattern Evaluator for Attribute Relevance",
            "text": "We first choose the prompts and verbalizers which have been shown to work well in the existing works on few-shot text classification (Schick and Sch\u00fctze, 2021a; Gao et al., 2021) and generation (Schick and Sch\u00fctze, 2021b) as the seed prompts and verbalizers. Then, we expand our prompt set with the following rules: 1) Switching the order of generated texts, prompts, and mask tokens; 2) Collecting the paraphrases of seed prompts just as BARTScore (Yuan et al., 2021) does. All the prompts and verbalizers which are used in our experiments are shown in Table 8."
        },
        {
            "heading": "B Analysis on the Number of Parameters",
            "text": "We further conduct experiments on the base model with different numbers of parameters. Since the authors of PEGASUS (Zhang et al., 2020a) do not release the model checkpoint of PEGASUSbase, we choose T5-small, T5-base and T5-large (Raffel et al., 2020) as our base models respectively, and present the results in Table 7. The results show that larger numbers of parameters can benefit the model performance while degrading the computation efficiency."
        },
        {
            "heading": "C Case Study",
            "text": "To intuitively show how our metric works in the evaluation of controlled text generation, we provide some cases on the three evaluation aspects, including coherence (Figure 6), consistency (Figure 6), and attribute relevance (Figure 7). Since the range of various metrics is always different, it\nmay be less meaningful to directly compare the absolute value of each metric. Thus, we follow the existing works (Guan and Huang, 2020; Liu et al., 2021b) to conduct a pairwise comparison on different samples.\nThe results in Figure 6 and 7 show that our metric can give accordant preferences with human judgments, indicating the effectiveness of our metric on all three evaluation aspects. To further show how each pattern evaluator works in the overall evaluation result, we take the second sample in Figure 7 as an example and visualize the weight \u03b2(I) and score s(I) in Figure 5. We can observe that most of the pattern evaluators assign high scores to this sample which agree with the human judgment. Simultaneously, the weight factor automatically reduces the effect of low-quality evaluators which also plays an important role in the final evaluation result.\n(a) Comparison between CTRLEval and other baselines on attribute relevance in sentiment-controlled text generation"
        }
    ],
    "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
    "year": 2022
}