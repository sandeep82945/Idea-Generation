{
    "abstractText": "Diabetic retinopathy (DR) is a microvascular complication of diabetes. Microaneurysms (MAs) are often observed in the retinal vessels of diabetic patients and represent one of the earliest signs of DR. Accurate and efficient detection of MAs is crucial for the diagnosis of DR. In this study, an automatic model (MA-YOLO) is proposed for MA detection in fluorescein angiography (FFA) images. To obtain detailed features and improve the discriminability of MAs in FFA images, SwinIR was utilized to reconstruct super-resolution images. To solve the problems of missed detection of small features and feature information loss, an MA detection layer was added between the neck and the head sections of YOLOv8. To enhance the generalization ability of the MA-YOLO model, transfer learning was conducted between high-resolution images and low-resolution images. To avoid excessive penalization due to geometric factors and address sample distribution imbalance, the loss function was optimized by taking the Wise-IoU loss as a bounding box regression loss. The performance of the MA-YOLO model in MA detection was compared with that of other state-ofthe-art models, including SSD, RetinaNet, YOLOv5, YOLOX, and YOLOv7. The results showed that the MA-YOLO model had the best performance in MA detection, as shown by its optimal metrics, including recall, precision, F1 score, and AP, which were 88.23%, 97.98%, 92.85%, and 94.62%, respectively. Collectively, the proposed MA-YOLO model is suitable for the automatic detection of MAs in FFA images, which can assist ophthalmologists in the diagnosis of the progression of DR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bowei Zhang"
        },
        {
            "affiliations": [],
            "name": "Jing Li"
        },
        {
            "affiliations": [],
            "name": "Yun Bai"
        },
        {
            "affiliations": [],
            "name": "Qing Jiang"
        },
        {
            "affiliations": [],
            "name": "Biao Yan"
        },
        {
            "affiliations": [],
            "name": "Zhenhua Wang"
        }
    ],
    "id": "SP:70c5f62926c6acfa31503789e07d551c87af1547",
    "references": [
        {
            "authors": [
                "I. Alifanov",
                "V. Sakovych"
            ],
            "title": "Prognostic risk factors for diabetic retinopathy in patients with type 2 diabetes mellitus",
            "venue": "J. Ophthalmol. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J.W. Yau",
                "S.L. Rogers",
                "R. Kawasaki",
                "E.L. Lamoureux",
                "J.W. Kowalski",
                "T. Bek",
                "S.-J. Chen",
                "J.M. Dekker",
                "A. Fletcher",
                "J. Grauslund"
            ],
            "title": "Global prevalence and major risk factors of diabetic retinopathy",
            "venue": "Diabetes Care 2012,",
            "year": 2012
        },
        {
            "authors": [
                "T. Walter",
                "P. Massin",
                "A. Erginay",
                "R. Ordonez",
                "C. Jeulin",
                "J.-C. Klein"
            ],
            "title": "Automatic detection of microaneurysms in color fundus images. Med",
            "venue": "Image Anal",
            "year": 2007
        },
        {
            "authors": [
                "A. Couturier",
                "V. Man\u00e9",
                "S. Bonnin",
                "A. Erginay",
                "P. Massin",
                "A. Gaudric",
                "R. Tadayoni"
            ],
            "title": "Capillary plexus anomalies in diabetic retinopathy on optical coherence tomography angiography",
            "venue": "Retina",
            "year": 2015
        },
        {
            "authors": [
                "B. Wu",
                "W. Zhu",
                "F. Shi",
                "S. Zhu",
                "X. Chen"
            ],
            "title": "Automatic detection of microaneurysms in retinal fundus images",
            "venue": "Comput. Med. Imaging Graph",
            "year": 2017
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.-Y. Fu",
                "A.C. Berg"
            ],
            "title": "SSD: Single shot multibox detector",
            "venue": "In Proceedings of the Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,",
            "year": 2016
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "C.-Y. Wang",
                "A. Bochkovskiy",
                "H.-Y.M. Liao"
            ],
            "title": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Z. Ge",
                "S. Liu",
                "F. Wang",
                "Z. Li",
                "J. Sun"
            ],
            "title": "Yolox: Exceeding yolo series in 2021",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liao",
                "H. Xia",
                "S. Song",
                "H. Li"
            ],
            "title": "Microaneurysm detection in fundus images based on a novel end-to-end convolutional neural network. Biocybern",
            "venue": "Biomed. Eng",
            "year": 2021
        },
        {
            "authors": [
                "H. Xia",
                "Y. Lan",
                "S. Song",
                "H. Li"
            ],
            "title": "A multi-scale segmentation-to-classification network for tiny microaneurysm detection in fundus images. Knowl.-Based Syst",
            "year": 2021
        },
        {
            "authors": [
                "P. Chudzik",
                "S. Majumdar",
                "F. Caliv\u00e1",
                "B. Al-Diri",
                "A. Hunter"
            ],
            "title": "Microaneurysm detection using fully convolutional neural networks",
            "venue": "Comput. Methods Programs Biomed",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhou",
                "X. He",
                "L. Huang",
                "L. Liu",
                "F. Zhu",
                "S. Cui",
                "L. Shao"
            ],
            "title": "Collaborative learning of semi-supervised segmentation and classification for medical images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xie",
                "J. Zhang",
                "H. Lu",
                "C. Shen",
                "Y. Xia"
            ],
            "title": "SESV: Accurate medical image segmentation by predicting and correcting errors",
            "venue": "IEEE Trans. Med. Imaging",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "J. Luo",
                "B. Liu",
                "R. Feng",
                "L. Lu",
                "H. Zou"
            ],
            "title": "Automated diabetic retinopathy grading and lesion detection based on the modified R-FCN object-detection algorithm",
            "venue": "IET Comput. Vis",
            "year": 2020
        },
        {
            "authors": [
                "S. Guo",
                "T. Li",
                "H. Kang",
                "N. Li",
                "Y. Zhang",
                "K. Wang"
            ],
            "title": "L-Seg: An end-to-end unified framework for multi-lesion segmentation of fundus images",
            "venue": "Neurocomputing",
            "year": 2019
        },
        {
            "authors": [
                "M. Mateen",
                "T.S. Malik",
                "S. Hayat",
                "M. Hameed",
                "S. Sun",
                "J. Wen"
            ],
            "title": "Deep Learning Approach for Automatic Microaneurysms Detection",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Kumar",
                "A. Adarsh",
                "B. Kumar",
                "A.K. Singh"
            ],
            "title": "An automated early diabetic retinopathy detection through improved blood vessel and optic disc segmentation",
            "venue": "Opt. Laser Technol",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC,",
            "year": 2021
        },
        {
            "authors": [
                "J. Liang",
                "J. Cao",
                "G. Sun",
                "K. Zhang",
                "L. Van Gool",
                "R. Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC,",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "L. Qi",
                "H. Qin",
                "J. Shi",
                "J. Jia"
            ],
            "title": "Path aggregation network for instance segmentation",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "S. Hajeb Mohammad Alipour",
                "H. Rabbani",
                "M. Akhlaghi"
            ],
            "title": "A new combined method based on curvelet transform and morphological operators for automatic detection of foveal avascular zone",
            "venue": "Signal Image Video Process",
            "year": 2014
        },
        {
            "authors": [
                "W. Gao",
                "M. Shan",
                "N. Song",
                "B. Fan",
                "Y. Fang"
            ],
            "title": "Detection of microaneurysms in fundus images based on improved YOLOv4 with SENet",
            "year": 2022
        },
        {
            "authors": [
                "W. Shi",
                "J. Caballero",
                "F. Husz\u00e1r",
                "J. Totz",
                "A.P. Aitken",
                "R. Bishop",
                "D. Rueckert",
                "Z. Wang"
            ],
            "title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "S.J. Pan",
                "Q. Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Trans. Knowl. Data Eng",
            "year": 2009
        },
        {
            "authors": [
                "X. Li",
                "W. Wang",
                "L. Wu",
                "S. Chen",
                "X. Hu",
                "J. Li",
                "J. Tang",
                "J. Yang"
            ],
            "title": "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zheng",
                "P. Wang",
                "W. Liu",
                "J. Li",
                "R. Ye",
                "D. Ren"
            ],
            "title": "Distance-IoU loss: Faster and better learning for bounding box regression",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Tong",
                "Y. Chen",
                "Z. Xu",
                "R. Yu"
            ],
            "title": "Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism",
            "venue": "arXiv 2023,",
            "year": 2023
        },
        {
            "authors": [
                "R.R. Akut"
            ],
            "title": "FILM: Finding the location of microaneurysms on the retina",
            "venue": "Biomed. Eng. Lett. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "M. Abdar",
                "F. Pourpanah",
                "S. Hussain",
                "D. Rezazadegan",
                "L. Liu",
                "M. Ghavamzadeh",
                "P. Fieguth",
                "X. Cao",
                "A. Khosravi",
                "U.R. Acharya"
            ],
            "title": "A review of uncertainty quantification in deep learning: Techniques, applications and challenges",
            "venue": "Inf. Fusion",
            "year": 2021
        },
        {
            "authors": [
                "S. Seoni",
                "V. Jahmunah",
                "M. Salvi",
                "P.D. Barua",
                "F. Molinari",
                "U.R. Acharya"
            ],
            "title": "Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013\u20132023)",
            "venue": "Comput. Biol. Med",
            "year": 2023
        },
        {
            "authors": [
                "S.K. Khare",
                "U.R. Acharya"
            ],
            "title": "Adazd-Net: Automated adaptive and explainable Alzheimer\u2019s disease detection system using EEG signals",
            "venue": "Knowl.-Based Syst",
            "year": 2023
        },
        {
            "authors": [
                "S.K. Khare",
                "V. Blanes-Vidal",
                "E.S. Nadimi",
                "U.R. Acharya"
            ],
            "title": "Emotion recognition and artificial intelligence: A systematic review (2014\u20132023) and research recommendations",
            "venue": "Inf. Fusion 2023,",
            "year": 1020
        }
    ],
    "sections": [
        {
            "text": "Citation: Zhang, B.; Li, J.; Bai, Y.;\nJiang, Q.; Yan, B.; Wang, Z. An\nImproved Microaneurysm Detection\nModel Based on SwinIR and\nYOLOv8. Bioengineering 2023, 10,\n1405. https://doi.org/10.3390/\nbioengineering10121405\nAcademic Editors: Antonio Brunetti\nand Andrea Cataldo\nReceived: 9 November 2023\nRevised: 23 November 2023\nAccepted: 26 November 2023\nPublished: 8 December 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: diabetic retinopathy; microaneurysm; deep learning; YOLOv8; SwinIR"
        },
        {
            "heading": "1. Introduction",
            "text": "Diabetic retinopathy (DR) is one of microvascular complications affecting the retina, caused by diabetes. It is also known as a major cause of blindness worldwide [1,2]. The pathogenesis of DR is tightly associated with an altered vessel structure due to increased blood glucose levels. Initially, DR presents as tiny dilations of capillaries, known as microaneurysms (MAs) [3,4]. MAs are primarily distributed in the inner nuclear layer and deep capillary plexus and are often an early clinical manifestation of various retinal and systemic diseases, including DR, retinal vein occlusion, and infections. In fundus images, MAs appear as small dots and a visible pathology at the early stages of DR. Therefore, the accurate detection of MA is crucial for the prevention, diagnosis, and treatment of DR [5]. The advancement of modern retinal imaging techniques, such as fundus fluorescein angiography (FFA) and non-mydriatic fundus photography (NMFCS), has improved the identification of MAs. FFA is a technique that uses the injection of a contrast agent to observe the retinal vessels. NMFCS refers to a non-invasive imaging technique of capturing retinal images through fundus photography. Figure 1 shows images obtained by FFA and NMFCS. The fundus images obtained by FFA exhibit higher contrast and clearer\nBioengineering 2023, 10, 1405. https://doi.org/10.3390/bioengineering10121405 https://www.mdpi.com/journal/bioengineering\nBioengineering 2023, 10, 1405 2 of 16\npresentation of the features of the retinal structure compared to the images obtained by NMFCS. In the clinical practice, FFA is widely recognized as an important standard for visualizing the retinal vasculature and describing subtle vascular changes.\nBioengineering 2023, 10, x FOR PEER REVIEW 2 of 17\nNMFCS. The fundus images obtained by FFA exhibit higher contrast and clearer presen-\ntation of the features of the retinal structure compared to the images obtained by NMFCS.\nIn the clinical practice, FFA is widely recognized as a importa t standard for visualizing\nthe reti al vasculature and describing subtle vasc lar ch nges.\nFigure 1. Fundus images. (a) FFA image; (b) NMFCS image.\nFigure 2 shows a normal FFA image and an FFA image with microaneurysms. In the\nFFA image, MAs typically appear as round and bright spot-like structures with diameters\nranging from 10 \u00b5m to 100 \u00b5m. MAs hold an important value in disease diagnosis and\nscreening. Their objective quantitative evaluation is still limited as it requires manual de-\ntection by experienced technicians.\nFigure 2. FFA images. (a) Normal FFA image; (b) FFA image with MAs.\nOver the last two decades, automatic detection models for MAs have rapidly devel-\noped based on deep learning. The convolutional neural network (CNN) is a deep learning\nalgorithm that extracts features from images through multiple layers of convolution and\npooling operations and utilizes the fully connected layers for classification or regression\ntasks. The CNN has achieved great success in the field of image processing and is widely\nused for object recognition and semantic segmentation. Object recognition using CNNs\nhas great advantages such as high accuracy, application flexibility, automation, and real-\ntime performance, providing support for practical applications such as SSD [6], RetinaNet\n[7], YOLOv5, YOLOv7 [8], and YOLOX [9]. Meanwhile, previous studies reported several\nsegmentation models for the automatic detection of MAs. Liao et al. proposed a deep con-\nvolutional encoder\u2013decoder network with a weighted dice loss for MA localization [10].\nXia et al. introduced a multi-scale model for detecting and classifying MAs using residual\nand efficient networks [11]. Chudzik et al. proposed a three-stage detection method as an\nalternative to the traditional five-stage MA detection. This study demonstrated successful\ntransfer learning between small MA datasets [12]. Zhou et al. proposed a collaborative\nlearning model based on a fine-tuning detection module in a semi-supervised manner to\nimprove the performance of MA detection [13]. Xie et al. proposed a segmentation\u2013emen-\ndation\u2013resegmentation\u2013verification framework to predict and correct detection errors in\n(a) (b)\n(a) (b)\nFigure 1. Fundus images. (a) F A image; (b) NMFCS image.\nFigure 2 shows a normal F A image and an F A image with microaneurysms. In the F A image, MAs typically ap ear as round and bright spot-like structures with diameters ranging from 10 \u00b5m to 100 \u00b5m. MAs hold an important value in disease diagnosis and screening. heir objective quantitative evaluation is still limited as it requires manual detection by experienced technicians.\nBioengineering 2023, 10, x FOR PEER REVIEW 2 of 17\nNMFCS. The fundus images obtained by FFA exhibit higher contrast and clearer presen-\ntation of the features of the retinal structure compared to the images obtained by NMFCS.\nIn the clinical practice, FFA is widely recognized as an important standard for visualizing\nthe retinal vasculature and describing subtle vascular changes.\nFigure 1. Fundus images. (a) FFA image; (b) NMFCS image.\nFigure 2 shows a nor al FFA image and an FFA image with microaneurysms. In the\nFFA i age, As typically appear as round and bright spot-like structures ith dia eters\nranging fro 10 \u00b5m to 100 \u00b5m. MAs hold an important value in disease diagnosis and\nscreening. Their objective quantitative evaluation is still limited as it requires manual de-\ntection by experienced technicians.\nFigure 2. FFA images. (a) Normal FFA image; (b) FFA image with MAs.\nOver the last two decades, automatic detection models for MAs have rapidly devel-\noped based on deep learning. The convolutional neural network (CNN) is a deep learning\nalgorithm that extracts features from images through multiple layers of convolution and\npooling operations and utilizes the fully connected layers for classification or regression\ntasks. The CNN has achieved great success in the field of image processing and is widely\nused for object recognition and semantic segmentation. Object recognition using CNNs\nhas great advantages such as high accuracy, application flexibility, automation, and real-\ntime performance, providing support for practical applications such as SSD [6], RetinaNet\n[7], YOLOv5, YOLOv7 [8], and YOLOX [9]. Meanwhile, previous studies reported several\nsegmentation models for the automatic detection of MAs. Liao et al. proposed a deep con-\nvolutional encoder\u2013decoder network with a weighted dice loss for MA localization [10].\nXia et al. introduced a multi-scale model for detecting and classifying MAs using residual\nand efficient networks [11]. Chudzik et al. proposed a three-stage detection method as an\nalternative to the traditional five-stage MA detection. This study demonstrated successful\ntransfer learning between small MA datasets [12]. Zhou et al. proposed a collaborative\nlearning model based on a fine-tuning detection module in a semi-supervised manner to\nimprove the performance of MA detection [13]. Xie et al. proposed a segmentation\u2013emen-\ndation\u2013resegmentation\u2013verification framework to predict and correct detection errors in\n(a) (b)\n(a) (b)\nFigure 2. FFA i ages. (a) or al FFA i age; (b) FFA image with MAs.\nOver t ca es, automatic detection models for MAs have rapidly developed based on de p learning. The convolutional neural network (CNN) is a deep learning algorithm that extract eatures from images through multiple layers of convoluti n and pool ng operations and utilizes the fully connected layers for classification r regression tasks. The CNN has achieved great success in the field of image processing and is widely used for object recog ition and semantic segmentation. Object recognition using CNNs h s gre t advantages such as high ccuracy, application flexibility, automation, and real-time performance, providing support for practical applications such as SSD [6], RetinaNet [7], YOL v5, YOLOv7 [8], and YOLOX [9]. Meanwhile, previous studies reported several segmentation models for the automatic detection of MAs. Liao et al. proposed a deep convolutional encoder\u2013decoder network with a weighted dice loss for MA localization [10]. Xia et al. introduced a multi-scale model for detecting and classifying MAs using residual and efficient networks [11]. Chudzik et al. proposed a three-stage detection method as an alternative to the traditional five-stage MA detection. This study demonstrated successful transfer learning between small MA datasets [12]. Zhou et al. proposed a collaborative learning model based on a fine-tuning detection module in a semisupervised manner to improve the performance of MA detection [13]. Xie et al. proposed a segmentation\u2013emendation\u2013resegmentation\u2013verification framework to predict and correct detection errors in models, enhancing the detection of MAs [14]. Wang et al. utilized a region-based fully convolutional network (R-FCN) incorporating a feature pyramid net-\nBioengineering 2023, 10, 1405 3 of 16\nwork and an improved region proposal network for MA detection [15]. Guo et al. proposed a novel end-to-end unified framework for MA detection that utilizes multi-scale feature fusion and multi-channel bin loss [16]. Mateen et al. proposed a hybrid feature embedding approach using pre-trained VGG-19 and Inception-v3 for MA detection [17]. Kumar et al. trained a model of radial basis function neural network for MA detection [18]. Table 1 shows the strengths and weaknesses of the reported models for MA detection. The abovementioned MA detection models based on deep learning have enhanced the efficiency of MA detection in FFA images. However, the tiny size of MAs, their low contrast with the background, and the lack of an annotated MA database still pose a great challenge for MA detection. Thus, further study is still required to design a novel detection method to enhance MA detection efficiency.\nMAs are relatively small in size and often appear as tiny and blurry lesions in retinal images, which are particularly pronounced in low-resolution images. They are often similar to the pixels of blood vessels. Super-resolution reconstruction is an image processing technique that can enhance the spatial resolution and detail clarity of an image by recovering high-resolution details from a low-resolution image. The Swin Transformer [19] has shown great promise as it integrates the advantages of both CNN and Transformer. The Swin Transformer processes large images using a self-attention mechanism and models long-range dependencies with a shifted window scheme. An image restoration model, SwinIR [20], was designed based on the Swin Transformer. SwinIR could not only enhance the detail features of MAs, but also improve the visibility and discriminability of MAs in FFA images. Except for tiny MAs in FFA images, sample imbalance and loss of information are two problems to be solved that affect the accuracy and efficiency of MA detection. YOLOv8 is an object recognition algorithm that is characterized by its ability to perform object localization and classification in a single forward pass. YOLOv8 contains a backbone, a neck, and a head. The neck section utilizes the path aggregation network (PAN)\u2013feature pyramid network (FPN) structure for feature fusion [21,22]. FPN constructs a multi-scale feature pyramid by adding lateral connections and up-sampling layers to capture rich semantic information and better detect objects of different sizes. PAN addresses the issue of feature propagation in FPN by aggregating and propagating features through horizontal and vertical paths. PAN\u2013FPN combines the strengths of FPN and PAN to provide powerful feature representation capabilities. The backbone and neck sections of YOLOv8 draw inspiration from the design principles of YOLOv7 ELAN. The C3 structure of YOLOv5\nBioengineering 2023, 10, 1405 4 of 16\nis replaced with the C2f structure in YOLOv8, which has a richer gradient flow, allowing for a better capturing of image details and contextual information. Given its powerful identification efficiency, multi-scale feature fusion, and contextual information capturing, the proposed MA detection model was designed based on YOLOv8. Therefore, an improved MA detection model for FFA images based on SwinIR and YOLOv8 is proposed, called MA-YOLO. The major contributions of this study proposing the new MA-YOLO model are as follows:\n\u2022 SwinIR was used to reconstruct high-resolution FFA images, which could enhance the visibility and discriminability of MAs in FFA images. \u2022 A detection layer was added to the YOLOv8 model, which could avoid feature information loss in shallow layers and improve the performance of MA detection. \u2022 Transfer learning was utilized between high- and low-resolution images to expand the data samples and improve the generalization ability. \u2022 Taking Wise-IoU as the bounding box regression loss, the loss function of MA-YOLO was improved, which could relieve the sample distribution imbalance problem and enhance the generalization performance.\nIn addition, the proposed MA-YOLO model could calculate the MA area in FFA images, which would assist ophthalmologists in assessing the progression of DR."
        },
        {
            "heading": "2. Materials and Methods",
            "text": "2.1. Materials 2.1.1. Datasets\nThe experimental dataset used in this study contained two datasets. The first dataset was collaboratively constructed by the Nanjing Medical University-Affiliated Eye Hospital and includes 1200 FFA images (768 \u00d7 868 pixels) from 1200 eyes of DR patients (age range, 31\u201381 years). Image acquisition was performed using the Heidelberg retina angiograph (Heidelberg Engineering, Germany) device. To ensure data quality, the dataset excluded images that were blurry or overexposed because of environmental factors or equipment materials. The second dataset originated from a study conducted at the Persian Eye Clinic (Feiz Hospital) at the Isfahan University of Medical Sciences. The dataset includes 70 retinal images (576 \u00d7 720 pixels) from a total of 70 patients, with 30 images classified as normal, and 40 images representing different stages of abnormality. Prior to image collection, each patient underwent a comprehensive ophthalmic evaluation, which involved medical history assessment, applanation tonometry, slit-lamp examination, dilated fundus biomicroscopy, and ophthalmoscopy [23]. Based on the above datasets, a total of 1240 FFA images were selected as the experimental dataset. All images were resized to 768 \u00d7 768 pixels and annotated by clinical doctors with more than 10 years of clinical experiences. The 1240 FFA images were independently divided into 992 training FFA images, 124 validation FFA images, and 124 test FFA images.\n2.1.2. Implementation\nThe hardware configuration used for the experiment was Ubuntu 20.04.5, 2GPUs, GPU NVIDIA RTX 2080ti and 1 GPU memory (11 GB). As software, we used the deep-learning framework Pytorch 2.0.0 and the programming language python 3.8.\n2.1.3. Evaluation Metrics\nFive metrics were calculated to estimate the performance of MA detection [24], i.e., recall (Re), precision (Pre), F1 score (F1), average precision (AP), and frames per second (FPS).\nRe = TP/(TP + FN) (1)\nPre = TP/(TP + FP) (2)\nBioengineering 2023, 10, 1405 5 of 16\nF1 = 2\u00d7 Pre\u00d7 Re/(Pre + Re) (3)\nAP = \u222b 1\n0 Pre(Re)dRe (4)\nFPS = frameNum\nelapsedTime (5)\nTP, FP, and FN denote true positive regions, false positive regions, and false negative regions, respectively. frameNum is the number of FFA images inputted into the detection model; elapsedTime is the time consumed by the detection model. Re and Pre are the proportion of correct predictions in all MAs and the proportion of real MAs in the samples predicted as MAs. F1 is a balanced metric determined by precision and recall. AP is the area under the precision\u2013recall (PR) curve, obtained by plotting recall on the x-axis and precision on the y-axis, based on a set of precision and recall values calculated at different thresholds. FPS is the number of FFA images inferred per second."
        },
        {
            "heading": "2.2. Methods",
            "text": "Figure 3 illustrates the flowchart of the proposed MA-YOLO model. In this model, SwinIR was used for the super-resolution reconstruction of FFA images (Figure 3A). The MA detection layer was added to modify YOLOv8 (Figure 3B), and transfer learning was applied to increase the amount of data. The Wise-IoU loss was utilized to enhance the detection capability (Figure 3C). Bioengineering 2023, 10, x FOR PEER REVIEW 6 of 17\nBioengineering 2023, 10, 1405 6 of 16\n2.2.1. Super-Resolution FFA Image Reconstruction Based on SwinIR\nMAs are small in size and usually appear as tiny and blurry structures in FFA images. The subtle features of MAs are easily lost in low-resolution images. Therefore, reconstructing high-resolution images is helpful for MA detection in FFA images. Here, SwinIR was employed to perform the super-resolution reconstruction of FFA images. SwinIR, an image restoration technique, contains three modules, i.e., shallow feature extraction, deep feature extraction, and HQ image reconstruction (high-quality image reconstruction modules). The shallow feature extraction module uses a convolution layer to extract shallow features, which are directly transmitted to the reconstruction module and preserve low-frequency information. The deep feature extraction module is mainly composed of residual Swin Transformer blocks (RSTB), each of which utilizes several Swin Transformer layers for self-attention and cross-window interaction. Additionally, a convolution layer was incorporated at the end of the block to enhance features. A residual connection was utilized to establish a shortcut for feature aggregation. Finally, both shallow and deep features were transmitted to the HQ image reconstruction module, which used the sub-pixel convolution layer [25] to up-sample the features for high-quality image reconstruction. Figure 4 illustrates the structure of the residual Swin Transformer block and the Swin Transformer layer. Bioengineering 2023, 10, x FOR PEER REVIEW 7 of 17\nBased on SwinIR, the original FFA images with a size of 768 \u00d7 768 pixels were recon-\nstructed into super-resolution FFA images with a size of 1536 \u00d7 1536 pixels and 2304 \u00d7 2304\npixels, respectively, which effectively enhanced the detail features of MAs.\n2.2.2. YOLOv8 Modified by MA Detection Layer and Transfer Learning\nDuring the down-sampling of the convolution layers of YOLOv8, the regions con-\ntaining MAs become blurred, and it was difficult to accurately localize the MAs. Thus,\ndown-sampling convolution caused the loss of small features and missed and false MA\ndetection.\nHere, an MA detection layer was introduced into the neck and head of YOLOv8 to\nhandle shallow feature maps from the P2 layer of the backbone network and integrate\nthem into the PAN\u2013FPN structure. The architecture of the MA detection layer is shown in\nFigure 5. The MA detection layer up-sampled deep-level feature maps with stronger semantic features from the FPN structure and then concatenated them with shallow-level feature maps outpu ed by the P2 layer of the backbone network, enhancing the semantic expression of the shallow-level features. After feature extraction by the c2f module, the resulting features were passed into the added detection head. Simultaneously, the MA detection layer down-sampled the obtained feature maps using convolution, concatenated them with the deep-level feature maps, and then underwent another feature extraction by the c2f module. This process integrated the feature information extracted from the shallow levels into the PAN structure, enhancing the model\u2019s localization capability at various scales. Based on the modified YOLOv8, small MA features could be obtained, and the accuracy of MA detection could be enhanced.\ni r 4. . r f l ( ; ( ) i Transfor er layer (STL).\nS inIR, the original FFA images with a size of 768 \u00d7 768 pixels were reconstruc ed into super-resolution FFA images with a size of 1536 \u00d7 1536 pixels and 2304 \u00d7 2304 pixels, respectively, which effectively nhanced the detail features of MAs.\n2.2.2. YOLOv8 odified by A Detection Layer and Transfer Learning\nDuring the down-sampling of the convolution layers of YOLOv8, the regions containing MAs become blurred, and it was difficult to accurately localize the MAs. Thus, down-sampling convolution caused the loss of small features and missed and false MA detection. Here, an MA detection layer was introduced into the neck and head of YOLOv8 to handle shallow feature maps from the P2 layer of the backbone network and integrate them into the PAN\u2013FPN structure. The architecture of the MA detection layer is shown in Figure 5. The MA detection layer up-sampled deep-level feature maps with stronger semantic features from the FPN structure and then concatenated them with shallow-level\nfeature maps outputted by the P2 layer of the backbone network, enhancing the semantic\nexpression of the shallow-level features. After feature extraction by the c2f module, the\nresulting features were passed into the added detection head. Simultaneously, the MA\ndetection layer down-sampled the obtained feature maps using convolution, concatenated\nthem with the deep-level feature maps, and then underwent another feature extraction by\nthe c2f module. This process integrated the feature information extracted from the shallow\nlevels into the PAN structure, enhancing the model\u2019s localization capability at various\nscales. Based on the modified YOLOv8, small MA features could be obtained, and the\naccuracy of MA detection could be enhanced.\nBioengineering 2023, 10, 1405 7 of 16\nBioengineering 2023, 10, x FOR PEER REVIEW 7 of 17 Figure 4. Architecture of RSTB and STL. (a) Residual Swin Transformer block (RSTB); (b) Swin Transformer layer (STL). Based on SwinIR, the original FFA images with a size of 768 \u00d7 768 pixels were reconstructed into super-resolution FFA images with a size of 1536 \u00d7 1536 pixels and 2304 \u00d7 2304 pixels, respectively, which effectively enhanced the detail features of MAs. 2.2.2. YOLOv8 Modified by MA Detection Layer and Transfer Learning During the down-sampling of the convolution layers of YOLOv8, the regions containing MAs become blurred, and it was difficult to accurately localize the MAs. Thus, down-sampling convolution caused the loss of small features and missed and false MA detection. Here, an MA detection layer was introduced into the neck and head of YOLOv8 to handle shallow feature maps from the P2 layer of the backbone network and integrate them into the PAN\u2013FPN structure. The architecture of the MA detection layer is shown in Figure 5. The MA detection layer up-sampled deep-level feature maps with stronger semantic features from the FPN structure and then concatenated them with shallow-level feature maps outpu ed by the P2 layer of the backbone network, enhancing the semantic expression of the shallow-level features. After feature extraction by the c2f module, the\nresulting features were passed into the added detection head. Simultaneously, the MA\ndetection layer down-sampled the obtained feature maps using convolution, concate-\nnated them with the deep-level feature maps, and then underwent another feature extrac-\ntion by the c2f module. This process integrated the feature information extracted from the\nshallow levels into the PAN structure, enhancing the model\u2019s localization capability at\nvarious scales. Based on the modified YOLOv8, small MA features could be obtained, and\nthe accuracy of MA detection could be enhanced.\nFigure 5. Architecture of the MA detection layer.\nDue to the limited amount of MA data, few annotated samples are available for train-\ning and evaluation, and it is a challenge to construct accurate and reliable models. Here,\nFig re 5. rchitecture f t e et cti l r\nue to the limited amount of MA data, few annotated samples are available for training and evaluation, and it is a challenge to construct accurate and reliable models. Here, transfer learning was used to modify YOLOv8. Transfer learning [26] is a machine learning technique that leverages the knowledge gained from one task to improve its performance on different but related tasks. Transfer learning is usually used to transfer pre-trained models or features. Using transfer learning, three different datasets were leveraged while training the model, including the original MA images with a size of 768 \u00d7 768 pixels and two superresolution reconstructed images with a size of 1536 \u00d7 1536 and 2304 \u00d7 2304 pixels. Figure 6 shows the flowchart of transfer learning applied to these three different datasets. Based on the original MA images of 768 \u00d7 768 pixels, the detection model was pre-trained, and the learned knowledge was retained. Based on the super-resolution reconstructed images of 1536 \u00d7 1536 pixels, the detection model was transferred and fine-tuned. The learned knowledge was updated. Based on the super-resolution reconstructed images of 2304 \u00d7 2304 pixels, the detection model was transferred and fine-tuned, and the learned knowledge was updated again.\nBioengineering 2023, 10, x FOR PEER REVIEW 8 of 17\ntransfer learning was used to modify YOLOv8. Transfer learning [26] is a machine learn-\ning technique that leverages the knowledge gained from one task to improve its perfor-\nmance on different but related tasks. Transfer learning is usually used to transfer pre-\ntrained models or features.\nUsing transfer learning, three different datasets were leveraged while training the\nmodel, including the original MA images with a size of 768 \u00d7 768 pixels and two super-\nresolution reconstructed images with a size of 1536 \u00d7 1536 and 2304 \u00d7 2304 pixels. Figure\n6 shows the flowchart of transfer learning applied to these three different datasets. Based\non the original MA images of 768 \u00d7 768 pixels, the detection model was pre-trained, and\nthe learned knowledge was retained. Based on the super-resolution reconstructed images\nof 1536 \u00d7 1536 pixels, the detection model was transferred and fine-tuned. The learned\nknowledge was updated. Based on the super-resolution reconstructed images of 2304 \u00d7\n2304 pixels, the detection model was transferred and fine-tuned, and the learned\nknowledge was updated again.\nFigure 6. Flowchart of transfer learning.\n2.2.3. Loss Function Optimization Based on Wise-IoU\nThe loss function of the official YOLOv8 consists of two components: classification\nand regression. For classification, binary cross-entropy loss (BCEL) is used as the loss func-\ntion, while for regression, distribution focal loss (DFL) [27] and CIoU [28] bounding box\nregression loss (CIoUL) are incorporated.\nThe loss function of YOLOv8 is represented as\nloss = + + , (6)\nOn the basis of the official YOLOv8 weight parameter se ings, the weight parameters\n, , and were always set to 0.05, 0.15, and 0.75, respectively.\nBCEL is defined as\n= weight [ class ] \u2212 [ class ] + \u2211 ( [ ]) , (7)\nwhere class is the number of categories, weight [ class ] denotes the weight for each class,\nand is the probability value after sigmoid activation.\nDFL is an optimization of the focal loss function, which generalizes the discrete re-\nsults of classification into continuous results through integration, denoted as\n( , ) = \u2212 ( \u2212 )log ( ) + ( \u2212 )log ( ) , (8)\nwhere , represent the values from the left and right sides near the consecutive la-\nbels , satisfying < < , = \u2211 ( ) , ( ) = ; can be implemented\nthrough a softmax layer.\nAccording to the calculation of the overlap between the ground truth box and the\npredicted box and the differences in center point distance and aspect ratio, CIoUL reflects\nthe similarity and accuracy of two bounding boxes and is defined as\n= 1 \u2212 + ,\n+ , (9)\nFigure 6. Flowchart of transfer learning.\n2.2.3. Loss Function Optimization Based on Wise-IoU\nThe loss function of the official YOLOv8 consists of two components: classification and regression. For classification, binary cross-entropy loss (BCEL) is used as the loss function, while for regression, distribution focal loss (DFL) [27] and CIoU [28] bounding box regression loss (CIoUL) are incorporated.\nThe loss function of YOLOv8 is represented as\nfloss = \u03bb1 fBCEL + \u03bb2 fDFL + \u03bb3 fCIoUL, (6)\nBioengineering 2023, 10, 1405 8 of 16\nOn the basis of the official YOLOv8 weight parameter settings, the weight parameters \u03bb1, \u03bb2, and \u03bb3 were always set to 0.05, 0.15, and 0.75, respectively.\nBCEL is defined as\nfBCEL = weight [ class ](\u2212x[ class ] + log(\u2211 j exp(x[j]))), (7)\nwhere class is the number of categories, weight [ class ] denotes the weight for each class, and x is the probability value after sigmoid activation. DFL is an optimization of the focal loss function, which generalizes the discrete results of classification into continuous results through integration, denoted as\nfDFL(Si, Si+1) = \u2212((yi+1 \u2212 y)log(Si) + (y\u2212 yi)log(Si+1)), (8)\nwhere yi, yi+1 represent the values from the left and right sides near the consecutive labels y, satisfying yi < y < yi+1, y = \u2211 ni=0 P(yi)yi , P(yi) = Si; P can be implemented through a softmax layer. According to the calculation of the overlap between the ground truth box and the predicted box and the differences in center point distance and aspect ratio, CIoUL reflects the similarity and accuracy of two bounding boxes and is defined as\nfCIoUL = 1\u2212 IOU + \u03c12 ( b, bgt ) c2 + \u03b1v, (9)\nv = 4\n\u03c02\n( arctan wgt\nhgt \u2212 arctan w h\n)2 , (10)\n\u03b1 = v\n1\u2212 IOU + v , (11)\nwhere \u03c12(b,bgt)\nc2 is the distance between the centers of the target box and the prediction box, c is the distance between the diagonal points of the smallest enclosing box, wgt and hgt represent the size of the target box, and w and h represent the size of the prediction box. However, CIoUL ignores the issue of sample distribution imbalance and presents limitations in relation to small MAs and in the presence of a large background noise. Here, CIoUL was replaced with Wise-IoU [29] bounding box regression loss. The Wise-IoU loss function uses a dynamic focusing mechanism to evaluate the quality of the anchor box, where an \u201coutlier\u201d is used to avoid excessive penalties for geometric factors (such as distance and aspect ratio). Additionally, the Wise-IoU loss borrows the idea of focal loss, using a focus coefficient constructed to reduce the contribution of samples easy to evaluate to the loss value. The Wise-IoU loss function is defined as\nLWIoU = \u03b2\u03b3exp (x\u2212 xgt)2 + (y\u2212 ygt)2( W2g + H2g )\u2217 LIoU , (12)\n\u03b2 = L\u2217IoU LIoU \u2208 [0,+\u221e), (13)\nwhere wg, Hg are the size of the smallest enclosing box, x and y represent the coordinate values of the prediction box, xgt and ygt represent the coordinate values of the ground truth, \u03b3 is an adjustable hyperparameter, set to 0.5, and \u03b2 indicates the degree of abnormality of the prediction box (a small degree of abnormality means that the quality of the anchor box is high). Therefore, \u03b2 can assign small gradient gains to prediction boxes with large outliers, effectively reducing the harmful gradients of low-quality training samples.\nBioengineering 2023, 10, 1405 9 of 16"
        },
        {
            "heading": "3. Results",
            "text": "To evaluate the detection performance of the MA-YOLO model, two comparative experiments were performed. Experiment one was an ablation experiment, where the MA-YOLO model was compared with YOLOv8 with different settings. Experiment two was a comparative experiment, where the detection performance of the MA-YOLO model was compared with that of other models, including SSD, RetinaNet, YOLOv5, YOLOX, and YOLOv7."
        },
        {
            "heading": "3.1. Ablation Experiment",
            "text": "The proposed MA-YOLO model was compared to YOLOv8 with different settings, including YOLOv8 with SwinIR (YOLOv8-A), YOLOv8 with SwinIR and transfer learning (YOLOv8-B), YOLOv8 with the Wise-IoU loss function (YOLOv8-C), and YOLOv8 with the MA detection layer (YOLOv8-D). Figure 7 and Table 2 show the results of MA detection and the evaluation metrics for MA detection by YOLOv8 with different settings. Bioengineering 2023, 10, x FOR PEER REVIEW 11 of 17\nBioengineering 2023, 10, 1405 10 of 16\nTable 2. Comparison of the MA detection performance between YOLOv8 models with different settings.\nModel\nImprovement Strategy\nRe (%) Pre (%) F1 (%) AP (%) FPS (It/s) SwinIR TransferLearning Wise-IoU MA Detection\nLayer\nYOLOv8 80.81 \u00b1 0.03 86.07 \u00b1 0.03 83.36 \u00b1 0.03 82.09 \u00b1 0.03 16.11 \u00b1 0.02 YOLOv8-A X 83.44 \u00b1 0.01 85.50 \u00b1 0.04 84.46 \u00b1 0.04 83.46 \u00b1 0.09 1.99 \u00b1 0.05 YOLOv8-B X X 85.22 \u00b1 0.12 88.07 \u00b1 0.14 86.62 \u00b1 0.11 84.13 \u00b1 0.10 1.99 \u00b1 0.04 YOLOv8-C X 84.65 \u00b1 0.07 86.73 \u00b1 0.06 85.68 \u00b1 0.08 87.29 \u00b1 0.12 16.11 \u00b1 0.01 YOLOv8-D X 86.15 \u00b1 0.05 93.19 \u00b1 0.09 89.53 \u00b1 0.06 88.67 \u00b1 0.03 12.79 \u00b1 0.04 MA-YOLO X X X X 88.23 \u00b1 0.11 97.98 \u00b1 0.06 92.85 \u00b1 0.09 94.62 \u00b1 0.06 1.51 \u00b1 0.03\nBased on Table 2, MA-YOLO provided the best scores of Re, Pre, F1, and AP in MA detection, which were 88.23, 97.98, 92.85, and 94.62, respectively. However, due to the addition of the MA detection layer, FPS was 1.51, which was lower than that of YOLOv8. Compared with the YOLOv8 model, YOLOv8-A allowed improving the scores of Re, F1, and AP, which were 83.44 (2.63\u2191), 84.46 (1.1\u2191), and 83.46 (1.37\u2191), respectively. YOLOv8-B also allowed improving the scores of Re, Pre, F1, and AP, which were 85.22 (4.41\u2191), 88.07 (2\u2191), 86.62 (3.26\u2191), and 84.13 (2.04\u2191), respectively. The same was observed for YOLOv8-C with scores of Re, Pre, F1, and AP of 84.65 (3.84\u2191), 86.73 (0.66\u2191), 85.68 (2.32\u2191), and 87.29 (5.2\u2191), respectively. Also YOLOv8-D allowed improving the scores of Re, Pre, F1, and AP, which were 86.15 (5.34\u2191), 93.19 (7.12\u2191), 89.53 (6.17\u2191), and 88.67 (6.58\u2191), respectively. As shown in Figure 7, MA-YOLO provided the best performance for MA detection, with few missed and false detection results. We observed some false MA detection with the YOLOv8-A and YOLOv8-B models and some missed MA detection with the YOLOv8, YOLOv8-A, YOLOv8-B, YOLOv8-C, and YOLOv8-D models. Figures 8 and 9 illustrate the comparison of the loss curves and AP curves of the validation set between the original images and the super-resolution FFA images, where X1 denotes the original images with a size of 768 \u00d7 768 pixels, X2 the super-resolution images with a size of 1536 \u00d7 1536 pixels, and X3 the super-resolution images with a size of 2304 \u00d7 2304 pixels. Based on Figures 8 and 9, it is evident that the model trained with super-resolution images demonstrated superior convergence trends and detection performance compared to the model trained with the original images. Bioengineering 2023, 10, x FOR PEER REVIEW 12 of 17 (5.2\u2191), respectively. Also YOLOv8-D allowed improving the scores of Re, Pre, F1, and AP, which were 86.15 (5.34\u2191), 93.19 (7.12\u2191), 89.53 (6.17\u2191), and 88.67 (6.58\u2191), respectively. As shown in Figure 7, MA-YOLO provided the best performance for MA detection, ith few missed and false detection results. We observed som false MA detection with the YOLOv8-A and YOLOv8-B models an som missed MA detection with the YOLOv8, YOLOv8-A, YOLOv8-B, YOLOv8-C, and YOLOv8-D models. Figures 8 and 9 illustrate the comparison of the loss curves and AP curves of the validation set between the original images and the super-resolution FFA images, where X1 denotes the original images with a size of 768 \u00d7 768 pixels, X2 the super-resolution images with a size of 1536 \u00d7 1536 pixels, nd X3 the super-resolution imag s with a size of 2304 \u00d7"
        },
        {
            "heading": "2304 pixels. Based on Figures 8 and 9, it is evident that the model trained with superresolution images demonstrated superior convergence trends and detection performance compared to the model trained with the original images.",
            "text": "Figure 9. Comparison of the AP curves between the original images and the super-resolution images.\nre 8. Comparison f the loss curves between the original images and the super-resolution images.\nBioengineering 2023, 10, 1405 11 of 16\nBioengineering 2023, 10, x FOR PEER REVIEW 12 of 17 (5.2\u2191), respectively. Also YOLOv8-D allowed improving the scores of Re, Pre, F1, and AP, which were 86.15 (5.34\u2191), 93.19 (7.12\u2191), 89.53 (6.17\u2191), and 88.67 (6.58\u2191), respectively. As shown in Figure 7, MA-YOLO provided the best performance for MA detection, with few missed and false detection results. We observed some false MA detection with the YOLOv8-A and YOLOv8-B models and some missed MA detection with the YOLOv8, YOLOv8-A, YOLOv8-B, YOLOv8-C, and YOLOv8-D models. Figures 8 and 9 illustrate the comparison of the loss curves and AP curves of the validation set between the original images and the super-resolution FFA images, where X1 denotes the original images with a size of 768 \u00d7 768 pixels, X2 the super-resolution images with a size of 1536 \u00d7 1536 pixels, and X3 the super-resolution images with a size of 2304 \u00d7 2304 pixels. Based on Figures 8 and 9, it is evident that the model trained with superresolution images demonstrated superior convergence trends and detection performance compared to the model trained with the original images.\nFigure 8. Comparison of the loss curves between the original images and the super-resolution im-\nages.\nFigure 9. Comparison of the AP curves between the original images and the super-resolution images.\nigure 9. Comparison f the AP curves between the original ima es nd the super-resolution imag ."
        },
        {
            "heading": "3.2. Comparison Experiment",
            "text": "To evaluate the performance of MA detection, the proposed MA-YOLO model was compared with other models, including SSD, RetinaNet, YOLOv5, YOLOX, and YOLOv7. SSD is a classic one-stage object recognition algorithm, and its high detection speed makes it highly valuable for practical applications. RetinaNet has enhanced the ability of object recognition models to detect small objects by introducing focal loss. YOLOv5, YOLOX, and YOLOv7 are all part of the series of YOLO algorithms, representing newer models introduced in recent years. In addition, two reports were also selected to evaluate the proposed model\u2019s performance in detecting MAs [24,30]. Tables 3 and 4 show the comparison of MA detection performance and tuning parameters during the training phase among different models. Figure 10 shows the MA detection results of different models, where the red boxes represent the detection results with a confidence score greater than 0.5, the yellow boxes indicate missed detection, and the green boxes represent false positive detection. Table 5 shows the comparison of the MA detection performance of different object recognition models reported in various studies. According to Figure 10 and Tables 3 and 4, the detection results of MA-YOLO were close to ground truth. Part of the background was mistakenly detected by the YOLOv5, YOLOX, and YOLOv7 models. We observed some missed MA detections by the SSD, RetinaNet, YOLOv5, YOLOX, and YOLOv7 models. MA-YOLO achieved the highest Re, Pre, F1, and AP scores compared to the other models and a higher FPS score than RetinaNet. According to Table 5, the detection performance of MA-YOLO was superior to that of the other examined methods.\nBioengineering 2023, 10, 1405 12 of 16\nFigure 10. MA detection results by different models.\n3.3. Calculation of the MA Region\nIn addition to MA detection, the MA area was calculated by the inscribed circle\nwithin the detection bounding box. The MA area could serve as an indicator to assess the\nprogression of DR. Figure 11 shows the calculation results for the MA area, where the unit of measure is \u03bcm2. The MA area was calculated in FFA images captured by a Heidelberg retina angiograph with a 55\u00b0 lens and included 768 \u00d7 768 pixels, with each pixel corre-\nsponding to 25 micrometers in reality.\nFigure 10. MA detection results by different models.\nBioengineering 2023, 10, 1405 13 of 16\nTable 5. Comparison of MA detection performance among different studies.\nModel Re (%) Pre (%) F1 (%) AP (%)\nRohan [30] 78.9 86.7 82.61 81.3 Gao [24] 89.77 87.13 88.51 88.92 MA-YOLO 88.23 97.98 92.85 94.62"
        },
        {
            "heading": "3.3. Calculation of the MA Region",
            "text": "In addition to MA detection, the MA area was calculated by the inscribed circle within the detection bounding box. The MA area could serve as an indicator to assess the progression of DR. Figure 11 shows the calculation results for the MA area, where the unit of measure is \u00b5m2. The MA area was calculated in FFA images captured by a Heidelberg retina angiograph with a 55\u25e6 lens and included 768 \u00d7 768 pixels, with each pixel corresponding to 25 \u00b5m in reality. Bioengineering 2023, 10, x FOR PEER REVIEW 15 of 17\nFigure 11. Calculation of the MA region."
        },
        {
            "heading": "4. Discussion",
            "text": "Microaneurysms (MA) are recognized as the earliest symptom of DR that leads to reti-\nnal blood injury. The detection of MAs within FFA images facilitates the early DR diagnosis\nand prevents vision loss. However, MAs are extremely small, and their contrast with the\nsurrounding background is very subtle, which makes MA detection challenging. MA objec-\ntive and quantitative evaluation is still limited because it requires manual detection by ex-\nperienced technicians. This study has great potential by allowing the detection and precise\nlocalization of MAs in retinal images. The proposed model\u2019s outputs can be directly utilized\nby ophthalmologists for MA detection, eliminating the need for manual intervention. It con-\ntributes to the automation of MA detection, effectively guiding and assisting ophthalmolo-\ngists in the treatment and elimination of MAs. The MA area can serve as an indicator to\nassess the progression of DR. A large area indicates a more severe condition, requiring more\nproactive treatment and management measures. Changes in the MA area can provide infor-\nmation about the stability or deterioration of the condition. The proposed model can be used\nto calculate the MA area in FFA images. By regularly calculating the MA area, the progres-\nsion of DR and the effectiveness of treatments can be monitored.\nDue to the addition of the MA detection layer and the model\u2019s handling of higher-res-\nolution images, the improved performance of MA detection may result in a decrease in the\nspeed of MA detection to some extent. In addition, this proposed model was only applied\non a limited dataset, and the validation of the model performance still requires its applica-\ntion on independent data from different patient cohorts across various medical centers. Fu-\nture research will concentrate on addressing the aforementioned issues, by quantifying the\nmodel\u2019s uncertainty [31,32], enhancing the detection speed through parameter pruning, and\nconducting an in-depth analysis of the model\u2019s interpretability [33\u201335]."
        },
        {
            "heading": "5. Conclusions",
            "text": "This study proposes the MA-YOLO model for the automatic detection of MAs in FFA\nimages, based on image super-resolution reconstruction for data enhancement. This\nmethod can accurately and effectively detect MAs in FFA images. The algorithm utilized\nSwinIR for image super-resolution reconstruction, transforming the size of FFA images\nfrom 768 \u00d7 768 pixels to 1536 \u00d7 1536 pixels and 2304 \u00d7 2304 pixels. By reconstructing low-\nresolution FFA images, the details of MAs as well as their visibility and discriminability\nFigure 11. Calculation of the MA region."
        },
        {
            "heading": "4. Discus ion",
            "text": "Microaneurysms ( ized as the earliest symptom of DR that leads to retina blood injury. The detection of MAs within FFA images facilitates the early DR diagnosis a d preve ts vision loss. Howeve , MAs ar extremely small, and their con rast with the surrounding background is v ry subtle, which makes MA detection challenging. MA objective and quantitative evaluation is still limited beca se it requires manual detection by experienced technicians. T is study has great potential by allowing the detection and precise l calization of MAs in retinal images. The prop sed model\u2019s outputs can be directly utilized by ophthalmologists for MA detection, eliminating the need for manual intervention. It contributes to the automation of MA detection, effectively guiding and assisting ophthal ologists in the treatment and elimination of MAs. The MA area can serve as an indicator to assess the progression of DR. A large area indicates a more severe condition, requiring more proactive treatment and management measures. Changes in the MA area can provide information about the stability or deterioration of the condition. The proposed model can be used to calculate the MA area in FFA images. By regularly calculating the MA area, the progression of DR and the effectiveness of treatments can be monitored. Due to the addition of the MA detection layer and the model\u2019s handling of higherresolution images, the improved performance of MA detection may result in a decrease\nBioengineering 2023, 10, 1405 14 of 16\nin the speed of MA detection to some extent. In addition, this proposed model was only applied on a limited dataset, and the validation of the model performance still requires its application on independent data from different patient cohorts across various medical centers. Future research will concentrate on addressing the aforementioned issues, by quantifying the model\u2019s uncertainty [31,32], enhancing the detection speed through parameter pruning, and conducting an in-depth analysis of the model\u2019s interpretability [33\u201335]."
        },
        {
            "heading": "5. Conclusions",
            "text": "This study proposes the MA-YOLO model for the automatic detection of MAs in FFA images, based on image super-resolution reconstruction for data enhancement. This method can accurately and effectively detect MAs in FFA images. The algorithm utilized SwinIR for image super-resolution reconstruction, transforming the size of FFA images from 768 \u00d7 768 pixels to 1536 \u00d7 1536 pixels and 2304 \u00d7 2304 pixels. By reconstructing low-resolution FFA images, the details of MAs as well as their visibility and discriminability in the images were improved. Based on these improvements, the structure and loss function of the YOLOv8 model were further optimized. To address the challenges of extracting small features and the loss of feature information for MA detection, an MA detection layer was added to enhance feature extraction. Additionally, transfer learning was conducted between high-resolution and low-resolution datasets to enhance the model\u2019s generalization. The Wise-IoU bounding box regression loss was employed to avoid excessive penalization due to geometric factors, improving the model\u2019s generalization performance and addressing the problem of sample distribution imbalance. In addition, the MA-YOLO model can be used to calculate the MA area in FFA images to assist ophthalmologists in assessing the progression of DR. Using the FFA dataset, ablation experiments were conducted to analyze and validate the effectiveness of the proposed model in the automatic detection of MAs. Furthermore, the proposed model was compared with five detection algorithms, i.e., SSD, YOLOv5, YOLOv7, YOLOX, and RetinaNet. The results showed that the proposed model outperformed these algorithms in terms of MA detection. The MA-YOLO model is thus a prospective approach for the early diagnosis of DR. In the future, the model will be further improved by incorporating more feature learning capabilities to achieve a higher detection speed.\nAuthor Contributions: Z.W. and Q.J. were responsible for the conceptualization and data collection. Z.W. and B.Z. were responsible for the experiment design and manuscript writing; J.L. and Y.B. conducted the data collection and data entry; B.Y. was responsible for overall supervision and manuscript revision. All authors have read and agreed to the published version of the manuscript.\nFunding: National Natural Science Foundation of China (Grant No. 82271101 and 82070983) and Natural Science Foundation of Jiangsu Province (Grant No. BK20211020).\nInstitutional Review Board Statement: The study was conducted in accordance with the Declaration of Helsinki and was approved by the Ethics Committee of the Affiliated Eye Hospital, Nanjing Medical University (Identifier: NJMUEH-2021-08-16).\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the study.\nData Availability Statement: The data used to support the findings of this study are included within the article.\nConflicts of Interest: The authors declare that there are no conflict of interest regarding the publication of this paper.\nReferences 1. Alifanov, I.; Sakovych, V. Prognostic risk factors for diabetic retinopathy in patients with type 2 diabetes mellitus. J. Ophthalmol. 2022, 6, 19\u201323. [CrossRef] 2. Yau, J.W.; Rogers, S.L.; Kawasaki, R.; Lamoureux, E.L.; Kowalski, J.W.; Bek, T.; Chen, S.-J.; Dekker, J.M.; Fletcher, A.; Grauslund, J.\nGlobal prevalence and major risk factors of diabetic retinopathy. Diabetes Care 2012, 35, 556\u2013564. [CrossRef] [PubMed]\nBioengineering 2023, 10, 1405 15 of 16\n3. Walter, T.; Massin, P.; Erginay, A.; Ordonez, R.; Jeulin, C.; Klein, J.-C. Automatic detection of microaneurysms in color fundus images. Med. Image Anal. 2007, 11, 555\u2013566. [CrossRef] [PubMed] 4. Couturier, A.; Man\u00e9, V.; Bonnin, S.; Erginay, A.; Massin, P.; Gaudric, A.; Tadayoni, R. Capillary plexus anomalies in diabetic retinopathy on optical coherence tomography angiography. Retina 2015, 35, 2384\u20132391. [CrossRef] [PubMed] 5. Wu, B.; Zhu, W.; Shi, F.; Zhu, S.; Chen, X. Automatic detection of microaneurysms in retinal fundus images. Comput. Med. Imaging Graph. 2017, 55, 106\u2013112. [CrossRef] [PubMed] 6. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.-Y.; Berg, A.C. SSD: Single shot multibox detector. In Proceedings of the Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, 11\u201314 October 2016; Proceedings, Part 14; pp. 21\u201337. 7. Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 8. Wang, C.-Y.; Bochkovskiy, A.; Liao, H.-Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 17\u201324 June 2023; pp. 7464\u20137475. 9. Ge, Z.; Liu, S.; Wang, F.; Li, Z.; Sun, J. Yolox: Exceeding yolo series in 2021. arXiv 2021, arXiv:2107.08430. 10. Liao, Y.; Xia, H.; Song, S.; Li, H. Microaneurysm detection in fundus images based on a novel end-to-end convolutional neural network. Biocybern. Biomed. Eng. 2021, 41, 589\u2013604. [CrossRef] 11. Xia, H.; Lan, Y.; Song, S.; Li, H. A multi-scale segmentation-to-classification network for tiny microaneurysm detection in fundus images. Knowl.-Based Syst. 2021, 226, 107140. [CrossRef] 12. Chudzik, P.; Majumdar, S.; Caliv\u00e1, F.; Al-Diri, B.; Hunter, A. Microaneurysm detection using fully convolutional neural networks. Comput. Methods Programs Biomed. 2018, 158, 185\u2013192. [CrossRef] 13. Zhou, Y.; He, X.; Huang, L.; Liu, L.; Zhu, F.; Cui, S.; Shao, L. Collaborative learning of semi-supervised segmentation and\nclassification for medical images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 2079\u20132088.\n14. Xie, Y.; Zhang, J.; Lu, H.; Shen, C.; Xia, Y. SESV: Accurate medical image segmentation by predicting and correcting errors. IEEE Trans. Med. Imaging 2020, 40, 286\u2013296. [CrossRef] 15. Wang, J.; Luo, J.; Liu, B.; Feng, R.; Lu, L.; Zou, H. Automated diabetic retinopathy grading and lesion detection based on the modified R-FCN object-detection algorithm. IET Comput. Vis. 2020, 14, 1\u20138. [CrossRef] 16. Guo, S.; Li, T.; Kang, H.; Li, N.; Zhang, Y.; Wang, K. L-Seg: An end-to-end unified framework for multi-lesion segmentation of fundus images. Neurocomputing 2019, 349, 52\u201363. [CrossRef] 17. Mateen, M.; Malik, T.S.; Hayat, S.; Hameed, M.; Sun, S.; Wen, J. Deep Learning Approach for Automatic Microaneurysms Detection. Sensors 2022, 22, 542. [CrossRef] [PubMed] 18. Kumar, S.; Adarsh, A.; Kumar, B.; Singh, A.K. An automated early diabetic retinopathy detection through improved blood vessel and optic disc segmentation. Opt. Laser Technol. 2020, 121, 105815. [CrossRef] 19. Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC, Canada, 10\u201317 October 2021; pp. 10012\u201310022. 20. Liang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; Timofte, R. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC, Canada, 10\u201317 October 2021; pp. 1833\u20131844. 21. Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201323 June 2018; pp. 8759\u20138768. 22. Lin, T.-Y.; Doll\u00e1r, P.; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 2117\u20132125. 23. Hajeb Mohammad Alipour, S.; Rabbani, H.; Akhlaghi, M. A new combined method based on curvelet transform and morphological operators for automatic detection of foveal avascular zone. Signal Image Video Process. 2014, 8, 205\u2013222. [CrossRef] 24. Gao, W.; Shan, M.; Song, N.; Fan, B.; Fang, Y. Detection of microaneurysms in fundus images based on improved YOLOv4 with SENet embedded. Sheng Wu Yi Xue Gong Cheng Xue Za Zhi = J. Biomed. Eng. 2022, 39, 713\u2013720. 25. Shi, W.; Caballero, J.; Husz\u00e1r, F.; Totz, J.; Aitken, A.P.; Bishop, R.; Rueckert, D.; Wang, Z. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 1874\u20131883. 26. Pan, S.J.; Yang, Q. A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 2009, 22, 1345\u20131359. [CrossRef] 27. Li, X.; Wang, W.; Wu, L.; Chen, S.; Hu, X.; Li, J.; Tang, J.; Yang, J. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Adv. Neural Inf. Process. Syst. 2020, 33, 21002\u201321012. 28. Zheng, Z.; Wang, P.; Liu, W.; Li, J.; Ye, R.; Ren, D. Distance-IoU loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI Conference on Artificial Intelligence, New York, NY, USA, 7\u201312 February 2020; pp. 12993\u201313000. 29. Tong, Z.; Chen, Y.; Xu, Z.; Yu, R. Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism. arXiv 2023, arXiv:2301.10051. 30. Akut, R.R. FILM: Finding the location of microaneurysms on the retina. Biomed. Eng. Lett. 2019, 9, 497\u2013506. [CrossRef]\nBioengineering 2023, 10, 1405 16 of 16\n31. Abdar, M.; Pourpanah, F.; Hussain, S.; Rezazadegan, D.; Liu, L.; Ghavamzadeh, M.; Fieguth, P.; Cao, X.; Khosravi, A.; Acharya, U.R. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Inf. Fusion 2021, 76, 243\u2013297. [CrossRef] 32. Seoni, S.; Jahmunah, V.; Salvi, M.; Barua, P.D.; Molinari, F.; Acharya, U.R. Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013\u20132023). Comput. Biol. Med. 2023, 165, 107441. [CrossRef] 33. Khare, S.K.; Acharya, U.R. Adazd-Net: Automated adaptive and explainable Alzheimer\u2019s disease detection system using EEG signals. Knowl.-Based Syst. 2023, 278, 110858. [CrossRef] 34. Khare, S.K.; Blanes-Vidal, V.; Nadimi, E.S.; Acharya, U.R. Emotion recognition and artificial intelligence: A systematic review (2014\u20132023) and research recommendations. Inf. Fusion 2023, 102, 102019. [CrossRef] 35. Khare, S.K.; March, S.; Barua, P.D.; Gadre, V.M.; Acharya, U.R. Application of data fusion for automated detection of children with developmental and mental disorders: A systematic review of the last decade. Inf. Fusion 2023, 99, 101898. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "An Improved Microaneurysm Detection Model Based on SwinIR and YOLOv8",
    "year": 2023
}