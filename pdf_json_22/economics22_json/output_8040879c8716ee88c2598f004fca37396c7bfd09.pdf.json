{
    "abstractText": "In today\u2019s economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs). Specifically, in an MPP, a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximizes the sender\u2019s cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. \u01eb-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration. Our main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender\u2019s utility functions, prior distributions, and the Markov transition kernels. For such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment. Meanwhile, we additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver\u2019s detrimental equilibrium behavior. Our algorithm enjoys *University of Virginia. Email: jw7jb@virginia.edu. University of Science and Technology of China. Email: zhangzixuan@mail.ustc.edu.cn. Google. Email: zhef@google.com. Northwestern University. Email: zhaoranwang@gmail.com. Yale University. Email: zhuoran.yang@yale.edu. UC Berkeley. Email: jordan@cs.berkeley.edu. **University of Virginia. Email: hx4ad@virginia.edu.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jibang Wu"
        },
        {
            "affiliations": [],
            "name": "Zixuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhe Feng"
        },
        {
            "affiliations": [],
            "name": "Zhaoran Wang"
        },
        {
            "affiliations": [],
            "name": "Zhuoran Yang"
        },
        {
            "affiliations": [],
            "name": "Michael I. Jordan"
        },
        {
            "affiliations": [],
            "name": "Haifeng Xu"
        }
    ],
    "id": "SP:47c0e12f7a0ce35751e6891af6d956f7cbb51e25",
    "references": [
        {
            "authors": [
                "A. Agarwal",
                "N. Jiang",
                "S.M. Kakade",
                "W. Sun"
            ],
            "title": "Reinforcement learning: Theory and algorithms",
            "venue": "CS Dept., UW Seattle,",
            "year": 2019
        },
        {
            "authors": [
                "P. Auer",
                "T. Jaksch",
                "R. Ortner"
            ],
            "title": "Near-optimal regret bounds for reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "A. Ayoub",
                "Z. Jia",
                "C. Szepesvari",
                "M. Wang",
                "L. Yang"
            ],
            "title": "Model-based reinforcement learning with value-targeted regression",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "M.G. Azar",
                "I. Osband",
                "R. Munos"
            ],
            "title": "Minimax regret bounds for reinforcement learning",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "K. Azuma"
            ],
            "title": "Weighted sums of certain dependent random variables",
            "venue": "Tohoku Mathematical Journal, Second Series,",
            "year": 1967
        },
        {
            "authors": [
                "A. Badanidiyuru",
                "K. Bhawalkar",
                "H. Xu"
            ],
            "title": "Targeting and signaling in ad auctions",
            "venue": "In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM",
            "year": 2018
        },
        {
            "authors": [
                "R. Bellman"
            ],
            "title": "A markovian decision process",
            "venue": "Indiana Univ. Math. J.,",
            "year": 1957
        },
        {
            "authors": [
                "D. Bergemann",
                "S. Morris"
            ],
            "title": "Information design: A unified perspective",
            "venue": "Journal of Economic Literature,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Cai",
                "Z. Yang",
                "C. Jin",
                "Z. Wang"
            ],
            "title": "Provably efficient exploration in policy optimization",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "V. Conitzer",
                "T. Sandholm"
            ],
            "title": "Computing the optimal strategy to commit to",
            "venue": "In Proceedings of the 7th ACM conference on Electronic commerce",
            "year": 2006
        },
        {
            "authors": [
                "C. Dann",
                "N. Jiang",
                "A. Krishnamurthy",
                "A. Agarwal",
                "J. Langford",
                "R.E. Schapire"
            ],
            "title": "On oracle-efficient pac rl with rich observations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "C. Dann",
                "T. Lattimore",
                "E. Brunskill"
            ],
            "title": "Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "S. Du",
                "S. Kakade",
                "J. Lee",
                "S. Lovett",
                "G. Mahajan",
                "W. Sun",
                "R. Wang"
            ],
            "title": "Bilinear classes: A structural framework for provable generalization in rl",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "S.S. Du",
                "S.M. Kakade",
                "R. Wang",
                "L.F. Yang"
            ],
            "title": "Is a good representation sufficient for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016",
            "year": 2019
        },
        {
            "authors": [
                "S. Dughmi",
                "H. Xu"
            ],
            "title": "Algorithmic bayesian persuasion",
            "venue": "SIAM Journal on Computing",
            "year": 2019
        },
        {
            "authors": [
                "F. Farhadi",
                "D. Teneketzis"
            ],
            "title": "Dynamic information design: a simple problem on optimal sequential information disclosure",
            "venue": "Dynamic Games and Applications",
            "year": 2021
        },
        {
            "authors": [
                "S. Filippi",
                "O. Cappe",
                "A. Garivier",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Parametric bandits: The generalized linear case",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "J. Gan",
                "R. Majumdar",
                "G. Radanovic",
                "A. Singla"
            ],
            "title": "Bayesian persuasion in sequential decision-making",
            "venue": "arXiv preprint arXiv:2106.05137",
            "year": 2021
        },
        {
            "authors": [
                "I. Giannoccaro",
                "P. Pontrandolfo"
            ],
            "title": "Inventory management in supply chains: a reinforcement learning approach",
            "venue": "International Journal of Production Economics,",
            "year": 2002
        },
        {
            "authors": [
                "I. Goldstein",
                "Y. Leitner"
            ],
            "title": "Stress tests and information disclosure",
            "venue": "Journal of Economic Theory,",
            "year": 2018
        },
        {
            "authors": [
                "J. He",
                "D. Zhou",
                "Q. Gu"
            ],
            "title": "Logarithmic regret for reinforcement learning with linear function approximation",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "N. Jiang",
                "A. Krishnamurthy",
                "A. Agarwal",
                "J. Langford",
                "R.E. Schapire"
            ],
            "title": "Contextual decision processes with low bellman rank are pac-learnable",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "C. Jin",
                "Z. Allen-Zhu",
                "S. Bubeck",
                "M.I. Jordan"
            ],
            "title": "Is q-learning provably efficient",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "C. Jin",
                "Z. Yang",
                "Z. Wang",
                "M.I. Jordan"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation",
            "venue": "In Conference on Learning Theory. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "J. Kober",
                "J.A. Bagnell",
                "J. Peters"
            ],
            "title": "Reinforcement learning in robotics: A survey",
            "venue": "The International Journal of Robotics Research,",
            "year": 2013
        },
        {
            "authors": [
                "J. Li",
                "W. Monroe",
                "A. Ritter",
                "M. Galley",
                "J. Gao",
                "D. Jurafsky"
            ],
            "title": "Deep reinforcement learning for dialogue generation",
            "venue": "arXiv preprint arXiv:1606.01541",
            "year": 2016
        },
        {
            "authors": [
                "L. Li",
                "Y. Lu",
                "D. Zhou"
            ],
            "title": "Provable optimal algorithms for generalized linear contextual bandits. CoRR, abs/1703.00048",
            "year": 2017
        },
        {
            "authors": [
                "L. Li",
                "Y. Lu",
                "D. Zhou"
            ],
            "title": "Provably optimal algorithms for generalized linear contextual bandits",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "M. Li",
                "Z. Qin",
                "Y. Jiao",
                "Y. Yang",
                "J. Wang",
                "C. Wang",
                "G. Wu",
                "J. Ye"
            ],
            "title": "Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "E. Liang",
                "K. Wen",
                "W.H. Lam",
                "A. Sumalee",
                "R. Zhong"
            ],
            "title": "An integrated reinforcement learning and centralized programming approach for online taxi dispatching",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2021
        },
        {
            "authors": [
                "Y. Mansour",
                "A. Slivkins",
                "V. Syrgkanis",
                "Z.S. Wu"
            ],
            "title": "Bayesian exploration: Incentivizing exploration in bayesian games. Operations Research",
            "year": 2021
        },
        {
            "authors": [
                "H. Meisheri",
                "V. Baniwal",
                "N.N. Sultana",
                "H. Khadilkar",
                "B. Ravindran"
            ],
            "title": "Using reinforcement learning for a large variable-dimensional inventory management problem",
            "year": 2020
        },
        {
            "authors": [
                "S. Milano",
                "M. Taddeo",
                "L. Floridi"
            ],
            "title": "Recommender systems and their ethical challenges",
            "venue": "Ai & Society,",
            "year": 2020
        },
        {
            "authors": [
                "G. Neu",
                "C. Pike-Burke"
            ],
            "title": "A unifying view of optimism in episodic reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "I. Osband",
                "B. Van Roy",
                "Z. Wen"
            ],
            "title": "Generalization and exploration via randomized value functions",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2016
        },
        {
            "authors": [
                "B. Peng",
                "W. Shen",
                "P. Tang",
                "S. Zuo"
            ],
            "title": "Learning optimal strategies to commit to",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "M.L. Puterman"
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Z. Qin",
                "X. Tang",
                "Y. Jiao",
                "F. Zhang",
                "Z. Xu",
                "H. Zhu",
                "J. Ye"
            ],
            "title": "Ride-hailing order dispatching at didi via reinforcement learning",
            "venue": "INFORMS Journal on Applied Analytics,",
            "year": 2020
        },
        {
            "authors": [
                "Z.T. Qin",
                "H. Zhu",
                "J. Ye"
            ],
            "title": "Reinforcement learning for ridesharing: A survey",
            "venue": "IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE",
            "year": 2021
        },
        {
            "authors": [
                "Z. Rabinovich",
                "A.X. Jiang",
                "M. Jain",
                "H. Xu"
            ],
            "title": "Information disclosure as a means to security",
            "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. Citeseer",
            "year": 2015
        },
        {
            "authors": [
                "J. Renault",
                "E. Solan",
                "N. Vieille"
            ],
            "title": "Optimal dynamic information provision",
            "venue": "Games and Economic Behavior,",
            "year": 2017
        },
        {
            "authors": [
                "D. Russo"
            ],
            "title": "Worst-case regret bounds for exploration via randomized value functions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "A.L. Strehl",
                "L. Li",
                "E. Wiewiora",
                "J. Langford",
                "M.L. Littman"
            ],
            "title": "Pac model-free reinforcement learning",
            "venue": "In Proceedings of the 23rd international conference on Machine learning",
            "year": 2006
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 2018
        },
        {
            "authors": [
                "T.Y. Tang",
                "P. Winoto"
            ],
            "title": "I should not recommend it to you even if you will like it: the ethics of recommender systems",
            "venue": "New Review of Hypermedia and Multimedia,",
            "year": 2016
        },
        {
            "authors": [
                "R. Wang",
                "R. Salakhutdinov",
                "L.F. Yang"
            ],
            "title": "Provably efficient reinforcement learning with general value function approximation",
            "year": 2020
        },
        {
            "authors": [
                "T. Wang",
                "D. Zhou",
                "Q. Gu"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation under adaptivity constraints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "R. Wang",
                "S.S. Du",
                "A. Krishnamurthy"
            ],
            "title": "Optimism in reinforcement learning with generalized linear function approximation",
            "venue": "arXiv preprint arXiv:1912.04136",
            "year": 2019
        },
        {
            "authors": [
                "W. Xiao",
                "H. Zhao",
                "H. Pan",
                "Y. Song",
                "V.W. Zheng",
                "Q. Yang"
            ],
            "title": "Beyond personalization: Social content recommendation for creator equality and consumer satisfaction",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "year": 2019
        },
        {
            "authors": [
                "H. Xu",
                "Z. Rabinovich",
                "S. Dughmi",
                "M. Tambe"
            ],
            "title": "Exploring information asymmetry in twostage security games",
            "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence",
            "year": 2015
        },
        {
            "authors": [
                "L. Yang",
                "M. Wang"
            ],
            "title": "Sample-optimal parametric q-learning using linearly additive features",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yang",
                "C. Jin",
                "Z. Wang",
                "M. Wang",
                "M.I. Jordan"
            ],
            "title": "Bridging exploration and general function approximation in reinforcement learning: Provably efficient kernel and neural value iterations",
            "venue": "CoRR, abs/2011.04622",
            "year": 2020
        },
        {
            "authors": [
                "A. Zanette",
                "A. Lazaric",
                "M. Kochenderfer",
                "E. Brunskill"
            ],
            "title": "Learning near optimal policies with low inherent bellman error",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "S. Zheng",
                "A. Trott",
                "S. Srinivasa",
                "N. Naik",
                "M. Gruesbeck",
                "D.C. Parkes",
                "R. Socher"
            ],
            "title": "The ai economist: Improving equality and productivity with ai-driven tax policies",
            "venue": "arXiv preprint arXiv:2004.13332",
            "year": 2020
        },
        {
            "authors": [
                "D. Zhou",
                "Q. Gu",
                "C. Szepesvari"
            ],
            "title": "Nearly minimax optimal reinforcement learning for linear mixture markov decision processes",
            "venue": "In Conference on Learning Theory. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "D. Zhou",
                "J. He",
                "Q. Gu"
            ],
            "title": "Provably efficient reinforcement learning for discounted mdps with feature mapping",
            "venue": "In International Conference on Machine Learning. PMLR",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 2.\n10 67\n8v 1\n[ cs\n.A I]\n2 2\nFe b\nIn today\u2019s economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs). Specifically, in an MPP, a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximizes the sender\u2019s cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. \u01eb-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration.\nOur main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender\u2019s utility functions, prior distributions, and the Markov transition kernels. For such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment. Meanwhile, we additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver\u2019s detrimental equilibrium behavior. Our algorithm enjoys\n*University of Virginia. Email: jw7jb@virginia.edu. \u2020University of Science and Technology of China. Email: zhangzixuan@mail.ustc.edu.cn. \u2021Google. Email: zhef@google.com. \u00a7Northwestern University. Email: zhaoranwang@gmail.com. \u00b6Yale University. Email: zhuoran.yang@yale.edu. ||UC Berkeley. Email: jordan@cs.berkeley.edu.\n**University of Virginia. Email: hx4ad@virginia.edu.\nsample efficiency by achieving a sublinear \u221a T -regret upper bound. Furthermore, both our algorithm and theory can be applied to MPPs with large space of outcomes and states via function approximation, and we showcase such a success under the linear setting."
        },
        {
            "heading": "1 Introduction",
            "text": "Most sequential decision models assume that there is a sole agent who possesses and processes all relevant (online or offline) information and takes an action accordingly. However, the economic literature on information design [26, 8] highlights the importance of considering information asymmetry in decision making, where the decision maker and information possessor may be two parties having different interests and goals. For example, a ride-sharing platform holds historical and real-time data on active riders and driver types in different locations, based on which they have developed centralized combinatorial optimization algorithms and reinforcement learning algorithms for vehicle repositioning, routing and order matching to optimize their operational efficiency and profit [33, 42, 34, 43]. But the de facto decision makers are the drivers. Moreover, as increasingly many drivers are freelancers instead of employees, the platform cannot expect to give mandatory orders to them. On the other hand, if the platform shares no information on rider demand, most drivers will not be able to efficiently find profitable trips. Therefore, it is not only realistic but also necessary to consider an information design problem that aligns the interests of the two parties in sequential decision making processes of this kind.\nGiven the large data sets being collected by corporations and governments, with avowed goals that relate data analysis to social welfare, it is timely to pursue formal treatments of sequential information design, to understand how to strategically inform the (sequential) decision makers (e.g., users, clients or citizens) impacted by centralized data analysis. In particular, we wish to understand the resulting equilibrium outcomes of both parties. As a concrete example, consider an online shopping platform which may make use of learning tools such as reinforcement learning or online convex optimization to manage inventory and ensure profitability [20, 36]. The platform cannot single-handedly manage its inventory, instead it requires information design (a.k.a., Bayesian persuasion) in its interactions with its suppliers and consumers. On the supply side, it could strategically reveal aspects of consumer sentiment (e.g., rough number of visits, search) to the suppliers in order to guide their sales expectation and negotiate for lower unit prices. On the demand side, it could tactically control displayed product information (e.g., last five remaining, editor\u2019s choice) so as to influence consumers\u2019 perception of products and consequently their purchase decisions. Similar situations can be anticipated for a recommendation platform. On the one hand, it should recommend most relevant items to its users for click-through and engagement. On the other hand, its recommendations are subject to misalignments with long-term objectives such as profits (e.g., from paid promotion), social impact (e.g., to prevent misinformation and filter bubbles) or development of a creator ecosystem [49, 53, 37]."
        },
        {
            "heading": "1.1 Our Results and Contributions",
            "text": "To provide a formal foundation for the study of sequential information design, we introduce the Markov persuasion process (MPP), where a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximize the sender\u2019s cumulative utility in a finite-horizon Markovian environment with varying prior and utility functions. We need to address a key challenge regarding the planning problem in MPPs; specifically, how to find persuasive signaling policies that are also optimized for the sender\u2019s long-term objective. Moreover, in face of the uncertainty for both the environment and receivers, there is a dilemma that the optimal policy based on estimated prior is not necessarily persuasive and thus cannot induce the desired trajectory, whereas a full information revelation policy is always persuasive but usually leads to suboptimal cumulative utility. So the reinforcement learning algorithm in MPPs has to ensure optimality under the premise of robust persuasiveness. This makes our algorithm design non-trivial and regret analysis highly challenging.\nWe show how to surmount these analysis and design challenges, and present a no-regret learning algorithm, which we refer to as Optimism-Pessimism Principle for Persuasion Process (OP4), that provably achieves a O\u0303 (\u221a\nd2\u03c6d 3 \u03c8H\n4T )\nregret with high probability, where d\u03c6, d\u03c8 are dimensions of the feature spaces,\nH is the horizon length in each episode, T is the number of episodes, and O\u0303(\u00b7) hides logarithmic factors as well as problem-dependent parameters. To establish this result, in Section 3.3 we start by constructing a modified formulation of the Bellman equation that can efficiently determine the optimal (resp. \u01eb-optimal) policy with finite (resp. infinite) states and outcomes. Section 4.2 then considers the learning problem, in particular the design of the OP4 that adopts both the optimistic principle in utility estimation to incentivize exploration and the pessimism principle in prior estimation to prevent a detrimental equilibrium for the receiver. In Sections 4.3 and 4.4, we showcase OP4 in the tabular MPPs and contextual Bayesian persuasion problem, respectively, both of which are practical special cases of MPPs. In Section 5, we then generalize these positive results to MPPs with large outcome and state spaces via linear function approximation and generalized linear models.\nIn summary, our contributions are threefold. At the conceptual level, we identify the need for sequential information design in real-world problems and accordingly formulate a novel model, the MPP, to capture the misaligned incentives between the (sequential) decision makers and information possessors. At the methodological level, our key insight is a new algorithmic principle\u2014optimism to encourage exploration and pessimism to induce robust equilibrium behavior. Finally, at the technical level, we develop a novel regret decomposition tailored to this combination of optimism and pessimism in the design of online learning algorithms. The fact that the combined optimism-pessimism concept can still lead to O( \u221a T ) regret for strategic setups was not clear before our new regret decomposition lemma. We expect this design principle and our proof techniques can be useful for other strategic learning problems."
        },
        {
            "heading": "1.2 Related Work",
            "text": "Our work is built on the foundation of information design and reinforcement learning. We refer the readers to Section 2.1 and 2.2 for background and formal introductions. Here we focus on the technical and modeling comparisons with related work from dynamic Bayesian persuasion and efficient reinforcement learning.\nDynamic Bayesian persuasion. Starting from seminal work by Kamenica and Gentzkow [26], the study of Bayesian persuasion looks at the design problem to influence an uninformed decision maker through strategic information revelation. Many variants of this model have been studied, with applications in security, advertising, finance, etc. [44, 54, 21, 6]. More recently, several dynamic Bayesian persuasion frameworks have been proposed to model the long-term interest of the sender. Many papers [16, 45, 17, 29] consider the setting where the sender observes the evolving states of a Markov chain, seeks to influence the receiver\u2019s belief of the state through signaling and thereby persuade him to take certain actions. In contrast to our setting, at each round, the receiver\u2019s action has no influence on the evolution of the Markov process and thus can only maximizes his utility on his belief of current state, given all the historical signals received from the sender. In Ely [16], Farhadi and Teneketzis [17], the Markov chain has two states (one is absorbing): the receiver is interested in detecting the jump to the absorbing state, whereas the sender seeks to prolong the time to detection of such a jump. Renault et al. [45] shows a greedy disclosure policy that ignores its influence to the future utility can be optimal in Markov chain with special utility functions. Lehrer and Shaiderman [29] characterize optimal strategies under different discount factors as well as the optimal values the sender could achieve. Closer to our model is that of Gan et al. [19]\u2014we both assume the Markov environment with state transition influenced by receiver\u2019s action, as well as a separate persuasion state drawn from a prior independent of receiver\u2019s action. However, Gan et al. [19] focus on the planning problem for the infinite-horizon MDP, solving sender\u2019s optimal signaling policy when the environment is known in cases when the receiver is myoptic or far-sighted. In particular, it is shown as NP-hard to approximate an optimal policy against a far-sighted receiver, which also justifies our interest on the myoptic receiver. Another closely related work [61] studies the learning problem in repeated persuasion setting (without Markov state transition) between a stream of myopic receivers and a sender without initial knowledge of the prior. It introduces the notion of regret as well as the robustness principle to this learning problem that we adopt and generalize to our model.\nBayesian Incentive-Compatible Bandit Exploration. Our work is also loosely related to a seminal result by Mansour et al. [35], who model the misaligned incentives between a system (i.e., sender) and a stream of myopic agents (i.e., receivers). Mansour et al. [35] shows that using information asymmetry, the system can create intrinsic incentives for agents to follow its recommendations. In this problem, the sender\u2019s objective is limited to the social welfare, i.e, the cumulative utility of all agents, whereas we make no assumption on the sender\u2019s utility function and thus her long-term objective. Besides our model is designed to capture more general situations where each receiver could have different priors and utility functions, and the environment might be Markovian with dynamics under the influence of the receivers\u2019 actions.\nEfficient Reinforcement Learning. Reinforcement learning has seen its successful applications in various domains, such as robotics, finance and dialogue systems [27, 58, 30]. Along with the empirical success, we have seen a growing quest to establish provably efficient RL methods. Classical sample efficiency results focus on tabular environments with small, finite state spaces [2, 39, 4, 12, 47, 24, 46]. Notably, through the design principle, known as optimism in the face of uncertainty [28], an RL algorithm would provably incur a \u2126( \u221a |S||A|T ) regret under the tabular setting, where S and A are the state and action spaces respectively [24, 4]. More recently, there have been advances in RL with function approximation, especially the linear case. Jin et al. [25] proposed an efficient algorithm for a setting where the transition kernel and the utility function are both linear functions with respect to a feature mapping: \u03c6 : S \u00d7 A \u2192 Rd. A similar assumption has been studied for different settings and has led to sample efficiency results [55, 14, 38, 57, 22]. Moreover, other general function approximations have been studied in parallel, including generalized linear function approximation [52], linear mixture MDPs based on a ternary feature mapping [3, 60, 9, 59], kernel approximation [56] as well as models based on the low Bellman rank assumption [23, 11]. We make use of these function approximation techniques to model our conditional prior, and we show how to integrate the persuasion structure into these efficient reinforcement learning frameworks, thereby obtaining sample efficient result for large-scale MPPs."
        },
        {
            "heading": "2 Preliminaries",
            "text": "This section provides some necessary background in information design and Markov decision processes, as preparation for our model of Markov persuasion processes presented in the next section."
        },
        {
            "heading": "2.1 Basics of Information Design",
            "text": "Classic information design [26] considers the persuasion problem between a single sender (she) and receiver (he). The receiver is the only actor, and looks to take an action a \u2208 Awhich results in receiver utility u(\u03c9, a) and sender utility v(\u03c9, a). Here \u03c9 \u2208 \u2126 is the realized outcome of certain environment uncertainty, which is drawn from a prior distribution \u00b5 \u2208 \u2206(\u2126), and A is a finite set of available actions for the receiver. While u, v : \u2126 \u00d7 A \u2192 [0, 1] and the prior distribution \u00b5 are all common knowledge, the sender possesses an informational advantage and can privately observe the realized outcome \u03c9. The persuasion problem studies how the sender can selectively reveal her private information about \u03c9 to influence the receiver\u2019s decisions and ultimately maximize her own expected utility v.\nTo model the sender\u2019s strategic revelation of information, it is standard to use a signaling scheme, which essentially specifies the conditional distribution of a random variable (namely the signal), given the outcome \u03c9. Before the realization of the outcome, the sender commits to such a signaling scheme. Given the realized outcome, the sender samples a signal from the conditional distribution according to the signaling scheme and reveals it to the receiver. Upon receiving this signal, the receiver infers a posterior belief about the\noutcome via Bayes\u2019 theorem (based on the correlation between the signal and outcome \u03c9 as promised by the signaling scheme) and then chooses an action a that maximizes the expected utility.\nA standard revelation-principle-style argument shows that it is without loss of generality to focus on direct and persuasive signaling schemes [26]. A scheme is direct if each signal corresponds to an action recommendation to the receiver, and is persuasive if the recommended action indeed maximizes the receiver\u2019s a posteriori expected utility. More formally, in a direct signaling scheme , \u03c0 = (\u03c0(a|\u03c9) : \u03c9 \u2208 \u2126, a \u2208 A), \u03c0(a|\u03c9) denotes the probability of recommending action a given realized outcome \u03c9. Upon receiving an action recommendation a, the receiver computes a posterior belief for \u03c9: Pr(\u03c9|a) = \u00b5(\u03c9)\u03c0(a|\u03c9)\u2211\n\u03c9\u2032 \u00b5(\u03c9 \u2032)\u03c0(a|\u03c9\u2032) . Thus,\nthe action recommendation a is persuasive if and only if amaximizes the expected utility w.r.t. the posterior belief about \u03c9; i.e., \u2211 \u03c9 Pr(\u03c9|a) \u00b7 u(\u03c9, a) \u2265 \u2211\n\u03c9 Pr(\u03c9|a) \u00b7 u(\u03c9, a\u2032) for any a\u2032 \u2208 A. Equivalently, we define persuasiveness as\nPersuasiveness: \u2211\n\u03c9\u2208\u2126\n\u00b5(\u03c9)\u03c0(a|\u03c9) \u00b7 [u(\u03c9, a)\u2212 u(\u03c9, a\u2032)] \u2265 0,\u2200a, a\u2032 \u2208 A.\nLet P = {\u03c0 : \u03c0(\u00b7|\u03c9) \u2208 \u2206(A) for each \u03c9 \u2208 \u2126} denote the set of all signaling schemes. To emphasize that the definition of persuasiveness depends on the prior \u00b5, we denote the set of persuasive schemes on prior \u00b5 by\nPers(\u00b5) := { \u03c0 \u2208 P : \u2211\n\u03c9\u2208\u2126\n\u00b5(\u03c9)\u03c0(a|\u03c9) [ u(\u03c9, a)\u2212 u ( \u03c9, a\u2032 )] \u2265 0, \u2200a, a\u2032 \u2208 A } .\nGiven a persuasive signaling scheme \u03c0 \u2208 Pers(\u00b5), it is in the receiver\u2019s best interest to take the recommended action and thus the sender\u2019s expected utility becomes V (\u00b5, \u03c0) := \u2211 \u03c9\u2208\u2126 \u2211 a\u2208A \u00b5(\u03c9)\u03c0(a|\u03c9)v(\u03c9, a).\nTherefore, given full knowledge of the persuasion instance, the sender can solve for an optimal persuasive signaling scheme that maximizes her expected utility through the following linear program (LP) which searches for a persuasive signaling scheme that maximizes V (\u00b5, \u03c0) (see, e.g., [15] for details):\nPersuasion as an LP: OPT(\u00b5) := max \u03c0\u2208Pers(\u00b5) V (\u00b5, \u03c0)."
        },
        {
            "heading": "2.2 Basics of Reinforcement Learning and Markov Decision Processes",
            "text": "The Markov decision process (MDP) [41, 48] is a classic mathematical framework for the sequential decision making problem. In this work, we focus on the model of episodic MDP. Specifically, at the beginning of the episode, the environment has an initial state s1 (possibly picked by an adversary). Then, at each step h \u2265 1, the agent takes some action ah \u2208 A to interact with environment at state sh \u2208 S . The state sh obeys a Markov property and thus captures all relevant information in the history {si}i<h. Accordingly, the agent receives the utility vh(sh, ah) \u2208 [0, 1] and the system evolves to the state of the next step sh+1 \u223c Ph(\u00b7|sh, ah). Such a process terminates after h = H , where H is also known as the horizon length. Here, A is a finite set of available actions for the agent, S is the (possibly infinite) set of MDP states. The utility\nfunction vh : S \u00d7 A \u2192 [0, 1] and transition kernel Ph : S \u00d7 A \u2192 \u2206(S) may vary at each step. A policy of the agent \u03c0h : S \u2192 \u2206(A) characterizes her decision making process at step h\u2014after observing the state s, the agent takes action a with probability \u03c0h(a|s).\nIn an episodic MDP with H steps, under policy \u03c0 = {\u03c0h}h\u2208[H], we define the value function as the expected value of cumulative utilities starting from an arbitrary state,\nV \u03c0h (s) := EP,\u03c0\n[ H\u2211\nh\u2032=h\nvh(sh\u2032 , ah\u2032) \u2223\u2223\u2223\u2223sh\u2032 = s ] , \u2200s \u2208 S, h \u2208 [H].\nHere EP,\u03c0 means that the expectation is taken with respect to the trajectory {sh, ah}h\u2208[H], which is generated by policy \u03c0 on the transition model P = {Ph}h\u2208[H]. Similarly, we define the action-value function as the expected value of cumulative utilities starting from an arbitrary state-action pair,\nQ\u03c0h(s, a) := vh(sh, ah) + EP,\u03c0\n[ H\u2211\nh\u2032=h+1\nvh(sh\u2032 , ah\u2032) \u2223\u2223\u2223\u2223sh\u2032 = s, ah\u2032 = a ] , \u2200s \u2208 S, a \u2208 A, h \u2208 [H].\nThe optimal policy is defined as \u03c0\u2217 := argmax\u03c0 V \u03c0 h (s1), which maximizes the (expected) cumulative utility. Since the agent\u2019s action affects both immediate utility and next states that influences its future utility, it thus demands careful planning to maximize total utility. Notably, \u03c0\u2217 can solved by dynamic programming based on the Bellman equation [7]. Specifically, with V \u2217H+1(s) = 0 and for each h from H to 1, iteratively update Q\u2217h(s, a) = vh(s, a) + Es\u2032\u223cP (\u00b7|s,a)V \u2217 h+1(s \u2032, a), V \u2217h (s) = maxa\u2208AQ \u2217 h(s, a), and determine the optimal policy \u03c0\u2217 as the greedy policy with respect to {Q\u2217h}h\u2208[H]. In online reinforcement learning, the agent has no prior knowledge of the environment, namely, {vh, Ph}h\u2208[H], but aims to learn the optimal policy by interacting with the environment for T episodes. For each t \u2208 [T ], at the beginning of the t-th episode, after observing the initial state st1, the agent chooses a policy \u03c0 t based on the observations before t-th episode. The discrepancy between V \u03c0 t\n1 (s t 1) and V \u2217 1 (s t 1) serves as the subop-\ntimality of the agent at the t-th episode. The performance of the online learning algorithm is measured by the expected regret, Reg(T ) := \u2211T\nt=1[V \u2217 1 (s t 1)\u2212 V \u03c0\nt\n1 (s t 1)]."
        },
        {
            "heading": "3 Markov Persuasion Processes",
            "text": "This section introduces the Markov Persuasion Process (MPP), a novel model for sequential information design in Markovian environments. It notably captures the motivating yet intricate real-world problems in Section 1. Furthermore, our MPP model is readily applicable to generalized settings with large state spaces by incorporating function approximation techniques."
        },
        {
            "heading": "3.1 A Model of Markov Persuasion Processes (MPPs)",
            "text": "We start by abstracting the sequential information design problem instances in Section 1 into MPPs. Taking as an example recommendation platform for ad keywords, we view the platform as the sender, the advertisers as the receivers. The advertisers decide the actions a \u2208 A such as whether to accept the recommended keyword. To better reflect the nature of reality, we model two types of information for MPPs, outcome and state. We use the notion of outcome \u03c9 \u2208 \u2126 to characterize the sender\u2019s private information in face of each receiver, such as the features of searchers for some keyword. The outcome follows a prior distribution such as the general demographics of Internet users on the platform. The platform can thus leverage such fine-grained knowledge on keyword features, matching with the specific ad features of each advertiser, to persuade the advertisers to take a recommendation of keywords. Meanwhile, we use the notion of state s \u2208 S to characterize the Markovian state of the environment, e.g., the availability of ad keyword slots. The state is affected by the receiver\u2019s action, as the availability changes after some keywords get brought.1 Naturally, both sender\u2019s and receiver\u2019s utility are determined by the receiver\u2019s action a jointly with the state of environment s and realized outcome \u03c9, i.e., u, v : S \u00d7 \u2126 \u00d7 A \u2192 [0, 1]. Meanwhile, as these applications could serve thousands or millions of receivers every day, to reduce the complexity of our model we assume each receiver appears only once and thus will myopically maximizes his utility at that particular step, whereas the sender is a system planner who aim to maximizes her long-term accumulated expected utility.\nMore specifically, an MPP is built on top of a standard episodic MDP with state space S , action spaceA, and transition kernel P . In this paper, we restrict our attention to finite-horizon (i.e., episodic) MPPs with H steps denoted by [H] = {1, \u00b7 \u00b7 \u00b7 ,H}, and leave the study of infinite-horizon MPPs as an interesting future direction. At a high level, there are two major differences between MPPs and MDPs. First, in a MPP, the planner cannot directly take an action but instead can leverage its information advantage and \u201cpersuade\u201d a receiver to take a desired action ah at each step h \u2208 [H]. Second, in an MPP, the state transition is affected not only by the current action ah and state sh, but also by the realized outcome \u03c9h of Nature\u2019s probability distribution. Specifically, the state transition kernel at step h is denoted as Ph(sh+1|sh, \u03c9h, ah). To capture the sender\u2019s persuasion of a receiver to take actions at step h, we assume that a fresh receiver arrives at time h with a prior \u00b5h over the outcome \u03c9h. The planner, who is the sender here, can observe the realized outcome \u03c9h and would like to strategically reveal information about \u03c9h in order to persuade the receiver to take a certain action ah.\nDiffering from classical single-shot information design, the immediate utility functions uh, vh for the 1Similarly, we can view the online shopping platform as the sender who persuades a stream of receivers (supplier, consumer) to take certain action, whether to take an offer or make a purchase. In this case, sender can privately observe the outcomes such as the consumer sentiments on some random products based on the search and click logs, whereas the states are product reviews, sales or shipping time commonly known to the public and affected by the actions of both supply and demand sides. In case of ridersharing, outcome represents the fine-grained knowledge of currently active rider types that are privately known to the platform and generally stochastic in accordance to some user demographics, whereas the state captures the general driver supply or rider demand at locations that is affected by the drivers\u2019 decisions.\nreceiver and sender vary not only at each step h but also additionally depend on the commonly observed state sh of the environment. We assume the receiver to have full knowledge of his utility uh and prior \u00b5h at each step h, and would take the recommended action ah if and only if ah maximizes his expected utility under the posterior for \u03c9h. 2\nFormally, an MPP with a horizon length H proceeds as follows at each step h \u2208 [H]:\n1. A fresh receiver with prior distribution \u00b5h \u2208 \u2206(\u2126) and utility uh : S \u00d7 \u2126\u00d7A \u2192 [0, 1] arrives.\n2. The sender commits to a persuasive signaling policy \u03c0h : S \u2192 P, which is public knowledge.\n3. After observing the realized state sh and outcome \u03c9h, the sender accordingly recommends the re-\nceiver to take an action ah \u223c \u03c0h(\u00b7|sh, \u03c9h).\n4. Given the recommended action ah, the receiver takes an action a \u2032 h, receives utility uh(sh, \u03c9h, a \u2032 h) and\nthen leaves the system. Meanwhile, the sender receives utility vh(sh, \u03c9h, a \u2032 h).\n5. The next state sh+1 \u223c Ph(\u00b7|sh, \u03c9h, a\u2032h) is generated according to Ph : S \u00d7\u2126\u00d7A \u2192 \u2206(S), the state transition kernel at the h-th step.\nHere we coin the notion of a signaling policy \u03c0h as a mapping from state to a signaling scheme at the hth step. It captures a possibly multi-step procedure in which the sender commits to a signaling scheme after observing the realized state and then samples a signal after observing the realized outcome. For notational convenience, we denote \u03c0(a|s, \u03c9) as the probability of recommending action a given state s and realized outcome \u03c9. We can also generalize the notion of persuasiveness from classic information design to MPPs. Specifically, we define Pers(\u00b5, u) as the persuasive set that contains all signaling policies that are persuasive to the receiver with utility u and prior \u00b5 for all possible state s \u2208 S:\nPers(\u00b5, u) := { \u03c0 : S \u2192 P :\n\u222b\n\u03c9\u2208\u2126 \u00b5(\u03c9)\u03c0(a|s, \u03c9)\n[ u(s, \u03c9, a)\u2212 u(s, \u03c9, a\u2032) ] d\u03c9 \u2265 0, \u2200a, a\u2032 \u2208 A, s \u2208 S } .\nRecall that P consists of all mappings from \u2126 to \u2206(A). As such, the sender\u2019s persuasive signaling scheme \u03c0h \u2208 Pers(\u00b5h, uh) is essentially a stochastic policy as defined in standard MDPs\u2014\u03c0h maps a state sh to a stochastic action ah\u2014except that here the probability of suggesting action ah by policy \u03c0h depends additionally on the realized outcome \u03c9h that is only known to the sender.\n2This assumption is not essential but just for technical rigor. Because even if receivers have limited knowledge or computational power to accurately determine the utility-maximizing actions, the sender should have sufficient ethical or legal reasons to comply with the persuasive constraints in practice. And the receivers would only take the recommendation if the platform has good reputation (i.e., persuasive with high probability).\nWe say \u03c0 := {\u03c0h}h\u2208[H] is a feasible policy of the MPP if \u03c0h \u2208 Pers(\u00b5h, uh),\u2200h \u2208 [H], because the state transition trajectory would otherwise be infeasible if the receiver is not guaranteed to take the recommended action, i.e., a\u2032h 6= ah. We denote the set of all feasible policies as PH := \u220f h\u2208[H]Pers(\u00b5h, uh)."
        },
        {
            "heading": "3.2 MPPs: the Generalized Version with Contexts and Linear Parameterization",
            "text": "To provide a broadly useful modeling concept, we also study a generalized setting of the Markov Persuasion Process with contextual prior and a possibly large space of states, outcomes and contexts.\nContextual Prior. At the beginning of each episode, a sequence of contexts C = {ch \u2208 C}h\u2208[H] is realized by Nature and becomes public knowledge. And we allow the prior \u00b5h to be influenced by the context ch at each step h, and thus denote it by \u00b5h(\u00b7|ch). Specifically, the contextual information is able to model the uncertainty such as the varying demographics of active user group affected by events (e.g., scheduled concerts or sport games in ride-sharing) at different time of the day.3 Here we allow the sequence of contexts to be adversarially generated.\nLinear Parameterization. We also relax the state, context and outcome space S, C,\u2126 to be continuous and additionally assume that the transition kernels and utility functions are linear, and the conditional priors of outcomes are generalized linear models (GLM) of the context at each steps. More formally, for each step h \u2208 [H], our linearity condition assumes:\n\u2022 The sender\u2019s utility is vh := v \u2217 h(sh, \u03c9h, ah) = \u03c8(sh, \u03c9h, ah) \u22a4\u03b3\u2217h, where (1) \u03c8(\u00b7, \u00b7, \u00b7) \u2208 Rd\u03c8 is a known feature vector; (2) \u03b3\u2217h \u2208 Rd\u03c8 is the unknown linear parameter at step h.\n\u2022 The next state sh+1 is drawn from the distribution PM,h(\u00b7|sh, \u03c9h, ah) = \u03c8(sh, \u03c9h, ah)\u22a4Mh(\u00b7), where Mh = (M (1) h ,M (2) h , . . . ,M (d\u03c8) h ) is a vector of d\u03c8 unknown measures over S at step h. \u2022 The outcome \u03c9h \u2208 R subjects to a generalized linear model (GLM), which models a wider range of hypothesis function classes.4 Given the context ch, there exists a link function f : R \u2192 R such that \u03c9h = f(\u03c6(ch)\n\u22a4\u03b8\u2217h)+zh, where \u03c6(\u00b7) \u2208 Rd\u03c6 is a feature vector and \u03b8\u2217h \u2208 Rd\u03c6 is an unknown parameter. The noises {zh}h\u2208[H] are independent \u03c3-sub-Gaussian variables with zero mean. We denote the prior of \u03c9h with parameter \u03b8 at context c as \u00b5\u03b8(\u00b7|c). Without loss of generality, we assume that there exist \u03a6,\u03a8 such that \u2016\u03c6(s)\u2016 \u2264 \u03a6,5 \u2016\u03c8(s, \u03c9, a)\u2016 \u2264 \u03a8 for all s \u2208 S, \u03c9 \u2208 \u2126 and a \u2208 A. We also assume that \u2016\u03b8\u2217h\u2016 \u2264 L\u03b8, \u2016\u03b3\u2217h\u2016 \u2264 L\u03b3 , \u2016M\u2217h\u2016 \u2264 LM , |A| \u2265 2, |\u2126| \u2265 2. Such a regularity condition is common in the RL literature.\n3In the case of the online shopping platform, the prior of consumer interests may be affected by the different holidays or seasons at different time of year. 4We note that GLM is a strictly generalization of the linear model assumption that we have for the distribution of transition kernel P . While we could use similar technique to extend the distribution of P to GLM using techniques similar to that in Wang et al. [52], but we save such an extension for simplicity, since it is not the primary focus of our work. 5For the simplicity of notation, we will omit the subscript of the norm whenever it is an L2 norm in this paper."
        },
        {
            "heading": "3.3 Optimal Signaling Policy in MPPs",
            "text": "In order to maximize the sender\u2019s utility, we study the optimal policy in MPPs, in analogy to that of standard MDPs. We start by considering the value of any feasible policy \u03c0. For each step h \u2208 [H], we define the value function for the sender V \u03c0h : S \u2192 R as the expected value of cumulative utilities under \u03c0 when starting from an arbitrary state at the h-th step. That is, for any s \u2208 S, h \u2208 [H], we define\nV \u03c0h (s) := EP,\u00b5,\u03c0\n[ H\u2211\nh\u2032=h\nvh\u2032 ( sh\u2032 , \u03c9h\u2032 , ah\u2032 )\u2223\u2223\u2223\u2223sh = s ] ,\nwhere the expectation EP,\u00b5,\u03c0 is taken with respect to the randomness of the trajectory (i.e., randomness of state transition), realized outcome and the stochasticity of \u03c0. Accordingly, we define the Q-function (actionvalue function) Q\u03c0h : S \u00d7 \u2126\u00d7A \u2192 R which gives the expected value of cumulative utilities when starting from an arbitrary state-action pair at the h-step following the signaling policy \u03c0, that is,\nQ\u03c0h(s, \u03c9, a) := vh(s, \u03c9, a) + EP,\u00b5,\u03c0\n[ H\u2211\nh\u2032=h+1\nvh\u2032 ( sh\u2032 , \u03c9h\u2032 , ah\u2032 )\u2223\u2223\u2223\u2223sh = s, \u03c9h = \u03c9, ah = a ] .\nBy definition, Qh(\u00b7, \u00b7, \u00b7), Vh(\u00b7) \u2208 [0, h], since vh(\u00b7, \u00b7, \u00b7) \u2208 [0, 1]. To simplify notation, for any Q-function Qh and any distributions \u00b5h and \u03c0h over \u2126 and A, we additionally denote\n\u2329 Qh, \u00b5h \u2297 \u03c0h \u232a \u2126\u00d7A (s) := E\u03c9\u223c\u00b5h,a\u223c\u03c0h(\u00b7|s,\u03c9) [Qh(s, \u03c9, a)] .\nUsing this notation, the Bellman equation associated with signaling policy \u03c0 becomes\nQ\u03c0h(s, \u03c9, a) = (vh + PhV \u03c0 h+1)(s, \u03c9, a), V \u03c0 h (s) = \u2329 Q\u03c0h, \u00b5h \u2297 \u03c0h \u232a \u2126\u00d7A (s), V \u03c0H+1(s) = 0, (3.1)\nwhich holds for all s \u2208 S, \u03c9 \u2208 \u2126, a \u2208 A. Similarly, the Bellman optimality equation is\nQ\u2217h(s, \u03c9, a) = (vh + PhV \u2217 h+1)(s, \u03c9, a), V \u2217 h (s) = max\n\u03c0\u2032 h \u2208Pers(\u00b5h,uh)\n\u2329 Q\u2217h, \u00b5h \u2297 \u03c0\u2032h \u232a \u2126\u00d7A (s), V \u2217H+1(s) = 0.\n(3.2)\nWe remark that the above equations implicitly assume the context C = {ch}h\u2208[H] (and thus the priors) are determined in advance. To emphasize the values\u2019 dependence on context which will be useful for the analysis of later learning algorithms, we extend the notation to V \u03c0h (s;C), Q \u03c0 h(s, \u03c9, a;C) to specify that the value (resp. Q) function is estimated based on which prior \u00b5 conditioned on which sequence of context C .\nA Note on Computational Efficiency. We note that the above Bellman Optimality Equation in (3.2) also implies an efficient dynamic program to compute the optimal policy \u03c0\u2217 in the basic tabular model of MPP in Subsection 3.1, i.e., when s \u2208 S, \u03c9 \u2208 \u2126, a \u2208 A are all discrete. This is because the maximization problem in\nequation (3.2) can be solved efficiently be a linear program. The generalized MPP of subsection 3.2 imposes some computational challenge due to infinitely many outcomes and states. Fortunately, it is already known that planning in the infinite state MDP with linear function approximation can also be solved efficiently [25]. Following a similar analysis, we can determine Q\u2217h(\u00b7, \u00b7, \u00b7) through a linear function of q\u2217h \u2208 Rd\u03c8 with the observed feature \u03c8(\u00b7, \u00b7, \u00b7). Hence, the dominating operation is to compute max\u03c0\u2208Pers(\u00b5h,uh)\u3008Q\u2217h, \u00b5h \u2297 \u03c0h\u3009\u2126\u00d7A(s) at each step. Let the sender utility function be Q\u2217h; such an optimization is exactly the problem of optimal information design with infinitely many outcomes but finitely many actions, which has been studied in previous work [15]. It turns out that there is an efficient algorithm that can signal on the fly for any given outcome \u03c9 and obtains an \u01eb-optimal persuasive signaling scheme in poly(1/\u01eb) time. Therefore, in our later studies of learning, we will take these algorithms as given and simply assume that we can compute the optimal signaling scheme efficiently at any given state s. One caveat is that our regret guarantee will additionally lose an additive \u01eb factor at each step due to the availability of only an \u01eb-optimal algorithm, but this loss can be negligible when we set \u01eb = O(1/(TH)) by using a poly(TH) time algorithm."
        },
        {
            "heading": "4 Reinforcement Learning in MPPs and the Optimism-Pessimism Principle",
            "text": "In this section, we study online reinforcement learning (RL) for learning the optimal signaling policy on an MPP. Here the learner only knows the utility functions of the receivers6 and has no prior knowledge about the prior distribution, the sender\u2019s utility function, and the transition kernel. While the computation of optimal policy in MPPs in Section 3.3 may appear analogous to that of a standard MDP, as we will see that the corresponding RL problem turns out to be significantly different, partially due to the presence of the stream of receivers, whose decisions are self-interested and not under the learner\u2019s control. This makes the learning challenging because if the receivers\u2019 incentives are not carefully addressed, they may take actions that are extremely undesirable to the learner. Such concern leads to the integration of the pessimism principle into our learning algorithm design. Specifically, our learner will be optimistic to the estimation of theQ-function, similar to many other RL algorithms, in order to encourage exploration. But more interestingly, it will be pessimistic to the uncertainty in the estimation of the prior distributions in order to prepare for detrimental equilibrium behavior. Such dual considerations lead to an interesting optimism-pessimism principle (OPP) for learning MPPs under the online setting. From a technical point of view, our main contribution is to prove how the mixture of optimism and pessimism principle can still lead to no regret algorithms, and this proof crucially hinges on a robust property of the MPP model which we develop and carefully apply to the regret analysis. To the best of our knowledge, this is the first time that OPP is employed to learn the optimal information design in an online fashion. We prove that it can not only satisfy incentive constraints but also guarantees efficiency in terms of both sample complexity and computational complexity.\nIn order to convey our key design ideas before diving into the intricate technicalities, this section singles\n6 The receiver\u2019s utility is known to the sender because the pricing rules are usually transparent, some are even set by the platform.\nFor example, a rider-sharing platform usually sets per hour or mile payment rules for the drivers.\nout two representative special cases of the online sequential information design problem. In a nutshell, we present a learning algorithm OP4 that combines the principle of optimism and pessimism such that the sender can learn to persuade without initially knowing her own utility or the prior distribution of outcomes. In the tabular MPP, we illustrate the unique challenges of learning to persuade arising from the dynamically evolving environment state according to a Markov process. Through the contextual Bayesian persuasion, we showcase the techniques necessary for learning to persuade with infinitely many states (i.e., contexts) and outcomes. We shall omit most proofs in this section to focus on the high-level ideas, because the proof for the general setting presented in Section 5 suffices to imply all results for the two special cases here."
        },
        {
            "heading": "4.1 Learning Optimal Policies in MPPs: Setups and Benchmarks",
            "text": "We consider the episodic reinforcement learning problem in finite-horizon MPPs. Different from the full knowledge setting in Section 3.3, the transition kernel, the sender\u2019s utility function and the outcome prior at each step of the episode, {Ph, vh, \u00b5h}h\u2208[H], are all unknown. The sender has to learn the optimal signaling policy by interacting with the environment as well as a stream of receivers in T number of episodes. For each t \u2208 [T ] = {1, \u00b7 \u00b7 \u00b7 , T}, at the beginning of t-th episode, given the data {(c\u03c4h, s\u03c4h, \u03c9\u03c4h, a\u03c4h, v\u03c4h)}h\u2208[H],\u03c4\u2208[t\u22121], the adversary picks the context sequence {cth}h\u2208[H] as well as the initial state st1, and the agent accordingly chooses a signaling policy \u03c0t = {\u03c0th}h\u2208[H]. Here v\u03c4h is the utility collected by the sender at step h of episode \u03c4 .\nRegret To evaluate the online learning performance, given the ground-truth outcome prior \u00b5\u2217 = {\u00b5\u2217h}h\u2208[H], we define the sender\u2019s total (expected) regret over the all T episodes as\nReg(T,\u00b5\u2217) := T\u2211\nt=1\n[ V \u22171 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct) ] . (4.1)\nNote that if \u03c0t is not always feasible under \u00b5\u2217, but is only persuasive with high probability, so the corresponding regret under \u03c0t should be also in high probability sense.\nIt turns out that in certain degenerate cases it is impossible to achieve a sublinear regret. For example, if the set of possible posterior outcome distributions that induce some a \u2208 A as the optimal receiver action has zero measure, then such posterior within a zero-measure set can never be exactly induced by a signaling scheme without a precise knowledge of the prior. Thus, the regret could be \u2126(T ) if receiver cannot be persuaded to play such action a. Therefore, to guarantee no regret, it is necessary to introduce certain regularity assumption on the MPP instance. Towards that end, we shall assume that the receivers\u2019 utility u and prior \u00b5 at any step of the MPP instance always satisfies a minor assumption of (p0,D)-regularity as defined below.\nRegularity Conditions An instance satisfies (p0,D)-regularity, if for any feasible state s \u2208 S and context c \u2208 C, we have\nP\u03c9\u223c\u00b5(\u00b7|c) [\u03c9 \u2208 Ws,a(D)] \u2265 p0, \u2200a \u2208 A,\nwhere \u00b5 is the ground-truth prior of outcomes and Ws,a(D) , {\u03c9 : u(s, \u03c9, a) \u2212 u(s, \u03c9, a\u2032) \u2265 D,\u2200a\u2032 \u2208 A/{a}} is the set of outcomes \u03c9 for which the action a is optimal for the receiver by at least D at state s. . In other words, an instance is (p0,D)-regular if every action a has at least probability p0, under randomness of the outcome, to be strictly better than other actions by at least D. This regularity condition is analogous to a regularity condition of Zu et al. [61] but is generalizable to infinite outcomes as we consider here."
        },
        {
            "heading": "4.2 Algorithm: Optimism-Pessimism Principle for Persuasion Process (OP4)",
            "text": "The learning task in MPPs involves two intertwined challenges: (1) How to persuade the receiver to take desired actions under unknown \u00b5h? (2) Which action to persuade the receiver to take in order to explore the underlying environment? For the first challenge, due to having finite data, it is impossible to perfectly recover \u00b5h. We can only hope to construct an approximately accurate estimator of \u00b5h. To guard against potentially detrimental equilibrium behavior of the receivers due to the prior estimation error, we propose to adopt the pessimism principle. Specifically, before each episode, we conduct uncertainty quantification for the estimator of the prior distributions, which enables us to construct a confidence region containing the true prior with high probability. Then we propose to find the signaling policy within a pessimistic candidate set\u2014signaling policies that are simultaneously persuasive with respect to all prior distributions in the confidence region. When the confidence region is valid, such a pessimism principle ensures that the executed signaling policy is always persuasive with respect to the true prior. Furthermore, to address the second challenge, we adopt the celebrated principle of optimism in the face of uncertainty [28], which has played a key role in the online RL literature. The main idea of this principle is that, the uncertainty of the Q-function estimates essentially reflects our uncertainty about the underlying model. By adding the uncertainty as a bonus function, we encourage actions with high uncertainty to be recommended and thus taken by the receiver when persuasiveness is satisfied. We then fuse the two principles into the OP4 algorithm in Algorithm 1.\nAlgorithm 1 OP4 Overview\n1: for episode t = 1 . . . T do 2: Receive the initial state {st1} and context Ct = {cth}Hh=1. 3: For each step h \u2208 [H], estimate prior \u00b5th along with the confidence region \u00b5Bth , and construct an\noptimistic Q-function Qth iteratively with the value function V t h .\n4: for step h = 1, . . . ,H do 5: Choose robust signaling scheme \u03c0th \u2208 argmax\u03c0h\u2208Pers(\u00b5Bt\nh ,uh)\n\u2329 Qth, \u00b5 t h \u2297 \u03c0h \u232a \u2126\u00d7A (sth;C t).\n6: Observe state sh, outcome \u03c9h and accordingly recommend action a \u223c \u03c0th(\u03c9h, \u00b7) to the receiver. 7: end for 8: end for\nPessimism to Induce Robust Equilibrium Behavior From the data in the past episode, the sender can estimate the mean of the prior as well as obtain a confidence region through concentration inequalities. Given this partial knowledge of the prior distribution, the sender needs to design a signaling scheme that works in the face of any possible priors in the confidence region in order to ensure the receiver will take its recommended action with high probability. Specifically, we let B\u03a3(\u03b8, \u03b2) := {\u03b8\u2032 : \u2016\u03b8\u2032 \u2212 \u03b8\u2016\u03a3 \u2264 \u03b2} denote the closed ball in \u2016\u00b7\u2016\u03a3 norm of radius \u03b2 > 0 centered at \u03b8 \u2208 Rd\u03b8 . For any set B \u2286 Rd\u03b8 , we let Pers(\u00b5B, u) denote the set of signaling policies that are simultaneously persuasive under all weigh vectors \u03b8 \u2208 B: Pers(\u00b5B, u) := \u22c2 \u03b8\u2208B Pers(\u00b5\u03b8, u). For any non-empty set B, the set Pers(\u00b5B, u) is convex since it is an intersection of convex sets Pers(\u00b5\u03b8, u), and is non-empty since it must contain the full-information signaling scheme. We note that since Pers(\u00b5B, u) is a convex set, we can solve the linear optimization among the policies in Pers(\u00b5B, u) in polynomial time (see e.g., [61]).\nOptimism to Encourage Exploration In order to balance exploration and exploitation, we adopt the principle of optimism in face of uncertainty to the value iteration algorithm based on Bellman equation, following in a line of work in online RL such as Q-learning with UCB exploration [24], UCBVI [4], LSVIUCB [25] (also see [50, 56, 51] and the references therein). The additional UCB bonus on the Q-value encourages exploration and has been shown to be a provably efficient online method to improve policies in MDPs. Moreover, this method not only works for the simple tabular setting, but also generalizes to settings with infinite state spaces by exploiting linearity of the Q-function and a regularized least-squares program to determine the optimal estimation of Q-value. In fact, within our framework, we could obtain efficient learning result in the infinite state space setting through other optimism-based online RL methods and general function approximators, such as linear mixture MDPs [3, 60, 9, 59], or kernel approximation [56] or bilinear classes [13].\nTo provide a concrete picture of the learning process, we instantiate the OP4 algorithm in two special cases and showcase our key ideas and techniques before delving into the more involved analysis of the generalized MPP setting. Nevertheless, we remark that whether the problem instance is tabular or in the form of linear or generalized linear approximations is not essential and not the focus of our study. OP4 itself\nonly relies on two things, i.e., the uncertainty quantification for Q-function and prior estimation. So even the model-free RL framework can be replaced by model-based RL, as we can just construct confidence region for the transition models."
        },
        {
            "heading": "4.3 Warm-up I: Reinforcement Learning in the Tabular MPP",
            "text": "We first consider MPPs in tabular setting with finite states and outcomes, as described in Section 3.1. In this case, the prior on outcomes at each step degenerates to an unknown but fixed discrete distribution independent of context. As linear parameterization is not required for discrete probability distribution, the algorithm can simply update the empirical estimation of \u00b5th through counting. Similarly, the transition kernel P \u2217h is estimated through the occurrence of observed samples, and we uses this estimated transition to compute theQ-function Q\u0302th from the observed utility and estimated value function in the next step, according to the Bellman equation. To be specific, for each s \u2208 S, \u03c9 \u2208 \u2126, a \u2208 A, \u00b5th and Q\u0302th are estimated through the following equations:\n\u00b5th(\u03c9)\u2190 \u03bb/|\u2126|+Nt,h(\u03c9)\n\u03bb+ t\u2212 1 ,\nQ\u0302th(s, \u03c9, a)\u2190 1\n\u03bb+Nt,h(s, \u03c9, a)\n\u2211\n\u03c4\u2208[t\u22121]\n{ I(s\u03c4h = s, \u03c9 \u03c4 h = \u03c9, a \u03c4 h = a) [ v\u03c4h + V t h+1(s \u03c4 h+1) ]} ,\nwhere Nt,h(\u03c9) = \u2211 \u03c4\u2208[t\u22121] I(\u03c9 \u03c4 h = \u03c9) and Nt,h(s, \u03c9, a) = \u2211 \u03c4\u2208[t\u22121] I(s \u03c4 h = s, \u03c9 t h = \u03c9, a t h = a) respectively count the effective number of samples that the sender has observed arriving at \u03c9, or the combination {s, \u03c9, a}), and \u03bb > 0 is a constant for regularization. In our learning algorithm, we determine the radius of confidence region Bth for \u00b5th according to confidence bound \u01ebth = O( \u221a log(HT )/t). Moreover, we add a UCB bonus term of form \u03c1/ \u221a Nt,h(s, \u03c9, a) to Q\u0302th to obtain the optimistic Q-function Q t h. Then, it selects a robustly persuasive signaling scheme that maximizes an optimistic estimation of Q-function with respect to the current prior estimation \u00b5th. Finally, it makes an action recommendation ath using this signaling scheme, given the state and outcome realization {sth, \u03c9th}. Theorem 4.1. Let \u01ebth = O\u0303( \u221a\n1/t), and \u03c1 = O\u0303(|S| \u00b7 |\u2126| \u00b7 |A|H). Then under (p0,D)-regularity, with probability at least 1 \u2212 3H\u22121T\u22121, OP4 has regret of order O\u0303 ( |C|(|S| \u00b7 |\u2126| \u00b7 |A|)3/2 \u00b7 H2 \u221a T/(p0D) ) in tabular MPPs.\nTo obtain the regret of OP4, we have to consider the regret arising from different procedures. Formal decomposition of the regret is described in Lemma 6.1. Separately, we upper bound errors incurred from estimating Q-function (Lemma 6.2), the randomness of of choosing the outcome, action and next state (Lemma A.5) as well as estimating the prior of outcome and choosing a persuasive signaling scheme that is robustly persuasive for a subset of priors (Lemmas 6.3 and 6.4). As the two warm-up models are special\ncases of the general MPP, the proof of the above properties follows from that of the general MPP setting in Section 6, and thus is omitted here."
        },
        {
            "heading": "4.4 Warm-up II: Reinforcement Learning in Contextual Bayesian Persuasion",
            "text": "We now move to another special case with H = 1, such that the MPP problem reduces to a contextualbandit-like problem, where transitions no longer exist. Given a context c and a persuasive signaling policy \u03c0, the value function is simply the sender\u2019s expected utility for any s \u2208 S ,\nV \u03c0(s; c) :=\n\u222b\n\u03c9\n\u2211\na\u2208A\n\u00b5(\u03c9|c)\u03c0(a|s, \u03c9)v(s, \u03c9, a)d\u03c9.\nThe sender\u2019s optimal expected utility is defined as V \u2217(s; c) := max\u03c0\u2208Pers(\u00b5(\u00b7|c),u) V \u03c0(s; c).\nMeanwhile, we consider the general setting where outcome \u03c9 is a continuous random variable that subjects to a generalized linear model. To be specific, the prior \u00b5 is conditioned on the context c with the mean value f(\u03c6(c)\u22a4\u03b8). For the prior \u00b5 and link function f , we assume the smoothness of the prior and the bounded derivatives of the link function: Assumption 4.2. There exists a constant L\u00b5 > 0 such that for any parameter \u03b81, \u03b82, we have \u2225\u2225\u00b5\u03b81(\u00b7|c) \u2212 \u00b5\u03b82(\u00b7|c) \u2225\u2225 1 \u2264 L\u00b5 \u2225\u2225f ( \u03c6(c)\u22a4\u03b81 ) \u2212 f ( \u03c6(c)\u22a4\u03b82 )\u2225\u2225 for any given context c.\nAssumption 4.3. The link function f is either monotonically increasing or decreasing. Moreover, there exists absolute constants 0 < \u03ba < K <\u221e and 0 < M <\u221e such that \u03ba \u2264 |f \u2032(z)| \u2264 K and |f \u2032\u2032(z)| \u2264M for all |z| \u2264 \u03a6L\u03b8.\nIt is natural to assume a Lipschitz property of the distribution in Assumption 4.2. For instance, Gaussian distributions and uniform distributions satisfy this property. Assumption 4.3 is standard in the literature [18, 52, 32]. Two example link functions are the identity map f(z) = z and the logistic map f(z) = 1/(1 + e\u2212z) with bounded z. It is easy to verify that both maps satisfy this assumption.\nDifferent from the tabular setting, we are now unable to use the counting-based estimator to keep track of the distribution of the possibly infinite states and outcomes. Instead, we resort to function approximation techniques and estimate the linear parameters \u03b8\u2217 and \u03b3\u2217. In each episode, OP4 respectively updates the estimation and confidence region of \u03b8t and \u03b3t, with which it can determine the outcome prior under pessimism and sender\u2019s utility under optimism. To be specific, the update of \u03b8t solves a constrained least-squares problem and the update of qt solves precisely a regularized one:\n\u03b8t \u2190 arg min \u2016\u03b8\u2016\u2264L\u03b8\n\u2211\n\u03c4\u2208[t\u22121]\n[ \u03c9\u03c4 \u2212 f(\u03c6(c\u03c4 )\u22a4\u03b8h) ]2 ,\n\u03b3t \u2190 arg min \u03b3\u2208R\u03c8\n\u2211\n\u03c4\u2208[t\u22121]\n\u2225\u2225\u2225v\u03c4 \u2212 \u03c8(s\u03c4 , \u03c9\u03c4 , a\u03c4 )\u22a4\u03b3 \u2225\u2225\u2225 2 + \u03bb \u2016\u03b3\u20162 .\nWe then estimate the prior by setting \u00b5t(\u00b7|c) to the distribution of f ( \u03c6(c)\u22a4\u03b8t ) + z and estimate the sender\u2019s utility by setting vt(\u00b7, \u00b7, \u00b7) = \u03c8(\u00b7, \u00b7, \u00b7)\u22a4\u03b3t. On one hand, to encourage exploration, OP4 adds the UCB bonus term of form \u03c1 \u2016\u03c8(\u00b7, \u00b7, \u00b7)\u2016(\u0393t)\u22121 to the Q-function, where \u0393t = \u03bbId\u03c8 + \u2211 \u03c4\u2208[t] \u03c8(s \u03c4 , \u03c9\u03c4 , a\u03c4 )\u03c8(s\u03c4 , \u03c9\u03c4 , a\u03c4 )\u22a4 is the Gram matrix of the regularized least-squares problem and \u03c1 is equivalent to a scalar. This is a common technique for linear bandits. On the other hand, OP4 determines the confidence region of \u03b8t with radius \u03b2, and ensures that signaling scheme is robustly persuasive for any possible (worst case) prior induced by linear parameters \u03b8 in this region. Combining optimism and pessimism, OP4 picks the signaling scheme among the robust persuasive set that maximizes the sender\u2019s optimistic utility.\nTheorem 4.4. Under (p0,D)-regularity and Assumption 4.2 and 4.3, there exists an absolute constant C1, C2 > 0 such that, if we set \u03bb = max{1,\u03a82}, \u03b2 = C1(1 + \u03ba\u22121 \u221a K +M + d\u03c6\u03c32 log(T )), and \u03c1 = C2d\u03c8 \u221a log(4d\u03c8\u03a82T 3), then with probability at least 1\u22123T\u22121, OP4 has regret of order O\u0303 ( d\u03c6 \u221a d3\u03c8 \u221a T/(p0D) ) in contextual Bayesian persuasion problems.\nSince we estimate the prior by computing an estimator \u03b8t, we evaluate the persuasiveness of OP4 through the probability that \u03b8\u2217 lies in the confidence region centered at \u03b8t with the radius \u03b2 = O( \u221a d\u03c6 log(T )) in weighted norm. Due to the smoothness of the prior and the assumption of link function, the error of the estimated prior is bounded by the product of \u03b2 and the weighted norm of feature vector \u2016\u03c6(ct)\u2016\u03a3t = O(1/ \u221a t), which yields the same conclusion for \u01ebt in the tabular MPP case. Also compared to Li et al. [31], we do not require any regularity for \u03a3t, since we add a constant matrix \u03a62I to the Gram matrix \u03a3t. This ensures that \u03a3t is always lower bounded by the constant \u03a62 > 0. The proof of the persuasiveness and sublinear regret of contextual bandit can be viewed as a direct reduction of the MPP case when the total step H = 1. We decompose the regret in the same way as that in Lemma 6.1 for MPPs and then estimate the upper bound for each item to measure the regret loss."
        },
        {
            "heading": "5 No-Regret Learning in the General Markov Persuasion Process",
            "text": "In this section, we present the full version of the OP4 algorithm for MPPs and show that it is persuasive with high probability and meanwhile achieves average regret O\u0303 ( d\u03c6 \u00b7 d3/2\u03c8 H2 \u221a T/(p0D) ) .\nIn the general MPP setting with the linear utility and transition, a crucial property is that the Q-functions under any signaling policy is always linear in the feature map \u03c8 (akin to linear MDPs [25]). Therefore, when designing learning algorithms, it suffices to focus on linear Q-functions. In our OP4 algorithm, we iteratively fit the optimal Q-function, which is parameterized by q\u2217h as \u03c8(\u00b7, \u00b7, \u00b7)\u22a4q\u2217h at each step h \u2208 [H]. OP4 learns the Q-functions of MPPs and the prior of persuasion states simultaneously. It operates similarly as that in tabular MPPs and contextual Bayesian persuasion. At the t-th episode, given the historical data {(c\u03c4h, s\u03c4h, \u03c9\u03c4h, a\u03c4h, v\u03c4h)}h\u2208[H],\u03c4\u2208[t\u22121], we can estimate the unknown vectors \u03b8\u2217h, q\u2217h,\u2200h \u2208 [H] by solving the following constrained or regularized least-squares problems:\n\u03b8th \u2190 argmin \u2016\u03b8h\u2016\u2264L\u03b8\n\u2211\n\u03c4\u2208[t\u22121]\n[ \u03c9\u03c4h \u2212 f(\u03c6(c\u03c4h)\u22a4\u03b8h) ]2 ,\nqth \u2190 argmin q\u2208R d\u03c8\n\u2211\n\u03c4\u2208[t\u22121]\n[ v\u03c4h + V t h+1(s \u03c4 h+1;C t)\u2212 \u03c8(s\u03c4h, \u03c9\u03c4h, a\u03c4h)\u22a4q ]2 + \u03bb\u2016q\u20162.\nAdditionally, V th+1 is the estimated value function with the observed context C t at the episode t, which we describe formally later. This estimator is used to replace the unknown transition Ph and distribution \u03bdh in equation (3.2). Moreover, we can update the estimate of outcome prior \u00b5th and Q-function Q t h respectively. Here OP4 adds UCB bonus to Qth to encourage exploration. The formal description is given in Algorithm 2.\nLikewise, the MPP setting inherits the regularity conditions and Assumption 4.2 and 4.3 in the last section. Combining the insights from both the tabular MPPs and contextual Bayesian persuasion, we can show that the OP4 is persuasive and guarantees sublinear regret with high probability for general MPPs.\nTheorem 5.1. Under (p0,D)-regularity and Assumption 4.2 and 4.3, there exists absolute constants C1, C2 > 0 such that, if we set \u03bb = max{1,\u03a82}, \u03b2 = C1(1+\u03ba\u22121 \u221a K +M + d\u03c6\u03c32 log(HT )),and \u03c1 = C2d\u03c8H \u221a log(4d\u03c8\u03a82H2T 3), then with probability at least 1\u2212 3H\u22121T\u22121, OP4 has regret of order O\u0303 ( d\u03c6d 3/2 \u03c8 H 2 \u221a T/(p0D) ) .\nRecall that the novelty of OP4 is that we adopt pessimism and optimism to induce robust equilibrium behavior and encourage exploration simultaneously. Specifically, pessimism tackles the uncertainty in the prior estimation by selecting a signaling policy that is persuasive w.r.t. all the priors in the confidence region, while optimism in Q-function estimation encourages exploration. To evaluate the regret of OP4, we provide a novel regret decomposition, which is tailored to this pessimism and optimism combination. Each term represents different aspects of regret loss incurred by either estimation or randomness."
        },
        {
            "heading": "6 Proof Sketch and Technical Highlights",
            "text": "In this section, we present the proof sketch for Theorem 5.1. We first decompose the regret into several terms tailored to MPPs and briefly introduce how to bound each term. Then we highlight our technical contribution about regularity when measuring the loss in the sender\u2019s utility for choosing a signaling scheme that is persuasive for a subset of priors close to each other."
        },
        {
            "heading": "6.1 Proof of Theorem 5.1",
            "text": "In order to prove the sublinear regret for OP4, we construct a novel regret decomposition tailored to MPPs. Our proof starts from decomposing the regret into several terms, each of which indicates the regret loss either from estimation or from the randomness of trajectories. Next, we evaluate each term and then add\nthem together to conclude the upper bound of the regret of OP4. For simplicity of presentation, denote V\u0303 th(\u00b7;C) = \u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0th \u232a \u2126\u00d7A\n(\u00b7;C) as the expectation of Qth with respect to the ground-truth prior \u00b5\u2217h and signaling scheme \u03c0th at the h-th step. Then we can define the temporal-difference (TD) error as\n\u03b4th(s, \u03c9, a) = (v t h + PhV t h+1 \u2212Qth)(s, \u03c9, a;Ct). (6.1)\nHere \u03b4th is a function on S \u00d7\u2126\u00d7A for all h \u2208 [H] and t \u2208 [T ]. Intuitively, {\u03b4th}h\u2208[H] quantifies how far the Q-functions {Qth}h\u2208[H] are from satisfying the Bellman optimality equation in equation (3.2). Moreover, define \u03b61t,h and \u03b6 2 t,h for the trajectory {cth, sth, \u03c9th, ath}h\u2208[H] generated by Algorithm 2 at the t-th episode as follows\n\u03b61t,h = (V\u0303 t h \u2212 V \u03c0\nt\nh )(s t h;C t)\u2212 (Qth \u2212Q\u03c0 t h )(s t h, \u03c9 t h, a t h;C t),\n\u03b62t,h = Ph(V t h+1 \u2212 V \u2217h+1)(sth, \u03c9th, ath;Ct)\u2212 (V th+1 \u2212 V \u2217h+1)(sth+1;Ct).\n(6.2)\nBy definition, \u03b61t,h capture the randomness of realizing the outcome \u03c9 t h \u223c \u00b5\u2217h(\u00b7|ch) and signaling the action ath \u223c \u03c0th(sth, \u03c9th, \u00b7), while \u03b62t,h captures the randomness of drawing the next state sth+1 from Ph(\u00b7|sth, \u03c9th, \u00b7). With the notations above, we can decompose the regret into six parts to facilitate the establishment of the upper bound of the regret.\nLemma 6.1 (Regret Decomposition). With the notations defines in equation (6.1) and (6.2), we can write the regret as:\nReg(T, \u00b5\u2217) = \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n{ E\u00b5\u2217\nh ,\u03c0\u2217 h [\u03b4th(sh, \u03c9h, a t h)|s1 = st1]\u2212 \u03b4th(sth, \u03c9th, ath)\n}\n\ufe38 \ufe37\ufe37 \ufe38 (i)\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n(\u03b61t,h + \u03b6 2 t,h)\n\ufe38 \ufe37\ufe37 \ufe38 (ii)\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nE\u00b5\u2217 h ,\u03c0\u2217 h\n[\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t)|s1 = st1 ]\n\ufe38 \ufe37\ufe37 \ufe38 (iii)\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth, C t)\n\ufe38 \ufe37\ufe37 \ufe38 (iv)\n.\n(6.3)\nIn this novel regret decomposition, term (i) indicates the optimism in OP4. Provably, \u03b4th in term (i) is always non-positive due to the optimistic Q-value estimation, which could simplify this term. Term (iii) corresponds to the pessimism in OP4 for inducing a robust equilibria. It evaluates the regret loss incurred by choosing a robustly persuasive signaling policy. Since the signaling policy has to be persuasive to ensure that receivers will always take recommended actions, we cannot simply choose a greedy policy for a fixed prior\nestimation. Instead, we first apply optimism to construct the optimistic Q-value estimation and then apply pessimism to select a signaling policy that is robustly persuasive for all the priors in the confidence region. Therefore, we design the regret decomposition, especially term (iii) in this form to reflect the optimism and pessimism principle in OP4. Notice that this decomposition does not depend on specific function approximation forms in the algorithm, since not only the estimation of prior and Q-function but also the chosen signaling policy has no influence on this formula. Therefore, it generally suits all the algorithms for MPPs.\nUnlike the regret decomposition in [56], Lemma 6.1 also captures the randomness of realizing the outcome. Since we have to estimate the prior of the outcome and choose a robustly persuasive policy in MPPs, we add term (iii) and (iv) to evaluate the further regret loss.\nThe rigorous arguments turn out to be technical, and thus we shall defer the proof of most lemmas to the appendix while aiming to present all the key ideas and conclusions in the following. For term (i) in equation (6.3), although we do not observe the trajectories under prior \u00b5\u2217 and signaling policy \u03c0\u2217, we can upper bound both \u03b4th and \u2212\u03b4th. The following lemma states this result. Lemma 6.2 (Optimism). There exists an absolute constant c > 0 such that, for any fixed \u03b4 \u2208 (0, 1), if we set \u03bb = max{1,\u03a82} and \u03c1 = cd\u03c8H \u221a \u03b9 in Algorithm 2 with \u03b9 = log(2d\u03c8\u03a8 2T/\u03b4), then with probability at least 1\u2212 \u03b4/2, we have \u22122\u03c1\u2016\u03c8(s, \u03c9, a)\u2016(\u0393t\nh )\u22121 \u2264 \u03b4th(s, \u03c9, a) \u2264 0.\nfor all s \u2208 S, \u03c9 \u2208 \u2126, a \u2208 A, h \u2208 [H] and t \u2208 [T ].\nTerm (ii) in equation (6.3) can be bounded by Lemma 5.3 from [56] using martingale techniques and the Azuma-Hoeffding inequality [5]. We state the upper bound for term (ii) in Lemma A.5. Moreover, term (iii) in equation (6.3) evaluates the regret loss caused by estimating the prior and choosing a robustly persuasive signaling policy. Here, we apply the robustness gap Gap defined later to bound this term.\nLemma 6.3 (Bounding Term (iii)). On the event of {\u03b8\u2217h \u2208 Bth}, under Assumption 4.2 and 4.3, we have \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nE\u00b5\u2217 h ,\u03c0\u2217 h\n[\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t)|s1 = st1 ]\n\u2264 ( 3HL\u00b5K\np0D + HL\u00b5K 2\n) \u03b2 \u2211\nh\u2208[H]\n\u2211\nt\u2208[T ]\n\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nIt remains to bound term (iv) in equation (6.3). This bound can be derived from Holder inequality and\nthe property of the prior.\nLemma 6.4 (Bounding Term (iv)). On the event of {\u03b8\u2217h \u2208 Bth}, under Assumption 4.2 and 4.3, we have \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t) \u2264 HL\u00b5K\u03b2 \u2211\nh\u2208[H]\n\u2211\nt\u2208[T ]\n\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nNow we are ready to prove our main result, Theorem 5.1. By the decomposition in Lemma 6.1 and all previous lemmas, let \u03b2 = C(1 + \u03ba\u22121 \u221a K +M + d\u03c6\u03c32 log(HT )), and then we obtain the following upper bound for regret:\nReg(T, \u00b5\u2217) \u22644 \u221a 2TH3 log(2HT )\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n[ 2\u03c1\n\u2225\u2225\u03c8(sth, \u03c9th, ath)\u2016(\u0393t h )\u22121 +\n( 3HL\u00b5K\np0D +\n3HL\u00b5K\n2\n) \u03b2\u2016\u03c6(cth)\u2016(\u03a3t\nh )\u22121\n] ,\nWith the probability of the given event by Lemma 6.8 and appropriately chosen \u03b4 in previous lemmas, the above inequality holds for the probability at least 1\u2212 3H\u22121T\u22121. By Lemma A.6, we have\nReg(T, \u00b5\u2217) \u2264 2 \u221a 2TH3 log(2HT ) + 2\u03c1H \u221a 2d\u03c8T log ( 1 + T\u03a82/(\u03bbd\u03c8) )\n+ \u03b2H2L\u00b5K\n( 3\np0D +\n3 2\n)\u221a 2d\u03c6T log ( 1 + T/(d\u03c6) ) .\nSince \u03b2 is in O\u0303( \u221a d\u03c6) and \u03c1 is in O\u0303(d\u03c8H), we can conclude that the regret of Algorithm 2 is O\u0303 ( d\u03c6d 3/2 \u03c8 H 2 \u221a T/(p0D) ) ."
        },
        {
            "heading": "6.2 Inducing Robust Equilibria via Pessimism",
            "text": "One necessary prerequisite is that the signaling policy given by OP4 has to be persuasive to ensure receivers to take recommended actions. However, the optimal signaling policy that is persuasive for the estimated prior can hardly be also persuasive for the true prior, even if the estimation is quite close to it. To ensure persuasiveness under the prior estimation error, we adopt pessimism principle to select a signaling policy that is robustly persuasive for all the priors in the confidence region. And we shall quantify the extra utility loss suffered by the pessimism principle. In this subsection, we start by showing that there exists a robust signaling scheme that suffers only O(\u01eb) utility loss compared to the optimal expected utility of persuasion algorithm designed with precise knowledge of the prior. Formally, in basic MPP, given any fixed Q-function Q(\u00b7, \u00b7, \u00b7), we define the robustness gap for some state s \u2208 S and any prior \u00b5 \u2208 B \u2286 \u2206(\u2126) as\nGap ( s, \u00b5,B;Q ) , max\n\u03c0\u2208Pers(\u00b5,u)\n\u2329 Q,\u00b5\u2297 \u03c0 \u232a \u2126\u00d7A\n(s)\u2212 max \u03c0\u2208Pers(B,u)\n\u2329 Q,\u00b5\u2297 \u03c0 \u232a \u2126\u00d7A (s). (6.4)\nWe let B(\u00b5, \u01eb) = {\u00b5\u2032 \u2208 \u2206(\u2126) : \u2016\u00b5\u2212 \u00b5\u2032\u20161 \u2264 \u01eb} be the \u21131-norm ball centered the prior distribution \u00b5 with radius \u01eb.\nLemma 6.5 (Pessimism). Under (p0,D)-regularity, for all \u01eb > 0, given aQ-function Q, for any state s \u2208 S , we have\nGap ( s, \u00b5,B(\u00b5, \u01eb);Q ) \u2264 H\u01eb p0D .\nThe proof is given in Appendix A.2. This result extends Proposition 1 in [61]. Notice that the upper bound of Gap(\u00b7; \u00b7) does not depend on the value of Q, which is important for our analysis. Once given a signaling algorithm, at each episode t \u2208 [T ] and each step h \u2208 [H], we are able to obtain an estimation of Q-function with an explicit form. It is equivalent to the \u201cknown\u201d Q-function mentioned in equation (6.4). Using Gap(\u00b7; \u00b7), we can estimate the expected sender\u2019s utility loss for choosing a signaling mechanism that is persuasive for all priors in a subset. Moreover, if we consider the dependence on context for priors and add the linear assumption of priors to the proceeding lemma, we can bound Gap(\u00b7; \u00b7) by the difference of linearity parameter \u03b8.\nCorollary 6.6. Under (p0,D)-regularity and Assumption 4.2 and 4.3, given a Q-function Q and context c, for any state s \u2208 S , prior \u00b5\u03b8(\u00b7|c) and confidence region B = {\u00b5\u03b8\u2032(\u00b7|c) : \u03b8\u2032 \u2208 B\u03a3(\u03b8, \u01eb)}, we have Gap(s, \u00b5\u03b8(\u00b7|c),B;Q) \u2264 HL\u00b5K\u2016\u03c6(c)\u2016\u03a3\u22121\u01eb/(p0D).\nIn MPPs, we have to estimate the prior of the outcome since we cannot observe the ground-truth prior. However, the estimation may not satisfy the regularity conditions, which conflicts with the requirements for the prior when proving Lemma 6.5. To address this problem, we give another upper bound of the robustness gap for the prior estimation in Lemma A.1. In addition, to handle the regret loss incurred by estimating the prior, we compute the difference in Q-functions when choosing respectively persuasive scheme for different priors in Lemma A.2.\nWe now prove that the above pessimism design guarantees persuasiveness w.r.t. the true prior with high probability. And it suffices to show that the estimation \u03b8th is close enough to the real parameter \u03b8 \u2217 h such that the confidence region Bth centered at \u03b8th given in Algorithm 2 contains \u03b8\u2217h. If so, the signaling scheme chosen to be persuasive for the whole set \u00b5Bt\nh is also persuasive for \u00b5\u2217h, where \u00b5B := {\u00b5\u03b8\u2032 : \u03b8\u2032 \u2208 B} denotes the set\nof priors that are determined by the parameters \u03b8\u2032 \u2208 B. Lemma 6.7. There exists a constant C > 0, such that for \u03b2 = C(1 + \u03ba\u22121 \u221a K +M + d\u03c6\u03c32 log(HT )), OP4 Algorithm is persuasive with probability at least 1\u2212H\u22121T\u22121, i.e.,\nP\u03b8\u2217\n( \u22c3\nh\u2208[H]\n{ \u03b8\u2217h /\u2208 \u2229t\u2208[T ]Bth }) \u2264 H\u22121T\u22121.\nProof. We first analyze the probability for being non-persuasive. For any \u2016\u03b8\u2217h\u2016 \u2264 L\u03b8, using the union bound, we have\nP\u03b8\u2217\n( \u22c3\nt\u2208[T ],h\u2208[H]\n{ \u03b8\u2217h /\u2208 \u2229t\u2208[T ]Bth }) \u2264 \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nP\u03b8\u2217 h ( \u03b8\u2217h /\u2208 \u2229t\u2208[T ]Bth )\n\u2264 \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nP\u03b8\u2217 h ( \u2016\u03b8th \u2212 \u03b8\u2217h\u2016\u03a3t h > \u03b2 ) .\nThe following lemma gives the belief of confidence region for the linear parameter \u03b8\u2217h. The proof can be\ndirectly derived from Lemma 6 in Wang et al. [52].\nLemma 6.8 (Belief of Confidence Region). For any t \u2208 [T ] and h \u2208 [H], there exists a constant C > 0, such that for \u03b2 = C(1 + \u03ba\u22121 \u221a K +M + d\u03c6\u03c32 log(1/\u03b4)), given \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, we have \u2016\u03b8th \u2212 \u03b8\u2217h\u2016\u03a3th \u2264 \u03b2.\nBy Lemma 6.8, taking \u03b4 = H\u22122T\u22122, then we have P\u03b8\u2217(\u2016\u03b8th \u2212 \u03b8\u2217h\u2016\u03a3th > \u03b2) \u2264 H \u22122T\u22122. Summing up\nthe failure probabilities over t \u2208 [T ], we have P\u03b8\u2217(\u03b8\u2217 /\u2208 \u2229t\u2208[T ]Bt) \u2264 H\u22121T\u22121."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have presented a novel model, the MPP, which captures the misaligned incentives of uninformed decision makers and the long-term objective of an information possessor for the first time. We then provide a reinforcement learning algorithm, OP4, that is provably efficient in terms of both computational complexity and sample complexity, under mild assumptions. We remark that while we showcase this algorithm in particular problem instances with linear approximation or GLMs, the framework of OP4 does not rely on the function approximation form, as long as we can quantify the uncertainty of the prior estimation and Q-function (or transition model). In addition, we expect this optimism-pessimism design principle and its corresponding proof techniques to be generally useful for some other strategic learning problems with misaligned incentives involved.\nBesides extending our techniques to other design problems, we point out that several other open problems arises from our work. First, while it is natural that the sender have knowledge of receiver\u2019s utility functions in many cases (see Footnote 6), we hope to also study the problem even without initially knowing receiver\u2019s utility. Similar problem has been studied in Stackelberg games [40, 10] yet without measuring the performance in terms of the cumulative utility of sender (leader). Second, another interesting direction is to study the setting of Markov Bayesian persuasion with one sender and one receiver, both aiming at maximizing their own long-term cumulative utilities when the environment involves Markovian transitions."
        },
        {
            "heading": "A Omitted Proofs and Descriptions",
            "text": "A.1 Formal Description of the OP4\nThe formal description of the OP4 for MPPs is stated as follows:\nAlgorithm 2 The OP4 for MPPs\n1: Input: Number of Episodes T , Number of Step H 2: Parameters: \u03b2 > 0, \u03c1 > 0, \u03bb \u2208 R+. 3: Output: ath \u2208 A for each h \u2208 [H], t \u2208 [T ] 4: for episode t = 1 . . . T do 5: Receive the initial state st1 and context C t = (ct1, . . . , c t H). 6: for step h = H, . . . , 1 do 7: Compute the constrained least square problem\n\u03b8th \u2190 argmin \u2016\u03b8h\u2016\u2264L\u03b8\n\u2211\n\u03c4\u2208[t\u22121]\n[ \u03c9\u03c4h \u2212 f(\u03c6(c\u03c4h)\u22a4\u03b8h) ]2 .\n8: Calculate \u03a3th = \u03a6 2Id\u03c6 + \u2211 \u03c4\u2208[t\u22121] \u03c6(c \u03c4 h)\u03c6(c \u03c4 h) \u22a4. Update Bth \u2190 B\u03a3th(\u03b8 t h, \u03b2).\n9: Set \u00b5th(\u00b7|c) to the distribution of f ( \u03c6(c)\u22a4\u03b8th ) + zh.\n10: Calculate\n\u0393th = \u03bbId\u03c8 + \u2211\n\u03c4\u2208[t\u22121]\n\u03c8(s\u03c4h, \u03c9 \u03c4 h, a \u03c4 h)\u03c8(s \u03c4 h, \u03c9 \u03c4 h, a \u03c4 h) \u22a4,\n\u03b9th = \u2211\n\u03c4\u2208[t\u22121]\n\u03c8(s\u03c4h, \u03c9 \u03c4 h, a \u03c4 h)[v \u03c4 h + V t h+1(s \u03c4 h+1;C t)]\n11: Update qth \u2190 (\u0393th)\u22121\u03b9th.\n12: Set    Qth(\u00b7, \u00b7, \u00b7;Ct)\u2190 min{\u03c8(\u00b7, \u00b7, \u00b7)\u22a4qth + \u03c1\u2016\u03c8(\u00b7, \u00b7, \u00b7)\u2016(\u0393th )\u22121 ,H}, V th(\u00b7;Ct)\u2190 max\u03c0h\u2208Pers(\u00b5Bt\nh ,uh)\n\u2329 Qth, \u00b5 t h \u2297 \u03c0h \u232a \u2126\u00d7A\n(\u00b7;Ct). 13: end for 14: for step h = 1, . . . ,H do 15: Choose \u03c0th \u2208 argmax\u03c0h\u2208Pers(\u00b5Bt\nh ,uh)\n\u2329 Qth, \u00b5 t h \u2297 \u03c0h \u232a \u2126\u00d7A (sth;C t).\n16: end for 17: Execute \u03c0t to sample a trajectory {(sth, \u03c9th, ath, vth)}h\u2208[H]. 18: end for\nA.2 Proof of Lemma 6.5\nProof. We prove with an explicit construction of a signaling scheme that is robustly persuasive for any prior in B(\u00b5, \u01eb) and achieve the expected utility at least max\u03c0\u2208Pers(\u00b5,u) \u2329 Q,\u00b5 \u2297 \u03c0 \u232a \u2126\u00d7A\n(s) \u2212 H\u01eb/(p0D). To simplify the notation, we omit the s in u, Q andW .\nLet \u03c0\u2217 = argmax\u03c0\u2208Pers(\u00b5,u) \u2329 Q,\u00b5 \u2297 \u03c0 \u232a \u2126\u00d7A be a direct scheme without loss of generality [26]. For\neach a \u2208 A, let \u00b5a(\u00b7) := \u00b5(\u00b7) \u2299 \u03c0\u2217(a|\u00b7) denote the posterior of outcome (i.e., kernel 7) that action a is recommended by \u03c0, so the prior can be composed as \u00b5(\u00b7) = \u2211a\u2208A \u00b5a(\u00b7). Since \u03c0 is persuasive, we know\u222b \u03c9\u2208\u2126 \u00b5a(\u03c9) [u(\u03c9, a)\u2212 u(\u03c9, a\u2032)] \u2265 0,\u2200a\u2032 \u2208 A.\nLet \u03c00 be the fully revealing signaling scheme that always recommends (signals) the action that maximizes the receivers\u2019 utility at the realized outcome. For each a \u2208 A, let \u03b7a(\u00b7) := \u00b5(\u00b7) \u2299 \u03c00(a|\u00b7) denote the posterior of outcome that action a is recommended by \u03c00, so the prior can be composed as \u00b5(\u00b7) = \u2211a\u2208A \u03b7a(\u00b7). By regularity condition, we have \u222b\n\u03c9\u2208\u2126 \u03b7a(\u03c9)\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] \u2265\n\u222b\n\u03c9\u2208Wa(D) \u03b7a(\u03c9)\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] \u2265 p0D, \u2200a\u2032 \u2208 A.\nWe now show that the signaling scheme \u03c0\u2032 = (1\u2212\u03b4)\u03c0\u2217+\u03b4\u03c00 is persuasive for any prior \u00b5\u0303 \u2208 B(\u00b5, \u01eb) with \u03b4 = \u01ebp0D . One simple way to interpret this \u201ccompound\u201d signaling scheme is to follow \u03c0 \u2217 with probability (1\u2212\u03b4) and follow \u03c00 with probability \u03b4. Hence, given a recommended action a, the receiver would compute the posterior as \u00b5\u2032a = (1\u2212\u03b4)\u00b5a(\u03c9)+\u03b4\u03b7a(\u03c9). Let \u00b5\u2032a, \u00b5\u0303a be the outcome posterior of \u03c0\u2032 recommending action a under the true prior \u00b5 (resp. the perturbed prior \u00b5\u0303). So \u00b5\u2032a(\u00b7) = \u00b5(\u00b7)\u2299 \u03c0\u2032(a|\u00b7) and \u00b5\u0303a(\u00b7) = \u00b5\u0303(\u00b7) \u2299 \u03c0\u2032(a|\u00b7). By definition of persuasiveness, we need to show that for any recommended action (signal from \u03c0\u2032) a \u2208 A, the action a maximizes the receiver\u2019s utility under \u00b5\u2032a. This follows from the decomposition below,\n\u222b\n\u03c9\u2208\u2126 \u00b5\u0303a \u00b7\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ]\n\u2265 \u222b\n\u03c9\u2208\u2126 \u00b5\u2032a \u00b7\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] \u2212 \u2225\u2225\u00b5\u0303a \u2212 \u00b5\u2032a \u2225\u2225 1\n\u2265 \u222b\n\u03c9\u2208\u2126 [(1\u2212 \u03b4)\u00b5a(\u03c9) + \u03b4\u03b7a(\u03c9)] \u00b7\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] \u2212 \u2016\u00b5\u0303a \u2212 \u00b5a\u20161\n=\n\u222b\n\u03c9\u2208\u2126 (1\u2212 \u03b4)\u00b5a(\u03c9)\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] +\n\u222b\n\u03c9\u2208\u2126 \u03b4\u03b7a(\u03c9)\n[ u(\u03c9, a)\u2212 u(\u03c9, a\u2032) ] \u2212 \u2016\u00b5\u0303a \u2212 \u00b5a\u20161\n\u2265 \u03b4p0D \u2212 \u2016\u00b5\u0303a \u2212 \u00b5a\u20161 = \u01eb\u2212 \u2016\u00b5\u0303a \u2212 \u00b5a\u20161 \u2265 0.\nThe first inequality is by the fact that u(\u03c9, a) \u2208 [0, 1] for any \u03c9, a and thus \u2211a(\u00b5\u0303a\u2212\u00b5\u2032a)\u00b7[u(\u03c9, a)\u2212 u(\u03c9, a\u2032)] \u2264 \u2016\u00b5\u0303a \u2212 \u00b5\u2032a\u20161. The second inequality is from \u00b5\u2032a = (1 \u2212 \u03b4)\u00b5a(\u03c9) + \u03b4\u03b7a(\u03c9). The third inequality is by construction of \u00b5a and \u03b7a induced by signaling scheme \u03c0 and \u03c0 0. The last inequality is by the fact that \u2016\u00b5\u0303a \u2212 \u00b5\u2032a\u20161 = \u2016(\u00b5\u0303\u2212 \u00b5\u2032)\u2299 \u03c0\u2032(a|\u00b7)\u20161 \u2264 \u2016\u00b5\u0303\u2212 \u00b5\u2032\u20161 = \u01eb, since \u2016\u03c0\u2032(a|\u00b7)\u2016\u221e \u2264 1 It remains to show the expected utility under signaling scheme \u03c0\u2032 is at least \u2329 Q,\u00b5\u2297\u03c0\u2217\n\u232a \u2126\u00d7A \u2212H\u01eb/(p0D).\n7In this proof, we will directly work with the posterior without normalization (kernel) to simplify our notations and derivations,\nbecause \u222b \u03c9\u2208\u2126 \u00b5a(\u03c9) [u(\u03c9, a)\u2212 u(\u03c9, a \u2032)] \u2265 0 \u21d0\u21d2 \u222b \u03c9\u2208\u2126 \u00b5a(\u03c9)\u222b \u03c9\u2208\u2126 \u00b5a(\u03c9) [u(\u03c9, a)\u2212 u(\u03c9, a\u2032)] \u2265 0. We use \u2299 to denote the Hadamard product.\nThis is due to the following inequalities,\n\u2329 Q,\u00b5\u2297 \u03c0\u2032 \u232a \u2126\u00d7A \u2212 \u2329 Q,\u00b5\u2297 \u03c0\u2217 \u232a \u2126\u00d7A =\n\u222b\n\u03c9\u2208\u2126,a\u2208A \u00b5(\u03c9)\n[ \u03c0\u2032(a|\u03c9)\u2212 \u03c0\u2217(a|\u03c9) ] Q(\u03c9, a)\n=\n\u222b\n\u03c9\u2208\u2126,a\u2208A \u00b5(\u03c9)\n[ \u03b4\u03c00(a|\u03c9)\u2212 \u03b4\u03c0\u2217(a|\u03c9) ] Q(\u03c9, a)\n\u2265 \u2212\u03b4 \u222b\n\u03c9\u2208\u2126,a\u2208A \u00b5(\u03c9)\u03c0(a|\u03c9)Q(\u03c9, a)\n\u2265 \u2212H\u03b4 = \u2212 H\u01eb p0D .\nThe first and second equalities use the definition and linearity. The third and last inequalities use the fact that E[Q(\u03c9, a)] \u2208 [0,H] and remove the positive term.\nA.3 Properties for the Robustness Gap\nWe present the robustness gap Gap for the ground-truth prior in Lemma 6.5. For the estimation of prior \u00b5th given in Algorithm 2 which may not satisfy the regularity condition, we also have corresponding robustness gap.\nLemma A.1. For any h \u2208 [H], t \u2208 [T ] and s \u2208 S , on the event of {\u03b8\u2217h \u2208 Bth}, we have\nGap(s, \u00b5th,B(\u00b5 t h, \u01eb t h);Q t h) \u2264\n2H\u01eb p0D .\nProof. For any fixed action a \u2208 A, on the given event, we have\nP\u03c9\u223c\u00b5t h (\u00b7)[\u03c9 \u2208 Ws,a(D)] =\n\u222b\n\u03c9\u2208\u2126 \u00b5th(\u03c9)I(\u03c9 \u2208 Ws,a(D))d\u03c9\n=\n\u222b\n\u03c9\u2208\u2126 \u00b5\u2217h(\u03c9)I(\u03c9 \u2208 Ws,a(D))d\u03c9 +\n\u222b\n\u03c9\u2208\u2126 [\u00b5th(\u03c9)\u2212 \u00b5\u2217h(\u03c9)]I(\u03c9 \u2208 Ws,a(D))d\u03c9\n\u2265 \u222b\n\u03c9\u2208\u2126 \u00b5\u2217h(\u03c9)I(\u03c9 \u2208 Ws,a(D))d\u03c9 + \u2016\u00b5th \u2212 \u00b5\u2217h\u20161\n\u2265 p0 \u2212 \u01ebth,\nwhere I is the indicating function. The last inequality uses the regularity condition for the real prior \u00b5\u2217h. For \u01ebth \u2264 p0/2, we have P\u03c9\u223c\u00b5th(\u00b7)[\u03c9 \u2208 Ws,a] \u2264 p0/2. Then by Lemma 6.5, we can arrive at\nGap(s, \u00b5th,B(\u00b5 t h, \u01eb t h);Q t h) \u2264 2H\u01ebth p0D .\nFor \u01ebth > p0/2, the bound holds trivially since 2H\u01eb t h/(p0D) > H .\nThe robustness gap Gap defined in equation (6.4) measures the loss in value functions for being robustly\npersuasive for a subset of priors. In the following lemma, we show that we can also use Gap to bound the difference in expected optimal Q-functions between different priors. Lemma A.2. Denote B1,2 := B ( \u00b51, \u2016\u00b51 \u2212 \u00b52\u20161 ) for any fixed state s \u2208 S and \u00b51, \u00b52 \u2208 \u2206(\u2126). Then given a known Q-function Q(\u00b7, \u00b7, \u00b7), we have\nmax \u03c01\u2208Pers(\u00b51,u)\n\u2329 Q,\u00b51\u2297\u03c01 \u232a \u2126\u00d7A\n(s)\u2212 max \u03c02\u2208Pers(\u00b52,u)\n\u2329 Q,\u00b52\u2297\u03c02 \u232a \u2126\u00d7A (s) \u2264 Gap(s, \u00b51,B1,2;Q)+ H\n2 \u2016\u00b51\u2212\u00b52\u20161.\nProof. Fix \u00b51, \u00b52 \u2208 \u2206(\u2126), we respectively choose the optimal signaling scheme\n\u03c0i = argmax \u03c0i\u2208Pers(\u00b5i,u)\n\u2329 Q,\u00b5i \u2297 \u03c0i \u232a \u2126\u00d7A (s), i = 1, 2.\nThen among all the signaling schemes persuasive for all B1,2, let \u03c03 maximize \u2329 Q,\u00b51 \u2297 \u03c0 \u232a \u2126\u00d7A (s). Since\n\u03c03 is persuasive for \u00b52, we know \u2329 Q,\u00b52\u2297\u03c02 \u232a \u2126\u00d7A (s) \u2265 \u2329 Q,\u00b52\u2297\u03c03 \u232a \u2126\u00d7A (s) by definition. Therefore, we have\n\u2329 Q,\u00b51 \u2297 \u03c01 \u2212 \u00b52 \u2297 \u03c02 \u232a \u2126\u00d7A (s) \u2264 \u2329 Q,\u00b51 \u2297 \u03c01 \u2212 \u00b52 \u2297 \u03c03 \u232a \u2126\u00d7A (s)\n\u2264 \u2329 Q,\u00b51 \u2297 \u03c01 \u2212 \u00b51 \u2297 \u03c03 \u232a \u2126\u00d7A (s) + \u2329 Q,\u00b51 \u2297 \u03c03 \u2212 \u00b52 \u2297 \u03c03 \u232a \u2126\u00d7A (s) = Gap(s, \u00b51,B1,2;Q) + H\n2 \u2016\u00b51 \u2212 \u00b52\u20161.\nThe last equality uses the definition of Gap and Lemma A.3.\nLemma A.3. Given a Q-function Q(\u00b7, \u00b7, \u00b7) \u2208 [0,H], for any fixed state s \u2208 S , \u00b51, \u00b52 \u2208 \u2206(\u2126) and any signaling scheme \u03c0, we have\n\u2223\u2223\u2329Q,\u00b51 \u2297 \u03c0 \u232a \u2126\u00d7A (s)\u2212 \u2329 Q,\u00b52 \u2297 \u03c0 \u232a \u2126\u00d7A (s) \u2223\u2223 \u2264 H\n2 \u2016\u00b51 \u2212 \u00b52\u20161.\nProof. Fix \u00b51(\u00b7), \u00b52(\u00b7) \u2208 \u2206(\u2126). For any x \u2208 R, we have\n\u2223\u2223\u2329Q,\u00b51 \u2297 \u03c0 \u2212 \u00b52 \u2297 \u03c0 \u232a \u2126\u00d7A (s) = \u2223\u2223\u2223\u2223 \u222b\n\u03c9\u2208\u2126 [\u00b51(\u03c9)\u2212 \u00b52(\u03c9)]\n[ \u222b\na\u2208A \u03c0(a|s, \u03c9)Q(s, \u03c9, a)da \u2212 x\n] d\u03c9 \u2223\u2223\u2223\u2223\n\u2264 \u2016\u00b51 \u2212 \u00b52\u20161 \u00b7 sup \u03c9\u2208\u2126\n\u2223\u2223\u2223\u2223 \u222b\na\u2208A \u03c0(a|s, \u03c9)Q(s, \u03c9, a)da \u2212 x\n\u2223\u2223\u2223\u2223,\nwhere the last inequality is derived from Holder\u2019s inequality. With Q-function taking values in [0,H], we can set x = H/2 and achieve the optimality.\nA.4 Proof of Lemma 6.1\nProof. Before presenting the proof, we first define two operators J\u2217h and J t h:\n(J\u2217hf)(s;C) = \u3008f, \u00b5\u2217h \u2297 \u03c0\u2217h\u3009\u2126\u00d7A(s;C), (Jthf)(s;C) = \u3008f, \u00b5th \u2297 \u03c0th\u3009\u2126\u00d7A(s;C), (A.1)\nfor any h \u2208 [H], t \u2208 [T ] and any function f(\u00b7, \u00b7, \u00b7;C) : S \u00d7\u2126\u00d7A \u2192 R under the context C . Moreover, for any h \u2208 [H], t \u2208 [T ] and any state s \u2208 S , we define\n\u03beth(s;C) = (J \u2217 hQ t h)(s;C)\u2212 (JthQth)(s;C) = \u3008Qth, \u00b5\u2217h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th\u3009\u2126\u00d7A(s;C). (A.2)\nAfter introducing these notations, we decompose the instantaneous regret at the t-th episode into two terms,\nV \u22171 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct) = V \u22171 (st1;Ct)\u2212 V t1 (st1;Ct)\ufe38 \ufe37\ufe37 \ufe38 p1 +V t1 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct)\ufe38 \ufe37\ufe37 \ufe38 p2 . (A.3)\nThen we consider these two terms separately. By the definition of value functions in (3.1) and the operator J\u2217h in (A.1), we have V \u2217 h = J \u2217 hQ \u2217 h. By the construction of Algorithm 2, we have V t h = J t hQ t h similarly. Thus, for the first term p1 defined in equation (A.3), using \u03be t h defined in (A.2), for any h \u2208 [H], t \u2208 [T ], we have\nV \u2217h \u2212 V th = J\u2217hQ\u2217h \u2212 JthQth = (J\u2217hQ\u2217h \u2212 J\u2217hQth) + (J\u2217hQth \u2212 JthQth) = J\u2217h(Q \u2217 h \u2212Qth) + \u03beth.\nNext, by the definition of the temporal-difference error \u03b4th in (6.1) and the Bellman optimality equation in equation (3.2), we have\nQ\u2217h \u2212Qth = (vh + PhV \u2217h+1)\u2212 (vh + PhV th+1 \u2212 \u03b4th) = Ph(V \u2217h+1 \u2212 V th+1) + \u03b4th.\nHence we get\nV \u2217h \u2212 V th = J\u2217hPh(V \u2217h+1 \u2212 V th+1) + +J\u2217h\u03b4th + \u03beth.\nThen, by recursively applying the above formula, we have\nV \u22171 \u2212 V t1 = ( \u220f\nh\u2208[H]\nJ \u2217 hPh ) (V \u2217H+1 \u2212 V tH+1) + \u2211\nh\u2208[H]\n( \u220f\ni\u2208[h]\nJ \u2217 iPi ) J \u2217 h\u03b4 t h + \u2211\nh\u2208[H]\n( \u220f\ni\u2208[h]\nJ \u2217 iPi ) \u03beth.\nBy the definition of \u03beth in equation (A.2) and \u03b6 3 t,h in equation (6.2), we get\n\u2211\nh\u2208[H]\n( \u220f\ni\u2208[h]\nJ \u2217 iPi ) \u03beth(sh;C t) = \u2211\nh\u2208[H]\nE\u00b5\u2217,\u03c0\u2217 {[ \u3008Qth, \u00b5\u2217h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th\u3009\u2126\u00d7A(sh;Ct)|s1 = st1 ]} .\nNotice that V \u2217H+1 = V t H+1 = 0. Therefore, for any episode t \u2208 [T ], we have\nV \u22171 (s t 1;C t)\u2212 V t1 (st1;Ct) = \u2211\nh\u2208[H]\nE\u00b5\u2217,\u03c0\u2217 {[ \u3008Qth, \u00b5\u2217h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th\u3009\u2126\u00d7A(sh;Ct)|s1 = st1 ]}\n+ \u2211\nh\u2208[H]\nE\u00b5\u2217,\u03c0\u2217 [ \u03b4th(sh, \u03c9h, ah)|s1 = st1 ] .\nNow we come to bound the second term p2 in equation (A.3). By the definition of the temporal-difference error \u03b4th in (6.1), for any h \u2208 [H], t \u2208 [T ], we note that\n\u03b4th(s t h, \u03c9 t h, a t h) = (v t h + PhV t h+1 \u2212Qth)(sth, \u03c9th, ath;Ct)\n= (vth + PhV t h+1 \u2212Q\u03c0\nt\nh )(s t h, \u03c9 t h, a t h;C t) + (Q\u03c0 t h \u2212Qth)(sth, \u03c9th, ath;Ct) = (PhV t h+1 \u2212 PhV \u03c0 t h+1)(s t h, \u03c9 t h, a t h) + (Q \u03c0t h \u2212Qth)(sth, \u03c9th, ath).\nwhere the last equality follows the Bellman equation (3.1). Furthermore, using \u03b61t,h and \u03b6 2 t,h defined in (6.2), we have\nV th(s t h;C t)\u2212 V \u03c0th (sth;Ct) =(V th \u2212 V \u03c0 t h )(s t h;C t)\u2212 \u03b4th(sth, \u03c9th, ath) + (Q\u03c0 t\nh \u2212Qth)(sth, \u03c9th, ath;Ct) + (PhV t h+1 \u2212 PhV \u03c0 t h+1)(s t h, \u03c9 t h, a t h;C t)\n=(V th \u2212 V\u0303 th)(sth;Ct)\u2212 \u03b4th(sth, \u03c9th, ath) + (V\u0303 th \u2212 V \u03c0 t h )(s t h;C t) + (Q\u03c0 t h \u2212Qth)(sth, \u03c9th, ath;Ct) + ( Ph(V t h+1 \u2212 V \u03c0 t h+1) ) (sth, \u03c9 t h, a t h;C t)\u2212 (V th+1 \u2212 V \u03c0 t h+1)(s t h+1;C t) + (V th+1 \u2212 V \u03c0 t h+1)(s t h+1;C t) = [ V th+1(s t h+1;C t)\u2212 V \u03c0th+1(sth+1;Ct) ] + [ V th(s t h;C t)\u2212 V\u0303 th(sth;Ct) ] \u2212 \u03b4th(sth, \u03c9th, ath) + \u03b61t,h + \u03b62t,h.\nApplying the above equation recursively, we get that\nV t1 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct) =V tH+1(stH ;Ct)\u2212 V \u03c0 t H+1(s t H ;C\nt) + \u2211\nh\u2208[H]\n[ V th(s t h;C t)\u2212 V\u0303 th(sth;Ct) ]\n\u2212 \u2211\nh\u2208[H]\n\u03b4th(s t h, \u03c9 t h, a t h) +\n\u2211\nh\u2208[H]\n(\u03b61t,h + \u03b6 2 t,h).\nAgain by Bellman equation (3.1), we have,\nV th(s t h;C t)\u2212 V\u0303 th(sth;Ct) = \u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t).\nThen we use V tH+1 = V \u03c0t H+1 = 0 to simplify the decomposition to the following form:\nV t1 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct) = \u2211\nh\u2208[H]\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t)\n\u2212 \u2211\nh\u2208[H]\n\u03b4th(s t h, \u03c9 t h, a t h) +\n\u2211\nh\u2208[H]\n(\u03b61t,h + \u03b6 2 t,h).\nTherefore, combining p1 and p2, we can conclude the proof of this lemma.\nReg(T, \u00b5\u2217) = \u2211\nt\u2208[T ]\n[ V \u22171 (s t 1;C t)\u2212 V \u03c0t1 (st1;Ct) ]\n= \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n{ E\u00b5\u2217\nh ,\u03c0\u2217 h [\u03b4th(sh, \u03c9h, ah)|s1 = st1]\u2212 \u03b4th(sth, \u03c9th, ath)\n} + \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n(\u03b61t,h, \u03b6 2 t,h)\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nE\u00b5\u2217 h ,\u03c0\u2217 h\n[\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t)|s1 = st1 ]\n+ \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t).\nTherefore, we conclude the proof of the lemma.\nA.5 Proof of Lemma 6.2\nProof. In the following lemma, we firstly bound the difference between the Q-function maintained in Algorithm 2 (without bonus) and the real Q-function of any policy \u03c0 by their expected difference at next step, plus an error term. This error term can be upper bounded by our bonus with high probability. This lemma can be derived from Lemma B.4 in [25] with slight revisions. Lemma A.4. Set \u03bb = max{1,\u03a82}. There exists an absolute constant c\u03c1 such that for \u03c1 = c\u03c1d\u03c8H \u221a \u03b9 where \u03b9 = log(2d\u03c8\u03a8 2T/\u03b4), and for any fixed policy \u03c0, with probability at least 1 \u2212 \u03b4/2, we have for all s \u2208 S , \u03c9 \u2208 \u2126, a \u2208 A, h \u2208 [H], t \u2208 [T ],\n\u03c8(s, \u03c9, a)\u22a4qth \u2212Q\u03c0h(s, \u03c9, a) = Ph(V th+1 \u2212 V \u03c0h+1)(s, \u03c9, a) +\u25b3th(s, \u03c9, a),\nfor some\u25b3th(s, \u03c9, a) that satisfies |\u25b3th(s, \u03c9, a)| \u2264 \u03c1\u2016\u03c8(s, \u03c9, a)\u2016(\u0393th)\u22121 .\nNow we are ready to prove Lemma 6.2. By the definition of \u03b4th in (6.1), we have \u03b4 t h = (vh + PhV t h+1 \u2212\nQth) = (PhV t h+1\u2212PhV \u03c0\nt h+1)+ (Q \u03c0t h \u2212Qth). Therefore, by the construction of Qth in Algorithm 2, we obtain\nthat\n\u03b4th(s, \u03c9, a) \u2265 (PhV th+1 \u2212 PhV \u03c0 t h+1)(s, \u03c9, a) +Q \u03c0t h (s, \u03c9, a)\u2212 ( \u03c8(s, \u03c9, a)\u22a4qth + \u03c1\u2016\u03c8(s, \u03c9, a)\u2016(\u0393th)\u22121 )\n= \u2212\u25b3th(s, \u03c9, a) \u2212 \u03c1\u2016\u03c8(s, \u03c9, a)\u2016(\u0393t h )\u22121 \u2265 \u22122\u03c1\u2016\u03c8(s, \u03c9, a)\u2016(\u0393t h )\u22121 ,\nwhich concludes the proof.\nA.6 Proof of Lemma 6.3\nProof. Denote the optimal signaling schemes corresponding to the real prior \u00b5\u2217h and the estimated prior \u00b5 t h respectively as\n\u03c0\u2032h = argmax \u03c0h\u2208Pers(\u00b5 \u2217 h )\n\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0h \u232a \u2126\u00d7A\n(\u00b7;Ct) and \u03c0\u2032\u2032h = argmax \u03c0h\u2208Pers(\u00b5 t h )\n\u2329 Qth, \u00b5 t h \u2297 \u03c0h \u232a \u2126\u00d7A (\u00b7;Ct),\nwhere the Q-function Qth is given by Algorithm 2. Notably, \u03c0 \u2032 h is different from the truly optimal policy \u00b5\u2217h, since \u03c0 \u2032 h is computed based on the approximate Q-function Q t h. By definition, we can decompose the difference as follows:\n\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t) = \u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5\u2217h \u2297 \u03c0\u2032h \u232a \u2126\u00d7A (sh;C t) (A.4)\n+ \u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2032h \u2212 \u00b5th \u2297 \u03c0\u2032\u2032h \u232a \u2126\u00d7A (sh;C t) (A.5) + \u2329 Qth, \u00b5 t h \u2297 \u03c0\u2032\u2032h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t). (A.6)\nBy definition, equation (A.4) is always non-positive. Apply Lemma A.2 to equation (A.5) and we can get\n\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2032h \u2212 \u00b5th \u2297 \u03c0\u2032\u2032h \u232a \u2126\u00d7A (sh;C t) \u2264Gap ( sh, \u00b5 \u2217 h(\u00b7|cth),B1 ( \u00b5\u2217h(\u00b7|cth), \u2225\u2225\u00b5\u2217h(\u00b7|cth)\u2212 \u00b5th(\u00b7|cth) \u2225\u2225 1 ) ;Qth )\n+ H\n2\n\u2225\u2225\u00b5\u2217h(\u00b7|cth)\u2212 \u00b5th(\u00b7|cth) \u2225\u2225 1 .\nAccording to Corollary 6.6, we can bound the above equation with the norm of feature vector and the radius of confidence region for \u03b8h.\n\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2032h \u2212 \u00b5th \u2297 \u03c0\u2032\u2032h \u232a \u2126\u00d7A (sh;C t) \u2264 (HL\u00b5K p0D + HL\u00b5K 2 ) \u03b2\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nWe also note that equation (A.6) is equal to Gap ( sh, \u00b5\nt h(\u00b7|cth), \u00b5Bth(\u00b7|c t h);Q t h\n) . By Lemma A.1, on the\nevent {\u03b8\u2217h \u2208 Bth}, we have\nGap ( sh, \u00b5 t h(\u00b7|cth), \u00b5Bt h (\u00b7|cth);Qth ) \u2264 2HL\u00b5K\np0D \u03b2\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nTherefore, on the given event, we have\nE\u00b5\u2217 h ,\u03c0\u2217 h\n[\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t)|s1 = st1 ] \u2264\n( 3HL\u00b5K\np0D + HL\u00b5K 2\n) \u03b2\u2016\u03c6(cth)\u2016(\u03a3t\nh )\u22121 .\nSumming up together, we get\n\u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\nE\u00b5\u2217 h ,\u03c0\u2217 h\n[\u2329 Qth, \u00b5 \u2217 h \u2297 \u03c0\u2217h \u2212 \u00b5th \u2297 \u03c0th \u232a \u2126\u00d7A (sh;C t)|s1 = st1 ]\n\u2264 ( 3HL\u00b5K\np0D + HL\u00b5K 2\n) \u03b2 \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nTherefore, we conclude the proof of Lemma 6.3.\nA.7 Proof of Lemma 6.4\nProof. By definition, we can rewrite the difference in Lemma 6.4 as\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t) =\n\u222b\n\u2126\u00d7A\n[ \u00b5th(\u03c9|cth)\u2212 \u00b5\u2217h(\u03c9|cth) ] \u03c0th(s, \u03c9, a)Q t h(s, \u03c9, a)dad\u03c9\n=\n\u222b\n\u2126\n[ \u00b5th(\u03c9|cth)\u2212 \u00b5\u2217h(\u03c9|cth)\n] \u222b\nA \u03c0th(s, \u03c9, a)Q t h(s, \u03c9, a)dad\u03c9.\nBy Holder\u2019s inequality, we have\n\u2223\u2223\u2223\u2223 \u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t) \u2223\u2223\u2223\u2223 \u2264 \u2225\u2225\u00b5th(\u00b7|cth)\u2212 \u00b5\u2217h(\u00b7|cth) \u2225\u2225 1 sup \u03c9\u2208\u2126 \u2223\u2223\u2223\u2223 \u222b A \u03c0th(s, \u03c9, a)Q t h(s, \u03c9, a)da \u2223\u2223\u2223\u2223.\nSince Qth \u2264 H for any h \u2208 [H] and t \u2208 [T ], the inequality can be simplified to \u2223\u2223\u2223\u2223 \u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t) \u2223\u2223\u2223\u2223 \u2264 H \u2225\u2225\u00b5th(\u00b7|cth)\u2212 \u00b5\u2217h(\u00b7|cth) \u2225\u2225 1 .\nWith the assumption of the prior and link function, on the given event, we obtain that\n\u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n\u2329 Qth, (\u00b5 t h \u2212 \u00b5\u2217h)\u2297 \u03c0th \u232a \u2126\u00d7A (sth;C t) \u2264 HL\u00b5K\u03b2 \u2211\nh\u2208[H]\n\u2211\nt\u2208[T ]\n\u2016\u03c6(cth)\u2016(\u03a3t h )\u22121 .\nTherefore, we conclude the proof of Lemma 6.4.\nA.8 Proof of Corollary 6.6\nProof. According to Assumption 4.2 for the prior, we can show that for any \u00b5\u03b8\u2032(\u00b7|c) \u2208 B,\n\u2016\u00b5\u03b8(\u00b7|c) \u2212 \u00b5\u03b8\u2032(\u00b7|c)\u20161 \u2264 L\u00b5 \u2225\u2225f(\u03c6(c)\u22a4\u03b8)\u2212 f(\u03c6(c)\u22a4\u03b8\u2032) \u2225\u2225.\nMoreover, by Assumption 4.3 for the link function f(\u00b7), we have\n\u2016\u00b5\u03b8(\u00b7|c) \u2212 \u00b5\u03b8\u2032(\u00b7|c)\u20161 \u2264 L\u00b5K \u2225\u2225\u03c6(c)\u22a4(\u03b8 \u2212 \u03b8\u2032) \u2225\u2225 \u2264 L\u00b5K \u2016\u03c6(c)\u2016\u03a3\u22121 \u01eb.\nTherefore, B \u2286 B(\u00b5\u03b8(\u00b7|c), L\u00b5K\u2016\u03c6(c)\u2016\u03a3\u22121\u01eb), and by Lemma 6.5, we can conclude the result.\nA.9 Auxiliary Lemmas\nThis section presents several auxiliary lemmas and their proofs.\nLemma A.5 (Martingale Bound; [9]). For \u03b61t,h and \u03b6 2 t,h defined in (6.2) and for any fixed \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4/2, we have \u2211\nt\u2208[T ]\n\u2211\nh\u2208[H]\n(\u03b61t,h + \u03b6 2 t,h) \u2264\n\u221a 16TH3 log(4/\u03b4).\nProof. See [9] for a detailed proof.\nLemma A.6. Suppose that \u03c61, \u03c62, . . . , \u03c6T \u2208 Rd\u03c6\u00d7d and for any 1 \u2264 i \u2264 T , there exists a constant \u03a6 > 0 such that \u2016\u03c6i\u2016 \u2264 \u03a6. Let \u03a3t = \u03bbId\u03c6 + \u2211 i\u2208[t\u22121] \u03c6i\u03c6 \u2032 i for some \u03bb \u2265 \u03a62. Then,\n\u2211\nt\u2208[T ]\n\u2016\u03c6t\u2016(\u03a3t)\u22121 \u2264 \u221a 2d\u03c6T log(1 + T\u03a62/(\u03bbd\u03c6)).\nProof. Firstly, we apply Cauchy-Schwartz inequality,\n\u2211\nt\u2208[T ]\n\u2016\u03c6t\u2016(\u03a3t)\u22121 \u2264 \u221a T \u2211\nt\u2208[T ]\n\u2016\u03c6t\u20162(\u03a3t)\u22121 .\nSince \u2016\u03c6t\u2016(\u03a3t)\u22121 = \u221a \u03c6\u22a4t (\u03a3t) \u22121\u03c6t \u2264 \u221a \u03bb\u22121\u03c6\u22a4t \u03c6t \u2264 \u03a6/ \u221a \u03bb \u2264 1, we can use Lemma A.7 to bound the sum of squares:\n\u2211\nt\u2208[T ]\n\u2016\u03c6t\u2016(\u03a3t)\u22121 \u2264 \u221a 2T log(det(\u03a3T ) det(\u03a31)\u22121)\n\u2264 \u221a\n2d\u03c6T log(1 + T\u03a62/(\u03bbd\u03c6)).\nThe last inequality is derived from Lemma A.8. Lemma A.7 (Sum of Potential Function; [1]). For any sequence of {\u03c6t}t\u2208[T ], let \u03a3t = \u03bbIh+ \u2211 t\u2208[t\u22121] \u03c6i\u03c6 \u2032 i for some \u03bb \u2265 0. Then we have \u2211\nt\u2208[T ]\nmin{\u2016\u03c6t\u20162(\u03a3t)\u22121 , 1} \u2264 2 log(det(\u03a3T ) det(\u03a31) \u22121).\nProof. See [1] for a detailed proof.\nLemma A.8 (Determinant-Trace Inequality). Suppose that \u03c61, \u03c62, . . . , \u03c6T \u2208 Rd\u03c6\u00d7d and for any 1 \u2264 i \u2264 T , there exists a constant \u03a6 > 0 such that \u2016\u03c6i\u2016 \u2264 \u03a6. Let \u03a3t = \u03bbId\u03c6 + \u2211 i\u2208[t\u22121] \u03c6i\u03c6 \u2032 i for some \u03bb \u2265 0. Then,\ndet(\u03a3t) \u2264 ( \u03bb+ t\u03a62/d\u03c6 )d\u03c6 .\nProof. Let \u03bb1, \u03bb2, . . . , \u03bbh be the eigenvalues of \u03a3t. Since \u03a3t is positive definite, its eigenvalues are positive. Also, note that det(\u03a3t) = \u220fd\u03c6 s=1 \u03bbs and tr(\u03a3t) = \u2211h s=1 \u03bbs. By inequality of arithmetic and geometric means\ndet(\u03a3t) \u2264 (tr(\u03a3t)/d\u03c6)d\u03c6 .\nIt remains to upper bound the trace:\ntr(\u03a3t) = tr(\u03bbId\u03c6) + t\u22121\u2211\ni=1\ntr(\u03c6i\u03c6 \u2032 i) = d\u03c6\u03bb+\nt\u22121\u2211\ni=1\n\u2016\u03c6i\u20162 \u2264 d\u03c6\u03bb+ t\u03a62\nand the lemma follows."
        }
    ],
    "title": "Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning",
    "year": 2022
}