{
    "abstractText": "Generalized random forests (Athey, Tibshirani, and Wager, 2019) build upon the wellestablished success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems (Yang et al., 2021), we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators obtained from forests whose trees are grown through the fixed-point splitting rule, and provide numerical simulations demonstrating that the estimators obtained from such forests are comparable to those obtained from the more costly gradient-based rule.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Fleischer"
        },
        {
            "affiliations": [],
            "name": "David A. Stephens"
        },
        {
            "affiliations": [],
            "name": "Archer Yang"
        }
    ],
    "id": "SP:1d7e873bbe8e47752cd205354f42dab5610696f2",
    "references": [
        {
            "authors": [
                "Angrist",
                "Joshua D",
                "J\u00f6rn-Steffen Pischke"
            ],
            "title": "Mostly harmless econometrics: An empiricist\u2019s",
            "year": 2009
        },
        {
            "authors": [
                "Athey",
                "Susan",
                "Julie Tibshirani",
                "Stefan Wager"
            ],
            "title": "Generalized random forests",
            "venue": "Proceedings of the National Academy of Sciences",
            "year": 2019
        },
        {
            "authors": [
                "R Forests"
            ],
            "title": "Estimation and inference of heterogeneous treatment",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Random forests Breiman (2001) are a popular method for non-parametric learning, conventionally presented in the context of regression/classification of a response given a set of predictors. Given samples (Xi, Yi) \u2208 X \u00d7 Y of a p-dimensional predictor Xi = (Xi1, ..., Xip)T and response Yi, random forests are used to produce estimates of the local mean function \u00b5\u2217(x) := E [Y |X = x] by aggregating the individual predictions made by an ensemble of decision trees. Regression trees fit the response at a given predictor level as the mean training response of all samples appearing in the same partition of the predictor space, while the partition structure is determined through recursive application of a loss-minimizing splitting rule. (Breiman et al., 1984; Hastie et al., 2009). However, despite offering a powerful method for conditional mean estimation, this procedure fundamentally relies on having access to the response in order to grow the forest of trees. Indeed, there are many statistically-interesting quantities which are not typically observed as part of a sample (e.g. treatment effects), making them unamenable towards direct loss minimization.\nThe approach taken by Athey, Tibshirani, and Wager (2019) is to use forest-based estimation as the first step in a two-part procedure: Casting random forests as an adaptive kernel method,\n\u2217Department of Mathematics and Statistics, McGill University, Montreal, Canada.\nar X\niv :2\n30 6.\n11 90\n8v 1\n[ st\nat .M\nL ]\n2 0\nJu n\nfollowed by using the induced weights to solve a set of estimating equations. In the absence of any feasible loss minimization, such generalized random forests (GRF) exploit much of the success of conventional forests towards estimating any quantity identifiable as a solution to local moment conditions. A distinguishing trait of GRF can be found in its use of a tree-growing scheme which targets heterogeneity-maximization in place of loss-minimization, recursively seeking partitions such as to maximize cross-split fitted heterogeneity rather than minimizing within-split fitted loss. Furthermore, Athey, Tibshirani, and Wager (2019) show that the heterogeneity-maximizing splitting rule is equivalent to a conventional regression tree split over pseudo-outcomes computed as a gradientbased approximation of the regional fits. The problem-specific pseudo-outcomes are re-computed for all non-terminal parent nodes within each tree of the forest, making this step of the GRF procedure potentially onerous when the dimension of the target quantity is large, e.g. multivariate treatment effects.\nOur goal is to offer an acceleration of the GRF tree-growing procedure by using pseudo-outcomes obtained as the result of a fixed-point approximation rather than the original gradient-based approximation. We present a general framework for the substitution of the fixed-point rule in place of the gradient-based rule, targeted towards problems for which dimension of the underlying quantity is large, and show the equivalence of the two methods in the one-dimensional setting. The primary application of the fixed-point method presented herein focuses on heterogeneous treatment effect estimation for multi-leveled treatment assignment and multivariate continuous treatments."
        },
        {
            "heading": "2 Generalized Random Forests",
            "text": ""
        },
        {
            "heading": "2.1 Forest-Based Estimation of Heterogeneous Estimating Equations",
            "text": "Let (Xi, Oi) \u2208 X \u00d7 O denote a set of samples and suppose that \u03b8\u2217(x) is any quantity identifiable by local moment conditions on an estimating function \u03c8\u03b8\u2217(x),\u03bd\u2217(x)(\u00b7) of the form\n0 = EO|X [ \u03c8\u03b8\u2217(x),\u03bd\u2217(x)(Oi)|Xi = x ] , (1)\nfor \u03bd\u2217(x) denoting any optional nuisance quantities. Classically, we might think of the Oi as the outcomes associated with predictors Xi, i.e. Oi = Yi as in the case of conditional mean estimation. However, in general, we permit the observable quantities Oi to contain more structured information, e.g. Oi = {Yi,Wi} corresponding to an outcome Yi and treatment Wi, or Oi = {Yi,Wi, Zi} for an outcome, treatment, and instrument Zi. There is a rich body of literature studying solutions to heterogeneous estimating equations of the form (1), particularly in the context of local maximum likelihood estimation (see Athey, Tibshirani, and Wager (2019) for a review) owing in part to the observation that, for many choices of the \u03c8-function, one can derive consistent non-parametric covariance estimates via the sandwich estimator (Carroll, Ruppert, and Welsh, 1998; Huber, 1967). A common theme across this scholarship is the use of a kernel function to assign greater weight to nearby samples when computing the local estimates \u03b8\u0302(x), e.g. for kernel weights \u03b1i(x), estimates of local mean of the form \u00b5\u0302(x) = \u2211 \u03b1i(x)Yi. For quantities beyond conditional means, and following the two-step estimation procedure of Newey, 1994, we consider estimators obtained as the result of a weighted empirical version of (1),\n( \u03b8\u0302(x), \u03bd\u0302(x) ) \u2208 argmin\n\u03b8,\u03bd\n{\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03b1i(x)\u03c8\u03b8,\u03bd(Oi) \u2225\u2225\u2225\u2225\u2225 } . (2)\nUsing samples (Xi, Oi) to solve local moment conditions of the form (1) is a sufficiently general framework to characterize many interesting statistical problems. Nevertheless, it differs from using (Xi, Yi) to produce least-squares estimates of a conditional mean function in at least one important way: Without access to observed values of \u03b8\u2217(Xi), conventional random forests can neither produce tree-wise estimates using the regional mean \u03b8\u2217(Xi) values, nor grow trees in the first place via minimization of loss function L(\u03b8\u2217(Xi), \u03b8\u0302i). The generalized random forests of Athey, Tibshirani, and Wager (2019) address these problems separately. First, the problem of infeasible loss minimization is confronted by GRF through the use of a tree-growing procedure whose recursive partitioning scheme is instead designed to maximize training heterogeneity across the possible splits of a given parent node. Second, GRF is able to forgo traditional tree-wise estimation by instead using forests as the first step of the two-step method of Newey (1994), exploiting the neighbourhoods produced by the ensemble of trees in order to cast forests an adaptive kernel method. The GRF algorithm instead produces its estimates via solutions of the form (2) using kernel weights in the spirit the previously mentioned work on local maxmimum likelihood estimation. At a high level, GRF uses forests to determine its data-adaptive weights \u03b1i(x) by averaging the partition structure across a set of independently grown trees. Given a forest of B trees, let Lb(x) denote the set of training samples appearing in the same terminal node as some x \u2208 X . The induced weights \u03b1i(x) are defined such as to measure the relative frequency that training sample Xi appears alongside x throughout the forest,\n\u03b1i(x) := 1\nB B\u2211 b=1 \u03b1bi(x), for \u03b1bi(x) := 1({Xi \u2208 Lb(x)}) |Lb(x)| . (3)\nWe emphasize that this application of random forests differs quite substantially from their conventional interpretation. Although GRF preserves the principles of a recursively partitioned the predictor space, trees grown via observation subsampling, and randomized feature selection, we no longer interpret GRF estimators as an average of trees since GRF confers final estimation to solutions of equations of the form (2)."
        },
        {
            "heading": "2.2 Partitions Maximizing Fitted Heterogeneity",
            "text": "Given a set P containing samples Xi \u2208 P \u2282 X , we seek a binary, axis-aligned partition yielding the greatest improvement in fit for the corresponding estimates of \u03b8\u2217(Xi). We refer to P as the parent node for which we are presently seek the optimal split, and C1, C2 the candidate child nodes obtained as the result of some split of P . Similar to conventional forests, we define regional estimates of (\u03b8\u2217(x), \u03bd\u2217(x)) over the samples in a node N as being constant-valued over all x \u2208 N . However, rather than defining the regional estimates (\u03b8\u0302N , \u03bd\u0302N ) as the (infeasible) mean \u03b8\n\u2217(Xi) over all Xi \u2208 N , we define (\u03b8\u0302N , \u03bd\u0302N ) as the solution to the empirical estimating equation over the samples in N ,\n( \u03b8\u0302N , \u03bd\u0302N ) \u2208 argmin\n\u03b8,\u03bd  \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 {i:Xi\u2208N} \u03c8\u03b8,\u03bd(Oi) \u2225\u2225\u2225\u2225\u2225\u2225  . (4)\nIn principle, we would want to find child nodes C1, C2 of P minimizing the squared error of with respect to the candidate child estimates \u03b8\u0302C1 , \u03b8\u0302C2 ,\nerr(C1, C2) := \u2211 j=1,2 P (X \u2208 Cj |X \u2208 P )E [( \u03b8\u0302Cj \u2212 \u03b8\u2217(X) )2\u2223\u2223\u2223\u2223X \u2208 Cj] . (5)\nHowever, we are unable to minimize err(C1, C2) as we only require \u03b8 \u2217(x) to be identifiable via moment equations of the form (1) and not necessarily through loss-function estimates of a leastsquares criterion. To address this, Athey, Tibshirani, and Wager, 2019 put forward the following criterion measuring the heterogeneity of a candidate split,\n\u2206(C1, C2) := |C1||C2| |P |2\n( \u03b8\u0302C1 \u2212 \u03b8\u0302C2 )2 , (6)\nwhere each \u03b8\u0302Cj is found following (4) over a candidate child node Cj . Proposition 1 of Athey, Tibshirani, and Wager (2019) establishes the asymptotic equivalence of partitions obtained as a result of minimizing the squared-error criterion (5) and those obtained by maximizing the heterogeneity criterion (6)."
        },
        {
            "heading": "2.2.1 The Gradient Tree Algorithm",
            "text": "Maximizing the \u2206-criterion (6) requires computing \u03b8\u0302C1 , \u03b8\u0302C2 via (4) for all possible axis-aligned splits of parent node P . The GRF algorithm of Athey, Tibshirani, and Wager (2019) eschews such\na burden by constructing an approximate criterion \u2206\u0303(C1, C2) implicitly formed as the result of a gradient-based approximation for the \u03b8\u0302Cj . Given a candidate child node C of parent P , let \u03b8\u0303C denote the gradient-based approximation of \u03b8\u0302C found by taking a single gradient step away from the parent estimate \u03b8\u0302P ,\n\u03b8\u0303C := \u03b8\u0302P \u2212 1 |C| \u2211\n{i:Xi\u2208C}\n\u03beTA\u22121P \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi), (7)\nwhere \u03be is a vector selecting the \u03b8-components from a (\u03b8, \u03bd)-vector, and AP is any consistent estimate for \u2207E[\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi)|Xi \u2208 P ]. For example, when \u03c8 is continuously differentiable in (\u03b8, \u03bd),\nAP = 1 |P | \u2211\n{i:Xi\u2208P}\n\u2207\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi). (8)\nThe quantities \u03b8\u0302P and A \u22121 P remain constant over all possible splits of P , allowing them to be computed once when searching for the optimal split of P . Following an influence function heuristic, Athey, Tibshirani, and Wager (2019) use the above quantities to propose a less costly splitting procedure on pseudo-outcomes \u2212\u03beTA\u22121P \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi),\n\u2022 (Labeling Step). Given parent estimates (\u03b8\u0302P , \u03bd\u0302P ), compute pseudo-outcomes over P ,\n\u03c1i := \u2212\u03beTA\u22121P \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi). (9)\n\u2022 (Regression Step). Perform a single conventional regression tree split on the \u03c1i \u2208 P .\nAn immediate consequence of the regression step is to obtain child nodes C1, C2 minimizing the total squared-error loss between the observed pseudo-outcomes and their sample means across the two child partition,\n1 |C1| \u2211\n{i:Xi\u2208C1}\n(\u03c1i \u2212 \u03c1\u0304C1)2 + 1 |C2| \u2211\n{i:Xi\u2208C2}\n(\u03c1i \u2212 \u03c1\u0304C2)2, for \u03c1\u0304Cj = 1 |Cj | \u2211\n{i:Xi\u2208Cj}\n\u03c1i. (10)\nFurthermore, one can show that the partitions obtained by minimizing (10) are equivalent to those obtained by maximizing a criterion of the form\n1\n|C1|  \u2211 {i:Xi\u2208C1} \u03c1i 2 + 1 |C2|  \u2211 {i:Xi\u2208C2} \u03c1i 2 =: \u2206\u0303(C1, C2). (11) That is, the GRF algorithm grows its trees via recursive application of a splitting rule implicitly maximizing the \u2206\u0303-criterion (11) over the pseudo-outcomes (9) within parent P . The use of this approximate splitting scheme is formalized by Proposition 2 of Athey, Tibshirani, and Wager (2019),\ndescribing the asymptotic equivalence of the \u2206 and \u2206\u0303 criteria, and permitting the GRF algorithm to search for splits over the gradient-induced \u2206\u0303-criterion (11) as a surrogate for splits maximizing heterogeneity-measuring \u2206-criterion (6)."
        },
        {
            "heading": "3 Fixed-Point Approximation",
            "text": ""
        },
        {
            "heading": "3.1 The Gradient Tree Algorithm as a Root-Finding Procedure",
            "text": "Write U(\u03b8) := \u2211\n{i\u2208C} \u03c8\u03b8,\u03bd(Oi) so that we may express the gradient-based approximation (7) in\nterms of a single Newton-Raphson root-finding iteration on U(\u03b8) of the form \u03b8(t+1) = \u03b8(t) \u2212 \u03beT [ \u2207U(\u03b8(t)) ]\u22121 U(\u03b8(t)), (12)\nwith initial value \u03b8(t) = \u03b8\u0302P , one-step update \u03b8 (t+1) = \u03b8\u0303C , and where (7) uses a consistent estimator AP for the gradient matrix \u2207U(\u03b8(t)) = \u2207E[\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi)|Xi \u2208 P ]. With this interpretation of \u03b8\u0303C as the result of a single root-finding iteration, we may be inclined to consider approximations for \u03b8\u0302C inspired by other root-finding algorithms."
        },
        {
            "heading": "3.2 Estimating Equations and Fixed Point Problems",
            "text": "Our algorithm is motivated by the analyses of Yang et al. (2021), discussing estimating equations and fixed-point problems in the parametric setting. Let U : Rp \u2192 Rp denote an empirical estimating function for \u03b2\u2217 = (\u03b2\u22171 , ...\u03b2 \u2217 p) T through a finite-sample estimating equation of the form\n0 = U(\u03b2). (13)\nIf a solution to (13) exists, which we denote by \u03b2\u0302, then for any \u03b7 > 0,\nU(\u03b2\u0302) = 0 \u21d0\u21d2 \u03b2\u0302 = \u03b2\u0302 \u2212 \u03b7U(\u03b2\u0302). (14)\nSetting f(\u03b2) := \u03b2\u2212\u03b7U(\u03b2), we find that the search for \u03b2\u0302 solving (13) can be expressed in terms of the equivalent fixed-point problem of searching for \u03b2\u0302 solving \u03b2\u0302 = f(\u03b2\u0302). Although this characterization is presented in the parametric setting, we can nonetheless leverage it towards our search for nonparametric estimates of \u03b8\u2217(x) through an expression of the form\n\u03b8(t+1) = f(\u03b8(t)), with f(\u03b8) := \u03b8 \u2212 \u03b7U(\u03b8), (15)\nfor some step size \u03b7 > 0."
        },
        {
            "heading": "3.3 The Fixed-Point Tree Algorithm",
            "text": "By analogy with the Newton-Raphson interpretation for the gradient-based approximations of \u03b8\u0302C used by the GRF gradient tree algorithm, we express a novel approximation \u03b8\u0303C borne out of a single fixed-point iteration following (15),\n\u03b8\u0303C := \u03b8\u0302P \u2212 \u03b7\u03beT \u2211\n{i:Xi\u2208C}\n\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi), (16)\nwhere \u03b7 > 0 is some step size governing the fixed-point iterations, and, as before, (\u03b8\u0302P , \u03bd\u0302P ) are the solutions of (4) over the parent node and \u03be is a vector selecting the \u03b8-components of a (\u03b8, \u03bd)-vector.\nAs is the case with the gradient tree algorithm, our method, which we refer to as the fixed-point tree algorithm, searches for splits optimizing an approximate criterion \u2206\u0303 in place of the original \u2206- criterion. However, the fixed-point tree algorithm instead induces its criterion via approximations \u03b8\u0303Cj for \u03b8\u0302Cj constructed according to (16). Algorithmically, a generalized random forest invoking fixed-point trees only differs from the those using the gradient trees in the labeling step of the tree-growing procedure:\n\u2022 (Labeling Step). Given parent estimates (\u03b8\u0302P , \u03bd\u0302P ), compute pseudo-outcomes over P\n\u03c1i := \u2212\u03beT\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi). (17)\n\u2022 (Regression Step). Perform a single conventional regression tree split on the \u03c1i \u2208 P , yielding partition {C1, C2} of P which maximize the criterion\n1\n|C1|  \u2211 {i:Xi\u2208C1} \u03c1i 2 + 1 |C2|  \u2211 {i:Xi\u2208C2} \u03c1i 2 =: \u2206\u0303(C1, C2). (18) We note that the step size parameter \u03b7 found in (16) is not present in the pseudo-outcome calculation, as one may expect by using the gradient tree pseudo-outcomes derivation as an analogy. However, if \u03b7 remains fixed over the samples in P then the transformation of \u2212\u03beT\u03c8\u03b8,\u03bd(Oi) to \u2212\u03b7\u03beT\u03c8\u03b8,\u03bd(Oi) amounts to a rescaling of the data and would have no effect on the optimal regressiontree partition over the samples.\nProposition 1. Suppose that the assumptions of Propositions 1 and 2 of Athey, Tibshirani, and Wager (2019) hold, elaborated upon in Section 5. The induced fixed-point \u2206\u0303-criterion (18) and the heterogeneity-measuring \u2206-criterion (6) are asymptotically equivalent according to\n\u2206\u0303(C1, C2) = \u2206(C1, C2) +OP\n( max { r2, 1\n|C1| ,\n1\n|C2|\n}) , (19)\nwhere r > 0 denotes the radius of the parent node P .\nWe note that the rate (19) is precisely the same as that guaranteed for the gradient tree algorithm (Proposition 2 of Athey, Tibshirani, and Wager, 2019). Indeed, since our proposal preserves the remainder of the GRF algorithm, we argue that all other results are left unaffected by the use of fixed-point trees."
        },
        {
            "heading": "3.3.1 Example: Conditional Mean Estimation",
            "text": "The conditional mean function \u03b8\u2217(x) = E[Y |X = x] can be identified through moment condition (1) via estimating function \u03c8\u03b8\u2217(x)(y) = y \u2212 \u03b8\u2217(x). We obtain fixed-point-based pseudo-outcomes \u03c1i = \u2212(Yi \u2212 \u03b8\u0302P ), and parent estimates \u03b8\u0302P given by the solution to (4) over P ,\n0 = \u2211\n{i:Xi\u2208P}\n\u03c8\u03b8\u0302P (Yi) = \u2211\n{i:Xi\u2208P}\n( Yi \u2212 \u03b8\u0302P ) \u21d0\u21d2 \u03b8\u0302P = 1 |P | \u2211\n{i:Xi\u2208P}\nYi =: Y P , (20)\nand hence, \u03c1i = \u2212(Yi\u2212Y P ). As splits are invariant under constant rescalings, we rewrite the fixedpoint pseudo-outcomes as \u03c1i = Yi \u2212 Y P , observing that this form is identical to the corresponding gradient tree pseudo-outcomes.\nWhenever \u03b8\u2217(\u00b7) is a function from the p-dimensional X -space to a one-dimensional parameter space \u03b8\u2217 : X \u2192 R (e.g. conditional mean estimation, quantile regression) the gradient tree pseudooutcomes (9) simplify to the fixed-point tree pseudo-outcomes (17) as the (1\u00d7 1)-dimensional AP matrix implies that A\u22121P \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi) is simply a constant rescaling of the fixed-point pseudo-outcomes across the samples in P ."
        },
        {
            "heading": "3.3.2 Example: Heterogeneous Treatment Effects",
            "text": "Let Wi \u2208 {0, 1}K denote a set of mutually exclusive indicators denoting assignment to one of K distinct treatment levels, Yi(k) the potential outcome associated with the i-th sample had it received treatment level k \u2208 {1, ...,K}, and Xi a set of auxiliary covariates for which we believe will plausibly account for confounding. Let \u03c4\u2217k (x) denote the conditional average treatment effect of receiving treatment level k \u2208 {2, ...,K} over baseline treatment level 1, where, under exogeneity of the causal effects, we have\n\u03c4\u2217k (x) = E [Yi(k)\u2212 Yi(1)|Xi = x] . (21)\nWe set \u03c4\u22171 (x) = 0 and denote \u03c4 \u2217(x) := (\u03c4\u22171 (x), ..., \u03c4 \u2217 K(x)) T , and propose the following outcome model for Yi\nYi = \u00b5 \u2217(Xi) +Wi \u00b7 \u03c4\u2217(x) + \u03f5i, (22)\nwhere we regard the intercept term \u00b5\u2217 as a nuisance quantity. The contrasts \u03c4\u2217(x) can be identified using moment condition (1) via the estimating function (Angrist and Pischke, 2009; Athey, Tibshirani, and Wager, 2019)\n\u03c8\u03c4\u2217(x),\u00b5\u2217(x)(Yi,Wi) :=\n[ Yi \u2212Wi \u00b7 \u03c4\u2217(x)\u2212 \u00b5\u2217(x)\n(Yi \u2212Wi \u00b7 \u03c4\u2217(x)\u2212 \u00b5\u2217(x))Wi\n] , (23)\nwhich, following (17), admits fixed-point pseudo-outcomes of the form \u03c1i = (Wi \u2212WP ) ( Yi \u2212 Y P \u2212 (Wi \u2212WP )\u03c4\u0302P ) , (24)\nwhere WP and Y P are the mean Wi and Yi over the samples in P , and \u03c4\u0302P is the OLS solution to the regression of the Yi\u2212Y P onWi\u2212WP . Compare (24) with the gradient-based pseudo-outcomes derived in Section 6 of Athey, Tibshirani, and Wager (2019)\nA\u22121P (Wi \u2212WP ) ( Yi \u2212 Y P \u2212 (Wi \u2212WP )\u03c4\u0302P ) , for AP = 1 |P | \u2211\n{i:Xi\u2208P}\n(Wi \u2212WP )(Wi \u2212WP )T .\n(25)\nDespite only computing A\u22121P once while searching for the optimal partition of P , the matrix alongside the products A\u22121P \u03c8\u03b8\u0302P ,\u00b5\u0302P (Yi,Wi) must nonetheless be found for every non-terminal partition across each tree of the forest, suggesting a tangible benefit through the use of the fixed-point tree algorithm.\nComputational Considerations. Both the fixed-point pseudo-outcomes (24) and their gradientbased counterparts (25) require the OLS coefficients \u03c4\u0302P = A \u22121 P W\u0303\nT Y\u0303 for W\u0303 \u2208 R|P |\u00d7K the matrix with rows Wi \u2212WP and Y\u0303 \u2208 R|P |\u00d71 the matrix with rows Yi \u2212Y P . In principle, one may compute A\u22121P to obtain the fixed-point pseudo-outcomes in spite of the fact that the matrix is absent in the general formulation (17). However, as the fixed-point expression only makes use of the A\u22121P matrix through \u03c4\u0302P , we are free to obtain \u03c4\u0302P in ways which may not explicitly require or yield A \u22121 P . Furthermore, the estimates \u03b8\u0303C obtained by the tree-growing procedure are only used to determine the weight-inducing partition structure, and are not (directly) used as part of the final GRF estimator, suggesting that approximations for \u03c4\u0302P may be sufficient. In particular, we propose a further acceleration for heterogeneous treatment effect estimation by settling for an approximate solution to the OLS coefficients \u03c4\u0302P : Rather than computing the exact \u03c4\u0302P , use an approximation given by a single gradient descent step away from the origin, using the exact line search step size."
        },
        {
            "heading": "4 Simulations",
            "text": ""
        },
        {
            "heading": "4.1 Heterogeneous Treatment Effect Estimation",
            "text": "We present empirical tests evaluating the performance of the fixed-point tree\u2019s ability to estimate heterogeneous treatment effects. Following the observations made in Section 3.3.2, we compare the fixed-point tree algorithm to the existing gradient tree implementation using 1) a Cholesky-based routine to calculate the regression coefficients \u03c4\u0302P , and 2) a routine using a one-step approximation for \u03c4\u0302P . The methods were implemented in a modified version of the R package grf (Tibshirani et al., 2023) in order for ensure that timing results be maximally comparable. In the figures and tables below, we use grad to abbreviate the GRF estimates obtained using the gradient tree algorithm, fp1 the GRF estimates obtained using the fixed-point tree algorithm using exact OLS coefficients \u03c4\u0302P via a Cholesky solver, and fp2 the GRF estimates obtained by the fixed-point tree algorithm using a single-step approximation for \u03c4\u0302P ."
        },
        {
            "heading": "4.1.1 Multi-Level Treatments",
            "text": "Let Wi \u2208 {0, 1}K denote a set of K indicators corresponding to assignment to one of K different treatment levels k \u2208 {1, ...,K}, where we treat level k = 1 as the baseline treatment. We draw Xi \u223c U([0, 1]p), Wi \u223c Multinom(1, (\u03c01, ..., \u03c0K)) with uniform probabilities \u03c0k = 1/K, noise \u03f5i \u223c N (0, 1), and generate outcomes according to\nYi =Wi \u00b7 \u03c4\u2217(Xi) + \u03f5i. (26)\nIn all data-generating regimes we seek to provide estimates of \u03c4\u2217(x) := (\u03c4\u22171 (x), ..., \u03c4 \u2217 K(x)) and fit the forests via grf::multi arm causal forest (or modifications thereof to implement the fixedpoint methods). The true treatment effects were set as \u03c4\u22171 (x) := 0 for the baseline treatment, and, for treatments k \u2208 {2, ...,K} we set \u03c4\u2217k (x) := \u03b2kx1 for fixed constants \u03b2k \u2208 R so that the local\ntreatment effects are linearly heterogeneous in the first auxiliary covariate. The constants \u03b2k were randomly generated across each replication as \u03b2k \u223c N (0, 1). Letting wi \u2208 {1, ...,K} denote the treatment level received by the i-th observation (corresponding to the non-zero entry of Wi) we can express our outcome model as\nYi = K\u2211 k=2 \u03b2kXi11({wi = k}) + \u03f5i, (27)\nso that, for counterfactual outcomes Yi(k), the true conditional average treatment effects are identified by the causal contrasts (relative to the baseline)\nE [Yi(k)\u2212 Yi(1)|Xi = x] = \u03b2kx1 = \u03c4\u2217k (x). (28)\nSingle Tree Timings. The fixed-point proposal only differs from the existing gradient-based method in how individual trees are grown, and so we begin by presenting single tree timings to highlight the benefit of the fixed-point method. We reserve any evaluations of prediction accuracy for the full forest predictors as the individual tree learners are assumed to be quite noisy. For the same reason we made no effort to tune the model parameters beyond the default settings of the grf::multi arm causal forest function, with exception to the subsampling and honest sample splitting parameters which were effectively disabled (sample.fraction = 1 and honesty = F), as well as the centering arguments (Y.hat and W.hat) which would otherwise cause unrelated overhead.\nResults over 500 replications of the simulated study are summarized in Figure 1. For both the exact fp1 and one-step approximate fp2 implementations of the fixed-point tree algorithm we find an appreciable decrease in fit times relative to the gradient-tree algorithm. A natural question to ask is whether this benefit is a consequence of a corresponding decrease in fitted tree complexity. We choose to measure tree complexity by the total number of splits, and display the relative split counts of the different methods in the lower panel of Figure 1.\nForest Predictor Performance. To illustrate the accuracy of the two fixed-point methods we once again draw samples according to outcome model (27) under the original unconfounded regime of uniform probabilities \u03c0k = 1/K, as well as under confounding between the treatment and the auxiliary covariates. In the confounded setting we use class probabilities dependent on the covariate features \u03c0k \u2261 \u03c0k(x) according to\n\u03c0k(x) = { x1, k = 1, 1\u2212x1 K\u22121 , k \u2208 {2, ...,K}.\n(29)\nAll forests use B = 2000 trees with all other settings left as the grf::multi arm causal forest default, with exception to the centering arguments Y.hat and W.hat which were computed outside of to the causal forest function according to the usual implementation (via grf::regression forest and grf::probability forest) so that identical values could be passed to each of the three methods. An example of the fits provided by this heterogeneous treatment effect model is given in Figure 2.\nSimulation results are described in Figure 3 in terms of a normalized \u21132 loss for the treatment effect over a test set, i.e. estimates of E[\u2225(\u03c4\u2217(X)\u2212 \u03c4\u0302(X))/K\u22252] for X \u223c U([0, 1]p) from a test set of 1000 samples, and where normalization by K\u22121 was done for the sake of comparison across treatment level settings. Due to computational constraints we limit the simulations to 50 replications.\nTraining Samples (n)\n2500 5000 10000\nAuxiliary Covariate Features (p) 2\n2 4 8 16 32 64 2 4 8 16 32 64 2 4 8 16 32 64 0.0\n0.4\n0.8\n1.2\nTreatment Levels (K)\nRe lat\nive F\nit T im\ne\nComparison fp1/grad\nfp2/grad\nOne tree, 500 replications Multi-Level Treatments: Fixed-Point Tree Fit Timings vs. Gradient-Based Trees\nTraining Samples (n)\n2500 5000 10000\nAuxiliary Covariate Features (p) 2\n2 4 8 16 32 64 2 4 8 16 32 64 2 4 8 16 32 64\n0.75\n1.00\n1.25\n1.50\nTreatment Levels (K)\nRe lat\nive S\npli t C\nou nt\nComparison fp1/grad\nfp2/grad\nOne tree, 500 replications Multi-Level Treatments: Fixed-Point Tree Split Counts vs. Gradient-Based Trees\nFigure 1: Single-tree computational performance of the exact fixed-point tree algorithm (fp1) and the approximate fixed-point tree algorithm (fp1) relative to the existing gradient tree algorithm (grad) for heterogeneous treatment effect estimation given multi-level treatments. Results include the relative time taken to fit a single tree (top panel) as well the relative tree complexity as measured by the total number of splits (bottom panel).\nDespite the improved fit times of both fixed-point tree implementations, neither method suffers from any discernible loss in prediction accuracy relative to the original gradient-based method."
        },
        {
            "heading": "4.1.2 Continuous Treatments",
            "text": "We generate samples following the same structural model (26), where Wi \u2208 RK now denotes a set of K continuous treatment regressors. We draw Xi \u223c N (0,1p), Wi \u223c N (0,1K), and noise \u03f5i \u223c N (0, 1). We set the true local effects \u03c4\u2217(x) := (\u03c4\u22171 (x), ...\u03c4\u2217K(x)) of the K regressors as \u03c4\u2217k (x) := \u03b2kx1, so, as in Section 4.1.1, the effects are linearly heterogeneous in the first auxiliary covariate. All forests were fit via grf::lm forest (or modifications thereof to implement the fixed-point method).\nSingle Tree Timings. Results over 500 replications of the simulated study are summarized in Figure 4. We find the performance benefit gained through the use of the fixed-point algorithms to be even more dramatic than the previous case of Wi corresponding to discrete treatment assignment. For continuous Wi we observe considerably larger trees (for all methods) than trees obtained for binaryWi, and so we find a corresponding increase in the applications of the gradient tree/fixed-point tree splitting rules per tree. As before, the bottom panel in Figure 4 verifies that the performance increase of fixed-point trees is not a consequence of fitting less complex trees, which, if anything, tend fit slightly more complex trees than the gradient tree algorithm.\nForest Predictor Performance. Using the same the data-generating design as above, we repeat the forest test procedure used to assess predictor accuracy for multi-leveled treatments carried out in Section 4.1.1. Forests were fit using grf::lm forest (or modifications thereof) with all settings left as the default, with exception to Y.hat and W.hat which, as before, were computed outside of the main forest function in accordance with their default implementations (via grf::regression forest for both Y.hat and W.hat). Simulation results are displayed in Figure 5 and once show that the dramatic decrease in fit times found in Figure 4 do not come at the cost of a corresponding decrease in prediction accuracy, relative to the gradient-tree algorithm."
        },
        {
            "heading": "5 Theoretical Analysis",
            "text": "The primary goal of this work is to offer a computationally frugal alternative to the splitting rule and pseudo-outcomes induced by the gradient-based approximations of \u03b8\u0302C . In doing so we mirror the notation & structure of the relevant analyses made by Athey, Tibshirani, and Wager (2019) and Athey and Imbens (2016) to show that the approximations \u03b8\u0303C (16) which induce the fixed-point splitting rule are asymptotically coupled to the original estimates \u03b8\u0302C which define the \u2206(C1, C2) and err(C1, C2) splitting rules. Moreover, the consistency of the final GRF estimates \u03b8\u0302(x) defined by (2) for the true \u03b8\u2217(x) is given by Theorem 3 of Athey, Tibshirani, and Wager\n(2019). Any mechanism producing the kernel weights \u03b1i(x) used to specify \u03b8\u0302(x) will not affect consistency so long as they satisfy the assumptions imposed by Theorem 3 on the forest of weightinducing trees. For this reason, our focus is directed towards results coupling \u03b8\u0303C with \u03b8\u0302C in order to establish the equivalence of trees grown by optimizing the \u2206\u0303 criterion and those grown via the \u2206 criterion. In particular, we prove fixed-point tree equivalents to Lemma 4 and Proposition 2 of Athey, Tibshirani, and Wager (2019) which are used to establish the large sample properties of the gradient tree algorithm.\nOne tree, 500 replications Continuous Treatments: Fixed-Point Tree Fit Timings vs. Gradient-Based Trees\nTraining Samples (n)\n5000 10000\nAuxiliary Covariate Features (p) 5 10\n2 4 8 2 4 8\n0.04\n0.06\n0.08\n0.10\n0.04\n0.06\n0.08\n0.10\nTreatment Regressors (K)\nNo rm\nali ze\nd Tr\nea tm\nen t E\nffe ct\nL 2\nLo ss\nMethod\ngrad fp1 fp2\n2000 trees, 50 replications, 1000 test samples Continuous Treatments: Test Errors\nFigure 5: Treatment effect prediction accuracy for the gradient-based method (grad), the exact fixed-point method (fp1), and the approximate fixed-point method (fp2). Training samples were drawn following the heterogeneous treatment model for continuous treatments described in Section 4.1.2. Test errors were computed over a test set of 1000 samples and averaged over the replications."
        },
        {
            "heading": "5.1 Notation & Assumptions",
            "text": "Assumption 1. The predictor and parameter spaces are both subsets of Euclidean space, X = [0, 1]p and (\u03b8, \u03bd) \u2208 B \u2282 Rk for p, k > 0, and B a compact subset of Rk. In accordance with the arguments of Wager and Walther (2015), assume that the covariate features X have density f bounded away from 0 and \u221e, i.e. c \u2264 fX(x) \u2264 C for some c > 0, C <\u221e, for all x \u2208 X .\nAssumption 2. The expected estimating function\nM\u03b8,\u03bd(x) := EO|X [\u03c8\u03b8,\u03bd(Oi)|Xi = x] , (30)\nis smooth in (\u03b8, \u03bd).\nAssumption 3. For fixed (\u03b8, \u03bd), the M -function (30) is Lipschitz continuous in x.\nAssumption 4. For fixed x, theM -function is twice-differentiable in (\u03b8, \u03bd) with uniformly bounded second derivative, \u22072(\u03b8,\u03bd)M\u03b8,\u03bd(x) <\u221e, (31) where \u00b7 denotes the appropriate tensor norm for the second derivative ofM\u03b8,\u03bd taken with respect to (\u03b8, \u03bd). Moreover, assume V (x) := \u2207(\u03b8,\u03bd)M\u03b8,\u03bd(x) \u2223\u2223 \u03b8=\u03b8\u2217(x),\u03bd=\u03bd\u2217(x) is invertible for all x \u2208 X .\nAssumption 5. The score functions \u03c8\u03b8,\u03bd(Oi) have a continuous covariance structure in the following sense: For \u03b3(\u00b7, \u00b7) denoting the worst-case variogram,\n\u03b3 ([ \u03b81 \u03bd1 ] , [ \u03b82 \u03bd2 ]) := sup\nx\u2208X {\u2225\u2225VarO|X (\u03c8\u03b81,\u03bd1(Oi)\u2212 \u03c8\u03b82,\u03bd2(Oi) |Xi = x)\u2225\u2225F} , (32) then, for some L > 0,\n\u03b3 ([ \u03b81 \u03bd1 ] , [ \u03b82 \u03bd2 ]) \u2264 L \u2225\u2225\u2225\u2225[\u03b81\u03bd1 ] \u2212 [ \u03b82 \u03bd2 ]\u2225\u2225\u2225\u2225 2 , for all (\u03b81, \u03bd1), (\u03b82, \u03bd2). (33)\nAssumption 6. The estimating \u03c8-function can be written as\n\u03c8\u03b8,\u03bd(Oi) = \u03bb(\u03b8, \u03bd;Oi) + \u03b6\u03b8,\u03bd(g(Oi)), (34)\nfor \u03bb a Lipschitz-continuous function in (\u03b8, \u03bd), g : {Oi} \u2192 R a univariate summary of the observables Oi, and \u03b6\u03b8 : R \u2192 R any family of monotone and bounded functions.\nAssumption 7. For any weights \u03b1i, \u2211 \u03b1i = 1, the minimizer (\u03b8\u0302, \u03bd\u0302) of the weighted empirical\nestimation problem (2) is at least an approximate root of the objective function \u2211n\ni=1 \u03b1i\u03c8\u03b8,\u03bd(Oi),\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03b1i\u03c8\u03b8\u0302,\u03bd\u0302(Oi) \u2225\u2225\u2225\u2225\u2225 2 \u2264 C max 1\u2264i\u2264n {\u03b1i}, for C \u2265 0. (35)\nWe present the final assumption and model specification for completeness, which are only required to guarantee the consistency of the final GRF estimates \u03b8\u0302(x) with the true \u03b8\u2217(x), and not the\ncoupling of fixed-point/gradient approximations \u03b8\u0303C with \u03b8\u0302C .\nAssumption 8: The estimating function \u03c8\u03b8,\u03bd(Oi) is a negative sub-gradient of a convex function, and the expected estimating function M\u03b8,\u03bd(Xi) is the negative gradient of a strongly convex function.\nSpecification 1: Individual tree predictors are symmetric, balanced, and randomized. The forest predictor is built with subsampled trees as well as honest sample splitting. We provide a summary of these notions below, but for a more detailed description see Athey and Imbens, 2016; Athey, Tibshirani, and Wager, 2019; Wager and Athey, 2018.\n\u2022 (Symmetric). Tree predictions do not depend upon the order in which training samples are indexed.\n\u2022 (Balanced/\u03c9-Regular). Each split puts at least a fraction \u03c9 > 0 of parent observations into each child node.\n\u2022 (Randomized/Random-split). The probability that a split is made on feature j is bound below by \u03c0 > 0.\n\u2022 (Subsampled). Trees are trained using a subsample of size s drawn from the n training samples (without replacement), with s/n\u2192 0 and s\u2192 \u221e as n\u2192 \u221e.\n\u2022 (Honest sample splitting). Tree growth and tree estimation carried out using mutually exclusive subsets of the subsampled data. For each tree,\n(i) randomly partition the subsampled training data into disjoint sets J1 and J2 of size |J1| = \u230as/2\u230b and |J2| = \u2308s/2\u2309,\n(ii) determine the tree\u2019s splits using the data from J1, (iii) set the fitted leaf-wise responses using the data from J2. In the context of computing GRF weights \u03b1i(x), we interpret the final step as using the samples from J2 to determine the neighbourhood sets Lb(x), i.e. the set of training samples Xi \u2208 J2 appearing in the same leaf as test point x for tree b."
        },
        {
            "heading": "5.2 Approximating Forests of Fixed-Point Trees with Regression Forests",
            "text": "Let \u03c1\u2217i (x) denote an infeasible version of the fixed-point pseudo-outcomes (17), now defined using the true values (\u03b8\u2217(x), \u03bd\u2217(x)) rather than the parent estimates (\u03b8\u0302P , \u03bd\u0302P ),\n\u03c1\u2217i (x) := \u2212\u03beT\u03c8\u03b8\u2217(x),\u03bd\u2217(x)(Oi). (36)\nFor any forest weights \u03b1i(x) used to specify the final GRF predictor \u03b8\u0302(x) via (2), define\n\u03b8\u0303\u2217(x) := \u03b8\u2217(x) + n\u2211 i=1 \u03b1i(x)\u03c1 \u2217 i (x). (37)\nBy writing Y\u0303i = \u03b8 \u2217(x) + \u03c1\u2217i (x), Athey, Tibshirani, and Wager (2019) argue that \u03b8\u0303 \u2217(x) is precisely a regression forest predictor with weights \u03b1i(x) and response Y\u0303i. Moreover, given forest weights \u03b1i(x), the GRF estimator \u00b5\u0302(x) for the conditional mean function \u00b5 \u2217(x) is found as the solution to\n\u00b5\u0302(x) \u2208 argmin \u00b5 {\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03b1i(x)\u03c8\u00b5(Y\u0303i) \u2225\u2225\u2225\u2225\u2225 } \u21d0\u21d2 \u00b5\u0302(x) = n\u2211\ni=1\n\u03b1i(x)Y\u0303i. (38)\nwhere it is easy to show that \u03c8\u00b5(y) := y\u2212\u00b5 is the estimating function identifying the true conditional mean at \u00b5 = \u00b5\u2217(x) via moment equations (1). In general, GRF estimators are not easily expressed as average of trees, as is the case for conventional forests. However, in the case of a GRF regression forest of the form (38) we find\n\u00b5\u0302(x) = n\u2211 i=1 \u03b1i(x)Y\u0303i,\n= n\u2211 i=1\n( 1\nB B\u2211 b=1 1({Xi \u2208 Lb(x)}) |Lb(x)|\n) Y\u0303i,\n= 1\nB B\u2211 b=1 n\u2211 i=1 1({Xi \u2208 Lb(x)}) |Lb(x)| Y\u0303i,\n= 1\nB B\u2211 b=1 n\u2211 i=1 \u03b1bi(x)Y\u0303i,\n= 1\nB B\u2211 b=1 \u00b5\u0302b(x), for \u00b5\u0302b(x) := n\u2211 i=1 \u03b1bi(x)Y\u0303i.\nTherefore, any \u03b8\u0303\u2217(x) defined as (37) can be expressed as the result of a regression forest in terms of an average of B pseudo-tree predictors \u03b8\u0303\u2217b (x),\n\u03b8\u0303\u2217(x) = 1\nB B\u2211 b=1 \u03b8\u0303\u2217b (x), for \u03b8\u0303 \u2217 b (x) := n\u2211 i=1 \u03b1bi(x) (\u03b8 \u2217(x) + \u03c1\u2217i (x)) . (39)\nLemma 1. Under Assumptions 1-7 and given a forest trained satisfying Specification 1, suppose that the final GRF estimator \u03b8\u0302(x) is consistent for \u03b8\u2217(x), guaranteed under the conditions of Theorem 3 of Athey, Tibshirani, and Wager (2019). Then, \u03b8\u0302(x) and the fixed-point-inspired \u03b8\u0303\u2217(x) are asymptotically equivalent as\u221a\nn\ns\n( \u03b8\u0303\u2217(x)\u2212 \u03b8\u0302(x) ) = OP ( max {( s n ) 1 6 , s \u2212\u03c02 log((1\u2212\u03c9)\u22121) log(\u03c9\u22121) }) (40)"
        },
        {
            "heading": "6 Proofs",
            "text": ""
        },
        {
            "heading": "6.1 Proof of Lemma 1",
            "text": "Proof. Given any forest weights \u03b1i(x), define \u03a8(\u03b8, \u03bd) := \u2211n i=1 \u03b1i(x)\u03c8\u03b8,\u03bd(Oi). By definition of the fixed-point-based \u03b8\u0303\u2217(x),\u2225\u2225\u2225\u2225[\u03b8\u0302(x)\u2212 \u03b8\u0303\u2217(x)\u03bd\u0302(x)\u2212 \u03bd\u0303\u2217(x) ]\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 [ \u03b8\u0302(x)\u2212 \u03b8\u2217(x) \u03bd\u0302(x)\u2212 \u03bd\u2217(x) ] + n\u2211 i=1 \u03b1i(x)\u03c8\u03b8\u2217(x),\u03bd\u2217(x)(Oi)\n\u2225\u2225\u2225\u2225\u2225 , = \u2225\u2225\u2225\u2225[\u03b8\u0302(x)\u2212 \u03b8\u2217(x)\u03bd\u0302(x)\u2212 \u03bd\u2217(x) ] +\u03a8(\u03b8\u2217(x), \u03bd\u2217(x)) \u2225\u2225\u2225\u2225 .\nThis expression is analogous to that of the gradient-based approximation found in Lemma 4 of Athey, Tibshirani, and Wager (2019), differing only in that the fixed-point expression no longer includes the V (x)\u22121 matrix in the second term: V (x)\u22121\u03a8(\u03b8\u2217(x), \u03bd\u2217(x)). However, as the V (x)\u22121\nis well-behaved and non-random it does not affect the asymptotic behaviour coupling the \u03b8\u0302(x) and \u03b8\u0303\u2217(x) quantities. Therefore, by the same arguments in Lemma 4 of Athey, Tibshirani, and Wager (2019) we obtain the desired coupling between the fixed-point-based \u03b8\u0303\u2217(x) and the final\nGRF predictor \u03b8\u0302(x)."
        },
        {
            "heading": "6.2 Proof of Proposition 1",
            "text": "Proof. We wish to couple the estimates \u03b8\u0302Cj fit over samples from Cj via (4) with a fixed-pointbased approximation \u03b8\u0303Cj expressed as the result of a single fixed-point iteration taken away from the parent estimate via (16). To do so we will establish the equivalence of both quantities to an analogue of the infeasible fixed-point approximation (37), taken over the samples in child Cj , evaluated at x = xP the center of mass of parent P , and with uniform forest weights \u03b1i(xP ) = 1/|Cj |. That is, we consider\n\u03b8\u0303\u2217Cj (xP ) = \u03b8(xP ) + 1 |Cj | \u2211\n{i:Xi\u2208Cj}\n\u03c1\u2217i (xP ),\nand let r := sup{i:Xi\u2208Cj} \u2225Xi \u2212 xP \u2225 denote the radius of leaf Cj . Bounding EO,X [ \u03b8\u0303\u2217Cj(xP )\u2212 \u03b8 \u2217(xP ) ] = O(r). We have\nEO,X [ \u03b8\u0303\u2217Cj (xP )\u2212 \u03b8 \u2217(xP ) ] = EO,X \u03b8\u2217(xP ) + 1|Cj | \u2211{i:Xi\u2208Cj} \u03c1\u2217i (xP ) \u2212 \u03b8\u2217(xP )  , = 1\n|Cj | \u2211\n{i:Xi\u2208Cj}\nEO,X [\u03c1\u2217i (xP )] ,\n= 1 |Cj | \u2211\n{i:Xi\u2208Cj}\nEO,X [ \u2212\u03beT\u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi) ] ,\n= \u2212\u03beTEO,X [ \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi) ] ,\n= \u2212\u03beTEX [ EO|X [ \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi) \u2223\u2223Xi = Xi]] , = \u2212\u03beTEX [ M\u03b8\u2217(xP ),\u03bd\u2217(xP )(Xi) ] .\nBy the Lipschitz continuity of the M -function in x, for any r > 0 we have\n\u2225M\u03b8,\u03bd(x+ r)\u2212M\u03b8,\u03bd(x)\u2225 \u2264 Lr.\nMoreover, by definition (1), the M -function will vanish when it is evaluated at point x and the true (\u03b8\u2217(x), \u03bd\u2217(x)), also evaluated at the same point x,\nM\u03b8\u2217(x),\u03bd\u2217(x)(x) = E [ \u03c8\u03b8\u2217(x),\u03bd\u2217(x)(Oi) \u2223\u2223Xi = x] = 0.\nHence,\nLr \u2265 \u2225\u2225M\u03b8\u2217(xP ),\u03bd\u2217(xP )(x)\u2212M\u03b8\u2217(xP ),\u03bd\u2217(xP )(xP )\u2225\u2225 ,\n= \u2225\u2225M\u03b8\u2217(xP ),\u03bd\u2217(xP )(x)\u2212 0\u2225\u2225 ,\n= \u2225\u2225M\u03b8\u2217(xP ),\u03bd\u2217(xP )(x)\u2225\u2225 .\nThe radius r depends on the variability of the samples within parent P , and so, taking the expectation of the previous norm, EX [ M\u03b8\u2217(xP ),\u03bd\u2217(xP )(Xi) ] = O(r).\nHence, continuing from the initial expansion, EO,X [ \u03b8\u0303\u2217Cj (xP )\u2212 \u03b8 \u2217(xP ) ] = \u2212\u03beTEX [ M\u03b8\u2217(xP ),\u03bd\u2217(xP )(Xi) ] ,\n= O(r).\nBounding Var ( \u03b8\u0303\u2217Cj(xP ) ) = O(1/|Cj|). We have\nVar ( \u03b8\u0303\u2217Cj (xP ) ) = Var \u03b8(xP ) + 1|Cj | \u2211{i:Xi\u2208Cj} \u03c1\u2217i (xP )  ,\n= Var  1 |Cj | \u2211 {i:Xi\u2208Cj} \u03c1\u2217i (xP )  , = Var \u2212 1 |Cj | \u2211 {i:Xi\u2208Cj} \u03beT\u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi)\n , = \u03beTVar\n \u2211 {i:Xi\u2208Cj} 1 |Cj | \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi)  \u03be Once again, this argument only differs from the gradient-based counterpart in the proof of Proposition 2 of Athey, Tibshirani, and Wager (2019) through its lack of the V (xP )\n\u22121 matrix. In the case of the gradient-based approximation, the matrix is non-random and comes out of the variance as \u03beTV (xP )\n\u22121Var(\u00b7 \u00b7 \u00b7 )(\u03beTV (xP )\u22121)T , leaving us with the same quantity inside the variance as well as the same bound on both fixed-point and gradient approximations,\nVar ( \u03b8\u0303\u2217Cj (xP ) ) = O ( 1\n|Cj |\n) .\nCoupling \u03b8\u0303\u2217Cj(xP ) and \u03b8\u0302Cj . We have\n\u03b8\u0303\u2217Cj (xP )\u2212 \u03b8\u0302Cj = \u03b8\u2217(xP ) + 1|Cj | \u2211{i:Xi\u2208Cj} \u03c1\u2217i (xP ) \u2212 \u03b8\u0302Cj\n= ( \u03b8\u2217(xP )\u2212 \u03b8\u0302Cj ) + 1 |Cj | \u2211\n{i:Xi\u2208Cj}\n\u03c1\u2217i (xP ),\n= OP\n( max { r,\n1\u221a |Cj |\n}) ,\nwhere the \u03b8\u2217(xP )\u2212\u03b8\u0302Cj = OP (r) follows from the consistency of \u03b8\u0302(x) for \u03b8\u2217(x) and |Cj |\u22121 \u2211 {i:Xi\u2208Cj} \u03c1 \u2217 i (xP ) =\nOP (1/ \u221a |Cj |) follows from the second moment bounds established above.\nCoupling \u03b8\u0303\u2217Cj(xP ) and \u03b8\u0303Cj . By definition of \u03b8\u0303Cj and \u03b8\u0303 \u2217 Cj (xP ),\n\u03b8\u0303Cj \u2212 \u03b8\u0303\u2217Cj (xP ) = \u03b8\u0302P \u2212 \u03b7 \u2211 {i:Xi\u2208Cj} \u03beT\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi) \u2212 \u03b8\u2217(xP )\u2212 1|Cj | \u2211{i:Xi\u2208Cj} \u03beT\u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi)  , = \u03b8\u0302P \u2212 \u03b8\u2217(xP )\u2212 \u03b7\u03beT\n\u2211 {i:Xi\u2208Cj} \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi) + 1 |Cj | \u03beT \u2211 {i:Xi\u2208Cj} \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi),\n= \u03b8\u0302P \u2212 \u03b8\u2217(xP ) \u2212 \u03b7\u03beT \u2211\n{i:Xi\u2208Cj}\n\u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi)\n+ 1\n|Cj | \u03beT \u2211 {i:Xi\u2208Cj} ( \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi) + \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi)\u2212 \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi) ) ,\n= \u03b8\u0302P \u2212 \u03b8\u2217(xP )\n\u2212 1 |Cj |\n\u03beT \u2211\n{i:Xi\u2208Cj}\n( \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi)\u2212 \u03c8\u03b8\u2217(xP ),\u03bd\u2217(xP )(Oi) ) (\u2020)\n\u2212 \u03beT ( \u03b7 \u2212 1\n|Cj | ) \u2211 {i:Xi\u2208Cj} \u03c8\u03b8\u0302P ,\u03bd\u0302P (Oi). (\u2021)\nFollowing the arguments used in the proof of Proposition 2 of Athey, Tibshirani, and Wager (2019) we find that (\u2020) is bound by OP (r), once again recognizing that the above expression differs only from its gradient-based counterpart up to the non-random V (xP )\n\u22121 matrix. Meanwhile, since \u2212\u03beT (\u03b7 \u2212 1/|Cj |) is non-random we once again mirror the bound offered by Athey, Tibshirani, and Wager (2019) to bound (\u2021) by OP ( max { r, 1/ \u221a |Cj | }) . Therefore,\n\u03b8\u0303Cj \u2212 \u03b8\u0303\u2217Cj (xP ) = \u03b8\u0302P \u2212 \u03b8 \u2217(xP ) +OP\n( max { r,\n1\u221a |Cj |\n}) ,\nand as \u03b8\u0302(xP ) is itself OP (r)-consistent for \u03b8 \u2217(xP ), it follows that\n\u03b8\u0303Cj \u2212 \u03b8\u0303\u2217Cj (xP ) = OP\n( max { r,\n1\u221a |Cj |\n}) .\nThis is precisely the form of the corresponding term found in Athey, Tibshirani, and Wager (2019)\u2019s proof of Proposition 2 for the gradient-based approximation. The remainder of the argument remains unchanged and so follows the desired result."
        }
    ],
    "title": "Accelerating Generalized Random Forests with Fixed-Point Trees",
    "year": 2023
}