{
    "abstractText": "In this research, we present an end-to-end data-driven pipeline for determining the long-term stability status of objects within a given environment, specifically distinguishing between static and dynamic objects. Understanding object stability is key for mobile robots since longterm stable objects can be exploited as landmarks for long-term localisation. Our pipeline includes a labelling method that utilizes historical data from the environment to generate training data for a neural network. Rather than utilizing discrete labels, we propose the use of pointwise continuous label values, indicating the spatio-temporal stability of individual points, to train a point cloud regression network named LTSNET. Our approach is evaluated on point cloud data from two parking lots in the NCLT dataset, and the results show that our proposed solution, outperforms direct training of a classification model for static vs dynamic object classification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ibrahim Hroob"
        },
        {
            "affiliations": [],
            "name": "Sergi Molina"
        },
        {
            "affiliations": [],
            "name": "Riccardo Polvara"
        },
        {
            "affiliations": [],
            "name": "Grzegorz Cielniak"
        },
        {
            "affiliations": [],
            "name": "Marc Hanheide"
        }
    ],
    "id": "SP:4b0f3fa62ece28b8a19aa42f0d6480ea9a777887",
    "references": [
        {
            "authors": [
                "F. Pomerleau",
                "P. Kr\u00fcsi",
                "F. Colas",
                "P. Furgale",
                "R. Siegwart"
            ],
            "title": "Long-term 3d map maintenance in dynamic environments",
            "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA), IEEE",
            "year": 2014
        },
        {
            "authors": [
                "Z. Hong",
                "Y. Petillot",
                "A. Wallace",
                "S. Wang"
            ],
            "title": "Radarslam: A robust simultaneous localization and mapping system for all weather conditions",
            "venue": "The International Journal of Robotics Research",
            "year": 2022
        },
        {
            "authors": [
                "M. Arora",
                "L. Wiesmann",
                "X. Chen",
                "C. Stachniss"
            ],
            "title": "Mapping the static parts of dynamic scenes from 3d lidar point clouds exploiting ground segmentation",
            "venue": "2021 European Conference on Mobile Robots (ECMR), IEEE",
            "year": 2021
        },
        {
            "authors": [
                "H. Lim",
                "S. Hwang",
                "H. Myung"
            ],
            "title": "Erasor: Egocentric ratio of pseudo occupancybased dynamic object removal for static 3d point cloud map building",
            "venue": "IEEE Robotics and Automation Letters 6(2)",
            "year": 2021
        },
        {
            "authors": [
                "J. Schauer",
                "A. N\u00fcchter"
            ],
            "title": "The peopleremover\u2014removing dynamic objects from 3-d point cloud data by traversing a voxel occupancy grid",
            "venue": "IEEE robotics and automation letters 3(3)",
            "year": 2018
        },
        {
            "authors": [
                "A. Dewan",
                "G.L. Oliveira",
                "W. Burgard"
            ],
            "title": "Deep semantic classification for 3d lidar data",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhou",
                "O. Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "year": 2018
        },
        {
            "authors": [
                "T. Cortinhal",
                "G. Tzelepis",
                "E. Erdal Aksoy"
            ],
            "title": "Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds",
            "venue": "International Symposium on Visual Computing, Springer",
            "year": 2020
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "year": 2017
        },
        {
            "authors": [
                "G. Kim",
                "A. Kim"
            ],
            "title": "Remove, then revert: Static point cloud map construction using multiresolution range images",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE",
            "year": 2020
        },
        {
            "authors": [
                "A. Hornung",
                "K.M. Wurm",
                "M. Bennewitz",
                "C. Stachniss",
                "W. Burgard"
            ],
            "title": "Octomap: An efficient probabilistic 3d mapping framework based on octrees",
            "venue": "Autonomous robots 34(3)",
            "year": 2013
        },
        {
            "authors": [
                "S. Li",
                "X. Chen",
                "Y. Liu",
                "D. Dai",
                "C. Stachniss",
                "J. Gall"
            ],
            "title": "Multi-scale interaction for real-time lidar data segmentation on an embedded platform",
            "venue": "IEEE Robotics and Automation Letters 7(2)",
            "year": 2021
        },
        {
            "authors": [
                "K. Wong",
                "S. Wang",
                "M. Ren",
                "M. Liang",
                "R. Urtasun"
            ],
            "title": "Identifying unknown instances for autonomous driving",
            "venue": "Conference on Robot Learning, PMLR",
            "year": 2020
        },
        {
            "authors": [
                "H. Blum",
                "F. Milano",
                "R. Zurbr\u00fcgg",
                "R. Siegwart",
                "C. Cadena",
                "A. Gawel"
            ],
            "title": "Selfimproving semantic perception for indoor localisation",
            "venue": "Conference on Robot Learning, PMLR",
            "year": 2022
        },
        {
            "authors": [
                "G. Wang",
                "X. Tian",
                "R. Ding",
                "H. Wang"
            ],
            "title": "Unsupervised learning of 3d scene flow from monocular camera",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), IEEE",
            "year": 2021
        },
        {
            "authors": [
                "A. Dewan",
                "T. Caselitz",
                "G.D. Tipaldi",
                "W. Burgard"
            ],
            "title": "Rigid scene flow for 3d lidar scans",
            "venue": "2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE",
            "year": 2016
        },
        {
            "authors": [
                "A. Schaefer",
                "D. B\u00fcscher",
                "J. Vertens",
                "L. Luft",
                "W. Burgard"
            ],
            "title": "Long-term urban vehicle localization using pole landmarks extracted from 3-d lidar scans",
            "venue": "2019 European Conference on Mobile Robots (ECMR), IEEE",
            "year": 2019
        },
        {
            "authors": [
                "R.B. Rusu",
                "N. Blodow",
                "Z. Marton",
                "A. Soos",
                "M. Beetz"
            ],
            "title": "Towards 3d object maps for autonomous household robots",
            "venue": "2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2007
        },
        {
            "authors": [
                "M. Steininger",
                "K. Kobs",
                "P. Davidson",
                "A. Krause",
                "A. Hotho"
            ],
            "title": "Density-based weighting for imbalanced regression",
            "venue": "Machine Learning 110(8)",
            "year": 2021
        },
        {
            "authors": [
                "B. Krawczyk"
            ],
            "title": "Learning from imbalanced data: open challenges and future directions",
            "venue": "Progress in Artificial Intelligence 5(4)",
            "year": 2016
        },
        {
            "authors": [
                "N. Carlevaris-Bianco",
                "A.K. Ushani",
                "R.M. Eustice"
            ],
            "title": "University of michigan north campus long-term vision and lidar dataset",
            "venue": "The International Journal of Robotics Research 35(9)",
            "year": 2016
        },
        {
            "authors": [
                "W. Xu",
                "Y. Cai",
                "D. He",
                "J. Lin",
                "F. Zhang"
            ],
            "title": "Fast-lio2: Fast direct lidar-inertial odometry",
            "venue": "IEEE Transactions on Robotics",
            "year": 2022
        },
        {
            "authors": [
                "K.H. Zou",
                "C.R. Yu",
                "K. Liu",
                "M.O. Carlsson",
                "J. Cabrera"
            ],
            "title": "Optimal thresholds by maximizing or minimizing various metrics via roc-type analysis",
            "venue": "Academic radiology 20(7)",
            "year": 2013
        },
        {
            "authors": [
                "J.D. Lawson",
                "Y. Lim"
            ],
            "title": "The geometric mean, matrices, metrics, and more",
            "venue": "The American Mathematical Monthly 108(9)",
            "year": 2001
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The identification of object stability within a 3D point cloud of a given environment is crucial for mobile robots conducting long-term missions. These objects serve as landmarks for localization, particularly in environments that undergo significant changes over time, such as parking lots. Usually, the localization process is performed with respect to an internal representation of the environment, such as a 3D point cloud map for LiDAR-based localization, which represents a snapshot of the static state of the environment at the time of map acquisition [1].\nHowever, the initial map may not be suitable for reliable long-term operations and can result in degradation of pose estimation for future sessions [2]. This is primarily due to two reasons: (i) the inclusion of dynamic objects as static, e.g. parked cars, even though such objects may appear static in the current scene, they are considered to be dynamic objects from the long-term perspective. (ii) The presence of \u201dflying ghost\u201d artifacts [3] caused by moving objects while recording the data, such as pedestrians or cars in motion.\nVarious solutions have been proposed for identifying the motion or stability status of objects, such as detecting and filtering dynamic objects using classical geometrical methods [4,3,5]. However, these methods primarily rely on motion\nar X\niv :2\n30 1.\n03 42\n6v 3\n[ cs\n.C V\n] 1\n2 Ju\nn 20\ninformation and may not detect dynamic objects that are stable in the current scene, such as parked cars. On the other hand, deep learning approaches can be used to identify object stability by achieving dense full-class segmentation [6,7,8]. These methods can be used to infer and detect object stability status, but they heavily rely on supervised annotated training data, which is not always available and can be costly to generate.\nIn order to address the limitations of existing approaches, this paper proposes an end-to-end unsupervised learning method for determining the long-term stability status of objects within a 3D point cloud map. The output of this method is a point-wise spatio-temporal stability score, where higher values indicate that the point belongs to a dynamic object, such as a car or pedestrian, and lower values are associated to long-term stable objects such as buildings and trees. To achieve this, we propose an automatic labelling algorithm to generate training data with a point-wise labelling by utilizing different time instances of the environment. For the learning step, we propose a regression neural network based on the hierarchical PointNet++ [9] architecture.\nThe main contributions of this research are three-fold: (i) an unsupervised automatic labelling algorithm that utilizes long-term observations of a given environment, (ii) LTS-NET a regression network based on PointNet++ for pointwise long-term stability score estimation, and (iii) a comprehensive evaluation of the proposed auto-labelling algorithm and regression neural network using real-world data, which demonstrates the effectiveness and convenience of the proposed approach as it does not require manual annotation."
        },
        {
            "heading": "2 Related work",
            "text": "Static and dynamic object segmentation is an active area of research, with methods broadly classified into geometry-based and deep learning approaches.\nGeometry-based methods are based on motion cues [10,1], ray tracing [3,11], or voxel traversal [5]. Motion cues (visibility-based) approaches identify dynamic points by comparing the current laser scan with previous scans. For example, Pomerleau et al [1]. infer the dynamic part of a scene by comparing the incoming scan with a global map based on visibility assumptions. Ray tracing methods rely on shooting rays and checking for occlusions. These methods are typically computationally expensive and run offline. An example of a method using this strategy is OctoMap [11], which is a probabilistic 3D mapping framework based on the octree data structure. The peopleremover [5] filters dynamic points using a voxel grid instead of an octree to store the identifier of all laser rays that hit the voxel.\nDeep learning approaches can be either supervised or unsupervised. Supervised method s[7,8,12] can achieve full classes semantic segmentation, but currently rely heavily on hand-annotated data and are prone to human error or unknown classes [13]. Unsupervised methods are a more interesting choice for learning object semantics, usually in the form of dynamic or static binary objects. These methods are data-driven and require minimal or no supervision [14]. For example, scene flow [15,16] approaches are being applied to point clouds directly in an unsupervised way to label points into moving or rigid objects between lidar frames. These methods are paired with a deep neural network for an end-to-end object semantic estimation.\nHowever, most methods mentioned above require motion information to infer dynamic objects; therefore, they cannot detect objects that can potentially move but are static in the current observation. In contrast to other works, our approach is more focused on identifying long-term stable objects in a given environment, as those objects are a key landmark to guarantee long-term localization without degradation in performance [17]. Our method is unsupervised and does not require human input as it implicitly learns the long-term stable objects in an environment by exploiting previous temporal observations."
        },
        {
            "heading": "3 Proposed method",
            "text": "In this paper, we propose a data-driven approach for identifying long-term stable objects in a given environment. We frame the problem as a regression task to associate a long-term stability score to each point in a point cloud. In particular, our model is able to predict that objects whose points have a high score are typically dynamic, we define a dynamic object as objects that can move on their own (e.g. cars, bikes, pedestrians, animals), while those with low-value score points are static, which are objects cannot move on their own (e.g. trees, poles, buildings).\nTo accomplish this, we utilize a temporal sequence of 3D point cloud maps, denoted as O0:K , where K is the total number of observations and Ok =\n{p1, . . . ,pN} is the k-th observation (k \u2208 K), with N being the number of point clouds. Each point cloud pi = {xi, yi, zi, Nxi, Nyi, Nzi} is defined by its 3D coordinates and associated surface normal vector.\nOur approach consists of two main steps: 1) designing an unsupervised labelling pipeline that assigns a stability score to each point cloud based on its spatial and temporal existence in all observations, where a score close to 0 indicates a long-term stable object and a score close to 1 indicates a dynamic object; 2) designing a regression network, denoted as f(.), that can be trained on the stability scores to predict the stability of objects in a new given point cloud map."
        },
        {
            "heading": "3.1 Unsupervised point wise spatio-temporal labelling",
            "text": "In order to assign a stability score to each point in a cloud associated with an environment, we require at least two observation of the same environment. The process of generating the stability score is illustrated in Figure 2. The first step of the labelling pipeline is to filter the observations. This is achieved by using the Cloth Simulation Filtering (CSF) algorithm [3] to remove the ground plane points, resulting in the set of off-ground points Ooffgroundk . The motivation for removing the ground plane points is threefold: (i) the ground plane is stable, (ii) it increases the disparity of point labels when extracting the spatial distance to the nearest neighbour point in other observations/maps at later stages, and (iii) it reduces the overall map size. After applying CSF, we remove the outliers using the Statistical Outlier Removal (SOR) filter [18] from the off-ground points. The output of this step is {Of0 , . . . ,O f K} indicating the filtered observations.\nThe next step is to register the filtered observations with respect to the first observation. This step is crucial for accurately associating features between all filtered observations. To perform the registration, we use the Iterative Closest\nPoint (ICP) algorithm to find the best transformation matrix T Of0 Ofk , which is then used to transform the observation as follows:\nMk = T Of0 Ofk \u00b7Ofk (1)\nwhereMk is the transformed observation with respect to the first observation, and we note that M0 = O f 0 .\nAfter pre-processing the data, the spatio-temporal label for each point pi \u2208 Mk is determined as follows: we first find a vector d of the spatial distance to the closest point in all other observations {Mj}Kj=0, j \u0338= k, where for example, d0 is the spatial distance to the closest point q in M0, and the size of d is K. Then, a point label is assigned as:\nli = 1\u2212 e\u2212\u03bb.max(d) (2)\nwhere \u03bb is a hyperparameter that controls the labelling sensitivity. We use the maximum spatial distance across the temporal slices as a stability feature because a dynamic object may not appear or change location in one of the temporal slices resulting in a larger spatial distance feature associated with it. While on the other hand, stable objects will appear in all slices in the same location, resulting in a small distance feature. Additionally, we use the Cumulative Distribution Function (CDF) of an exponential function to map the spatial distance into CDF space and bound the value between 0 and 1."
        },
        {
            "heading": "3.2 Regression Network",
            "text": "The stability labels generated in this work are continuous values that range between 0 and 1, which is not compatible with classical segmentation or binary classification networks that expect discrete labels. While it is possible to add a threshold to the input data to split the labels into two states, we are interested in directly learning and making use of the stability labels. To accomplish this, we propose a regression network, called LTS-NET, that can learn the stability labels directly from point cloud data.\nLTS-NET is based on the pioneering work of PointNet++ [9], which is composed of an encoder-decoder structure with multiple layers to enable the learning of spatial features at different scales. The encoder layers are referred to as abstraction layers (AL) and the decoder layers are referred to as feature propagation layers (PL) as introduced in the original implementation of PointNet++. For LTS-NET, we use 5 AL and 5 PL. The input number of points for each AL is as follows: N1 : 1024, N2 : 512, N3 : 256, N4 : 128 and N5 : 32 with the following sampling radius at each layer r1 : 0.1 m, r2 : 0.2 m, r3 : 0.4 m, r4 : 0.8 m and r5 : 1.4 m. In the output layer, we used the Sigmoid activation function to bound the estimates between [0, 1]; It is worth stating that we have also experimented with other activation functions such as CDF of an exponential function, but found that they gave similar results. This is likely due to the weights of the network adapting to the output activation layer.\nRegression Labels imbalance: To address the imbalance in continuous labels, we adopted a sample weighting approach solution proposed by Steininger et al. [19]. The weight for each sample is based on the rarity of the label, so the weight is inversely proportional to the probability of the label occurrence. This will help the model to better estimate the rare cases [20]. The weighting function is defined as\nfw(\u03b1, y) = max(1\u2212 \u03b1p\u2032(y), \u03f5)\n1 N \u2211N i=1(max(1\u2212 \u03b1p\u2032(yi), \u03f5)) , (3)\nwhere p is the target variable density function, N is the number of data points, y = y1, ...yN is the target values, p\n\u2032 = ( p(y) \u2212 min(p(y)) )/( max(p(y)) \u2212 min(p(y)) ) is the normalized density function \u2208 [0, 1] , hyperparameter \u03b1 \u2208 [0,\u221e) which emphasize the weighting scheme, and \u03f5 is a small positive real number to avoid negative or 0 weights. For more details and experiments of the effectiveness of this weighting scheme, we refer to the original density-based weighting article [19].\nRegression Loss function: We apply the weighted Root Mean Square Error to supervise the spatio-temporal score prediction for LTS-NET:\nL = ( 1 N N\u2211 i=1 fw(\u03b1, yi)(y\u0302i \u2212 yi)2) 1 2 . (4)\nCombining the sample gradients with the weight fw(\u03b1, yi) leads to larger gradients for the rare cases; in this way, the model is forced to have better estimates for the rare values as discussed in [19].\nData loader: When receiving in input a map, the first layer of LTS-NET subsample a fixed number of points in order to make the learning task more tractable. Because of this, it makes inconvenient to feed the entire map at once, because the local geometry of the environment will be lost. In order to keep the network\u2019s input representative enough, we divide the original map into multiple submaps and we feed them to the network in an iterative fashion. Choosing the appropriate submap size and number of points is a challenging problem due to the uneven distribution of data and the scale of features. In this work, we used a submap size of 10 \u00d7 10 m in the x and y axis, with no constraints in the z axis, and a number of points set to 4096, we found those values empirically while designing and testing the model. The submaps are generated in a convolutional way by moving with a fixed grid in the x and y axis (as illustrated in Fig. 1 data-loader block). The approach guarantees full map coverage and captures all features. The grid size for our experiments is 50% of the submap size, which gives 50% overlap.\nVoting layer: At the inference stage of the network, some points may be assigned multiple predictions due to overlapping submaps. In PointNet [9], they use a voting pool, where the class that gets more votes will be the point label. However, in regression, the prediction value is continuous; therefore, we take the mean of predictions to get the point estimated stability score."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "In this study, we evaluate our proposed approach using the North Campus LongTerm (NCLT) dataset [21]. The NCLT dataset is well-suited for our purposes as it was collected over a 15-month period, consisting of 27 recordings, providing a rich source of data for long-term operation analysis. The data was collected using a two-wheeled robot equipped with a Velodyne HDL-32E LiDAR, wheel encoders, GPS, IMU, and a gyroscope, on the University of Michigan campus.\nTo construct the 3D point cloud maps for our experiments, we employed the Simultaneous Localization and Mapping (SLAM) system FAST-LIO [22], which only requires 3D LiDAR data from the Velodyne and IMU data. From the 27 available data recording sessions, we selected 5 recordings that had the most overlap, specifically: 2012-01-15, 2012-02-02, 2012-04-29, 2012-05-26, and 2012-08-04. Within these observations, we focused on two areas, as shown in Fig. 3, which are two parking lots. The size of Area 1 is 70\u00d790 m, and Area 2 is 40\u00d730 m. The raw observations are denoted as {O0, . . . ,O4}, and the processed observations as {M0, . . . ,M4} throughout the rest of the manuscript.\nGround Truth Binary Maps for Object Stability: to evaluate the quality of the auto-labelled maps and the predictions of LTS-NET, it is necessary to use a ground truth binary labelled map as a reference. However, long-term object stability annotations are not available for the NCLT dataset. Therefore, we manually generated ground truth maps using the CloudCompare software1.\n1 CloudCompare (version 2.12) [GPL software]. (2022). Retrieved from http://www.cloudcompare.org/\nThe classification was performed as follows: trees, light posts, and poles were labelled as stable objects, while all other objects were labelled as dynamic.\nGenerating stability labels: in order to generate the stability labels for each map in the experimental area, we selected a reference map and then generated the stability labels with respect to all other maps in the experimental area by using the method described in Section 3.1. For example, the stability labels for M1, is found w.r.t {Mj}Kj=0, j \u0338= 1. It is worth noting that the temporal sequence of the maps is not important for identifying the static objects as they are not affected by the order in which the observations are being processed."
        },
        {
            "heading": "4.2 Baseline",
            "text": "To demonstrate that the proposed approach of using a regression network to learn a stability score for point cloud data outperforms traditional binary classification methods, we used the PointNet++ network as a baseline. The baseline is a PyTorch implementation of PointNet++ 2 that uses the weighted negative log-likelihood loss, where the weights are based on the distribution of the classes. Additionally, we used the same submap size to generate data for both the baseline and the proposed regression model. The baseline was trained and evaluated on the ground truth binary data, while the regression model was trained on the stability labels generated by the unsupervised auto-labelling method."
        },
        {
            "heading": "4.3 Experimental scenarios and setup",
            "text": "We conducted two experiments: Experiment 1 and Experiment 2. For Exp.1, both models were trained on the labelled map ML0 of Area 1 and evaluated in all other maps. The second Exp is similar to the first one, but both models were trained on ML0 of Area 2. The motivation behind these experiments is twofold: first, we want to assess the spatio-temporal generalization capability of the network, to evaluate if LTS-NET can infer stable object at different time slice of the same environment; second, we want to assess the generalization capability in unseen environments, which were not part of the training set.\nTo compare the output of LTS-NET with a classification mode, we converted the regression output into binary classes using a threshold value \u03f5. An optimal threshold for the regression model can be found with the Receiver Operating Characteristic (ROC) curve, which could be found by minimizing or maximizing a certain metric [23]. In our case, the metric that we are trying to maximize is the geometric mean [24], which is a metric for imbalanced classification that, if optimized, gives a balance between the sensitivity (true positives rate) of the model and the specificity (inverse of false positive).\nThe optimal threshold \u03f5 for Exp.1 is found using the ROC curve of the inferred labels of M0 of Area 1, which is equal to 0.269 corresponding to 0.626 m . Then, for consistency and to not over-fit the results, we used this value to convert the regression output to binary for all evaluated maps in Exp.1. Those\n2 https://github.com/yanx27/Pointnet Pointnet2 pytorch\nbinary labels are evaluated w.r.t the ground truth labels. For Exp.2, the optimal threshold was found for (M0) of Area 2 that is equal to 0.3593 (0.89 m), then we did the same as for Exp.1.\nImplementation: We train and evaluate both the binary classification (baseline) and LTS-NET on a workstation with Intel Core i7-6850K CPU, 64GB RAM and an NVidia GTX 1080ti GPU with 12 GB RAM. The model is implemented using PyTorch framework. We used a learning rate of 0.001, momentum 0.9, and trained for 60 epochs for both the baseline and our model (regression). The training time for both models was 4 hours for training on M0 of Area 1 and around 2 hours for training on M0 of Area 2.\nMetric: The metric used to evaluate the baseline segmentation model and the thresholded values of the regression model is the mean intersection over union (mIoU), defined as mIoU = 1N \u2211N c=0 IoUc, where N is the number of classes, IoUc = (|pc \u2229 Gc|)/(pc \u222a Gc), c is the point class (stable/unstable), pc is the predicted set, Gc is the ground truth set.\nTo evaluate LTS-NET predictions w.r.t to the auto labelled data, we used Root Mean Square Error (RMSE) metric expressed as RMSE = ( 1N \u2211N i=1(l\u0302i \u2212 li) 2) 1 2 , where N is the number of labels (points) and l\u0302i is the predicted label."
        },
        {
            "heading": "4.4 Results on NCLT parking lot areas",
            "text": "Evaluating the unsupervised labelling To evaluate the accuracy of the autolabelled data, we use the area under the ROC curve, known as ROC AUC, which summarizes the performance of the auto-labelling by a single number with values between 0.5 (random labelling) and 1.0 (perfect labelling). The ROC curves are computed by comparing the stability scores with the ground truth binary data at different thresholds. Tab. 1 summarize ROC AUC for Areas 1 and 2, which indicates a good performance of the auto-labelling algorithm.\nEvaluating maps inference As shown in Tab. 2, when both models were trained and evaluated in the same area, they showed a comparable performance despite the fact that LTS-NET was trained on the unsupervised labels only. The evaluation for both models is w.r.t the ground truth labels. The regression output was converted to binary using the optimal threshold for each Test as explained in Sec. 4.3. However, the interesting results are when evaluating the models in the opposite area used for training. For instance, in Exp.1, LTS-NET outperforms the binary classification model by a large margin in most maps;\non average, the mIoU score improved by 34.2% over the baseline. For Exp.2 on Area 1, the regression model shows improvement over the baseline with an average increase by 14.6% in the mIoU score. However, when training on Area 2 and testing on Area 1 we obtain a significant lower score (still superior to the one of the baseline). This is due tot he fact that Area 2 is smaller than Area 1, presenting way fewer feature from which the network can learn from. Overall, the results confirm that the continuous labels can better utilize the 3D spatial information in the point cloud data. A visualization of the best and worst results of Exp.1 are shown in Fig. 4."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we have proposed a novel end-to-end unsupervised deep learning method for estimating the long-term stability of objects in a 3D point cloud map. Our approach consists of two parts: an unsupervised labelling algorithm that generates a point-wise stability score by utilizing the temporal observations of a given environment, and LTS-NET, a stability point-wise regression network based on PointNet++ that is trained on the stability labels and can be used to infer objects\u2019 stability in similar environments with no previous observations.\nExperimental results have shown that the proposed method can efficiently identify which points in a map belong to long-term stable objects (i.e. poles, tree trunks, buildings), which can improve long-term localization in environments that are subject to continuous changes. Additionally, our method has revealed that long-term stable object classification is best performed by training a regression model on stability scores, followed by thresholding, compared to directly training a binary classifier. To the best of the authors\u2019 knowledge, this has not been previously investigated and reported in the literature.\nAs future work, we plan to explore extracting long-term stable objects directly from individual 3D LiDAR scans. We also aim at using stable feature maps obtained by LTS-NET for evaluating and improving upon long-term robot localization."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work has been supported by the European Commission as part of H2020 under grant number 871704 (BACCHUS)."
        }
    ],
    "title": "LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects",
    "year": 2023
}