{
    "abstractText": "With the rapid development of high-speed communication and artificial intelligence technologies, human perception of real-world scenes is no longer limited to the use of small Field of View (FoV) and low-dimensional scene detection devices. Panoramic imaging emerges as the next generation of innovative intelligent instruments for environmental perception and measurement. However, while satisfying the need for large-FoV photographic imaging, panoramic imaging instruments are expected to have high resolution, no blind area, miniaturization, and multidimensional intelligent perception, and can be combined with artificial intelligence methods towards the next generation of intelligent instruments, enabling deeper understanding and more holistic perception of 360\u25e6 real-world surrounding environments. Fortunately, recent advances in freeform surfaces, thinplate optics, and metasurfaces provide innovative approaches to address human perception of the environment, offering promising ideas beyond conventional optical imaging. In this review, we begin with introducing the basic principles of panoramic imaging systems, and then describe the architectures, features, and functions of various panoramic imaging systems. Afterwards, we discuss in detail the broad application prospects and great design potential of freeform surfaces, thin-plate optics, and metasurfaces in panoramic imaging. We then provide a detailed analysis on how these techniques can help enhance the performance of panoramic imaging systems. We further offer a detailed analysis of applications of panoramic imaging in scene understanding for autonomous driving and robotics, spanning panoramic semantic image segmentation, panoramic depth estimation, panoramic visual localization, and so on. Finally, we cast a perspective on future potential and research directions for panoramic imaging instruments.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaohua Gao"
        },
        {
            "affiliations": [],
            "name": "Kailun Yang"
        },
        {
            "affiliations": [],
            "name": "Hao Shi"
        },
        {
            "affiliations": [],
            "name": "Kaiwei Wang"
        },
        {
            "affiliations": [],
            "name": "Jian Bai"
        }
    ],
    "id": "SP:107b46655277c6231fb49518543c26bf3f947112",
    "references": [
        {
            "authors": [
                "D. Gledhill",
                "G.-Y. Tian",
                "D. Taylor",
                "D. Clarke"
            ],
            "title": "Panoramic imaging - a review",
            "venue": "Comput. Graph., vol. 27, pp. 435\u2013445, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "A. Bonarini",
                "P. Aliverti",
                "M. Lucioni"
            ],
            "title": "An omnidirectional vision sensor for fast tracking for mobile robots",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 49, no. 3, pp. 509\u2013512, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "M. Jiang",
                "R. Sogabe",
                "K. Shimasaki",
                "S. Hu",
                "T. Senoo",
                "I. Ishii"
            ],
            "title": "500-Fps omnidirectional visual tracking using three-axis active vision system",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "H. Yu",
                "B. Huang",
                "G. Yue",
                "B. Song"
            ],
            "title": "Blind omnidirectional image quality assessment based on structure and natural features",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Kholodilin",
                "Y. Li",
                "Q. Wang"
            ],
            "title": "Omnidirectional vision system with laser illumination in a flexible configuration and its calibration by one single snapshot",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 69, no. 11, pp. 9105\u20139118, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Cao",
                "G. Jiang",
                "Z. Jiang",
                "M. Yu",
                "Y. Qi",
                "Y.-S. Ho"
            ],
            "title": "Quality measurement for high dynamic range omnidirectional image systems",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1\u201315, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Hu",
                "Y. An",
                "C. Shao",
                "H. Hu"
            ],
            "title": "Distortion convolution module for semantic segmentation of panoramic images based on the imageforming principle",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201312, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Roriz",
                "J. Cabral",
                "T. Gomes"
            ],
            "title": "Automotive LiDAR technology: A survey",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Gao",
                "Y. Pan",
                "C. Li",
                "S. Geng",
                "H. Zhao"
            ],
            "title": "Are we hungry for 3D LiDAR data for semantic segmentation? A survey of datasets and methods",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Feng",
                "C. Haase-Sch\u00fctz",
                "L. Rosenbaum",
                "H. Hertlein",
                "C. Glaeser",
                "F. Timm",
                "W. Wiesbeck",
                "K. Dietmayer"
            ],
            "title": "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 3, pp. 1341\u20131360, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "D. Sidib\u00e9",
                "O. Morel",
                "F. M\u00e9riaudeau"
            ],
            "title": "Deep multimodal fusion for semantic image segmentation: A survey",
            "venue": "Image and Vision Computing, vol. 105, p. 104042, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ming",
                "X. Meng",
                "C. Fan",
                "H. Yu"
            ],
            "title": "Deep learning for monocular depth estimation: A review",
            "venue": "Neurocomputing, vol. 438, pp. 14\u201333, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.P. Rolland",
                "M.A. Davies",
                "T.J. Suleski",
                "C. Evans",
                "A. Bauer",
                "J.C. Lambropoulos",
                "K. Falaggis"
            ],
            "title": "Freeform optics for imaging",
            "venue": "Optica, vol. 8, no. 2, pp. 161\u2013176, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.Y. Shalaginov",
                "S. An",
                "F. Yang",
                "P. Su",
                "D. Lyzwa",
                "A.M. Agarwal",
                "H. Zhang",
                "J. Hu",
                "T. Gu"
            ],
            "title": "Single-element diffraction-limited fisheye metalens.",
            "venue": "Nano Letters,",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "X. Huang",
                "J. Bai",
                "K. Wang",
                "Y. Hua"
            ],
            "title": "Design of high resolution panoramic annular lens system",
            "venue": "Applied Optics and Photonics China, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Gao",
                "L. Sun",
                "Q. Jiang",
                "H. Shi",
                "J. Wang",
                "K. Wang",
                "J. Bai"
            ],
            "title": "Compact and lightweight panoramic annular lens for computer vision tasks",
            "venue": "Optics Express, vol. 30, no. 17, pp. 29 940\u201329 956, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.M. Geary"
            ],
            "title": "Introduction to lens design: with practical ZEMAX examples",
            "venue": "Willmann-Bell Richmond, VA,",
            "year": 2002
        },
        {
            "authors": [
                "F. Song"
            ],
            "title": "Introduction to Modern Optical System Design",
            "year": 2019
        },
        {
            "authors": [
                "J. Xiong",
                "E. Hsiang",
                "Z. He",
                "T. Zhan",
                "S. Wu"
            ],
            "title": "Augmented reality and virtual reality displays: emerging technologies and future perspectives",
            "venue": "Light, Science & Applications, vol. 10, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Cheng",
                "Q. Wang",
                "Y. Liu",
                "H. Chen",
                "D. Ni",
                "X. Wang",
                "C. Yao",
                "Q. Hou",
                "W. Hou",
                "G. Luo",
                "Y. Wang"
            ],
            "title": "Design and manufacture AR head-mounted displays: A review and outlook",
            "venue": "Light: Advanced Manufacturing, vol. 2, no. 3, pp. 350\u2013369, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Hecht"
            ],
            "title": "Optics 4th edition. optics",
            "venue": "Addison Wesley Longman Inc, vol. 1, p. 1998, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "D.H. Sliney"
            ],
            "title": "What is light? the visible spectrum and beyond",
            "venue": "Eye, vol. 30, pp. 222\u2013229, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Arnold"
            ],
            "title": "The electromagnetic spectrum",
            "venue": "The Patrick Moore Practical Astronomy Series, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Tobiska",
                "A. Nusinov"
            ],
            "title": "Iso 21348-process for determining solar irradiances",
            "venue": "36th COSPAR Scientific Assembly, vol. 36, 2006, p. 2621.",
            "year": 2006
        },
        {
            "authors": [
                "E.D. Cyan"
            ],
            "title": "Handbook of chemistry and physics",
            "venue": "JAMA Internal Medicine, vol. 126, pp. 335\u2013336, 1970.",
            "year": 1970
        },
        {
            "authors": [
                "L. Wang",
                "C. Li",
                "C. Jin"
            ],
            "title": "Design of catadioptric omnidirectional imaging system in solar blind UV",
            "venue": "Optics and Precision Engineering, vol. 19, pp. 1503\u20131509, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "I. Powell"
            ],
            "title": "Design study of an infrared panoramic optical system.",
            "venue": "Applied Optics,",
            "year": 1996
        },
        {
            "authors": [
                "E. J"
            ],
            "title": "Greivenkamp, Field guide to geometrical optics",
            "venue": "SPIE press Bellingham, Washington,",
            "year": 2004
        },
        {
            "authors": [
                "M. Bass"
            ],
            "title": "Handbook of optics: volume ii-design, fabrication, and testing; sources and detectors; radiometry and photometry",
            "venue": "McGraw- Hill Education,",
            "year": 2010
        },
        {
            "authors": [
                "G.C. Holst",
                "R.G. Driggers"
            ],
            "title": "Small detectors in infrared system design",
            "venue": "Optical Engineering, vol. 51, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "E.M. Robinson"
            ],
            "title": "Chapter 5 - focus, depth of field, and lenses",
            "venue": "Crime Scene Photography (Third Edition), E. M. Robinson, Ed. San Diego: Academic Press, 2016, pp. 201\u2013266.",
            "year": 2016
        },
        {
            "authors": [
                "D.S. Goodman"
            ],
            "title": "General principles of geometric optics",
            "venue": "Hand Book of Optics, vol. 1, pp. 68\u201369, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "F. Bettonvil"
            ],
            "title": "Fisheye lenses",
            "venue": "WGN, Journal of the International Meteor Organization, vol. 33, pp. 9\u201314, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "D. Schneider",
                "E. Schwalbe",
                "H.-G. Maas"
            ],
            "title": "Validation of geometric models for fisheye lenses",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 64, pp. 259\u2013266, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "J.S. Warren"
            ],
            "title": "Modern optical engineering",
            "venue": "The Design of Optical Systems, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "M.C. Dudzik",
                "J.S. Accetta",
                "D.L. Shumaker"
            ],
            "title": "The infrared & electro-optical systems handbook. electro-optical systems design, analysis, and testing, volume 4",
            "venue": "Infrared information and analysis center ann arbor mi, Tech. Rep., 1993.",
            "year": 1993
        },
        {
            "authors": [
                "B.S. Carlson"
            ],
            "title": "Comparison of modern ccd and cmos image sensor technologies and systems for low resolution imaging",
            "venue": "Proceedings of IEEE Sensors, vol. 1, pp. 171\u2013176 vol.1, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "R.K. Lenz",
                "U. Lenz"
            ],
            "title": "New developments in high-resolution image acquisition with ccd area sensors",
            "venue": "Other Conferences, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "V. Bockaert"
            ],
            "title": "Sensor sizes",
            "venue": "Digital Photography Review., 2002.",
            "year": 2002
        },
        {
            "authors": [
                "B. Benjamin"
            ],
            "title": "Practical optics: Testing different sensor sizes",
            "venue": "American Cinematographer: The International Journal of Film & Digital Production Techniques, vol. 97, pp. 90\u201399, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Uvarova",
                "D.H. Nguyen",
                "S. Gao",
                "A. Bakholdin"
            ],
            "title": "Optical design of microscope objectives with low magnification",
            "venue": "SPIE/COS Photonics Asia, 2021. IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, OCTOBER 2022 27",
            "year": 2021
        },
        {
            "authors": [
                "E.A. Tsyganok",
                "A. Kozhina",
                "S. Gao"
            ],
            "title": "Development of microscope lenses with uniform separation of optical illumination",
            "venue": "Optical Design and Testing XI, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Luo",
                "X. Huang",
                "J. Bai",
                "R. Liang"
            ],
            "title": "Compact polarization-based dual-view panoramic lens.",
            "venue": "Applied Optics,",
            "year": 2017
        },
        {
            "authors": [
                "D.-H. Hwang",
                "K. Aso",
                "H. Koike"
            ],
            "title": "Toward human motion capturing with an ultra-wide fisheye camera on the chest",
            "venue": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 1524\u20131526, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Wagner",
                "A. Mulloni",
                "T. Langlotz",
                "D. Schmalstieg"
            ],
            "title": "Real-time panoramic mapping and tracking on mobile phones",
            "venue": "2010 IEEE Virtual Reality Conference (VR), pp. 211\u2013218, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "C.-H. Lin",
                "L. Hsiao",
                "J.-T. Hsaio",
                "H.Y. Lin"
            ],
            "title": "Front view and panoramic side view videoscope lens system design.",
            "venue": "Applied Optics,",
            "year": 2014
        },
        {
            "authors": [
                "H. Yuan",
                "B. Wang",
                "J.A. Zhang",
                "H. Li"
            ],
            "title": "A novel method for geometric correction of multi-cameras in panoramic video system",
            "venue": "2010 International Conference on Measuring Technology and Mechatronics Automation, vol. 1, pp. 248\u2013251, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "L.V. Cowan",
                "J. Babington",
                "G. Carles",
                "M.A. Perciado",
                "A.P. Wood",
                "A.R. Harvey"
            ],
            "title": "360\u00b0 snapshot imaging with a convex array of longwave infrared cameras",
            "venue": "Imaging and Applied Optics 2019 (COSI, IS, MATH, pcAOP), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.S. Lin",
                "C.-C. Chang",
                "H.-Y. Chang",
                "Y.-Y. Chuang",
                "T. Lin",
                "M. Ouhyoung"
            ],
            "title": "A low-cost portable polycamera for stereoscopic 360\u00b0 imaging",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 29, pp. 915\u2013929, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Thibault",
                "J.R. Gauvin",
                "M. Doucet",
                "M. Wang"
            ],
            "title": "Enhanced optical design by distortion control",
            "venue": "SPIE Optical Systems Design, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "A. Ning"
            ],
            "title": "Wide-angle lenses with reduced ghost reflections",
            "venue": "Oct. 28 2014, US Patent 8,873,167.",
            "year": 2014
        },
        {
            "authors": [
                "L. Fan",
                "L. Lu"
            ],
            "title": "Design of a simple fisheye lens",
            "venue": "Appl. Opt., vol. 58, no. 19, pp. 5311\u20135319, Jul 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Thibault",
                "J.C. Artonne"
            ],
            "title": "Panomorph lenses: a low-cost solution for panoramic surveillance",
            "venue": "SPIE Defense + Commercial Sensing, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "Panomorph lenses: a new type of panoramic lens",
            "venue": "International Optical Design Conference, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S. Thibault",
                "J. Parent",
                "H. Zhang",
                "P. Roulet"
            ],
            "title": "Design, fabrication and test of miniature plastic panomorph lenses with 180\u00b0 field of view",
            "venue": "International Optical Design Conference, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W. Zhuang"
            ],
            "title": "Research on key technology of single chip 360\u00b0 panoramic imaging optical system",
            "venue": "Master\u2019s thesis, Changchun University of Science and Technology, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Wu",
                "X. Zeng",
                "P. Wang"
            ],
            "title": "Design of catadioptric IR panoramic imaging optical system",
            "venue": "Infrared, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Huang",
                "Y. Fu",
                "G. Zhang",
                "Z. Liu"
            ],
            "title": "Modeling and analysis of a monocentric multi-scale optical system.",
            "venue": "Optics Express,",
            "year": 2020
        },
        {
            "authors": [
                "E.J. Tremblay",
                "D.L. Marks",
                "D.J. Brady",
                "J.E. Ford"
            ],
            "title": "Design and scaling of monocentric multiscale imagers.",
            "venue": "Applied Optics,",
            "year": 2012
        },
        {
            "authors": [
                "I. Stamenov",
                "S. Olivas",
                "A. Arianpour",
                "I.P. Agurok",
                "J.E. Ford",
                "A. Johnson",
                "R. Stack"
            ],
            "title": "Broad-spectrum fiber-coupled monocentric lens imaging",
            "venue": "International Optical Design Conference, 2014, pp. IM3B\u20135.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Wang",
                "X. Meng",
                "Z. Gu",
                "X. Mu"
            ],
            "title": "The design of miniaturization and super wide angle monitor lens based on monocentric lens",
            "venue": "9th International Symposium on Advanced Optical Manufacturing and Testing Technologies: Meta-Surface-Wave and Planar Optics, vol. 10841, 2019, pp. 276\u2013281.",
            "year": 2019
        },
        {
            "authors": [
                "C. Pernechele"
            ],
            "title": "Introduction to Panoramic Lenses",
            "venue": "ser. SPIE Digital Library",
            "year": 2018
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Hyper-hemispheric and bifocal panoramic lenses",
            "venue": "Electro- Optical and Infrared Systems: Technology and Applications X, vol. 8896, 2013, p. 889603.",
            "year": 2013
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Hyper hemispheric lens",
            "venue": "Optics Express, vol. 24, no. 5, pp. 5014\u20135019, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Zhou",
                "Y. Tian",
                "J. Wang",
                "M. Xu"
            ],
            "title": "Design and implementation of a high-performance panoramic annular lens",
            "venue": "Applied Optics, vol. 59, no. 36, pp. 11 246\u201311 252, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Gao",
                "E.A. Tsyganok",
                "X. Xu"
            ],
            "title": "Design of a compact dual-channel panoramic annular lens with a large aperture and high resolution",
            "venue": "Applied Optics, vol. 60, no. 11, pp. 3094\u20133102, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Wang",
                "K. Yang",
                "S. Gao",
                "L. Sun",
                "C. Zhu",
                "K. Wang",
                "J. Bai"
            ],
            "title": "Highperformance panoramic annular lens design for real-time semantic segmentation on aerial imagery",
            "venue": "Optical Engineering, vol. 61, no. 3, p. 035101, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Zhang",
                "X. Zhong",
                "L. Zhang",
                "T. Zhang"
            ],
            "title": "Design of a panoramic annular lens with ultrawide angle and small blind area",
            "venue": "Applied Optics, vol. 59, no. 19, pp. 5737\u20135744, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Brown",
                "R.I. Hartley",
                "D. Nist\u00e9r"
            ],
            "title": "Minimal solutions for panoramic stitching",
            "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J.W. Kim",
                "J.M. Ryu",
                "Y.-J. Kim"
            ],
            "title": "Tolerance analysis and compensation method using zernike polynomial coefficients of omnidirectional and fisheye varifocal lens",
            "venue": "Journal of The Optical Society of Korea, vol. 18, pp. 720\u2013731, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Pernechele",
                "L. Consolaro",
                "G.H. Jones",
                "G.M. Brydon",
                "V.D. Deppo"
            ],
            "title": "Telecentric F-theta fisheye lens for space applications",
            "venue": "OSA Continuum, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Niu",
                "J. Bai",
                "X.Y. Hou",
                "G. Yang"
            ],
            "title": "Design of a panoramic annular lens with a long focal length.",
            "venue": "Applied Optics,",
            "year": 2007
        },
        {
            "authors": [
                "Y. Yao",
                "A. Geng",
                "J. Bai",
                "H. Wang",
                "J. Guo",
                "T. Xiong",
                "Y. Luo",
                "Z. Huang",
                "X.Y. Hou"
            ],
            "title": "Design of a panoramic long-wave infrared athermal system",
            "venue": "Optical Engineering, vol. 55, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Wang",
                "Y. Liang",
                "M. Xu"
            ],
            "title": "Design of panoramic lens based on ogive and aspheric surface.",
            "venue": "Optics Express,",
            "year": 2015
        },
        {
            "authors": [
                "Q. Xue",
                "W. Chen"
            ],
            "title": "Optical system design of space-based UV panoramic imager",
            "venue": "Infrared and Laser Engineering, vol. 43, no. 2, pp. 517\u2013522, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T. Ma",
                "J. Yu",
                "P. Liang",
                "C. Wang"
            ],
            "title": "Design of a freeform varifocal panoramic optical system with specified annular center of field of view",
            "venue": "Optics Express, vol. 19, no. 5, pp. 3843\u20133853, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J. Wang",
                "A. Amani",
                "C. Zhu",
                "J. Bai"
            ],
            "title": "Design of a compact varifocal panoramic system based on the mechanical zoom method",
            "venue": "Applied Optics, vol. 60, no. 22, pp. 6448\u20136455, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y.-h. Huang",
                "Z.-y. Liu",
                "Y.-g. Fu",
                "H. Zhang"
            ],
            "title": "Design of a compact two-channel panoramic optical system",
            "venue": "Optics Express, vol. 25, no. 22, pp. 27 691\u201327 705, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Liu",
                "Y. Wang"
            ],
            "title": "Design of cemented compact two channel panoramic annular optical system",
            "venue": "Applied Optics and Photonics China, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Luo",
                "J. Bai",
                "X. Zhou",
                "X. Huang",
                "Q. Liu",
                "Y. Yao"
            ],
            "title": "Non-blind area pal system design based on dichroic filter.",
            "venue": "Optics Express,",
            "year": 2016
        },
        {
            "authors": [
                "N. Yu",
                "F. Capasso"
            ],
            "title": "Flat optics with designer metasurfaces",
            "venue": "Nature Materials, vol. 13, no. 2, pp. 139\u2013150, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T. Yang",
                "Y. Duan",
                "D. Cheng",
                "Y. Wang"
            ],
            "title": "Freeform imaging optical system design: Theories, development, and applications",
            "venue": "Acta Optica Sinica, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Peng",
                "Q. Sun",
                "X. Dun",
                "G. Wetzstein",
                "W. Heidrich",
                "F. Heide"
            ],
            "title": "Learned large field-of-view imaging with thin-plate optics",
            "venue": "ACM Transactions on Graphics (TOG), vol. 38, pp. 1 \u2013 14, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.Y. Shalaginov",
                "S. An",
                "F. Yang",
                "P. Su",
                "D. Lyzwa",
                "A. Agarwal",
                "H. Zhang",
                "J. Hu",
                "T. Gu"
            ],
            "title": "A single-layer panoramic metalens with > 170\u00b0 diffraction-limited field of view",
            "venue": "arXiv preprint arXiv:1908.03626, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "D. Wang",
                "J. Wang",
                "Y. Tian",
                "K. Hu",
                "M. Xu"
            ],
            "title": "PAL-SLAM: a featurebased SLAM system for a panoramic annular lens",
            "venue": "Optics Express, vol. 30, no. 2, pp. 1099\u20131113, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L.E. Gurrieri",
                "E. Dubois"
            ],
            "title": "Acquisition of omnidirectional stereoscopic images and videos of dynamic scenes: a review",
            "venue": "Journal of Electronic Imaging, vol. 22, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Ji",
                "Z. yang Qin",
                "J. Shan",
                "M. Lu"
            ],
            "title": "Panoramic slam from a multiple fisheye camera rig",
            "venue": "Isprs Journal of Photogrammetry and Remote Sensing, vol. 159, pp. 169\u2013183, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Scaramuzza"
            ],
            "title": "Omnidirectional camera",
            "venue": "Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K.-C. Huang",
                "P.-Y. Chien",
                "C.-A. Chien",
                "H.-C. Chang",
                "J.-I. Guo"
            ],
            "title": "A 360-degree panoramic video system design",
            "venue": "Technical Papers of 2014 International Symposium on VLSI Design, Automation and Test, 2014, pp. 1\u20134.",
            "year": 2014
        },
        {
            "authors": [
                "H. Hua",
                "N. Ahuja",
                "C. Gao"
            ],
            "title": "Design analysis of a high-resolution panoramic camera using conventional imagers and a mirror pyramid",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, pp. 356\u2013361, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "O. Haggui",
                "H. Bayd",
                "B. Magnier",
                "A. Aberkane"
            ],
            "title": "Human detection in moving fisheye camera using an improved YOLOv3 framework",
            "venue": "2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP), pp. 1\u20136, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Bond"
            ],
            "title": "Lxxxix. a wide angle lens for cloud recording",
            "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 44, no. 263, pp. 999\u20131001, 1922.",
            "year": 1922
        },
        {
            "authors": [
                "R. Hill"
            ],
            "title": "A lens for whole sky photographs",
            "venue": "Quarterly Journal of the Royal Meteorological Society, vol. 50, no. 211, pp. 227\u2013235, 1924.",
            "year": 1924
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "New generation of high-resolution panoramic lenses",
            "venue": "SPIE Optical Engineering + Applications, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "Y. Yan",
                "J.M. Sasi\u00e1n"
            ],
            "title": "Photographic zoom fisheye lens design for dslr cameras",
            "venue": "Optical Engineering, vol. 56, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "Panoramic lens applications revisited",
            "venue": "SPIE Photonics Europe, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "R. Kingslake"
            ],
            "title": "Development of the photographic objective",
            "venue": "Photonics West - Lasers and Applications in Science and Engineering, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "Panoramic lens an historical perspective: from sky lens to consumer wide angle freeform optics",
            "venue": "International Optical Design Conference, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "AEG"
            ],
            "title": "Ab\u00e4nderung eines weitwinkelobjektivs (modification of a wideangle lens",
            "venue": "Aug. 6 1935, DE patent 620538C.",
            "year": 1935
        },
        {
            "authors": [
                "K. Miyamoto"
            ],
            "title": "Fish eye lens",
            "venue": "J. Opt. Soc. Am., vol. 54, no. 8, pp. 1060\u20131061, Aug 1964.",
            "year": 1964
        },
        {
            "authors": [
                "C.B. Martin"
            ],
            "title": "Design issues of a hyperfield fisheye lens",
            "venue": "Novel Optical Systems Design and Optimization VII, vol. 5524, 2004, pp. 84\u201392.",
            "year": 2004
        },
        {
            "authors": [
                "F.E. Sahin",
                "A.R. Tanguay"
            ],
            "title": "Distortion optimization for wide-angle computational cameras.",
            "venue": "Optics Express,",
            "year": 2018
        },
        {
            "authors": [
                "G.-Y. Tian",
                "D. Gledhill",
                "D. Taylor",
                "D. Clarke"
            ],
            "title": "Colour correction for panoramic imaging",
            "venue": "Proceedings Sixth International Conference on Information Visualisation, pp. 483\u2013488, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "A. Samy",
                "Z. shan Gao"
            ],
            "title": "Simplified compact fisheye lens challenges and design",
            "venue": "Journal of Optics, vol. 44, pp. 409\u2013416, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L.-J. Lu",
                "X.-Y. Hu",
                "C.-Y. Sheng"
            ],
            "title": "Optimization method for ultrawide-angle and panoramic optical systems",
            "venue": "Applied Optics, vol. 51, no. 17, pp. 3776\u20133786, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "L. Fan",
                "L. Lu"
            ],
            "title": "Calculation of the wave aberration of field curvature and color aberrations of an ultrawide-angle optical system",
            "venue": "Optics Communications, vol. 479, p. 126414, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. il Kweon",
                "Y.-H. Choi"
            ],
            "title": "Image-processing based panoramic camera employing single fisheye lens",
            "venue": "Journal of The Optical Society of Korea, vol. 14, pp. 245\u2013259, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "T. Ho",
                "M. Budagavi"
            ],
            "title": "Dual-fisheye lens stitching for 360-degree imaging",
            "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 2172\u20132176.",
            "year": 2017
        },
        {
            "authors": [
                "W. Gao",
                "S. Shen"
            ],
            "title": "Dual-fisheye omnidirectional stereo",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 6715\u20136722.",
            "year": 2017
        },
        {
            "authors": [
                "W. Song",
                "X. Liu",
                "Q. Cheng",
                "Y. Huang",
                "Y. Zheng",
                "Y. Liu",
                "Y. Wang"
            ],
            "title": "Design of a 360-deg panoramic capture system based on a smart phone",
            "venue": "Optical Engineering, vol. 59, pp. 015 101 \u2013 015 101, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Ye",
                "Z.H. Zeng",
                "F. Lin"
            ],
            "title": "Design of compact panoramic lens",
            "venue": "International Symposium on Advanced Optical Manufacturing and Testing Technologies (AOMATT), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Song",
                "X. Liu",
                "P. Lu",
                "Y. Huang",
                "D. Weng",
                "Y. Zheng",
                "Y. Liu",
                "Y. Wang"
            ],
            "title": "Design and assessment of a 360\u00b0 panoramic and high-performance capture system with two tiled catadioptric imaging channels.",
            "venue": "Applied Optics,",
            "year": 2018
        },
        {
            "authors": [
                "D. Geng",
                "H. tao Yang",
                "C. Mei",
                "Y. Li"
            ],
            "title": "Optical system design of space fisheye lens and performance analysis",
            "venue": "Applied Optics and Photonics China, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "O.-Y. Mang",
                "S.-W. Huang",
                "Y.-L. Chen",
                "H.-H. Lee",
                "P.-K. Weng"
            ],
            "title": "Design of wide-angle lenses for wireless capsule endoscopes",
            "venue": "Optical Engineering, vol. 46, p. 103002, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "C.-T. Yen",
                "J.-M. Zhang"
            ],
            "title": "The vehicle zoom ultra wide angle lens design by using liquid lens technology",
            "venue": "Microsystem Technologies, vol. 28, pp. 195\u2013208, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Peng",
                "L.B. Kong"
            ],
            "title": "Design of a real-time fiber-optic infrared imaging system with wide-angle and large depth of field",
            "venue": "Chinese Optics Letters, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "Panomorph based panoramic vision sensors",
            "venue": "Vision Sensors and Edge Detection, p. 1, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Optical design of an hemispherical, long-wave infrared panomorph lens for total situational awareness",
            "venue": "Defense + Commercial Sensing, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "S. Thibault",
                "J. Parent",
                "H. Zhang",
                "X. Du",
                "P. Roulet"
            ],
            "title": "Consumer electronic optics: how small can a lens be: the case of panomorph lenses",
            "venue": "Optics & Photonics - Optical Engineering + Applications, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "Enhanced surveillance system based on panomorph panoramic lenses",
            "venue": "SPIE Defense + Commercial Sensing, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Novel compact panomorph lens based vision system for monitoring around a vehicle",
            "venue": "Photonics Europe, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "J. Parent",
                "S. Thibault"
            ],
            "title": "Tolerancing panoramic lenses",
            "venue": "Optical Engineering + Applications, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Spatial dependence of surface error slopes on tolerancing panoramic lenses",
            "venue": "Applied Optics, vol. 49, pp. 2686\u20132693, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Thibault",
                "J. Parent",
                "H. Zhang",
                "M. Larivi\u00e8re-Bastien",
                "A.-S. Poulin- Girard",
                "A. Arfaoui",
                "P. D\u00e9saulniers"
            ],
            "title": "Developments in modern panoramic lenses: lens design, controlled distortion, and characterization",
            "venue": "International Conference on Optical Instruments and Technology, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "A.-S. Poulin-Girard",
                "S. Thibault"
            ],
            "title": "Optical testing of panoramic lenses",
            "venue": "Optical Engineering, vol. 51, p. 053603, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "H. Kim",
                "J. Jung",
                "J. Paik"
            ],
            "title": "Fisheye lens camera based surveillance system for wide field of view monitoring",
            "venue": "Optik, vol. 127, pp. 5636\u2013 5646, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Zhuang",
                "S. Thibault"
            ],
            "title": "Numerical investigation of threedimensional pupil model impact on the relative illumination in panomorph lenses",
            "venue": "Optical Engineering, vol. 56, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhuang",
                "S. Thibault",
                "J. Parent"
            ],
            "title": "Analysis of three-dimensional entrance pupil model in panomorph lenses",
            "venue": "International Optical Design Conference, 2017, pp. ITh2A\u20132.",
            "year": 2017
        },
        {
            "authors": [
                "B. Akdemir",
                "A.N. Belbachir",
                "L.M. Svendsen"
            ],
            "title": "Real-time vehicle localization and tracking using monocular panomorph panoramic vision",
            "venue": "2018 24th International Conference on Pattern Recognition (ICPR), 2018, pp. 2350\u20132355.",
            "year": 2018
        },
        {
            "authors": [
                "P. Roulet",
                "P. Konen",
                "M. Villegas",
                "S. Thibault",
                "P. Garneau"
            ],
            "title": "360\u00b0 endoscopy using panomorph lens technology",
            "venue": "BiOS, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Thibault"
            ],
            "title": "IR panomorph lens imager and applications",
            "venue": "SPIE Defense + Commercial Sensing, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "J.S. Chahl",
                "M.V. Srinivasan"
            ],
            "title": "Reflective surfaces for panoramic imaging.",
            "venue": "Applied Optics,",
            "year": 1997
        },
        {
            "authors": [
                "R.B. Benosman",
                "S.B. Kang"
            ],
            "title": "Panoramic vision : sensors, theory, and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2001
        },
        {
            "authors": [
                "G. il Kweon",
                "K.T. Kim",
                "G.-H. Kim",
                "H. sik Kim"
            ],
            "title": "Folded catadioptric panoramic lens with an equidistance projection scheme.",
            "venue": "Applied Optics,",
            "year": 2005
        },
        {
            "authors": [
                "Y. Wang",
                "Y. Lv",
                "X. Xu",
                "X. Gong",
                "Z. Yu",
                "J. Geng"
            ],
            "title": "Single-view calibration of a catadioptric camera based on a theodolite model",
            "venue": "Applied Optics, vol. 61, no. 9, pp. 2256\u20132266, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Gong",
                "Y. Lv",
                "X.-P. Xu",
                "Z. Jiang",
                "Z. Sun"
            ],
            "title": "High-precision calibration of omnidirectional camera using an iterative method",
            "venue": "IEEE Access, vol. 7, pp. 152 179\u2013152 186, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Aziz",
                "O. Labbani-Igbida",
                "A. Radgui",
                "A. Tamtaoui"
            ],
            "title": "Color-metric tensor for catadioptric systems",
            "venue": "2016 IEEE International Conference on Image Processing (ICIP), pp. 1594\u20131598, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "W. Wei",
                "C. Zongsheng"
            ],
            "title": "A cylindrical virtual space based catadioptric real-time panorama imaging system",
            "venue": "2006 IEEE Conference on Robotics, Automation and Mechatronics, pp. 1\u20135, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "F. Aziz",
                "O. Labbani-Igbida",
                "A. Radgui",
                "A. Tamtaoui"
            ],
            "title": "A riemannian approach for free-space extraction and path planning using catadioptric omnidirectional vision",
            "venue": "Image and Vision Computing, vol. 95, p. 103872, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L.N. Arkhipova",
                "A.A. Bagdasarov",
                "O.V. Bagdasarova",
                "D.N. Shevchenko"
            ],
            "title": "Circular-scan panoramic systems",
            "venue": "Journal of Optical Technology, vol. 83, p. 342, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G. il Kweon",
                "M. Laikin"
            ],
            "title": "Design of a mega-pixel grade catadioptric panoramic lens with the rectilinear projection scheme",
            "venue": "Journal of The Optical Society of Korea, vol. 10, pp. 67\u201375, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "A. Torii",
                "R. Klette"
            ],
            "title": "Panoramic and 3D computer vision",
            "venue": "ArtsIT, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "W. Li",
                "Y.F. Li"
            ],
            "title": "Single-camera panoramic stereo imaging system with a fisheye lens and a convex mirror.",
            "venue": "Optics Express,",
            "year": 2011
        },
        {
            "authors": [
                "S.-M. Tseng",
                "C.-W. Huang",
                "Y.-T. Hsu",
                "J.-C. Yu"
            ],
            "title": "Panoramic annular lens design of endoscope",
            "venue": "2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW), 2017, pp. 101\u2013102.",
            "year": 2017
        },
        {
            "authors": [
                "S.-M. Tseng",
                "J.-C. Yu",
                "Y.-T. Hsu",
                "C.-W. Huang",
                "Y.F. Tsai",
                "W. Cheng"
            ],
            "title": "Panoramic endoscope based on convex parabolic mirrors",
            "venue": "Optical Engineering, vol. 57, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Ou-Yang",
                "W. de Jeng"
            ],
            "title": "Design and analysis of radial imaging capsule endoscope (rice) system.",
            "venue": "Optics Express,",
            "year": 2011
        },
        {
            "authors": [
                "R. Katkam",
                "B. Banerjee",
                "C.Y. Huang",
                "X. Zhu",
                "L.H. Ocampo",
                "J.-L. Kincade",
                "R. Liang"
            ],
            "title": "Compact dual-view endoscope without field obscuration",
            "venue": "Journal of Biomedical Optics, vol. 20, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M.-J. Sheu",
                "C.-W. Chiang",
                "W.-S. Sun",
                "J.-J. Wang",
                "J.-W. Pan"
            ],
            "title": "Dual view capsule endoscopic lens design",
            "venue": "Optics Express, vol. 23 7, pp. 8565\u201375, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Lopes",
                "A.F. Ribeiro",
                "N. Pereira"
            ],
            "title": "Catadioptric system optimisation for omnidirectional robocup msl robots",
            "venue": "RoboCup, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "G. Krishnan",
                "S.K. Nayar"
            ],
            "title": "Cata-fisheye camera for panoramic imaging",
            "venue": "2008 IEEE Workshop on Applications of Computer Vision, 2008, pp. 1\u20138.",
            "year": 2008
        },
        {
            "authors": [
                "H.M. Spencer",
                "J.M. Rodgers",
                "J.M. Hoffman"
            ],
            "title": "Optical design of a panoramic, wide spectral band, infrared fisheye lens",
            "venue": "International Optical Design Conference, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "I.L. Kecskes",
                "E.M. Engel",
                "C. Wolfe",
                "G. Thomson"
            ],
            "title": "Low-cost panoramic infrared surveillance system",
            "venue": "Defense + Security, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.M. Nichols",
                "J.R. Waterman",
                "R.T. Menon",
                "J.W. Devitt"
            ],
            "title": "Performance characteristics of a submarine panoramic infrared imaging sensor",
            "venue": "Defense + Commercial Sensing, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "X. Zeng",
                "J. Lan",
                "X. Gao"
            ],
            "title": "The design of a catadioptric omnidirectional thermal imaging system based on the combination of genetic algorithm and gradient descent",
            "venue": "Optics and Laser Technology, vol. 122, p. 105861, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Aburmad"
            ],
            "title": "Panoramic thermal imaging: challenges and tradeoffs",
            "venue": "Defense + Security Symposium, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "R. Qiu",
                "W. Dou",
                "J. Kan",
                "K. Yu",
                "H. Wu",
                "J. Yang"
            ],
            "title": "Optical design of wide-angle catadioptric lens for LWIR earth sensors",
            "venue": "Applied Optics and Photonics China, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T.-Y. Lim",
                "S.-C. Park"
            ],
            "title": "Design of a catadioptric system with corrected color aberration and flat petzval curvature using a graphically symmetric method",
            "venue": "Current Optics and Photonics, vol. 2, no. 4, pp. 324\u2013331, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Gimkiewicz",
                "C. Urban",
                "E. Innerhofer",
                "P. Ferrat",
                "S. Neukom",
                "G. Vanstraelen",
                "P. Seitz"
            ],
            "title": "Ultra-miniature catadioptrical system for an omnidirectional camera",
            "venue": "SPIE Photonics Europe, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "W. St\u00fcrzl",
                "D. Soccol",
                "J. Zeil",
                "N. Boeddeker",
                "M.V. Srinivasan"
            ],
            "title": "Rugged, obstruction-free, mirror-lens combination for panoramic imaging.",
            "venue": "Applied Optics,",
            "year": 2008
        },
        {
            "authors": [
                "P.-H. Huang",
                "M.-F. Chen",
                "Y.-H. Chen",
                "T.-M. Huang",
                "C.-Y. Chan"
            ],
            "title": "Geometric calibration of omnidirectional images for panoramic total internal reflection lens",
            "venue": "Frontiers in Optics, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M. Aikio",
                "J.-T. M\u00e4kinen"
            ],
            "title": "Omnidirectional lens captures 360 degree panoramic view",
            "venue": "Applied Industrial Optics: Spectroscopy, Imaging and Metrology, 2015, pp. AITh2F\u20131.",
            "year": 2015
        },
        {
            "authors": [
                "M. Aikio"
            ],
            "title": "Omnidirectional lenses for low cost laser scanners",
            "venue": "Advanced Microsystems for Automotive Applications 2011, 2011, pp. 167\u2013174.",
            "year": 2011
        },
        {
            "authors": [
                "W. St\u00fcrzl",
                "N. Boeddeker",
                "L. Dittmar",
                "M. Egelhaaf"
            ],
            "title": "Mimicking honeybee eyes with a 280 degrees field of view catadioptric imaging system.",
            "venue": "Bioinspiration & Biomimetics,",
            "year": 2010
        },
        {
            "authors": [
                "R.C.C. Wang",
                "M.J. Deen",
                "D. Armstrong",
                "Q. Fang"
            ],
            "title": "Development of a catadioptric endoscope objective with forward and side views.",
            "venue": "Journal of Biomedical Optics,",
            "year": 2011
        },
        {
            "authors": [
                "J. Ma",
                "M. Simkulet",
                "J. Smith"
            ],
            "title": "C-view omnidirectional endoscope for minimally invasive surgery/diagnostics",
            "venue": "Medical Imaging 2007: Visualization and Image-Guided Procedures, vol. 6509, 2007, p. 65090C.",
            "year": 2007
        },
        {
            "authors": [
                "D. Cheng",
                "C. Gong",
                "C. Xu",
                "Y. Wang"
            ],
            "title": "Design of an ultrawide angle catadioptric lens with an annularly stitched aspherical surface.",
            "venue": "Optics Express,",
            "year": 2016
        },
        {
            "authors": [
                "F. Zhou",
                "X. Chen",
                "H. Tan"
            ],
            "title": "Double distortion correction method in a catadioptric vision system with a conic mirror.",
            "venue": "Journal of the Optical Society of America. A, Optics, Image Science, and Vision, vol",
            "year": 2017
        },
        {
            "authors": [
                "Z. Ye",
                "H.-B. Wang",
                "J. Xiong",
                "K. Wang"
            ],
            "title": "Ghost panorama using a convex mirror",
            "venue": "Optics Letters, vol. 46, no. 21, pp. 5389\u20135392, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yang",
                "B. Hu",
                "X. Zhang",
                "F. Liu",
                "L. Yan"
            ],
            "title": "Design of low-lightlevel panoramic imaging system",
            "venue": "Journal of Applied Optics, vol. 36, pp. 24\u201328, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "O. Furxhi",
                "J. Frascati",
                "R.G. Driggers"
            ],
            "title": "Design, demonstration and testing of low F-number LWIR panoramic imaging relay optics",
            "venue": "Defense + Security, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "E.A. Tsyganok",
                "X.-P. Xu",
                "N. Zhang",
                "J. Lv"
            ],
            "title": "A novel car panoramic system based on catadioptric structure with dual-band operating mode",
            "venue": "SPIE/COS Photonics Asia, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Wang"
            ],
            "title": "Optical design for dual spectral panoramic imaging system applied in corona detector",
            "venue": "Acta Photonica Sinica, vol. 39, pp. 1770\u2013 1774, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "D.-M. C\u00f3rdova-Esparza",
                "J.R. Terven",
                "J.-A. Romero-Gonz\u00e1lez",
                "A. Ramirez-Pedraza"
            ],
            "title": "Three-dimensional reconstruction of indoor and outdoor environments using a stereo catadioptric system",
            "venue": "Applied Sciences, vol. 10, p. 8851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y.J. Ju",
                "K. Lee",
                "J.H. Jo"
            ],
            "title": "Optical design of a reflective omnidirectional optical system to be used simultaneously in visible and lwir wavelength range",
            "venue": "Optical Systems Design, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y.J. Ju",
                "J.H. Jo",
                "J.M. Ryu"
            ],
            "title": "Optical design of reflecting omnidirectional zoom optical system with peripheral half-field of view from 110\u00b0 to 72\u00b0 for day and night surveillance",
            "venue": "Optik, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L.C. Kogos",
                "Y. Li",
                "J. Liu",
                "Y. Li",
                "L. Tian",
                "R. Paiella"
            ],
            "title": "Plasmonic ommatidia for lensless compound-eye vision",
            "venue": "Nature Communications, vol. 11, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Viollet",
                "S. Godiot",
                "R. Leitel",
                "W. Buss",
                "P. Breugnon",
                "M. Menouni",
                "R. Juston",
                "F. Expert",
                "F. Colonnier",
                "G. L\u2019Eplattenier",
                "A. Br\u00fcckner",
                "F. Kraze",
                "H.A. Mallot",
                "N.H. Franceschini",
                "R. Pericet-Camara",
                "F. Ruffier",
                "D. Floreano"
            ],
            "title": "Hardware architecture and cutting-edge assembly process of a tiny curved compound eye",
            "venue": "Sensors, vol. 14, pp. 21 702 \u2013 21 721, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S.J. Olivas",
                "A. Arianpour",
                "I. Stamenov",
                "R. Morrison",
                "R.A. Stack",
                "A.R. Johnson",
                "I.P. Agurok",
                "J.E. Ford"
            ],
            "title": "Image processing for cameras with fiber bundle image relay",
            "venue": "Applied Optics, vol. 54, no. 5, pp. 1124\u20131137, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Kinoshita",
                "K. Hoshino",
                "K. Matsumoto",
                "I. Shimoyama"
            ],
            "title": "Thin compound eye camera with a zooming function by reflective optics",
            "venue": "18th IEEE International Conference on Micro Electro Mechanical Systems, 2005. MEMS 2005., pp. 235\u2013238, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "R. Ueno",
                "K. Suzuki",
                "M. Kobayashi",
                "H. Kwon",
                "H. Honda",
                "H. Funaki"
            ],
            "title": "Compound-eye camera module as small as 8.5 \u00d7 8.5 \u00d7 6.0 mm for 26 k-resolution depth map and 2-mpix 2d imaging",
            "venue": "IEEE Photonics Journal, vol. 5, pp. 6 801 212\u20136 801 212, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Wu",
                "T. Jiang",
                "G. Zhang",
                "B. Schoenemann",
                "F. Neri",
                "M. Zhu",
                "C. Bu",
                "J. Han",
                "K.-D. Kuhnert"
            ],
            "title": "Artificial compound eye: a survey of the state-of-the-art",
            "venue": "Artificial Intelligence Review, vol. 48, pp. 573\u2013603, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Cheng",
                "J. Cao",
                "Y. Zhang",
                "Q. Hao"
            ],
            "title": "Review of state-ofthe-art artificial compound eye imaging systems",
            "venue": "Bioinspiration & Biomimetics, vol. 14, no. 3, p. 031002, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W.-L. Liang",
                "J.-G. Pan",
                "G.-D.J. Su"
            ],
            "title": "One-lens camera using a biologically based artificial compound eye with multiple focal lengths",
            "venue": "Optica, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sato",
                "T. Ebihara",
                "K. Mizutani",
                "N. Wakatsuki"
            ],
            "title": "Design of a wide-angle compound eye acoustic lens system suitable for multiuser underwater acoustic communication",
            "venue": "Japanese Journal of Applied Physics, vol. 60, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Zhang",
                "Q. Wu",
                "C. Liu",
                "T. Wang",
                "H. Zhang",
                "J. Wang",
                "Y. Ding",
                "J. Chi",
                "W. Xu",
                "Y. Xiang"
            ],
            "title": "Bio-inspired spherical compound eye camera for simultaneous wide-band and large field of view imaging",
            "venue": "Optics Express, vol. 30, no. 12, pp. 20 952\u201320 962, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H.L. Phan",
                "J. Yi",
                "J. Bae",
                "H. Ko",
                "S. Lee",
                "D. Cho",
                "J.-M. Seo",
                "K. in Koo"
            ],
            "title": "Artificial compound eye systems and their application: A review",
            "venue": "Micromachines, vol. 12, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H.S. Son",
                "D.L. Marks",
                "E. Tremblay",
                "J.E. Ford",
                "J. Hahn",
                "R.A. Stack",
                "A. Johnson",
                "P. McLaughlin",
                "J.M. Shaw",
                "J. Kim"
            ],
            "title": "A multiscale, wide field, gigapixel camera",
            "venue": "Computational Optical Sensing and Imaging, 2011, p. JTuE2.",
            "year": 2011
        },
        {
            "authors": [
                "D.J. Brady",
                "M.E. Gehm",
                "R.A. Stack",
                "D.L. Marks",
                "D.S. Kittle",
                "D.R. Golish",
                "E. Vera",
                "S.D. Feller"
            ],
            "title": "Multiscale gigapixel photography",
            "venue": "Nature, vol. 486, pp. 386\u2013389, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "I.N. Stamenov",
                "I.P. Agurok",
                "J.E. Ford"
            ],
            "title": "Optimization of two-glass monocentric lenses for compact panoramic imagers: general aberration analysis and specific designs.",
            "venue": "Applied Optics,",
            "year": 2012
        },
        {
            "authors": [
                "J.E. Ford"
            ],
            "title": "System optimization of compact monocentric lens imagers",
            "venue": "Frontiers in Optics, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J.E. Ford",
                "I.N. Stamenov",
                "S. Karbasi",
                "A. Arianpour",
                "N. Motamedi",
                "I.P. Agurok",
                "R.A. Stack",
                "A.R. Johnson",
                "R.L. Morrison",
                "J. Mott",
                "E.K. Martin",
                "C. LaReau",
                "B. Giffel",
                "J. Pessin",
                "R. Tennill",
                "P. Onorato"
            ],
            "title": "Panoramic imaging via curved fiber bundles",
            "venue": "Rundbrief Der Gifachgruppe 5.10 Informationssystem-architekturen, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "I.N. Stamenov",
                "I.P. Agurok",
                "J.E. Ford"
            ],
            "title": "Optimization of highperformance monocentric lenses.",
            "venue": "Applied Optics,",
            "year": 2013
        },
        {
            "authors": [
                "R. Kingslake"
            ],
            "title": "A history of the photographic lens",
            "year": 1989
        },
        {
            "authors": [
                "I.N. Stamenov",
                "I.P. Agurok",
                "J.E. Ford"
            ],
            "title": "Capabilities of monocentric objective lenses",
            "venue": "Rundbrief Der Gi-fachgruppe 5.10 Informationssystem-architekturen, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "I. Stamenov"
            ],
            "title": "Panoramic monocentric lens imaging",
            "venue": "University of California, San Diego,",
            "year": 2014
        },
        {
            "authors": [
                "I.N. Stamenov",
                "A. Arianpour",
                "S.J. Olivas",
                "I.P. Agurok",
                "A.R. Johnson",
                "R.A. Stack",
                "R.L. Morrison",
                "J.E. Ford"
            ],
            "title": "Panoramic monocentric imaging using fiber-coupled focal planes.",
            "venue": "Optics Express,",
            "year": 2014
        },
        {
            "authors": [
                "W. Pang",
                "D.J. Brady"
            ],
            "title": "Galilean monocentric multiscale optical systems.",
            "venue": "Optics Express,",
            "year": 2017
        },
        {
            "authors": [
                "G. Schuster",
                "W.M. Mellette",
                "J.E. Ford"
            ],
            "title": "Folded monocentric imager with deformable mirror focus.",
            "venue": "Applied Optics,",
            "year": 2017
        },
        {
            "authors": [
                "F. Liu",
                "Y. Wei",
                "P. li Han",
                "J.-W. Liu",
                "X. Shao"
            ],
            "title": "Design of monocentric wide field-of-view and high-resolution computational imaging system",
            "venue": "Acta Physica Sinica, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G.M. Schuster",
                "D.G. Dansereau",
                "G. Wetzstein",
                "J.E. Ford"
            ],
            "title": "Panoramic single-aperture multi-sensor light field camera",
            "venue": "Optics Express, vol. 27, no. 26, pp. 37 257\u201337 273, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Jin",
                "K. Li",
                "C. Li",
                "X. Sun"
            ],
            "title": "Point spread function for the wide-field-of-view plenoptic cameras.",
            "venue": "Optics Express,",
            "year": 2021
        },
        {
            "authors": [
                "E. Simioni",
                "C. Pernechele",
                "C. Re",
                "L. Lessio",
                "G. Cremonese"
            ],
            "title": "Geometrical calibration for the panrover: A stereo omnidirectional system for planetary rover",
            "venue": "ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 1151\u20131158, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Gong",
                "D. Cheng",
                "C. Xu",
                "Y. Wang"
            ],
            "title": "Design of a novel panoramic lens without central blindness",
            "venue": "International Conference on Optical Instruments and Technology, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Opromolla",
                "G. Fasano",
                "G. Rufino",
                "M. Grassi",
                "C. Pernechele",
                "C. Dionisio"
            ],
            "title": "A new star tracker concept for satellite attitude determination based on a multi-purpose panoramic camera",
            "venue": "Acta Astronautica, vol. 140, pp. 166\u2013175, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B.A. Palmer",
                "G. Taylor",
                "V. Brumfeld",
                "D. Gur",
                "M. Shemesh",
                "N. Elad",
                "A. Osherov",
                "D. Oron",
                "S. Weiner",
                "L. Addadi"
            ],
            "title": "The image-forming mirror in the eye of the scallop",
            "venue": "Science, vol. 358, pp. 1172 \u2013 1175, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Yang",
                "X. Hu",
                "L.M. Bergasa",
                "E. Romera",
                "K. Wang"
            ],
            "title": "PASS: Panoramic annular semantic segmentation",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 10, pp. 4171\u20134185, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Greguss"
            ],
            "title": "Panoramic security",
            "venue": "Holographic Optical Security Systems, vol. 1509, 1991, pp. 55\u201366.",
            "year": 1991
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Panoramic imaging block for three-dimensional space",
            "venue": "Jan. 28 1986, US Patent 4,566,763.",
            "year": 1986
        },
        {
            "authors": [
                "I. Powell"
            ],
            "title": "Panoramic lens",
            "venue": "Applied Optics, vol. 33, no. 31, pp. 7356\u2013 7361, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "V.A. Solomatin"
            ],
            "title": "A panoramic video camera",
            "venue": "Journal of Optical Technology, vol. 74, pp. 815\u2013817, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "Z. Huang",
                "J. Bai",
                "X.Y. Hou"
            ],
            "title": "Design of panoramic stereo imaging with single optical system.",
            "venue": "Optics Express,",
            "year": 2012
        },
        {
            "authors": [
                "Z. Huang",
                "J. Bai",
                "T. Lu",
                "X.Y. Hou"
            ],
            "title": "Stray light analysis and suppression of panoramic annular lens.",
            "venue": "Optics Express,",
            "year": 2013
        },
        {
            "authors": [
                "W. Zhu",
                "M.S. Zhang",
                "X. Kang",
                "L. Ren",
                "L. Tian"
            ],
            "title": "Catadioptric lens optical simulation and unwrapped distortion correction algorithm",
            "venue": "Other Conferences, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "D.R. Matthys",
                "J.A. Gilbert",
                "P. Greguss"
            ],
            "title": "Endoscopic measurement using radial metrology with digital correlation",
            "venue": "Optical Engineering, vol. 30, pp. 1455\u20131460, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "D.R. Matthys",
                "J.A. Gilbert",
                "J.T. Puliparambil"
            ],
            "title": "Endoscopic inspection using a panoramic annular lens",
            "venue": "SPIE Optics + Photonics, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "J.T. Puliparambil"
            ],
            "title": "Panoramic imaging and holographic interferometry using a panoramic annular lens",
            "venue": "Ph.D. dissertation, Marquette University, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "D. Hui",
                "M. Zhang",
                "Z. Geng",
                "Y. Zhang",
                "J. Duan",
                "A. Shi",
                "L. Hui",
                "Q. Fang",
                "Y. Liu"
            ],
            "title": "Designs for high performance PAL-based imaging systems.",
            "venue": "Applied Optics,",
            "year": 2012
        },
        {
            "authors": [
                "Q. Liu",
                "J. Bai",
                "Y. Luo"
            ],
            "title": "Design of high resolution panoramic endoscope imaging system based on freeform surface",
            "venue": "Journal of Physics: Conference Series, vol. 680, no. 1, 2016, p. 012011.",
            "year": 2016
        },
        {
            "authors": [
                "C. Su",
                "X. Zhou",
                "H. Li",
                "Q. Yang",
                "Z. Wang",
                "X. Liu"
            ],
            "title": "360 deg fullparallax light-field display using panoramic camera.",
            "venue": "Applied Optics,",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhou",
                "J. Bai"
            ],
            "title": "Distortion control for panoramic annular lens with Q-type aspheres",
            "venue": "Photonics Asia, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Zhou",
                "J. Bai",
                "C. Wang",
                "X. Hou",
                "K. Wang"
            ],
            "title": "Comparison of two panoramic front unit arrangements in design of a super wide angle panoramic annular lens",
            "venue": "Applied Optics, vol. 55, no. 12, pp. 3219\u2013 3225, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H. Yu",
                "J. Zhou",
                "J. Li"
            ],
            "title": "Research on fov extension for eva astronaut based on panoramic annular optical system",
            "venue": "Manned Spaceflight, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Huang",
                "J. Bai"
            ],
            "title": "Analysis of the imaging performance of panoramic annular lens with conic conformal dome",
            "venue": "Applied Optics and Photonics China, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Wang",
                "Q. Xue"
            ],
            "title": "Optical system design of an atmospheric detector with nadir view and omnidirectional limb view.",
            "venue": "Applied Optics,",
            "year": 2017
        },
        {
            "authors": [
                "X. Wang",
                "X. Zhong",
                "R. Zhu",
                "F. Gao"
            ],
            "title": "Nadir and omnidirectional limb imaging spectrometer based on acousto-optic tunable filter",
            "venue": "Optics Communications, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "D. Feng",
                "Y. Xiang"
            ],
            "title": "Refractive-diffractive hybrid panoramic annular optical system design",
            "venue": "Laser & Optoelectronics Progress, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "X. Zhong",
                "R. Zhu",
                "F. Gao",
                "Z. Li"
            ],
            "title": "Extremely wide-angle lens with transmissive and catadioptric integration",
            "venue": "Applied Optics, vol. 58, no. 16, pp. 4381\u20134389, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Amani",
                "J. Bai",
                "X. Huang"
            ],
            "title": "Dual-view catadioptric panoramic system based on even aspheric elements",
            "venue": "Applied Optics, vol. 59, no. 25, pp. 7630\u20137637, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "V. Gruev",
                "T. York"
            ],
            "title": "High resolution ccd polarization imaging sensor",
            "venue": "Proceedings of the International Image Sensor Workshop, Sapporo, Japan, 2011, pp. 8\u201311.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Horita",
                "K. Shibata",
                "K. Maeda",
                "Y. Hayashi"
            ],
            "title": "Omni-directional polarization image sensor based on an omni-directional camera and a polarization filter",
            "venue": "2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance, 2009, pp. 280\u2013285.",
            "year": 2009
        },
        {
            "authors": [
                "S. Araki",
                "K. Maeda",
                "K. Shibata",
                "Y. Horita"
            ],
            "title": "High quality panoramic image generation using multiple panoramic annular lens images",
            "venue": "2010 IEEE International Conference on Image Processing, 2010, pp. 1213\u20131216.",
            "year": 2010
        },
        {
            "authors": [
                "S. Araki",
                "K. Shibata",
                "Y. Horita"
            ],
            "title": "Subjective verification of high quality panoramic image generated by multiple PAL images",
            "venue": "2012 IEEE International Conference on Consumer Electronics (ICCE), 2012, pp. 126\u2013127.",
            "year": 2012
        },
        {
            "authors": [
                "K. Shibata",
                "S. Araki",
                "K. Maeda",
                "Y. Horita"
            ],
            "title": "High-quality panoramic image generation using multiple pal images",
            "venue": "Electronics and Communications in Japan, vol. 97, pp. 58\u201366, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K. Katayama",
                "K. Shibata",
                "Y. Horita"
            ],
            "title": "Detection and tracking an object in omni-directional images using particle filter",
            "venue": "2012 Proceedings of SICE Annual Conference (SICE), pp. 1040\u20131043, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "D. Sun",
                "X. Huang",
                "K. Yang"
            ],
            "title": "A multimodal vision sensor for autonomous driving",
            "venue": "Counterterrorism, Crime Fighting, Forensics, and Surveillance Technologies III, vol. 11166, 2019, p. 111660L.",
            "year": 2019
        },
        {
            "authors": [
                "C. Xu",
                "D. Cheng",
                "Y. Wang"
            ],
            "title": "Optical design of a dual-channel twofocal-length system by utilizing azimuth property of pal structure",
            "venue": "SPIE/COS Photonics Asia, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Luo",
                "J. Bai",
                "Y. Yao"
            ],
            "title": "Design of vari-focal panoramic annular lenses based on alvarez surfaces",
            "venue": "Photonics Asia, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Bian",
                "T. Ma",
                "J. Zhang"
            ],
            "title": "A design of panoramic lens system to realize the projection from the local annular object field to rectangular image field by using freeform surfaces",
            "venue": "SPIE/COS Photonics Asia, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "L. Chen",
                "Q. Yuan",
                "J. Ye",
                "N. Xu",
                "X. Cao",
                "Z. Gao"
            ],
            "title": "Design of a compact dual-view endoscope based on a hybrid lens with annularly stitched aspheres",
            "venue": "Optics Communications, vol. 453, p. 124346, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Chen",
                "Q. Yuan",
                "N. Xu",
                "X. Cao",
                "Z. Gao"
            ],
            "title": "Compact dual-view endoscope imaging system based on annularly stitched aspheres",
            "venue": "Optics, Photonics and Digital Technologies for Imaging Applications VI, vol. 11353, 2020, p. 113531M.",
            "year": 2020
        },
        {
            "authors": [
                "D. Gledhill",
                "G.Y. Tian",
                "D. Taylor",
                "D. Clarke"
            ],
            "title": "3D reconstruction of a region of interest using structured light and stereo panoramic images",
            "venue": "Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004., 2004, pp. 1007\u20131012.",
            "year": 2004
        },
        {
            "authors": [
                "J. Wang",
                "J. Bai",
                "K. Wang",
                "S. Gao"
            ],
            "title": "Design of stereo imaging system with a panoramic annular lens and a convex mirror",
            "venue": "Optics Express, vol. 30, no. 11, pp. 19 017\u201319 029, 2022. IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, OCTOBER 2022 31",
            "year": 2022
        },
        {
            "authors": [
                "L. Li",
                "A.Y. Yi"
            ],
            "title": "Design and fabrication of a freeform microlens array for a compact large-field-of-view compound-eye camera.",
            "venue": "Applied Optics,",
            "year": 2012
        },
        {
            "authors": [
                "J. Ye",
                "L. Chen",
                "X. Li",
                "Q. Yuan",
                "Z. Gao"
            ],
            "title": "Review of optical freeform surface representation technique and its application",
            "venue": "Optical Engineering, vol. 56, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Duerr",
                "H. Thienpont"
            ],
            "title": "Freeform imaging systems: Fermat\u2019s principle unlocks \u201cfirst time right\u201d design",
            "venue": "Light, Science & Applications, vol. 10, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K.P. Thompson",
                "J.P. Rolland"
            ],
            "title": "Freeform optical surfaces: A revolution in imaging optical design",
            "venue": "Optics & Photonics News, vol. 23, pp. 30\u201335, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "S. Wills"
            ],
            "title": "Freeform optics: Notes from the revolution",
            "venue": "Optics & Photonics News, vol. 28, pp. 34\u201341, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Reimers",
                "A. Bauer",
                "K.P. Thompson",
                "J.P. Rolland"
            ],
            "title": "Freeform spectrometer enabling increased compactness",
            "venue": "Light, Science & Applications, vol. 6, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Bauer",
                "E.M. Schiesser",
                "J.P. Rolland"
            ],
            "title": "Starting geometry creation and design method for freeform optics",
            "venue": "Nature Communications, vol. 9, no. 1, pp. 1\u201311, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "F. Heide",
                "Q. Fu",
                "Y. Peng",
                "W. Heidrich"
            ],
            "title": "Encoded diffractive optics for full-spectrum computational imaging",
            "venue": "Scientific Reports, vol. 6, no. 1, pp. 1\u201310, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G.W. Forbes"
            ],
            "title": "Shape specification for axially symmetric optical surfaces.",
            "venue": "Optics Express,",
            "year": 2007
        },
        {
            "authors": [
                "D. Cheng",
                "X.-J. Chen",
                "C. Xu",
                "Y. Hu",
                "Y. tian Wang"
            ],
            "title": "Optical description and design method with annularly stitched aspheric surface.",
            "venue": "Applied Optics,",
            "year": 2015
        },
        {
            "authors": [
                "X. Shao",
                "F. Liu",
                "W. Li",
                "L. Yang",
                "S. Yang",
                "J. Liu"
            ],
            "title": "Latest progress in computational imaging technology and application",
            "venue": "Laser & Optoelectronics Progress, vol. 57, no. 2, p. 020001, 2020.",
            "year": 2000
        },
        {
            "authors": [
                "Y. Peng",
                "Q. Fu",
                "H. Amata",
                "S. Su",
                "F. Heide",
                "W. Heidrich"
            ],
            "title": "Computational imaging using lightweight diffractive-refractive optics.",
            "venue": "Optics Express,",
            "year": 2015
        },
        {
            "authors": [
                "Q. Jiang",
                "H. Shi",
                "L. Sun",
                "S. Gao",
                "K. Yang",
                "K. Wang"
            ],
            "title": "Annular computational imaging: Capture clear panoramic images through simple lens",
            "venue": "arXiv preprint arXiv:2206.06070, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V. Sitzmann",
                "S. Diamond",
                "Y. Peng",
                "X. Dun",
                "S.P. Boyd",
                "W. Heidrich",
                "F. Heide",
                "G. Wetzstein"
            ],
            "title": "End-to-end optimization of optics and image processing for achromatic extended depth of field and superresolution imaging",
            "venue": "ACM Transactions on Graphics (TOG), vol. 37, pp. 1 \u2013 13, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Sun",
                "J. Zhang",
                "X. Dun",
                "B. Ghanem",
                "Y. Peng",
                "W. Heidrich"
            ],
            "title": "End-to-end learned, optically coded super-resolution spad camera",
            "venue": "ACM Transactions on Graphics (TOG), vol. 39, pp. 1 \u2013 14, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Peng",
                "Q. Fu",
                "F. Heide",
                "W. Heidrich"
            ],
            "title": "The diffractive achromat full spectrum computational imaging with diffractive optics",
            "venue": "SIG- GRAPH ASIA 2016 Virtual Reality meets Physical Reality: Modelling and Simulating Virtual Humans and Environments, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "B. Qi",
                "W. Chen",
                "X. Dun",
                "X. Hao",
                "R. Wang",
                "X. Liu",
                "H. Li",
                "Y. Peng"
            ],
            "title": "All-day thin-lens computational imaging with scene-specific learning recovery.",
            "venue": "Applied Optics,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "L.G. Zimin",
                "J. Ji",
                "S. Ikezawa",
                "T. Ueda"
            ],
            "title": "Real scene capturing using spherical single-element lens camera and improved restoration algorithm for radially variant blur.",
            "venue": "Optics Express,",
            "year": 2012
        },
        {
            "authors": [
                "Q. Sun",
                "C. Wang",
                "Q. Fu",
                "X. Dun",
                "W. Heidrich"
            ],
            "title": "End-to-end complex lens design with differentiate ray tracing",
            "venue": "ACM Transactions on Graphics (TOG), vol. 40, pp. 1 \u2013 13, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W.T. Xie",
                "Y. Dai",
                "R. Wang",
                "K. Sumathy"
            ],
            "title": "Concentrated solar energy applications using fresnel lenses: A review",
            "venue": "Renewable & Sustainable Energy Reviews, vol. 15, pp. 2588\u20132606, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "X. Dun",
                "H. Ikoma",
                "G. Wetzstein",
                "Z. Wang",
                "X. Cheng",
                "Y. Peng"
            ],
            "title": "Learned rotationally symmetric diffractive achromat for full-spectrum computational imaging",
            "venue": "Optica, vol. 7, no. 8, pp. 913\u2013922, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Huo",
                "Y. Hu",
                "B. Cheng"
            ],
            "title": "History and application of diffractive optics technology",
            "venue": "Laser & Optoelectronics Progress, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Li",
                "W. Piyawattanametha",
                "Z. Qiu"
            ],
            "title": "Metalens-based miniaturized optical systems",
            "venue": "Micromachines, vol. 10, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "Y. Zhao",
                "L. Xing",
                "Z. He",
                "L. Sun"
            ],
            "title": "Broadband bifunctional luneburg\u2013fisheye lens based on anisotropic metasurface",
            "venue": "Scientific Reports, vol. 10, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.-H. Chen",
                "C.-W. Yen",
                "C.-C. Guo",
                "V.-C. Su",
                "C.-H. Kuan",
                "H.Y. Lin"
            ],
            "title": "Polarization-insensitive gan metalenses at visible wavelengths",
            "venue": "Scientific Reports, vol. 11, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.S. Solntsev",
                "G.S. Agarwal",
                "Y.S. Kivshar"
            ],
            "title": "Metasurfaces for quantum photonics",
            "venue": "Nature Photonics, vol. 15, pp. 327\u2013336, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Ali",
                "S.I. Aksu"
            ],
            "title": "A hybrid broadband metalens operating at ultraviolet frequencies",
            "venue": "Scientific Reports, vol. 11, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W.T. Chen",
                "A.Y. Zhu",
                "F. Capasso"
            ],
            "title": "Flat optics with dispersionengineered metasurfaces",
            "venue": "Nature Reviews Materials, pp. 1\u201317, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Engelberg",
                "U. Levy"
            ],
            "title": "The advantages of metalenses over diffractive lenses",
            "venue": "Nature Communications, vol. 11, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W.J. Padilla",
                "R.D. Averitt"
            ],
            "title": "Imaging with metamaterials",
            "venue": "Nature Reviews Physics, vol. 4, no. 2, pp. 85\u2013100, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "X. Zhang"
            ],
            "title": "Metamaterials: a new frontier of science and technology.",
            "venue": "Chemical Society Reviews,",
            "year": 2011
        },
        {
            "authors": [
                "Smith",
                "Padilla",
                "Vier",
                "Nemat-Nasser",
                "Schultz"
            ],
            "title": "Composite medium with simultaneously negative permeability and permittivity",
            "venue": "Physical Review Letters, vol. 84 18, pp. 4184\u20137, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "R.A. Shelby",
                "D.R. Smith",
                "S. Schultz"
            ],
            "title": "Experimental verification of a negative index of refraction",
            "venue": "Science, vol. 292, pp. 77 \u2013 79, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "N. Yu",
                "P. Genevet",
                "M.A. Kats",
                "F. Aieta",
                "J. Tetienne",
                "F. Capasso",
                "Z. Gaburro"
            ],
            "title": "Light propagation with phase discontinuities: Generalized laws of reflection and refraction",
            "venue": "Science, vol. 334, pp. 333 \u2013 337, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Khorasaninejad",
                "W.T. Chen",
                "R.C. Devlin",
                "J. Oh",
                "A.Y. Zhu",
                "F. Capasso"
            ],
            "title": "Metalenses at visible wavelengths: Diffraction-limited focusing and subwavelength resolution imaging",
            "venue": "Science, vol. 352, pp. 1190 \u2013 1194, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H. Lin",
                "Z. Xu",
                "G. Cao",
                "Y. Zhang",
                "J. Zhou",
                "Z. Wang",
                "Z. Wan",
                "Z. Liu",
                "K.P. Loh",
                "C. wei Qiu",
                "Q. Bao",
                "B. Jia"
            ],
            "title": "Diffraction-limited imaging with monolayer 2d material-based ultrathin flat lenses",
            "venue": "Light, Science & Applications, vol. 9, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "L. Huang",
                "H. M\u00fchlenbernd",
                "G. Li",
                "B.-F. Bai",
                "Q. Tan",
                "G. Jin",
                "C. wei Qiu",
                "S. Zhang",
                "T. Zentgraf"
            ],
            "title": "Dual-polarity plasmonic metalens for visible light",
            "venue": "Nature Communications, vol. 3, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "H. Chen",
                "A.J. Taylor",
                "N. Yu"
            ],
            "title": "A review of metasurfaces: physics and applications",
            "venue": "Reports on Progress in Physics. Physical Society, vol. 79 7, p. 076401, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "O. Quevedo-Teruel",
                "H. Chen",
                "A. D\u0131\u0301az-Rubio",
                "G. Gok",
                "A. Grbic",
                "G. Minatti",
                "E. Martini",
                "S. Maci",
                "G.V. Eleftheriades",
                "M. Chen",
                "N.I. Zheludev",
                "N. Papasimakis",
                "S.M. Choudhury",
                "Z.A. Kudyshev",
                "S. Saha",
                "H. Reddy",
                "A. Boltasseva",
                "V.M. Shalaev",
                "A.V. Kildishev",
                "D.F. Sievenpiper",
                "C. Caloz",
                "A. Al\u00fa",
                "Q. He",
                "L. Zhou",
                "G. Val\u00e9rio",
                "E. Rajo-Iglesias",
                "Z. Sipus",
                "F. Mesa",
                "R. Rodr\u0131\u0301guez-Berral",
                "F. Medina",
                "V. Asadchy",
                "S.A. Tretyakov",
                "C. Craeye"
            ],
            "title": "Roadmap on metasurfaces",
            "venue": "Journal of Optics, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang",
                "S. Divitt",
                "Q. Fan",
                "W. Zhu",
                "A. Agrawal",
                "Y. Lu",
                "T. Xu",
                "H.J. Lezec"
            ],
            "title": "Low-loss metasurface optics down to the deep ultraviolet region",
            "venue": "Light, Science & Applications, vol. 9, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Zheng",
                "Y. Liu",
                "H. Liu"
            ],
            "title": "Single-pixel imaging and metasurface imaging",
            "venue": "Infrared and Laser Engineering, vol. 50, no. 12, pp. 20 211 058\u20131, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Arbabi",
                "E. Arbabi",
                "S.M. Kamali",
                "Y. Horie",
                "S. hoon Han",
                "A. Faraon"
            ],
            "title": "Miniature optical planar camera based on a wide-angle metasurface doublet corrected for monochromatic aberrations",
            "venue": "Nature Communications, vol. 7, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "F. Balli",
                "M.A. Sultan",
                "S.K. Lami",
                "J.T. Hastings"
            ],
            "title": "A hybrid achromatic metalens",
            "venue": "Nature Communications, vol. 11, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C.-Y. Fan",
                "C.-P. Lin",
                "G.-D.J. Su"
            ],
            "title": "Ultrawide-angle and highefficiency metalens in hexagonal arrangement",
            "venue": "Scientific Reports, vol. 10, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W.T. Chen",
                "A.Y. Zhu",
                "V. Sanjeev",
                "M. Khorasaninejad",
                "Z. Shi",
                "E. Lee",
                "F. Capasso"
            ],
            "title": "A broadband achromatic metalens for focusing and imaging in the visible",
            "venue": "Nature Nanotechnology, vol. 13, pp. 220\u2013226, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Wang",
                "P.C. Wu",
                "V.-C. Su",
                "Y.C. Lai",
                "M.K. Chen",
                "H.Y. Kuo",
                "B.H. Chen",
                "Y.H. Chen",
                "T. Huang",
                "J.-H. Wang",
                "R.-M. Lin",
                "C.-H. Kuan",
                "T. Li",
                "Z. Wang",
                "S. ning Zhu",
                "D.P. Tsai"
            ],
            "title": "A broadband achromatic metalens in the visible",
            "venue": "Nature Nanotechnology, vol. 13, pp. 227\u2013232, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Chen",
                "W. Song",
                "J.-W. Chen",
                "J.-H. Wang",
                "Y.H. Chen",
                "B. Xu",
                "M.K. Chen",
                "H. Li",
                "B. Fang",
                "J. Chen",
                "H.Y. Kuo",
                "S. Wang",
                "D.P. Tsai",
                "S. Zhu",
                "T. Li"
            ],
            "title": "Spectral tomographic imaging with aplanatic metalens",
            "venue": "Light, Science & Applications, vol. 8, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Yang",
                "M.Y. Shalaginov",
                "S. An",
                "H. Zhang",
                "C. Rivero-Baleine",
                "T. Gu",
                "J. Hu"
            ],
            "title": "Wide field-of-view achromatic metalenses",
            "venue": "OSA Optical Design and Fabrication 2021 (Flat Optics, Freeform, IODC, OFT), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Hua",
                "Y. Wang",
                "S. Wang",
                "X. Zou",
                "Y. Zhou",
                "L. Li",
                "F. Yan",
                "X. Cao",
                "S. Xiao",
                "D.P. Tsai",
                "J. Han",
                "Z. Wang",
                "S. Zhu"
            ],
            "title": "Ultra-compact snapshot spectral light-field imaging",
            "venue": "Nature Communications, vol. 13, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Tseng",
                "S. Colburn",
                "J.E.M. Whitehead",
                "L. Huang",
                "S.-H. Baek",
                "A. Majumdar",
                "F. Heide"
            ],
            "title": "Neural nano-optics for high-quality thin lens imaging",
            "venue": "Nature Communications, vol. 12, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "Q. Chen",
                "W. Yang",
                "Z. Ji",
                "L. Jin",
                "X. Ma",
                "Q. Song",
                "A. Boltasseva",
                "J. Han",
                "V.M. Shalaev",
                "S. Xiao"
            ],
            "title": "High-efficiency broadband achromatic metalens for near-ir biological imaging window",
            "venue": "Nature Communications, vol. 12, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Pan",
                "Y. Fu",
                "M. Zheng",
                "H. Chen",
                "Y. Zang",
                "H. Duan",
                "Q. Li",
                "M. Qiu",
                "Y. Hu"
            ],
            "title": "Dielectric metalens for miniaturized imaging systems: progress and challenges",
            "venue": "Light, Science & Applications, vol. 11, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Groever",
                "W.T. Chen",
                "F. Capasso"
            ],
            "title": "Meta-lens doublet in the visible region.",
            "venue": "Nano Letters,",
            "year": 2017
        },
        {
            "authors": [
                "M. Jang",
                "Y. Horie",
                "A. Shibukawa",
                "J. Brake",
                "Y. Liu",
                "S.M. Kamali",
                "A. Arbabi",
                "H. Ruan",
                "A. Faraon",
                "C. Yang"
            ],
            "title": "Wavefront shaping with disorder-engineered metasurfaces",
            "venue": "Nature Photonics, vol. 12, no. 2, pp. 84\u201390, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Guo",
                "X. Ma",
                "M. Pu",
                "X. Li",
                "Z. Zhao",
                "X. Luo"
            ],
            "title": "High-efficiency and wide-angle beam steering based on catenary optical fields in ultrathin metalens",
            "venue": "Advanced Optical Materials, vol. 6, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Xu",
                "M. Cua",
                "E.H. Zhou",
                "Y. Horie",
                "A. Faraon",
                "C. Yang"
            ],
            "title": "Wideangular-range and high-resolution beam steering by a metasurfacecoupled phased array",
            "venue": "Optics Letters, vol. 43, no. 21, pp. 5255\u20135258, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Engelberg",
                "C. Zhou",
                "N. Mazurski",
                "J. Bar-David",
                "A. Kristensen",
                "U. Levy"
            ],
            "title": "Near-IR wide-field-of-view huygens metalens for outdoor imaging applications",
            "venue": "Nanophotonics, vol. 9, pp. 361 \u2013 370, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Yang",
                "X. Hu",
                "L.M. Bergasa",
                "E. Romera",
                "X. Huang",
                "D. Sun",
                "K. Wang"
            ],
            "title": "Can we PASS beyond the field of view? Panoramic annular semantic segmentation for real-world surrounding perception",
            "venue": "2019 IEEE Intelligent Vehicles Symposium (IV), 2019, pp. 446\u2013453.",
            "year": 2019
        },
        {
            "authors": [
                "L. Deng",
                "M. Yang",
                "Y. Qian",
                "C. Wang",
                "B. Wang"
            ],
            "title": "CNN based semantic segmentation for urban traffic scenes using fisheye camera",
            "venue": "2017 IEEE Intelligent Vehicles Symposium (IV), 2017, pp. 231\u2013236.",
            "year": 2017
        },
        {
            "authors": [
                "L. Deng",
                "M. Yang",
                "H. Li",
                "T. Li",
                "B. Hu",
                "C. Wang"
            ],
            "title": "Restricted deformable convolution-based road scene semantic segmentation using surround view cameras",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 10, pp. 4350\u20134362, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Ma",
                "J. Zhang",
                "K. Yang",
                "A. Roitberg",
                "R. Stiefelhagen"
            ],
            "title": "DenseP- ASS: Dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange",
            "venue": "2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 2021, pp. 2766\u20132772.",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "C. Ma",
                "K. Yang",
                "A. Roitberg",
                "K. Peng",
                "R. Stiefelhagen"
            ],
            "title": "Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised domain adaptation",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 7, pp. 9478\u20139491, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Jaus",
                "K. Yang",
                "R. Stiefelhagen"
            ],
            "title": "Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning",
            "venue": "2021 IEEE Intelligent Vehicles Symposium (IV), 2021, pp. 1421\u20131427.",
            "year": 2021
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3431\u20133440.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Ye",
                "K. Yang",
                "K. Xiang",
                "J. Wang",
                "K. Wang"
            ],
            "title": "Universal semantic segmentation for fisheye urban driving images",
            "venue": "2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2020, pp. 648\u2013655.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yang",
                "X. Hu",
                "H. Chen",
                "K. Xiang",
                "K. Wang",
                "R. Stiefelhagen"
            ],
            "title": "DS-PASS: Detail-sensitive panoramic annular semantic segmentation through SwaftNet for surrounding sensing",
            "venue": "2020 IEEE Intelligent Vehicles Symposium (IV), 2020, pp. 457\u2013464.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yang",
                "X. Hu",
                "Y. Fang",
                "K. Wang",
                "R. Stiefelhagen"
            ],
            "title": "Omnisupervised omnidirectional semantic segmentation",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 2, pp. 1184\u20131199, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Yang",
                "X. Hu",
                "R. Stiefelhagen"
            ],
            "title": "Is context-aware CNN ready for the surroundings? Panoramic semantic segmentation in the wild",
            "venue": "IEEE Transactions on Image Processing, vol. 30, pp. 1866\u20131881, 2021.",
            "year": 1866
        },
        {
            "authors": [
                "K. Yang",
                "J. Zhang",
                "S. Rei\u00df",
                "X. Hu",
                "R. Stiefelhagen"
            ],
            "title": "Capturing omni-range context for omnidirectional segmentation",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 1376\u20131386.",
            "year": 2021
        },
        {
            "authors": [
                "L. Sun",
                "J. Wang",
                "K. Yang",
                "K. Wu",
                "X. Zhou",
                "K. Wang",
                "J. Bai"
            ],
            "title": "Aerial-PASS: Panoramic annular scene segmentation in drone videos",
            "venue": "2021 European Conference on Mobile Robots (ECMR), 2021, pp. 1\u20136.",
            "year": 2021
        },
        {
            "authors": [
                "A. Jaus",
                "K. Yang",
                "R. Stiefelhagen"
            ],
            "title": "Panoramic panoptic segmentation: Insights into surrounding parsing for mobile agents via unsupervised contrastive learning",
            "venue": "arXiv preprint arXiv:2206.10711, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Mei",
                "A.Z. Zhu",
                "X. Yan",
                "H. Yan",
                "S. Qiao",
                "Y. Zhu",
                "L.-C. Chen",
                "H. Kretzschmar",
                "D. Anguelov"
            ],
            "title": "Waymo open dataset: Panoramic video panoptic segmentation",
            "venue": "European Conference on Computer Vision (ECCV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "K. Yang",
                "C. Ma",
                "S. Rei\u00df",
                "K. Peng",
                "R. Stiefelhagen"
            ],
            "title": "Bending reality: Distortion-aware transformers for adapting to panoramic semantic segmentation",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 16 917\u2013 16 927.",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "K. Yang",
                "H. Shi",
                "S. Rei\u00df",
                "K. Peng",
                "C. Ma",
                "H. Fu",
                "K. Wang",
                "R. Stiefelhagen"
            ],
            "title": "Behind every domain there is a shift: Adapting distortion-aware vision transformers for panoramic semantic segmentation",
            "venue": "arXiv preprint arXiv:2207.11860, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Zheng",
                "C. Lin",
                "L. Nie",
                "K. Liao",
                "Z. Shen",
                "Y. Zhao"
            ],
            "title": "Complementary bi-directional feature compression for indoor 360\u00b0 semantic segmentation with self-distillation",
            "venue": "arXiv preprint arXiv:2207.02437, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Yogamani",
                "C. Witt",
                "H. Rashed",
                "S. Nayak",
                "S. Mansoor",
                "P. Varley",
                "X. Perrotton",
                "D. O\u2019Dea",
                "P. P\u00e9rez",
                "C. Hughes",
                "J. Horgan",
                "G. Sistu",
                "S. Chennupati",
                "M. Uric\u00e1r",
                "S. Milz",
                "M. Simon",
                "K. Amende"
            ],
            "title": "Wood- Scape: A multi-task, multi-camera fisheye dataset for autonomous driving",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9307\u20139317.",
            "year": 2019
        },
        {
            "authors": [
                "V.R. Kumar",
                "S. Yogamani",
                "H. Rashed",
                "G. Sitsu",
                "C. Witt",
                "I. Leang",
                "S. Milz",
                "P. M\u00e4der"
            ],
            "title": "OmniDet: Surround view cameras based multitask visual perception network for autonomous driving",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 2830\u20132837, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Eising",
                "J. Horgan",
                "S. Yogamani"
            ],
            "title": "Near-field perception for lowspeed vehicle automation using surround-view fisheye cameras",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "O. Ahmad",
                "F. Lecue"
            ],
            "title": "FisheyeHDK: Hyperbolic deformable kernel learning for ultra-wide field-of-view image recognition",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2022, pp. 5968\u20135975.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xu",
                "K. Wang",
                "K. Yang",
                "D. Sun",
                "J. Fu"
            ],
            "title": "Semantic segmentation of panoramic images using a synthetic dataset",
            "venue": "Artificial Intelligence and Machine Learning in Defense Applications, vol. 11169, 2019, p. 111690B.",
            "year": 2019
        },
        {
            "authors": [
                "S. Orhan",
                "Y. Bastanlar"
            ],
            "title": "Semantic segmentation of outdoor panoramic images",
            "venue": "Signal, Image and Video Processing, vol. 16, no. 3, pp. 643\u2013650, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Liu",
                "S. Wang",
                "Y. Guo",
                "Y. He",
                "H. Xue"
            ],
            "title": "Pano-SfMLearner: Selfsupervised multi-task learning of depth and semantics in panoramic videos",
            "venue": "IEEE Signal Processing Letters, vol. 28, pp. 832\u2013836, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Zhang",
                "S. Liwicki",
                "W. Smith",
                "R. Cipolla"
            ],
            "title": "Orientation-aware semantic segmentation on icosahedron spheres",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 3532\u2013 3540.",
            "year": 2019
        },
        {
            "authors": [
                "A.R. Sekkat",
                "Y. Dupuis",
                "P. Vasseur",
                "P. Honeine"
            ],
            "title": "The OmniScape dataset",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 1603\u20131608.",
            "year": 2020
        },
        {
            "authors": [
                "A.R. Sekkat",
                "Y. Dupuis",
                "P. Honeine",
                "P. Vasseur"
            ],
            "title": "A comparative study of semantic segmentation of omnidirectional images from a motorcycle perspective",
            "venue": "Scientific Reports, vol. 12, no. 1, pp. 1\u201314, 2022. IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, OCTOBER 2022 33",
            "year": 2022
        },
        {
            "authors": [
                "W. Li",
                "Y. Lai",
                "L. Xu",
                "Y. Xiangli",
                "J. Yu",
                "C. He",
                "G.-S. Xia",
                "D. Lin"
            ],
            "title": "OmniCity: Omnipotent city understanding with multi-level and multiview images",
            "venue": "arXiv preprint arXiv:2208.00928, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "F.-E. Wang",
                "Y.-H. Yeh",
                "M. Sun",
                "W.-C. Chiu",
                "Y.-H. Tsai"
            ],
            "title": "BiFuse: Monocular 360 depth estimation via bi-projection fusion",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 459\u2013468.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Lai",
                "D. Chen",
                "K. Su"
            ],
            "title": "OlaNet: Self-supervised 360\u00b0 depth estimation with effective distortion-aware view synthesis and L1 smooth regularization",
            "venue": "2021 IEEE International Conference on Multimedia and Expo (ICME), 2021, pp. 1\u20136.",
            "year": 2021
        },
        {
            "authors": [
                "N. Zioulis",
                "A. Karakottas",
                "D. Zarpalas",
                "P. Daras"
            ],
            "title": "OmniDepth: Dense depth estimation for indoors spherical panoramas",
            "venue": "European Conference on Computer Vision (ECCV), 2018, pp. 448\u2013465.",
            "year": 2018
        },
        {
            "authors": [
                "K. Tateno",
                "N. Navab",
                "F. Tombari"
            ],
            "title": "Distortion-aware convolutional filters for dense prediction in panoramic images",
            "venue": "European Conference on Computer Vision (ECCV), 2018, pp. 707\u2013722.",
            "year": 2018
        },
        {
            "authors": [
                "G.P. de La Garanderie",
                "A.A. Abarghouei",
                "T.P. Breckon"
            ],
            "title": "Eliminating the blind spot: Adapting 3D object detection and monocular depth estimation to 360\u00b0 panoramic imagery",
            "venue": "European Conference on Computer Vision (ECCV), 2018, pp. 789\u2013807.",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhou",
                "K. Wang",
                "K. Yang"
            ],
            "title": "PADENet: An efficient and robust panoramic monocular depth estimation network for outdoor scenes",
            "venue": "2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "L. Jin",
                "Y. Xu",
                "J. Zheng",
                "J. Zhang",
                "R. Tang",
                "S. Xu",
                "J. Yu",
                "S. Gao"
            ],
            "title": "Geometric structure based and regularized depth estimation from 360 indoor imagery",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 886\u2013895.",
            "year": 2020
        },
        {
            "authors": [
                "C. Sun",
                "M. Sun",
                "H.-T. Chen"
            ],
            "title": "HoHoNet: 360 indoor holistic understanding with latent horizontal features",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 2573\u20132582.",
            "year": 2021
        },
        {
            "authors": [
                "G. Pintore",
                "M. Agus",
                "E. Almansa",
                "J. Schneider",
                "E. Gobbetti"
            ],
            "title": "SliceNet: deep dense depth estimation from a single indoor panorama using a slice-based representation",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 11 531\u2013 11 540.",
            "year": 2021
        },
        {
            "authors": [
                "C. Zhuang",
                "Z. Lu",
                "Y. Wang",
                "J. Xiao",
                "Y. Wang"
            ],
            "title": "ACDNet: Adaptively combined dilated convolution for monocular panorama depth estimation",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2022, pp. 3653\u20133661.",
            "year": 2022
        },
        {
            "authors": [
                "H. Jiang",
                "Z. Sheng",
                "S. Zhu",
                "Z. Dong",
                "R. Huang"
            ],
            "title": "UniFuse: Unidirectional fusion for 360\u00b0 panorama depth estimation",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 1519\u20131526, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Bai",
                "S. Lai",
                "H. Qin",
                "J. Guo",
                "Y. Guo"
            ],
            "title": "GLPanoDepth: Global-tolocal panoramic depth estimation",
            "venue": "arXiv preprint arXiv:2202.02796, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F.-E. Wang",
                "Y.-H. Yeh",
                "Y.-H. Tsai",
                "W.-C. Chiu",
                "M. Sun"
            ],
            "title": "Bi- Fuse++: Self-supervised and efficient bi-projection fusion for 360\u00b0 depth estimation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "Y. Guo",
                "Z. Yan",
                "X. Huang",
                "Y. Duan",
                "L. Ren"
            ],
            "title": "OmniFusion: 360 monocular depth estimation via geometry-aware fusion",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 2801\u20132810.",
            "year": 2022
        },
        {
            "authors": [
                "M. Rey-Area",
                "M. Yuan",
                "C. Richardt"
            ],
            "title": "360MonoDepth: Highresolution 360\u00b0 monocular depth estimation",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 3762\u20133772.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Shen",
                "C. Lin",
                "K. Liao",
                "L. Nie",
                "Z. Zheng",
                "Y. Zhao"
            ],
            "title": "PanoFormer: Panorama transformer for indoor 360\u00b0 depth estimation",
            "venue": "European Conference on Computer Vision (ECCV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Zioulis",
                "A. Karakottas",
                "D. Zarpalas",
                "F. Alvarez",
                "P. Daras"
            ],
            "title": "Spherical view synthesis for self-supervised 360 depth estimation",
            "venue": "2019 International Conference on 3D Vision (3DV), 2019, pp. 690\u2013 699.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhou",
                "K. Yang",
                "K. Wang"
            ],
            "title": "Panoramic depth estimation via supervised and unsupervised learning in indoor scenes",
            "venue": "Applied Optics, vol. 60, no. 26, pp. 8188\u20138197, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Yun",
                "H.-J. Lee",
                "C.E. Rhee"
            ],
            "title": "Improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2022, pp. 3224\u20133233.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Yan",
                "X. Li",
                "K. Wang",
                "Z. Zhang",
                "J. Li",
                "J. Yang"
            ],
            "title": "Multi-modal masked pre-training for monocular panoramic depth completion",
            "venue": "European Conference on Computer Vision (ECCV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Liu",
                "G. Zhang",
                "J. Wang",
                "S. Zhao"
            ],
            "title": "Cross-modal 360\u00b0 depth completion and reconstruction for large-scale indoor environment",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Cheng",
                "K. Wang",
                "S. Lin",
                "W. Hu",
                "K. Yang",
                "X. Huang",
                "H. Li",
                "D. Sun",
                "J. Bai"
            ],
            "title": "Panoramic annular localizer: Tackling the variation challenges of outdoor localization using panoramic annular images and active deep descriptors",
            "venue": "2019 IEEE Intelligent Transportation Systems Conference (ITSC), 2019, pp. 920\u2013925.",
            "year": 2019
        },
        {
            "authors": [
                "A. Iscen",
                "G. Tolias",
                "Y. Avrithis",
                "T. Furon",
                "O. Chum"
            ],
            "title": "Panorama to panorama matching for location recognition",
            "venue": "2017 ACM International Conference on Multimedia Retrieval, 2017, pp. 392\u2013396.",
            "year": 2017
        },
        {
            "authors": [
                "S. Siva",
                "H. Zhang"
            ],
            "title": "Omnidirectional multisensory perception fusion for long-term place recognition",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 5175\u20135181.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Fang",
                "K. Wang",
                "R. Cheng",
                "K. Yang"
            ],
            "title": "CFVL: A coarse-tofine vehicle localizer with omnidirectional perception across severe appearance variations",
            "venue": "2020 IEEE Intelligent Vehicles Symposium (IV), 2020, pp. 1885\u20131891.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Fang",
                "K. Yang",
                "R. Cheng",
                "L. Sun",
                "K. Wang"
            ],
            "title": "A panoramic localizer based on coarse-to-fine descriptors for navigation assistance",
            "venue": "Sensors, vol. 20, no. 15, p. 4177, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Jayasuriya",
                "R. Ranasinghe",
                "G. Dissanayake"
            ],
            "title": "Active perception for outdoor localisation with an omnidirectional camera",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 4567\u20134574.",
            "year": 2020
        },
        {
            "authors": [
                "S. Orhan",
                "J.J. Guerrero",
                "Y. Bastanlar"
            ],
            "title": "Semantic pose verification for outdoor visual localization with self-supervised contrastive learning",
            "venue": "arXiv preprint arXiv:2203.16945, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Zhang",
                "I. Budvytis",
                "S. Liwicki",
                "R. Cipolla"
            ],
            "title": "Rotation equivariant orientation estimation for omnidirectional localization",
            "venue": "Asian Conference on Computer Vision, 2020, pp. 334\u2013350.",
            "year": 2020
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Lifted semantic graph embedding for omnidirectional place recognition",
            "venue": "2021 International Conference on 3D Vision (3DV), 2021, pp. 1401\u20131410.",
            "year": 2021
        },
        {
            "authors": [
                "R. Arandjelovic",
                "P. Gronat",
                "A. Torii",
                "T. Pajdla",
                "J. Sivic"
            ],
            "title": "NetVLAD: CNN architecture for weakly supervised place recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5297\u20135307.",
            "year": 2016
        },
        {
            "authors": [
                "H. Chen",
                "K. Wang",
                "W. Hu",
                "K. Yang",
                "R. Cheng",
                "X. Huang",
                "J. Bai"
            ],
            "title": "PALVO: visual odometry based on panoramic annular lens",
            "venue": "Optics Express, vol. 27, no. 17, pp. 24 481\u201324 497, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Wang",
                "J. Yue",
                "Y. Dong",
                "R. Shen",
                "X. Zhang"
            ],
            "title": "Real-time omnidirectional visual SLAM with semi-dense mapping",
            "venue": "2018 IEEE Intelligent Vehicles Symposium (IV), 2018, pp. 695\u2013700.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "H. Rebecq",
                "C. Forster",
                "D. Scaramuzza"
            ],
            "title": "Benefit of large field-of-view cameras for visual odometry",
            "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), 2016, pp. 801\u2013808.",
            "year": 2016
        },
        {
            "authors": [
                "M. Lin",
                "Q. Cao",
                "H. Zhang"
            ],
            "title": "PVO: Panoramic visual odometry",
            "venue": "2018 3rd International Conference on Advanced Robotics and Mechatronics (ICARM), 2018, pp. 491\u2013496.",
            "year": 2018
        },
        {
            "authors": [
                "H. Matsuki",
                "L. Von Stumberg",
                "V. Usenko",
                "J. St\u00fcckler",
                "D. Cremers"
            ],
            "title": "Omnidirectional DSO: Direct sparse odometry with fisheye cameras",
            "venue": "IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3693\u20133700, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Jaramillo",
                "L. Yang",
                "J.P. Mu\u00f1oz",
                "Y. Taguchi",
                "J. Xiao"
            ],
            "title": "Visual odometry with a single-camera stereo omnidirectional system",
            "venue": "Machine Vision and Applications, vol. 30, no. 7, pp. 1145\u20131155, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Chen",
                "K. Yang",
                "W. Hu",
                "J. Bai",
                "K. Wang"
            ],
            "title": "Semantic visual odometry based on panoramic annular imaging",
            "venue": "Acta Optica Sinica, vol. 41, no. 22, p. 2215002, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Engel",
                "V. Koltun",
                "D. Cremers"
            ],
            "title": "Direct sparse odometry",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 3, pp. 611\u2013625, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Huang",
                "S.-K. Yeung"
            ],
            "title": "360VO: Visual odometry using a single 360 camera",
            "venue": "2022 International Conference on Robotics and Automation (ICRA), 2022, pp. 5594\u20135600.",
            "year": 2022
        },
        {
            "authors": [
                "W. Hu",
                "K. Wang",
                "H. Chen",
                "R. Cheng",
                "K. Yang"
            ],
            "title": "An indoor positioning framework based on panoramic visual odometry for visually impaired people",
            "venue": "Measurement Science and Technology, vol. 31, no. 1, p. 014006, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Caruso",
                "J. Engel",
                "D. Cremers"
            ],
            "title": "Large-scale direct SLAM for omnidirectional cameras",
            "venue": "2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015, pp. 141\u2013148. IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, OCTOBER 2022 34",
            "year": 2015
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.D. Tard\u00f3s"
            ],
            "title": "ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras",
            "venue": "IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255\u20131262, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Chen",
                "W. Hu",
                "K. Yang",
                "J. Bai",
                "K. Wang"
            ],
            "title": "Panoramic annular SLAM with loop closure and global optimization",
            "venue": "Applied Optics, vol. 60, no. 21, pp. 6264\u20136274, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "S. Cai",
                "S.-J. Li",
                "Y. Liu",
                "Y. Guo",
                "T. Li",
                "M.-M. Cheng"
            ],
            "title": "CubemapSLAM: A piecewise-pinhole monocular fisheye SLAM system",
            "venue": "Asian Conference on Computer Vision, 2018, pp. 34\u201349.",
            "year": 2018
        },
        {
            "authors": [
                "B. J\u00e4ger",
                "E. Mair",
                "C. Brand",
                "W. St\u00fcrzl",
                "M. Suppa"
            ],
            "title": "Efficient navigation based on the landmark-tree map and the z\u221e algorithm using an omnidirectional camera",
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1930\u2013 1937.",
            "year": 2013
        },
        {
            "authors": [
                "R. Lukierski",
                "S. Leutenegger",
                "A.J. Davison"
            ],
            "title": "Rapid free-space mapping from a single omnidirectional camera",
            "venue": "2015 European Conference on Mobile Robots (ECMR), 2015, pp. 1\u20138.",
            "year": 2015
        },
        {
            "authors": [
                "L.F. Posada",
                "A. Velasquez-Lopez",
                "F. Hoffmann",
                "T. Bertram"
            ],
            "title": "Semantic mapping with omnidirectional vision",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 1901\u20131907.",
            "year": 2018
        },
        {
            "authors": [
                "H. Liu",
                "H. Huang",
                "S.-K. Yeung",
                "M. Liu"
            ],
            "title": "360ST-Mapping: An online semantics-guided topological mapping module for omnidirectional visual SLAM",
            "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Cowley",
                "I.D. Miller",
                "C.J. Taylor"
            ],
            "title": "UPSLAM: Union of panoramas SLAM",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 1103\u20131109.",
            "year": 2021
        },
        {
            "authors": [
                "H. Seok",
                "J. Lim"
            ],
            "title": "ROVO: Robust omnidirectional visual odometry for wide-baseline wide-FOV camera systems",
            "venue": "2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 6344\u20136350.",
            "year": 2019
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "ROVINS: Robust omnidirectional visual inertial navigation system",
            "venue": "IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 6225\u20136232, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Won",
                "H. Seok",
                "Z. Cui",
                "M. Pollefeys",
                "J. Lim"
            ],
            "title": "OmniSLAM: Omnidirectional localization and dense mapping for wide-baseline multi-camera systems",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 559\u2013566.",
            "year": 2020
        },
        {
            "authors": [
                "M. Ramezani",
                "K. Khoshelham",
                "L. Kneip"
            ],
            "title": "Omnidirectional visualinertial odometry using multi-state constraint kalman filter",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 1317\u20131323.",
            "year": 2017
        },
        {
            "authors": [
                "M. Ramezani",
                "K. Khoshelham",
                "C. Fraser"
            ],
            "title": "Pose estimation by omnidirectional visual-inertial odometry",
            "venue": "Robotics and Autonomous Systems, vol. 105, pp. 26\u201337, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Gao",
                "D. Wang",
                "B. Lian",
                "C. Tang"
            ],
            "title": "LOVINS: Lightweight omnidirectional visual-inertial navigation system",
            "venue": "2021 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 2021, pp. 1\u20136.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "K. Yang",
                "H. Shi",
                "F. Gao",
                "P. Li",
                "K. Wang"
            ],
            "title": "LF-VIO: A visual-inertial-odometry framework for large field-of-view cameras with negative plane",
            "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yu",
                "W. Gao",
                "C. Liu",
                "S. Shen",
                "M. Liu"
            ],
            "title": "A GPS-aided omnidirectional visual-inertial state estimator in ubiquitous environments",
            "venue": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019, pp. 7750\u20137755.",
            "year": 2019
        },
        {
            "authors": [
                "J. Kang",
                "Y. Zhang",
                "Z. Liu",
                "A. Sit",
                "G. Sohn"
            ],
            "title": "RPV-SLAM: Rangeaugmented panoramic visual SLAM for mobile mapping system with panoramic camera and tilted LiDAR",
            "venue": "2021 20th International Conference on Advanced Robotics (ICAR), 2021, pp. 1066\u20131072.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "K. Yang",
                "P. Li",
                "F. Gao",
                "K. Wang"
            ],
            "title": "Attitudeguided loop closure for cameras with negative plane",
            "venue": "arXiv preprint arXiv:2209.05167, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Shi",
                "Y. Zhou",
                "K. Yang",
                "X. Yin",
                "Z. Wang",
                "Y. Ye",
                "Z. Yin",
                "S. Meng",
                "P. Li",
                "K. Wang"
            ],
            "title": "PanoFlow: Learning 360\u00b0 optical flow for surrounding temporal understanding",
            "venue": "arXiv preprint arXiv:2202.13388, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K.-H. Wang",
                "S.-H. Lai"
            ],
            "title": "Object detection in curved space for 360degree camera",
            "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 3642\u20133646.",
            "year": 2019
        },
        {
            "authors": [
                "S.-T. Yang",
                "F.-E. Wang",
                "C.-H. Peng",
                "P. Wonka",
                "M. Sun",
                "H.-K. Chu"
            ],
            "title": "DuLa-Net: A dual-projection network for estimating room layouts from a single RGB panorama",
            "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 3363\u2013 3372.",
            "year": 2019
        },
        {
            "authors": [
                "H.-T. Cheng",
                "C.-H. Chao",
                "J.-D. Dong",
                "H.-K. Wen",
                "T.-L. Liu",
                "M. Sun"
            ],
            "title": "Cube padding for weakly-supervised saliency prediction in 360\u00b0 videos",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 1420\u20131429.",
            "year": 2018
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "E. Ilg",
                "P. Hausser",
                "C. Hazirbas",
                "V. Golkov",
                "P. Van Der Smagt",
                "D. Cremers",
                "T. Brox"
            ],
            "title": "FlowNet: Learning optical flow with convolutional networks",
            "venue": "2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2758\u20132766.",
            "year": 2015
        },
        {
            "authors": [
                "C.-O. Artizzu",
                "H. Zhang",
                "G. Allibert",
                "C. Demonceaux"
            ],
            "title": "Omni- FlowNet: a perspective neural network adaptation for optical flow estimation in omnidirectional images",
            "venue": "2020 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 2657\u20132662.",
            "year": 2020
        },
        {
            "authors": [
                "T.-W. Hui",
                "X. Tang",
                "C.C. Loy"
            ],
            "title": "LiteFlowNet: A lightweight convolutional neural network for optical flow estimation",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8981\u20138989.",
            "year": 2018
        },
        {
            "authors": [
                "K. Bhandari",
                "Z. Zong",
                "Y. Yan"
            ],
            "title": "Revisiting optical flow estimation in 360 videos",
            "venue": "2020 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 8196\u20138203.",
            "year": 2020
        },
        {
            "authors": [
                "M. Yuan",
                "C. Richardt"
            ],
            "title": "360\u00b0 optical flow using tangent images",
            "venue": "British Machine Vision Conference (BMVC), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Seidel",
                "A. Apitzsch",
                "G. Hirtz"
            ],
            "title": "OmniFlow: Human omnidirectional optical flow",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021, pp. 3678\u20133681.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Teed",
                "J. Deng"
            ],
            "title": "RAFT: Recurrent all-pairs field transforms for optical flow",
            "venue": "European Conference on Computer Vision (ECCV), 2020, pp. 402\u2013419.",
            "year": 2020
        },
        {
            "authors": [
                "H. Shi",
                "Y. Zhou",
                "K. Yang",
                "X. Yin",
                "K. Wang"
            ],
            "title": "CSFlow: Learning optical flow via cross strip correlation for autonomous driving",
            "venue": "2022 IEEE Intelligent Vehicles Symposium (IV), 2022, pp. 1851\u20131858.",
            "year": 2022
        },
        {
            "authors": [
                "I. Cinaroglu",
                "Y. Bastanlar"
            ],
            "title": "A direct approach for object detection with catadioptric omnidirectional cameras",
            "venue": "Signal, Image and Video Processing, vol. 10, no. 2, pp. 413\u2013420, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M.-L. Wang",
                "H.-Y. Lin"
            ],
            "title": "Object recognition from omnidirectional visual sensing for mobile robot applications",
            "venue": "2009 IEEE International Conference on Systems, Man and Cybernetics (SMC), 2009, pp. 1941\u20131946.",
            "year": 2009
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards realtime object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems, vol. 28, 2015, pp. 91\u201399.",
            "year": 2015
        },
        {
            "authors": [
                "W. Yang",
                "Y. Qian",
                "J.-K. K\u00e4m\u00e4r\u00e4inen",
                "F. Cricri",
                "L. Fan"
            ],
            "title": "Object detection in equirectangular panorama",
            "venue": "2018 24th International Conference on Pattern Recognition (ICPR), 2018, pp. 2190\u20132195.",
            "year": 2018
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLO9000: Better, faster, stronger",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 7263\u20137271.",
            "year": 2017
        },
        {
            "authors": [
                "J. Guerrero-Viu",
                "C. Fernandez-Labrador",
                "C. Demonceaux",
                "J.J. Guerrero"
            ],
            "title": "What\u2019s in my room? Object recognition on indoor panoramic images",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 567\u2013573.",
            "year": 2020
        },
        {
            "authors": [
                "C. Fernandez-Labrador",
                "J.M. Facil",
                "A. Perez-Yus",
                "C. Demonceaux",
                "J. Civera",
                "J.J. Guerrero"
            ],
            "title": "Corners for layout: End-to-end layout recovery from 360 images",
            "venue": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1255\u20131262, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zou",
                "A. Colburn",
                "Q. Shan",
                "D. Hoiem"
            ],
            "title": "LayoutNet: Reconstructing the 3D room layout from a single RGB image",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 2051\u20132059.",
            "year": 2018
        },
        {
            "authors": [
                "I. Armeni",
                "S. Sax",
                "A.R. Zamir",
                "S. Savarese"
            ],
            "title": "Joint 2D-3D-semantic data for indoor scene understanding",
            "venue": "arXiv preprint arXiv:1702.01105, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Zhang",
                "Z. Cui",
                "C. Chen",
                "S. Liu",
                "B. Zeng",
                "H. Bao",
                "Y. Zhang"
            ],
            "title": "DeepPanoContext: Panoramic 3D scene understanding with holistic scene context graph and relation-based optimization",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 12 632\u201312 641.",
            "year": 2021
        },
        {
            "authors": [
                "J. Benito-Picazo",
                "E. Dom\u0131\u0301nguez",
                "E.J. Palomo",
                "G. Ramos-Jim\u00e9nez",
                "E. L\u00f3pez-Rubio"
            ],
            "title": "Deep learning-based anomalous object detection system for panoramic cameras managed by a jetson TX2 board",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "T. Liu",
                "Z. Yuan",
                "J. Sun",
                "J. Wang",
                "N. Zheng",
                "X. Tang",
                "H.-Y. Shum"
            ],
            "title": "Learning to detect a salient object",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 353\u2013367, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "J. Zhang",
                "S. Sclaroff"
            ],
            "title": "Exploiting surroundedness for saliency detection: A boolean map approach",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 5, pp. 889\u2013902, 2016.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Panoramic imaging, panoramic optical system, ultra-wide-angle optical system, computational imaging, intelligent instrument, computer vision, multidimensional perception, scene understanding.\nI. INTRODUCTION\nRECENT advances in high-speed communication andartificial intelligence have led to stronger demand for upgrading traditional imaging systems. Panoramic imaging has a wider Field of View (FoV) than traditional panoramic optical systems and can capture more information about the\nThis work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No. 12174341, in part by the Federal Ministry of Labor and Social Affairs (BMAS) through the AccessibleMaps project under Grant 01KM151112, in part by the University of Excellence through the \u201cKIT Future Fields\u201d project, in part by the Hangzhou Huanjun Technology Company Ltd., and in part by Hangzhou SurImage Technology Company Ltd. (Corresponding authors: Kaiwei Wang and Kailun Yang.)\n1S. Gao, H. Shi, K. Wang, and J. Bai are with State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China {gaoshaohua, haoshi, wangkaiwei, bai}@zju.edu.cn\n2K. Yang is with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany kailun.yang@kit.edu\nManuscript received May 11, 2022.\nsurrounding environment at a time. It is becoming the next generation of intelligent sensing instruments. They are used in autonomous driving, machine vision inspection, endoscopic medicine, satellite atmospheric inspection, and many other applications. In terms of imaging performance, panoramic imaging faces several common challenges to satisfy the requirements of applications such as machine vision, including FoV, resolution, no blind area, and imaging quality. Another urgent requirement is multidimensional intelligent perception, where panoramic imaging is expected to be combined with intelligent sensors to record, fuse, and perceive higher dimensional information about the surrounding environment. In addition, panoramic imaging is expected to evolve towards lightweight and small-volume structural forms for applications in more space- and weight-constrained scenarios. However, the above requirements usually entail addressing different tradeoffs, which make the design of panoramic imaging instruments particularly challenging. Compared with the only previous panoramic imaging survey [1], it is so far back that it does not provide an overview of the flourishing progress in panoramic imaging in the last 20 years [2]\u2013[7], which is addressed in this review. Surveys on other sensing have emerged, such as LiDAR [8], [9] or applications in scene understanding [10]\u2013 [12]. However, a detailed review of panoramic imaging and its applications has not appeared so far. This review fills the gap by combing panoramic imaging and its scenario applications from their origin to the latest developments. Panoramic imaging experienced its first boom in the 1990s, but this boom gradually declined due to the lack of qualified manufacturing equipment and digital information processing systems.\nIn the past decade, the concept of panoramic imaging has been revisited and has enjoyed a new boom. Emerging technologies such as freeform surfaces [13] and metasurfaces [14] have dramatically reshaped panoramic imaging systems and lit a bright light in the panoramic imaging field. The application of these emerging optical technologies has made a strong impetus for the performance improvement of panoramic optical system architectures (Fig. 1(a)). At the same time, the proposal of the multidimensional panoramic imaging system has enriched the panoramic imaging and played a more powerful role in different application fields (Fig. 1(b)). In this review, we first introduce the basic concepts of panoramic imaging, related design challenges, and potential solutions. Afterwards, we review different architectures of panoramic imaging systems and their characteristics in detail. In addition, we classify and analyze different panoramic imaging systems according to their multidimensional imaging modalities and briefly introduce the emerging optical technologies that are ar X\niv :2\n20 5.\n05 57\n0v 2\n[ cs\n.C V\n] 1\n4 O\nct 2\n02 2\ncurrently being equipped for panoramic imaging. Then, we summarize and discuss the applications of panoramic imaging in scene understanding in detail. Finally, this review provides a comprehensive overview and outlook in the field of panoramic imaging."
        },
        {
            "heading": "II. KEY PARAMETERS OF PANORAMIC IMAGING SYSTEMS",
            "text": "To satisfy the panoramic environment perception, panoramic imaging systems usually face several common challenges, such as field of view, wavelength, F-number, focal length, resolution, imaging quality, and volume [15], [16]. To achieve panoramic imaging systems that meet application scenarios, these key parameters and their trade-offs usually need to be considered before design. Potential solutions can facilitate the design and manufacture of panoramic imaging systems with higher parametric performance. Before discussing each\nparameter in detail, it is helpful to review the basic definitions of the above parameters."
        },
        {
            "heading": "A. Definition of Parameters",
            "text": "For panoramic imaging systems, the performance parameters of the optical system are particularly important. In this section, the definitions of the key parameters of panoramic imaging systems are summarized.\n1) Field of View (FoV): In applied optics, there are four ways to express the FoV: angle, object height, paraxial image height, and actual image height. Panoramic imaging instruments usually take the angle as the evaluation standard of the FoV. In practical applications, full angle FoV is generally used. When designing a panoramic optical system, the expression of 360\u25e6\u00d7(Half FoV) is also usually used to represent a panoramic system with a blind area. In optical instruments, the angle\nformed by the two edges of the maximum range through which the object of the measured target can pass the camera, with the camera of the optical instrument as the vertex, is called the Field of View (FoV) [17], [18]. Here, we take the most common fisheye optical system (Fig. 2) as an example. The FoV determines the ability of the optical instrument to observe the range of the surrounding scene. Compared to other common imaging optical systems, the FoV of a fisheye optical system can usually reach 180\u25e6 or more. This unique imaging characteristic enables more information about the surrounding environment to be recorded at a time. Common imaging systems typically have a small FoV that captures less information each time. Therefore, panoramic imaging systems have more environmental perception advantages over traditional optical systems with common FoVs and have become a popular development direction for future optical instruments. The concept of metaverse has brought Augmented Reality (AR) and Virtual Reality (VR) display fields into a new round of development, and has also led to the rise of panoramic environment perception [19], [20]. New panoramic imaging instruments enables a broader environmental perception of the real world and transmits the data to artificial intelligence and big data processing for further human awareness and understanding of real-world scenes.\n2) Wavelength: In physics, wavelength is the spatial period of a periodic wave [21]. Electromagnetic radiation can be classified by wavelength as radio waves, microwaves, infrared, visible spectrum, ultraviolet, X-rays, and Gamma rays [22], [23]. The spectral range in which the human eye can perceive the world can be described as visible light, usually defined as wavelengths in the range of 400\u223c700 nanometers (nm) [23]. Light wavelengths longer than the visible spectrum are called infrared. Light wavelengths shorter than the visible spectrum are called ultraviolet. Ultraviolet (UV) wavelengths range from 10 nm to 400 nm and are shorter than visible light, but longer than X-rays [24]. Typically, infrared (IR) wavelengths range from 700 nm to 1 millimeter (mm) [25]. Visible light wavelengths are the most common range for designing panoramic imaging optical systems [16]. To expand human perception of the surrounding environment, UV and IR panoramic imaging systems have emerged to enable a wider spectrum of environmental perception beyond the human visible spectrum wavelengths [26], [27].\n3) F-number: It is defined as the ratio of the focal length (in the image space) of an optical system to its entrance pupil diameter [28], [29]. The smaller the F-number, the more light is fed into the optical system and the larger the size of the aperture diaphragm (aperture stop). Small F-number optical systems are also called high-speed imaging systems. Small F-number optical systems generally have a large aperture and greater luminous flux, which can improve shutter speed. In low light condition, this optical system can maintain more light flux, which is good for a night- or darkfield shooting [30]. When shooting moving objects, a small F-number optical system is more suitable for shooting the object clearly with a high-speed shutter. Optical system with small F-number has small depth of field [31]. When shooting, the optical system with smaller depth of field will highlight the target\nwhile defocusing the background. A large F-number optical system has a large depth of field, enabling clear imaging of a wide range of scenes at both distant and close range, but with reduced illumination. F-number is a crucial parameter. The smaller the F-number, the wider the scope of applications of the camera. When designing an imaging system, a small F-number optical system is more difficult to design. But it typically has higher imaging quality. Therefore, the F-number of a panoramic system is usually designed to be small enough to achieve a large luminous flux and excellent imaging quality.\n4) Focal length: The focal length is the distance from the primary point of the optical system to the focal point [32]. The review of the projection models can help us understand the principles of panoramic imaging. Optical design usually follows five classical projection models [33], [34]. They are rectilinear projection model, equidistant projection model, equisolid angle projection model, orthographic projection model, and stereographic projection model. The equidistant projection model is widely used in the design of large FoV optical systems, and its expression in Equation (1) is:\ny = f \u00b7 \u03b8, (1)\nwhere f is the focal length, \u03b8 is the half FoV, and y is the image height. The selection of the sensor is required to be matched with the optical system. Similar to other panoramic optical systems, the fisheye optical system contains two imaging modes: the Inscribed circle mode is illustrated in the bottom left corner of Fig. 2 and the Circumscribed circle mode is illustrated in the bottom right corner of Fig. 2. The rectangle is the sensor and the circle is the imaging circle of the fisheye optical system in the image plane. Panoramic images of these two imaging modes are presented below the corresponding imaging mode schematic. A panoramic system designed with the Inscribed circle imaging mode do not lose the FoV, but waste pixels at the edge of the sensor. To achieve panoramic scene understanding with the largest FoV, this mode is used more frequently in the design stage of panoramic systems. Panoramic systems designed with Circumscribed circle imaging modes lose the FoV at the edges. Sometimes this solution is adopted in the design of optical systems without central blind areas such as fisheye optical systems. When the FoV of a panoramic optical system remains the same value, the larger the focal length and the larger the image height. At the same time, the volume of the designed system will be larger. Its matched sensor tends to have a higher resolution (In the case of a constant sensor pixel size).\n5) Resolution: Image resolution generally refers to the ability of a measurement or display system to resolve details. This concept can be measured in the fields of time, space, etc. In panoramic system design, resolution generally refers to the total number of arrays of horizontal and vertical pixels of the sensor, such as 4216(H)\u00d73128(V ) for a 13 megapixel sensor [35]\u2013[38]. The pixel size and resolution of the sensor jointly determine the optical format of the sensor [39], [40]. For the optical system, optical resolution is relevant to the wavelength and aperture diaphragm size. In specific, the shorter the wavelength and the larger the aperture diaphragm,\nthe higher the resolution of the optical system (In the case of great system aberration correction).\n6) Imaging quality: The imaging quality of a panoramic optical system is not a single evaluation criterion. The biggest influence on imaging quality is aberration. Aberrations are mainly divided into spherical aberration, coma, astigmatism, field curvature, distortion, chromatic aberration, and wave aberration [28]. Chromatic aberration can be divided into longitudinal chromatic aberration and lateral chromatic aberration. Longitudinal chromatic aberration is also known as positional chromatic aberration, and lateral chromatic aberration is also known as transverse chromatic aberration. In the process of designing a panoramic optical system, the image quality of a panoramic imaging system is usually evaluated by the spot diagram and the Modulation Transfer Function (MTF) [16]. The optical system needs to follow the aberration design theory to correct the aberration in the design process, and the final design needs to satisfy the uniform relative illumination of each FoVs [41], [42]. In order to make the panoramic imaging instrument more conducive to scene understanding, the relative illumination of the panoramic optical system also needs to be as uniform as possible, and the distortion should be as small as possible to meet the perception accuracy requirements [16].\n7) Volume: Miniaturized and lightweight panoramic optical systems will be suitable for more space and weight-constrained scenarios [16]. To facilitate the characterization and comparison of the compactness of panoramic systems, a better way is to use the panoramic system compactness ratio as the volume compactness parameter.\nThe compact ratio of the panoramic system as shown in Equation (2) can be defined as the ratio of the maximum diameter of the panoramic system to the diameter size of an image on the sensor [43]:\nRcompact = Dpanoramic Dsensor , (2)\nwhere Rcompact is the ratio of compactness, Dsensor is the maximum diameter of the imaging circle on the sensor, and Dpanoramic is the maximum lateral diameter of the panoramic optical system.\nAt present, researchers in the panoramic field are inclined to design small volume, small F-number, and high-resolution panoramic optical systems to satisfy the demand for higher optical performance and compactness for panoramic field photography. At the beginning of the design, the parameters need to be considered and weighed in detail."
        },
        {
            "heading": "B. Key Parameters Trade-offs and Potential Solutions",
            "text": "In the design process of panoramic optical systems, a larger FoV is usually pursued [68]. The increased FoV will bring greater difficulty in aberration correction. The influence of the aberration of the panoramic imaging system on the imaging quality will be solved in the hardware design stage. Therefore, it is particularly important to evaluate which panoramic system architecture to be used in the optical design process to obtain panoramic images with high imaging quality and a large FoV. At the beginning of the development of panoramic\n2\noptical systems, multiple cameras stitching [47], [48] or single camera scanning [1] techniques were usually used. Singlecamera scanning requires high speed, high frame rate, and high precision to maintain scanning stability. However, this method cannot capture the surrounding environment in real time [69]. On the other hand, multiple cameras stitching technology needs to rely on post-algorithm stitching and correction, and there are still errors in camera calibration and correction [47]. The multiple cameras stitching approach usually brings greater device volume and power consumption, as well as the inevitable high price and arithmetic consumption. Subsequent developments in the fisheye optical system use a refractive optical path for single-camera large FoV imaging [51], [70]. The optical path to satisfy the large FoV aberration correction makes the optical path smooth, usually using more negative lenses in the front to deflect the direction of the light path for large FoV [71]. To reduce the number of lenses used, the difficulties of design, processing, and assembly, high refractive index materials are usually used in the design [52]. Although highly refractive materials have a greater ability to refract light, they tend to be more expensive. To further reduce the volume of the system, new panoramic imaging architectures such as catadioptric panoramic system [57], hyper-hemispheric lens [63], panoramic annular lens [16], etc. have emerged. These new architectures usually use a catadioptric optical path to collect the light from a large FoV into a relay lens group and\nthen perform aberration correction. The parameter comparison of different panoramic imaging system architectures is shown in Table I. Each of these panoramic imaging systems has its own architecture, which will be discussed in detail in the next section.\nTo achieve higher relative illumination and higher optical resolution, small F-number panoramic optical systems have emerged [66]. Long focal length panoramic imaging systems [15], [72] are able to match high-resolution imaging sensors, thus, enabling detailed observation of the panoramic environment. The human eye can only perceive a very narrow spectrum of visible light in electromagnetic waves, which is approximately 400\u223c700 nm [23]. To provide higherdimensional detection of the real environment, panoramic imaging systems in the infrared [48], [57], [73], visible [68], [74], and ultraviolet [26], [75] wavelengths have been designed in succession. Not only from the light wave spectrum but also the polarized light [43], [66] can be used for the design of panoramic systems. The use of panoramic optical systems to broaden the human perception of the surrounding environment has become a new trend in the development of panoramic imaging systems. Multidimensional panoramic perception will broaden human understanding of the real environment and provide a deeper understanding and awareness of the real world. Meanwhile, zoom panoramic optical systems [76], [77] can switch between large and small FoVs in the panoramic range to achieve detailed detection of key areas of interest. To obtain a wider observation FoV, multi-channel panoramic systems [78], [79] have also been proposed. These multichannel panoramic imaging systems usually have larger FoVs with different views. They have certain advantages over singlechannel panoramic systems. Catadioptric panoramic optical system usually have a central blind area [62]. To eliminate this tricky drawback, dichroic films [80] and polarization techniques [43], [66] were introduced to eliminate the blind areas. Conventional panoramic systems are designed using spherical surfaces [15], [67]. The optical lens design techniques use the accumulation of optical paths to modulate the beam [81]. As the FoV increases, it becomes more difficult to correct the optical aberration and more lenses are usually used. Such techniques make the design and manufacture of higherperformance compact panoramic systems more difficult. It also makes assembly more difficult and brings an inevitable\nincrease in volume and price. Fortunately, the rapid development of high-precision manufacturing technology has made new types of optical surfaces possible. Freeform surface [76], [82], thin-plate optics [83], and metasurface [84] technologies provide powerful engines for miniaturization and high imaging performance of panoramic imaging systems. These technologies offer more freedom degrees to optimize the design of panoramic optical systems, allowing for better image quality to an extreme extent, while reducing the system volume [82]. In application areas, panoramic imaging devices are becoming promising filming devices for autonomous driving, medical treatment, satellite navigation, etc [66], [85]. Combined with artificial intelligence and multidimensional sensor technology, panoramic imaging will have broader application scenarios and become a new trend in photographic instruments."
        },
        {
            "heading": "III. ARCHITECTURES AND PROPERTIES OF PANORAMIC IMAGING SYSTEMS",
            "text": ""
        },
        {
            "heading": "A. Single Camera Scanning and Multiple Cameras Stitching",
            "text": "FoVs stitching is a method of using multiple FoVs to capture and stitch together to achieve large-FoV images [86]\u2013[88]. This technology is implemented in single-camera scanning and multi-camera stitching methods. Single-camera scanning uses a high-precision mechanical rotating stage to rotate a single camera to create a panoramic view. The shooting principle is shown in Fig. 3(a), where the optical system is scanned around the center point O to form an image with a large FoV. Using a mobile phone, a large-FoV panoramic image can be captured, as shown in Fig. 3(b), and the captured image is illustrated in Fig. 3(c) [45]. A similar single-camera scan case is the installation of Nikon 990 mounted on Kaidan Kiwi 990. It can form a large FoV image of 120\u25e6 [1]. This imaging method can be applied to shoot scenes in a relatively static environment, which requires high scanning accuracy for mechanical devices. The camera cannot achieve gaze imaging because it takes some time to rotate and scan during the shooting. Using a prism to flip the FoV and rotate the shot is a similar scheme [46].\nAnother common FoV-stitching technique is to use multiple cameras for stitching shots [89]. Multiple cameras shot simultaneously to achieve 360\u25e6 surround-view image, and the shooting schematic is shown in Fig. 3(d). In [47], a case of\nFoVs stitching using six visible light cameras is discussed (Fig. 3(e)). The captured image is shown in Fig. 3(f). Due to the inconsistency of the parameter settings and installation angles of each camera, the image stitching is affected. Longwave infrared optical materials are usually costly. Adopting the multi-camera stitching panoramic imaging technique, Cowan et al. [48] achieved a low-cost long-wave infrared panoramic photography using nine circumferential arrays lined up with small-sized long-wave infrared cameras. This stitching method requires multiple sensors and image stitching algorithms. It requires high positioning and calibration between multiple cameras. A similar design is proposed in [49], which used four visible band cameras to synthesize a lowcost portable polycamera for stereo 360\u25e6 imaging. With the example of a two-mirror pyramid panoramic cameras, [90] illustrated optimizing the pyramid\u2019s geometry and the selection and placement of imager clusters to maximize the FoV, sensor utilization, and image uniformity. This analysis can be extended and applied to other designs based on pyramid panoramic cameras."
        },
        {
            "heading": "B. Fisheye Panoramic System",
            "text": "Compared with the FoVs stitching method to obtain panoramic images, using a single camera has the advantages of simple system structure, no need for stitching algorithms, low cost, and stable installation. The most classic way is to use a fisheye optical system. It is called the fisheye optical system because its first lens is protruding and its structure is similar to a fisheye, as shown in Fig. 4(a) [91]. In 1905, Wood [92] presented a prototype of an underwater wideangle pinhole camera in chapter four of his book Physical Optics, as shown in Fig. 4(b). In 1922, Bond [93] designed a hemispherical lens with a pupil in the center of curvature for cloud recording, as illustrated in Fig. 4(c). In 1924, Hill [94] proposed a hill sky lens, adding a meniscus lens before the hemispherical lens, as depicted in Fig. 4(d). Fig. 4(e) was the first prototype of the modern fisheye lens [95]\u2013[99], patented by AEG company in 1935 [100]. The FoV of the fisheye optical system usually exceeds 180\u25e6, which is an ultra-wideangle optical system [101], [102]. Due to its large FoV, it is usually used in panoramic photography and other fields. This large FoV imaging optical system generally consists of two or three negative meniscus lenses as the front group, which compresses the large FoV on the object side to the FoV required by the conventional lens, and then performs aberration correction through the relay lens group. Because the optical path of the fisheye optical system needs to be folded through multiple lenses in the front, the distortion of the optical system is large. The f -\u03b8 distortion is usually can be as large as 15%\u223c20% [50]. The distortion control of a wide FoV lens is particularly important. [103] described the design process of an imaging system for a micro wideangle computing camera, and simulated the generation and correction method of distorted images. As shown in Fig. 4(f), a panoramic image captured by a fisheye lens Entaniya M12280 can achieve a large-FoV imaging with no blind area of 280\u25e6, but there is a large barrel distortion in the edge FoV of the image [44]. In addition, the first lens of fisheye optical system usually has a diameter five times greater than that of the rear correction lenses. Relatively speaking, the compactness of the fisheye optical system caused by the large diameter difference between the front and rear lens groups is still poor. There is no blind area in the center of the fisheye systems, but the distortion of the edge FoV will compress the image. Therefore, at the beginning of the design, the compactness of the system structure, color correction, and distortion correction need to be considered [104], [105]. For the ultra-wide-angle panoramic system, there are two kinds of aberrations of offaxis point object in each FoV: aperture-ray aberration of offaxis point object and chief ray aberration [106]. In addition to distortion correction, the correction of wave aberration of field curvature and chromatic aberration of each optical surface in\nfisheye optical system is also very important. The calculation method of field curvature aberration and chromatic aberration in [107] provided a theoretical basis for aberration correction of ultra-wide-angle systems.\nIn terms of image acquisition, fisheye images can be extracted by precise numerical image processing algorithms [108]. Image stitching using two fisheye lenses enables a larger panoramic FoV [109]\u2013[111]. A more compact volume can be obtained by refraction of the optical path using prisms [112], [113]. In the vacuum environment of \u221240\u25e6C\u223c+60\u25e6C, the space fisheye system can still ensure stable imaging performance [114]. The use of capsule endoscopy to examine the pathological changes of the digestive system, especially the intestine, has recently become a major breakthrough in medical engineering. To solve the shortcomings of the traditional endoscope capsule that the FoV is not wide enough and the imaging quality is not good enough, a novel design of micro lens with wide-angle FoV and good imaging quality is proposed in [115]. The system is designed with a plastic aspheric lens and a glass lens. The length and width of the prototype are 9.8 mm and 10.7 mm high. The new zoom function of fisheye optical system can be realized by using the liquid lens technology [116]. In the field of medical and health care, the presence of blood will lead to scattering and absorption in the process of optical imaging. The fiber-optic infrared wide-angle imaging system [117] can capture wide FoV and large depth of field infrared images in real time."
        },
        {
            "heading": "C. Panomorph Imaging System",
            "text": "Thibault [54] proposed a novel type of panoramic camera: Panomorph lenses in 2006, and discussed the ways to control anamorphic and distortion profile. Such systems have higher sensor area usage and more pixels in the region of interest [53], [118]. The panomorph systems are usually equipped with cylindrical and toroidal surfaces to achieve different anamorphic ratios [55], [119], [120]. The optical path of this optical system is shown in Fig. 5(a), which can be used in a surveillance system to monitor driving conditions [55], [120]. As shown in Fig. 5(b), the fisheye system images the distant house A, the car B, and the edge-FoV C on the sensor. The image size distribution is shown on the left. The image taken by the panomorph system is shown in the right picture [121]. The far house A occupies a smaller area, and the car B is larger, and the edge C has a more obvious compression effect, indicating that the system has more pixels in the area of interest and is more suitable for enhanced surveillance and driving monitoring systems. The realized panomorph systems and imaging results are shown in Fig. 5(c) [55]. Fig. 5(d) shows the parking lot image, taken by a fisheye lens (Left) and a panoramic lens with equidistant projection (Middle). The yellow area is the relative size of the objects in the image. The panomorph lens provides a resolution gain by providing an anamorphic correction. The resolution on the borders is twice as high as the resolution in the center taken by a panomorph lens, also shown in Fig. 5(d)(Right) [122]. Tolerancing is the basic procedure of lens design. [123] studied and showed\nhow the entrance pupil changes with the FoV of these optical systems (position and size), and come to the conclusion that the distortion comes from the front surface of the lens. Surface irregularity error is usually used to specify the manufacturing accuracy of spherical-, aspherical-, or flat surfaces. A spatially correlated representation of irregular slope is proposed and implemented to specify surface accuracy in [124] and the front surface cases of fisheye and panoramic lenses are investigated in detail. In the design of modern panoramic optical systems, distortion control is particularly important. The design and distortion control of modern panoramic systems are analyzed and discussed in [125]. The optical testing of modern panoramic lenses on the market for safety and monitoring applications is reported for the first time in [126]. The first test is on the measurement of image mapping, especially on the reciprocal of Instantaneous Field of View (IFoV) expressed in pixels/degrees. The second test is to measure the MTF of the system. All tested lenses are coupled to the same camera to measure the MTF of the system. Therefore, any change in the front surface will greatly affect the image footprint. Similar image effects can be achieved by using fisheye lens image and one-directional linear interpolation algorithms [127]. To improve the relative illuminance of panomorph lenses, it is necessary to study their 3D entrance pupil mode [128]. It is shown that with the increase of FoV and pupil size, the entrance pupil moves forward, the optical axis is shifted, tilted, and deformed [129]. This increased-resolution panomorph lens is particularly suitable for use in implementing vehicle positioning and tracking [130]. [131] demonstrated that wideangle FoV, enhanced resolution, close focusing, and distortionfree multivisualization software can improve laparoscopic and other endoscopic procedures. Furthermore, infrared panoramic lens can play a huge role in military short-distance positioning and urban security surveillance tasks [119], [132]."
        },
        {
            "heading": "D. Catadioptric Panoramic System",
            "text": "Catadioptric panoramic system is mainly composed of two parts, one part is the reflective optical element, and the other part is the refractive optical element [133]\u2013[136]. The reflective element is generally the mirror of the front element of the catadioptric system (Fig. 6(a)). The fisheye optical system uses multiple negative meniscus lenses for refraction, compressing the direction of the light entering relay lens group. Different from the fisheye optical system, the catadioptric panoramic optical system uses a mirror to reflect the surrounding 360\u25e6 light into the relay lens group. The refractive element of the catadioptric panoramic optical system is the relay lens group, which is used to correct aberrations for imaging. The mirror is placed in front of the aperture diaphragm and relay lens group. Also, the f -\u03b8 distortion of such panoramic optical systems is usually greater than 10% due to the large FoV for imaging [56], [57]. Compared to fisheye optics, it has fewer front lenses and focuses on surround-view imaging. Due to its characteristic as catadioptric imaging, this op-\ntical system images itself in the center of the image, which can also be considered as a blind area or area of noninterest. Due to the inaccurate localization of feature points, the distorted chessboard image will affect the accuracy of panoramic camera calibration. Distorted checkerboard images can affect the calibration accuracy of panoramic cameras due to the inaccurate positioning of feature points. Compared with the conventional approach, iterative refinement method [137] reduces the reprojection error of feature points by 39%.\nThe catadioptric panoramic system is usually equipped with a transparent glass support or center mount to connect a conventional camera and a mirror, as depicted in Fig. 6(b) [138], [140]. The center of the captured image images the sensor, which is also a kind of blind area (Fig. 6(c)) [139]. A catafisheye camera with a 360\u25e6\u00d755\u25e6 FoV can be achieved by using a fisheye camera and a mirror [141]. To achieve the ideal object-image relationship, catadioptric panoramic optical systems that use a single mirror on the front element are usually designed with aspheric surfaces such as paraboloids, hyperboloids, and ellipsoids. The corresponding area of the blind area of single mirror catadioptric panoramic system is the ground, its optical path diagram is shown in Fig. 6(d). To achieve panoramic images in four directions with VGA video standard, a catadioptric panoramic system using an odd aspheric design is proposed in [142], its image distortion is less than 0.5%. Over the past few decades, the technology of image sensor provides a platform for large FoV and highresolution 3D shooting, analysis, and static- or dynamic scenes modeling [143]. For example, vision-based driver assistance can obtain real-time 3D data from mobile platforms. The use of convex mirrors and fisheye cameras can form a panoramic stereo imaging system and can adapt to non-singleview imaging conditions for 3D reconstruction in Euclidean space [144]. The concept of the catadioptric mirror can also be applied to endoscope design to simplify and reduce the volume of the system [145], [146]. The catadioptric panorama makes it possible that endoscope capsule can also use a conic mirror to observe the inner wall of the human body [147]. [148] demonstrated that a compact dual-channel endoscopic probe can simultaneously observe the needs of forward and backward FoVs in the colon. Similar designs can also be used in endoscopic capsules [149]. Developing a new convex mirror and repositioning the camera viewpoint can improve the efficiency of the catadioptric camera in the RoboCup MSL robot [150]. This design can be used not only in the visible light band but also in a wide-spectrum infrared panoramic catadioptric system [151], [152]. In the military field, infrared panoramic system is also widely used [153]. The US Navy Research laboratory has developed a high-resolution mediumwave-infrared panoramic periscope sensor system [154]. The compact system can provide 360\u25e6 horizontal azimuth and \u221210\u25e6\u223c+30\u25e6 elevation FoV without moving components. The design difficulty of a catadioptric imaging system mainly lies in determining the initial structural parameters of the quadric\nmirror of the system. In conjunction with genetic algorithm and gradient descent, Zeng et al. [155] proposed a panoramic thermal imaging system to demonstrate the effectiveness of the method. In [156], the challenges involved in the three schemes to realize 360\u25e6 panoramic thermal imaging are discussed, and the spatial resolution, FoV, data complexity, system complexity, and cost of different solutions are analyzed in detail. The three solutions are a 360\u25e6 camera design of a long-wavelength infrared XGA sensor, the splicing of three adjacent longwave infrared sensors equipped with a low distortion 120\u25e6 camera, and a half-FoV 180\u25e6 fisheye camera with an XGA sensor. Using a progressive design approach, Qiu et al. [157] designed a 14\u223c16 \u00b5m long-wave infrared earth panorama system. Lim et al. [158] used the graphically symmetric method to correct for chromatic aberration and planar Pitzval field curvature, and designed a catadioptric system with a planar mirror. The space between the mirror surface and the relay lens group can also be changed from air to plastic [159]\u2013 [162]. Using plastic lenses, a 360\u25e6 low-cost laser scanner for smart vehicle perception was designed in [163]. A front lens group can be set in front of the central FoV of the solid reflector to realize a 280\u25e6 catadioptric imaging system to simulate honeybee eyes [164]. The use of catadioptric optics with both forward and radial imaging channels enables minimally invasive endoscopic screening for gastrointestinal diseases, extending the detection range [165]. To significantly improve the visual authenticity of minimally invasive surgery and diagnosis in vivo, a dual-channel panoramic endoscope is proposed in [166], whose FoV is \u00b1135\u25e6 and 360\u25e6 from the optical axis. The system uses optical fiber or LED to illuminate the whole FoV. The panoramic endoscope can prevent repeated insertion of traditional endoscopes with different perspectives and reduce the risk of misleading due to the limited FoV of traditional endoscopes. Using an annularly-stitched aspherical surface, Chen et al. [167] realized a 360\u25e6\u00d7270\u25e6 ultra-wideangle three-channel catadioptric optical system. The research results show that the annularly-stitched asphere surface can significantly improve the imaging quality of different channels and realize the optimization of multi-channel and multi-FoVs imaging performance. In [168], a double distortion correction method for panoramic images is proposed, which solves the problem of low calibration accuracy caused by the mirror distortion of the catadioptric system. Computational ghost imaging and single-pixel imaging enable imaging and sensing under many challenging circumstances (e.g., scattering/turbulence, low temperature, unconventional spectroscopy). The ghost panoramic single-pixel imaging proposed in [169] has broad application prospects in fast-moving target localization and situational awareness for autonomous driving.\nCatadioptric panoramic imaging systems using a two-mirror design have also appeared. To reduce the blind area ratio, a catadioptric panoramic system using two mirrors with the same bending direction was proposed in [68]. The system realized that the blind area is less than 3.67%. Yang et al. [170] adopted a similar structure and designed a catadioptric panoramic system for imaging at 0.4\u223c0.9 \u00b5m. This catadioptric panoramic architecture can be used to design low F-number long-wave infrared panorama systems [171]. The\ndesign and performance comparison of the two mirrors in the same (Fig. 6(e)) and opposite (Fig. 6(f)) directions is discussed in detail in [57]. In the field of automatic driving, the use of visible and infrared dual bands can help to realize day- and night panoramic scene detection [172].\nUltraviolet detection technology is concentrated in the 240\u223c280 nm ultraviolet band. The solar radiation in this band is strongly absorbed by the ozone layer and hardly exists in the near-earth atmosphere. It is called sun blind ultraviolet. In the solar-blind area, the ultraviolet radiation generated by targets such as flames and high-voltage discharge coronas is easily revealed under the weak radiation background noise. The ultraviolet detection technology realizes the monitoring of dangerous targets by detecting these spectral signals. The front element of the system can be constructed by using two mirrors and a transmissive meniscus lens to realize target detection in the ultraviolet spectrum [26]. Panoramic detection in both visible and ultraviolet wavelengths can be achieved by using a beam splitter [173]. Using two Charge-Coupled Device (CCD) cameras and two parabolic convex mirrors, combined with an enhancement of existing deep feature matching methods with epipolar constraints, a practical panoramic stereo sensor can be realized [174].\nFurther, in [175], [176], the researchers designed a mirror panoramic system composed of four mirrors. It is composed of pseudo-Cassegrain collecting mirror and a reverse pseudoCassegrain imaging mirror, which can image visible light and long-wave infrared light simultaneously. The FoV of the system is 360\u25e6\u00d7(40\u25e6\u223c110\u25e6) and the F-number is 1.55."
        },
        {
            "heading": "E. Monocentric Panoramic System",
            "text": "Compound eye (Fig. 7(a)) [177], [180]\u2013[186] is a typical feature of arthropod in nature. Compared with a single aperture vision system, the compound eye possesses many excellent imaging characteristics, such as compact size, wide FoV, great perception of moving objects and sensitivity to light intensity [187]. Benefiting from the development of electronic sensors and advances in micro-nano fabrication, the realization of curved sensors makes traditional optical systems no longer limited to imaging on flat sensors. In 2014, Viollet et al. [178] proposed a Curved Artificial Compound Eye (CurvACE) (Fig. 7(b)) inspired by drosophila. It is composed of a microlens array, a neuromorphic photodetector array, and a flexible Printed Circuie Board (PCB). It consists of 630 ommatidia and CurvACE\u2019s FoV is 180\u25e6\u00d760\u25e6. To satisfy the requirements of miniaturization, high imaging quality, large FoV of monitoring camera, and the development of curved image sensor, Wang et al. [61] designed a monocentric monitoring lens optical system. The monocentric system has a FoV of 140\u25e6, 7.88 mm focal length, 14.47 mm total length, and F/1.5. A monocentric lens with multi-aperture integration was proposed (Fig. 7(c)) [179] in 2015. This optical system has the characteristics of all optical surfaces are spherical and share a common center of curvature. The monocentric lens with compact imager volume has no coma or astigmatic aberration. A F/1.35, 30 megapixels, 126\u25e6 fiber-coupled monocentric lens imager prototype greatly reduces the volume of\nthe wide-FoV imaging system compared to a commercial F/4 camera (Fig. 7(d)) [179]. The image processing methodology can significantly improve the quality of the fiber relayed prototype image, as illustrated in Fig. 7(e) [179]. A gigapixel monocentric multiscale imager has been shown to integrate a two-dimensional mosaic of subimages [59], [188], [189]. This wide- and uniform field can also be realized by waveguide method [190]. Instead of relay optics, one or more multimode fiber bundles [191], [192] can be used to transfer the spherical image surface to a conventional planar image sensor. To help determine the best specific design for a 4GA-8 lens, a wide FoV monocentric lens is studied from simple structures to moderately complex in [193], and a system optimization method for global optimization of dual-glass lenses are proposed for a series of large FoV imaging systems to offer practical high-performance options. The monocentric lens has recently shifted from the initial historical curiosity [194] to a promising approach for panoramic high-resolution imagers. In [195], [196], the technique of monocentric lens using analytical aberration calculation and constrained numerical optimization was introduced, and its application in fully symmetric and hemispherical symmetric lenses was demonstrated. The performance summary of the monocentric lenses in the visible and near-infrared spectral bands can be used to guide the design of various monocentric lenses. [197] established the potential of a fiber-coupled monocentric imager in the novel panoramic high-resolution imaging and verified the ability of the monocentric lens to focus large FoV images at a series of object depths. By changing the fiber bundle structure can improve spatial resolution. Thanks to digital image processing, imaging quality can be further enhanced. In [60], the visible light and visible near-infrared spectrum monocentric system which transmits the curved image to the Complementary Metal Oxide Semiconductor (CMOS) focal plane through the fiber bundle to generate the high-resolution panoramic image was described. By eliminating the overlapping requirements of adjacent sub-images, the monocentric multi-scale system volume was reduced and the relative illumination and imaging quality were improved [198]. Compared with the previous first generation of monocentric multi-scale gigabit pixel cameras with Keplerian design, the total volume of Galilean monocentric multi-scale system can be reduced by 10 times. In [199], a novel architecture was defined for near monocentric imaging. The folding architecture provides a more miniaturized structure, especially when the structure includes auxiliary folding mirrors. In [200], researchers applied computational imaging theory to optical system design and developed a geometric aberration optimization function, which avoids using repeated iteration of optical system to define initial value of required system. It helped to improve the time efficiency of the system design. This design not only solved the contradiction between wide FoV and high resolution but also provided a new design idea for computational imaging. A new analytical model of Galileo Monocentric Multi-Scale (GMMS) and Keplerian Monocentric Multi-Scale (KMMS) was proposed in [58], which is more accurate than the paraxial form. The model\navoids the laborious analysis of different monocentric lens forms and maintains the key points of the Monocentric MultiScale (MMS) system. In addition, the correlation between GMMS and KMMS system design parameters was discussed in detail. The results showed that the GMMS system behaves better in aberration performance. The research provided a useful reference for the further application and development of monocentric lens system. The light field camera can still focus accurately and take clear photos under the condition of low-light and high-speed image movement. The concept of monocentric panoramic light field is proposed by combining a monocentric lens and light field sensors in [201]. The latest research showed that the system using the combination of monocentric lens and microlens array can significantly improve the imaging quality and light field reconstruction using the calculated point spread function model [202]."
        },
        {
            "heading": "F. Hyper-hemispheric Lens",
            "text": "The FoV of a common panoramic camera is greater than 200\u25e6. To realize super-large FoV imaging, a hyper-hemispheric lens [63] was proposed by Pernechele. The FoV angle of this ultra-wide-angle system is 360\u25e6\u00d7260\u25e6. A kind of bifocal\nhyper-hemispheric lens was mentioned in [62], [203]. The system layout of this hyper-hemispheric lens is shown in Fig. 8(a) and its unwrapped image is shown in Fig. 8(b). The panoramic channel uses a meniscus lens to collect and image 360\u25e6 of the surrounding light. There is a blind area in the center of the system. To avoid the waste of the imaging area of the sensor in the central blind area, the front lens group and the mirror can be used for imaging, and the imaging area can be regarded as the area of interest. The entire system has two focal lengths for the front channel and the panoramic channel. Using an even aspheric surface, Gong et al. [204] used a similar structure to design a panoramic imaging system with a full FoV of 360\u25e6\u00d7200\u25e6 without blind area. Another design concept of the dual-channel panoramic system was mentioned in [63]. By coating semi-reflective film, continuous imaging of front FoV and panoramic FoV can be realized. As shown in Fig. 8(c), the oval and square red targets and their connecting lines can form a continuous image in the image plane of the sensor. In [62], [64], a hyper-hemispheric lens (Fig. 8(d)) is capable of imaging the FoV with an azimuth angle of 360\u25e6 and a zenithal angle of 270\u25e6 was proposed. In this study, the theoretical optical quality, projection, and angular resolution of the hyper-hemispheric lens with a paraxial focal length of\n2 mm were discussed and analyzed in detail. The example pictures of the displayed hyper-hemispheric image proved that the design can successfully obtain the panoramic image with a ultra large FoV. The hyper-hemispheric lens can also be used as a multi-purpose panoramic camera for a star tracker to capture images. The attitude determination algorithm of space platform was proposed in [205]. The most advanced star tracker can accurately image as many stars as possible in a narrow- or moderate FoV, but its observation ability is limited by the FoV characteristics of the optical system. By combining algorithmic concepts from the fields of computer vision and robotics researches, such as template matching and point cloud registration, the method was used for star recognition. The system provides a stable and trustworthy initial attitude approach for space platforms such as satellites. It can estimate the attitude with an accuracy better than 1\u25e6 and evaluate the success rate of about 98%."
        },
        {
            "heading": "G. Panoramic Annular Lens",
            "text": "Nature provides a steady stream of inspiration for human innovations in bionic optics. In nature, a scallop usually possesses a visual system comprising up to 200 eyes that allow them to observe the surroundings through slits in the shell (Fig. 9(a)) [206]. Each eye of the Pecten scallop consists of a highly unusual double-layered retina, a cornea, a weakly refractive lens, and a concave mirror. Inspired by the distinctive imaging light path of refraction and reflection\nin the Pecten scallop eye [208], panoramic annular lens was first formally proposed by Greguss in 1986 [209]. The whole system is mainly composed of three parts: a panoramic annular lens block, a relay lens group and a sensor, as shown in Fig. 9(b). Different from the traditional fisheye lens that refracts ambient light through multiple lenses to form a panoramic image (Fig. 2), the panoramic annular lens is equipped with a catadioptric panoramic annular lens block to replace the lens group in the front of the fisheye lens. This catadioptric structure makes the panoramic annular lens more compact than the fisheye optical system. The 360\u25e6 ambient light around the panoramic annular system is converted into a small-angle beam after two refractions and two reflections in the panoramic annular lens block during imaging. The smallangle beam is imaged on the sensor after passing through the aperture diaphragm and the relay lens group to form a twodimensional annular panoramic image. According to the flat cylinder projection principle (Fig. 9(c)), the system can form a 360\u25e6 annular image with a half-FoV \u03b2, so it is called a panoramic annular lens. Due to the occlusion of the small mirror in the front part of the panoramic annular lens block, a blind area of the half-FoV \u03b1 will be formed in the center of the FoV.\nAfterwards, Powell studied and designed the infrared panoramic annular system [27], [210]. The prototype, raw image, and unwrapped image of the panoramic annular imaging system are shown in Fig. 9(d), which can realize\nhigh-definition panoramic imaging [207]. Using a multilayer diffractive optical element, Niu et al. [72] realized a longfocal-length panoramic annular system with an effective focal length of 10.8 mm and a diffraction efficiency greater than 99.3%. The designed panoramic annular system can be used to monitor cavity pipes, water wells, and mines, detect and record the rapid process of the rocket launch and explosion, spacecraft orientation, navigation system, and many other spaces that need to be measured [211]. Binocular stereo panoramic imaging can obtain and measure depth information [212]. Stray light will seriously affect the imaging quality in the panoramic optical system. The analysis and suppression of stray light in the optimization design can eliminate the influence of stray light and ensure that the captured image can still have good imaging ability in the face of strong light sources such as the sun [213]. [214] proposed new unwrapped and distortion correction methods to extract information from panoramic shots. The concept of using a panoramic annular lens as an endoscope was first proposed in 1991 [215], [216]. It is proven that this ultra-wide-angle optical system can also be used in holographic interferometry [217]. For a variety of use scenarios, [218] described the design details of a panoramic annular monitoring system, a deformation monitoring system, a mobile camera with panoramic annular accessories, and a panoramic annular endoscope system. [219] presented a panoramic endoscope imaging system based on the freeform surface. The front channel of the system uses a steering mirror, so that the front channel can enlarge the local details of the panoramic channel. Compared with the traditional wide FoV endoscope, this design does not need to rotate and uses the dual-channel FoV to realize panoramic observation and local magnification, which can reduce the diagnosis time and improve the lesion detection rate. In 2015, Wang et al. [74] applied ogive surface to the design of panoramic annular lens for the first time and obtained a panoramic annular system with an effective focal length of 10.375 mm.\nTo obtain more visual field information while keeping the system compact, Huang et al. [78] proposed an even ogive surface and a compact dual-channel panoramic optical system with FoVs of 360\u25e6\u00d7(38\u25e6\u223c80\u25e6) and 360\u25e6\u00d7(102\u25e6\u223c140\u25e6) in 2017. In 2020, a kind of ogive panoramic annular lens was fabricated [65]. It realized the ultra-low distortion design with a FoV of 360\u25e6\u00d7(40\u25e6\u223c95\u25e6) and f -\u03b8 distortion less than 0.8%. This ultra-low distortion design is especially suitable for computer vision tasks such as Simultaneous Localization and Mapping (SLAM) [85]. In terms of light field display, the pure horizontal parallax light field display of a panoramic camera was adopted to determine that it can provide smooth and continuous full parallax presentation for multiple viewers within panoramic FoV [220]. Q bfs aspheres can significantly reduce distortion in the design of panoramic annular lens [221], [222].\nStar tracker plays an essential role in satellite navigation. Star trackers on satellites in low earth orbit usually have two optical systems: one is to observe the contour of the earth and the other is to capture the position of stars. In [219], a star tracker optical observation system with a dual-channel panoramic annular system based on a dichroic filter was shown. It can observe the earth with a panoramic FoV range\nof 350\u223c360 nm band of 40\u25e6\u223c60\u25e6, and the front channel can capture stars far away from the system with a FoV range of 500\u223c800 nm band of 0\u25e6\u223c20\u25e6. In the aerospace field, the panoramic annular lens can be used to expand the FoV of astronauts in performing extravehicular activity [223]. The study [224] showed that the conic conformal dome of the aircraft has no obvious impact on the panoramic annular lens, and can even improve the imaging quality. It indicates that the panoramic annular lens is also suitable to be equipped in an optical window with conic conformal dome. The dualchannel panoramic annular lens structure can be used as an atmospheric detection optical system [225]. Compared with the traditional optical system, the optical system can observe the sky bottom view and panoramic side view at the same time, which improves the efficiency and accuracy of observation data inversion. The imaging spectrometer of traditional atmospheric detection usually uses a prism or grating to split the beam, and can only explore the atmosphere through singlemode single detection. Using the electronic filter developed based on the crystal acoustooptic effect, a novel panoramic imaging spectrometer [226] for atmospheric detection was designed. It can detect the hyperspectral atmospheric data of 10\u25e6 nadir FoV, 360\u25e6 limb FoV, and 0\u223c100 km limb height.\nThe use of binary diffractive surfaces in the design process of the panoramic annular lens is beneficial for improving the imaging effect and simplifying the optical structure [227]. To realize the design of super large FoV without a blind area, Wang et al. [228] proposed a dual-channel panoramic system without a blind area. The FoV of the whole system is 360\u25e6\u00d7230\u25e6. The FoV of the front channel is 360\u25e6\u00d7(0\u25e6\u223c56\u25e6), and the FoV of the panoramic channel is 360\u25e6\u00d7(55\u25e6\u223c115\u25e6). More optimization parameters can be obtained by using even aspheric surfaces, and the imaging performance of a larger FoV can be improved [229].\nWith the rapid development of sensor technology and the emergence of polarization sensor [230], panoramic perception is no longer limited to visible and infrared bands. Polarization sensing can make the traditional panoramic optical system obtain higher dimensional information and realize panoramic multidimensional sensing. An effective way to detect pavement conditions is to use polarization characteristics. The use of a panoramic annular camera and a polarization filter can capture vertical and horizontal polarization images at the same time without camera calibration [231]. The low spatial resolution of elevation angle of panoramic annular image in the vertical direction will lead to quality degradation of unfolded panoramic image. Using multiple panoramic annular lens images can generate high quality panoramic images, which can effectively solve this drawback [232]\u2013[234]. Using the particle filter can detect and track the target in panoramic image with high precision [235]. The multimodal vision sensor fusion technology [236] used a polarization camera, a stereo camera, and a panoramic annular lens to realize multidimensional environment perception for autonomous driving.\nWith the development of information technology (such as 5G and 6G), the high-resolution panoramic annular system [15] can record and transmit more panoramic information data in real-time. With the continuous deepening of the\npanoramic annular system, researchers have begun to focus on the realization of panoramic detection of different FoVs. The ultimate goal is to achieve search in a large FoV and accurate target observation and detection in a small FoV. By optimizing the azimuth characteristics of the panoramic annular lens, a two-channel panoramic annular system with double focal lengths was proposed in [237]. One of the channels has a smaller focal length and a wider-FoV coarse search channel and the other longer focal length and smaller-FoV channel is called the fine observation channel. Using XY polynomials freeform surface, Ma et al. [76] proposed a new zoom mechanism to achieve a panoramic zoom that does not deviate from the center of the FoV of the observation target. Afterwards, Wang et al. [77] proposed a zoom panorama optical system designed by a mechanical zoom method, which achieved a focal length change of 3.8\u223c6 mm and a maximum half FoV ranging from 65\u25e6 to 100\u25e6. The common zoom panoramic annular lens is based on the axial displacement between optical lens groups, which will enlarge the blind area. The zoom panoramic annular lens proposed in [238] is based on the Alvarez freedom surface of transverse displacement, which can enlarge the imaging area and keep the blind area unchanged. To improve design freedom degree and correct the color difference, Liu et al. [79] used a three-cemented panoramic annular block to design a panoramic annular lens with FoVs of 360\u25e6\u00d7(66\u25e6\u223c99\u25e6) and 360\u25e6\u00d7(94\u25e6\u223c120\u25e6), respectively. To design a new freeform surface panoramic system, using the YOZ plane-symmetric extended polynomial freeform surface, Bian et al. [239] offered a rectangular image field panoramic system with a FoV of 140\u25e6\u00d7(70\u25e6\u223c110\u25e6). Using annularlystitched aspheres technology, Chen et al. [240], [241] demonstrated a dual-view endoscope without blind area. The FoV of the front channel is 360\u25e6\u00d7(0\u25e6\u223c55\u25e6), and the FoV of the panoramic channel is 360\u25e6\u00d7(55\u25e6\u223c80\u25e6). The blind area of the panoramic annular lens reduces the sensor utilization. To solve this problem, Luo et al. proposed the methods of dichroic filter [80] and polarizing film [43] to eliminate the center blind area. To achieve a high-resolution panoramic system while maintaining a compact structure, [66] proposed a ray-tracing theory on the influence between the entrance pupil diameter and panoramic annular block parameters and successfully presented a high-resolution panoramic system with a compact ratio of 1.5 without blind area. There is no vignetting in the system and two channels share the relay lens group with the same parameters. To achieve a more compact and lightweight design, a panoramic annular system based on focal power distribution theory with Petzval sum correction was presented in [16]. The whole system with only three spherical lenses achieves a large FoV of 360\u25e6\u00d7(25\u25e6\u223c100\u25e6), and the enhancement of the panoramic image based on PALRestormer enables higher performance in various computer vision tasks.\nTo more clearly describe the vigorous progress of panoramic imaging systems in the last 20 years, the history of panoramic development and the comparison of key parameters of different panoramic imaging architectures are summarized in Fig. 10 and Table II, respectively."
        },
        {
            "heading": "IV. NEW ENGINES OF PANORAMIC OPTICAL SYSTEMS",
            "text": ""
        },
        {
            "heading": "A. Freeform Surface",
            "text": "With the continuous development of advanced manufacturing technology and testing technology, freeform optical elements make it possible for compact and high-image-quality imaging optical systems [82], [244]\u2013[246]. Optical freeform surfaces in imaging optical design generally refer to optical surfaces that do not have axis-rotational symmetry or translation-rotational symmetry [82], [247]\u2013[249]. Compared with the traditional spherical surface, the freeform surface can provide more degrees of freedom, significantly improve the optical performance of the imaging system, and reduce the volume of the system [250] (Fig. 11(a)). This technology has significantly promoted applications from science to a wide range of fields, such as extreme ultraviolet lithography and space optics [13]. Freeform optics is a concurrent engineering process that combines design, manufacturing, testing, and assembly. The continuous development of freeform optics has important significance and value for the whole human society to perceive the panoramic environment.\nThe traditional panoramic lens design uses spherical lens. To improve the freedom of optical designers in designing the panoramic system, the use of rotationally symmetric aspheric surfaces can give panoramic imaging systems a new opportunity. Thanks to the multi-degree of freedom design parameters of the aspheric surface, the panoramic imaging system can realize the system parameters, structure, and functions that are difficult to be realized in traditional panoramic systems. The proposed odd aspheric surface [142], even aspheric surface [229], Q-type aspheric surface [252]\u2013[254], extended polynomial aspheric surface [239], annularly-stitched aspheric surface [167], [255], ogive aspheric surface [65], [74], and even ogive aspheric surface [78] have been proved to be successfully applied to the design of panoramic systems. The asymmetric freeform surface also makes the panoramic optical system have a new zoom function, including Alvarez surface freeform surface [238] and XY polynomial aspheric surface [76]. In the future, we believe that the new freeform surface technology can make panoramic imaging optical systems obtain more exciting new functions, high imaging quality, compact volume, and so on.\nAlthough freeform surfaces have revolutionized the field of optical design in the last decade, there are still some challenges and some directions to be explored in this technology [82]. In the design area, more powerful global optimization algorithms need to be explored to achieve fast, accurate, and generalized direct point-by-point algorithms. In terms of applications, there is an urgent need to further improve the accuracy and efficiency of freeform surfaces for fabrication, testing, and assembly. In conclusion, the freeform technology has much potential for exploration and research, and is a powerful engine for the development of panoramic imaging technology."
        },
        {
            "heading": "B. Thin-plate Optics",
            "text": "Thin-plate optics is a new imaging approach that uses Fresnel optics or diffractive optics or other optical elements to design thin-plate lenses that are combined with computational\nimaging techniques to recover or enhance images [83]. This technique allows the use of lightweight optical elements to build a compact optical system with the same or better image quality than conventional bulky imaging system. The thinplate optical technology will facilitate the lightweight and miniaturization of the panoramic imaging system.\nDifferent from traditional optical imaging, computational imaging technology [256]\u2013[258] can encode and decode optical information captured by optical instruments in all directions from information acquisition, information transmission, and information conversion in terms of imaging principles. Computational imaging can acquire and analyze multidimensional information of the light field through scattering, polarization, and bionic technologies, and has many advantages in achieving large detection distance, high resolution [259], [260], high signal-to-noise ratio, multidimensional information [261], light weight [262], simplicity, and cheapness. For decades, optics researchers have been working to design compact optical systems with large FoV and light weight [263], [264]. A Fresnel lens [265] is an optical element that reduces the thickness of the lens while maintaining the shape of the lens curvature and can be used as a lightweight alternative to traditional continuous surface lenses. Due to its ability to reduce the thickness of the optical system, it is widely used in non-imaging fields such as illumination, solar concentrators, and collimators. When the processing of the optical lens\nsurface is close to the light wavelength of the imaging band, the transmission of light will no longer conform to the three transmission laws of geometric optics (straight propagation, refraction, and reflection), and a diffraction effect will occur. At this time, the optical element is called diffractive optical elements. An optical system designed with diffractive elements needs to ensure that the entire optical system has a high diffraction efficiency. Combining computational imaging technology and Fresnel/diffractive optical elements, thin-plate optics technology came into being [83]. Using diffraction-coded lens to form significantly different point spread functions for different spectral distributions, joint computational imaging technology can achieve effective phase difference recovery and image reconstruction [251] (Fig. 11(b)). With computational imaging technology, lightweight optical systems with large FoVs using Fresnel lenses or diffractive optical elements can achieve imaging quality close to that of traditional complex optical systems, enabling simple systems perception of large FoVs [266]. This next-generation imaging optical system using Fresnel or Refractive-Diffractive hybrid and computational imaging can be used to build the future of computational imaging for thin- and lightweight panoramic imaging.\nWith the continuous development of optical micro-nano processing technology, the processing of thin-plate optical technology has gradually matured, but there are still some challenges to be solved. In terms of principle, diffractive optical elements need to be combined with new design algorithms to improve the numerical stability and convergence speed of the design results [267]. In terms of performance, the diffraction efficiency needs to be improved and the wavelength range needs to be further expanded. In terms of applications, the exploration of thin-plate optical technology in the field of\ncomputational imaging will be more far-reaching, expanding the direction of new panoramic intelligent perception."
        },
        {
            "heading": "C. Wide-FoV Metalens",
            "text": "At present, with the rapid development of optical technology and manufacturing technology, the miniaturization of optical systems has become a focus of researches. For scene perception, wearable devices, medical devices, aerial photography, and other fields, miniaturized optical systems are favored [187], [268]\u2013[272]. The miniaturization design of traditional optical system checks and balances with high resolution, high imaging quality, and processability, which has great design challenges and processing difficulties. As a new micro-nano surface technology, metasurface has shown great potential and the ability to overcome the physical limitations of traditional optical lenses [273], [274]. The metasurface is a kind of two-dimensional metamaterial [275]. Metamaterials are generally composed of subwavelength metal or dielectric units, which show electromagnetic characteristics different from existing materials in nature, such as negative refraction, optical stealth, and so on [276]\u2013[278]. Traditional optical lenses accumulate optical path through the change of thickness and produce phase gradient, to realize the regulation of the wavefront. When light hits a subwavelength scatterer, its phase will undergo a sudden change, that is, a discontinuous change. By arranging this scatterer into a surface, and then precisely controlling the structure of each unit to control the phase of the light, it is possible to make the light converges to a point [81], [279]. This is called a metalens. Compared with the traditional optical lens, the metalens has ultra-thin size and weight. The beam can be focused to the diffraction limit\nand has an ultra-short focal length [280], [281]. It has broad application prospects in the visible light band, terahertz band, microwave band, and so on [282]\u2013[286].\nTo realize large FoV [14], [84], [287]\u2013[289] (Fig. 11(c)), achromatic [290]\u2013[293], broadband [294], [295], high spectral resolution [296] and other characteristics of metalenses, researchers have carried out a series of in-depth studies on metalenses, and several representative works are shown in Table III [268]. As an ultrathin optical components [297], metalens technology provides a new design idea for the panoramic system, and the commercial panoramic imaging system with small volume and high performance will become possible.\nMetalens has great potential and value in miniaturized imaging, but there are still some challenges [298], [299]. In terms of design, the chromatic aberration of the metalens needs to be solved, so the metalens with broad spectrum would still a research hotspot in the future. The focusing efficiency of metalens still needs to be improved [300]. In the overall optimization of meta-based imaging systems, it is still difficult to achieve efficient loop data transfer and iteration without the help of custom codes.\nIn terms of fabrication, the diameter size of metalens needs to be increased in order to integrate into conventional optical systems to realize novel optical systems. In the future, panoramic imaging instruments with hyper-hemispheric FoV can be realized by combining conventional optics and metalens technology to achieve higher-performance miniature panoramic scene perception."
        },
        {
            "heading": "V. APPLICATIONS OF PANORAMIC IMAGING SYSTEMS",
            "text": "Thanks to the ultra-wide 360\u25e6 FoV offered by panoramic cameras, they have been applied in many scene understanding tasks [236]. In the following subsections, we review the applications with panoramic image systems, for robotics applications like robot navigation and autonomous driving."
        },
        {
            "heading": "A. Semantic Scene Understanding with Image Segmentation",
            "text": "Semantic scene perception is an essential task in robotics, as it enables a dense understanding at the pixel level, where the location and category information can be both precisely extracted, offering abundant cues for upper-level navigation operations. Semantic scene understanding is usually achieved via semantic image segmentation. When working with 360\u25e6 panoramic cameras, a holistic and comprehensive scene segmentation can be attained for an entire surrounding perception [306], as shown in Fig. 12. Fisheye camera based semantic segmentation. Thanks to the breakthrough of Fully Convolutional Networks (FCNs) [312], semantic segmentation can be performed in an end-to-end fashion and the field has been significantly advanced with diverse segmentation model architecture- and dataset developments. In the wide-FoV semantic segmentation, fisheye- and panoramic cameras have been applied. Yet, current deep segmentation models are data-driven, and most existing semantic segmentation datasets are established for pinhole, narrowFoV, distortion-free images. To train an accurate and robust semantic segmentation model that can generalize well in realworld scenes, large-scale annotated datasets are important. Annotating semantic labels at the pixel level is extremely laborintensive and time-consuming, in particular for panoramic images as they have larger distortions, higher complexities due to the wide-FoV, and more small objects in general. Thereby, most wide-FoV semantic segmentation research works have to deal with the data scarcity issue, aiming to produce robust real-world surrounding parsing models, which are vital for reliable scene understanding. Deng et al. [307] first presented a fisheye image semantic segmentation framework (Fig. 12(a)), which transforms an existing pinhole urban scene segmentation dataset to a synthetic dataset for training and designs an overlapping pyramid pooling based segmentation module to explore both local, global, and region context information. In particular, the transformation is based on a zoom augmentation method by changing the focal length of the simulated fisheye camera. They further built a surround-view surrounding road scene segmentation system by using four fisheye cameras and a deformable-convolution-based multi-task learning regimen [308], involving real-world and transformed surroundview images for training. Extending the zoom transformation method, Ye et al. [313] designed an online seven-DoF augmentation method, to synthesize fisheye images captured via sensors of different shooting orientations, postures, and focal lengths, improving the robustness against different distorted fisheye image data. Panoramic camera based semantic segmentation. For panoramic surrounding perception based on a single camera, Yang et al. [207] designed a Panoramic Annular Semantic Segmentation (PASS) framework (Fig. 12(b)) by using a panoramic annular camera with an entire FoV of\n360\u25e6\u00d7(30\u25e6\u223c105\u25e6). Considering that the distortion in such cameras are lowered in less than 1% [222] and the imaging model complies with a clear f -\u03b8 law, the PASS framework reused large-scale pinhole data for training with a network adaptation method that exploits the correspondence between convolutional features learned from pinhole data and features inferred from panorama segments with a similar FoV. They achieved seamless semantic segmentation results by using ring-padding and cross-segment padding, together with a variety of data augmentation methods including texture-, geometric-, distortion-, and style augmentation to robustify panoramic segmentation. DS-PASS [314] improves PASS by using a more efficient segmentation model and augmenting the detail sensitivity via attention connections between detailpreserved low-level shallow encoder layers and semanticrich high-level deep decoder layers. They have deployed the framework on both mobile robotics and instrumented vehicles with a lightweight panoramic annular camera with a FoV of 360\u25e6\u00d7(50\u25e6\u223c120\u25e6), and verified the benefits of surrounding semantic sensing for upper-level navigation applications like visual odometry. Further, multi-source omni-supervised learning was leveraged in [315] to cover panoramic data in the training stage. This helps to reach robust segmentation performance, meanwhile bypassing complex panorama separation and network adaptation used in PASS [207] which incurs high computation overhead and cannot make use of the important global context cues as the panorama is divided into discrete segments. The omni-supervised learning framework was extended by learning spatial positional priors [316] and omni-range contextual dependencies [317] that can stretch across the entire 360\u25e6. Panoramic semantic segmentation has also been applied with unmanned aerial vehicles in [67], [318] for drone-perspective panoramic remote sensing with a FoV of 360\u25e6\u00d7(30\u25e6\u223c100\u25e6). Panoramic segmentation from an adaptation perspective. More recently, to address the dearth of annotated panoramic images, researchers have explicitly formalized panoramic segmentation as an unsupervised domain adaptation problem or a domain generalization problem by transferring from the data-rich pinhole domain to the data-scarce panoramic domain [309], [311]. For domain adaptation, one can use labeled pinhole data as the source domain and unlabeled panoramic images as the target domain. For domain generalization, one can only use the source domain images, with the aim to produce a robust, generalized segmentation model in the target domain. P2PDA [309], [310] (Fig. 12(c)) designed attentionaugmented domain adaptation modules to detect and magnify the pinhole-panoramic correspondences in multiple spaces. Jaus et al. [311], [319] introduced panoramic panoptic segmentation, which extended panoramic semantic segmentation by also offering instance-level understanding (Fig. 12(d)). It outputs both pixel-level semantic classes and instance IDs, and thereby enabling a more holistic surrounding perception. By appending a dense contrastive pretraining stage, they obtained significant generalization gains when segmenting unseen panoramic images. Panoramic panoptic video segmen-\ntation is also addressed with the Waymo open dataset [320] covering a FoV of 220\u25e6. Distortions in panoramic images have been taken into considerations. Many researchers opt to directly perform segmentation models on the equirectangular panoramic images with distortions and come up with distortion-aware model designs. Hu et al. [7] considered the panoramic image generation process and tackled distortions via a deformable-convolution-based module. Zhang et al. [321], [322] developed a distortion-aware transformer model for adapting to panoramic images. The transformer model is integrated with both deformable patch embedding and deformable MLP modules, which well tackle the distortions and enhance the robustness of both indoor and outdoor panoramic segmentation. Zheng et al. [323] used complementary feature representations to provide implicit distortion distribution priors for guiding indoor panoramic semantic segmentation. Moreover, there are many other fisheye [324]\u2013 [327], panoramic [328]\u2013[330], and omnidirectional [331]\u2013 [334] semantic segmentation works in the field, evidencing the relevance of wide-FoV semantic scene understanding for real-world applications.\nAs panoramic images usually have a large FoV and high resolution, researchers have also taken this into account by designing compact models to enable fast response for efficiency-critical applications. For example, PASS [207] and DS-PASS [314] use efficient backbones, but enhance the performance by using attention blocks either in the decoder or in the lateral connections. The framework in [315] makes use of knowledge distillation to boost the performance of the compact models. The panoramic panoptic segmentation framework [319] also substitutes all investigated panoptic backbones with the efficient ResNet-18 [335] to support mobile agents operating in real-world traffic scenarios. They also show that their efficient modifications become more important when feeding images of higher resolutions into the model."
        },
        {
            "heading": "B. Geometric Scene Understanding with Depth Estimation",
            "text": "Panoramic depth estimation is another important task, which aims to predict dense depth maps based on a panoramic camera, and it is relevant for many robotics applications as it enables computer systems to obtain 360\u25e6 geometric information and understand the surrounding 3D world. There are two main categories of panoramic depth estimation methods, which are supervised and unsupervised methods, whose representative examples are provided in Fig. 13. Supervised methods. \u201cSupervised\u201d means that the depth estimation methods directly have the corresponding ground-truth depth information to learn. In the supervised category, OmniDepth [338] generated synthetic panoramic datasets for supervised learning, via rending existing large-scale 3D datasets, which enables direct depth estimation on omnidirectional content in the form of equirectangular images. DistConv [339] put forward distortion-aware convolution filters, which endows a pinhole-trained model to smoothly work on equirectangular projection of panoramic data. In [340], [341], projection transformation is used to adapt existing rectilinear image datasets into equirectangular ones for learning panoramic\ndepth estimation. They also leverage ring padding to enhance the continuity at the panorama border for a seamless estimation, which is similarly considered in panoramic semantic segmentation [207], [306] In [342], geometric structures are used as a prior to aid supervised panoramic depth estimation. Moreover, HoHoNet [343] and SliceNet [344] first squeezed the extracted feature maps into a horizontal 1D representation due to the assumption that indoor panoramas are aligned to the gravity vector, and then recovered the dense depth map predictions in the equirectangular projection. Unlike previous methods that calculated the shapes of convolution kernels according to the latitude coordinates [339] in a fixed manner, ACDNet [345] combined the convolution kernels with different dilation rates in an adaptive manner and improved the estimation in the equirectangular projection. There are also fusiondriven methods with the aim to exploit the advantages of both equirectangular- and cube-map projections [336], [346], [347], with methods including bidirectional fusion [336], [348], unidirectional fusion [346], and usage of an asymmetric architecture that leverages a transformer model in the cube-map branch and a convolutional neural network in the equirectangular branch [347]. OmniFusion [349] transforms a 360\u25e6 panorama into perspective image patches (i.e., tangent images) to attain patch-wise predictions and fuses the image features with 3D geometric features to enhance depth estimation quality. Such tangent image representations have also been used in [350] for estimating high-resolution depth maps. Regarding the advantages of the projection representations, equirectangular projection provides full-FoV observations and cube-map projection offers distortion-free representations [336] (see Fig. 13(a)). More recently, PanoFormer [351] put forward a panoramic transformer model to perform depth estimation, where the patch division is conducted in the spherical tangent domain to tackle distortions and geometric structures are modeled by using adding learnable token flows in the self-attention layers. Unsupervised methods. \u201cUnsupervised\u201d means that the depth estimation methods do not require ground-truth depth information at the training time and thereby reduce the data preparation burden, which are beneficial for real applications. Unsupervised panoramic depth estimation is less visited compared to the supervised category, but it started to receive more attention. Zioulis et al. [352] formalized a full spherical disparity model and established a new untouched supervision scheme for spherical view synthesis using depth-image-basedrendering combined with spherical attention. Thereby, they facilitated a self-supervised monocular 360\u25e6 depth learning paradigm via spherical view synthesis. OlaNet [337] made an early self-supervised attempt to predict depth information in the equirectangular projection of 360\u25e6 images. They leveraged a combination of an anto-encoder based on convolutional neural networks, distortion-aware spherical view synthesis with depth-image-based rendering, and polar angle distortion compensation, to tackle the challenging self-supervised 360\u25e6 depth prediction task (Fig. 13(b)). Zhou et al. [353] extended PADENet [341] and combined unsupervised and supervised learning to improve panoramic depth estimation specifically in indoor scenes. In [354], gravity-aligned videos were used to facilitate self-supervised 360\u25e6 depth learning. They also com-\nbined supervised- and self-supervised learning to compensate the weaknesses of each learning method towards more accurate depth estimation. More recently, in [355], multi-modal masked pretraining was used to enhance panoramic depth completion, which aims to complete the depth channel of a single 360\u25e6 RGB-D pair captured from a panoramic 3D camera. Panoramic depth estimation and completion were also potentially useful for improving the perception ability and operational efficiency of mobile robots in large-scale environments [356].\nC. Visual Localization, Odometry, and SLAM In this subsection, we review important applications of panoramic cameras related to the localization and mapping in robotic environments, including panoramic visual localization, panoramic visual odometry, and panoramic Simultaneous Localization And Mapping (SLAM), as shown in Fig. 14. Panoramic visual localization. Visual localization is a task that recovers the camera location from database images based on the query images, which is important for many applications like intelligent vehicles and assisted pedestrian navigation [357]. To overcome the challenges in matching query and database images with various appearance variations like illumination variations, moving object variations, and orientation variations, panoramic cameras are usually applied. In [358], location recognition was revisited by conducting a panorama-to-panorama matching process and they found that it is preferable to aggregate individual pinhole views into a vector for matching, rather than extracting a single descriptor from an explicit panorama. In [359], omnidirectional observation was integrated to perform visual localization and found that the front- and back views are the most descriptive for long-term place recognition. Cheng et al. [357] proposed a Panoramic Annular Localizer (see Fig. 14(a)), which incorporates panoramic annular lens and active deep image descriptors in a visual localization system. CFVL [360], [361] designed a coarse-to-fine visual localization framework by using equirectangular representations in the coarse stage to keep feature consistency with the panorama-trained descriptor, and cube-map representations in the fine stage of key-point matching to conform to the planarity of the images. In [362], a localization framework is designed by using high-level semantic information such as detected landmarks and ground surface\nboundaries, which are obtained via an omnidirectional camera, and their localization is conducted via extended Kalman filter. In [363], semantic similarity was checked between query- and database images based on contrastive learning on a dataset of semantically-segmented images. Their database images are generated from panoramas which present wide-FoV that facilitates to correctly localize the query images where standardFoV cameras may fail with non-overlapping FoVs. Moreover, rotation equivariant orientation estimation [364] and semantic graph embedding lifted in 3D space [365] are explored for panoramic place recognition based on omnidirectional images.\nPanoramic visual odometry. Visual odometry estimates the egomotion of an agent by using only the continuous images captured by cameras attached to it [367]. One of the shared premise in visual odometry methods is that there must be enough overlaps between two adjacent frames for computing the relative motion. Pinhole-camera-based methods inevitably suffer in rapid-motion scenarios or dynamic scenarios with moving objects which may occupy most of the limited FoV. Incorporating panoramic observations is potential to help address the challenges in such scenarios. In [369], the benefits of using large-FoV cameras (e.g., fisheye or catadioptric) were investigated. They pointed out that wide-angle observations are advantageous in indoor scenes with more evenly-distributed features and the camera can help track features for long-term autonomy. However, in outdoor scenes, the loss of angular resolution for wider FoVs is intensified due to the higher depth range, and using large-FoV cameras is less accurate. PVO [370] took advantage of panoramic cameras by modifying the initialization process in the visual odometry and redefining the re-projection errors according to the panoramic camera model, which achieved higher precision in translation and rotation. Further, stereo omnidirectional cameras have been put use in [371], [372]. For example, dual fisheye cameras are used in [371] to obtain an ultra-wide FoV that ensures big image overlaps between frames and more spatially distributed points, where their joint optimization model helped to attain improved robustness over a pinhole camera model. Chen et al. [367] presented PALVO (Fig. 14(b)) by applying a Panoramic Annular Lens. Based on the panoramic annular camera model, they adapted initialization-, tracking-, and\ndepth filter modules, which enabled PALVO to reach high robustness to rapid motion and dynamic objects. In [373], semantic information were used to further enhance PALVO, with semantic-guided keypoint extraction and correspondence search along epipolar curve, as well as semantic-based examination in pose optimization, which achieved more robust pose estimation. The Direct Sparse Odometry (DSO) [374] has also been augmented by attaining omnidirectional perception towards real-world applications [371], [375], [376]. For example, panoramic visual odometry has been applied with wearable devices to help people with visual impairments in indoor positioning and navigation [376].\nPanoramic visual SLAM. Visual SLAM is a technology that estimates the poses of cameras and reconstructs the scenes, which usually extends the methods of visual odoemtry. In [377], a large-scale direct SLAM method is developed for panoramic- or wide-FoV fisheye cameras by directly formulating the tracking and mapping according to the omnidirectional camera model. By using two different omnidirectional imaging models, their system makes it possible to use a broad range of classical dioptric- or catadioptric imaging systems. They have verified their approach on data captured by a 185\u25e6- FoV fisheye lens, and observed improved localization accuracy and enhanced robustness against fast camera rotations compared to the baseline approach using a standard camera. In [368], the ORB-SLAM2 framework [378] was extended by utilizing a unified spherical camera model and the semidense feature mapping. They validated their approach on realworld data collected by fisheye cameras set up on a car, of which the capturing FoV is around 185\u25e6 (see Fig. 14(c)). Panoramic annular camera has been applied in [85], [379]. PA-SLAM [379] extended PALVO [367] to a full panoramic visual SLAM system with loop closure and global optimization. They used a panoramic annular lens with a FoV of 360\u25e6\u00d7(30\u25e6\u223c92\u25e6) and a global shutter camera for experiments. They confirmed the better performance of PA-SLAM with respect to ORB-SLAM2 [378] and a cube-map-based fisheye SLAM method [380] on real-world data. PAL-SLAM [85] designed a feature-based SLAM for panoramic annular lens with a FoV of 360\u25e6\u00d7(45\u25e6\u223c85\u25e6). They used a filtering mask to acquire features from the annular useful area, in order to conduct carefully-designed initialization, tracking, mapping, loop detecting, and global optimization. In this pipeline, they utilized the calibrated panoramic annular camera model to translate the mapped features onto a unit vector for the subsequent algorithms. The robust localization performance of\nPAL-SLAM are confirmed on large-scale outdoor and indoor panorama sequences, showing the advantages in comparison to a traditional SLAM method based on a camera with a narrow FoV. Additionally, there are some more mapping systems like landmark-tree mapping [381] and freespace mapping [382], [383] based on a single omnidirectional camera, 360ST-Mapping [384] based on online semantic-guided scene topological map reconstruction, as well as UPSLAM [385] based on a graph of panoramic images.\nFurthermore, panoramic visual odometry and SLAM have also been implemented based on multiple sensors, e.g., using multi-cameras [386]\u2013[388], with inertial sensors [389]\u2013[392], GPS [393], and LiDAR sensors [394]. Among them, LFVIO [392], [395] put forward a universal framework that can work with diverse large-FoV cameras, where imaging points may even appear on the negative half plane. They verified the effectiveness of their visual-inertial-odometry framework on panoramic annular cameras with a FoV of 360\u25e6\u00d7(40\u25e6\u223c120\u25e6) and fisheye cameras with a FoV of 360\u25e6\u00d7(0\u25e6\u223c93.5\u25e6), as well as sensor fusion systems with LiDAR measurements."
        },
        {
            "heading": "D. Optical Flow Estimation, Object Detection, and Others",
            "text": "In addition to semantic segmentation and depth estimation, there are many potential information worth mining in panoramic images, such as optical flow estimation and object detection, as shown in Fig. 15. In this subsection, we review these panoramic vision tasks respectively. Optical flow estimation. Optical flow reflects pixel-by-pixel correspondence on time-series images and can be obtained by learning-based methods [400]. Flow estimation in panoramic content is an essential task, which enables autonomous vehicles and robots to acquire temporal cues of the surrounding scenes. Due to the lack of large-scale panoramic optical flow datasets, research on 360\u25e6 flow estimation mainly focuses on the domain adaptation. In order to leverage existing perspective models and benefit from advances in optical flow networks, two common perspectives are adapting models and datasets respectively. OmniFlowNet [401] adapted the famous LiteFlowNet [402] that designed for pinhole cameras to panoramic images and does not require retraining. Considering the equirectangular distortion imported by the spherical camera, they used the distortion-aware convolution to replace the regular grid-based 2D convolution via revising the coordinates of the convolution kernel. To address the distortion that introduced by spherical-to-plane projection in panoramic content, Bhandari et al. [403] proposed a spherical data augmentation to project perspective flow field onto the sphere and then reprojected to the equirectangular format. However, this data augmentation is offline and not flexible, and the perspective dataset should be transformed in advance for training. Yuan et al. [404] introduced a 360\u25e6 flow optical flow method by estimating on the tangent images. They directly use the off-the-shelf perspective optical flow method to estimate the flow field of panoramic images, and then apply the optical flow estimated under cubemap and icosahedron projection to benefit the panoramic flow estimation. OmniFlow [405] explored the human omnidirectional optical flow in a 3D indoor environment with a rendering engine, and generated a synthetic 360\u25e6 human optical flow dataset. Among these approaches, PanoFlow [396] (see Fig. 15(a)) presented a generic\ncyclic flow estimation framework, which can be adapted to any existing perspective learning-based model, e.g., RAFT [406] and CSFlow [407]. They generated the publicly available panoramic flow dataset Flow360 under city street scenes to facilitate training and quantitative analysis, and verified the precision and efficiency of their approach via a panoramic annular camera with a FoV of 360\u25e6\u00d7(30\u25e6\u223c90\u25e6). Object detection. Object detection in the surrounding environment is also a demanded task as the increasing interest in autonomous mobile systems such as drones and assistant robots. Early panoramic object detection systems [408], [409] used hand-designed operators, and the subsequent emergence of CNN has given new vitality to this field. Garanderie et al. [340] extended the Faster R-CNN [410] to support 3D object pose regression. Without changes to the network architecture, their extended model can be used on either rectilinear or equirectangular content by recovering 3D position using image transformation matrix and spherical coordinate projection. Yang et al. [411] leveraged VR videos on YouTube and proposed a bounding-FoV annotation to create a equirectangular panorama dataset for object detection. They then adapted YOLO [412] to panoramic contents with re-projections to realize panoramic object detection. Wang et al. [397] (Fig. 15(b)) proposed a distortion data augmentation method that uses the cropped car images to randomly back project in the equirectangular image plane. Considering that distortion of large objects is more pronounced, they aggregated the position information of anchor box into the network. Guerrero-Viu et al. [413] took advantage of the off-the-shelf layout estimation method [414] to simultaneously obtain the 2D object segmentation masks and the 3D bounding boxes. They also replaced all standard convolutions with EquiConvs [414] to implicitly adapt convolution kernels\u2019 shape and size according to the distortions. Other downstream tasks. In addition to the above applications, panoramic cameras are also integrated with deep learning algorithms in other fields. Layout Estimation: The 3D layout of a room specifies the positions, orientations, and the heights of the walls according to the vision point. Estimating the layout of a room by a single panorama can benefit robotics and VR/AR applications. LayoutNet [415] is a deep CNN that estimates the layout of an indoor scene from a single perspective or a panoramic image. They extended the annotations for the Stanford2D3D dataset [416] with room layout annotations. DuLa-Net [398] (Fig. 15(c)) fused the estimation under equirectangular images and perspective ceiling images via a two-stream network architecture. They also introduced Realtor360 dataset, which consist of Manhattan-world room layouts in panoramic format. CFL [414] proposed EquiConvs by deforming the shape of the kernels according to the geometrical priors of panoramas. It can predict complex geometries like non-Manhattan structures. More recently, DeepPanoContext [417] was introduced as the first learning-based pipeline that recovers 3D room layout and detailed shape, pose, and location for objects in the scene from a single panoramic image for holistic 3D scene understanding. Anomalous Detection: Anomalous detection is a task to detect anomalous objects that in a specific scene or behaving dangerously, which is important for intelligent panoramic video surveillance systems. BenitoPicazo et al. [418] developed a low energy consumption system on a raspberry for video anomalous object detection. The panorama videos are obtained via a Point Grey Ladybug 3 Spherical camera and then fed to the CNN to determine whether it contains any anomalous object. Saliency Prediction: Saliency prediction is a popular computer vision task which means predicting where humans look in an given image [419]. Consecutive saliency prediction in panoramic videos is vital for applications such as viewpoint guidance. Zhang et al. [420] explored the utility of the surrounding cue for saliency detection, and proposed BMS, which is a saliency model using simple image processing operations to leverage the topological structural cue in panoramas. Cheng et al. [399] (Fig. 15(d)) put forward a weakly-supervised spatial-temporal saliency prediction model to overcome large viewpoint variations in 360\u25e6 videos. They also collected a Wild-360 dataset where one-third frames are annotated with saliency heatmaps."
        },
        {
            "heading": "VI. CONCLUSIONS AND PERSPECTIVES",
            "text": "The panoramic imaging optical system is given a high expectation of future intelligent perception sensor. It has shown great application potential in current scene understanding applications such as autonomous driving and robotics systems. Associated with the expectations are the challenges of design and fabrication, compressing a high-performance panoramic system into a compact space for multidimensional panoramic information acquisition. Although the parametric limitations of optical systems have constituted a huge obstacle on this path, innovative optical architectures, and emerging optical technologies have lit the light of this path. The rise of ultra-precision manufacturing provides strong support for micro-nano processing such as freeform surfaces, thin-plate optics, and metalens. These new optical engines provide an effective alternative to traditional optical components and support more design freedom to achieve new capabilities and high performance beyond traditional geometric optics. In the future, the panoramic imaging system will combine these emerging technologies towards compact, intelligent and multidimensional development to achieve more powerful panoramic scene environment perception and surrounding understanding. This review article conducts a systematic and detailed literature analysis on the knowledge of panoramic optical systems and downstream perception applications, and clarifies the advantages, progress, limitations, future challenges, and directions for the improvement of panoramic optical systems. The article identifies the valuable challenges that panoramic perception needs to face from the aspects of optical system and algorithm application, including the following: 1) The new optical surface technology will contribute to the miniaturization, lightweight, and high performance of panoramic imaging systems. 2) Multi-FoV, multi-functional (e.g. zoom, polarization), high temporal resolution, and high spectral multidimensional perception of the panoramic optical systems will facilitate more new applications of scene understanding.\n3) Based on the fusion of new intelligent sensors and computational imaging, the development of panoramic optical instruments for scene understanding will be promoted.\n4) Combining a panoramic optical system with an ultrawide FoV and an event sensor with an ultra dynamic range or an active sensing LiDAR with a same projection behavior for multi-modal robust surrounding perception.\n5) Considering that the annotation on the entire panoramas is difficult and expensive to obtain, exploring algorithms with unsupervised or weakly supervised learning, or adapted from the existing pinhole domain to the panoramic domain will benefit practical applications.\nIn general, with the unique advantages of a 360\u25e6 large FoV, the panoramic optical systems will combine emerging sensors, ultra-precise optical manufacturing, computational imaging, and artificial intelligence to achieve ultra-high performance, new functions, small volume, and multidimensional sensing and low-power panoramic intelligent instruments to enhance the way humans observe the world and have a deeper and more realistic perception of the surroundings. The panoramic imaging system has transformed from traditional optics to intelligent computing optics, and will be applied in various applications such as autonomous driving, medical health, security monitoring, deep space exploration, deep sea exploration, celestial terrain detection, and intelligent robots, greatly improving our life and the way to perceive the environment."
        }
    ],
    "title": "Review on Panoramic Imaging and Its Applications in Scene Understanding",
    "year": 2022
}