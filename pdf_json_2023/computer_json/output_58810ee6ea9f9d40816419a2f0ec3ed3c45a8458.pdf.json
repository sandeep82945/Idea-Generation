{
    "abstractText": "Contrastive learning has emerged as a premier method for learning representations with or without supervision. Recent studies have shown its utility in graph representation learning for pretraining. Despite successes, the understanding of how to design effective graph augmentations that can capture structural properties common to many different types of downstream graphs remains incomplete. We propose a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective, enabling contrastive learning to capture useful structural representation from pretraining graph datasets. We first present a spectral graph cropping augmentation that involves filtering nodes by applying thresholds to the eigenvalues of the leading Laplacian eigenvectors. Our second novel augmentation reorders the graph frequency components in a structural Laplacianderived position graph embedding. Further, we introduce a method that leads to improved views of local subgraphs by performing alignment via global random walk embeddings. Our experimental results indicate consistent improvements in out-of-domain graph data transfer compared to state-of-the-art graph contrastive learning methods, shedding light on how to design a graph learner that is able to learn structural properties common to diverse graph types.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amur Ghose"
        },
        {
            "affiliations": [],
            "name": "Yingxue Zhang"
        },
        {
            "affiliations": [],
            "name": "Jianye Hao"
        },
        {
            "affiliations": [],
            "name": "Mark Coates"
        }
    ],
    "id": "SP:d2a52715fefef830e38b0fc6931b4b9de911e9dd",
    "references": [
        {
            "authors": [
                "M. Belkin",
                "P. Niyogi"
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "S. Chanpuriya",
                "C. Musco"
            ],
            "title": "Infinitewalk: Deep network embeddings as laplacian embeddings with a nonlinearity",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "M. Chen",
                "Z. Wei",
                "Z. Huang",
                "B. Ding",
                "Y. Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "M. Heimann",
                "F. Vahedian",
                "D. Koutra"
            ],
            "title": "Conealign: Consistent network alignment with proximitypreserving node embedding",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "G. Chu",
                "X. Wang",
                "C. Shi",
                "X. Jiang"
            ],
            "title": "Cuco: Graph representation with curriculum contrastive learning",
            "venue": "In Proc. Int. Joint Conf. Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "F.R. Chung"
            ],
            "title": "Spectral graph theory, volume 92",
            "venue": "American Mathematical Soc.,",
            "year": 1997
        },
        {
            "authors": [
                "F.R. Chung",
                "F.C. Graham"
            ],
            "title": "Spectral graph theory",
            "venue": "American Mathematical Soc.,",
            "year": 1997
        },
        {
            "authors": [
                "E.B. Davies",
                "J. Leydold",
                "P.F. Stadler"
            ],
            "title": "Discrete nodal domain theorems",
            "venue": "arXiv preprint math/0009120,",
            "year": 2000
        },
        {
            "authors": [
                "J.W. Demmel"
            ],
            "title": "Applied numerical linear algebra",
            "year": 1997
        },
        {
            "authors": [
                "C. Donnat",
                "M. Zitnik",
                "D. Hallac",
                "J. Leskovec"
            ],
            "title": "Learning structural node embeddings via diffusion wavelets",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "V.P. Dwivedi",
                "C.K. Joshi",
                "T. Laurent",
                "Y. Bengio",
                "X. Bresson"
            ],
            "title": "Benchmarking graph neural networks",
            "venue": "arXiv preprint arXiv:2003.00982,",
            "year": 2020
        },
        {
            "authors": [
                "F. Errica",
                "M. Podda",
                "D. Bacciu",
                "A. Micheli"
            ],
            "title": "A fair comparison of graph neural networks for graph classification",
            "venue": "arXiv preprint arXiv:1912.09893,",
            "year": 2020
        },
        {
            "authors": [
                "M. Fiedler"
            ],
            "title": "Algebraic connectivity of graphs",
            "venue": "Czechoslovak mathematical journal,",
            "year": 1973
        },
        {
            "authors": [
                "J. Gilmer",
                "S.S. Schoenholz",
                "P.F. Riley",
                "O. Vinyals",
                "G.E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "E. Grave",
                "A. Joulin",
                "Q. Berthet"
            ],
            "title": "Unsupervised alignment of embeddings with wasserstein procrustes",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "J.-B. Grill",
                "F. Strub",
                "F. Altch\u00e9",
                "C. Tallec",
                "P. Richemond",
                "E. Buchatskaya",
                "C. Doersch",
                "B. Avila Pires",
                "Z. Guo",
                "M. Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "A. Grover",
                "J. Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "M. Gutmann",
                "A. Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "M.U. Gutmann",
                "A. Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
            "venue": "Journal of machine learning research,",
            "year": 2012
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "X. Han",
                "Z. Jiang",
                "N. Liu",
                "X. Hu"
            ],
            "title": "G-mixup: Graph data augmentation for graph classification",
            "venue": "arXiv preprint arXiv:2202.07179,",
            "year": 2022
        },
        {
            "authors": [
                "K. Hassani",
                "A.H. Khasahmadi"
            ],
            "title": "Contrastive multi-view representation learning on graphs",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "M. Heimann",
                "H. Shen",
                "T. Safavi",
                "D. Koutra"
            ],
            "title": "REGAL: Representation learning-based graph alignment",
            "venue": "In Proc. of CIKM,",
            "year": 2018
        },
        {
            "authors": [
                "K. Henderson",
                "B. Gallagher",
                "T. Eliassi-Rad",
                "H. Tong",
                "S. Basu",
                "L. Akoglu",
                "D. Koutra",
                "C. Faloutsos",
                "L. Li"
            ],
            "title": "Rolx: structural role extraction & mining in large graphs",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "R.D. Hjelm",
                "A. Fedorov",
                "S. Lavoie-Marchildon",
                "K. Grewal",
                "P. Bachman",
                "A. Trischler",
                "Y. Bengio"
            ],
            "title": "Learning deep representations by mutual information estimation and maximization",
            "venue": "arXiv preprint arXiv:1808.06670,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hou",
                "X. Liu",
                "Y. Dong",
                "C. Wang",
                "J. Tang"
            ],
            "title": "Graphmae: Self-supervised masked graph autoencoders",
            "venue": "In Proc. ACM SIG Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "W. Hu",
                "M. Fey",
                "M. Zitnik",
                "Y. Dong",
                "H. Ren",
                "B. Liu",
                "M. Catasta",
                "J. Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Hu",
                "Y. Dong",
                "K. Wang",
                "K.-W. Chang",
                "Y. Sun"
            ],
            "title": "Gptgnn: Generative pre-training of graph neural networks",
            "venue": "In Proc. ACM SIG Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "N. Kahale"
            ],
            "title": "Eigenvalues and expansion of regular graphs",
            "venue": "Journal of the ACM (JACM),",
            "year": 1995
        },
        {
            "authors": [
                "P. Khosla",
                "P. Teterwak",
                "C. Wang",
                "A. Sarna",
                "Y. Tian",
                "P. Isola",
                "A. Maschinot",
                "C. Liu",
                "D. Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In Proc. Int. Conf. Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "R.I. Kondor",
                "J. Lafferty"
            ],
            "title": "Diffusion kernels on graphs and other discrete structures",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2002
        },
        {
            "authors": [
                "T.C. Kwok",
                "L.C. Lau",
                "Y.T. Lee",
                "S. Oveis Gharan",
                "L. Trevisan"
            ],
            "title": "Improved cheeger\u2019s inequality: Analysis of spectral partitioning algorithms through higher order spectral gap",
            "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
            "year": 2013
        },
        {
            "authors": [
                "J.R. Lee",
                "S.O. Gharan",
                "L. Trevisan"
            ],
            "title": "Multiway spectral partitioning and higher-order cheeger inequalities",
            "venue": "Journal of the ACM (JACM),",
            "year": 2014
        },
        {
            "authors": [
                "N. Lee",
                "J. Lee",
                "C. Park"
            ],
            "title": "Augmentation-free selfsupervised learning on graphs",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "H. Li",
                "X. Wang",
                "Z. Zhang",
                "Z. Yuan",
                "W. Zhu"
            ],
            "title": "Disentangled contrastive learning on graphs",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "P.E. McKnight",
                "J. Najab"
            ],
            "title": "Mann-whitney u test",
            "venue": "The Corsini encyclopedia of psychology,",
            "year": 2010
        },
        {
            "authors": [
                "C. Morris",
                "N.M. Kriege",
                "F. Bause",
                "K. Kersting",
                "P. Mutzel",
                "M. Neumann"
            ],
            "title": "Tudataset: A collection of benchmark datasets for learning with graphs",
            "year": 2007
        },
        {
            "authors": [
                "A. Narayanan",
                "M. Chandramohan",
                "R. Venkatesan",
                "L. Chen",
                "Y. Liu",
                "S. Jaiswal"
            ],
            "title": "graph2vec: Learning distributed representations of graphs",
            "venue": "arXiv preprint arXiv:1707.05005,",
            "year": 2017
        },
        {
            "authors": [
                "A. Ortega",
                "P. Frossard",
                "J. Kova\u010devi\u0107",
                "J.M. Moura",
                "P. Vandergheynst"
            ],
            "title": "Graph signal processing: Overview, challenges, and applications",
            "venue": "Proceedings of the IEEE,",
            "year": 2018
        },
        {
            "authors": [
                "L. Page",
                "S. Brin",
                "R. Motwani",
                "T. Winograd"
            ],
            "title": "The PageRank citation ranking: Bringing order to the web",
            "venue": "Technical report, Stanford InfoLab,",
            "year": 1999
        },
        {
            "authors": [
                "J. Palowitch",
                "A. Tsitsulin",
                "B. Mayer",
                "B. Perozzi"
            ],
            "title": "Graphworld: Fake graphs bring real insights for gnns",
            "venue": "arXiv preprint arXiv:2203.00112,",
            "year": 2022
        },
        {
            "authors": [
                "B. Perozzi",
                "R. Al-Rfou",
                "S. Skiena"
            ],
            "title": "DeepWalk: Online learning of social representations",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2014
        },
        {
            "authors": [
                "K.K. Qin",
                "F.D. Salim",
                "Y. Ren",
                "W. Shao",
                "M. Heimann",
                "D. Koutra"
            ],
            "title": "G-crewe: Graph compression with embedding for network alignment",
            "venue": "In Proc. ACM SIG Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "J. Qiu",
                "Y. Dong",
                "H. Ma",
                "J. Li",
                "K. Wang",
                "J. Tang"
            ],
            "title": "Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec",
            "venue": "In WSDM",
            "year": 2018
        },
        {
            "authors": [
                "J. Qiu",
                "Q. Chen",
                "Y. Dong",
                "J. Zhang",
                "H. Yang",
                "M. Ding",
                "K. Wang",
                "J. Tang"
            ],
            "title": "Gcc: Graph contrastive coding for graph neural network pre-training",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "L.F. Ribeiro",
                "P.H. Saverese",
                "D.R. Figueiredo"
            ],
            "title": "struc2vec: Learning node representations from structural identity",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2017
        },
        {
            "authors": [
                "K. Rohe",
                "S. Chatterjee",
                "B. Yu"
            ],
            "title": "Spectral clustering and the high-dimensional stochastic blockmodel",
            "venue": "The Annals of Statistics,",
            "year": 1878
        },
        {
            "authors": [
                "P. Sarkar",
                "P.J. Bickel"
            ],
            "title": "Role of normalization in spectral clustering for stochastic blockmodels",
            "venue": "The Annals of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "R. Singh",
                "J. Xu",
                "B. Berger"
            ],
            "title": "Pairwise global alignment of protein interaction networks by matching neighborhood topology",
            "venue": "In Annual International Conference on Research in Computational Molecular Biology,",
            "year": 2007
        },
        {
            "authors": [
                "D.A. Spielman"
            ],
            "title": "Spectral graph theory and its applications",
            "venue": "Annual IEEE Symposium on Foundations of Computer Science",
            "year": 2007
        },
        {
            "authors": [
                "D.A. Spielman",
                "N. Srivastava"
            ],
            "title": "Graph sparsification by effective resistances",
            "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
            "year": 2008
        },
        {
            "authors": [
                "F.-Y. Sun",
                "J. Hoffman",
                "V. Verma",
                "J. Tang"
            ],
            "title": "Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization",
            "venue": "In Proc. Int. Conf. Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "J. Tang",
                "M. Qu",
                "M. Wang",
                "M. Zhang",
                "J. Yan",
                "Q. Mei"
            ],
            "title": "LINE: Large-scale information network embedding",
            "venue": "In WWW",
            "year": 2015
        },
        {
            "authors": [
                "H. Tong",
                "C. Faloutsos",
                "J.-Y. Pan"
            ],
            "title": "Fast random walk with restart and its applications",
            "venue": "In ICDM",
            "year": 2006
        },
        {
            "authors": [
                "L. Torres",
                "K.S. Chan",
                "T. Eliassi-Rad"
            ],
            "title": "Glee: Geometric laplacian eigenmap embedding",
            "venue": "Journal of Complex Networks,",
            "year": 2020
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Lio",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In Proc. Int. Conf. Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "P. Velickovic",
                "W. Fedus",
                "W.L. Hamilton",
                "P. Li\u00f2",
                "Y. Bengio",
                "R.D. Hjelm"
            ],
            "title": "Deep graph infomax",
            "venue": "In Proc. Int. Conf. Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "U. Von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Statistics and computing,",
            "year": 2007
        },
        {
            "authors": [
                "U. Von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Statistics and computing,",
            "year": 2007
        },
        {
            "authors": [
                "V.H. Vu"
            ],
            "title": "Spectral norm of random matrices",
            "year": 2007
        },
        {
            "authors": [
                "V.H. Vu"
            ],
            "title": "Modern Aspects of Random Matrix Theory, volume 72",
            "venue": "American Mathematical Society,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Wang",
                "W. Wang",
                "Y. Liang",
                "Y. Cai",
                "B. Hooi"
            ],
            "title": "Graphcrop: Subgraph cropping for graph classification",
            "venue": "arXiv preprint arXiv:2009.10564,",
            "year": 2020
        },
        {
            "authors": [
                "H. Weyl"
            ],
            "title": "Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung)",
            "venue": "Mathematische Annalen,",
            "year": 1912
        },
        {
            "authors": [
                "R.F. Woolson"
            ],
            "title": "Wilcoxon signed-rank test",
            "venue": "Wiley encyclopedia of clinical trials,",
            "year": 2007
        },
        {
            "authors": [
                "F. Wu",
                "A. Souza",
                "T. Zhang",
                "C. Fifty",
                "T. Yu",
                "K. Weinberger"
            ],
            "title": "Simplifying graph convolutional networks",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "D. Xu",
                "W. Cheng",
                "D. Luo",
                "H. Chen",
                "X. Zhang"
            ],
            "title": "InfoGCL: Information-aware graph contrastive learning",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "K. Xu",
                "W. Hu",
                "J. Leskovec",
                "S. Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "In Proc. Int. Conf. Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "P. Yanardag",
                "S. Vishwanathan"
            ],
            "title": "Deep graph kernels",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2015
        },
        {
            "authors": [
                "G. Yehudai",
                "E. Fetaya",
                "E. Meirom",
                "G. Chechik",
                "H. Maron"
            ],
            "title": "From local structures to size generalization in graph neural networks",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ying",
                "J. You",
                "C. Morris",
                "X. Ren",
                "W. Hamilton",
                "J. Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Y. You",
                "T. Chen",
                "Y. Sui",
                "Z. Wang",
                "Y. Shen"
            ],
            "title": "Graph contrastive learning with augmentations",
            "venue": "In Proc. Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. You",
                "T. Chen",
                "Y. Shen",
                "Z. Wang"
            ],
            "title": "Graph contrastive learning automated",
            "venue": "In Proc. Int. Conf. Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Y. You",
                "T. Chen",
                "Z. Wang",
                "Y. Shen"
            ],
            "title": "Bringing your own view: Graph contrastive learning without prefabricated data augmentations",
            "year": 2022
        },
        {
            "authors": [
                "F. Zhang",
                "X. Liu",
                "J. Tang",
                "Y. Dong",
                "P. Yao",
                "J. Zhang",
                "X. Gu",
                "Y. Wang",
                "B. Shao",
                "R. Li"
            ],
            "title": "OAG: Toward linking large-scale heterogeneous entity graphs",
            "venue": "In KDD,",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "J. Tang",
                "C. Ma",
                "H. Tong",
                "Y. Jing",
                "J. Li"
            ],
            "title": "Panther: Fast top-k similarity search on large networks",
            "venue": "In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining,",
            "year": 2015
        },
        {
            "authors": [
                "J. Zhang",
                "X. Shi",
                "J. Xie",
                "H. Ma",
                "I. King",
                "D.-Y. Yeung"
            ],
            "title": "Gaan: Gated attention networks for learning on large and spatiotemporal graphs",
            "venue": "arXiv preprint arXiv:1803.07294,",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Dong",
                "Y. Wang",
                "J. Tang"
            ],
            "title": "Prone: fast and scalable network representation learning",
            "venue": "In Proc. Int. Joint Conf. Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhang",
                "Z. Hu",
                "A. Subramonian",
                "Y. Sun"
            ],
            "title": "Motif-driven contrastive learning of graph representations",
            "venue": "arXiv preprint arXiv:2012.12533,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhu",
                "Y. Xu",
                "F. Yu",
                "Q. Liu",
                "S. Wu",
                "L. Wang"
            ],
            "title": "Deep graph contrastive representation learning",
            "venue": "arXiv preprint arXiv:2006.04131,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhu",
                "Y. Xu",
                "F. Yu",
                "Q. Liu",
                "S. Wu",
                "L. Wang"
            ],
            "title": "Graph contrastive learning with adaptive augmentation",
            "venue": "In WWW,",
            "year": 2021
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "2020c), GraphCL (You et al., 2020), MICRO-Graph (Zhang et al., 2020)",
            "venue": "JOAO (You et al.,",
            "year": 2021
        },
        {
            "authors": [
                "tasks. DiffPool (Ying"
            ],
            "title": "2018) adds the degree and clustering coefficient to each node feature vector",
            "venue": "GIN (Xu et al.,",
            "year": 2018
        },
        {
            "authors": [
                "Kondor",
                "Lafferty"
            ],
            "title": "The graph diffusion operation is more costly than calculation and application",
            "year": 2002
        },
        {
            "authors": [
                "Narayanan"
            ],
            "title": "Spectral graph theory provides the foundation for modelling structural similarity in (Qiu",
            "year": 2017
        },
        {
            "authors": [],
            "title": "Now consider the second eigenvector i.e. the \u03bb2 function from A against the calculated \u03bb2 above for A\u2217. We need to use the Davis Kahan theorem (Demmel, 1997) (theorem 5.4) which states that if the angle between these is \u03b8, then sin 2\u03b8 \u2264 2||A\u2212A \u2217 ||op",
            "year": 1997
        }
    ],
    "sections": [
        {
            "text": "Contrastive learning has emerged as a premier method for learning representations with or without supervision. Recent studies have shown its utility in graph representation learning for pretraining. Despite successes, the understanding of how to design effective graph augmentations that can capture structural properties common to many different types of downstream graphs remains incomplete. We propose a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective, enabling contrastive learning to capture useful structural representation from pretraining graph datasets. We first present a spectral graph cropping augmentation that involves filtering nodes by applying thresholds to the eigenvalues of the leading Laplacian eigenvectors. Our second novel augmentation reorders the graph frequency components in a structural Laplacianderived position graph embedding. Further, we introduce a method that leads to improved views of local subgraphs by performing alignment via global random walk embeddings. Our experimental results indicate consistent improvements in out-of-domain graph data transfer compared to state-of-the-art graph contrastive learning methods, shedding light on how to design a graph learner that is able to learn structural properties common to diverse graph types."
        },
        {
            "heading": "1 Introduction",
            "text": "Representation learning is of perennial importance, with contrastive learning being a recent prominent technique. Taking images as an example, under this framework, a set\nProceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS) 2023, TBD, USA. PMLR: Volume XXX. Copyright 2023 by the author(s).\nof transformations is applied to image samples, without changing the represented object or its label. Candidate transformations include cropping, resizing, Gaussian blur, and color distortion. These transformations are termed augmentations (Chen et al., 2020b; Grill et al., 2020). A pair of augmentations from the same sample are termed positive pairs. During training, their representations are pulled together (Khosla et al., 2020). In parallel, the representations from negative pairs, consisting of augmentations from different samples, are pushed apart. The contrastive objective encourages representations that are invariant to distortions but capture useful features. This constructs general representations, even without labels, that are usable downstream.\nRecently, self-supervision has been employed to support the training process for graph neural networks (GNNs). Several approaches (e.g., Deep Graph Infomax (DGI) (Velickovic et al., 2019), InfoGCL (Xu et al., 2021)) rely on mutual information maximization or information bottlenecking between pairs of positive views. Other GNN pre-training strategies construct objectives or views that rely heavily on domainspecific features (Hu et al., 2020b,c). This inhibits their ability to generalize to other application domains. Some recent graph contrastive learning strategies such as GCC (Qiu et al., 2020) and GraphCL (You et al., 2020) can more readily transfer knowledge to out-of-domain graph domains, because they derive embeddings based solely on local graph structure, avoiding possibly unshared attributes entirely. However, these approaches employ heuristic augmentations such as random walk with restart and edge-drop, which are not designed to preserve graph properties and might lead to unexpected changes in structural semantics (Lee et al., 2022). There is a lack of diverse and effective graph transformation operations to generate augmentations. We aim to fill this gap with a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective. This allows the graph encoder to learn structural properties that are common for graph data spanning multiple graphs and domains.\nContributions. We introduce three novel methods: (i) spectral graph cropping, (ii) graph frequency component reordering, both being graph data augmentations, and a postprocessing step termed (iii) local-global embedding align-\nar X\niv :2\n30 2.\n02 90\n9v 1\n[ cs\n.L G\n] 6\nF eb\n2 02\n3\nment. We also propose a strategy to select from candidate augmentations, termed post augmentation filtering. First, we define a graph transformation that removes nodes based on the graph Laplacian eigenvectors. This generalizes the image crop augmentation. Second, we introduce an augmentation that reorders graph frequency components in a structural Laplacian-derived position embedding. We motivate this by showing its equivalence to seeking alternative diffusion matrices instead of the Laplacian for factorization. This resembles image color channel manipulation. Third, we introduce the approach of aligning local structural positional embeddings with a global embedding view to better capture structural properties that are common for graph data. Taken together, we improve state-of-the-art methods for contrastive learning on graphs for out-of-domain graph data transfer. We term our overall suite of augmentations SGCL (Spectral Graph Contrastive Learning)."
        },
        {
            "heading": "2 Related Work",
            "text": "Graph contrastive methods. Table 1 divides existing work into five categories. Category 1 methods rely on mutual information maximization or bottlenecking. Category 2 methods require that pre-train and downstream task graphs come from the same domain. Category 3 includes random walk based embedding methods and Category 4 includes structural similarity-based methods. These methods do not provide shareable parameters (You et al., 2020). Category 5 (our setting): These methods explicitly target pre-training or transfer. Two of the more closely related approaches are Graph Contrastive Coding (GCC) (Qiu et al., 2020) and GraphCL (You et al., 2020). In GCC, the core augmentation is random walk with return (Tong et al., 2006) and Laplacian positional encoding is used to improve out-of-domain generalization. GraphCL (You et al., 2020) expands this augmentation suite by including node dropping, edge perturbations, and attribute masking. Other methods in Category 5 construct adaptive/learnable contrastive views (Zhu et al., 2021; Chu et al., 2021; You et al., 2022; Lee et al., 2022). Please see Appendix 1 for more detailed discussion.\nGraph structural augmentations. We focus on the most general, adaptable and transferable structure-only scenario \u2014 learning a GNN encoder using a large scale pre-training\ndataset with solely structural data and no attributes or labels. While not all methods in category 5 address this setting, they can be adapted to run in such conditions by removing domain or attribute-reliant steps. The graph augmentation strategy plays a key role in the success of graph contrastive learning (Qiu et al., 2020; You et al., 2020; Li et al., 2021; Sun et al., 2019; Hassani and Khasahmadi, 2020; Xu et al., 2021) and is a natural target as our area of focus. Commonlyused graph augmentations include: 1) attribute dropping or masking (You et al., 2020; Hu et al., 2020c); 2) random edge/node dropping (Li et al., 2021; Xu et al., 2021; Zhu et al., 2020, 2021); 3) graph diffusion (Hassani and Khasahmadi, 2020) and 4) random walks around a center node (Tong et al., 2006; Qiu et al., 2020). Additionally, there is an augmentation called GraphCrop (Wang et al., 2020), which uses a node-centric strategy to crop a contiguous subgraph from the original graph while maintaining its connectivity; this is different from the spectral graph cropping we propose. Existing structure augmentation strategies are not tailored to any special graph properties and might unexpectedly change the semantics (Lee et al., 2022).\nPositioning our work. Encoding human-interpretable structural patterns such as degree, triangle count, and graph motifs, is key to successful architectures such as GIN (Xu et al., 2019) or DiffPool (Ying et al., 2018) and these patterns control the quality of out-of distribution transfer (Yehudai et al., 2021) for graph tasks, which naturally relates to the pre-train framework where the downstream dataset may differ in distribution from the pre-train corpus. We seek a GNN which learns to capture structural properties common to diverse types of downstream graphs.\nThese commonly used structural patterns (e.g., degree, triangle count) are handcrafted. It is preferable to learn these features instead of defining them by fiat. Our goal is to create an unsupervised method that learns functions of the graph structure alone, which can freely transfer downstream to any task. The use of spectral features to learn these structural embeddings is a natural choice; spectral features such as the second eigenvalue or the spectral gap relate strongly to purely structural features such as the number of clusters in a graph, the number of connected components, and the d-regularity (Spielman, 2007). Methods based on spectral eigendecomposition such as Laplacian embeddings are ubiq-\nuitous, and even random-walk based embeddings such as LINE (Tang et al., 2015) are simply eigendecompositions of transformed adjacency matrices. Instead of handcrafting degree-like features, we strive to construct a learning process that allows the GNN to learn, in an unsupervised fashion, useful structural motifs. By founding the process on the spectrum of the graph, learning can move freely between the combinatorial, discrete domain of the nodes and the algebraic domain of embeddings.\nSuch structural features are required for the structure-only case, where we have large, unlabeled, pre-train graphs, and no guarantee that any attributes are shared with the downstream task. This is the most challenging setting in graph pre-training. In such a setting, it is only structural patterns that can be learned from the corpus and potentially transferred and employed in the downstream phase."
        },
        {
            "heading": "3 Graph Contrastive Learning",
            "text": "We consider a setting where we have a set of graphs G = {Gt} available for pre-training using contrastive learning. If we are addressing graph-level downstream tasks, then we work directly with the Gt. However, if the task is focused on nodes (e.g., node classification), then we associate with each node i \u2208 Gt a subgraph Gi, constructed as the r-ego subnetwork around i in Gt, defined as\nGi , Gt[{v \u2208 Gt : d(i, v) \u2264 r}] , (1)\nwhere d(u, v) is the shortest path distance between nodes i and v andG[S] denotes the subgraph induced fromG by the subset of vertices S. During pre-training there are no labels, but in a fine-tuning phase when labels may be available, a subgraph Gi inherits any label associated with node i. Thus, node classification is treated as graph classification, finding the label of Gi. This processing step allows us to treat node and graph classification tasks in a common framework.\nOur goal is to construct an encoder parametrized by \u03b8, denoted E\u03b8, such that for a set of instances Gi \u2208 G, the output E\u03b8(Gi) captures the essential information about Gi required for downstream tasks. We employ instance discrimination as a contrastive learning objective and minimize (Gutmann and Hyv\u00e4rinen, 2010, 2012; Hjelm et al., 2018):\n\u2212log exp\u3008E\u03b8(G \u2032+), E\u03b8\u2032(G+)\u3009 \u3008E\u03b8(G\u2032+), E\u03b8\u2032(G+)\u3009+ \u2211r j=1\u3008E\u03b8(G\u2032+), E\u03b8\u2032(Gj\u2212)\u3009 . (2) Here, G+, G\u2032+ may be any augmented version of G, and one of them can be G itself. There is an additional sum in the denominator, denoting the number of negative instances.\nFor the encoder, we construct structure positional embeddings generalizable to unseen graphs. Let Gi have N nodes, adjacency matrix Ai, diagonal degree matrix Di. The normalized Laplacian of Gi is Li, which is eigendecomposed:\nLi = I \u2212D\u22121/2i AiD \u22121/2 i , Ui\u039biU T i = Li . (3)\nWith the \u039bi (eigenvalues) sorted in ascending order of magnitude, the first k columns of Ui yield the k-dimensional positional embedding, Xi, of shape N \u00d7 k. The pair (Gi,Xi) then serves as input to a GNN graph encoder (in our case GIN (Xu et al., 2019)), which creates a corresponding hidden vector Hi of shapeN\u00d7h, where h is the dimensionality of the final GNN layer. Each row corresponds to a vertex v \u2208 Gi. A readout function (Gilmer et al., 2017; Xu et al., 2019), which can be a simple permutation invariant function such as summation, or a more complex graph-level pooling function, takes the hidden states over v \u2208 Gi and creates an h-dimensional graph representation ri. A view of Gi can be created by conducting an independent random walk (with return) from node i, and collecting all the nodes visited in the walk to form G\u2032i. The random walk captures the local structure around i in Gi while perturbing it, and is inherently structural. A random walk originating from another node j \u2208 Gj , j 6= i leads to a negative example Gj\u2212."
        },
        {
            "heading": "4 Spectral Graph Contrastive Augmentation Framework",
            "text": "In this work, we introduce two novel graph data augmentation strategies: graph cropping and reordering of graph frequency components. We also propose two important quality-enhancing mechanisms. The first, which we call augmentation filtering, selects among candidate augmentations based on their representation similarity. The second, called local-global embedding view alignment, aligns the representations of the nodes that are shared between augmentations. We add the masking attribute augmentation (Hu et al., 2020b) which randomly replaces embeddings with zeros to form our overall flow of operations for augmentation construction, as depicted in Figure 1. The first two mandatory steps are ego-net formation and random walk. Subsequent steps may occur (with probabilities as pfilter, pcrop, palign, pmask, preorder) or may not. Two of the steps \u2014 mask and reorder \u2014 are mutually exclusive. For more detail, see Appendix 4.4. In the remainder of the section, we provide a detailed description of the core novel elements in the augmentation construction procedure: (i) spectral cropping; (ii) frequency component reordering; (iii) similar filtering; and (iv) embedding alignment. We aim to be as general as possible and graphs are a general class of data - images, for instance, may be represented as grid graphs. Our general graph augmentations such as \u201ccropping\" reduce to successful augmentations in the image domain, lending them credence, as a general method should excel in all sub-classes it contains."
        },
        {
            "heading": "4.1 Graph cropping using eigenvectors.",
            "text": "The image cropping augmentation is extremely effective (Chen et al., 2020b; Grill et al., 2020). It trims pixels along the (x, y) axes. There is no obvious way to extend this\noperation to general (non-grid) graphs. We now introduce a graph cropping augmentation that removes nodes using the eigenvectors corresponding to the two smallest non-zero eigenvalues of the graph Laplacian Li. When eigenvalues are non-decreasingly sorted, the second eigenvector (corresponding to the lowest nonzero eigenvalue) provides a wellknown method to partition the graph \u2014 the Fiedler cut. We use the eigenvectors corresponding to the first two nonzero eigenvalues, \u03bb2 and \u03bb3. Let x(v) denote the value assigned to node v in the second eigenvector, and similarly y(v) with the third eigenvector corresponding to \u03bb3. We define the spectral crop augmentation as : Gi[xmin, xmax, ymin, ymax] (a cropped view) being the set of vertices v \u2208 Gi satisfying xmin \u2264 x(v) \u2264 xmax and ymin \u2264 y(v) \u2264 ymax.\nLink to image cropping: We claim that the proposed graph cropping generalizes image cropping. Let us view the values of the eigenvector corresponding to \u03bb2 on a line graph (Figure 2). If we set a threshold t, and retain only the nodes with eigenvector values below (above) the threshold, we recover a contiguous horizontal segment of the graph. Thus, the eigenvector for \u03bb2 corresponds to variation along an axis (Ortega et al., 2018; Chung and Graham, 1997; Davies et al., 2000), much like the x or y axis in an image.\nWe consider now a product graph. A product of two graphs A,B with vertex sets (vA, vB) and edge sets (eA, eB) is a graphA.B where each v \u2208 A.B can be identified with an ordered pair (i, j), i \u2208 vA, j \u2208 vB . Two nodes corresponding\nto (i, j), (i\u2032, j\u2032) in A.B have an edge between them if and only if either i\u2032 = i, (j, j\u2032) \u2208 vB or (i, i\u2032) \u2208 vA, j = j\u2032. The product of two line graphs of length M,N respectively is representable as a planar rectangular grid of lengths M,N .\nDenote by Pn the path-graph on n vertices, which has n\u22121 edges of form (i, i + 1) for i = 1, . . . , n\u22121. This corresponds to the line graph. Denote by Ga,b the rectangular grid graph formed by the product Pa.Pb. Structurally, this graph represents an image with dimensions a \u00d7 b. The eigenvectors of the (un-normalized) Laplacian of Pn, for n > k \u2265 0, are of the form: xk(u) = cos(\u03c0ku/n\u2212\u03c0k/2n), with eigenvalues 2 \u2212 2 cos(\u03c0k/n). Clearly, k = 0 yields the constant eigenvector. The first nonzero eigenvalue corresponds to k = 1, where the eigenvector completes one \u201cperiod\" (with respect to the cosine\u2019s argument) over the path, and it is this pattern that is shown in Figure 2.\nThe following properties are well-known for the spectrum of product graphs (Brouwer and Haemers, 2011). Each eigenvalue is of the form \u03bbi+\u03bbj , where \u03bbi is from the spectrum of Pa and \u03bbj from Pb. Further, the corresponding eigenvector vi,j satisfies vi,j(u, v) = xi(u)yj(v), where xi,yj denote the eigenvectors from the respective path graphs. This means, for the spectra of Ga,b, that the lowest eigenvalue of the Laplacian corresponds to the constant eigenvector, and the second lowest eigenvalue corresponds to the constant eigenvector along one axis (path) and cos(\u03c0u/n \u2212 \u03c0/2n) along another. The variation is along the larger axis, i.e., along a, because the 2\u2212 2 cos(\u03c0k/n) term is smaller. This implies that for Ga,b, 2b > a > b, \u03bb2, \u03bb3 correspond to eigenvectors that recover axes in the grid graph (Figure 2)."
        },
        {
            "heading": "4.2 Frequency-based positional embedding reordering",
            "text": "Images have multi-channel data, derived from the RGB encoding. The channels correspond to different frequencies of the visible spectrum. The successful color reordering augmentation for images (Chen et al., 2020b) thus corresponds to a permutation of frequency components. This motivates us to introduce a novel augmentation that is derived by reordering the graph frequency components in a structural position embedding. A structural position embed-\nding can be obtained by factorization of the graph Laplacian. The Laplacian eigendecomposition corresponds to a frequency-based decomposition of signals defined on the graph (Von Luxburg, 2007a; Chung and Graham, 1997). We thus consider augmentations that permute, i.e., reorder, the columns of the structural positional embedding Xi.\nHowever, arbitrary permutations do not lead to good augmentations. In deriving a position embedding, the normalized Laplacian Li is not the only valid choice of matrix to factorize. Qiu et al. (2018) show that popular random walk embedding methods arise from the eigendecompositions of:\nlog( r\u2211 j=1 (I\u2212Li)r)D\u22121i = log ( Ui ( r\u2211 j=1 (I\u2212\u039bi)r ) UTi ) D\u22121i .\n(4) We have excluded negative sampling and graph volume terms for clarity. We observe that \u2211r j=1(I \u2212Li)r replaces (I\u2212Li) in the spectral decomposition. Just as the adjacency matrix Ai encodes the first order proximity (edges), A2i encodes second order connectivity, A3i third order and so on. Using larger values of r in equation 4 thus integrates higher order information in the embedding. The soughtafter eigenvectors in X are the columns in U corresponding to the top k values of \u2211r j=1(1 \u2212 \u03bb)j . There is no need to repeat the eigendecomposition to obtain a new embedding. The higher-order embedding is obtained by reordering the eigenvectors (in descending order of \u2211r j=1(1\u2212 \u03bbw)j).\nThis motivates our proposed reordering augmentation and identifies suitable permutation matrices. Rather than permute all of the eigenvectors in the eigendecomposition, for computational efficiency, we first extract the k eigenvectors with the highest corresponding eigenvalues in the first order positional embedding derived using (I \u2212Li). The reordering augmentation only permutes those k eigenvectors. The augmentation thus forms XiPr where Pr is a permutation matrix of shape k \u00d7 k. The permutation matrix Pr sorts eigenvectors with respect to the values \u2211r j=1(1\u2212\u03bbw)j . We randomize the permutation matrix generation step by sampling an integer uniformly in the range [1, rmax] to serve as r and apply the permutation to produce the view G\u2032i."
        },
        {
            "heading": "4.3 Embedding alignment",
            "text": "In this subsection and the next, we present two quality enhancing mechanisms that are incorporated in our spectral augmentation generation process and lead to superior augmentations. Both use auxiliary global structure information.\nConsider two vertices v and v\u2032 in the same graph Gt. Methods such as Node2vec (Grover and Leskovec, 2016), LINE (Tang et al., 2015), & DeepWalk (Perozzi et al., 2014) operate on Gt outputting an embedding matrix Et. The row corresponding to vertex v provides a node embedding ev .\nNode embedding alignment allows comparing embeddings between disconnected graphs G1, G2 utilizing the structural\nconnections in each graph (Singh et al., 2007; Chen et al., 2020c; Heimann et al., 2018; Grave et al., 2019). Consider two views G\u2032i, G \u2032\u2032 i and a node vi such that vi \u2208 G\u2032i, vi \u2208 G\u2032\u2032i . Given the embeddings X\u2032i,X \u2032\u2032 i forG \u2032 i, G \u2032\u2032 i , ignoring permutation terms, alignment seeks to find an orthogonal matrix Q satisfying X\u2032\u2032i Q \u2248 X\u2032i. If the embedding is computed via eigendecomposition of L\u2032i, L \u2032\u2032 i , the final structural node embeddings (rows corresponding to vi in X\u2032i,X \u2032\u2032 i ) for vi may differ. To correct this, we align the structural features X\u2032i,X \u2032\u2032 i , using the global matrix Et as a bridge.\nSpecifically, let NG\u2032i be the sub-matrix of Et obtained by collecting all rows j such that vj \u2208 Gi. Define NG\u2032\u2032i similarly. We find an orthogonal matrix Q\u2217 = minQ ||X\u2032iQ\u2212 NG\u2032i ||\n2. The solution is ACT , where ABCT is the singular value decomposition (SVD) of (X\u2032i)\nTNG\u2032i (Heimann et al., 2018; Chen et al., 2020c). Similarly, we compute Q\u2217\u2217 for G\u2032\u2032i . We consider the resulting matrices X \u2032 iQ \u2217 \u2248NG\u2032i and X\u2032\u2032i Q \u2217\u2217 \u2248 NG\u2032\u2032i . Since NG\u2032i ,NG\u2032\u2032i are both derived from Et, the rows (embeddings) corresponding to a common node are the same. We can thus derive improved augmentations by reducing the undesirable disparity induced by misalignment and replacing X\u2032i,X \u2032\u2032 i with their aligned counterparts X\u2032iQ \u2217,X\u2032\u2032i Q \u2217\u2217, terming this as align."
        },
        {
            "heading": "4.4 Augmentation filter.",
            "text": "Consider two views G\u2032i, G \u2032\u2032 i resulting from random walks from a node ai of which Gi is the ego-network in Gt. Let EG\u2032i = \u2211 vz\u2208G\u2032i\nez. We can measure the similarity of the views as \u3008EG\u2032i ,EG\u2032\u2032i \u3009. To enforce similar filtering of views, we accept the views if they are similar to avoid potential noisy augmentations: \u3008EG\u2032 ,EG\u2032\u2032 \u3009||EG\u2032 ||||EG\u2032\u2032 || > 1\u2212 c, for some constant 0 \u2264 c \u2264 1 . (For choice of c, see appendix 4.3.) We couple this filtering step with the random walk to accept candidates (Figure 1). Please note that applying similarity filtering empirically works much better than the other possible alternative, diverse filtering. We present the ablation study in appendix section 4.5."
        },
        {
            "heading": "4.5 Theoretical analysis.",
            "text": "We conduct a theoretical analysis of the spectral crop augmentation. In the Appendix, we extend this to a variant of the similar filtering operation. We investigate a simple case of the two-component stochastic block model (SBM) with 2N nodes divided equally between classes 0, 1. These results are also extensible to certain multi-component SBMs. Let the edge probabilities be p for edges between nodes of class 0, q for edges between nodes of class 1, and z for edges between nodes of different classes. We assume that p > q > z > 0.\nDenote by G a random graph from this SBM. We define the class, Y (G), to be the majority of the classes of its nodes, with Y (v) being the class of a node v. Let Ed,v(G) denote\nthe ego-network of v up to distance d in G. Let C (v) be the cropped local neighbourhood around node v defined as {v\u2032 : ||\u03bb(v\u2032) \u2212 \u03bb(v)|| \u2264 } where \u03bb(v) = [\u03bb2(v), \u03bb3(v)], with \u03bbj as the j-th eigenvector (sorted in ascending order by eigenvalue) of the Laplacian of G. In the Appendix, we prove the following result:\nTheorem 1 Let node v be chosen uniformly at random from G, a 2N -node graph generated according to the SBM described above. With probability \u2265 1\u2212 f(N) for a function f(N)\u2192 0 as N \u2192\u221e, \u2203 \u2208 R+, kmax \u2208 N such that :\n\u2200k \u2208 N \u2264 kmax, Y (Ek,v(G)) = Y (C (v)) = Y (v) (5)\nThis theorem states that for the SBM, both a view generated by the ego-network and a view generated by the crop augmentation acquire, with high probability as the number of nodes grows, graph class labels that coincide with the class of the centre node. This supports the validity of the crop augmentation \u2014 it constructs a valid \u201cpositive\u201d view.\nWe further analyze global structural embeddings and similar/diverse filtering, and specify f(N), in Appendix 10.\nThe proof of Theorem 1 relies on the Davis-Kahan theorem. Let A,H \u2208 RN\u00d7N ,A = AT ,H = HT with \u00b51 \u2265 \u00b52 \u2265 . . . \u00b5N the eigenvalues of A, v1,v2, . . . ,vN the corresponding eigenvectors of A, and v\u20321,v \u2032 2, . . .v \u2032 N those of A + H . By the Davis-Kahan theorem (Demmel, 1997) (Theorem 5.4), if the angle between vi,v\u2032i is \u03b8i, then, with \u2016H\u2016op as the max eigenvalue by magnitude of H\nsin(2\u03b8i) \u2264 2\u2016H\u2016op\nN \u00d7minj 6=i |\u00b5i \u2212 \u00b5j | (6)\nIn our setting, we consider A + H to be the adjacency matrix of the observed graph, which is corrupted by some noise H applied to a \u201ctrue\u201d adjacency matrix A. The angle \u03b8i measures how this noise H impacts the eigenvectors, which are used in forming Laplacian embeddings and also in the cropping step. Consider \u03b82, the angular error in the second eigenvector. For a normalized adjacency matrix, such that \u00b51 = 1, this error scales as 1min(\u00b52\u2212\u00b53,\u00b51\u2212\u00b52) . We can anticipate that the error is larger as \u00b52 becomes larger (\u00b51 \u2212 \u00b52 falls) or smaller (\u00b52 \u2212 \u00b53 falls). The error affects the quality of the crop augmentation and the quality of generated embeddings. In Section 5.2, we explore the effectiveness of the augmentations as we split datasets by their spectral properties (by an estimate of \u00b52). As expected, we observe that the crop augmentation is less effective for graphs with large or small (estimated) \u00b52."
        },
        {
            "heading": "5 Experiments",
            "text": "Datasets. The datasets for pretraining are summarized in Table 2. They are relatively large, with the largest graph having \u223c4.8 million nodes and \u223c85 million edges. Key statistics of the downstream datasets are summarized in the individual result tables. Our primary node-level datasets are US-Airport (Ribeiro et al., 2017) and H-index (Zhang et al., 2019a) while our graph datasets derive from (Yanardag and Vishwanathan, 2015) as collated in (Qiu et al., 2020). Nodelevel tasks are at all times converted to graph-level tasks by forming an ego-graph around each node, as described in Section 3. We conduct similarity search tasks over the academic graphs of data mining conferences following (Zhang et al., 2019a). Full dataset details are in Appendix section 4.\nTraining scheme. We use two representative contrastive learning training schemes for the graph encoder via minibatch-level contrasting (E2E) and MoCo (He et al., 2020) (Momentum-Contrasting). In all experiment tables, we present results where the encoder only trains on pre-train graphs and never sees target domain graphs. In Appendix 4, we provide an additional setting where we fully fine-tune all parameters with the target domain graph after pre-training. We construct all graph encoders (ours and other baselines) as a 5 layer GIN (Xu et al., 2019) for fair comparison.\nCompeting baselines. As noted in our categorization of existing methods in Table 1, the closest analogues to our approach are GraphCL (You et al., 2020) and GCC (Qiu et al., 2020) which serve as our key benchmarks. Additionally, although they are not designed for pre-training, we integrated the augmentation strategies from MVGRL (Hassani and Khasahmadi, 2020), Grace (Zhu et al., 2020), Cuco (Chu et al., 2021), and Bringing Your Own View (BYOV) (You et al., 2022) to work with the pre-train setup and datasets we use. We include additional baselines that are specifically tailored for each downstream task and require unsupervised pre-training on target domain graphs instead of our pre-train graphs. We include Struc2vec (Ribeiro et al., 2017), ProNE (Zhang et al., 2019b), and GraphWave (Donnat et al., 2018) as baselines for the node classification task. For the graph classification task, we include Deep Graph Kernel (DGK) (Yanardag and Vishwanathan, 2015), graph2vec (Narayanan et al., 2017), and InfoGraph (Sun et al., 2019) as baselines. For the top-k similarity search method, two specialized methods are included: Panther (Zhang et al., 2015) and RolX (Henderson et al., 2012). All results for GCC are copied from (Qiu et al., 2020). For GraphCL (You et al., 2020), we re-implemented the described augmentations to work with the pre-training\nset up and datasets we use. We also add two strong recent benchmarks, namely InfoGCL (Xu et al., 2021) and GCA (You et al., 2021). Some strong baselines such as G-MIXUP (Han et al., 2022) were excluded because they require labels during the pre-training phase.\nS-Crop Mask S-Reorder Align Filtering\nSCr op M as k\nSRe\nor de r Al ig n\nFi lte\nrin g\n0.22 0.47 0.43 0.54 0.71\n0.47 0.15 0.26 0.35 0.42\n0.43 0.26 0.18 0.19 0.58\n0.54 0.35 0.19 0.16 0.56\n0.71 0.42 0.58 0.56 0.31 0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nFigure 4: Heatmap indicating the effectiveness of each augmentation, pairwise, on reddit-binary. Numbers are percentage improvement relative to the SOTA method GCC (Qiu et al., 2020).\nPerformance metrics. After pre-training, we train a regularized logistic regression model (node classification) or SVM classifier (graph classification) from the scikit-learn package on the obtained representations using the target graph data, and evaluate using k = 10 fold splits of the dataset labels. Following (Qiu et al., 2020), we use F-1 score (out of 100) as the metric for node classification tasks, accuracy percentages for graph classification, and HITS@10 (top 10 accuracy) at top k = 20, 40 for similarity search.\nExperimental procedure. We carefully ensure our reported results are reproducible and accurate. We run our model 80 times with different random seeds; the seed controls the random sampler for the augmentation generation and the initialization of neural network weights. We conduct three statistical tests to compare our method with the second best baseline under both E2E and MoCo training schemes: Wilcoxon signed-rank (Woolson, 2007), WhitneyMann (McKnight and Najab, 2010), and the t-test. Statistical significance is declared if the p-values for all tests are\nless than 10\u22126. Appendix 4 details hyperparameters, experimental choices and statistical methodologies. Standard deviations, statistical test results, and confidence intervals are provided in Appendix 5, and additional experimental results for the CIFAR-10, MNIST, and OGB datasets are presented in Appendix 6."
        },
        {
            "heading": "5.1 Runtime and scaling considerations",
            "text": ""
        },
        {
            "heading": "5.2 Results and Discussion",
            "text": "Our design achieves robust improvement on both node and graph classification tasks over other baselines for the domain\ntransfer setting. We emphasize that the graph encoders for all the baselines from Category 5 in Table 1 are not trained on the target source dataset, whereas other baselines use this as training data (in an unsupervised fashion). Although this handicaps the domain transfer-based methods, our proposed method performs competitively or even significantly better compared to classic unsupervised learning approaches including ProNE (Zhang et al., 2019b), GraphWave (Donnat et al., 2018) and Struc2vec (Ribeiro et al., 2017) for node-level classification tasks and DGK, graph2vec and InfoGraph for graph level classification. We observe similar improvements relative to baselines for both the E2E and MoCo training schemes. These improvements are also evident for the similarity search task. The performance gains are also present when the encoder is fully fine-tuned on graphs from the downstream task, but due to space limitations, we present the results in Appendix 4.\nEffectiveness of individual augmentations, processing/selection steps, and pairwise compositions. We show evaluation results (average over 80 trials) for both individual augmentations or filtering/selection steps and their pairwise compositions in Figure 4. For a clear demonstration, we select Reddit-binary as the downstream task and the smallest pre-train DBLP (SNAP) dataset. Using more pre-train datasets should result in further performance improvements. The full ablation study results are presented in Appendix 4. As noted previously (You et al., 2020), combining augmentations often improves the outcome. We report improvement relative to the SOTA method GCC (Qiu et al., 2020). Performance gains are observed for all augmentations. On average across 7 datasets, spectral crop emerges as the best augmentation of those we proposed. Appendix 4.5 reports the results of ablations against random variants of the crop and\nreorder augmentations; the specific procedures we propose lead to a substantial performance improvement.\nPerformance variations due to spectral properties. We split the test graphs into quintiles based on their \u03bb2 values to explore whether the test graph spectrum impacts the performance of the proposed augmentation process. Figure 5 displays the improvements obtained for each quintile. As suggested by our theoretical analysis in Section 4.5, we see a marked elevation for the middle quintiles of \u03bb2. These results support the conjecture that small or large values of \u03bb2 (an approximation of \u00b52 in Section 4.5) adversely affect the embedding quality and the crop augmentation."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce SGCL, a comprehensive suite of spectral augmentation methods suited to pre-training graph neural networks contrastively over large scale pre-train datasets. The proposed methods do not require labels or attributes, being reliant only on structure, and thus are applicable to a wide variety of settings. We show that our designed augmentations can aid the pre-training procedure to capture generalizable structural properties that are agnostic to downstream tasks. Our designs are not ad hoc, but are well motivated through spectral analysis of the graph and its connections to augmentations and other techniques in the domains of vision and network embedding analysis. The proposed augmentations make the graph encoder \u2014 trained by either E2E or MoCo \u2014 able to adapt to new datasets without fine-tuning. The suite outperforms the previous state-of-the-art methods with statistical significance. The observed improvements persist across multiple datasets for the three tasks of node classification, graph classification and similarity search."
        },
        {
            "heading": "7 Position of our work",
            "text": "We emphasize that there are three distinct cases of consideration in the field of contrastive learning in graphs. We hope to make a clear separation between them to help readers understand our model design and the choice of baselines as well as the datasets we conduct experiments on. A summarizing of the position of our work vs. prior works is presented in Table 5.\nFirst, we have out of domain pre-training, where the encoder only sees a large pre-training corpus in the training phase that may share no node attributes at all with the downstream task. A representative method is GCC (Qiu et al., 2020). For example, this pre-training dataset can be a large citation network or social network (as the pre-train corpus used in our paper), while the downstream task may be on a completely different domain such as molecules. Since no node attributes are shared between the two domains, the initial node attributes have to rely solely on structure, e.g., the adjacency or Laplacian matrices and embeddings derived from their eigendecomposition . Importantly, in this case, the encoder is never trained on any labeled or unlabeled instances for the downstream graph related tasks before doing the inference. It allows the model to obtain the results on the downstream task very fast (since only the model inference step is applied to obtain the node representations for the downstream tasks). We call this setting pre-training with frozen encoder (out of domain). This is the most difficult graph contrastive learning (GCL) related task. In our paper, we strictly follow this setup. The downstream task performance can be further improved if the downstream training instances (data, but also possibly with labels) are shown to the GNN encoder. We call this setting pre-training with fine-tuning (out of domain).\nSecond, we have the domain specific pre-training method where the encoder sees a large pre-training corpus which shares similar features or the same feature tokenization method as the downstream task. The representative methods that fall under this category include GPT-GNN Hu et al. (2020c), GraphCL (You et al., 2020), MICRO-Graph (Zhang et al., 2020), JOAO (You et al., 2021), BYOV (You et al., 2022), and GraphMAE (Hou et al., 2022). The typical experiment design for this setting is to pre-train the GNN encoder on a large-scale bioinformatics dataset, and then fine-tune and evaluate on smaller datasets of the same category. Since the feature space is properly aligned between the pre-train dataset and the downstream datasets, the node attributes usually are fully exploited during the pre-training stage. Similarly, the downstream task performance can be further improved if the downstream tasks (data and/or their labels) are shown to the GNN encoder. We call these two setting pre-training with frozen encoder (same domain) and pre-training with encoder fine-tuning (same domain).\nThird, in the unsupervised learning setting for GCL, there is no large pre-training corpus that is distinct from the downstream task data. Rather, the large training set of the downstream task is the sole material for contrastive pre-training. Note that in this case, if there are multiple unrelated downstream tasks, e.g., a citation network and also a molecule task, a separate pre-training procedure must be conducted for each task and a separate network must be trained. The representative methods that fall under this category include InfoGCL (Xu et al., 2021), MVGRL (Hassani and Khasahmadi, 2020), GraphCL (You et al., 2020), GCA (Zhu et al., 2021). Generally speaking, for tasks that rely heavily on the node attributes (such as citations, and molecule graphs), such unsupervised methods, when the training set data (adjacency matrix and node attributes) is available for the unsupervised training phase, can potentially outperform the out of domain pre-trained frozen encoder case. But this is natural, and expected, because in the out-of-domain pre-training with a frozen encoder setting the pre-trained network never even sees the source domain. It can never take advantage of node attributes because the pre-train datasets do not share the same feature space as the downstream task. It can only rely on the potential transferable structural features. But this is also not its purpose - its purpose is to act like a general large language model (LLM) or a Foundation Model like GPT-3. Such a model is not necessarily an expert in every area and can be outperformed by, for instance, specific question-answer-specialized language models for answering questions, but it performs relatively well zero-shot in most tasks without needing any pre-training. This is why in our main paper, we did not compare with the commonly used small-scale datasets (Cora, Citesser) for the unsupervised learning tasks.\nPrevious papers in this field such as GraphMAE (Hou et al., 2022) often include the frozen, pre-trained models under\nthe unsupervised category in the experiments, which is not completely accurate or fair. In fact, this category is relatively understudied and introduces unique challenges for the out-of-domain transfer setting. Its importance, and relative lack of study, is precisely why it deserves attention - it is a step toward out-of-domain generalization on graphs and avoids expensive pre-training in every domain. In the following table, we provide a novel way to categorize the existing graph contrastive learning work and we hope it provides better insight to the readers in terms of the position of our work.\nPlease note that even though not directly applicable for the pre-train (out of domain) mode, for the existing methods under the category of pre-train (same domain) and unsupervised learning, we are able to make modifications to allow them to be applied in the out of domain settings. The main changes are 1) we use the pre-train out of domain corpus {XLP ,A}pretrain to train the GNN encoder instead of {X,A}downstream; 2) since the feature space between the pre-train domains and the target domain are not aligned, we use XLP instead of the original feature X. We conduct the above modification to some of the existing unsupervised learning based methods such as MVGRL (Hassani and Khasahmadi, 2020) and GraphCL (You et al., 2020). This is also why we cannot, without running the out of domain pre-training experiment setups, directly report the experimental performance in the previous papers (largely unsupervised, except GCC), and why some of our numbers do not always agree with those reported in the original paper, e.g., MVGRL on IMDB-BINARY. The entire training process is completely different with a new pre-training corpus, and the same numbers are not guaranteed to occur."
        },
        {
            "heading": "8 Reasons to pursue augmentations based on graph spectra",
            "text": "First, we want to address the question of what is meant by the term \"universal topological properties\". Our method is inherently focused on transferring the pre-trained GNN encoder to any domain of graphs, including those that share no node attributes with the pre-train data corpus. This means that the only properties the encoder can use when building its representation are transferable structural clues. We use the word topological to denote this structure-only learning. The word universal denotes the idea of being able to easily transfer from pre-train graphs to any downstream graphs. It is a common practice to augment node descriptors with structural features (Errica et al., 2020), especially for graph classification tasks. DiffPool (Ying et al., 2018) adds the degree and clustering coefficient to each node feature vector. GIN (Xu et al., 2019) adds a one-hot representation of node degrees. In short, a universal topological property is some property such as the human-defined property of \"degree\" that we hope the GNN will learn in an unsupervised fashion. Just as degree - a very useful attribute to know for any graph for many downstream tasks - is derivable from the adjacency matrix by taking a row sum, we hope the GNN will learn a sequence of operations that distill some concept that is even more meaningful than the degree and other basic graph statistics.\nSince structural clues are the only ones that can be transferable between pre-train graphs and the downstream graphs, the next part to answer is why spectral methods, and why should we use the spectral-inspired augmentations to achieve the out-of-domain generalization goal. We elaborate as follows.\nFor multiple decades, researchers have demonstrated the success of graph spectral signals with respect to preserving the unique structural characteristics of graphs (see (Torres et al., 2020) and references therein). Graph spectral analysis has also been the subject of extensive theoretical study and it has been established that the graph spectral information is important to characterize the graph properties. For example, graph spectral values (such as the Fiedler eigenvalue) related directly to fundamental properties such as graph partitioning properties (Kwok et al., 2013; Lee et al., 2014) and graph connectivity (Chung, 1997; Kahale, 1995; Fiedler, 1973). Spectral analyses of the Laplacian matrix have well-established applications in graph theory, network science, graph mining, and dimensionality reduction for graphs (Torres et al., 2020). They have also been used for important tasks such as clustering (Belkin and Niyogi, 2001; Von Luxburg, 2007b) and sparsification (Spielman and Srivastava, 2008). Moreover, many network embedding methods such as LINE (Tang et al., 2015) and DeepWalk reduce to factorizing a matrix derived from the Laplacian, as addressed in NetMF (Qiu et al., 2018). These graph spectral clues allow us to extract transferable structural features and structural commonality across graphs from different domains. All of these considerations motivate us to use spectral-inspired augmentations for graph contrastive learning to fully exploit the potential universal topological properties across graphs from different domains."
        },
        {
            "heading": "9 Related Work Extension",
            "text": ""
        },
        {
            "heading": "9.1 Representation learning on graphs",
            "text": "A significant body of research focuses on using graph neural networks to encode both the underlying graph describing relationships between nodes as well as the attributes for each node (Kipf and Welling, 2017; Gilmer et al., 2017; Hamilton\net al., 2017; Velic\u030ckovic\u0301 et al., 2018; Xu et al., 2019). The core idea for GNNs is to perform feature mapping and recursive neighborhood aggregation based on the local neighborhood using shared aggregation functions. The neighborhood feature mapping and aggregation steps can be parameterized by learnable weights, which together constitute the graph encoder.\nThere has been a rich vein of literature that discusses how to design an effective graph encoding function that can leverage both node attributes and structure information to learn representations (Kipf and Welling, 2017; Hamilton et al., 2017; Velic\u030ckovic\u0301 et al., 2018; Zhang et al., 2018; Wu et al., 2019; Chen et al., 2020a; Xu et al., 2019). In particular, we highlight the Graph Isomorphism Network (GIN) (Xu et al., 2019), which is an architecture that is provably one of the most expressive among the class of GNNs and is as powerful as the Weisfeiler Lehman graph isomorphism test. Graph encoding is a crucial component of GNN pre-training and self-supervised learning methods. However, most existing graph encoders are based on message passing and the transformation of the initial node attributes. Such encoders can only capture vertex similarity based on features or node proximity, and are thus restricted to being domain-specific, incapable of achieving transfer to unseen or out-of-distribution graphs. In this work, to circumvent this issue, we employ structural positional encoding to construct the initial node attributes. By focusing on each node\u2019s local subgraph level representation, we can extract universal topological properties that apply across multiple graphs. This endows the resultant graph encoder with the potential to achieve out-of-domain graph data transfer."
        },
        {
            "heading": "9.2 Data augmentations for contrastive learning",
            "text": "Augmentations for image data. Representation learning is of perennial importance in machine learning with contrastive learning being a recent prominent technique. In the field of representation learning for image data, under this framework, there has been an active research theme in terms of defining a set of transformations applied to image samples, which do not change the semantics of the image. Candidate transformations include cropping, resizing, Gaussian blur, rotation, and color distortion. Recent experimental studies (Chen et al., 2020b; Grill et al., 2020) have highlighted that the combination of random crop and color distortion can lead to significantly improved performance for image contrastive learning. Inspired by this observation, we seek analogous graph augmentations.\nAugmentations for graph data. The unique nature of graph data means that the augmentation strategy plays a key role in the success of graph contrastive learning (Qiu et al., 2020; You et al., 2020; Li et al., 2021; Sun et al., 2019; Hassani and Khasahmadi, 2020; Xu et al., 2021). Commonly-used graph augmentations include: 1) attribute dropping or masking (You et al., 2020; Hu et al., 2020c): these graph feature augmentations rely heavily on domain knowledge and this prevents learning a domain invariant encoder that can transfer to out-of-domain downstream tasks; 2) random edge/node dropping (Li et al., 2021; Xu et al., 2021; Zhu et al., 2020, 2021): these augmentations are based on heuristics and they are not tailored to preserve any special graph properties; 3) graph diffusion (Hassani and Khasahmadi, 2020): this operation offers a novel way to generate positive samples, but it has a large additional computation cost (Hassani and Khasahmadi, 2020; Page et al., 1999; Kondor and Lafferty, 2002). The graph diffusion operation is more costly than calculation and application of the Personalized Page Rank (PPR) (Page et al., 1999) based transition matrix since it requires an inversion of the adjacency matrix; and 4) random walks around a center node (Tong et al., 2006; Qiu et al., 2020): this augmentation creates two independent random walks from each vertex that explore its ego network and these form multiple views (subgraphs) of each node. Additionally, there is an augmentation called GraphCrop (Wang et al., 2020), which uses a node-centric strategy to crop a contiguous subgraph from the original graph while maintaining its connectivity; this is different from the spectral graph cropping we propose. Existing structure augmentation strategies are not tailored to any special graph properties and might unexpectedly change the semantics (Lee et al., 2022)."
        },
        {
            "heading": "9.3 Pre-training, self-supervision, unsupervised & contrastive graph representation learning",
            "text": "Though not identical, pre-training, self-supervised learning, and contrastive learning approaches in the graph learning domain use many of the same underlying methods. A simple technique such as attribute masking can, for example, be used in pre-training as a surrogate task of predicting the masked attribute, while in the contrastive learning scenario, the masking is treated as an augmentation. We categorize the existing work into the following 5 categories.\nCategory 1. One of the early works in the contrastive graph learning direction is Deep Graph Infomax (DGI) (Velickovic et al., 2019). Though not formally identified as contrastive learning, the method aims to maximize the mutual information between the patch-level and high-level summaries of a graph, which may be thought of as two views. Infomax is a similar method that uses a GIN (Graph Isomorphism Network) and avoids the costly negative sampling by using batch-wise generation (Sun et al., 2019). MVGRL (Hassani and Khasahmadi, 2020) tackles the case of multiple views, i.e., positive\npairs per instance, similar to Contrastive Multiview coding for images. DGCL (Li et al., 2021) adopts a disentanglement approach, ensuring that the representation can be factored into components that capture distinct aspects of the graph. InfoGCL (Xu et al., 2021) learns representations using the Information Bottleneck (IB) to ensure that the views minimize overlapping information while preserving as much label-relevant information as possible. None of the methods in this category is capable of capturing universal topological properties that extend across multiple graphs from different domains.\nCategory 2. Predicting masked edges/attributes in chemical and biological contexts has emerged as a successful pre-train task. GPT-GNN (Hu et al., 2020c) performs generative pre-training successively over the graph structure and relevant attributes. In (Hu et al., 2020b), Hu et al. propose several strategies (attribute masking, context structure prediction) to pre-train GNNs with joint node-level and graph-level contrastive objectives. This allows the model to better encode domain-specific knowledge. However, the predictive task for these methods relies heavily on the features and domain knowledge. As a result, the methods are not easily applied to general graph learning problems.\nCategory 3 Random-walk-based embedding methods like Deepwalk (Perozzi et al., 2014), LINE (Tang et al., 2015), and Node2vec (Grover and Leskovec, 2016) are widely used to learn network embeddings in an unsupervised way. The main purpose is to encode the similarity by measuring the proximity between nodes. The embeddings are derived from the skip-gram encoding method, Word2vec, in Natural Language Processing (NLP). However, the proximity similarity information can only be applied within the same graph. Transfer to unseen graphs is challenging since the embeddings learned on different graphs are not naturally aligned.\nCategory 4 To aid transferring learned representations, another approach of unsupervised learning attempts encoding structural similarities. Two nodes can be structurally similar while belonging to two different graphs. Handcrafted domain knowledge based representative structural patterns are proposed in (Yanardag and Vishwanathan, 2015; Ribeiro et al., 2017; Narayanan et al., 2017). Spectral graph theory provides the foundation for modelling structural similarity in (Qiu et al., 2018; Donnat et al., 2018; Chanpuriya and Musco, 2020).\nCategory 5 In the domain of explicitly contrastive graph learning, we consider Graph Contrastive Coding (GCC) (Qiu et al., 2020) as the closest approach to our work. In GCC, the core augmentation used is random walk with return (Tong et al., 2006). This forms multiple views (subgraphs) of each node. GraphCL (You et al., 2020) expands this augmentation suite to add node dropping, edge perturbations, and attribute masking. Additionally, although they are not designed for pre-training, we integrated the augmentation strategies from MVGRL (Hassani and Khasahmadi, 2020), Grace (Zhu et al., 2020), Cuco (Chu et al., 2021), and Bringing Your Own View (BYOV) (You et al., 2022) to work with the pre-train setup in category 5."
        },
        {
            "heading": "10 Implementation details, additional results, and ablations",
            "text": ""
        },
        {
            "heading": "10.1 Codebase references",
            "text": "In general, we follow the code base of GCC (Qiu et al., 2020), provided at : https://github.com/THUDM/GCC. We use it as a base for our own implementation (provided along with supplement). Please refer to it in general with this section. For results using struc2vec (Ribeiro et al., 2017), ProNE (Zhang et al., 2019b), Panther (Zhang et al., 2015), RolX (Henderson et al., 2012) and graphwave (Donnat et al., 2018) we report the results directly from (Qiu et al., 2020) wherever applicable."
        },
        {
            "heading": "10.2 Dataset details",
            "text": "We provide the important details of the pre-training datasets in the main paper, so here we describe the downstream datasets. We obtain US-airport from the core repository of GCC (Qiu et al., 2020) which itself obtains it from (Ribeiro et al., 2017). H-index is obtained from GCC as well via OAG (Zhang et al., 2019a). COLLAB, REDDIT-BINARY, REDDIT-MULTI5K, IMDB-BINARY, IMDB-MULTI all originally derive from the graph kernel benchmarks (Morris et al., 2020), provided at : https://chrsmrrs.github.io/datasets/. Finally, the top-k similarity datasets namely KDD-ICDM,SIGIR-CIKM,and SIGMODICDE, are obtained from the GCC repository (Qiu et al., 2020); these were obtained from the original source, Panther (Zhang et al., 2015)."
        },
        {
            "heading": "10.3 Hyperparameters and statistical experimental methodology",
            "text": "Hyperparameters: Training occurs over 75, 000 steps with a linear ramping-on (over the first 10%) and linear decay (over the last 10%) using the ADAM optimizer, with an initial learning rate of 0.005, \u03b21 = 0.9, \u03b22 = 0.999, = 10\u22128. The random walk return probability is 0.8. The E2E dictionary size K = 1023, for MoCo 16384. The batch size is 1024 for E2E and 32 for MoCo. The dropout is set to 0.5 with a degree embedding of dimension 16 and positional embedding of dimension 64. These hyperparameters are retained from GCC and do not require grid search. The hyperparameter c for alignment is chosen by grid search from 6 values, namely 0.2, 0.25, 0.3, 0.35, 0.4, 0.45.\nRuntime: Using DBLP as the test bed, we observed 33.27 seconds per epoch for baseline GCC, which was only increased to at most 41.08 seconds in the settings with the most augmentations. Note that epoch time is largely CPU controlled in our experience and may vary from server to server. However, we found that the ratios between different methods were far more stable. The main paper reports these values on a per graph basis.\nConfidence intervals and statistical methodology : To construct confidence bounds around our results, we carry out the following procedure. We introduce some randomness through seeds. The seed is employed twice: once during training the encoder, and again while fine-tuning the encoder on downstream tasks on the datasets. We carry out training with 8 random seeds, resulting in 8 encoders. From each of these encoders, the representation of the graphs in the downstream datasets is extracted.\nNext, we train a SVC (for graph datasets) or a logistic regression module (for node datasets), both with a regularization co-efficient over 10 stratified K-fold splits. Before testing the model obtained from the train fraction of any split, we sample uniformly with replacement from the test set of the split of size T until we draw T samples. These instances are graphs (ego-graphs for the case of node datasets and distinct graphs for the case of graph datasets). After this we report the testing result. This is a bootstrapping procedure that leads to a random re-weighing of test samples. This entire process - i.e., generating a new 10-fold split, training, bootstrapping, testing - is repeated 10 times per encoder.\nThis leads to a total of 800 data points per encoder, allowing fine-grained confidence intervals. However, we found that performance varied too strongly as a function of the splits, leading us to average over the splits instead. Therefore, each determination carries an effective sample size of 80. Upon this, the Whitney-Mann, Wilcoxon signed rank, and t-tests are carried out to determine p-values, 5 to 95 percentile confidence bounds, and standard deviations."
        },
        {
            "heading": "10.4 Augmentation details and sequences",
            "text": "Masking: As mentioned in the main paper, we follow previous work (Hu et al., 2020b) and add simple masking of Xi. The masking involves setting some columns to zero. Since we consider smaller eigenvalues of Li to be more important, we draw an integer z uniformly in the range [0,M ] and mask out z eigenvectors corresponding to the top z eigenvalues of Li.\nSequence of augmentations: We have discussed the creation of views of G\u2032i from graph instances Gi. However, in our case, the goal is to create two positive views G\u2032i, G \u2032\u2032 i per mini-batch for an instance Gi. Let us now clarify the sequence of augmentations we employ. It should be understood that for any Gi, the negative view is any augmented or un-augmented view of Gj , j 6= i.\n\u2022 First, we create G\u2032i, G \u2032\u2032 i using random walk with return on Gi. We use the random walk hyperparameters identified\nin (Qiu et al., 2020).\n\u2022 With probability pfilter, we then test G\u2032i, G \u2032\u2032 i using similarity or diversity thresholds 1\u2212c and repeat the first step if the\ntest fails. If G\u2032i, G \u2032\u2032 i do not pass in tmax tries, we proceed to the next step.\n\u2022 We randomly crop both G\u2032i, G \u2032\u2032 i independently with probabilities over different crops c1, c2, . . . (including no crop). In total, we allow five outcomes, i.e. c1, c2, c3, c4, c5 = 1\u2212 \u22114 i=1 ci. The last outcome is the case of no cropping. We\nkeep c1 = c2, c3 = c4. These correspond to different types of crops, explained below. We can term c1 + c2 + c3 + c4 as pcrop.\n\u2022 With probability palign, we replace X\u2032i,X \u2032\u2032 i with X \u2032 iQ \u2217,X\u2032\u2032i Q \u2217\u2217 or keep X\u2032i,X \u2032\u2032 i unchanged with probability 1\u2212palign.\n\u2022 We apply one of the mask and reorder augmentations on both G\u2032i, G \u2032\u2032 i independently to form the final positive pairs.\nThat is, for G\u2032i, we mask it with pmask, or reorder it with preorder, or keep it unchanged with 1\u2212 pmask \u2212 preorder. The same process is then done, independently, for G\u2032\u2032i .\nFor a graph G, the xmin, xmax, ymin, ymax values for cropping are chosen as follows. We calculate the values taken by the second eigenvector over G and rank them, and set xmin on the basis of the rank. That is, xmin = R0.2 would correspond to xmin being set as the 20-th percentile value over G. This is done instead of absolute thresholds to sidestep the changes of the second eigenvector over the different Gs. The corresponding different types of crop are, written as R values [xmin, xmax, ymin, ymax] tuples :\n\u2022 [R0.2, R0.8, R0.2, R0.8] with c1 = 0.1\n\u2022 [R0.1, R0.9, R0.1, R0.9] with c2 = 0.1\n\u2022 [R0, R0.8, R0, R0.8] with c3 = 0.05\n\u2022 [R0.2, R1.0, R0.2, R1.0] with c4 = 0.05\n\u2022 No crop, with c5 = 0.7\nNote that in terms of alignment augmentations, our arguments regarding X\u2032i,X \u2032\u2032 i being transformed to X \u2032 iQ \u2217,X\u2032\u2032i Q \u2217\u2217 as it respects the inner product carry over if we instead use X\u2032i,X \u2032\u2032 i Q \u2217\u2217(Q\u2217)T or X\u2032iQ \u2217(Q\u2217\u2217)T,X\u2032\u2032i as the augmented views.\nOrder of augmentations. We have chosen the augmentations to proceed in this order due to the following reasons.\n\u2022 Of all the candidates for the first augmentation in our sequence, the random walk is supreme as it cuts down on the size of the ego-net for the future steps and the Laplacian eigendecomposition\u2019s complexity as well. Doing away with it greatly increases the runtime for any other step preceding it.\n\u2022 Filtering is best done as early as possible to reject candidates on its basis before expensive augmentation steps have already been performed. Hence, we place it second.\n\u2022 Cropping precedes align, mask and ordering as these change the attribute vectors, and cropping uses the second eigenvector which is part of the embedding itself.\n\u2022 Alignment precedes mask and reorder, as alignment on shifted embeddings post-mask or post-reorder no longer follows from our arguments of its necessity.\n\u2022 Mask and reorder are mutually exclusive as reordering a masked matrix does not obey the diffusion matrix argument we make for reordering as an augmentation. While masking a diffused matrix is logically allowed, we did not experiment on this case thoroughly and did not find any encouraging preliminary empirical results for this case.\nWe do not claim our order is the best of all possible permutations. Nevertheless, it can be seen the choice is not entirely ad hoc."
        },
        {
            "heading": "10.5 Ablation tables",
            "text": "The necessity of our spectral crop and reorder frequency components. We first report in Table 1 the results of ablations that involve replacing the proposed crop and reorder augmentations with random analogues. The results validate the necessity of following our eigenspectrum-designed approaches for cropping and reordering. We explore replacing the proposed crop with a random crop (randomly selecting a subgraph by excluding nodes). For reordering, we compare to a random permutation of spectral positional encoding. We observe a consistent drop in performance across all datasets when\nwe replace either augmentation with its random counterpart. This indicates that our spectral augmentation designs can make a significant difference in terms of capturing more effective universal topological properties.\nIn this section we now report the results for all other datasets when subjected to pairwise augmentations. We do want to re-iterate that since we have ten downstream datasets and five large-scale pre-train datasets, we select Reddit-binary as the downstream task and the smallest pre-train DBLP (SNAP) dataset as a demonstration. Using more pre-train datasets should result in further performance improvements, but due to computation time constraints, we focus on the simpler setting. We present these results (raw percentage gains over GCC) in tables 2,3,4,5,6,7,8. Statistically positive and negative cases are marked accordingly."
        },
        {
            "heading": "10.6 Additional results with MoCo and Fine-tuning",
            "text": "Pre-trained encoders fine-tuned using ADAM with 3 epochs warmup and 3 epochs ramp-down with a learning rate of 0.005 are used for the fine-tuned case. These results appear in tables 9, 10 and 11. We present results for E2E and MoCo (He et al., 2020) in both the frozen and fine-tuned setting."
        },
        {
            "heading": "10.7 Datasets and benchmark code",
            "text": "We obtain the datasets from the following sources :\n\u2022 https://github.com/leoribeiro/struc2vec/tree/master/graph\n\u2022 https://www.openacademic.ai/oag/\n\u2022 https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\nAnd the relevant benchmarks from :\n\u2022 GCC : https://github.com/THUDM/GCC\n\u2022 GraphCL : https://github.com/Shen-Lab/GraphCL\n\u2022 MVGRL : https://github.com/kavehhassani/mvgrl\n\u2022 BYOV : https://github.com/Shen-Lab/GraphCL_Automated\n\u2022 CuCo : https://github.com/BUPT-GAMMA/CuCo\n\u2022 GRACE : https://github.com/CRIPAC-DIG/GRACE"
        },
        {
            "heading": "11 Hardware details and statistical confidence intervals of results",
            "text": "Hardware and software: We tested all code on Python 3.7 with PyTorch 1.3.1, CUDA 10.1, scikit-learn 0.20.3. The Tesla V 100 (one per model per run) served as the GPU.\nWe compute statistical confidence bounds only for the methods whose results we do not copy over from the GCC paper."
        },
        {
            "heading": "12 Auxiliary experiments on OGB datasets, MNIST and CIFAR-10",
            "text": ""
        },
        {
            "heading": "12.1 MNIST and CIFAR-10 motivation - implicit assumptions for the crop augment",
            "text": "We now revisit the chain of reasoning that motivates the crop augmentation, enumerated below sequentially :\n\u2022 Images are a very important, naturally occurring subclass of attributed grid graphs (products of line graphs). Indeed, for any grid graph, assigning the nodes a 3-dimensional attribute corresponding to the RGB intensity assigns the colour part of the image. For the spatial aspect, every grid graph is enumerable in the indices of its constituent line graphs that it is a product of, i.e. we may denote a node of the grid graph as vij where 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n for a grid graph that is the product of two line graphs with m,n nodes. Associate xi,j , yi,j with every such vij , with the condition that :\nxi,j+1 \u2212 xi,j = xi,j \u2212 xi,j\u22121\nyi+1,j \u2212 yi,j = yi,j \u2212 yi\u22121,j\nClearly, then, every image can be expressed as a grid graph, while the converse is not true. We assume that this generalization is meaningful - after all, an image of shape (m,n) can equally be flattened and written as a 1-dimensional sequence with its dimensions appended separately, yielding a length of mn + 2 per channel. Every image can be expressed this way while not every 1d sequence of length mn + 2 can be formed into an image, making this a generalization. We need to demonstrate that the grid graph form of generalizing what an image is, turns out to be more meaningful via some metric than, for example, ad hoc flattening.\n\u2022 The crop operation in images, when they are considered equivalent to grid graphs, is equivalent to a value-based thresholding on nodes, depending on the values induced on them using the first two eigenvectors corresponding to the first two nonzero eigenvalues of the Laplacian. This is indeed true, ignoring numerical errors in the eigendecomposition, when the dimensions m,n with m > n of the image are such that 2n > m. However, the crop operation for images happens to be functional even when 2n > m, which is not true for the eigenvector-based cropping we propose.\n\u2022 The crop augment is known to be - practically and empirically - a runaway success among the candidate augmentations in contrastive learning so far, when the representations to be learnt are to be evaluated for image classification.\n\u2022 Clearly, if the image is to be thought of as a graph, the corresponding expectation is that our proposed graph-level crop succeed for graph classification. Therefore, we investigate if value thresholding based on the two eigenvectors, which is strictly a generalization of the crop operation, is a similar success on graphs in general.\nWhat are the questionable steps taken above ? First, using the first two eigenvectors is one of infinitely many generalizations possible of the crop augmentation. We cannot investigate all such generalizations, but we can instead check if this particular generalization continues to hold when the domain (images) is perturbed.\nSecondly, to what extent is an image actually a grid graph ? Does such a generalization remove key aspects of the image ?\nWe can see that for the latter assumption, a start would be to consider the image classification tasks such as the ubiquitous tasks on MNIST and CIFAR-10, and turn them instead into graph classification tasks, after converting the images into grid graphs. If this process makes the task hopeless, the assumption is assuredly more questionable.\nIn fact, such benchmarking (Dwivedi et al., 2020) on MNIST and CIFAR-10 has already been carried out with Graph neural networks. The accuracy obtained is close to 100% for MNIST, and above 65% for CIFAR-10, which, while not exceptional, clearly shows that some reasonable information is retained relevant to the class labels by converting images to a grid graph.\nImportantly, given such a grid graph, the nodes i.e. the pixels are initialized with their positions for such a graph classification task. We recall from our discussion of the spectra of grid graphs, that it is precisely the (x, y) positions that will be recovered via the two relevant eigenvectors.\nIf our generalization is correct, then we expect that at the point of generalization - i.e. in the original domain, the generalization and the specific operation it is generalizing (crop) will be identical operations. We now need to change the domain as slightly as possible to the level where the generalization remains valid, but the specific operation can no longer be performed.\nThis is easily achievable by replacing images (grid graphs) with their subgraphs and assuming we have no clue how these graphs came to be (an usual assumption made for graph datasets). Recall that the (x, y) positions to grid graphs were assigned using the knowledge that they were images. However, if we do not know that they are images, we can only use their adjacency matrices.\nIn the case of the complete grid graph, the adjacency matrix will be enough to recover the (x, y) co-ordinates of each pixel. However, for a subgraph, the two eigenvectors induce different values that need not correlate to (x, y) co-ordinates.\nRecall that we have claimed that the values induced by these eigenvectors are useful for segmenting (selecting subgraphs from) graphs of arbitrary kinds for contrastive learning in the view creation process, using the images as an example. If they are useful for arbitrary graphs as our graph classification benchmarks indicate, they must be useful for slightly perturbed (transformed into subgraph) version of images. It should be understood that we are talking of usefulness solely in the sense of learning optimal representations for downstream classification tasks. If they cannot even succeed at this, then our reasoning is likely to be questionable.\nTherefore, if the first two eigenvectors yield a positional encoding that is useful for the image classification task when the images are transformed into grid graphs and then made subgraphs of, the results will be consistent with our assumptions. Further, since the image has only meaningful co-ordinates upto 2 axes, we expect no benefits for increasing the dimensionality of such spectral embeddings beyond 2.\n(5,5) (?,?) Label : 1 Label : 1 (unchanged)\nNature of testing\nWe consider the following cases, on top of a previously investigated baseline scenario, where each image is converted to a grid graph and each pixel to a node, with edges between adjacent pixels, and the node attribute is 1 or 3 dimensional respectively for MNIST and CIFAR-10, to which 2 dimensions are added via the Laplacian decomposition\u2019s eigenvectors corresponding to first two nonzero eigenvalues, bringing the problem into a graph classification problem, where a GCN (5-layer GIN of the same architecture for consistency) is used to process this grid graph, and the node-level representations pooled.\nThe variants we investigate are :\n\u2022 Keeping every graph as-is\n\u2022 Replacing each graph with a subgraph, which consists of the nodes visited by a random walk starting from the center of each graph, taking 512 steps with a return probability of 0.1\n\u2022 Performing the subgraph step with a random graph crop on top of each subgraph, to simulate our augment scenario.\n\u2022 Change the positional embedding to either be absent, have only the first dimension, or have 5 dimensions.\nIn each of the following tables, namely tables 15, 16, 17, 18 for MNIST and 19, 20, 21, 22 for CIFAR-10, rows signify train sets, and columns signify test sets in terms of the modifications performed on them. Overall, we see the same pattern. The random walk, or the subsequent cropping, do not significantly harm the accuracy. There are large gains from going from 0-dimensional positional embeddings to 1, smaller ones from 1 to 2 and beyond 2, a significant drop at 5. This matches what we expect and justifies our assumptions."
        },
        {
            "heading": "12.2 Results on OGB datasets of Arxiv (accuracy) and molhiv (HIV) (ROC-AUC)",
            "text": "We also tested our pre-trained models on datasets associated with the Open Graph Benchmark aka OGB (Hu et al., 2020a). Here, the entire test occurs in the fine-tuned setting. We observed some mild benefits associated with pre-training over the common sense GIN benchmark, even when both networks had the advantage of utilizing the structural embedding (Recall, of course, that only the structural embedding aspect can transfer between widely divergent datasets that share no attributes otherwise). These results are summarized in table 23."
        },
        {
            "heading": "12.3 Citeseer and Cora",
            "text": "We ran the frozen E2E transfer case for Citeseer and Cora datasets. When we transferred our structure-only models to these datasets and did not use any node attributes, we observed 50.8 (1.6) and 68.7 (2.1) percent accuracy on Citeseer and Cora respectively (standard deviations in brackets).\nWith node features included along with the frozen encoder, the performance rose to 71.5 (1.2) and 82.1 (1.6) respectively. The fact that these values (50.8 and 68.7) are significantly higher than a random guess (approx. 14.3 and 16.7) indicates that the structure-only encoder trained on a completely different pre-training corpus is still able to learn important topological characteristics."
        },
        {
            "heading": "13 Limitations, societal impact and reproducibility",
            "text": "Limitations. Our paper is not without limitations. Currently, the pre-train datasets we use in our paper are mostly inherited from established work focusing on GNN pre-training (Qiu et al., 2020; You et al., 2020). Even though they are sufficiently large in terms of the scale of the graph, we believe our model can be further strengthened by more diverse graph datasets. As a very recent paper GraphWorld (Palowitch et al., 2022) addressed, the commonly used datasets have graph statistics that lie in a limited and sparsely-populated region in terms of metrics such as the clustering coefficient, degree distribution, or Gini coefficient. Thus, to fully benefit from the power of pre-training techniques for graph data, it would be interesting and important to extend the use of pre-train datasets to graphs with diverse structural properties.\nAnother limitation of our work is that the pre-training and transfer focuses exclusively on the graph structure information; this is a common approach for cross-domain training (Qiu et al., 2020). We believe that there is value in further investigation into techniques that can process the node feature information as well as the structure information during the pre-train stage. This especially can be seen with the OGB datasets, which may share structural information between, for example, molecules and citation networks, while sharing no attribute related information.\nPotential societal impact. Graph neural network techniques have been commonly used for prediction tasks in social networks and recommender systems. Our techniques, as a variant of graph neural networks, can be used in those scenarios to further improve the model performance. However, having such an ability is a double-edged sword. On one hand, it can be beneficial to improve user experience. On the other hand, if these techniques are used purely for a profit-driven or political driven reason, they can aim to 1) monopolize user attention for as long as possible by seducing users into carrying out actions that will make them happy on very short timescales and addictive to their product or even worse to 2) shape and influence public opinion on the key matters of the time. Thus, researchers and service providers should pay vigilant attention to ensure the research does end up being used positively for the social good.\nReproducibility. We have included the code and all hyperparameters, hardware details etc. to facilitate reproducibility."
        },
        {
            "heading": "14 Additional ablations",
            "text": "In this section, we present the variation of the model\u2019s success with changes in train dataset, degree, the standard deviation of the degree, and \u03bb2 (the second eigenvalue of the Laplacian).\nIn the ablations against \u03bb2, degree, and standard deviation of the degree we see a pronounced U-curve where the middle quintiles perform best. This could be due to the hypothesized spectral gap effect that we derive. The results in degree statistics could well be due to the fact that such statistics in turn depend greatly on the \u03bb2 values, and cannot be considered truly independent findings."
        },
        {
            "heading": "15 Theorems underlying the augmentations",
            "text": ""
        },
        {
            "heading": "15.1 Crop augmentation",
            "text": "Let us derive a few key claims that will help us put the crop augment on a surer footing.\nDenote by Pn the path-graph on n vertices, which has n\u22121 edges of form (i, i+ 1) for i = 1, . . . , n\u22121. This corresponds to the line graph.\nDefine also Rn, n \u2265 3, the ring graph on n vertices defined as Pn with an extra edge between 1 and n.\nRecall the product graph : A product of two graphs A,B with vertex sets (vA, vB) and edge sets (eA, eB) is a graph A.B where each v \u2208 A.B can be identified with an ordered pair (i, j), i \u2208 vA, j \u2208 vB . Two nodes corresponding to (i, j), (i\u2032, j\u2032) in A.B have an edge between them if and only if either i\u2032 = i, (j, j\u2032) \u2208 vB or (i, i\u2032) \u2208 vA, j = j\u2032. The product of two line graphs of length M,N respectively can be represented as a planar rectangular grid of lengths M,N . Denote by Ga,b the rectangular grid graph formed by the product Pa.Pb. Structurally, this graph represents an image with dimensions a\u00d7 b.\nFor simplicity, we will prove our work for unweighted and undirected graphs, but the properties are general and do not require these conditions.\nTheorem 2 Let A be a graph with eigenvalues of the Laplacian as \u03bb1, \u03bb2, . . . , \u03bbN and corresponding eigenvectors v1, . . . ,vN . Similarly consider B another graph with eigenvalues \u00b51, . . . \u00b5M and eigenvectors u1, . . .uM . Let the product of graphs A,B be C. Then, identifying each node in C with an ordered pair (x, y), the Laplacian of C has an eigenvector wij with eigenvalue \u03bbi + \u00b5j , such that\nwij(x, y) = vi(x)\u00d7 uj(y)\nProof : let the laplacian of C be LC . We need only compute the term\nLC(wij(x, y))\nThis is equivalent to (with eA, eB being the edge set of A,B respectively) :\n\u2211 (x,x\u2032)\u2208eA (wij(x, y)\u2212wij(x\u2032, y)) + \u2211 (y,y\u2032)\u2208eB (wij(x, y)\u2212wij(x, y\u2032))\nHowever, taking \u2211\n(x,x\u2032)\u2208eA(wij(x, y)\u2212wij(x \u2032, y)), we observe that :\n\u2211 (x,x\u2032)\u2208eA (wij(x, y)\u2212wij(x\u2032, y))\nbecomes, applying the hypothesized wij = vi(x)\u00d7 uj(y)\n\u2211 (x,x\u2032)\u2208eA (vi(x)uj(y)\u2212 vi(x\u2032)uj(y))\nTaking uj(y) in common, we recognize that \u2211\n(x,x\u2032)\u2208eA vi(x)\u2212vi(x \u2032) will yield just vi scaled by \u03bbi as vi is the eigenvector\nof the Laplacian.\nTherefore this term becomes\n\u2211 (x,x\u2032)\u2208eA uj(y)\u00d7 vi(x)\u00d7 \u03bbi\nWhile the other term, i.e. \u2211 (y,y\u2032)\u2208eB (wij(x, y)\u2212wij(x, y\u2032))\nyields similarly\n\u2211 (y,y\u2032)\u2208eB uj(y)\u00d7 vi(x)\u00d7 \u00b5j\nAdding the two, we see that the final matrix-vector product is parallel to the original vector (thus an eigenvector) with eigenvalue \u03bbi + \u00b5j .\nTheorem 3 The eigenvectors of the (un-normalized) Laplacian of Pn, for n > k \u2265 0, are of the form:\nxk(u) = cos(\u03c0ku/n\u2212 \u03c0k/2n)\nwith eigenvalues \u03bbk\n2\u2212 2 cos(\u03c0k/n)\n.\nProof : We will use the ring graph defined above. Let Pn be the path graph. R2n+2 is clearly the ring graph obtained by having two copies of Pn with 2 additional links.\nNow, Rn can be drawn on the plane with the vertex i located at (cos(\u03b1i), sin(\u03b1i)) where \u03b1 = 2\u03c0n . Observe that each vertex i has a position in the plane which is parallel to the sum of the position vectors of i+ 1 and i\u2212 1. From this, it naturally follows (by the definition of the Laplacian operator which subtracts the value of the neighbour vectors from that at the node) that the valid eigenvectors for Rn are :\nxk(i) = cos(\u03b1ki),yk(i) = sin(\u03b1ki)\nRegarding the eigenvalue, the node itself contributes 2 (as it appears in the sum twice) and each neighbour contributes \u2212 cos(\u03b1k) with 2 neighbours, leading to an eigenvalue of 2\u2212 2 cos(\u03b1k).\nNow it is trivial to find the eigenvectors of Pn from R2n. Simply take any eigenvector of R2n which has the same value for i, i+ n for i \u2264 n. Then the restriction of this eigenvector to 1 \u2264 i \u2264 n defines a valid eigenvector for Pn with the same eigenvalue. This is why the terms of the angles in the theorem are the same as path graphs with \u03c0 taking the place of 2\u03c0 as, for example\n2\u2212 2 cos(2\u03c0 2n k) = 2\u2212 2 cos(\u03c0 n k)\nWhich is the sought result."
        },
        {
            "heading": "15.2 Reordering augmentation",
            "text": "Theorem 4 Let A be the adjacency matrix of an undirected unweighted graph, D the degree matrix and P = D\u22121/2AD\u22121/2 the normalized adjacency matrix. Let Dk be the k-th order normalized diffusion matrix :\nDk = k\u2211 i=1 P k\nThen, the j-th eigenvector (sorted in order of eigenvalues and ties broken consistently) is the same for all j for any odd k. That is, for any odd k, Dk and P have eigenvectors ordered in the same sequence.\nProof : First, since the normalized Laplacian matrix L is related to P as L = I \u2212 P , and it has eigenvalues in the range [0, 2], P has eigenvalues lying in the range [\u22121, 1].\nNow, observe that P shares the same eigenvectors with P k for any k, however, eigenvalue \u03bb changes to \u03bbk. It can be seen that since the permutation is on the basis of sorting eigenvalues, the view for A + A2 + \u00b7 \u00b7 \u00b7+ Ak will coincide with A if fk(x) = x+ x\n2 + \u00b7 \u00b7 \u00b7+ xk is monotonic in the range [\u22121, 1], which is the range of allowed eigenvalues of the normalized adjacency matrix. It is trivial to note that fk(x) is monotonically increasing for x \u2208 [0, 1] for any k. Now, ignoring the case |x| = 1, observe that 1 + fk(x) = 1\u2212x k+1\n1\u2212x by the geometric progression formula. If k is odd, k + 1 is even, and thus xk+1 is positive for x \u2208 [\u22121, 0]. As we move x from \u22121 to 0 the numerator monotonically rises, and the denominator monotonically falls from 2 to 1, meaning that overall the function is monotonic and the ordering will just mirror x. This is not true when the sum terminates at an even power, for instance, x + x2 which is 0 at \u22121 and 0 but negative at \u22121/2, indicating that it cannot be monotonic. The case where x is 1 or \u22121 is trivially true."
        },
        {
            "heading": "15.3 Alignment closed forms",
            "text": "Given a function of the following nature where Q is orthogonal\n||XQ\u2212 Y ||2\nMinimization of the above function can be done by noting that this is equivalent to working with :\n||XQ||2 + ||Y ||2 \u2212 2\u3008XQ,Y \u3009\n||XQ|| = ||X||, and we only have \u3008XQ,Y \u3009 to maximize. This is Y TXQ, which is equal to \u3008Q,XTY \u3009. Maximizing this boils down to the projection of the matrix XTY on the set of orthogonal matrices under the square Frobenius norm. Let the SVD of XTY be USV T , then we have \u3008Q,USV T \u3009 being minimized. This becomes \u3008UTQV ,S\u3009, with UTQV orthogonal and made to maximize inner product with diagonal S, implying that Q = UV T is the solution."
        },
        {
            "heading": "16 Proofs on the stochastic block model",
            "text": "Usually, one divides graph contrastive learning and more generally all of contrastive learning into two categories :\n\u2022 Supervised contrastive learning : There are unseen labels for each node. Our proofs will center on showing that with high probability, augmentations either preserve these unseen labels or some straightforward function of them that are made to suit the situation. For example, in a graph classification setting formed from the ego-graph of the nodes, the graph can be given the label of the node it is formed as an ego-graph from or the majority label of the nodes it contains. Our proofs in this case deal with the graph label and not the node label.\n\u2022 Unsupervised graph contrastive learning : Each node is its own class (the classic setting of graph contrastive learning). In this scenario, it is not possible to work with the node label. Since in our setting, the nodes also possess no nodespecific attributes beyond the structural information, our work here must focus on the structures obtained under spectral augmentation only.\nIn both cases we assume contrastive learning in general to work. That is, we show that the process of generating positive pairs etc. continues properly, but not anything about whether contrastive learning as a whole can generalize better or learn better representations. We view such a proof as outside the scope of this paper.\nIn the paper, we have worked with six distinct augmentations , of which two modify the structures chosen : Crop and Similar/Diverse. Three of them modify the attributes alone : Mask, reorder, and align. In general, nothing can be proven about the latter three without assuming an easy function class such as linear classifiers, which we view as unrealistic. Hence, our work focuses on the first two.\nSecondly, we work with the stochastic block model and the two-cluster case where differing label of a node indicates a different propensity to create edges. We only focus on the case where there are seen or unseen labels which are related to the structure of graphs. This can be seen as a scenario intermediate between the supervised and unsupervised contrastive learning case, and the block model a natural reification to study it, for it is well known (Rohe et al., 2011) that conditional on the label of a node being known, the degree structure and density etc. strongly concentrate around their fixed values for the stochastic block model. Indeed, no other parameter except the label which directly determines the edge density even exists to provide information about the structure. Proving that nodes of similar (seen or unseen) labels are brought together by our augmentations carries over to the unsupervised case fully as these parameters are the only ones directly determining the structure.\nBy assuming that (unseen) labels exist, our proof is quite relevant to the actual use case of the paper. This is because in the downstream usage, the classifier is used, zero-shot, to provide representations that are used to predict the label. In other words, hidden latent labels are assumed to be predictable from the structure. Our case should be understood as a special case of unsupervised representation learning that shares some conditions with the supervised scenario."
        },
        {
            "heading": "16.1 Proof sketches and overall meaning",
            "text": "Our proofs center around the stochastic block model. In this setting the spectrum is well known and analyzed. We show that in this case, the \u201ccrop\" operation around a node v extracts a sub-graph of nodes which largely possess the same label as v itself, where the label is considered to coincide with the cluster(block). Under the contrastive learning assumption, then, \u201ccrop\" recovers positive pairs.\nWe also show that common embedding methods such as LINE, DeepWalk etc. are meaningful in terms of establishing \u201csimilar\" and \u201cdiverse\" views in the stochastic block model and that \u201csimilar\" filtering would indeed yield a pathway to setting positive pairs apart. This is done by re-using our analysis for the supervised case which looks at the spectrum, and re-using the results from NETMF (Qiu et al., 2018) which connects the spectral results to embeddings obtained by random walks. In short, random walks and corresponding embeddings on stochastic block models can be seen, in the limit, as spectral decompositions of a block model with parameters that depend on the original block model. After this, we can recognize that the analysis for \u201ccrop\", which essentially shows that the spectral embeddings form a meaningful metric of closeness in terms of label, cluster etc. on the original model, fully carries over with transformed parameters."
        },
        {
            "heading": "16.2 Supervised contrastive learning derivation",
            "text": "We define our stochastic block model (Rohe et al., 2011) as follows in keeping with conventions of the field. We will consider one with two components.\n\u2022 There are N0 nodes generated with label 0, and N1 with label 1. Denote the group generated with label 0 as C0 and the other as C1. For simplicity, set N0 = N1 = N\n\u2022 An edge occurs with probability p among two nodes of label 0, with probability q between two nodes of label 1, and with z among two nodes of different labels. z < min(p, q) is a common assumption. Without loss of generality we can take p > q > z. We also consider the self-edges to be valid.\nNote that in the setting of GCC, different local structures encode different labels. Hence p 6= q, as if they were equal it would imply the same structural generation process gives rise to two different labels.\nLet A be the adjacency matrix of the stochastic block model and L the laplacian. Let \u03bbn(v) be the function that assigns to a node v its value under the n-th eigenvector of the Laplacian. Let C (v) be the cropped local neighbourhood around any node v defined as {v\u2032 : ||\u03bb(v\u2032)\u2212 \u03bb(v)|| \u2264 } where \u03bb(v) = [\u03bb2(v), \u03bb3(v)]."
        },
        {
            "heading": "16.3 Factorizing the adjacency matrix",
            "text": "The overall matrix A is of shape 2N \u00d7 2N . Recall that we have allowed self-edges and diagonal entries of A are not zero. Consider the matrix :\n[ p z z q ] Let W be a 2N \u00d7 2 matrix formed by repeating the row vector [ 1 0 ] N times and then the row vector [ 0 1 ] N times. W denotes the membership matrix. The first N rows of W denote that first N nodes belong to label 0 (and hence their zero-th entry is 1) and the next N rows likewise have a 1 on their 2-nd column signifying that they have label 1.\nNow, we can see that if we multiply W [ p z z q ] , the first N rows of the resulting 2N \u00d7 2 matrix will be [ p z ] and the next\nN will be [ z q ] . Consider now multiplying from the right side with WT i.e. forming, overall,W [ p z z q ] WT . This matrix will be of shape 2N \u00d7 2N and it can be seen that it has a block structure of form :\n[ p(1N1 T N ) z(1N1 T N )\nz(1N1 T N ) q(1N1 T N ) ] Where, 1N is the N \u00d7 1 vector of all 1-s, and 1N1TN the N \u00d7N matrix of all 1-s. So, it can be seen that the above matrix is nothing but the expectation of the stochastic block model\u2019s adjacency matrix.\nNow, can we avoid analyzing this matrix and instead settle for analyzing the comparatively simpler [ p z z q ] ? Let v be an\neigenvector of [ p z z q ] with eigenvalue \u03bb. Let v have entries [ x y ] . By hypothesis, [ p z z q ] v = \u03bbv. Then, if we have the vector v\u2032 of shape 2N \u00d7 1 with first N entries as x, and the next N as y\n[ p(1N1 T N ) z(1N1 T N )\nz(1N1 T N ) q(1N1 T N )\n] v\u2032 = (\u03bbN)v\u2032\n.\nIt can be seen that v\u2032 is parallel to an eigenvector of the expectation of the adjacency matrix and the corresponding eigenvalue is \u03bbN . However, we always assume eigenvectors are of unit norm, i.e. x2 + y2 = 1. So, v\u2032 is going to be not x repeated N times, but x\u221a\nN N times and then y\u221a N N times. This makes \u2016v\u2032\u2016 = 1 = \u2016v\u2016. Therefore, for every pair \u03bb, v in the spectra of\nthe 2\u00d7 2 matrix, there is a corresponding \u03bbN, v\u2032 in the spectra of the expectation of the adjacency matrix. Next, note that\nthe rank of the expectation of the adjacency is \u2264 Rank( [ p z z q ] ) \u2264 2. So if the 2\u00d7 2 matrix has a full rank, there can be no extra eigenvalue-eigenvector pairs for the corresponding expectation of the adjacency matrix. All the nonzero eigenvalues and corresponding eigenvectors of the expectation of the adjacency are derivable from the 2\u00d7 2 matrix\u2019s spectra. In short, to understand the spectrum of the expectation of adjacency, we can just study the 2\u00d7 2 matrix, as there is a one to one relationship between the nonzero eigenvalues and corresponding eigenvectors."
        },
        {
            "heading": "16.4 Crop augmentation",
            "text": "Notation of probability. In proofs involving convergence, it is customary to provide a guarantee that a statement holds with probability \u2265 1\u2212 F (N) where F (N) tends to zero as N goes to infinity. We will somewhat abuse the notation and say the probability\u2192 1 as N \u2192\u221e to denote this. We do not distinguish between things such as convergence in probability, convergence in distribution, almost surely convergence etc. and provide a largely concentration-inequality based proof overview.\nWe can state our proof for the crop augmentation as follows. All of the following statements hold with high probability (i.e. hold with a probability that\u2192 1 as N \u2192\u221e.)\nTheorem 5 Let the number of samples N \u2192 \u221e. Let v be chosen uniformly at random from the nodes. The following statements hold with a probability that\u2192 1 as N \u2192\u221e :\n\u2022 Proposition 1 : The majority label in C (v) is the label of v.\n\u2022 Proposition 2 : Two nodes v, v\u2032 of different labels generate non-isomorphic cropped subgraphs C (v), C (v\u2032), if v\u2032 is chosen uniformly at random as well.\n\u2022 Proposition 3 : For any v and C (v), there is no differently labeled v\u2032 and C (v\u2032) which is isomorphic to C (v) for a high enough , if both are chosen uniformly at random.\nNote that in the main text, we state a slightly different version of the theorem (Theorem 1 of the main text) involving ego networks as well, which we restate here :\nTheorem 6 Let node v be chosen uniformly at random fromG, a 2N -node graph generated according to the SBM described above. With probability \u2265 1\u2212 f(N) for a function f(N)\u2192 0 as N \u2192\u221e, \u2203 \u2208 R+, kmax \u2208 N such that :\n\u2200k \u2208 N \u2264 kmax, Y (Ek,v(G)) = Y (C (v)) = Y (v) (7)\nTerming kmax used above as kcrit, we can see that this is adding an extra part that agrees with the node label, namely the majority label of an ego network. However, the majority label for the k ego network when k \u2264 kcrit is trivially equal to the node\u2019s label for at least some values of kcrit allowing k beyond 0 (i.e. the node itself is the ego network). To see this, take k = kcrit = 1. The expected number of nodes of the same label - considering label 0 for simplicity - in its ego network is p(N \u2212 1) + 1, and the ones of a different label are of expected number zN . By Hoeffding\u2019s inequality, both quantities with high probability i.e. with probability \u2265 1\u2212O(exp(\u2212\u03b42)) have deviations of only O(\u03b4 \u221a N) from their expectations which are terms of O(N). Therefore, the majority label agrees with the node\u2019s own label with high probability (note that this requires p, q > z). We assume this step to hold with high probability and focus now on proving the equality of the cropped subgraph\u2019s majority label and the node label. We discuss ego networks other than the 1-ego network at the end of the section. It can be easily checked that at least for the 1-ego network, our proof involving mostly the vertex label case requires no changes.\nTo prove this, consider the random matrix A. We can denote the expectation of A as A\u2217. We can see the rank of A\u2217 is 2 as it has exactly 2 possible types of columns in it. It remains to find the corresponding two eigenvectors and eigenvalues. Clearly, by the structure of the matrix, the eigenvectors are of the form of repeating one value c N times and then another value c\u2032 N times, and we can WLOG set c = 1 and replace c\u2032 with c. Then it remains to solve\n[ p z z q ] [ 1 c ] Which becomes, by the definition of eigenvector,\nz + qc p+ zc = c\nor, simplifying :\nzc2 + (p\u2212 q)c\u2212 z = 0\nTherefore, by the quadratic formula :\nc = (q \u2212 p)\u00b1 \u221a (p\u2212 q)2 + 4z2 2z\nThe eigenvalue for a corresponding c is p+ zc. Since p, z \u2265 0 the larger eigenvalue \u03bb1 corresponds always to the larger value of c. Therefore, \u03bb2 takes the value gained by plugging in the negative root above, for c. This yields the unnormalized eigenvector, the actual entries assigned under the eigendecomposition are respectively 1\u221a\n1+c2 , c\u221a 1+c2 when a vertex v is\nassigned its value under the eigenvector.\nWe re-use previous results in random matrix theory (Vu, 2007) (theorem 1.4) that imply that with a probability \u2192 1, ||A\u2212A \u2217 ||op \u2264 \u221a 18pN , when p is \u2126(logN)4/N . Since the lower bound on p\u2192 0 as N \u2192\u221e we can assume it to hold for large N . The \u2016 \u00b7 \u2016op notation denotes operator norm.\nExplanation of the order through Hoeffding and RIP property. To intuitively understand the above result, we can consider each entry of A\u2212A\u2217. This is a random variable (blockwise) that takes one among the following set of values : \u2212p, 1\u2212p (among the N \u00d7N entries of label 0), \u2212z, 1\u2212 z (among the 2\u00d7N \u00d7N entries between labels 0, 1) and\u2212q, 1\u2212 q (among label 1). Now, only the upper triangle and diagonal are independent as the edges are symmetric, and the matrix is symmetric about the diagonal. We can write that :\n||A\u2212A \u2217 ||2F = 2||A\u2212A \u2217 ||2F,UT + ||A\u2212A \u2217 ||2F,D\nWhere, F denotes Frobenius norm, and UT,D respectively denote summing over upper triangular and diagonal indices.\nTherefore, if each entry of A\u2212A\u2217 be denoted as \u2206i, enumerated in any order over the diagonal and upper triangle of the 2N \u00d7 2N matrix (1 \u2264 i \u2264 N(2N + 1)), \u2206i,\u2206j are independent r.v.s for any i 6= j. Further, \u22121 \u2264 \u2206i \u2264 1. Therefore, 0 \u2264 \u22062i \u2264 1. Further, ||A\u2212A \u2217 ||F \u2264 \u221a\u2211 2\u22062i .\u2211\n\u22062i is a sum of independent random variables in a fixed, finite range of size 1. Therefore, Hoeffding\u2019s inequality applies, which yields that with probability \u2265 1\u2212O(1/N) :\nE( \u2211 \u22062i )\u2212 \u221a N(N + 1/2) log(N) \u2264 \u2211 \u22062i \u2264 E( \u2211 \u22062i ) + \u221a N(N + 1/2) log(N)\nE( \u2211\n\u22062i ) is the sum of the variances of random variables \u2206i over N(2N + 1) entries, bounded above by 1. Hence, it follows that ||A\u2212A \u2217 ||2F is O(N2) with probability \u2265 1\u2212O(1/N), therefore, ||A\u2212A \u2217 ||F is O(N).\nFurther, we can see that A\u2212A\u2217 has the following structure :\n[ J K K L ] Where, J, L are symmetric matrices with each upper triangular and diagonal entry as i.i.d random variables Z satisfying :\nZ = a with probability p else Z = b, E(Z) = 0\nIn addition, K is a matrix (not necessarily symmetric) which has every entry as i.i.d realizations of such Z. Then, such a matrix [ J K K L ] by Restricted Isometry Property (Vu, 2014) has the property that, with high probability, there is a constant K \u2032 independent of N :\nmax i \u03bbi(A\u2212A\u2217) \u2248 \u221a\u221a\u221a\u221aK \u2032 N N\u2211 i=1 \u03bb2i [(A\u2212A\u2217)]\nThe left hand side is the maximum eigenvalue, which we recognize as \u2016A\u2212A \u2217 \u2016op i.e. the operator norm.\nNow, using the relation between eigenvalues and the Frobenius norm :\nN\u2211 i=1 \u03bb2i [(A\u2212A\u2217)] = ||A\u2212A \u2217 ||2F = O(N2)\nThe RHS comes from plugging in the Frobenius norm bound from the Hoeffding\u2019s inequality step. Finally, this yields :\n||A\u2212A \u2217 ||op = O( \u221a N)\nThe result of Vu\u2019s we state above is merely a formalization of this sketch with constants, the order is the same i.e. \u221a N .\nNow consider the second eigenvector i.e. the \u03bb2 function from A against the calculated \u03bb2 above for A\u2217. We need to use the Davis Kahan theorem (Demmel, 1997) (theorem 5.4) which states that if the angle between these is \u03b8, then\nsin 2\u03b8 \u2264 2||A\u2212A \u2217 ||op 2N \u2217min(|\u00b51 \u2212 \u00b52|, \u00b52)\nAs both eigenvectors are unit vectors, we can use the property that if two unit vectors have angle \u03b8 between them, the norm of their difference is bounded above by \u221a 2sin2\u03b8. Ignoring constants, we end up with the result that for some constant c0, and denoting vM,i the i-th eigenvector of the Laplacian formed from some adjacency matrix M\n||vA,2 \u2212 vA\u2217,2|| \u2264 c0 \u221a pN\nN \u2217min(|\u00b51 \u2212 \u00b52|, \u00b52)\nThe LHS however works with the eigenvector of the overall adjacency matrix formed by the multiplication by W we have discussed above. Recall that we have already noted the adjacency matrix and the 2\u00d7 2 matrix share eigenvalues upto scaling in N . Their eigenvectors are also likewise related, and since eigenvectors are always of unit norm, an eigenvector of the 2 \u00d7 2 matrix is first repeated in its entries and then normalized by a factor of 1\u221a\nN by virtue of being an eigenvector, to\nbecome an eigenvector of the overall adjacency matrix.\nBy substituting the eigenvectors we found earlier, i.e. 1\u221a 1+c2\n[1, c]T into the LHS, scaling by this \u221a N factor cancels the\nextra \u221a N on the RHS. Recalling that\nc = q1 = (q \u2212 p)\u2212 \u221a (p\u2212 q)2 + 4z2 2z\nLet :\nS1 : {x : vA\u2217,2(x) = 1\u221a\n1 + q21 , vA,2(x) \u2264\n1 + q1 2 \u221a 1 + q21 + 2 }\nS2 : {x : vA\u2217,2(x) = q1\u221a\n1 + q21 , vA,2(x) \u2265\n1 + q1 2 \u221a 1 + q21 \u2212 2 }\nLet v1, v2 be any pair of nodes that satisfy the conditions of :\n\u2022 Labels of v1, v2 are different. WLOG, let v1 have label 0, v2 label 1.\n\u2022 C (v1) contains v2 (and by symmetry, C (v2) contains v1)\nClearly, we can see that either v1 \u2208 S1 or v2 \u2208 S2. Let K1 = |S1|,K2 = |S2|. Summing only over S1 :\nK( 1\u2212 q1 2 \u221a 1 + q21 \u2212 /2)2 \u2264 p(c0) 2 min((\u00b51 \u2212 \u00b52)2, \u00b522)\nThis yields a bound on K1, which we can term K1,max = p(c0)\n2\n( 1\u2212q1 2 \u221a\n1+q21\n\u2212 /2)2\u00d7min((\u00b51\u2212\u00b52)2,\u00b522) . Similarly, we can consider K2\nto get K2,max as : p(c0)\n2\n( 1\u2212q1 2 \u221a\n1+q21\n\u2212 /2)2\u00d7min((\u00b51\u2212\u00b52)2,\u00b522) . Set = 1\u2212q1 2 \u221a 1+q21 to get :\nK1,max +K2,max \u2264 2p(c0)\n2\n( 1\u2212q1 4 \u221a 1+q21 )2 \u00d7min((\u00b51 \u2212 \u00b52)2, \u00b522)\nSince K1,max = K2,max we can term it Kmax. Simultaneously, consider :\nS3 : {x : vA\u2217,2(x) = 1\u221a\n1 + q21 , vA,2(x) \u2265\n1 + q1 2 \u221a 1 + q21 + 3 2 }, = 1\u2212 q1 2 \u221a 1 + q21\n1 + q1 2 \u221a 1 + q21 + 3 2 = 5 4 \u2212 q1 4\u221a 1 + q21 \u2265 1\u221a 1 + q21 +\n1 4 \u221a\n1 + q21\nLast inequality is by the property q1 < 0. By a similar argument as for S1, S2, S3 is of constant size and S3N \u2192 0 as N \u2192 \u221e. Let the maximum size of S3 be K3,max. Now, if we pick a vertex v of label 0 at random, with probability \u2265 1\u2212 K3,max+KmaxN , v /\u2208 S1, v /\u2208 S3. If both these conditions hold, in C (v) any v\n\u2032 which does not have the same label must have v\u2032 \u2208 S2. (Because any such pair must have at least one element in S1, S2 and v /\u2208 S1). Simultaneously, C (v) contains at least all vertices of label 0 not in S1 \u22c3 S3 i.e. has vertices of label 0 \u2265 N \u2212K3,max \u2212Kmax. Noting that K values are all constants and N \u2192\u221e implies that majority label in C (v) will agree with v as Kmax/N \u2192 0 completes the proof. The only aspect of the proof which required high probability was the norm of ||A\u2212A \u2217 ||op varying as \u221a N , this step may be assumed to be true with probability \u2265 1\u2212O(1/N3) (tighter bounds are possible but this suffices). This concludes the proof of proposition 1.\nTightness of operator norm. Consider the statement that :\n||A\u2212A \u2217 ||2F = O(N2)\nRecall that we showed :\nE( \u2211 \u22062i )\u2212 \u221a N(N + 1/2) log(N) \u2264 \u2211 \u22062i \u2264 E( \u2211 \u22062i ) + \u221a N(N + 1/2) log(N)\nWith high probability. We used the bound of the RHS, but the bound of the LHS is also true. Hence, ||A\u2212A \u2217 ||2F = \u0398(N2).\nNext, we used :\nmax i \u03bbi(A\u2212A\u2217) \u2248 \u221a\u221a\u221a\u221aK \u2032 N N\u2211 i=1 \u03bb2i [(A\u2212A\u2217)]\nHowever :\nmax i \u03bbi(A\u2212A\u2217) \u2265 \u221a\u221a\u221a\u221a 1 N N\u2211 i=1 \u03bb2i [(A\u2212A\u2217)]\nTherefore, maxi \u03bbi(A\u2212A\u2217) = ||A\u2212A \u2217 ||op = O( \u221a N).\nIn short, every step till we apply the Davis-Kahan bound, i.e. :\n||vA,2 \u2212 vA\u2217,2|| \u2264 c0 \u221a pN\nN \u2217min(|\u00b51 \u2212 \u00b52|, \u00b52)\nIs as tight as possible.\nTightness of Davis-Kahan bound. The Davis-Kahan upper bound is sharp. That is, \u2203S,H , S = ST , H = HT , with \u00b51 \u2265 \u00b52 \u2265 . . . ..\u00b5N the eigenvalues of S, v1, v2, . . . , vN the corresponding eigenvectors of S, v\u20321, v\u20322, . . . , vN the eigenvectors of S +H , \u03b8i the angle between vi, v\u2032i satisfying :\nsin(2\u03b8i) = c \u2032 2||H||op minj 6=i |\u00b5i \u2212 \u00b5j |\nWhere c\u2032 is a constant \u2264 1 that does not depend on N. And at the same time, \u2200S,H, \u00b5i, vi, v\u2032i :\nsin(2\u03b8i) \u2264 2||H||op\nminj 6=i |\u00b5i \u2212 \u00b5j |\nHowever, in our case, S is not arbitrary, but S = A\u2217. When we take \u2203S, it allows taking e.g. S = [ 0.6 0.8 0.8 0.7 ] . But this cannot be A\u2217 with N = 1, as it violates all our assumptions for A\u2217, here p = 0.6, q = 0.7, z = 0.8 violating p > q > z assumptions. We need to show that \u2203S,N,H such that H = HT and :\nS = WS\u2032WT , S\u2032 = [ p z z q ] , 0 \u2264 z \u2264 q \u2264 p \u2264 1\nwith W of shape 2N \u00d7 2 such that first N rows of W are [1, 0], next N are [1, 0]. This makes S = ST and we already constrain H = HT . With \u00b51 \u2265 \u00b52 \u2265 . . . ..\u00b52N as eigenvalues of S, v1, v2, . . . , v2N ,v\u20321, v\u20322, . . . , v2N corresponding eigenvectors of S, S +H we must show \u2203i\nsin(2\u03b8i) = c\u201d 2||H||op\nminj 6=i |\u00b5i \u2212 \u00b5j |\nWhere c\u201d is constant, not a function of N . This is reached at :\nS\u2032 = [ 0.6 0 0 0.4 ] , H \u2032 = [ \u22120.1 0.1 0.1 0.1 ]\nS = WS\u2032WT , H = WH \u2032WT\nWhere W is as specified and of shape 2N \u00d7 2.\nProof of proposition two.\nRecall that by the proof of proposition one, C (v) contains M nodes of label 0, where if v is selected randomly over all nodes with label 0, with probability\u2192 1, MN \u2192 1. This step is with probability \u2265 1\u2212O(1/N 3).\nLet E0 be the set of all edges (vi, vj) with vi, vj having label both labels 0. Let M(0, v) be the set of all edges (vi, vj) s.t. (vi, vj) \u2208 E0, vi, vj \u2208 C (v). Clearly, M0,v \u2286 E0, and since MN \u2192 1, |M0,v| |E0| \u2192 1.\nBy a similar argument, let M1,v\u2032 , E1 be the corresponding edge sets for C (v\u2032) with label of v\u2032 being 1, |M1,v\u2032 | |E1| \u2192 1.\nE0, E1 are, denoting Bw(p) as an independent Bernoulli random variable of bias p :\nE0 = N(N+1)/2\u2211 w=1 Bw(p), E0 = N(N+1)/2\u2211 w=1 Bw(q)\nVia Hoeffding\u2019s inequality, E0 = (pN(N+1)/2)+O(N \u221a logN), E1 = (qN(N+1)/2)+O(N \u221a\nlogN) with probability \u2265 1\u2212O(1/N3). Therefore, with probability \u2265 1\u2212O(1/N3), C (v), C (v\u2032) are not isomorphic. This proves proposition two. Applying the union bound over all choices of (v, v\u2032) proves proposition three, because number of possible pairs is O(N2) and the property holds with \u2265 1\u2212O(1/N3), leading to overall probability \u2265 1\u2212O(1/N).\nNote that the step of the 1 ego-network\u2019s majority label agreeing with the node\u2019s label was derived by Hoeffding\u2019s inequality and does not affect any order used so far. Hence, this completes the proof of propositions 1, 2, 3."
        },
        {
            "heading": "16.5 Embedding-based similarity",
            "text": "First, we remind the reader that usually, each node embedding method such as LINE (Tang et al., 2015) always normalizes its embedding per node. That is, each node v receives some vector ev with \u2016ev\u2016 = 1. But that, in turn implies that given two distinct embeddings ev, ev\u2032 ,\n\u3008ev, ev\u2032\u3009 \u2016ev\u2016\u2016ev\u2032\u2016 = \u3008ev, ev\u2032\u3009\n\u2016ev \u2212 ev\u2032\u20162 = \u2016ev\u20162 + \u2016ev\u2032\u20162 \u2212 2\u3008ev, ev\u2032\u3009 = 2\u2212 2\u3008ev, ev\u2032\u3009\nThat is, selecting on the more similar cosine distance (similar filtering) is equivalent to selecting on lower values of \u2016ev\u2212ev\u2032\u2016 - the type of proximity analyzed in crop. This simplifies our analysis, allowing re-use of crop results.\nIn the context of embeddings, let us analyze two in particular : DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015). It is known previously from the analysis of NETMF (Qiu et al., 2018) that these methods replicate matrix factorization. Specifically, let A,D be the adjacency matrices and degree matrices, then :\nPr = 1\nT ( T\u2211 r=1 (D\u22121A)r)D\u22121\nLet the volume of a graph G V (G) be the sum of the number of edges, then log(V (G)Pr)\u2212 log b where b is the negative sampling rate is a matrix Zr. Under the framing above, DeepWalk factors any Zr, while LINE factors Z1, i.e. LINE is a special case of DeepWalk. This log is taken per element.\nNow, we are ready to state our theorems for similarity. Let E(v) be the embedding assigned to a node v. Let p, q, z be as before.\nTheorem 7 Let the number of samples N \u2192\u221e, pq 6=\u221e, \u2203 crit such that \u2200v, let {Sv : v \u2032, label(v\u2032) = label(v), \u2016E(v)\u2212 E(v\u2032)\u2016 \u2264 crit }, then, |Sv|N \u2192 1 and C crit(v) satisfies the three propositions of theorem 1, when v is chosen uniformly at random.\nOur proof for this will first translate the graph adjacency to familiar matrix forms. Note that V (G) is equal to, in expectation and allowing self-loops, as :\n2N2(p/2 + q/2 + z)\nFurther, we can set b = 1 to remove it from consideration. Set r = 1 to recreate the case of LINE. Now, by a similar argument as for the cropping analysis for SBM where the matrix W generates the A\u2217 matrix using a 2\u00d7 2 matrix, we can examine the 2\u00d7 2 matrix which is :\n[ p\n(p+z)2 z\n(p+z)(q+z) z\n(p+z)(q+z) q (q+z)2\n]\nNow, let us apply the logarithm to get our new values of p\u2032, q\u2032, z\u2032 which will be fed back to crop analysis and behave equivalently to the original parameters (as before in the crop case, the N2 term in V (G) can be ignored while reducing to the 2\u00d7 2 case) :\np\u2032 = log p\u2212 2 log(p+ z) + log(p+ q + 2z)\nq\u2032 = log q \u2212 2 log(q + z) + log(p+ q + 2z)\nz\u2032 = log z \u2212 log(p+ z)\u2212 log(q + z) + log(p+ q + 2z)\nNote that p > q implies p\u2032 < q\u2032, as :\np\u2032 \u2212 q\u2032 = log p\u2212 log q + 2 log(q + z)\u2212 2 log(p+ z)\np(q + z)2 \u2212 q(p+ z)2 = pq2 + pz2 \u2212 qp2 \u2212 qz2 = (p\u2212 q)(z2 \u2212 pq) < 0\nAnd, p > z implies p\u2032 > z\u2032, as :\np\u2032 \u2212 z\u2032 = log p\u2212 log(p+ z) + log(q + z)\u2212 log z\np(q + z)\u2212 z(p+ z) = pq \u2212 z2 > 0\nTherefore, the analysis from the crop case carries over, except we swap the order of p, q. This is equivalent to swapping the labels of the nodes, and makes no difference. Recall that the error rate for the spectral analysis in the crop section depends on p\u2032 (numerator) and min(\u00b52, |\u00b51 \u2212 \u00b52|) (denominator) =\u21d2 : the error will remain bounded above iff : |p\u2032|, |q\u2032| are bounded above and min(\u00b52, |\u00b51 \u2212 \u00b52|) are bounded below.\nCases of issues in p\u2032, q\u2032. Can be ruled out as follows :\n\u2022 p\u2032 = log p\u2212 2 log(p+ z) + log(p+ q + 2z) \u2264 log p\u2212 2 log p+ log 4p = log 4.\n\u2022 q\u2032 \u2264 log q \u2212 2 log q + log(4p) = log 4 + log(p/q) <\u221e by hypothesis p/q 6=\u221e.\nCase of eigenvalue issues, i.e. : min(\u00b52, |\u00b51 \u2212 \u00b52|) \u2192 0 =\u21d2 \u00b52 \u2192 0, \u00b51 \u2212 \u00b52 \u2192 0. From the analysis of crop, the eigenvalues are of the form p\u2032 + z\u2032c\u2032 where :\nc\u2032 = (q\u2032 \u2212 p\u2032)\u00b1 \u221a (p\u2032 \u2212 q\u2032)2 + 4z\u20322 2z\u2032\nNote that z\u2032 is < 0. To see this, observe that :\n(p+ q + 2z)z = 2z2 + pz + qz = (p+ z)(q + z)\u2212 pq + z2 < (p+ z)(q + z)\nSimultaneously, p\u2032 > 0, as :\np(p+ q + 2z)\u2212 (p+ z)2 = pq \u2212 z2\nSo the eigenvalues are :\n\u00b51 = (q\u2032 + p\u2032) + \u221a (p\u2032 \u2212 q\u2032)2 + 4z\u20322 2 , \u00b52 = (q\u2032 + p\u2032)\u2212 \u221a (p\u2032 \u2212 q\u2032)2 + 4z\u20322 2\nmin\u00b52, |\u00b52 \u2212 \u00b51| = 0 \u21d0\u21d2 z\u2032 \u2192 0\n0 > z\u2032 = log(pz + qz + 2z2)\u2212 log(pq + pz + qz + z2) = log(1\u2212 pq \u2212 z 2\npq + z2 + pz + qz )\nSince p > q > z, pq \u2212 z2 > 0, z\u2032 \u2192 0 is not possible, and we are done. We simply need to adjust for the final step of vertex-wise normalization. Since in this case, \u00b51 and its corresponding eigenvector is also utilized, we have that all elements of cluster 0 receive embeddings of form (recall q1, q2 are the roots of the quadratic for c) [ 1\u221a\nN(1+q21) , 1\u221a N(1+q22) ] pre-\nnormalization, and upon vertex-wise normalization this becomes [ \u221a\n1+q22\u221a 2+q21+q 2 2 ,\n\u221a 1+q21\u221a\n2+q21+q 2 2\n]. For cluster 2, the corresponding\npost-normalization embeddings are [ q1 \u221a\n1+q22\u221a q21+q 2 2+2q 2 1q 2 2\n, q2 \u221a\n1+q21\u221a q21+q 2 2+2q 2 1q 2 2 ]. These are both on expectation and all the gaps in\nexpectation remain O(N) with deviations of O( \u221a N). Re-applying the results from the crop section, we get that there is\ncrit such that with probability \u2265 1 \u2212 O(1/N3) with a fixed v, a fraction v\u2032 \u2192 1 sharing the label of v lies within crit, while at most O(1) (the Kmax terms derived earlier) do not fall within this crit or are v\u2032\u2032 not sharing the label of v but lying within crit. Most importantly, q1, q2 differ in sign, making the inner product consist of two positive terms between nodes of same label with high probability and one positive and one negative term between nodes of differing labels - the inner product is now a meaningful similarity metric ! For an easy example, we can see that setting p = q leads to (in expectation) embeddings [ 1\u221a\n2 , 1\u221a 2 ] for one label/cluster and [ 1\u221a 2 , \u22121\u221a 2 ] for the other. This means nodes of two differing labels have \u2248 0\ninner product and two of the same have inner product \u2248 1 with high probability.\nClearly, in the SBM case, similar filtering is the correct course of action with the threshold being this crit. It is plausible that in other generative graph cases, this would not be the case. However, in our experiments, similar was always superior to diverse filtering, possibly reflecting that real life graphs are well-modeled by SBMs in this aspect. Note also that our graphs such as LiveJournal, FaceBook etc. arrive from communities in social networks which may be expected to display a clustered / stochastic block model type of pattern. Note that the factorization under the transformed parameters is not necessarily a descending algebraic decomposition, but one where we perform the decomposition in order of magnitude."
        },
        {
            "heading": "16.6 Ignorable perturbation effect from third eigenvector",
            "text": "In both the proofs, we have examined only 2-component SBMs. In these cases, \u00b53 and its corresponding eigenvector plays no role and indeed we have proven everything in terms of \u00b52, \u03bb2 alone. This is because in the expected adjacency matrix, \u00b53 = 0.\nThe proof fully extends to the case where \u03bb3 is added. For simplicity, we did not add it, and the only change required is that for every node, we instead use \u00b52\u03bb2 instead of \u03bb2, and \u00b53\u03bb3 instead of \u03bb3. Since \u00b53 in the original expected adjacency (A\u2217) is zero, it is solely from A\u2212A\u2217 that \u00b53 arises. By Weyl\u2019s eigenvalue perturbation theorem (Weyl, 1912),\n\u03bb3(A\u2212A\u2217) \u2264 \u00b53 \u2264 \u03bb1(A\u2212A\u2217)\nWe know, however, that A\u2212A\u2217 is a RIP matrix (Vu, 2014) and thus \u00b53 \u2248 \u221a N . This is with respect to the full matrix, i.e.\nW [ p z z q ] WT being A. This has \u00b52 as O(N), making \u00b53 and thus \u00b53v3 only 1\u221aN relative to the other terms, and thus ignorable in the final embedding."
        },
        {
            "heading": "16.7 The case of DeepWalk",
            "text": "We sketch here the proof extension of the LINE case to DeepWalk. In DeepWalk, the matrix D\u22121A is replaced with the average over the first T powers, that is :\n1\nT T\u2211 r=1 (D\u22121A)r\nFirst note a few things. A is the adjacency matrix which we know to be of form (in expectation) as WBWT where B is 2\u00d7 2. A2 is WBWTWBWT . But, WTW is a scaled identity matrix. Thus A2 in expectation is WB2WT (times factors purely in N ). Ergo, the analysis can still be carried out in terms of B, only this time using B2. Next, note that replacing A with any sum of powers of K does not change the eigenvectors, it only changes the eigenvalues, because :\nM = USUT \u2192M +M2 . . .MK = U(S + S2 \u00b7 \u00b7 \u00b7+ SK)UT\nThe final right multiplication with D\u22121 does not affect this conclusion, since right multiplication with any diagonal matrix simply changes eigenvectors by inverse of the said matrix. Since the eigenvectors remain the same, all the steps crop onward to filtering continue to function, but the scaling factors might change due to eigenvalue changes. Since the eigenvalue change does not change finitely large quantities to infinitely small quantities and we only use this step to rule out noise from the third eigenvector, which after normalization contributes a term of order O( 1\u221a\nN ) relative to the other terms, all the steps\ncontinue to work.\nCaveats and extensions. In the following subsections, we check some alternate scenarios of other ways to do NETMF, \u2265 3 components, and most importantly how random walk augmentations and ego networks of distances \u2265 2 may shift our analysis. These subsections should be considered as extensions of the main proof and mostly expository."
        },
        {
            "heading": "16.8 NETMF rounding vs no rounding.",
            "text": "In the NETMF implementation, terms< 1 before taking the log can be rounded up to 1 in an alternate usage. This case either results in no off diagonal entries after the log (making the analysis trivial as it is the adjacency matrix of two disconnected blocks) or a zero matrix making it nonsensical. Thus, we did not analyze this case, as in this case our augmentations either trivially work or no sensible embedding is produced at all due to a matrix of all zeroes."
        },
        {
            "heading": "16.9 Extension to 3 and greater components",
            "text": "In the cases of \u2265 3 component SBMs, the eigenvectors are significantly more complicated than for a 2\u00d7 2 case. However, the consistency of spectral clustering under the L2 norm - which is, as one might recognize, what we have shown here with some modifications - is proven in alternate ways, and convergence is guaranteed so long as the number of eigenvectors used is k and equal to the number of components. However, these proofs carry over to the case where the eigenvalues after 3 satisfy magnitude constraints (Rohe et al., 2011; Sarkar and Bickel, 2015; Von Luxburg, 2007b), and also for product graphs such as the grid graph (shown in main text). Therefore, using eigenvectors upto \u03bb3 would suffice in these low rank cases even if the overall number of components was high."
        },
        {
            "heading": "16.10 Notes on the random walk augmentation in the SBM scenario",
            "text": "The random walk step in GCC (Qiu et al., 2020) is essential for the purposes of scaling Graph Contrastive methods. This is because in most cases, the ego-networks obtained by taking the simple neighbours within d steps of a node v are too large (over 1000) whereas a random walk on these ego networks, and then collecting the nodes visited, yields much smaller graphs (\u2264 256 for our implementation, with averages much lower, < 100). This naturally leads us to ask if this step is only required for scalability - does it also have other desirable properties ?\nIn the stochastic block model, at least, it does. Consider the adjacency matrix of a 2-component SBM as A, with N nodes each of two classes. Let a node be v, and keep inter-connection probabilities as p, q, z.\nFor any p, q, z which are not arbitrarily low, i.e. do not\u2192 0 as N \u2192 1, it can be seen that any k-ego network of v for k \u2265 2 covers a fraction of nodes\u2192 1 of the entire SBM. This can be understood by considering any node v\u2032 6= v. There will be no path v\u2032 \u2192 z \u2192 v (and in the other direction - we are dealing with undirected graphs) with z 6= v, v\u2032 iff \u2200z there is either no edge (z, v) or no edge (z, v\u2032).\nThe probability of such a path existing for a particular z is \u2265 pmin = min{p2, q2, z2, pz, qz, pq}. Therefore, any z will not have such a path with a probability \u2264 1\u2212 pmin. And, since there are O(N) independent choices of z, such a path will not\nexist between v, v\u2032 through such a z with a probability \u220fO(N) i=1 (1\u2212 pmin). Thus, with probability\u2192 1 as N \u2192\u221e, all such v, v\u2032 have a path between them of length 2. This implies all but 1-ego networks are unsuitable as they will include such paths and cover almost the entire graph.\nThe probability of a random walk, on the other hand, should be analyzed as follows. While it is tempting to consider the transition matrix (the adjacency matrix normalized by degree) for analysis and take its limit (as it enjoys well-understood properties), the random walks utilized in practice have a very high return probability to the origin (0.8). This implies, in turn, that we should only consider the random walk lengths of low step number, as the probability of visiting a node even at distance 3 is 0.2\u00d7 0.2 = 0.04. Over a typical random walk of transition length 256, only 10 nodes at distance \u2265 3 occur.\nWith this in mind, consider a random walk starting WLOG from a node of class 0. Now :\n\u2022 At walk length 1, the random walk has as neighbours, on expectation, p(N \u2212 1) nodes of class 0 and zN nodes of class 1.\n\u2022 At the beginning of walk step 2, there is a roughly \u2248 pp+z probability the random walk is at label 0, and z p+z that it is\nat label 1. The corresponding probabilities at the end are : p 2 (p+z)2 + z2 (z+q)(p+z) for class 0 and zq (z+q)(p+z) + pz (p+z)2 for class 1.\nCompared to blindly taking the 2-ego network, this can be seen to notably bias the the network in favor of the first class, by a ratio equal to :\np2(z + q) + z2(p+ z) zq(p+ z) + pz(z + q) = p2z + p2q + z2p+ z3 pqz + z2q + pz2 + pqz\nTo see the numerator is greater, observe that p2z + z3 \u2265 2pz2 \u2265 pz2 + qz2 (AM-GM). This leaves us with proving that p2q + z2p \u2265 2pqz. However, p2q + z2p \u2265 pq2 + z2p \u2265 2pqz (AM-GM). Therefore, unlike the 2-ego network case which virtually has the same number of nodes of either class with high probability, the random walk slants the node label ratio, as desired, in the 2-nd step (it trivially does so in the first step simply due to the 1-ego network case and this case has no interesting differences between the random walk and directly taking the ego network).\nErgo, the random walk may help offset nonsense nodes included in the ego-network, at least in the block model setting. The fact it is run with a high return probability aids this - were it allowed to run longer, it would approach its mixing time and be closer to uniform in its probability of being found over nodes of either class."
        },
        {
            "heading": "16.11 Larger ego networks",
            "text": "In this section, we have implicitly considered 1-ego networks when taking the full ego network into consideration. It is clear from our analysis in the random walk section that 2-ego networks or higher can only become feasible as at least one of p, q, z go to zero as N goes to infinity. Clearly, we cannot have z remain finitely large while p, q go to zero (as this violates our assumptions) and so either :\n\u2022 all of p, q, z go to zero\n\u2022 p, q stay finite, and z goes to zero.\nCase 1 is much harder to tackle. For instance, our argument re : the Frobenius deviation in the adjacency matrix assumes that the expected adjacency matrix has a Frobenius norm of order O(N2). This may not be true when p, q, z are allowed to be infinitely small.\nInstead, let p, q remain finite and z \u2192 0 as N \u2192\u221e. Assume that the graph remains connected with high probability. This is true when, for instance, z = 1/N . The number of edges on expectation cross cluster is still O(N). Observe that the crop proof we have used continues to work assuming no numerical problems. This is because the eigenvalues use p+ zc, and c inversely varies as z. There are no infinities incurred as a result of z except for c itself. The value of c\u2192\u221e, implying that post-normalization of eigenvectors, the cluster of label 0 receives embeddings which approach [0, 0], and the cluster of label 1 receives embeddings which approach [1,\u22121]. The result also thus carries over to the similar embedding proof, which re-uses the crop result. In this particular case, 2 and higher ego networks are viable, and depend on the value of z. For\nexample, if z = O(1/N), each node v in the cluster of label 0 has O(1) neighbours in cluster 2, and qN \u2217O(1) neighbours at distance 2 of label 1, allowing 2-ego networks (3-ego networks fail in this case). We did not analyze such scenarios in depth, but we put the kmax in our theorem to allow such cases."
        },
        {
            "heading": "17 Time complexity analysis",
            "text": "First, we present the ego graph sizes as a function of average degree of the graphs and also as functions of the overall graph size. Then, we try to plot the time taken to process each graph instance as a function of these parameters."
        },
        {
            "heading": "18 Negative transfer effects",
            "text": "In some of our datasets, there is a noted negative transfer effect. What we mean by this is that further training actually decreases the performance on the dataset. One should be wary of this when looking at result pairs where, for instance, the pre-trained model performs worse than a non pre-trained model or a fresh model.\nWe repeat that the goal of pre-training is to come up with a general model that is trying to excel at all tasks, simultaneously - it is optimizing an average over all tasks. Optimizing such an average may come at the cost of a particular task. We illustrate this effect with IMDB-BINARY. We show 3 consecutive results on this dataset, E2E Frozen, at 5, 10, 20 epochs of training on DBLP. The results actually progressively worsen."
        },
        {
            "heading": "19 Visualizations of need for alignment",
            "text": "Here, we provide two illustrative figures 9, 10 we make that respectively demonstrate:\n\u2022 The case where the global graph, after a random walk, can yield two views, which after Laplacian eigendecomposition end up with inconsistent embeddings for the same node, and thus requires alignment.\n\u2022 The Wasserstein-Procrustes alignment process which is used as a subprocess to correct the inconsistent embeddings. Representative papers that explain the Wasserstein Procrustes method include CONE-ALIGN (Chen et al., 2020c)especially in figures 1 and 2 and section 4.2 of the main text of the CONE-ALIGN paper, as well as REGAL (Heimann et al., 2018) and G-CREWE (Qin et al., 2020)."
        }
    ],
    "title": "Spectral Augmentations for Graph Contrastive Learning",
    "year": 2023
}