{
    "abstractText": "Story generation aims to generate a long narrative conditioned on a given input. In spite of the success of prior works with the application of pre-trained models, current neural models for Chinese stories still struggle to generate high-quality long text narratives. We hypothesise that this stems from ambiguity in syntactically parsing the Chinese language, which does not have explicit delimiters for word segmentation. Consequently, neural models suffer from the inefficient capturing of features in Chinese narratives. In this paper, we present a new generation framework that enhances the feature capturing mechanism by informing the generation model of dependencies between words and additionally augmenting the semantic representation learning through synonym denoising training. We conduct a range of experiments, and the results demonstrate that our framework outperforms the state-ofthe-art Chinese generation models on all evaluation metrics, demonstrating the benefits of enhanced dependency and semantic representation learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Henglin Huang"
        },
        {
            "affiliations": [],
            "name": "Chen Tang"
        },
        {
            "affiliations": [],
            "name": "Tyler Loakman"
        },
        {
            "affiliations": [],
            "name": "Frank Guerin"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        }
    ],
    "id": "SP:d6e5a0db00fce9758e23c1713910a8b457ee103e",
    "references": [
        {
            "authors": [
                "Faeze Brahman",
                "Snigdha Chaturvedi."
            ],
            "title": "Modeling protagonist emotions for emotion-aware storytelling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5277\u20135294, Online. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Guanyi Chen",
                "Kees van Deemter",
                "Chenghua Lin."
            ],
            "title": "SimpleNLG-ZH: a linguistic realisation engine for Mandarin",
            "venue": "Proceedings of the 11th International Conference on Natural Language Generation, pages 57\u201366.",
            "year": 2018
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Christopher D Manning."
            ],
            "title": "The stanford typed dependencies representation",
            "venue": "Coling 2008: proceedings of the workshop on cross-framework and cross-domain parser evaluation, pages 1\u20138.",
            "year": 2008
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898.",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "International Conference on Machine Learning, pages 1243\u20131252. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Tuhin Chakrabarty",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Content planning for neural story generation with aristotelian rescoring",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Zhuoer Feng",
                "Yamei Chen",
                "Ruilin He",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Minlie Huang."
            ],
            "title": "LOT: A story-centric benchmark for evaluating Chinese long text understanding and generation",
            "venue": "Transactions of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Han He",
                "Jinho D. Choi."
            ],
            "title": "The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5555\u20135577, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "CoRR, abs/1510.03055.",
            "year": 2015
        },
        {
            "authors": [
                "Yucheng Li",
                "Chenghua Lin",
                "Frank Guerin"
            ],
            "title": "Cm-gen: A neural framework for chinese metaphor generation with explicit context modelling",
            "venue": "In Proceedings of the 29th International Conference on Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Asli Celikyilmaz",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "PlotMachines: Outlineconditioned generation with dynamic plot state tracking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Asli Celikyilmaz",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Plotmachines: Outlineconditioned generation with dynamic plot state tracking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Jianlin Su."
            ],
            "title": "Simbert: integrating retrieval and generation into bert",
            "venue": "Technical report, Technical report.",
            "year": 2020
        },
        {
            "authors": [
                "Chen Tang",
                "Frank Guerin",
                "Yucheng Li",
                "Chenghua Lin."
            ],
            "title": "Recent advances in neural text generation: A task-agnostic survey",
            "venue": "arXiv preprint arXiv:2203.03047.",
            "year": 2022
        },
        {
            "authors": [
                "Yuanhe Tian",
                "Yan Song",
                "Fei Xia",
                "Tong Zhang",
                "Yonggang Wang."
            ],
            "title": "Improving Chinese word segmentation with wordhood memory networks",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8274\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Peng Xu",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Raul Puri",
                "Pascale Fung",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "MEGATRON-CNTRL: Controllable story generation with external knowledge using large-scale language models",
            "venue": "Proceedings of",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Planand-write: Towards better automatic storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378\u20137385.",
            "year": 2019
        },
        {
            "authors": [
                "Chengkun Zeng",
                "Guanyi Chen",
                "Chenghua Lin",
                "Ruizhe Li",
                "Zhi Chen."
            ],
            "title": "Affective decoding for empathetic response generation",
            "venue": "Proceedings of the 14th International Conference on Natural Language Generation, pages 331\u2013340.",
            "year": 2021
        },
        {
            "authors": [
                "Zhe Zhao",
                "Hui Chen",
                "Jinbin Zhang",
                "Xin Zhao",
                "Tao Liu",
                "Wei Lu",
                "Xi Chen",
                "Haotang Deng",
                "Qi Ju",
                "Xiaoyong Du."
            ],
            "title": "Uer: An open-source toolkit for pretraining models",
            "venue": "EMNLP-IJCNLP 2019, page 241.",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Zhao",
                "Hui Chen",
                "Jinbin Zhang",
                "Xin Zhao",
                "Tao Liu",
                "Wei Lu",
                "Xi Chen",
                "Haotang Deng",
                "Qi Ju",
                "Xiaoyong Du."
            ],
            "title": "UER: An open-source toolkit for pre-training models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Story generation aims to generate a long narrative conditioned on a given input. In spite of the success of prior works with the application of pre-trained models, current neural models for Chinese stories still struggle to generate high-quality long text narratives. We hypothesise that this stems from ambiguity in syntactically parsing the Chinese language, which does not have explicit delimiters for word segmentation. Consequently, neural models suffer from the inefficient capturing of features in Chinese narratives. In this paper, we present a new generation framework that enhances the feature capturing mechanism by informing the generation model of dependencies between words and additionally augmenting the semantic representation learning through synonym denoising training. We conduct a range of experiments, and the results demonstrate that our framework outperforms the state-ofthe-art Chinese generation models on all evaluation metrics, demonstrating the benefits of enhanced dependency and semantic representation learning."
        },
        {
            "heading": "1 Introduction",
            "text": "Story generation presents a challenging task, primarily due to the difficulty that end-to-end neural models experience in maintaining logical coherence during long text generation (Tang et al., 2022). These challenges are even more prominent for the task of story generation in Chinese, due to increased levels of ambiguity stemming from the absence of explicit delimiters for character separation (Tian et al., 2020). Recent works, on the other hand, have aimed to improve long text generation through the proposal of more efficient frameworks (Rashkin et al., 2020a; Goldfarb-Tarrant et al., 2020; Brahman and Chaturvedi, 2020), or through augmenting\n*Equal contribution. \u2020Corresponding author.\nexisting frameworks with pre-training and the injection of extra knowledge (Xu et al., 2020; Guan et al., 2020, 2022).\nHowever, we observe that current generation models still struggle to generate fluent and coherent Chinese stories, which may be the result of the inefficient capturing of features in written Chinese. For example, Chinese characters have a range of morphological parsing strategies, e.g., \u201c \u5c0f\u5fc3\u5730 \u6ed1\u201d can be understood as \u201c\u5c0f\u5fc3 \u5730\u6ed1\u201d (caution wet floor) or \u201c\u5c0f\u5fc3\u5730\u6ed1\u201d (carefully slide), whose meaning is highly dependent on context (Chen et al., 2018; Li et al., 2022). This may cause important sentential roles such as subjects, predicates, and objects to be difficult to identify and process by neural models. Additionally, when neural networks learn the semantics of an utterance, synonymous expressions may lead to confusion, damaging the robustness of the generation model, e.g., \u201c\u6e38\u5386\u201d, \u201c\u5468\u6e38\u201d, and \u201c\u6e38\u89c8\u201d are different Chinese words but all express \"travelling\" in Chinese. We therefore propose to train neural networks to learn the semantic-level features contained in context, rather than the low-level features of characters.\nTo this end, we propose a novel data augmented story generation framework illustrated in Figure 1,\nar X\niv :2\n21 0.\n10 61\n8v 1\n[ cs\n.C L\n] 1\n9 O\nct 2\n02 2\nincluding a LongLM (Guan et al., 2022) based conditional generator, a dependency tagger, and a semantic denoising module. The generator, LongLM (Guan et al., 2022), is a SOTA pre-trained model that has been demonstrated to be effective at multiple Chinese NLG tasks. The dependency tagger, powered by HanLP1 (He and Choi, 2021), recognises the root of a sentence, usually the verb, as well as related subjects and objects via dependency parsing, all of which are essential in expressing the event represented within a sentence. The semantic denoising module, based on SimBert2 (Su, 2020), generates a range of different, yet essentially synonymous sentences, to force the neural network learn the semantic representations of key entities and different expressions. Overall, our proposed framework enhances the ability for language understanding in written Chinese via training to capture the dependencies and semantics contained within sentences, in order to then generate stories.\nWe conduct a range of experiments on the latest public benchmark for Chinese story generation (Guan et al., 2022), and the results indicate that the model trained with our framework substantially outperforms the state-of-the-art (SOTA) baselines on all metrics.3 This indicates that our framework improves the generated stories via enhanced capturing of syntactic dependencies and semantic features."
        },
        {
            "heading": "2 Methodology",
            "text": "We formulate our story generation task based on the OutGen task from LOT (Guan et al., 2022), a Chinese story generation benchmark. The definition of the task is: An outline X , which contains an unordered list of an arbitrary number of Chinese phrases concerning characters and events, is given as the input. The model is required to generate a coherent story Y = {y1, y2, ..., yn} where yi denotes the i-th token (Chinese character) in the story."
        },
        {
            "heading": "2.1 Dependency Tagging",
            "text": "We employ HanLP (He and Choi, 2021) to parse dependencies within Chinese stories. Unlike in English, the basic unit of Chinese dependency parsing is the word segment denoted as Seg = {token1, ..., tokenm}, which contains m tokens.\n1https://github.com/hankcs/HanLP 2https://github.com/ZhuiyiTechnology/\nsimbert 3Our code for reproduction is available at https://github.com/hehedaozuiteng/ Chinese-Story-Generation.\nTherefore, a story can be represented as Y = {Seg1, ...}. For each story, we firstly identify the set of dependencies T = {Segh,Dtag ,Seg t}, and then select target labels Ttarget to insert into the original stories. These target labels are nsubj (representing subjects), root (usually representing verbs), dobj (representing direct objects), and pobj (representing indirect objects following prepositions) (De Marneffe and Manning, 2008). The process is depicted as below:\nTtarget = Dtag\u2208{nsubj ,root ,dobj ,pobj} (1)\nTagger(Seg i) = { Seg i,Dtag Dtag \u2208 Ttarget Seg i otherwise\n(2)\nY D = Tagger(Y,Ttarget) (3)\nwhere Y D is a story with target dependency labels. For instance, the input \u201c\u4ed6\u4eec \u6e38\u5386 \u4e86 \u6240\u6709 \u7684 \u56fd\u5bb6\u201d (\"They visited all the countries\") will be tagged, and the output would be \u201c\u4ed6\u4eec<nsubj>\u6e38 \u5386<root>\u4e86\u6240\u6709\u7684\u56fd\u5bb6<dobj>\u201d (They<nsubj> visited<root> all the countries<dobj>)."
        },
        {
            "heading": "2.2 Semantic Denoising",
            "text": "To help neural networks understand the semantics of Chinese segments implicitly contained in sentences, we employ SimBERT (Su, 2020), which inputs a sentence, and outputs a similar sentence with the same meaning in order to generate a training corpus with large number of synonymous sentences. We therefore aim to train neural networks to resist the semantic noise introduced by different Chinese expressions. For instance, the compound words \"\u53bb\u8fc7\" and \"\u53bb\u4e86\" both represent the meaning \"went\" in Chinese, in which \u201c\u53bb\u201d (go), with different auxiliary characters, may have the same meaning. As this phenomenon is ubiquitous in Chinese, we force the neural networks to denoise the changes in surface forms in order to better understand the semantics of these segments. Consequently, we obtain an augmented data corpus for semantic denoising:\n{...,Seg \u2032i, ...} = SimBERT ({...,Seg i, ...}) (4) {Y S1 , ..., Y S6 }\ufe38 \ufe37\ufe37 \ufe38\n6\n= SimBERT (Y ) (5)\nwhere Seg \u2032i is a synonym of Seg i. Y S is a story that is different from Y but has the same input X . We generate 6 similar stories for each X , and train our neural generator on the enlarged corpus."
        },
        {
            "heading": "2.3 Neural Generator",
            "text": "We employ LongLM (Guan et al., 2022), a Chinese long text pre-trained language model, as the base generator of our framework. It consists of Transformer-based neural blocks (Vaswani et al., 2017; Zeng et al., 2021) with an encoder-decoder architecture to generate narratives. The training process is as follows:\nF = Encoder(X) (6)\nTagger({Y, Y S1 , ...}) predict\u21d0= Decoder(F ) (7)\nwhere the maximum sequence length is set to 512 for both the Encoder and Decoder . LongLM is then fine-tuned with standard cross-entropy loss."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "Dataset We conduct our experiments on the OutGen task of LOT (Guan et al., 2022), a Chinese story benchmark which consists of 2427 highquality filtered Chinese stories. Each input outline contains a sequence of 8 unordered phrases (i.e., their order does not necessarily reflect the order in which they would be present within a narrative). We follow the data split from the benchmark of 60/10/30 for training/validation/testing, respectively. The statistics are shown in Table 2."
        },
        {
            "heading": "3.2 Baselines",
            "text": "We compare our generation framework with a selection of competitive baselines, including the non-pretrained models ConvS2S (Gehring et al., 2017) and Fusion (Fan et al., 2018); pretrained GPT2 models including GPT2base (Zhao et al., 2019a) and GPT2\u2020base (the latter of which\nis pretrained on the benchmark corpus) (Guan et al., 2022)); PlotMachines (PM) (Rashkin et al., 2020b); Plan&Write (PW) (Yao et al., 2019); and mT5 (based on google/mt5-base) (Xue et al., 2021). Specifically, the pre-trained models of baselines are implemented and restored from the prior works on the Chinese language. GPT2 based models are based on uer/gpt2- chinese-cluecorpussmall (Zhao et al., 2019b)."
        },
        {
            "heading": "3.3 Implementation Details",
            "text": "We restore the publicly available checkpoint4 from Huggingface, and fine-tune LongLMbase within our framework. LongLM has 12 attention heads and 12 hidden layers in each encoder and decoder, leading to a total of 223M parameters. We set the maximum sequence length to 512, the batch size to 3, and use a linear schedule to set the warm up step to 100 and the learning rate to 0.0001 for the Adam optimiser. All models are fine-tuned on 2 Nvidia RTX A5000 GPUs.\n4https://huggingface.co/thu-coai/ LongLM-base"
        },
        {
            "heading": "3.4 Evaluation Metrics",
            "text": "Following the LOT benchmark (Guan et al., 2022), we perform automatic evaluation on the metrics of BLEU-n (B-n) (Papineni et al., 2002), Distinctn (D-n) (Li et al., 2015), Coverage (cover), and Order (order). The BLEU-n score measures the quality of generated text by comparing the degree of n-gram overlap with the ground-truth texts; the Distinct score measures the n-gram diversity of the generated text; Coverage is the same as ROUGE-L (Lin, 2004), which measures the recall rate between generated text and input outline phrases; and Order measures the difference between the positional orders of the input phrases in the generated text and the ground-truth text (which is calculated by dividing the number of positional order inversions in the generated story by the number of position pairs between any two phrases) (Guan et al., 2022). We compute the overall aggregate score with the metric weighting scheme presented in LOT."
        },
        {
            "heading": "3.5 Experimental Result",
            "text": "Comparison with Baselines As shown in Table 1, our proposed model substantially outperforms all competitive baselines by a considerable margin. We implement LongLMbase (223M hyperparameters) as our conditional generator. However, the results indicate our model can also significantly outperform LongLMlarge (1B hyper-parameters), on all metrics. Compared to the SOTA model (LongLMlarge), our proposed model achieves up to a 10% improvement on several metrics for both the validation and test sets, and around 5% for the overall score. Additionally, when compared to LongLMbase , our model demonstrates a performance uplift of around 10% on the overall score.\nThe performance improvements seen on BLEUn and Coverage indicate that our generated stories have a higher degree of overlap with the reference stories. Considering the input outline is unordered, this indicates that via the awareness of dependencies and semantics, our proposed model\ncan better leverage syntactic features, and generate more fluent narratives as a result. The scores on Order (computed by the order of outlines in the generated stories compared to the reference), further demonstrate the improvement on language discourse. Meanwhile, the diversity of stories is also substantially raised, for which we argue that semantic denoising contributes significantly.\nConsidering the results as a whole, the significant improvements of our model over existing baslines demonstrates that the enhanced capturing of dependencies and semantics contribute to the language understanding task. This is particularly apparent for Chinese, where expressions are more ambiguous due to the lack of explicit delimiters. Using this increased level of understanding, conditional generators can therefore generate more fluent and diverse stories.\nAblation Study We conduct ablation experiments presented in Table 3 to analyse the individual contributions of each module. We observe that the enhanced feature capturing of both the dependencies and semantics substantially improves on the original neural generator, and combining both approaches further improves performance. This indicates that these two features largely perform different functions that contribute to language generation. Whilst our proposed model outperforms all ablated models when considering most metrics, performance of a single module on some metrics is still close to or even slightly better than the combined model (e.g., on coverage). This phenomenon implies that the two proposed modules may have a small degree of shared function when exploiting features from text. In addition, insufficient training may also lead to the inadequacy of incorporating both features for decoding. We leave further study of incorporating both features to future work.\nCase Study Several generated Chinese stories are presented in subsection A.1 to further demonstrate the effectiveness of our framework."
        },
        {
            "heading": "4 Conclusion",
            "text": "We propose a novel story generation framework for Chinese, which includes a dependency tagging module, a semantic denoising module, and a neural conditional generator. We aim to improve the generation of Chinese through more effectively incorporating the features of dependencies and semantics. The performance improvements shown in our experiments and ablation study demonstrate that these features significantly benefit the task of Chinese story generation."
        },
        {
            "heading": "Acknowledgements",
            "text": "Chen Tang is supported by the China Scholarship Council (CSC) for his doctoral study (File No.202006120039). Tyler Loakman is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. We also gratefully acknowledge the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Case Study In Table 4 we present an example for the basis of a case study. Table 5 presents the generated stories from the neural generation models, including the SOTA baseline LongLMbase , our proposed framework, and its ablated models.\nFirstly, with large-scale pre-training on narrative corpora, the generated stories have relatively less repetition and diversity problems than traditional text generation methods. The main issues are now located in linguistic aspects such as fluency, coherence, and relevance. It can be observed that the generated story from the SOTA baseline model suffers from the ambiguity of the Chinese language, which leads to grammatical and semantic errors. For instance, the sentence \u201c\u4ece\u524d\uff0c\u6709 \u4e2a\u6311\u6c34\u592b\uff0c\u4ed6\u628a\u8def\u65c1\u6492\u7684\u534a\u6876\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u7684 \u7834\u6876\u7559\u610f\u8def\u65c1\u201d (Once upon a time, there was a water-carrier who sent half a bucket of water sprinkled by the roadside to the broken bucket at the master\u2019s house keep an eye on the roadside) has grammatical errors. This may result from inadequate understanding of the dependency roles of each part of the sentences, which leads to misusing two verb phrases (\"sent\", \"keep and eye on\"). For the same reason, the linguistic ambiguity makes the model struggle to capture the semantic meaning of each sentence constituent. For example, the sentence \u201c\u7ed3\u679c\uff0c\u8def\u65c1\u5c31\u5b8c\u597d\u65e0\u635f\u4e86\u201d (As a result, the roadside was intact) contains no grammatical errors, but also makes no sense to the story. It can be intuitively supposed that the key words \"the roadside\" and \"intact\" in the given outline are directly composed here by the neural model without understanding their semantics.\nRegarding the ablation study, similar errors can also be observed in each ablated model: (1) \u201c\u4e3b \u4eba\u8ba9\u4ed6\u628a\u7834\u6876\u7559\u610f\u8def\u65c1\u6492\u7684\u6c34\u9001\u53bb\u4e3b\u4eba\u5bb6\u3002\u201d (The master asked him to send the water the broken bucket noticed sprinkled by the roadside to the master\u2019s house.), in the story generated by - w/ Dependencies, also has obvious grammatical errors; (2) - w/ Semantics generates the sentence \u201c\u4e3b\u4eba\u5bf9 \u4ed6\u9053\u6b49\uff0c\u5e76\u628a\u90a3\u6876\u6c34\u9001 \u5230\u4e86\u8def\u65c1\u6492\u4e86\u4e00\u6b21\u53c8 \u4e00\u6b21\u7f51\u3002\u201d (The master apologised to him and sent the bucket of water to the roadside for casting the\nOutline: \"\u7834\u6876\u7559\u610f\u8def\u65c1\", \"\u53ea\u80fd\u5269\u4e0b\u534a\u6876\u6c34\", \"\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\", \"\u6311\u6c34\u592b\u9053\u6b49\", \"\u8def\u65c1\u6492\", \"\u6311\u6c34\u592b\u8bf4\", \"\u8d9f \u6311\u8fd0\", \"\u5b8c\u597d\u65e0\u635f\" \"the broken bucket keeps an eye on the roadside\", \"only half a bucket of water is left\", \"deliver water to the master\u2019s house\", \"the water-bearer apologises\", \"sprinkled by the roadside\", \"the water-bearer said\", \"travel to pick up\", \"intact\"\nReference Story: \u6311\u6c34\u592b\u6709\u4e24\u4e2a\u6c34\u6876\uff0c\u4e00\u4e2a\u6876\u6709\u88c2\u7f1d\uff0c\u53e6\u4e00\u4e2a\u5b8c\u597d\u65e0\u635f\u3002\u6bcf\u8d9f\u6311\u8fd0\u4e4b\u540e\uff0c\u597d\u6876\u603b\u662f\u80fd \u5c06\u6ee1\u6ee1\u4e00\u6876\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u4e2d\uff0c\u4f46\u662f\u7834\u6876\u5374\u53ea\u80fd\u5269\u4e0b\u534a\u6876\u6c34\u3002\u7834\u6876\u975e\u5e38\u7f9e\u6127\u3002\u4e00\u5929\uff0c\u5b83\u5bf9\u6311\u6c34\u592b\u9053\u6b49\u3002\u6311\u6c34 \u592b\u5e76\u6ca1\u6709\u751f\u6c14\uff0c\u4ed6\u8ba9\u7834\u6876\u7559\u610f\u8def\u65c1\u7684\u82b1\u6735\u3002\u4ed6\u4eec\u8d70\u5728\u5c71\u5761\u4e0a\uff0c\u7834\u6876\u770b\u5230\u7f24\u7eb7\u7684\u82b1\u6735\uff0c\u5f00\u6ee1\u5728\u8def\u7684\u4e00\u65c1\u3002\u6311 \u6c34\u592b\u8bf4\uff0c\u53ea\u6709\u7834\u6876\u7684\u90a3\u4e00\u8fb9\u6709\u82b1\uff0c\u597d\u6876\u7684\u90a3\u4e00\u8fb9\u5374\u6ca1\u6709\u3002\u539f\u6765\u6311\u6c34\u592b\u77e5\u9053\u7834\u6876\u7684\u7f3a\u9677\uff0c\u56e0\u6b64\u5584\u52a0\u5229\u7528\uff0c\u5728 \u7834\u6876\u90a3\u8fb9\u7684\u8def\u65c1\u6492\u4e86\u82b1\u79cd\uff0c\u6bcf\u56de\u4ece\u6eaa\u8fb9\u8fc7\u6765\uff0c\u7834\u6876\u5c31\u66ff\u5b83\u4e00\u8def\u6d47\u4e86\u82b1\u3002\u5982\u679c\u4e0d\u662f\u56e0\u4e3a\u7834\u6876\uff0c\u4e3b\u4eba\u7684\u684c\u4e0a\u4e5f \u6ca1\u6709\u90a3\u4e48\u597d\u770b\u7684\u82b1\u6735\u4e86\u3002 The water-bearer had two buckets. One bucket is broken and the another is intact. After each pick-up, the good bucket can always deliver a full bucket of water to the master\u2019s house, but the broken bucket only has half a bucket of water left. The broken bucket feels very ashamed. One day, it apologised to the water bearer. The water-bearer was not angry, he told the broken bucket to keep an eye on the flowers by the roadside. As they walked down the hillside, Broken bucket saw colorful flowers that filled the side of the road. The water-bearer said that there were flowers only on the side of the broken bucket, but not on the side of the good bucket. It turned out that the water-bearer knew about the defects of the broken bucket, so he made good use of it. Water-bearer sowed flower seeds on the roadside over the broken bucket. Every time he came from the stream, the broken bucket would water the flowers along the way. If it weren\u2019t for the broken bucket, there would not be such beautiful flowers on the master\u2019s table.\nTable 4: An example of an outline and its reference story.\nnet again and again.), which is free of grammatical errors, but does not make sense semantically.\nFurthermore, when comparing sentences containing the same key words from outlines in different generated stories, it can be observed that our proposed techniques for dependency parsing and semantic denoising indeed improve the quality of generated sentences. For example, for the key word \"\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\" (deliver water to the master\u2019s house), the aforementioned LongLMbase and - w/ Dependencies all generate a sentence with grammatical errors. However, the sentences \u201c\u6311\u6c34 \u4eba\u95ee\u4ed6\u4e3a\u4ec0\u4e48\u4e0d\u628a\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u201d (The waterbearer asked him why he didn\u2019t deliver the water to the master\u2019s house.) by - w/ Semantics and \u201c\u6311\u6c34 \u7684\u4eba\u89c9\u5f97\uff0c\u8fd9\u8d9f\u6311\u8fd0\u771f\u662f\u5b8c\u597d\u65e0\u635f\u3002\u201d (Waterbearer felt that the trip was really intact.) by Ours, are all free of grammatical errors.\nCompared to other generated stories, the story from our proposed model contains less errors, and reads more fluently and coherently. Therefore, with the analysis of these errors, we can confirm that current neural models are likely to suffer from the challenge of parsing dependencies and semantics in Chinese narratives. However, our proposed generative framework can improve this uncertainty\nbrought by language ambiguity, and consequently generates a better story.\nOutline: \"\u7834\u6876\u7559\u610f\u8def\u65c1\", \"\u53ea\u80fd\u5269\u4e0b\u534a\u6876\u6c34\", \"\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\", \"\u6311\u6c34\u592b\u9053\u6b49\", \"\u8def\u65c1\u6492\", \"\u6311\u6c34\u592b\u8bf4\", \"\u8d9f \u6311\u8fd0\", \"\u5b8c\u597d\u65e0\u635f\" \"the broken bucket keeps an eye on the roadside\", \"Only half a bucket of water is left\", \"deliver water to the master\u2019s house\", \"The water-bearer apologises\", \"Sprinkled by the roadside\", \"the water-bearer said\", \"travel to pick up\", \"intact\"\nLongLMbase \u4ece\u524d\uff0c\u6709\u4e2a\u6311\u6c34\u592b\uff0c\u4ed6\u628a\u8def\u65c1\u6492\u7684\u534a\u6876\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u7684\u7834\u6876\u7559\u610f\u8def\u65c1\u3002\u4e3b\u4eba\u5f88\u751f\u6c14\uff0c\u5c31 \u628a\u6c34\u9001\u5230\u8def\u65c1\uff0c\u8fd8\u628a\u6c34\u9001\u4e86\u4e0a\u53bb\u3002\u7ed3\u679c\uff0c\u8def\u65c1\u5c31\u5b8c\u597d\u65e0\u635f\u4e86\u3002\u8fd9\u4e0b\uff0c\u6311\u6c34\u8001\u5927\u4e0d\u9ad8\u5174\u4e86\u3002\u4ed6\u5411\u4e3b\u4eba\u9053\u6b49\uff0c \u8bf4\u81ea\u5df1\u6ca1\u6709\u505a\u597d\u4e8b\u3002\u4e3b\u4eba\u542c\u540e\uff0c\u89c9\u5f97\u8fd9\u4ef6\u4e8b\u662f\u4e2a\u9519\u8bef\uff0c\u53ea\u80fd\u5269\u4e0b\u534a\u6876\u7684\u6c34\u4e86\u3002\u4e8e\u662f\uff0c\u4ed6\u51b3\u5b9a\u628a\u8fd9\u8d9f\u6311\u8fd0\u7684 \u6536\u83b7\u5168\u90e8\u5f52\u5230\u4e3b\u4eba\u7684\u5934\u4e0a\u3002\u4ece\u6b64\uff0c\u4e3b\u4eba\u518d\u4e5f\u4e0d\u6562\u6b3a\u8d1f\u4ed6\u4e86\uff0c\u518d\u4e5f\u6ca1\u6709\u6b3a\u8d1f\u8fc7\u4ed6\u4e86\u3002 Once upon a time, there was a water-bearer who sent half a bucket of water sprinkled by the roadside to the master\u2019s house and the broken bucket kept an eye on the roadside. The master was very angry, so he sent the water to the side of the road, and also sent the water up. As a result, the roadside was intact. This time, the water-bearer boss was not happy. He apologised to his master and said that he had done nothing good. When the master heard this, the master felt that the matter was a mistake and only half a bucket of water could be left. Therefore, he decided to attribute all the harvest of this fortune to the master. From then on, the master never dared to bully him again, never bullied him again.\n- w/ Dependencies \u4ece\u524d\uff0c\u6709\u4e2a\u6311\u6c34\u592b\uff0c\u4ed6\u60f3\u628a\u8def\u65c1\u6492\u7684\u534a\u6876\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u3002\u4e3b\u4eba\u8ba9\u4ed6\u628a\u7834\u6876\u7559\u610f\u8def \u65c1\u6492\u7684\u6c34\u9001\u53bb\u4e3b\u4eba\u5bb6\u3002\u4e8e\u662f\uff0c\u4e3b\u4eba\u5bb6\u628a\u6c34\u5728\u8def\u65c1\u6492\u4e86\u5341\u51e0\u5206\u949f\uff0c\u624d\u628a\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u7684\u9662\u5b50\u91cc\u3002\u8fd9\u5929\uff0c\u4e3b\u4eba \u53c8\u8ba9\u4ed6\u4eec\u4e00\u8d77\u53bb\u6311\u8fd0\u6c34\u56de\u6765\u3002\u4ed6\u4eec\u4e00\u8d9f\u6311\u8fd0\u56de\u6765\uff0c\u90fd\u5b8c\u597d\u65e0\u635f\uff0c\u5c31\u8fd9\u6837\u8fc7\u4e86\u5341\u51e0\u5929\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u53c8\u53bb\u6311\u8fd0 \u56de\u6765\u7684\u90a3\u5341\u51e0\u6876\u6c34\u3002\u8fd9\u4e0b\uff0c\u4ed6\u4eec\u4fe9\u53ef\u4ee5\u518d\u53bb\u8def\u65c1\u91c7\u4e00\u6876\u6c34\u4e86\u3002\u4ece\u6b64\uff0c\u4ed6\u4eec\u4e24\u4eba\u518d\u4e5f\u6ca1\u6709\u8fc7\u534a\u6876\u7684\u6c34\u4e86\uff0c\u518d \u4e5f\u6ca1\u6709\u78b0\u5230\u8fc7\u522b\u7684\u6311\u8fd0\u6765\u91c7\u6c34\u7684\u4e86\u3002 Once upon a time, there was a water-bearer who wanted to deliver half a bucket of water sprinkled by the roadside to the master\u2019s house. The master asked him to send the water the broken bucket noticed sprinkled by the roadside to the master\u2019s house. So, the master sprinkled water on the roadside for more than ten minutes, until the water was delivered to the master\u2019s yard. On this day, the master asked them to carry water together again. They came back from a single trip, and all trips were all intact, and it was like this for more than ten days. Finally, they went to pick up the dozen or so buckets of water that were brought back. Now, the two of them can go to the roadside to get a bucket of water. Since then, the two of them have never had more than half a bucket of water, and have never encountered other people who came to collect water.\n- w/ Semantics \u6709\u4e00\u4e2a\u6311\u6c34\u592b\u56e0\u4e3a\u5b8c\u597d\u65e0\u635f\uff0c\u53ea\u80fd\u5269\u4e0b\u534a\u6876\u6c34\u3002\u6311\u6c34\u4eba\u95ee\u4ed6\u4e3a\u4ec0\u4e48\u4e0d\u628a\u6c34\u9001\u5230\u4e3b\u4eba \u5bb6\uff0c\u4e3b\u4eba\u8bf4\uff0c\u8fd9\u8d9f\u6311\u8fd0\u8fc7\u53bb\uff0c\u53ea\u7528\u4e86\u4e09\u79d2\u949f\uff0c\u5c31\u628a\u6c34\u8fd0\u8fc7\u53bb\u4e86\u3002\u6ca1\u591a\u4e45\uff0c\u6311\u6c34\u7684\u4eba\u5c31\u997f\u6b7b\u4e86\uff01\u4e3b\u4eba\u5bf9\u4ed6\u9053 \u6b49\uff0c\u5e76\u628a\u90a3\u6876\u6c34\u9001\u5230\u4e86\u8def\u65c1\u6492\u4e86\u4e00\u6b21\u53c8\u4e00\u6b21\u7f51\u3002\u6700\u540e\uff0c\u90a3\u4eba\u5f97\u6551\u4e86\uff0c\u4e5f\u7ec8\u4e8e\u548c\u4e3b\u4eba\u5bb6\u7684\u5bb6\u957f\u4eec\u9053\u4e86\u6b49\u3002 There was a water-bearer who had only half a bucket of water left because it was intact. The water-bearer asked him why he didn\u2019t deliver the water to the master\u2019s house. The master said that this trip took only three seconds to deliver the water. Not long after, the water-bearer starved to death! The master apologised to him and sent the bucket of water to the roadside for casting the net again and again. In the end, the man was saved, and he finally apologised to the parents of the master.\nOurs \u4ece\u524d\u6709\u4e2a\u6311\u6c34\u592b\uff0c\u6bcf\u5929\u90fd\u8981\u628a\u6c34\u9001\u5230\u4e3b\u4eba\u5bb6\u3002\u6311\u6c34\u7684\u4eba\u89c9\u5f97\uff0c\u8fd9\u8d9f\u6311\u8fd0\u771f\u662f\u5b8c\u597d\u65e0\u635f\u3002\u4e8e\u662f\uff0c\u4ed6 \u5c31\u53bb\u8def\u65c1\u6361\u4e86\u4e00\u4e2a\u7834\u6876\uff0c\u5e0c\u671b\u80fd\u591f\u7528\u5b83\u628a\u6c34\u8fd0\u8fc7\u53bb\u3002\u7ed3\u679c\u7834\u6876\u91cc\u7684\u6c34\u88ab\u6492\u4e86\u4e2a\u7cbe\u5149\u3002\u4e3b\u4eba\u5bb6\u77e5\u9053\u540e\uff0c\u975e\u5e38 \u751f\u6c14\u3002\u7136\u540e\u5c31\u4e0b\u4ee4\u628a\u90a3\u4e2a\u6311\u6c34\u4eba\u6df9\u6b7b\u5728\u8def\u65c1\u3002 Once upon a time, there was a water-bearer who deliver water to the master\u2019s house every day. Water-bearer felt that the trip was really intact. So he went to the roadside to pick up a broken bucket, hoping to use it to deliver the water. As a result, the water in the broken bucket was sprinkled all over. When the master found out, he was very angry. Then, the master ordered the water-bearer to be drowned by the roadside.\nTable 5: The given outline and corresponding generated stories for the case study."
        }
    ],
    "title": "Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics",
    "year": 2022
}