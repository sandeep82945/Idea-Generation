{
    "abstractText": "Domain adaptive semantic segmentation aims to learn a model with the supervision of source domain data, and produce satisfactory dense predictions on unlabeled target domain. One popular solution to this challenging task is self-training, which selects high-scoring predictions on target samples as pseudo labels for training. However, the produced pseudo labels often contain much noise because the model is biased to source domain as well as majority categories. To address the above issues, we propose to directly explore the intrinsic pixel distributions of target domain data, instead of heavily relying on the source domain. Specifically, we simultaneously cluster pixels and rectify pseudo labels with the obtained cluster assignments. This process is done in an online fashion so that pseudo labels could co-evolve with the segmentation model without extra training rounds. To overcome the class imbalance problem on long-tailed categories, we employ a distribution alignment technique to enforce the marginal class distribution of cluster assignments to be close to that of pseudo labels. The proposed method, namely Class-balanced Pixel-level SelfLabeling (CPSL), improves the segmentation performance on target domain over state-of-the-arts by a large margin, especially on long-tailed categories. The source code is available at https://github.com/lslrh/CPSL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruihuang Li"
        },
        {
            "affiliations": [],
            "name": "Shuai Li"
        },
        {
            "affiliations": [],
            "name": "Chenhang He"
        },
        {
            "affiliations": [],
            "name": "Yabin Zhang"
        },
        {
            "affiliations": [],
            "name": "Xu Jia"
        },
        {
            "affiliations": [],
            "name": "Lei Zhang"
        }
    ],
    "id": "SP:94bcfb1f2e887d778ae8767eb5e4f53cdb107596",
    "references": [
        {
            "authors": [
                "Nikita Araslanov",
                "Stefan Roth"
            ],
            "title": "Self-supervised augmentation consistency for adapting semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yuki Asano",
                "Mandela Patrick",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Labelling unlabelled videos from scratch with multi-modal self-supervision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y M Asano",
                "C Rupprecht",
                "A Vedaldi"
            ],
            "title": "Selflabelling via simultaneous clustering and representation learning",
            "venue": "In ICLR 2020 : Eighth International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Matthijs Douze"
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "In Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lun Chang",
                "Hui-Po Wang",
                "Wen-Hsiao Peng",
                "Wei-Chen Chiu"
            ],
            "title": "All about structure: Adapting structural information across domains for boosting semantic segmentation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Chaoqi Chen",
                "Weiping Xie",
                "Wenbing Huang",
                "Yu Rong",
                "Xinghao Ding",
                "Yue Huang",
                "Tingyang Xu",
                "Junzhou Huang"
            ],
            "title": "Progressive feature alignment for unsupervised domain adaptation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yun-Chun Chen",
                "Yen-Yu Lin",
                "Ming-Hsuan Yang",
                "Jia-Bin Huang"
            ],
            "title": "Crdoco: Pixel-level domain transfer with cross-domain consistency",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Jaehoon Choi",
                "Taekyung Kim",
                "Changick Kim"
            ],
            "title": "Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "In Advances in Neural Information Processing Systems 26,",
            "year": 2013
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Fu",
                "Jing Liu",
                "Haijie Tian",
                "Yong Li",
                "Yongjun Bao",
                "Zhiwei Fang",
                "Hanqing Lu"
            ],
            "title": "Dual attention network for scene segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Judy Hoffman",
                "Eric Tzeng",
                "Taesung Park",
                "Jun-Yan Zhu",
                "Phillip Isola",
                "Kate Saenko",
                "Alexei A. Efros",
                "Trevor Darrell"
            ],
            "title": "Cycada: Cycle-consistent adversarial domain adaptation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Judy Hoffman",
                "Dequan Wang",
                "Fisher Yu",
                "Trevor Darrell"
            ],
            "title": "Fcns in the wild: Pixel-level adversarial and constraint-based adaptation",
            "venue": "arXiv preprint arXiv:1612.02649,",
            "year": 2016
        },
        {
            "authors": [
                "Weixiang Hong",
                "Zhenzhen Wang",
                "Ming Yang",
                "Junsong Yuan"
            ],
            "title": "Conditional generative adversarial network for structured domain adaptation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-andexcitation networks",
            "venue": "In Proceedings of the IEEE con- 9 ference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jiabo Huang",
                "Qi Dong",
                "Shaogang Gong",
                "Xiatian Zhu"
            ],
            "title": "Unsupervised deep learning by neighbourhood discovery",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zilong Huang",
                "Xinggang Wang",
                "Lichao Huang",
                "Chang Huang",
                "Yunchao Wei",
                "Wenyu Liu"
            ],
            "title": "Ccnet: Criss-cross attention for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Myeongjin Kim",
                "Hyeran Byun"
            ],
            "title": "Learning texture invariant representation for domain adaptation of semantic segmentation",
            "venue": "In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Ruihuang Li",
                "Xu Jia",
                "Jianzhong He",
                "Shuaijun Chen",
                "Qinghua Hu"
            ],
            "title": "T-svdnet: Exploring high-order prototypical correlations for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yunsheng Li",
                "Lu Yuan",
                "Nuno Vasconcelos"
            ],
            "title": "Bidirectional learning for domain adaptation of semantic segmentation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Qing Lian",
                "Lixin Duan",
                "Fengmao Lv",
                "Boqing Gong"
            ],
            "title": "Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A nonadversarial approach",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Guosheng Lin",
                "Anton Milan",
                "Chunhua Shen",
                "Ian Reid"
            ],
            "title": "Refinenet: Multi-path refinement networks for high-resolution semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Yawei Luo",
                "Liang Zheng",
                "Tao Guan",
                "Junqing Yu",
                "Yi Yang"
            ],
            "title": "Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Fengmao Lv",
                "Tao Liang",
                "Xiang Chen",
                "Guosheng Lin"
            ],
            "title": "Cross-domain semantic segmentation via domain-invariant interactive relation transfer",
            "venue": "In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Ke Mei",
                "Chuang Zhu",
                "Jiaqi Zou",
                "Shanghang Zhang"
            ],
            "title": "Instance adaptive self-training for unsupervised domain adaptation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Arjun K Manrai"
            ],
            "title": "Pixmatch: Unsupervised domain adaptation via pixelwise consistency training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zak Murez",
                "Soheil Kolouri",
                "David Kriegman",
                "Ravi Ramamoorthi",
                "Kyungnam Kim"
            ],
            "title": "Image to image translation for domain adaptation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Fei Pan",
                "Inkyu Shin",
                "Francois Rameau",
                "Seokju Lee",
                "In So Kweon"
            ],
            "title": "Unsupervised intra-domain adaptation for semantic segmentation through selfsupervision",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Hang Qi",
                "Matthew Brown",
                "David G Lowe"
            ],
            "title": "Lowshot learning with imprinted weights",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Stephan R Richter",
                "Vibhav Vineet",
                "Stefan Roth",
                "Vladlen Koltun"
            ],
            "title": "Playing for data: Ground truth from computer games",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "German Ros",
                "Laura Sellart",
                "Joanna Materzynska",
                "David Vazquez",
                "Antonio M Lopez"
            ],
            "title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard S. Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yi-Hsuan Tsai",
                "Wei-Chih Hung",
                "Samuel Schulter",
                "Kihyuk Sohn",
                "Ming-Hsuan Yang",
                "Manmohan Chandraker"
            ],
            "title": "Learning to adapt structured output space for semantic segmentation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yi-Hsuan Tsai",
                "Kihyuk Sohn",
                "Samuel Schulter",
                "Manmohan Chandraker"
            ],
            "title": "Domain adaptation for structured output via discriminative patch representations",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Tuan-Hung Vu",
                "Himalaya Jain",
                "Maxime Bucher",
                "Matthieu Cord",
                "Patrick Perez"
            ],
            "title": "Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Haoran Wang",
                "Tong Shen",
                "Wei Zhang",
                "Lingyu Duan",
                "Tao Mei"
            ],
            "title": "Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation",
            "venue": "In ECCV",
            "year": 2020
        },
        {
            "authors": [
                "Wenguan Wang",
                "Tianfei Zhou",
                "Fisher Yu",
                "Jifeng Dai",
                "Ender Konukoglu",
                "Luc Van Gool"
            ],
            "title": "Exploring cross-image pixel contrast for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zuxuan Wu",
                "Xintong Han",
                "Yen-Liang Lin",
                "Mustafa G\u00f6khan Uzunbas",
                "Tom Goldstein",
                "Ser Nam Lim",
                "Larry S. Davis"
            ],
            "title": "Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Junyuan Xie",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume",
            "year": 2016
        },
        {
            "authors": [
                "Xueting Yan",
                "Ishan Misra",
                "Abhinav Gupta",
                "Deepti Ghadiyaram",
                "Dhruv Mahajan"
            ],
            "title": "Clusterfit: Improving generalization of visual representations",
            "venue": "In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Jianwei Yang",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Joint unsupervised learning of deep representations and image clusters",
            "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Yanchao Yang",
                "Stefano Soatto"
            ],
            "title": "Fda: Fourier domain adaptation for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Qiming Zhang",
                "Jing Zhang",
                "Wei Liu",
                "Dacheng Tao"
            ],
            "title": "Category anchor-guided unsupervised domain adaptation for semantic segmentation",
            "venue": "arXiv preprint arXiv:1910.13049,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Zhang",
                "Philip David",
                "Boqing Gong"
            ],
            "title": "Curriculum domain adaptation for semantic segmentation of urban scenes",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Chengxu Zhuang",
                "Alex Zhai",
                "Daniel Yamins"
            ],
            "title": "Local aggregation for unsupervised learning of visual embeddings",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Yang Zou",
                "Zhiding Yu",
                "BVK Kumar",
                "Jinsong Wang"
            ],
            "title": "Unsupervised domain adaptation for semantic segmentation via class-balanced self-training",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yang Zou",
                "Zhiding Yu",
                "B.V.K. Vijaya Kumar",
                "Jinsong Wang"
            ],
            "title": "Unsupervised domain adaptation for semantic segmentation via class-balanced selftraining",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yang Zou",
                "Zhiding Yu",
                "Xiaofeng Liu",
                "B.V.K. Vijaya Kumar",
                "Jinsong Wang"
            ],
            "title": "Confidence regularized self-training",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Semantic segmentation is a fundamental computer vision task, which aims to make dense semantic-level predictions on images [8,27,28,43,53]. It is a key step in numerous applications, including autonomous driving, human-machine interaction, and augmented reality, to name a few. In the past few years, the rapid development of deep Convolutional Neural Networks (CNNs) has boosted semantic segmentation significantly in terms of accuracy and efficiency. However, the performance of deep models trained in one\n*Corresponding Author\ndomain often drops largely when they are applied to unseen domains. For example, in autonomous driving the segmentation model is confronted with great challenges when weather conditions are changing constantly [56]. A natural way to improve the generalization ability of segmentation model is to collect data from as many scenarios as possible. However, it is very costly to annotate pixel-wise labels for a large amount of images [11]. More effective and practical approaches are required to address the domain shifts of semantic segmentation.\nUnsupervised Domain Adaptation (UDA) provides an important way to transfer the knowledge learned from one labeled source domain to another unlabeled target domain. For example, we can collect many synthetic data whose dense annotations are easy to get by using game engines such as GTA5 [36] and SYNTHIA [37]. Then the question turns to how to adapt the model trained from a labeled synthetic domain to an unlabeled real image domain. Most previous works of UDA bridge the domain gap by aligning data distributions at the image level [17, 25, 33], feature level [7, 17, 18, 24] or output level [29, 32, 39], through adversarial training or auxiliary style transfer networks. However, these techniques will increase the model complexity and make the training process unstable, which impedes their reproducibility and robustness.\nAnother important approach is self-training [52, 56, 57], which alternatively generates pseudo labels by selecting high-scoring predictions on target domain and provides supervision for the next round of training. Though these methods have produced promising performance, there are still some major limitations. On one hand, the segmentation model tends to be biased to source domain so that the pseudo labels produced on target domain are error-prone; on the other hand, highly-confident predictions may only provide very limited supervision information for the model training. To solve these issues, some methods [50, 51] have been proposed to produce more accurate and informative pseudo labels. For example, instead of using the classifier trained on source domain to generate pseudo labels, Zhang et al. [51] assigned pseudo labels to pixels based on\nar X\niv :2\n20 3.\n09 74\n4v 1\n[ cs\n.C V\n] 1\n8 M\nar 2\n02 2\ntheir distances to the category prototypes. These prototypes, however, were built in source domain and usually deviated much from the target domain. ProDA [50] leveraged the feature distances from prototypes to perform online rectification, but it was challenging to construct prototypes for long-tailed categories, which often led to unsatisfactory performance.\nDifferent from previous self-training methods which use classifier-based noisy pseudo labels for supervision, in this paper we propose to perform online pixel-level self-labeling via clustering on target domain, and use the resulting soft cluster assignments to correct pseudo labels. Our idea comes from the fact that pixel-wise cluster assignments could reveal the intrinsic distributions of pixels in target domain, and provide useful supervision for model training. Compared to conventional label generation methods that are often biased towards source domain, cluster assignment in target domain is more reliable as it explores inherent data distribution. Considering that the classes of segmentation dataset are highly imbalanced (please refer to Fig. 2), we employ a distribution alignment technique to enforce the class distribution of cluster assignments to be close to that of pseudo labels, which is more favorable to class-imbalanced dense prediction tasks. The proposed Class-balanced Pixellevel Self-Labeling (CPSL) module works in a plug-andplay fashion, which could be seamlessly incorporated into existing self-training framework for UDA. The major contributions of this work are summarized as follows:\n\u2022 A pixel-level self-labeling module is developed for domain adaptive semantic segmentation. We cluster pixels in an online fashion and simultaneously rectify pseudo labels based on the resulting cluster assignments.\n\u2022 A distribution alignment technique is introduced to align the class distribution of cluster assignments to that of pseudo labels, aiming to improve the performance over long-tailed categories. A class-balanced sampling strategy is adopted to avoid the dominance of majority categories in pseudo label generation.\n\u2022 Extensive experiments demonstrate that the proposed CPSL module improves the segmentation performance on target domain over state-of-the-arts by a large margin. It especially shows outstanding results on long-tailed classes such as \u201cmotorbike\u201d, \u201ctrain\u201d, \u201clight\u201d, etc."
        },
        {
            "heading": "2. Related Work",
            "text": "Semantic Segmentation. The goal of semantic segmentation is to segment an image into regions of different semantic categories. While the Fully Convolutional Networks (FCNs) [28] have greatly boosted the performance of semantic segmentation, they have relatively small receptive field to explore visual context. Many later works focus on\nhow to enlarge the receptive field of FCNs to model longrange context dependencies of images, such as dilated convolution [8], multi-layer feature fusion [27], spatial pyramid pooling [53] and variants of non-local blocks [15, 20, 22]. However, directly applying these models to unseen domains will induce poor segmentation performance because of their weak generalization ability. Therefore, many domain adaptation techniques have been proposed to improve model generalization ability on new domains.\nDomain Adaptation for Semantic Segmentation. Recently, many works have been proposed to bridge the domain gap and improve the adaptation performance. The most representative ones are adversarial training-based methods [19, 23, 34, 39, 40], which aim to align different domains on intermediate features or network predictions. Style transfer-based methods [6, 9, 10, 44, 48] minimize domain gap at the image level. For example, Chang et al. [6] proposed to disentangle an image into domaininvariant structures and domain-specific textures for image translation. The training process of these models is rather complex since multiple networks, such as discriminators or style transfer networks, have to be trained concurrently.\nAnother important technique for UDA is selftraining [24, 26, 32, 51, 55, 57], which iteratively generates pseudo labels on target data for model update. Zou et al. [55] proposed a class-balanced self-training method for domain adaption of semantic segmentation. To reduce the noise in pseudo labels, Zou et al. [57] further proposed a confidence regularized self-training method, which treated pseudo labels as trainable latent variables. Lian et al. [26] constructed a pyramid curriculum for exploring various properties about the target domain. Zhang et al. [51] enforced category-aware feature alignment by choosing the prototypes of source domain as guided anchors. ProDA [50] went further by employing the feature distances from each pixel to prototypes to correct pseudo labels pre-computed by the source model. These methods, however, neglect either the pixel-wise intrinsic structures or inherent class distribution of target domain images, tending to be biased to source domain or majority classes.\nClustering-based Representation Learning. Our work is also related to clustering-based methods [2\u20134,21,45\u201347,54, 58]. Caron et al. [4] iteratively performed k-means on latent representations and used the produced cluster assignments to update network parameters. Recently, Asano et al. [3] cast the cluster assignment problem as an optimal transport problem which can be solved efficiently through a fast variant of the Sinkhorn-Knopp algorithm. SwAV [5] performed clustering while enforcing consistency among the cluster assignments of different augmentations of the same image. In this paper, we extend self-labeling from image-level classification to pixel-level semantic segmentation. In addition, different from Asano et al. [3] and Caron et al. [4], we com-\npute cluster assignments in an online fashion, making our method scalable to dense pixel-wise prediction tasks."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Overall Framework",
            "text": "In the setting of unsupervised domain adaptation for semantic segmentation, we are provided with a set of labeled data in source domain DS = {(Xsn, Y sn )} NS n=1, where X s n is the source image with label Y sn and NS is the number of images, as well as a set of NT unlabeled images Xtn in target domain DT = {Xtn} NT n=1. Both domains share the same C classes. Our goal is to learn a model by using the labeled source data in DS and unlabeled target data in DT , which could perform well on unseen test data in the target domain.\nThe overall framework of our proposed CPSL is shown in Fig. 1. We propose a pixel-level self-labeling module (highlighted in the green color box) to explore the intrinsic pixel-wise distributions of the target domain data via clustering, and to reduce the noise in pseudo labels. Before the training, we first generate a soft pseudo label map PST \u2208 RH\u00d7W\u00d7C for each target domain image by a warmed-up model that is pre-trained on the source domain data. The obtained PST is usually error-prone because of the large domain shift. Therefore, in the training process, we rectify PST incrementally with the soft cluster assignment, denoted by PSL \u2208 RH\u00d7W\u00d7C . Specifically, the rectification of PST is conducted as follows:\nY\u0302 t,(c) n,i = 1, if c = argmaxc\u2217 (P (c\u2217) SL,n,i \u00b7 P (c\u2217) ST,n,i)\n0, otherwise , (1)\nwhere Y\u0302 t,(c)n,i denotes the c-th element of rectified pseudo label at the i-th pixel of target image Xtn. P (c\u2217) SL,n,i represents the probability that the i-th pixel of Xtn belongs to the c\u2217th category. Eq. 1 has a similar formulation to [35, 38, 50], where PSL can be regarded as the weight map to modulate the softmax probability map PST. The cluster assignment PSL exploits the inherent data distribution of target domain, thus it is highly complementary to the classifier-based pseudo label PST which heavily relies on source domain.\nWe define the segmentation loss on target domain, denoted by LtSEG, as the pixel-level cross-entropy loss between the segmentation probability map Pn \u2208 RH\u00d7W\u00d7C and the rectified pseudo label Y\u0302 tn of target image X t n:\nLtSEG = \u2212 NT\u2211 n=1 H\u00d7W\u2211 i=1 C\u2211 c=1 Y\u0302 t,(c) n,i logP (c) n,i . (2)\nIn addition, the loss on source domain, denoted by LsSEG, can be defined as the standard pixel-wise cross-entropy on the labeled images:\nLsSEG = \u2212 NS\u2211 n=1 H\u00d7W\u2211 i=1 C\u2211 c=1 Y s,(c) n,i logP (c) n,i . (3)\nThen the total segmentation loss LSEG is obtained as the sum of them: LSEG = LtSEG + LsSEG.\nIn the following subsections, we will explain in detail the design of our CPSL module."
        },
        {
            "heading": "3.2. Online Pixel-Level Self-Labeling",
            "text": "Pixel-Level Self-Labeling. Conventional self-training based methods usually use a model pre-trained on source domain to produce pseudo labels, which often contain much noise [51,55,57]. To clean the pseudo labels, we propose to perform pixel-level self-labeling via clustering on target domain and use the obtained cluster assignments to rectify the pseudo labels. The basic motivation is that pixel-wise clustering could reveal the intrinsic structures of target domain data, and it is complementary to the classifier trained on source domain data. Thus, cluster assignments could provide extra supervision for training a domain adaptive segmentation model.\nSpecifically, we first extract features from an input image to obtain Z \u2208 RH\u00d7W\u00d7D and normalize it with zi = zi||zi||2 , where zi is the i-th feature vector of Z with length D. Then we randomly sample a group of pixels Z\u0302 = [z1, \u00b7 \u00b7 \u00b7 , zM ] from each image, and pass them through a self-labeling head fSL. Finally, we obtain their class probability vectors P\u0302 = [p1, \u00b7 \u00b7 \u00b7 , pM ] by taking a softmax operation:\np(c)m = exp( 1\u03c4 f (c) SL (zm))\u2211\nc\u2032 exp( 1 \u03c4 f (c\u2032) SL (zm))\n, c \u2208 {1, \u00b7 \u00b7 \u00b7 , C}, (4)\nwhere f (c)SL (zm) is the c-th element of the output of zm from self-labeling head. p(c)m denotes the probability that them-th pixel belongs to the c-th category. \u03c4 is a temperature parameter. Considering there is no ground truth label available for target data, we train the head fSL through a self-labeling mechanism [3] with the following objective function:\nLSL = \u2212 1\nM M\u2211 m=1 C\u2211 c=1 q(c)m log p (c) m s.t.Q \u2208 Q,\nwith Q := {Q \u2208 RC\u00d7M+ |Q1M = r,Q T1C = h}.\n(5)\nThe above formula is an instance of the optimal transport problem [13], where Q = 1M [q1, \u00b7 \u00b7 \u00b7 , qM ] is a transport assignment and it is restricted to be a probability matrix by\nsatisfying the constraint Q. 1C and 1M denote the vectors of ones with dimension C and M , respectively. r and h are the marginal projections of Q onto its rows and columns, respectively.\nBy formulating the cluster assignment problem as an optimal transport problem, the optimization of Eq. 5 with respect to variable Q can be solved efficiently by the iterative Sinkhorn-Knopp algorithm [13]. The optimal solution is obtained by:\nQ\u2217 = diag(\u03b1) exp( fSL(Z\u0302)\n\u03b5 ) diag(\u03b2), (6)\nwhere \u03b1 \u2208 RC and \u03b2 \u2208 RM are two renormalization vectors which can be computed efficiently in linear time even for dense prediction tasks. \u03b5 is a temperature parameter.\nThen by fixing label assignment Q, the self-labeling head fSL is updated by minimizing LSL with respect to P\u0302 , which is the same as training with cross-entropy loss.\nWeight Initialization. We use the soft cluster assignment PSL to rectify the classifier-based pseudo label PST. However, the clustering categories usually mismatch those of the classifier, resulting in performance degradation. To overcome this issue, we initialize the weight of self-labeling head fSL with category prototypes. Specifically, we compute the prototypes [z\u03041, \u00b7 \u00b7 \u00b7 , z\u0304C ] for each category through:\nz\u0304c = 1\n|\u0393c| NT\u2211 n=1 H\u00d7W\u2211 i=1 Y (c) ST,n,i \u00b7 zn,i, (7)\nwhere |\u0393c| denotes the number of pixels belonging to the c-th category in all images. YST is the hard version of PST. Then the self-labeling process can be regarded as assigning pixels to different prototypes. In this way, the clustering categories are able to match classification categories.\nOnline Cluster Assignment. Different from Asano et al. [3], where the assignment Q is computed over the full dataset, we conduct online clustering on data batches during training. Considering that the number of samples in a mini-batch is often too small to cover all categories, and the class distribution varies largely across different batches, we augment the features Z\u0302 with a memory bankM, which is updated on-the-fly, to reduce the randomness of sampling. Specifically, throughout the training process, we maintain a queue of 65,536 pixel features from previous batches in M. In each iteration, we compute the optimal transport assignment on the augmented data Zaug , denoted by Qaug , but only the assignment of current batch, denoted by Qcur, is used to compute the self-labeling loss LSL. In this way, we could alternatively update the self-labeling head fSL and use it to generate more accurate cluster assignment PSL online. Hence, the pseudo labels will be improved incrementally by the resulting cluster assignments, and the noise will be gradually reduced without extra rounds of training."
        },
        {
            "heading": "3.3. Class-Balanced Self-Labeling",
            "text": "As shown in Fig. 2, there exists severe class-imbalance in current semantic segmentation datasets. Some long-tailed classes have very limited pixels (e.g., \u201ctraffic light\u201d, \u201csign\u201d), and some classes only appear in a few images (e.g., \u201cmotorbike\u201d, \u201ctrain\u201d). Such a problem will make it difficult to train a robust segmentation model, especially for those long-tailed classes. In this work, we propose two techniques to address this issue, i.e., class-balanced sampling and distribution alignment.\nClass-Balanced Sampling. We randomly sample pixels from each image, which makes the class distribution of data in memory bankM approach to that of the whole dataset. In order to make sure that the pixels of long-tailed categories can be selected equally, we sample from different categories with the same proportion, i.e., MH\u00d7W , where M is the number of pixels to be sampled in each image. For each input image Xtn, we first compute its class distribution \u03b4n through\n\u03b4(c)n = 1\nH \u00d7W H\u00d7W\u2211 i Y\u0302 t,(c) n,i , (8)\nwhere \u03b4(c)n denotes the proportion of pixels belonging to the c-th category in image Xtn. Then the number of samples Mc for each category c is decided by:\nMc = \u230a M \u00d7 \u03b4(c)n \u230b . (9)\nIf image Xtn does not contain certain classes of pixels, we will randomly sample the rest pixels from other categories to make up M samples.\nDistribution Alignment. As discussed in [3,4], simultaneously optimizing Q and P\u0302 in Eq. 5 may lead to degenerated results that all data points are trivially assigned to a single cluster. To avoid this, Asano et al. [3] constrained that Q should induce an equipartition of the data. However, this constraint is not reasonable and it will degrade the performance if the ground truth class distribution of the data, denoted by \u03b4gt, is not uniform. In the Cityscapes dataset [11], for example, the number of pixels of the largest category (\u201croad\u201d) is approximately 300 times that of the smallest category (\u201cmotorbike\u201d).\nTo overcome this problem, we propose a novel technique, namely distribution alignment, to align the distribution of cluster assignments to ground truth class distribution \u03b4gt, aiming at partitioning pixels into subsets of unequal sizes. However, \u03b4gt is unknown since the true labels of target domain data are unavailable. Thus we propose to employ the moving average of pseudo labels\u2019 class distribution \u03b4pseudo to approximate \u03b4gt. Specifically, we first initialize\n\u03b4pseudo based on the fixed pseudo labels Y tST as follows:\n\u03b4 (c) pseudo|0 =\n1\nNT \u00d7H \u00d7W NT\u2211 n H\u00d7W\u2211 i Y t,(c) ST,n,i. (10)\nOver the course of training, we compute the class distribution \u03b4n of each image through Eq. 8. Then the class distribution \u03b4pseudo after each training iteration k is updated with a momentum \u03b1 \u2208 [0, 1]:\n\u03b4 (c) pseudo|k = \u03b1\u03b4 (c) pseudo|k\u22121 + (1\u2212 \u03b1)\u03b4 (c) n . (11)\nFinally, we enforce the class distribution of cluster assignments, denoted by r in Eq. 5, to be close to \u03b4pseudo:\nr = \u03b4pseudo, h = 1\nM 1M . (12)\nOur empirical results (please refer to Fig. 6) demonstrate that the proposed distribution alignment technique effectively avoids the dominance of majority classes during training. Please refer to Sec. 4.3 for more discussions."
        },
        {
            "heading": "3.4. Loss Function",
            "text": "As shown in Fig. 1, we employ momentum encoder to stabilize the self-labeling process. To further improve the model generalization ability on target domain and alleviate the bias inherited from source domain, following [1, 50], we impose consistency regularization on the segmentation network. Specifically, we generate a weakly-augmented image Xw and a strongly-augmented image Xs from the same input image X , and pass Xw through the momentum segmentation network f \u2032SEG to generate a probability map Pw, which is used to supervise the output Ps of stronglyaugmented image Xs from fSEG. Then we enforce Pw and Ps to be consistent via: LREG = NT\u2211 n=1 H\u00d7W\u2211 i=1 (`KL (Pw,n,i, Ps,n,i) +`KL (Ps,n,i, Pw,n,i)) , (13) where `KL denotes the KL-divergence. Ps,n,i and Pw,n,i represent the i-th pixel of the segmentation probability maps Ps and Pw of image Xn, respectively.\nThe overall loss function is defined as:\nLTOTAL = LSEG + \u03bb1LSL + \u03bb2LREG, (14)\nwhere \u03bb1 and \u03bb2 are trade-off parameters. LSL and LREG are complementary to each other. The former uses pixellevel cluster assignment PSL to rectify the pseudo label PST, which effectively dilutes the bias to source domain, while the latter improves model generalization ability by applying data augmentations on inputs and consistency regularization on outputs."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Settings",
            "text": "Implementation Details. We implement the segmentation model with DeepLabv2 [8] and employ ResNet-101 [16] as the backbone, which is pre-trained on ImageNet. The segmentation model is warmed up by applying adversarial training like [39]. The input images are randomly cropped to 896\u00d7512, and the batch size is set as 4. We employ a series of data augmentations such as RandAugment [12], Cutout [14], CutMix [49], and add photometric noise, including color jitter, random blur, etc. SGDM is used as the optimizer. The initial learning rate of segmentation model and self-labeling head are set to 10\u22124 and 5\u00d7 10\u22124, which decay exponentially with power 0.9. The weight decay and momentum are set to 2 \u00d7 10\u22124 and 0.9, respectively. The trade-off parameters \u03bb1, \u03bb2 and the temperature parameters \u03c4 , \u03b5 are empirically set to 0.1, 5, 0.08, and 0.05, respectively. The length of memory bank is set to 65, 536 and we sample 512 pixels per image for clustering (M = 512), that is, there are 128 images in the memory bank. For the momentum networks, the momentum is set to 0.999. Our model is trained with four Tesla V100 GPUs on PyTorch. Datasets. Following [31, 51, 52], we adopt two synthetic datasets (GTA5 [36], SYNTHIA [37]) and one real dataset (Cityscapes [11]) in the experiments. The GTA5 dataset\ncontains 24,966 images with resolution 1914\u00d71052. The corresponding dense annotations are generated by game engine. The SYNTHIA dataset contains 9,400 images of 1280\u00d7760 pixels and it has 16 common categories with Cityscapes, which contains 2,975 training images and 500 validation images of resolution 2048\u00d71024."
        },
        {
            "heading": "4.2. Comparisons with State-of-the-Arts",
            "text": "We name the proposed method as Class-balanced Pixellevel Self-Labeling (CPSL). Following [50], after the training converges, we also conduct two more knowledge distillation rounds to transfer the knowledge to a student model pre-trained in a self-supervised manner, and the resulting model is called \u201cCPSL+distill\u201d. We compare our models with representative and state-of-the-art methods, which can be categorized to two main groups: adversarial trainingbased methods, including AdaptSeg [39], CyCADA [17], FADA [42], ADVENT [41], and self-training based methods, including CBST [55], IAST [31], CAG UDA [51], ProDA [50], SAC [1]. Following previous works, the results on validation set are reported in terms of category-wise Intersection over Union (IoU) and mean IoU (mIoU). GTA5\u2192Cityscapes. The results on GTA5\u2192Cityscapes task are reported in Tab. 1. Our CPSL achieves the best IoU score on 7 out of 19 categories, and it achieves the highest mIoU score, outperforming the second best method\nProDA [50] by a large margin of 2.0. This can be attributed to the exploration of inherent data distribution of target domain, which provides extra supervision for training. By applying knowledge distillation, there is a further performance gain of 5.1, achieving 60.8 mIoU, which is by far the new state-of-the-art. It is worth mentioning that our method performs especially well on long-tailed categories, such as \u201cpole\u201d, \u201clight\u201d, \u201ctrain\u201d, and \u201cmotor\u201d. For example, ProDA fails on the small class \u201ctrain\u201d due to the difficulties in constructing prototypes for long-tailed categories. By applying distribution alignment, CPSL alleviates the class-imbalance problem, attaining 24.9 IoU on \u201ctrain\u201d without sacrificing the performance on other categories. SYNTHIA\u2192Cityscapes. This adaptation task is more challenging than the previous one because of the large domain gap. The mIoUs over 13 classes (mIoU13) and 16 classes (mIoU16) are reported in Tab. 2. Our model still achieves significant improvements over competing methods on this task. Specifically, CPSL achieves the mIoU of 54.4 and 61.7 over 16 and 13 categories, surpassing the second best method SAC [1] by 1.8 and 2.4, respectively. This owes to the fact that CPSL reduces the label noise and calibrates the bias to source domain. The results are further improved to 57.9 and 65.3 in terms of mIoU after distillation. Among all the 16 categories, our method tops over six of them, especially on the hardest categories, such as \u201clight\u201d, \u201cmotorbike\u201d, \u201cbike\u201d, and so on. Qualitative Results. Fig. 3 shows the qualitative seg-\nmentation results of our method and ProDA [50] on GTA5\u2192Cityscapes task. As can be seen, our method improves the performance on long-tailed classes substantially, e.g. \u201cpole\u201d, \u201clight\u201d, \u201cbus\u201d, thanks to the class-balanced sampling and distribution alignment techniques. ProDA [50] does not perform well on these categories since it does not explicitly enforce class balance in training."
        },
        {
            "heading": "4.3. Discussions",
            "text": "Ablation Study. We conduct ablation studies on the GTA5\u2192Cityscapes task to investigate the role of each component in CPSL. For the convenience of expression, we abbreviate \u2018self-labeling\u2019, \u2018self-training\u2019, \u2018class balance\u2019, \u2018weight initialization\u2019, \u2018data augmentation\u2019, and \u2018momentum encoder\u2019 with \u2018SL\u2019, \u2018ST\u2019, \u2018CB\u2019, \u2018Init\u2019, \u2018Aug\u2019, \u2018Mom\u2019. Tab. 3 shows the corresponding results by switching off each component. We have the following observations.\nFirst, removing the SL component leads to a drop of 7.9 in mIoU, while disabling CB component leads to a drop of 3.9 in mIoU. This demonstrates they play key roles in improving the segmentation performance by exploring the intrinsic data structures of target domain images. Second, training without the pseudo labels produced by ST causes a significant drop of 16.3 in mIoU. This is not surprising because simultaneously updating network parameters and generating pseudo labels will lead to a degenerate solution [50, 51]. Third, randomly initializing the self-labeling head (w/o Init) results in a decline of 5.8 in mIoU, which is attributed to the mismatch between clustering and classifi-\ncation categories. Fourth, Aug and Mom components bring an improvement of 1.7 and 1.1 in mIoU.\nUnequal Partition Constraint. To further analyze the effect of unequal partition on class-imbalanced dataset, we plot the curves of mIoU and MPA scores with different partition constraints in Fig. 4, where a huge gap can be observed in terms of mIoU. However, equal partition slightly outperforms unequal partition in terms of MPA. This is not surprising because many pixels belonging to large categories are assigned to small categories under the equal partition constraint, largely improving pixel accuracy of small classes without influencing much large classes. Thus the MPA score is improved. More details can be found in the supplemental files.\nSelf-Training (ST) vs. Self-Labeling (SL). We explore the complementarity of label assignments produced by ST and SL, and visualize the results in Fig. 5. One can draw a conclusion that the integration of ST and SL in our CPSL leads to better results than any one of them. Specifically, ST performs better on large categories which are easy to transfer, such as \u201csky\u201d and \u201cbuilding\u201d, while SL has advantages on small categories such as \u201clight\u201d and \u201cpole\u201d. Therefore, the\npixels that are wrongly classified in one view will be corrected in another view. The Effect of Distribution Alignment. We compare the class distributions of labels produced by CPSL and conventional self-training (ST). As illustrated in Fig. 6, the results of ST mismatch heavily to ground truth (GT). Its predictions are biased towards majority categories, e.g. \u2018road\u2019 and \u2018building\u2019, ignoring small categories such as \u2018train\u2019, \u2018sign\u2019 and \u2018bike\u2019. CPSL calibrates the bias and produces a class distribution closer to GT. This demonstrates that CPSL can capture the inherent class distribution of target domain and avoids gradual dominance of majority classes. Parameter Sensitivity Analysis. In Tab. 4, we evaluate the segmentation performance on GTA5\u2192Cityscapes task with different number of samples per image. Our method is robust to this parameter within a wide range. More analyses can be found in supplemental materials. Limitation. Although the proposed CPSL alleviates the bias to source domain with the self-labeling assignment, it still relies on the self-training based pseudo labels, which may lead to confirmation bias. We consider to develop a fully clustering-based assignment method in future works."
        },
        {
            "heading": "5. Conclusion",
            "text": "We proposed a plug-and-play module, namely Classbalanced Pixel-level Self-Labeling (CPSL), which could be seamlessly incorporated into self-training pipelines to improve the domain adaptive semantic segmentation performance. Specifically, we conducted pixel-level clustering online and used the resulting cluster assignments to rectify pseudo labels. On one hand, the label noise was reduced and the bias to source domain was calibrated by exploring pixel-level intrinsic structures of target domain images. On the other hand, CPSL captured inherent class distribution of target domain, which effectively avoided gradual dominance of majority classes. Both the qualitative and quantitative analyses demonstrated that CPSL outperformed the existing state-of-the-arts by a large margin. In particular, it achieved great performance gains on long-tailed classes without sacrificing the performance on other categories."
        },
        {
            "heading": "6. Supplemental Materials",
            "text": "In this supplemental file, we provide the following materials:\n\u2022 The training procedure of CPSL;\n\u2022 The definition of mean pixel accuracy (MPA) (referring to Sec4.3-Unequal partition constraint in the main paper);\n\u2022 Ablation studies in terms of per-category IoU (referring to Sec4.3-Ablation study in the main paper);\n\u2022 Comparisons on the training process on the GTA5\u2192Cityscapes task;\n\u2022 More parameter sensitivity analyses (referring to Sec4.3-Parameter sensitivity analysis in the main paper);\n\u2022 More qualitative results (referring to Sec4.2Qualitative results in the main paper)."
        },
        {
            "heading": "6.1. Algorithm",
            "text": "The training procedure of our CPSL is summarized in Algorithm. 1. For detailed equations and loss functions, please refer to our main paper."
        },
        {
            "heading": "6.2. Mean pixel accuracy (MPA)",
            "text": "Denoting by C the number of classes, by pij the number of pixels which belong to the i-th class but are wrongly classified into the j-th class, and by pii the number of pixels which belong to the i-th class and are accurately classified into the i-th class, the pixel accuracy (PA) of the i-th class is defined as:\nPA = pii\u2211C j=1 pij . (15)\nThen the mean pixel accuracy (MPA) is defined as:\nMPA = 1\nC C\u2211 i=1 pii\u2211C j=1 pij . (16)\nAs discussed in Sec. 4.3 of our manuscript, under the constraint of equal partition, many pixels belonging to large categories are assigned to small categories, largely improving the pixel accuracy of small classes. However, this constraint has very small influences on large categories because these categories contain a great number of pixels. Therefore, the MPA is improved."
        },
        {
            "heading": "6.3. Ablation study",
            "text": "We only reported the mIoU scores in Tab.3 of the main paper. Here we present in Tab. 5 the per-class IoU scores of ablation studies. Note that \u201cw/o CB\u201d denotes that we do not\nAlgorithm 1: Training Procedure of CPSL Input : Training data DS = {(Xsn, Y sn )} NS n=1 and\nDT = {Xtn} NT n=1;\nOutput: The output model fSEG ; 1 Generate soft pseudo labels PST with the\nwarmed-up model; 2 Initialize the weight of fSL and f \u2032SL with the\nprototypes [z\u03041, \u00b7 \u00b7 \u00b7 , z\u0304C ] for each category computed by Eq. 7;\n3 for i = 1 to max epochs do 4 for n = 1 to NS do 5 Get source image Xsn; 6 Train the model fSEG using loss LsSEG;\n7 Get target image Xtn; 8 Extract features from Xtn to obtain\nZ \u2208 RH\u00d7W\u00d7D and normalize it with zi =\nzi ||zi||2 ;\n9 Sample a group of pixels Z\u0302 = [z1, \u00b7 \u00b7 \u00b7 , zM ] from Z randomly;\n10 Augment the features Z\u0302 with a memory bankM and obtain Zaug = [Z\u0302;M]; 11 for k = 1 to sinkhorn iterations do 12 Q\u2217aug =\ndiag(\u03b1) exp( fSL(Zaug)\n\u03b5 ) diag(\u03b2);"
        },
        {
            "heading": "13 end",
            "text": "14 Compute the self-labeling loss LSL through\nEq. 5 using the cluster assignment of current batch Qcur;\n15 Train the self-labeling head fSL using loss LSL.\n16 Update the momentum self-labeling head f \u2032SL in an EMA manner; 17 Pass Xtn through f \u2032 SEG and f \u2032 SL to obtain self-labeling assignment PSL; 18 Use PSL to rectify PST and obtain the rectified pseudo labels Y\u0302 tn through Eq. 1; 19 Update fSEG using loss LtSEG; 20 Update the momentum segmentation model f \u2032SEG in an EMA manner. 21 end"
        },
        {
            "heading": "22 end",
            "text": "employ the class-balanced sampling techniques, and constrain that Q should induce an equipartition of data rather than an unequal partition. One can see that this leads to a degradation of 3.9 in terms of mIoU, demonstrating that the equal partition is not reasonable when the class distribution of data is highly imbalanced."
        },
        {
            "heading": "6.4. Training process of CPSL and ProDA",
            "text": "To further highlight the improvement of CPSL during training, we plot the curves of mIoU and MPA scores on the GTA5\u2192Cityscapes task in Fig. 7. A large performance improvement of CPSL over ProDA can be observed in terms of both mIoU and MPA."
        },
        {
            "heading": "6.5. Parameter analysis",
            "text": "Tab. 6 and Tab. 7 show the segmentation results by using different self-labeling loss weight \u03bb1 and consistency regularization loss weight \u03bb2, respectively. One can see that our method is insensitive to these two parameters. Tab. 8 shows the effect of temperature \u03c4 . We employ the cluster assignment PSL as a weight map to online modulate the softmax probability of pseudo labels PST, where the temperature \u03c4 controls the modulation intensity. When \u03c4 \u2192 0, the modulation intensity increases so that the rectified pseudo labels Y\u0302 t will rely heavily on PSL. When \u03c4 \u2192 \u221e, the modulation intensity decreases so that the rectified pseudo labels Y\u0302 t will rely heavily on PST."
        },
        {
            "heading": "6.6. Qualitative results",
            "text": "PSL vs. CPSL. To better illustrate the performance of our method, we implement a variant of CPSL without class-balanced training, i.e., purely Pixel-level SelfLabeling (PSL). The qualitative results of PSL and CPSL are shown in Fig. 8. Overall, CPSL is capable of producing more accurate segments across various scenes. Specifically, our method performs better on long-tailed categories, e.g. \u201cbus\u201d, \u201cbicycle\u201d, \u201cperson\u201d, \u201clight\u201d. Compared to PSL, the segment boundaries of CPSL tend to be clearer and closer to object boundaries, such as \u201cbicycle\u201d and \u201cperson\u201d. Besides, it is noteworthy that PSL wrongly classifies the \u201croad\u201d class into the \u201csidewalk\u201d class in a large area, which is attributed to the equipartition constraint applied on cluster assignments. This constraint is not useful and would even degrade the performance if the real class distribution is not uniform. However, this issue is solved by aligning class distribution of cluster assignments to that of pseudo labels.\nFigure 8. Qualitative results of PSL and CPSL on the GTA5\u2192Cityscapes task.\nComparisons with state-of-the-arts. As in Fig. 3 of the main manuscript, we compare our CPSL with other stateof-the-art methods. Here we provide more visualization results in Fig. 9 - Fig. 15. Our method performs better on long-tailed categories, such as \u201cperson\u201d, \u201cpole\u201d, \u201ctraffic light\u201d, \u201cbus\u201d, and \u201crider\u201d."
        }
    ],
    "title": "Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation",
    "year": 2022
}