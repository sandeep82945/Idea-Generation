{
    "abstractText": "Recent progress in Geometric Deep Learning (GDL) has shown its potential to provide powerful data-driven models. This gives momentum to explore new methods for learning physical systems governed by Partial Differential Equations (PDEs) from Graph-Mesh data. However, despite the efforts and recent achievements, several research directions remain unexplored and progress is still far from satisfying the physical requirements of real-world phenomena. One of the major impediments is the absence of benchmarking datasets and common physics evaluation protocols. In this paper, we propose a 2-D graph-mesh dataset to study the airflow over airfoils at high Reynolds regime (from 10 and beyond). We also introduce metrics on the stress forces over the airfoil in order to evaluate GDL models on important physical quantities. Moreover, we provide extensive GDL baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "BENCHMARKING GRAPH-MESH"
        },
        {
            "affiliations": [],
            "name": "Florent Bonnet"
        },
        {
            "affiliations": [],
            "name": "Jocelyn Ahmed Mazari"
        },
        {
            "affiliations": [],
            "name": "Thibaut Munzer"
        },
        {
            "affiliations": [],
            "name": "Pierre Yser"
        },
        {
            "affiliations": [],
            "name": "Patrick Gallinari"
        }
    ],
    "id": "SP:02a4699992c836cc517b5dc371edc833657f4ff8",
    "references": [
        {
            "authors": [
                "gatama",
                "Jun Zhan",
                "Zhenyao Zhu"
            ],
            "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
            "venue": "CoRR, abs/1512.02595,",
            "year": 2015
        },
        {
            "authors": [
                "Anima Anandkumar",
                "Kamyar Azizzadenesheli",
                "Kaushik Bhattacharya",
                "Nikola Kovachki",
                "Zongyi Li",
                "Burigede Liu",
                "Andrew Stuart"
            ],
            "title": "Neural operator: Graph kernel network for partial differential equations",
            "venue": "In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations,",
            "year": 2020
        },
        {
            "authors": [
                "Utkarsh Ayachit"
            ],
            "title": "The ParaView Guide: A Parallel Visualization Application",
            "year": 2015
        },
        {
            "authors": [
                "Filipe de Avila Belbute-Peres",
                "Thomas D. Economon",
                "J. Zico Kolter"
            ],
            "title": "Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Daniel E. Worrall",
                "Max Welling"
            ],
            "title": "Message passing neural PDE solvers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Michael M. Bronstein",
                "Joan Bruna",
                "Yann LeCun",
                "Arthur Szlam",
                "Pierre Vandergheynst"
            ],
            "title": "Geometric deep learning: Going beyond euclidean data",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2017
        },
        {
            "authors": [
                "R. Qi Charles",
                "Hao Su",
                "Mo Kaichun",
                "Leonidas J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Axel Pinz",
                "Andrew Zisserman"
            ],
            "title": "Convolutional two-stream network fusion for video action recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast graph representation learning with pytorch geometric, 2019",
            "venue": "URL http://arxiv.org/abs/1903.02428. cite arxiv:1903.02428",
            "year": 1903
        },
        {
            "authors": [
                "Hongyang Gao",
                "Shuiwang Ji"
            ],
            "title": "Graph u-nets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Christophe Geuzaine",
                "Jean-Fran\u00e7ois Remacle"
            ],
            "title": "Gmsh: A 3-d finite element mesh generator with built-in pre- and post-processing facilities",
            "venue": "International Journal for Numerical Methods in Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
            "year": 2017
        },
        {
            "authors": [
                "M. Gori",
                "G. Monfardini",
                "F. Scarselli"
            ],
            "title": "A new model for learning in graph domains",
            "venue": "In Proceedings",
            "year": 2005
        },
        {
            "authors": [
                "Gaurav Gupta",
                "Xiongye Xiao",
                "Paul Bogdan"
            ],
            "title": "Multiwavelet-based operator learning for differential equations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aric A. Hagberg",
                "Daniel A. Schult",
                "Pieter J. Swart"
            ],
            "title": "Exploring network structure, dynamics, and function using networkx",
            "venue": "Proceedings of the 7th Python in Science Conference,",
            "year": 2008
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Netw.,",
            "year": 1989
        },
        {
            "authors": [
                "Hrvoje Jasak",
                "Ar Jemcov",
                "United Kingdom"
            ],
            "title": "Openfoam: A c++ library for complex physics simulations",
            "venue": "In International Workshop on Coupled Methods in Numerical Dynamics, IUC, pp",
            "year": 2007
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization, 2014. URL http://arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations",
            "venue": "San Diego,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Nikola B Kovachki",
                "Samuel Lanthaler",
                "Siddhartha Mishra"
            ],
            "title": "On universal approximation and error bounds for fourier neural operators",
            "venue": "Accepted: Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2012
        },
        {
            "authors": [
                "Yujia Li",
                "Daniel Tarlow",
                "Marc Brockschmidt",
                "Richard S. Zemel"
            ],
            "title": "Gated graph sequence neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Andrew Stuart",
                "Kaushik Bhattacharya",
                "Anima Anandkumar"
            ],
            "title": "Multipole graph neural operator for parametric partial differential equations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yulong Lu",
                "Jianfeng Lu"
            ],
            "title": "A universal approximation theorem of deep neural networks for expressing probability distributions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shervin Minaee",
                "Nal Kalchbrenner",
                "Erik Cambria",
                "Narjes Nikzad",
                "Meysam Chenaghlu",
                "Jianfeng Gao"
            ],
            "title": "Deep learning\u2013based text classification: A comprehensive review",
            "venue": "ACM Comput. Surv.,",
            "year": 2021
        },
        {
            "authors": [
                "Arvind T. Mohan",
                "Nicholas Lubbers",
                "Daniel Livescu",
                "Michael Chertkov"
            ],
            "title": "Embedding hard physical constraints in convolutional neural networks for 3d turbulence",
            "venue": "In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations,",
            "year": 2020
        },
        {
            "authors": [
                "Octavi Obiols-Sales",
                "Abhinav Vishnu",
                "Nicholas Malaya",
                "Aparna Chandramowliswharan"
            ],
            "title": "CFDNet: A Deep Learning-Based Accelerator for Fluid Simulations",
            "venue": "Association for Computing Machinery,",
            "year": 2020
        },
        {
            "authors": [
                "Travis E. Oliphant"
            ],
            "title": "Guide to NumPy",
            "venue": "Trelgol,",
            "year": 2006
        },
        {
            "authors": [
                "Karl Otness",
                "Arvi Gjoka",
                "Joan Bruna",
                "Daniele Panozzo",
                "Benjamin Peherstorfer",
                "Teseo Schneider",
                "Denis Zorin"
            ],
            "title": "An extensible benchmark suite for learning to simulate physical systems",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Pfaff",
                "Meire Fortunato",
                "Alvaro Sanchez-Gonzalez",
                "Peter W. Battaglia"
            ],
            "title": "Learning meshbased simulation with graph networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Maziar Raissi",
                "Paris Perdikaris",
                "George E Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Alvaro Sanchez-Gonzalez",
                "Nicolas Heess",
                "Jost Tobias Springenberg",
                "Josh Merel",
                "Martin Riedmiller",
                "Raia Hadsell",
                "Peter Battaglia"
            ],
            "title": "Graph networks as learnable physics engines for inference and control",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Franco Scarselli",
                "Marco Gori",
                "Ah Chung Tsoi",
                "Markus Hagenbuchner",
                "Gabriele Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2009
        },
        {
            "authors": [
                "Leslie N. Smith",
                "Nicholay Topin"
            ],
            "title": "Super-convergence: Very fast training of residual networks using large learning rates, 2018",
            "venue": "URL https://openreview.net/forum?id= H1A5ztj3b",
            "year": 2018
        },
        {
            "authors": [
                "P. Spalart",
                "S. Allmaras. A one-equation turbulence model for aerodynamic flows."
            ],
            "title": "doi: 10",
            "venue": "2514/6.1992-439. URL https://arc.aiaa.org/doi/abs/10.2514/6.1992-439.",
            "year": 1992
        },
        {
            "authors": [
                "Julian Suk",
                "Pim de Haan",
                "Phillip Lippe",
                "Christoph Brune",
                "Jelmer M. Wolterink"
            ],
            "title": "Mesh convolutional neural networks for wall shear stress estimation in 3d artery models. In Statistical Atlases and Computational Models of the Heart. Multi-Disease, Multi-View, and Multi-Center Right Ventricular Segmentation in Cardiac MRI Challenge, 2022",
            "year": 2022
        },
        {
            "authors": [
                "C. Bane Sullivan",
                "Alexander Kaszynski"
            ],
            "title": "PyVista: 3d plotting and mesh analysis through a streamlined interface for the visualization toolkit (VTK)",
            "venue": "Journal of Open Source Software,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Thuerey",
                "Konstantin Wei\u00dfenow",
                "Lukas Prantl",
                "Xiangyu Hu"
            ],
            "title": "Deep learning methods for reynolds-averaged navier\u2013stokes simulations of airfoil flows",
            "venue": "AIAA Journal,",
            "year": 2020
        },
        {
            "authors": [
                "Kiwon Um",
                "Robert Brand",
                "Yun Fei",
                "Philipp Holl",
                "Nils"
            ],
            "title": "Thuerey. Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Wandel",
                "Michael Weinmann",
                "Reinhard Klein"
            ],
            "title": "Learning incompressible fluid dynamics from scratch - towards fast, differentiable fluid models that generalize",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sifan Wang",
                "Yujun Teng",
                "Paris Perdikaris"
            ],
            "title": "Understanding and mitigating gradient pathologies in physics-informed neural networks",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2021
        },
        {
            "authors": [
                "Jiayang Xu",
                "Aniruddhe Pradhan",
                "Karthikeyan Duraisamy"
            ],
            "title": "Conditionally parameterized, discretization-aware neural networks for mesh-based modeling of physical systems",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "PyTorch (Paszke"
            ],
            "title": "2019) is an open-source library for DL using GPUs and CPUs. In this work, we use PyTorch to build our training protocol. In this work, we have used version 1.9.1 of Pytorch along with CUDA 11.1. PyTorch is released as free and open-source Berkeley Software Distribution License",
            "year": 2019
        },
        {
            "authors": [
                "PointNet (Charles"
            ],
            "title": "2017) is not a GNN but a model that operate on point clouds. It is a two scales architecture that is better equipped to handle long-range interactions between nodes compared to the aforementioned GNNs. Moreover, it can be built with a GraphSAGE or a GAT layer for the representation task. We chose to keep a MLP for the representation task as it gives similar scores and is faster to train",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION AND MOTIVATION",
            "text": "The conception of cars, planes, rockets, wind turbines, boats, etc. requires the analysis of surrounding physical fields. Measuring them experimentally by building prototypes is time-consuming, computationally expensive, and sometimes dangerous. Having a measurement in a particular point or reproducing a complex configuration is sometimes impossible during the test phase. Virtual testing allows us to tackle many of these constraints and to test configurations that could not be possible in reality. Hence, numerical simulations are crucial for modeling physical phenomenas and are especially used in Computational Fluid Dynamics (CFD) to solve Navier-Stokes equations.\nAt high Reynolds number, these equations involve a complex dissipation process that cascade from large length scales to small ones which make direct resolutions challenging. Traditional CFD frameworks rely on turbulence models and the power of intensive parallel computations to simulate and analyze fluid dynamics. Despite the efficiency of existing tools, fluid numerical simulations are still computationally expensive and can take several weeks to converge to an accurate solution.\nNevertheless, a huge quantity of data can be extracted from numerical simulation solutions and are today available to assess a new way to recover fluid flow solutions. These data could be exploited to explore the potential of data-driven models in approximating Navier-Stokes PDEs by capturing highly non-linear phenomena. The framework of Deep Learning (DL) is one of the successful datadriven methods that have drawn lots of attention recently (Krizhevsky et al., 2012; Feichtenhofer et al., 2016; Minaee et al., 2021; Amodei et al., 2015). DL methods are particularly interesting thanks to their universal approximation properties (Lu & Lu, 2020; Hornik et al., 1989); capable of approximating a wide range of continuous functions, which makes it a relevant candidate to tackle physics problems while leading to new perspectives for CFD. PDE and DL offer complementary strengths: the modeling power, interpretability, and the accuracy of differential equations solutions, as well as the approximation capabilities and inference speed of DL methods. In practice, CFD solvers operate on meshes to solve Navier-Stokes equations. However, standard DL\nar X\niv :2\n20 6.\n14 70\n9v 1\n[ cs\n.L G\n] 2\n9 Ju\nmodels such as Convolutional Neural Networks (CNNs) (Krizhevsky et al., 2012) achieve learning on regular grid data. Hence, CNNs are not designed to operate directly on meshes but several methods based on grid approaches (Um et al., 2020; Thuerey et al., 2020; Mohan et al., 2020; Wandel et al., 2021; Obiols-Sales et al., 2020; Gupta et al., 2021) have been proposed to approximate the functional space of PDEs. Unfortunately, they cannot correctly infer the physical fields close to obstacles, which make it difficult to correctly compute stress forces at the surface of a geometry. In addition to grid approaches, PDE-based supervised learning methods such as Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) have emerged and could help to solve the physical fields in the entire continuous domain but they are difficult to train (Wang et al., 2021) and are restricted to solving one predefined PDE. Recently, DL on unstructured data has been categorized under the name of Geometric Deep Learning (GDL) (Bronstein et al., 2017). It consists in designing geometrical and compositional inductive biases in DL, reflecting the rich and complex structure in the data. Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009; Li et al., 2016; Kipf & Welling, 2017; Gilmer et al., 2017; Sanchez-Gonzalez et al., 2018) are part of this category and several works on GNNs for approximating PDEs have been proposed (Pfaff et al., 2021; Xu et al., 2021; Brandstetter et al., 2022), including solver in the loop method (Belbute-Peres et al., 2020) and graph neural operator methods (Anandkumar et al., 2020; Li et al., 2020). An important advantage of GDL techniques is their ability to predict quantities over arbitrary shapes without requiring their voxelizations. In our case, this allows accurate computations of stress forces over airfoils.\nIn this work, we propose a benchmarking graph-mesh dataset for studying the problem of 2-D steady-state incompressible Reynolds-Averaged Navier-Stokes (RANS) equations along with an evaluation protocol, especially the computation of stress forces at the surface of the airfoils. Finally, we conduct extensive experiments to give insight about the potential of DL for solving physics problems. We would like to mention that we identified in the literature one similar work but based on structured grid data, that proposes a set of benchmarks to study physical systems (Otness et al., 2021) with classical DL approaches."
        },
        {
            "heading": "2 DATASET CONSTRUCTION AND DESCRIPTION",
            "text": "We chose OpenFOAM (Jasak et al., 2007), an open-source CFD software to run our simulations. We use the steady-state solver simpleFOAM to run Reynolds-Averaged-Simulation with the SpalartAllmaras model (Spalart & Allmaras, 1992) for the turbulence modeling. These simulations were done for multiple airfoil geometries1. We distinguish three different sets; namely training set, validation set, and test set that are taken from the same distribution. The overall 2-D dataset statistics are reported in Table 1. In this work, a geometry represents a shape of an airfoil and an angle of attack. For each geometry, we define a compact domain around it, on which a tetrahedral mesh is built (see Appendix B). Inlet velocity is uniformly sampled between 10 and 50 meters per second which corresponds to a Reynolds number between 106 and 5 \u00b7106 and a Mach number between 0.03 and 0.15. The characteristic length of the proposed airfoils is 1 meter, the kinematic viscosity of air is approximated by 10\u22125. The meshes and boundary conditions are fed to the CFD solver to run the simulations. Its outputs are four fields that represent the targets in our supervised learning task, namely the x and y components of the velocity vector, the pressure and the turbulent viscosity. All those information are wrapped up in a ready-to-use way via the framework of PyTorch Geometric (PyG). Figure 1 depicts an example of the input/outputs of CFD solvers and GNNs.\nThe related details to the construction of the dataset and the normalization procedure are described in the Appendices B and C."
        },
        {
            "heading": "3 TASK DEFINITION AND PHYSICAL METRICS",
            "text": "The incompressible steady-state RANS equations used for this dataset\ncan be expressed as: { (U \u00b7 \u2207)U = \u2212 1\u03c1\u2207p+ (\u03bd + \u03bdt)\u2206U \u2207 \u00b7 U = 0\n(1)\n1Geometries from the National Advisory Committee for Aeronautics (NACA).\nwhere U is the time-average velocity, \u03c1 the density of the fluid, p an effective time-average pressure, \u03bd the kinematic viscosity and \u03bdt the kinematic turbulent viscosity. Boundary conditions are added along with the Spalart-Allmaras turbulence equation to close the problem.\nThe loss L used in this work is the sum of two terms, a loss over the volume LV and a loss over the surface LS :\nL := 1 |V| \u2211 i\u2208V \u2016f\u03b8(xi)\u2212 yi\u201622\ufe38 \ufe37\ufe37 \ufe38\nLV loss over the volume\n+ \u03bb 1 |S| \u2211 i\u2208S \u2016f\u03b8(xi)\u2212 yi\u201622\ufe38 \ufe37\ufe37 \ufe38\nLS loss over the surface\n(2)\nwhere V , S are respectively the set of the indices of the nodes that lie in the volume and on the surface respectively, xi \u2208 R4 is the input at node i containing the 2-D spatial coordinates, the velocity of the flow at the inlet and the Euclidean distance function between the node and the airfoil, yi \u2208 R4 the targets at node i containing the 2-D velocity, the pressure and the kinematic turbulent viscosity at this node and f\u03b8 the model used. The coefficient \u03bb is used to balance the weight of the error at the surface of the geometry and over the volume2. We have to emphasize that this loss is not necessarily a good proxy when it comes, for instance, to compute the wall shear stress or to ensure that the inferred velocity field is divergence free.\nAt the end of the training, we compute the global Wall Shear Stress (WSS) and Wall Pressure (WP), see appendix G for the definition of those quantities. The WSS and WP allow to recover the stress forces at the surface of the geometry and the integral geometry forces\nwhich are the drag and the lift. To the best of our knowledge, this is the first work that proposes an evaluation protocol to assess DL models not only on the quantity regressed but also on more meaningful metrics\nfor real-world problems that require geometries.\nIn the literature most of the models are experimented over the volume, except the recent work (Suk et al., 2022) that proposes a model to regress WSS only on a surface mesh. It is not the direction we chose for our baselines, as we want our model to output only the velocity, pressure and turbulent viscosity fields in order to stick with the form of the RANS equations.\n2In this work \u03bb is set to 1.\nFrom a given geometry and physical parameters, the ultimate goal is to accurately regress the velocity, pressure, and turbulent viscosity fields, as well as to compute the stress forces acting on this geometry. Because data comes in the form of graphs, the task is a real challenge as most of the works focus on regular girds, though a few interesting solutions on meshes are currently emerging as mentioned in section 1. Furthermore, our inlet velocity corresponds to high Reynolds, from 106 to 5 \u00b7 106, which is closer to real-world problems while precedent works focus on Reynolds of some orders of magnitude smaller."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Task definition. For our baselines, we chose to regress the unknown fields involved in the RANS equations 1 and to compute the stress forces as a post-processing step.\nControlling the numerical complexity. The number of nodes and edges in CFD meshes are very high (especially in 3-D cases). Hence, the methods used have to be robust to downsampling to better generalize to more complex problems. This implies that we cannot directly use a CFD mesh as input. One way to overcome this problem is to uniformly draw a subsampling of points in the entire mesh and to build a graph on these point clouds. During the training, at each sample and at each epoch, 1600 different nodes are sampled which represent 10% to 20% of the total number of nodes (depending on the simulation). If needed, a graph is built over this point cloud by connecting the nodes that are closer than a fixed Euclidean distance. We call this graph, radius graph. We chose the radius of our graphs to be 0.1 and we set the maximum number of neighbor points to 64 in order to limit the memory footprint. The entire normalized domain is roughly square of length 8 (in normalized unit). Hence, we connect only very locally and the resulting graphs are not necessarily connected. For the inference, we keep all the points of the CFD meshes, rebuild graphs of radius 0.1 and set the maximum number of neighbor points to 512.\nBaselines. We apply several Geometric Deep Learning (GDL) models to the dataset developed in this work. We distinguish two families of methods, single-scale models and multi-scale models. The former include GraphSAGE (Hamilton et al., 2017), GAT (Velic\u030ckovic\u0301 et al., 2018), PointNet (Charles et al., 2017) and GNO (Anandkumar et al., 2020) while the latter is composed of GraphUnet (Gao & Ji, 2019), PointNet++ (Qi et al., 2017), and MGNO (Li et al., 2020). The GraphSAGE, GAT, GNO, Graph-Unet and MGNO are graph-based models while PointNet and PointNet++ act on point clouds. The GNO and MGNO belong to the class of neural operator methods that consist in learning a mapping between two Hilbert spaces. We use Adam (Kingma & Ba, 2014) along with one-cycle cosine learning rate scheduler (Smith & Topin, 2018) of maximum 3 \u00b7 10\u22123 to optimize our neural networks.\nDetails of the architectures are provided in the supplementary material along with an ablation study. We present in Table 2 scores for the trained models evidencing the difficulty of the various baselines to perform well on all real-world metrics (see Appendix F for results discussion).\nThe MSE for the stress forces is computed with the unnormalized inferred quantities whereas the MSE over the surface and the volume is computed with normalized quantities (see Appendix C)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we developed a preliminary version of a graph-mesh dataset to study 2-D steady-state incompressible RANS equations along with physics-based evaluation protocol. We also provided a set of appropriate baselines to illustrate the potential of GDL to partially replace CFD solvers leading to new perspectives for design and shape optimization processes. We proposed a new way of measuring the performance of DL models and we underlined the importance to use unstructured data in order to have access to more physical quantities such as stress forces. We argue that there is no step forward in a Machine Learning task without a decent set of data and well predefined metrics. This work is a first step to propose elements for the task of quantitative and real-world physics problems.\nFuture works will be dedicated to a brand new version of this dataset with more precision in the CFD meshing process and proof of convergence of the CFD simulations. Moreover, we would like to increase the level of difficulty of the test set by proposing test data out of the training distribution (for Reynolds slightly higher or lower from the training data). We also consider the importance of developing similar data for compressible, unsteady flows, and 3-D cases."
        },
        {
            "heading": "6 BROADER IMPACT",
            "text": "This work could be used to:\n1. experiment new GDL models in this area,\n2. study the capabilities of DL to capture physical phenomena relying on our physics-based evaluation protocol,\n3. give insights to establish new research directions for numerical simulation and ML following the behaviors that will be observed in our quantitative and qualitative results (as well as in the next version of the dataset) with the physical metrics,\n4. extend graph benchmarking datasets and applications of GNNs to physics problems,\n5. build surrogate solvers to help CFD engineers to optimize design cycles and iterate efficiently as much as needed on their designs,"
        },
        {
            "heading": "7 REPRODUCTIBILITY STATEMENT",
            "text": "We provide a GitHub repository to reproduce the experiments and a link to download the dataset. The experiments have been done with a NVIDIA GeForce RTX 3090 24Go."
        },
        {
            "heading": "A DESCRIPTION OF SOFTWARE",
            "text": "In this section, we describe the tools that we have used in this work to build the dataset, make the visualizations, and train the models. This work makes use of computational fluid dynamics (CFD) and ML tools.\nOpenFOAM (Jasak et al., 2007) stands for Open-source Field Operation And Manipulation, a C++ software for developing custom numerical solvers to study continuum mechanics and CFD problems. In this work, we have used version 8.0 of OpenFOAM to make our simulations. OpenFOAM is released as free and open-source software under the GNU General Public Licence.\nGmsh (Geuzaine & Remacle, 2009) is an open-source meshing tool based on 3-D finite element mesh generator with a built-in CAD engine. It supports an Application Programming Interface (API) in four languages: C , C++, Python and Julia. This tool is also able to build meshes in 2-D. We have used version 4.9.3 of Gmsh in this work. Gmsh is released as free and open-source software under the GNU General Public Licence.\nParaView (Ayachit, 2015) is an open-source visualization tool designed to explore and visualize efficiently large data using quantitative and qualitative metrics. ParaView runs on distributed and shared memory parallel and single processor systems. In this work, we have used it to visualize the following: point clouds, meshes, the predicted (as well as the ground truth) physical fields. We have used version 5.7.0 of ParaView in this work. ParaView is released as free and open-source software under the Berkeley Software Distribution License.\nPyVista (Sullivan & Kaszynski, 2019) is an open-source tool based on a handy interface for the Visualization ToolKit (VTK). It is simple to use in interaction with NumPy (Oliphant, 2006) and other Python libraries. It is mainly used for mesh analysis. In this work, we use PyVista to build the inputs of our DL models. We have used version 0.33.0 of PyVista in this work. PyVista is released as free and open-source software under the MIT License.\nNetworkX (Hagberg et al., 2008) is an open-source Python Library for creating and studying complex networks. It is endowed with several standard graph algorithms and data structures for graphs. In this work, we use NetworkX to make statistics on graphs, namely number of nodes, number of edges, and density, as well connectivity. We have used version 2.6 of NetworkX in this work. NetworkX is released as free and open-source Berkeley Software Distribution License.\nPyTorch (Paszke et al., 2019) is an open-source library for DL using GPUs and CPUs. In this work, we use PyTorch to build our training protocol. In this work, we have used version 1.9.1 of Pytorch along with CUDA 11.1. PyTorch is released as free and open-source Berkeley Software Distribution License.\nPyTorch Geometric (PyG) (Fey & Lenssen, 2019) is an open-source library for GDL built upon PyTorch which targets the training of geometric neural networks, including point clouds, graphs and meshes. We use PyG to design our message passing schemes. In this work, we have used version 2.0.2 of PyG along with CUDA 11.1. PyG is released as free and open-source software under the MIT License.\nIn Figure 2, we illustrate the whole pipeline to make the experiments. Starting from mesh generation to model training and output visualizations. We show the connection between all the aforementioned tools to perform our task."
        },
        {
            "heading": "B GRAPH GENERATION: FROM GEOMETRIES TO CFD MESHES AND RADIUS GRAPHS.",
            "text": "We start the CFD process with only the different airfoils as input. We load these geometries (under the form of point clouds) in Gmsh and reconstruct a spline interpolating the point clouds in order to recover continuous geometries. We specify the density of points we want at the surface of those geometries and at the boundaries of the domains. Then, we use Gmsh to create 2-D conformal meshes made of triangles and extrude them along the z-direction to get 3-D conformal meshes made of one tetrahedral in the z-direction. We need to do the extrusion step as OpenFOAM only works with 3-D meshes. An example of such mesh is given in Figure 3."
        },
        {
            "heading": "C PREPROCESSING",
            "text": "In this section, we describe the process of input and output variables normalization that we use prior to feeding them to DL models. We compute the mean value \u00b5k and the standard deviation \u03c3k, of each component k \u2208 {0, 1, 2, 3} of all the nodes inputs of all the samples in the training set and we normalize each sample\u2019s inputs x as follow:\nx \u2032 k = xk \u2212 \u00b5k \u03c3k + 10\u22128\n(3)\nwhere the factor 10\u22128 is added for numerical stability. For the targets, a similar process is applied. However, we observed a particularity in the distribution of the turbulent viscosity at the surface compared to the one in the volume. The values of the turbulent viscosity at the surface are of, at least, one order of magnitude smaller. Hence, we have chosen to normalize it independently from the volume values. This trick leads to a better performance on the inference of the turbulent viscosity over the surface. The transformation applied to the targets are as follow, we first normalize the targets y via:\ny \u2032 k = yk \u2212 \u00b5k \u03c3k + 10\u22128\n(4)\nwhere \u00b5k is the mean value of the k-component of all of the targets of all the samples in the training set and \u03c3k their standard deviation. In addition to that, for the turbulent viscosity component (i.e. for k = 4) associated to the nodes at the surface of airfoils, we apply a second transformation as follow:\ny \u2032\u2032 4 = y\n\u2032 4(\u03c34 + 10 \u22128) + \u00b54 \u2212 \u00b5(s)4\n\u03c3 (s) 4 + 10\n\u22128 (5)\n= y (s) 4 \u2212 \u00b5 (s) 4\n\u03c3 (s) 4 + 10\n\u22128 (6)\nwhere \u00b5s4 is the mean value of the turbulent viscosity of all surface nodes of all samples in the training set and \u03c3s4 their standard deviation. These mean and standard deviation values are used to normalize the validation set and test set."
        },
        {
            "heading": "D DATASET FIGURES",
            "text": "In Figure 3 are displayed the CFD meshes of samples from the training, validation and test sets. In Figure 4 are displayed the simulated pressure field of the same samples from the training, validation and test sets. It would be better to visualize them using Paraview for more flexibility. We release the code for that as already mentioned in Appendix 7."
        },
        {
            "heading": "E DETAILS ABOUT EXPERIMENTS",
            "text": "First of all, each type of model used is preceded by an encoder and followed by a decoder. Those encoder and decoder have the same architecture for each model tested, we chose a MLP with 4 \u2212 64 \u2212 64 \u2212 8 neurons for the encoder and a MLP with 8 \u2212 64 \u2212 64 \u2212 4 neurons for the decoder. Both with ReLU activation function. Moreover, for each model tested, the encoder and decoder are\ntrained from scratch together with the new model between. In order to have a standard deviation for the scores, we trained 10 times each model and did the statistics of the obtained results. All the models are trained using Pytorch Geometric on a single GPU (nVidia Tesla P100, 16 Go).\nIn Figure 7 and Figure 9, we show respectively the pressure and the x-component of the velocity fields, inferred by the different models compared to the ground truth and in Figure 8 and Figure 10 we show the difference between the ground truth and the predicted fields."
        },
        {
            "heading": "E.1 SINGLE-SCALE MODELS",
            "text": ""
        },
        {
            "heading": "E.1.1 GRAPHSAGE",
            "text": "The GraphSAGE layer (Hamilton et al., 2017) is the basic inductive type layer, it gives us a first indicator of the difficulty of the task. We used a 4-layers GraphSAGE network 8\u2212 64\u2212 64\u2212 64\u2212 8 channels, each layer is followed by a batchnorm layer to make the optimization problem easier and a ReLU activation function. Also, as the number of nodes is still pretty small in our dataset, the CFD mesh holds in the memory of our GPU. Hence, we compared the scores of the same model trained over the CFD mesh and with our downsampling technique using the radius graph. Table 3 reports the results of this comparison. We observe that even though the CFD mesh includes 5 to 10 times more points than the radius graph during training, the model has similar scores with the radius graph setting. In Figure 11, we also show the inferred WSS and WP with the radius graph methods and compare it with the ground truth.\nE.1.2 GAT The GAT layer (Velic\u030ckovic\u0301 et al., 2018) is designed for inductive tasks too but is often better performing than GraphSAGE. We used a 4-layers GAT network 8\u2212 64\u2212 64\u2212 64\u2212 8 channels, each layer is followed by a batchnorm layer and a ReLU activation function."
        },
        {
            "heading": "E.1.3 POINTNET",
            "text": "PointNet (Charles et al., 2017) is not a GNN but a model that operate on point clouds. It is a two scales architecture that is better equipped to handle long-range interactions between nodes compared to the aforementioned GNNs. Moreover, it can be built with a GraphSAGE or a GAT layer for the representation task. We chose to keep a MLP for the representation task as it gives similar scores and is faster to train. For the architecture, see (Charles et al., 2017) for the segmentation task, we just changed the output in order to fit with our problem and get rid of the batchnorm layers and dropout as it was performing poorly with."
        },
        {
            "heading": "E.2 MULTI-SCALE MODELS",
            "text": "The resolution of PDEs may involve long-range interactions where all the points of the domain influence the solution at a certain position. It is difficult to take into account those interactions between far-away nodes in architectures such as the GraphSAGE or GAT as it would imply stacking a consequent number of layers (equivalent to the diameter of a graph) which would make the training process difficult. Multi-scale models can overcome this issue by representing the signal at different scales allowing long-range interactions by coarsening the input graphs or point clouds."
        },
        {
            "heading": "E.2.1 GRAPH-UNET",
            "text": "The Graph-Unet (Gao & Ji, 2019) is the archetypal multi-scale architecture. It extends the wellknown U-Net architecture (Ronneberger et al., 2015) initially designed for image segmentation, to graph data. Our Graph-Unet makes use of GraphSAGE layers instead of GCN (Kipf & Welling, 2017) layers as in the historical paper. Moreover, we did not use the gPool technique developed in the Graph-Unet as we found that it is not robust to the downsampling. We replaced it by a simple random downsampling and nearest neighbour upsampling. Our architecture is composed of 5 scales\nwith downsampling ratio of 3/4 \u2212 3/4 \u2212 2/3 \u2212 2/3 and at each scale a radius graph is built with radius of 0.1\u22120.2\u22120.5\u22121\u221210 where the last radius is taken big enough for the graph to be totally connected. In the training process, each radius graph is set to produce a maximum of 64 neighbors whereas for inference we raise this number to 512.\nThe architecture is composed of 9 layers, each followed by a batchnorm layer, a ReLU activation function and a downsampling or upsampling layer (for all but the last block). The number of channels is doubled at each scale starting at 8 and the intra-scale information of the Graph U-net are aggregated via concatenations. Hence, the number of channels is 8 \u2212 16 \u2212 32 \u2212 64 \u2212 128 in the downward pass and 192\u2212 96\u2212 48\u2212 24\u2212 8 in the upward pass."
        },
        {
            "heading": "E.2.2 POINTNET++",
            "text": "The PointNet++ (Qi et al., 2017) is a multi-scale extension of the PointNet model discussed above. It allows refinement in the global representation of the PointNet by using multiple PointNet on local areas leading to a multi-scale representation of the task. Our architecture is the same as described in the original paper for the segmentation task. We chose 7 scales with downsampling ratio of 3/4 \u2212 3/4 \u2212 2/3 \u2212 2/3 \u2212 3/4 \u2212 2/3 and on each scale a radius graph is built with radius of 0.1\u2212 0.2\u2212 0.4\u2212 0.8\u2212 1.6\u2212 10 where the last radius is taken big enough for the graph to be totally connected. In the training process, each radius graph is set to produce a maximum of 64 neighbors whereas for inference we raise this number to 512. No dropout is used."
        },
        {
            "heading": "E.3 GRAPH NEURAL OPERATORS",
            "text": "Neural operators are a new paradigm in DL that aim at approximating operators between Hilbert spaces instead of applications between real and finite dimensional vector spaces (Kovachki et al., 2022). Those models are often composed of an encoder which is in charge of finding a finite dimensional representation of the infinite dimensional input space, an approximator that transforms this representation and a decoder that maps this finite representation of the solution to the infinite dimensional output space.\nE.3.1 GRAPH NEURAL OPERATOR (GNO)\nOur model is a modulation of the architecture proposed in (Anandkumar et al., 2020). It is a recurrent network that mimics the resolution of Poisson\u2019s equation through a convolutional operator and repeats it several times to approximate the solutions. We can write it as:\nhti = 1 |Ni| \u2211 j\u2208Ni \u03ba\u03b8k(e t\u22121 ij )h t\u22121 j + h t\u22121 i (7)\nwhere Ni is the set of neighbors of the node i, \u03ba\u03b8k a neural network, etij the attributes of the edge between the nodes i and j at iteration t and hti the hidden state at iteration t and node i. We also use an encoder and a decoder at the beginning and the end of our network, such that h0i = \u03c6\u03b8e(xi) and u\u0302(xi) = \u03c8\u03b8d(h T i ) for a network with T iterations, where \u03c6\u03b8e and \u03c8\u03b8d are also both neural networks. Note that just after each graph convolution block, we use a batchnorm layer. The edge attributes etij are defined as a vector containing the relative position, velocity and pressure between nodes i and j at iteration t, the signed distance function of node i and j and the inlet velocity. The computation of the velocity field and pressure field at an intermediate iteration is done through the decoding of the hidden state at that iteration. Also, a multiscale architecture has been designed with the same philosophy in order to capture long-range interactions between nodes without blowing up the numerical complexity. In Figure 5 we depicts our modified Graph Neural Operator (GNO) architecture. For the kernel in the graph convolution, we used a 4- layers MLP with 8\u221264\u221264\u221264\u221264 neurons and ReLU activation function. This kernel takes as inputs the edge attributes of the graph and outputs a convolution matrix of size 8\u00d7 8."
        },
        {
            "heading": "E.3.2 MULTIPOLE GRAPH NEURAL OPERATOR (MGNO)",
            "text": "We also propose a multiscale version of the GNO, namely the Multipole Graph Neural Operator (MGNO) which is a modulation of (Li et al., 2020). The architecture follows the same form as depicted in Figure 5 but the graph convolution here is different from the one in the GNO. We replaced the multipole technique used in the original paper to a more classical UNet like architecture for the block of convolution. Figure 6 depicts the details of the block. A convolution is done at multiple scales by downsampling the input graph via a mean pooling. Then, these convolutions are aggregated scale by scale through a nearest neighbors upsampling. At each scale, a kernel is needed for the convolution. All the kernels are 4-layers MLP with 8\u2212 64\u2212 64\u2212 64\u2212 64 neurons and ReLU activation function as in the GNO model."
        },
        {
            "heading": "F RESULTS DISCUSSION",
            "text": "In this section, we discuss the results reported in Table 2. First remark is that multi-scale models do not necessarily perform better than single-scale models. A second remark is that the MSE over the points of the domain is not necessarily a good proxy for good performance on the stress forces. Actually, a simple GraphSAGE model gives similar results compared to more complicated architectures such as GNO or MGNO on stress forces whereas the latter gives better results for the loss function. The training process of the PointNet and PointNet++ networks seems less robust compared to the GraphSAGE or the GNO/MGNO one. However, no model outperforms all the other in every aspect. MGNO yields the best results in all our metrics but the y component of the wall pressure may suffer from scalability problem as it is memory and time consuming to train it. Hence, in this supervised problem settings, we cannot conclude on the efficiency of a particular design such as classical GNNs, neural operators or network acting on point clouds."
        },
        {
            "heading": "G STRESS FORCES",
            "text": "We call stress the force \u03c3(n) that is acting (by contact) on a surface of unit normal n. For an infinitesimal surface dS which normal n points towards the fluid that is acting on it, the resulting force df can be written as:\ndf = \u03c3(n)dS (8)\nLet us take an infinitesimal cube, we look only at three contiguous faces, and call \u03c3ij the force per unit of surface acting on the ith face on the jth direction. We then define the second order stress tensor (also known as the Cauchy stress tensor) \u03c3 whose components are \u03c3ij . By the third law of Newton, we find that, at first order, for an infinitesimal cube, \u03c3(n) = \u2212\u03c3(\u2212n) which tells us that we only need to know the tensor \u03c3 to know completely the surface forces acting on the entire cube (and not only on the three contiguous faces chosen previously). Moreover, by an argument on the kinetic moment of this infinitesimal cube, we find that \u03c3 needs to be symmetric.\nWe conclude that for an arbitrary normal n, we only need to project the stress tensor on this normal to have the force acting on it, we find:\n\u03c3(n) = \u03c3 \u00b7 n (9)\nIn the case of a fluid with a null velocity field, there are only normal stresses acting on our infinitesimal cube. Moreover, those stresses are isotropic. We call pressure, denoted by p, the intensity of those stresses. We then find \u03c3 = \u2212pI , where I is the identity matrix of dimension 3. The minus sign is because we took the convention of outward normal when we defined \u03c3(n).\nIn the case of a general velocity field, we define the viscous stress tensor \u03c3\u2032 by:\n\u03c3 = \u2212pI + \u03c3\u2032 (10)\nWe remark that \u03c3\u2032 is also a second order symmetric tensor.\nOn the other hand, the deformation of an infinitesimal volume of control can be quantified thanks to the Jacobian J of the velocity. This Jacobian can be decoupled in a symmetric tensor S and a skew-symmetric tensor W via:\nJ = S +W (11)\nS = 1\n2 (J + J t) (12)\nW = 1\n2 (J \u2212 J t) (13)\nwhere J t correspond to the transpose of J .\nThe tensor W represents the pure rotation of the volume of control and the tensor S represents the compression and dilation of the volume of control with respect to a certain basis (as it is symmetric, it can be diagonalizable in an orthonormal basis). Moreover, we remark that\u2207\u00b7v = Tr(J) = Tr(S), hence, if the fluid is incompressible, J , S and W are traceless.\nFor newtonian and incompressible fluids, we find the relation:\n\u03c3\u2032 = 2\u00b5S (14)\nwhere \u00b5 is the coefficient that quantifies the dissipation property of the fluids through shear stresses, we call it the dynamic viscosity.\nHence, we find that the stress force df acting on a face of area dS and normal n of our infinitesimal cube is:\ndf = \u2212pn+ 2\u00b5S \u00b7 n (15)\nAnd we can conclude that for a geometry of surface S, the stress force F acting on it can be computed via:\nF = \u222e S \u03c3 \u00b7 ndS (16)\n= \u2212 \u222e S pndS + \u222e S 2\u00b5S \u00b7 ndS (17)\nWe call the term P := \u2212pn the wall pressure and the term \u03c4 := 2\u00b5S \u00b7 n the wall shear stress. Ultimately, we call drag D and lift L the component of F that are respectively parallel and orthogonal to the main direction of the flow. If the fluid flows in the x-direction, we have:\nD = (\u222e S PdS + \u222e S \u03c4dS ) x\n(18)\nL = (\u222e S PdS + \u222e S \u03c4dS ) y\n(19)\nIn the case of RANS equations, we add terms that take in account the effect of turbulence over the geometry. The pressure p is replaced by an effective pressure pe and the wall shear stress is given by \u03c4 = 2(\u00b5+ \u00b5t)S \u00b7 n where \u00b5t is the dynamic turbulent viscosity.\nFor incompressible fluids we also often divide those quantities by \u03c1 the density of the fluid and solvers often express the results in terms of reduced pressure p\u2032 := pe/\u03c1 and kinematic (turbulent) viscosity \u03bd := \u00b5/\u03c1 (\u03bdt := \u00b5t/\u03c1). We use this convention in this work."
        },
        {
            "heading": "H NON-DIMENSIONALIZATION OF THE NAVIER-STOKES EQUATION",
            "text": "Solving Navier-Stokes\u2019 equations for a set of parameters \u03c1 and \u03bd and boundary conditions may actually be equivalent to solving a whole family of equations. To enlighten this phenomena we can work with non-dimensional quantities. Moreover, such formulation will help us to see the importance of each term in the system of partial differential equations.\nIn order to do so, let T , L, V , P be characteristics time scale, length scale, velocity scale and pressure scale (respectively) of the problem. We define:\nt = T t\u0302 r = Lr\u0302 v = V v\u0302 p = P p\u0302 (20)\nAll the quantities with a hat are dimensionless and we can update the incompressible Navier-Stokes equations without source term:\nV T \u2202t\u0302v\u0302 +\nV 2\nL (v\u0302 \u00b7 \u2207\u0302)v\u0302 = \u2212 P \u03c1L \u2207\u0302p\u0302+ \u03bdV L2 \u2206\u0302v\u0302 (21)\nLet us take T equal to L/V and P equal to \u03c1V 2, we find:\nV 2\nL \u2202t\u0302v\u0302 +\nV 2\nL (v\u0302 \u00b7 \u2207\u0302)v\u0302 = \u2212V\n2\nL \u2207\u0302p\u0302+ \u03bdV L2 \u2206\u0302v\u0302 (22)\nIn total, this gives:\n\u2202t\u0302v\u0302 + (v\u0302 \u00b7 \u2207\u0302)v\u0302 = \u2212\u2207\u0302p\u0302+ 1\nRe \u2206\u0302v\u0302 (23)\nRe = V L\n\u03bd (24)\nThe dimensionless number Re is called the Reynolds number. This equation only depends on the Reynolds number and two flows with the same Reynolds number will have the same dimensionless solution.\nMoreover, the Reynolds number can be seen as the ratio of the order of magnitude of the convective term over the order of magnitude of the viscous term:\nRe = convective term viscous term = \u2016(v \u00b7 \u2207)v\u2016 \u2016\u03bd\u2206v\u2016 = V 2/L \u03bdV/L2 = V L \u03bd (25)\nWe then have two particular regimes:\n\u2022 Re\u2192 0, viscous term dominates the flow (we call this a Stokes flow) \u2022 Re \u2192 \u221e, convective term dominates the flow (Navier-Stokes equations tends towards\nEuler\u2019s equations for inviscid fluids)"
        }
    ],
    "title": "PRESSIBLE NAVIER-STOKES EQUATIONS",
    "year": 2022
}