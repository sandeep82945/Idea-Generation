{
    "abstractText": "We present a new dataset for 6-DoF pose estimation of known objects, with a focus on robotic manipulation research. We propose a set of toy grocery objects, whose physical instantiations are readily available for purchase and are appropriately sized for robotic grasping and manipulation. We provide 3D scanned textured models of these objects, suitable for generating synthetic training data, as well as RGBD images of the objects in challenging, cluttered scenes exhibiting partial occlusion, extreme lighting variations, multiple instances per image, and a large variety of poses. Using semi-automated RGBD-to-model texture correspondences, the images are annotated with ground truth poses accurate within a few millimeters. We also propose a new pose evaluation metric called ADD-H based on the Hungarian assignment algorithm that is robust to symmetries in object geometry without requiring their explicit enumeration. We share pre-trained pose estimators for all the toy grocery objects, along with their baseline performance on both validation and test sets. We offer this dataset to the community to help connect the efforts of computer vision researchers with the needs of roboticists.",
    "authors": [
        {
            "affiliations": [],
            "name": "Stephen Tyree"
        },
        {
            "affiliations": [],
            "name": "Jonathan Tremblay"
        },
        {
            "affiliations": [],
            "name": "Thang To"
        },
        {
            "affiliations": [],
            "name": "Jia Cheng"
        },
        {
            "affiliations": [],
            "name": "Terry Mosier"
        },
        {
            "affiliations": [],
            "name": "Jeffrey Smith"
        },
        {
            "affiliations": [],
            "name": "Stan Birchfield"
        }
    ],
    "id": "SP:25ee1d6de038d48705ef4e75bc3d3cd5689bde0c",
    "references": [
        {
            "authors": [
                "M. Fiala"
            ],
            "title": "ARTag, a fiducial marker system using digital techniques",
            "venue": "CVPR, 2005. 1",
            "year": 2005
        },
        {
            "authors": [
                "E. Olson"
            ],
            "title": "AprilTag: A robust and flexible visual fiducial system",
            "venue": "ICRA, 2011. 1",
            "year": 2011
        },
        {
            "authors": [
                "Y. Labbe",
                "J. Carpentier",
                "M. Aubry",
                "J. Sivic"
            ],
            "title": "CosyPose: Consistent multi-view multi-object 6D pose estimation",
            "venue": "ECCV, 2020. 1, 2, 5",
            "year": 2020
        },
        {
            "authors": [
                "J. Tremblay",
                "T. To",
                "B. Sundaralingam",
                "Y. Xiang",
                "D. Fox",
                "S. Birchfield"
            ],
            "title": "Deep object pose estimation for semantic robotic grasping of household objects",
            "venue": "CoRL, 2018. 1, 2, 5",
            "year": 2018
        },
        {
            "authors": [
                "Y. Hu",
                "J. Hugonot",
                "P. Fua",
                "M. Salzmann"
            ],
            "title": "Segmentation-driven 6D object pose estimation",
            "venue": "CVPR, 2019. 1",
            "year": 2019
        },
        {
            "authors": [
                "S. Peng",
                "Y. Liu",
                "Q. Huang",
                "H. Bao",
                "X. Zhou"
            ],
            "title": "PVNet: Pixel-wise voting network for 6DoF pose estimation",
            "venue": "CVPR, 2019. 1",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "G. Wang",
                "X. Ji",
                "Y. Xiang",
                "D. Fox"
            ],
            "title": "DeepIM: Deep iterative matching for 6D pose estimation",
            "venue": "ECCV, 2018. 1",
            "year": 2018
        },
        {
            "authors": [
                "M. Rad",
                "V. Lepetit"
            ],
            "title": "BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth",
            "venue": "ICCV, 2017. 1",
            "year": 2017
        },
        {
            "authors": [
                "C. Wang",
                "R. Mart\u0131\u0301n-Mart\u0131\u0301n",
                "D. Xu",
                "J. Lv",
                "C. Lu",
                "L. Fei-Fei",
                "S. Savarese",
                "Y. Zhu"
            ],
            "title": "6-PACK: Category-level 6D pose tracker with anchor-based keypoints",
            "venue": "ICRA, 2020. 1",
            "year": 2020
        },
        {
            "authors": [
                "C. Wang",
                "D. Xu",
                "Y. Zhu",
                "R. Mart\u0131\u0301n-Mart\u0131\u0301n",
                "C. Lu",
                "L. Fei-Fei",
                "S. Savarese"
            ],
            "title": "DenseFusion: 6D object pose estimation by iterative dense fusion",
            "venue": "CVPR, 2019. 1",
            "year": 2019
        },
        {
            "authors": [
                "B. Wen",
                "C. Mitash",
                "B. Ren",
                "K.E. Bekris"
            ],
            "title": "se(3)-TrackNet: Datadriven 6D pose tracking by calibrating image residuals in synthetic domains",
            "venue": "IROS, 2020. 1, 7",
            "year": 2020
        },
        {
            "authors": [
                "B. Wen",
                "K. Bekris"
            ],
            "title": "BundleTrack: 6D pose tracking for novel objects without instance or category-level 3D models",
            "venue": "IROS, 2021. 1",
            "year": 2021
        },
        {
            "authors": [
                "T. Hoda\u0148",
                "F. Michel",
                "E. Brachmann",
                "W. Kehl",
                "A.G. Buch",
                "D. Kraft",
                "B. Drost",
                "J. Vidal",
                "S. Ihrke",
                "X. Zabulis",
                "C. Sahin",
                "F. Manhardt",
                "F. Tombari",
                "T.-K. Kim",
                "J. Matas",
                "C. Rother"
            ],
            "title": "BOP: Benchmark for 6D object pose estimation",
            "venue": "ECCV, 2018. 1, 5, 7",
            "year": 2018
        },
        {
            "authors": [
                "B. Calli",
                "A. Walsman",
                "A. Singh",
                "S. Srinivasa",
                "P. Abbeel",
                "A.M. Dollar"
            ],
            "title": "Benchmarking in manipulation research: Using the Yale- CMU-Berkeley object and model set",
            "venue": "IEEE Robotics and Automation Magazine, vol. 22, no. 3, Sep. 2015. 1, 7",
            "year": 2015
        },
        {
            "authors": [
                "C. Rennie",
                "R. Shome",
                "K.E. Bekris",
                "A.F. De Souza"
            ],
            "title": "A dataset for improved RGBD-based object detection and pose estimation for warehouse pick-and-place",
            "venue": "Robotics and Automation Letters (RAL), vol. 1, no. 2, 2016. 1, 7",
            "year": 2016
        },
        {
            "authors": [
                "N. Morrical",
                "J. Tremblay",
                "Y. Lin",
                "S. Tyree",
                "S. Birchfield",
                "V. Pascucci",
                "I. Wald"
            ],
            "title": "NViSII: A scriptable tool for photorealistic image generation",
            "venue": "ICLR Workshop on Synthetic Data Generation (SDG), 2021. 2",
            "year": 2021
        },
        {
            "authors": [
                "M. Denninger",
                "M. Sundermeyer",
                "D. Winkelbauer",
                "Y. Zidan",
                "D. Olefir",
                "M. Elbadrawy",
                "A. Lodhi",
                "H. Katam"
            ],
            "title": "BlenderProc",
            "venue": "arXiv preprint arXiv:1911.01911, 2019. 2, 5",
            "year": 1911
        },
        {
            "authors": [
                "K. Pauwels",
                "D. Kragic"
            ],
            "title": "SimTrack: A simulation-based framework for scalable real-time object pose detection and tracking",
            "venue": "IROS, 2015. 3",
            "year": 2015
        },
        {
            "authors": [
                "T. Hoda\u0148",
                "P. Haluza",
                "\u0160. Obdr\u017e\u00e1lek",
                "J. Matas",
                "M. Lourakis",
                "X. Zabulis"
            ],
            "title": "T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects",
            "venue": "WACV, 2017. 3, 7",
            "year": 2017
        },
        {
            "authors": [
                "S. Hinterstoisser",
                "V. Lepetit",
                "S. Ilic",
                "S. Holzer",
                "G. Bradski",
                "K. Konolige",
                "N. Navab"
            ],
            "title": "Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes",
            "venue": "ACCV, 2012. 4, 7",
            "year": 2012
        },
        {
            "authors": [
                "Y. Xiang",
                "T. Schmidt",
                "V. Narayanan",
                "D. Fox"
            ],
            "title": "PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes",
            "venue": "RSS, 2018. 4, 7",
            "year": 2018
        },
        {
            "authors": [
                "T. Hoda\u0148",
                "M. Sundermeyer",
                "B. Drost",
                "Y. Labb\u00e9",
                "E. Brachmann",
                "F. Michel",
                "C. Rother",
                "J. Matas"
            ],
            "title": "BOP challenge 2020 on 6D object localization",
            "venue": "ECCV Workshop, 2020. 4, 6",
            "year": 2020
        },
        {
            "authors": [
                "D.F. Crouse"
            ],
            "title": "On implementing 2D rectangular assignment algorithms",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems, vol. 52, no. 4, pp. 1679\u20131696, 2016. 4",
            "year": 2016
        },
        {
            "authors": [
                "J. Tobin",
                "R. Fong",
                "A. Ray",
                "J. Schneider",
                "W. Zaremba",
                "P. Abbeel"
            ],
            "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "venue": "IROS, 2017. 5",
            "year": 2017
        },
        {
            "authors": [
                "J. Tremblay",
                "T. To",
                "S. Birchfield"
            ],
            "title": "Falling things: A synthetic dataset for 3D object detection and pose estimation",
            "venue": "CVPR Workshop on Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision, Jun. 2018. 7",
            "year": 2018
        },
        {
            "authors": [
                "Z. Tang",
                "M. Naphade",
                "M.-Y. Liu",
                "X. Yang",
                "S. Birchfield",
                "S. Wang",
                "R. Kumar",
                "D. Anastasiu",
                "J.-N. Hwang"
            ],
            "title": "CityFlow: A city-scale benchmark for multi-target multi-camera vehicle tracking and reidentification",
            "venue": "CVPR, 2019. 7",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xiang",
                "W. Kim",
                "W. Chen",
                "J. Ji",
                "C. Choy",
                "H. Su",
                "R. Mottaghi",
                "L. Guibas",
                "S. Savarese"
            ],
            "title": "ObjectNet3D: A large scale database for 3D object recognition",
            "venue": "ECCV, 2016. 7",
            "year": 2016
        },
        {
            "authors": [
                "Y. Xiang",
                "R. Mottaghi",
                "S. Savarese"
            ],
            "title": "Beyond PASCAL: A benchmark for 3D object detection in the wild",
            "venue": "WACV, 2014. 7",
            "year": 2014
        },
        {
            "authors": [
                "E. Brachmann",
                "A. Krull",
                "F. Michel",
                "S. Gumhold",
                "J. Shotton",
                "C. Rother"
            ],
            "title": "Learning 6D object pose estimation using 3D object coordinates",
            "venue": "ECCV, 2014. 7",
            "year": 2014
        },
        {
            "authors": [
                "B. Calli",
                "A. Walsman",
                "A. Singh",
                "S. Srinivasa",
                "P. Abbeel",
                "A.M. Dollar"
            ],
            "title": "The YCB object and model set: Towards common benchmarks for manipulation research",
            "venue": "Intl. Conf. on Advanced Robotics (ICAR), 2015. 7",
            "year": 2015
        },
        {
            "authors": [
                "Y.-W. Chao",
                "W. Yang",
                "Y. Xiang",
                "P. Molchanov",
                "A. Handa",
                "J. Tremblay",
                "Y.S. Narang",
                "K.V. Wyk",
                "U. Iqbal",
                "S. Birchfield",
                "J. Kautz",
                "D. Fox"
            ],
            "title": "DexYCB: A benchmark for capturing hand grasping of objects",
            "venue": "CVPR, 2021. 7",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lin",
                "J. Tremblay",
                "S. Tyree",
                "P.A. Vela",
                "S. Birchfield"
            ],
            "title": "Multi-view fusion for multi-level robotic scene understanding",
            "venue": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 6817\u20136824. 7",
            "year": 2021
        },
        {
            "authors": [
                "S. Hinterstoisser",
                "C. Cagniart",
                "S. Ilic",
                "P. Sturm",
                "N. Navab",
                "P. Fua",
                "V. Lepetit"
            ],
            "title": "Gradient response maps for real-time detection of textureless objects",
            "venue": "PAMI, vol. 34, no. 5, pp. 876\u2013888, 2012. 7",
            "year": 2012
        },
        {
            "authors": [
                "B. Drost",
                "M. Ulrich",
                "P. Bergmann",
                "P. H\u00e4rtinger",
                "C. Steger"
            ],
            "title": "Introducing MVTec ITODD\u2013a dataset for 3D object recognition in industry",
            "venue": "ICCV Workshop on Recovering 6D Object Pose, 2017. 7",
            "year": 2017
        },
        {
            "authors": [
                "R. Kaskman",
                "S. Zakharov",
                "I. Shugurov",
                "S. Ilic"
            ],
            "title": "HomebrewedDB: RGB-D dataset for 6D pose estimation of 3D objects",
            "venue": "arXiv:1904.03167, 2019. 7",
            "year": 1904
        },
        {
            "authors": [
                "A. Doumanoglou",
                "R. Kouskouridas",
                "S. Malassiotis",
                "T.-K. Kim"
            ],
            "title": "Recovering 6D object pose and predicting next-best-view in the crowd",
            "venue": "CVPR, 2016. 7",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nEstimating the poses of objects in a scene is important for robotic grasping and manipulation in a variety of domains, such as manufacturing, healthcare, commercial, and households. In pick-and-place tasks, for example, it is necessary for the robot to know the identities and locations of objects in order to grasp them, to avoid collision with other objects during movement, and to place them relative to other objects. Yet, even today, it is not uncommon for objects in robotics labs to be modified with fiducial markers such as ARTags [1] or AprilTags [2] to support detection and pose estimation. Such modifications are needed because no widely used, offthe-shelf, general-purpose techniques currently exist.\nTo bridge the gap between research labs and real-world scenarios, many researchers have developed methods to detect objects and estimate their poses from RGB or RGBD images. Recent progress on this 6-DoF (\u201cdegrees of freedom\u201d) pose estimation problem has been significant [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]. Although there remain many open research problems, the basics are in place.\nNevertheless, from a robotics point of view, another important limitation remains: Existing pose estimators are trained for objects that are not available to most researchers. This limitation is easily observed from the comprehensive list of datasets in the BOP Benchmark for 6D Object Pose\n1https://github.com/swtyree/hope-dataset\nEstimation [14].2 Because there is no easy way to acquire most of the physical objects, robotics researchers are often unable to conduct real robotic experiments leveraging pose networks already trained using these datasets.\nOf these existing datasets, arguably the most accessible are YCB [15] and RU-APC [16]. However, over time it is becoming increasingly difficult to find matching objects from these datasets in stores. For example, even though we live near the researchers who assembled the YCB dataset, we have been unsuccessful in locating a sugar box (YCB item 004) or gelatin box (YCB item 009) whose appearance matches that of the original. One of the reasons for this failure is the changing appearance of such products. In particular, food manufacturers frequently change the design on their products based on seasonal or promotional considerations or updates to their branding. As a result, networks trained on the YCB dataset exhibit degraded performance when applied to actual objects acquired from a store. A further problem is that some of the items are not of the appropriate size, shape, and weight for common robotic grippers.\nTo address these problems, we release a dataset and benchmark for 6-DoF pose estimation for robotic manipulation research. The dataset is designed to be:\n2https://bop.felk.cvut.cz/datasets/\nar X\niv :2\n20 3.\n05 70\n1v 2\n[ cs\n.R O\n] 1\n5 D\nec 2\n02 2\n\u2022 Accessible. The physical objects should ideally be accessible to anyone in the world. As a result, we selected toy grocery objects that can be purchased online. We also scanned these objects and share the resulting 3D textured models to aid in generating training images. \u2022 Challenging. We captured real images of these objects with the following characteristics: 1) significant occlusions and clutter, 2) varying number of instances of each object, 3) a wide variety of poses, and 4) extreme and challenging lighting conditions. \u2022 Accurate. We carefully labeled the images with ground truth poses, and errors in the labeled poses have been quantitatively assessed to be on the order of a few millimeters in world coordinates.\nThe dataset of test and validation images, with annotations for the latter, is available for download. Additionally, metrics against the test set annotations are computed by the BOP Benchmark evaluation server. We include initial results using both DOPE [4] and CosyPose [3], whose pre-trained network weights are released as an off-the-shelf system for use in robotics labs as well as a baseline method for further research in this area. We also provide 3D object meshes for generating synthetic data for training. We call our dataset HOPE, for Household Objects for Pose Estimation."
        },
        {
            "heading": "II. METHOD",
            "text": "In this section we describe the approach behind the dataset and benchmark."
        },
        {
            "heading": "A. Set of Objects",
            "text": "Because our goal is to support robotic manipulation research, it is imperative that the set of objects be: 1) somewhat realistic, 2) of the proper size and shape for grasping by a variety of robotic end effectors, and 3) accessible to researchers throughout the world. An emerging area in robotic manipulation is that of household robots for automating daily chores in a laundry room or kitchen. Such applications are crucial for facilitating aging-in-place, assisted living, and similar healthcare-related challenges. While we initially intended to scan real items from a grocery store, we quickly realized that such an approach comes with several fundamental limitations: 1) such objects are not widely available to researchers in other parts of the world, 2) the reflective metallic properties of many of these objects make for difficult scanning and rendering, and 3) the surface textures of realworld objects frequently change due to seasonal marketing campaigns. Again, it is not enough for us to provide a dataset for perception researchers to design and test their algorithms\u2014we also want to support robotics researchers who need to leverage the latest perception research in their own labs. As a result, the trained networks from the former should be immediately usable by the latter.\nTo this end, we chose a set of 28 toy grocery objects, depicted in Fig. 1. These toys are widely available online and can be purchased for less than 60 USD total. Because they are not real grocery items, there are no issues with perishability or transporting of the contents. Moreover, these\nobjects are of the proper size and shape for typical robotic grippers, with all objects having at least one dimension between 2.4 and 7.2 cm.\nB. 3D Textured Object Models\nEach object was scanned with a low-cost EinScan-SE desktop 3D scanner to generate textured 3D object meshes. The scanning process introduced another limitation on both the size and materials of the objects, due to the inability of the scanner to handle objects larger than about 20 cm on any side. The software tends to produce fragmented texture maps, so we used Maya to further refine the textures. Fig. 2 shows a synthetic rendering of the 3D textured object models, rendered using NViSII, a Python-interfaced ray tracing library [17]. Textured object models are useful for generating training data, as in DOPE [4] or BlenderProc [18]."
        },
        {
            "heading": "C. Capturing Real Images",
            "text": "To acquire images for the dataset, we placed the real objects in ten different environments, with five object arrangements / camera poses per environment. The environments are shown in Fig. 3, and the object arrangements for one environment are depicted in Fig. 4. These 50 different scenes exhibit a wide variety of backgrounds, clutter, poses, and lighting. With some arrangements, objects are placed inside other containers (e.g., boxes, bags, or drawers) to provide additional clutter and partial occlusion. Both RGB and depth images were collected for each scene using an Intel RealSense D415 RGBD camera at full HD resolution (1920\u00d7 1080). Images were captured from distances of 0.5 to 1.0 m, which are typical of robotic grasping.\nOnce the objects were arranged in an environment and the camera was set, we acquired multiple images with various lighting conditions by turning on/off the lights, opening window blinds, and so forth. Since the scene was static, the annotations (described below) do not change and thus required no additional work. In this manner we were able to collect a large variety of lighting conditions, see Fig. 5.\nIn total, the dataset contains 50 unique scenes, 238 images (an average of 4.8 lighting variations per scene), and 914 object poses. Images from two environments (\u201cchair\u201d) and (\u201cwindow 1\u201d) are made available with ground truth for a validation set, while the remaining scenes are reserved as the test set. Fig. 6 summarizes the object sizes, as well as the variety of object pose angles, estimated visibility, and distance to the camera across all pose instances in the entire dataset. Minimum and maximum object dimensions are in the ranges 2.4\u20137.2 cm and 6.8\u201325.0 cm, respectively. Average object visibility is 83%, with only 39% of objects in unoccluded poses (>95% visible). Objects are set in a wide variety of orientations, and object distances range from 39.0\u2013141.0 cm with an average distance of 73.8 cm."
        },
        {
            "heading": "D. Annotating Images with Ground Truth",
            "text": "We annotated the dataset with ground truth object poses by manually identifying point correspondences between images and 3D textured object models. In the PnP version of our\nannotation tool, the annotator selects corresponding points in the 2D texture map of the object model and the image in which the object appears, after which the 3D model is aligned to the image by PnP with RANSAC. Alternatively, in the RGBD version, correspondences are made between the 3D textured model and the 2.5D RGBD depth map, with alignment made by Procrustes. The RGBD annotation tool is typically faster to operate but may suffer from noise or bias in depth measurements, while PnP is more errorprone along the projection ray from the camera to the object. Most annotations were performed with the RGBD tool, resorting to PnP only when the depth-based annotation was not satisfactory. All annotations were automatically refined by SimTrack [19]. Then, if necessary, the alignment was refined manually until the reprojected model was visually aligned with both the image and the depth map."
        },
        {
            "heading": "E. Depth Calibration",
            "text": "Following Hodan\u030c et al. [20], we checked the depth calibration of the RealSense camera and found a small systematic\nerror. We captured over 50 images of a checkerboard at distances between 0.5 and 2 m using the RealSense leftinfrared camera. Checkerboard corners were automatically detected, then unprojected to 3D using PnP and the camera intrinsics. The resulting coordinates were compared with the corresponding 3D coordinates obtained from the 2.5D depth map. Fitting a scale factor of 0.9804 reduced the mean absolute difference between measurements from 19.3\nto 7.6 mm. The depth images included in the dataset were scaled by this factor before registering to RGB."
        },
        {
            "heading": "F. Symmetry-Aware Metrics",
            "text": "Perhaps the most common and easily interpretable metric for evaluating pose estimates is the average distance (ADD) metric [21], which computes the mean pairwise distance between corresponding vertices in the 3D object model under the ground truth (P\u0304) and predicted (P\u0302) poses:\neADD = mean x\u2208O\n\u2016P\u0304x\u2212 P\u0302x\u2016, (1)\nwhere x \u2208 O are the positions of model vertices in the object coordinate frame, and \u2016 \u00b7 \u2016 is the `2-norm.\nBecause ADD makes no allowances for object symmetries, it provides only limited insight into the graspability of detected objects based on the predicted pose, as translation errors are penalized similarly to rotation errors. Moreover, in the case of symmetric objects, ADD penalizes predictions even when the true pose cannot be determined from the input image. To overcome these limitations, some researchers adopt a mean closest point distance (ADD-S) [22]:\neADD-S = mean x1\u2208O min x2\u2208O\n\u2016P\u0304x1 \u2212 P\u0302x2\u2016, (2)\nwhere each vertex in the ground truth model is paired with the nearest vertex in the prediction model, irrespective of consistency with other assignments. As shown later in this section, this metric can significantly underestimate pose error due to unrealistic pairings.\nAnother way to handle symmetries is to directly model the set SO of symmetry transformations corresponding to the object. Starting from ADD in Eq. (1), this leads to the Mean Symmetry-Aware Surface Distance (MeanSSD):\neMeanSSD = min S\u2208SO mean x\u2208O\n\u2016P\u0304Sx\u2212 P\u0302x\u2016. (3)\nMeanSSD requires calculating or manually identifying all valid (discrete and/or continuous) symmetries for a given object, a potentially burdensome task for a large and varied set of objects. If the mean operator in Eq. (3) is replaced with max, MeanSSD becomes the Maximum Symmetry-Aware Surface Distance (MSSD) proposed by Hodan\u030c et al. [23]:\neMSSD = min S\u2208SO max x\u2208O\n\u2016P\u0304Sx\u2212 P\u0302x\u2016. (4)\nAs a compromise between the inaccurate ADD-S metric and the labor-intensive MeanSSD/MSSD metrics, we propose a variant of the average distance metric in which the vertex correspondences between ground truth and predicted models are made by solving a linear sum assignment problem. The most famous solution to this assignment problem is the Hungarian algorithm [24], hence we denote this metric ADD-H. The assignment fA : x2 7\u2192 x1 produces a bijective mapping from the vertices of the model in the predicted pose to those in the ground truth pose. If we let A = {(fA(x2),x2)} be the set of such correspondences, then\nthe set that minimizes the sum of distances between paired vertices is calculated as:\nA\u0303 = arg min A \u2211 (x1,x2)\u2208A \u2016P\u0304x1 \u2212 P\u0302x2\u2016. (5)\nWe compute Eq. (5) using a modern variation of the Hungarian algorithm.3 The average distance is then computed between assigned pairs (x1,x2) \u2208 A\u0303:\neADD-H = mean (x1,x2)\u2208A\u0303\n\u2016P\u0304x1 \u2212 P\u0302x2\u2016. (6)\nFig. 7 visualizes the vertex correspondences used by each metric when computing the error between the ground truth and predicted pose for a bottle. The inset rendering on the left depicts the scene, where the predicted pose (in translucent gray) is offset from the ground truth pose (with colored vertices) by a 150\u25e6 rotation about the vertical axis and a horizontal translation of about half the object width (3 cm).\nThe right side of the figure depicts the vertex assignments for each metric: vertices in each model (x2 \u2208 O) are colored according to the paired vertices in the ground truth model (x1 \u2208 O). Since ADD uses a fixed, bijective assignment based on the original vertex ordering, the error is large. In contrast, ADD-S assigns the nearest ground truth vertex to each target, resulting in a non-injective mapping, in which only points from the right side of the ground truth are used, as reflected in the green and purple color across the entire\n3Implemented as scipy.optimize.linear sum assignment. Note that versions of SciPy before 0.17.0 use a much slower algorithm.\ntarget mesh. MeanSSD uses the same vertex assignment as ADD but explicitly selects the symmetry-preserving rotation that minimizes error, resulting in a rotational alignment that is more representative of the grasp-relevant geometry of the prediction. Finally, ADD-H optimizes a bijective mapping between poses to minimize error\u2014achieving a vertex assignment that reflects the pose symmetry, similar to MeanSSD.\nTo further compare the metrics, Fig. 8 shows the error, averaged over 5 trials for each of the 28 HOPE objects, as each object is translated from its initial position in increments of 1 cm. In the left plot, each object is first randomly rotated by one of the symmetry-preserving transformations, whereas in the right plot, each object is first randomly rotated by an arbitrary transformation. ADD-H closely matches the MeanSSD error without any explicit enumeration of object symmetries, especially when the rotation is symmetrypreserving. In contrast, ADD and ADD-S significantly overand under-estimate the error, respectively.\nAlthough ADD-H is computationally more expensive than related methods, by using an efficient algorithm to solve the assignment problem it can be applied to meshes with several hundred vertices. In our experiments, we use 500 vertices, which yields consistent results with sufficiently fast computation."
        },
        {
            "heading": "III. EXPERIMENTS",
            "text": "We first present a validation experiment that examines the accuracy of our annotations. Then we provide a baseline experiment where we train several pose detectors and report accuracy metrics for the dataset."
        },
        {
            "heading": "A. Annotation Validation Experiment",
            "text": "To estimate the error in our ground truth pose annotations, we captured orthogonal views (\u223c90 degrees between camera\naxes) of three static scenes. Objects were annotated independently in both views, as shown in Fig. 9. Using a robust Procrustes alignment of all but one annotated object pose, we estimated the extrinsics between the two camera views. The pose of the held-out object was projected from the first view to the second, and ADD error was computed between the transferred pose and the annotation made directly in the second view. By repeating the process while holding out each object in turn, we estimated the annotation error for all objects in the scene. Fig. 10 shows a histogram of ADD for the 64 object instances contained in the three scenes. The mean and median ADD are 5.7 and 4.3 mm, respectively. We conclude, therefore, that ground truth is accurate to several millimeters."
        },
        {
            "heading": "B. Pose Prediction Baselines",
            "text": "We trained baseline pose prediction models for each object class using the Deep Object Pose Estimation (DOPE) [4] and CosyPose [3] methods for detecting and predicting object poses in RGB images. For DOPE, models were trained from synthetic images using domain randomization [25]. Unlike the original DOPE paper, we did not train using any photorealistic rendered images, which no doubt degrades performance. Accordingly, we denote the baseline as DOPEDR. Two architecture variants are considered with 50 \u00d7 50 and 400 \u00d7 400 output maps, denoted DOPE-DR-50 and DOPE-DR-400, respectively. For CosyPose, we trained using photorealistic images rendered by the BlenderProc tool [18].\nAs both DOPE and CosyPose are RGB-only methods, both can suffer from errors along the projection ray from the camera to the object. We consider a simple method for refinement using the RGB-D point clouds: given a predicted pose, visible model vertices are aligned with the depth map by adjusting the predicted translation by a scalar factor determined by a line search. We denote this method with \u201c-LS\u201d. CosyPose has a distinct advantage in that the object mask from the Mask R-CNN detector can be used to filter both the object mesh and RGB-D point cloud, thereby limiting the effect of occluding objects."
        },
        {
            "heading": "C. BOP Challenge",
            "text": "Table I shows the average recall (AR) under three metrics as computed by the BOP benchmark [14]: Maximum\n4If the vertices in two random poses have Gaussian errors, then the sum of their squared differences is given by a gamma distribution. If z follows a gamma distribution with parameters \u03b1 and \u03b2, then \u221a z follows a Nakagami distribution with parameters m = \u03b1 and \u2126 = \u03b1\u03b2. Thus the mean Euclidean distance in Eq. (1) is expected to follow a Nakagami distribution.\nSymmetry-Aware Surface Distance (MSSD), Visible Surface Discrepancy (VSD), and Maximum Symmetry-Aware Projection Distance (MSPD). MSSD, which measures 3D alignment of model vertices, is the most relevant to robotics applications. VSD measures the discrepancy between 2D distance maps, rendered for GT and predicted poses, respectively. MSPD measures the discrepancy between 2D projections of model vertices, thereby capturing visual alignment with an eye toward augmented reality applications.\nCosyPose-LS yields a nearly 2x improvement in ARMSSD over DOPE-400-LS. Furthermore, mistakes made along the projection ray to the object are shown to be a significant source of error, as evidenced by the dramatic improvement made by our simple depth refinement (-LS). The refinement improves MSSD and VSD scores dramatically for all three methods, including a 25% improvement in CosyPose. Because MSPD is sensitive to subtle changes in visual alignment, it suffers slightly under depth refinement despite overall better 3D alignment.\nTwo aspects of the BOP metrics are worth noting: 1) as mentioned previously, the BOP definition of symmetries requires both geometric and (nearly) exact visual similarity, meaning that none of the HOPE objects are considered symmetric in the BOP challenge; and 2) the recall error threshold is defined relative to object size, varying from 5% to 50% of the object diameter [23]."
        },
        {
            "heading": "D. Detailed Experiments",
            "text": "Because of our focus on graspability, we present additional results using absolute detection thresholds, varying from\n0.5 to 10 cm. We also consider all of our objects to be geometrically symmetric, ignoring small details like tabs on can lids. For defining geometric symmetries, meshes were manually aligned to a consistent set of coordinate axes and grouped into three categories: cylinders (360\u25e6 rotation around vertical axis in 1\u25e6 increments, and 180\u25e6 flip top to bottom), cuboids (180\u25e6 flip along any axis), and bottles (180\u25e6 flip front to back). Fig. 11 presents the results of four experiments, depicting the detection rate of ground truth objects (recall) on the y-axis while varying the maximum error threshold for a positive detection on the x-axis. We show results on both validation and test sets. To facilitate interpretability by those familiar with existing metrics, we use the MeanSSD metric.5\nIn the analysis that follows, we specifically highlight performance at two particular error thresholds: 2 cm and 10 cm. The smaller threshold (2 cm) approximately indicates errors sufficient for grasping, although this will of course vary by gripper. The larger threshold (10 cm) indicates errors sufficient for a system that is able to gather additional views of a detected object for refined results, e.g., when the camera is in the robot hand\u2014although this threshold also is an approximation.\nFig. 11 (a) compares CosyPose and DOPE using the MeanSSD metric. CosyPose-LS, trained with photorealistic images, incorporating a visual refinement step, and augmented with our simple depth refinement, predicts over 70%\n5For versions of Fig. 11 (a-c) and Table II using the ADD-H metric, see Fig. 12 and Table III, respectively, in the Appendix.\nof objects in the HOPE test set within 2 cm. Considering DOPE, the figure shows the orthogonal benefits of a larger output map (DOPE-DR-400-*)\u2014which improves the precision of keypoint prediction in the DOPE method\u2014and line search depth refinement (DOPE-DR-*-LS). With line search and depth refinement, more than 30% of objects in the HOPE test set are predicted within 2 cm.\nFig. 11 (b) shows the performance of CosyPose-LS using the MeanSSD metric across three object categories. Cuboids present a particular challenge since they are less resistant to small rotational errors. In this dataset, cuboids also tend to be larger objects that are placed further from the camera and behind other objects in some scene arrangements. Highest success is obtained with bottles, with almost 90% predicted within 2 cm.\nFig. 11 (c) depicts the effect of lighting variations by comparing detection rate (again using MeanSSD) between the most favorable image and most difficult image in each scene (determined subjectively). Particularly on the test set, which is much larger than the validation set, the difference in accuracy is small, indicating a promising level of robustness in these synthetically trained methods.\nFinally, Fig. 11 (d) compares the various metrics using CosyPose-LS predictions, confirming previous observations that ADD-H is a reasonable compromise that matches the results of MeanSSD without requiring explicit enumeration of object symmetries. As mentioned earlier, ADD overestimates, while ADD-S under-estimates, the error.\nFinally, Table II presents detailed statistics of CosyPoseLS predictions for each object class and category, using the MeanSSD metric, along with some statistics of the ground truth (GT) object poses. The table includes the median of the MeanSSD error among objects detected within a 10 cm threshold, as well as precision and recall at both 2 cm and 10 cm thresholds. At the most generous 10 cm detection threshold, 83% of ground truth objects (in the test set) are detected by CosyPose-LS, and at the tighter 2 cm threshold, 72% are detected, meaning that grasping may be feasible. CosyPose-LS could benefit from an improved object detection step, as nearly 20% of objects are not detected. False positives appear to be less of a problem, with >98% precision at the 10 cm threshold. These results offer room for improvement, while showing that an existing method like CosyPose can be reasonably successful as an off-the-shelf predictor for these objects."
        },
        {
            "heading": "IV. RELATIONSHIP TO PREVIOUS WORK",
            "text": "Many existing datasets focus on the task of 6-DoF pose estimation of known objects [22], [26], [21], [20], [14], [27], [28], [29]. The original LineMOD dataset [21], for example, offers manual object annotations for approximately 1000 images for each of the 15 objects in the dataset. The LineMODOccluded dataset consists of additional annotations of the original dataset [30]. T-LESS consists of real images of untextured manufacturing parts [20]. The falling things (FAT) dataset [26] consists of synthetically generated images of random YCB [15] objects which fall under the influence\nof simulated gravity in 3D scenes. The YCB-Video dataset provides a large number of images from video sequences, with high correlation between images and an average of five objects visible per image [22], [31]. The YCBInEOAT (\u201cin the end of arm tooling\u201d) dataset [12] consists of videos of 5 YCB objects, one at a time, being held and moved. The DexYCB dataset [32] contains multi-camera videos of humans grasping 20 YCB objects, one at a time. Our HOPE dataset contains 28 objects in 50 cluttered scenes, nearly 5 lighting variations per scene (for a total of 238 images), and an average of more than 18 objects per scene. The HOPE objects also appear in our HOPE-Video dataset [33], a collection of 10 short RGBD video sequences captured by a robot arm-mounted camera, each depicting 5-20 objects on a tabletop workspace.\nRecently the BOP (Benchmark for 6D Object Pose Estimation) challenge [14] proposed assembling multiple 6- DoF pose estimation datasets as a central resource for evaluating algorithms. It is composed of the following datasets: LineMOD [34], LineMOD-Occluded [30], T-LESS [20], ITODD [35], HomebrewedDB [36], YCB-Video [22], Rutgers APC [16], IC-BIN [37], IC-MI [38], TUD Light [14], Toyota Light [14], and now HOPE. With the addition of these HOPE objects, we believe that this benchmark is more relevant to robotics researchers."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work we offer a pose estimation benchmark that is immediately applicable to robotic manipulation research. Researchers can render synthetic images using the 3D textured meshes accompanying our dataset to train pose estimation models for 28 different objects. These models can be evaluated using our challenging, accurately annotated real-world images. Finally, pose predictions can be used directly in robotics labs for detecting and manipulating physical copies of the same objects, which are readily available from online retailers. We hope that this dataset will be a useful and practical bridge between researchers in computer vision and robotics."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "The authors would like to thank Toma\u0301s\u030c Hodan\u030c for his feedback and assistance throughout this work and for integrating the HOPE dataset into the BOP benchmark."
        },
        {
            "heading": "A. HOPE Object Set",
            "text": "The 28 toy grocery objects come from the following products, with links for purchasing:\n\u2022 Melissa & Doug Let\u2019s Play House! Grocery Shelf Boxes \u201cChocolate Pudding\u201d, \u201cCookies\u201d, \u201cGranola Bars\u201d, \u201cMac & Cheese\u201d, \u201cPopcorn\u201d, \u201cRaisins\u201d, and \u201cSpaghetti\u201d Links: Melissa & Doug, Amazon, Google Shopping\n\u2022 Melissa & Doug Let\u2019s Play House! Grocery Cans \u201cAlphabet Soup\u201d, \u201cCherries\u201d, \u201cCorn\u201d, \u201cGreen Beans\u201d, \u201cMushrooms\u201d, \u201cPeaches\u201d, \u201cPeas & Carrots\u201d, \u201cPineapple\u201d, \u201cTomato Sauce\u201d, and \u201cTuna\u201d Links: Melissa & Doug, Amazon, Google Shopping\n\u2022 Melissa & Doug Let\u2019s Play House! Fridge Fillers \u201cButter\u201d, \u201cCream Cheese\u201d, \u201cMilk\u201d, \u201cOrange Juice\u201d, \u201cParmesan\u201d, and \u201cYogurt\u201d Links: Melissa & Doug, Amazon, Google Shopping\n\u2022 Melissa & Doug Favorite Condiments \u201cBBQ Sauce\u201d, \u201cKetchup\u201d, \u201cMayo\u201d, \u201cMustard\u201d, and \u201cSalad Dressing\u201d Links: Melissa & Doug, Amazon, Google Shopping\nNote that some items included in these products (viz., \u201cCrackers\u201d, \u201cCrispy Crisps\u201d, \u201cPancake Mix\u201d, \u201cDeli Cheese Slices\u201d, and \u201cDeli Slice Meats\u201d) were omitted from our dataset because their sizes made scanning difficult."
        },
        {
            "heading": "B. Results with ADD-H Metric",
            "text": "Here we include versions of both Fig. 11 (a-c) and Table II using the ADD-H metric instead of MeanSSD."
        }
    ],
    "title": "6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark",
    "year": 2022
}