{
    "abstractText": "This paper investigated the finite-time extended dissipativity for generalized neural networks with discrete and distributed time-varying delays via the improved Lyapunov-Krasovskii functional (LKF). We constructed an appropriate LKF by employing more neural network information and consisting of quadratic functions. By combining the proposed LKF, Jensen\u2019s integral inequality, orthogonal polynomialsbased integral inequality, and extended Wirtinger\u2019s integral inequality, new delay-dependent conditions are achieved in the form of linear matrix inequalities (LMIs), which can be verified via MATLAB\u2019s LMI toolbox. In addition, we concentrate on the extended dissipative analysis problem, which is a unified formulation of L2 \u2212L\u221e, H\u221e, passivity, and dissipative performance. This paper is less conservative delay bound than some recently published literature by stability criteria. In addition, we presented seven numerical examples to illustrate the effectiveness of the obtained results. INDEX TERMS Extended dissipative, Neural networks, Time-varying delays, Finite-time bounded, Lyapunov-Krasovskii functional.",
    "authors": [
        {
            "affiliations": [],
            "name": "CHALIDA PHANLERT"
        },
        {
            "affiliations": [],
            "name": "THONGCHAI"
        },
        {
            "affiliations": [],
            "name": "WAJAREE WEERA"
        }
    ],
    "id": "SP:67bb48bee2d83c00b0f4864d9a71a12c3ed4a845",
    "references": [
        {
            "authors": [
                "T.H. Lee",
                "J.H. Park",
                "M.J. Park",
                "O.M. Kwon",
                "H.Y. Jung"
            ],
            "title": "On stability criteria for neural networks with time-varying delay using Wirtinger-based multiple integral inequality",
            "venue": "J. Franklin Inst, vol. 352, no. 12, pp. 5627\u2013 5645, Sep. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Cheng",
                "S. Zhong",
                "Q. Zhong",
                "H. Zhu",
                "Y. Du"
            ],
            "title": "Finite-time boundedness of state estimation for neural networks with time-varying delays",
            "venue": "Neurocomputing, vol. 129, pp. 257\u2013264, Dec. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Shanmugam",
                "S.A. Muhammed",
                "G.M. Lee"
            ],
            "title": "Finite-time extended dissipativity of delayed Takagi\u2013Sugeno fuzzy neural networks using a free-matrix-based double integral inequality",
            "venue": "Neural Comput. Appl., vol. 32, pp. 8517\u20138528, Jul. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Saravanan",
                "M.S. Ali",
                "A. Alsaedi",
                "B. Ahmad"
            ],
            "title": "Finite-time passivity for neutral-type neural networks with time-varying delays\u2013via auxiliary function-based integral inequalities",
            "venue": "Nonlinear Anal, vol. 25, no. 2, pp. 206\u2013224, Mar. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Feng",
                "H. Shao",
                "L. Shao"
            ],
            "title": "Further improved stability results for generalized neural networks with time-varying delays",
            "venue": "Neurocomputing, vol. 367, pp. 308\u2013318, Jul. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Lv",
                "X. Li"
            ],
            "title": "Delay-dependent dissipativity of neural networks with mixed non-differentiable interval delays",
            "venue": "Neurocomputing, vol. 267, pp. 85\u201394, Apr. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Li",
                "G. Feng"
            ],
            "title": "Delay-interval-dependent stability of recurrent neural networks with time-varying delay",
            "venue": "Neurocomputing, vol. 72, pp. 1179\u2013 1183, Jan. 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Liu",
                "X. Liu"
            ],
            "title": " On global asymptotic stability of neural networks with discrete and distributed delays",
            "venue": "Physics Letters A, vol. 345, pp. 299\u2013308, Oct. 2005.",
            "year": 2005
        },
        {
            "authors": [
                "R. Manivannan",
                "R. Samidurai",
                "J. Cao",
                "A. Alsaedi",
                "F.E. Alsaadi"
            ],
            "title": "Non-fragile extended dissipativity control design for generalized neural networks with interval time-delay signals",
            "venue": "Asian J. Control, vol. 21, no. 1, pp. 559\u2013580, Mar. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Niamsup",
                "K. Ratchagit",
                "V.N. Phat"
            ],
            "title": "Novel criteria for finite-time stabilization and guaranteed cost control of delayed neural networks",
            "venue": "Neurocomputing, vol. 160, pp. 281\u2013286, Feb. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H.B. Zeng",
                "Y. He",
                "M. Wu",
                "S.P. Xiao"
            ],
            "title": "Stability analysis of generalized neural networks with time-varying delays via a new integral inequality",
            "venue": "Neurocomputing, vol. 161, no. 1, pp. 148\u2013154, Aug. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Sun",
                "J. Chen"
            ],
            "title": "Stability analysis of static recurrent neural networks with interval time-varying delay",
            "venue": "Appl. Math. Comput, vol. 221, pp. 111\u2013 120, Sep. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Chen",
                "J. Sun",
                "G.P. Liu",
                "D. Rees"
            ],
            "title": "New delay-dependent stability criteria for neural networks with time-varying interval delay",
            "venue": "Phys. Lett. A, vol. 374, no. 43, pp. 4397\u20134405, Sep. 2010.",
            "year": 2010
        },
        {
            "authors": [
                "X.M. Zhang",
                "Q.L. Han"
            ],
            "title": "Global asymptotic stability for a class of generalized neural networks with interval time-varying delays",
            "venue": "IEEE trans. neural netw, vol. 22, no. 8, pp. 1180\u20131192, Jun. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "Z.W. Chen",
                "J. Yang",
                "S.M. Zhong"
            ],
            "title": "Delay-partitioning approach to stability analysis of generalized neural networks with time-varying delay via new integral inequality",
            "venue": "Neurocomputing, vol. 191, pp. 380\u2013387, May. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J.A. Wang",
                "L. Fan",
                "X.Y. Wen",
                "Y. Wang"
            ],
            "title": "Enhanced stability results for generalized neural networks with time-varying delay",
            "venue": "J Franklin Inst, vol.357 , no. 11, pp. 6932\u20136950, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Phanlert",
                "T. Botmart",
                "W. Weera",
                "P. Junsawang"
            ],
            "title": "Finite-Time Mixed H\u221e/passivity for neural networks with mixed interval time-varying delays using the multiple integral Lyapunov-Krasovskii functional",
            "venue": "IEEE Access, vol. 9, pp. 89461-89475, Jun. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "O.M. Kwon",
                "M.J. Park",
                "J.H. Park",
                "S.M. Lee",
                "E.J. Cha"
            ],
            "title": "On less conservative stability criteria for neural networks with time-varying delays utilizing Wirtinger-based integral inequality",
            "venue": "Math. Probl. Eng, vol. 2014, pp. 1\u201313, Jun. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H.B. Zeng",
                "Y. He",
                "M. Wu",
                "J. She"
            ],
            "title": "New results on stability analysis for systems with discrete distributed delay",
            "venue": "Automatica, vol. 60, pp. 189-192, Oct. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Shi",
                "H. Zhu",
                "S. Zhong",
                "Y. Zeng",
                "Y. Zhang",
                "W. Wang"
            ],
            "title": "Stability analysis of neutral type neural networks with mixed time-varying delays using triple-integral and delay-partitioning methods",
            "venue": "ISA Trans, vol. 58, pp. 85\u201395, Sep. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Briat"
            ],
            "title": "Convergence and equivalence results for the Jensen\u2019s inequality- Application to time-delay and sampled-data systems",
            "venue": "IEEE Trans. Autom. Control, vol. 56, no. 7, pp. 1660\u20131665, Feb. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "X. Wang",
                "K. She",
                "S. Zhong",
                "J. Cheng"
            ],
            "title": "On extended dissipativity analysis for neural networks with time-varying delay and general activation functions",
            "venue": "Adv. Differ. Equ, vol. 2016, no. 1, pp. 1\u201316, Mar. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R. Manivannan",
                "G. Mahendrakumar",
                "R. Samidurai",
                "J. Cao",
                "A. Alsaedi"
            ],
            "title": "Exponential stability and extended dissipativity criteria for generalized neural networks with interval time-varying delay signals",
            "venue": "Asian J. Control, vol. 354, no. 11, pp. 4353\u20134376, Apr. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Manivannan",
                "R. Samidurai",
                "J. Cao",
                "A. Alsaedi",
                "F.E. Alsaadi"
            ],
            "title": "Non-fragile extended dissipativity control design for generalized neural networks with interval time-delay signals",
            "venue": "Asian J. Control, vol. 2 no. 1, pp. 559\u2013580, May. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Saravanakumar",
                "H. Mukaidani",
                "P. Muthukumar"
            ],
            "title": "Extended dissipative state estimation of delayed stochastic neural networks",
            "venue": "Neurocomputing, vol. 406, pp. 244\u2013252, Apr. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Xiao",
                "S. Zhong"
            ],
            "title": "Extended dissipative conditions for memristive neural networks with multiple time delays",
            "venue": "Appl. Math. Comput, vol. 323, pp. 145-163, Apr. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liu",
                "Z. Deng",
                "P. Li",
                "B. Zhang"
            ],
            "title": "Finite-time non-fragile extended dissipative control of periodic piecewise time-varying systems",
            "venue": "IEEE Access, vol. 8, pp. 136512\u2013136523, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Dorato"
            ],
            "title": "Short-time stability in linear time-varying systems",
            "venue": "Proc. IRT Int. Conv. Rec, vol. 4, pp. 83-\u201387, Jun. 1961.",
            "year": 1961
        },
        {
            "authors": [
                "F. Amato",
                "M. Ariola",
                "P. Dorato"
            ],
            "title": "Finite-time control of linear systems subject to parametric uncertainties and disturbances",
            "venue": "Automatica, vol. 37, no. 9, pp. 1459\u20131463, Sep. 2001.",
            "year": 2001
        },
        {
            "authors": [
                "S. He",
                "Q. Ai",
                "C. Ren",
                "J. Dong",
                "F. Liu"
            ],
            "title": "Finite-time resilient controller design of a class of uncertain nonlinear systems with time-delays under asynchronous switching",
            "venue": "IEEE Trans. Syst. Man Cybern. Syst, vol. 49, no. 2, pp. 281\u2013286, Feb. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Liu",
                "J.H. Park",
                "N. Jiang",
                "J. Cao"
            ],
            "title": "Nonsmooth finite-time stabilization of neural networks with discontinuous activations",
            "venue": "Neural Netw, vol. 52, pp. 25\u201332, Apr. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M.S. Ali",
                "S. Saravanan"
            ],
            "title": "Robust finite-time H\u221e control for a class of uncertain switched neural networks of neutral-type with distributed timevarying delays",
            "venue": "Neurocomputing, vol. 177, pp. 454\u2013468, Feb. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. He",
                "F. Liu"
            ],
            "title": "Optimal finite-time passive controller design for uncertain nonlinear Markovian jumping systems",
            "venue": "J. Franklin Inst, vol. 351, no. 7, pp. 3782\u20133796, Jul. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Liu",
                "Z. Deng",
                "P. Li",
                "B. Zhang"
            ],
            "title": "Finite-time non-fragile extended dissipative control of periodic piecewise time-varying systems",
            "venue": "IEEE Access, vol. 8, pp. 136512\u2013136523, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Hua",
                "Y. Wang",
                "S. Wu"
            ],
            "title": "Stability analysis of neural networks with time-varying delay using a new augmented Lyapunov\u2013Krasovskii functional",
            "venue": "Neurocomputing, vol. 332, pp. 1\u20139, Mar. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Phanlert",
                "T. Botmart",
                "W. Weera",
                "P. Junsawang"
            ],
            "title": "Finite-Time Mixed H\u221e/passivity for neural networks with mixed interval time-varying delays using the multiple integral Lyapunov-Krasovskii functional",
            "venue": "IEEE Access, vol. 9, pp. 89461\u201389475, Jun. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Zamart",
                "T. Botmart",
                "W. Weera",
                "S. Charoensin"
            ],
            "title": "New delaydependent conditions for finite-time extended dissipativity based nonfragile feedback control for neural networks with mixed interval timevarying delays",
            "venue": "Math Comput Simul, Vol. 201, pp. 684\u2013713, Nov. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Shao",
                "H. Li",
                "C. Zhu"
            ],
            "title": "New stability results for delayed neural networks",
            "venue": "Appl. Math. Comput., Vol. 311, pp. 324\u2013334, Oct. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Shao",
                "H. Li",
                "L. Shao"
            ],
            "title": "Improved delay-dependent stability result for neural networks with time-varying delays",
            "venue": "ISA Trans, vol. 80, pp. 35\u201342, Sep. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.D. Ji",
                "Y. He",
                "C.K. Zhang",
                "M. Wu"
            ],
            "title": "Novel stability criteria for recurrent neural networks with time-varying delay",
            "venue": "Neurocomputing, vol. 138, pp. 383\u2013391, Aug. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "B. Wang",
                "J. Yan",
                "J. Cheng",
                "S. Zhong"
            ],
            "title": "New criteria of stability analysis for gen- eralized neural networks subject to time-varying delayed signals",
            "venue": "Appl. Math. Comput, vol. 314, pp. 322\u2014333, Dec. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.K. Zhang",
                "Y. He",
                "L. Jiang",
                "W.J. Lin",
                "M. Wu"
            ],
            "title": "Delay-dependent stability analysis of neural networks with time-varying delay: A generalized free-weighting-matrix approach",
            "venue": "Appl. Math. Comput, vol. 294, pp. 102\u2013120, Feb. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X.M. Zhang",
                "Q.L. Han"
            ],
            "title": "Global asymptotic stability analysis for delayed neural networks using a matrix-based quadratic convex approach",
            "venue": "Neural Netw, vol. 54, pp. 57\u201369, Jun. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M.J. Park",
                "S.H. Lee",
                "O.M. Kwon",
                "J.H. Ryu"
            ],
            "title": "Enhanced stability criteria of neural networks with time-varying delays via a generalized freeweighting matrix in- tegral inequality",
            "venue": "J. Frankl. Inst, vol. 355, no. 14, pp. 6531\u20146548, Sep. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X.L. Zhu",
                "D. Yue",
                "Y. Wang"
            ],
            "title": "Delay-dependent stability analysis for neural networks with additive time-varying delay components",
            "venue": "IET Control Theory Appl, vol. 7, no. 3, pp. 354\u2014362, Feb. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "T. Li",
                "T. Wang",
                "A. Song",
                "S. Fei"
            ],
            "title": "Combined convex technique on delay-dependent stability for delayed neural networks",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst, vol. 24, no. 9, pp. 1459\u20131466, Apr. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "C.K. Zhang",
                "Y. He",
                "L. Jiang",
                "M. Wu"
            ],
            "title": "Stability analysis for delayed neural net- works considering both conservativeness and complexity",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 7, pp. 1486\u20141501, Jul. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C.K. Zhang",
                "Y. He",
                "L. Jiang",
                "W.J. Lin",
                "M. Wu"
            ],
            "title": "Delay-dependent stability analysis of neural networks with time-varying delay: A generalized free-weighting-matrix approach",
            "venue": "Appl. Math. Comput, vol. 294, pp. 102\u2013120, Feb. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "O.M. Kwon",
                "J.H. Park",
                "S.M. Lee",
                "E.J. Cha"
            ],
            "title": "New augmented Lyapunov\u2013Krasovskii functional approach to stability analysis of neural networks with time-varying delays",
            "venue": "Nonlinear Dyn, vol. 76, no. 1, pp. 221\u2013236, Nov. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "B. Yang",
                "J. Wang",
                "J. Wang"
            ],
            "title": "Stability analysis of delayed neural networks via a new integral inequality",
            "venue": "Neural Netw, vol. 88, no. 1, pp. 49\u201357, Apr. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Hu",
                "H. Gao",
                "W.X. Zheng"
            ],
            "title": "Novel stability of cellular neural networks with interval time-varying delay",
            "venue": "Neural Networks, Vol. 21, no. 10, pp. 1458\u20131463, Dec. 2008.",
            "year": 2008
        },
        {
            "authors": [
                "T. Li",
                "A. Song",
                "M. Xue",
                "H. Zhang"
            ],
            "title": "Stability analysis on delayed neural networks based on an improved delay-partitioning approach",
            "venue": "J. Comput. Appl. Math, Vol. 235, no. 9, pp. 3086\u20133095, Mar. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "O.M. Kwon",
                "S.M. Lee",
                "J.H. Park",
                "E.J. Cha"
            ],
            "title": "New approaches on stability criteria for neural networks with interval time-varying delays",
            "venue": "Appl. Math. Comput, Vol. 218, no. 19, pp. 9953\u20139964, Jun. 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J.K. Tian",
                "Y.M. Liu"
            ],
            "title": "Improved delay-dependent stability analysis for neural networks with interval time-varying delays",
            "venue": "Math. Probl. Eng, Vol. 2015, pp. 1\u201310, Nov. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H.B. Zeng",
                "X.G. Liu",
                "W. Wang"
            ],
            "title": "A generalized free-matrix-based integral inequality for stability analysis of time-varying delay systems",
            "venue": "Appl. Math. Comput., Vol. 354, pp. 1\u20138, Aug. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.C. Lin",
                "H.B. Zeng",
                "X.M. Zhang",
                "W. Wang"
            ],
            "title": "Stability analysis for delayed neural networks via a generalized reciprocally convex inequality",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst., early access, Feb. 02, 2022, doi:10.1109/TNNLS.2022.3144032.",
            "year": 2022
        },
        {
            "authors": [
                "H.B. Zeng",
                "H.C. Lin",
                "Y. He",
                "K.L. Teo",
                "W. Wang"
            ],
            "title": "Hierarchical stability conditions for time-varying delay systems via an extended reciprocally convex quadratic inequality",
            "venue": "J. Frank. Inst., Vol. 357, no. 14, pp. 9930- 9941, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.B. Zeng",
                "H.C. Lin",
                "Y. He",
                "C.K. Zhang",
                "K.L. Teo"
            ],
            "title": "Improved negativity condition for a quadratic function and its application to systems with time-varying delay",
            "venue": "IET Control Theory Appl., Vol. 14, no. 18, pp. 2989-2993, Oct. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.B. Zeng",
                "Z.L. Zhai",
                "W. Wang"
            ],
            "title": "Hierarchical stability conditions of systems with time-varying delay. Applied Mathematics and Computation",
            "venue": "Appl. Math. Comput., Vol. 404, 126222, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Liu",
                "A. Seuret",
                "Y. Xia"
            ],
            "title": "Stability analysis of systems with timevarying delays via the second-order Bessel\u2013Legendre inequality",
            "venue": "Automatica, Vol. 76, pp. 138-142, Feb. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Seuret",
                "F. Gouaisbaut"
            ],
            "title": "Stability of linear systems with timevarying delays using Bessel\u2013Legendre inequalities",
            "venue": "IEEE Trans. Autom. Control, Vol. 63, no. 1, pp. 225-232, Jul. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.K. Zhang",
                "Y. He",
                "L. Jiang",
                "M. Wu",
                "Q.G. Wang"
            ],
            "title": "An extended reciprocally convex matrix inequality for stability analysis of systems with time-varying delay",
            "venue": "Automatica, Vol. 85, pp. 481-485, Nov. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.K. Zhang",
                "F. Long",
                "Y. He",
                "W. Yao",
                "L. Jiang",
                "M. Wu"
            ],
            "title": "A relaxed quadratic function negative-determination lemma and its application to time-delay systems",
            "venue": "Automatica, Vol. 113, 108764, Mar. 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Extended dissipative, Neural networks, Time-varying delays, Finite-time bounded, Lyapunov-Krasovskii functional.\nI. INTRODUCTION\nIN the last two decades, neural networks (NNs) havebeen extensively investigated because of their successful applications in many practical systems, such as pattern recognition, signal processing, associative memories, and other engineering and scientific areas [1]\u2013[5]. In the process of investigating neural networks, time delays are unavoidable as a result of the dynamical behaviors of networks generating instability, oscillation, divergence, the inherent communication time between neurons, and the finite switching speed of amplifiers [6]\u2013[8]. Therefore, the stability of neural networks with a time-varying delay has received considerable attention from many researchers [9]\u2013[11]. The stability criteria developed for DNNs can be divided into two categories: delay-independent ones and delay-dependent ones. Since the delay-dependent conditions, which include the size information of time-delayed are usually less conservative than\ndelay-independent ones, especially for neural networks with small delays, more attentions have been paid to the delaydependent stability analysis of time delay neural network [1]\u2013[11].\nRecent studies have examined the dynamical behaviors of static neural networks (SNNs) [12] or local field neural networks (LFNNs) [13] separately due to differences in a neuron or local field state. In addition, these two models are not equivalent, but they can be combined into a more concise model by making reasonable assumptions. Thus, Zhang and Han [14] created the first unified system model, generalized neural networks (GNNs), which incorporated both LFNNs and SNNs. Furthermore, in recent years, there has been a heightened interest in analyzing the stability and performance of GNNs with time delay [15]\u2013[17].\nThe neural network stability problem is to find a less conservative condition that guarantees the system\u2019s stability. The\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLyapunov-Krasovskii functional (LKF) and many inequality techniques have been widely used to reduce the conservatism of stability criteria [4], [5]. For example, Jensen\u2019s integral inequality was presented to determine the new stability conditions for the NNs [5]. To obtain the conditions with the decline of conservatism, Wirtinger\u2019s integral inequality and reciprocally convex optimization are presented [18]. The free-weighting-based inequality has been shown as a powerful tool for analyzing the stability problem of NNs [11]. The orthogonal polynomials-based inequality was first introduced as an effective tool for analyzing the stability problem of NNs [19]. In addition, to derive better conditions, various types of LKF have been adopted, for instance, multiple integralsbased LKF [20], activation function-based LKF [21], and so on.\nRecently, the performances of a neural network, which are usually characterized by an input-output relationship, played an important role in various science and engineering applications, such asH\u221e control problem, passivity, and passification problems, L2 \u2212L\u221e performance, and dissipativity performance [22]. Up to now, a lot of researchers have paid increasing attention to the dissipativity analysis since it does not only linked with the H\u221e and passivity performance but also recommends a good comfortable control structure in many engineering applications, such as electrical networks, nonlinear control, power converters, and chemical process control [23]. Recently, the (Q,S,R)-dissipativity concept has been proposed in [22], [24]. However, the L2 \u2212 L\u221e performance is not contained in the (Q,S,R)-dissipativity. In order to overcome this problem, Zhang et al. [25] introduced a more general performance called extended dissipativity which can integrate several well-known performance indices such as passivity performance, (Q,S,R)-dissipativity performance, H\u221e performance, and L2 \u2212 L\u221e performance in a unified framework by setting the corresponding values of weighting matrices [24]\u2013[27]. More recently, the issue of the extended dissipative analysis has been applied to some NNs [22]\u2013[25].\nIn the previous decades, the existing literature has typically been concerned with asymptotic stability, which is defined over an infinite-time interval. Nonetheless, there is a bound for system trajectories over a fixed short time interval in some practical applications, such as rockets and airplanes, rather than asymptotic stability over an infinite-time interval. Our main objective lies in the behavior of dynamic systems over a given finite-time interval. More clearly, the state of dynamic systems does not exceed a special threshold of its state space for a given a priori bound of its initial state in a short time interval, which is called finite-time stability (FTS). In 1961, Dorato [28] first introduced the concept of FTS to the control framework. Subsequent work by Amato et al. [29] extends FTS to finite-time bound (FTB) by taking external disturbances into account. The FTS and FTB for NNs with time-varying delays have received a lot of attention [30]\u2013 [34].\nIn this paper, Jensen\u2019s integral inequality, orthogo-\nnal polynomials-based integral inequality, and extended Wirtinger\u2019s integral inequality are used to study finite-time extended dissipativity for generalized neural networks with mixed discrete and distributed time-varying delay problems. In addition, numerical examples are provided to demonstrate the efficiency of the theorems. Finally, numerical examples are presented to demonstrate the feasibility and effectiveness of the theorem. In addition, the major contributions and highlights of this paper are summarized in the following key points:\n\u2022 We investigate finite-time extended dissipativity for generalized neural network problems with distributed and discrete time-varying delays. \u2022 An enhanced LKF is constructed by optimizing the information of the time delay neural network as follows: firstly, the time-varying delay and its maximum are all employed, together with the activation function, the state, and its derivative. Secondly, the LKF includes more cross terms among the state, the integral of the state, the integral of the derivative of the state, terms among the state, the delayed state, the activation function, and the integral of the activation function. \u2022 We estimate the bound of the time derivative of LKF using Jensen\u2019s integral inequality, an extended Wirtinger\u2019s integral inequality, and orthogonal polynomials-based integral inequality, which results in less conservatism than the other references, as demonstrated by numerical examples.\nThe framework of this paper is structured as follows: In Section 2, the system model, definitions, assumptions, and lemmas are described. Section 3 presents the main results, which include finite-time stability, finite-time boundedness, and finite-time extended dissipativity. Section 4 provides seven numerical examples to demonstrate the effectiveness of the obtained criteria. Finally, in Section 5, we present the conclusion of our work.\nNotations: This paper contains the following notations, Rn denotes the n\u2212 dimensional Euclidean space, and Rm\u00d7n is the set of all m \u00d7 n real matrices. Sn, S+n are the set of symmetric and positive definite n \u00d7 n real matrices, respectively. PT and P\u22121 indicate the matrix P transport and matrix P inverse. The symmetric matrix P refers to P = PT . The matrix P is positive definite that the symmetric matrix P > 0. \u03bbmin(P ) and \u03bbmax(P ) are the minimum and maximum eigenvalues for real symmetric matrix P , respectively. diag{. . .} denotes the block diagonal matrix. Sym{P} = P + PT . \u22c6 represents the symmetric forms in a symmetric matrix.\n2 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nII. PRELIMINARIES Consider the following generalized neural networks with discrete and distributed time-varying delays:\nz\u0307(t) = \u2212A0z(t) +A1f(Wz(t)) +A2g(Wz(t\u2212 \u03c4(t)))\n+A3 \u222b t t\u2212\u03b3(t) h(Wz(s))ds+A4\u03c9(t),\ny(t) = B0z(t),\nz(t) = \u03d5(t), t \u2208 [\u2212\u03c4, 0], (1)\nwhere z(t) = [z1(t), z2(t), . . . , zn(t)]T \u2208 Rn is the neuron state vector; A0 = diag{a1, a2, . . . , an} with ai > 0 is a positive diagonal matrix; A1, A2, and A3 are the connection weight matrices; A4 is the connection disturbance; f(Wz(\u00b7)) = [f1(W1z(\u00b7)), f2(W2z(\u00b7)), . . . , fn(W2z(\u00b7))]T , g(Wz(\u00b7)) = [g1(W1z(\u00b7)), g2(W2z(\u00b7)), . . . , gn(W2z(\u00b7))]T and h(Wz(\u00b7)) = [h1(W1z(\u00b7)), h2(W2z(\u00b7)), . . . , hn(W2z(\u00b7))]T are the neuron activation functions with Wi denoting the ith row of W ; \u03c9(t) \u2208 Rn is the external disturbance vector that belongs to the class L2[0,\u221e); y(t) is the output vector of the system; B0 is known real constant matrices of suitable dimension; \u03d5(t) is the initial function; The variable \u03c4(t) and \u03b3(t) represent the discrete and distributed time-varying delays, respectively. \u03c4(t) is an discrete time-varying differentiable function satisfying\n0 \u2264 \u03c4(t) \u2264 \u03c4, \u03c4\u0307(t) \u2264 \u03c4d, (2)\n\u03b3(t) is an distributed time-varying satisfying\n0 \u2264 \u03b3(t) \u2264 \u03b3d. (3)\nAssumption 1. [9]: The activation function fi(Wiz(\u00b7))(i = 1, 2, . . . , n) is continuous and bounded satisfying the following inequality\nF\u2212i \u2264 fi(u)\u2212 fi(v)\nu\u2212 v \u2264 F+i ,\nu, v \u2208 R, u \u0338= v where fi(0) = 0, F\u2212i and F + i are known real scalars.\nFor the convenience of presentation, we denote\nFm = diag{ F\u22121 + F + 1 2 , F\u22122 + F + 2 2 , . . . , F\u2212n + F + n 2 },\nFp = diag { F+1 , F + 2 , . . . , F + n } ,\nGm = diag\n{ G\u22121 +G + 1\n2 , G\u22122 +G + 2 2 , . . . , G\u2212n +G + n 2\n} ,\nGp = diag { G+1 , G + 2 , . . . , G + n } ,\nHm = diag\n{ H\u22121 +H + 1\n2 , H\u22122 +H + 2 2 , . . . , H\u2212n +H + n 2\n} ,\nHp = diag { H+1 , H + 2 , . . . ,H + n } .\nRemark 1. The neuron activation functions may be nondifferentiable, non-monotonic, and unbounded by the timevarying delay. The variablesF\u2212i ,F + i ,G \u2212 i ,G + i ,H \u2212 i , andH + i can all be zero, positive, or negative. Notably, the assumption\nused in this study is weaker and more general than the usual Lipschitz condition, |f(u) \u2212 f(v)| \u2264 F |u \u2212 v|. Therefore, our stability criteria with Assumption 1 are less conservative compared to the usual Lipschitz condition.\nAssumption 2. [35]: For any positive constant \u03c9f and time constant Tf , the external disturbance satisfies\u222b Tf\n0\n\u03c9T (t)\u03c9(t)dt \u2264 \u03c9f .\nAssumption 3. [35]: For any time constant Tf , the state vector of time-varying z(t) satisfies\u222b Tf\n0\nzT (t)z(t)dt \u2264 d,\nwhere d denotes a sufficiently large fixed constant.\nAssumption 4. [35]: For any matrices \u21261, \u21262, \u21263 and \u21264 satisfy the following conditions:\n1) \u21261 = \u2126T1 \u2264 0, 2) \u21263 = \u2126T3 > 0, 3) \u21264 = \u2126T4 \u2265 0, 4) (\u2225\u21261\u2225+ \u2225\u21262\u2225)\u21264 = 0.\nDefinition 1. [35]: For any matrices \u21261, \u21262, \u21263 and \u21264 satisfying Assumption 4, system (1) is said to be extended dissipativity performance if the following inequality holds for any Tf > 0 and for all \u03c9(t) \u2208 L2[0,\u221e):\u222b Tf\n0 J(t)dt\u2212 sup 0\u2264t\u2264Tf yT (t)\u21264y(t) \u2265 0, (4)\nwhere J(t) = yT (t)\u21261y(t)+2yT (t)\u21262\u03c9(t)+\u03c9T (t)\u21263\u03c9(t).\nRemark 2. The concept of extended dissipativity performance proposed in Definition 1 contains some well-known performances as special cases by adjusting the weighting matrices \u21261, \u21262, \u21263, \u21264 and given constant matrices Q \u2208 Rn\u00d7n, S \u2208 Rn\u00d7n, and R \u2208 Rn\u00d7n with Q and R symmetric as follows:\n\u2022 If \u21261 = \u2212I , \u21262 = 0, \u21263 = \u03b32I and \u21264 = 0, then Definition 1 refers to the H\u221e performance; \u2022 If \u21261 = 0, \u21262 = 0, \u21263 = \u03b32I and \u21264 = I , then Definition 1 refers to the L2 \u2212 L\u221e performance; \u2022 If \u21261 = 0, \u21262 = I , \u21263 = \u03b3I and \u21264 = 0, then Definition 1 refers to the passivity performance; \u2022 If \u21261 = Q, \u21262 = S, \u21263 = R \u2212 \u03b2I and \u21264 = 0, then Definition 1 refers to the (Q,S,R)-dissipativity performance.\nDefinition 2. [Finite-time bounded] [10]: The system (1) is finite-time bounded with reference to (c1, c2, Tf , V, \u03c9f ) with time constant Tf > 0, a matrix V > 0, and numbers c2 > c1 > 0, \u03c9f > 0, if the following inequality holds:\nsup \u2212\u03c4\u2264s\u22640\n{zT (s)V z(s), z\u0307T (s)V z\u0307(s)} \u2264 c1\n\u21d2 zT (t)V z(t) < c2,\u2200t \u2208 [0, Tf ].\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDefinition 3. [Finite-time stable] [10]: For a given time Tf > 0, numbers c2 > c1 > 0, and a matrix V > 0, the system (1) with \u03c9(t) = 0 is finite-time stable with respect to (c1, c2, Tf , V ), if the following inequality holds:\nsup \u2212\u03c4\u2264s\u22640\n{zT (s)V z(s), z\u0307T (s)V z\u0307(s)} \u2264 c1\n\u21d2 zT (t)V z(t) < c2,\u2200t \u2208 [0, Tf ].\nLemma 1. [5]: For any matrix R1 \u2208 S+n1, R2 \u2208 S + n2, \u03b1 \u2208 (0, 1) and any matrix Z \u2208 R(n1+n2)\u00d7(n1+n2) the following inequality holds:[\n1 \u03b1R1 0 0 11\u2212\u03b1R2\n] \u2265 Z + ZT\n\u2212 Z [ \u03b1R\u221211 0 0 (1\u2212 \u03b1)R\u221212 ] ZT .\nLemma 2. [Jensen\u2019s integral inequality] [5]: For a given matrix R > 0 scalar \u03b11 < \u03b12 and vector z : [\u03b11, \u03b12] \u2192 Rn such that the following integrals are well defined, then the inequality holds:\n(\u03b12 \u2212 \u03b11) \u222b \u03b12 \u03b11 zT (s)Rz(s)ds\n\u2265 \u222b \u03b12 \u03b11 zT (s)dsR \u222b \u03b12 \u03b11 z(s)ds.\nLemma 3. [orthogonal polynomials-based integral inequality] [36]: Let z(s) be a differentiable function z : [\u03b11, \u03b12] \u2192 Rn for any matrices R \u2208 S+n , Mi \u2208 R(k\u00d7n)(i = 1, 2, 3) and any vector \u03be \u2208 Rk, the following inequality holds:\n\u2212 \u222b \u03b12 \u03b11 z\u0307T (s)Rz\u0307(s)ds \u2264 \u03beT [ 3\u2211 i=1 \u03b12 \u2212 \u03b11 2i\u2212 1 MiR \u22121MTi\n+ 3\u2211 i=1 Sym{MiEi}\n] \u03be,\nwhere\nE1\u03be = z(\u03b12)\u2212 z(\u03b11),\nE2\u03be = z(\u03b12) + z(\u03b11)\u2212 2\n\u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 z(s)ds,\nE3\u03be = z(\u03b12)\u2212 z(\u03b11) + 6\n\u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 z(s)ds\n\u2212 12 (\u03b12 \u2212 \u03b11)2 \u222b \u03b12 \u03b11 \u222b \u03b12 s z(u)duds.\nLemma 4. [36]: Let z(s) be a differentiable function z : [\u03b11, \u03b12] \u2192 Rn for any matrices R \u2208 S+n , Ni \u2208 R(k\u00d7n)(i = 1, 2), any vector \u03be \u2208 Rk and all continuous function z : [\u03b11, \u03b12] \u2192 Rn, then the following holds:\n\u2212 \u222b \u03b12 \u03b11 zT (s)Rz(s)ds\n\u2264 \u03beT [ (\u03b12 \u2212 \u03b11) ( N1R \u22121NT1 + 1\n3 N2R\n\u22121NT2 ) + Sym{N1F1 +N2F2} ] \u03be,\nwhere\nF1\u03be = \u222b \u03b12 \u03b11 z(s)ds,\nF2\u03be = \u2212 \u222b \u03b12 \u03b11 z(s)ds+ 2 \u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 \u222b \u03b12 s z(u)duds.\nLemma 5. [Extended Wirtinger\u2019s integral inequality] [37]: For any matrix R \u2208 S+n , and any continuously differentiable function z : [\u03b11, \u03b12] \u2192 Rn, the following inequality holds:\u222b \u03b12\n\u03b11\n\u222b \u03b12 s z\u0307T (u)Rz\u0307(u)du\n\u2265 2\u03c7T1 R\u03c71 + 4\u03c7T2 R\u03c72 + 6\u03c7T6 R\u03c73,\nwhere\n\u03c71 = z(\u03b12)\u2212 1\n\u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 z(s)ds,\n\u03c72 = z(\u03b12) + 2\n\u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 z(s)ds\n\u2212 6 (\u03b12 \u2212 \u03b11)2 \u222b \u03b12 \u03b11 \u222b \u03b12 s z(u)duds,\n\u03c73 = z(\u03b12)\u2212 3\n\u03b12 \u2212 \u03b11 \u222b \u03b12 \u03b11 z(s)ds\n+ 24\n(\u03b12 \u2212 \u03b11)2 \u222b \u03b12 \u03b11 \u222b \u03b12 s z(u)duds\n\u2212 60 (\u03b12 \u2212 \u03b11)3 \u222b \u03b12 \u03b11 \u222b \u03b12 s \u222b \u03b12 u z(v)dvduds.\nLemma 6. [38]: For given real matrices R1 and R2 with appropriate dimensions, they satisfy 2RT1 R1 +R T 2 R2.\nLemma 7. [Schur Complement] [38]: Let R1, R2, and R3 be given constant matrices with appropriate dimensions which satisfy R1 = RT1 , R2 = R T 2 > 0, then R1 + RT3 R \u22121 2 R3 < 0 if and only if[\nR1 R T 3 R3 \u2212R2\n] < 0 or [ \u2212R2 R3 RT3 R1 ] < 0.\nLemma 8. [36]: For a quadratic function f(z) = a2z2 + a1z + a0 where a2, a1, a0 \u2208 R. if the following inequalities hold\n(i)f(0) < 0, (ii)f(\u03c4) < 0, (iii)\u2212 \u03c42a2 + f(0) < 0\nthen f(z) < 0, \u2200z \u2208 [0, \u03c4 ].\nRemark 3. Lemma 3 in [56] with N = 2 is Lemma 3 in this work, and it can reduce the complexity of parameter calculations for obtaining sufficient conditions, making this work more efficient than other works.\nRemark 4. Improved conditions for Lemma 8 have been proposed in [58]. Lemma 8 in this work provides sufficient conditions and makes this work more efficient than others.\n4 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nIII. MAIN RESULTS\nIn this section, we will present the sufficient conditions of the main theorems for generalized neural networks with mixed time-varying delays. Firstly, the following notations for vectors and matrices are introduced to simplify the illustration:\nei = [ 0n\u00d7(i\u22121)n In\u00d7n 0n\u00d7(21\u2212i)n ] (i = 1, 2, \u00b7 \u00b7 \u00b7 , 21),\ne\u0302j = [ 0n\u00d7(j\u22121)n In\u00d7n 0n\u00d7(5\u2212j)n ] (j = 1, 2, \u00b7 \u00b7 \u00b7 , 5), es = \u2212A0e1 +A1e7 +A2e13 +A3e16 +A4e21, e0 = 021n\u00d7n, fa(s) = f(Wz(s)),\nga(s) = g(Wz(s)), ha(s) = h(Wz(s)), \u03c4f = 1\n\u03c4 ,\nD1 = e1 \u2212 e2, D2 = e1 + e2 \u2212 2e5, D3 = e1 \u2212 e2 + 6e5 \u2212 12e17, E1 = e2 \u2212 e3, E2 = e2 + e3 \u2212 2e6, E3 = e2 \u2212 e3 + 6e6 \u2212 12e18,\nD4 =  e1 \u2212 e2\u03c4(t)e5 \u03c4(t)(e1 \u2212 e5)  , D5 =  e1 + e2 \u2212 2e5\u03c4(t)(\u2212e5 + 2e17) \u03c4(t)(e5 \u2212 2e17)  , E4 =\n e2 \u2212 e3(\u03c4 \u2212 \u03c4(t))e6 (\u03c4 \u2212 \u03c4(t))(e1 \u2212 e6)  , E5 =\n e2 + e3 \u2212 2e6(\u03c4 \u2212 \u03c4(t))(\u2212e6 + 2e18) (\u03c4 \u2212 \u03c4(t))(e6 \u2212 2e18)  , \u03c61(t) = [ zT (t) zT (t\u2212 \u03c4) \u222b t t\u2212\u03c4\nzT (s)ds\u222b t t\u2212\u03c4 fTa (s)ds \u222b t t\u2212\u03c4 \u222b t s zT (u)duds ]T ,\n\u03c62(t, s) = [ zT (t) zT (s) fTa (s) \u222b t s\nz\u0307T (u)du\u222b t s zT (u)du zT (t\u2212 \u03c4) ]T ,\n\u03c63(t, s) = [ zT (t) zT (s) z\u0307T (s) fTa (s)\u222b t s zT (u)du zT (t\u2212 \u03c4) ]T ,\n\u03c64(t, s) =\n[ z\u0307T (s) zT (s) \u222b t s z\u0307T (u)du ]T ,\n\u03be1(t) = [ zT (t) zT (t\u2212 \u03c4(t)) zT (t\u2212 \u03c4) z\u0307T (t\u2212 \u03c4) ]T ,\n\u03be2(t) = [\u222b t t\u2212\u03c4(t) zT (s) \u03c4(t) ds \u222b t\u2212\u03c4(t) t\u2212\u03c4 zT (s) \u03c4 \u2212 \u03c4(t) ]T ,\n\u03be3(t) = [ fTa (t) f T a (t\u2212 \u03c4(t)) fTa (t\u2212 \u03c4) ]T ,\n\u03be4(t) = [\u222b t t\u2212\u03c4(t) fTa (s)ds \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)ds ]T ,\n\u03be5(t) = [ gTa (t) g T a (t\u2212 \u03c4(t)) gTa (t\u2212 \u03c4) hTa (t) ]T ,\n\u03be6(t) = [\u222b t t\u2212\u03b3(t) hTa (s)ds \u222b t t\u2212\u03c4(t) \u222b t s zT (u) \u03c42(t) duds ]T ,\n\u03be7(t) = [\u222b t\u2212\u03c4(t) t\u2212\u03c4 \u222b t\u2212\u03c4(t) s zT (u) (\u03c4 \u2212 \u03c4(t))2 duds ]T ,\n\u03be8(t) = [\u222b t t\u2212\u03c4 \u222b t s zT (u)duds ]T ,\n\u03be9(t) = [\u222b t t\u2212\u03c4 \u222b t s \u222b t u zT (v)dvduds \u03c9T (t) ]T ,\n\u03be(t) = [ \u03beT1 (t) \u03be T 2 (t) \u03be T 3 (t) \u03be T 4 (t) \u03be T 5 (t)\n\u03beT6 (t) \u03be T 7 (t) \u03be T 8 (t) \u03be T 9 (t)\n]T ,\n\u039e[\u03c4(t)] = \u03a61[\u03c4(t)] +\u03a62 +\u03a62[\u03c4(t)] +\u03a63 +\u03a63[\u03c4(t)] +\u03a64[\u03c4(t)]\n+\u03a65 +\u03a6z + \u03bd1 + \u03bd2 + \u03bd3 + \u03bd4 + \u03bd5 \u2212 \u03c42\u03f1 \u2212 \u03b1eT1 P1e1 \u2212 eT21Mae21,\n\u03a61[\u03c4(t)] = Sym{eT1 P1es}+ Sym   e1 e3\n\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6 e10 + e11 e19\n T\nP2  es e4 e1 \u2212 e3 e7 \u2212 e9\n\u03c4e1 \u2212 \u03c4(t)e5 \u2212 (\u03c4 \u2212 \u03c4(t))e6\n  ,\n\u03a62 =  e1 e1 e7 e0 e0 e3  T Q1  e1 e1 e7 e0 e0 e3 +  e1 e1 es e7 e0 e3  T Q2  e1 e1 es e7 e0 e3  ,\n\u03a62[\u03c4(t)] = Sym   \u03c4(t)e1 \u03c4(t)e5 e10\n\u03c4(t)(e1 \u2212 e5) \u03c42(t)e17 \u03c4(t)e18\n T Q1  es e0 e0 es e1 e4  \n\u2212 (1\u2212 \u00b5)  e1 e2 e8\ne1 \u2212 e2 \u03c4(t)e5 e3\n T Q1  e1 e2 e8\ne1 \u2212 e2 \u03c4(t)e5 e3\n\n\u2212  e1 e3 e4 e9\n\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6 e3\n T \u00d7\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nQ2  e1 e3 e4 e9\n\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6 e3\n\n+ Sym   \u03c4e1 \u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6 e1 \u2212 e3 e10 + e11 e19 \u03c4e3  T Q2  es e0 e0 e0 e1 e4   ,\n\u03a63 = \u03c4e T s R1es + \u03c4 2eT7 R2e7 + e T 1 Pae1 \u2212 eT2 Pae2\n+ eT2 Pbe2 \u2212 eT3 Pbe3 + Sym{N1D1 +N2D2 +N3D3} + Sym{M1E1 +M2E2 +M3E3} + Sym{N4D4 +N5D5}+ Sym{M4E4 +M5E5}\n+ \u03c4 ese1 e0 T R3 ese1 e0  \u2212 [ e10 e11 ]T (Sym{[X1 X2]}) [ e10 e11 ] ,\n\u03a63[\u03c4(t)] = Sym  \u03c4e1 \u2212 \u03c4(t)e5 \u2212 (\u03c4 \u2212 \u03c4(t))e6e19 \u03c42\n2 e1 \u2212 e19\nT R3 e0e0 es   ,\n\u03a64[\u03c4(t)] = \u03c42\n2 eTs S1es \u2212 2 [e1 \u2212 \u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6)]\nT\n\u00d7 S1 [e1 \u2212 \u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t)e6))] \u2212 4 [ e1 + 2\u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6)\u2212 6\u03c42f e19 ]T \u00d7 S1 [ e1 + 2\u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6)\u2212 6\u03c42f e19\n] \u2212 6 [ e1 \u2212 3\u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6) + 24\u03c42f e19\n\u221260\u03c43f e20 ]T S1 [e1 \u2212 3\u03c4f (\u03c4(t)e5 + (\u03c4 \u2212 \u03c4(t))e6)\n+24\u03c42f e19 \u2212 60\u03c43f e20 ] ,\n\u03a65 = \u03b3de T 15Y e15 \u2212 eT16Y e16, \u03a6z = \u03c4 2(t)e17 + (\u03c4 \u2212 \u03c4(t))2e18 + (\u03c4 \u2212 \u03c4(t))\u03c4(t)e5\n\u2212 e19, \u03bd1 = Sym{[e7 \u2212 e8 \u2212 (FmW (e1 \u2212 e2))]TLf1\n\u00d7 [(FpW (e1 \u2212 e2))\u2212 e7 + e8] + [e8 \u2212 e9 \u2212 (FmW (e2 \u2212 e3))]TLf2 \u00d7 [(FpW (e2 \u2212 e3))\u2212 e8 + e9] + [e7 \u2212 e9 \u2212 (FmW (e1 \u2212 e3))]TLf3 \u00d7 [(FpW (e1 \u2212 e3))\u2212 e7 + e9]},\n\u03bd2 = Sym{[e7 \u2212 FmWe1]TVf1[FpWe1 \u2212 e7]} + [e8 \u2212 FmWe2]TVf2[FpWe2 \u2212 e8] + [e9 \u2212 FmWe3]TVf3[FpWe3 \u2212 e9]},\n\u03bd3 = Sym{[e12 \u2212 e13 \u2212 (GmW (e1 \u2212 e2))]TLg1 \u00d7 [(GpW (e1 \u2212 e2))\u2212 e12 + e13] + [e13 \u2212 e14 \u2212 (GmW (e2 \u2212 e3))]TLg2 \u00d7 [(GpW (e2 \u2212 e3))\u2212 e13 + e14] + [e12 \u2212 e14 \u2212 (GmW (e1 \u2212 e3))]TLg3 \u00d7 [(GpW (e1 \u2212 e3))\u2212 e12 + e14]}, \u03bd4 = Sym{[e12 \u2212GmWe1]TVg1[GpWe1 \u2212 e12] + [e13 \u2212GmWe2]TVg2[GpWe2 \u2212 e13] + [e14 \u2212GmWe3]TVg3[GpWe3 \u2212 e14]}, \u03bd5 = Sym{[e15 \u2212HmWe1]TVh1[HpWe1 \u2212 e15]}, \u03f1 = Sym{[eT0 eT0 eT5 \u2212 eT6 eT0 eT0 ]P2\n\u00d7 [eT0 eT0 eT0 eT0 eT6 \u2212 eT5 ]T } + Sym{[eT0 eT0 eT0 eT0 e12T eT0 ]Q1 \u00d7 [eTs eT0 eT0 eTs eT1 eT4 ]T } \u2212 (1\u2212 \u03c4d)[eT0 eT0 eT0 eT0 eT5 eT0 ]Q1 \u00d7 [eT0 eT0 eT0 eT0 eT5 eT0 ]T \u2212 [eT0 eT0 eT0 eT0 eT5 \u2212 eT6 eT0 ]Q2 \u00d7 [eT0 eT0 eT0 eT0 eT5 \u2212 eT6 eT0 ]T + Sym{L(e12 + e13 \u2212 e5)},\n\u03a01 = [ \u03c4N1 \u03c4N2 \u03c4N3 \u03c4N4 \u03c4N5 ( [ e10 e11 ]T X1) ] ,\n\u03a02 = [ \u03c4M1 \u03c4M2 \u03c4M3 \u03c4M4 \u03c4M5 ( [ e10 e11 ]T X2) ] ,\n\u03a51 = diag{\u2212\u03c4R1 \u2212 3\u03c4R1 \u2212 5\u03c4R1 \u2212 \u03c4Ra \u2212 3\u03c4Ra \u2212R2},\n\u03a52 = diag{\u2212\u03c4R1 \u2212 3\u03c4R1 \u2212 5\u03c4R1 \u2212 \u03c4Rb \u2212 3\u03c4Rb \u2212R2},\n\u03b8p = \u03c4f  \u03c4 e\u03021 e\u03023\ne\u03021 \u2212 e\u03022 e\u03024 e\u03025\n T Q2  \u03c4 e\u03021 e\u03023\ne\u03021 \u2212 e\u03022 e\u03024 e\u03025\n ,\nRa = R3 + Sym  0I 0 Pa [I 0 0]  ,\nRb = R3 + Sym  0I 0 Pb [I 0 0]  ,\n\u03f50 = \u03bbmin(P\u03021), \u03f51 = \u03bbmax(P\u03021), \u03f52 = \u03bbmax(P\u03022),\n\u03f53 = \u03bbmax(Q\u03021), \u03f54 = \u03bbmax(Q\u03022), \u03f55 = \u03bbmax(R\u03021),\n\u03f56 = \u03bbmax(R\u03022), \u03f57 = \u03bbmax(R\u03023), \u03f58 = \u03bbmax(S\u03021) ,\n\u03f59 = \u03bbmax(Y\u0302 ) , \u03f510 = \u03bbmax(Ma).\nA. FINITE-TIME BOUNDEDNESS In this subsection, we study finite-time boundedness for the generalized neural networks with mixed time-varying delays\n6 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nin the following form:\nz\u0307(t) = \u2212A0z(t) +A1f(Wz(t)) +A2g(Wz(t\u2212 \u03c4(t)))\n+A3 \u222b t t\u2212\u03b3(t) h(Wz(s))ds+A4\u03c9(t),\nz(t) = \u03d5(t), t \u2208 [\u2212\u03c4, 0]. (5)\nTheorem 1. For given positive scalars \u03c4 , \u03c4d and \u03b3d, the system (5) is finite-time bounded if there exist matrices P1 \u2208 S+n , P2 \u2208 S5n, Qi(i = 1, 2) \u2208 S+6n, Rj(j = 1, 2) \u2208 S+n , R3 \u2208 S+3n S1, Y, Ma \u2208 S+n , any matrices X1, X2 \u2208 R2n\u00d7n, L \u2208 R21n\u00d7n such that the following LMIs hold:[\n\u039e[\u03c4(t)=\u03c4 ] \u03a01 \u2217 \u03a51\n] < 0, [ \u039e[\u03c4(t)=0] \u03a02\n\u2217 \u03a52 ] < 0,[\n\u039e[\u03c4(t)=0] \u2212 \u03c42\u03f1 \u03a02 \u2217 \u03a52\n] < 0, (6)\nP2 + \u03b8p > 0, (7)\nRa > 0, Rb > 0, (8)\n\u03f50I \u2264 P\u03021 \u2264 \u03f51I, 0 \u2264 P\u03022 \u2264 \u03f52I, 0 \u2264 Q\u03021 \u2264 \u03f53I, 0 \u2264 Q\u03022 \u2264 \u03f54I, 0 \u2264 R\u03021 \u2264 \u03f55I, 0 \u2264 R\u03022,\u2264 \u03f56I, 0 \u2264 R\u03023 \u2264 \u03f57I, 0 \u2264 S\u03021 \u2264 \u03f58I, 0 \u2264 Y\u0302 \u2264 \u03f59I, 0 \u2264Ma \u2264 \u03f510I, (9)\ne\u03b1Tf [ \u03a0c1 + \u03c9f \u03f510(1\u2212 e\u2212\u03b1Tf ) ] < \u03f50c2. (10)\nProof. We construct the Lyapunov\u2013Krasovskii functional as follows:\nV (zt, t) = 5\u2211 j=1 Vj(zt, t), (11)\nwhere\nV1(zt, t) = z T (t)P1z(t) + \u03c6 T 1 (t)P2\u03c61(t),\nV2(zt, t) = \u222b t t\u2212\u03c4(t) \u03c6T2 (t, s)Q1\u03c62(t, s)ds\n+ \u222b t t\u2212\u03c4 \u03c6T3 (t, s)Q2\u03c63(t, s)ds,\nV3(zt, t) = \u222b t t\u2212\u03c4 \u222b t s z\u0307T (u)R1z\u0307(u)duds\n+ \u03c4 \u222b t t\u2212\u03c4 \u222b t s fTa (u)R2fa(u)duds\n+ \u222b t t\u2212\u03c4 \u222b t s \u03c6T4 (t, u)R3\u03c64(t, u)duds,\nV4(zt, t) = \u222b t t\u2212\u03c4 \u222b t s \u222b t u z\u0307T (v)S1z\u0307(v)dvduds,\nV5(zt, t) = \u222b t t\u2212\u03b3d \u222b t s hTa (u)Y ha(u)duds.\nThen, the time derivatives of (10) are calculated as follows:\nV\u03071(zt, t) = 2z T (t)P1z\u0307(t) + 2\u03c6 T 1 (t)P2\u03c6\u03071(t)\n= \u03beT (t)\u03a61[\u03c4(t)]\u03be(t), (12)\nV\u03072(zt, t) = \u03c6 T 2 (t, t)Q1\u03c62(t, t)\n\u2212 (1\u2212 \u03c4\u0307(t))\u03c6T2 (t, t\u2212 \u03c4(t))Q1\u03c62(t, t\u2212 \u03c4(t))\n+ 2 \u222b t t\u2212\u03c4(t) \u03c6T2 (t, s)Q1\u03c6\u03072(t, s)ds + \u03c6T3 (t, t)Q2\u03c63(t, t) \u2212 \u03c6T3 (t, t\u2212 \u03c4)Q2\u03c63(t, t\u2212 \u03c4)\n+ 2 \u222b t t\u2212\u03c4 \u03c6T3 (t, s)Q2\u03c6\u03073(t, s)ds\n\u2264 \u03beT (t)(\u03a62 +\u03a62[\u03c4(t)])\u03be(t). (13)\nBefore calculating V\u03073(zt, t), we present two zero equations with the symmetric matrices Pa and Pb \u2208 Rn\u00d7n inspired by the work of [5] as follows:\n0 = zT (t)Paz(t)\u2212 zT (t\u2212 \u03c4(t))Paz(t\u2212 \u03c4(t)) \u2212 2 \u222b t t\u2212\u03c4(t) zT (s)Paz\u0307(s)ds, 0 = zT (t\u2212 \u03c4(t))Pbz(t\u2212 \u03c4(t))\u2212 zT (t\u2212 \u03c4)Pbz(t\u2212 \u03c4)\n\u2212 2 \u222b t\u2212\u03c4(t) t\u2212\u03c4 zT (s)Pbz\u0307(s)ds.\nAs a result, the sum of V\u03073(zt, t) and two zero items can be written as\nV\u03073(zt, t) = \u03c4 z\u0307 T (t)R1z\u0307(t)\u2212 \u222b t t\u2212\u03c4(t) z\u0307T (s)R1z\u0307(s)ds\n\u2212 \u222b t\u2212\u03c4(t) t\u2212\u03c4 z\u0307T (s)R1z\u0307(s)ds\n+ \u03c42fTa (t)R2fa(t)\u2212 \u03c4 \u222b t t\u2212\u03c4(t) fTa (s)R2fa(s)ds\n\u2212 \u03c4 \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)R2fa(s)ds\n+ \u03c4 z\u0307(t)z(t) 0 T R3 z\u0307(t)z(t) 0 \n+ 2  \u03c4z(t)\u2212 \u222b t t\u2212\u03c4 z(s)ds\u222b t t\u2212\u03c4 \u222b t s z(u)duds\n\u03c42 2 z(t)\u2212 \u222b t t\u2212\u03c4 \u222b t s z(u)duds\n T R3  00 z\u0307(t)  \u2212 \u222b t t\u2212\u03c4 \u03c6T4 (t, s)R3\u03c64(t, s)ds+ z T (t)Paz(t) \u2212 zT (t\u2212 \u03c4(t))Paz(t\u2212 \u03c4(t))\n\u2212 2 \u222b t t\u2212\u03c4(t) zT (s)Paz\u0307(s)ds + zT (t\u2212 \u03c4(t))Pbz(t\u2212 \u03c4(t)) \u2212 zT (t\u2212 \u03c4)Pbz(t\u2212 \u03c4)\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n\u2212 2 \u222b t\u2212\u03c4(t) t\u2212\u03c4(t\u2212\u03c4) zT (s)Pbz\u0307(s)ds\n= \u03c4 z\u0307T (t)R1z\u0307(t)\u2212 \u222b t t\u2212\u03c4(t) z\u0307T (s)R1z\u0307(s)ds\n\u2212 \u222b t\u2212\u03c4(t) t\u2212\u03c4 z\u0307T (s)R1z\u0307(s)ds+ \u03c4 2fTa (t)R2fa(t)\n\u2212 \u03c4 \u222b t t\u2212\u03c4(t) fTa (s)R2fa(s)ds\n\u2212 \u03c4 \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)R2fa(s)ds\n+ \u03c4 z\u0307(t)z(t) 0 T R3 z\u0307(t)z(t) 0 \n+ 2  \u03c4z(t)\u2212 \u222b t t\u2212\u03c4 z(s)ds\u222b t t\u2212\u03c4 \u222b t s z(u)duds\n\u03c42 2 z(t)\u2212 \u222b t t\u2212\u03c4 \u222b t s z(u)duds\n T R3  00 z\u0307(t)  + zT (t)Paz(t)\u2212 zT (t\u2212 \u03c4(t))Paz(t\u2212 \u03c4(t)) + zT (t\u2212 \u03c4(t))Pbz(t\u2212 \u03c4(t))\u2212 zT (t\u2212 \u03c4)Pbz(t\u2212 \u03c4)\n\u2212 \u222b t t\u2212\u03c4(t) \u03c6T4 (t, s)Ra\u03c64(t, s)ds\n\u2212 \u222b t\u2212\u03c4(t) t\u2212\u03c4 \u03c6T4 (t, s)Rb\u03c64(t, s)ds.\nUsing Lemma 3, we have\n\u2212 \u222b t t\u2212\u03c4(t) z\u0307T (s)R1z\u0307(s)ds\u2212 \u222b t\u2212\u03c4(t) t\u2212\u03c4 z\u0307T (s)R1z\u0307(s)ds\n\u2264 \u03beT (t) { \u03c4(t) ( N1R \u22121 1 N T 1 + 1\n3 N2R\n\u22121 1 N T 2\n+ 1\n5 N3R\n\u22121 1 N T 3 ) + Sym{N1D1 +N2D2 +N3D3}\n+ (\u03c4 \u2212 \u03c4(t)) ( M1R \u22121 1 M T 1 + 1\n3 M2R\n\u22121 1 M T 2\n+ 1\n5 M3R\n\u22121 1 M T 3 ) + Sym{M1E1 +M2E2\n+M3E3} } \u03be(t).\nBy applying Lemma 1 and Lemma 2, we obtain\n\u2212 \u03c4 \u222b t t\u2212\u03c4(t) fTa (s)R2fa(s)ds\u2212 \u03c4 \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)R2fa(s)ds\n\u2264 \u2212 \u03c4 \u03c4(t) \u222b t t\u2212\u03c4(t) fTa (s)dsR2 \u222b t t\u2212\u03c4(t) fa(s)ds\n\u2212 \u03c4 \u03c4 \u2212 \u03c4(t) \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)dsR2 \u222b t\u2212\u03c4(t) t\u2212\u03c4 fa(s)ds\n= \u2212 [\u222b t t\u2212\u03c4(t) fa(s)ds\u222b t\u2212\u03c4(t) t\u2212\u03c4 fa(s)ds ]T [ \u03c4 \u03c4(t)R2 0 0 \u03c4\u03c4\u2212\u03c4(t)R2 ]\n\u00d7 [\u222b t t\u2212\u03c4(t) fa(s)ds\u222b t\u2212\u03c4(t) t\u2212\u03c4 fa(s)ds ]\n\u2264 [\u222b t t\u2212\u03c4(t) fa(s)ds\u222b t\u2212\u03c4(t) t\u2212\u03c4 fa(s)ds ]T \u00d7 ( \u2212Sym{[X1 X2]}+ \u03c4(t)\n\u03c4 X1R\n\u22121 2 X T 1\n+ \u03c4 \u2212 \u03c4(t)\n\u03c4 X2R\n\u22121 2 X T 2 )[\u222b t t\u2212\u03c4(t) fa(s)ds\u222b t\u2212\u03c4(t) t\u2212\u03c4 fa(s)ds ] .\nBy utilizing Lemma 4, we get \u2212 \u222b t t\u2212\u03c4(t) \u03c6T4 (t, s)Ra\u03c64(t, s)ds\n\u2212 \u222b t\u2212\u03c4(t) t\u2212\u03c4 \u03c6T4 (t, s)Rb\u03c64(t, s)ds\n\u2264 \u03beT (t) { \u03c4(t) ( N4R \u22121 a N T 4 + 1\n3 N5R\n\u22121 a N T 5 ) + Sym{N4D4 +N5D5}\n+ (\u03c4 \u2212 \u03c4(t)) ( M4R \u22121 b M T 4 + 1\n3 M5R\n\u22121 b M T 5 ) + Sym{M4E4 +M5E5} } \u03be(t).\nTherefore, we obtain\nV\u03073 \u2264 \u03beT (t){\u03a63 +\u03a63[\u03c4(t)]}\u03be(t). (14)\nFurther, the calculation of V\u03074(zt, t) can be presented as\nV\u03074(zt, t) = \u03c42\n2 z\u0307T (t)S1z\u0307(t)\u2212 \u222b t t\u2212\u03c4 \u222b t s z\u0307T (u)S1z\u0307(u)duds.\nBy applying Lemma 5, we deduce \u2212 \u222b t t\u2212\u03c4 \u222b t s z\u0307T (u)S1z\u0307(u)duds\n\u2264 \u22122 [ z(t)\u2212 1\n\u03c4 \u222b t t\u2212\u03c4 z(s)ds ]T S1 [ z(t)\u2212 1\u03c4 \u222b t t\u2212\u03c4 z(s)ds ] \u2212 4 [ z(t) + 2\n\u03c4 \u222b t t\u2212\u03c4 z(s)ds\u2212 6 \u03c42 \u222b t t\u2212\u03c4 \u222b t s z(u)duds ]T \u00d7 S1 [ z(t) + 2\n\u03c4 \u222b t t\u2212\u03c4 z(s)ds\u2212 6 \u03c42 \u222b t t\u2212\u03c4 \u222b t s z(u)duds ] \u2212 6 [ z(t)\u2212 3\n\u03c4 \u222b t t\u2212\u03c4 z(s)ds+ 24 \u03c42 \u222b t t\u2212\u03c4 \u222b t s z(u)duds\n\u221260 \u03c43 \u222b t t\u2212\u03c4 \u222b t s \u222b t u z(v)dvduds ] S1 [ z(t)\u2212 3 \u03c4 \u222b t t\u2212\u03c4 z(s)ds\n+ 24\n\u03c42 \u222b t t\u2212\u03c4 \u222b t s z(u)duds\u2212 60 \u03c43 \u222b t t\u2212\u03c4 \u222b t s \u222b t u z(v)dvduds ] .\n8 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThen, we obtain\nV\u03074(zt, t) \u2264 \u03beT (t)\u03a64[\u03c4(t)]\u03be(t). (15)\nCalculation of V\u03075(zt, t) is\nV\u03075(zt, t) = \u03b3dh T a (t)Y ha(t)\u2212 \u222b t t\u2212\u03b3d hTa (s)Y ha(s)ds\n\u2264 \u03b3dhTa (t)Y ha(t)\u2212 \u222b t t\u2212\u03b3(t) hTa (s)Y ha(s)ds.\nBy Lemma2, we obtain \u2212 \u222b t t\u2212\u03b3(t) hTa (s)Y ha(s)ds\n\u2264 \u2212 \u222b t t\u2212\u03b3(t) hTa (s)dsY \u222b t t\u2212\u03b3(t) ha(s)ds.\nThen, we get\nV\u03075(zt, t) \u2264 \u03beT (t)\u03a65\u03be(t). (16)\nBy utilizing Assumption 1, we get\nlfi(v1, v2) :\n= 2[fa(v1)\u2212 fa(v2)\u2212 FmW (z(v1)\u2212 z(v2))]TLfi \u00d7 [FpW (z(v1)\u2212 z(v2))\u2212 fa(v1) + fa(v2)] \u2265 0,\nvfi(v) : = 2[fa(v)\u2212 FmWz(v)]TVfi[FpWz(v)\u2212 fa(v)] \u2265 0, lgi(v1, v2) :\n= 2[ga(v1)\u2212 ga(v2)\u2212GmW (z(v1)\u2212 z(v2))]TLgi \u00d7 [GpW (z(v1)\u2212 z(v2))\u2212 ga(v1) + ga(v2)] \u2265 0,\nvgi(v) := 2[ga(v)\u2212GmWz(v)]TVgi[GpWz(v)\u2212 ga(v)] \u2265 0, vh(v) := 2[ha(v)\u2212HmWz(v)]TVh[HpWz(v)\u2212 ha(v)] \u2265 0,\nwhere\nLfi = diag{l1fi, l2fi, . . . , lnfi}, Vfi = diag{v1fi, v2fi, . . . , vnfi}, Lgi = diag{l1gi, l2gi, . . . , lngi}, Vgi = diag{v1gi, v2gi, . . . , vngi}, Vh = diag{v1h, v2h, . . . , vnh}, i = 1, 2, 3.\nTherefore, we have\nlf1(t, t\u2212 \u03c4(t)) + lf2(t\u2212 \u03c4(t), t\u2212 \u03c4) + lf3(t, t\u2212 \u03c4) = \u03beT (t)\u03bd1\u03be(t) \u2265 0, (17)\nvf1(t) + vf2(t\u2212 \u03c4(t)) + vf3(t\u2212 \u03c4) = \u03beT (t)\u03bd2\u03be(t) \u2265 0, (18)\nlg1(t, t\u2212 \u03c4(t)) + lg2(t\u2212 \u03c4(t), t\u2212 \u03c4) + lg3(t, t\u2212 \u03c4) = \u03beT (t)\u03bd3\u03be(t) \u2265 0, (19)\nvg1(t) + vg2(t\u2212 \u03c4(t)) + vg3(t\u2212 \u03c4) = \u03beT (t)\u03bd4\u03be(t) \u2265 0, (20) vh(t) = \u03be T (t)\u03bd5\u03be(t) \u2265 0. (21)\nNote that\u222b t t\u2212\u03c4 \u222b t s z(u)duds = \u222b t t\u2212\u03c4(t) \u222b t s z(u)duds\n+ (\u03c4 \u2212 \u03c4(t)) \u222b t t\u2212\u03c4 z(s)ds\n+ \u222b t\u2212\u03c4(t) t\u2212\u03c4 \u222b t\u2212\u03c4(t) s z(u)duds.\nThen, we obtain\n0 = 2\u03beT (t)L (\u222b t t\u2212\u03c4(t) \u222b t s\nz(u)duds+ (\u03c4 \u2212 \u03c4(t))\u00d7\u222b t t\u2212\u03c4 z(s)ds+ \u222b t\u2212\u03c4(t) t\u2212\u03c4 \u222b t\u2212\u03c4(t) s z(u)duds\n\u2212 \u222b t t\u2212\u03c4 \u222b t s z(u)duds ) = \u03beT (t)\u03a6z\u03be(t). (22)\nCombining (12)-(22), it can be inferred that\nV\u0307 (zt, t)\u2212 \u03b1V (zt, t)\u2212 \u03b1\u03c9T (t)M\u03c9(t) \u2264 \u03beT (t)\u039e[\u03c4(t)]\u03be(t). (23)\nObviously the equation (23) is quadratic. By Lemma 8 if\n\u039e[\u03c4(t) = \u03c4 ] < 0, \u039e[\u03c4(t) = 0] < 0, \u039e[\u03c4(t) = 0]\u2212 \u03c42\u03f1 < 0.\nTherefore, we obtain\n\u039e[\u03c4(t)] < 0. (24)\nIt follows from (23) and (24), we have\nV\u0307 (zt, t)\u2212 \u03b1V (zt, t)\u2212 \u03b1\u03c9T (t)M\u03c9(t) \u2264 \u03beT (t)\u039e[\u03c4(t)]\u03be(t) < 0. (25)\nBy multiplying of (25) with e\u2212\u03b1t, then (25) becomes\nd\ndt\n( e\u2212\u03b1tV (zt, t) ) < \u03b1e\u2212\u03b1t\u03c9T (t)M\u03c9(t). (26)\nBy integrating (26) on [0, t] where t \u2208 [0, Tf ] and Assumption 2, we obtain\nV (zt, t) < e \u03b1Tf [ V (z0, 0) + \u03b1 \u222b Tf 0 e\u2212\u03b1s\u03c9T (s)M\u03c9(s)ds ] < e\u03b1Tf [ V (z0, 0) + \u03c9\u03f510(1\u2212 e\u2212\u03b1Tf ) ] .\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nNext, we consider V (z0, 0) by Assumption 1, we get\nV (z0, 0) \u2264 zT (0)P1z(0) + \u03c6T1 (0)P2\u03c61(0)\n+ \u222b 0 \u2212\u03c4(0) \u03c6T2 (0, s)Q1\u03c62(0, s)ds\n+ \u222b 0 \u2212\u03c4 \u03c6T3 (0, s)Q2\u03c63(0, s)ds\n+ \u222b 0 \u2212\u03c4 \u222b 0 s z\u0307T (u)R1z\u0307(u)duds\n+ \u03c4 \u222b 0 \u2212\u03c4 \u222b t s fTa (u)R2fa(u)duds\n+ \u222b 0 \u03c4 \u222b 0 s \u03c6T4 (0, u)(s)R3\u03c64(0, u)duds\n+ \u222b 0 \u2212\u03c4 \u222b 0 s \u222b 0 u z\u0307T (v)S1z\u0307(v)dvduds\n+ \u222b 0 \u2212\u03b3d \u222b 0 s H\u0302T (u)Y H\u0302(u)duds,\nwhere H\u0302 = diag{H+1 , . . . ,H+n }. Furthermore, we let P\u0302i = V \u22121 2 PiV \u22121 2 , Q\u0302i = V \u22121 2 QiV \u22121 2 , i = 1, 2, R\u0302j = V \u22121 2 RjV \u22121 2 , j = 1, 2, 3,S\u0302 = V \u22121 2 SV \u22121 2 , Y\u0302 = V \u22121 2 H\u0302TY H\u0302V \u22121 2 . We can derive that\nV (z0, 0) \u2264 zT (0)V 1 2P1V 1 2 z(0) + \u03c6T1 (0)V 1 2P2V 1 2\u03c61(0)\n+ \u222b 0 \u2212\u03c4(0) \u03c6T2 (0, s)V 1 2Q1V 1 2\u03c62(0, s)ds\n+ \u222b 0 \u2212\u03c4 \u03c6T3 (0, s)V 1 2Q2V 1 2\u03c63(0, s)ds\n+ \u222b 0 \u2212\u03c4 \u222b 0 s z\u0307T (u)V 1 2R1V 1 2 z\u0307(u)duds\n+ \u03c4 \u222b 0 \u2212\u03c4 \u222b t s fTa (u)V 1 2R2V 1 2 fa(u)duds\n+ \u222b 0 \u03c4 \u222b 0 s \u03c6T4 (0, u)(s)V 1 2R3V 1 2\u03c64(0, u)duds\n+ \u222b 0 \u2212\u03c4 \u222b 0 s \u222b 0 u z\u0307T (v)S1z\u0307(v)dvduds\n+ \u222b 0 \u2212\u03b3d \u222b 0 s H\u0302T (u)Y H\u0302(u)duds.\n\u2264 { \u03bbmax(P\u03021) + \u03bbmax(P\u03022) + \u03c4\u03bbmax(Q\u03021)\n+ \u03c4\u03bbmax(Q\u03022) + \u03c42\n2 \u03bbmax(R\u03021) +\n\u03c43\n2 \u03bbmax(R\u03022)\n+ \u03c42\n2 \u03bbmax(R\u03023) +\n\u03c43\n6 \u03bbmax(S\u03021) + \u03b32d 2 \u03bbmax(Y\u0302 ) } \u00d7 sup\n\u03c42\u2264s\u22640 {zT (s)V z(s), z\u0307T (s)V z\u0307(s)}\n\u2264 \u0393c1,\nwhere\n\u0393 = \u03f51 + \u03f52 + \u03c4\u03f53 + \u03c4\u03f54 + \u03c42\n2 \u03f55 +\n\u03c43\n2 \u03f56\n+ \u03c42\n2 \u03f57 +\n\u03c43\n6 \u03f58 + \u03b32d 2 \u03f59. (27)\nIn addition, it follows from (11) that\nV (zt, t) \u2265 zT (t)P1z(t) \u2265 \u03bbmin(P\u03021)zT (t)V z(t) = \u03f50zT (t)V z(t). (28)\nThen, from the inequalities (27)-(28) and the condition (10), we obtain\nzT (t)V z(t) \u2264 e \u03b1Tf\n\u03f50\n[ \u0393c1 + \u03c9f \u03f510(1\u2212 e\u2212\u03b1Tf ) ] < c2.\nBy definition (3), the system (5) is finite-time bounded. The proof is complete.\nRemark 5. In Assumption 1, select (v1, v2) as (t, t \u2212 \u03c4(t)), (t \u2212 \u03c4(t), and (t, t \u2212 \u03c4). As a result, we incorporated more information on cross terms between the terms t, t \u2212 \u03c4 , and t\u2212\u03c4(t). Thus, our method leads to less conservative stability criteria.\nRemark 6. In this research, the LKFs consist of single, double, and triple integral terms that make utilize additional information regarding the delays \u03c4 and \u03b3d, and a state variable. We improved LKFs and compared them to LKFs reported in recent publications [5], [36], [39], [40]. In addition, the LKFs consisting of the triple integral term\u222b t t\u2212\u03c4 \u222b t s \u222b t u z\u0307T (v)S1z\u0307(v)dvduds that were not used in [5], [36], [39], [40]. Moreover, the stability and performance analysis has employed more information on activation functions, as demonstrated by the inclusion of f(z), g(z), and h(z) in the proof. Constructing improved LKFs and employing techniques for estimating the time derivatives, which result in less conservatism.\nB. FINITE-TIME STABLE\nIn this subsection, we investigate finite-time stability for the GNNs with mixed time-varying delays and asymptotically stable for the NNs with discrete time-varying delays. We defined:\nei = [ 0n\u00d7(i\u22121)n In\u00d7n 0n\u00d7(20\u2212i)n ] , (i = 1, 2, \u00b7 \u00b7 \u00b7 , 20)\ne0 = 020n\u00d7n, es = \u2212A0e1 +A1e7 +A2e13 +A3e16, \u03c81(t) = [ zT (t) zT (t\u2212 \u03c4(t)) zT (t\u2212 \u03c4) z\u0307T (t\u2212 \u03c4)\u222b t\nt\u2212\u03c4(t)\nzT (s)\n\u03c4(t) ds \u222b t\u2212\u03c4(t) t\u2212\u03c4 zT (s) \u03c4 \u2212 \u03c4(t) ]T ,\n10 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n\u03c82(t) = [ fTa (t) f\nT a (t\u2212 \u03c4(t)) fTa (t\u2212 \u03c4)\u222b t\nt\u2212\u03c4(t) fTa (s)ds \u222b t\u2212\u03c4(t) t\u2212\u03c4 fTa (s)ds ]T ,\n\u03c83(t) = [ gTa (t) g T a (t\u2212 \u03c4(t)) gTa (t\u2212 \u03c4)\nhTa (t) \u222b t t\u2212\u03b3(t) hTa (s)ds ]T ,\n\u03c84(t) = [\u222b t t\u2212\u03c4(t) \u222b t s zT (u) \u03c42(t) duds\n\u222b t\u2212\u03c4(t) t\u2212\u03c4 \u222b t\u2212\u03c4(t) s zT (u) (\u03c4 \u2212 \u03c4(t))2 duds ]T ,\n\u03c85(t) = [\u222b t t\u2212\u03c4 \u222b t s zT (u)duds \u222b t t\u2212\u03c4 \u222b t s \u222b t u zT (v)dvduds ]T ,\n\u03a8(t) = [ \u03c8T1 (t) \u03c8 T 2 (t) \u03c8 T 3 (t) \u03c8 T 4 (t) \u03c8 T 5 (t) ]T ,\n\u0398[\u03c4(t)] = \u03a61[\u03c4(t)] +\u03a62 +\u03a62[\u03c4(t)] +\u03a63 +\u03a63[\u03c4(t)] +\u03a64[\u03c4(t)]\n+\u03a65 +\u03a6z + \u03bd1 + \u03bd2 + \u03bd3 + \u03bd4 + \u03bd5 \u2212 \u03c42\u03f1 \u2212 \u03b1eT1 P1e1,\n\u03a3[\u03c4(t)] = \u03a61[\u03c4(t)] +\u03a62 +\u03a62[\u03c4(t)] +\u03a63 +\u03a63[\u03c4(t)] +\u03a64[\u03c4(t)]\n+\u03a65 +\u03a6z + \u03bd1 + \u03bd2 \u2212 \u03c42\u03f1.\nRemark 7. The generalized neural networks (5) without external disturbance (\u03c9(t) = 0) satisfying (2)-(3) becomes\nz\u0307(t) = \u2212A0z(t) +A1f(Wz(t)) +A2g(Wz(t\u2212 \u03c4(t)))\n+A3 \u222b t t\u2212\u03b3(t) h(Wz(s))ds,\nz(t) = \u03d5(t), t \u2208 [\u2212\u03c4, 0], (29)\nCorollary 1. For given positive scalars \u03c4 , \u03c4d and \u03b3d, the system (29) is finite-time stable if there exist matrices P1 \u2208 S+n , P2 \u2208 S5n, Qi(i = 1, 2) \u2208 S+6n, Rj(j = 1, 2) \u2208 S+n , R3 \u2208 S+3n S1, Y \u2208 S+n , any matrices X1, X2 \u2208 R2n\u00d7n, L \u2208 R20n\u00d7n such that the following LMIs hold:[\n\u0398[\u03c4(t)=\u03c4 ] \u03a01 \u2217 \u03a51\n] < 0, [ \u0398[\u03c4(t)=0] \u03a02\n\u2217 \u03a52 ] < 0,[\n\u0398[\u03c4(t)=0] \u2212 \u03c42\u03f1 \u03a02 \u2217 \u03a52\n] < 0, (30)\nP2 + \u03b8p > 0, (31)\nRa > 0, Rb > 0, (32)\n\u03f50I \u2264 P\u03021 \u2264 \u03f51I, 0 \u2264 P\u03022 \u2264 \u03f52I, 0 \u2264 Q\u03021 \u2264 \u03f53I, 0 \u2264 Q\u03022 \u2264 \u03f54I, 0 \u2264 R\u03021 \u2264 \u03f55I, 0 \u2264 R\u03022,\u2264 \u03f56I, 0 \u2264 R\u03023 \u2264 \u03f57I, 0 \u2264 S\u03021 \u2264 \u03f58I, 0 \u2264 Y\u0302 \u2264 \u03f59I, (33)\ne\u03b1Tf\u03a0c1 < \u03f50c2. (34)\nProof. Similarly to the proof of Theorem 1, therefore, it is omitted here.\nRemark 8. The generalized neural networks (29) without distributed delay and W is identity matrix (B2 = 0 and W = I) can be written as follows:\nz\u0307(t) = \u2212A0z(t) +A1f(z(t)) +A2g(z(t\u2212 \u03c4(t))) z(t) = \u03d5(t), t \u2208 [\u2212\u03c4, 0] (35)\nsatisfying 0 \u2264 \u03c4(t) \u2264 \u03c4 and \u03c4\u0307(t) \u2264 \u03c4d, which mean that the system (35) becomes a special case of the system (29).\nCorollary 2. For given positive scalars \u03c4 and \u03c4d, the system (35) is asymptotically stable if there exist matrices P1 \u2208 S+n , P2 \u2208 S5n, Qi(i = 1, 2) \u2208 S+6n, Rj(j = 1, 2) \u2208 S+n , R3 \u2208 S+3n S1, Y \u2208 S+n , any matrices X1, X2 \u2208 R2n\u00d7n, L \u2208 R20n\u00d7n such that the following LMIs hold:[\n\u03a3[\u03c4(t)=\u03c4 ] \u03a01 \u2217 \u03a51\n] < 0, [ \u03a3[\u03c4(t)=0] \u03a02\n\u2217 \u03a52 ] < 0,[\n\u03a3[\u03c4(t)=0] \u2212 \u03c42\u03f1 \u03a02 \u2217 \u03a52\n] < 0, (36)\nP2 + \u03b8p > 0, (37)\nRa > 0, Rb > 0. (38)\nProof. The proof of Corollary 2 is similar to the proof of Theorem 1, hence it is omitted here.\nRemark 9. As demonstrated previously, we can derive a stability criterion for neural networks with time-varying delay, even if the delay rate is \u03c4 . Our results are more effective, as illustrated by the numerical example section.\nC. FINITE-TIME EXTENDED DISSIPATIVITY ANALYSIS In this section, we look at the finite-time extended dissipativity performance of generalized neural networks with discrete and distributed time-varying delays as follows:\nz\u0307(t) = \u2212A0z(t) +A1f(Wz(t)) +A2g(Wz(t\u2212 \u03c4(t)))\n+A3 \u222b t t\u2212\u03b3(t) h(Wz(s))ds+A4\u03c9(t),\ny(t) = B0z(t),\nz(t) = \u03d5(t), t \u2208 [\u2212\u03c4, 0]. (39)\nWe define:\n\u039e\u0304[\u03c4(t)] = \u03a61[\u03c4(t)] +\u03a62 +\u03a62[\u03c4(t)] +\u03a63 +\u03a63[\u03c4(t)]\n+\u03a64[\u03c4(t)] +\u03a65 +\u03a6z + \u03bd1 + \u03bd2 + \u03bd3 + \u03bd4 + \u03bd5 \u2212 \u03c42\u03f1\u2212 \u03b1eT1 P1e1 \u2212 eT1 BT0 \u21261B0e1 \u2212 Sym{eT1 BT0 \u21262e21} \u2212 eT21\u21263e21,\n\u03f511 = \u03bbmax(B T 0 B0), \u03f512 = \u03bbmax(\u2126 T 2 \u21262),\n\u03f513 = \u03bbmax(\u21263).\nTheorem 2. For given positive scalars \u03c4 , \u03c4d and \u03b3d, the system (39) is finite-time extened dissipativity respecting (c1, c2, Tf , V, \u03c9) if there exist matrices P1 \u2208 S+n , P2 \u2208 S5n, Qi(i = 1, 2) \u2208 S+6n, Rj(j = 1, 2) \u2208 S+n , R3 \u2208 S + 3n\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nS1, Y, M \u2208 S+n , any matrices X1, X2 \u2208 R2n\u00d7n, L \u2208 R21n\u00d7n such that the following LMIs hold:[\n\u039e\u0304[\u03c4(t)=\u03c4 ] \u03a01 \u2217 \u03a51\n] < 0, [ \u039e\u0304[\u03c4(t)=0] \u03a02\n\u2217 \u03a52 ] < 0,[\n\u039e\u0304[\u03c4(t)=0] \u2212 \u03c42\u03f1 \u03a02 \u2217 \u03a52\n] < 0, (40)\nP2 + \u03b8p > 0, (41)\nRa > 0, Rb > 0, (42)\ne\u2212\u03b1TfP1 \u2212BT0 \u03c94B0 > 0, (43)\ne\u03b1Tf [\u03f511d+ (\u03f512 + \u03f513)\u03c9f ] < \u03f50c2. (44)\n\u03f50I \u2264 P\u03021 \u2264 \u03f51I, 0 \u2264 P\u03022 \u2264 \u03f52I, 0 \u2264 Q\u03021 \u2264 \u03f53I, 0 \u2264 Q\u03022 \u2264 \u03f54I, 0 \u2264 R\u03021 \u2264 \u03f55I, 0 \u2264 R\u03022,\u2264 \u03f56I, 0 \u2264 R\u03023 \u2264 \u03f57I, 0 \u2264 S\u03021 \u2264 \u03f58I, 0 \u2264 Y\u0302 \u2264 \u03f59I, 0 \u2264Ma \u2264 \u03f510I, (45)\nProof. By using LKF and the proof of Theorem 1, we have\nV\u0307 (zt, t)\u2212 \u03b1V (zt, t)\u2212 J(t) < \u03beT (t)\u039e\u0304\u03c4(t)\u03be(t) < 0. (46)\nBy multiplying of (46) with e\u2212\u03b1t and integrating on [0, t], we obtain\nV (zt, t) < e \u03b1t [ V (z0, 0) + \u222b t 0 J(s)ds ] .\nFrom the condition V (z0, 0) = 0 and 0 < zT (t)P1z(t) < V (zt, t), we get\n0 < e\u2212\u03b1tzT (t)P1z(t) < e \u2212\u03b1tV (zt, t) < \u222b t 0 J(s)ds. (47)\nAccording to Assumption 4, consider the two case \u21264 = 0 and \u21264 > 0.\ncase I When \u21264 = 0,\u222b Tf 0 J(t)dt\u2212 sup 0\u2264t\u2264Tf yT (t)\u21264y(t) = \u222b Tf 0 J(t)dt \u2265 0.\ncase II When \u21264 > 0, we have \u21261 = 0, \u21262 = 0 and \u21263 > 0. From (47) and for all t \u2208 [0, Tf ], we can get\u222b Tf\n0 J(s)ds \u2265 \u222b t 0 J(s)ds > e\u2212\u03b1tz(t)P1z(t) > 0.\nAccording to condition (43), we obtain\u222b Tf 0 J(s)ds \u2265 zT (t)BT0 \u21264B0z(t) = yT (t)\u21264y(t). Hence, we get\u222b Tf 0 J(s)ds\u2212 sup 0\u2264t\u2264Tf yT (t)\u21264y(t) \u2265 0.\nSo, the extended dissipativity performance proof is finished. Next, we prove the finite time boundedness as follows.\nV (zt, t) < e \u2212\u03b1t \u222b t 0 J(s)ds.\nFor \u21261 \u2264 0, we get\nV (zt, t) < e \u2212\u03b1t \u222b t 0 [2yT (s)\u21262\u03c9(s) + \u03c9 T (s)\u21263\u03c9(s)]ds.\nFrom V (zt, t) \u2265 zT (t)P1z(t) \u2265 \u03bbmin(P\u0302 )zT (t)V z(t) = \u03f50z T (t)V z(t), it can be expressed as\nzT (t)V z(t)\n\u2264 e \u03b1Tf\n\u03f50 \u222b Tf 0 [2yT (s)\u21262\u03c9(s) + \u03c9 T (s)\u21263\u03c9(s)]ds\n= e\u03b1Tf\n\u03f50 \u222b Tf 0 [2zT (s)BT0 \u21262\u03c9(s) + \u03c9 T (s)\u21263\u03c9(s)]ds.\nBy applying Lemma 6, we obtain\n2zT (t)BT0 \u21262\u03c9(t) \u2264 zT (t)BT0 B0z(t) + \u03c9T (t)\u2126T2 \u21262\u03c9(t).\nFrom Assumption (2) and (3), we get\nzT (t)V z(t)\n\u2264 e \u03b1Tf\n\u03f50 \u222b Tf 0 [zT (t)BT0 B0z(t) + \u03c9 T (t)\u2126T2 \u21262\u03c9(t)\n+ \u03c9T (s)\u21263\u03c9(s)]ds\n\u2264 e \u03b1Tf\n\u03f50 [\u03f511d+ (\u03f512 + \u03f513)\u03c9f ] .\nFrom condition (44), we obtain\nzT (t)V z(t) < c2.\nAs a result, the system (39) is finite-time bounded with an extended dissipativity. The proof is now complete.\nIV. NUMERICAL EXAMPLES\nNext, we show numerical examples to demonstrate the efficientcy of the present results.\n12 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nExample 1. Consider the generalized neural networks described in (5) with the following matrix parameters:\nA0 = diag{8.2345, 7.1258, 6.9563}, Fm = Gm = Hm = diag{0, 0, 0}, Fp = diag{0.3457, 0.5378, 0.1852}, GP = diag{1.2539, 0.1258, 0.5971}, HP = diag{1.7509, 0.0211, 0.0913},\nA1 =  1.2357 \u22121.5634 1.6938\u22121.5361 1.3208 \u22121.7030 1.8239 \u22121.4675 1.6998  , A2 =\n0.88 1.22 1.021.57 1.07 0.33 1.55 0.92 1.11  , A3 =\n 1.35 0.25 0.64\u22121.82 \u22120.29 \u22120.12 0.36 0.87 1.11  , A4 =\n0.2 \u22120.6 0.80.3 \u22120.2 0.2 0.1 \u22120.5 0.7  , W =\n12.3654 2.5876 \u22120.97827.5867 22.5513 3.5236 0.8562 \u22122.7190 \u221221.5037  , f(z) = [0.3457 tanh(z1), 0.5378 tanh(z2), 0.1852 tanh(z3)] T , g(z) = [1.2539 tanh(z1), 0.1258 tanh(z2), 0.5971 tanh(z3)] T , h(z) = [1.7509 tanh(z1), 0.0211 tanh(z2), 0.0913 tanh(z3)] T .\nLet the discrete time-varying is \u03c4(t) = 0.8+0.5 sin(t), the distributed time-varying delays is \u03b3(t) = 0.4+0.2 sin(t) and the external disturbance is \u03c9(t) = 11+et . For given scalars \u03c4 = 0.5, \u03c9f = 0.1, c1 = 1.12, T = 30, \u03b1 = 0.1 and V is identity matrix. Solving LMIs (6)-(10) in Theorem 1, we obtain c2 = 3.56.\nFor an initial condition \u03d5(t) = [\u22120.1 0.4 0.1]T , figure 1 demonstrates the trajectories of solutions z1(t), z2(t), and z3(t) of generalized neural networks (5) with discrete time-varying delay (\u03c4(t)) and distributed time-varying delay (\u03b3(t)) via various activation functions f(z), g(z), and h(z). Figure 2 illustrates the time history of zT (t)z(t) for the delay generalized neural network system (5). In conclusion, system (5) is finite-time boundedness with respect to (1.12, 3.56, 30, I, 0.1). Thus, this proves the effectiveness of our obtained results in Theorem 1.\nExample 2. Consider the generalized neural networks described in (29) with the following matrix parameters:\nA0 = diag{2, 2}, Fm = Gm = Hm = diag{0, 0}, Fp = diag{0.2, 0.3}, GP = diag{0.4, 0.6},\nHP = diag{1, 0.5}, A1 = [ 1 1 \u22121 \u22121 ] ,\nA2 = [ 0.88 1 1 1 ] , A3 = [ 0.2 \u22120.6 0.3 0.2 ] , W = [ 1.35 0.45 0.21 1.29 ] ,\nf(z) = [0.2 tanh(z1), 0.3 tanh(z2)] T ,\n0 10 20 30 40 50 60 70 80 90 100\nTime t\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nz 1 (t\n), z\n2 (t\n), z\n3 (t\n)\nz 1 (t) z 2 (t) z 3 (t)\nLet the discrete time-varying is \u03c4(t) = 0.7+0.4 sin(t), the distributed time-varying delays is \u03b3(t) = 0.9+0.3 sin(t). For given scalars \u03c4 = 0.5, c1 = 1, \u03c9f = 0.1, T = 30, \u03b1 = 0.1 and V is identity matrix. Solving LMIs (30)-(34) in Corollary 1, we obtain c2 = 3.89.\nFor an initial condition \u03d5(t) = [\u22120.2 0.2]T , figure 5 demonstrates the trajectories of solution z1(t) and z2(t) of generalized neural networks (29) with various activation functions and mixed time-varying. Figure 6 illustrates the time history of zT (t)z(t) for the delay generalized neural network system (29). In conclusion, system (29) is finite-time stable with respect to (1, 3.89, 30, I, 0.1). Thus, this proves the effectiveness of our obtained results in Corollary 1.\nExample 3. Consider the neural networks described in (35)\nVOLUME 4, 2016 13\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwith the following matrix parameters:\nA = diag{7.3458, 6.9987, 5.5949}, W0 = diag{0, 0, 0}, W1 = diag{1, 1, 1}, Fm = diag{0, 0, 0},\nFp = 0.3680 0 00 0.1795 0 0 0 0.2876  , W =\n13.6014 \u22122.9616 \u22120.69367.4736 21.6810 3.2100 0.7290 \u22122.6334 \u221220.1300  , f(z) = [0.3680 tanh(z1), 0.1795 tanh(z2), 0.2876 tanh(z3)] T .\nTable 1 lists the proposed criteria, the maximum delay bounds with \u03c4 calculated by the Corollary 2. Furthermore particular, we compare the obtained results to those that have already been published. The results demonstrate that the stability conditions given in this article are more efficient than those described in the previous literature.\nExample 4. Consider the neural networks described in (35)\nTABLE 1. Maximum allowable bounds of \u03c4 with different \u03c4d in Example 3\n\u03c4 0.00 0.10 0.50 [41] 1.5575 0.9430 0.4417 [42] 1.6409 0.9962 0.4470 [11] 1.7250 1.0408 0.4480 [43] 1.7302 1.0453 0.4486 [44] 1.8898 1.1114 0.4514 [45] 1.8899 1.1194 0.4599 [5] 1.9349 1.1365 0.4678\n[37] 3.1150 1.4410 1.0299 Corollary 2 3.9574 1.9521 1.8366\nwith the matrix parameters in the following:\nA = diag{1.5, 0.7}, W = diag{1, 1}, Fp = diag{0.3, 0.8}, Fm = diag{0, 0},\nW0 = [ 0.0503 0.0454 0.0987 0.2075 ] , W1 = [ 0.2381 0.9320 0.0388 0.5062 ] .\nLet the neuron activation function is taken as f(z) = [0.3 tanh(z1), 0.8 tanh(z2)]\nT . Table 2 displays the proposed conditions and maximum delay bounds computed by Corollary 2. In addition, we compare the obtained results to those of previously published studies. The results demonstrate that the stability conditions presented in this paper are greater than those found in the existing literature.\nTABLE 2. Maximum allowable bounds of \u03c4 with different \u03c4d in Example 4\n\u03c4 0.40 0.45 0.50 0.55 [46] 4.6569 3.7268 3.4076 3.2841 [47] 4.5543 3.8364 3.5583 3.4110 [48] 7.6697 6.7287 6.4126 3.2569 [49] 8.3498 7.3817 7.0219 6.8156 [40] 10.1095 8.6732 8.1733 7.8993 [36] 10.5730 9.3566 8.8467 8.5176 [45] 16.8020 11.6745 9.9098 9.0062 [5] 17.2697 12.0698 10.2903 9.3879 [37] 19.5194 12.2110 12.4201 10.3990 Corollary 2 20.0598 13.0115 13.2116 11.3612\nExample 5. Consider the neural networks described in (35) with the matrix parameters as follows:\nA = diag{2, 2}, W = diag{1, 1}, Fp = diag{0.4, 0.8}, Fm = diag{0, 0},\nW0 = [ 1 1 \u22121 \u22121 ] , W1 = [ 0.88 1 1 1 ] .\nLet the neuron activation function is taken as f(z) = [0.4 tanh(z1), 0.8 tanh(z2)]\nT . The proposed criteria, the maximum delay bounds with \u03c4 estimated by the Corollary 2 are shown in Table 3. Furthermore, we compare the results with previously published research. The results suggest that the stability conditions shown in this paper are superior to those previously outlined in the literature.\n14 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nA0 = diag{1.2769, 0.6231, 0.9230, 0.4480}, W = diag{1, 1, 1, 1}, Fm = diag{0, 0, 0, 0}, Fp = diag{0.1137, 0.1278, 0.7994, 0.2368},\nA1 =  \u22120.0373 0.4852 \u22120.3351 0.2336 \u22121.6033 0.5988 \u22120.3224 1.2352 0.3394 \u22120.0860 \u22120.3824 \u22120.5785 \u22120.1311 0.3253 \u22120.9534 \u22120.5015  ,\nA2 =  0.8674 \u22121.2405 \u22120.5325 0.0220 0.0474 \u22120.9164 0.0360 0.9816 1.8495 2.6117 \u22120.3788 0.8428 \u22122.0413 0.5179 1.1734 \u22120.2775  . Table 4 displays the proposed criteria, the maximum delay bounds with \u03c4 computed by Corollary 2. Also, we compare the obtained results to previously published research. The results suggest that this paper\u2019s stability conditions are better than those stated in previous publications.\nRemark 10. This paper extends the proof by incorporating Jensen\u2019s integral inequality, extended Wirtinger\u2019s integral inequalities, and orthogonal polynomials-based integral inequality with improved LKFs. Consequently, our maximum delay outperforms the existing literature, as presented in Tables 1\u20134.\nExample 7. Consider the generalized neural networks described in (39) with the following matrix parameters:\nA0 = diag{1, 1}, W = diag{1, 1}, Fm = Gm = Hm = diag{0, 0}, Fp = diag{0.12, 0.28}, GP = diag{0.24, 0.38},\nHP = diag{0.35, 0.49}, A1 = [ 1.188 0.09 0.09 1.188 ] ,\nA2 = [ 0.09 0.14 0.05 0.09 ] , A3 = [ 0.44 \u22120.21 0.29 0.41 ] ,\nLet the discrete time-varying is \u03c4(t) = 0.7+0.5 sin(t), the distributed time-varying delays is \u03b3(t) = 0.9+0.5 sin(t) and the external disturbance is \u03c9(t) = \u221a 0.1cos(t). For given scalars \u03c4 = 0.5, c1 = 1.2, \u03c9f = 0.1, T = 30, \u03b1 = 0.1 and V is identity matrix. Solving LMIs (40)-(44) in Theorem 2, we obtain c2 = 4.12.\nFor an initial condition \u03d5(t) = [\u22121 1]T , figure 5 demonstrates the trajectories of solution z1(t) and z2(t) of generalized neural networks (39) with discrete time-varying delay (\u03c4(t)) and distributed time-varying delay (\u03b3(t)) via various activation functions f(z) = [0.12 tanh(z1), 0.28 tanh(z2)]\nT , g(z) = [0.12(|z1 + 1| \u2212 |z1 \u2212 1|), 0.19(|z2 + 1| \u2212 |z2 \u2212 1|)]T , and h(z) = [0.35 tanh(z1), 0.49 tanh(z2)]\nT . Figure 6 illustrates the time history of zT (t)z(t) for the delay generalized neural network system (39). In conclusion, system (39) is finite-time stable with respect to (1.2, 4.12, 30, I, 0.1). Thus, this proves the effectiveness of our obtained results in Theorem 2.\nV. CONCLUSION This paper employed the improved LKF to investigate the problem of finite-time extended dissipativity for generalized\nVOLUME 4, 2016 15\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5. Matrices for each case of extend dissipativity performance.\nMethod \u21261 \u21262 \u21263 \u21264 H\u221e performance -I 0 \u03b32dI 0 L2 \u2212 L\u221e performance 0 0 \u03b32dI I Passivity performance 0 I \u03b3dI 0 Dissipativity performance -I I (2\u2212 \u03b2)I 0\nTABLE 6. Minimum \u03b3d and Maximum \u03b2 for different values of \u03c4d in Example 6\nMethod \u03c4d = 0.10 \u03c4d = 0.50 \u03c4d = 0.90 H\u221e performance 0.0871 0.1521 0.1856 L2 \u2212 L\u221e performance 0.2511 0.3678 0.5013 Passivity performance 0.0197 0.0427 0.0599 Dissipativity performance 1.9895 1.9912 1.9976\nneural networks with time-varying delays. To estimate the bound of the time derivative, we constructed a suitable LKF and utilized effective inequalities, including orthogonal polynomials-based integral inequality, Jensen\u2019s integral inequality, and extended Wirtinger\u2019s integral inequality. This allowed us to obtain several sufficient conditions as linear matrix inequalities (LMIs). This article is less conservative than other recently published publications by stability criteria. However, there are numerical examples to demonstrate that the presented results work and are better compared to [5], [11], [36]\u2013[38], [40]\u2013[55]. In future work, this work can be extended to many dynamical systems, such as neutral-type neural networks and T-Sfuzzy neural networks, with more efficient techniques [56]\u2013[65].\nREFERENCES [1] T. H. Lee, J. H. Park, M. J. Park, O. M. Kwon, and H. Y. Jung, \u201cOn stability\ncriteria for neural networks with time-varying delay using Wirtinger-based multiple integral inequality,\u201d J. Franklin Inst, vol. 352, no. 12, pp. 5627\u2013 5645, Sep. 2015. [2] J. Cheng, S. Zhong, Q. Zhong, H. Zhu, and Y. Du, \u201c Finite-time boundedness of state estimation for neural networks with time-varying delays,\u201d Neurocomputing, vol. 129, pp. 257\u2013264, Dec. 2017. [3] S. Shanmugam, S. A. Muhammed, and G. M. Lee, \u201cFinite-time extended dissipativity of delayed Takagi\u2013Sugeno fuzzy neural networks using a free-matrix-based double integral inequality,\u201d Neural Comput. Appl., vol. 32, pp. 8517\u20138528, Jul. 2019. [4] S. Saravanan, M. S. Ali, A. Alsaedi, and B. Ahmad, \u201cFinite-time passivity for neutral-type neural networks with time-varying delays\u2013via auxiliary function-based integral inequalities,\u201d Nonlinear Anal, vol. 25, no. 2, pp. 206\u2013224, Mar. 2020. [5] Z. Feng, H. Shao, and L. Shao, \u201cFurther improved stability results for generalized neural networks with time-varying delays,\u201d Neurocomputing, vol. 367, pp. 308\u2013318, Jul. 2019. [6] X. Lv and X. Li, \u201cDelay-dependent dissipativity of neural networks with mixed non-differentiable interval delays,\u201d Neurocomputing, vol. 267, pp. 85\u201394, Apr. 2017. [7] C. Li, G. Feng, \u201cDelay-interval-dependent stability of recurrent neural networks with time-varying delay,\u201d Neurocomputing, vol. 72, pp. 1179\u2013 1183, Jan. 2009. [8] Z. Wang, Y. Liu, X. Liu, \u201c, On global asymptotic stability of neural networks with discrete and distributed delays,\u201d Physics Letters A, vol. 345, pp. 299\u2013308, Oct. 2005. [9] R. Manivannan, R. Samidurai, J. Cao, A. Alsaedi, and F. E. Alsaadi, \u201cNon-fragile extended dissipativity control design for generalized neural networks with interval time-delay signals,\u201d Asian J. Control, vol. 21, no. 1, pp. 559\u2013580, Mar. 2019. [10] P. Niamsup, K. Ratchagit, and V. N. Phat, \u201cNovel criteria for finite-time stabilization and guaranteed cost control of delayed neural networks,\u201d Neurocomputing, vol. 160, pp. 281\u2013286, Feb. 2015.\n[11] H. B. Zeng, Y. He, M. Wu, and S. P. Xiao, \u201cStability analysis of generalized neural networks with time-varying delays via a new integral inequality,\u201d Neurocomputing, vol. 161, no. 1, pp. 148\u2013154, Aug. 2015. [12] J. Sun, and J. Chen, \u201cStability analysis of static recurrent neural networks with interval time-varying delay,\u201d Appl. Math. Comput, vol. 221, pp. 111\u2013 120, Sep. 2013. [13] J. Chen, J. Sun, G. P. Liu, and D. Rees, \u201cNew delay-dependent stability criteria for neural networks with time-varying interval delay,\u201d Phys. Lett. A, vol. 374, no. 43, pp. 4397\u20134405, Sep. 2010. [14] X. M. Zhang, and Q. L. Han, \u201cGlobal asymptotic stability for a class of generalized neural networks with interval time-varying delays,\u201d IEEE trans. neural netw, vol. 22, no. 8, pp. 1180\u20131192, Jun. 2011. [15] Z. W. Chen, J. Yang, and S. M. Zhong, \u201cDelay-partitioning approach to stability analysis of generalized neural networks with time-varying delay via new integral inequality,\u201d Neurocomputing, vol. 191, pp. 380\u2013387, May. 2016. [16] J. A. Wang, L. Fan, X. Y. Wen, and Y. Wang, \u201cEnhanced stability results for generalized neural networks with time-varying delay,\u201d J Franklin Inst, vol.357 , no. 11, pp. 6932\u20136950, Jul. 2020. [17] C. Phanlert, T. Botmart, W. Weera, and P. Junsawang, \u201cFinite-Time Mixed H\u221e/passivity for neural networks with mixed interval time-varying delays using the multiple integral Lyapunov-Krasovskii functional,\u201d IEEE Access, vol. 9, pp. 89461-89475, Jun. 2021. [18] O. M. Kwon, M. J. Park, J. H. Park, S. M. Lee, and E. J. Cha, \u201cOn less conservative stability criteria for neural networks with time-varying delays utilizing Wirtinger-based integral inequality,\u201d Math. Probl. Eng, vol. 2014, pp. 1\u201313, Jun. 2014. [19] H. B. Zeng, Y. He, M. Wu, and J. She, \u201cNew results on stability analysis for systems with discrete distributed delay,\u201d Automatica, vol. 60, pp. 189-192, Oct. 2015. [20] K. Shi, H. Zhu, S. Zhong, Y. Zeng, Y. Zhang, and W. Wang, \u201cStability analysis of neutral type neural networks with mixed time-varying delays using triple-integral and delay-partitioning methods,\u201d ISA Trans, vol. 58, pp. 85\u201395, Sep. 2015. [21] C. Briat, \u201cConvergence and equivalence results for the Jensen\u2019s inequalityApplication to time-delay and sampled-data systems,\u201d IEEE Trans. Autom. Control, vol. 56, no. 7, pp. 1660\u20131665, Feb. 2011. [22] X. Wang, K. She, S. Zhong, and J. Cheng, \u201cOn extended dissipativity analysis for neural networks with time-varying delay and general activation functions,\u201d Adv. Differ. Equ, vol. 2016, no. 1, pp. 1\u201316, Mar. 2016. [23] R. Manivannan, G. Mahendrakumar, R. Samidurai, J. Cao, and A. Alsaedi, \u201cExponential stability and extended dissipativity criteria for generalized neural networks with interval time-varying delay signals,\u201d Asian J. Control, vol. 354, no. 11, pp. 4353\u20134376, Apr. 2017. [24] R. Manivannan, R. Samidurai, J. Cao, A. Alsaedi, and F. E. Alsaadi, \u201cNon-fragile extended dissipativity control design for generalized neural networks with interval time-delay signals,\u201d Asian J. Control, vol. 2 no. 1, pp. 559\u2013580, May. 2019. [25] R. Saravanakumar, H. Mukaidani, and P. Muthukumar, \u2018Extended dissipative state estimation of delayed stochastic neural networks,\u201d Neurocomputing, vol. 406, pp. 244\u2013252, Apr. 2020. [26] J. Xiao, and S. Zhong, \u201cExtended dissipative conditions for memristive neural networks with multiple time delays,\u201d Appl. Math. Comput, vol. 323, pp. 145-163, Apr. 2018. [27] Y. Liu, Z. Deng, P. Li, and B. Zhang, \u201cFinite-time non-fragile extended dissipative control of periodic piecewise time-varying systems,\u201d IEEE Access, vol. 8, pp. 136512\u2013136523, Jul. 2020. [28] P. Dorato, \u201cShort-time stability in linear time-varying systems,\u201d Proc. IRT Int. Conv. Rec, vol. 4, pp. 83-\u201387, Jun. 1961. [29] F. Amato, M. Ariola, and P. Dorato, \u201cFinite-time control of linear systems subject to parametric uncertainties and disturbances,\u201d Automatica, vol. 37, no. 9, pp. 1459\u20131463, Sep. 2001. [30] S. He, Q. Ai, C. Ren, J. Dong, and F. Liu, \u201cFinite-time resilient controller design of a class of uncertain nonlinear systems with time-delays under asynchronous switching,\u201d IEEE Trans. Syst. Man Cybern. Syst, vol. 49, no. 2, pp. 281\u2013286, Feb. 2014. [31] X. Liu, J. H. Park, N. Jiang, and J. Cao, \u201cNonsmooth finite-time stabilization of neural networks with discontinuous activations,\u201d Neural Netw, vol. 52, pp. 25\u201332, Apr. 2014. [32] M. S. Ali, S. Saravanan, and S. Arik, \u201cFinite-time H\u221e state estimation for switched neural networks with time-varying delays,\u201d Neurocomputing, vol. 207, pp. 580\u2013589, Sep. 2016.\n16 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[33] M. S. Ali, and S. Saravanan, \u201cRobust finite-time H\u221e control for a class of uncertain switched neural networks of neutral-type with distributed timevarying delays,\u201d Neurocomputing, vol. 177, pp. 454\u2013468, Feb. 2016. [34] S. He and F. Liu, \u201cOptimal finite-time passive controller design for uncertain nonlinear Markovian jumping systems,\u201d J. Franklin Inst, vol. 351, no. 7, pp. 3782\u20133796, Jul. 2014. [35] Y. Liu, Z. Deng, P. Li, and B. Zhang, \u201cFinite-time non-fragile extended dissipative control of periodic piecewise time-varying systems,\u201d IEEE Access, vol. 8, pp. 136512\u2013136523, Jul. 2020. [36] C. Hua, Y. Wang, and S. Wu, \u201cStability analysis of neural networks with time-varying delay using a new augmented Lyapunov\u2013Krasovskii functional,\u201d Neurocomputing, vol. 332, pp. 1\u20139, Mar. 2019. [37] C. Phanlert, T. Botmart, W. Weera, and P. Junsawang, \u201cFinite-Time Mixed H\u221e/passivity for neural networks with mixed interval time-varying delays using the multiple integral Lyapunov-Krasovskii functional,\u201d IEEE Access, vol. 9, pp. 89461\u201389475, Jun. 2021. [38] C. Zamart, T. Botmart, W. Weera, and S. Charoensin, \u201cNew delaydependent conditions for finite-time extended dissipativity based nonfragile feedback control for neural networks with mixed interval timevarying delays,\u201d Math Comput Simul, Vol. 201, pp. 684\u2013713, Nov. 2022. [39] H. Shao, H. Li, and C. Zhu, \u201cNew stability results for delayed neural networks,\u201d Appl. Math. Comput., Vol. 311, pp. 324\u2013334, Oct. 2017. [40] H. Shao, H. Li, and L. Shao, \u201cImproved delay-dependent stability result for neural networks with time-varying delays,\u201d ISA Trans, vol. 80, pp. 35\u201342, Sep. 2018. [41] M. D. Ji, Y. He, C. K. Zhang, and M. Wu, \u201cNovel stability criteria for recurrent neural networks with time-varying delay,\u201d Neurocomputing, vol. 138, pp. 383\u2013391, Aug. 2014. [42] B. Wang, J. Yan, J. Cheng, and S. Zhong, \u201cNew criteria of stability analysis for gen- eralized neural networks subject to time-varying delayed signals,\u201d Appl. Math. Comput, vol. 314, pp. 322\u2014333, Dec. 2017. [43] C. K. Zhang, Y. He, L. Jiang, W. J. Lin, and M. Wu, \u201cDelay-dependent stability analysis of neural networks with time-varying delay: A generalized free-weighting-matrix approach,\u201d Appl. Math. Comput, vol. 294, pp. 102\u2013120, Feb. 2017. [44] X. M. Zhang, and Q. L. Han, \u201cGlobal asymptotic stability analysis for delayed neural networks using a matrix-based quadratic convex approach,\u201d Neural Netw, vol. 54, pp. 57\u201369, Jun. 2014. [45] M. J. Park, S. H. Lee, O. M. Kwon, and J. H. Ryu, \u201cEnhanced stability criteria of neural networks with time-varying delays via a generalized freeweighting matrix in- tegral inequality,\u201d J. Frankl. Inst, vol. 355, no. 14, pp. 6531\u20146548, Sep. 2018. [46] X. L. Zhu, D. Yue, and Y. Wang, \u201cDelay-dependent stability analysis for neural networks with additive time-varying delay components,\u201d IET Control Theory Appl, vol. 7, no. 3, pp. 354\u2014362, Feb. 2013. [47] T. Li, T. Wang, A. Song, and S. Fei, \u201cCombined convex technique on delay-dependent stability for delayed neural networks,\u201d IEEE Trans. Neural Netw. Learn. Syst, vol. 24, no. 9, pp. 1459\u20131466, Apr. 2013. [48] C. K. Zhang, Y. He, L. Jiang, and M. Wu, \u201cStability analysis for delayed neural net- works considering both conservativeness and complexity,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 7, pp. 1486\u20141501, Jul. 2015. [49] C. K. Zhang, Y. He, L. Jiang, W. J. Lin, and M. Wu, \u201cDelay-dependent stability analysis of neural networks with time-varying delay: A generalized free-weighting-matrix approach,\u201d Appl. Math. Comput, vol. 294, pp. 102\u2013120, Feb. 2017. [50] O. M. Kwon, J. H. Park, S. M. Lee, and E. J. Cha, \u201cNew augmented Lyapunov\u2013Krasovskii functional approach to stability analysis of neural networks with time-varying delays,\u201d Nonlinear Dyn, vol. 76, no. 1, pp. 221\u2013236, Nov. 2013. [51] B. Yang, J. Wang, and J. Wang, \u201cStability analysis of delayed neural networks via a new integral inequality,\u201d Neural Netw, vol. 88, no. 1, pp. 49\u201357, Apr. 2017. [52] L. Hu, H. Gao, and W. X. Zheng, \u201cNovel stability of cellular neural networks with interval time-varying delay,\u201d Neural Networks, Vol. 21, no. 10, pp. 1458\u20131463, Dec. 2008. [53] T. Li, A. Song, M. Xue, and H. Zhang, \u201cStability analysis on delayed neural networks based on an improved delay-partitioning approach,\u201d J. Comput. Appl. Math, Vol. 235, no. 9, pp. 3086\u20133095, Mar. 2011. [54] O. M. Kwon, S. M. Lee, J. H. Park, and E. J. Cha, \u201cNew approaches on stability criteria for neural networks with interval time-varying delays,\u201d Appl. Math. Comput, Vol. 218, no. 19, pp. 9953\u20139964, Jun. 2012.\n[55] J. K. Tian and Y. M. Liu, \u201cImproved delay-dependent stability analysis for neural networks with interval time-varying delays,\u201d Math. Probl. Eng, Vol. 2015, pp. 1\u201310, Nov. 2015. [56] H. B. Zeng, X. G. Liu, and W. Wang, \u201cA generalized free-matrix-based integral inequality for stability analysis of time-varying delay systems,\u201d Appl. Math. Comput., Vol. 354, pp. 1\u20138, Aug. 2019. [57] H. C. Lin, H. B. Zeng, X. M. Zhang, and W. Wang, \u201cStability analysis for delayed neural networks via a generalized reciprocally convex inequality,\u201d IEEE Trans. Neural Netw. Learn. Syst., early access, Feb. 02, 2022, doi:10.1109/TNNLS.2022.3144032. [58] H. B. Zeng, H. C. Lin, Y. He, K. L. Teo and W. Wang, \u201cHierarchical stability conditions for time-varying delay systems via an extended reciprocally convex quadratic inequality,\u201d J. Frank. Inst., Vol. 357, no. 14, pp. 9930- 9941, Sep. 2020. [59] H. B. Zeng, H. C. Lin, Y. He, C. K. Zhang, and K. L. Teo, \u201cImproved negativity condition for a quadratic function and its application to systems with time-varying delay,\u201d IET Control Theory Appl., Vol. 14, no. 18, pp. 2989-2993, Oct. 2020. [60] H. B. Zeng, Z. L. Zhai, and W. Wang, \u201cHierarchical stability conditions of systems with time-varying delay. Applied Mathematics and Computation,\u201d Appl. Math. Comput., Vol. 404, 126222, Sep. 2021. [61] K. Liu, A. Seuret, and Y. Xia, \u201cStability analysis of systems with timevarying delays via the second-order Bessel\u2013Legendre inequality,\u201d Automatica, Vol. 76, pp. 138-142, Feb. 2017. [62] A. Seuret, and F. Gouaisbaut, \u201cStability of linear systems with timevarying delays using Bessel\u2013Legendre inequalities,\u201d IEEE Trans. Autom. Control, Vol. 63, no. 1, pp. 225-232, Jul. 2017. [63] C. K. Zhang, Y. He, L. Jiang, M. Wu, and Q. G. Wang, \u201cAn extended reciprocally convex matrix inequality for stability analysis of systems with time-varying delay,\u201d Automatica, Vol. 85, pp. 481-485, Nov. 2017. [64] C. K. Zhang, F. Long, Y. He, W. Yao, L. Jiang, and M. Wu, \u201cA relaxed quadratic function negative-determination lemma and its application to time-delay systems,\u201d Automatica, Vol. 113, 108764, Mar. 2020. [65] W. Wang, H. B. Zeng, K. L. Teo, and Y. J. Chen, \u201cRelaxed stability criteria of time-varying delay systems via delay-derivative-dependent slack matrices,\u201d Automatica, Vol. 360, no. 9, pp. 6099-6109, Jun. 2023.\nCHALIDA PHANLERT received the B.S. degree in mathematics and the M.S. degree in applied mathematics from Khon Kaen University, Khon Kaen, Thailand, in 2017 and 2019, respectively, where she is currently pursuing the Ph.D. degree in mathematics with the Department of Mathematics, Faculty of Science. She has been supported by the Science Achievement Scholarship of Thailand (SAST). Her research interests include stability of time-delay systems and stability of artificial neural\nnetworks.\nTHONGCHAI BOTMART received the B.S. degree in mathematics from Khon Kaen University, Khon Kaen, Thailand, in 2002, and the M.S. degree in applied mathematics and the Ph.D. degree in mathematics from Chiang Mai University, Chiang Mai, Thailand, in 2005 and 2011, respectively. He is currently an Associate Professor with the Department of Mathematics, Faculty of Science, Khon Kaen University. His research interests include stability theory of time-delay systems, non-\nautonomous systems, switched systems, artificial neural networks, complex dynamical networks, synchronization, control theory, and chaos theory.\nVOLUME 4, 2016 17\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWAJAREE WEERA received the B.S. degree in mathematics, the M.S. degree in applied mathematics, and the Ph.D. degree in mathematics from Chiang Mai University, Chiang Mai, Thailand, in 2005, 2007, and 2015, respectively. She is currently an Assistant Professor with the Department of Mathematics, Faculty of Science, Khon Kaen University, Thailand. Her research interests include stability theory of time-delay systems, stability analysis, neutral systems, switched systems,\nand artificial neural networks.\nPREM JUNSAWANG received the B.Sc. degree (Hons.) in mathematics from Khon Kaen University, Khon Kaen, Thailand, in 2004, and the M.Sc. degree in computational science and the Ph.D. degree in computer science from Chulalongkorn University, Bangkok, Thailand, in 2008 and 2018, respectively. He is currently a Lecturer with the Department of Statistics, Faculty of Science, Khon Kaen University. His research interests include artificial neural networks, synchronization, pattern\n18 VOLUME 4, 2016\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Finite-time extended dissipativity analysis for generalized neural networks with discrete and distributed time-varying delays",
    "year": 2023
}