{
    "abstractText": "Data analysis based on information from several sources is common in economic and biomedical studies. This setting is often referred to as the data fusion problem, which differs from traditional missing data problems since no complete data is observed for any subject. We consider a regression analysis when the outcome variable and some covariates are collected from two different sources. By leveraging the common variables observed in both data sets, doubly robust estimation procedures are proposed in the literature to protect against possible model misspecifications. However, they employ only a single propensity score model for the data fusion process and a single imputation model for the covariates available in one data set. It may be questionable to assume that either model is correctly specified in practice. We therefore propose an approach that calibrates multiple propensity score and imputation models to gain more protection based on empirical likelihood methods. The resulting estimator is consistent when any one of those models is correctly specified and is robust against extreme values of the fitted propensity scores. We also establish its asymptotic normality property and discuss the semiparametric estimation efficiency. Simulation studies show that the proposed estimator has substantial advantages over existing doubly robust estimators, and an assembled U.S. household expenditure data example is used for illustration.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Li"
        },
        {
            "affiliations": [],
            "name": "Shanshan Luo"
        },
        {
            "affiliations": [],
            "name": "Wangli Xu"
        }
    ],
    "id": "SP:cce53e9b45520cb38ce137542a6a2b208204e86b",
    "references": [
        {
            "authors": [
                "J.D. Angrist",
                "A.B. Krueger"
            ],
            "title": "The effect of age at school entry on educational attainment: an application of instrumental variables with moments from two samples",
            "venue": "Journal of the American Statistical Association,",
            "year": 1992
        },
        {
            "authors": [
                "K. Bandeen-roche",
                "D.L. Miglioretti",
                "S.L. Zeger",
                "P.J. Rathouz"
            ],
            "title": "Latent variable regression for multiple discrete outcomes",
            "venue": "Journal of the American Statistical Association,",
            "year": 1997
        },
        {
            "authors": [
                "R. Blundell",
                "L. Pistaferri",
                "I. Preston"
            ],
            "title": "Consumption inequality and partial insurance",
            "venue": "American Economic Review,",
            "year": 2008
        },
        {
            "authors": [
                "R. Bostic",
                "S. Gabriel",
                "G. Painter"
            ],
            "title": "Housing wealth, financial wealth, and consumption: New evidence from micro data",
            "venue": "Regional Science and Urban Economics,",
            "year": 2009
        },
        {
            "authors": [
                "M. Buchinsky",
                "F. Li",
                "Z. Liao"
            ],
            "title": "Estimation and inference of semiparametric models using data from several sources",
            "venue": "Journal of Econometrics,",
            "year": 2022
        },
        {
            "authors": [
                "W. Cao",
                "A.A. Tsiatis",
                "M. Davidian"
            ],
            "title": "Improving efficiency and robustness of the doubly robust estimator for a population mean with incomplete data",
            "year": 2009
        },
        {
            "authors": [
                "K.C.G. Chan",
                "S.C.P. Yam"
            ],
            "title": "Oracle, multiple robust and multipurpose calibration in a missing response problem",
            "year": 2014
        },
        {
            "authors": [
                "S. Chen",
                "D. Haziza"
            ],
            "title": "Multiply robust imputation procedures for the treatment of item nonresponse in surveys",
            "year": 2017
        },
        {
            "authors": [
                "X. Chen",
                "H. Hong",
                "A. Tarozzi"
            ],
            "title": "Semiparametric efficiency in gmm models with auxiliary data",
            "venue": "The Annals of Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "J. Currie",
                "A. Yelowitz"
            ],
            "title": "Are public housing projects good for kids",
            "venue": "Journal of Public Economics,",
            "year": 2000
        },
        {
            "authors": [
                "M. D\u2019Orazio",
                "M. Di Zio",
                "M. Scanu"
            ],
            "title": "Statistical Matching: Theory and Practice",
            "year": 2006
        },
        {
            "authors": [
                "X. d\u2019Haultfoeuille"
            ],
            "title": "A new instrumental method for dealing with endogenous selection",
            "venue": "Journal of Econometrics,",
            "year": 2010
        },
        {
            "authors": [
                "K. Evans",
                "B. Sun",
                "J. Robins",
                "E.J. Tchetgen Tchetgen"
            ],
            "title": "Doubly robust regression analysis for data fusion",
            "year": 2021
        },
        {
            "authors": [
                "B.S. Graham",
                "Pinto",
                "C.C. d. X",
                "D. Egel"
            ],
            "title": "Efficient estimation of data combination models by the method of auxiliary-to-study tilting (ast)",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "P. Han"
            ],
            "title": "Multiply robust estimation in regression analysis with missing data",
            "venue": "Journal of the American Statistical Association,",
            "year": 2014
        },
        {
            "authors": [
                "P. Han",
                "L. Kong",
                "J. Zhao",
                "X. Zhou"
            ],
            "title": "A general framework for quantile estimation with incomplete data",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2019
        },
        {
            "authors": [
                "P. Han",
                "L. Wang"
            ],
            "title": "Estimation with missing data: beyond double robustness",
            "year": 2013
        },
        {
            "authors": [
                "M. Hirukawa",
                "A. Prokhorov"
            ],
            "title": "Consistent estimation of linear regression models using matched data",
            "venue": "Journal of Econometrics,",
            "year": 2018
        },
        {
            "authors": [
                "W. Li",
                "Y. Gu",
                "L. Liu"
            ],
            "title": "Demystifying a class of multiply robust estimators",
            "year": 2020
        },
        {
            "authors": [
                "W. Miao",
                "W. Li",
                "W. Hu",
                "R. Wang",
                "Z. Geng"
            ],
            "title": "Invited commentary: estimation and bounds under data fusion",
            "venue": "American Journal of Epidemiology",
            "year": 2021
        },
        {
            "authors": [
                "W.K. Newey",
                "J.L. Powell"
            ],
            "title": "Instrumental variable estimation of nonparametric models",
            "year": 2003
        },
        {
            "authors": [
                "E.L. Ogburn",
                "K.E. Rudolph",
                "R. Morello-Frosch",
                "A. Khan",
                "J.A. Casey"
            ],
            "title": "A warning about using predicted values from regression models for epidemiologic inquiry",
            "venue": "American Journal of Epidemiology,",
            "year": 2021
        },
        {
            "authors": [
                "A.B. Owen"
            ],
            "title": "Empirical Likelihood. New York: Chapman and Hall-CRC",
            "year": 2001
        },
        {
            "authors": [
                "B.L. Pierce",
                "S. Burgess"
            ],
            "title": "Efficient design for mendelian randomization studies: subsample and 2-sample instrumental variable estimators",
            "venue": "American Journal of Epidemiology,",
            "year": 2013
        },
        {
            "authors": [
                "J. Qin",
                "J. Lawless"
            ],
            "title": "Empirical likelihood and general estimating equations",
            "venue": "The Annals of Statistics,",
            "year": 1994
        },
        {
            "authors": [
                "J. Qin",
                "B. Zhang"
            ],
            "title": "Empirical-likelihood-based inference in missing response problems and its application in observational studies",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2007
        },
        {
            "authors": [
                "J. Qin",
                "B. Zhang",
                "D.H. Leung"
            ],
            "title": "Empirical likelihood in missing data problems",
            "venue": "Journal of the American Statistical Association,",
            "year": 2009
        },
        {
            "authors": [
                "S. R\u00e4ssler"
            ],
            "title": "Statistical Matching: A Frequentist Theory, Practical Applications, and Alternative Bayesian Approaches, volume 168",
            "year": 2012
        },
        {
            "authors": [
                "G. Ridder",
                "R. Moffitt"
            ],
            "title": "The econometrics of data combination",
            "venue": "Handbook of Econometrics,",
            "year": 2007
        },
        {
            "authors": [
                "J.M. Robins",
                "A. Rotnitzky"
            ],
            "title": "Semiparametric efficiency in multivariate regression models with missing data",
            "venue": "Journal of the American Statistical Association,",
            "year": 1995
        },
        {
            "authors": [
                "D.B. Rubin"
            ],
            "title": "Multiple Imputation for Nonresponse in Surveys, volume 81",
            "year": 2004
        },
        {
            "authors": [
                "H. Shu",
                "Z. Tan"
            ],
            "title": "Improved methods for moment restriction models with data combination and an application to two-sample instrumental variable estimation",
            "venue": "Canadian Journal of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "A.A. Tsiatis"
            ],
            "title": "Semiparametric Theory and Missing Data",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 4.\n02 65\n7v 1\n[ st\nat .M\nE ]\nData analysis based on information from several sources is common in economic and biomedical studies. This setting is often referred to as the data fusion problem, which differs from traditional missing data problems since no complete data is observed for any subject. We consider a regression analysis when the outcome variable and some covariates are collected from two different sources. By leveraging the common variables observed in both data sets, doubly robust estimation procedures are proposed in the literature to protect against possible model misspecifications. However, they employ only a single propensity score model for the data fusion process and a single imputation model for the covariates available in one data set. It may be questionable to assume that either model is correctly specified in practice. We therefore propose an approach that calibrates multiple propensity score and imputation models to gain more protection based on empirical likelihood methods. The resulting estimator is consistent when any one of those models is correctly specified and is robust against extreme values of the fitted propensity scores. We also establish its asymptotic normality property and discuss the semiparametric estimation efficiency. Simulation studies show that the proposed estimator has substantial advantages over existing doubly robust estimators, and an assembled U.S. household expenditure data example is used for illustration.\nKeywords: Data fusion; Double robustness; Empirical likelihood; Extreme weights; Model calibration."
        },
        {
            "heading": "1 Introduction",
            "text": "There are many situations in practice where all relevant variables for addressing particular scientific hypotheses cannot be collected from a single data source. For example, in a study investigating effects of housing project participation on living qualities of poor families, the Current Population Survey data consists of project participation and the Census data consists of living quality attributes (Currie and Yelowitz, 2000; Shu and Tan, 2020). Another example studies the relationship between households\u2019 consumption and saving behavior, where information on consumption is available in the Panel Study of Income Dynamics, and information on wealth is obtained from separate data sources, e.g., Health and Retirement Survey (Blundell et al., 2008; Buchinsky et al., 2022). Two-sample instrumental variable analysis in causal inference literature is an additional example with important variables collected separately, and it includes two-sample Mendelian randomization as one of the most exciting applications in genetic epidemiology (Angrist and Krueger, 1992; Pierce and Burgess, 2013). It is natural to try to combine the two data sets to answer specific research questions in these scenarios. Analysis of such combined data is challenging, because no subject belongs to both sources and some variables available in one source are never observed in the other one. This setting is typically referred to as the data fusion problem (D\u2019Orazio et al., 2006; Ridder and Moffitt, 2007; Ra\u0308ssler, 2012). Due to violations of the positivity assumption for any subject, data fusion problems are distinct from most missing-data problems and require specific analysis techniques.\nIn this article, we consider the setting where a primary sample provides measurements of the outcome Y and covariates V , while an auxiliary sample contains measurements of V and additional covariates W . That is, the outcome Y is collected only in the primary data, a subset of covariates W is observed only in the auxiliary data, and the common variables V are available in both data sets. We are interested in regression analysis of\nthe outcome given all covariates, and our aim is to estimate the p-dimensional parameter \u03b80 = (\u03b8 T 10, \u03b8 T 20) T defined through E(Y |W,V ) = \u00b5(\u03b8T10W +\u03b8T20V ), where \u00b5(\u00b7) is some known monotone and continuously differentiable function. If a random sample of all variables is available, then consistent estimation of \u03b80 under regularity conditions is straightforward by solving a set of properly-chosen estimating equations. However, when two data sets from separate sources are fused together, the parameter of interest \u03b80 cannot even be simply identified from the observed data without additional conditions. A prominent strategy in existing literature is to impose one of the following conditional independence assumptions: Y \u22a5W | V or Y \u22a5 V |W (Ogburn et al., 2021; Miao et al., 2021). The former assumption is fundamental in statistical matching literature (D\u2019Orazio et al., 2006; Ra\u0308ssler, 2012; Hirukawa and Prokhorov, 2018), where the variables Y and W collected from separate samples are matched by the common variables V . The latter is analogous to exclusion restrictions in instrumental variable analysis, which forms the basis for the validity of two-sample instrumental variable estimators (Angrist and Krueger, 1992; Graham et al., 2016). However, these two assumptions are problematic in our setting if a potential nonnull association between Y and W (or V ) is the scientific hypothesis under consideration. We clearly pinpoint more reasonable conditions under which \u03b80 is identifiable in Section 2.\nWhen model identifiability has been guaranteed, a large class of semiparametric and parametric estimation methods for fused data sets is proposed. Chen et al. (2008) utilized nonparametric series estimation on a propensity score model for the data source process or an imputation model for the partly observed covariates to achieve consistent results in separable moment restriction models. However, such methods hinge on certain smoothness conditions that are often assumed and can be problematic with a high-dimensional vector of the common variables V . Recently, Graham et al. (2016) proposed a more flexibly parametric modeling approach, which is doubly robust in that it is consistent if the\npropensity score or the imputation model is correctly specified in a certain class of nuisance models. To break this limitation, Shu and Tan (2020) and Evans et al. (2021) developed general doubly robust estimators with unrestricted nuisance model specifications. Doubly robust procedures offer some protection against model misspecification, but as only a single propensity score model and a single imputation model are allowed, they may not provide sufficient protection and it is restrictive to assume that either model is correctly specified in practice. In addition, since these doubly robust procedures are based on inverse probability weighting, they may suffer from large variances due to extreme probability values.\nWe propose a calibration procedure that allows multiple models for the propensity score and imputation models to gain more protection in this paper. It is appealing to fit multiple models in data fusion problems, especially when there are a large number of common variables, since a correct specification for the nuisance models in such a case is difficult. Although a variety of variable selection techniques can be employed for model building, their performances typically rely on levels of tuning parameters, and different tuning parameters may lead to different models. Selecting tuning parameters based on some information criterion would be an option, but the selection itself brings additional uncertainty in the working model specifications. It thus seems desirable to postulate a set of reasonable models, each involving different subsets of covariates and possibly different link functions, and to incorporate them simultaneously. While the idea of fitting multiple models has been well developed in missing data literature (Han and Wang, 2013; Han, 2014; Chan and Yam, 2014; Chen and Haziza, 2017; Li et al., 2020), it remains unclutivated in the area of data fusion studies. The implementation of this idea in such studies becomes more complicated and requires additional techniques. For example, Han (2014) considered regression analysis with missing outcome data, in which only one calibration\nweight is needed to incorporate multiple models. The calibration weight is constructed by estimating the conditional expecation of the outcome given all covariates that can be easily implemented. However, such a strategy cannot be directly applied to data fusion problems since some covariates are not fully observed. We thus employ an imputation approach to obtain approximations of the conditional expectation of the outcome given only always-observed covariates, and use these approximations to construct two different calibration weights that are needed for data fusion analysis. This leads to complexity in both the implementation and theoretical investigations, where some additional empirical process theories are required for the asymptotic results of the proposed estimator. Different from existing doubly robust procedures for data fusion problems, our estimation strategy relies on the empirical likelihood method (Owen, 2001; Qin and Lawless, 1994; Qin and Zhang, 2007), which circumvents the use of inverse probabilities. The resulting estimator is robust against extreme values of the fitted propensity scores. Furthermore, the proposed estimator enjoys well-established theoretical properties. Under some regularity conditions, it is consistent and asymptotically normal if any one of the propensity score and imputation models is correct. The estimator also attains semiparametric efficiency bound when both one of propensity scores and one of imputation models are correctly specified, without requiring knowledge of which two models are correct.\nThe remainder of this paper is organized as follows. In Section 2, we introduce notation and discuss identifiability of the parameter of interest. In Section 3, we provide a calibration-based estimation approach. We then establish the asymptotic results for the proposed estimator in Section 4. We study the finite-sample performance of the proposed approach via both simulation studies and an assembled U.S. household expenditure data example in Sections 5 and 6, respectively. We conclude with a discussion in Section 7 and relegate proofs to the Appendix."
        },
        {
            "heading": "2 Notation, assumptions and identifiability",
            "text": "Suppose there are n individuals who are merged from two different sources. Let R denote the data source indicator with R = 1 if a subject is observed in the primary sample and R = 0 if it is observed in the auxiliary sample. Let m = \u2211n\ni=1Ri be the number of\nsubjects who are observed in the primary sample, and index those subjects by i = 1, . . . , m without loss of generality. The outcome Y is only available in the primary sample, some covariates W are only available in the auxiliary sample, and the other covariates V are observed in both data sources. Let f(\u00b7) denote the probability density or mass function of a random variable (vector), and let \u03c0(v) = f(R = 1 | V = v) denote the probability that a subject is observed in the primary sample given the common variables V = v. We make the following assumptions.\nAssumption 1. R\u22a5 (Y,W ) | V .\nAssumption 2. \u03b4 < \u03c0(v) < 1\u2212 \u03b4 for all v and some fixed constant \u03b4 \u2208 (0, 1).\nAssumption 1 is similar to missing at random in missing data problems. It implies that the conditional distribution of Y or W given the always-measured variables V does not vary across the primary and auxiliary populations, but allows the marginal distributions of V to differ between these two populations. Assumption 2 indicates that for each subject in one data source, the probability of observing a matching unit in the other data source with similar values of V is positive. It is different from the usual positivity assumption in missing data problems which requires a positive chance of observing complete data for each subject. Both assumptions are basic and frequently made in data fusion problems (Chen et al., 2008). The identifiability of the parameter \u03b80 in our regression problem can be guaranteed if the conditional distribution f(Y | W,V ) is identified. For identification of this conditional distribution, we need further assumptions.\nAssumption 3. There exists a variable Z in the always-observed covariates V such that Y \u22a5 Z |W,X, where X denotes the remaining covariates in V .\nAssumption 4. For any function \u03c6(W,X) with finite mean, E{\u03c6(W,X) | Z,X} = 0 implies \u03c6(W,X) = 0 almost surely.\nFor convenience, we may use the notation V and (Z,XT)T interchangeably below. Assumption 3 reveals that the scalar covariate Z affects the outcome Y only through its association with W and X . In contrast to the commonly-used exclusion restriction assumption Y \u22a5 V | W , this assumption requires only one of the covariates in V to be conditionally independent of Y , which is weaker and more reasonable in practice. Ridder and Moffitt (2007) proposed similar assumptions and discussed identification in the context of categorical variables. To achieve a general identification result, we impose the completeness condition in Assumption 4. This condition is widely used for model identifiability across various disciplines and can be satisfied for many commonly-used parametric models (Newey and Powell, 2003; d\u2019Haultfoeuille, 2010).\nProposition 1. Under Assumptions 1\u20134, the conditional distribution f(Y | W,V ) is identifiable.\nThe identifiability of f(Y | W,V ) in Proposition 1 implies that the regression parameter \u03b80 is identifiable under Assumptions 1\u20134. When Assumptions 3 and 4 are removed, one may also achieve (local) identifiability of \u03b80 based on the moment condition:\nE [{ Y \u2212 \u00b5(\u03b8T1W + \u03b8T2V ) } t(V ; \u03b8) ] = 0, (1)\nwhere t(V ; \u03b8) is a user-specified p-dimensional vector function that may depend on V and \u03b8. Denote \u0393(\u03b8) to be the p \u00d7 p derivative matrix of the left-hand side of the above equation, i.e., \u0393(\u03b8) = E{Y \u2202t(V ; \u03b8)/\u2202\u03b8 \u2212 \u2202s(W,V ; \u03b8)/\u2202\u03b8}, where s(W,V ; \u03b8) = \u00b5(\u03b8T1W + \u03b8T2V )t(V ; \u03b8). We simply write \u0393 = \u0393(\u03b80). If \u0393 is of full rank, then \u03b80 is (locally) identifiable\n(Bandeen-roche et al., 1997). Under some scenarios, the global identifiability of \u03b80 can be guaranteed. For example, when \u00b5(\u00b7) is the identity function, \u03b80 is identifiable if there exists a nonlinear term of V in E(W | V ) (Evans et al., 2021; Miao et al., 2021). In the next section, we assume \u03b80 has been identified and propose a procedure to estimate \u03b80 based on the moment condition (1) with a fixed function t(V ; \u03b8)."
        },
        {
            "heading": "3 Proposed estimator",
            "text": "Let C1 = {\u03c0j(\u03b7j) : j = 1, . . . J} be a set of J propensity score models for \u03c0(V ) and C2 = {ak(\u03b3k) : k = 1, . . . , K} be a set of K imputation models for f(W | V ). Here \u03b7j and \u03b3k are the corresponding parameters, and we let \u03b7\u0302j and \u03b3\u0302k denote their estimators. Usually, each \u03b7\u0302j is obtained by maximizing the binomial likelihood function:\nn\u220f\ni=1\n{ \u03c0ji (\u03b7 j) }Ri{1\u2212 \u03c0ji (\u03b7j) }1\u2212Ri ,\nand each \u03b3\u0302k is similarly obtained by maximum likelihood estimation based on auxiliary data. Let \u03b8\u0302k denote the solution to\n1 n\nn\u2211\ni=1\nt(Vi; \u03b8) [ Ri { Yi \u2212 E(Y | Vi; \u03b3\u0302k, \u03b8) }\n+ (1\u2212 Ri) { E(Y | Vi; \u03b3\u0302k, \u03b8)\u2212 \u00b5(\u03b8T1Wi + \u03b8T2Vi) }] = 0.\n(2)\nIt is easy to verify that if the kth imputation model is correctly specified, then \u03b8\u0302k is a consistent estimator of \u03b80. Let {W di (\u03b3\u0302k) : d = 1, . . . , D} denote D random draws from f(W | Vi; \u03b3\u0302k), and define gi(\u03b3\u0302k, \u03b8\u0302k) = D\u22121 \u2211D d=1 s{W di (\u03b3\u0302k), Vi; \u03b8\u0302k} for i = 1, . . . , n, where s(W,V ; \u03b8) is defined below (1). Because E(Y | V ) = E{E(Y | W,V ) | V }, the quantity gi(\u03b3\u0302 k, \u03b8\u0302k) can be seen as an estimate of E(Y | Vi; \u03b3\u0302k, \u03b8\u0302k)t(Vi; \u03b8\u0302k) by averaging over the D random draws taken from f(W | Vi; \u03b3\u0302k).\nOur procedure consists of three steps. In the first step, we obtain the calibration weights \u03c91i for subjects in the primary sample {i : i = 1, . . . , m} by imposing the following\nconstraints:\n\u03c91i \u2265 0, m\u2211\ni=1\n\u03c91i = 1,\nm\u2211\ni=1\n\u03c91i\u03c0 j i (\u03b7\u0302 j) = \u03c4\u0302 j (j = 1, . . . , J),\nm\u2211\ni=1\n\u03c91igi(\u03b3\u0302 k, \u03b8\u0302k) = \u03c8\u0302k (k = 1, . . . , K),\n(3)\nwhere \u03c4\u0302 j = n\u22121 \u2211n\ni=1 \u03c0 j i (\u03b7\u0302\nj) and \u03c8\u0302k = n\u22121 \u2211n\ni=1 gi(\u03b3\u0302 k, \u03b8\u0302k). The rationale behind these\nconstraints is as follows. Note that for any function b(V ) with finite expectation, we have\nE [ \u03c9(V ) { b(V )\u2212E(b(V )) } | R = 1 ] = 0, (4)\nwhere \u03c9(V ) = 1/\u03c0(V ). We then take b(V ) to be \u03c0j(\u03b7\u0302j) and E(Y | V ; \u03b3\u0302k, \u03b8\u0302k)t(V ; \u03b8\u0302k) to obtain the above constraints. We choose \u03c9\u03021i\u2019s that maximize \u220fm i=1 \u03c91i subject to the constraints in (3). To give an explicit form for the estimates \u03c9\u03021i\u2019s, we write\n\u03b7\u0302 = { (\u03b7\u03021)T, . . . , (\u03b7\u0302J)T } T , \u03b3\u0302 = { (\u03b3\u03021)T, . . . , (\u03b3\u0302K)T } T , \u03b8\u0302 = { (\u03b8\u03021)T, . . . , (\u03b8\u0302K) } T ,\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) = [ \u03c01i (\u03b7\u0302 1)\u2212 \u03c4\u0302 1, . . . , \u03c0\u0302Ji (\u03b7\u0302J)\u2212 \u03c4\u0302J , {gi(\u03b3\u03021, \u03b8\u03021)\u2212 \u03c8\u03021}T, . . . , {gi(\u03b3\u0302K , \u03b8\u0302K)\u2212 \u03c8\u0302K}T ] T .\nThen by Lagrange multipliers method, we have\n\u03c9\u03021i = 1\nm\n1\n1 + \u03c1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n/{ 1\nm\nm\u2211\ni=1\n1\n1 + \u03c1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n} (i = 1, . . . , m),\nwhere \u03c1\u0302 = (\u03c1\u03021, . . . , \u03c1\u0302J+pK) T is a (J + pK)-dimensional vector satisfying the equation\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1 + \u03c1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) = 0.\nTo guarantee nonnegativity of \u03c9\u03021i, we further impose the condition 1 + \u03c1\u0302 Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) > 0 for i = 1, . . . , m. Then under this condition, we can obtain \u03c1\u0302 by minimizing a convex function F (\u03c1) = n\u22121 \u2211n\ni=1Ri log{1 + \u03c1Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)}.\nIn the second step, we obtain the weights \u03c90i\u2019s for subjects in the auxiliary data\n{i : i = m+ 1, . . . , n} similarly by the following constraints:\n\u03c90i \u2265 0, n\u2211\ni=m+1\n\u03c90i = 1, n\u2211\ni=m+1\n\u03c90i\u03c0 j i (\u03b7\u0302 j) = \u03c4\u0302 j (j = 1, . . . , J),\nn\u2211\ni=m+1\n\u03c90igi(\u03b3\u0302 k, \u03b8\u0302k) = \u03c8\u0302k (k = 1, . . . , K),\n(5)\nThen the estimates \u03c9\u03020i\u2019s that maximize \u220fn i=m+1 \u03c90i under constraints in (5) are given by\n\u03c9\u03020i = 1 n\u2212m 1\n1 + \u03b1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n/{ 1\nn\u2212m\nn\u2211\ni=m+1\n1\n1 + \u03b1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n} (i = m+ 1, . . . , n),\nwhere \u03b1\u0302 = (\u03b1\u03021, . . . , \u03b1\u0302J+pK) T is the (J + pK)-dimensional Lagrange multipliers solving\nn\u2211\ni=m+1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1 + \u03b1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) = 0, and 1 + \u03b1\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) > 0 (i = m+ 1, . . . , n).\nFinally, the proposed estimator of \u03b80 based on the calibration weights, denoted by\n\u03b8\u0302CAL, is the solution to\nm\u2211\ni=1\n\u03c9\u03021iYit(Vi; \u03b8)\u2212 n\u2211\ni=m+1\n\u03c9\u03020is(Wi, Vi; \u03b8) = 0.\nCompared to existing doubly robust estimators that weight each subject in the primary data by 1/{n\u03c0\u0302(V )} and subject in the auxiliary data by 1/[n{1 \u2212 \u03c0\u0302(V )}], the proposed calibration estimator \u03b8\u0302CAL use weights \u03c9\u03021i and \u03c9\u03020i, respectively. Those doubly robust estimators may be sensitive to near-zero or near-one values of \u03c0\u0302(V ), which can yield extremely large weights that may make the numerical performance be quite unstable. In our procedure, we obtain the calibration weights through maximization of \u220fm\ni=1 \u03c91i and\n\u220fn i=m+1 \u03c90i that satisfy (3) and (5), respectively. These two objective functions increase if their separate weights are more evenly distributed, because the weights are restricted to be nonnegative and sum-to-one. Thus, our procedure will not be affected dramatically by extreme values of propensity score and should lead to more stable performance. This property inherits from the empirical likelihood method, which has been successfully applied to address missing data problems (Qin and Zhang, 2007; Qin et al., 2009; Cao et al., 2009; Han, 2014; Han et al., 2019)."
        },
        {
            "heading": "4 Asymptotic results",
            "text": "In this section, we show that the estimator \u03b8\u0302CAL is consistent when one of the propensity scores or one of the imputation models is correctly specified. We also establish\nits asymptotic normality property and discuss the estimation efficiency. We introduce more notations that will be used later. Let \u03b7j\u2217, \u03b3 k \u2217 , \u03b8 k \u2217 , \u03c4 j \u2217 and \u03c8 k \u2217 denote the probability limits of \u03b7\u0302j, \u03b3\u0302k, \u03b8\u0302k, \u03c4\u0302 j and \u03c8\u0302k respectively, as n \u2192 \u221e, where j = 1, . . . , J and k = 1, . . . , K. It is clear that \u03c4 j\u2217 = E{\u03c0j(\u03b7j\u2217)} and \u03c8k\u2217 = E[s{W d(\u03b3k\u2217 ), V ; \u03b8k\u2217}]. Write \u03b7T\u2217 = {(\u03b71\u2217)T, . . . , (\u03b7J\u2217 )T}, \u03b3T\u2217 = {(\u03b31\u2217)T, . . . , (\u03b3K\u2217 )T}, \u03b8T\u2217 = {(\u03b81\u2217)T, . . . , (\u03b8K\u2217 )T}, and hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) T = [\u03c01i (\u03b7 1 \u2217)\u2212 \u03c4 1\u2217 , . . . , \u03c0Ji (\u03b7J\u2217 )\u2212 \u03c4J\u2217 , {gi(\u03b31\u2217 , \u03b81\u2217)\u2212 \u03c81\u2217}T, . . . , {gi(\u03b3K\u2217 , \u03b8K\u2217 )\u2212\u03c8K\u2217 }T] for i = 1, . . . , n.\nWe first consider the case where one of the models in C1 is correctly specified. Without loss of generality, let \u03c01(\u03b71) be the correct model in the sense that there exists some value \u03b710 such that \u03c0 1(\u03b710) = \u03c0(V ). To establish the consistency property of \u03b8\u0302CAL, we build the connection between \u03c9\u03021i and another version of the empirical likelihood estimator obtained from the primary sample {i : i = 1, . . . , m}. Let pi denote the conditional empirical probability mass on (Yi,Wi, Vi) given Ri = 1 for i = 1, . . . , m. According to (4) and the fact that \u03c9(V ) = 1/\u03c01(\u03b710), the estimator of pi is given by the following constrained optimization:\nmax p1,...,pm\nm\u220f\ni=1\npi subject to pi \u2265 0, m\u2211\ni=1\npi{\u03c0ji (\u03b7\u0302j)\u2212 \u03c4\u0302 j}/\u03c01i (\u03b7\u03021) = 0 (j = 1, . . . , J),\nm\u2211\ni=1\npi{gi(\u03b3\u0302k, \u03b8\u0302k)\u2212 \u03c8\u0302k}/\u03c01i (\u03b7\u03021) = 0 (k = 1, . . . , K).\nUsing the Lagrange multipliers again yields that\np\u0302i = 1\nm\n1\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302\n1) (i = 1, . . . , m),\nwhere \u03bb\u0302 = (\u03bb\u03021, . . . , \u03bb\u0302J+pK) T is the (J + pK)-dimensional Lagrange multiplier satisfying\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302\n1) = 0.\nWith some simple algebra given in the supplementary material, one can show that\n\u03c9\u03021i = 1\nm\n\u03c4\u0302 1/\u03c01i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302\n1) .\nThis equation links the calibration weight \u03c9\u03021i with the Lagrange multiplier \u03bb\u0302 of the empirical likelihood estimator p\u0302i. Based on the empirical likelihood theory, one can show that \u03bb\u0302 = op(1). In addition, since \u03c0 1(\u03b71) is a correctly-specified propensity score model, \u03c4\u0302 1 \u2212 m/n = op(1). We then conclude from the above equation that \u03c9\u03021i = 1/{n\u03c01i (\u03b7\u03021)} + op(1). Similarly, \u03c9\u03020i = 1/[n{1 \u2212 \u03c01i (\u03b7\u03021)}] + op(1). We would like to point out that these results do not contradict with our previous discussions that the proposed calibration weights are less sensitive to extreme propensity score values, because the discussions therein mainly emphasize the finite sample performance of those estimators. In an asymptotic way or when the sample size goes to infinity, the calibration weights are equivalent to the inverse propensity score weights. Based on the above intermediate results, we obtain the following theorem.\nTheorem 1. When one of the models in C1 is correctly specified, \u03b8\u0302CAL is a consistent estimator of \u03b80 as n\u2192 \u221e.\nNext, we consider the case where one of the models in C2 is correct. Without loss of generality, we assume that a1(\u03b31) is correctly specified, i.e., a1(\u03b310) = f(W | V ) for some \u03b310 . Then we have \u03b3 1 0 = \u03b3 1 \u2217 ; that is, \u03b3\u0302 1 is a consistent estimator of \u03b310 . This implies that the estimator \u03b8\u03021 obtained from (2) is a consistent estimator of \u03b80. By utilizing constraints in (3) and (5), we can also obtain the consistency of \u03b8\u0302CAL in this case.\nTheorem 2. When one of the models in C2 is correctly specified, \u03b8\u0302CAL is a consistent estimator of \u03b80 as n\u2192 \u221e.\nDifferent from the consistency property of \u03b8\u0302CAL, the derivation of the asymptotic distribution is asymmetric. In other words, the asymptotic normality property depends on which of the J+K candidate models is correctly specified, and the asymptotic variance is different when one of the propensity score models or one of the imputation models is correct. The usual strategy in traditional missing data analysis for semiparametric theory\nis to assume the propensity score model is correctly specified (Robins and Rotnitzky, 1995; Tsiatis, 2006). We follow this way in the current data fusion setting. Without loss of generality, we assume \u03c01(\u03b71) is a correctly specified model for \u03c0(V ), and let \u03a8(\u03b71) denote the score function of \u03b71, that is,\n\u03a8(\u03b71) = R\u2212 \u03c01(\u03b71) \u03c01(\u03b71){1\u2212 \u03c01(\u03b71)} \u2202\u03c01(\u03b71) \u2202\u03b71 .\nWe simply write \u03a8 = \u03a8(\u03b710) and define the following matrices:\nF =E\n[ Y t(V ; \u03b80)\u2212 E{Y t(V ; \u03b80)}\n\u03c01(\u03b710)\n{ h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) } T ] , H = E { h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) \u22972\n\u03c01(\u03b710)\n} ,\nG =E\n[ s(W,V ; \u03b80)\u2212E{s(W,V ; \u03b80)}\n1\u2212 \u03c01(\u03b710) { h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) } T\n] , T = E { h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) \u22972\n1\u2212 \u03c01(\u03b710)\n} ,\nwhere for any matrix C, C\u22972 = CCT. We further define\nQ(\u03b71) = R\n\u03c01(\u03b71)\n[ Y t(V ; \u03b80)\u2212E{Y t(V ; \u03b80)} ] \u2212 R\u2212 \u03c0 1(\u03b71)\n\u03c01(\u03b71) FH\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u2212 1\u2212R 1\u2212 \u03c01(\u03b71) [ s(W,V ; \u03b80)\u2212 E{s(W,V ; \u03b80)} ] + \u03c01(\u03b71)\u2212R 1\u2212 \u03c01(\u03b71) GT \u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217),\nand write Q = Q(\u03b710). The asymptotic distribution is given by the following theorem.\nTheorem 3. When C1 contains a correctly specified model for \u03c0(V ), n1/2(\u03b8\u0302CAL \u2212 \u03b80) has an asymptotic normal distribution with mean zero and variance Var(L), where L = \u0393\u22121[Q\u2212 E(Q\u03a8T){E(\u03a8\u22972)}\u22121\u03a8].\nIt is clear to see that L involves the residual of the projection of Q on \u03a8, so Var(L) \u2264 Var(\u0393\u22121Q). Note that the latter is the variance of the inverse probability weighting estimator when \u03c0(V ) is known in a data fusion setting. This implies that the efficiency of \u03b8\u0302CAL can be improved by modeling \u03c0(V ) even when it is known. Such a counterintuitive fact has been studied in traditional missing data problems; see, for example, Robins and Rotnitzky (1995) and Han (2014). The following proposition presents the efficient influence function of \u03b80 defined through (1) and provides the corresponding semiparametric efficiency bound.\nProposition 2. The efficient influence function of \u03b80 is given by \u0393\u22121 [ R\n\u03c0(V )\n{ Y t(V ; \u03b80)\u2212E(Y t(V ; \u03b80) | V ) } \u2212 1\u2212 R 1\u2212 \u03c0(V ) { s(W,V ; \u03b80)\u2212 E(s(W,V ; \u03b80) | V )\n}] ,\nand the semiparametric efficiency bound is equal to \u0393\u22121\u2126(\u0393\u22121)T, where\n\u2126 = E\n[ 1\n\u03c0(V ) Var\n{ Y t(V ; \u03b80) | V } +\n1 1\u2212 \u03c0(V )Var { s(W,V ; \u03b80) | V\n}] .\nChen et al. (2008) and Shu and Tan (2020) presented similar efficiency bounds in other data fusion settings. Evans et al. (2021) proposed a doubly robust estimator based on the influence function given in Proposition 2 and mentioned that the estimator is the most efficient when both the propensity score and imputation models are correctly specified. We formally state this result here as a supplement and provide more rigorous analysis in the supplementary material. Evans et al. (2021) also discussed how to choose a proper function t(V ; \u03b8) to further improve estimation efficiency.\nTheorem 4. Suppose that C1 contains a correctly specified model for \u03c0(V ) and C2 contains a correctly specified model for f(W | V ). Then n1/2(\u03b8\u0302CAL \u2212 \u03b80) has an asymptotic normal distribution with mean zero and variance equal to the semiparametric efficiency bound.\nDifferent from the local efficiency of the doubly robust estimator proposed by Evans et al.\n(2021), the efficiency gain in Theorem 4 can be achieved without exactly knowing which two among the multiple models are correctly specified. To make inference, we need to provide a consistent estimator for the asymptotic variance. Such an estimator can be achieved by replacing the expectations involved in the asymptotic variance with their corresponding sample averages."
        },
        {
            "heading": "5 Simulation studies",
            "text": "In this section, we conduct simulation studies to evaluate the finite sample performance of the proposed procedure. The simulation model has two fully observed covariates V =\n(V1, V2) T that are both generated from the standard normal distribution. We generate W fromW | V \u223c N(\u22120.5+1.5V1+V2+3V1\u2217V2, 1) and R from the Bernoulli distribution with R | V \u223c Ber{\u03c0(V )}, where \u03c0(V ) = {1+ exp(\u22120.3+ 0.75V1 \u2212 0.75V2)}\u22121. The proportion of samples from the primary sample is about 56%. We finally generate the outcome from Y |W,V \u223c N(1+2W +2V1\u22121.5V2, 0.4). The correct models for \u03c0(V ) and E(W | V ) are given by logit{\u03c01(\u03b71)} = \u03b711+\u03b712V1+\u03b713V2 and a1(\u03b31) = \u03b311+\u03b312V1+\u03b313V2+\u03b314V1\u2217V2. We also consider the following incorrect working models: logit{\u03c02(\u03b72)} = \u03b721 + \u03b722V1 and a2(\u03b32) = \u03b321 + \u03b3 2 2V1 + \u03b3 2 3V1 \u2217 V2. The true value of the parameter of interest \u03b8T = (\u03b81, \u03b82, \u03b83, \u03b84) = (1, 2, 2,\u22121.5).\nWe apply the proposed calibration procedure to estimate the parameters and use the doubly robust estimator by Evans et al. (2021) for comparison. Because the performances of both methods may depend on different combinations of models, we use the four-digit zero-one string to indicate which subset of the working models is used, with 1 denoting use and 0 non-use. The first two digits correspond to the correct and incorrect propensity score models, respectively. The last two digits correspond to the correct and incorrect imputation models, respectively. For example, CAL-1010 represents the proposed estimator using the correct propensity score and correct imputation models, whereas CAL-1111 indicates that all models are used. For each estimator, we compute the Monte Carlo bias, root mean squared error and 95% confidence interval coverage probability. The results for sample size n = 500 and n = 2000 based on 1000 replications are summarized in Table 1.\nWhen one propensity score and one imputation model are used, both the doubly robust estimator and the calibartion estimator show negligible bias if either model is correctly specified; see DR-1010, DR-0110, DR-1001, CAL-1010, CAL-0110, and CAL1001 in Table 1. However, the peformance of DR-1001 based on the correct propensity score and incorrect imputation model is not satisfactory judging from its large root mean\nsquared error. In contrast, the proposed calibration estimator CAL-1001 is more efficient with smaller root mean squared error. Similar findings are observed when neither model is correct, although both of these two estimators are significantly biased. This shows that the proposed procedure is less sensitive to extreme propensity score values and produces nottoo-bad estimates when both models are misspecified. The calibration estimators based on more than two models show ignorable bias and are efficient in almost all scenarios. None of the existing doubly robust estimators can achieve such robustness. Through a calibration strategy, our method effectively accommodates multiple models and delivers more robust and stable estimation."
        },
        {
            "heading": "6 Application",
            "text": "As an illustration of the proposed method, we consider an assembled data set from the Consumer Expenditure Survey (CEX) and the Survey of Consumer Finance (SCF) to estimate the effects of household asset value on consumption. The CEX is a nationwide annual survey conducted by the U.S. Bureau of Labor Statistics to collect detailed information about household expenditures and some demographic variables. Unfortunately, the CEX provides limited information on wealth data and thus the CEX alone is not sufficient for our purposes. Previous research (Bostic et al., 2009) has turned to a different triennial survey SCF conducted by Federal Reserve Board to obtain information on U.S. households\u2019 assets, liabilities, income and other demographic characteristics. Both the CEX and SCF began in the early 1980s, but for illustration, we focus on the data from CEX\u2019s 1997 fourth quarter survey and 1998 SCF, and restrict the sample with household heads between 25 and 65 years of age. Besides, since the SCF oversamples relatively wealthy households (Bostic et al., 2009), we truncate the SCF sample at 90th percentiles of observed total household income and net worth, as was done in Evans et al. (2021).\nThe resulting dataset consists of 5904 households: 3388 from CEX and 2516 from SCF.\nOur main interest is the household total net worth (netw) effect on total expenditures (expd) in 1997, adjusting for the total income before taxes and certain baseline characteristics of household head, including the continuous covariate age and the binary covariates sex (1 = female), marital status (1 = married), education levels: edu1 (1 = high school diploma or general educational development); edu2 (1 = some college or Associate degree); edu3 (1 = Bachelors degree or higher), and race types: white (1 = White); black (1 = Black/African American). A logarithmic transformation is required to linearize netw, expd and income, and so the model specification is:\nlexpd =\u03b81 + \u03b82lnetw + \u03b83lincome + \u03b84age + \u03b85sex + \u03b86marital\n+ \u03b87edu1 + \u03b88edu2 + \u03b89edu3 + \u03b810white + \u03b811black + \u01eb,\nwhere lexpd, lnetw, and lincome denote respectively the logarithmic transformations of netw, expd and income, and \u01eb has mean zero conditional on all covariates. Since the data for lexpd and lnetw are collected from two different sources, none of the subjects can simultaneously observe these two variables. It is thus challenging to estimate the effects of household net worth on expenditures while accounting for potential confounders.\nWe apply the proposed method to estimate coefficients in the linear model. To implement our procedure, we consider the following two imputation models: a linear regression model a1(\u03b31) with all main effects and quadratic terms for both age and lincome; a second linear regression model a2(\u03b32) with all main effects and an interaction term between age and lincome. For the propensity score model, we posit two working models that employ the same regressors as the imputation regression models in the logit transformation, that is, logit{\u03c0j(\u03b7j)} = aj(\u03b3j) for j = 1, 2. Because there are missing values in the original survey data, the publicly available data set consists of five imputed replicates. We thus follow Evans et al. (2021) to perform estimation for each replicate and combine the results\nusing Rubin\u2019s rule (Rubin, 2004). We report the point estimates and standard errors of our analysis results using all the four working models in Table 2. We also include two classes of the doubly robust estimator results that separately use the two working models {\u03c01(\u03b71), a1(\u03b31)} and the other two working models {\u03c02(\u03b72), a2(\u03b32)} for comparison. As indicated by Evans et al. (2021), a1(\u03b31) is nearly a correct specification of the imputation model. Consequently, the results from the doubly robust method using {\u03c01(\u03b71), a1(\u03b31)} are very similar to the proposed calibration method, as shown in Table 2. Based on these results, households with higher net worth have significantly higher total expenditure, adjusting for household incomes and other covariates. The covariates income, sex, marital, edu1, edu2, edu3, white and black do not have substantial impacts on household expenditure, whereas age exhibits a significantly negative impact, in that the older household heads have lower total expenditure. These associations generally agree with previous findings from Bostic et al. (2009) and Evans et al. (2021). However, when \u03c02(\u03b72) and a2(\u03b32) are chosen to be the working models for the doubly robust estimator, the empirical results are found to be significantly different and may lead to incorrect conclusions. This implies that the doubly robust estimator could suffer from severe bias if both the propensity score and imputation models are unfortunately misspecified. To achieve the double robustness property, practitioners should collect more information about the underlying mechanism and scrutinize the modelling carefully. The proposed calibration procedure provides a way of improving specifications by incorporating multiple working models. Although incorrect models may be included, the resulting estimator is guaranteed to be consistent as long as there exists one correctly specified model in the estimating procedure."
        },
        {
            "heading": "7 Discussion",
            "text": "We have proposed a calibration approach to regression analysis problems where the outcome and covariates data are fused from two different sources. Since no subject has complete information in the data fusion setting, existing methods developed for missing data problems cannot be directly applied here. A class of doubly robust estimators based on inverse propensity score weighting is designed particularly for this setting (Shu and Tan, 2020; Evans et al., 2021). Although these estimators offer two chances of achieving consistent estimation, it may be risky to assume either the propensity score or imputation model is correct in practice and they are also sensitive to extreme propensity score values. Our proposed estimator mitigates these issues. It involves multiple working models that could provide more opportunities to achieve correct specification of the two nuisance models. Since our development builds on the empirical likelihood method that circumvents the use of inverse propensity score, the resulting estimator is not affected dramatically by extreme values. Simulation results also demonstrate that the proposed estimator performs not too bad even if no model is correctly specified due to the nature of calibration.\nThe proposed approach may be improved or extended in several directions. Firstly, the working models are all parametric in this paper, and one can use modern machine learning techniques to further improve robustness. The resulting estimator should still be consistent when one working model is correct, but the rate of convergence may affect the asymptotic distribution. Secondly, it would be interesting to extend the mean regression to quantile regression problems in data fusion settings. The study of these issues is beyond the scope of this paper and we leave them as future research topics.\nAppendix\nProof of Proposition 1. Under Assumption 1, we have\nf(Y | V ) = f(Y | V,R = 1), f(W | V ) = f(W | V,R = 0).\nThis implies that f(Y | V ) and f(W | V ) are identifiable. Under Assumption 3, Y \u22a5 Z | W,X and V = (Z,XT)T. Thus, for any given y, we have\nf(y | v) = E{f(y |W,x) | z, x}.\nIf there exists f1(y | w, x) and f2(y | w, x) satisfying the above equation, then the completeness condition in Assumption 4 implies that f1(y | w, x) = f2(y | w, x); that is, f(y | w, v) is identifiable, and hence, the parameter \u03b80 is identifiable.\nLemma 1. Suppose that \u03c01(\u03b71) is correctly specified for \u03c0(V ). We have\n\u03c9\u03021i = 1\nn\u03c01i (\u03b7\u0302 1)\n+ op(1), (i = 1, . . . , m),\n\u03c9\u03020i = 1\nn{1\u2212 \u03c01i (\u03b7\u03021)} + op(1), (i = m+ 1, . . . , n).\nProof. We first show the results for \u03c9\u03021i. Based on the main text, the (J+pK)-dimensional Lagrange multipliers \u03bb\u0302 = (\u03bb\u03021, . . . , \u03bb\u0302J+pK) T satisfy:\n1\nm\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c01i (\u03b7\u0302 1)\n= 0, and 1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302 1) \u2265 0 for i = 1, . . . , m.\nNote that 1\nm\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302 1)\n= 1 \u03c4\u0302 1 1 m\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1 + \u03c01 i (\u03b7\u03021)\u2212\u03c4\u03021 \u03c4\u03021 + ( \u03bb \u03c4\u03021 ) T h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n= 1 \u03c4\u0302 1 1 m\nm\u2211\ni=1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1 + (\n\u03bb1+1 \u03c4\u03021 , \u03bb2 \u03c4\u03021 , . . . , \u03bbJ+pK \u03c4\u03021\n) T\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) .\n(6)\nComparing this equation with the equation of \u03c1\u0302 in the main text, we have \u03c1\u03021 = (\u03bb\u03021+1)/\u03c4\u0302 1 and \u03c1\u0302l = \u03bb\u0302l/\u03c4\u0302 1 for l = 2, . . . , J + pK. This implies that\n\u03c9\u03021i = 1\nm\n\u03c4\u0302 1/\u03c01i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302\n1) =\np\u0302i\u03c4\u0302 1\n\u03c01i (\u03b7\u0302 1) .\nSince E{Rh(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/\u03c0(V )} = 0 and \u03b71\u2217 = \u03b710, 0 is the solution to\nE\n{ Rh(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/\u03c0 1(\u03b71\u2217)\n1 + \u03bbTh(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/\u03c01(\u03b71\u2217)\n} = 0\nas an equation of \u03bb. Then from the theory of empirical likelihood (Owen, 2001), \u03bb\u0302 = op(1). Since \u03c4\u0302 1 \u2212m/n = op(1), we have \u03c9\u03021i = 1/{n\u03c01i (\u03b7\u03021)}+ op(1).\nNext, we show the results for \u03c9\u03020i. we build the connection between \u03c9\u03020i and another empirical likelihood estimator based on the auxiliary sample {i : i = m + 1, . . . , n} using the prior knowledge that \u03c01(\u03b71) is correctly specified. Let qi denote the conditional empirical probability mass on (Yi,Wi, Vi) given Ri = 0, i = m + 1, . . . , n. Then the estimator of qi is obtained by solving the following constrained optimization:\nmax qm+1,...,qn\nn\u220f\ni=m+1\nqi subject to qi \u2265 0, n\u2211\ni=m+1\nqi \u03c0ji (\u03b7\u0302 j)\u2212 \u03c4\u0302 j 1\u2212 \u03c01i (\u03b7\u03021) = 0 (j = 1, . . . , J),\nn\u2211\ni=m+1\nqi gi(\u03b3\u0302 k, \u03b8\u0302k)\u2212 \u03c8\u0302k 1\u2212 \u03c01i (\u03b7\u03021) = 0 (k = 1, . . . , K).\nBy the Lagrange multipliers method, we have\nq\u0302i = 1 n\u2212m 1\n1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} , (i = m+ 1, . . . , n),\nwhere the (J + pK)-dimensional Lagrange multipliers \u03b4\u0302 = (\u03b4\u03021, . . . , \u03b4\u0302J+pK) T solves the following equation:\n1\nn\u2212m\nn\u2211\ni=m+1 h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} = 0,\nand satisfies 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1 \u2212 \u03c01i (\u03b7\u03021)} \u2265 0 for i = m + 1, . . . , n. Similar to the\nderivation in (6), we have\n1\nn\u2212m\nn\u2211\ni=m+1 h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)}\n= 1 1\u2212 \u03c4\u0302 1 1 n\u2212m\nn\u2211\ni=m+1\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1 + (\n\u03b41\u22121 1\u2212\u03c4\u03021 , \u03b42 1\u2212\u03c4\u03021 . . . , \u03b4J+pK 1\u2212\u03c4\u03021\n) T\nh\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) .\nBy comparing this equation with the equation of \u03b1\u0302 in the main text, we have \u03b1\u03021 = (\u03b4\u0302 \u2212 1)/(1\u2212 \u03c4\u0302 1), and \u03b1\u0302l = \u03b4\u0302l/(1\u2212 \u03c4\u0302 1) for l = 2, . . . , J + pK. Therefore,\n\u03c9\u03020i = 1 n\u2212m (1\u2212 \u03c4\u0302 1)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} = qi(1\u2212 \u03c4\u0302 1) 1\u2212 \u03c01i (\u03b7\u03021) , (i = m+ 1, . . . , n).\nSince E[(1\u2212 R)h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/{1\u2212 \u03c0(V )}] = 0 and \u03b71\u2217 = \u03b710, 0 is the solution to\nE [ (1\u2212 R)h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/{1\u2212 \u03c01(\u03b71\u2217)} 1 + \u03b4Th(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/{1\u2212 \u03c01(\u03b71\u2217)} ] = 0\nas an equation of \u03bb. Thus, \u03b4\u0302 = op(1). In addition, because 1 \u2212 \u03c4\u0302 1 \u2212 (n \u2212m)/n = op(1), we then have \u03c9\u03020i = 1/[n{1\u2212 \u03c01i (\u03b7\u03021)}] + op(1).\nProof of Theorem 1. We aim to show that \u03b80 is the solution to the equation for \u03b8\u0302CAL as n \u2192 \u221e. Then the estimator \u03b8\u0302CAL is a consistent estimator of \u03b80. Without loss of generality, we assume \u03c01(\u03b71) is correctly specified for \u03c0(V ). As shown in Lemma 1, we have \u03c9\u03021i = 1/{n\u03c01i (\u03b7\u03021)}+ op(1), and \u03c9\u03020i = 1/[n{1\u2212 \u03c01i (\u03b7\u03021)}] + op(1). Thus, \u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021iYit(Vi; \u03b80)\u2212E{Y t(V ; \u03b80)} \u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021iYit(Vi; \u03b80)\u2212 1\nn\nn\u2211\ni=1\nRi \u03c01(\u03b7\u03021) Yit(Vi; \u03b80) \u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\nRi \u03c01(\u03b7\u03021) Yit(Vi; \u03b80)\u2212 1 n\nn\u2211\ni=1\nRi \u03c01(\u03b710) Yit(Vi; \u03b80)\n\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\nRi \u03c01(\u03b710)\nYit(Vi; \u03b80)\u2212E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223 = op(1).\n(7)\nAll the three terms in the above inequality are equal to op(1). Among them, the first term holds due to Lemma 1. The second term holds due to the consistency of \u03b7\u03021. Specifically, \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\nRi \u03c01 (\u03b7\u03021) Yit (Vi; \u03b80)\u2212 1 n\nn\u2211\ni=1\nRi \u03c01 (\u03b710) Yit (Vi; \u03b80)\n\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n{ 1\n\u03c01 (\u03b7\u03021) \u2212 1 \u03c01 (\u03b710)\n} YiRiYit (Vi; \u03b80) \u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n1\n{\u03c01 (\u03b7\u0302\u2020)}2 YiRiYit (Vi; \u03b80)\n\u2223\u2223\u2223\u2223 \u2223\u2223\u03b7\u03021 \u2212 \u03b710 \u2223\u2223 = op(1),\n(8)\nwhere \u03b7\u2020 is an intermediate value between \u03b7\u03021 and \u03b710. The third term holds due to the law of large numbers. Similarly, we have \u2223\u2223\u2223\u2223 n\u2211\ni=m+1\n\u03c9\u03020is(Wi, Vi; \u03b80)\u2212 E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 n\u2211\ni=m+1\n\u03c9\u03020iE{Y t(V ; \u03b80) |Wi, Vi; \u03b80} \u2212E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 n\u2211\ni=m+1\n\u03c90iE{Y t(V ; \u03b80) |Wi, Vi; \u03b80} \u2212 1\nn\nn\u2211\ni=1\n1\u2212 Ri 1\u2212 \u03c01i (\u03b7\u03021)\nE{Y t(V ; \u03b80) | Wi, Vi; \u03b80} \u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b7\u03021) E{Y t(V ; \u03b80) |Wi, Vi; \u03b80} \u2212 1 n\nn\u2211\ni=1\n1\u2212 Ri 1\u2212 \u03c01i (\u03b710)\nE{Y t(V ; \u03b80) |Wi, Vi; \u03b80} \u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b710)\nE{Y t(V ; \u03b80) |Wi, Vi; \u03b80} \u2212 E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223 = op(1).\nCombining this equation with (7) implies that \u03b80 is a solution to the equation for \u03b8\u0302CAL as n \u2192 \u221e, and hence, \u03b8\u0302CAL is a consistent estimator of \u03b80 when one of the models in C1 is correctly specified.\nProof of Theorem 2. We aim to show that \u03b80 is the solution to the equation for \u03b8\u0302CAL again as n \u2192 \u221e. Then the estimator \u03b8\u0302CAL is a consistent estimator of \u03b80. Without loss of generality, we assume that a1(\u03b31) is correctly specified for f(W | V ). Then we have \u03b3\u03021 p\u2212\u2192 \u03b31 and \u03b8\u03021 p\u2212\u2192 \u03b80. Let \u03c1\u2217 denote the probability limit of \u03c1\u0302. Note that one of the constraints in (3) is:\nm\u2211\ni=1\n\u03c9\u03021i\n[ 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ] = \u03c8\u03021 = 1\nn\nn\u2211\ni=1\n[ 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ] .\nThen we have\n\u2223\u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021iYit(Vi; \u03b80)\u2212 E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021i [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ]\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u03c8\u0302 1 \u2212E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021i [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ]\n\u2212 1 m\n1\n1 + \u03c1T\u2217h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\nn\u2211\ni=1\nRi [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ]\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nm\n1\n1 + \u03c1T\u2217h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\nn\u2211\ni=1\nRi 1\nD\nD\u2211\nd=1\n[ s{W di (\u03b3\u03021), Vi; \u03b8\u03021} \u2212 s{W di (\u03b310), Vi; \u03b80} ]\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nm\n1\n1 + \u03c1T\u2217h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\nn\u2211\ni=1\nRi [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns{W di (\u03b310), Vi; \u03b80} ]\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} \u2212 1\nn\nn\u2211\ni=1\n1\nD\nD\u2211\nd=1\ns{W di (\u03b310), Vi; \u03b80} \u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\n1\nD\nD\u2211\nd=1\ns{W di (\u03b310), Vi; \u03b810} \u2212 E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223 = op(1).\nAll terms in the last inequality are equal to op(1). Specifically, the first term holds because\n\u2223\u2223\u2223\u2223 m\u2211\ni=1\n\u03c9\u03021i [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns { W di (\u03b3\u0302 1), Vi; \u03b8\u0302 1 }]\n\u2212 1 m\n1\n1 + \u03c1T\u2217h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\nn\u2211\ni=1\nRi [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns { W di (\u03b3\u0302 1), Vi; \u03b8\u0302 1 }]\u2223\u2223\u2223\u2223\n=\n\u2223\u2223\u2223\u2223 1\nm\nn\u2211\ni=1\nRi\n{ 1\n1 + \u03c1\u0302Th(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) \u2212 1 1 + \u03c1T\u2217h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n}\n\u00d7 [ Yit(Vi; \u03b80)\u2212 1\nD\nD\u2211\nd=1\ns { W di (\u03b3\u0302 1), Vi; \u03b8\u0302 1 }]\u2223\u2223\u2223\u2223\n= op(1).\nThe second to the fourth terms can be similarly proved as in (8), and the final term holds due to the law of large numbers. We also note that one of the constraints in (5) is:\nn\u2211\ni=m+1\n\u03c9\u03020i\n[ 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ] = 1\nn\nn\u2211\ni=1\n[ 1\nD\nD\u2211\nd=1\ns{W di (\u03b3\u03021), Vi; \u03b8\u03021} ] .\nThen similar to the above derivation, we can also show that\n\u2223\u2223\u2223\u2223 n\u2211\ni=m+1\n\u03c9\u03020is(W,V ; \u03b80)\u2212 E{Y t(V ; \u03b80)} \u2223\u2223\u2223\u2223 = op(1).\nCombining all these equations implies that \u03b80 is a solution to the equation for \u03b8\u0302CAL as n\u2192 \u221e. This shows that the proposed estimator \u03b8\u0302CAL is also a consistent estimator of \u03b80 when one of the models in C2 is correctly specified.\nTo prove Theorem 3, we first present the following lemma.\nLemma 2. When \u03c01(\u03b71) is a correctly specified model for \u03c0(V ), we have\n\u221a n\u03bb\u0302 = H\u22121 [ 1\u221a n n\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) \u03c01i (\u03b7 1 0) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\u2212 1\u221a n\nn\u2211\ni=1\nA { E(\u03a8\u22972) }\u22121 \u03a8i ] + op(1),\n\u221a n\u03b4\u0302 = T\u22121 [ \u2212 1\u221a\nn\nn\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) 1\u2212 \u03c01i (\u03b710) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) + 1\u221a n\nn\u2211\ni=1\nB { E(\u03a8\u22972) }\u22121 \u03a8i ] + op(1).\nwhere\nA = E\n[ h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u03c01(\u03b710)\n{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T ] , and B = E [ h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n1\u2212 \u03c01(\u03b710)\n{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T ] .\nProof. By conditions imposed on the Lagrange multiplies \u03bb\u0302 in the main text, we have\n0 = 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0\n1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c01i (\u03b7\u0302 1) .\nConsequently,\n0 = 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0\n1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/\u03c0 1 i (\u03b7\u0302\n1) \u2212 1 n\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n\u03c01i (\u03b7\u0302 1)\n(9)\n+ 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n\u03c01i (\u03b7\u0302 1)\n\u2212 1 n\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302)\n\u03c01i (\u03b7 1 \u2217)\n(10)\n+ 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302)\n\u03c01i (\u03b7 1 \u2217)\n\u2212 1 n\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)\n\u03c01i (\u03b7 1 \u2217)\n(11)\n+ 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)\n\u03c01i (\u03b7 1 \u2217)\n\u2212 1 n\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u03c01i (\u03b7 1 \u2217)\n(12)\n+ 1\nn\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u03c01i (\u03b7 1 \u2217)\n. (13)\nTaking Taylor expansion of the right-hand side of (9) around \u03bb = 0 leads to\n(9) = \u22121 n\nn\u2211\ni=1\nRi h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n\u22972\n{\u03c01i (\u03b7\u03021)}2 \u03bb\u0302+ op(n\n\u22121/2) = \u2212H\u03bb\u0302+ op(n\u22121/2).\nTaking Taylor expansion of (10) around \u03b7 = \u03b7\u2217 leads to (10) = \u22121 n n\u2211\ni=1\nRi h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302) {\u03c01i (\u03b710)}2 { \u2202\u03c01i (\u03b7 1 0) \u2202\u03b71 } T (\u03b7\u03021 \u2212 \u03b710) + op(n\u22121/2) = \u2212A(\u03b7\u03021 \u2212 \u03b710) + op(n\u22121/2).\nFor expression (11), one can show that {Rih\u0302i(\u03b7\u2217, \u03b3, \u03b8\u0302)/\u03c01i (\u03b710) : \u2016\u03b3 \u2212 \u03b3\u2217\u2016 \u2264 \u03b5} forms a Donsker class and Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)/\u03c0 1 i (\u03b7 1 0) is L2 continuous at \u03b3\u2217. Therefore, we have\n(11) = 1\nn\nn\u2211\ni=1\n\u2202E { Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)/\u03c0 1 i (\u03b7 1 0) }\n\u2202\u03b3 (\u03b3\u0302 \u2212 \u03b3\u2217) + op(n\u22121/2).\nSimilarly, for expression (12), one can show that {Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8)/\u03c01i (\u03b710) : \u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2264 \u03b5} forms a Donsker class and Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) is L2 continuous at \u03b8\u2217. Therefore, we have\n(12) = 1\nn\nn\u2211\ni=1\n\u2202E { Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/\u03c0 1 i (\u03b7 1 0) }\n\u2202\u03b8 (\u03b8\u0302 \u2212 \u03b8\u2217) + op(n\u22121/2).\nIt is straightforward to show that bothE{Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)/\u03c01i (\u03b710)} andE{Rih\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)/\u03c01i (\u03b710)} are equal to zero. Hence, both expressions (11) and (12) are op(n \u22121/2). For expression (13), it is easy to verify that\n(13) = 1\nn\nn\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) \u03c01i (\u03b7 1 0) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) + op(n \u22121/2).\nCombining all the above results yields that\n\u221a n\u03bb\u0302 = H\u22121 [ 1\u221a n n\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) \u03c01i (\u03b7 1 0) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\u2212 \u221a nA(\u03b7\u03021 \u2212 \u03b710)\n] + op(1).\nIn addition, since \u03b7\u03021 is obtained via maximum likelihood estimation, we have\n\u221a n(\u03b7\u03021 \u2212 \u03b710) = 1\u221a n\nn\u2211\ni=1\n{ E(\u03a8\u22972) }\u22121 \u03a8i + op(1).\nThus,\n\u221a n\u03bb\u0302 = H\u22121 [ 1\u221a n n\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) \u03c01i (\u03b7 1 0) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\u2212 1\u221a n\nn\u2211\ni=1\nA { E(\u03a8\u22972) }\u22121 \u03a8i ] + op(1).\nThis shows the result for \u03bb\u0302. Next, by conditions on the Lagrange multipliers \u03b4\u0302 in the proof of Lemma 1, we have\n0 = 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)}\n1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} .\nThen,\n0 = 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} \u2212 1 n\nn\u2211\ni=1\n(1\u2212Ri) h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n1\u2212 \u03c01i (\u03b7\u03021) (14)\n+ 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) 1\u2212 \u03c01i (\u03b7\u03021) \u2212 1 n\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302)\n1\u2212 \u03c01i (\u03b710) (15)\n+ 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302) 1\u2212 \u03c01i (\u03b710) \u2212 1 n\nn\u2211\ni=1\n(1\u2212Ri) h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)\n1\u2212 \u03c01i (\u03b710) (16)\n+ 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302) 1\u2212 \u03c01i (\u03b710) \u2212 1 n\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n1\u2212 \u03c01i (\u03b710) (17)\n+ 1\nn\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n1\u2212 \u03c01i (\u03b710) . (18)\nSimilar to derivations for expressions (9)\u2013(13), we have\n(14) = \u22121 n\nn\u2211\ni=1\n(1\u2212 Ri) h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)\n\u22972\n{1\u2212 \u03c01i (\u03b7\u03021)}2 \u03b4\u0302 + op(n\n\u22121/2) = \u2212T \u03b4\u0302 + op(n\u22121/2),\n(15) = 1\nn\nn\u2211\ni=1\n(1\u2212Ri) h\u0302i(\u03b7\u2217, \u03b3\u0302, \u03b8\u0302) {1\u2212 \u03c01i (\u03b710)}2 { \u2202\u03c01i (\u03b7 1 0) \u2202\u03b71 } T (\u03b7\u03021 \u2212 \u03b710) + op(n\u22121/2) = B(\u03b7\u03021 \u2212 \u03b710) + op(n\u22121/2),\n(16) = 1\nn\nn\u2211\ni=1\n\u2202E [ (1\u2212Ri)h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b710)} ]\n\u2202\u03b3 (\u03b3\u0302 \u2212 \u03b3\u2217) + op(n\u22121/2) = op(n\u22121/2),\n(17) = 1\nn\nn\u2211\ni=1\n\u2202E [ (1\u2212Ri)h\u0302i(\u03b7\u2217, \u03b3\u2217, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b710)} ]\n\u2202\u03b8 (\u03b8\u0302 \u2212 \u03b8\u2217) + op(n\u22121/2) = op(n\u22121/2),\n(18) = \u22121 n\nn\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) 1\u2212 \u03c01i (\u03b710) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) + op(n \u22121/2).\nFinally, we obtain that\n\u221a n\u03b4\u0302 = T\u22121 [ \u2212 1\u221a\nn\nn\u2211\ni=1\nRi \u2212 \u03c01i (\u03b710) 1\u2212 \u03c01i (\u03b710) hi(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) + 1\u221a n\nn\u2211\ni=1\nB { E(\u03a8\u22972) }\u22121 \u03a8i ] + op(1).\nProof of Theorem 3. When one of propensity score models is correctly specified, \u03b8\u0302CAL converges in probability to \u03b80 as n \u2192 \u221e. Note that \u2211m i=1 \u03c9\u03021i = \u2211n i=m+1 \u03c9\u03020i = 1 and E{Y t(V ; \u03b80)} = E{s(W,V ; \u03b80)}. These results combined with the explicit forms of \u03c9\u03021i and \u03c9\u03020i given in the proof of Lemma 1 imply that\nop(1) =\n\u221a n\nm\nn\u2211\ni=1\nRi\u03c4\u0302 1/\u03c01i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302CAL)/\u03c01i (\u03b7\u0302 1)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212E{Y t(V ; \u03b8\u0302CAL)} ]\n\u2212 \u221a n\nn\u2212m\nn\u2211\ni=1 (1\u2212 Ri)(1\u2212 \u03c4\u0302 1)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Tt\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302CAL)/{1\u2212 \u03c01i (\u03b7\u03021)} [ s(Wi, Vi; \u03b8\u0302CAL)\u2212E{s(W,V ; \u03b8\u0302CAL)} ]\n\u2261C1 \u2212 C2.\nWe rewrite C1 and C2 respectively as\nC1 =\n\u221a n\u03c4\u0302 1\nm\nn\u2211\ni=1\nRi/\u03c0 1 i (\u03b7\u0302 1)\n1 + \u03bb\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302CAL)/\u03c01i (\u03b7\u0302 1)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212E{Y t(V ; \u03b8\u0302CAL)} ] (19)\n\u2212 \u221a n\u03c4\u0302 1\nm\nn\u2211\ni=1\nRi \u03c01i (\u03b7\u0302 1)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212 E{Y t(V ; \u03b8\u0302CAL)} ] (20)\n+ \u03c4\u0302 1\nm\n\u221a n\nn\u2211\ni=1\nRi \u03c01i (\u03b7\u0302 1)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212E{Y t(V ; \u03b8\u0302CAL)} ] (21)\n\u2212 \u03c4\u0302 1\nm\n\u221a n\nn\u2211\ni=1\nRi \u03c01i (\u03b7 1 0)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212 E{Y t(V ; \u03b8\u0302CAL)} ] (22)\n+ \u03c4\u0302 1\nm\n\u221a n\nn\u2211\ni=1\nRi \u03c01i (\u03b7 1 0)\n[ Yit(Vi; \u03b8\u0302CAL)\u2212E{Y t(V ; \u03b8\u0302CAL)} ] (23)\n\u2212 \u03c4\u0302 1\nm\n\u221a n\nn\u2211\ni=1\nRi \u03c01i (\u03b7 1 0)\n[ Yit(Vi; \u03b8\u0302\u2217)\u2212 E{Y t(V ; \u03b8\u0302\u2217)} ] (24)\n+ \u03c4\u0302 1\nm\n\u221a n\nn\u2211\ni=1\nRi \u03c01i (\u03b7 1 0)\n[ Yit(Vi; \u03b8\u0302\u2217)\u2212 E{Y t(V ; \u03b8\u0302\u2217)} ] , (25)\nand\nC2 = \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1 (1\u2212 Ri)/{1\u2212 \u03c01i (\u03b7\u03021)} 1 + \u03b4\u0302Th\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302)/{1\u2212 \u03c01i (\u03b7\u03021)} [ s(Wi, Vi; \u03b8\u0302CAL)\u2212 E{s(W,V ; \u03b8\u0302CAL)} ] (26)\n\u2212 \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b7\u03021) [ s(Wi, Vi; \u03b8\u0302CAL)\u2212E{s(W,V ; \u03b8\u0302CAL)} ] (27)\n+ \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b7\u03021) [ s(Wi, Vi; \u03b8\u0302CAL)\u2212E{s(W,V ; \u03b8\u0302CAL)} ] (28)\n\u2212 \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b710) [ s(Wi, Vi; \u03b8\u0302CAL)\u2212E{s(W,V ; \u03b8\u0302CAL)} ] (29)\n+ \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b710) [ s(Wi, Vi; \u03b8\u0302CAL)\u2212E{s(W,V ; \u03b8\u0302CAL)} ] (30)\n\u2212 \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b710) [ s(Wi, Vi; \u03b8\u2217)\u2212 E{s(W,V ; \u03b8\u2217)} ] (31)\n+ \u221a n(1\u2212 \u03c4\u0302 1) n\u2212m n\u2211\ni=1\n1\u2212Ri 1\u2212 \u03c01i (\u03b710) [ s(Wi, Vi; \u03b8\u2217)\u2212 E{s(W,V ; \u03b8\u2217)} ] . (32)\nFor expressions (19) and (20), taking Taylor expansion around \u03bb = 0 leads to\n(19) + (20) =\u2212 \u03c4\u0302 1\nm\n[ n\u2211\ni=1\nRi Yit(Vi; \u03b8\u0302CAL)\u2212 E{Y t(V ; \u03b8\u0302CAL)} {\u03c01i (\u03b7\u03021)}2 { h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) } T\n]\u221a n\u03bb\u0302+ op(1)\n=\u2212 F \u221a n\u03bb\u0302+ op(1).\nSimilarly, for expressions (26) and (27), taking Taylor expansion around \u03b4 = 0 leads to\n(26) + (27)\n=\u2212 1\u2212 \u03c4\u0302 1\nn\u2212m\n[ n\u2211\ni=1\n(1\u2212 Ri) s(Wi, Vi; \u03b8\u0302CAL)\u2212 E{s(W,V ; \u03b8\u0302CAL)}{\n1\u2212 \u03c01i (\u03b7\u03021) }2\n{ h\u0302i(\u03b7\u0302, \u03b3\u0302, \u03b8\u0302) } T ]\u221a n\u03b4\u0302 + op(1)\n=\u2212G \u221a n\u03b4\u0302 + op(1).\nFor the expression {(21) + (22)} \u2212 {(28) + (29)}, taking Taylor expansion around \u03b7 = \u03b710 yields that\n{(21) + (22)} \u2212 {(28) + (29)}\n= 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7\u0302 1)\n{ Yit(Vi; \u03b8\u0302CAL)\u2212E ( Y t(V ; \u03b8\u0302CAL) )}\n\u2212 1\u2212Ri 1\u2212 \u03c01i (\u03b7\u03021)\n{ s(Wi, Vi; \u03b8\u0302CAL)\u2212E ( s(W,V ; \u03b8\u0302CAL)\n)}]\n\u2212 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7 1 0)\n{ Yit(Vi; \u03b8\u0302CAL)\u2212E ( Y t(V ; \u03b8\u0302CAL) )}\n\u2212 1\u2212Ri 1\u2212 \u03c01i (\u03b710)\n{ s(Wi, Vi; \u03b8\u0302CAL)\u2212 E ( s(W,V ; \u03b8\u0302CAL) )}] + op(1)\n=\n[ 1\nn\nn\u2211\ni=1\n{ \u2212Ri \u03c01i (\u03b7 1 0) 2 ( Yit(Vi; \u03b8\u0302CAL)\u2212E ( Y t(V ; \u03b8\u0302CAL) ))(\u2202\u03c01i (\u03b710) \u2202\u03b71 ) T\n\u2212 1\u2212 Ri (1\u2212 \u03c01i (\u03b710))2\n( s(Wi, Vi; \u03b8\u0302CAL)\u2212 E ( s(W,V ; \u03b8\u0302CAL) ))(\u2202\u03c01i (\u03b710) \u2202\u03b71\n) T }]\n\u00d7 \u221a n(\u03b7\u03021 \u2212 \u03b710) + op(1)\n= \u2212E [{\nY t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80)) \u03c0(V ) + s(W,V ; \u03b80)\u2212 E(s(W,V ; \u03b80)) 1\u2212 \u03c0(V )\n}{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T\n]\n\u00d7 \u221a n(\u03b7\u03021 \u2212 \u03b710) + op(1).\nFor the expression {(23) + (24)} \u2212 {(30) + (31)}, taking Taylor expansion around \u03b8 = \u03b8\u2217 = \u03b80 yields that\n{(23) + (24)} \u2212 {(30) + (31)}\n= 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7 1 0)\n{ Yit(Vi; \u03b8\u0302CAL)\u2212 E ( Y t(V ; \u03b8\u0302CAL) )}\n\u2212 1\u2212 Ri 1\u2212 \u03c01i (\u03b710)\n{ s(Wi, Vi; \u03b8\u0302CAL)\u2212 E ( s(W,V ; \u03b8\u0302CAL)\n)}]\n\u2212 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7 1 0)\n{ Yit(Vi; \u03b80)\u2212E ( Y t(V ; \u03b80) )}\n\u2212 1\u2212 Ri 1\u2212 \u03c01i (\u03b710)\n{ s(Wi, Vi; \u03b8\u03020)\u2212E ( s(W,V ; \u03b8\u03020) )}] + op(1)\n=\n[ 1\nn\nn\u2211\ni=1\n{ Ri\n\u03c01i (\u03b7 1 0)\n( Yi \u2202t(Vi; \u03b80)\n\u2202\u03b8 \u2212 \u2202E(Y t(V ; \u03b80)) \u2202\u03b8\n)\n\u2212 1\u2212Ri 1\u2212 \u03c01i (\u03b710)\n( \u2202s(Wi, Vi; \u03b80)\n\u2202\u03b8 \u2212 \u2202E(s(Wi, Vi; \u03b80)) \u2202\u03b8\n)}]\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1)\n= E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(Wi, Vi; \u03b80) \u2202\u03b8\n}\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1).\nFor the expression (25)\u2212 (32), we have\n(25)\u2212 (32)\n= 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7 1 0)\n{ Yit(Vi; \u03b80)\u2212 E ( Y t(V ; \u03b80) )}\n\u2212 1\u2212Ri 1\u2212 \u03c01i (\u03b710)\n{ s(Wi, Vi; \u03b80)\u2212 E ( s(W,V ; \u03b80) )}] + op(1).\nCombining all the above results yields that\nop(1) = C1 \u2212 C2\n= \u2212F \u221a n\u03bb\u0302+G \u221a n\u03b4\u0302\n+ 1\u221a n\nn\u2211\ni=1\n[ Ri\n\u03c01i (\u03b7 1 0)\n{ Yit(Vi; \u03b80)\u2212 E ( Y t(V ; \u03b80) )}\n\u2212 1\u2212 Ri 1\u2212 \u03c01i (\u03b710)\n{ s(Wi, Vi; \u03b80)\u2212E ( s(W,V ; \u03b80)\n)}]\n\u2212 E [{\nY t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80)) \u03c0(V ) + s(W,V ; \u03b80)\u2212E(s(W,V ; \u03b80)) 1\u2212 \u03c0(V )\n}{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T\n]\n\u00d7 \u221a n(\u03b7\u03021 \u2212 \u03b710) + E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(Wi, Vi; \u03b80) \u2202\u03b8\n}\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1).\nBy Lemma 2 and the expressions of Q(\u03b71), A, B, we obtain from the above expression that 0 = 1\u221a n n\u2211\ni=1\nQi(\u03b7 1 0)\u2212E\n[{ Y t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80))\u2212 FH\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u03c01(\u03b710)\n+ s(W,V ; \u03b80)\u2212 E(s(W,V ; \u03b80))\u2212GT\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n1\u2212 \u03c01(\u03b710)\n}{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T ] \u221a n(\u03b7\u03021 \u2212 \u03b710)\n+ E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(Wi, Vi; \u03b80) \u2202\u03b8\n}\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1).\nIt is easy to verify that\nE\n[{ Y t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80))\u2212 FH\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n\u03c01(\u03b710)\n+ s(W,V ; \u03b80)\u2212 E(s(W,V ; \u03b80))\u2212GT\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n1\u2212 \u03c01(\u03b710)\n}{ \u2202\u03c01(\u03b710)\n\u2202\u03b71\n} T ] = E ( Q\u03a8T ) .\nThus, we have\n0 = 1\u221a n\nn\u2211\ni=1\nQi(\u03b7 1 0)\u2212E\n( Q\u03a8T )\u221a n(\u03b7\u03021 \u2212 \u03b710)\n+ E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(Wi, Vi; \u03b80) \u2202\u03b8\n}\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1)\n= 1\u221a n\nn\u2211\ni=1\n[ Qi(\u03b7 1 0)\u2212E ( Q\u03a8T ){ E(\u03a8\u22972) }\u22121 \u03a8i ]\n+ E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(Wi, Vi; \u03b80) \u2202\u03b8\n}\u221a n(\u03b8\u0302CAL \u2212 \u03b80) + op(1).\nThis implies that\n\u221a n(\u03b8\u0302CAL \u2212 \u03b80) d\u2212\u2192 N { 0,Var(Z) } ,\nwhere\nZ = [ E { Y \u2202t(V ; \u03b80)\n\u2202\u03b8 \u2212 \u2202s(W,V ; \u03b80) \u2202\u03b8\n}]\u22121[ Q\u2212E(Q\u03a8T) { \u03a8\u22972 }\u22121 \u03a8 ] .\nThis completes the proof of Theorem 3.\nProof of Proposition 2. Consider a parametric path \u03b2 for the joint distribution of Y , W , V and R. In order to find the efficient influence function for \u03b80, we need to first find a random variable \u03a6 := \u03a6(Y,W, V,R) with mean 0 and\n\u2202\u03b80(\u03b2)\n\u2202\u03b2 |\u03b2=0= E\n{ \u03a6S\u03b2(Y,W, V,R) } |\u03b2=0,\nwhere S\u03b2(Y,W, V,R) is the observed data distribution score, and \u03b80(\u03b2) is the parameter of interest \u03b80 defined through moment condition (1) under a regular parametric submodel indexed by \u03b2 that includes the true data generating mechanism at \u03b2 = 0.\nDefine \u03c0\u03b2(v) = f\u03b2(R = 1 | V = v). The observed distribution function for Y , W , V\nand R is given by\nf\u03b2(y, w, v, r) = f\u03b2(v){\u03c0\u03b2(v)}r{1\u2212 \u03c0\u03b2(v)}1\u2212rf\u03b2(y | v)rf\u03b2(w | v)1\u2212r.\nThe resulting score function is then given by\nS\u03b2(y, w, v, r) = (1\u2212 r)s\u03b2(w | v) + rs\u03b2(y | v) + r \u2212 \u03c0\u03b2(v)\n\u03c0\u03b2(v){1\u2212 \u03c0\u03b2(v)} \u03c0\u0307\u03b2(v) + s\u03b2(v),\nwhere\ns\u03b2(w | v) = \u2202\n\u2202\u03b2 log f\u03b2(w | v), s\u03b2(y | v) =\n\u2202\n\u2202\u03b2 log f\u03b2(y | v), s\u03b2(v) =\n\u2202\n\u2202\u03b2 log f\u03b2(v).\nThe tangent space of this model is therefore given by:\nT = { (1\u2212 r)s\u03b2(w | v) + rs\u03b2(y | v) + a(v)(r \u2212 \u03c0\u03b2(v)) + s\u03b2(v) } , (33)\nwhere \u222b s\u03b2(w | v)f\u03b2(w | v)dw = 0, \u222b s\u03b2(y | v)f\u03b2(y | v)dy = 0, \u222b s\u03b2(v)f\u03b2(v)dv = 0, and a(v) is any square integrable function.\nRecall that\nE{Y t(V ; \u03b80)\u2212 s(W,V ; \u03b80)} = 0,\nso we have\n\u2202E\u03b2 [Y t{V ; \u03b80(\u03b2)} \u2212 s{W,V ; \u03b80(\u03b2)}] \u2202\u03b2 |\u03b2=0= 0.\nDifferentiating under integral gives\n\u2202\u03b80(\u03b2)\n\u2202\u03b2 |\u03b2=0= \u2212\u0393\u22121\n[ E { Y t(V ; \u03b80)s\u03b2(Y | V )T \u2212 s(W,V ; \u03b80)s\u03b2(W | V )T } |\u03b2=0 ] .\nChoose the random variable \u03a6 to be \u2212\u0393\u22121 [ R\n\u03c0(V )\n{ Y t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80) | V ) } \u2212 1\u2212R 1\u2212 \u03c0(V ) { s(W,V ; \u03b80)\u2212E(s(W,V ; \u03b80) | V )\n}] .\nIt then follows that\nE{\u03a6\u00d7 S\u03b2(Y,W,X,R)}|\u03b2=0\n=\u2212 \u0393\u22121E [ R\n\u03c0(V ) Y t(V ; \u03b80)s\u03b2(Y | V )\u2212\nR\n\u03c0(V ) E{Y t(V ; \u03b80) | V }s\u03b2(Y | V )\n+ R\u2212 R\u03c0(V ) \u03c0(V )2{1\u2212 \u03c0(V )} { Y t(V ; \u03b80)\u2212 E(Y t(V ; \u03b80) | V ) } \u03c0\u0307\u03b2(V ) \u2212 1\u2212 R 1\u2212 \u03c0(V )s(W,V ; \u03b80)s\u03b2(W | V ) + 1\u2212 R 1\u2212 \u03c0(V )E{s(W,V ; \u03b80) | V }s\u03b2(W | V ) \u2212 R\u03c0(V )\u2212 R \u03c0(V ){1\u2212 \u03c0(V )}2 { s(W,V ; \u03b80)\u2212E(s(W,V ; \u03b80) | V ) } \u03c0\u0307\u03b2(V ) ]\n|\u03b2=0\n=\u2212 \u0393\u22121 [ E { Y t(V ; \u03b80)s\u03b2(Y | V )T \u2212 s(W,V ; \u03b80)s\u03b2(W | V )T } |\u03b2=0 ] .\nNow one can also verify that \u03a6 belongs to the tangent space T in (33), with the first and second terms of \u03a6 taking the role of rs\u03b2(y | v) and (1\u2212 r)s\u03b2(w | v), respectively, and the two other components of (33) being identically equal to 0. Therefore, \u03a6 is the efficient influence function of \u03b80.\nProof of Theorem 4. Define\nA1 = R\n\u03c01(\u03b710)\n[ Y t(V ; \u03b80)\u2212 E { Y t(V ; \u03b80) }] , B1 =\nR\n\u03c01(\u03b710) h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217).\nThen F = E(A1B T 1 ) and H = E(B \u22972 1 ). We first invoke the following two facts:\n(1) For any function b(V ), we have that\nE [{ A1 \u2212\nR\n\u03c01(\u03b710)\n( E(Y t(V ; \u03b80) | V )\u2212 E(Y t(V ; \u03b80)) )}{ R \u03c01(\u03b710) b(V ) }] = 0.\n(2) When C2 contains a correctly specified model for f(W | V ), E{Y t(V ; \u03b80) | V } \u2212\nE{Y t(V ; \u03b80)} is a component of h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217). Thus, the vector function [E{Y t(V ; \u03b80) | V } \u2212 E{Y t(V ; \u03b80)}]R/\u03c01(\u03b710) is in the linear space spanned by B1.\nSince all components of h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) are functions of V only, we have that\nFH\u22121B1 = E(A1B T 1 ) { E(B\u229721 ) }\u22121 B1\n=E\n[ R\n\u03c01(\u03b710)\n{ E ( Y t(V ; \u03b80) | V ) \u2212 E ( Y t(V ; \u03b80) )} BT1 ]{ E(B\u229721 ) }\u22121 B1\n= R\n\u03c01(\u03b710)\n[ E { Y t(V ; \u03b80) | V } \u2212 E { Y t(V ; \u03b80) }] ,\nwhere the first equality holds due to the fact (1) and the second equality follows from the fact (2). This shows that FH\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) = E{Y t(V ; \u03b80) | V } \u2212 E{Y t(V ; \u03b80)}. Similarly, we can show that GT\u22121h(\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) = E{s(W,V ; \u03b80) | V } \u2212 E{s(W,V ; \u03b80)}. Note that E{Y t(V ; \u03b80)} = E{s(W,V ; \u03b80)}. Then we can simplify the expression of Q = Q(\u03b71) as\nQ ( \u03b71 ) = R\n\u03c01 (\u03b71) [Y t (V ; \u03b80)\u2212 E {Y t (V ; \u03b80)}]\u2212 R\u2212 \u03c01 (\u03b71) \u03c01 (\u03b71) FH\u22121h (\u03b7\u2217, \u03b3\u2217, \u03b8\u2217) \u2212 1\u2212R 1\u2212 \u03c01 (\u03b71) [s (W,V ; \u03b80)\u2212 E {s (W,V ; \u03b80)}] + \u03c01 (\u03b71)\u2212 R 1\u2212 \u03c01 (\u03b71) GT \u22121h (\u03b7\u2217, \u03b3\u2217, \u03b8\u2217)\n= R\n\u03c01 (\u03b71)\n[ Y t (V ; \u03b80)\u2212 E {Y t (V ; \u03b80) | V } ]\n\u2212 1\u2212R 1\u2212 \u03c01 (\u03b71) [ s (W,V ; \u03b80)\u2212 E {s (W,V ; \u03b80) | V } ] .\nA simple calculation yields that E(Q\u03a8T) = 0. The desired result then follows from Theorem 3."
        }
    ],
    "title": "Calibrated regression estimation using empirical likelihood under data fusion",
    "year": 2022
}