{
    "abstractText": "Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (JOYFUL), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter-view and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that JOYFUL achieved state-of-the-art (SOTA) performance compared to all baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dongyuan Li"
        },
        {
            "affiliations": [],
            "name": "Yusong Wang"
        },
        {
            "affiliations": [],
            "name": "Kotaro Funakoshi"
        },
        {
            "affiliations": [],
            "name": "Manabu Okumura"
        }
    ],
    "id": "SP:76ea13db58c215a2bec686df21b022c38cf28a54",
    "references": [
        {
            "authors": [
                "Tadas Baltrusaitis",
                "Amir Zadeh",
                "Yao Chong Lim",
                "Louis-Philippe Morency"
            ],
            "title": "Openface 2.0: Facial behavior analysis toolkit",
            "venue": "In Proc. of FG,",
            "year": 2018
        },
        {
            "authors": [
                "Yoav Benjamini",
                "Yosef Hochberg"
            ],
            "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
            "venue": "Journal of the Royal statistical society,",
            "year": 1995
        },
        {
            "authors": [
                "Carlos Busso",
                "Murtaza Bulut",
                "Chi-Chun Lee",
                "Abe Kazemzadeh",
                "Emily Mower",
                "Samuel Kim",
                "Jeannette N. Chang",
                "Sungbok Lee",
                "Shrikanth S. Narayanan."
            ],
            "title": "IEMOCAP: interactive emotional dyadic motion capture database",
            "venue": "Lang. Resour. Eval-",
            "year": 2008
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan."
            ],
            "title": "Cross-modal memory networks for radiology report generation",
            "venue": "Proc. of ACL/IJCNLP, pages 5904\u20135914.",
            "year": 2021
        },
        {
            "authors": [
                "Wenliang Dai",
                "Samuel Cahyawijaya",
                "Zihan Liu",
                "Pascale Fung."
            ],
            "title": "Multimodal end-to-end sparse model for emotion recognition",
            "venue": "Proc. of NAACLHLT, pages 5305\u20135316.",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Benoit Delbrouck",
                "No\u00e9 Tits",
                "Mathilde Brousmiche",
                "St\u00e9phane Dupont."
            ],
            "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
            "venue": "Workshop on Multimodal Language (Challenge-HML), pages 1\u20137.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proc. of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proc. of ACL, pages 1383\u20131392.",
            "year": 2018
        },
        {
            "authors": [
                "Florian Eyben",
                "Martin W\u00f6llmer",
                "Bj\u00f6rn Schuller."
            ],
            "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
            "venue": "Proc. of MM, page 1459\u20131462.",
            "year": 2010
        },
        {
            "authors": [
                "Yahui Fu",
                "Shogo Okada",
                "Longbiao Wang",
                "Lili Guo",
                "Yaodong Song",
                "Jiaxing Liu",
                "Jianwu Dang."
            ],
            "title": "CONSK-GCN: conversational semantic- and knowledge-oriented graph convolutional network for multimodal emotion recognition",
            "venue": "Proc. of ICME,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "Proc. of EMNLP, pages 6894\u20136910.",
            "year": 2021
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Alexander Gelbukh",
                "Rada Mihalcea",
                "Soujanya Poria."
            ],
            "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
            "venue": "Findings of EMNLP, pages 2470\u20132481.",
            "year": 2020
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Soujanya Poria",
                "Niyati Chhaya",
                "Alexander Gelbukh."
            ],
            "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
            "venue": "Proc. of EMNLP-IJCNLP, pages 154\u2013164.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaobao Guo",
                "Adams Kong",
                "Huan Zhou",
                "Xianfeng Wang",
                "Min Wang."
            ],
            "title": "Unimodal and crossmodal refinement network for multimodal sequence fusion",
            "venue": "Proc. of EMNLP, pages 9143\u20139153.",
            "year": 2021
        },
        {
            "authors": [
                "Vikram Gupta",
                "Trisha Mittal",
                "Puneet Mathur",
                "Vaibhav Mishra",
                "Mayank Maheshwari",
                "Aniket Bera",
                "Debdoot Mukherjee",
                "Dinesh Manocha."
            ],
            "title": "3massiv: Multilingual, multimodal and multi-aspect dataset of social media short videos",
            "venue": "Proc. of CVPR, pages",
            "year": 2022
        },
        {
            "authors": [
                "William L. Hamilton",
                "Zhitao Ying",
                "Jure Leskovec."
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Proc. of NeurIPS, pages 1024\u20131034.",
            "year": 2017
        },
        {
            "authors": [
                "Wei Han",
                "Hui Chen",
                "Alexander F. Gelbukh",
                "Amir Zadeh",
                "Louis-Philippe Morency",
                "Soujanya Poria."
            ],
            "title": "Bi-bimodal modality fusion for correlationcontrolled multimodal sentiment analysis",
            "venue": "Proc. of ICMI, pages 6\u201315.",
            "year": 2021
        },
        {
            "authors": [
                "Wei Han",
                "Hui Chen",
                "Soujanya Poria."
            ],
            "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
            "venue": "Proc. of EMNLP, pages 9180\u20139192.",
            "year": 2021
        },
        {
            "authors": [
                "Devamanyu Hazarika",
                "Roger Zimmermann",
                "Soujanya Poria."
            ],
            "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
            "venue": "Proc. of MM, page 1122\u20131131.",
            "year": 2020
        },
        {
            "authors": [
                "Dou Hu",
                "Xiaolong Hou",
                "Lingwei Wei",
                "Lian-Xin Jiang",
                "Yang Mo."
            ],
            "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
            "venue": "Proc. of ICASSP, pages 7037\u20137041.",
            "year": 2022
        },
        {
            "authors": [
                "Dou Hu",
                "Lingwei Wei",
                "Xiaoyong Huai."
            ],
            "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
            "venue": "Proc. of ACL, pages 7042\u20137052.",
            "year": 2021
        },
        {
            "authors": [
                "Guimin Hu",
                "Ting-En Lin",
                "Yi Zhao",
                "Guangming Lu",
                "Yuchuan Wu",
                "Yongbin Li."
            ],
            "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
            "venue": "Proc. of EMNLP, pages 7837\u20137851.",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger."
            ],
            "title": "Densely connected convolutional networks",
            "venue": "Proc. of CVPR, pages 2261\u2013 2269.",
            "year": 2017
        },
        {
            "authors": [
                "Taichi Ishiwatari",
                "Yuki Yasuda",
                "Taro Miyazaki",
                "Jun Goto."
            ],
            "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
            "venue": "Proc. of EMNLP, pages 7360\u20137370.",
            "year": 2020
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Michael R. Lyu",
                "Irwin King."
            ],
            "title": "Real-time emotion recognition via attention gated hierarchical memory network",
            "venue": "Proc. of AAAI, pages 8002\u20138009.",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Joshi",
                "Ashwani Bhat",
                "Ayush Jain",
                "Atin Singh",
                "Ashutosh Modi."
            ],
            "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
            "venue": "Proc. of NAACL, pages 4148\u20134164.",
            "year": 2022
        },
        {
            "authors": [
                "Leo Katz."
            ],
            "title": "A new status index derived from sociometric analysis",
            "venue": "Psychometrika, 18:39\u201343.",
            "year": 1953
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "Proc. of EMNLP, pages 1746\u20131751.",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proc. of ICLR, pages 1\u201315.",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "Proc. of ICLR, pages 1\u201314.",
            "year": 2017
        },
        {
            "authors": [
                "Hung Le",
                "Nancy Chen",
                "Steven Hoi."
            ],
            "title": "Multimodal dialogue state tracking",
            "venue": "Proc. of NAACL, pages 3394\u20133415.",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "Yoshua Bengio"
            ],
            "title": "Convolutional networks for images, speech, and time series",
            "venue": "The handbook of brain theory and neural networks,",
            "year": 1995
        },
        {
            "authors": [
                "Howard Levene"
            ],
            "title": "Contributions to probability and statistics",
            "venue": "Essays in honor of Harold Hotelling, 278:292.",
            "year": 1960
        },
        {
            "authors": [
                "Dongyuan Li",
                "Jingyi You",
                "Kotaro Funakoshi",
                "Manabu Okumura."
            ],
            "title": "A-TIP: attribute-aware text infilling via pre-trained language model",
            "venue": "Proc. of COLING, pages 5857\u20135869.",
            "year": 2022
        },
        {
            "authors": [
                "Sha Li",
                "Madhi Namazifar",
                "Di Jin",
                "MOHIT BANSAL",
                "Heng Ji",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Enhanced knowledge selection for grounded dialogues via document semantic graphs",
            "venue": "NAACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Shimin Li",
                "Hang Yan",
                "Xipeng Qiu."
            ],
            "title": "Contrast and generation make BART a good dialogue emotion recognizer",
            "venue": "Proc. of AAAI, pages 11002\u201311010.",
            "year": 2022
        },
        {
            "authors": [
                "Shuangli Li",
                "Jingbo Zhou",
                "Tong Xu",
                "Dejing Dou",
                "Hui Xiong."
            ],
            "title": "Geomgcl: Geometric graph contrastive learning for molecular property prediction",
            "venue": "Proc. of AAAI, pages 4541\u20134549.",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Li",
                "Bing Xu",
                "Conghui Zhu",
                "Tiejun Zhao."
            ],
            "title": "CLMLF:a contrastive learning and multilayer fusion method for multimodal sentiment detection",
            "venue": "Findings of NAACL, pages 2282\u20132294.",
            "year": 2022
        },
        {
            "authors": [
                "Sheng Liang",
                "Mengjie Zhao",
                "Hinrich Schuetze."
            ],
            "title": "Modular and parameter-efficient multimodal fusion with prompting",
            "venue": "Findings of ACL, pages 2976\u20132985.",
            "year": 2022
        },
        {
            "authors": [
                "Tao Liang",
                "Guosheng Lin",
                "Lei Feng",
                "Yan Zhang",
                "Fengmao Lv."
            ],
            "title": "Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion",
            "venue": "Proc. of ICCV, pages 8128\u20138136.",
            "year": 2021
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinan Xu",
                "Jiaan Wang",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "Summary-oriented vision modeling for multimodal abstractive summarization",
            "venue": "Proc. of ACL, pages 2934\u20132951.",
            "year": 2023
        },
        {
            "authors": [
                "Yan Ling",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Visionlanguage pre-training for multimodal aspect-based sentiment analysis",
            "venue": "Proc. of ACL, pages 2149\u2013 2159.",
            "year": 2022
        },
        {
            "authors": [
                "Nayu Liu",
                "Xian Sun",
                "Hongfeng Yu",
                "Wenkai Zhang",
                "Guangluan Xu."
            ],
            "title": "Multistage fusion with forget gate for multimodal summarization in open-domain videos",
            "venue": "Proc. of EMNLP, pages 1834\u20131845.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Fengmao Lv",
                "Xiang Chen",
                "Yanyong Huang",
                "Lixin Duan",
                "Guosheng Lin."
            ],
            "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
            "venue": "Proc. of CVPR, pages 2554\u20132562.",
            "year": 2021
        },
        {
            "authors": [
                "Navonil Majumder",
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Rada Mihalcea",
                "Alexander F. Gelbukh",
                "Erik Cambria."
            ],
            "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
            "venue": "Proc. of AAAI, pages 6818\u20136825.",
            "year": 2019
        },
        {
            "authors": [
                "Huisheng Mao",
                "Ziqi Yuan",
                "Hua Xu",
                "Wenmeng Yu",
                "Yihe Liu",
                "Kai Gao."
            ],
            "title": "M-SENA: An integrated platform for multimodal sentiment analysis",
            "venue": "Proc. of ACL, pages 204\u2013213.",
            "year": 2022
        },
        {
            "authors": [
                "Arsha Nagrani",
                "Shan Yang",
                "Anurag Arnab",
                "Aren Jansen",
                "Cordelia Schmid",
                "Chen Sun."
            ],
            "title": "Attention bottlenecks for multimodal fusion",
            "venue": "Proc. of NeurIPS, pages 14200\u201314213.",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Nicolson",
                "Jason Dowling",
                "Bevan Koopman."
            ],
            "title": "e-health CSIRO at radsum23: Adapting a chest x-ray report generator to multimodal radiology report summarisation",
            "venue": "The 22nd Workshop on BioNLP@ACL, pages 545\u2013549.",
            "year": 2023
        },
        {
            "authors": [
                "Timothy Ossowski",
                "Junjie Hu."
            ],
            "title": "Retrieving multimodal prompts for generative visual question answering",
            "venue": "Findings of the ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Sarah Partan",
                "Peter Marler."
            ],
            "title": "Communication goes multimodal",
            "venue": "Science, 283(5406):1272\u20131273.",
            "year": 1999
        },
        {
            "authors": [
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Navonil Majumder",
                "Gautam Naik",
                "Erik Cambria",
                "Rada Mihalcea."
            ],
            "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
            "venue": "Proc. of ACL, pages 527\u2013536.",
            "year": 2019
        },
        {
            "authors": [
                "Soujanya Poria",
                "Navonil Majumder",
                "Devamanyu Hazarika",
                "Deepanway Ghosal",
                "Rishabh Bhardwaj",
                "Samson Yu Bai Jian",
                "Pengfei Hong",
                "Romila Ghosh",
                "Abhinaba Roy",
                "Niyati Chhaya",
                "Alexander F. Gelbukh",
                "Rada Mihalcea"
            ],
            "title": "Recognizing emotion",
            "year": 2021
        },
        {
            "authors": [
                "Soujanya Poria",
                "Navonil Majumder",
                "Rada Mihalcea",
                "Eduard H. Hovy."
            ],
            "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
            "venue": "IEEE Access, 7:100943\u2013100953.",
            "year": 2019
        },
        {
            "authors": [
                "Tuomas Puoliv\u00e4li",
                "Satu Palva",
                "J. Matias Palva."
            ],
            "title": "Influence of multiple hypothesis testing on reproducibility in neuroimaging research: A simulation study and python-based software",
            "venue": "Journal of Neuroscience Methods, 337:108654.",
            "year": 2020
        },
        {
            "authors": [
                "Preeth Raguraman",
                "Mohan Ramasundaram",
                "Midhula Vijayan."
            ],
            "title": "Librosa based assessment tool for music information retrieval systems",
            "venue": "Proc. of MIPR, pages 109\u2013114.",
            "year": 2019
        },
        {
            "authors": [
                "Wasifur Rahman",
                "Md. Kamrul Hasan",
                "Sangwu Lee",
                "AmirAli Bagher Zadeh",
                "Chengfeng Mao",
                "LouisPhilippe Morency",
                "Mohammed E. Hoque."
            ],
            "title": "Integrating multimodal information in large pretrained transformers",
            "venue": "Proc. of ACL, pages 2359\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Vandana Rajan",
                "Alessio Brutti",
                "Andrea Cavallaro."
            ],
            "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition? In Proc",
            "venue": "of ICASSP, pages 4693\u20134697.",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proc. of EMNLP-IJCNLP, pages 3982\u2013 3992.",
            "year": 2019
        },
        {
            "authors": [
                "Franco Scarselli",
                "Marco Gori",
                "Ah Chung Tsoi",
                "Markus Hagenbuchner",
                "Gabriele Monfardini."
            ],
            "title": "The graph neural network model",
            "venue": "IEEE Trans. Neural Networks, 20(1):61\u201380.",
            "year": 2009
        },
        {
            "authors": [
                "Brian B Schultz."
            ],
            "title": "Levene\u2019s test for relative variation",
            "venue": "Systematic Zoology, 34(4):449\u2013456.",
            "year": 1985
        },
        {
            "authors": [
                "Shiv Shankar."
            ],
            "title": "Multimodal fusion via cortical network inspired losses",
            "venue": "Proc. of ACL, pages 1167\u2013 1178.",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Sanford Shapiro",
                "Martin B Wilk."
            ],
            "title": "An analysis of variance test for normality (complete samples)",
            "venue": "Biometrika, 52(3/4):591\u2013611.",
            "year": 1965
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "Proc. of ICML, volume 80, pages 4603\u20134611.",
            "year": 2018
        },
        {
            "authors": [
                "Weizhou Shen",
                "Junqing Chen",
                "Xiaojun Quan",
                "Zhixian Xie."
            ],
            "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
            "venue": "Proc. of AAAI, pages 13789\u201313797.",
            "year": 2021
        },
        {
            "authors": [
                "Weizhou Shen",
                "Siyue Wu",
                "Yunyi Yang",
                "Xiaojun Quan."
            ],
            "title": "Directed acyclic graph network for conversational emotion recognition",
            "venue": "Proc. of ACL/IJCNLP, pages 1551\u20131560.",
            "year": 2021
        },
        {
            "authors": [
                "Dongming Sheng",
                "Dong Wang",
                "Ying Shen",
                "Haitao Zheng",
                "Haozhuang Liu."
            ],
            "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
            "venue": "Proc. of COLING, pages 4153\u20134163.",
            "year": 2020
        },
        {
            "authors": [
                "Aman Shenoy",
                "Ashish Sardana"
            ],
            "title": "Multilogue-net: A context-aware RNN for multimodal emotion detection and sentiment analysis in conversation",
            "venue": "In Workshop on Multimodal Language (Challenge-HML),",
            "year": 2020
        },
        {
            "authors": [
                "Apoorva Singh",
                "Soumyodeep Dey",
                "Anamitra Singha",
                "Sriparna Saha."
            ],
            "title": "Sentiment and emotionaware multi-modal complaint identification",
            "venue": "Proc. of AAAI, pages 12163\u201312171.",
            "year": 2022
        },
        {
            "authors": [
                "Tiening Sun",
                "Zhong Qian",
                "Sujun Dong",
                "Peifeng Li",
                "Qiaoming Zhu."
            ],
            "title": "Rumor detection on social media with graph adversarial contrastive learning",
            "venue": "Proc. of WWW, pages 2789\u20132797.",
            "year": 2022
        },
        {
            "authors": [
                "Shiyin Tan",
                "Jingyi You",
                "Dongyuan Li."
            ],
            "title": "Temporality- and frequency-aware graph contrastive learning for temporal network",
            "venue": "Proc. of CIKM, pages 1878\u20131888.",
            "year": 2022
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Shaojie Bai",
                "Paul Pu Liang",
                "J. Zico Kolter",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "venue": "Proc. of ACL, pages 6558\u20136569.",
            "year": 2019
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Learning factorized multimodal representations",
            "venue": "proc. of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Martin Ma",
                "Muqiao Yang",
                "Ruslan Salakhutdinov",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal routing: Improving local and global interpretability of multimodal language analysis",
            "venue": "Proc. of EMNLP, pages 1823\u20131833.",
            "year": 2020
        },
        {
            "authors": [
                "Tana Wang",
                "Yaqing Hou",
                "Dongsheng Zhou",
                "Qiang Zhang."
            ],
            "title": "A contextual attention network for multimodal emotion recognition in conversation",
            "venue": "Proc. of IJCNN, pages 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "Yan Wang",
                "Jiayu Zhang",
                "Jun Ma",
                "Shaojun Wang",
                "Jing Xiao."
            ],
            "title": "Contextualized emotion recognition in conversation as sequence tagging",
            "venue": "Proc. of SIGDIAL, pages 186\u2013195.",
            "year": 2020
        },
        {
            "authors": [
                "Yansen Wang",
                "Ying Shen",
                "Zhun Liu",
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency."
            ],
            "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
            "venue": "Proc. of AAAI, pages 7216\u20137223.",
            "year": 2019
        },
        {
            "authors": [
                "Yikai Wang",
                "Xinghao Chen",
                "Lele Cao",
                "Wenbing Huang",
                "Fuchun Sun",
                "Yunhe Wang."
            ],
            "title": "Multimodal token fusion for vision transformers",
            "venue": "Proc. of CVPR, pages 12176\u201312185.",
            "year": 2022
        },
        {
            "authors": [
                "Yusong Wang",
                "Dongyuan Li",
                "Kotaro Funakoshi",
                "Manabu Okumura."
            ],
            "title": "Emp: Emotion-guided multi-modal fusion and contrastive learning for personality traits recognition",
            "venue": "Proc. of ICMR, page 243\u2013252.",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Wang."
            ],
            "title": "Modern question answering datasets and benchmarks: A survey",
            "venue": "CoRR, abs/2206.15030.",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Wang",
                "Xu Shan",
                "Xiangxie Zhang",
                "Jie Yang."
            ],
            "title": "N24news: A new dataset for multimodal news classification",
            "venue": "Proc. of LREC, pages 6768\u2013 6775.",
            "year": 2022
        },
        {
            "authors": [
                "Yinwei Wei",
                "Xiang Wang",
                "Liqiang Nie",
                "Xiangnan He",
                "Richang Hong",
                "Tat-Seng Chua."
            ],
            "title": "MMGCN: multi-modal graph convolution network for personalized recommendation of micro-video",
            "venue": "Proc. of MM, pages 1437\u20131445.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Wu",
                "Pengwei Zhan",
                "Yunjian Zhang",
                "LiMing Wang",
                "Zhen Xu."
            ],
            "title": "Multimodal fusion with coattention networks for fake news detection",
            "venue": "Findings of the ACL/IJCNLP, pages 2560\u20132569.",
            "year": 2021
        },
        {
            "authors": [
                "Dongkuan Xu",
                "Wei Cheng",
                "Dongsheng Luo",
                "Haifeng Chen",
                "Xiang Zhang."
            ],
            "title": "Infogcl: Informationaware graph contrastive learning",
            "venue": "Proc. of NeurIPS, pages 30414\u201330425.",
            "year": 2021
        },
        {
            "authors": [
                "Jianing Yang",
                "Yongxin Wang",
                "Ruitao Yi",
                "Yuying Zhu",
                "Azaan Rehman",
                "Amir Zadeh",
                "Soujanya Poria",
                "Louis-Philippe Morency."
            ],
            "title": "MTAG: modaltemporal attention graph for unaligned human multimodal language sequences",
            "venue": "Proc. of NAACL-HLT,",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Yang",
                "Xin Liu"
            ],
            "title": "A reexamination of text categorization methods",
            "venue": "In Proc. of SIGIR,",
            "year": 1999
        },
        {
            "authors": [
                "Jingyi You",
                "Dongyuan Li",
                "Manabu Okumura",
                "Kenji Suzuki."
            ],
            "title": "JPG - jointly learn to align: Automated disease prediction and radiology report generation",
            "venue": "Proc. of COLING, pages 5989\u20136001.",
            "year": 2022
        },
        {
            "authors": [
                "Yuning You",
                "Tianlong Chen",
                "Yongduo Sui",
                "Ting Chen",
                "Zhangyang Wang",
                "Yang Shen."
            ],
            "title": "Graph contrastive learning with augmentations",
            "venue": "Proc. of NeurIPS, pages 1\u201312.",
            "year": 2020
        },
        {
            "authors": [
                "Wenmeng Yu",
                "Hua Xu",
                "Ziqi Yuan",
                "Jiele Wu."
            ],
            "title": "Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
            "venue": "Proc. of AAAI, pages 10790\u201310797.",
            "year": 2021
        },
        {
            "authors": [
                "Amir Zadeh",
                "Minghai Chen",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency."
            ],
            "title": "Tensor fusion network for multimodal sentiment analysis",
            "venue": "Proc. of EMNLP, pages 1103\u20131114.",
            "year": 2017
        },
        {
            "authors": [
                "Amir Zadeh",
                "Paul Pu Liang",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
            "venue": "Proc. of ACL, pages 2236\u20132246.",
            "year": 2018
        },
        {
            "authors": [
                "Amir Zadeh",
                "Rowan Zellers",
                "Eli Pincus",
                "LouisPhilippe Morency."
            ],
            "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
            "venue": "IEEE Intelligent Systems, 31(6):82\u201388.",
            "year": 2016
        },
        {
            "authors": [
                "Jiaqi Zeng",
                "Pengtao Xie."
            ],
            "title": "Contrastive selfsupervised learning for graph classification",
            "venue": "Proc. of AAAI, pages 10824\u201310832.",
            "year": 2021
        },
        {
            "authors": [
                "Dong Zhang",
                "Xincheng Ju",
                "Wei Zhang",
                "Junhui Li",
                "Shoushan Li",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Multi-modal multi-label emotion recognition with heterogeneous hierarchical message passing",
            "venue": "Proc. of AAAI, pages 14338\u201314346.",
            "year": 2021
        },
        {
            "authors": [
                "Yifei Zhang",
                "Hao Zhu",
                "Zixing Song",
                "Piotr Koniusz",
                "Irwin King."
            ],
            "title": "COSTA: covariance-preserving feature augmentation for graph contrastive learning",
            "venue": "Proc. of KDD, pages 1\u201318.",
            "year": 2022
        },
        {
            "authors": [
                "Ying Zhang",
                "Hidetaka Kamigaito",
                "Manabu Okumura."
            ],
            "title": "Bidirectional transformer reranker for grammatical error correction",
            "venue": "Findings of ACL, pages 3801\u20133825.",
            "year": 2023
        },
        {
            "authors": [
                "Yanqiao Zhu",
                "Yichen Xu",
                "Feng Yu",
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang."
            ],
            "title": "Deep Graph Contrastive Representation Learning",
            "venue": "ICML Workshop on Graph Representation Learning and Beyond.",
            "year": 2020
        },
        {
            "authors": [
                "Ghosal"
            ],
            "title": "2019)) and saving the additional time-consuming on fine-tuning, we directly adopt Sentence-BERT as our text encoder for IEMOCAP",
            "venue": "F Pseudo-Code of JOYFUL As shown in Algorithm 1,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16051\u201316069 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "\u201cIntegration of information from multiple sensory channels is crucial for understanding tendencies and reactions in humans\u201d (Partan and Marler, 1999). Multimodal emotion recognition in conversations (MERC) aims exactly to identify and track the emotional state of each utterance from heterogeneous visual, audio, and text channels. Due to its potential applications in creating human-computer interaction systems (Li et al., 2022b), social media analysis (Gupta et al., 2022; Wang et al., 2023), and recommendation systems (Singh et al., 2022), MERC has received increasing attention in the natural language processing (NLP) community (Poria et al., 2019b, 2021), which even has the potential\nto be widely applied in other tasks such as question answering (Ossowski and Hu, 2023; Wang et al., 2022b; Wang, 2022), text generation (Liang et al., 2023; Zhang et al., 2023; Li et al., 2022a) and bioinformatics (Nicolson et al., 2023; You et al., 2022).\nFigure 1 shows that emotions expressed in a dialogue are affected by three main factors: 1) multiple uni-modalities (different modalities complete each other to provide a more informative utterance representation); 2) global contextual information (uA3 depends on the topic \u201cThe ship sank into the sea\u201d, indicating fear); and 3) intra-person and interperson dependencies (uA6 becomes sad affected by sadness in uB4 &u B 5 ). Depending on how to model intra-person and inter-person dependencies, current MERC methods can be categorized into Sequencebased and Graph-based methods. The former (Dai et al., 2021; Mao et al., 2022; Liang et al., 2022) use recurrent neural networks or Transformers to model the temporal interaction between utterances. However, they failed to distinguish intra-speaker and inter-speaker dependencies and easily lost unimodal specific features by the cross-modal attention mechanism (Rajan et al., 2022). Graph struc-\n16051\nture (Joshi et al., 2022; Wei et al., 2019) solves these issues by using edges between nodes (speakers) to distinguish intra-speaker and inter-speaker dependencies. Graph Neural Networks (GNNs) further help nodes learn common features by aggregating information from neighbours while maintaining their uni-modal specific features.\nAlthough graph-based MERC methods have achieved great success, there still remain problems that need to be solved: 1) Current methods directly aggregate features of multiple modalities (Joshi et al., 2022) or project modalities into a latent space to learn representations (Li et al., 2022e), which ignores the diversity of each modality and fails to capture richer semantic information from each modality. They also ignore global contextual information during the feature fusion process, leading to poor performance. 2) Since all graphbased methods adopt GNN (Scarselli et al., 2009) or Graph Convolutional Networks (GCNs) (Kipf and Welling, 2017), with the number of layers deepening, the phenomenon of over-smoothing starts to appear, resulting in the representation of similar sentiments being indistinguishable. 3) Most methods use a two-phase pipeline (Fu et al., 2021; Joshi et al., 2022), where they first extract and fuse uni-modal features as utterance representations and then fix them as input for graph models. However, the two-phase pipeline will lead to sub-optimal performance since the fused representations are fixed and cannot be further improved to benefit from the downstream supervisory signals.\nTo solve the above-mentioned problems, we propose Joint multimodality fusion and graph contrastive learning for MERC (JOYFUL), where multimodality fusion, graph contrastive learning (GCL), and multimodal emotion recognition are jointly optimized in an overall objective function. 1) We first design a new multimodal fusion mechanism that can simultaneously learn and fuse a global contextual representation and uni-modal specific representations. For the global contextual representation, we smooth it with a proposed topic-related vector to maintain its consistency, where the topicrelated vector is temporally updated since the topic usually changes. For uni-modal specific representations, we project them into a shared subspace to fully explore their richer semantics without losing alignment with other modalities. 2) To alleviate the over-smoothing issue of deeper GNN layers, inspired by You et al. (2020), that showed con-\ntrastive learning could provide more distinguishable node representations to benefit various downstream tasks, we propose a cross-view GCL-based framework to alleviate the difficulty of categorizing similar emotions, which helps to learn more distinctive utterance representations by making samples with the same sentiment cohesive and those with different sentiments mutually exclusive. Furthermore, graph augmentation strategies are designed to improve JOYFUL\u2019s robustness and generalizability. 3) We jointly optimize each part of JOYFUL in an end-to-end manner to ensure global optimized performance. The main contributions of this study can be summarized as follows:\n\u2022 We propose a novel joint leaning framework for MERC, where multimodality fusion, GCL, and emotion recognition are jointly optimized for global optimal performance. Our new multimodal fusion mechanism can obtain better representations by simultaneously depicting global contextual and local uni-modal specific features.\n\u2022 To the best of our knowledge, JOYFUL is the first method to utilize graph contrastive learning for MERC, which significantly improves the model\u2019s ability to distinguish different sentiments. Multiple graph augmentation strategies further improve the model\u2019s stability and generalization.\n\u2022 Extensive experiments conducted on three multimodal benchmark datasets demonstrated the effectiveness and robustness of JOYFUL."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal Emotion Recognition",
            "text": "Depending on how to model the context of utterances, existing MERC methods are categorized into three classes: Recurrent-based methods (Majumder et al., 2019; Mao et al., 2022) adopt RNN or LSTM to model the sequential context for each utterance. Transformers-based methods (Ling et al., 2022; Liang et al., 2022; Le et al., 2022) use Transformers with cross-modal attention to model the intra- and inter-speaker dependencies. Graphbased methods (Joshi et al., 2022; Zhang et al., 2021; Fu et al., 2021) can control context information for each utterance and provide accurate intraand inter-speaker dependencies, achieving SOTA performance on many MERC benchmark datasets."
        },
        {
            "heading": "2.2 Multimodal Fusion Mechanism",
            "text": "Learning effective fusion mechanisms is one of the core challenges in multimodal learning (Shankar,\n2022). By capturing the interactions between different modalities more reasonably, deep models can acquire more comprehensive information. Current fusion methods can be classified into aggregationbased (Wu et al., 2021; Guo et al., 2021), alignmentbased (Liu et al., 2020; Li et al., 2022e), and their mixture (Wei et al., 2019; Nagrani et al., 2021). Aggregation-based fusion methods (Zadeh et al., 2017; Chen et al., 2021) adopt concatenation, tensor fusion and memory fusion to combine multiple modalities. Alignment-based fusion centers on latent cross-modal adaptation, which adapts streams from one modality to another (Wang et al., 2022a). Different from the above methods, we learn global contextual information by concatenation while fully exploring the specific patterns of each modality in an alignment manner."
        },
        {
            "heading": "2.3 Graph Contrastive Learning",
            "text": "GCL aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data- or task-specific augmentations, to inject the desired feature invariance (You et al., 2020). GCL has been well used in the NLP community via self-supervised and supervised settings. Self-supervised GCL first creates augmented graphs by edge/node deletion and insertion (Zeng and Xie, 2021), or attribute masking (Zhang et al., 2022). It then captures the intrinsic patterns and properties in the augmented graphs without using human provided labels. Supervised GCL designs adversarial (Sun et al., 2022) or geometric (Li et al., 2022d) contrastive loss to make full use of label information. For example, Li et al. (2022c) first used supervised CL for emotion recognition, greatly improving the performance. Inspired by previous studies, we jointly consider self-supervised (suit-\nable graph augmentation) and supervised (crossentropy) manners to fully explore graph structural information and downstream supervisory signals."
        },
        {
            "heading": "3 Methodology",
            "text": "Figure 2 shows an overview of JOYFUL, which mainly consists of four components: (A) a unimodal extractor, (B) a multimodal fusion (MF) module, (C) a graph contrastive learning module, and (D) a classifier. Hereafter, we give formal notations and the task definition of JOYFUL, and introduce each component subsequently in detail."
        },
        {
            "heading": "3.1 Notations and Task Definition",
            "text": "In dialogue emotion recognition, a training dataset D = {(Ci,Yi)}Ni=1 is given, where Ci represents the i-th conversation, each conversation contains several utterances Ci = {u1, . . . ,um}, and Yi \u2208 Ym, given label set Y = {y1, . . . , yk} of k emotion classes. Let Xv, Xa, Xt be the visual, audio, and text feature spaces, respectively. The goal of MERC is to learn a function F : Xv\u00d7Xa\u00d7Xt \u2192 Y that can recognize the emotion label for each utterance. We utilize three widely used multimodal conversational benchmark datasets, namely IEMOCAP, MOSEI, and MELD, to evaluate the performance of our model. Please see Section 4.1 for their detailed statistical information."
        },
        {
            "heading": "3.2 Uni-modal Extractor",
            "text": "For IEMOCAP (Busso et al., 2008), video features xv \u2208 R512, audio features xa \u2208 R100, and text features xt \u2208 R768 are obtained from OpenFace (Baltrusaitis et al., 2018), OpenSmile (Eyben et al., 2010) and SBERT (Reimers and Gurevych, 2019), respectively. For MELD (Poria et al., 2019a), xv \u2208\nR342, xa \u2208 R300, and xt \u2208 R768 are obtained from DenseNet (Huang et al., 2017), OpenSmile, and TextCNN (Kim, 2014). For MOSEI (Zadeh et al., 2018), xv \u2208 R35, xa \u2208 R80, and xt \u2208 R768 are obtained from TBJE (Delbrouck et al., 2020), LibROSA (Raguraman et al., 2019), and SBERT. Textual features are sentence-level static features. Audio and visual modalities are utterance-level features by averaging all the token features."
        },
        {
            "heading": "3.3 Multimodal Fusion Module",
            "text": "Though the uni-modal extractors can capture longterm temporal context, they are unable to handle feature redundancy and noise due to the modality gap. Thus, we design a new multimodal fusion module (Figure 2 (B)) to inherently separate multiple modalities into two disjoint parts, contextual representations and specific representations, to extract the consistency and specificity of heterogeneous modalities collaboratively and individually."
        },
        {
            "heading": "3.3.1 Contextual Representation Learning",
            "text": "Contextual representation learning aims to explore and learn hidden contextual intent/topic knowledge of the dialogue, which can greatly improve the performance of JOYFUL. In Figure 2 (B1), we first project all uni-modal inputs x{v,a,t} into a latent space by using three separate connected deep neural networks fg{v,a,t}(\u00b7) to obtain hidden representations zg{v,a,t}. Then, we concatenate them as z g m and apply it to a multi-layer transformer to maximize the correlation between multimodal features, where we learn a global contextual multimodal representation z\u0302gm. Considering that the contextual information will change over time, we design a temporal smoothing strategy for z\u0302gm as\nJsmooth = \u2225z\u0302gm \u2212 zcon\u22252, (1)\nwhere zcon is the topic-related vector describing the high-level global contextual information without requiring topic-related inputs, following the definition in Joshi et al. (2022). We update the (i+1)-th utterance as zcon \u2190 zcon+e\u03b7\u2217iz\u0302gm, and \u03b7 is the exponential smoothing parameter (Shazeer and Stern, 2018), indicating that more recent information will be more important.\nTo ensure fused contextual representations capture enough details from hidden layers, Hazarika et al. (2020) minimized the reconstruction error between fused representations with hidden representations. Inspired by their work, to ensure that z\u0302gm\ncontains essential modality cues for downstream emotion recognition, we reconstruct zgm from z\u0302 g m by minimizing their Euclidean distance:\nJ grec = \u2225z\u0302gm \u2212 zgm\u22252. (2)"
        },
        {
            "heading": "3.3.2 Specific Representation Learning",
            "text": "Specific representation learning aims to fully explore specific information from each modality to complement one another. Figure 2 (B2) shows that we first use three fully connected deep neural networks f \u2113{v,a,t}(\u00b7) to project uni-modal embeddings x{v,a,t} into a hidden space with representations as z\u2113{v,a,t}. Considering that visual, audio, and text features are extracted with different encoding methods, directly applying multiple specific features as an input for the downstream emotion recognition task will degrade the model\u2019s accuracy. To solve it, the multimodal features are projected into a shared subspace, and a shared trainable basis matrix is designed to learn aligned representations for them. Therefore, the multimodal features can be fully integrated and interacted to mitigate feature discontinuity and remove noise across modalities. We define a shared trainable basis matrix B with q basis vectors as B = (b1, . . . , bq)T \u2208 Rq\u00d7db with db representing the dimensionality of each basis vector. Here, T indicates transposition. Then, z\u2113{v,a,t} and B are projected into the shared subspace:\nz\u0303\u2113{v,a,t} = W{v,a,t}z \u2113 {v,a,t}, B\u0303 = BWb, (3)\nwhere W{v,a,t,b} are trainable parameters. To learn new representations for each modality, we calculate the cosine similarity between them and B as\nS {v,a,t} ij = (z\u0303 \u2113 {v,a,t})i \u00b7 b\u0303j , (4)\nwhere Svij denotes the similarity between the i-th visual feature (z\u0303\u2113v)i and the j-th basis vector representation b\u0303j . To prevent inaccurate representation learning caused by an excessive weight of a certain item, the similarities are further normalized by\nS {v,a,t} ij = exp (S {v,a,t} ij )\u2211q\nk=1 exp (S {v,a,t} ik )\n. (5)\nThen, the new representations are obtained as\n(z\u0302\u2113{v,a,t})i = q\u2211\nk=1\nS {v,a,t} ik \u00b7 b\u0303k, (6)\nwhere z\u0302\u2113{v,a,t} are new representations, and we also use reconstruction loss for their combinations\nJ \u2113rec = \u2225z\u0302\u2113m \u2212 z\u2113m\u22252, (7)\nwhere Concat( , ) indicating the concatenation, i.e., z\u0302\u2113m=Concat(z\u0302 \u2113 v, z\u0302 \u2113 a, z\u0302 \u2113 t ), z \u2113 m = Concat(z \u2113 v, z \u2113 a, z \u2113 t ).\nFinally, we define the multimodal fusion loss by combining Eqs.(1), (2), and (7) as:\nLmf = Jsmooth + J grec + J \u2113rec. (8)\n3.4 Graph Contrastive Learning Module"
        },
        {
            "heading": "3.4.1 Graph Construction",
            "text": "Graph construction aims to establish relations between past and future utterances that preserve both intra- and inter-speaker dependencies in a dialogue. We define the i-th dialogue with P speakers as Ci = {US1 , . . . ,USP }, where USi = {uSi1 , . . . ,uSim} represents the set of utterances spoken by speaker Si. Following Ghosal et al. (2019), we define a graph with nodes representing utterances and directed edges representing their relations: Rij = ui \u2192 uj , where the arrow represents the speaking order. Intra-Dependency (Rintra \u2208 {USi \u2192 USi}) represents intra-relations between the utterances (red lines), and Inter-Dependency (Rinter \u2208 {USi \u2192 USj}, i \u0338= j) represents the inter-relations between the utterances (purple lines), as shown in Figure 3. All nodes are initialized by concatenating contextual and specific representations as hm = Concat(z\u0302gm, z\u0302\u2113m). And we show that window size is a hyper-parameter that controls the context information for each utterance and provide accurate intraand inter-speaker dependencies."
        },
        {
            "heading": "3.4.2 Graph Augmentation",
            "text": "Graph Augmentation (GA): Inspired by Zhu et al. (2020), creating two augmented views by using different ways to corrupt the original graph can\nprovide highly heterogeneous contexts for nodes. By maximizing the mutual information between two augmented views, we can improve the robustness of the model and obtain distinguishable node representations (You et al., 2020). However, there are no universally appropriate GA methods for various downstream tasks (Xu et al., 2021), which motivates us to design specific GA strategies for MERC. Considering that MERC is sensitive to initialized representations of utterances, intra-speaker and inter-speaker dependencies, we design three corresponding GA methods:\n- Feature Masking (FM): given the initialized representations of utterances, we randomly select p dimensions of the initialized representations and mask their elements with zero, which is expected to enhance the robustness of JOYFUL to multimodal feature variations;\n- Edge Perturbation (EP): given the graph G, we randomly drop and add p% of intra- and inter-speaker edges, which is expected to enhance the robustness of JOYFUL to local structural variations;\n- Global Proximity (GP): given the graph G, we first use the Katz index (Katz, 1953) to calculate high-order similarity between intra- and inter-speakers, and randomly add p% highorder edges between speakers, which is expected to enhance the robustness of JOYFUL to global structural variations (Examples in Appendix A).\nWe propose a hybrid scheme for generating graph views on both structure and attribute levels to provide diverse node contexts for the contrastive objective. Figure 2 (C) shows that the combination of (FM & EP) and (FM & GP) are adopted to obtain two correlated views."
        },
        {
            "heading": "3.4.3 Graph Contrastive Learning",
            "text": "Graph contrastive learning adopts an L-th layer GCNs as a graph encoder to extract node hidden representations H(1) = {h(1)1 , . . . , h (1) m } and H(2) = {h(2)1 , . . . , h (2) m } for two augmented graphs, where hi is the hidden representation for the i-th node. We follow an iterative neighborhood aggregation (or message passing) scheme to capture the structural information within the nodes\u2019 neighborhood. Formally, the propagation and aggregation\nof the \u2113-th GCN layer is:\na(i, \u2113) = AGG(\u2113) ({h(j, \u2113\u22121)|j \u2208 Ni}) (9) h(i, \u2113) = COM(\u2113) (h(i, \u2113\u22121) \u2295 a(i, \u2113)), (10)\nwhere h(i, \u2113) is the embedding of the i-th node at the \u2113-th layer, h(i, 0) is the initialization of the ith utterance, Ni represents all neighbour nodes of the i-th node, and AGG(\u2113)(\u00b7) and COM(\u2113)(\u00b7) are aggregation and combination of the \u2113-th GCN layer (Hamilton et al., 2017). For convenience, we define hi = h(i,L). After the L-th GCN layer, final node representations of two views are H(1) / H(2).\nIn Figure 2 (C3), we design the intra- and interview graph contrastive losses to learn distinctive node representations. We start with the inter-view contrastiveness, which pulls closer the representations of the same nodes in two augmented views while pushing other nodes away, as depicted by the red and blue dash lines in Figure 2 (C3). Given the definition of our positive and negative pairs as (h\n(1) i ,h (2) i ) + and (h(1)i ,h (2) j ) \u2212, where i \u0338= j, the inter-view loss for the i-th node is formulated as:\nLiinter = \u2212 log exp(sim(h(1)i ,h (2) i ))\nm\u2211 j=1 exp(sim(h(1)i ,h (2) j ))\n, (11)\nwhere sim(\u00b7, \u00b7) denotes the similarity between two vectors, i.e., the cosine similarity in this paper.\nIntra-view contrastiveness regards all nodes except the anchor node as negatives within a particular view (green dash lines in Figure 2 (C3)), as defined (h(1)i ,h (1) j )\n\u2212 where i \u0338= j. The intra-view contrastive loss for the i-th node is defined as:\nLiintra = \u2212 log exp(sim(h(1)i ,h (2) i ))\nm\u2211 j=1 exp(sim(h(1)i ,h (1) j ))\n. (12)\nBy combining the inter- and intra-view contrastive losses of Eqs.(11) and (12), the contrastive objective function Lct is formulated as:\nLct = 1\n2m\nm\u2211\ni=1\n(Liinter + Liintra). (13)"
        },
        {
            "heading": "3.5 Emotion Recognition Classifier",
            "text": "We use cross-entropy loss for classification as:\nLce = \u2212 1\nm\nm\u2211\ni=1\nk\u2211\nj=1\nyji log (y\u0302 j i ), (14)\nwhere k is the number of emotion classes, m is the number of utterances, y\u0302ji is the i-th predicted label, and yji is the i-th ground truth of j-th class.\nAbove all, combining the MF loss of Eq.(8), contrastive loss of Eq.(13), and classification loss of Eq.(14) together, the final objective function is\nLall = \u03b1Lmf + \u03b2Lct + Lce, (15)\nwhere \u03b1 and \u03b2 are the trade-off hyper-parameters. We give our pseudo-code in Appendix F."
        },
        {
            "heading": "4 Experiments and Result Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets and Metrics. In Table 1, IEMOCAP is a conversational dataset where each utterance was labeled with one of the six emotion categories (Anger, Excited, Sadness, Happiness, Frustrated and Neutral). Following COGMEN, two IEMOCAP settings were used for testing, one with four emotions (Anger, Sadness, Happiness and Neutral) and one with all six emotions, where 4-way directly removes the additional two emotion labels (Excited and Frustrated). MOSEI was labeled with six emotion labels (Anger, Disgust, Fear, Happiness, Sadness, and Surprise). For six emotion labels, we conducted two settings: binary classification considers the target emotion as one class and all other emotions as another class, and multilabel classification tags multiple labels for each utterance. MELD was labeled with six universal emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust). We split the datasets into 70%/10%/20% as training/validation/test data, respectively. Following Joshi et al. (2022), we used Accuracy and Weighted F1-score (WF1) as evaluation metrics. Please note that the detailed label distribution of the datasets is given in Appendix I. Implementation Details. We selected the augmentation pairs (FM & EP) and (FM & GP) for two views. We set the augmentation ratio p=20% and smoothing parameter \u03b7=0.2, and applied the Adam (Kingma and Ba, 2015) optimizer with an initial learning rate of 3e-5. For a fair comparison,\nwe followed the default parameter settings of the baselines and repeated all experiments ten times to report the average accuracy. We conducted the significance by t-test with Benjamini-Hochberg (Benjamini and Hochberg, 1995) correction (Please see details in Appendix G). Baselines. Different MERC datasets have different best system results, following COGMEN, we selected SOTA baselines for each dataset. For IEMOCAP-4, we selected Mult (Tsai et al., 2019a), RAVEN (Wang et al., 2019), MTAG (Yang et al., 2021), PMR (Lv et al., 2021), COGMEN and MICA (Liang et al., 2021) as our baselines. For IEMOCAP-6, we selected Mult, FE2E (Dai et al., 2021), DiaRNN (Majumder et al., 2019), COSMIC (Ghosal et al., 2020), AfCAN (Wang et al., 2021), AGHMN (Jiao et al., 2020), COGMEN and RGAT (Ishiwatari et al., 2020) as our baselines. For MELD, we selected DiaGCN (Ghosal et al., 2019), DiaCRN (Hu et al., 2021), MMGCN (Wei et al., 2019), UniMSE (Hu et al., 2022b), COGMEN and MM-DFN (Hu et al., 2022a) as baselines. For MOSEI, we selected MulNet (Shenoy et al., 2020), TBJE (Delbrouck et al., 2020), COGMEN and MR (Tsai et al., 2020)."
        },
        {
            "heading": "4.2 Parameter Sensitive Study",
            "text": "We first examined whether applying different data augmentation methods improves JOYFUL. We observed in Figure 4 (A) that 1) all data augmentation strategies are effective 2) applying augmentation pairs of the same type cannot result in the best performance; and 3) applying augmentation pairs of different types improves performance. Thus, we selected (FM & EP) and (FM & GP) as the default augmentation strategy since they achieved the best performance (More details please see Appendix C).\nJOYFUL has three hyperparameters. \u03b1 and \u03b2 determine the importance of MF and GCL in Eq.(15), and window size controls the contextual length of conversations. In Figure 4 (B), we observed how \u03b1 and \u03b2 affect the performance of JOYFUL by varying \u03b1 from 0.02 to 0.10 in 0.02 intervals and \u03b2 from 0.1 to 0.5 in 0.1 intervals. The results indicated that JOYFUL achieved the best performance when \u03b1 \u2208 [0.06, 0.08] and \u03b2 = 0.3. Figure 4 (C) shows that when window_size = 8, JOYFUL achieved the best performance. A small window size will miss much contextual information, and a longer one contains too much noise, we set it as 8 in experiments (Details in Appendix D)."
        },
        {
            "heading": "4.3 Performance of JOYFUL",
            "text": "Tables 2 & 3 show that JOYFUL outperformed all baselines in terms of accuracy and WF1, improving 5.0% and 1.3% in WF1 for 6-way and 4-way, respectively. Graph-based methods, COGMEN and JOYFUL, outperform Transformers-based methods, Mult and FE2E. Transformers-based methods cannot distinguish intra- and inter-speaker dependencies, distracting their attention to important utterances. Furthermore, they use the cross-modal attention layer, which can enhance common features among modalities while losing uni-modal specific features (Rajan et al., 2022). JOYFUL outperforms other GNN-based methods since it explored features from both the contextual and specific levels, and used GCL to obtain more distinguishable features. However, JOYFUL cannot improve in Happy for 4-way and in Excited for 6-way since samples in IEMOCAP were insufficient for distinguishing these similar emotions (Happy is 1/3 of Neutral in Fig. 4 (D)). Without labels\u2019 guidance to re-sample or re-weight the underrepresented samples, selfsupervised GCL, utilized in JOYFUL, cannot ensure distinguishable representations for samples of minor classes by only exploring graph topological information and vertex attributes.\nTables 4 & 5 show that JOYFUL outperformed\n(B) IEMOCAP (4-way) Classification (D) IEMOCAP (4-way) Error Visualization\nW ei\ngh te\nd F\n1 S\nco re 76 78 80 82 84 86\n0.1 0.2\n0.3 0.4\n0.5 0.02 0.060.04\n0.08 0.10\n1 432 5 6 7 8 9 10 11 0.81\n0.82\n0.83\n0.84\n0.86\n0.85\n(C) IEMOCAP (4-way) Window Size\nPr ed\nic te\nd L\nab el\nTrue Label (A) IEMOCAP (4-way) Classification\nW ei\ngh te\nd F\n1 S\nco re\nW ei\ngh te\nd F\n1 S\nco re\nN um\nbe r\nof sa\nm pl\nes\nFigure 4: (A) WF1 gain with different augmentation pairs; (B\u223cC) Parameter tuning; (D) Imbalanced dataset.\nthe baselines in more complex scenes with multiple speakers or various emotional labels. Compared with COGMEN and MM-DFN, which directly aggregate multimodal features, JOYFUL can fully explore features from each uni-modality by specific representation learning to improve the performance. The GCL module can better aggregate similar emotional features for utterances to obtain better performance for multi-label classification. We cannot improve in Happy on MOSEI since the samples are imbalanced and Happy has only 1/6 of Surprise, making JOYFUL hard to identify it.\nTo verify the performance gain from each component, we conducted additional ablation studies. Table 6 shows multi-modalities can greatly improve JOYFUL\u2019s performance compared with each single modality. GCL and each component of MF can\nseparately improve the performance of JOYFUL, showing their effectiveness (Visualization in Appendix H). JOYFUL w/o GCL and COGMEN w/o GNN utilize only a multimodal fusion mechanism for classification without additional modules for optimizing node representations. The comparison between them demonstrates the effectiveness of the multimodal fusion mechanism in JOYFUL.\nMethod One-Layer (WF1) Two-Layer (WF1) Four-Layer (WF1)\nWe deepened the GNN layers to verify JOYFUL\u2019s ability to alleviate the over-smoothing. In Table 7, COGMEN with four-layer GNN was 9.24% lower than that with one-layer, demonstrating that the over-smoothing decreases performance, while JOYFUL relieved this issue by using the GCL framework. To verify the robustness, following Tan et al. (2022), we randomly added 5%\u223c20% noisy edges to the training data. In Table 7, COGMEN was\n(A) Initialized Features (B) Output Features"
        },
        {
            "heading": "So what, I'm not fast with women. (Frustrated)",
            "text": "easily affected by the noise, decreasing 10.8% performance in average with 20% noisy edges, while JOYFUL had strong robustness with only an average 2.8% performance reduction for 20% noisy edges.\nTo show the distinguishability of the node representations, we visualize the node representations of FE2E, COGMEN, and JOYFUL on 6-way IEMOCAP. In Figure 5, COGMEN and JOYFUL obtained more distinguishable node representations than FE2E, demonstrating that graph structure is more suitable for MERC than Transformers. JOYFUL performed better than COGMEN, illustrating the effectiveness of GCL. In Figure 6, we randomly sampled one example from each emotion of IEMOCAP (6-way) and chose best-performing COGMEN for comparison. JOYFUL obtained more discriminate prediction scores among emotion classes, showing GCL can push samples from different emotion class farther apart."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed a joint learning model (JOYFUL) for MERC, that involves a new multimodal fusion mechanism and GCL module to effectively improve the performance of MERC. The MR mechanism can extract and fuse contextual and uni-modal specific emotion features, and the GCL module can help learn more distinguishable representations.\nFor future work, we plan to investigate the performance of using supervised GCL for JOYFUL on unbalanced and small-scale emotional datasets."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank Ying Zhang 1 for her advice and assistance. We gratefully acknowledge anonymous reviewers for their helpful comments and feedback. We also acknowledge the authors of COGMEN (Joshi et al., 2022): Abhinav Joshi and Ashutosh Modi for sharing codes and datasets. Finally, Dongyuan Li acknowledges the support of the China Scholarship Council (CSC).\nLimitations\nJOYFUL has a limited ability to classify minority classes with fewer samples in unbalanced datasets. Although we utilized self-supervised graph contrastive learning to learn a distinguishable representation for each utterance by exploring vertex attributes, graph structure, and contextual information, GCL failed to separate classes with fewer samples from the ones with more samples because the utilized self-supervised learning lacks the label information and does not balance the label distribution. Another limitation of JOYFUL is that its framework was designed specifically for multimodal emotion recognition tasks, which is not straightforward and general as language models (Devlin et al., 2019; Liu et al., 2019) or image processing techniques (LeCun et al., 1995). This setting may limit the applications of JOYFUL for other multimodal tasks, such as the multimodal sentiment analysis task (Detailed experiments in Appendix J) and the multimodal retrieval task. Finally, although JOYFUL achieved SOTA performances on three widely-used MERC benchmark datasets, its performance on larger-scale and more heterogeneous data in real-world scenarios is still unclear.\n1scholar.google.com/citations?user=tbDNsHs"
        },
        {
            "heading": "A Example for Global Proximity",
            "text": "In Figure 7, given the network G and a modified p, we first used the Katz index (Katz, 1953) to calculate a high-order similarity between the vertices. We considered the arbitrary number of high-order distances. For example, second-order similarity between uA1 and u B 4 as u A 1 \u2192 uB4 = 0.83, third-order similarity between uA1 and u B 5 as u A 1 \u2192 uB5 = 0.63, and fourth-order similarity between uA1 and uB7 as u A 1 \u2192 uB7 = 0.21. We then define the threshold score as 0.5, where a high-order similarity score less than the threshold will not be selected as added edges. Finally, we randomly selected p% edges (whose scores are higher than the threshold score) and added them to the original graph G to construct the augmented graph."
        },
        {
            "heading": "B Dimensions of Mathematical Symbols",
            "text": "Since we do not have much space to introduce details about the dimensions of the mathematical symbols in our main body. We carefully list all the dimensions of the mathematical symbols of IEMOCAP in Table 8. Mathematical symbols for other two datasets please see our source code."
        },
        {
            "heading": "C Observations of Graph Augmentation",
            "text": "As shown in Figure 8, when we consider the combinations of (FM & EP) and (FP & GP) as two graph augmentation methods of the original graph, we could achieve the best performance. Furthermore, we have the following observations:\nObs.1: Graph augmentations are crucial. Without any data augmentation, GCL module will not improve\naccuracy, judging from the averaged WF1 gain of the pair (None, None) in the upper left corners of Figure 8. In contrast, composing an original graph and its appropriate augmentation can benefit the averaged WF1 of emotion recognition, judging from the pairs (None, any) in the top rows or the left-most columns of Figure 8. Similar observation were in graphCL (You et al., 2020), without augmentation, GCL simply compares two original samples as a negative pair with the positive pair loss becoming zero, which leads to homogeneously pushes all graph representations away from each other. Appropriate augmentations can enforce the model to learn representations invariant to the desired perturbations through maximizing the agreement between a graph and its augmentation.\nObs.2: Composing different augmentations benefits the model\u2019s performance more. Applying augmentation pairs of the same type does not often result in the best performance (see diagonals in Figure 8). In contrast, applying augmentation pairs of different types result in better performance gain (see offdiagonals of Figure 8). Similar observations were in SimCSE (Gao et al., 2021). As mentioned in that study, composing augmentation pairs of different types correspond to a \u201charder\u201d contrastive\nprediction task, which could enable learning more generalizable representations.\nObs.3: One view having two augmentations result in better performance. Generating each view by two augmentations further improve performance, i.e., the augmentations FM & EP, or FM & GP. The augmentation pair (FM & EP, FM & GP) results in the largest performance gain compared with other augmentation pairs. We conjectured the reason is that simultaneously changing structural and attribute information of the original graph can obtain more heterogeneous contextual information for nodes, which can be consider as \u201charder\u201d example to prompt the GCL model to obtain more generalizable and robust representations."
        },
        {
            "heading": "D Parameters Sensitivity Study",
            "text": "In this section, we give more details about parameter sensitivity. First, as shown in Tables 9 & 10, when the window size \u2208 [6, 8] for IEMOCAP (6- way) and the window size is 6 for IEMOCAP (4- way), JOYFUL achieved the best performance. A small window size will miss much contextual information, and a large-scale window size contains too much noise (topic will change over time). We set the window size for past and future to 6.\nJOYFUL also has two hyper-parameters: \u03b1 and \u03b2, which balance the importance of MF module and GCL module in Eq.(15). Specifically, as shown in Figure 9, we observed how \u03b1 and \u03b2 affect the performance of JOYFUL by varying \u03b1 from 0.02 to 0.10 in 0.02 intervals and \u03b2 from 0.1 to 0.5 in 0.1 intervals. The results indicate that JOYFUL achieved the best performance when \u03b1 \u2208 [0.06, 0.08] and \u03b2 \u2208 [0.2, 0.3] on IEMOCAP and and when \u03b1 \u2208 [0.06, 0.1] and \u03b2 = 0.1 on MOSEI. The reason why these parameters can affect the results is that when \u03b1< 0.06, MF becomes weaker and representations contain too much noise, which cannot provide a good initialization for downstream MERC tasks. When \u03b1 >0.1, it tends to make reconstruction loss more important and JOYFUL tends to extract more common features among multiple modalities and loses attention to explore features from uni-modality. When \u03b2 is small, graph contrastive loss becomes weaker, which leads to indistinguishable representation. A larger \u03b2 wakes the effect of MF, leading to a local optimal solution. We set \u03b1=0.06 and \u03b2=0.3 for IEMOCAP and MELD. We set \u03b1=0.06 and \u03b2 =0.1 for MOSEI."
        },
        {
            "heading": "E Uni-modal Performance",
            "text": "The focus of this study was multimodal emotion recognition. However, we also compared JOYFUL with uni-modal methods to evaluate its performance of JOYFUL. We compared it with\nDAG-ERC (Shen et al., 2021b), CESTa (Wang et al., 2020), SumAggGIN (Sheng et al., 2020), DiaCRN (Hu et al., 2021), DialogXL (Shen et al., 2021a), DiaGCN (Ghosal et al., 2019), and COGMEN (Joshi et al., 2022). Following COGMEN, text-based models were specifically optimized for text modalities and incorporated changes to architectures to cater to text. As shown in Table 11, JOYFUL, being a fairly generic architecture, still achieved better or comparable performance with respect to the state-of-the-art uni-modal methods. Adding more information via other modalities helped to further improve the performance of JOYFUL (Text vs A+T+V). When using only text modality, the DAG-ERC baseline could achieve higher WF1 than JOYFUL. And we conjecture the main reasons is: DAG-ERC (Shen et al., 2021b) fine-tuned RoBERTa large model (Liu et al., 2019), with 354 million parameters, as their text encoder. By fine-tuning on RoBERTa large model under the guidance of downstream emotion recognition signals, RoBERTa large model can provide the most suitable text features for ERC. Compared with DAG-ERC, JOYFUL and other methods directly use Sentence-BERT (Reimers and Gurevych,\n2019), with 110 million parameters, as their text encoder without fine-tuning on ERC datasets.\nTo verify whether the above inference is reasonable, we used RoBERTa large model as our text feature extractor called Text (RoBERTa-large). And we fine-tuned RoBERTa large model on the downstream IEMPCAP (6-way) dataset, following the same method of DAG-ERC called Fine-tune Text (RoBERTA-large). The observation meets our intuition. With RoBERTa large model, JOYFUL improved the performance (68.05 vs 67.48) compared with Sentence-BERT as our text encoder. And JOYFUL could obtain better performance (68.45 vs 68.03) in terms of WF1 than DAG-ERC with fine-tuned RoBERTa-large, demonstrating that finetuning large-scale model can help obtain richer text features to improve the performance. However, considering a fair comparison with other multimodal emotion recognition baselines (they do not have the fine-tuning process (Joshi et al., 2022; Ghosal et al., 2019)) and saving the additional time-consuming on fine-tuning, we directly adopt Sentence-BERT as our text encoder for IEMOCAP."
        },
        {
            "heading": "F Pseudo-Code of JOYFUL",
            "text": "As shown in Algorithm 1, to make JOYFUL easy to understand, we also provide a pseudo-code."
        },
        {
            "heading": "G Benjamini-Hochberg Correction",
            "text": "Benjamini-Hochberg Correction (B-H) (Benjamini and Hochberg, 1995) is a powerful tool that decreases the false discovery rate. Considering the reproducibility of the multiple significant test, we introduce how we adopt the B-H correction and give the hyper-parameter values that we used. We first conduct a t-test (Yang et al., 1999) with default parameters2 to calculate the p-value between each comparison method with JOYFUL. We then put the individual p-values in ascending order as input to\n2scipy.stats.ttest_ind.html\nAlgorithm 1: Overall process of JOYFUL input :Visual features xv;\nAudio features xa; Text features xt; Parameters: \u03b1, \u03b2, Window size\noutput :Emotion recognition label. Initialize trainable parameters; for epoch\u2190 1 to epoch num do\nGlobal Contextual Fusion z\u0302gm; Specific Modality Fusion z\u0302\u2113m=(zgv\u2225zga\u2225zgt ); // Compute multimodal fusion loss Compute Lmf , in accordance with Eq.(8); Feature Concatenation h = (z\u0302gm\u2225z\u0302\u2113m); Adopt h as initialization for Graph; // Generate two augmented views Apply FM & EP to generate view: G(1); Apply FM & GP to generate view: G(2); // Extract features of two views H(1) = GCNs(G(1)), H(2) = GCNs(G(2)) ; // Compute contrastive learning loss Compute Lct, in accordance with Eq.(13) ; // Aggregate extracted features H = H(1) +H(2) ; // Compute emotion recognition loss Compute Lce, in accordance with Eq.(14); // Joint training Compute Lall, in accordance with Eq.(15); // Optimize with Adam optimizer\nAdopt classifier on H to predict the emotional label.\ncalculate the p-value corrected using the B-H correction. We directly use the \u201cmultipletests(*args)\u201d function from python package3 and set the hyperparameter of the false discovery rate Q = 0.05, which is a widely used default value (Puoliv\u00e4li et al., 2020). Finally, we obtain a cut-off value as the output of the multipletests function, where cut-off is a dividing line that distinguishes whether two groups of data are significant. If the p-value is smaller than the cut-off value, we can conclude that two groups of data are significantly different.\nThe use of t-test for testing statistical significance may not be appropriate for F-scores, as mentioned in Dror et al. (2018), as we cannot assume normality. To verify whether our data meet the normality assumption and the homogeneity of variances required for the t-test, following Shapiro and Wilk (1965) and Levene et al. (1960), we conducted the following validation. First, we performed the Shapiro-Wilk test on each group of experimental results to determine whether they are normally distributed. Under the constraint of a significance level (alpha=0.05), all p-values resulting from the Shapiro-Wilk test 4 for the baselines and our model\n3statsmodels.stats.multitest.multipletests.html 4scipy.stats.shapiro.html\nwere greater than 0.05. This indicates that the results of the baselines and our model all adhere to the assumption of normality. For example, in IEMOCAP-4, p-values for [Mult, RAVEN, MTAG, PMR, MICA, COGMEN, JOYFUL] are [0.903, 0.957, 0.858, 0.978, 0.970, 0.969, 0.862]. Furthermore, we used the Levene\u2019s test (Schultz, 1985) to check for homogeneity of variances between baselines and our model. Under the constraint of a significance level (alpha = 0.05), we found that our p-values are greater than 0.05, indicating the homogeneity of the variances between the baselines and our model. For example, we obtained p-values 0.3101 and 0.3848 for group-based baselines on IEMOCAP-4 and IEMOCAP-6, respectively. Since we were able to demonstrate that all baselines and our model conform to the assumptions of normality and homogeneity of variances, we believe that the significance tests we reported are accurate."
        },
        {
            "heading": "H Representation Visualization",
            "text": "We visualized the node features to understand the function of the multimodal fusion mechanism and the GCL-based node representation learning component, as shown in Figure 10. Figure 10 (A) shows the concatenated multimodal features on the input side. Figure 10 (B) shows the representation of utterances after the feature fusion module. Figure 10 (C) shows the representation of the utterances after the GCL module (Eq.(10)) and before the pre-softmax layer (Eq.(11)). We observed that utterances could be roughly separated after the feature fusion mechanism, which indicates that the multimodal fusion mechanism can learn distinctive features to a certain extent. After GCL-based module, JOYFUL can be easily separated, demonstrating that GCL can provide distinguishable representation by exploring vertex attributes, graph structure, and contextual information from datasets."
        },
        {
            "heading": "I Labels Distribution of Datasets",
            "text": "In this section, we list the detailed label distribution of the three multimodal emotion recognition datasets MELD (Table 12), IEMOCAP 4-way (Table 13), IEMOCAP 6-way (Table 14) and MOSEI (Table 15) in the draft."
        },
        {
            "heading": "J Multimodal Sentiment Analysis",
            "text": "We conducted experiments on two publicly available datasets, MOSI (Zadeh et al., 2016) and MO-\nSEI (Zadeh et al., 2018), to investigate the performance of JOYFUL on the multimodal sentiment analysis (MSA) task. ) Datasets: MOSI contains 2,199 utterance video segments, and each segment is manually annotated with a sentiment score ranging from -3 to +3 to indicate the sentiment polarity and relative sentiment strength of the segment. MOSEI contains 22,856 movie review clips from the YouTube website. Each clip is annotated with a sentiment score and an emotion label. And the exact number of samples for training/validation/test are 1,284/229/686 for MOSI and 16,326/1,871/4,659 for MOSEI. ) Metrics: Following previous studies (Han et al., 2021a; Yu et al., 2021), we utilized evaluation metrics: mean absolute error (MAE) measures the absolute error between predicted and true values. Person correlation (Corr) measures the degree of prediction skew. Seven-class classification accuracy (ACC-7) indicates the proportion of predictions that correctly fall into the same interval of seven\nintervals between -3 and +3 as the corresponding truths. And binary classification accuracy (ACC-2) was computed for non-negative/negative classification results. ) Baselines: We compared JOYFUL with three types of advanced multimodal fusion frameworks for the MSA task as follows, including current SOTA baselines MMIM (Han et al., 2021b) and BBFN (Han et al., 2021a): (1) Early multimodal fusion methods, which combine the different modalities before they are processed by any neural network models. We utilized Multimodal Factorization Model (MFM) (Tsai et al., 2019b), and Multimodal Adaptation Gate BERT (MAGBERT) (Rahman et al., 2020) as baselines. (2) Late multimodal fusion methods, which combine the different modalities before the final decision or prediction layer. We utilized multimodal Transformer (MuIT) (Tsai et al., 2019a), and modaltemporal attention graph (MTAG) (Yang et al., 2021) as baselines. (3) Hybrid multimodal fusion\nmethods combine early and late multimodal fusion mechanisms to capture the consistency and the difference between different modalities simultaneously. We utilized modality-invariant and modalityspecific representations for MSA (MISA) (Hazarika et al., 2020), Self-Supervised multi-task learning for MSA (Self-MM) (Yu et al., 2021), BiBimodal Fusion Network (BBFN) (Han et al., 2021a), and MultiModal InfoMax (MMIM) (Han et al., 2021b) as baselines. ) Implementation Details: The results of proposed JOYFUL were averaged over ten runs using random seeds. We keep all hyper-parameters and implementations the same as in the MERC task reported in Sections 4.1 and 4.2. To make JOYFUL fit in the MSA task, we replace the current crossentropy loss Lce in Eq. (15) by mean absolute error loss Lmae as follows:\nLmae = 1 m\nm\u2211\ni=1\n|y\u0302i \u2212 yi|, (16)\nwhere y\u0302i is the predicted value for the i-th sample, yi is the truth label for the i-th label, m is the total number of samples, and | \u00b7 | is the L1 norm. We denote this model as JOYFUL+MAE.\nExperimental results on the MOSI and MOSEI datasets are listed in Table 17. Although the proposed JOYFUL could outperform most of the baselines (above the blue line), it performs worse than current SOTA models: BBFN and MMIM (below the blue line). We conjecture the main reasons are: when determining the strength of sentiments, compared with visual and acoustic modalities that may contain much noise data, text modality is more important for prediction (Han et al., 2021a). Table 16 lists such examples, where textual modality is more indicative than other modalities for the MSA task. Because the two baselines: BBFN (Han et al., 2021a) and MMIN (Han et al., 2021b), pay\nMethod MOSI MOSEI\nMAE \u2193Corr \u2191Acc-7 \u2191Acc-2 \u2191 MAE \u2193Corr \u2191Acc-7 \u2191Acc-2 \u2191\nMFM 0.877 0.706 35.4 81.7 0.568 0.717 51.3 84.4 MAG-BERT 0.731 0.789 \u2717 84.3 0.539 0.753 \u2717 85.2 MulT 0.861 0.711 \u2717 84.1 0.580 0.703 \u2717 82.5 MTAG 0.866 0.722 0.389 82.3 \u2717 \u2717 \u2717 \u2717 MISA 0.804 0.764 \u2717 82.10 0.568 0.724 \u2717 84.2 Self-MM 0.713 0.789 \u2717 85.98 0.530 0.765 \u2717 85.17\nBBFN 0.776 0.755 45.00 84.30 0.529 0.767 54.80 86.20 MMIM 0.700 0.800 46.65 86.06 0.526 0.772 54.24 85.97\nJOYFUL+MAE 0.711 0.792 45.58 85.87 0.529 0.768 53.94 85.68\nTable 17: Experimental results on the MOSI and MOSEI datasets. \u2717 indicates unreported results. Bold indicates the least MAE, highest Corr, Acc-7, and Acc-2 scores for each dataset.\nmore attention to the text modality than visual and acoustic modalities during multimodal feature fusion, they may achieve low MAE, high Corr, Acc-2, and Acc-7. Specifically, BBFN (Han et al., 2021a) proposed a Bi-bimodal fusion network to enhance the text modality\u2019s importance by only considered text-visual and text-acoustic interaction for features fusion. Conversely, considering the three modalities are all important for the MERC task as presented in Table 16, we designed JOYFUL to utilize the concatenation of the three modalities representations for prediction. Similar to our proposal, MISA and MAG-BERT considered the three modalities equally important during feature fusion but performed worse than SOTA baselines on the MSA task. In our consideration, because of such attention to modalities, JOYFUL outperformed SOTA baselines on the MERC task but underperformed SOTA baselines on the MSA task."
        }
    ],
    "title": "Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition",
    "year": 2023
}