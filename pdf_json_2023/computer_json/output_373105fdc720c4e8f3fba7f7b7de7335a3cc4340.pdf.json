{
    "abstractText": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2\u00d7. NarrowBERT sparsifies the transformer model such that the selfattention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5\u00d7 with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoxin Li"
        },
        {
            "affiliations": [],
            "name": "Phillip Keung"
        },
        {
            "affiliations": [],
            "name": "Daniel Cheng"
        },
        {
            "affiliations": [],
            "name": "Jungo Kasai"
        },
        {
            "affiliations": [],
            "name": "Noah A. Smith"
        },
        {
            "affiliations": [],
            "name": "Paul G. Allen"
        }
    ],
    "id": "SP:059abb0179b1716fcb73b42200761bb5c0b2fd17",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "Proc. of ICLR.",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055.",
            "year": 2017
        },
        {
            "authors": [
                "Krzysztof Choromanski",
                "Valerii Likhosherstov",
                "David Dohan",
                "Xingyou Song",
                "Andreea Gane",
                "Tam\u00e1s Sarl\u00f3s",
                "Peter Hawkins",
                "Jared Davis",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "David Belanger",
                "Lucy Colwell",
                "Adrian Weller"
            ],
            "title": "Rethinking attention with Per",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: Pretraining text encoders as discriminators rather than generators",
            "venue": "Proc. of ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Zihang Dai",
                "Guokun Lai",
                "Yiming Yang",
                "Quoc Le."
            ],
            "title": "Funnel-transformer: Filtering out sequential redundancy for efficient language processing",
            "venue": "Proc. of NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proc. of NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTa: decoding-enhanced bert with disentangled attention",
            "venue": "Proc. of ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Peter Izsak",
                "Moshe Berchansky",
                "Omer Levy."
            ],
            "title": "How to train BERT with an academic budget",
            "venue": "Proc. o EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret."
            ],
            "title": "Transformers are RNNs: Fast autoregressive transformers with linear attention",
            "venue": "Proc. of ICML.",
            "year": 2020
        },
        {
            "authors": [
                "Phillip Keung",
                "Yichao Lu",
                "Gy\u00f6rgy Szarvas",
                "Noah A Smith."
            ],
            "title": "The multilingual amazon reviews corpus",
            "venue": "arXiv preprint arXiv:2010.02573.",
            "year": 2020
        },
        {
            "authors": [
                "Cheolhyoung Lee",
                "Kyunghyun Cho",
                "Wanmo Kang."
            ],
            "title": "Mixout: Effective regularization to finetune large-scale pretrained language models",
            "venue": "arXiv preprint arXiv:1909.11299.",
            "year": 2019
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Proc. of KR.",
            "year": 2012
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke S. Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Marius Mosbach",
                "Maksym Andriushchenko",
                "Dietrich Klakow."
            ],
            "title": "On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines",
            "venue": "arXiv preprint arXiv:2006.04884.",
            "year": 2020
        },
        {
            "authors": [
                "Hao Peng",
                "Jungo Kasai",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Zhaofeng Wu",
                "Lingpeng Kong",
                "Roy Schwartz",
                "Noah A. Smith."
            ],
            "title": "ABC: Attention with bounded-memory control",
            "venue": "Proc. of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Peng",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Roy Schwartz",
                "Noah A. Smith",
                "Lingpeng Kong."
            ],
            "title": "Random feature attention",
            "venue": "Proc. of ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Omer Levy."
            ],
            "title": "Improving transformer models by reordering their sublayers",
            "venue": "Proc. of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proc. of EMNLP.",
            "year": 2016
        },
        {
            "authors": [
                "Lakshay Sharma",
                "Laura Graesser",
                "Nikita Nangia",
                "Utku Evci"
            ],
            "title": "Natural language understanding with the quora question pairs dataset",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proc. of EMNLP.",
            "year": 2013
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proc. of CoNLL.",
            "year": 2003
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proc. of NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Proc. of NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proc. of BlackboxNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z. Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "year": 2020
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proc. of NAACL.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretrained masked language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and DeBERTa (He et al., 2021), have pushed the state-of-the-art on a wide range of downstream tasks in natural language processing. At their core is the transformer architecture (Vaswani et al., 2017) that consists of interleaved self-attention and feedforward sublayers. Since the former sublayer implies quadratic time complexity in the input sequence length (Vaswani et al., 2017), many have proposed methods to make the self-attention computation more efficient (Katharopoulos et al., 2020; Choromanski et al., 2021; Wang et al., 2020; Peng et al., 2021, 2022, inter alia).\nIn this work, we explore an orthogonal approach to efficiency: can we make masked language models efficient by reducing the length of the input sequence that each layer needs to process? In particu-\nlar, pretraining by masked language modeling only involves prediction of masked tokens (typically, only 15% of the input tokens; Devlin et al., 2019; Liu et al., 2019). Despite this sparse pretraining objective, each transformer layer computes a representation for every token. In addition to pretraining, many downstream applications only use a single vector representation (i.e., only the [CLS] token) for prediction purposes, which is much smaller than the number of input tokens (e.g., sequence classification tasks as in GLUE/SuperGLUE; Wang et al., 2018, 2019). By narrowing the input sequence for transformer layers, we can accelerate both pretraining and inference.\nWe present NarrowBERT, a new architecture that takes advantage of the sparsity in the training objective. We present two NarrowBERT methods in the sections that follow (Figure 1). We provide the code to reproduce our experiments at https:// github.com/lihaoxin2020/narrowbert. The first method reduces the input sequence for the feedforward sublayers by reordering the interleaved self-attention and feedforward sublayers in the standard transformer architecture (Press et al., 2020): after two standard, interleaved transformer layers, self-attention sublayers are first applied, followed only by feedforward sublayers. This way, the feedforward sublayer computations are only performed for masked tokens, resulting in a 1.3\u00d7 speedup in pretraining (\u00a73). The second approach reduces the input length to the attention sublayers: queries are only computed for masked tokens in the attention mechanism (Bahdanau et al., 2015), while the keys and values are not recomputed for non-masked tokens, which leads to a greater than 2\u00d7 speedup in pretraining.\nWe extensively evaluate our efficient pretrained models on well-established downstream tasks (e.g., Wang et al., 2018; Tjong Kim Sang and De Meulder, 2003.) We find that our modifications result in almost no drop in downstream performance,\nar X\niv :2\n30 1.\n04 76\n1v 2\n[ cs\n.C L\n] 5\nJ un\n2 02\n3\n(a) {6,sf} model: standard BERT with the transformer encoder, trained on MLM loss.\n(b) sf{5,s}:{5,f} ContextFirst model: Transformer encoder with re-ordered layers. Attentional contextualization is performed all-at-once near the beginning of the model.\n(c) sf:{5,sf} SparseQueries model: Transformer encoder with sparsified queries. Contextualization is focused on [MASK] tokens only. (See Fig. 2.)\nFigure 1: Examples of standard BERT and NarrowBERT variations. NarrowBERT takes advantage of the sparsity in the masking (i.e., only 15% of tokens need to be predicted) to reduce the amount of computation in the transformer encoder.\nwhile providing substantial pretraining and inference speedups (\u00a73). While efficient attention variants are a promising research direction, this work presents a different and simple approach to making transformers efficient, with minimal changes in architecture."
        },
        {
            "heading": "2 NarrowBERT",
            "text": "In Figures 1b and 1c, we illustrate two variations of NarrowBERT. We define some notation to describe the configuration of our models. s refers to a single self-attention layer and f refers to a single feedforward layer. The colon : refers to the \u2018narrowing\u2019 operation, which gathers the masked positions from the output of the previous layer.\nThe first variation (\u2018ContextFirst\u2019 in Fig. 1b) uses attention to contextualize all-at-once at the beginning of the model. In short, the transformer layers have been rearranged to frontload the attention components. The example given in the figure specifies the model as sf{5,s}:{5,f}, which means that the input sentence is encoded by a selfattention layer, a feedforward layer, and 5 consecutive self-attention layers. At that point, the masked positions from the encoded sentence are gathered into a tensor and passed through 5 feedforward lay-\ners, thereby avoiding further computations for all unmasked tokens. Finally, the masked positions are unmasked and the MLM loss is computed.\nThe second variation (\u2018SparseQueries\u2019 in Fig. 1c) does not reorder the layers at all. Instead, the sf:{5,sf} model contextualizes the input sentence in a more limited way. As shown in Figure 2, the input sentence is first contextualized by a s and a f layer, but the non-masked tokens are never contextualized again afterwards. Only the masked tokens are contextualized by the remaining {5,sf} layers.\nSince the masked tokens are only about 15% of the total sentence length, the potential speedup is ~6.6\u00d7 for every feedforward or attention layer downstream of a narrowing : operation. The memory usage can also decrease by ~6.6\u00d7 for those layers since the sequence length has decreased, which allows us to use larger batch sizes during training.\nFor GLUE, Amazon, and IMDB text classification tasks, only the [CLS] token is used for prediction. When we finetune or predict with ContextFirst on a GLUE/Amazon/IMDB task, the feedforward layers only need to operate on the [CLS] token. When we finetune or predict with SparseQueries, only the [CLS] token is used in the queries of the\nattention layers. Everything after the narrowing : operation only operates on the [CLS] token, which dramatically speeds up the NarrowBERT variants."
        },
        {
            "heading": "3 Experiments",
            "text": "We focus on 2 models in our experiments: ContextFirst (sfsf{10,s}:{10,f}) and SparseQueries ({1,sf}:{11,sf}, \u00b7 \u00b7 \u00b7 , {4,sf}:{8,sf}). Our NarrowBERT models all contain 12 selfattention and 12 feedforward layers in total, with the narrowing operation used at different points in the model. We compare NarrowBERT with the baseline BERT model and the Funnel Transformer model (Dai et al., 2020), which is a pretrained encoder-decoder transformer model where the encoder goes through a sequence of length bottlenecks.\nIn our experiments, we use 15% masking in masked language model (MLM) training. Following Liu et al. (2019), we do not use next sentence prediction as a pretraining task. We use large batch sizes and high learning rates to fully utilize GPU memory, as suggested in Izsak et al. (2021). Batches are sized to be the largest that fit in GPU\nmemory. We use a learning rate of 0.0005. Models are trained for 70k steps, where each step contains 1728 sequences of 512 tokens, and gradient accumulation is used to accumulate the minibatches needed per step. Models were trained on hosts with 8 Nvidia A100 GPUs. We used the Hugging Face implementations of the baseline BERT and Funnel Transformer models. We pretrained the baseline BERT, Funnel Transformer, and NarrowBERT models using the same Wikipedia and Books corpora and total number of steps.\nIn Figure 3, we see the evolution of the development MLM loss over the course of model training. The BERT and NarrowBERT models all converge to similar values, with the NarrowBERT models reaching a slightly higher MLM loss near the end of training.\nWe report the accuracy for MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), SST2 (Socher et al., 2013), WNLI (Levesque et al., 2012), IMDB (Maas et al., 2011), and English Amazon reviews (Keung et al., 2020), F1 for QQP (Sharma et al., 2019) and CoNLL-2003 NER (Tjong Kim Sang and De Meulder, 2003), and\nSpearman correlation for STS-B (Cer et al., 2017). For the Amazon reviews corpus, we consider both the usual 5-star prediction task and the binarized (i.e., 1\u20132 stars versus 4\u20135 stars) task.\nIn Table 1, we present the results for our extrinsic evaluation on various GLUE tasks. The reduction in performance is small or non-existent, and on WNLI, the NarrowBERT variations perform better than the baseline. For SparseQueries, it is clear that using more layers prior to the narrowing operation improves performance, though the training and inference speedups become smaller. We note that the Funnel Transformer implementation in Pytorch is slower than the baseline BERT model; this may be due to the fact that the original implementation was written in Tensorflow and optimized for Google TPUs.1\nIt is well known that the variability in the performance of BERT on certain GLUE tasks is extreme (Mosbach et al., 2020; Dodge et al., 2020; Lee et al., 2019), where the differences in performance between finetuning runs can exceed 20% (absolute). We have also observed this extreme variability in the course of our own GLUE finetuning experiments. While many techniques have been proposed to address this issue, it is not the\n1Dai et al. (2020) claim to achieve finetuning FLOPs 0.58\u00d7 the BERT baseline\u2019s. See https://github.com/ laiguokun/Funnel-Transformer.\ngoal of this work to apply finetuning stabilization methods to maximize BERT\u2019s performance. For this reason, we have excluded the RTE, MRPC, and COLA tasks (which are high-variance tasks studied in the aforementioned papers) from our evaluation.\nIn Table 2, we provide results on the IMDB and Amazon reviews classification tasks and the CoNLL NER task. Generally, NarrowBERT is close to the baseline in performance, and the SparseQueries performance improves as more layers are used before the narrowing operation."
        },
        {
            "heading": "4 Discussion and Conclusion",
            "text": "We have explored two straightforward ways of exploiting the sparsity in the masked language model loss computations: rearranging the layers of the transformer encoder to allow the feedforward components to avoid computations on the non-masked positions, and sparsifying the queries in the attention mechanism to only contextualize the masked positions. The NarrowBERT variants can speed up training by a factor of ~2\u00d7 and inference by a factor of ~3\u00d7, while maintaining very similar performance on GLUE, IMDB, Amazon, and CoNLL NER tasks. Based on the favorable trade-off between speed and performance seen in Section 3, we recommend that practitioners consider using the SparseQueries NarrowBERT model with 2 or 3 layers before narrowing.\nLimitations\nDue to our budget constraint, we only performed pretraining and downstream experiments with basesized transformer models. We also only applied the masked language modeling objective, but there are other effective pretraining objectives (e.g., Clark et al., 2020). Nonetheless, since we introduced minimal changes in architecture, we hope that subsequent work will benefit from our narrowing operations and conduct a wider range of pretraining and downstream experiments. While pretrained models can be applied to even more downstream tasks, we designed a reasonable task suite in this work, consisting of both GLUE sentence classification and the CoNLL NER sequential classification tasks."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors thank the anonymous reviewers and Ofir Press at the University of Washington for helpful feedback. This research was supported in part by NSF grant 2113530."
        }
    ],
    "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
    "year": 2023
}