{
    "abstractText": "Medical image analysis using computer-based algorithms has attracted considerable attention from the research community and achieved tremendous progress in the last decade. With recent advances in computing resources and availability of large-scale medical image datasets, many deep learning models have been developed for disease diagnosis from medical images. However, existing techniques focus on sub-tasks, e.g., disease classification and identification, individually, while there is a lack of a unified framework enabling multi-task diagnosis. Inspired by the capability of Vision Transformers in both local and global representation learning, we propose in this paper a new method, namely Multi-task Vision Transformer (MVC) for simultaneously classifying chest X-ray images and identifying affected regions from the input data. Our method is built upon the Vision Transformer but extends its learning capability in a multi-task setting. We evaluated our proposed method and compared it with existing baselines on a benchmark dataset of COVID-19 chest X-ray images. Experimental results verified the superiority of the proposed method over the baselines on both the image classification and affected region identification tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Huyen Tran"
        },
        {
            "affiliations": [],
            "name": "Duc Thanh Nguyen"
        },
        {
            "affiliations": [],
            "name": "John Yearwood"
        }
    ],
    "id": "SP:cb0e961500875a3bf69d01fc5ba60640ae8603fc",
    "references": [
        {
            "authors": [
                "G. Litjens",
                "T. Kooi",
                "B.E. Bejnordi",
                "A.A.A. Setio",
                "F. Ciompi",
                "M. Ghafoorian",
                "J.A. Van Der Laak",
                "B. Van Ginneken",
                "C.I. S\u00e1nchez"
            ],
            "title": "A survey on deep learning in medical image analysis",
            "venue": "Medical Image Analysis, vol. 42, pp. 60\u201388, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E. \u00c7all\u0131",
                "E. Sogancioglu",
                "B. van Ginneken",
                "K.G. van Leeuwen",
                "K. Murphy"
            ],
            "title": "Deep learning for chest X-ray analysis: A survey",
            "venue": "Medical Image Analysis, vol. 72, p. 102125, 2021. 10",
            "year": 2021
        },
        {
            "authors": [
                "A. Narin",
                "C. Kaya",
                "Z. Pamuk"
            ],
            "title": "Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks",
            "venue": "Pattern Analysis and Applications, vol. 24, no. 3, pp. 1207\u2013 1220, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "U. Kamal",
                "M. Zunaed",
                "N.B. Nizam",
                "T. Hasan"
            ],
            "title": "Anatomy-XNet: An anatomy aware convolutional neural network for thoracic disease classification in chest X-rays",
            "venue": "IEEE Journal of Biomedical and Health Informatics, pp. 1\u201311, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y.-C. Lee",
                "M.A. Khalil",
                "J.-H. Lee",
                "A. Syakura",
                "Y.-F. Ding",
                "C.-W. Wang"
            ],
            "title": "Fully automatic registration methods for chest X-ray images",
            "venue": "Journal of Medical and Biological Engineering, vol. 41, no. 6, pp. 826\u2013 843, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: a large-scale hierarchical image database",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "D. Cozzi",
                "M. Albanesi",
                "E. Cavigli",
                "C. Moroni",
                "A. Bindi",
                "S. Luvar\u00e0",
                "S. Lucarini",
                "S. Busoni",
                "L.N. Mazzoni",
                "V. Miele"
            ],
            "title": "Chest X-ray in new coronavirus disease 2019 (COVID-19) infection: findings and correlation with clinical outcome",
            "venue": "La Radiologia Medica, vol. 125, no. 8, pp. 730\u2013737, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "International Conference on Learning Representations, pp. 1\u201314, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "I.D. Apostolopoulos",
                "T.A. Mpesiana"
            ],
            "title": "COVID-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks",
            "venue": "Physical and Engineering Sciences in Medicine, vol. 43, no. 2, pp. 635\u2013640, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Shelke",
                "M. Inamdar",
                "V. Shah",
                "A. Tiwari",
                "A. Hussain",
                "T. Chafekar",
                "N. Mehendale"
            ],
            "title": "Chest X-ray classification using deep learning for automated COVID-19 screening",
            "venue": "SN Computer Science, vol. 2, no. 4, pp. 1\u20139, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Sitaula",
                "M.B. Hossain"
            ],
            "title": "Attention-based VGG-16 model for COVID-19 chest X-ray image classification",
            "venue": "Applied Intelligence, vol. 51, no. 5, pp. 2850\u20132863, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P.K. Sethy",
                "S.K. Behera",
                "P.K. Ratha",
                "P. Biswas"
            ],
            "title": "Detection of coronavirus disease (COVID-19) based on deep features and support vector machine",
            "venue": "International Journal of Mathematical Engineering and Management Science, vol. 5, no. 4, pp. 643\u2013651, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4700\u20134708, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Singh",
                "V. Kumar",
                "M. Kaur"
            ],
            "title": "Densely connected convolutional networks-based covid-19 screening model",
            "venue": "Applied Intelligence, vol. 51, no. 5, pp. 3044\u20133051, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Rajpurkar",
                "J. Irvin",
                "K. Zhu",
                "B. Yang",
                "H. Mehta",
                "T. Duan",
                "D. Ding",
                "A. Bagul",
                "C. Langlotz",
                "K. Shpanskaya"
            ],
            "title": "CheXNet: Radiologistlevel pneumonia detection on chest X-rays with deep learning",
            "venue": "arXiv:1711.05225, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B. Chen",
                "J. Li",
                "X. Guo",
                "G. Lua"
            ],
            "title": "DualCheXNet: dual asymmetric feature learning for thoracic disease classification in chest X-rays",
            "venue": "Biomedical Signal Processing and Control, vol. 53, pp. 1\u201311, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.R. Nayak",
                "D.R. Nayak",
                "U. Sinha",
                "V. Arora",
                "R.B. Pachori"
            ],
            "title": "Application of deep learning techniques for detection of COVID-19 cases using chest X-ray images: A comprehensive study",
            "venue": "Biomedical Signal Processing and Control, vol. 64, p. 102365, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Roberts",
                "D. Driggs",
                "M. Thorpe",
                "J. Gilbey",
                "M. Yeung",
                "S. Ursprung",
                "A.I. Aviles-Rivero",
                "C. Etmann",
                "C. McCague",
                "L. Beer"
            ],
            "title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans",
            "venue": "Nature Machine Intelligence, vol. 3, no. 3, pp. 199\u2013217, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.R. Zech",
                "M.A. Badgeley",
                "M. Liu",
                "A.B. Costa",
                "J.J. Titano",
                "E.K. Oermann"
            ],
            "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study",
            "venue": "PLoS medicine, vol. 15, no. 11, p. e1002683, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Shamshad",
                "S.H. Khan",
                "S.W. Zamir",
                "M.H. Khan",
                "M. Hayat",
                "F.S. Khan",
                "H. Fu"
            ],
            "title": "Transformers in medical imaging: A survey",
            "venue": "Intelligent Medicine, vol. 88, p. 102802, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Li",
                "J. Chen",
                "Y. Tang",
                "C. Wang",
                "B.A. Landman",
                "S.K. Zhou"
            ],
            "title": "Transforming medical imaging with transformers? a comparative review of key properties, current progresses, and future perspectives",
            "venue": "CoRR, vol. abs/2206.01136, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Matsoukas",
                "J.F. Haslum",
                "M. S\u00f6derberg",
                "K. Smith"
            ],
            "title": "Is it time to replace CNNs with transformers for medical images",
            "venue": "IEEE/CVF International Conference on Computer Vision - Workshop on Computer Vision for Automated Medical Diagnosis, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask R-CNN",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2961\u20132969, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Kendall",
                "Y. Gal",
                "R. Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7482\u2013 7491, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Huang",
                "Q. Xu",
                "Y. Wang",
                "Y. Wang",
                "Y. Zhang"
            ],
            "title": "Self-supervised masking for unsupervised anomaly detection and localization",
            "venue": "IEEE Transactions on Multimedia, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "Y. Peng",
                "L. Lu",
                "Z. Lu",
                "M. Bagheri",
                "R.M. Summers"
            ],
            "title": "ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2097\u20132106, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Yan",
                "J. Yao",
                "R. Li",
                "Z. Xu",
                "J. Huang"
            ],
            "title": "Weakly supervised deep learning for thoracic disease classification and localization on chest X-rays",
            "venue": "ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics, pp. 103\u2013110, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "O. Viniavskyi",
                "M. Dobko",
                "O. Dobosevych"
            ],
            "title": "Weakly-supervised segmentation for disease localization in chest X-ray images",
            "venue": "International Conference on Artificial Intelligence in Medicine, pp. 249\u2013259, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Ouyang",
                "Z. Xue",
                "Y. Zhan",
                "X.S. Zhou",
                "Q. Wang",
                "Y. Zhou",
                "Q. Wang",
                "J.-Z. Cheng"
            ],
            "title": "Weakly supervised segmentation framework with uncertainty: A study on pneumothorax segmentation in chest X-ray",
            "venue": "International Conference on Medical Image Computing and Computer- Assisted Intervention, pp. 613\u2013621, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Singla",
                "S. Feizi"
            ],
            "title": "Salient ImageNet: How to discover spurious features in deep learning",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Cao",
                "L. Xu",
                "D.Z. Chen",
                "H. Gao",
                "J. Wu"
            ],
            "title": "A robust shape-aware rib fracture detection and segmentation framework with contrastive learning",
            "venue": "IEEE Transactions on Multimedia, vol. 25, pp. 1584\u20131591, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Xie",
                "R.B. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987\u20135995, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Gu",
                "Y. Li",
                "F. Jiang",
                "Z. Wen",
                "S. Liu",
                "W. Shi",
                "G. Lu",
                "C. Zhou"
            ],
            "title": "VINet: A visually interpretable image diagnosis network",
            "venue": "IEEE Transactions on Multimedia, vol. 22, no. 7, pp. 1720\u20131729, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. d. l. I. Vay\u00e1",
                "J.M. Saborit",
                "J.A. Montell",
                "A. Pertusa",
                "A. Bustos",
                "M. Cazorla",
                "J. Galant",
                "X. Barber",
                "D. Orozco-Beltr\u00e1n",
                "F. Garc\u0131\u0301a-Garc\u0131\u0301a"
            ],
            "title": "BIMVC COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients",
            "venue": "arXiv:2006.01174, 2021.",
            "year": 2006
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. Kopf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in Neural Information Processing Systems, pp. 8024\u20138035, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems (P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.), pp. 1106\u20131114, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "F.N. Iandola",
                "M.W. Moskewicz",
                "K. Ashraf",
                "S. Han",
                "W.J. Dally",
                "K. Keutzer"
            ],
            "title": "SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and <1MB model size",
            "venue": "CoRR, vol. abs/1602.07360, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "Proceedings of the IEEE international conference on computer vision, pp. 618\u2013626, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Vision Transformer, chest X-ray image classification, multi-task learning.\nI. INTRODUCTION\nCHEST X-RAY (CXR) imaging is a common methodfor diagnosing lung related diseases. Performing CXR is relatively cheap and quick, yet effective as a CXR image captures sufficient details about the condition of a patient\u2019s lungs. Well-trained doctors and radiologists can study known patterns of lung diseases from text books and then practice diagnosis on CXR databases. However, identifying disease patterns from CXR images is challenging, especially when nodules in the patterns are small and/or appear at positions that align with organs.\nCXR image examination is critical to deciding correct treatments and has often been performed manually. However, during the outbreak of the COVID-19 pandemic, where the sheer number of patients outnumbers the number of doctors and radiologists, manual diagnoses are too expensive and time consuming for even a fraction of patients.\nLiterature has shown that it is possible to automate parts of the diagnosis process using computer-based medical image analysis techniques [1], [2]. Specifically, computer-based algorithms can be used to predict the health condition of patients based on their CXR images. Those algorithms are trained to extract complex patterns from CXR data using training signals\nHuyen Tran, Duc Thanh Nguyen, and John Yearwood are with the School of Information Technology, Deakin University, 75 Pigdons Road, Waurn Ponds, VIC 3216, Australia (e-mail: {trhu,duc.nguyen,john.yearwood}@deakin.edu.au).\nprovided by radiologists and experts. After being trained and validated carefully, the algorithms are applied to diagnose lung related diseases from CXR images. There are three main challenges to be addressed: (i) CXR patterns are different from normal images and unclear even to human experts. (ii) Nodules in a CXR image vary in their shapes and sizes and can be found at arbitrary locations. (iii) It is important to point out regions affected by the disease. These regions are evidence to help doctors and radiologists consolidate their diagnoses.\nWith recent developments of hardware devices that enable complex computations and availability of large-scale datasets that provide abundant training data, deep learning algorithms have been widely applied to medical image analysis and achieved impressive outcomes in many tasks including image classification [3], segmentation [4], and registration [5]. CXR research has also benefited from such high capacity deep neural network architectures to learn and extract complex patterns from CXR data.\nA common practice of deep learning-based CXR image analysis is to adopt a pre-trained convolutional neural network (CNN), trained on large-scale imagery datasets such as ImageNet [6], then fine-tune it on a CXR dataset for a specific diagnosis task. However, this approach shows several limitations. Firstly, pre-trained CNN architectures are not specifically designed to work with CXR patterns covering bilateral involvement, peripheral and lower zone dominance of ground glass opacities, and patchy consolidations [7]. Secondly, CNNs often make use of simple feature concatenation and pooling operations to fuse local features into global ones. However, feature interactions between different spatial regions are not taken into account. In addition, existing methods focus on individual diagnosis tasks, i.e., a CNN is designed per diagnosis task, while lacking a unified framework to support multiple tasks, e.g., disease recognition and affected region identification. Different tasks may have dual and correlation relationships, leveraging the overall performance.\nIn this paper, we propose a unified framework to address the above issues. Our method is designed to learn both local patterns and global correlations between them from CXR images while simultaneously performing both chest X-ray image classification and affected region identification. Our method is built upon the Vision Transformer (ViT) [8] but extends the learning capability via incorporation of both local and global information, and multi-task learning. To this end, we make the following contributions.\n\u2022 Multi-task Vision Transformer (MVC), a network architecture that simultaneously enables two common diagnosis tasks from CXR images: disease recognition and\nar X\niv :2\n31 0.\n00 41\n8v 1\n[ ee\nss .I\nV ]\n3 0\nSe p\n20 23\n2 affected region identification. Our architecture can help radiologists determine a disease from a CXR image and, at the same time, localise image regions relevant to the disease. This region-based information is important as it would help consolidate and support decision making to medical treatments. To the best of our knowledge, MVC is the first method tackling these tasks explicitly and simultaneously.\n\u2022 Effective incorporation of local patterns and global correlations via self-attention and multi-task learning. It is proven that such an incorporation improves the learning capability of the architecture, while multi-task learning further boosts up the performance of each individual task. \u2022 Extensive experiments on a benchmark dataset to validate and compare our method with existing baselines.\nThe remainder of our paper is organised as follows. Section II discuss related work. Section III presents our proposed method, including the MVC\u2019s architecture and its training. We report experimental results in Section IV. Section V summarises our paper with remarks and discusses future work."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. CNN-based methods",
            "text": "Literature in deep learning-based CXR image analysis has mainly focused on applying CNN architectures to learn complex patterns from CXR data for different diagnosis tasks [1], [2]. Since training of CNNs requires a lot of training data, one often adopts architectures, pre-trained on public and largescale datasets, such as ImageNet [6], and customise them, for instance by replacing the last layer, to fit with a medical task, such as a disease recognition task. These CNNs model, after being customised, are fine-tuned on a smaller domainspecific dataset relevant to the task. Although the CXR image domain is very different from those in natural images in ImageNet dataset, it is empirically shown that shallow layers in pre-trained models could still capture useful information for downstream applications.\nExample architectures commonly adopted in CXR image analysis includes VGG [9] in [10], [11], ResNet [12] in [3], [13], [14], DenseNet [15] in [16], [17], and combination of ResNet and DenseNet in [18]. To summarise this trend, the work in [19] compared several popular pre-trained models for automated COVID-19 screening from CXR images. However, domain shift is a well-known issue of this approach, and becomes more severe when there are limited labelled data in the CXR domain. For instance, as shown in [20], [21], the performance of pre-trained CNN models significantly drops when adapting to small-scale CXR datasets. In addition, global representations in existing CNNs are formed by simple concatenation and pooling operations. Meanwhile, interactions between local features extracted from different spatial regions and between local and global features are not taken into account. These interactions are not only important to determining the disease from CXR images but also crucial to localising affected regions, consolidating medial diagnoses and treatments.\nB. Vision Transformer-based methods\nVision Transformer (ViT) [8] is a neural network architecture built upon the encoder of the so-called Transformer [22], that was originally proposed for language translation tasks. When applying to visual domain, ViT decomposes an image into local patches which correspond to tokens in text and natural language processing. These local patches capture local information and can be transformed sequentially multiple times through the architecture. To make ViT an image classifier, a learnable classification token is inserted into the sequence of the local patches.\nViT has also shown its superiority over traditional CNNs in many medical image analysis tasks [23], [24]. For instance, Matsoukas et al. [25] compared the performance of ViT and CNNs in CXR image classification and showed that ViT can capture more complex patterns in CXR images than traditional CNNs. In addition, the authors indicated that using pretrained weights obtained from ImageNet could further boost up the performance of ViT in CXR image classification. They eventually reached out a conclusion that it is time to switch to ViT for medical image classification.\nIn general, the success of ViT lies in several factors. Firstly, this architecture can model both local patterns (captured by local patches) and global correlations between them via attention mechanism. Secondly, although the model is robust and can generalise well to different domains, it is simple in computation yet adaptive and scalable to various problem settings (as the architecture of ViT is mainly made of multilayer perceptrons). However, existing work focuses only on the representation learning ability of the architecture while overlooking its potential in multi-task learning. Local patterns learnt in ViT are mostly used to support a final goal, e.g., classifying an input image. Meanwhile, these local patterns could be learnt more effectively in a supervised manner via a multi-task learning setting."
        },
        {
            "heading": "C. Multi-task methods",
            "text": "Multi-task learning has been widely applied in computer vision and image understanding. Examples of the multitask setting in computer vision include object detection and instance segmentation [26], semantic and instance segmentation (panoptic segmentation) [27], anomaly detection and localisation [28]. In general, an architecture enabling multitask learning includes a general backbone for feature learning followed by several heads, each of which handles a sub-task. For instance, Kendall et al. [27] adopted a shared encoder to learn common features and two decoders specialised for each semantic and instance segmentation task. Nevertheless, joint learning of multiple tasks from shared features learnt in early stages of a deep architecture weakly incorporates the sub-tasks. As a consequence, a well-known issue of this method is the multi-task learning may degrade the performance of individual tasks. In [28], the authors applied self-supervised learning on local image regions (masks) for image inpainting in a training set (of normal images). They then used this network on an abnormal image to identify regions of abnormality by identifying high inpainting error regions. In training of the inpainting\n3 network, random masks were created and applied to an input image. An autoencoder was then trained to reconstruct both the input image and the mask. At inference phase, masks were initialised as checker board masks at different resolutions. An iterative process was then repeated until convergence where the masks of regions with high reconstruction error were kept while the ones with low error were removed. The final mask was the weighted average and used as the anomaly region prediction for the input image.\nDespite existing application of multi-task learning, there are less works in medical image analysis taking the multitask learning approach, probably due to a lack of multilabelled medical image datasets in the field. There are methods jointly performing classification of lung related diseases and identification of affected regions from CXR images, e.g., [29]\u2013 [32]. These methods make use of saliency maps (also called attention maps) to define affected regions. However, those attention maps are not learnt explicitly but interpreted additionally to the classification task. Therefore, as shown in experiments, they often include spurious features. For instance, Singla and Feizi [33] showed that an attention map can result in spurious features that are likely to co-occur with the region of interest but not a part of it, e.g., the attribute \u201cfingers\u201d was used for class \u201cband aid\u201d since they usually co-occur. In addition, a common challenge of this approach is that it requires tremendous effort from human annotators for labelling of the attention maps.\nRecently, some medical image recognition methods focus on interpreting prediction outcomes. For instance, in [34], a three-stage network was proposed to jointly perform disease prediction and segmentation. In the first stage, a contrastive learning network was adopted to learn pixel-level spatial consistency. In the second stage, a fracture detection model with ResNeXt [35] was built to classify between positive class, with fracture, and negative class, without fracture. In the third stage, a multi-task fracture segmentation model was built to do semantic segmentation and boundary segmentation.\nLike [34], Gu et al. [36] developed a visually interpretable network in place of a traditional CNN to provide not only predictions but also visual hints that lead to predictions. To achieve this the network estimates the importance of every pixel on an input image using an auxiliary network. It then replaces unimportant pixels with random noise, and finally uses the resulting image for predicting an outcome. A mask of important pixels can be used to explain the prediction outcome. Our method also follows this direction, i.e., estimating the importance of image regions on a CXR image. However, we predict patch-level importance instead of pixel-level importance, and use this importance to leverage the estimation of an importance mask from the input CXR image. Our network, in addition, utilises limited amount of patch-level labels to improve the estimation of the importance mask."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": "The problem that we solve in this paper is to determine a COVID-19 related disease from a CXR image and, at the same time, to localise affected regions typical for the disease\non the input CXR image. In this work, we propose to solve both the sub-tasks simultaneously in a unified framework."
        },
        {
            "heading": "A. Architecture of MVC",
            "text": "To address the aforementioned problem, we propose a new architecture, namely multi-task vision transformer for chest X-ray images (MVC). We adopt the ViT model in [8] as the main backbone for our architecture for several reasons. Firstly, ViT is proven for its capability of learning both local patterns and global correlations between the local patterns. This localglobal representation learning well fits our problem as we aim to simultaneously classify a CXR image and identify affected regions (from local patterns). Secondly, ViT is scalable to different problem settings and domains. Thirdly, as shown in the literature and also from our experimental results, ViT outperforms all existing backbones.\nOur proposed MVC extends the traditional ViT [8] in the following aspects. (i) In addition to predicting a lung related disease for an input CXR image as conventional CXR image classification, we also classify local image regions, called patches, on the input CXR image into two classes: positive (affected region) and negative (non-affected region). (ii) We combine learnt representations of local patches with patch classification outcomes in a weighted linear combination where weights are derived from self-attention and patch classification. (iii) We formulate our problem in a multi-task learning setting where sub-tasks include image classification and patch classification. These two sub-tasks are jointly optimised. As shown in our experiments, the tasks support each other, and one task can leverage the other one, leading to improved overall performance.\nThe MVC consists of four modules: a local representation extraction module built upon the ViT [8] to learn representations for local patches, a self-attention module for calculating attention scores from local patches, a patch classification module for classifying local patches, and an image classification module for classifying the entire input image. We illustrate our proposed MVC in Fig. 1. and describe its main components in corresponding sub-sections.\n1) Local representation extraction via ViT: Let X be an input CXR image of size n\u00d7 n (pixels), and m\u00d7m (pixels) be the resolution of each local patch on the image X. We first apply a uniform grid of size N = nm \u00d7 n m to X. We choose n and m such that n/m is integer for computational convenience. This operation results in a sequence of local patches (x1, . . . ,xN ) ordered from left-to-right top-to-bottom, where xi \u2208 Rm\u00d7m, i = 1, . . . , N . We then apply the pretrained ViT model in [8] to the local patches (x1, . . . ,xN ) to obtain local representations (zi, . . . , zN ) as,\n(z1, . . . , zN ) = ViTm(x1, . . . ,xN ) (1)\nwhere ViTm denotes the ViT model with m\u00d7m-pixel patches. Note that, for local representation extraction, we only use patch embedding and attention layers except for prediction heads from the original ViT model. In general, this step transforms a local image patch xi \u2208 Rm\u00d7m into an embedding vector zi \u2208 Rd which is more informative about the importance of the patch xi in relation to other patches.\n4\n2) Patch classification: Local representations zi encoded by the ViT model are classified by a patch classifier into two classes: positive or negative. A positive label indicates a disease related region (affected region) while a negative label implies a normal region (non-affected region). We realise the patch classifier with a 2-layer MLP with ReLU activation function and a softmax layer for the output layer. The MLP takes input as a vector zi and returns a label l\u0302i \u2208 {0, 1} with a probability p\u0302i \u2208 [0, 1] that xi is a positive (affected) patch,\n(l\u0302i, p\u0302i) = Patch-Classifier(zi) (2)\n3) Self-attention: Local representations zi are also passed to a self-attention module to calculate a set of attention scores ai. Specifically, each representation zi is fed to a 2-layer MLP with ReLU activation function, then normalised by a softmax function to produce a score ai,\nai = exp(w\u22a4i ReLU(Vzi))\u2211N j=1 exp(w \u22a4 j ReLU(Vzj)) , i = 1, . . . , N (3)\nwhere we use a separate parameter vector wi \u2208 Rh for each representation zi and a shared parameter matrix V \u2208 Rh\u00d7d for all the representations; d is the dimension of zi and h is the size of the hidden layer in the MLP (see the second row in Table II).\n4) Image classification: Attention scores ai achieved from the self-attention module and positive probabilities p\u0302i calculated from the patch classification module are then combined via an element-wise multiplication operator to produce a set of attention-based positive scores (p\u0302\u20321, . . . , p\u0302 \u2032 N ),\n(p\u0302\u20321, . . . , p\u0302 \u2032 N ) = (a1p\u03021, . . . , aN p\u0302N ) (4)\nThis combination aims to regulate the prediction of each local patch by the attention score of the patch. For instance, the attention score of a false positive patch can mitigate\nthe prediction probability of that patch, affecting to the final decision to a disease on the entire input image. The scores p\u0302\u2032i are then used to weight the local representations zi to create a global representation Z \u2208 Rd as,\nZ = N\u2211 i=1 p\u0302\u2032izi (5)\nThe global representation Z is finally fed to an image classifier, which is another 2-layer MLP with ReLU activation function and a softmax layer for the last layer, to identify a lung related disease y\u0302 and its probability P\u0302 (y\u0302),\n(y\u0302, P\u0302 (y\u0302)) = Image-Classifier(Z) (6)\nwhere y\u0302 is from a set of predefined disease classes Y , and P\u0302 is a probability distribution of y\u0302 achieved from the softmax layer, i.e., y\u0302 = argmaxy\u0304\u2208Y P\u0302 (y\u0304)."
        },
        {
            "heading": "B. Multi-task learning",
            "text": "The proposed MVC can be trained end-to-end, and subtasks (i.e., patch classification and image classification) can be jointly optimised during training. Specifically, suppose that each training CXR image X is associated with a label y \u2208 Y and a sequence of true labels l = (l1, . . . , lN ) \u2208 {0, 1}N for the local patches (x1, . . . ,xN ).\nThe label set Y includes lung related diseases, and is predefined. We obtain the patch labels l1, . . . , lN as follows. Suppose that affected regions of an image X are delineated as bounding boxes and provided in the ground-truth. A patch xi is determined as positive (li = 1), if it matches an affect region in the ground-truth, and negative (li = 0), otherwise. A match is confirmed if there exists an affected region R in the ground-truth data such that |xi\u2229R||xi| > 0.5, where \u2229 denotes\n5\nthe intersection of two regions and | \u00b7 | represents the area of a region (in number of pixels).\nNote that, we do not formulate the task of finding affected regions as an object detection problem. This is because, although affected regions are provided as bounding boxes in the ground-truth, they do not really represent meaningful objects as in the object detection setting. Instead, those regions vary in their shape and size, and can be scattered in a CXR image. In our method, we design the local patches such that they can capture enough information to determine a disease while being able to represent smallest affected regions. Specifically, we set the size of the local patches to 16\u00d716-pixels in relation to a resolution of 224 \u00d7 224-pixels. This size is estimated empirically from the minimum of the dimensions of affected regions\u2019 bounding boxes from the ground-truth. Fig. 2 shows the distributions of the width and height of affected regions\u2019 bounding boxes.\nIn Eq. (6), a predicted class label y\u0302 for the input image X is obtained via a softmax layer. Let P be the target probability distribution of true labels y, i.e., P (y\u0304) = 1 if y\u0304 = y, and P (y\u0304) = 0, otherwise. We define a classification loss Limage for the image classification task using cross-entropy loss as,\nLimage(y, y\u0302) = \u2212 \u2211 y\u0304\u2208Y P (y\u0304) log P\u0302 (y\u0304) = \u2212 log P\u0302 (y) (7)\nFrom Eq. (2), we construct l\u0302 = (l\u03021, . . . , l\u0302N ) and p\u0302 = (p\u03021, . . . , p\u0302N ). Let p = (p1, . . . , pN ) be the sequence of probabilities of the sequence of true labels l = (l1, . . . , lN ) being positive, i.e., pi = 1 if li = 1 and pi = 0, otherwise. We define a loss Lpatch for the patch classification module using binary cross-entropy loss as,\nLpatch(l, l\u0302) = \u2212 N\u2211 i=1 pi log p\u0302i + (1\u2212 pi) log(1\u2212 p\u0302i) (8)\nFinally, we define a loss L to train the entire MVC as,\nL = Limage(y, y\u0302) + 1\nN Lpatch(l, l\u0302) (9)"
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experimental setup",
            "text": "1) Dataset: Since we aim to both classify CXR images and identify affected regions, we chose the COVID-19 Chest-Xray dataset in [37], which is associated with both image-level\nand region-level labels, to conduct experiments. We note that the labels in the COVID-19 Chest-X-ray dataset are reliable as they are provided by medical experts. In addition, COVID-19 Chest-X-ray is the largest public COVID-19 image dataset in the field.\nThis dataset consists of 5,937 images, categorised into 4 classes: Typical, Atypical, Indeterminate, and Negative. The original CXR images in the dataset are in 256 \u00d7 256-pixel resolution. We first resized all the images to 224\u00d7 224-pixel resolution. The dataset is split into a training set including 4,749 images and a test set including 1,188 images. We describe the COVID-19 Chest-X-ray dataset in Table I. We also present the average ratio of affected regions (in pixels) and their entire image on classes labelled with affected regions in Table I.\n2) Implementation details: We initialised the local representation extraction module from a pre-trained ViT model [8], trained on ImageNet [6]. Recall that, the local representation extraction module is constructed from all the layers in the ViT except for the last layer.\nOur architecture makes use of several MLPs in the submodules (patch classification, self-attention scores calculation, and image classification). We describe the details of these MLPs in Table II. Note that, ReLU activation function is used in all the MLPs.\nWe trained the MVC using 15 epochs, learning rate of 0.0001, batch size of 16, and ADAM optimiser. We also applied data augmentation to the training of our model and other baselines for fair comparisons. The data augmentation includes horizontal flipping, affine transformations, and colour jittering. The augmentation was performed randomly on training batches. This helps to prevent overfitting in the training and makes the model more robust against noise and variations such as image misalignment, image scaling, making bounding boxes not fitting well into affected regions.\nWe implemented our proposed MVC and other methods in Pytorch 1.10 [38] and conducted all experiments on 2 NVIDIA GeForce RTX 2080 Ti."
        },
        {
            "heading": "B. Result analysis",
            "text": "1) CXR image classification: We evaluated our method in the CXR image classification task and measure its performance via the recognition accuracy on every disease class and overall. We provide a confusion matrix showing the recognition accuracy of our method on the test set of the COVID-19 Chest-Xray dataset in Fig. 3.\n6\nTABLE II DETAILS OF THE MLPS USED IN OUR MVC. THE INPUT SIZE OF ALL THE MLPS IS 384, WHICH IS THE DIMENSION d OF LOCAL REPRESENTATIONS zi . THE OUTPUT SIZE OF THE MLP USED IN SELF-ATTENTION IS 1 AS THIS MLP PRODUCES AN ATTENTION SCORE FOR EACH LOCAL PATCH. FOR THE MLPS USED IN PATCH CLASSIFICATION AND IMAGE CLASSIFICATION, THE OUTPUT SIZE IS THE NUMBER OF CLASSES, E.G., 2 CLASSES (BINARY OUTPUT) FOR PATCH CLASSIFICATION AND 4 CLASSES (4 LUNG RELATED DISEASES) FOR IMAGE CLASSIFICATION.\nMLP Input size Hidden size Output size\nPatch classification 384 500 2 Self-attention 384 500 1 Image classification 384 500 4\nN eg\nat iv\ne\nTy pi\nca l\nAt yp\nic al\nIn de\nte rm\nin at\ne\nPredicted labels\nNegative\nTypical\nAtypical\nIndeterminate\nTr ue\nla be\nls\n84.5 3.6 1.2 10.6\n14.2 63.2 7.9 14.7\n24.4 15.4 28.2 32.1\n31.4 27.1 7.6 33.8 15\n30\n45\n60\n75\nFig. 3. Confusion matrix of our MVC on the test set of the COVID-19 ChestX-ray dataset.\nWe also compared our proposed method with prevailing image classification baselines including AlexNet [39], DenseNet [15] (used in [17]), ResNet-50 [12] (used in [3]), SqueezeNet [40], VGG [9] (used in [10]), Vision Transformer [8] at resolutions 8 \u00d7 8 (ViT8) and 16 \u00d7 16 (ViT16) for local patches. For the baselines, we adopted pre-trained models and customised them by replacing the last layer to fit with the disease classes in our problem. We then fine-tuned the baselines on the COVID-19 Chest-X-ray dataset.\nWe summarise the specification of all the baselines and their recognition accuracy (on every class and overall) in the image classification task in Table III. As shown in the results, compared with the existing baselines, our proposed MVC achieves the best overall performance. ViT16 ranks second and surpasses traditional CNNs. Our MVC also outperforms ViT16, which is used as the backbone for local representation extraction. This is evident for advantages brought by the incorporation of local and global information, and by patch classification to image classification.\nExcept for SqueezNet, all other baselines (including the MVC) achieve the best and second best accuracy on the Negative and Typical class respectively. Atypical and Indeterminate classes remain challenging to all the methods, probably due to limited training data.\nIn this experiment, we also compared two variants of the ViT: ViT8 and ViT16 corresponding to (8\u00d78)- and (16\u00d716)- pixel local patch setting. We observed that ViT16 outperforms ViT8 in the image classification task. Hence, we adopted ViT16\nas the backbone for local representation extraction in our proposed MVC.\n2) Affected region identification: Since we formulate the task of affected region identification as binary classification of local patches, we measured the performance of our method in this task using common metrics in binary classification including F1-score, area under receiver operating characteristic curve (AU-ROC), and area under precision-recall curve (AUPR). Those metrics reflect the trade-off between true positive and false positive rates in classification of local patches. Recall that the label (affected vs non-affected) for each patch is determined based on the overlapping between the patch and an affected region\u2019s bounding box in the ground-truth. We also measured the Jaccard similarity between all affected regions on a CXR image identified by our method and regions\u2019 bounding boxes given in the ground-truth. Different from F1-score, AUROC, and AU-PR, the Jaccard similarity measures the coincidence of predicted regions and ground-truth regions at pixel level as the Jaccard similarity is calculated on region masks generated from affected patches predicted by our method and regions\u2019 bounding boxes from the ground-truth.\nWe report F1-score, AU-ROC, AU-PR, and the Jaccard similarity of our MVC on every class and overall in Table IV. We illustrate several region identification results of our method in Fig. 4 (the first row). As shown in the results (square patches), our method can well identify affected regions. Moreover, our identified regions are even better localised in lungs areas, compared with the ground-truth data (red boxes).\nFig. 4 also visually compare our method with existing baselines in affected region identification. Since our method is the only method explicitly aiming to identify affected regions in addition to disease classification, we showcase the ability of affected region identification from existing methods via heat maps. Particularly, we applied the Gradient-weighted Class Activation Mapping (Grad-CAM) in [41] to extract the heat maps. For each model, we used the gradients of positive scores from the final convolutional layer/feature map in the model to produce a course heat map. The heat maps highlight important regions from an image which make the most influence to the final classification outcome (a disease). As shown in Fig. 4, affected regions are not clearly indicated in the heat maps of existing methods. AlexNet, DenseNet, and ResNet tend to use information from regions spreading a large proportion of the input image whereas positive regions only are accounted for a small proportion. SqueezeNet and VGG on the other hand are more localised but their identified important regions do not well cover real affected regions. Therefore, to make predicted regions more accurately, a post-processing step is required to the baseline methods.\n3) Ablation study: We investigated different settings in the design of our MVC. In particular, we compared two variants of the MVC built upon two patch size setting, 8 \u00d7 8-pixels (ViT8) and 16\u00d7 16-pixels (ViT16) for local patches. Table V shows the performances of the MVC with ViT16 and ViT8 backbones in both the image classification and affected region identification tasks. We observed that the MVC built with ViT16 consistently outperforms that built with ViT8 on all the performance metrics. Recall that ViT16 also surpasses ViT8\n7 Our MVC\nAlexNet [39]\nDenseNet [15]\nResNet-50 [12]\nSqueezeNet [40]\nVGG [9]\nFig. 4. Results of affected regions identified by our method (marked by patches classified as positive) and by other methods (shown in Grad-CAM heat maps [41]). Ground-truth regions are highlighted (in red boxes).\n8\non the overall image recognition accuracy (see Table III). The novelty of our proposed MVC relies on the joint image classification and affected region identification, leading to improved performance in both the tasks. To validate this capability, we created two variants of the MVC as follows. In the first variant, we skipped the patch classification module and simply set l\u0302i = 1 for all local patches xi. This leads to Z = \u2211N i=1 aizi. We call this variant \u201cMVC-image\u201d as it aims to perform image classification solely. In the second variant, we only trained the patch classification module while freezing both the self-attention and image classification modules. This variant is referred to as \u201cMVC-patch\u201d. We compared these two variants with the full version of the MVC in Table VI. Since \u201cMVC-image\u201d is not designed for patch classification, region identification metrics are not measured in this variant. Similarly, recognition accuracy is not applied to \u201cMVC-patch\u201d.\nWe observed that pre-trained models (e.g., the ViT model pre-trained on ImageNet [6]) also bring benefits to construction of the MVC despite of domain shift. This observation is consistent with findings indicated in other studies [1], [2]. In this ablation study, we compared the performances of our MVC with and without using pre-trained models (i.e., training from scratch) in both image classification and affected\nregion identification. We report results of this comparison in Table VII.\nWe investigated the learning in the MVC in both image classification and affected region identification using different metrics and under various epochs in Fig. 5. It is shown that the model starts converting at 15 epochs and its performance (on all metrics) on the test set saturates from that point. In our experiments, we stopped the training at 15 epochs even though the model keeps slightly improving on the training set. This early stopping helps the model avoid being overfitted."
        },
        {
            "heading": "V. DISCUSSION AND CONCLUSION",
            "text": ""
        },
        {
            "heading": "A. Contributions",
            "text": "This paper proposes a multi-task vision transformer network, named MVC, for COVID-19 disease recognition and affected region identification from chest x-ray images. Technically, we design our network in a two-task setting: image recognition and sub-region classification, and make the two tasks dually related. To enable such ability, we adopt the backbone of Vision Transformer for learning of local representations. These local representations are used to identify affected regions and combined via self-attention into global representations for disease recognition. One advantage of\n9 (a) (b) (c)\n(d) (e)\nFig. 5. Learning in the MVC model on the training and test set using different metrics and various epochs. (a) Overall accuracy, (b) F1-score, (c) AU-ROC. (d) AU-PR. (e) Jaccard similarity\nusing local features is their power in describing regions of complex shapes without adding extra effort. Self-attention is also useful to encode the contextual information of the local structures. The entire network can be trained end-toend and performs both disease recognition and affected region identification simultaneously. These sub-tasks support each other, leveraging the overall performance. To the best our knowledge, such a chest X-ray-based COVID-19 diagnosis approach is novel."
        },
        {
            "heading": "B. Findings",
            "text": "The proposed MVC was thoroughly evaluated and compared with existing baselines on a benchmark COVID-19 chest-x-ray dataset to recognise 4 types of lung related diseases including Negative, Typical, Atypical, and Indeterminate.\nThe MVC achieved an overall accuracy of 61.5% in the task of disease recognition. The model performed well on Negative and Typical images. Atypical and Indeterminate appear as challenging classes; they have less training data, compared with other classes. Misclassified cases within the Atypical and Indeterminate classes distribute uniformly across all the classes, meaning the uncertainty of the model to those classes. We also found that, there are much fewer positive (affected) regions in the Atypical and Indeterminate classes, compared with those in the Typical class. Overall, the proposed MVC outperforms existing chest x-ray image classification baselines.\nIn addition to recognising diseases from chest x-ray images, the MVC can also explicitly locate affected regions relevant to\na classified disease. This is another advantage of our method and would benefit for doctors and radiologists in consolidating their diagnoses.\nExperimental results also confirm the advantage of multitask learning over its single-task counterparts. Despite improved overall accuracy, the proposed MVC does not increase much computational overhead, additionally to its baseline architecture (the Vision Transformer [8])."
        },
        {
            "heading": "C. Future work",
            "text": "The MVC is trained using supervised learning for both image recognition and patch classification. This requires data labelling at both image and region level, limiting the applicability of the method. Since the MVC supports both local and global representation learning, it fits well the formulation of multiple-instance learning where local patches are considered as \u201cinstances\u201d and images are treated as \u201cbags\u201d [42]. This formulation can relax the requirement of patch labelling and is considered as our future work."
        }
    ],
    "title": "MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images",
    "year": 2023
}