{
    "abstractText": "Adversarial training is widely used to make classifiers robust to a specific threat or adversary, such as `p-norm bounded perturbations of a given p-norm. However, existing methods for training classifiers robust to multiple threats require knowledge of all attacks during training and remain vulnerable to unseen distribution shifts. In this work, we describe how to obtain adversarially-robust model soups (i.e., linear combinations of parameters) that smoothly trade-off robustness to different `p-norm bounded adversaries. We demonstrate that such soups allow us to control the type and level of robustness, and can achieve robustness to all threats without jointly training on all of them. In some cases, the resulting model soups are more robust to a given `p-norm adversary than the constituent model specialized against that same adversary. Finally, we show that adversarially-robust model soups can be a viable tool to adapt to distribution shifts from a few examples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Francesco Croce"
        },
        {
            "affiliations": [],
            "name": "Evan Shelhamer"
        },
        {
            "affiliations": [],
            "name": "DeepMind Sven Gowal"
        }
    ],
    "id": "SP:799084c215c9a22f157f73dfa15cbaf95672abab",
    "references": [
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J. H\u00e9naff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "A\u00e4ron van den Oord"
            ],
            "title": "Are we done with ImageNet",
            "venue": "arXiv preprint arXiv:2006.07159,",
            "year": 2020
        },
        {
            "authors": [
                "Battista Biggio",
                "Igino Corona",
                "Davide Maiorca",
                "Blaine Nelson",
                "Nedim Srndic",
                "Pavel Laskov",
                "Giorgio Giacinto",
                "Fabio Roli"
            ],
            "title": "Evasion Attacks against Machine Learning at Test Time",
            "venue": "arXiv preprint arXiv:1708.06131,",
            "year": 2013
        },
        {
            "authors": [
                "Dan A. Calian",
                "Florian Stimberg",
                "Olivia Wiles",
                "Sylvestre- Alvise Rebuffi",
                "Andras Gyorgy",
                "Timothy Mann",
                "Sven Gowal"
            ],
            "title": "Defending Against Image Corruptions Through Adversarial Augmentations",
            "venue": "arXiv preprint arXiv:2104.01086,",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "arXiv preprint arXiv:2003.01690,",
            "year": 2003
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Mind the box: $l 1$- APGD for sparse adversarial attacks on image classifiers",
            "venue": "arXiv preprint arXiv:2103.01208,",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Adversarial robustness against multiple and single lp-threat models via quick finetuning of robust classifiers",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ekin D. Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V. Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "year": 2010
        },
        {
            "authors": [
                "N Benjamin Erichson",
                "Soon Hoe Lim",
                "Francisco Utrera",
                "Winnie Xu",
                "Ziang Cao",
                "Michael W Mahoney"
            ],
            "title": "NoisyMix: Boosting Robustness by Combining Data Augmentations, Stability Training, and Noise Injections",
            "venue": "arXiv preprint arXiv:2202.01263,",
            "year": 2022
        },
        {
            "authors": [
                "Robert Geirhos",
                "Patricia Rubisch",
                "Claudio Michaelis",
                "Matthias Bethge",
                "Felix A Wichmann",
                "Wieland Brendel"
            ],
            "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Robert Geirhos",
                "Carlos RM Temme",
                "Jonas Rauber",
                "Heiko H Sch\u00fctt",
                "Matthias Bethge",
                "Felix A Wichmann"
            ],
            "title": "Generalisation in humans and deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sven Gowal",
                "Chongli Qin",
                "Jonathan Uesato",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples",
            "venue": "arXiv preprint arXiv:2010.03593,",
            "year": 2020
        },
        {
            "authors": [
                "Sven Gowal",
                "Sylvestre-Alvise Rebuffi",
                "Olivia Wiles",
                "Florian Stimberg",
                "Dan Andrei Calian",
                "Timothy Mann"
            ],
            "title": "Improving Robustness using Generated Data",
            "venue": "arXiv preprint arXiv:2110.09468,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Pieter Noordhuis",
                "Lukasz Wesolowski",
                "Aapo Kyrola",
                "Andrew Tulloch",
                "Yangqing Jia",
                "Kaiming He"
            ],
            "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "venue": "arXiv preprint arXiv:2111.06377,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo",
                "Dawn Song",
                "Jacob Steinhardt",
                "Justin Gilmer"
            ],
            "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
            "venue": "arXiv preprint arXiv:2006.16241,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Norman Mu",
                "Ekin D. Cubuk",
                "Barret Zoph",
                "Justin Gilmer",
                "Balaji Lakshminarayanan"
            ],
            "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
            "venue": "arXiv preprint arXiv:1912.02781,",
            "year": 1912
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "arXiv preprint arXiv:1907.07174,",
            "year": 1907
        },
        {
            "authors": [
                "Charles Herrmann",
                "Kyle Sargent",
                "Lu Jiang",
                "Ramin Zabih",
                "Huiwen Chang",
                "Ce Liu",
                "Dilip Krishnan",
                "Deqing Sun"
            ],
            "title": "Pyramid Adversarial Training Improves ViT Performance",
            "venue": "arXiv preprint arXiv:2111.15121,",
            "year": 2021
        },
        {
            "authors": [
                "Gao Huang",
                "Yixuan Li",
                "Geoff Pleiss",
                "Zhuang Liu",
                "John E Hopcroft",
                "Kilian Q Weinberger"
            ],
            "title": "Snapshot ensembles: Train 1, get m for free",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Samir Yitzhak Gadre",
                "Shuran Song",
                "Hannaneh Hajishirzi",
                "Simon Kornblith",
                "Ali Farhadi",
                "Ludwig Schmidt"
            ],
            "title": "Patching openvocabulary models by interpolating weights",
            "venue": "arXiv preprint arXiv:2208.05592,",
            "year": 2022
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging Weights Leads to Wider Optima and Better Generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Kang",
                "Yi Sun",
                "Dan Hendrycks",
                "Tom Brown",
                "Jacob Steinhardt"
            ],
            "title": "Testing Robustness Against Unforeseen Adversaries",
            "venue": "arXiv preprint arXiv:1908.08016,",
            "year": 1908
        },
        {
            "authors": [
                "Klim Kireev",
                "Maksym Andriushchenko",
                "Nicolas Flammarion"
            ],
            "title": "On the effectiveness of adversarial training against common corruptions",
            "venue": "arXiv preprint arXiv:2103.02325,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "In Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Divyam Madaan",
                "Jinwoo Shin",
                "Sung Ju Hwang"
            ],
            "title": "Learning to generate noise for multi-attack robustness",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Pratyush Maini",
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Adversarial Robustness Against the Union of Multiple Perturbation Models",
            "venue": "arXiv preprint arXiv:1909.04068,",
            "year": 2019
        },
        {
            "authors": [
                "Shuaicheng Niu",
                "Jiaxiang Wu",
                "Yifan Zhang",
                "Yaofo Chen",
                "Shijian Zheng",
                "Peilin Zhao",
                "Mingkui Tan"
            ],
            "title": "Efficient testtime model adaptation without forgetting",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Joaquin Quinonero-Candela",
                "Masashi Sugiyama",
                "Anton Schwaighofer",
                "Neil D Lawrence"
            ],
            "title": "Dataset shift in machine learning",
            "year": 2009
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Sven Gowal",
                "Dan A. Calian",
                "Florian Stimberg",
                "Olivia Wiles",
                "Timothy Mann"
            ],
            "title": "Data Augmentation Can Improve Robustness",
            "venue": "arXiv preprint arXiv:2111.05328,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do ImageNet Classifiers Generalize to ImageNet",
            "venue": "arXiv preprint arXiv:1902.10811,",
            "year": 1902
        },
        {
            "authors": [
                "Leslie Rice",
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Overfitting in adversarially robust deep learning",
            "venue": "arXiv preprint arXiv:2002.11569,",
            "year": 2020
        },
        {
            "authors": [
                "Kate Saenko",
                "Brian Kulis",
                "Mario Fritz",
                "Trevor Darrell"
            ],
            "title": "Adapting visual category models to new domains",
            "venue": "In European conference on computer vision,",
            "year": 2010
        },
        {
            "authors": [
                "Steffen Schneider",
                "Evgenia Rusak",
                "Luisa Eck",
                "Oliver Bringmann",
                "Wieland Brendel",
                "Matthias Bethge"
            ],
            "title": "Improving robustness against common corruptions by covariate shift adaptation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Sun",
                "Xiaolong Wang",
                "Zhuang Liu",
                "John Miller",
                "Alexei Efros",
                "Moritz Hardt"
            ],
            "title": "Test-time training with selfsupervision for generalization under distribution shifts",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Dan Boneh"
            ],
            "title": "Adversarial Training and Robustness for Multiple Perturbations",
            "venue": "In Advances in Neural Information Processing Systems. 2019",
            "year": 2019
        },
        {
            "authors": [
                "Dequan Wang",
                "Evan Shelhamer",
                "Shaoteng Liu",
                "Bruno Olshausen",
                "Trevor Darrell"
            ],
            "title": "Tent: Fully test-time adaptation by entropy minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models",
            "venue": "https: //github.com/rwightman/pytorch- imagemodels,",
            "year": 2019
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Yitzhak Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S. Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith",
                "Ludwig Schmidt"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "arXiv preprint arXiv:2203.05482,",
            "year": 2022
        },
        {
            "authors": [
                "Cihang Xie",
                "Mingxing Tan",
                "Boqing Gong",
                "Jiang Wang",
                "Alan Yuille",
                "Quoc V Le"
            ],
            "title": "Adversarial Examples Improve Image Recognition",
            "venue": "arXiv preprint arXiv:1911.09665,",
            "year": 2019
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "arXiv preprint arXiv:1605.07146,",
            "year": 2016
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In ICLR,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep networks have achieved great success on several computer vision tasks and have even reached super-human accuracy [16, 28]. However, the outputs of such models are often brittle, and tend to perform poorly on inputs that differ from the distribution of inputs at training time, in a condition known as distribution shift [34]. Adversarial perturbations are a prominent example of this condition: small, even imperceptible, changes to images can alter predictions to cause errors [2, 41]. In addition to adversarial inputs, it has been noted that even natural shifts, e.g. different weather conditions, can significantly reduce the accuracy of even the best vision models [11, 18, 36]. Such drops in accuracy are undesirable for robust deployment, and so a lot of effort has been invested in correcting them. Adversarial training [31] and its extensions [13, 35, 51] are currently the most effective methods to improve empirical robustness to adversarial attacks. Similarly, data augmen-\n*Work done during an internship at DeepMind.\ntation is the basis of several techniques that improve robustness to non-adversarial/natural shifts [3, 9, 19]. While significant progress has been made on defending against a specific, selected type of perturbations (whether adversarial or natural), it is still challenging to make a single model robust to a broad set of threats and shifts. For example, a classifier adversarially-trained for robustness to `p-norm bounded attacks is still vulnerable to attacks in other `qthreat models [26, 42]. Moreover, methods for simultaneous robustness to multiple attacks require jointly training on all [30, 32] or a subset of them [6]. Most importantly, controlling the trade-off between different types of robustness (and nominal performance) remains difficult and requires training several classifiers.\nInspired by model soups [46], which interpolate the parameters of a set of vision models to achieve state-of-the-art accuracy on IMAGENET, we investigate the effects of interpolating robust image classifiers. We complement their original recipe for soups by own study of how to pretrain, fine-tune, and combine the parameters of models adversarially-trained against `p-norm bounded attacks for different p-norms. To create models for soups, we pre-train a single robust model and fine-tune it to the target threat models (using the efficient technique of [6]). We then establish that it is possible to smoothly trade-off robustness to different threat models by moving in the convex hull of the parameters of each robust classifier, while achieving competitive performance with methods that train on multiple p-norm adversaries simultaneously. Unlike alternatives, our soups can uniquely (1) choose the level of robustness to each threat model without any further training and (2) quickly adapt to new unseen attacks or shifts by simply tuning the weighting of the soup.\nPrevious works [21, 27, 47] have shown that adversarial training with `p-norm bounded attacks can help to improve performance on natural shifts if carefully tuned. We show that model soups of diverse classifiers, with different types of robustness, offer greater flexibility for finding models that perform well across various shifts, such as IMAGENET variants. Furthermore, we show that a limited number of images of the new distribution are sufficient to select the\nar X\niv :2\n30 2.\n10 16\n4v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n23\nweights of such a model soup. Examining the composition of the best soups brings insights about which features are important for each dataset and shift. Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [15,21]. Contributions. In summary, we show that soups \u2022 can merge nominal and `p-robust models (for various p):\nefficient fine-tuning from one robust model obtains a set of models with diverse robustness [6] and compatible parameters for creating model soups [46] (Sec. 3), \u2022 can control the level of robustness to each threat model and achieve, without more training, competitive performance against multi-norm robustness training (Sec. 4), \u2022 are not limited to interpolation, but can find more effective classifiers by extrapolation (Sec. 4.3), \u2022 enable adaptation to unseen distribution shifts on only a few examples (Sec. 5)."
        },
        {
            "heading": "2. Related Work",
            "text": "Adversarial robustness to multiple threat models. Most methods focus on achieving robustness in a single threat model, i.e. to a specific type of attacks used during training. However, this is not sufficient to obtain robustness to unseen attacks. As a result, further work aims to train classifiers for simultaneous robustness to multiple attacks, and the most popular scenario considers a set of `p-norm bounded perturbations. The most successful methods [30, 32, 42] are based on adversarial training and differ in how the multiple threats are combined. Notably, all need to use every attack at training time. To reduce the computational cost of obtaining multiply-robust models, one can fine-tune a singly-robust model by one of the above mentioned methods [6], even for only a small number of epochs. Our soups are more flexible by skipping simultaneous adversarial training across multiple attacks.\nAdversarial training for distribution shifts. While robustness to adversarial attacks and natural shifts are not the same goal, previous works nevertheless show that it is possible to leverage adversarial training with `p-norm bounded attacks to improve performance on the common corruptions of [18]. First, AdvProp [47] co-trains models on clean and adversarial images (in the `\u221e-threat model) with dual normalization layers that specialize to each type of input. The clean branch of the dual model achieves higher accuracy than nominal training on IMAGENET and its variants. Similar results are obtained by Pyramid-AT [21] by its design of a specific attack to adversarially train vision transformers. Finally, [27] carefully selecting the size of the adversarial perturbations, i.e. their `\u221e- or `2-norm, for standard adversarial training [31] achieves competitive performance on\ncommon corruptions on CIFAR-10 and IMAGENET-100. Model soups. Ensembling or averaging the parameters of intermediate models found during training is an effective technique to improve both clean accuracy [22, 25] and robustness [12, 35]. Recently, [46] propose model soups which interpolate the parameters of networks finetuned with different hyperparameters configurations from the same pre-trained model. This yields improved classification accuracy on IMAGENET. Along the same line, [24] fine-tune a model trained on IMAGENET on several new image classification datasets, and show that interpolating the original and fine-tuned parameters yields classifiers that perform well on all tasks."
        },
        {
            "heading": "3. Model Interpolation across Different Tasks",
            "text": "In the following, we formally introduce the two main components of our procedure to merge adversarially robust models: (1) obtaining models which can interpolated by fine-tuning a single `p-robust classifiers, and (2) interpolation of their weights to balance their different types of robustness. We highlight that our setup diverges from that of prior works about parameters averaging: in fact, both [24, 46] combine models fine-tuned on the same task, i.e. achieving high classification accuracy of unperturbed images, either on a fixed dataset and different hyperparameter configurations [46], or varying datasets [24]. In our case, the individual models are trained for robustness to different types of attacks, i.e. with distinct loss functions."
        },
        {
            "heading": "3.1. Adversarial training and fine-tuning",
            "text": "Let us denote D = {(xi, yi)}i the training set, with xi \u2208 Rd indicating an image and yi \u2208 {1, . . . ,K} the corresponding label, and \u2206 : Rd \u2192 Rd the function which characterizes a threat model, that maps an input x to a set \u2206(x) \u2282 Rd of possible perturbed versions of the original image. For example, `p-norm bounded adversarial attacks with budget > 0 in the image space can be described by\n\u2206(x) = {\u03b4 \u2208 Rd : \u2016\u03b4\u2016p \u2264 ,x+ \u03b4 \u2208 [0, 1]d}. (1)\nThen, one can train a classifier f : \u03b8 \u00d7 Rd \u2192 RK parameterized by \u03b8 \u2208 \u0398 with adversarial training [31] by solving\nmin \u03b8\u2208\u0398 \u2211 (x,y)\u2208D max \u03b4\u2208\u2206(x) L(f(\u03b8, x+ \u03b4), y), (2)\nfor a given loss function L : RK \u00d7 RK \u2192 R (e.g., crossentropy), with the goal of obtaining a model robust to the perturbations described by \u2206. Note that this boils down to nominal training when \u2206(\u00b7) = {0} and no perturbation is applied on the training images. We are interested in the case where multiple threat models are available, and indicate with \u2206nominal nominal training, and \u2206p for p \u2208 {\u221e, 2, 1}\nthe perturbations with bounded `p-norm as described in Eq. 1. We focus on such tasks since they are the most common choices for adversarial defenses, in particular by methods focusing on multiple norm robustness [32, 42].\nNotably, it is possible to efficiently obtain a model robust to \u2206q by fine-tuning for a single (or few) epoch with adversarial training w.r.t. \u2206q a classifier pre-trained to be robust in \u2206p, with p 6= q and p, q \u2208 {\u221e, 2, 1} [6]. For example, in this way one can efficiently derive specialized classifiers \u22062,\u22061,\u2206nominal for each task from a single model f robust w.r.t. `\u221e. However, this does not work well when a nominal classifier is used as starting point for the short fine-tuning. In the following, we denote with \u03b8p\u2192q the parameters resulting from fine-tuning to \u2206q a base model \u03b8p."
        },
        {
            "heading": "3.2. Merging different types of robustness via linear",
            "text": "combinations in parameter space\nWe want to explore the properties of the models obtained by taking linear combinations of the parameters of classifiers with different types of robustness. To do so, there needs to be a correspondence among the parameters of different individual networks: [46] achieve this by merging differently fine-tuned versions of the same pre-trained model. As mentioned above, fine-tuning an `p-robust classifier allows to change it to achieve robustness in a new\nthreat model [6]: we exploit such property to create model soups, as named by [46]. Formally, we create a model soup from n individual networks with parameters \u03b81, . . . ,\u03b8n with weights w = (w1, . . . , wn) \u2208 Rn as\n\u03b8w = n\u2211 i=1 wi \u00b7 \u03b8i, (3)\nand the corresponding classifier is given by f(\u03b8w, \u00b7) : Rd \u2192 RK . While any choice ofw is possible, we focus on the case of affine combinations, i.e. \u2211 i wi = 1. Moreover, we consider soups which are either convex combinations of the individual models, with w1, . . . , wn \u2265 0, or obtained by extrapolations, i.e. with negative elements in w.\n4. Soups for `p-robustness\nWe measure adversarial robustness in the `p-threat model with bounds p: on CIFAR-10 we use \u221e = 8/255, 2 = 128/255, 1 = 12, on IMAGENET \u221e = 4/255, 2 = 4, 1 = 255. If not specified otherwise, we use the full test set for CIFAR-10 and 5000 images from the IMAGENET validation set, and attack by AUTOPGD [4, 5] with 40 steps. More details are provided in App. A."
        },
        {
            "heading": "4.1. Soups with two threat models",
            "text": "CIFAR-10. We explore the effect of interpolating two models robust to different `p-norm bounded attacks for p \u2208 {\u221e, 2, 1}. We consider classifiers with WIDERESNET-28-10 [49] architecture trained on CIFAR-10. For every threat model, we first train a robust classifier with adversarial training from random initialization. Then, we finetune the resulting model with adversarial training on each of the other threat models for 10 epochs. In Fig. 1 we show, for each pair of threat models (\u2206p,\u2206q), the trade-off of robust accuracy w.r.t. `p and `q for the soups\nw \u00b7 \u03b8p + (1\u2212 w) \u00b7 \u03b8p\u2192q for w \u2208 [0, 1]\nand symmetrically\nw \u00b7 \u03b8q + (1\u2212 w) \u00b7 \u03b8q\u2192p for w \u2208 [0, 1].\nInterpolating the parameters of models trained with a single `p-norm controls the balance between the two types of robustness: for example, Fig. 1 (middle plot) shows that moving from \u03b8\u221e to \u03b8\u221e\u21921 (blue curve), i.e. decreasing w from 1 to 0 in the corresponding soup, progressively reduces the robust accuracy w.r.t. `\u221e to improve robustness w.r.t. `1. Moreover, for similar threat models (i.e. the pairs (`2, `1) and (`2, `\u221e)) some intermediate networks are more robust than the extremes trained specifically for each threat model.\nIMAGENET. We now fine-tune a VIT-B16 [8] robust w.r.t. `\u221e on IMAGENET to the other threat models, including nominal training, for either 1/3, 1 or 3 epochs. Fig. 2 shows that interpolation of parameters is effective even in this setup, and allows to easily balance nominal and robust accuracy (fourth plot). Moreover, it is possible to create soups with two fine-tuned models, i.e. \u03b8\u221e\u21922 and \u03b8\u221e\u21921. Finally, increasing the number of fine-tuning steps yields better performance in the target threat model, which in turn generally leads to better soups.\nComparison to multi-norm robustness methods. Fig. 1 and Fig. 2 compare the performance of the model soups to that of models trained with methods for robustness in the union of multiple threat models. In particular, we show the results of MAX [42], which computes perturbations for each threat model and trains on that attaining the highest loss, and SAT [30], which samples uniformly at random for each training batch the attack to use. We train models with both methods for all pairs of threat models: as expected, MAX tends to focus on the most challenging threat model, sacrificing some robustness in the other one compared to SAT, since it uses only the strongest attack for each training point. When training for `\u221e and `1, i.e. the extreme `p-norms we consider, MAX and SAT models lie above the front drawn by the model soups, hinting that the more diverse the attacks, the more difficult it is to preserve high robustness to both. In the other cases, both methods behave\nsimilarly to the soups. The main advantage given by interpolation is however the option of moving along the front without additional training cost: while one might tune the trade-off between the robustness in the two threat models in SAT, e.g. changing the sampling probability, this would still require training a new classifier for each setup."
        },
        {
            "heading": "4.2. Soups with three threat models",
            "text": "We here study the convex combination of three models with different types of robustness. For CIFAR-10 we create soups with each `p-robust classifier for p \u2208 {\u221e, 2, 1} and its fine-tuned version into the other two threat models. We use the same models of the previous section, and sweep the interpolation weights w \u2208 R3 such that wi \u2208 {0, 0.2, 0.4, 0.6, 0.8, 1} and \u2211 i wi = 1. In Fig. 3 and Fig. 10 (in the Appendix) we show clean accuracy (first column) and robust accuracy in `\u221e, `2 and `1 (second to fourth columns) and their union, when a point is considered robust only if it is such against all attacks (last column).\nOne can observe that, independently from the type of robustness of the base model (used as initialization for the fine-tuning), moving in the convex hull of the three parameters (e.g. \u03b8\u221e+\u03b8\u221e\u21922+\u03b8\u221e\u21921 in Fig. 3) allows to smoothly control the trade-off of the different types of robustness. Interestingly, the highest `2-robustness is attained by intermediate soups, not by the model specifically fine-tuned w.r.t. `2, suggesting that model soups might even be beneficial to robustness in individual threat models (see more below). Moreover, the highest robustness in the union is given by interpolating only the models robust w.r.t. `\u221e and `1, which is in line with the observation of [6] that training for the extreme norms is sufficient for robustness in the union of the three threat models. Although the robust accuracy in the union is lower than that of training simultaneously with all attacks e.g. with MAX (42.0% vs 47.2%), the model soups deliver competitive results without the need of co-training.\nFinally, Fig. 4 shows that similar observations hold on IMAGENET, where we create soups fine-tuning classifiers robust w.r.t. `\u221e, with either RESNET-50 [16] or VIT-B16 as architecture, for 1 epoch."
        },
        {
            "heading": "4.3. Soups for improving individual threat model",
            "text": "robustness\nWe notice in Fig. 1 and Fig. 2 that in a few cases the intermediate models obtain via parameters interpolation have higher robustness than the extreme ones, which are trained or fine-tuned with a specific threat model. As such, we analyze in more details the soups w \u00b7 \u03b8\u221e + (1 \u2212 w) \u00b7 \u03b8\u221e\u21922, i.e. using the original classifier robust in `\u221e and the one fine-tuned to `2, on both CIFAR-10 and IMAGENET. Fig. 5 shows the robust accuracy w.r.t. `\u221e when varying the value of w: in both case the original model \u03b8\u221e, highlighted in red, does not attain the best robustness. Interestingly, on\nCIFAR-10 the best soup is found with w = 0.9, while for IMAGENET with w > 1: this suggests that the model soups\nshould not be constrained to the convex hull of the base models, and extrapolation can lead to improvement."
        },
        {
            "heading": "5. Soups for Distribution Shifts",
            "text": "Prior works [27] have shown that adversarial training w.r.t. an `p-norm is able to provide some improvement in the performance in presence of non-adversarial distribution shifts, e.g. the common corruptions of IMAGENET-C [18]. However, to see such gains it is necessary to carefully select the threat model, for example which `p-norm and size to bound the perturbations, to use during training. The experiments in Sec. 4 suggest that model soups of nominal and adversarially robust classifiers yield models with a variety of intermediate behaviors, and extrapolation might even deliver models which do not merely trade-off the robust-\nness of the initial classifiers but amplify it. This flexibility could suit adaptation to various distribution shifts: that is, the various corruption types might more closely resemble the geometry of different `p-balls or their union. Moreover, including a nominally fine-tuned model in the soup allows it to maintain, if necessary, high accuracy on the original dataset, which is often degraded by adversarial training [31] or test-time adaptation on shifted data [33].\n5.1. Soups for IMAGENET variants\nSetup. In the following, we use models soups consisting of robust VIT fine-tuned from `\u221e to the other threat models, and one more VIT nominally fine-tuned for 100 epochs to obtain slightly higher accuracy on clean data. For shifts, we consider several variants of IMAGENET, providing a broad and diverse benchmark for our soups: IMAGENET-REAL [1], IMAGENET-V2 [36], IMAGENET-C [18], IMAGENETA [20], IMAGENET-R [17], IMAGENET-SKETCH [44], and CONFLICT STIMULI [10]. We consider the setting of fewshot supervised adaptation, with a small set of labelled images from each shift, which we use to select the best soups.\nSoup selection via grid search. Since evaluating the accuracy of many soups on the entirety of the datasets would be extremely expensive, we search for the best combination of the four models on a random subset of 1000 points from each dataset, with the exception of CONFLICT STIMULI for which all 1280 images are used (for IMAGENET-C we use all corruption types and severities, then aggregate the results). Restricting our search to a subset also serves our aim of finding a model soup which generalizes to the new distribution by only seeing a few examples. We eval-\nuate all the possible affine combinations with weights in the range [\u22120.4, 1.4] with granularity 0.2, which amounts to 460 models in total. In Fig. 6 we compare, for each dataset, the accuracy of the 5 best soups to that of each individual classifier used for creating the soups and of a nominal model trained independently: for all datasets apart from IMAGENET the top soups outperform the individual models. Moreover, we notice that the best individual model varies across datasets, indicating that it might be helpful to merge networks with different types of robustness.\nComparison to existing methods. Having selected the best soup for each variant (dataset-specific soups) on its chosen few-shot adaptation set, we evaluate the soup on the test set of the variant (results in Table 1). We also evaluate the model soup that attains the best average case accuracy over the adaptation sets for all variants (single soup), in order to gauge the best performance of a single, general model soup. We compare the soups to a nominal model, the `\u221e-robust classifier used in the soups, their ensemble, the Masked AutoEncoders of [15], AdvProp [47], PyramidAT [21], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups. Selecting the best soup on 1000 images of each datasets (results in the last row of Table 1) leads in 4 out of the 8 datasets to the best accuracy, and only slightly suboptimal values in the other cases: in particular, parameters interpolations is very effective on stronger shifts like IMAGENET-R and CONFLICT STIMULI, where it attains almost 8% better performance than the closest baseline. Unsurprisingly, it is more challenging to improve on datasets like IMAGENETV2 which are very close to the original IMAGENET. Over-\nall, The soup selected for best average accuracy (across all datasets) outperforms all baselines, except for the ensemble of four models (with 4\u00d7 the inference cost), and AdvProp, which requires co-training of clean and adversarial points. These results show that soups with robust classifiers are a promising avenue for quickly adapting to distribution shifts.\nComposition of the soups. To analyze which types of classifiers are most relevant for performance on every distribution shift, we plot in Fig. 7 the breakdown of the weights of the five best soups (more intense colors indicate that the corresponding weight or a larger one is used more often in the top-5 soups). First, one can see that the nominally fine-tuned model (in black) is dominant, with weights of 0.8 or 1, on IMAGENET, IMAGENET-REAL, IMAGENETV2, IMAGENET-A and IMAGENET-C: this could be expected since these datasets are closer to IMAGENET itself, i.e. the distribution shift is smaller, which is what nominal training optimizes for (in fact, the nominal models achieve\nhigher accuracy than adversarially trained ones on these datasets in Fig. 6). However, in all cases there is a contribution of some of the `p-robust networks. On IMAGENET-R, IMAGENET-SKETCH and CONFLICT STIMULI, the model robust w.r.t. `\u221e plays instead the most relevant role, again in line with the results in Table 1. Interestingly, in the case of CONFLICT STIMULI, the nominal classifier has a weight -0.4 (the smallest in the grid search) for all top performing soups: we hypothesize that this has the effect of reduce the texture bias typical of nominal model and emphasize the attention to shapes already important in adversarially trained classifiers. Finally, we show the composition of the soup which has the best average accuracy over all datasets (last column of Fig. 7), where the nominal and `\u221e-robust models have similar positive weight.\nHow many images does it take to find a good soup? To identify the practical limit of supervision for soup selection, we study the effect of varying the number of labelled images\nused to select the best soup on a new dataset. For this analysis we randomly choose 500 images from the adaptation set used for the grid search to create a held-out test set. From the remaining images, we uniformly sample k elements, select the soup which performs best on such k points, and then evaluate it on this test set. We repeat this procedure for k \u2208 {10, 30, 100, 300, 500} for 50 times each with different random seeds. In Fig. 8 we plot the average accuracy on the held-out test set, with standard deviation, when varying k: increasing the number of points above 100 achieves high test accuracy with limited variance. This suggests that soup selection, and thus model adaptation, can be carried out with as few as 100 examples of the new distribution.\n5.2. A closer look at IMAGENET-C\nWhile our experiments have considered IMAGENET-C as a single dataset, it consists of 15 corruptions types, each with 5 severity levels. As the various corruptions have different characteristics, one might expect the best soup to vary across them. In Fig. 9 we plot the composition of the top5 soups for each severity level for two corruption types (as done in Fig. 7 ). The weights of the individual classifiers significantly change across distribution shifts: for both corruption types, increasing the severity (making perturbations stronger) leads to a reduction in the nominal weight in favor of a robust weight. However, in the case of \u201cpixelate\u201d the soups concentrate on the `1-robust network, while for \u201cjpeg compression\u201d this happens for `\u221e. Similar visualization for the remaining IMAGENET-C subsets are found in Fig. 11 of the Appendix. This highlights the importance of interpolating models with different types of robustness, and implies that considering each corruption type (including severity levels) as independent datasets could further improve the performance of the soups on IMAGENET-C."
        },
        {
            "heading": "6. Discussion and Limitations",
            "text": "Merging models with different types of robustness enables strong control of classifier performance by tuning only a few soup weights. Soups can find models which perform well even on distributions unseen during training (e.g. the IMAGENET variants). Moreover, our framework avoids cotraining on multiple threats: this makes it possible to finetune models with additional attacks as they present themselves, and enrich the soups with them.\nAt the moment, our soups contain only nominal or `probust models, but expanding the diversity of models might aid adaptation to new datasets. We selected our soups with few-shot supervision, but other settings could potentially use soups, such as unsupervised domain adaptation [34,38], on labeled clean and unlabeled shifted data, and test-time adaptation [39,40,43], on unlabeled examples alone. Moreover, in our evaluation we have constrained the soups to belong to a fixed grid, which might miss better models: future work could develop automatic schemes to optimize the soup weights, possibly with even fewer examples, or without labeled examples (as done for test-time adaptation of non-robust models)."
        },
        {
            "heading": "7. Conclusion",
            "text": "We show that combining the parameters of robust classifiers, without additional training, achieves a smooth tradeoff of robustness in different `p-threat models. This allows us to discover models which perform well on distribution shifts with only a limited number of examples of each shift. In these ways, model soups serve as a good starting point to efficiently adapt classifiers to changes in data distributions."
        },
        {
            "heading": "A. Experimental details",
            "text": "A.1. Training setup.\nCIFAR-10. We train robust models from random initialization for 200 epochs with SGD with momentum as optimizer, an initial learning rate of 0.1 (reduced 10 times at epochs 100 and 150), and a batch size of 128. For finetuning, we train for 10 epochs with cosine schedule for the learning rate, with peak value of 0.1 (we only use 0.5 for fine-tuning the model trained w.r.t. `1 to the `2-threat model) and linear ramp-up in the first 1/10 of training steps. We generate adversarial perturbations by AUTOPGD with 10 steps. We select checkpoints according to robustness on a validation set as suggested by [37].\nIMAGENET. We follow the setup of [15]: for full training, we use 300 epochs, AdamW optimizer [29] with momenta \u03b21 = 0.9, \u03b22 = 0.95, weight decay of 0.3 and a cosine learning rate decay with base learning rate 10\u22124 (scale as in [14]) and linear ramp-up of 20 epochs, batch size of 4096, label smoothing of 0.1, stochastic depth [23] with base value 0.1 and with a dropping probability linearly increasing with depth. As data augmentation, we use random crops resized to 224 \u00d7 224 images, mixup [50], CutMix [48] and RandAugment [7] with two layers, magnitude 9 and a random probability of 0.5. We note that our implementation of RandAugment is based on the version in the timm library [45]. For VIT architectures, we adopt exponential moving average with momentum 0.9999. For fine-tuning we keep the same hyperparameters except for reducing the base learning rate from 10\u22124 to 10\u22125 since this leads to better performance in the target threat model. For adversarial training we use AUTOPGD on the KL divergence loss with 2 steps for `\u221e- and `2-norm bounded attacks, 20 steps for `1 (as it is a more challenging threat model for optimization [6]).\nBaselines. For MAX and SAT, we fine-tune the singlyrobust models with the same scheme above for the networks used in the model soups. We generate adversarial perturbations with the the same attacks, and use 10 and 1 epoch of fine-tuning in the case of CIFAR-10 and IMAGENET respectively. For the baselines in Table 1, we use the same scheme (except for technique-specific components which follow the original papers). For AdvProp we use dual normalization layers and train with random targeted attacks with bound = 4/255 on the `\u221e-norm.\nA.2. Evaluation setup.\nAdversarial robustness. As default evaluation we use AUTOPGD with 40 steps and default parameters, with the DLR loss [4] for `\u221e- and `2-attacks, cross entropy for `1. As a test against stronger attacks, for the evaluation of the robustness of the soups of three threat models on CIFAR-10 (Fig. 3 and Fig. 10) we increase the budget of AUTOPGD to\n100 steps and 5 random restarts (in this case we use targeted DLR loss for `\u221e and `2, and 1000 test points).\nDistribution shifts. For all IMAGENET variants we evaluate the classification accuracy on the entire dataset."
        },
        {
            "heading": "B. Additional experiments",
            "text": "B.1. Soups with three threat models\nWe show in Fig. 10 the clean accuracy, robust accuracy for each `p-norm and their union of the soups obtained merging three classifiers. We use either a pre-trained classifier robust w.r.t. `2 (top row) or `1 (bottom row) and finetune them to the remaining threat models.\nB.2. Model soups on IMAGENET variants\nWe show additional results for model soups on IMAGENET variants: first, in Table 2 we report the results on the full datasets of the second and third best soups according to the grid search on 1000 points for the shift (we also show the best soup from Table 1 in the main part). When selecting the best soup on average across datasets, all three classifiers have very close performance (59.46% to 59.48%), while the accuracy on the individual datasets may vary e.g. on IMAGENET-A and CONFLICT STIMULI. For the datasetspecific soups the results on the entire datasets respect the ranking given by the grid search (the three values are similar to each other), further suggesting that even a limited number of points can serve to tune a suitable soup.\nSecond, we study the effect of varying the radii of the `p-threat models used by the robust classifiers in the soups. In this case, we fine-tune for 1 epoch the original model robust w.r.t. `\u221e at = 4/255 with adversarial training w.r.t. `\u221e at \u221e \u2208 {1/255, 2/255}, `2 at 2 \u2208 {1, 2} ( 2 = 4 above), `2 at 1 \u2208 {64, 128} ( 1 = 255 above). In this way we have three sets of four models to create soups, where the nominal one is fixed and the robust ones have radii p, p/2 and p/4 for p \u2208 {\u221e, 2, 1}. Table 3 reports the results of the various sets of models: for the single soup optimized for average performance, the smaller p slightly reduce the performance. Looking at the individual datasets, in some cases like IMAGENET-V2 and IMAGENET-C using smaller values of p yields some improvements, but it also leads to severe drops on the distribution shifts where having robust models is more relevant like CONFLICT STIMULI and IMAGENET-R. This suggests that it might be useful to have models robust w.r.t. the same `p-norm but with different radii in the set of the networks used for creating the soups.\nB.3. Composition of soups on IMAGENET-C\nIn Fig. 11 we visualize the composition of the top-5 soups for each corruption type and severity level: one can observe that the weights of the four networks in the soups varies across IMAGENET subset.\n0 1 zo om _b lu r\nsev.=1 sev.=2 sev.=3 sev.=4 sev.=5\n0\n1\nim pu\nlse _n\noi se\n0\n1\nsh ot\n_n oi\nse\n0\n1\nco nt\nra st\n0\n1\ngl as\ns_ bl\nur\n0\n1\nde fo\ncu s_\nbl ur\n0\n1\nga us\nsia n_\nno ise\n0\n1\npi xe\nla te\n0\n1\nel as\ntic _t\nra ns\nfo rm\nsev.=1 sev.=2 sev.=3 sev.=4 sev.=5\n0\n1\nbr ig\nht ne ss 0 1 sn ow\n0\n1\nfo g\n0\n1\nm ot\nio n_\nbl ur\n0\n1\nfro st\n0\n1\njp eg\n_c om\npr es\nsio n\n-> nominal -> 2 -> 1\nFigure 11. Best soups over IMAGENET-C subsets: we plot the composition of the top 5 soups found by the grid search for each corruption type and severity level."
        }
    ],
    "title": "Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts",
    "year": 2023
}