{
    "abstractText": "Low\u2010fidelity data is typically inexpensive to generate but inaccurate, whereas high\u2010fidelity data is accurate but expensive. To address this, multi\u2010fidelity methods use a small set of high\u2010fidelity data to enhance the accuracy of a large set of low\u2010fidelity data. In the approach described in this paper, this is accomplished by constructing a graph Laplacian from the low\u2010fidelity data and computing its low\u2010lying spectrum. This is used to cluster the data and identify points closest to the cluster centroids, where high\u2010fidelity data is acquired. Thereafter, a transformation that maps every low\u2010fidelity data point to a multi\u2010fidelity counterpart is determined by minimizing the discrepancy between the multi\u2010 and high\u2010fidelity data while preserving the underlying structure of the low\u2010fidelity data distribution. The method is tested with problems in solid and fluid mechanics. By utilizing only a small fraction of high\u2010fidelity data, the accuracy of a large set of low\u2010fidelity data is significantly improved.",
    "authors": [
        {
            "affiliations": [],
            "name": "Orazio Pinti"
        },
        {
            "affiliations": [],
            "name": "Assad A. Oberai"
        }
    ],
    "id": "SP:94fde60c0aa0268a8c886edbc72ccf1c05a02c08",
    "references": [
        {
            "authors": [
                "R.B. Gramacy",
                "H.K.H. Lee"
            ],
            "title": "Adaptive design and analysis of supercomputer experiments",
            "venue": "Technometrics 51,",
            "year": 2009
        },
        {
            "authors": [
                "D. Allaire",
                "K. Willcox"
            ],
            "title": "A mathematical and computational framework for multifidelity design and analysis with computer models",
            "venue": "Int. J. Uncertain. Quantif",
            "year": 2014
        },
        {
            "authors": [
                "L.W.T. Ng",
                "K.E. Willcox"
            ],
            "title": "Multifidelity approaches for optimization under uncertainty",
            "venue": "Int. J. Numer. Method Eng",
            "year": 2014
        },
        {
            "authors": [
                "B. Peherstorfer",
                "K. Willcox",
                "M. Gunzburger"
            ],
            "title": "Optimal model management for multifidelity Monte Carlo estimation",
            "venue": "SIAM J. Sci. Comput. 38, A3163\u2013A3194. https:// doi",
            "year": 2016
        },
        {
            "authors": [
                "J. Kaipio",
                "Somersalo",
                "E. Statistical inverse problems"
            ],
            "title": "Discretization, model reduction and inverse crimes",
            "venue": "J. Comput. Appl. Math. 198, 493\u2013504. https:// doi. org/ 10. 1016/j. cam. 2005. 09. 027",
            "year": 2007
        },
        {
            "authors": [
                "M. Giselle Fern\u00e1ndez-Godino",
                "C. Park",
                "N.H. Kim",
                "R.T. Haftka"
            ],
            "title": "Issues in deciding whether to use multifidelity surrogates",
            "venue": "AIAA J. 57,",
            "year": 2054
        },
        {
            "authors": [
                "A.I. Forrester",
                "A. S\u00f3bester",
                "A.J. Keane"
            ],
            "title": "Multi-fidelity optimization via surrogate modelling",
            "venue": "Proc. R. Soc. A Math. Phys. Eng. Sci",
            "year": 2007
        },
        {
            "authors": [
                "P. Perdikaris",
                "D. Venturi",
                "J.O. Royset",
                "G.E. Karniadakis"
            ],
            "title": "Multi-fidelity modelling via recursive co-kriging and Gaussian-Markov random fields",
            "venue": "Proc. R. Soc. A Math. Phys. Eng. Sci",
            "year": 2015
        },
        {
            "authors": [
                "C. Park",
                "R.T. Haftka",
                "N.H. Kim"
            ],
            "title": "Remarks on multi-fidelity surrogates",
            "venue": "Struct. Multidiscip. Optim",
            "year": 2017
        },
        {
            "authors": [
                "C. Durantin",
                "J. Rouxel",
                "D\u00e9sid\u00e9ri",
                "J.-A",
                "A. Gli\u00e8re"
            ],
            "title": "Multifidelity surrogate modeling based on radial basis functions",
            "venue": "Struct. Multidiscip. Optim",
            "year": 2017
        },
        {
            "authors": [
                "X. Song",
                "L. Lv",
                "W. Sun",
                "Zhang",
                "J. A radial basis function-based multi-fidelity surrogate model"
            ],
            "title": "Exploring correlation between high-fidelity and low-fidelity models",
            "venue": "Struct. Multidiscip. Optim. 60, 965\u2013981. https:// doi. org/ 10. 1007/ s0015801902248-0",
            "year": 2019
        },
        {
            "authors": [
                "Q Zhou"
            ],
            "title": "A variable fidelity information fusion method based on radial basis function",
            "venue": "Adv. Eng. Inform",
            "year": 1016
        },
        {
            "authors": [
                "S. Chakraborty"
            ],
            "title": "Transfer learning based multi-fidelity physics informed deep neural network",
            "venue": "J. Comput. Phys",
            "year": 2021
        },
        {
            "authors": [
                "S De"
            ],
            "title": "On transfer learning of neural networks using bi-fidelity data for uncertainty propagation",
            "venue": "Int. J. Uncertain. Quantif",
            "year": 2020
        },
        {
            "authors": [
                "M. Penwarden",
                "S. Zhe",
                "A. Narayan",
                "R.M. Kirby"
            ],
            "title": "Multifidelity modeling for physics-informed neural networks (PINNs)",
            "venue": "J. Comput",
            "year": 2022
        },
        {
            "authors": [
                "X. Meng",
                "Karniadakis",
                "G.E. A composite neural network that learns from multi-fidelity data"
            ],
            "title": "Application to function approximation and inverse PDE problems",
            "venue": "J. Comput. Phys. 401, 109020. https:// doi. org/ 10. 1016/j. jcp. 2019. 109020",
            "year": 2020
        },
        {
            "authors": [
                "X. Meng",
                "H. Babaee",
                "Karniadakis",
                "G.E. Multi-fidelity Bayesian neural networks"
            ],
            "title": "Algorithms and applications",
            "venue": "J. Comput. Phys. 438, 110361. https:// doi. org/ 10. 1016/j. jcp. 2021. 110361",
            "year": 2021
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "Karniadakis",
                "G. Physics-informed neural networks"
            ],
            "title": "A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Comput. Phys. 378, 686\u2013707. https:// doi. org/ 10. 1016/j. jcp. 2018. 10. 045",
            "year": 2019
        },
        {
            "authors": [
                "S. Li",
                "W. Xing",
                "R. Kirby",
                "S. Zhe"
            ],
            "title": "Multi-fidelity Bayesian optimization via deep neural networks",
            "venue": "Adv. Neural. Inf. Process. Syst",
            "year": 2020
        },
        {
            "authors": [
                "N. Geneva",
                "N. Zabaras"
            ],
            "title": "Multi-fidelity generative deep learning turbulent flows",
            "venue": "Found. Data Sci",
            "year": 2020
        },
        {
            "authors": [
                "C. Perron",
                "D. Rajaram",
                "D. Mavris"
            ],
            "title": "Development of a multi-fidelity reduced-order model based on manifold alignment",
            "venue": "In AIAA Aviation 2020 Forum 3124. https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "A. Narayan",
                "C. Gittelson",
                "D. Xiu"
            ],
            "title": "A stochastic collocation algorithm with multifidelity models",
            "venue": "SIAM J. Sci. Comput. 36,",
            "year": 2014
        },
        {
            "authors": [
                "V. Keshavarzzadeh",
                "R.M. Kirby",
                "A. Narayan"
            ],
            "title": "Parametric topology optimization with multiresolution finite element models",
            "venue": "Int. J. Numer. Methods Eng",
            "year": 2019
        },
        {
            "authors": [
                "O. Pinti",
                "A.A. Oberai",
                "R. Healy",
                "R.J. Niemiec",
                "F. Gandhi"
            ],
            "title": "Multi-fidelity approach to predicting multi-rotor aerodynamic interactions",
            "venue": "AIAA J",
            "year": 2022
        },
        {
            "authors": [
                "A.L. Bertozzi",
                "X. Luo",
                "A.M. Stuart",
                "K.C. Zygalakis"
            ],
            "title": "Uncertainty quantification in graph-based classification of high dimensional data",
            "venue": "SIAM/ASA J. Uncertain. Quantif",
            "year": 2018
        },
        {
            "authors": [
                "D. Slepcev",
                "M. Thorpe"
            ],
            "title": "Analysis of p-Laplacian regularization in semisupervised learning",
            "venue": "SIAM J. Math. Anal",
            "year": 2019
        },
        {
            "authors": [
                "M. Belkin",
                "I. Matveeva",
                "P. Niyogi"
            ],
            "title": "Regularization and semi-supervised learning on large graphs",
            "venue": "In International Conference on Computational Learning Theory 624\u2013638. https:// doi",
            "year": 2004
        },
        {
            "authors": [
                "A.L. Bertozzi",
                "B. Hosseini",
                "H. Li",
                "K. Miller",
                "A.M. Stuart"
            ],
            "title": "Posterior consistency of semi-supervised regression on graphs",
            "venue": "Inverse Prob",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Dunlop",
                "D. Slep\u010dev",
                "A.M. Stuart",
                "M. Thorpe"
            ],
            "title": "Large data and zero noise limits of graph-based semi-supervised learning algorithms",
            "venue": "Appl. Comput. Harmon. Anal",
            "year": 2020
        },
        {
            "authors": [
                "F. Hoffmann",
                "B. Hosseini",
                "Z. Ren",
                "Stuart",
                "A.M. Consistency of semi-supervised learning algorithms on graphs"
            ],
            "title": "Probit and one-hot methods",
            "venue": "J. Mach. Learn. Res. 21, 1\u201355",
            "year": 2020
        },
        {
            "authors": [
                "N. Garc\u00eda Trillos",
                "D. Slep\u010dev"
            ],
            "title": "A variational approach to the consistency of spectral clustering",
            "venue": "Appl. Comput. Harmonic Anal",
            "year": 1016
        },
        {
            "authors": [
                "J.S. Gray",
                "J.T. Hwang",
                "Martins",
                "J.R.R.A.",
                "K.T. Moore",
                "Naylor",
                "B.A. OpenMDAO"
            ],
            "title": "An open-source framework for multidisciplinary design, analysis, and optimization",
            "venue": "Struct. Multidiscip. Optim. 59, 1075\u20131104. https:// doi. org/ 10. 1007/ s00158019- 02211-z",
            "year": 2019
        },
        {
            "authors": [
                "A. Sarvazyan",
                "Egorov",
                "V. Mechanical imaging\u2014A technology for 3-d visualization",
                "characterization of soft tissue abnormalities"
            ],
            "title": "A review",
            "venue": "Curr. Med. Imaging 8, 64\u201373. https:// doi. org/ 10. 2174/ 15734 05127 99220 571",
            "year": 2012
        },
        {
            "authors": [
                "P.E. Barbone",
                "A.A. Oberai"
            ],
            "title": "A review of the mathematical and computational foundations of biomechanical imaging",
            "venue": "Comput. Model. Biomech",
            "year": 2010
        },
        {
            "authors": [
                "Drela",
                "M. Xfoil"
            ],
            "title": "An analysis and design system for low reynolds number airfoils",
            "venue": "In Low Reynolds Number Aerodynamics (ed. Mueller, T. J.) 1\u201312",
            "year": 1989
        },
        {
            "authors": [
                "H.B. Squire",
                "A.D. Young"
            ],
            "title": "The calculation of the profile drag of aerofoils",
            "venue": "In Aeronautical Research Committee, Repts. and Memoranda,",
            "year": 1938
        },
        {
            "authors": [
                "F. Menter"
            ],
            "title": "Zonal two equation k-w turbulence models for aerodynamic flows",
            "venue": "In 23rd Fluid Dynamics, Plasmadynamics, and Lasers Conference",
            "year": 1993
        },
        {
            "authors": [
                "Chen",
                "P.-Y.",
                "B. Zhang",
                "Hasan",
                "M.A. Incremental eigenpair computation for graph Laplacian matrices"
            ],
            "title": "Theory and applications",
            "venue": "So. Netw. Anal. Min. 8, 4. https:// doi. org/ 10. 1007/ s13278- 017- 0481-y",
            "year": 2018
        },
        {
            "authors": [
                "J.W. Demmel"
            ],
            "title": "Iterative methods for eigenvalue problems",
            "venue": "In Applied Numerical Linear Algebra,",
            "year": 1997
        },
        {
            "authors": [
                "H. Ma",
                "J. Li"
            ],
            "title": "A true O(\\ log\\ ) algorithm for the all-k-nearest-neighbors problem",
            "venue": "In Combinatorial Optimization and Applications (eds Li, Y. et al.)",
            "year": 2019
        },
        {
            "authors": [
                "M. Belkin",
                "P. Niyogi"
            ],
            "title": "Convergence of Laplacian eigenmaps",
            "venue": "Adv. Neural Inf. Process. Syst. 19,",
            "year": 2006
        },
        {
            "authors": [
                "N.G. Trillos",
                "M. Gerlach",
                "M. Hein",
                "D. Slepcev"
            ],
            "title": "Error estimates for spectral convergence of the graph Laplacian on random geometric graphs towards the laplace-beltrami operator",
            "venue": "Found. Comput. Math",
            "year": 1909
        },
        {
            "authors": [
                "S. Guattery",
                "G.L. Miller"
            ],
            "title": "Graph embeddings and Laplacian eigenvalues",
            "venue": "SIAM J. Matrix Anal. Appl",
            "year": 2000
        },
        {
            "authors": [
                "U. Von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Stat. Comput",
            "year": 2007
        },
        {
            "authors": [
                "M. Belkin",
                "P. Niyogi"
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "Adv. Neural. Inf. Process. Syst",
            "year": 2001
        },
        {
            "authors": [
                "L. Zelnik-Manor",
                "P. Perona"
            ],
            "title": "Self-tuning spectral clustering",
            "venue": "Adv. Neural Inf. Process. Syst. 17,",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nwww.nature.com/scientificreports"
        },
        {
            "heading": "Graph Laplacian\u2011based spectral",
            "text": "multi\u2011fidelity modeling"
        },
        {
            "heading": "Orazio Pinti * & Assad A. Oberai",
            "text": "Low\u2011fidelity data is typically inexpensive to generate but inaccurate, whereas high\u2011fidelity data is accurate but expensive. To address this, multi\u2011fidelity methods use a small set of high\u2011fidelity data to enhance the accuracy of a large set of low\u2011fidelity data. In the approach described in this paper, this is accomplished by constructing a graph Laplacian from the low\u2011fidelity data and computing its low\u2011lying spectrum. This is used to cluster the data and identify points closest to the cluster centroids, where high\u2011fidelity data is acquired. Thereafter, a transformation that maps every low\u2011fidelity data point to a multi\u2011fidelity counterpart is determined by minimizing the discrepancy between the multi\u2011 and high\u2011fidelity data while preserving the underlying structure of the low\u2011fidelity data distribution. The method is tested with problems in solid and fluid mechanics. By utilizing only a small fraction of high\u2011fidelity data, the accuracy of a large set of low\u2011fidelity data is significantly improved.\nMulti-fidelity methods have been widely used in different areas of science and engineering, including optimization1,2, uncertainty quantification3, uncertainty propagation4,5, and statistical inference6 (see Fern\u00e1ndezGodino et\u00a0al.7 and Peherstofer et\u00a0al.5 for two comprehensive reviews). The fundamental idea behind these methods is to combine a large amount of low-fidelity data, which is relatively inexpensive to compute or measure, with a much smaller set of high-fidelity data acquired at higher cost. The objective of multi-fidelity modeling is to improve the accuracy of the large low-fidelity data set by utilizing the small set of high-fidelity data.\nIn a typical multi-fidelity framework, low-fidelity data is acquired to obtain an approximation of the response of the system. Then, a limited number of high-fidelity data points are computed or measured. Finally, techniques that learn the response from the low-fidelity data, and improve it by using the high-fidelity data are applied. Co-kriging methods have been extensively investigated in this context8\u201311, where the multi-fidelity response is expressed as a weighted sum of two Gaussian processes, one modeling the low-fidelity data, and the other representing the discrepancy between the low- and high-fidelity data. The parameters of the mean and correlation functions of these processes are determined by maximizing the log-likelihood of the available data.\nOther methods make use of radial basis functions (RBFs) to model the low-fidelity response. Specifically, the low-fidelity surrogate is written as an expansion in terms of a set of RBFs, and the coefficients are determined by interpolating the available low-fidelity data. The multi-fidelity approximation is then obtained in different ways. These include determining a scaling factor and a discrepancy function, which can be modeled using a kriging surrogate11, or another expansion in terms of RBFs12,13. In some cases the multi-fidelity surrogate is constructed by mapping the low-fidelity response directly to the high-fidelity response14.\nMore recently, deep neural networks have been used to fit low-fidelity data and learn the complex map between the input and output vectors in the low-fidelity model. Then, the relatively small amount of high-fidelity data is used in combination with techniques such as transfer learning15,16, embedding the knowledge of a physical law through physics-informed loss functions17\u201320, or, in the case of multiple levels of fidelity, concatenating multiple neural networks together21. An approach that involves training a physics-constrained generative model, conditioned on the low-fidelity snapshots, to produce solutions that are higher-fidelity and higher-resolution has also been proposed22.\nAnother class of methods, suitable when the response of the system consists of a high-dimensional vector, first performs order-reduction using the low-fidelity data, and then inject accuracy using the high-fidelity data in a reduced-dimensional latent space. This has been accomplished by computing the low- and high-fidelity proper orthogonal decomposition (POD) manifolds, aligning them with each other, and replacing the low-fidelity POD modes with their high-fidelity counterparts23. This has also been done by first solving a subset selection problem to construct a surrogate model of the low-fidelity response in terms of a few important snapshots, then generating their high-fidelity counterparts, and finally using these in the multi-fidelity surrogate model24\u201326.\nIn contrast to the methods described above, the approach developed in this manuscript relies on the spectral properties of the graph Laplacian constructed from the low-fidelity data. It uses these properties to determine the\nOPEN"
        },
        {
            "heading": "Aerospace and Mechanical Engineering Department, University of Southern California, Los Angeles 90007, USA.",
            "text": "*email: pinti@usc.edu\n2 Vol:.(1234567890) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\npoints at which high-fidelity data ought to be acquired, and to embed the structure of the low-fidelity data into the multi-fidelity model. It also differs from most co-kriging and RBFs-based methods in how it treats outputs with multiple quantities of interest. While most methods tend to ignore the joint distribution of these quantities, the proposed method explicitly utilizes it in constructing the multi-fidelity response. The proposed approach also has strong connections with semi-supervised classification algorithms on graphs27\u201331, and relies on theoretical results on consistency of graph-based methods in the limit of infinite data32,33."
        },
        {
            "heading": "Overview of the methodology",
            "text": "Given a parametric physical problem, we denote by \u00b5 \u2208 RP the vector of input parameters, and by q\u0304(\u00b5) \u2208 RQ and q(\u00b5) \u2208 RQ the low- and high-fidelity output vectors, respectively. These vectors represent a set of Q output quantities of interest of the problem. Low- and high-fidelity data points, denoted by u\u0304 \u2208 RD and u \u2208 RD , respectively, with Q \u2264 D \u2264 Q + P , are constructed from components of input parameters and output quantities. That is, u\u0304 = R(\u00b5, q\u0304) and u = R(\u00b5, q) , where R is a restriction operator that extracts the appropriate components of the input parameters and output quantities of interest. The choice of R is problem dependent and is described in Supplementary Appendix H. Two obvious choices are when the data comprises all input and output components, that is, R(\u00b5, q) = [\u00b5, q] , and when it comprises only of the output components, that is, R(\u00b5, q) = q.\nThe multi-fidelity method combines a dense set of low-fidelity data points with a few, select, high-fidelity points to generate a dense set of multi-fidelity points. The steps to accomplish this are outlined in Fig.\u00a01 in the context of a two-dimensional illustrative example. Specifically, a large set of low-fidelity data {u\u0304i}N\u0304i=1 is generated for different instances of input parameters {\u00b5i}N\u0304i=1 . These data points are treated as the nodes of a weighted graph with weight matrix W \u2208 RN\u0304\u00d7N\u0304 in the normalized data coordinates, and a normalized graph Laplacian L = D\u22121/2(D \u2212W)D\u22121/2 is evaluated. Here D is a diagonal matrix whose entries are the row-sum of the weight matrix W . The low-lying eigen-spectrum ( i , \u03c6i), i = 1, . . . , K , of the graph Laplacian is computed and used to cluster the low-fidelity data by employing K-means clustering in the eigenfunction coordinates. Data points that are closest to the centroids of the clusters are determined (marked as blue dots in Fig.\u00a01) and their high-fidelity counterparts {ui}N\u0304i=1 , N \u226a N\u0304 , are evaluated. The low-lying eigenfunctions of the graph Laplacian are used once again to construct a set of influence functions \u03c8 (i) , i = 1, . . . , N , which are expressed as linear combination of the eigenfunctions followed by a soft-max transform. The mapping from low- to multi-fidelity data points is written as a linear combination of the influence functions times the displacement vector that maps each low-fidelity point to its high-fidelity counterpart. The coefficients linking the influence functions to the eigenfunctions are determined by solving a convex minimization problem which minimizes the distance between the transformed low-fidelity points and their high-fidelity counterparts while penalizing the use of eigenfunctions with large\nFigure\u00a01. Workflow for the spectral multi-fidelity (SpecMF) method applied to an illustrative problem. (a) Generate low-fidelity data. (b) Compute a graph Laplacian using the low-fidelity data. (c) Compute the eigen-decomposition of the graph Laplacian. (d) Perform spectral clustering of the low-fidelity data and find the points closest to the clusters centroids. (e) Acquire high-fidelity data only for these points. (f) Solve a convex minimization problem to find one influence function for each point with a high-fidelity counterpart. The influence functions are constructed from the low-lying eigenfunctions of graph Laplacian. (g) Generate the multi-fidelity approximation of the data set. (h) For this illustrative example, this is compared with the corresponding high-fidelity data set.\n3 Vol.:(0123456789) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\neigenvalues. This penalty term preserves the structure found in the low-fidelity data. This overall approach is referred to as Spectral Multi-Fidelity (SpecMF) method, since the spectral properties of the graph Laplacian are utilized to embed the structure of the low-fidelity data into the multi-fidelity model.\nThe proposed method is endowed with some desirable theoretical properties. These are described below and the results are derived in Supplementary Appendices A, B, and C.\nProperty 1 An explicit expression for the gradient of the loss function with respect to the optimization parameters can be computed, lowering the computational cost of the algorithm.\nProperty 2 In the limit of a small data misfit term (which happens as the optimization iterations converge), the Hessian of the loss function is positive definite. This proof is based on recognizing that (i) the data misfit term is in the form of a least-squares residual and (ii) the regularization term is a positive-definite quadratic form. This ensures that the resulting optimization problem is solved easily.\nProperty 3 Under the assumptions (a) the low-fidelity data is partitioned into M clusters, (b) the high-fidelity data differs from the low-fidelity data by distinct rigid translations applied to each cluster, and (c) the high-fidelity version of one point per cluster is known, the proposed approach permits a transformation that maps each low-fidelity point to the true high-fidelity point in the limit of infinite low-fidelity data and as the regularization parameter tends to zero. That is, the multi-fidelity data set converges to its high-fidelity counterpart. This is a consistency result that demonstrates the proposed method can solve this special problem exactly."
        },
        {
            "heading": "Results",
            "text": "In what follows, we apply the proposed method to two numerical problems. The first is an application to linear elasticity, where the dimension of the input parameters space and the data space are P = 5 and D = 5 , respectively. The second is a fluid dynamics problem, with P = 5 and D = 3 . Furthermore, for the elasticity problem, we also apply the SpecMF method to an entire field discretized on a grid with 100 points, so that D = 100 . These problems were solved on a Apple M1 Pro processor with 8 cores, and the computational time for a fixed value of regularization parameter is around 1\u20132 min.\nWe compare our results with a co-kriging method8,9 applied to each output quantity of interest individually, i.e. qk = qk(\u00b5), k = 1, . . . , Q . In co-kriging, the multi-fidelity approximation Zk(\u00b5) is constructed as a weighted sum of two Gaussian processes, Zk(\u00b5) = \u03b3 Z\u0304k(\u00b5)+ Zdk (\u00b5) , where Z\u0304k models the low-fidelity data {(\u00b5 i , q\u0304ik)} N\u0304 i=1 , \u03b3 is a scaling factor, and Zdk models the discrepancy between the high-fidelity data {(\u00b5 i , qik)} N i=1 and \u03b3 Z\u0304k . For both these Gaussian processes the covariance function is an anisotropic Gaussian kernel, where different length scales are used for each coordinate of the input parameter space:\nThe hyper-parameters of these kernels, such as \u03c3\u0304k , \u03c3k , l\u0304k, p, lk, p , the scaling factor \u03b3 and the mean values of Z\u0304k and Zdk , are computed by maximizing the log-likelihood of the data\n9. In our experience, this optimization problem could be very sensitive to the initial guess and the prescribed bounds in the search space. We addressed this issue by utilizing a grid search\u00a0for the\u00a0initial guesses to arrive at the best results. All computations were performed using the open-source computing platform OpenMDAO34.\nTraction on a soft material with a stiff inclusion"
        },
        {
            "heading": "Problem description",
            "text": "We examine a problem of linear elasticity which involves a soft square sheet in plane stress with an internal stiffer elliptic inclusion. The length of the edge of the square is L = 10 cm , its Young\u2019s modulus is E = 1kPa , and the Young\u2019s modulus of the inclusion is Ei = 4E . Both the body and the inclusion are incompressible. The bottom edge of the square is fixed, and a uniform downward displacement v0 = \u22125mm is applied to the top edge. The top edge is traction free in the horizontal direction while the vertical edges are traction-free in both directions (Fig.\u00a02a). We wish to predict attributes of the vertical traction field on the upper edge as a function of the inclusion shape, orientation and location. This problem is motivated by the need to identify stiff tumors within a soft background tissue, which is particularly relevant to detecting and diagnosing breast cancer tumors35,36."
        },
        {
            "heading": "Parameters and quantities of interest",
            "text": "The input parameters of the problem are the coordinates of the center of the elliptical inclusion (xc , yc) , its orientation \u03b8 , and its major and minor semi-axes a and b (see Fig.\u00a02a). The minimum and maximum values for these parameters are reported in Table\u00a01. The output quantities of interest include the values of the localized\n(1)cov(Z\u0304k(\u00b5i), Z\u0304k(\u00b5j)) = \u03c3\u0304 2k exp ( \u2212 P \u2211\np=1\n(\u00b5ip \u2212 \u00b5 j p) 2\nl\u03042k, p\n)\n,\n(2)cov(Zdk (\u00b5 i), Zdk (\u00b5 j)) = \u03c3 2k exp ( \u2212 P \u2211\np=1\n(\u00b5ip \u2212 \u00b5 j p) 2\nl2k, p\n)\n.\n4 Vol:.(1234567890) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nvertical forces on the top edge. These are determined by dividing the top edge into M = 4 sections of equal length and integrating the vertical traction \u03c3yy over each section. This results in M values of localized forces fi , i = 1, . . . ,M (see Fig.\u00a02a),\nIn addition to these forces, the maximum value of traction on the top edge is included as a quantity of interest. Therefore, the M + 1 quantities of interest are qi = fi , i = 1, . . . , M , and qM+1 = maxx |\u03c3yy(x, L)| . As the location, orientation and size of the inclusion is varied, the traction field on the top surface changes, which in turn changes the M components of the localized forces, and the maximum value of traction.\nWe consider the case where the data space is constructed only from the output vector, that is u(\u00b5) = q(\u00b5) . The case of including the input vector in the data space, that is u(\u00b5) = [\u00b5, q(\u00b5)] , yields comparable results and is described in Supplementary Appendix H."
        },
        {
            "heading": "Low\u2011 and high\u2011fidelity models",
            "text": "We employ two finite element-based models that differ in the number of elements of the mesh. The low-fidelity model uses a coarse mesh with around 400 elements, whereas the high-fidelity model has a fine mesh with around 25,000 elements. It is verified that the high-fidelity model produces a solution that is mesh-converged.\n(3)fi =\ni LM \u222b\n(i\u22121) LM\n\u03c3yy(x, L)dx, i = 1, . . . , M.\nFigure\u00a02. (a) Schematic of the soft body (light grey) with the elliptic stiffer inclusion (dark grey). The square is compressed on top with a uniform displacement v = v0 , while the bottom is fixed. The vertical traction is integrated over the top side across equal sections to compute the localized forces fi . (b) Schematic of the airfoil with the input parameters.\n5 Vol.:(0123456789) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1"
        },
        {
            "heading": "Numerical results",
            "text": "We generate N\u0304 = 1120 samples of the input parameters by treating each parameter as an independent random variable that is uniformly distributed within its range. For each instance, we generate the low- and high-fidelity solutions. We use N = 30 high-fidelity data points to construct the multi-fidelity results, and the remainder for testing the performance of method. We observe that the low-fidelity traction field captures the correct trend, but tends to underestimate the true magnitude (see Supplementary Appendix F).\nTo visualize the five-dimensional data set, we project the data points in the (f1, f2) and (f2, f3) planes. In the first and fourth columns of Fig.\u00a03a, we show a comparison between the scaled low- and high-fidelity data sets. We notice that low-fidelity data captures the structure of the high-fidelity data, however has a smaller spread.\nThe graph Laplacian is constructed from the low-fidelity data points; its low-lying eigenfunctions are shown in Fig.\u00a03b in the (f2, f3) plane. We observe that the eigenfunctions localize different regions of the low-fidelity data. Thereafter, we determine the coordinates of each low-fidelity data point in the eigenfunctions space and then perform K-means clustering to find the points closest to the centroids of N = 30 clusters. These points are shown in blue in leftmost column of Fig.\u00a03a, and correspond to the points where we utilize the high-fidelity data. We observe that these points appear to be evenly distributed over the span of the low-fidelity data. Next, we determine the influence functions for the multi-fidelity approximation by solving the minimization problem described in (17). The selection of the hyper-parameters is described in Supplementary Appendix F. A subset of the influence functions and the corresponding data points are shown in Fig.\u00a03c. We observe that all the influence functions peak at their respective data point and vanish away from it.\nThe final multi-fidelity SpecMF approximation, which is generated via (14), is shown in third column of Fig.\u00a03a. The results of the co-kriging method are also shown in the second column of the same figure. We observe that the SpecMF approximation appears to stretch the low-fidelity data distribution to make it closer to the high-fidelity distribution. In contrast, the co-kriging method appears to have distorted the underlying structure of the data.\nTo quantify the error in the SpecMF data wi , we compute the relative absolute difference with respect to the high-fidelity data ui at each point i and for every component k,\nHere E(\u00b7) denotes the average over all validation points i, and Nval is the number of validation points (in this case, Nval = N\u0304 \u2212 N ). Similar errors are computed for the low-fidelity data and the co-kriging based multifidelity approximation. The histograms of the errors for the SpecMF and low-fidelity data are shown Fig.\u00a03d for each output component. In each case we observe that the error distribution for the SpecMF data is more closely centered around zero, and presents a much smaller spread with respect to the distribution of the low-fidelity errors. The mean value of these errors is reported in Table\u00a02 for each data component. We observe that the error for the low-fidelity data ranges between 5 and 10%. Co-kriging method reduces this error, but the SpecMF method is 1.5\u20132 times more accurate, improving the accuracy of the low-fidelity data by factor of 5\u20139 times."
        },
        {
            "heading": "Multi\u2011fidelity model for the entire traction field",
            "text": "To test the method with a higher dimensional data space, we consider a modified version of the traction problem. Instead of computing the four localized forces and the maximum value of the traction on the top surface, that is Q = D = 5 , we use the entire traction field discretized over 100 points. Thus the dimension of the output and data vectors is Q = D = 100 , and uj = \u03c3yy ( x j 99 , L )\n, j = 0, . . . , 99 . To generate the multi-fidelity approximations for this field, we use the same number of high-fidelity simulations as the previous case, that is N = 30 . The results are shown in Fig.\u00a04. In Fig.\u00a04a we show the low-, high-, and multi-fidelity traction fields for four test cases, where it is clearly seen that the multi-fidelity solution is significantly more accurate that its low-fidelity counterpart. In Fig.\u00a04b we have plotted the histograms of the errors ei , defined in an analogous way for both the SpecMF and low-fidelity data as,\nwhere the index i denotes the i-th validation sample, || \u00b7 ||2 is the l2 norm, and E(\u00b7) denotes the mean over all test samples. We observe that the error distribution for the multi-fidelity data is closer to zero, and that there is almost no overlap between the low- and multi-fidelity error distributions. The mean error for the low-fidelity data is around 7% whereas for the multi-fidelity data it is around 1.5%. These values are also reported in Table\u00a02. We note that this example demonstrates that the proposed method can be employed for constructing multi-fidelity approximations of fields, with large values of D.\nAerodynamic coefficients for a family of NACA airfoils"
        },
        {
            "heading": "Problem description",
            "text": "The multi-fidelity approach is used to tackle a problem in aerodynamics where the goal is to predict the lift, drag and pitching moment coefficients for a family of airfoils operating at different conditions. We consider the 4-digit NACA airfoils, whose shape is defined by three geometric parameters, and investigate how the aerodynamic performance of these airfoils changes at different Reynolds numbers and angles of attack.\n(4)eik = |wik \u2212 u i k|\nE(|uk|) \u00d7 100%, i = 1, . . . , Nval , k = 1, . . . , D.\n(5)ei = ||wi \u2212 ui||2 E(||ui||2|) \u00d7 100%, i = 1, . . . , Nval ,\n6 Vol:.(1234567890) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1"
        },
        {
            "heading": "Parameters and quantities of interest",
            "text": "The parameters of the problem comprise both design and operating condition variables. They are the maximum camber of the airfoil \u03b7 , the distance of the maximum camber from the leading edge x\u03b7 , the thickness of the airfoil t, the angle of attack \u03b1 , and the Reynolds number Re = Uc\n\u03bd (see Fig.\u00a02b). Here, U is the flow speed, c is the\nchord of the airfoil, and \u03bd is the kinematic viscosity of the fluid. In our analysis, the Reynolds number is varied by changing the flow speed U. The range of each parameter is reported in Table\u00a01.\nFigure\u00a03. Results for the traction problem. (a) Low-fidelity, co-kriging, SpecMF and high-fidelity data. Lowfidelity data points are shown together with the points closest to the centroids of the clusters (in blue). (b) Eight eigenfunctions from the low-lying spectrum projected onto the (f2, f3) plane. (c) Influence functions for eight control points projected onto the (f2, f3) plane. (d) Error distribution for the low-fidelity and SpecMF model for each output component.\n7 Vol.:(0123456789) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nThe quantities of interest are the aerodynamic coefficients CL, CD , and CM , defined as,\nwhere L, D and M are the lift, drag and the pitching moment about a point located at quarter chord from the leading edge, respectively, and \u03c1 is the density of the fluid. Hence, the vector of quantities of interest is q(\u00b5) = [CL, CD , CM ] , with input parameters \u00b5 = [\u03b7, x\u03b7 , t, \u03b1, Re] . The data space is formed by the output quantities of interest only, i.e. u(\u00b5) = q(\u00b5) . A different case, where the Reynolds number is included in the data space, is analyzed in Supplementary Appendix H."
        },
        {
            "heading": "Low\u2011 and high\u2011fidelity models",
            "text": "The low-fidelity data is generated using XFOIL37, a code based on the vortex panel method for the analysis of subsonic airfoils. In this case, the lift and moment coefficients are calculated by direct integration of surface pressure, whereas the drag is recovered by applying the Squire\u2013Young formula38. To generate the low-fidelity results, the surface of each airfoil is discretized with 40 panels, and the Reynolds and Mach numbers are set by using a kinematic viscosity of \u03bd = 10\u22125 m2 s\u22121 and speed of sound cs = 340 m s\u22121.\nHigh-fidelity results are generated via 2D Reynolds-averaged Navier\u2013Stokes (RANS) simulations with a SST k \u2212 \u03c9 turbulence model39 using OpenFOAM. The computational domain is a cuboid of dimension 1000c \u00d7 1000c \u00d7 c . A hybrid mesh is employed, comprising a C-grid structured mesh in the proximity of the airfoil of size 4c \u00d7 6c , and an unstructured mesh in the rest of the domain (see Supplementary Appendix G). The number of finite volumes in the mesh varies between 100,000 and 800,000, depending on the Reynolds number. At the outer boundary, Dirichlet boundary conditions for both velocity and pressure are prescribed, while on the airfoil surface a no-slip condition for velocity and zero-gradient condition for pressure are applied. The turbulence intensity of the flow at the outer boundary is set to 2%."
        },
        {
            "heading": "Numerical results",
            "text": "We sample N\u0304 = 5400 instances of the input parameter vector from a multivariate uniform distribution and employ XFOIL to generate the set of low-fidelity data (shown in the first column of Fig.\u00a05a in three independent\n(6)CL = L\n1 2\u03c1U\n2c , CD = D 1 2\u03c1U 2c , CM = M 1 2\u03c1U 2c2 ,\nError [%] Soft body with inclusion Airfoil\nQuantity of interest f1 f2 f3 f4 \u03c3maxyy \u03c3yy CL CD CM Low-fidelity 4.48 7.15 7.21 4.65 10.19 6.95 40.40 28.84 216.96\nCo-kriging 1.04 1.95 1.91 0.90 2.94 \u2013 30.21 34.33 57.44\nSpecMF 0.52 1.2 1.06 0.51 2.0 1.52 18.56 10.77 45.95\nImprovement factor (SpecMF) 8.60 5.95 6.8 9.16 5.09 4.58 2.18 2.68 4.73\nFigure\u00a04. Results for the entire traction field problem. (a) Low-, high-, and multi-fidelity solutions for four test cases. (b) Error distribution for the low-fidelity and SpecMF solutions.\n8 Vol:.(1234567890) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nplanes). Then, we construct the graph Laplacian and compute its eigendecomposition. The first three nontrivial eigenfunctions are shown in Fig.\u00a05c in the normalized 3-dimensional data space. We embed the lowfidelity data points in the eigenfunction space, and use K-means clustering to find N = 70 clusters and locate the points closest to their centroids (shown as blue dots in the first column of Fig.\u00a05a). Thereafter, we run CFD simulations to acquire the high-fidelity data at these points. We run Nval = 400 additional high-fidelity simulations, corresponding to randomly selected low-fidelity data points, to be used as a validation set to quantify the performance of the multi-fidelity models. These data points are shown in the fourth column of Fig.\u00a05a.\nWe solve the minimization problem (17) to determine the transformation (14) which yields the SpecMF data points. The hyper-parameters for this problem are reported in Supplementary Appendix G. Three typical influence functions plotted over the low-fidelity data set are shown in Fig.\u00a05d. The resulting multi-fidelity data points are plotted in the third column of Fig.\u00a05a. The results obtained using co-kriging are plotted in the second column of this figure. When compared to both the low-fidelity and co-kriging data sets, the SpecMF points appear to better represent the structure observed in the high-fidelity data points. This is accomplished by a significant upward shift in the low-fidelity values for the moment coefficient, and a compression in the lift versus drag plane.\nThe average error for the the low- and multi-fidelity models, as defined in (4), is reported in Table\u00a02. For the SpecMF method, the average error for lift and drag coefficients drops by a factor greater than two, while for the pitching moment it drops more than four times. The co-kriging method reduces the error for the lift and pitching moment only, and not as effectively. The distribution of errors for the low-fidelity and SpecMF data is plotted in Fig.\u00a05d. Once again we observe that the distribution of the error for the SpecMF data is centered closer to zero and displays a narrower spread."
        },
        {
            "heading": "Discussion",
            "text": "We have proposed a multi-fidelity approach to predict the response of a system when two mechanisms of generating data of different fidelity and cost are available. The method includes three steps: (i) acquire a large number of low-fidelity data, (ii) identify and acquire a small number of key high-fidelity data, and (iii) use the high-fidelity data to improve the accuracy of all low-fidelity data. This is accomplished by constructing an undirected, complete graph from the low-fidelity data and computing its graph Laplacian. The low-lying spectrum of the graph Lalpacian is then used to cluster the low-fidelity data and to determine points closest to the centroids of the clusters. Thereafter, high-fidelity data is acquired only for these select points. This data, along with the spectral decomposition of the graph Laplacian, is used to solve a minimization problem which yields a transformation that maps each low-fidelity data point to new multi-fidelity coordinates. It is shown that this minimzation problem is convex. In numerical experiments, the approach yields multi-fidelity data that is significantly more accurate that its low-fidelity counterpart. In particular, in a problem motivated by biomechanics, this approach improves the accuracy of 1120 low-fidelity data points by a factor of 5\u20139 (depending on the quantity of interest) by only relying on 30 high-fidelity simulations (less than 3% of the low-fidelity simulations). Similarly, in a problem of aerodynamics, it improves the accuracy of 5400 low-fidelity data points by a factor of 2\u20134 while only using 70 high-fidelity simulations (1.3% of the low-fidelity simulations). The computational cost of the method scales as O (D \u00afN2 + N \u00afN) , where D is the dimension of the data space, and N\u0304 and N are the number of low- and high-fidelity data points, respectively. The process of constructing constructing the adjacency matrix involves computing O ( \u00afN2) dot products of the differences between D-dimensional vectors, and therefore scales as O (D \u00afN2) . Further, the cost of computing the low-lying spectrum of the graph Laplacian using iterative methods scales as O ( \u00afN2 + N \u00afN)40,41. However, both these costs can be reduced by setting a cutoff on the number of edges per node on the graph using, for example, an efficient implementation of the k\u2212nearest neighbors algorithm42. In this case, the cost of the specMF algorithm scales as O (knnD \u00afN log \u00afN + N \u00afN) , where knn is the cutoff on the maximum number of neighbors for every node.\nSome limitations and remarks of the proposed method, which also serve to delineate future directions for improvement, are discussed next. In its present form, the choice of the input parameters and output quantities to be included in the data space is somewhat arbitrary. However, it might be possible to develop certain problemdependent heuristics to identify the parameters and quantities that yield better performance.\nThe SpecMF method learns the data distribution from the low-fidelity model, and then adjusts it based on a few higher-fidelity data points. Thus, the underlying requirement is that the structure of the low- and highfidelity data does not change significantly. If the structure arising form the low-fidelity model is fundamentally inaccurate, the benefit from using the method will be limited. We also note that this is a common requirement among most multi-fidelity models. In Supplementary Appendix C, we prove the convergence of the method for the case where the low- and high-fidelity clusters differ by translations. This suggests that the method will perform well when the low- and high-fidelity distributions have the same topology but differ by well-behaved transformations. The graph Laplacian spectrum yields additional insights. A clear spectral gap signifies that the data are effectively clustered, and also provides a way of choosing a suitable number of high-fidelity runs.\nFinally, theoretical analysis of the performance of the method in the limit of a large number of high-fidelity data points will lead to a better understanding of its properties."
        },
        {
            "heading": "Methods",
            "text": ""
        },
        {
            "heading": "Background",
            "text": "A complete, weighted graph is a pair G = (V , W) , where V = {u1, . . . , uN } is a set of vertices (or nodes) embedded in RD , and W = [Wij] is an adjacency matrix. We consider adjacency matrices of the type\n(7)Wij = d(||ui \u2212 uj||2),\n9 Vol.:(0123456789) Scientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nwhere d(\u00b7) is a monotonically decreasing function and || \u00b7 ||2 is the l2 norm. We define the degree matrix D to be a diagonal matrix with\nFigure\u00a05. Results for the airfoil problem. (a) Left: low-fidelity data with points closest to cluster centroids (in blue). Center: multi-fidelity data points obtained with co-kriging and our multi-fidelity approach (SpecMF). Right: high-fidelity data points used for validation. (b) Error distribution for the low- and multi-fidelity data. (c) The first three non-trivial eigenfunctions. (d) Three typical influence functions.\n10\nVol:.(1234567890)\nScientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nand a family of graph Laplacians,\nDifferent choices of p and q result in different normalizations of the graph Laplacian43\u201345. The graph Laplacian can be used to perform spectral clustering, which amounts to finding an optimal partition of the graph using the spectral properties of L46,47. The eigenfunctions of L form the set of orthonormal functions from the nodes of the graph to the real numbers \u03c6(m) : V \u2192 R that solve the eigenvalue problem\nwith \u03c6(m) = [\u03c6(m)1 , . . . , \u03c6 (m) N ]T and \u03c6 (m) i = \u03c6(m)(ui) . For the un-normalized graph Laplacian ( p = q = 0 ), the eigenfunctions satisfy the following property,\nThis implies that eigenfunctions with small eigenvalues provide a mapping of the graph to a line that promotes the clustering of vertices that are strongly connected. Note that the eigenvalue problem (10) admits the trivial solution that maps all vertices to a point, e.g. \u03c6(1) = 1\u221a\nN [1, . . . , 1]T , and has a zero eigenvalue. The eigenfunction\ncorresponding to the smallest non-zero eigenvalue, also called Fiedler vector, represents the non-trivial solution to the problem of embedding the graph onto a line so that connected vertices stay as close as possible48. Similarly, the eigenfunctions corresponding to the k lowest non-zero eigenvalues, [\u03c6(2), . . . , \u03c6(k+1)] , represent the optimal embedding of the graph into Rk , where the coordinates of a vertex ui are given by \u03be i = [\u03c6 (2) i , . . . ,\u03c6 (k+1) i ]."
        },
        {
            "heading": "Spectral Multi\u2011Fidelity method (SpecMF)",
            "text": "The three steps to construct the multi-fidelity data are: (1) generate a dense collection of low-fidelity data points, (2) identify key input parameter values at which to acquire the more expensive high-fidelity data, and finally (3) combine the low- and high-fidelity data to construct a multi-fidelity model. Below, we describe each step in detail.\nStep 1: construction of the low\u2011fidelity graph We begin by sampling N\u0304 \u226b 1 points in the parameter space from a simple prescribed probability density to generate the set S\u0304 = {\u00b5i}N\u0304i=1 . Then, we generate the low-fidelity data points u\u0304i = u\u0304(\u00b5i), i = 1, . . . , N\u0304 , and collect them in the set D\u0304 = { u\u0304i }N\u0304\ni=1 . For uniformity, we scale them so that each component lies within [\u22121, 1] . We apply the same scaling factors to the high-fidelity data collected in Step 2.\nWe treat each data point as the vertex of an undirected, complete, weighted graph, and exploit the useful properties of the associated matrices. Hence, we construct a graph with u\u0304i as vertices, and with weights given by the entries of the adjacency matrix (7) where d(\u00b7) is chosen to be a Gaussian kernel,\nIn the equation above, \u03c3 is a characteristic scale. It can be treated as a hyperparameter, or its value can also be determined for each vertex by analyzing the statistics of its neighborhood49. From the adjacency matrix, we construct the diagonal degree matrix D and a graph Laplacian L , using (8) and (9), respectively. For the applications in this paper, we have employed a normalized graph Laplacian with p, q = 0.5 and determined \u03c3 using the approach described in Ref.49.\nStep 2: selection strategy Next, we describe the strategy for selecting N \u226a N\u0304 nodes for which high-fidelity data is acquired. These nodes are chosen to be close to the centroids of the clusters associated with the low-fidelity data D\u0304 . To find these, we compute the eigendecomposition of the graph Laplacian and leverage the properties of its low-lying spectrum. We embed each data point into the eigenfunctions space and apply a standard clustering algorithm (e.g. K-means) to determine the clusters and their centroids. This is accomplished by,\n1. Compute the low-lying eigenfunctions of the graph Laplacian, \u03c6(m), m = 1, . . . , K , with K = 3N . During the selection strategy step (Step 2) we use only the first N of these eigenfunctions. However, in the multifidelity transformation step (Step 3) we utilize all of the K eigenfunctions. 2. For every low-fidelity data point, u\u0304i , compute the coordinates in the eigenfunction space \u03be i \u2208 RN . These are given by \u03be i = [\u03c6(1)i , . . . ,\u03c6 (N) i ], i = 1, . . . , N\u0304. 3. Use K-means clustering on the points {\u03be i}N\u0304i=1 to find N clusters. 4. For each cluster, determine the centroid and the low-fidelity data point closest to it. 5. Re-index the low-fidelity data set D\u0304 and the parameters set S\u0304 so that the points identified above correspond\nto the first N points.\n(8)Dii = N \u2211\nj=1 Wij ,\n(9)L = D\u2212p(D \u2212W)D\u2212q.\n(10)L\u03c6(m) = m\u03c6(m),\n(11)\u03c6(m) \u00b7 L\u03c6(m) = m\n(12)= 1\n2\n\u2211\ni, j\nWij\n(\n\u03c6 (m) i \u2212 \u03c6 (m) j\n)2 .\n(13)d(r) \u2261 exp(\u2212r2/\u03c3 2).\n11\nVol.:(0123456789)\nScientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\n6. Acquire high-fidelity data at the parameter values corresponding to these points, and assemble the data set D = { ui }N\ni=1 , with u i = u(\u00b5i) . Note that the elements of D are the high-fidelity counterparts of the first N\nelements of D\u0304. 7. Scale the high-fidelity data with the same scaling factors used in Step 1 for the low-fidelity data.\nStep 3: multi\u2011fidelity transformation In this step we generate a multi-fidelity approximation { wi }N\u0304\ni=1 that learns the data distribution from the lowfidelity data set and uses the select high-fidelity data to transform this distribution. The proposed multi-fidelity approach seeks a transformation that maps every low-fidelity data point to a new location in the data space, where the displacements are weighted sums of the N known displacements of the select points at which the high-fidelity counterpart is known. That is,\nHere wi are the multi-fidelity data points, uj \u2212 u\u0304j is the displacement vector that maps the j-th low-fidelity point to its high-fidelity location, and \u03c8(j)i , j = 1, . . . ,N , are the influence functions that determine the effect of the j-th displacement vector on the i-th point. We require the influence functions to encode the structure of the low-fidelity data distribution, and therefore a natural choice is to write them in terms of the eigenfunctions of the graph Laplacian. For consistency, we also require the influence functions to be a partition of unity. These requirements are satisfied by applying a softmax activation to a set of auxiliary functions v(j)i that are constructed from a linear combination of the low-lying eigenfunctions of the graph Laplacian. In particular, the influence functions are given by\nand the auxiliary functions v(j)i are,\nThe parameter \u03b1jm determines the contribution of the m-th eigenfunction to the j-th auxiliary function, and K denotes the cutoff in the spectrum of the graph Laplacian. This cutoff should be proportional to the number of high-fidelity data points. A suggested value, which is used in this study, is K = 3N.\nThe parameters \u03b1 = {\u03b1jm} are determined by solving the minimization problem\nwith\nThe first term in (17) is a data misfit term, which forces the multi-fidelity points to be close to the corresponding high-fidelity points. The second term is a structure-preserving term that promotes contributions from eigenfunctions with small eigenvalues. Its form is motivated by a similar term used in semi-supervised learning applications that utilize the graph Laplacian32. To examine the effect of this term, we substitute (16) in (19) to find\nThus, the values of \u03b1jm corresponding to larger values of m are penalized more. Since the structure of the low-fidelity data is encoded in the eigenfunctions corresponding to the smaller eigenvalues, this term helps in carrying this structure over to the multi-fidelity model. It also makes the proposed algorithm relatively insensitive to the selection of the spectrum cutoff K, as the contribution from the higher-order eigenfunctions is weighed less. The presence of \u03c4 > 0 makes the minimization problem convex and easy to solve32, and a good candidate for its value is the smallest non-zero eigenvalue, i.e. \u03c4 = 2 . This amounts to solving a problem with a scaled spectrum of the Laplacian.\nThe regularization constant \u03c9 in (17) balances the interplay between the data misfit term and the structurepreserving regularization term. If its value is too small, the multi-fidelity model is likely to over-fit the highfidelity data and ignore the structure learned from the low-fidelity data. On the other hand, if its value is too large, the method will yield multi-fidelity points that are significantly different from their high-fidelity counterpart.\n(14)wi = u\u0304i + N \u2211\nj=1 (uj \u2212 u\u0304j)\u03c8(j)i , i = 1, . . . , N\u0304 .\n(15)\u03c8(j)i = exp (v\n(j) i )\n\u2211N k=1 exp (v (k) i )\n,\n(16)v (j) i =\nK \u2211 m=1 \u03b1jm\u03c6 (m) i .\n(17)\u03b1 \u2217 = argmin\n\u03b1 J(\u03b1), J(\u03b1) = Jdata(\u03b1)+ \u03c9Jreg(\u03b1),\n(18)Jdata(\u03b1) = 1\nN\nN \u2211 i=1 ||wi(\u03b1)\u2212 ui||22, and\n(19)Jreg(\u03b1) = 1\n\u03c4 2KN\nN \u2211 j=1 v(j)(\u03b1) \u00b7 (L + \u03c4 I)2v(j)(\u03b1).\n(20)Jreg(\u03b1) = 1\nKN\nN \u2211\nj=1\nK \u2211 m=1 \u03b12jm ( 1+ m \u03c4 )2 .\n12\nVol:.(1234567890)\nScientific Reports | (2023) 13:16618 | https://doi.org/10.1038/s41598-023-43719-1\nAs described in Supplementary Appendix D, the optimal value of this parameter may be determined using the L-curve method50.\nFinally, we note that if we do not include the parameters \u00b5 in the definition of u , i.e. u = q(\u00b5) , they not appear explicitly in any of the equations. This means that the method is insensitive to the dimension of the input space, and it can be applied to a generic point cloud {u\u0304i}N\u0304i=1 embedded in RD that has to be transformed based on a few more accurate, or updated, control points {ui}Ni=1 , with N \u226a N\u0304 . This could represent a set of pointwise measurements that are dense in space but not very accurate, for which a smaller number of more precise measurements are available."
        },
        {
            "heading": "Data availability",
            "text": "The datasets generated and analyzed in the current study are available from the corresponding author on reasonable request.\nReceived: 5 June 2023; Accepted: 27 September 2023"
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by ARO Grant W911NF2010050 and the Army/Navy/NASA Vertical Lift Research Center of Excellence (VLRCOE) Program, grant number W911W61120012, with Mahendra Bhagwat as technical monitor."
        },
        {
            "heading": "Author contributions",
            "text": "All authors contributed to the development of the methodology, the design of the numerical experiments, the analysis of the results and writing of the manuscript. O.P. performed the numerical simulations."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Supplementary Information The online version contains supplementary material available at https:// doi. org/ 10. 1038/ s41598- 023- 43719-1.\nCorrespondence and requests for materials should be addressed to O.P.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2023"
        }
    ],
    "title": "Graph Laplacian\u2010based spectral multi\u2010fidelity modeling",
    "year": 2023
}