{
    "abstractText": "The graph coloring problem is an NP-hard problem. Currently, the most effective method to solve the problem is the hybrid algorithm. This paper proposes a hybrid evolutionary algorithm NERS HEAD with a new elite replacement strategy. In NERS HEAD, a method to detect the local optimal state is proposed so that the evolutionary process can jump out of the local optimal by introducing diversity on time. A new elite structure and replacement strategy are designed to increase the diversity of the evolutionary population so that the evolution process can not only converge quickly but also jump out of the local optimum in time. Experiments based on 34 instances of the DIMACS benchmark show that compared with the current excellent graph coloring algorithm, NERS HEAD can reduce the evolutionary generation by an average of 28.3% and the calculation time by 22.11%. Especially in the instance dsjc500.1, NERS HEAD can reduce 56% of evolution generations and 40% of computing time; On the r1000.1c instance, it can reduce the evolution generations by 82.30% and the calculation time by 73.45%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ping Guo"
        },
        {
            "affiliations": [],
            "name": "Bin Guo"
        }
    ],
    "id": "SP:88996fd9db4082de01865a06defffafb614d41f3",
    "references": [
        {
            "authors": [
                "PG A",
                "AH C B"
            ],
            "title": "An adaptive memory algorithm for the k -coloring problem",
            "venue": "NZ",
            "year": 2008
        },
        {
            "authors": [
                "I Bl\u00f6Chliger",
                "N Zufferey"
            ],
            "title": "A graph coloring heuristic using partial solutions and a reactive tabu scheme",
            "venue": "Computers & Operations Research 35(3):960\u2013975,",
            "year": 2008
        },
        {
            "authors": [
                "P Briggs"
            ],
            "title": "Register allocation via graph coloring",
            "venue": "Phd Thesis Rice University https://doi.org/10",
            "year": 1995
        },
        {
            "authors": [
                "D Br\u00e9"
            ],
            "title": "New methods to color the vertices of a graph",
            "venue": "laz",
            "year": 1979
        },
        {
            "authors": [
                "M Chams",
                "A Hertz",
                "DD Werra"
            ],
            "title": "Some experiments with simulated annealing for coloring graphs",
            "venue": "European Journal of Operational Research 32(2):260\u2013266,",
            "year": 1987
        },
        {
            "authors": [
                "O David",
                "GG Jose",
                "MM Ivan",
                "D Enrique"
            ],
            "title": "Spectrum graph coloring and applications to wifi channel assignment. Symmetry 10(3):65\u2013, https://doi",
            "year": 2018
        },
        {
            "authors": [
                "C Fleurent",
                "JA Ferland"
            ],
            "title": "Genetic and hybrid algorithms for graph coloring",
            "venue": "Annals of Operations Research 63(3):437\u2013461,",
            "year": 1996
        },
        {
            "authors": [
                "P Galinier",
                "JK Hao"
            ],
            "title": "Hybrid evolutionary algorithms for graph coloring",
            "venue": "Journal of Combinatorial Optimization",
            "year": 1999
        },
        {
            "authors": [
                "M Garey",
                "D Johnson",
                "H So"
            ],
            "title": "An application of graph coloring to printed circuit testing",
            "venue": "Circuits & Systems IEEE Transactions on 23(10):591\u2013599,",
            "year": 1975
        },
        {
            "authors": [
                "O Goudet",
                "B Duval",
                "JK Hao"
            ],
            "title": "Populationbased gradient descent weight learning for graph coloring problems",
            "venue": "Knowledge-Based Systems 212:106581,",
            "year": 2020
        },
        {
            "authors": [
                "N Grech"
            ],
            "title": "Efficient reflection string analysis via graph coloring https://doi.org/10.4230/LIPIcs",
            "year": 2018
        },
        {
            "authors": [
                "A Hertz",
                "D Werra"
            ],
            "title": "Using tabu search techniques for graph coloring",
            "venue": "Computing https://doi.org/",
            "year": 1987
        },
        {
            "authors": [
                "FT Leighton"
            ],
            "title": "A graph coloring algorithm for large scheduling problems",
            "venue": "Journal of Research of the National Bureau of Standards (United States)",
            "year": 1979
        },
        {
            "authors": [
                "Z L\u00fc",
                "JK Hao"
            ],
            "title": "A memetic algorithm for graph coloring",
            "year": 2010
        },
        {
            "authors": [
                "T Maitra",
                "AJ Pal",
                "SDE Bhattacharyya",
                "TH Kim"
            ],
            "title": "Noise reduction in vlsi circuits using modified ga based graph coloring",
            "venue": "International Journal of Control & Automation",
            "year": 2010
        },
        {
            "authors": [
                "R Marappan",
                "G Sethumadhavan"
            ],
            "title": "Complexity analysis and stochastic convergence of some well-known evolutionary operators for solving graph coloring problem",
            "year": 2020
        },
        {
            "authors": [
                "M Mitchell"
            ],
            "title": "L.d. davis, handbook of genetic algorithms",
            "venue": "Artificial Intelligence 100(1):325\u2013330,",
            "year": 1998
        },
        {
            "authors": [
                "L Moalic",
                "A Gondran"
            ],
            "title": "The new memetic algorithm head for graph coloring: an easy way for managing diversity",
            "year": 2015
        },
        {
            "authors": [
                "L Moalic",
                "A Gondran"
            ],
            "title": "Variations on memetic algorithms for graph coloring problems",
            "venue": "Journal of Heuristics",
            "year": 2017
        },
        {
            "authors": [
                "C Prakash",
                "Sharma",
                "S Narendra",
                "Chaudhari"
            ],
            "title": "A tree based novel approach for graph coloring problem using maximal independent set. Wireless Personal Communications",
            "year": 2020
        },
        {
            "authors": [
                "Titiloye O",
                "Crispin"
            ],
            "title": "A (2011a) Graph coloring with a distributed hybrid quantum annealing algorithm",
            "venue": "Kes International Conference on Agent & Multiagent Systems: Technologies",
            "year": 2011
        },
        {
            "authors": [
                "Titiloye O",
                "Crispin"
            ],
            "title": "A (2011b) Quantum annealing of the graph coloring problem. Discrete Optimization",
            "year": 2011
        },
        {
            "authors": [
                "TK Woo",
                "S Su",
                "R Newman-Wolfe"
            ],
            "title": "Resource allocation in a dynamically partitionable bus network using a graph coloring algorithm",
            "venue": "IEEE Transactions on Communications",
            "year": 2002
        },
        {
            "authors": [
                "Q Wu",
                "JK Hao"
            ],
            "title": "Coloring large graphs based on independent set extraction",
            "venue": "Computers & Operations Research 39(2):283\u2013290,",
            "year": 2012
        },
        {
            "authors": [
                "Y Zhou",
                "B Duval",
                "JK Hao"
            ],
            "title": "Improving probability learning based local search for graph coloring",
            "venue": "Applied Soft Computing",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "the problem is the hybrid algorithm. This paper proposes a hybrid evolutionary algorithm NERS HEAD with a new elite replacement strategy. In NERS HEAD, a method to detect the local optimal state is proposed so that the evolutionary process can jump out of the local optimal by introducing diversity on time. A new elite structure and replacement strategy are designed to increase the diversity of the evolutionary population so that the evolution process can not only converge quickly but also jump out of the local optimum in time. Experiments based on 34 instances of the DIMACS benchmark show that compared with the current excellent graph coloring algorithm, NERS HEAD can reduce the evolutionary generation by an average of 28.3% and the calculation time by 22.11%. Especially in the instance dsjc500.1, NERS HEAD can reduce 56% of evolution generations and 40% of computing time; On the r1000.1c instance, it can reduce the evolution generations by 82.30% and the calculation time by 73.45%.\nKeywords Graph coloring problem \u00b7 Hybrid evolutionary algorithm \u00b7 Tabu search \u00b7 Combinatorial optimization"
        },
        {
            "heading": "Ping Guo College of Computer Science, Chongqing University,",
            "text": "Chongqing 400044 ,China Chongqing Key Laboratory of Software Theory and Technology, Chongqing 400044, China E-mail: guoping@cqu.edu.cn"
        },
        {
            "heading": "Bin Guo College of Computer Science, Chongqing University,",
            "text": "Chongqing 400044, China E-mail: 15727653642@163.com"
        },
        {
            "heading": "1 Introduction",
            "text": "The graph coloring problem (GCP) is a typical combinatorial optimization problem that has been applied to many issues, such as: printed circuit boards testing (Garey et al., 1975), frequency assignment in mobile radio telephone systems(Gamst, 1986), compiler register allocation(Briggs, 1995), noise reduction in Very Large Scale Integration (VLSI) Circuits(Woo et al., 2002) and resource allocation in the bus networks(Maitra et al., 2010), minimize the maximum vertex interference in wifi channel assignment(David et al., 2018), efficient reflection string analysis (Grech, 2018).\nThe GCP is a well-known NP-hard problem, and\nthere are many approaches to solve it. The greedy heuristic is one of these methods: assign different colors to each vertex in a specific or random order of vertices and ensure that no conflicts occur, forming a K-coloring scheme step by step. Such a method is swift, but it does not give the minimum number of colors in most cases. Some efficient greedy heuristics are DSATUR (Bre\u0301 and laz, 1979)and RLF (Leighton, 1979).\nLocal search algorithms are also widely used in GCP. A random scheme with conflicting edges is generated for a fixed number of colors (e.g., the chromatic number). Then the conflicting edges are reduced by iteratively adjusting the vertex color in the neighbourhood. Hertz and Werra proposed the TabuCol (Hertz and Werra, 1987), which introduces the short-term memory mechanism of the tabu list. It records the last vertex color change in the tabu list and does not make the same color change within a certain tabu tenure, effectively preventing loops. In 2006, Ivo Blo\u0308chliger and Nicolas Zufferey (Blo\u0308Chliger and Zufferey, 2008)proposed two improved strategies for partial schemes and a reactive tabu tenure, achieving good results on complex bench-\nmark graphs. Other local search algorithms include simulated annealing algorithm(Chams et al., 1987), quantum annealing(Titiloye and Crispin, 2011b), improving probability learning based local search(Zhou et al., 2018), etc.\nPopulation-based evolutionary algorithms are also an effective approach to solve GCP. In 1991, Davis (Mitchell, 1998) introduces the genetic algorithm in detail. In 1996, Fleurent (Fleurent and Ferland, 1996) et al. improved the genetic algorithm to solve GCP. In 2013, Marappan (Marappan and Sethumadhavan, 2013)et al. proposed uniparental conflict gene crossover and conflict gene mutation operators to make the genetic algorithm more effective. In 1999, Galinier and Hao (Galinier and Hao, 1999) introduced the hybrid evolutionary algorithm (HEA) by combining the tabu search algorithm with the evolutionary algorithm framework. In 2008, Galinier and Hertz (A et al., 2008) proposed an adaptive memory algorithm based on recombination operators to achieve the same results as the HEA. However, this algorithm is more flexible than the HEA. In 2010, Lu\u0308 and Hao (Lu\u0308 and Hao, 2010)et al. proposed an adaptive multi-parent crossover operator and a pool updating strategies. In 2012, Titiloye (Titiloye and Crispin, 2011a)et al. combined an evolutionary algorithm and an improved simulated annealing algorithm to form a distributed hybrid quantum annealing algorithm, which led to further improvements in the results on the test dataset. For large graphs with high density and a large number of vertices, Wu and Hao (Wu and Hao, 2012) et al. first used a preprocessing method to extract larger independent sets. They then used a memetic algorithm to color the residual graphs, achieving improved results on the test dataset. In 2015, Moalic and Alexandre (Moalic and Gondran, 2015) proposed a hybrid evolutionary algorithm based on two individuals (HEAD), which reduced computation time. In 2017, Moalic and Alexandre(Moalic and Gondran, 2017)introduced random crossover and unbalanced crossover in HEAD to improve the performance of the algorithm. Goudet and Duval (Goudet et al., 2020)et al. proposed a population-based weight learning framework to solve the GCP. Prakash(Prakash et al., 2020) et al. proposed a tree-based maximum independent set extraction method in two steps to solve the GCP, obtaining some reasonable experimental results. Reducing the computation time and increasing the success rate are the goals of GCP improvement. For example, hybrid evolutionary algorithms use the tabu search to improve the obtained schemes locally and then use crossover operators to provide good diversity globally. Inspired by HEAD, we propose a hybrid evolution-\nary algorithm NERS HEAD for solving GCP. The main innovations include: 1) Propose strategy 1: a method for judging local optimal state in the evolutionary process that can introduce diversity at the right time;\n2) Propose strategy 2: a new method for managing diversity to make elite individuals more diverse;\n3) The strategies of 1 and 2 are combined to form NERS HEAD to improve the efficiency of solving GCP.\nThe organization of this paper is as follows: Section 2 presents some necessary knowledge needed for the GCP and the basic framework of the hybrid evolutionary algorithm. Section 3 focuses on the new elite individual replacement strategy proposed in this paper. Experimental results are given in Section 4, and the necessary analysis of the experimental results is presented. Section 5 presents the conclusion and possible future improvements."
        },
        {
            "heading": "2 Related work",
            "text": "This section introduces some concepts and some basic algorithms that need to be used when solving GCP with hybrid evolutionary algorithms and the basic framework of the HEAD algorithm referenced in this paper.\n2.1 Schemes and objective function\nGiven an undirected graph G = (V,E), the graph coloring problem(GCP) can be described as divide the vertex set V into K subsets V1, V2, \u00b7 \u00b7 \u00b7 , VK and Vi \u2229 Vj = \u2205(i 6= j),V = V1\u222aV2\u222a\u00b7 \u00b7 \u00b7\u222aVK , The vertices in each subset are assigned the same color, the vertices in different subsets are assigned different colors. Each such division is called a K-coloring scheme (also called scheme). If the K-coloring scheme s = {V1, V2, \u00b7 \u00b7 \u00b7 , VK} makes \u2200u, v \u2208 V, e(u, v) \u2208 E, and u and v have different colors, then s is called a K-coloring solution (solution) of the GCP. For a K-coloring scheme s, the objective function can be defined as for formula (1).\nf(s) = \u2211\nVi\u2208S\n\u2211\nu,v\u2208Vi\n\u03b4uv (1)\nwhere\n\u03b4uv =\n{\n1 \u3008u, v\u3009 \u2208 E, 0 otherwise .\nObviously, f(s) \u2265 0. In this way, only f(s) = 0, the scheme s is the solution of GCP. Therefore, GCP can be expressed as an optimization problem, as shown in formula (2):\ns = argmin s\u2208S f(s). (2)\nWhere S is all possible schemes.\nSolving GCP is finding (searching) K-coloring schemes\nto reduce the value of the objective function f to 0. The K-coloring scheme s1 is better than s2 means f(s1) < f(s2).\n2.2 The tabu search algorithm Tabucol\nTabu search algorithm has been widely used since it was proposed in 1987 (Hertz and Werra, 1987). Algorithm 1 gives a typical tabu search algorithm for solving the GCP(Galinier and Hao, 1999). Since the Tabu search does not depend on the quality of the initial scheme, an initial scheme is generally initialized at random: each vertex is assigned a color no greater than K to obtain a scheme with conflicting edges as the input to the algorithm. Resets the number of iterations and create the s\u2217 to save the gbest scheme(lines 1-2). Line 3 is the termination condition. The size of the MaxIter is determined by the experiment. In line 5, the color of the vertices is changed to minimize the objective function value(using a one-step move strategy, see Section 2.3). In line 6, the tabu list is introduced to avoid making the same vertex color transformation within a certain number of iterations, and it is a two-dimensional list. One dimension is the vertices, and the other dimension is the colors. When a color transformation is performed, it is recorded in the tabu list, and the same operation cannot be performed again within a certain tabu tenure tl. Line 9 updates the gbest scheme s\u2217. In line 12, after reaching the number of iterations, the gbest scheme s\u2217 is output.\nIn tabu search, the parameter tl is called tabu tenure.\nIts size directly affects the search effect on the neighbourhood schemes, a longer tabu tenure can explore a large search space, but if it is set too long, it will not play the role of tabu, if the tabu tenure is short, the search will be limited to a smaller range. A better method was obtained in (Galinier and Hao, 1999) using formula (3) to generate the tabu tenure in a semirandom manner.\ntl = random(A) + \u2202 \u2217 f(s) (3)\nwhere the range of the parameter A is [0,9], \u2202 = 0.6, and f(s) represents the number of conflicting edges (objective function value) of the current scheme s. In this paper, we also adopt such a configuration. Meanwhile, static, dynamic and reactive tabu tenure change strategies are introduced in (Blo\u0308Chliger and Zufferey, 2008).\nAlgorithm 1 :TabuCol; // The tabu search algorithm\nInput: A graph G = {V,E}, scheme s0; Output: the gbest scheme s\u2217; 1: s\u2190 s0; Iter \u2190 0; 2: Let s\u2217 be the gbest scheme; /* s\u2217 represents the scheme\nwith the smallest objective function in the whole iterative process */ 3: while Iter! = MaxIter do 4: Iter \u2190 Iter + 1; 5: Choose a best authorized move (v, i); 6: Introduce move (v, s(v)) in the Tabu list for tl itera-\ntions; 7: Perform the move (v, s(v)) in s; 8: if f(s) < f(s\u2217) then 9: s\u2217 \u2190 s; 10: end if 11: end while 12: return s\u2217;\n2.3 Distance between schemes\nFor any two schemes s1 = {V 1 1 , . . . , V 1 K} and s2 = {V 21 , . . . , V 2 K}. Changing the color of a vertex, e.g. changing the color of vertex u in s1 to j , is a move of vertex u from V 1i to V 1 j , called a one-step move. The distance between schemes s1 and s2 is defined as the number of moves in one-step needed to convert schemes s1 to s2 , denoted as d(s1, s2), obviously d(s1, s2) = d(s2, s1). When the distance between two schemes is 1, they are said to be neighbours of each other. s1 and s2 in Fig.1(a) are neighbours of each other, and when the vertex G \u2208 V 23 move to V 2 2 , s2 is the same as s1, and the transformation between them requires only one-step move, d(s1, s2) = 1, The distance between schemes is only related to the division of vertices. In Fig.1(b), the colors of the vertices in s1 and s2 are different, but the vertex division is exactly the same. So, d(s1, s2) = 0.\nThe distance between the schemes can be solved using the bipartite graph maximum weight matching. The schemes s1 and s2 as two disjoint sets, color subsets in each scheme are considered as the vertices of the bipartite graph, and the number of matches between the color subsets of the s1 and s2 is the weight. The distance between s1 and s2 is equal to the number of vertices reduced by the maximum matching number. Fig.2 gives an example of converting two schemes to a bipartite graph and calculating the distance between schemes. Fig.2(b) is the bipartite graph abstracted from Fig.2(a). The number of vertices in Fig.2(a) is 10, and the bigraph shows that the maximum matching number is 6(3+2+1), then the distance = 4 between s1 and s2 in Fig.2.\n2.4 Greedy Partition Crossover GPX\nThe Greedy Partition Crossover is a crossover operator. The two parents (schemes) s1 = {V 1 1 , . . . , V 1 K} and s2 = {V 2 1 , . . . , V 2 K}, partition the vertices into K subsets according to color, such as V 1i in s1, where 1 represents parent 1, i represents color i. All vertices in this subset are assigned color i. Algorithm 2 will get a new offspring combining the two parents. Alternately selecting the large color subset of the two parents in turn (lines 2-7). Then assign this subset to the offspring, and this subset in both parents is removed (lines 8-9), until all subsets in the parents have been selected. In line 11, the vertices that have not yet been assigned are randomly assigned to the subset of the offspring.\nAlgorithm 2 :GPX; // The GPX algorithm\nInput: scheme s1 = {V 11 , . . . , V 1 K} and s2 = {V 2 1 , . . . , V 2K}; Output: scheme s = {V1, . . . , VK}; 1: for l(l \u2264 l \u2264 K) do 2: if l is odd then 3: A = 1; 4: else 5: A = 2; 6: end if 7: Choose i such that V Ai has a large color subset; 8: Vl = V Al ; 9: remove the vertices of V Al from s1 and s2; 10: end for 11: Assign colors to the vertices V \u2212(V1\u222a\u00b7 \u00b7 \u00b7\u222aVK) randomly; 12: return s;\n2.5 hybrid evolutionary algorithm in Duet HEAD\nHybrid evolutionary algorithms are a class of algorithms that combine local search algorithms with evolutionary algorithms and are often used to solve optimization problems. In 2017, Moalic et al. gave a hybrid evolutionary algorithm HEAD (Moalic and Gondran, 2017), as in Algorithm 3. HEAD removes the complex selection and update operators from the evolutionary algorithm and uses an elite strategy to manage population diversity.\nAlgorithm 3 randomly initializes two parents(P1,P2),\nAlgorithm 3 :HEAD; // HEA in Duet\nInput: K is the number of colors, IterTC is the number of Tabucol iterations, Itercycle = 10 is the number of generations into one cycle, Maxgeneration is the maximum number of generation; Output: the gbest K-coloring scheme s; /* gbest represents the scheme with smallest objective function during the whole evolutionary process*/\n1: P = {P1, P2, elite1, elite2, gbest} \u2190 init(); /* Each vertex is randomly assigned a color no greater than K to obtain a scheme */ 2: generation \u2190 0; 3: while f(gbest) > 0 and P1 6= P2 and generation < Max-\ngeneration do 4: offspring1 \u2190 GPX(P1, P2); 5: offspring2 \u2190 GPX(P2, P1); 6: P1 \u2190 Tabucol(offspring1, IterTC); 7: P2 \u2190 Tabucol(offspring2, IterTC); 8: elite1 \u2190 saveBest(P1, P2, elite1) /* Compare the ob-\njective function value in the three schemes and return the scheme with the smallest objective function value*/ 9: gbest \u2190 saveBest(P1, P2, gbest); 10: if generation%Itercycle = 0 then 11: P1 \u2190 elite2; /* elite2 is the individual with the\nsmallest objective function value in previous cycle*/ 12: elite2 \u2190 elite1; /* elite1 is the individual with the\nsmallest objective function value in current cycle*/ 13: elite1 \u2190 init(); 14: end if 15: generation++; 16: end while 17: return the gbest scheme s\nelite individuals(elite1,elite2) and the gbest(line 1). The GPX was used to generate two different offspring, and then the Tabucol was used to improve the two offspring to replace the two parents (lines 4-7). The gbest and the elite1 are updated in lines 8 and 9. After each generation cycle (Itercycle=10), the elite2 from the previous generation cycle replaces the P1 (line 11). The final output is the gbest scheme.\nBecause the population uses only two individuals, there is no redundant selection, and each individual is involved in the evolutionary process. Experiments show that it can quickly find the solution of small and\nmedium-sized graphs. However, the introduction of the elite individual with a fixed generation cycle is not flexible. Moreover, there are fewer options for elite individuals, which sometimes do not effectively provide diversity. So this paper proposes a new elite individual replacement strategy to improve the HEAD algorithm."
        },
        {
            "heading": "3 hybrid evolutionary algorithm (NERS HEAD)",
            "text": "This section discusses the hybrid evolutionary algorithm NERS HEAD proposed in this paper to solve GCP and the replacement strategy of its elite individuals. Including:(1) The framework of NERS HEAD (section 3.1); (2)The local optimal state detection method based on the change of objective function value(section 3.2); (3) the elite construction method and fitness function for selecting elite individual replacements (section 3.3).\n3.1 NERS HEAD framework\nInspired by HEAD, we give a hybrid evolutionary algorithm NERS HEAD with a new elite replacement strategy, as Algorithm 4.\nIn Algorithm 4, six schemes are randomly generated, which are two parents (P1,P2), two elite individuals (elite1,elite2), temp is the individual with the smallest objective function in each generation period and the gbest is the individual with the smallest objective function during the whole Evolutionary process(line 1). The GPX was used to generate two different offspring, and then the Tabucol was used to improve the two offspring to replace the two parents (lines 4-7). Update temp and gbest (lines 8-9). During evolution, the change of the objective function value is used to determine whether it falls into a local optimal state (lines 10-13) (Section 3.2). If trapped in the local optimal state, a new scheme is obtained by MPX of elite individuals from the previous two periods(line 15). Then the scheme is added to the elite pool after the Tabucol (line 16) (Section 3.3). The fitness between elite and parents is calculated, and the smallest fitness is selected for elite replacement (lines 17-21) (Section 3.3). Finally, update the elite individual and the generations (lines 22-27). Output the gbest(line29).\n3.2 Method of detecting local optimal state\nMoalic and Alexandre proposed two versions of HEAD (Moalic and Gondran, 2017), the first version without the strategy of adding elite individuals, which allows\nAlgorithm 4 :NERS HEAD; // HEAD with new elite replacement strategy\nInput: K is the number of colors, IterTC is the number of Tabucol iterations, generation is the number of evolutive generations, Maxgeneration is the maximum number of evolutive generations, \u03d5 = 5, \u03b5 = 0.1; Output: the gbest scheme s; 1: P = {P1, P2, elite1, elite2, temp, gbest} \u2190 init(); 2: generation \u2190 0;fpre \u2190 999;fcurr \u2190 0; 3: while f(gbest) > 0 and P1 6= P2 and generation < Max-\ngeneration do 4: offspring1 \u2190 GPX(P1, P2); 5: offspring2 \u2190 GPX(P2, P1); 6: P1 \u2190 Tabucol(offspring1, IterTC); 7: P2 \u2190 Tabucol(offspring2, IterTC); 8: temp \u2190 saveBest(P1, P2, temp); 9: gbest \u2190 saveBest(P1, P2, gbest); 10: if generation%\u03d5 = 0 then 11: fcurr = f(gbest); 12: \u2206 = fpre \u2212 fcurr; /*(section 3.2)*/ 13: fpre = fcurr; 14: if \u2206 = 0 or d(P1, P2) < \u03b5 \u2217 n then /*(section\n3.3)*/ 15: elite3 \u2190MPX(elite1, elite2); 16: elite3 \u2190 Tabucol(elite3, 0.1 \u2217 IterTC); 17: for i=1,2,3, j=1,2 do 18: P(i,j) \u2190 fit(i,j) /*according to Eq.(4)*/ 19: end for 20: (i,j)=agrmin{P(i,j)| i=1,2,3,j=1,2}; 21: Pj \u2190 elitei; 22: elite2 \u2190 elite1; 23: elite1 \u2190 temp; 24: temp\u2190 init(); 25: end if 26: end if 27: generation ++; 28: end while 29: return the gbest scheme s;\nthe population to converge quickly but with a low success rate. The second version introduces elite individuals according to a fixed number of generations, which improves the success rate but reduces the efficiency. Inspired by these two versions, in NERS HEAD, only P1 and P2 are used to participate in evolution so that the evolution process can converge faster. At the same time, after every \u03d5 generation of evolution, it is judged whether the evolution process falls into a local optimal state. If it is, it needs to jump out in time. This section discusses the judgment method of whether the evolution process is in a local optimal state, and the strategy of jumping out of the local optimal state is discussed in section 3.3.\nIn NERS HEAD, the change of the objective function value and the distance between P1 and P2 are used to detect whether the evolutionary process falls into a local optimal state.\nFirst, we believe that the objective function values after \u03d5 generations are equal means that the evolution-\nary process has fallen into a local optimal state. For scheme s, searching for a feasible scheme according to Eq.(1) is the process of searching for the objective function value f(s) that decreases to 0. Every \u03d5 generations, the objective function value of the gbest is recorded in fcurr(Algorithm 4, line 11). The fpre records the objective function value of the gbest in the last \u03d5 generations(Algorithm 4, line 13). \u2206 is equal to the difference between fpre and fcurr (Algorithm 4, lines 12). If \u2206 = 0 means the population fall into a local optimal state. The number of the generation period \u03d5 is determined experimentally (section 4.2).\nSecond, when the d(P1, P2) < \u03b5 \u2217 n means that the evolutionary process is trapped in a local optimal state. n is the number of the vertices, \u03b5 is 0.1. This condition is to prevent the algorithm from stopping if the first condition is not satisfied since the algorithm ends when the distance between the parents is 0.\n3.3 Strategy for jumping out of local optimal state\nIn the process of finding the gbest scheme, HEAD can achieve fast convergence in the early stage, and after falling into a local optimal state, it is challenging to jump out. If the structure of the scheme is completely broken to increase diversity will lead to worse results, and too little diversity cannot be jumped out in time. Hence, it is crucial to introduce proper diversity.\nSince there are only two individuals, as much diversity as possible is obtained using the elite individuals in the pre-generation period. This paper proposes saving the elite individuals in the first two generation periods and combining two elites using multi-parental crossover (MPX) to be added to the elite pool. It can add more diversity while preserving the structure of most schemes.\nThis paper uses MPX for the case of 2 parents (m=2),\nand the pseudo-code is as Algorithm 5. The input of the algorithm is two schemes (parents), and the output is one scheme (offspring), which first selects the large color subset among the parents, assigns it to the offspring, and then removes all the vertices containing this subset from the parents (lines 2-4). After all the K color subsets are selected, the vertices in the offspring that are not assigned a color are randomly assigned a color (line 6). MPX(m=2) differs from GPX. Instead of alternating the selection of both parents, the large color subsets is selected in both parents each time. The color subsets of the same parent can be chosen consecutively and simultaneously. Since crossover does not improve the objective function value, but on the contrary, random coloring in the last step will also increase the objective function value, so this offspring has to be improved by Tabucol and then added to the elite\npool (Algorithm 4, line 16). The Tabucol is more timeconsuming, so we set the number of the iteration to be 0.1* IterTC . Because the purpose of this step is not to find the solution but to reduce the number of conflicts generated in the last step. In this way, the elite pool contains the elite individuals (elite1 and elite2) from the first two generation periods and the newly generated individuals(elite3) from MPX.\nMeanwhile, this paper proposes a new approach to\nAlgorithm 5 :MPX(m=2) // The MPX (m=2) algorithm Input: solution s1 = {V 11 , . . . , V 1 K} and s2 = {V 2 1 , . . . , V 2K}; Output: solution s = {V1, . . . , VK}; 1: for l(l \u2264 l \u2264 K) do 2: Choose i such that V Ai (A = 1 or 2) has a large color\nsubset; 3: Vl = V Al ; 4: remove the vertices of V Al from s1 and s2; 5: end for 6: Assign colors to the vertices V \u2212(V1\u222a\u00b7 \u00b7 \u00b7\u222aVK) randomly; 7: return S;\nchoose the elite for replacement, which calculates the fitness between the elite individuals(elite1,elite2,elite3) and the parents (P1,P2) based on the objective function and distance, as in formula (4).\nfit(i, j) = f 1 2 ij + exp dij \u03b2\u2217n (4)\nwhere fit(i, j) represents the fitness value of i and j, i represents elite individuals, j represents parents. fij represents the difference between the objective functions vaule of i and j, dij represents the distance between i and j, \u03b2 is 0.15, and n is the number of vertices. The proposed fitness calculation method combines distance and objective function.\nAccording to the experiment (Section 4.5), the difference of the objective function can reflect the degree of similarity between two individuals to some extent. If the difference is larger, the less similar the two individuals are. But dij is more responsive to the similarity of two individuals than fij because when the distance of two individuals is 0, these two individuals must be the same. Still, if fij is 0, it cannot be judged that two individuals must be the same. So the value of fij is weakened by opening the root sign on the fitness value. Adjusting the ratio of \u03b2 can influence the degree of distance on the fitness value. The smaller the value of \u03b2, the greater the influence of distance on the fitness value, and the larger the value of \u03b2, the smaller the influence on the fitness value. The idea of the formula is to find out the smallest fitness between elite individuals and parents, that is, the more similar individuals. Two\nvariables are used so that similar individuals can still be selected when one variable is the same. The purpose of increasing diversity is to prevent gradual homogenization between individuals. Replacing j with i corresponding to the minimum fitness value fit(i, j) prevents the two individuals after replacement from being too similar."
        },
        {
            "heading": "4 Experimental and analysis",
            "text": "This section will introduce the experiment datasets and the parameter settings of the experiments, and the detailed results and analysis. Experiments on the effectiveness of using strategy 2 are given in Section 4.3. A comparison with the result of the excellent current algorithms will also be made to verify the effectiveness of the algorithm improvements(section 4.5).\n4.1 Dataset and experimental environment\nThe datasets used in our experiments comes from the DIMACS challenge benchmark dataset, most of which are random or quasi-random. There are a total of 34 datasets, which can be divided roughly into four categories by name. These datasets are widely used in the research of graph coloring algorithms.\nThe dataset details are shown in Table 1, where the chromatic number is marked as \u03c7(G), \u2018?\u2019 means that the chromatic number has not been found yet. The Edge destiny is 2m/n(n-1), where m and n are the number of edges and vertices of the graph, respectively.\nNERS HEAD algorithm is coded in c++. The experimental environment is windows 10, and the processor is Intel Xeon Platinum 8369HC 3.3GHz processer-4 cores and 8GB of RAM. Since HEAD is open source, we run the HEAD algorithm and NERS HEAD under the same experimental conditions to compare the experimental results.\n4.2 Local optimal state detection period \u03d5\nDuring the evolution of NERS HEAD, every \u03d5 generation will detect whether the algorithm falls into a local optimal state. Two examples, dsjc1000.1 and dsjc500.5, are used to determine a more suitable value for the local optimal state detection period \u03d5 . The experiment results are shown in Fig.3, where the horizontal coordinates are the size of the period \u03d5, ranging from 1 to 15. Each value runs the example 50 times, the left vertical coordinate indicates the number of generations, and the\nright vertical coordinate indicates the success rate. As \u03d5 increases, the success rate tends to a stable value, but the average number of generations increases gradually and takes more time. Therefore, \u03d5 = 5 is chosen as the local optimal state detection period for the final experiment because the success rate is high and the number of generations is low at this point.\n4.3 Elite individual replacement effectiveness experiment\nFig.4 gives a comparison of the average evolutionary generations solved using the elite replacement of strategy 2 and the elite replacement strategy of HEAD. NERS HEAD can reduce the number of generations in most instances. Fig.4(a) gives instances for the generations less than 2000 and Fig.4(b) gives instances for the generations greater than 2000.\nIf better quality elites can be selected to provide diversity during the evolutionary process, it can reduce\n(a) Success rate and average number of generations of dsjc1000.1\n(b) Success rate and average number of generations of dsjc500.5\nFig. 3: The experiment of the local optimal state detection period \u03d5\nthe number of generations. So, strategy 2 is effective.\nMeanwhile, the dsjc1000.1 is used to explore the changes in the objective function value during the evolutionary process. HEAD and NERS HEAD output the objective function value of the gbest scheme at every 50 generations. After running 20 times, the objective function values of the same evolutionary generation were averaged and plotted as Fig.5.\nOverall, the value of the objective function decreases with the number of generations during the evolution. NERS HEAD can find the solution faster.\n4.4 Relationship between the difference of objective function and distance\nIn designing the fitness function, the objective function difference is needed to determine the degree of similarity between two individuals. There is no theoretical basis for whether individuals with larger or smaller objec-\n(a) generation less than 2000\n(b) generation more than 2000\nFig. 4: Comparison of the number of generations of the same instance\nFig. 5: The change of the objective function value on dsjc1000.1(k=20)\nFig. 6: Relationship between the difference of objective function and distance on dsjc1000.1(k=20)\ntive function differences are more similar, so we determine this roughly by experiment. The dsjc1000.1 example, during each generation, outputs the distance and the difference of the objective function between two parents. The difference of the objective function for each scheme after the Tabucol is in a small range, and two schemes of the same objective function difference may correspond to multiple distances. Therefore, the horizontal coordinate is the objective function difference, and the vertical coordinate is the average distance corresponding to each objective function difference. In Fig.6, the curve shows that as the difference in the objective function becomes larger, the distance between the two individuals increases. It shows that the difference of the objective function of the two schemes is inversely correlated with the degree of similarity.\n4.5 Algorithm comparison\nThe comparison result of HEAD and NERS HEAD is shown in Table 2. The form of each item is x(y), x and y are the experimental results of NERS HEAD and\nHEAD(Moalic and Gondran, 2017), respectively. When x=y, only x is listed. The first column is the name of the instance. The second column is chromatic number. The third column K represents the number of colors. The fourth column is the number of Tabucol iterations. The fifth column Success: success runs/total runs. The right side is the number of total runs, usually set to 20 (10 for C2000.5 and C4000.5 for larger graphs), and the left side is the number of times the solution was found successfully. The Generation in the sixth column represents the number of crossovers or generations. Bold font indicates better values. For the success rate, NERS HEAD has two data that are worse than HEAD, three data is better than HEAD, and the other success rates are the same. Only on the basis that the success rate is guaranteed, it makes sense to compare the generations and computational time reductions. For 12 instances of the first category(dsjc), it can reduce the number of generations for seven instances and the computation time for six instances. The reduction in computation time is not particularly significant because of the additional time cost required to construct the elite pool. In the second category(le450), most of the examples are relatively simple and can be solved quickly. NERS HEAD reduces the number of generations and computation time of 3 instances. For le450 15c and le450 15d, an alternative approach is used: adding a randomly generated scheme inside the elite pool and directly replacing one of the two parents with that approach gives a more significant success rate improvement (le450 15c improve from 3/20 to 20/20, and le450 15d improve from 1/20 to 20/20). In the remaining instances, the number of generations and the computation time can be reduced for most of the instances. From Table 2, among 34 instances and 41 data items, NERS HEAD can reduce the evolutionary generation of 21 data items and the computation time of 18 data items than HEAD. The average reduction in the number of evolutionary generations was calculated to be 28.3%. The average reduction in computation time is 22.11%. In particular, dsjc500.1 reduces the number of evolutionary generations by 51.40% and the computation time by 40%. r1000.1c reduces the number of evolutionary generations by 82.30% and the computation time by 73.45%. Although the number of generations is somewhat random in each calculation, the effectiveness of the strategies can be demonstrated if it is reduced in most instances. Therefore, NERS HEAD is effective in reducing the number of evolutionary generations and computation time."
        },
        {
            "heading": "5 Conclusion and future work",
            "text": "This paper proposes a hybrid evolutionary algorithm NERS HEAD based on an elite replacement to solve GCP. NERS HEAD guides the evolution direction of the population by setting the criteria of whether the evolutionary process is trapped in the Local optimal state and improves the global search ability by increasing the diversity of elites. The comparison experiment with the current excellent GCP solving algorithm on 34 DIMACS instances shows that NERS HEAD can reduce the number of evolutionary generations and the computing time of most instances while ensuring the success rate. The average reduction in evolution generations and calculation time reached 28.3% and 22.11%, respectively. Therefore, NERS HEAD is a more effective GCP solving algorithm.\nConstructing elite individuals suitable for general instances has always been one of the main problems in solving GCP. MPX is used in this paper, but it has many crossover operators and mutation operators (Marappan and Sethumadhavan, 2020) when solving\nthe GCP. For different types of graphs, changing the operator may get better results. In future work, we can study the characteristics of graphs and choose different operators to construct elite individuals."
        },
        {
            "heading": "Declarations",
            "text": "Conflict of interest The authors declare that they have no conflict of interest.\nHuman and animal rights This article does not contain any studies with human participants or animals performed by any of the authors."
        }
    ],
    "title": "NERS_HEAD: An New Hybrid Evolutionary Algorithm for Solving Graph Coloring Problem",
    "year": 2022
}