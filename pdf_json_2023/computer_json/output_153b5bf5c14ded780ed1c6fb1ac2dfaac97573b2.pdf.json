{
    "abstractText": "Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is \u03b1-IP stable if the average distance of every data point to its own cluster is at most \u03b1 times the average distance to any other cluster. Unfortunately, determining if a dataset admits a 1-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an o(n)-IP stable clustering always exists, as the prior state of the art only guaranteed an O(n)-IP stable clustering. We close this gap in understanding and show that an O(1)-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters. Supported by DFF-International Postdoc Grant 0164-00022B from the Independent Research Fund Denmark. Supported in part by an NSF Graduate Research Fellowship and a Hertz Fellowship Significant part of works was done while P.S. was a Ph.D. candidate at Northwestern University 1 ar X iv :2 30 9. 16 84 0v 1 [ cs .D S] 2 8 Se p 20 23",
    "authors": [
        {
            "affiliations": [],
            "name": "Anders Aamand"
        },
        {
            "affiliations": [],
            "name": "Justin Y. Chen"
        },
        {
            "affiliations": [],
            "name": "Allen Liu"
        },
        {
            "affiliations": [],
            "name": "Sandeep Silwal"
        }
    ],
    "id": "SP:3cc99289182f1e3716eee3540cce0d864495eb50",
    "references": [
        {
            "authors": [
                "Saba Ahmadi",
                "Pranjal Awasthi",
                "Samir Khuller",
                "Matth\u00e4us Kleindessner",
                "Jamie Morgenstern",
                "Pattara Sukprasert",
                "Ali Vakilian"
            ],
            "title": "Individual preference stability for clustering",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Maria-Florina Balcan"
            ],
            "title": "Center based clustering: A foundational perspective",
            "venue": "In Handbook of Cluster Analysis. CRC Press,",
            "year": 2014
        },
        {
            "authors": [
                "Nihesh Anderson",
                "Suman K Bera",
                "Syamantak Das",
                "Yang Liu"
            ],
            "title": "Distributional individual fairness in clustering",
            "venue": "arXiv preprint arXiv:2006.12589,",
            "year": 2020
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Avrim Blum",
                "Santosh Vempala"
            ],
            "title": "A discriminative framework for clustering via similarity functions",
            "venue": "In Symposium on Theory of computing (STOC),",
            "year": 2008
        },
        {
            "authors": [
                "Brian Brubach",
                "Darshan Chakrabarti",
                "John Dickerson",
                "Samir Khuller",
                "Aravind Srinivasan",
                "Leonidas"
            ],
            "title": "Tsepenekas. A pairwise fair and community-preserving approach to k-center clustering",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Brian Brubach",
                "Darshan Chakrabarti",
                "John P Dickerson",
                "Aravind Srinivasan",
                "Leonidas Tsepenekas"
            ],
            "title": "Fairness, semi-supervised learning, and more: A general framework for clustering with stochastic pairwise constraints",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "Anna Bogomolnaia",
                "Matthew O Jackson"
            ],
            "title": "The stability of hedonic coalition structures",
            "venue": "Games and Economic Behavior,",
            "year": 2002
        },
        {
            "authors": [
                "Darshan Chakrabarti",
                "John P Dickerson",
                "Seyed A Esmaeili",
                "Aravind Srinivasan",
                "Leonidas"
            ],
            "title": "Tsepenekas. A new notion of individually fair clustering: \u03b1-equitable k-center",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2022
        },
        {
            "authors": [
                "D. Dua",
                "C. Graff"
            ],
            "title": "Adult data set available on https://archive.ics.uci.edu/ml/datasets/adult. Breast Cancer Wisconsin data set available on https://archive.ics.uci.edu/ml/datasets/breast+cancer+ wisconsin+(diagnostic). Car Evaluation data set available on https://archive. ics.uci.edu/ml/datasets/Car+Evaluation. Drug Consumption data set available on",
            "venue": "UCI machine learning repository,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moritz Hardt",
                "Toniann Pitassi",
                "Omer Reingold",
                "Richard Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "In Innovations in Theoretical Computer Science (ITCS),",
            "year": 2012
        },
        {
            "authors": [
                "Amit Daniely",
                "Nati Linial",
                "Michael Saks"
            ],
            "title": "Clustering is difficult only when it does not matter",
            "venue": "arXiv preprint arXiv:1205.4891,",
            "year": 2012
        },
        {
            "authors": [
                "Edith Elkind",
                "Angelo Fanelli",
                "Michele Flammini"
            ],
            "title": "Price of pareto optimality in hedonic games",
            "venue": "Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "E. Fehrman",
                "A.K. Muhammad",
                "E.M. Mirkes",
                "V. Egan",
                "A.N. Gorban"
            ],
            "title": "The five factor model of personality and evaluation of drug consumption risk, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Teofilo F Gonzalez"
            ],
            "title": "Clustering to minimize the maximum intercluster distance",
            "venue": "Theoretical Computer Science,",
            "year": 1985
        },
        {
            "authors": [
                "Piotr Indyk",
                "Sandeep Silwal"
            ],
            "title": "Faster linear algebra for distance matrices",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Christopher. Jung",
                "Sampath Kannan",
                "Neil Lutz"
            ],
            "title": "A center in your neighborhood: Fairness in facility location",
            "venue": "In Symposium on Foundations of Responsible Computing (FORC),",
            "year": 2020
        },
        {
            "authors": [
                "Debajyoti Kar",
                "Mert Kosan",
                "Debmalya Mandal",
                "Sourav Medya",
                "Arlei Silva",
                "Palash Dey",
                "Swagato Sanyal"
            ],
            "title": "Feature-based individual fairness in k-clustering",
            "venue": "In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
            "year": 2023
        },
        {
            "authors": [
                "Ron Kohavi"
            ],
            "title": "Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid",
            "venue": "In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD),",
            "year": 1996
        },
        {
            "authors": [
                "Sepideh Mahabadi",
                "Ali Vakilian"
            ],
            "title": "Individual fairness for k-clustering",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Maryam Negahbani",
                "Deeparnab Chakrabarty"
            ],
            "title": "Better algorithms for individually fair k-clustering",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Fabian Pedregosa",
                "Ga\u00ebl Varoquaux",
                "Alexandre Gramfort",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg",
                "Jake Vanderplas",
                "Alexandre Passos",
                "David Cournapeau",
                "Matthieu Brucher",
                "Matthieu Perrot",
                "\u00c9douard Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Ali Vakilian",
                "Mustafa Yal\u00e7\u0131ner"
            ],
            "title": "Improved approximation algorithms for individually fair clustering",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u2217Supported by DFF-International Postdoc Grant 0164-00022B from the Independent Research Fund Denmark. \u2020Supported in part by an NSF Graduate Research Fellowship and a Hertz Fellowship \u2021Significant part of works was done while P.S. was a Ph.D. candidate at Northwestern University\nar X\niv :2\n30 9.\n16 84\n0v 1\n[ cs\n.D S]\n2 8"
        },
        {
            "heading": "1 Introduction",
            "text": "In applications involving and affecting people, socioeconomic concepts such as game theory, stability, and fairness are important considerations in algorithm design. Within this context, Ahmadi et al. [AAK+22] introduced the notion of individual preference stability (IP stability) for clustering. At a high-level, a clustering of an input dataset is called IP stable if, for each individual point, its average distance to any other cluster is larger than the average distance to its own cluster. Intuitively, each individual prefers its own cluster to any other, and so the clustering is stable.\nThere are plenty of applications of clustering in which the utility of each individual in any cluster is determined according to the other individuals who belong to the same cluster. For example, in designing personalized medicine, the more similar the individuals in each cluster are, the more effective medical decisions, interventions, and treatments can be made for each group of patients. Stability guarantees can also be used in personalized learning environments or marketing campaigns to ensure that no individual wants to deviate from their assigned cluster. Furthermore, the focus on individual utility in IP stability (a clustering is only stable if every individual is \u201chappy\u201d) enforces a sort of individual fairness in clustering.\nIn addition to its natural connections to cluster stability, algorithmic fairness, and Nash equilibria, IP stability is also algorithmically interesting in its own right. While clustering is well-studied with respect to global objective functions (e.g. the objectives of centroid-based clustering such as k-means or correlation/hierarchical clustering), less is known when the goal is to partition the dataset such that every point in the dataset is individually satisfied with the solution. Thus, IP stability also serves as a natural and motivated clustering framework with a non-global objective."
        },
        {
            "heading": "1.1 Problem Statement and Preliminaries",
            "text": "The main objective of our clustering algorithms is to achieve IP stability given a set P of n points lying in a metric space (M,d) and k, the number of clusters.\nDefinition 1.1 (Individual Preference (IP) Stability [AAK+22]). The goal is to find a disjoint k-clustering C = (C1, \u00b7 \u00b7 \u00b7 , Ck) of P such that every point, on average, is closer to the points of its own cluster than to the points in any other cluster. Formally, for all v \u2208 P , let C(v) denote the cluster that contains v. We say that v \u2208 P is IP stable with respect to C if either C(v) = {v} or for every C \u2032 \u2208 C with C \u2032 \u0338= C(v),\n1 |C(v)| \u2212 1 \u2211\nu\u2208C(v)\nd(v, u) \u2264 1 |C \u2032| \u2211 u\u2208C\u2032 d(v, u). (1)\nThe clustering C is 1-IP stable (or simply IP stable) if and only if every v \u2208 P is stable with respect to C.\nAhmadi et al. [AAK+22] showed that an arbitrary dataset may not admit an IP stable clustering. This can be the case even when n = 4. Furthermore, they proved that it is NP-hard to decide whether a given a set of points have an IP stable k-clustering, even for k = 2. This naturally motivates the study of the relaxations of IP stability.\nDefinition 1.2 (Approximate IP Stability). A k-clustering C = (C1, \u00b7 \u00b7 \u00b7 , Ck) of P is \u03b1-approximate IP stable, or simply \u03b1-IP stable, if for every point v \u2208 P , the following holds: either C(v) = {v} or\nfor every C \u2032 \u2208 C and C \u2032 \u0338= C,\n1 |C(v)| \u2212 1 \u2211\nu\u2208C(v)\nd(v, u) \u2264 \u03b1 |C \u2032| \u2211 u\u2208C\u2032 d(v, u). (2)\nThe work of [AAK+22] proposed algorithms to outputting IP stable clusterings on the onedimensional line for any value of k and on tree metrics for k = 2. The first result implies an O(n)-IP stable clustering for general metrics, by applying a standard O(n)-distortion embedding to one-dimensional Euclidean space. In addition, they give a bicriteria approximation that discards an \u03b5-fraction of the input points and outputs an O ( log2 n\n\u03b5\n) -IP stable clustering for the remaining\npoints. Given the prior results, it is natural to ask if the O(n) factor for IP stable clustering given in [AAK+22] can be improved."
        },
        {
            "heading": "1.2 Our Results",
            "text": "New Approximations. Improving on the O(n)-IP stable algorithm in [AAK+22], we present a deterministic algorithm which for general metrics obtains an O(1)-IP stable k-clustering, for any value of k. Note that given the existence of instances without 1-IP stable clusterings, our approximation factor is optimal up to a constant factor.\nTheorem 1.3. (Informal; see Theorem 3.1) Given a set P of n points in a metric space (M,d) and a number of desired clusters k \u2264 n, there exists an algorithm that computes an O(1)-IP stable k-clustering of P in polynomial time.\nOur algorithm outputs a clustering with an even stronger guarantee that we call uniform (approximate) IP stability. Specifically, for some global parameter r and for every point v \u2208 P , the average distance from v to points in its own cluster is upper bounded by O(r) and the average distance from v to points in any other cluster is lower bounded by \u2126(r). Note that the general condition of O(1)-IP stability would allow for a different value of r for each v.\nWe again emphasize that Theorem 1.3 implies that an O(1)-IP stable clustering always exists, where prior to this work, only the O(n) bound from [AAK+22] was known for general metrics.\nAdditional k-Center Clustering Guarantee. The clustering outputted by our algorithm satisfies additional desirable properties beyond O(1)-IP stability. In the k-center problem, we are given n points in a metric space, and our goal is to pick k centers as to minimize the maximal distance of any point to the nearest center. The clustering outputted by our algorithm from Theorem 1.3 has the added benefit of being a constant factor approximation to the k-center problem in the sense that if the optimal k-center solution has value r0, then the diameter of each cluster outputted by the algorithm is O(r0). In fact, we argue that IP stability is more meaningful when we also seek a solution that optimizes some clustering objective. If we only ask for IP stability, there are instances where it is easy to obtain O(1)-IP stable clusterings, but where such clusterings do not provide insightful information in a typical clustering application. Indeed, as we will show in Appendix B, randomly k-coloring the nodes of an unweighted, undirected graph (where the distance between two nodes is the number of edges on the shortest path between them), gives an O(1)-IP stable clustering when k \u2264 O ( \u221a n\nlogn\n) . Our result on trees demonstrates the idiosyncrasies of individual objectives thus\nour work raises further interesting questions about studying standard global clustering objectives under the restriction that the solutions are also (approximately) IP stable.\nMax and Min-IP Stability. Lastly, we introduce a notion of f -IP stability, generalizing IP stability.\nDefinition 1.4 (f -IP Stability). Let (M,d) be a metric space, P a set of n points of M , and k the desired number of partitions. Let f : P \u00d7 2P \u2192 R\u22650 be a function which takes in a point v \u2208 P , a subset C of P , and outputs a non-negative real number. we say that a k-clustering C = (C1, \u00b7 \u00b7 \u00b7 , Ck) of P is f -IP stable if for every point v \u2208 P , the following holds: either C(v) = {v} or for every C \u2032 \u2208 C and C \u2032 \u0338= C,\nf (v, C(v) \\ {v}) \u2264 f ( v, C \u2032 ) . (3)\nNote that the standard setting of IP stability given in Definition 1.1 corresponds to the case where f(v, C) = (1/|C|)\u00d7 \u2211 v\u2032\u2208C d(v, v\n\u2032). The formulation of f -IP stability, therefore, extends IP stability beyond average distances and allows for alternative objectives that may be more desirable in certain settings. For instance, in hierarchical clustering, average, minimum, and maximum distance measures are well-studied.\nIn particular, we focus on max-distance and min-distance in the definition of f -IP stable clustering in addition to average distance (which is just Definition 1.1), where f(v, C) = maxv\u2032\u2208C d(v, v\u2032) and f(v, C) = minv\u2032\u2208C d(v, v\n\u2032). We show that in both the max and min distance formulations, we can solve the corresponding f -IP stable clustering (nearly) optimally in polynomial time. We provide the following result:\nTheorem 1.5 (Informal; see Theorem 4.1 and Theorem 4.2). In any metric space, Min-IP stable clustering can be solved optimally and Max-IP stable clustering can be solved approximately within a factor of 3, in polynomial time.\nWe show that the standard greedy algorithm of k-center, a.k.a, the Gonzalez\u2019s algorithm [Gon85], yields a 3-approximate Max-IP stable clustering. Moreover, we present a conceptually clean algorithm which is motivated by considering the minimum spanning tree (MST) to output a Min-IP stable clustering. This implies that unlike the average distance formulation of IP stable clustering, a Min-IP stable clustering always exists. Both algorithms work in general metrics.\nEmpirical Evaluations. We experimentally evaluate our O(1)-IP stable clustering algorithm against k-means++, which is the empirically best-known algorithm in [AAK+22]. We also compare k-means++ with our optimal algorithm for Min-IP stability. We run experiments on the Adult\ndata set1 used by [AAK+22]. For IP stability, we also use four more datasets from UCI ML repositoriy [DG19] and a synthetic data set designed to be a hard instance for k-means++. On the Adult data set, our algorithm performs slightly worse than k-means++ for IP stability. This is consistent with the empirical results of [AAK+22]. On the hard instance2, our algorithm performs better than k-means++, demonstrating that the algorithm proposed in this paper is more robust than k-means++. Furthermore for Min-IP stability, we empirically demonstrate that k-means++ can have an approximation factors which are up to a factor of 5x worse than our algorithm. We refer to Section 5 and Appendix C for more details."
        },
        {
            "heading": "1.3 Technical Overview",
            "text": "The main contribution is our O(1)-approximation algorithm for IP stable clustering for general metrics. We discuss the proof technique used to obtain this result. Our algorithm comprises two steps. We first show that for any radius r, we can find a clustering C = (C1, . . . , Ct) such that (a) each cluster has diameter O(r), and (b) the average distance from a point in a cluster to the points of any other cluster is \u2126(r).\nConditions (a) and (b) are achieved through a ball carving technique, where we iteratively pick centers qi of distance > 6r to previous centers such that the radius r ball B(qi, r) centered at qi contains a maximal number of points, say si. For each of these balls, we initialize a cluster Di containing the si points of B(qi, r). We next consider the annulus B(qi, 3r)\\B(qi, 2r). If this annulus contains less than si points, we include all points from B(qi, 3r) in Di. Otherwise, we include any si points in Di from the annulus. We assign each unassigned point to the first center picked by our algorithm and is within distance O(r) to the point. This is a subtle but crucial component of the algorithm as the more natural \u201cassign to the closest center\u201d approach fails to obtain O(1)-IP stability.\nOne issue remains. With this approach, we have no guarantee on the number of clusters. We solve this by merging some of these clusters while still maintaining that the final clusters have radius O(r). This may not be possible for any choice of r. Thus the second step is to find the right choice of r. We first run the greedy algorithm of k-center and let r0 be the minimal distance between centers we can run the ball carving algorithm r = cr0 for a sufficiently small constant c < 1. Then if we assign each cluster of C to its nearest center among those returned by the greedy algorithm k-center, we do indeed maintain the property that all clusters have diameter O(r), and since c is a small enough constant, all the clusters will be non-empty. The final number of clusters will therefore be k. As an added benefit of using the greedy algorithm for k-center as a subroutine, we obtain that the diameter of each cluster is also O(r0), namely the output clustering is a constant factor approximation to k-center."
        },
        {
            "heading": "1.4 Related Work",
            "text": "Fair Clustering. One of the main motivations of IP stable clustering is its interpretation as a notion of individual fairness for clustering [AAK+22]. Individual fairness was first introduced by [DHP+12] for the classification task, where, at high-level, the authors aim for a classifier that gives \u201csimilar predictions\u201d for \u201csimilar\u201d data points. Recently, other formulations of individual fairness have been studied for clustering [JKL20, ABDL20, BCD+20, CDE+22, KKM+23], too. [JKL20]\n1https://archive.ics.uci.edu/ml/datasets/adult; see [Koh96]. 2The construction of this hard instance is available in the appendix of [AAK+22].\nproposed a notion of fairness for centroid-based clustering: given a set of n points P and the number of clusters k, for each point, a center must be picked among its (n/k)-th closest neighbors. The optimization variant of it was later studied by [MV20, NC21, VY22]. [BCD+20] studied a pairwise notion of fairness in which data points represent people who gain some benefit from being clustered together. In a subsequent work, [BCD+21] introduced a stochastic variant of this notion. [ABDL20] studied the setting in which the output is a distribution over centers and \u201csimilar\u201d points are required to have \u201csimilar\u201d centers distributions.\nStability in Clustering. Designing efficient clustering algorithms under notions of stability is a well-studied problem3. Among the various notion of stability, average stability is the most relevant to our model [BBV08]. In particular, they showed that if there is a ground-truth clustering satisfying the requirement of Equation (1) with an additive gap of \u03b3 > 0, then it is possible to recover the solution in the list model where the list size is exponential in 1/\u03b3. Similar types of guarantees are shown in the work by [DLS12]. While this line of research mainly focuses on presenting faster algorithms utilizing the strong stability conditions, the focus of IP stable clustering is whether we can recover such stability properties in general instances, either exactly or approximately.\nHedonic Games. Another game-theoretic study of clustering is hedonic games [DG80, BJ02, EFF20]. In a hedonic game, players choose to form coalitions (i.e., clusters) based on their utility. Our work differs from theirs, since we do not model the data points as selfish players. In a related work, [SP18] proposes another utility measure for hedonic clustering games on graphs. In particular, they define a closeness utility, where the utility of node i in cluster C is the ratio between the number of nodes in C adjacent to i and the sum of distances from i to other nodes in C. This measure is incomparable to IP stability. In addition, their work focuses only on clustering in graphs while we consider general metrics."
        },
        {
            "heading": "2 Preliminaries and Notations",
            "text": "We let (M,d) denote a metric space, where d is the underlying distance function. We let P denote a fixed set of points of M . Here P may contain multiple copies of the same point. For a given point x \u2208 P and radius r \u2265 0, we denote by B(x, r) = {y \u2208 P | d(x, y) \u2264 r}, the ball of radius r centered at x. For two subsets X,Y \u2286 P , we denote by d(X,Y ) = infx\u2208X,y\u2208Y d(x, y). Throughout the paper, X and Y will always be finite and then the infimum can be replaced by a minimum. For x \u2208 P and Y \u2286 P , we simply write d(x, Y ) for d({x}, Y ). Finally, for X \u2286 P , we denote by diam(X) = supx,y\u2208X d(x, y), the diameter of the set X. Again, X will always be finite, so the supremum can be replaced by a maximum."
        },
        {
            "heading": "3 Constant-Factor IP Stable Clustering",
            "text": "In this section, we prove our main result: For a set P = {x1, . . . , xn} of n points with a metric d and every k \u2264 n, there exists a k-clustering C = (C1, . . . , Ck) of P which is O(1)-approximate IP stable. Moreover, such a clustering can be found in time O\u0303(n2T ), where T is an upper bound on the time it takes to compute the distance between two points of P .\n3For a comprehensive survey on this topic, refer to [AB14].\nAlgorithm Our algorithm uses a subroutine, Algorithm 1, which takes as input P and a radius r \u2208 R and returns a t-clustering D = (D1, . . . , Dt) of P with the properties that (1) for any 1 \u2264 i \u2264 t, the maximum distance between any two points of Di is O(r), and (2) for any x \u2208 P and any i such that x /\u2208 Di, the average distance from x to points of Di is \u2126(r). These two properties ensure that D is O(1)-approximate IP stable. However, we have no control on the number of clusters t that the algorithm produces. To remedy this, we first run a greedy k-center algorithm on P to obtain a set of centers {c1, . . . , ck} and let r0 denote the maximum distance from a point of P to the nearest center. We then run Algorithm 1 with input radius r = cr0 for some small constant c. This gives a clustering D = (D1, . . . , Dt) where t \u2265 k. Moreover, we show that if we assign each cluster of D to the nearest center in {c1, . . . , ck} (in terms of the minimum distance from a point of the cluster to the center), we obtain a k-clustering C = (C1, . . . , Ck) which is O(1)-approximate IP stable. The combined algorithm is Algorithm 2.\nAlgorithm 1 Ball-Carving 1: Input: A set P = {x1, . . . , xn} of n points with a metric d and a radius r > 0. 2: Output: Clustering D = (D1, . . . , Dt) of P .\n3: Q\u2190 \u2205, i\u2190 1 4: while there exists x \u2208 P with d(x,Q) > 6r do 5: qi \u2190 argmaxx\u2208P :d(x,Q)>6r |B(x, r)| 6: Q\u2190 Q \u222a {qi}, si \u2190 |B(qi, r)|, Ai \u2190 B(qi, 3r) \\B(qi, 2r) 7: if |Ai| \u2265 si 8: Si \u2190 any set of si points from Ai 9: Di \u2190 B(qi, r) \u222a Si\n10: else Di \u2190 B(qi, 3ri) 11: i\u2190 i+ 1 12: end while 13: for x \u2208 P assigned to no Di do 14: j \u2190 min{i | d(x, qi) \u2264 7r} 15: Dj \u2190 Dj \u222a {x} 16: end for 17: t\u2190 |Q| 18: return D = (D1, . . . , Dt)\nWe now describe the details of Algorithm 1. The algorithm takes as input n points x1, . . . , xn of a metric space (M,d) and a radius r. It first initializes a set Q = \u2205 and then iteratively adds points x from P to Q that are of distance greater than 6r from points already in Q such that |B(x, r)|, the number of points of P within radius r of x, is maximized. This is line 5\u20136 of the algorithm. Whenever a point qi is added to Q, we define the annulus Ai := B(qi, 3r) \\B(qi, 2r). We further let si = |B(qi, r)|. At this point the algorithm splits into two cases.\n\u2022 If |Ai| \u2265 si, we initialize a cluster Di which consists of the si points in B(x, r) and any arbitrarily chosen si points in Ai. This is line 8\u20139 of the algorithm.\n\u2022 If, on the other hand, |Ai| < s, we define Di := B(qi, 3r), namely Di contains all points of P within distance 3r from qi. This is line 10 of the algorithm.\nAlgorithm 2 IP-Clustering 1: Input: Set P = {x1, . . . , xn} of n points with a metric d and integer k with 2 \u2264 k \u2264 n. 2: Output: k-clustering C = (C1, . . . , Ck) of P .\n3: S \u2190 \u2205 4: for i = 1, . . . , k do 5: ci \u2190 argmaxx\u2208P {d(x, S)} 6: S \u2190 S \u222a {ci}, Ci \u2190 {ci} 7: end for 8: r0 \u2190 min{d(ci, cj) | 1 \u2264 i < j \u2264 k} 9: D \u2190 Ball-Carving(P, r0/15)\n10: for D \u2208 D do 11: j \u2190 argmini{d(ci, D)} 12: Cj \u2190 Cj \u222aD 13: end for 14: return C = (C1, . . . , Ck)\nAfter iteratively picking the points qi and initializing the clusters Di, we assign the remaining points as follows. For any point x \u2208 P \\ \u22c3 iDi, we find the minimum i such that d(x, qi) \u2264 7r and assign x to Di. This is line 13\u201316 of the algorithm. We finally return the clustering D = (D1, . . . , Dt). We next describe the details of Algorithm 2. The algorithm iteratively pick k centers c1, . . . , ck from P for each center maximizing the minimum distance to previously chosen centers. For each center ci, it initializes a cluster, starting with Ci = {ci}. This is line 4\u20137 of the algorithm. Letting r0 be the minimum distance between pairs of distinct centers, the algorithm runs Algorithm 1 on P with input radius r = r0/15 (line 8\u20139). This produces a clustering D. In the final step, we iterate over the clusters D of D, assigning D to the Ci for which d(ci, D) is minimized (line 11\u201313). We finally return the clustering (C1, . . . , Ck).\nAnalysis We now analyze our algorithm and provide its main guarantees.\nTheorem 3.1. Algorithm 2 returns an O(1)-approximate IP stable k clustering in time O(n2T + n2 log n). Furthermore, the solution is also a constant factor approximation to the k-center problem.\nIn order to prove this theorem, we require the following lemma on Algorithm 1.\nLemma 3.2. Let (D1, . . . , Dt) be the clustering output by Algorithm 1. For each i \u2208 [t], the diameter of Di is at most 14r. Further, for x \u2208 Di and j \u0338= i, the average distance from x to points of Dj is at least r4 .\nGiven Lemma 3.2, we can prove the the main result.\nProof of Theorem 3.1. We first argue correctness. As each ci was chosen to maximize the minimal distance to points cj already in S, for any x \u2208 P , it holds that min{d(x, ci) | i \u2208 [k]} \u2264 r0. By Lemma 3.2, in the clustering D output by Ball-Carving(P, r0/15) each cluster has diameter at most 1415r0 < r0, and thus, for each i \u2208 [k], the cluster D \u2208 D which contains ci will be included in Ci in the final clustering. Indeed, in line 11 of Algorithm 2, d(ci, D) = 0 whereas d(cj , D) \u2265 115r0 for all j =\u0338 i. Thus, each cluster in (C1, . . . , Ck) is non-empty. Secondly, the diameter of each cluster\nis at most 4r0, namely, for each two points x, x\u2032 \u2208 Ci, they are both within distance r0 + 1415r0 < 2r0 of ci. Finally, by Lemma 3.2, for x \u2208 Di and j \u0338= i, the average distance from x to points of Dj is at least r060 . Since, C is a coarsening of D, i.e., each cluster of C is the disjoint union of some of the clusters in D, it is straightforward to check that the same property holds for the clustering C. Thus C is O(1)-approximate IP stable.\nWe now analyze the running time. We claim that Algorithm 2 can be implemented to run in O(n2T + n2 log n) time, where T is the time to compute the distance between any two points in the metric space. First, we can query all pairs to form the n\u00d7 n distance matrix A. Then we sort A along every row to form the matrix A\u2032. Given A and A\u2032, we easily implement our algorithms as follows.\nFirst, we argue about the greedy k-center steps of Algorithm 2, namely, the for loop on line 4. The most straightforward implementation computes the distance from every point to new chosen centers. At the end, we have computed at most nk distances from points to centers which can be looked up in A in time O(nk) = O(n2) as k \u2264 n. In line 8, we only look at every entry of A at most once so the total time is also O(n2). The same reasoning also holds for the for loop on line 10. It remains to analyze the runtime.\nGiven r, Algorithm 1 can be implemented as follows. First, we calculate the size of |B(x, r)| for every point x in our dataset. This can easily be done by binary searching on the value of r along each of the (sorted) rows of A\u2032, which takes O(n log n) time in total. We can similarly calculate the sizes of |B(x, 2r)| and |B(x, 3r)|, and thus the number of points in the annulus |B(x, 3r) \\B(x, 2r)| in the same time to initialize the clusters Di. Similar to the k-center reasoning above, we can also pick the centers in Algorithm 1 which are > 6r apart iteratively by just calculating the distances from points to the chosen centers so far. This costs at most O(n2) time, since there are at most n centers. After initializing the clusters Di, we finally need to assign the remaining unassigned points (line 13\u201316). This can easily be done in time O(n) per point, namely for each unassigned point x, we calculate its distance to each qi assigning it to Di where i is minimal such that d(x, qi) \u2264 7r. The total time for this is then O(n2). The k-center guarantees follow from our choice of r0 and Lemma 3.2.\nRemark 3.3. We note that the runtime can possibly be improved if we assume special structure about the metric space (e.g., Euclidean metric). See Appendix A for a discussion.\nWe now prove Lemma 3.2.\nProof of Lemma 3.2. The upper bound on the diameter of each cluster follows from the fact that for any cluster Di in the final clustering D = {D1, . . . , Dt}, and any x \u2208 Di, it holds that d(x, qi) \u2264 7r. The main challenge is to prove the lower bound on the average distance from x \u2208 Di to Dj where j \u0338= i.\nSuppose for contradiction that, there exists i, j with i \u0338= j and x \u2208 Di such that the average distance from x to Dj is smaller than r/4, i.e., 1|Dj | \u2211 y\u2208Dj d(x, y) < r/4. Then, it in particular holds that |B(x, r/2) \u2229Dj | > |Dj |/2, namely the ball of radius r/2 centered at x contains more than half the points of Dj . We split the analysis into two cases corresponding to the if-else statements in line 7\u201310 of the algorithm.\nCase 1: |Aj | \u2265 sj: In this case, cluster Dj consists of at least 2sj points, namely the sj points in B(qj , r) and the set Sj of sj points in Aj assigned to Dj in line 8\u20139 of the algorithm. It follows from\nthe preceding paragraph that, |B(x, r/2)\u2229Dj | > sj . Now, when qj was added to Q, it was chosen as to maximize the number of points in B(qj , r) under the constraint that qj had distance greater than 6r to previously chosen points of Q. Since |B(x, r)| \u2265 |B(x, r/2)| > |B(qj , r)|, at the point where qj was chosen, Q already contained some point qj0 (with j0 < j) of distance at most 6r to x and thus of distance at most 7r to any point of B(x, r/2). It follows that B(x, r/2) \u2229Dj contains no point assigned during line 13\u2013 16 of the algorithm. Indeed, by the assignment rule, such a point y would have been assigned to either Dj0 or potentially an even earlier initialized cluster of distance at most 7r to y. Thus, B(x, r/2) \u2229Dj is contained in the set B(qj , r) \u222a Sj . However, |B(qj , r)| = |Sj | = sj and moreover, for (y1, y2) \u2208 B(qj , r)\u00d7 Sj , it holds that d(y1, y2) > r. In particular, no ball of radius r/2 can contain more than sj points of B(qj , r)\u222aSj . As |B(x, r/2)\u2229Dj | > sj , this is a contradiction.\nCase 2: |Aj | < sj: In this case, Dj includes all points in B(qj , 3r). As x /\u2208 Dj , we must have that x /\u2208 B(qj , 3r) and in particular, the ball B(x, r/2) does not intersect B(qj , r). Thus,\n|Dj | \u2265 |B(x, r/2) \u2229Dj |+ |B(qj , r) \u2229Dj | > |Dj |/2 + sj ,\nso |Dj | > 2sj , and finally, |B(x, r/2) \u2229 Dj | > |Dj |/2 > sj . Similarly to case 1, B(x, r/2) \u2229 Dj contains no points assigned during line 13\u2013 16 of the algorithm. Moreover, B(x, r/2)\u2229B(qj , 3r) \u2286 Aj . In particular, B(x, r/2) \u2229Dj \u2286 Sj , a contradiction as |Sj | = sj but |B(x, r/2) \u2229Dj | > sj ."
        },
        {
            "heading": "4 Min and Max-IP Stable Clustering",
            "text": "The Min-IP stable clustering aims to ensure that for any point x, the minimum distance to a point in the cluster of x is at most the minimum distance to a point in any other cluster. We show that a Min-IP stable k-clustering always exists for any value of k \u2208 [n] and moreover, can be found by a simple algorithm (Algorithm 3).\nAlgorithm 3 Min-IP-Clustering 1: Input: Pointset P = {x1, . . . , xn} from a metric space (M,d) and integer k with 2 \u2264 k \u2264 n. 2: Output: k-clustering C = (C1, . . . , Ck) of P .\n3: L\u2190 {(xi, xj)}1\u2264i<j\u2264n sorted according to d(xi, xj) 4: E \u2190 \u2205 5: while G = (P,E) has > k connected components do 6: e\u2190 an edge e = (x, y) in L with d(x, y) minimal. 7: L\u2190 L \\ {e} 8: if e connects different connected components of G then E \u2190 E \u222a {e} 9: end while\n10: return the connected components (C1, . . . , Ck) of G.\nThe algorithm is identical to Kruskal\u2019s algorithm for finding a minimum spanning tree except that it stops as soon as it has constructed a forest with k connected components. First, it initializes a graph G = (V,E) with V = P and E = \u2205. Next, it computes all distances d(xi, xj) between pairs of points (xi, xj) of P and sorts the pairs (xi, xj) according to these distances. Finally, it goes through this sorted list adding each edge (xi, xj) to E if it connects different connected components of G. After computing the distances, it is well known that this algorithm can be made to run in\ntime O(n2 log n), so the total running time is O(n2(T + log n)) where T is the time to compute the distance between a single pair of points.\nTheorem 4.1. The k-clustering output by Algorithm 3 is a Min-IP stable clustering.\nProof. Let C be the clustering output by the algorithm. Conditions (1) and (2) in the definition of a min-stable clustering are trivially satisfied. To prove that (3) holds, let C \u2208 C with |C| \u2265 2 and x \u2208 C. Let y0 \u0338= x be a point in C such that (x, y0) \u2208 E (such an edge exists because C is the connected component of G containing x) and let y1 be the closest point to x in P \\ C. When the algorithm added (x, y0) to E, (x, y1) was also a candidate choice of an edge between connected components of G. Since the algorithm chose the edge of minimal length with this property, d(x, y0) \u2264 d(x, y1). Thus, we get the desired bound:\nmin y\u2208C\\{x} d(x, y) \u2264 d(x, y0) \u2264 d(x, y1) = min y\u2208P\\C d(x, y).\nTheorem 4.2. The solution output by the greedy algorithm of k-center is a 3-approximate Max-IP stable clustering.\nProof. To recall, the greedy algorithm of k-center (aka Gonzalez algorithm [Gon85]) starts with an arbitrary point as the first center and then goes through k\u2212 1 iterations. In each iteration, it picks a new point as a center which is furthest from all previously picked centers. Let c1, \u00b7 \u00b7 \u00b7 , ck denote the selected centers and let r := maxv\u2208P d(v, {c1, \u00b7 \u00b7 \u00b7 , ck}). Then, each point is assigned to the cluster of its closest center. We denote the constructed clusters as C1, \u00b7 \u00b7 \u00b7 , Ck. Now, for every i \u0338= j \u2208 [k] and each point v \u2208 Ci, we consider two cases:\n\u2022 d(v, ci) \u2264 r/2. Then\nmax ui\u2208Ci\nd(v, ui) \u2264 d(v, ci) + d(ui, ci) \u2264 3r/2,\nmax uj\u2208Cj\nd(v, uj) \u2265 d(v, cj) \u2265 d(ci, cj)\u2212 d(v, ci) \u2265 r/2.\n\u2022 d(v, ci) > r/2. Then\nmax ui\u2208Ci\nd(v, ui) \u2264 d(v, ci) + d(ui, ci) \u2264 3d(v, ci),\nmax uj\u2208Cj\nd(v, uj) \u2265 d(v, cj) \u2265 d(v, ci).\nIn both cases, maxui\u2208Ci d(v, ui) \u2264 3maxuj\u2208Cj d(v, uj)."
        },
        {
            "heading": "5 Experiments",
            "text": "While the goal and the main contributions of our paper are mainly theoretical, we also implement our optimal Min-IP clustering algorithm as well as extend the experimental results for IP stable clustering given in [AAK+22]. Our experiments demonstrate that our optimal Min-IP stable clustering algorithm is superior to k-means++, the strongest baseline in [AAK+22], and show that our IP clustering algorithm for average distances is practical on real world datasets and is competitive to k-means++ (which fails to find good stable clusterings in the worst case [AAK+22]). We give our experimental results for Min-IP stability and defer the rest of the empirical evaluations to Section C. All experiments were performed in Python 3. The results shown below are an average of 10 runs for k-means++.\nMetrics We measure the quality of a clustering using the same metrics used in [AAK+22] for standardization. Considering the question of f -IP stability (Definition 1.4), let the violation of a point x be defined as Vi(x) = maxCi \u0338=C(x) f(x,C(x)\\{x}) f(x,Ci) .\nFor example, setting f(x,C) = \u2211\ny\u2208C d(x, y)/|C| corresponds to the standard IP stability objective and f(x,C) = miny\u2208C d(x, y) is the Min-IP formulation. Note point x is stable iff Vi(x) \u2264 1.\nWe measure the extent to which a k-clustering C = (C1, . . . , Ck) of P is (un)stable by computing MaxVi = maxx\u2208P Vi(x) (maximum violation) and MeanVi = \u2211 x\u2208P Vi(x)/|P | (mean violation).\nResults For Min-IP stability, we have an optimal algorithm; it always returns a stable clustering for all k. We see in Figures 1 that for the max and mean violation metrics, our algorithm outperforms k-means++ by up to a factor of 5x, consistently across various values of k. k-means ++ can return a much worse clustering under Min-IP stability on real data, motivating the use of our theoretically-optimal algorithm in practice."
        },
        {
            "heading": "6 Conclusion",
            "text": "We presented a deterministic polynomial time algorithm which provides an O(1)-approximate IP stable clustering of n points in a general metric space, improving on prior works which only guaranteed an O(n)-approximate IP stable clustering. We also generalized IP stability to f -stability and provided an algorithm which finds an exact Min-IP stable clustering and a 3-approximation for Max-IP stability, both of which hold for all k and in general metric spaces.\nFuture directions There are multiple natural open questions following our work.\n\u2022 Note that in some cases, an \u03b1-IP stable clustering for \u03b1 < 1 may exist. On the other hand, in the hard example on n = 4 from [AAK+22], we know that there some constant C > 1 such that no C-IP stable clutering exists. For a given input, let \u03b1\u2217 be the minimum value such that an \u03b1\u2217-IP stable clustering exists. Is there an efficient algorithm which returns an O(\u03b1\u2217)-IP stable clustering? Note that our algorithm satisfies this for \u03b1 = \u2126(1). An even stronger result would be to find a PTAS which returns a (1 + \u03b5)\u03b1\u2217-IP stable clustering.\n\u2022 For what specific metrics (other than the line or tree metrics with k = 2) can we get 1-IP stable clusterings efficiently?\n\u2022 In addition to stability, it is desirable that a clustering algorithm also achieves strong global welfare guarantee. Our algorithm gives constant approximation for k-center. What about other standard objectives, such as k-median and k-means?"
        },
        {
            "heading": "A Discussion on the Run-time of Algorithm 2",
            "text": "We remark that the runtime of our O(1)-approximate IP-stable clustering algorithm can potentially be improved if we assume special structure about the metric space, such as a tree or Euclidean metric. In special cases, we can improve the running time by appealing to particular properties of the metric which allow us to either calculate distances or implement our subroutines faster. For example for tree metrics, all distances can be calculated in O(n2) time, even though T = O(n). Likewise for the Euclidean case, we can utilize specialized algorithms for computing the all pairs distance matrix, which obtain speedups over the naive methods [IS22], or use geometric point location data structures to quickly compute quantities such as |B(x, r)| [Sno04]. Our presentation is optimized for simplicity and generality so detailed discussions of specific metric spaces are beyond the scope of the work."
        },
        {
            "heading": "B Random Clustering in Unweighted Graphs",
            "text": "In this appendix, we show that for unweigthed, undirected, graphs (where the distance d(u, v) between two vertices u and v is the length of the shortest path between them), randomly k-coloring the nodes gives an O(1)-approximate IP-stable clustering whenever k = O(n1/2/ log n).\nWe start with the following lemma.\nLemma B.1. Let \u03b3 = O(1) be a constant. There exists a constant c > 0 (depending on \u03b3) such that the following holds: Let T = (V,E) be an unweighted tree on n nodes rooted at vertex r. Suppose that we randomly k-color the nodes of T . Let Vi \u2286 V be the nodes of color i, let Xi = \u2211 v\u2208Vi d(r, v), and\nlet X = \u2211 v\u2208V d(r, v). If k \u2264 c \u221a n logn , then with probability 1\u2212O(n \u2212\u03b3), it holds that X/2 \u2264 kXi \u2264 2X for all i \u2208 [k].\nProof. We will fix i, and prove that the bound X/2 \u2264 Xi \u2264 2X holds with probability 1\u2212O(n\u2212\u03b3\u22121). Union bounding over all i then gives the desired result. Let \u2206 = maxv\u2208V d(r, v) be the maximum distance from the root to any vertex of the tree. We may assume that \u2206 \u2265 5 as otherwise the result follows directly from a simple Chernoff bound. Since the tree is unweighted and there exists a node v of distance \u2206 to r, there must also exist nodes of distances 1, 2, . . . ,\u2206\u2212 1 to r, namely the nodes on the path from r to v. For the remaining nodes, we know that the distance is at least 1. Therefore,\u2211\nv\u2208V d(r, v) \u2265 (n\u2212\u2206\u2212 1) + \u2206\u2211 j=1 j = n+ ( \u2206 2 ) \u2212 1 \u2265 n+ \u2206 2 3 ,\nand so \u00b5i = E[Xi] \u2265 n+\u2206 2/3\nk . Since the variables (d(r, v)[v \u2208 Vi])v\u2208V sum to Xi and are independent and bounded by \u2206, it follows by a Chernoff bound that for any 0 \u2264 \u03b4 \u2264 1,\nPr [|Xi \u2212 \u00b5i| \u2265 \u03b4\u00b5i] \u2264 2 exp ( \u2212\u03b4\n2\u00b5i 3\u2206\n) .\nBy the AM-GM inequality, \u00b5i \u2206 \u2265 1 k\n( n\n\u2206 +\n\u2206\n3 ) \u2265 2 \u221a n\u221a 3k .\nPutting \u03b4 = 1/2, the bound above thus becomes Pr [ |Xi \u2212 \u00b5i| \u2265\n\u00b5i 3\n] \u2264 2 exp ( \u2212 \u221a n\n6 \u221a 3k\n) \u2264 2 exp ( \u2212 \u221a n\n11k\n) \u2264 2n\u2212 1 11c ,\nwhere the last bound uses the assumption on the magnitude of k in the lemma. Choosing c = 111(\u03b3+1) , the desired result follows.\nNext, we state our result on the O(1)-approximate IP-stability for randomly colored graphs.\nTheorem B.2. Let \u03b3 = O(1) and k \u2264 c \u221a n\nlogn for a sufficiently small constant c. Let G = (V,E) be an unweighted, undirected graph on n nodes, and suppose that we k-color the vertices of G randomly. Let Vi denote the nodes of color i. With probability at least 1\u2212 n\u2212\u03b3, (V1, . . . , Vk) forms an O(1)-approximate IP-clustering.\nProof. Consider a node u and let Xi = \u2211\nv\u2208Vi\\{u} d(u, v). Node that the distances d(u, v) are exactly the distances in a breath first search tree rooted at v. Thus, by Lemma B.1, the Xi\u2019s are all within a constant factor of each other with probability 1\u2212O(n\u2212\u03b3\u22121). Moreover, a simple Chernoff bound\nshows that with the same high probability, |Vi| = nk + O (\u221a n logn k ) = \u0398 ( n k ) for all i \u2208 [k]. In\nparticular, the values Yi = Xi|Vi\\{u}| for i \u2208 [k] also all lie within a constant factor of each other which implies that u is O(1)-stable in the clustering (V1, . . . , Vk). Union bounding over all nodes u, we find that with probability 1\u2212O(n\u2212\u03b3), (V1, . . . , Vk) is an O(1)-approximate IP-clustering.\nRemark B.3. The assumed upper bound on k in Theorem B.2 is necessary (even in terms of log n). Indeed, consider a tree T which is a star S on n\u2212k log k vertices along with a path P of length k log k having one endpoint at the center v of the star. With probability \u2126(1), some color does not appear on P . We refer to this color as color 1. Now consider the color of the star center. With probability at least 9/10, say, this color is different from 1 and appears \u2126(log k) times on P with average distance \u2126(k log k) to the star center v. Let the star center have color 2. With high probability, each color appears \u0398(n/k) times in S. Combining these bounds, we find that with constant probability, the average distance from v to vertices of color 1 is O(1), whereas the average distance from v to vertices of color 2 is \u2126 ( 1 + k 2(log k)2\nn\n) . In particular for the algorithm to give an O(1)-approximate IP-stable\nclustering, we need to assume that k = O ( \u221a\nn logn\n) ."
        },
        {
            "heading": "C Additional Empirical Evaluations",
            "text": "We implement our O(1)-approximation algorithm for IP-clustering. These experiments extend those of [AAK+22] and confirm their experimental findings: k-means++ is a strong baseline for IP-stable clustering. Nevertheless, our algorithm is competitive with it while guaranteeing robustness against worst-case datasets, a property which k-means++ does not posses.\nOur datasets are the following. There are three datasets from [DG19] used in [AAK+22], namely, Adult, Drug [FMM+17], and IndianLiver. We also add two additional datasets from UCI Machine Learning Repository [DG19], namely, BreastCancer and Car. For IP-clustering, we also consider a synthetic dataset which is the hard instance for k-means++ given in [AAK+22].\nOur goal is to show that our IP-clustering algorithm is practical and in real world datasets, is competitive with respect to k-means++, which was the best algorithm in the experiments in [AAK+22]. Furthermore, our algorithm is robust and outperform k-means++ for worst case datasets.\nAs before, all experiments were performed in Python 3. We use the k-means++ implementation of Scikit-learn package [PVG+11]. We note that in the default implementation in Scikit-learn, k-means++ is initiated many times with different centroid seeds. The output is the best of 10 runs\nby default. As we want to have control of this behavior, we set the parameter n_init=1 and then compute the average of many different runs.\nAdditionally to the metrics used in the main experimental section, we also compute the number of unstable points, defined as the size of the set U = {x \u2208 M : x is not stable}. In terms of clustering qualities, we additionally measure three quantities. First, we measure \u201ccost\u201d, which is the average within-cluster distances. Formally, Cost = \u2211k i=1 1\n(|Ci|2 )\n\u2211 x,y\u2208Ci,x \u0338=y d(x, y). We then\nmeasure k-center costs, defined as the maximum distances from any point to its center. Here, centers are given naturally from k-means++ and our algorithm. Finally, k-means costs, defined as k-means-cost = \u2211k i=1 1 |Ci| \u2211 x,y\u2208Ci,x \u0338=y d(x, y) 2.\nC.1 Hard Instance for k-means++ for IP-Stability\nWe briefly describe the hard instance for k-means++ for the standard IP-stability formulation given in [AAK+22]; see their paper for full details. The hard instance consists of a gadget of size 4. In the seed-finding phase of k-means++, if it incorrectly picks two centers in the gadget, then the final clustering is not \u03b2-approximate IP-stable, where \u03b2 is a configurable parameter. The instance for k-clustering is produced by concatenating these gadgets together. In such an instance, with a constant probability, the clustering returned by k-means++ is not \u03b2-approximate IP-stable and in particular. We remark that the proof of Theorem 2 in [AAK+22] easily implies that k-means++ cannot have an approximation factor better than nc for some absolute constant c > 0, i.e., we can insure \u03b2 = \u2126(nc). Here, we test both our algorithm and k-means++ in an instance with 8,000 points (for k = 2, 000 clusters).\nIP-Stability results We first discuss five real dataset. We tested the algorithms for the range of k up to 25. The result in Figures 2 and 3 is consistent with the experiments in the previous paper as we see that k-means++ is a very competitive algorithm for these datasets. For small number of clusters, our algorithm sometimes outperforms k-means++. We hypothesize that on these datasets, especially for large k, clusters which have low k-means cost separate the points well and therefore are good clusters for IP-stability.\nNext we discuss the k-means++ hard instance. The instance used in Figure 3 was constructed with \u03b2 = 50. We vary k but omit the results for higher k values since the outputs from both algorithms are stable. We remark that the empirical results with different \u03b2 gave qualitatively similar results. For maximum and mean violation, our algorithm outperforms k-means++ (Figure 3)."
        }
    ],
    "title": "Constant Approximation for Individual Preference Stable Clustering",
    "year": 2023
}