{
    "abstractText": "Parkinson\u2019s disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3, 871 videos from 1, 059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants\u2019 homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable performance, even on two external test sets the model has never seen during training, suggesting the potential for PD risk assessment from smiling selfie videos.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tariq Adnan"
        },
        {
            "affiliations": [],
            "name": "Md Saiful Islam"
        },
        {
            "affiliations": [],
            "name": "Wasifur Rahman"
        },
        {
            "affiliations": [],
            "name": "Sangwu Lee"
        },
        {
            "affiliations": [],
            "name": "Sutapa Dey Tithi"
        },
        {
            "affiliations": [],
            "name": "Kazi Noshin"
        },
        {
            "affiliations": [],
            "name": "Imran Sarker"
        },
        {
            "affiliations": [],
            "name": "M Saifur Rahman"
        },
        {
            "affiliations": [],
            "name": "Ehsan Hoque"
        }
    ],
    "id": "SP:47251b072268737194f50225e7983729747558bf",
    "references": [
        {
            "authors": [
                "A. Abrami",
                "S. Gunzler",
                "C. Kilbane",
                "R. Ostrand",
                "B. Ho",
                "G Cecchi"
            ],
            "title": "Automated computer vision assessment of hypomimia in Parkinson disease: proof-of-principle pilot study",
            "venue": "Journal of Medical Internet Research,",
            "year": 2021
        },
        {
            "authors": [
                "B. Amos",
                "B. Ludwiczuk",
                "M Satyanarayanan"
            ],
            "title": "Openface: A general-purpose face recognition library with mobile applications",
            "venue": "CMU School of Computer Science,",
            "year": 2016
        },
        {
            "authors": [
                "T. Baltrusaitis",
                "A. Zadeh",
                "Y.C. Lim",
                "L.-P. Morency"
            ],
            "title": "Openface 2.0: Facial behavior analysis toolkit",
            "venue": "In 2018 13th IEEE international conference on automatic face & gesture recognition",
            "year": 2018
        },
        {
            "authors": [
                "T. Baltru\u0161aitis",
                "P. Robinson",
                "L.-P. Morency"
            ],
            "title": "OpenFace: An open source facial behavior analysis toolkit",
            "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV), 1\u201310.",
            "year": 2016
        },
        {
            "authors": [
                "L. Biewald"
            ],
            "title": "Experiment Tracking with Weights and Biases",
            "venue": "Software available from wandb.com.",
            "year": 2020
        },
        {
            "authors": [
                "A. Blumer",
                "A. Ehrenfeucht",
                "D. Haussler",
                "M.K. Warmuth"
            ],
            "title": "Occam\u2019s razor",
            "venue": "Information processing letters, 24(6): 377\u2013380.",
            "year": 1987
        },
        {
            "authors": [
                "B.M. Bot",
                "C. Suver",
                "E.C. Neto",
                "M. Kellen",
                "A. Klein",
                "C. Bare",
                "M. Doerr",
                "A. Pratap",
                "J. Wilbanks",
                "E Dorsey"
            ],
            "title": "The mPower study, Parkinson disease mobile data collected using ResearchKit",
            "venue": "Scientific data,",
            "year": 2016
        },
        {
            "authors": [
                "R. Bro",
                "A.K. Smilde"
            ],
            "title": "Principal component analysis",
            "venue": "Analytical methods, 6(9): 2812\u20132831.",
            "year": 2014
        },
        {
            "authors": [
                "N.V. Chawla",
                "K.W. Bowyer",
                "L.O. Hall",
                "W.P. Kegelmeyer"
            ],
            "title": "SMOTE: synthetic minority over-sampling technique",
            "venue": "Journal of artificial intelligence research, 16: 321\u2013 357.",
            "year": 2002
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, 785\u2013794.",
            "year": 2016
        },
        {
            "authors": [
                "C. Cortes",
                "V. Vapnik"
            ],
            "title": "Support-vector networks",
            "venue": "Machine learning, 20: 273\u2013297.",
            "year": 1995
        },
        {
            "authors": [
                "K. Das",
                "C.J. Cockerell",
                "A. Patil",
                "P. Pietkiewicz",
                "M. Giulini",
                "S. Grabbe",
                "M. Goldust"
            ],
            "title": "Machine learning and its application in skin cancer",
            "venue": "International Journal of Environmental Research and Public Health, 18(24): 13409.",
            "year": 2021
        },
        {
            "authors": [
                "E. Dorsey",
                "T. Sherer",
                "M.S. Okun",
                "B.R. Bloem"
            ],
            "title": "The emerging evidence of the Parkinson pandemic",
            "venue": "Journal of Parkinson\u2019s disease, 8(s1): S3\u2013S8.",
            "year": 2018
        },
        {
            "authors": [
                "P. Ekman",
                "R.J. Davidson",
                "W.V. Friesen"
            ],
            "title": "The Duchenne smile: emotional expression and brain physiology: II",
            "venue": "Journal of personality and social psychology, 58(2): 342.",
            "year": 1990
        },
        {
            "authors": [
                "P. Ekman",
                "W.V. Friesen"
            ],
            "title": "Facial action coding system",
            "venue": "Environmental Psychology & Nonverbal Behavior.",
            "year": 1978
        },
        {
            "authors": [
                "M. Feurer",
                "F. Hutter"
            ],
            "title": "Hyperparameter optimization",
            "venue": "Automated machine learning: Methods, systems, challenges, 3\u201333.",
            "year": 2019
        },
        {
            "authors": [
                "T. Fujita",
                "A. Babazono",
                "S. Kim",
                "A. Jamal",
                "Y. Li"
            ],
            "title": "Effects of physician visit frequency for Parkinson\u2019s disease treatment on mortality, hospitalization, and costs: a retrospective cohort study",
            "venue": "BMC geriatrics, 21(1): 1\u201312.",
            "year": 2021
        },
        {
            "authors": [
                "C.G. Goetz",
                "B.C. Tilley",
                "S.R. Shaftman",
                "G.T. Stebbins",
                "S. Fahn",
                "P. Martinez-Martin",
                "W. Poewe",
                "C. Sampaio",
                "M.B. Stern",
                "R Dodel"
            ],
            "title": "Movement Disorder Society-sponsored revision of the Unified Parkinson\u2019s Disease Rating Scale (MDS-UPDRS): scale presentation",
            "year": 2008
        },
        {
            "authors": [
                "L.F. Gomez",
                "A. Morales",
                "J.R. Orozco-Arroyave",
                "R. Daza",
                "J. Fierrez"
            ],
            "title": "Improving parkinson detection using dynamic features from evoked expressions in video",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1562\u20131570.",
            "year": 2021
        },
        {
            "authors": [
                "A. Guryanov"
            ],
            "title": "Histogram-based algorithm for building gradient boosting ensembles of piecewise linear decision trees",
            "venue": "Analysis of Images, Social Networks and Texts: 8th International Conference, AIST 2019, Kazan, Russia, July 17\u201319, 2019, Revised Selected Papers 8, 39\u201350. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "E. Kazemian",
                "H.M. Schaffer",
                "A. Wozniak",
                "J.P. Leonetti"
            ],
            "title": "Economic impact of diagnostic imaging in the workup of uncomplicated Bell\u2019s palsy",
            "venue": "Journal of Neurological Surgery Part B: Skull Base, 83(03): 323\u2013327.",
            "year": 2021
        },
        {
            "authors": [
                "G. Ke",
                "Q. Meng",
                "T. Finley",
                "T. Wang",
                "W. Chen",
                "W. Ma",
                "Q. Ye",
                "T.-Y. Liu"
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "S. Khadilkar"
            ],
            "title": "Neurology in India",
            "venue": "Annals of Indian Academy of Neurology, 16(4): 465.",
            "year": 2013
        },
        {
            "authors": [
                "G. King",
                "A. Lovell",
                "L. Neufcourt",
                "F. Nunes"
            ],
            "title": "Direct comparison between Bayesian and frequentist uncertainty quantification for nuclear reactions",
            "venue": "Physical Review Letters, 122(23): 232502.",
            "year": 2019
        },
        {
            "authors": [
                "E. Kogan",
                "E.-M. Didden",
                "E. Lee",
                "A. Nnewihe",
                "D. Stamatiadis",
                "S. Mataraso",
                "D. Quinn",
                "D. Rosenberg",
                "C. Chehoud",
                "C. Bridges"
            ],
            "title": "A machine learning approach to identifying patients with pulmonary hypertension using real-world electronic health records",
            "venue": "International Journal",
            "year": 2023
        },
        {
            "authors": [
                "R Kohavi"
            ],
            "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection",
            "venue": "In Ijcai,",
            "year": 1995
        },
        {
            "authors": [
                "R. Langevin",
                "M.R. Ali",
                "T. Sen",
                "C. Snyder",
                "T. Myers",
                "E.R. Dorsey",
                "M.E. Hoque"
            ],
            "title": "The PARK Framework for Automated Analysis of Parkinson\u2019s Disease",
            "year": 2019
        },
        {
            "authors": [
                "C. Lugaresi",
                "J. Tang",
                "H. Nash",
                "C. McClanahan",
                "E. Uboweja",
                "M. Hays",
                "F. Zhang",
                "C.-L. Chang",
                "M. Yong",
                "J Lee"
            ],
            "title": "Mediapipe: A framework for perceiving and processing reality",
            "venue": "In Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "S.M. Lundberg",
                "S.-I. Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems 30, 4765\u20134774. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "E. M\u00e4kinen",
                "J. Joutsa",
                "E. Jaakkola",
                "T. Noponen",
                "J. Johansson",
                "M. Pitkonen",
                "R. Levo",
                "T. Mertsalmi",
                "F. Scheperjans",
                "V. Kaasinen"
            ],
            "title": "Individual parkinsonian motor signs and striatal dopamine transporter deficiency: a study with [I-123] FP-CIT SPECT",
            "venue": "Journal of neurology, 266(4):",
            "year": 2019
        },
        {
            "authors": [
                "T. Maycas-Cepeda",
                "P. L\u00f3pez-Ruiz",
                "C. Feliz-Feliz",
                "L. G\u00f3mezVicente",
                "R. Garc\u0131\u0301a-Cobos",
                "R. Arroyo",
                "P.J. Garc\u0131\u0301a-Ruiz"
            ],
            "title": "Hypomimia in Parkinson\u2019s disease: what is it telling us",
            "venue": "Frontiers in Neurology,",
            "year": 2021
        },
        {
            "authors": [
                "G. Morinan",
                "Y. Dushin",
                "G. Sarapata",
                "S. Rupprechter",
                "Y. Peng",
                "C. Girges",
                "M. Salazar",
                "C. Milabo",
                "K. Sibley",
                "T Foltynie"
            ],
            "title": "Computer vision quantification of whole-body Parkinsonian bradykinesia using a large multisite population. npj Parkinson\u2019s Disease",
            "year": 2023
        },
        {
            "authors": [
                "B. Post",
                "L. Van Den Heuvel",
                "T. Van Prooije",
                "X. Van Ruissen",
                "B. Van De Warrenburg",
                "J. Nonnekes"
            ],
            "title": "Young onset Parkinson\u2019s disease: a modern and tailored approach",
            "venue": "Journal of Parkinson\u2019s disease, 10(s1): S29\u2013S36.",
            "year": 2020
        },
        {
            "authors": [
                "M. Rajnoha",
                "J. Mekyska",
                "R. Burget",
                "I. Eliasova",
                "M. Kostalova",
                "I. Rektorov\u00e1"
            ],
            "title": "Towards identification of hypomimia in Parkinson\u2019s disease based on face recognition methods",
            "venue": "2018 10th International Congress on Ultra Modern Telecommunications and Control Systems",
            "year": 2018
        },
        {
            "authors": [
                "L. Ricciardi",
                "A. De Angelis",
                "L. Marsili",
                "I. Faiman",
                "P. Pradhan",
                "E. Pereira",
                "M. Edwards",
                "F. Morgante",
                "M. Bologna"
            ],
            "title": "Hypomimia in Parkinson\u2019s disease: an axial sign responsive to levodopa",
            "venue": "European Journal of Neurology, 27(12): 2422\u20132429.",
            "year": 2020
        },
        {
            "authors": [
                "P.J. Rousseeuw"
            ],
            "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
            "venue": "Journal of Computational and Applied Mathematics, 20: 53\u201365.",
            "year": 1987
        },
        {
            "authors": [
                "L. Seyyed-Kalantari",
                "H. Zhang",
                "M.B. McDermott",
                "I.Y. Chen",
                "M. Ghassemi"
            ],
            "title": "Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations",
            "venue": "Nature medicine, 27(12): 2176\u20132182.",
            "year": 2021
        },
        {
            "authors": [
                "K. Shimkin"
            ],
            "title": "From MicrobeWiki, the student-edited microbiology resource This is a curated page",
            "venue": "Report corrections to Microbewiki.",
            "year": 2015
        },
        {
            "authors": [
                "L. Tickle-Degnen",
                "K.D. Lyons"
            ],
            "title": "Practitioners\u2019 impressions of patients with Parkinson\u2019s disease: the social",
            "year": 2004
        },
        {
            "authors": [
                "B. vathe"
            ],
            "title": "A method to infer emotions from facial",
            "year": 2011
        },
        {
            "authors": [
                "D. Weinshall"
            ],
            "title": "Quantifying hypomimia in parkinson",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "There is no reliable biomarker for diagnosing Parkinson\u2019s disease (Yang et al. 2022), the fastest-growing neurological disorder in the world (Dorsey et al. 2018). In 2017, Parkinson\u2019s disease placed a significant economic burden of $52 billion in the United States (Yang et al. 2020). Given the projected doubling of PD patients by 2040, the scenario will worsen significantly, surpassing $79 billion even without accounting for inflation (Yang et al. 2020). The significance of timely diagnosis and regular checkups for PD cannot be overstated, as they can improve the quality of a patient\u2019s life and reduce the burden of neurological care (Fujita et al. 2021). However, there are challenges, particularly for the elderly population who are more likely to be diagnosed with PD (Dorsey et al. 2018). Consider the scenario of an individual in their sixties or seventies experiencing impaired cognitive and physical ability, leading to immobility. Accessing timely clinical diagnosis becomes challenging if they reside in a remote area where the nearest clinic is not within reasonable driving distance. Furthermore, in many\n*Equal contribution Manuscript submitted to Nature Medicine.\ndeveloping or underdeveloped countries, there is a significant scarcity of neurologists (e.g., in India, there were only 1200 neurologists available to serve over 1.3 billion people in 2013 (Khadilkar 2013).) This limited accessibility to clinical care results in many individuals remaining undiagnosed until the disease has progressed considerably, and the usefulness of medications available to manage involuntary tremors caused by PD becomes very limited.\nAt-home PD assessment has received significant attention from researchers in recent times (Bot et al. 2016; Morinan et al. 2023; Yang et al. 2022). One promising method involves analyzing nocturnal breathing signals obtained from radio waves reflected by the body. This approach has shown potential in detecting PD and could be a non-invasive and convenient way to monitor individuals at home for signs of the disease (Yang et al. 2022). Another characteristic often associated with PD is Hypomimia, which refers to a reduction in facial expressions. People with PD may experience Hypomimia due to a decrease in dopamine synthesis critical for facial expression, caused by the loss of certain neurons (Shimkin 2015; Ma\u0308kinen et al. 2019). While Hypomimia is not exclusive to PD, it is often considered an early and sensitive biomarker for the disease and can be utilized for early screening (Vinokurov et al. 2015; Rajnoha et al. 2018; Ricciardi et al. 2020; Abrami et al. 2021; Maycas-Cepeda et al. 2021). One notable advantage of assessing Hypomimia is that it can potentially be done with a computer or laptop that a participant may already have at home. Compared to measuring breathing signals which needs installation of sensors in people\u2019s homes, checking for Hypomimia through facial expressions is easier, more accessible, and can be used anywhere in the world.\nIn this study, we present an AI-based system capable of objectively quantifying signs of Hypomimia, and screening individuals for Parkinson\u2019s disease. We collected 3871 videos involving micro-expressions from 1059 global participants, where each participant was asked to mimic three different facial expressions: disgust, smile, and surprise on a web interface. Leveraging advanced computer vision tools like MediaPipe (Lugaresi et al. 2019) and OpenFace (Baltrus\u030caitis, Robinson, and Morency 2016), we analyze the facial landmarks and action units (Ekman and Friesen 1978) to objectively quantify features of Hypomimia following the Movement Disorder Society-Sponsored Revision of the Uni-\nar X\niv :2\n30 8.\n02 58\n8v 1\n[ ee\nss .I\nV ]\n3 A\nug 2\n02 3\nfied Parkinson\u2019s disease Rating Scale (MDS-UPDRS). The large video dataset collected from a diverse study population aligns with our effort to make the benefits of the PD screening system available to everyone, irrespective of age, ethnicity, and sex. Testing the system on external cohorts in Bangladesh (representing South-East Asia, 8.5% of global population) and the United States (representing Northern America, 4.7% of global population) further validates its effectiveness across diverse demographics. The majority of our data are collected from participants\u2019 homes encompassing a diverse range of environments and recording devices, thus eliminating the need for inconvenient travel and reducing the possibility of data shift when deployed to be used from participants\u2019 homes. Using an ensemble of multiple AI models (i.e., support vector machine) that incorporate features from all three micro-expressions, our system aims to deliver a reliable, interpretable screening for Parkinson\u2019s disease from the comfort of home and empower potential patients to seek clinical care and effectively manage their PD symptoms. Further analysis suggests that comparable performance is obtainable by merely using the smile videos. In Figure 1, we present an overview of our proposed framework.\n2 Results"
        },
        {
            "heading": "Data",
            "text": "In this research, we gathered video data from four distinct settings, comprising home-recorded videos from i) global participants (Home-Global)\u2013primarily from North America; ii) a clinic at the University of Rochester Medical Center (Clinic); iii) a Parkinson\u2019s disease care facility in Ohio (PD Care Facility); and iv) home-recorded videos from Bangladesh, a country in Southeast Asia (Home-BD). Our study involved 1059 participants, with 256 identifying themselves as PD patients. Each participant was instructed to mimic three facial expressions (disgust, smile, and surprise) and return to a neutral face, repeating this process three times for each expression. Notably, some participants from the U.S. clinic contributed data on multiple occasions, resulting in 1236 videos from participants with PD and 2665 videos from participants without it. Table 1 provides details regarding the demographic characteristics of the entire participant set. Data collection was facilitated through a web-based platform named PARK (Langevin et al. 2019)1, allowing participants to conveniently record themselves using their personal laptop\u2019s webcam. PARK was translated into the Bengali language to better instruct participants from Bangladesh. Data from Home-Global and PD Care Facility settings were used to train and evaluate the model on held-out data. Data collected in Clinic and Home-BD were solely used for external testing of the predictive model, and were never used during training. Supplementary information provides further specifics on the demographic properties of the data collected from each setting.\n1https://parktest.net/"
        },
        {
            "heading": "Facial Features as Potential Digital Biomarkers",
            "text": "The loss of facial expressivity, abnormal eye blinking frequency, and lack of spontaneity in smiles are key indicators associated with Parkinson\u2019s disease (PD) and are included in the MDS-UPDRS, a clinical guideline for PD assessment (Goetz et al. 2008). In this study, we aim to objectively and quantitatively capture these clinically relevant features using advanced facial landmark detection tools such as Mediapipe (Lugaresi et al. 2019) and OpenFace (Baltrusaitis et al. 2018). We extract the intensity of specific facial action units (Ekman and Friesen 1978) associated with each facial expression task (Velusamy et al. 2011). For instance, when someone smiles, various action units 2 including inner brow raiser, cheek raiser, lip corner puller, dimpler, lips part, jaw drop, and eye blink may be activated. Moreover, certain action units like lips part, jaw drop, and eye blink can be activated by any of the three facial expressions used in this study. Additionally, we measure the degree of eye openness, raised eyebrow height for both the right and left eyes, mouth width and openness, and jaw openness using Mediapipe. These features have shown promise in previous literature for PD detection (Gomez et al. 2021). All of these features are computed for each frame of the recorded videos, and statistical aggregates (mean, variance, and entropy) are used to summarize the features across all frames. This allows us to train simple machine-learning models to assess PD. For\n2https://github.com/TadasBaltrusaitis/OpenFace/blob/master/ imgs/au sample.png"
        },
        {
            "heading": "Expression Feature Statistic p-value Coefficient Rank",
            "text": "each participant, we combine the task-specific statistical aggregates for all three tasks, resulting in 126 comprehensive features. These features are then utilized as inputs for the machine learning models designed to differentiate between individuals with and without PD. Out of the 126 features, 43 features were significantly different across participants with and without PD (at significance level, \u03b1 = 0.01). To find out the most powerful features for discriminating between participants with and without PD, we ran a logistic regression analysis on the entire dataset after normalizing the feature values and ranked the features based on the coefficient of logistic regression. Table 2 identifies the top-10 most significant features."
        },
        {
            "heading": "Predictive Performance on Held-out Data",
            "text": "With all three facial expressions: Among the videos collected in four different settings, we utilized the datasets from Home-Global and PD Care Facility to train and validate the PD prediction model. To train the model, we combined features from all three facial expression tasks so that each unique participant corresponds to a single data point, ensuring that training and test subjects remain separated. Any participant who did not provide video recordings for all three facial expressions was excluded from the analysis, resulting in 827 data points for training and testing. To assess model performance, we ran a k-fold crossvalidation (k = 10) with stratified sampling (i.e., maintaining the same ratio of participants with and without PD across the train and test sets). This approach is also known as stratified k-fold cross-validation and is preferred over traditional k-fold cross-validation when the dataset is imbalanced (Kohavi et al. 1995). Out of several model choices, the best-performing model was an ensemble of 21 support vector machines (SVM). Note that these high-performing SVM models differ from each other by the hyper-parameters (including the set of features) they use and they were selected using a meticulous\noptimization via Bayesian hyper-parameter tuning (details in the Methods section). At each iteration of k-fold crossvalidation, the entire ensemble model was trained on (k\u22121) folds out of k folds, while the remaining fold was held out for testing. k different iterations selected different folds as the test set, ensuring that the model was evaluated on all the data samples. Given the imbalanced nature of the dataset, we used minority oversampling (Chawla et al. 2002) on the training sets, while the test sets remained unchanged. Our model exhibited robust performance based on numerous widely accepted metrics when tested on the held-out data (Figure 2). In differentiating between participants with and without Parkinson\u2019s disease, our ensemble model achieved an accuracy of 89.72%, with an Area Under the Receiver Operating Characteristic Curve (AUROC) of 89.31%. The model displayed a specificity of 93.67% and a sensitivity of 76.92%. Additionally, the positive predictive value (PPV) of 78.95% and the negative predictive value (NPV) of 92.94% further reinforce the model\u2019s reliability in predictive performance.\nWith only the smile expression We also explored whether we can differentiate individuals with and without PD by looking at a single facial expression. Although the performance of the predictive model notably degraded while using only one of the three facial expressions, the performance remained competitive for the smile expression. We followed the same training and testing strategy as previously described but with a slight modification. Rather than utilizing features from all three facial expressions (disgust, smile, and surprise), we focused on using features from a specific facial expression in each experiment. For the smile expression, an ensemble of 17 SVM models performed the best, achieving an accuracy of 87.30% and an AUROC score of 83.04%. In addition, the specificity, sensitivity, PPV, and NPV of the predictive model were 91.93%, 72.31%, 73.44%, and 91.50%, respectively. Unlike the smile expression, the performance of the predictive model was much worse when features from only disgust or surprise expressions were used. The accuracy and AUROC of the best predictive model were 77.51% and 76.24% respectively, when only disgust features were used. Similarly, the best model using only surprise features achieved an accuracy of 77.03% and an AUROC of 72.45%. Using principal component analysis (Bro and Smilde 2014), we further observed that the features extracted from the smile task are relatively more separable (silhouette score (Rousseeuw 1987) = 0.18) between subgroups of participants with and without PD compared to the disgust (silhouette score = 0.11) and surprise tasks (silhouette score = 0.14), as shown in Figure 4. This may potentially explain why the model trained on smile expression features performed better than the other expressions."
        },
        {
            "heading": "Generalization to External Test Data",
            "text": "We excluded data obtained from two settings, namely Clinic and Home-BD, from the training process of the model. Instead, we specifically reserved this data for external validation. Conducting external validation on data\nobtained from a clinic (located in New York, US) offers preliminary insights into the model\u2019s performance when implemented in a clinical environment. Additionally, testing the model on data collected from a different country (Bangladesh) enables us to gain a further understanding of how the features and potential PD symptoms may differ among individuals from a distinct cultural backgrounds compared to the United States. In general, achieving robust performance on data collected from previously unseen settings and diverse geographical locations, ranging from New York to Ohio to Bangladesh, serves as evidence of the model\u2019s reliability and suggests that it is less susceptible to data shift (Zhang et al. 2022).\nWhile the combination of features from all three facial expressions resulted in the highest performance on the held-out data, we observed a significant decrease in performance when testing the model on external test sets. Specifically, when the ensemble model trained on features from all three facial expressions was evaluated on data collected in the Clinic setting, the accuracy dropped to 72.0% (an absolute decrease of 17.73%) and the AUROC score dropped to 72.26% (an absolute decrease of 17.05%). Similarly, when tested on data collected from Bangladesh in the Home-BD setting, the predictive model achieved an accuracy of 79.87% and an AUROC of 77.38%. These results highlight a similar decline in performance when the model was applied to external data from different settings, demonstrating the challenges of generalization beyond the training data.\nHowever, when the model was trained solely on smile expression features, the decline in performance was significantly less pronounced. The ensemble of support vector machine (SVM) models trained on smile expression features achieved an accuracy of 81.33% and an AUROC score of 83.93% when evaluated on the Clinic setting. Although we observed a decrease of 5.98% in accuracy, the AUROC score even increased by a small margin of 0.89% compared to the performance on held-out data. The sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) on this external test data were 78.57%, 82.98%, 86.67%, and 73.33% respectively. For the external Home-BD test set, the model performed similarly to the one trained on all three facial expressions, achieving an accuracy of 78.52% and an AUROC score of 78.54%. It is worth noting that while the specificity (78.52%), sensitivity (78.57%), and NPV (97.25%) remained competitive, we observed a sharp decline in PPV (27.5% on the Home-BD setting compared to 73.44% on held-out data). This sharp drop in PPV (27.78% compared to 78.95% on held-out data) was also observed when all three facial expressions were used."
        },
        {
            "heading": "Bias Analysis on Held-out Data",
            "text": "In order to make the screening framework proposed in this study accessible to individuals worldwide, it is crucial to assess the model\u2019s performance across different subgroups based on factors such as sex, ethnicity, and age. As mentioned earlier, we performed k-fold cross-validation (k = 10) to evaluate the model using the data samples from Home-Global and PD Care Facility settings.\nFor each of the test data, we log the demographic information, the true label of PD diagnosis, and the model-predicted score. These logs are used to analyze whether the model demonstrated any systematic bias across subgroups. Participants were left out of a subgroup-specific analysis if the sub-grouping attribute (e.g., ethnicity) was missing. For all significance testing, we used a 95% confidence level. Figure 3 provides a visual overview of the subgroup-based analyses.\nMiss-classification. One criterion for bias assessment is whether the model has a higher miss-classification rate for a certain subgroup. To assess this, we performed a twosampled Z-test (for proportions) across population subgroups based on sex and ethnicity. On average, the predictive model (ensemble of SVM) exhibited a miss-classification rate of 11.91% for male subjects (n = 361) and a rate of 9.01% for female subjects (n = 466). However, the difference in miss-classification rates between the two groups was not statistically significant (test statistic, z = 1.36, p-value = 0.17). Similarly, there was no detectable bias when comparing white subjects (n = 574) with non-white subjects (n = 119), as the average miss-classification rates were 10.45% and 5.88% respectively (z = 1.54, p-value = 0.12). As age is a continuous variable, we performed a Spearman\u2019s correlation test to evaluate the relationship between age and the average miss-classification rate among subjects of that age. Age was found to be positively correlated with the miss-classification rate (Spearman\u2019s rank correlation, \u03c1 = 0.26, p-value = 0.03) meaning the model demonstrated lower accuracy for older subjects.\nUnderdiagnosis. Many AI models tend to selectively underdiagnose under-served patient populations which can lead to unequal access to clinical care (Seyyed-Kalantari et al. 2021). To investigate whether our predictive model demonstrated similar bias, we ran the subgroup analysis of miss-classification only for the subjects who actually had PD. We used a two-sampled Z-test (for proportions) to assess underdiagnosis bias based on sex. However, we had a small number of samples (n = 7) for non-white subjects with PD. Therefore, we used Fisher\u2019s exact test to investigate underdiagnosis bias based on ethnicity. To assess whether age is correlated with the underdiagnosis rate, we used Spearman\u2019s correlation test. We did not observe any underdiagnosis bias of the predictive model at a statistically significant level based on sex and gender. Specifically, the underdiagnosis rates for the male (n = 108) and female (n = 87) subjects were 22.22% and 24.13% respectively, showing no significant difference (z = 0.32, p-value = 0.75). Again, 37.14% (n = 70) white subjects were underdiagnosed by the model, while the rate was 28.6% (n = 7) for non-white subjects. Based on Fisher\u2019s exact test, this difference was also insignificant (Fisher\u2019s odd ratio = 0.68, p-value = 1). Although there was a slight negative correlation (\u03c1 = \u22120.28) between age and underdiagnosis rate, the correlation was not statistically significant (p-value = 0.07). However, this means that younger subjects were slightly more underdiagnosed to have PD compared to the elderly.\nOverdiagnosis. Overdiagnosis is when the model predicts someone to have a condition, although they do not actually have that condition. To test whether our predictive model overdiagnosed any of the subgroups, we used the same methods used for analyzing underdiagnosis bias. However, instead of analyzing the subjects who had PD, we analyzed the subjects who did not have PD. We did not detect any overdiagnosis bias based on sex (253 male subjects, 379 female subjects, z-score = 1.00, p-value = 0.32) and ethnicity (504 white subjects, 112 non-white subjects, Fisher\u2019s odd ratio = 0.65, p-value = 0.52). However, older participants were significantly more overdiagnosed by the predictive model, as we observed a positive correlation between age and overdiagnosis rate (\u03c1 = 0.26, p-value = 0.04)."
        },
        {
            "heading": "Bias Analysis on External Data",
            "text": "We also conducted an evaluation of our predictive model using two external test datasets, namely Clinic and Home-BD. However, our bias analysis for these datasets was limited due to a lack of representative subgroups. For instance, in the Home-BD dataset, all participants self-identified themselves as Asian, while in the Clinic dataset, all but two participants identified themselves as white. Consequently, we were unable to assess bias based on ethnicity. Furthermore, conducting underdiagnosis or overdiagnosis analyses required subgrouping participants with and without Parkinson\u2019s disease, respectively. Unfortunately, many of these subgroups contained fewer than five participants, rendering statistical inference highly unreliable. Therefore, we solely performed bias analysis based on miss-classification rates for these external test sets.\nIn the Clinic setting, the model exhibited a missclassification rate of 28.1% (n = 32) for female participants, compared to only 11.6% for male subjects (n = 43). However, this difference in performance between genders was not statistically significant according to a two-sample Z-test (for proportions) (z = \u22121.81, p-value = 0.07). On the other hand, in the Home-BD setting, the situation was reversed. The model miss-classified male participants (n = 103) 25.2% times while the rate was only 13.0% for the females (n = 46). Again, the two-sampled Z-test (for proportions) yielded a test statistic of 1.68 and a p-value of 0.09, indicating no significant difference in the miss-classification rates. Additionally, for both settings, the miss-classification rate was not significantly correlated with age (Spearman\u2019s rank correlation coefficients were \u22120.10 and \u22120.10 for the Clinic and Home-BD settings, respectively, with corresponding p-values of 0.60 and 0.87)."
        },
        {
            "heading": "Ablation Studies",
            "text": "We experimented with different setups, including several machine learning baselines to find the best predictive model (please see Table 3). As machine learning baselines, we tried XGBoost (Chen and Guestrin 2016), LightGBM (Ke et al. 2017), Random Forest, AdaBoost, Histogram-based Gradient Boosting (HistGradientBoosting) (Guryanov 2019), and Support Vector Machine (SVM) (Cortes and Vapnik 1995). Rather than relying on a single metric, we employed a holis-\ntic evaluation approach by considering all the reported metrics (i.e., AUROC, Accuracy, Sensitivity, Specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV)). Overall, when evaluated holistically, SVM outperformed all other baselines in terms of predictive performance.\nWe observed that an ensemble approach, incorporating the highest-performing models, delivered improved performance compared to a single model. This method particularly boosted the specificity, PPV, and accuracy, with other evaluation metrics demonstrating comparable performance. In order to consolidate the decisions from the top-performing models, we employed a secondary model, which operated as the final classifier in our ensemble system. The top-k models\u2019 outputs served as input to this secondary model (with k being a hyper-parameter), which then generated a single binary outcome, denoting the presence or absence of Parkinson\u2019s disease (PD) in an individual. We experimented with both Support Vector Machine (SVM) and logistic regression as our final classifier. Between the two, logistic regression demonstrated superior performance, making it our choice for the final stage of decision-making in the ensemble model. In addition, selecting top-n features (n is a hyper-parameter and the features are ranked based on the weights of a logistic regression model) and applying MinMax Scaler to ensure all the features are scaled into similar ranges helped boost the performance.\nWhen features from a single expression were used, the smile performed better than the other two expressions. We also experimented with combining features from all possible mixing of three facial expressions. The competitive performances were achieved by adding features from other expressions with the smile features. Moreover, it is noteworthy that all the competitive models included features from the smile task, underscoring the importance of utilizing smile expression as a promising biomarker for PD detection.\nWe used both Openface and MediaPipe tools to extract facial keypoints. However, only using one of them yielded a relatively poor performance as shown in Table 3. Finally, since the dataset was imbalanced (i.e., the ratio of participants without and with PD was more than 3 : 1), we applied a synthetic minority oversampling technique named SMOTE (Chawla et al. 2002) on the training data, leading to a better performance on the test data."
        },
        {
            "heading": "3 Discussion",
            "text": "This study presents an opportunity to revolutionize the future of technology in the diagnosis and treatment of facial muscle-related diseases. Similar to the advancements in home glucose monitoring devices that have greatly benefited individuals with diabetes, the utilization of AI models and home-recorded micro-expressions for screening Parkinson\u2019s disease (PD) holds immense potential. The current methods for diagnosing PD rely on subjective clinical assessments and trained healthcare professionals\u2019 observations, which can be time-consuming, costly, and prone to interobserver variability. In contrast, our proposed non-invasive and scalable method offers a convenient approach for PD\nscreening from the comfort of individuals\u2019 homes. The remarkable progress of AI in healthcare demonstrated in areas such as skin cancer detection from images (Das et al. 2021) and predicting heart failure risk using electronic health records (Kogan et al. 2023), further emphasizes the transformative power of AI in disease diagnosis. Neurological disorders like Blepharospasm, Meige syndrome, and facial nerve disorders such as Bell\u2019s palsy share similarities with Parkinson\u2019s disease, as all of these diseases profoundly affect facial muscle function and expressions. Leveraging videos of smiling selfies for diagnosing uncomplicated Bell\u2019s palsy could save a significant amount of resources currently spent on imaging (Kazemian et al. 2021). In addition, by allowing individuals to perform initial assessments at home, our proposed invention has the potential to expedite the evaluation of these disorders.\nAs we explore the potential integration of AI-driven predictive models into clinical care, it is imperative to evaluate the reliability of these models using data generated in environments different from those used for training. This step is crucial to ensure that the models can generalize effectively and maintain their performance when faced with real-world variations and diverse patient populations. To this end, we employed two distinct cohorts: a local clinical cohort consisting of 75 subjects (47 diagnosed with PD) from the United States, and a cohort from Bangladesh comprising 149 subjects (14 with PD). Bangladesh, being a developing country with a distinct culture compared to the United States, provides a valuable contrast in terms of environmental and socio-cultural factors.\nOn the clinical cohort, the predictive model achieved 81.33% accuracy and 83.93% AUROC score. The PPV of the model was 86.67%, meaning for every 100 participant classified by our model to have PD, on average, 87 of them actually had PD. Such high PPV is crucial for a screening tool, as it helps minimize false positives and prevents unnecessary distress for the individuals or burden on the already strained clinical care system. By ensuring that individuals with positive screening results are more likely to have PD, we can mitigate the risk of overwhelming the healthcare system with unnecessary clinic visits. However, it is important to note that the sensitivity of the model currently stands at 78.57% and requires further improvement. Currently, the model may miss about 21% of participants with PD, and caution should be exercised when interpreting the model\u2019s output. Individuals should be informed about the potential errors of the model, and it is recommended that those with access to neurological care and suspicion of having PD symptoms should schedule a clinical visit regardless of the model\u2019s output. Nevertheless, our model remains particularly suitable for individuals with limited access to clinical care, offering a valuable screening tool for such populations.\nThe cohort from Bangladesh presents a unique dataset due to its participants belonging to a culture distinct from that of the United States. The predictive model achieved 78.52% accuracy and 79.87% AUROC score when evaluated on the external Bangladeshi cohort. While the specificity and sensitivity of the model aligned with those of the clinical cohort,\na significant discrepancy emerged in the positive predictive value (PPV) at 27.5%. This finding suggests the presence of cultural variances in the expression of smiles, warranting further investigation. It is essential to recognize that for every successful PD diagnosis prompted by our model, there may be approximately four instances in which individuals seek clinical diagnosis but are ultimately found not to have PD. While this may introduce inconvenience and potentially result in unnecessary clinical visits for some individuals, it is crucial to consider the potential impact of the model on improving the quality of life for those who do receive an accurate PD diagnosis. This consideration is particularly important in a resource-constrained country like Bangladesh, where the model can still hold utility within a clinical setting by improving access to care.\nIn our study, we have identified that features extracted from the smile expression demonstrated superior discrimination between participants with and without PD (see Figure 4). When tested on external data that the predictive models had never seen during training, models trained entirely on smile features displayed greater generalizability (see Figure 2). These models focusing solely on smile features were relatively simpler, requiring fewer parameters, yet outperformed models considering combinations of disgust, smile, and surprise features. This observation aligns with the philosophical principle of Occam\u2019s razor (Blumer et al. 1987), suggesting that simpler theories are often better. Furthermore, during the analysis of facial expressions in this study, which included disgust, smile, and surprise, we observed that the smile expression exhibited the least interperson variability and was easily replicated compared to the other expressions. While individuals may express disgust or surprise in various ways, smiles were more universally recognizable and required fewer instructions for participants, making them highly suitable for our home deployable ubiquitous tool. Furthermore, the predictive model demonstrated its utility across different geographical locations, including the east and midwest regions of the United States, as well as a South Asian country. This observation raises the possibility of investigating the model\u2019s potential for screening individuals with PD in regions where access to neurologists is extremely limited (e.g., African countries). It is notable that we deployed a translated version of the PARK tool where the instructional videos were recorded by native speakers, which might be an important criterion to consider to investigate similar technology for countries with different languages and cultures.\nIt is imperative to enhance data diversity and representation within the training set before deploying the model in different socio-cultural settings to minimize potential risks and ensure its optimal performance. Although the webbased PARK tool we used for collecting training data is intended to be used from anywhere in the world, the participants were dominated by U.S. residents (754 out of 827, 91.2%). This lack of representation from diverse geographic regions is a limitation that should be addressed to improve the generalizability of the model. Additionally, it is worth mentioning that our evaluation of the predictive model was conducted on a single external clinical cohort and one cohort\nfrom a developing country. To draw more robust and conclusive results regarding the effectiveness of the proposed model, it may have been advantageous to have test data from multiple clinics, PD care facilities, and countries. Targeted advertisement and cross-country collaboration could help us obtain data from other countries, which remains a future goal of this study.\nEnsuring that the benefits of a predictive tool are accessible to individuals regardless of ethnicity, sex, and age requires a critical examination of potential biases within the model. To this end, we performed extensive bias analysis across held-out and external test data, although some analyses on the external test data were limited due to small subgroup sizes. On held-out data, our model did not demonstrate any detectable bias based on protected attributes such as ethnicity and sex. However, the model was less accurate for older participants, the underdiagnosis rate was slightly higher for the younger subjects, and the overdiagnosis rate was higher for the older subjects. Similar findings also apply to a clinical setting where young PD patients may have a longer journey to diagnosis because they do not fit the profile of a typical patient (Post et al. 2020). Propagation of existing bias in AI models is undesired and remains a limitation of this study.\nTo minimize the potential harm of mispredictions, it is indeed crucial to evaluate the reliability of model predictions. One approach to enhance the clinical utility of the model is to reframe the PD screening problem as a three-way classification instead of a binary (individuals with PD vs without it) classification. This revised approach would involve categorizing individuals into three groups: (i) likely to have PD, (ii) unlikely to have PD, and (iii) uncertain. An ideal screening framework should indeed demonstrate high performance in correctly classifying individuals into the likely and unlikely to have PD categories. These categories would encompass the majority of the data, where the model\u2019s predictions are expected to be reliable and accurate. This high-performance level ensures that individuals who likely have PD are appropriately identified for further evaluation and necessary care, while those without PD are accurately classified, minimizing false positives. However, it is important to acknowledge that no screening model is perfect, and there will always be a portion of data where the model\u2019s predictions are uncertain. This uncertainty category accounts for cases where the model\u2019s confidence falls below a certain threshold or where the features extracted from the data are inconclusive. This small subset of uncertain predictions serves as a reminder that further clinical evaluation and assessment are necessary to determine the true status of these individuals. However, reliably assessing uncertainty in a predictive model that follows a frequentist approach is indeed challenging (King et al. 2019). In the future, we plan to explore Bayesian approaches which can provide more direct and intuitive measures of uncertainty to investigate screening as a three-way classification problem."
        },
        {
            "heading": "4 Methods",
            "text": ""
        },
        {
            "heading": "Data Collection",
            "text": "Data Collection Framework. Our study employed a video dataset sourced from 1059 participants, comprising 256 of them diagnosed with Parkinson\u2019s disease (PD) and 803 without the condition. We used PARK3 (Langevin et al. 2019), a web-based framework for data collection. PARK is a web-based tool designed to collect videos of participants performing a series of tasks. For each task, participants are presented with a brief instructional video illustrating how to accurately perform it. Among the recorded videos of more than 20 tasks, we collected the videos corresponding to three facial expressions (disgust, smile, and surprise) for this study. In each of the facial expression tasks, participants were asked to portray that particular expression as expressively as possible, sustain the expression for a few seconds, and then revert to a neutral facial expression. This cycle was to be repeated three times in total for each of the facial expressions, resulting in videos that typically spanned 8 \u2212 12 seconds. In addition to the video recordings, PARK also collected demographic information about the participants such as their age, sex, ethnicity, and country of origin. Besides, participants self-reported whether they were diagnosed with Parkinson\u2019s disease or not, which was used as ground truth for training and evaluating the models.\nDataset Details. We collected data from four different settings:\n\u2022 Home recorded videos from global participants (Home-Global): We advertised the PARK tool using social media, emailed participants who expressed interest in PD research recorded in a clinical study registry, and verbally reached out to PD patients interested to contribute in PD research to collect home-recorded videos from global participants. This way, we collected data from 693 global participants who recorded themselves using a computer webcam PARK website. However, this cohort was mostly dominated by the US residents (620 out of 693), and participants who did not have PD (616 out of 693).\n\u2022 Videos recorded at a PD care facility (PD Care Facility): We deployed the PARK tool at InMotion4, a PD care facility located in Ohio, United States. The facility provides comprehensive support to PD patients, offering activities from exercises to educational sessions and counseling on maintaining motivation and leading a fulfilling life. We were able to collect video data (recorded by a laptop webcam) from both their clients (118 participants with PD) and their caregivers (24 participants without PD). Note that, in many cases, during data collection, the participants were assisted by caregivers at the InMotion facility.\n\u2022 Videos recorded in a US Clinic (Clinic): As part of a clinical study conducted by the University of Rochester Medical Center (URMC) located in New York, United\n3https://parktest.net/ 4https://beinmotion.org/\nStates, willing participants recorded themselves using the PARK tool. Some of the participants were supervised and/or assisted by clinical study team members during the recording process. We collected data from 75 participants (47 with PD) in this setting.\n\u2022 Home recorded videos from Bangladesh (Home-BD): In this setting, data collection took place in Bangladesh, a Southeast Asian country. This was specifically facilitated by the Bangladesh University of Engineering & Technology (BUET). In the previous three settings, the PARK tool was administered in English, which could present a language barrier for the elderly population whose native language is not English. To address this issue, we undertook the task of translating the entire PARK website into Bengali, the local language, and re-recording the instructional videos (in Bengali) with native speakers. A portion of data collection was conducted under the direct supervision of our study team members at BUET, and the rest of the participants recorded their videos at home. This setting contributed a valuable dataset from 149 participants, including 14 with Parkinson\u2019s disease (PD), significantly enhancing the geographical and cultural diversity of our data.\nData Quality Assurance. The performance of any machine learning model largely depends on the quality of the dataset. Therefore, to ensure the utmost quality of our data, we took extensive measures to facilitate clear and straightforward instruction videos for the participants. This was supplemented by guidelines encouraging participants to record themselves in well-lit environments, against distinguishable backgrounds, and to position their full faces within the video frame. Despite these guidelines, we recognized an inherent trade-off between the optimal quality of recorded videos and the home accessibility of our video recording framework. Notably, many of our participants were elderly individuals, and expressions of disgust and surprise could be nontrivial for them to mimic accurately. Additionally, in the case of Home-BD setting, some of the recordings took place under the direct supervision of our collaborative research group at BUET. While this offered greater control over the recording conditions, it occasionally made the participants feel uncomfortable or embarrassed, which may have influenced their ability to mimic the guided facial expressions authentically. Consequently, the data quality from Home-BD may not be as high as that from other settings."
        },
        {
            "heading": "Feature Extraction",
            "text": "In our study, we extracted two types of features from the facial expression videos collected from the participants \u2013 facial action units and facial landmarks. To extract these features, we used two very popular feature extraction tools OpenFace (Baltrusaitis et al. 2018) and MediaPipe (Lugaresi et al. 2019). Both OpenFace and MediaPipe are widely used due to their efficiency, accuracy, and the ability to process video streams in real-time. They also benefit from being open-source, allowing researchers and developers to use and extend their functionality to suit a wide range of applications.\nFacial Action Unit Features. We leveraged the Facial Action Coding System (FACS), developed by Ekman et al. (Ekman and Friesen 1978) to taxonomize human facial expressions. FACS describes facial expressions in terms of individual components of facial movement, or Facial Action Units (AUs). The AUs are associated with the muscle movements of the face and activation of a particular AU indicates the movement of a fixed set of facial muscles. In the literature, OpenFace has been proven to be very accurate in detecting the presence and intensity of these following AUs (Amos et al. 2016; Baltrusaitis et al. 2018).\nThese action units are contingent on the contraction or relaxation of one or more muscles. For instance, AU06 and AU12, provided by OpenFace, are crucial when examining facial expressions, particularly smiles. AU06, or \u201cCheek Raiser\u201d, involves the raising of the cheeks due to the contraction of the orbicularis oculi, pars orbitalis muscle around the eye socket. This results in a tightening of the skin around the eyes, often leading to the formation of \u2018crow\u2019s feet\u2019 wrinkles at the outer corners of the eyes. AU12, also known as \u201cLip Corner Puller\u201d, represents the movement caused by the zygomaticus major muscle, which pulls the corners of the lips upwards and outwards, creating a smile. These two action units, when combined, are key indicators of what is referred to as a Duchenne smile (Ekman, Davidson, and Friesen 1990), a sincere and genuine smile associated with spontaneous joy and happiness.\nUtilizing Openface, we extracted the AU values for each frame of the three distinct facial expression videos from each participant. OpenFace software gives a binary activation (0 or 1) and a raw magnitude (ranging 0 to 5) of each AU for each frame of a video that contains a human face. We evaluated three distinct statistical measures \u2013 mean, variance, and entropy \u2013 of the raw action unit when the corresponding action unit is active (i.e., the activation value is 1). The variance signals the extent of facial muscle movement during a facial expression, while the mean indicates the average intensity of muscle engagement for each facial expression throughout the video frames. Conversely, entropy provides a measurement of unpredictability or randomness in the activation of facial muscles during expressions. Higher entropy corresponds to more complex or varied facial movements.\nWhile assessing signs of Parkinson\u2019s disease from a facial expression task, The Movement Disorder SocietySponsored Revision of the Unified Parkinson\u2019s Disease Rating Scale (MDS-UPDRS) instructs the clinician to observe the patient\u2019s eye-blinking frequency, masked facies or loss of facial expression, spontaneous smiling, and the parting of lips. We engineered the facial features so that they reflect these four criteria of the MDS-UPDRS. The expressiveness (and spontaneity for smiles) of each of our three facial expressions is linked with four AUs. For example, AU01 (Inner Brow Raiser), AU06 (Cheek Raiser), AU12 (Lip Corner Puller), and AU14 (Dimpler) were observed to have three distinct peaks in smiling facial expression videos that ask participants to repeat the smile expression three times. Similarly, the expressiveness of a disgusted face is linked with AU04 (Brow Lowerer), AU07 (Eye Lid Tightener), AU09 (Nose Wrinkler), and AU10 (Upper Lip Raiser); and\na surprised face is linked with AU01 (Inner Brow Raiser), AU02 (Outer Brow Raiser), AU04 (Brow Lowerer), and AU05 (Upper Lip Raiser). In addition, to objectively gauge a participant\u2019s ability to keep their lips together while their mouths are at rest, we recorded the values of AU25 (Lips Part) and AU26 (Jaw Drop). These two action units are consistently selected across all three facial expressions. Finally, AU45 (Blink) offers an objective measurement for the frequency of eye blinking during all three facial expression videos. This way, we extracted frame-by-frame values of seven facial action units for each of the facial expression videos. We then summarized the frame-by-frame values to represent the entire video using three statistical aggregates \u2013 mean, variance, and entropy. Therefore, for each participant, the total number of digital features prepared from AUs using OpenFace is 63 (3 facial expressions \u00d7 7 AUs for each expression \u00d7 3 statistical aggregates for each AU).\nFacial Landmark Features. In addition to the action unit features, we extracted facial attributes that simulate clinical assessments typically carried out in person as suggested in prior literature (Gomez et al. 2021):\n\u2022 Opening of the left and right eye: Patients with PD may experience a decreased frequency of eye blinking. This can lead to more prolonged openings of the eyes, subtly altering the natural dynamics of facial expressions. By including the measurement of the degree of eye openings in our feature list, we aim to capture this nuanced change.\n\u2022 Rising of the left and right eyebrows: As a part of mimicking the surprise facial expression, the participants in our study were instructed to raise their eyebrows. However, in the case of Parkinson\u2019s patients, who may exhibit reduced facial expressiveness due to hypomimia, this eyebrow movement can be less pronounced. By taking into account the extent of eyebrow-raising, our analysis considers these subtle variations.\n\u2022 Opening of the mouth: As we have discussed earlier, one of the visible manifestations of PD can be an inability to fully close the mouth when the face is at rest, especially noticeable in the moderate to severe stages of the disease. We thus selected the extent of mouth opening as one of our target features as it can provide valuable insights into the degree of facial muscle control loss, a critical indicator of PD progression.\n\u2022 Width of the mouth: The width of the mouth during a smile can serve as an important indicator of facial muscle coordination and control. Given that Parkinson\u2019s patients often exhibit less expressive and smaller smiles \u2013 known as the \u201cParkinson\u2019s mask\u201d (Tickle-Degnen and Lyons 2004) \u2013 the width of the mouth during smiling can play a significant role in identifying the presence of the disease.\n\u2022 Opening of the jaw: The degree of jaw opening, often correlated with the symptom of masked faces, is another distinctive facial feature in Parkinson\u2019s disease. Patients may have difficulties fully controlling their jaw movement due to muscular rigidity, one of the primary motor\nsymptoms of Parkinson\u2019s. Therefore, we chose to include the measurement of jaw opening in our feature set.\nTo compute these attributes, we used the face mesh solution of MediaPipe. MediaPipe, a product developed by Google, provides 478 3D facial landmarks from a video frame containing a face, using a lightweight machine learning model. It first detects a face in the image or video stream and then applies the face mesh model to estimate the facial landmarks. Although distinct Action Units (AUs) with some overlaps were extracted for different facial expression videos using the OpenFace tool, we consistently extracted the same seven facial attribute features for each of the facial expressions using Mediapipe. This systematic approach ensures that each facial expression contributes an equivalent amount of information to the final feature space. Following the extraction, we performed a statistical aggregate on these features extracted at the frame level, computing mean, variance, and entropy. This process results in another set of 63 digital features (3 facial expressions \u00d7 7 facial attributes per expression \u00d7 3 statistical aggregates per attribute), further enriching the feature space."
        },
        {
            "heading": "Model Training, Inference, and Evaluation",
            "text": "After combining the features derived from facial action units and landmarks extracted by MediaPipe, we had a 126- dimensional feature space (63 from AU features and 63 from facial landmark features) to represent all three facial expressions for each participant. When all three facial expressions were used to train predictive models, all of these 126 features were considered. However, we also investigated using only one facial expression or a different combination of two facial expressions. In those cases, we only used the corresponding features related to the facial expressions we are using. For example, only 42 relevant features (i.e., one-third of all features) were considered when we trained the predictive model with merely the smile expression. The model training phase includes feature selection, feature scaling, creating synthetic samples for the minority class to account for class imbalance, and a training loop where the model gradually learned from data. During inference, we applied the same feature selection and scaling techniques and then used the learned model to make a prediction.\nFeature Selection. Feature selection is a widely adopted approach aimed at reducing the dimensionality of data. By selecting a subset of relevant features, this technique helps prevent overfitting and enables the training of simpler and more generalizable models. We tried three different feature selection techniques using (i) logistic regression coefficients, (ii) Boosted Recursive Feature Elimination (BoostRFE), and (iii) Boosted Recursive Feature Addition (BoostRFA). Each of these methods ranks the features based on their importance in constructing a predictive model that can effectively differentiate participants with PD from those without the condition. After the features are ranked, we select top-n features as input to the predictive model. n is considered a hyperparameter and tuned with other parameters in order to derive the best-performing model.\nFor the logistic regression-based feature ranking, we ran a logistic regression on the full dataset (after scaling the feature values), and use the absolute coefficient of each feature as a proxy of its importance. Both BoostRFE and BoostRFA approach build an estimator (i.e., gradient boosting) with all available features and rank them based on feature importance obtained by SHapley Additive exPlanations (SHAP) (Lundberg and Lee 2017). BoostRFE iteratively removes the least important feature one by one, as long as the model\u2019s performance improves, until only the top-n features remained. On the other hand, BoostRFA starts with the most important feature and incrementally adds the next most important feature if it led to performance improvement. The process is continued until n features are added to the model.\nIn our study, we found that feature selection, in general, led to performance improvement, and the logistic regression-based feature ranking outperformed the other two approaches (Table 3). Therefore, the logistic-regressionbased feature selection was used as the default for other experiments.\nFeature Scaling. Feature scaling is a crucial preprocessing step in machine learning. It ensures that all features are on a similar scale or range, and often leads to performance improvement. We have tried two different feature scaling methods mentioned below:\n\u2022 MinMax Scaling scales the values of each feature between 0 and 1 using the following formula:\nXscaled = (X \u2212Xmin)\n(Xmax \u2212Xmin)\nwhere X is the original feature value, and Xmin, Xmax are the minimum and the maximum value of the feature, respectively, in the entire dataset.\n\u2022 Standard Scaling ensures the values of the input features are scaled to have a mean of 0 and a standard deviation of 1. The formula for Standard Scaling is:\nXscaled = X \u2212 \u00b5\n\u03c3\nwhere X is the original feature vector, and \u00b5, \u03c3 are the mean and standard deviation of the feature, respectively, in the entire dataset.\nBased on ablation studies (Table 3), feature scaling helped boost the performance of the predictive model. Among the two methods, we select the MinMax Scaler as the default scaling method as it yielded the best performance.\nMinority Oversampling. The dataset we studied was imbalanced, as it contained 803 participants without PD compared to 256 with the condition. Such class imbalance poses a challenge for training machine learning models as it can lead to a biased model that might overfit the majority class and under-represent the minority class, resulting in poor generalization. To address this issue, we employed the Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al. 2002). SMOTE is a widely-used approach to balance\nclass distribution through the generation of synthetic instances of the minority class. As shown in Table 3, incorporating SMOTE in model training helped improve the performance of the predictive model. However, please note that the entire dataset was split into train and test sets first, and then SMOTE was applied only on the train set. This ensures that the test set only contained real data, and none of the synthetic data in the training set was derived from any test data.\nEvaluation. We used k-fold cross-validation with stratified sampling to assess the performance of the predictive model on held-out data. This approach involves dividing the dataset into k different folds, with each fold containing 1k portion of the dataset. The key distinction of stratified sampling is that it ensures each fold maintains a proportional representation of samples from different classes, mirroring the distribution in the entire dataset. To illustrate, consider an example where the dataset consists of 100 samples, with 10 samples belonging to class 1 and 90 samples belonging to class 2 (1 : 9 ratio). In 10-fold stratified cross-validation, each fold would include 1 samples from class 1 and 9 samples from class 2, preserving the ratio of samples per class observed in the full dataset. This ensures that the model is trained and evaluated on diverse samples from all classes. The evaluation process encompasses k iterations, with each iteration reserving one of the folds as the test set, while the model is trained on the remaining data. This procedure allows for a comprehensive assessment of the model\u2019s performance across the entire dataset while addressing potential biases introduced by class imbalances.\nIn addition, we used two different external test sets collected in (i) Clinic and (ii) Home-BD settings. For evaluating our model on these test sets, the model was solely trained on all data collected in the other two settings (i.e., Home-Global and PD Care Facility). As a result, the model has never seen any data from either Clinic or Home-BD setting. For each of the external test sets, we selected and scaled the features according to the training process, and ran inference using the trained model.\nWe used a comprehensive set of evaluation metrics widely adopted by machine learning for the healthcare community: (i) Area Under the Receiver Operating Characteristic (AUROC), (ii) binary Accuracy, (iii) Sensitivity, (iv) Specificity, (v) Positive Predictive Value (PPV), (vi) Negative Predictive Value (NPV). Instead of relying on a single metric, we selected the best model based on a holistic evaluation of all of the above metrics.\nHyper Parameter Tuning. Parameter tuning (Victoria and Maragatham 2021; Feurer and Hutter 2019) plays a crucial role in enhancing the performance of machine learning models. It involves adjusting the hyperparameters of these models to optimize their predictive performance. For this study, we employed a range of classifiers, including Support Vector Machine (SVM), AdaBoost, HistBoost, XGBoost, and Random Forest. The hyperparameters of these classifiers were tuned using Weights & Biases (WandB) (Biewald 2020), a machine learning tool designed for this specific purpose. WandB helped us identify the optimal combination of\nparameters that yielded the best model performance. The core principle underlying hyperparameter tuning is to traverse the search space of various parameter combinations with the goal of maximizing or minimizing a specific performance metric, for which, we used a Bayesian approach enabled by WandB.\nFor example, in our study, the parameters tuned for the SVM model included the number of top significant features (n), the penalty parameter of the error term (C), and the kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019 kernels (gamma). For AdaBoost, apart from n, the number of weak learners (n estimators) and learning rate were tuned. Similarly, for the XGBoost and HistBoost models, we tuned parameters such as maximum depth of a tree (max depth), minimum sum of instance weight needed in a child (min child weight), and boosting learning rate (eta), among others. In the Random Forest classifier, the number of trees in the forest (n estimators) and the maximum depth of the tree (max depth) were the key parameters tuned. These tuned models were then used in the subsequent steps of our experiment to achieve more robust and accurate results. The scripts for hyper-parameter tuning will be released alongside the code upon acceptance of the manuscript."
        },
        {
            "heading": "Use of Large Language Models",
            "text": "ChatGPT5, a sophisticated language model developed by OpenAI6, was employed as an editorial aid during the preparation of this manuscript. It facilitated the refinement of the manuscript\u2019s language, grammar, and stylistic elements through its intelligent suggestions. It\u2019s noteworthy that all recommendations proposed by ChatGPT underwent thorough examination by an author prior to their final incorporation into the text. Importantly, the function of ChatGPT was strictly limited to proposing amendments to the existing content and it was not utilized to generate any new material for this manuscript."
        },
        {
            "heading": "Ethics",
            "text": "This study received the necessary approval from the Institutional Review Board (IRB) of the University of Rochester, and all the experimental procedures were conducted in compliance with the approved study protocol. For data collected from Bangladesh, we received IRB approval from the Bangladesh University of Engineering & Technology (BUET). Given the predominantly remote administration of this study, written consent from participants was not obtained. However, informed consent was duly collected electronically from the participants, authorizing the use of their data for analysis and the inclusion of their photos in the figures presented within this study."
        },
        {
            "heading": "Code and Data Availability",
            "text": "The recorded videos were collected using a web-based tool. The tool is publicly accessible at https://parktest.net. The codes for video processing, feature extraction, and model\n5https://chat.openai.com/chat 6https://openai.com/\ntraining will be made publicly available upon the acceptance of this paper. We will provide a link to the repository containing the codes.\nUnfortunately, we are unable to share the raw videos due to the Health Insurance Portability and Accountability Act (HIPAA) compliance. However, we are committed to sharing the extracted features upon receiving an email request at rochesterhci@gmail.com. The features will be provided in a structured format that can be easily integrated with existing machine-learning workflows."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the U.S. Defense Advanced Research Projects Agency (DARPA) under grant W911NF191-0029, National Science Foundation Award IIS-1750380, National Institute of Neurological Disorders and Stroke of the National Institutes of Health under award number P50NS108676 and Moore Foundation.\nWe express our sincere gratitude to Cathe Schwartz, Karen Jaffe, and the dedicated members of the InMotion PD care facility for their invaluable assistance in collecting a diverse dataset from the residents in Ohio, United States. Their support was crucial in ensuring the success of our data collection efforts.\nWe would also like to extend our appreciation to Dr. Ray Dorsey, Dr. Ruth Schneider, Meghan Pawlik, Phillip Yang, and the esteemed team at the University of Rochester\u2019s Center for Health and Technology (CHeT). Their collaboration and support played a pivotal role in facilitating the data collection process from the clinical cohort. We are particularly grateful for their expertise and insights into Hypomimia and other symptoms associated with Parkinson\u2019s disease. Their contributions have significantly enriched our study and deepened our understanding of the condition.\nFinally, we would like to acknowledge the significant contributions of Dr. M Rafayet Ali, Dr. Raiyan Abdul Baten, and Abdelrahman Abdelkader from the University of Rochester.\nTheir diligent efforts in data collection and preliminary analysis laid the foundations for this work."
        },
        {
            "heading": "Author Contributions",
            "text": "M.S.I., T.A., W.R., M.S.R., and E.H. conceptualized and designed the experiments. T.A. developed the feature extraction process. S.D.T., K.N., and I.S. lead the data collection efforts from Bangladesh, and performed preliminary data analysis. T.A. and S.L. developed machine learning models. M.S.I. and T.A. designed the evaluation framework and worked on data analysis and visualization. M.S.I., T.A., and E.H. wrote the manuscript. All the authors read the manuscript and provided valuable suggestions for revising it. All authors accept the responsibility to submit it for publication.\nCompeting Interests The authors declare that there are no competing interests."
        },
        {
            "heading": "Supplementary Note 1 \u2013 Feature Extraction",
            "text": "A comprehensive list of the extracted features for each facial expression task is provided in Table 4. While the set of action unit features may vary depending on expressions, the MediaPipe attributes are the same for all expressions."
        },
        {
            "heading": "Supplementart Note 2 \u2013 Demographic Characteristics of Data Collected in Each Different Setting",
            "text": "In this section, we provide detailed demographic information for data collected in four different settings mentioned in the manuscript. Table 5, 6, 7, and 8 describe the details for Home-Global, PD Care Facility, Home-BD, and Clinic settings, respectively."
        }
    ],
    "title": "Unmasking Parkinson\u2019s Disease with Smile: An AI-enabled Screening Framework",
    "year": 2023
}