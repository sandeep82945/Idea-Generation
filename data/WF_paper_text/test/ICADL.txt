Scientific article summarization poses a challenge because the interpretability of the article depends on the objective, experience of the reader. Editors/Chairs assign experts in the domain as peer reviewers. These experts often write a summary of the article at the beginning of their reviews which offers a summarized view of their understanding (perspectives) on the given paper. Multiperspective summaries can provide multiple related but distinct perspectives of the reviewers rather than being influenced by a single summary. Here in this work, we propose a method to produce abstractive multiperspective summaries of scientific articles leveraging peer reviews. Our proposed method includes performing extractive summarization to identify the essential parts of the paper by extracting contributing sentences. In the subsequent step, we utilize the extracted pertinent information to condition a transformerbased language model comprising of a single encoder followed by multiple decoders that share weights. Our goal is to train the decoder to not only learn from a single reference summary but also to take into account multiple perspectives when generating the summary during the inference stage. Experimental results show that our approach achieves the best average ROUGE F1 Score, ROUGE-2 F1 Score, and ROUGE-L F1 Score with respect to the comparing systems. We make our code public for further research. 1 introduction Rapid increases in the number of publications in scientific fields motivates the development of automatic summarization tools for scientific articles. A summary
4 https://github.com/sandeep82945/Muti-percepective-summarization
of a scientific article is crucial as it provides a condensed overview of the main points and contributions of the research. It enables readers to quickly grasp the key findings and arguments presented in the paper, thus helping them decide whether they want to invest time in reading the full paper. Additionally, it serves as a reference for future research and discussions and aids editors and reviewers in understanding the paper and making an informed decision on its acceptance or rejection. Furthermore, it is useful for other researchers who wish to cite the paper and need to comprehend the content of the paper. Summarizing scientific articles is challenging as it requires understanding complex language and technical terms, condensing lengthy and detailed articles and having subject expertise to properly grasp and comprehend them [1, 27]. Also, it involves condensing multiple perspectives and findings on the same research, which may require subject expertise and critical evaluation skills. The standard approach for evaluating automated summary systems is to compare their output with human-written summaries. However, creating large annotated datasets of human-written summaries for scientific articles is challenging due to the time, effort, and expertise required in summarizing such complex and technical content. A common, yet underutilized, source of manuscript summaries lies within the peer review process. Peer reviewers, often experts in the field, provide detailed comments and summaries on the manuscripts they review. Each reviewer has a unique interpretation of the manuscript, bringing to light different aspects of the paper and offering a unique perspective. These multiple summaries of a single manuscript not only demonstrate to the editor how each reviewer has interpreted the manuscript, but also reveal significant differences in perspectives among the reviewers. This offers a rich set of data that can be utilized for generating summaries of scientific articles. This paper proposes a method that capitalizes on these multiple perspectives to create more comprehensive and inclusive summaries of scientific articles. The objective of multiperspective scientific summarization is not to be swayed by a single summary, but to incorporate multiple related but distinct perspectives of the reviewers. However, a limitation of previous research in this area has been the assumption that each document has only a single optimal, reference summary referred to as a ”gold summary”. Different readers can have different perspectives when summarizing the same document, which can result in variations among humanwritten summaries [19]. The accuracy of evaluating summarization systems using automated metrics is hindered by the use of only one reference or “gold” summary [19, 44]. The use of only one reference summary, particularly for longer and complex documents, may prevent the model from effectively capturing important information and different perspectives during training [21]. Our approach consists of two main stages. First, we employ an extractive summarization process to identify the essential parts of the paper by extracting contributing sentences. This step allows us to capture the most important information and perspectives from the paper based on the reviewers’ comments and interpretations. In the second stage, we use this extracted information to
condition a transformer-based language model. Our model architecture includes a single encoder followed by multiple decoders that share weights. This design allows us to train the decoder to not only learn from a single reference summary, but to consider multiple perspectives when generating the summary. The goal is to produce abstractive summaries of scientific documents that capture a holistic view of the manuscript as understood by multiple expert reviewers. By leveraging the insights of multiple experts, we aim to produce summaries that offer a comprehensive understanding of the paper, and allow readers to gain a multi-faceted view of the work. This method provides a promising approach for improving the quality and diversity of automatic scientific article summarization, addressing the challenges posed by the complexity and technicality of scientific content, and harnessing the power of multiple perspectives offered by expert peer reviewers. We summarize our contributions below:
– We present a novel two-stage method for multi-perspective summarization. The first stage performs extractive summarization to identify the essential parts of the paper, while the second stage employs an abstractive approach to generate comprehensive summaries. – We introduce a transformer-based model that uses a single encoder and multiple decoders, enabling the generation of summaries from different perspectives. – We leverage the wealth of peer review data as a source of multiple expertwritten summaries, which have been previously under-utilized in scientific summarization tasks. – Our experimental results show that our proposed method outperforms other state-of-the-art systems, achieving the highest average ROUGE-1, ROUGE2, and ROUGE-L F1 scores. The rest of this paper is organized as follows: Section 2 describes the related works of contributing sentence classification, types of summarization, and the scientific paper summarization task, and how our work is different from them. In Section 3, we describe our proposed methodology. In Section 4, we provide detailed information about the experimental setup, including the methodology, results, and human evaluation and analysis. Finally, in Section 5, we conclude the paper and discuss future directions. 2 related works In this section, we discuss the contributing sentences and the work related to their classification task. We then turn our attention to the topic of automatic text summarization before narrowing our focus to scientific paper summarization. Finally, we delineate how our work differs from previous studies in the field. 2.1 contributing sentence classification In the context of classifying contributing sentences in scholarly NLP contributions, a task featured in SemEval-2021, several strategies were observed [12]. The objective of identifying the contribution sentences from articles has typically been approached via two strategies: a binary classification or a multi-class classification. The binary classification approach designated sentences as either contributing or not. For instance, Teams YNU-HPCC [29] and INNOVATORS [2] both leveraged BERT as a binary classifier. The KnowGraph@IITK [37] team employed a SciBERT + BiLSTM architecture, while UIUC BioNLP [26] used BERT, also enhancing it with features related to sentence context. Alternatively, some teams employed a multi-class classification strategy, categorizing sentences into one of the 12 IUs or as non-contributing. Team DULUTH [30] used deBERTa [3] for this task, and ECNUICA [25] deployed an ensemble of RoBERTa, SciBERT, and BERT ([30]). They incorporated context features such as surrounding sentences and paragraph sub-titles. Team ITNLP [45] also used BERT for multi-class classification, adding sentence context and paragraph headings as features. These sentences contribute to the overall understanding of the main ideas, key points, and important details contained within the original source. So, we use contributing sentences to support and enhance the performance of our approach. 2.2 automatic text summarization Automatic text summarization can be categorized into two main strategies: extractive and abstractive. Extractive summarization focuses on selecting crucial segments from the text and presenting them as they appear in the original document. This is typically done by assessing each sentence’s importance in a document and subsequently selecting the sentences to be included in the summary (Erkan and Radev, 2004 [15]; Parveen, Ramsl, and Strube, 2015 [34]). Neural network-based techniques have shown effectiveness in summarizing news articles (Cao et al., 2015 [5]; Cheng and Lapata, 2016 [7]). On the other hand, abstractive summarization aims to rephrase the important information in a new and condensed form. Abstractive summarization is computationally more complex than extractive summarization and requires a deeper understanding of the original content. However, abstractive methods can generate more concise summaries, even shorter than any sentence in the source document [4]. In Nallapati et al. [33], the authors adapt a bidirectional GRU-RNN [8] as the encoder and an uni-directional GRU-RNN with the same hidden layer size as the decoder. Transformer introduced by Vaswani et al. [41], the Transformer model for abstractive text summarization replaced the recurrent layers with self-attention layers. Instead of processing the input sequentially, the Transformer uses a multi-head self-attention mechanism, which allows it to handle long-range dependencies effectively. It significantly outperformed the RNN/CNN-based models for various tasks, including summarization, due to its superior ability to capture the context and semantics of the input text. This study generates abstractive summary of scientific papers. 2.3 scientific paper summarization Scientific paper summarization refers to the method of extracting and condensing the key elements from a scholarly research article into a brief, concise overview. This enables the significant information of the research, often complex and extensive, to be communicated in a more digestible and accessible format. Scientific paper summarization is a fast-evolving research field. Extractive models perform sentence selection to create summaries( [35]; Cohan and Goharian [11]), while hybrid models identify salient text and summarize it [39]. Cohan et al. [9] proposed the first model for abstractive summarization of single, longer-form documents like research papers. Citation-based summarization leverages inter-paper reference relationships to formulate summaries, concentrating on the citation context and associated texts. This approach uses a paper’s references from other works to generate summaries, providing a significant understanding of the paper’s scientific contribution . Citation-based summarization has been an active area of research with the aim of summarizing the contribution of a target paper. Several studies have been conducted in this area [32, 13, 28, 22], which proposed methods for extracting a set of sentences from the citation sentences to summarize the contribution of the target paper. The generated summaries may not fully capture all important aspects of the target paper. To address this issue, Yasunaga et al. [43] proposed the integration of both the target paper’s abstract and its citation sentences. However, citation summarization helps improve a paper’s summary quality but doesn’t aid authors in drafting the summary while writing the paper. IBM Science Summarizer [14] is a system for retrieval and summarization of scientific articles in computer science. It summarizing each section of the relevant paper obtained by search and filtering process independently. Lloret et al. [28] suggested two approaches for generating research article abstracts, one is extractive, and the other is based on extractive and abstractive technique. LaySumm 2020 shared task [6] introduced a task of summarizing a technical or scientific document in simple, non-technical language that is comprehensible to a lay person (non-expert). Several teams contributed unique strategies for lay summarization in the competition. Gidiotis et al. [17] and Kim [23] used finetuned versions of the PEGASUS model, with Kim also incorporating a BERTbased extractive model. Reddy et al. [36] used an extractive sentence classification method. Roy et al. [16] leveraged the BART encoder in their systems, while Mishra et al. [31] utilized a standard encoder-decoder framework. Our approach is novel in its utilization of multiperspective review summaries written by expert reviewers as the basis for scientific article summarization. This method differs from previous works that rely on a single reference summary, as it leverages the power of multiple reference summaries for training. Unlike previous studies on citation-based summarization, our approach extracts the contributing sentences of the scientific document and uses them directly to generate the summary. The lack of annotated datasets for training models to produce abstractive, multiperspective scientific paper summaries presents a major challenge in the field. To address this issue, we make use of the summaries present in peer
review texts. Reviewers in various scientific fields often provide an introductory summary of the main points and key contributions of a paper, and each paper typically receives multiple reviews. Our proposed solution is an end-to-end extractive and abstractive architecture that generates a summary from the input scientific paper. During the training of the abstractive model, multiple golden summaries are used to teach the model how to generate multi-perspective summaries. 3 methodology Figure 1 shows the overall proposed architecture of our approach. The abstract and extracted contributing sentences from the full text are used to train the Multi-Perspective Framework. We explain each component of the architecture as follows:- 3.1 contributing sentence extraction The main difficulty in summarizing scientific papers is their length and complexity. In this part of our architecture, our goal is to assist the next abstractive model by selecting only the salient enriching part of the full text of the paper. We extract the contributing sentences from the full textual document of the paper using an attention-based deep neural model named ContriSci [18]. ContriSci is a deep neural architecture that leverages multi-task learning to identify statements from a given research article that mention a contribution of the study. 3.2 multi perspective framework Training: Given a set of extractively selected salient sentences, denoted as C, we initially transform them into a sequence of hidden representations, denoted as M , using a consistent encoder for each target summary. To cater to the task of multi-reference summarization, multiple decoder frameworks are utilized to uncover various possible golden summaries. For a single instance of M , there could be multiple, say k, golden summaries. The decoder functions at the word level to predict these summaries, denoted as Y . h (k) t = Decoder(M,yt−1) (1)
Here, k represents the kth decoder for kth golden summary. We implement the teacher forcing method on each decoder with each reference summary to train the decoder. P (yt|y<t,C)(k) = softmax(Wdht + bd) (2)
where ht is the hidden representation of yt (the t-th word in the target summary). We maximize the conditional log likelihood for a given N observation (C(i), Y (i))Ni=1
L (k) MLE = − i=N∑ i=1 t=T∑ t=1 logP (y (i) t |y (i) <tC (i)) (3)
We aggregated the losses using objective function f
LMuliMLE = Min(L (1) MLE , L (2) MLE , .., L (k) MLE) (4)
MultiRouge = fmean(Rouge (1), Rouge(2), .., Rouge(k)) (5)
We define our proposed two different aggregation objectives :- First among the losses from the multiple reference summary, we choose the minimum loss for training. In particular, the model was trained using the generated summary that had the smallest loss compared to the reference summary. We refer our proposed architecture with this training objective as Multibest. AggLoss = LMuliMLE (6)
Next, we took the weighted mean of the losses between the k generated summaries and the k reference summaries to generalize the shared weights of the decoders. fmean = t=k∑ t=1 βi(V t) (7)
βi = V i∑t=k t=1 V t (8)
Here, V i represents the loss associated with the t-th generated summary. AggLoss = α1LMuliMLE − α2MultiRouge (9)
We refer to our proposed architecture with this training objective asMultimean. Here, Rouge refers to the Rougue-1 F1 score between the predicted summary and the reference (or golden) summary. The objective during training is to minimize LMultiMLE and maximize the MultiRouge score
5. In particular, we combine the loss from each reference summary while training the models. Hence the prediction model is capable of generating predictions considering the multiple editor’s perspective learned during training. However, if one of the summaries of a paper deviates significantly from highlighting the overall contribution of the paper and that reference is used during training, the predictions are expected to be less accurate and is one of the main reasons for exploring a multi-perspective approach. For the two different objective functions fbest and fmean we name the corresponding architecture as Multibest and Multimean respectively. We call the final loss after aggregation of the multiple losses as AggLoss. Inference: During training, the decoder’s weights are shared, so the final weights after training have learned to take into account different perspectives. During inference we initialize a decoder with the weights to generate a single output summary as shown in Figure 1 (b). Given the extractively selected salient sentences C, the encoder first transforms C into a sequence of hidden representations M . After that, the decoder predicts Y at the word level for M . ht = Decoder(M,yt−1) (10)
5 We set the value of α1: 1 and α2: 100 empirically, weighing the importance and normalizing the two losses given their different scales. P (yt|y<t,C)(k) = softmax(Wdht + bd) (11) where ht is the hidden representation of yt (the t-th word in the target
summary). 4 experiments In this section, we discuss the results of our proposed model and compare it with other state-of-the-art systems. We aim to demonstrate that using multiple references for summary, rather than just one, can improve the performance compared to traditional abstractive and extractive methods. Additionally, we examine the effect of utilizing extractive summarization techniques, specifically the identification of contribution sentences, prior to the application of the abstractive summarization method. We use the BART autoencoder for pre-training sequence-to-sequence models for the encoder and decoder. The encoder part is a bidirectional encoder that corresponds to the structure of BERT [42], and the decoder part is an auto-regressive decoder following the settings of GPT. During the pre-training process, BART receives the corrupted document as input and predicts the original uncorrupted document. In this way, BART can effectively learn contextual representations. When fine-tuned for the summarization task, the bidirectional encoder part encodes the original document, and the decoder part predicts the reference summary. BART obtains excellent performance on the summarization task. We gave the input to BART as follows: Input text: Abstract [SEP ] Contributing sentences
Here, the input to the BART model is Abstract, and the contributing sentences separated by a token [SEP ]. 4.1 dataset We use the dataset collected from OpenReview by the MuP 2022 shared task for our task. This has been used for training while the hidden test set6 is used for final evaluation. The number of papers used for training, validation, and testing in this experiment are 18,934, 3,604, and 4,610 respectively. The brief description of the dataset can be found in [10]. 4.2 experimental settings To train the ContriSci, we use the default hyperparameters with which ContriSci is trained. We use the BART large fine-tuned on CNN/DailyMail dataset [20] to initialize both our encoder and decoder from the hugging face library 7. We use a dynamic learning rate, warm up 1000 iterations, and decay afterwards. We train the model for 10 epochs with the batch size of 4. We train all the models on a single GPU (NVIDIA A100 80GB). 6 https://github.com/allenai/mup 7 https://huggingface.co/ 4.3 result and analysis Automatic Evaluation: Table 1 shows that our method Multimean outperforms all the other systems for comparison. To evaluate the performance of our system during inference, we employ the same method used by other comparable systems. Specifically, we calculate the ROUGE score between the generated summary and each reference summary in the paper, then take the average of these scores. GATS describes an extractive summarization approach using GATs to rank sentences in discourse facets of a paper, creating a graph for each article. Our proposed method Multimean is abstractive and outperforms GATS by an average ROUGE score of 8.4. The LTRC system divides a paper into sections such as the abstract, introduction, and conclusion. They found that the best results were achieved when training the model on only the introduction of the paper. GUIR implemented a two-step summarization process. The first step involved extracting the most salient sentences from the document by training a classifier. In the second step, these sentences were used to write an abstractive summary. However, Multimean extracts the most important contributing sentences from each section of the paper to train our model, and it outperforms both LTRC and GUIR by 2.2 and 2.0 average ROUGE score, respectively. The AINLPML system uses a two-stage approach for the task, first an extractive summarization step with a contributing sentence identification model, and then a BART model is fine-tuned on the extracted summary generated from the previous step. They, along with other systems, used only one reference summary for training. However, we used multiple references for training and utilized multiple decoders to generate a multi-perspective summary. Our proposed Multimean outperforms AINLPML by 1.6 points. Ablation Study: We analyze the effectiveness of our model by performing an ablation study in Table 2. First, we trained the model on a single reference summary by randomly choosing one of the golden summaries from all available golden summaries. In the case of utilizing the abstract and full text of the paper as input text, our proposed approach achieved 25.79 avg. ROUGE F score. Next,
we took the abstract of the paper, along with a collection of contribution sentences from the remaining portion of the paper. Our proposed approach achieved 26.50 ROUGE F score. The improvement of 0.29 points in ROUGE F score between the previous architecture and this one clearly shows the significance of the contributing sentences in generating these summaries. Next, to understand the effectiveness of the multi-perspective training, we trained the previous architecture in the multi-objective training fashion, i.e., Multibest. The slight improvement observed may be attributed to the method’s similarity to a single reference summary, but with the added benefit of utilizing a best reference summary dynamic rather than a random one. However, we observed a surprising improvement of 1.63 points in the ROUGE F score in our Multimean architecture. This may be because the training objective of this architecture is not biased towards any particular reference summary, but rather aims to generalize the model towards all of the reference summaries. As a result, the model learns from each reference summary, taking into account the perspective of each reviewer. Human Evaluation: In order to conduct our study, we asked four domain experts in NLP (experts with 5+ years of experience in the field) to annotate a set of 150 randomly selected papers, along with their results. We provided the experts with access to the paper PDFs and the ground-truth reviews. The randomly selected papers were from the validation set because the ground-truth review for the test set is private and we do not have access to it. We reimplemented the comparison systems and evaluated the set on them. Following [10], we asked experts to rate the generated summaries according to three characteristics: faithfulness, readability, and coverage, on a Likert scale (1-5). To evaluate the multi-perceptiveness of our proposed architecture, we also defined a characteristic called P-Coverage. We asked experts to rate whether the generated summary captures the key points from each of the reference summaries. The results of
the human evaluation can be found in Table 3. Consistent with the automated evaluation results, our approach outperforms the rest of the systems in terms of readability and coverage, and is very close to leading in terms of faithfulness. GATS achieves a better faithfulness score than our proposed method since it is an extractive approach. Additionally, a higher P-Coverage compared to other systems indicates that our proposed system effectively captures a diverse range of perspectives. Case Study: The following case study helps us understand the efficacy of multiperspective settings compared to a traditional single one. Table 4 shows a comparison of the generated summary using a single reference (Multibest) and a multiperspective summary (Multimean). Our proposed multi-perspective framework captures various details from multiple target summaries, which are lacking when trained on only a single reference summary. We make the following two observations:
1) Relationship between parameter and function distances in a Hilbert space: The proposed model summary explains the nontrivial relationship and how it affects optimization, while the other does not mention the relationship between the two distances. 2) Direct application of L distance to optimization: The single setting summary does not mention how the L distance can be applied directly to optimization, while the multiperspective provides this information. One reviewer mentioned (1) in their summary, and the other mentioned (2), due to which both got included in our output. Now in the single reference setting, the model is trained against a single summary, and since it did not include (1) or (2) perspective, it never got included in the final summary. The multi-
perspective summary gives a more technical overview of the paper’s methods and results, allowing the reader to gain a deeper understanding of the content and make a more informed evaluation. The use of technical terms and definitions also helps readers who need to become familiar with the field. The proposed Multi-Perspective Framework can provide context for different interpretations of summaries and determine what various reviewers consider important in the final summary. We also performed an error analysis to analyze where our model fails. We discovered that in the case of highly technical papers abundant with mathematical symbols, the model tends to generate somewhat incoherent summaries. Additionally, we found that if the paper’s text, formatted using an automatic PDF parser, is not properly structured (e.g., disjointed sentences, unconnected words), the resultant summary tends to lack coherence. 5 conclusion and future work We addressed an interesting problem of multiperspective summarization of scientific papers by proposing an end-to-end extractive and abstractive architecture. Our results, based on automated and human evaluations, suggest that this architecture performs better than other comparable systems in addressing this task. We found that the extractive summarization technique, which extracts the paper’s contributions, assists in this task. The results of the experiment and analysis indicate that considering the reviewers’ multiperspective views enhances the summary’s quality and results in improved performance compared to utilizing just one perspective. 