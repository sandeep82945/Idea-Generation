{
    "abstractText": "We present HARP (HAnd Reconstruction and Personalization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-fidelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP models a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. The explicit nature of our representation enables a truly scalable, robust, and efficient approach to hand avatar creation as validated by our experiments. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with realtime rendering capability. To enable this, we carefully design and implement a shadow-aware differentiable rendering scheme that is robust to high degree articulations and self-shadowing regularly present in hand motions, as well as challenging lighting conditions. It also generalizes to unseen poses and novel viewpoints, producing photo-realistic renderings of hand animations. Furthermore, the learned HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel view and novel pose synthesis, and 3D hand pose refinement. It is an AR/VRready personalized hand representation that shows superior fidelity and scalability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Korrawe Karunratanakul"
        },
        {
            "affiliations": [],
            "name": "Sergey Prokudin"
        },
        {
            "affiliations": [],
            "name": "Otmar Hilliges"
        },
        {
            "affiliations": [],
            "name": "Siyu Tang"
        }
    ],
    "id": "SP:360d330e59f50ead66f1d0b4d49893c9ef6d61ee",
    "references": [
        {
            "authors": [
                "L. Ballan",
                "A. Taneja",
                "J. Gall",
                "L. van Gool",
                "M. Pollefeys"
            ],
            "title": "Motion capture of hands in action using discriminative salient points",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2012
        },
        {
            "authors": [
                "Marcel C. Buehler",
                "Abhimitra Meka",
                "Gengyan Li",
                "Thabo Beeler",
                "Otmar Hilliges"
            ],
            "title": "Varitex: Variational neural face textures",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yujun Cai",
                "Liuhao Ge",
                "Jianfei Cai",
                "Junsong Yuan"
            ],
            "title": "Weakly-supervised 3d hand pose estimation from monocular rgb images",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Cao",
                "Ilija Radosavovic",
                "Angjoo Kanazawa",
                "Jitendra Malik"
            ],
            "title": "Reconstructing hand-object interactions in the wild",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Wei Chao",
                "Wei Yang",
                "Yu Xiang",
                "Pavlo Molchanov",
                "Ankur Handa",
                "Jonathan Tremblay",
                "Yashraj S. Narang",
                "Karl Van Wyk",
                "Umar Iqbal",
                "Stan Birchfield",
                "Jan Kautz",
                "Dieter Fox"
            ],
            "title": "DexYCB: A benchmark for capturing hand grasping of objects",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Ping Chen",
                "Yujin Chen",
                "Dong Yang",
                "Fangyin Wu",
                "Qin Li",
                "Qingpei Xia",
                "Yong Tan"
            ],
            "title": "I2uv-handnet: Image-to-uv prediction network for accurate and high-fidelity 3d hand mesh modeling",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xingyu Chen",
                "Baoyuan Wang",
                "Heung-Yeung Shum"
            ],
            "title": "Hand avatar: Free-pose hand animation and rendering from monocular video",
            "year": 2022
        },
        {
            "authors": [
                "Yujin Chen",
                "Zhigang Tu",
                "Di Kang",
                "Linchao Bao",
                "Ying Zhang",
                "Xuefei Zhe",
                "Ruizhi Chen",
                "Junsong Yuan"
            ],
            "title": "Modelbased 3d hand reconstruction via self-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zhaoxi Chen",
                "Ziwei Liu"
            ],
            "title": "Relighting4d: Neural relightable human from videos",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Hongsuk Choi",
                "Gyeongsik Moon",
                "Kyoung Mu Lee"
            ],
            "title": "Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Enric Corona",
                "Tomas Hodan",
                "Minh Vo",
                "Francesc Moreno- Noguer",
                "Chris Sweeney",
                "Richard Newcombe",
                "Lingni Ma"
            ],
            "title": "Lisa: Learning implicit shape and appearance of hands",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mathieu Desbrun",
                "Mark Meyer",
                "Peter Schr\u00f6der",
                "Alan H Barr"
            ],
            "title": "Implicit fairing of irregular meshes using diffusion and curvature flow",
            "venue": "In Proceedings of the 26th annual conference on Computer graphics and interactive techniques,",
            "year": 1999
        },
        {
            "authors": [
                "Bardia Doosti",
                "Shujon Naha",
                "Majid Mirbagheri",
                "David J Crandall"
            ],
            "title": "Hope-net: A graph-based model for hand-object pose estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yao Feng",
                "Haiwen Feng",
                "Michael J. Black",
                "Timo Bolkart"
            ],
            "title": "Learning an animatable detailed 3D face model from in-the-wild images",
            "venue": "ACM Transactions on Graphics, (Proc. SIGGRAPH),",
            "year": 2021
        },
        {
            "authors": [
                "Guy Gafni",
                "Justus Thies",
                "Michael Zollhofer",
                "Matthias Nie\u00dfner"
            ],
            "title": "Dynamic neural radiance fields for monocular 4d facial avatar reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Guillermo Garcia-Hernando",
                "Shanxin Yuan",
                "Seungryul Baek",
                "Tae-Kyun Kim"
            ],
            "title": "First-person hand action benchmark with rgb-d videos and 3d hand pose annotations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Liuhao Ge",
                "Zhou Ren",
                "Yuncheng Li",
                "Zehao Xue",
                "Yingying Wang",
                "Jianfei Cai",
                "Junsong Yuan"
            ],
            "title": "3d hand shape and pose estimation from a single rgb image",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Philip-William Grassal",
                "Malte Prinzler",
                "Titus Leistner",
                "Carsten Rother",
                "Matthias Nie\u00dfner",
                "Justus Thies"
            ],
            "title": "Neural head avatars from monocular rgb videos",
            "venue": "arXiv preprint arXiv:2112.01554,",
            "year": 2021
        },
        {
            "authors": [
                "Jon Hasselgren",
                "Nikolai Hofmann",
                "Jacob Munkberg"
            ],
            "title": "Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising",
            "year": 2022
        },
        {
            "authors": [
                "Yana Hasson",
                "G\u00fcl Varol",
                "Dimitrios Tzionas",
                "Igor Kalevatykh",
                "Michael J. Black",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Learning joint reconstruction of hands and manipulated objects",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Tao Hu",
                "Tao Yu",
                "Zerong Zheng",
                "He Zhang",
                "Yebin Liu",
                "Matthias Zwicker"
            ],
            "title": "Hvtr: Hybrid volumetric-textural rendering for human avatars",
            "venue": "In 2022 International Conference on 3D Vision (3DV),",
            "year": 2022
        },
        {
            "authors": [
                "Umar Iqbal",
                "Pavlo Molchanov",
                "Thomas Breuel Juergen Gall",
                "Jan Kautz"
            ],
            "title": "Hand pose estimation via latent 2.5 d heatmap regression",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Korrawe Karunratanakul",
                "Adrian Spurr",
                "Zicong Fan",
                "Otmar Hilliges",
                "Siyu Tang"
            ],
            "title": "A skeleton-driven neural occupancy representation for articulated hands",
            "venue": "In International Conference on 3D Vision (3DV),",
            "year": 2021
        },
        {
            "authors": [
                "Korrawe Karunratanakul",
                "Jinlong Yang",
                "Yan Zhang",
                "Michael J. Black",
                "Krikamol Muandet",
                "Siyu Tang"
            ],
            "title": "Grasping field: Learning implicit representations for human grasps",
            "venue": "In 2020 International Conference on 3D Vision (3DV),",
            "year": 2020
        },
        {
            "authors": [
                "Giorgos Karvounas",
                "Nikolaos Kyriazis",
                "Iason Oikonomidis",
                "Aggeliki Tsoli",
                "Antonis A Argyros"
            ],
            "title": "Multi-view imagebased hand geometry refinement using differentiable monte carlo ray tracing",
            "year": 2021
        },
        {
            "authors": [
                "Hiroharu Kato",
                "Yoshitaka Ushiku",
                "Tatsuya Harada"
            ],
            "title": "Neural 3d mesh renderer",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Hyeongwoo Kim",
                "Pablo Garrido",
                "Ayush Tewari",
                "Weipeng Xu",
                "Justus Thies",
                "Matthias Niessner",
                "Patrick P\u00e9rez",
                "Christian Richardt",
                "Michael Zollh\u00f6fer",
                "Christian Theobalt"
            ],
            "title": "Deep video portraits",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Nikos Kolotouros",
                "Georgios Pavlakos",
                "Kostas Daniilidis"
            ],
            "title": "Convolutional mesh regression for single-image human shape reconstruction",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Dominik Kulon",
                "Riza Alp Guler",
                "Iasonas Kokkinos",
                "Michael M. Bronstein",
                "Stefanos Zafeiriou"
            ],
            "title": "Weaklysupervised mesh-convolutional hand reconstruction in the wild",
            "venue": "In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Husz\u00e1r",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang"
            ],
            "title": "Photorealistic single image super-resolution using a generative adversarial network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ruilong Li",
                "Julian Tanke",
                "Minh Vo",
                "Michael Zollhofer",
                "Jurgen Gall",
                "Angjoo Kanazawa",
                "Christoph Lassner"
            ],
            "title": "Tava: Template-free animatable volumetric actors. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tianye Li",
                "Timo Bolkart",
                "Michael. J. Black",
                "Hao Li",
                "Javier Romero"
            ],
            "title": "Learning a model of facial shape and expression from 4D scans",
            "venue": "ACM Transactions on Graphics, (Proc. SIGGRAPH Asia),",
            "year": 2017
        },
        {
            "authors": [
                "Yuwei Li",
                "Minye Wu",
                "Yuyao Zhang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "title": "Piano: A parametric hand bone model from magnetic resonance imaging",
            "venue": "In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yuwei Li",
                "Longwen Zhang",
                "Zesong Qiu",
                "Yingwenqi Jiang",
                "Yuyao Zhang",
                "Nianyi Li",
                "Yuexin Ma",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "title": "Nimble: A non-rigid hand model with bones and muscles",
            "venue": "arXiv preprint arXiv:2202.04533,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Lin",
                "Lijuan Wang",
                "Zicheng Liu"
            ],
            "title": "End-to-end human pose and mesh reconstruction with transformers",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Shanchuan Lin",
                "Linjie Yang",
                "Imran Saleemi",
                "Soumyadip Sengupta"
            ],
            "title": "Robust high-resolution video matting with temporal guidance, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Shichen Liu",
                "Tianye Li",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "venue": "The IEEE International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Marko Mihajlovic",
                "Aayush Bansal",
                "Michael Zollhoefer",
                "Siyu Tang",
                "Shunsuke Saito"
            ],
            "title": "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints",
            "venue": "In European conference on computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "Marko Mihajlovic",
                "Shunsuke Saito",
                "Aayush Bansal",
                "Michael Zollhoefer",
                "Siyu Tang"
            ],
            "title": "COAP: Compositional articulated occupancy of people",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Marko Mihajlovic",
                "Yan Zhang",
                "Michael J Black",
                "Siyu Tang"
            ],
            "title": "LEAP: Learning articulated occupancy of people",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Gyeongsik Moon",
                "Kyoung Mu Lee"
            ],
            "title": "I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Gyeongsik Moon",
                "Kyoung Mu Lee"
            ],
            "title": "Neuralannot: Neural annotator for in-the-wild expressive 3d human pose and mesh training sets",
            "venue": "arXiv preprint arXiv:2011.11232,",
            "year": 2020
        },
        {
            "authors": [
                "Gyeongsik Moon",
                "Takaaki Shiratori",
                "Kyoung Mu Lee"
            ],
            "title": "Deephandmesh: A weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Gyeongsik Moon",
                "Ju Yong Chang",
                "Kyoung Mu Lee"
            ],
            "title": "V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Gyeongsik Moon",
                "Shoou-I Yu",
                "He Wen",
                "Takaaki Shiratori",
                "Kyoung Mu Lee"
            ],
            "title": "Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Franziska Mueller",
                "Florian Bernard",
                "Oleksandr Sotnychenko",
                "Dushyant Mehta",
                "Srinath Sridhar",
                "Dan Casas",
                "Christian Theobalt"
            ],
            "title": "Ganerated hands for real-time 3d hand tracking from monocular rgb",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Franziska Mueller",
                "Micah Davis",
                "Florian Bernard",
                "Oleksandr Sotnychenko",
                "Mickeal Verschoor",
                "Miguel A. Otaduy",
                "Dan Casas",
                "Christian Theobalt"
            ],
            "title": "Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Evonne Ng",
                "Shiry Ginosar",
                "Trevor Darrell",
                "Hanbyul Joo"
            ],
            "title": "Body2hands: Learning to infer 3d hands from conversational gesture body dynamics",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Merlin Nimier-David",
                "Delio Vicini",
                "Tizian Zeltner",
                "Wenzel Jakob"
            ],
            "title": "Mitsuba 2: A retargetable forward and inverse renderer",
            "venue": "Transactions on Graphics (Proceedings of SIG- GRAPH Asia),",
            "year": 2019
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Neural articulated radiance field",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Neural articulated radiance field",
            "venue": "In International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Jonathan T. Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Steven M. Seitz",
                "Ricardo Martin-Brualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "ICCV, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T. Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin- Brualla",
                "Steven M. Seitz"
            ],
            "title": "Hypernerf: A higherdimensional representation for topologically varying neural radiance fields",
            "venue": "ACM Trans. Graph.,",
            "year": 2021
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Timo Bolkart",
                "Ahmed A.A. Osman",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Expressive body capture: 3d hands, face, and body from a single image",
            "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Sida Peng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Bui Tuong Phong"
            ],
            "title": "Illumination for computer generated pictures",
            "venue": "Communications of the ACM,",
            "year": 1975
        },
        {
            "authors": [
                "Sergey Prokudin",
                "Michael J Black",
                "Javier Romero"
            ],
            "title": "Smplpix: Neural avatars from 3d human models",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Neng Qian",
                "Jiayi Wang",
                "Franziska Mueller",
                "Florian Bernard",
                "Vladislav Golyanik",
                "Christian Theobalt"
            ],
            "title": "HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalization",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV). Springer,",
            "year": 2020
        },
        {
            "authors": [
                "Nikhila Ravi",
                "Jeremy Reizenstein",
                "David Novotny",
                "Taylor Gordon",
                "Wan-Yen Lo",
                "Justin Johnson",
                "Georgia Gkioxari"
            ],
            "title": "Accelerating 3d deep learning with pytorch3d",
            "year": 2007
        },
        {
            "authors": [
                "William T Reeves",
                "David H Salesin",
                "Robert L Cook"
            ],
            "title": "Rendering antialiased shadows with depth maps",
            "venue": "In Proceedings of the 14th annual conference on Computer graphics and interactive techniques,",
            "year": 1987
        },
        {
            "authors": [
                "Javier Romero",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Embodied hands: Modeling and capturing hands and bodies together",
            "venue": "ACM Transactions on Graphics, (Proc. SIG- GRAPH Asia),",
            "year": 2017
        },
        {
            "authors": [
                "Viktor Rudnev",
                "Vladislav Golyanik",
                "Jiayi Wang",
                "Hans-Peter Seidel",
                "Franziska Mueller",
                "Mohamed Elgharib",
                "Christian Theobalt"
            ],
            "title": "Eventhands: Real-time neural 3d hand pose estimation from an event stream",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Michael Seeber",
                "Roi Poranne",
                "Marc Polleyfeys",
                "Martin R Oswald"
            ],
            "title": "Realistichands: A hybrid model for 3d hand reconstruction",
            "venue": "In 2021 International Conference on 3D Vision (3DV),",
            "year": 2021
        },
        {
            "authors": [
                "Aliaksandr Siarohin",
                "St\u00e9phane Lathuili\u00e8re",
                "Sergey Tulyakov",
                "Elisa Ricci",
                "Nicu Sebe"
            ],
            "title": "First order motion model for image animation",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "B. Smith",
                "Chenglei Wu",
                "He Wen",
                "Patrick Peluse",
                "Yaser Sheikh",
                "J. Hodgins",
                "Takaaki Shiratori"
            ],
            "title": "Constraining dense hand surface tracking with elasticity",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Olga Sorkine",
                "Marc Alexa"
            ],
            "title": "As-rigid-as-possible surface modeling",
            "venue": "In Proceedings of EUROGRAPHICS/ACM SIG- GRAPH Symposium on Geometry Processing,",
            "year": 2007
        },
        {
            "authors": [
                "Adrian Spurr",
                "Umar Iqbal",
                "Pavlo Molchanov",
                "Otmar Hilliges",
                "Jan Kautz"
            ],
            "title": "Weakly supervised 3d hand pose estimation via biomechanical constraints",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Shih-Yang Su",
                "Frank Yu",
                "Michael Zollhoefer",
                "Helge Rhodin"
            ],
            "title": "A-nerf: Surface-free human 3d pose refinement via neural rendering",
            "venue": "arXiv preprint arXiv:2102.06199,",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Tang",
                "Tianyu Wang",
                "Chi-Wing Fu"
            ],
            "title": "Towards accurate alignment in real-time 3d hand-mesh reconstruction",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Bugra Tekin",
                "Federica Bogo",
                "Marc Pollefeys. H"
            ],
            "title": "o: Unified egocentric recognition of 3d hand-object poses and interactions",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Shaofei Wang",
                "Marko Mihajlovic",
                "Qianli Ma",
                "Andreas Geiger",
                "Siyu Tang"
            ],
            "title": "Metaavatar: Learning animatable clothed human models from few depth images",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Wang",
                "Aleksander Holynski",
                "Xiuming Zhang",
                "Xuaner Cecilia Zhang"
            ],
            "title": "Sunstage: Portrait reconstruction and relighting using the sun as a light stage",
            "venue": "arXiv preprint arXiv:2204.03648,",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Wang",
                "Eero P Simoncelli",
                "Alan C Bovik"
            ],
            "title": "Multiscale structural similarity for image quality assessment",
            "venue": "In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers,",
            "year": 2003
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P. Srinivasan",
                "Jonathan T. Barron",
                "Ira Kemelmacher-Shlizerman"
            ],
            "title": "HumanNeRF: Free-viewpoint rendering of moving people from monocular video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Lance Williams"
            ],
            "title": "Casting curved shadows on curved surfaces",
            "venue": "Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques,",
            "year": 1978
        },
        {
            "authors": [
                "Hongyi Xu",
                "Thiemo Alldieck",
                "Cristian Sminchisescu"
            ],
            "title": "H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Xu",
                "Eduard Gabriel Bazavan",
                "Andrei Zanfir",
                "William T Freeman",
                "Rahul Sukthankar",
                "Cristian Sminchisescu"
            ],
            "title": "Ghum & ghuml: Generative 3d human shape and articulated pose models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Linlin Yang",
                "Angela Yao"
            ],
            "title": "Disentangling latent hands for image synthesis and pose estimation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Zhang",
                "Yuxiao Zhou",
                "Yifei Tian",
                "Jun-Hai Yong",
                "Feng Xu"
            ],
            "title": "Single depth view based real-time reconstruction of hand-object interactions",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xiuming Zhang",
                "Pratul P Srinivasan",
                "Boyang Deng",
                "Paul Debevec",
                "William T Freeman",
                "Jonathan T Barron"
            ],
            "title": "Nerfactor: Neural factorization of shape and reflectance under an unknown illumination",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Christian Zimmermann",
                "Max Argus",
                "Thomas Brox"
            ],
            "title": "Contrastive representation learning for hand shape estimation",
            "venue": "arxive,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "ization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-fidelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP models a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. The explicit nature of our representation enables a truly scalable, robust, and efficient approach to hand avatar creation as validated by our experiments. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with realtime rendering capability. To enable this, we carefully design and implement a shadow-aware differentiable rendering scheme that is robust to high degree articulations and self-shadowing regularly present in hand motions, as well\nas challenging lighting conditions. It also generalizes to unseen poses and novel viewpoints, producing photo-realistic renderings of hand animations. Furthermore, the learned HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel view and novel pose synthesis, and 3D hand pose refinement. It is an AR/VRready personalized hand representation that shows superior fidelity and scalability."
        },
        {
            "heading": "1. Introduction",
            "text": "Advancements in AR/VR devices are introducing a new reality in which the physical and digital worlds merge. The human hand is a crucial element for an intimate and interactive experience in these environments, serving as the primary interface between humans and the digital world. Therefore, it is essential to capture, reconstruct, and animate life-like digital hands for AR and VR applications. Without\nar X\niv :2\n21 2.\nthis capability, the authenticity and practicality of AR/VR consumer products will always be limited.\nDespite its importance, the research into hand avatar creation has so far been limited. Most works [8, 37, 62] focus on creating an appearance space on top of a parametric hand model such as MANO [65]. Such an appearance space provides a compact way to represent hand texture but is rather limited in expressivity to handle non-standard textures. The recent LISA [12] model has emerged as an alternative, using an implicit function to represent hand geometry and texture color fields. Training a new identity in LISA, however, requires a multi-view capturing setup as well as a large amount of data and computing power. In the nearby fields of face and body avatar creation, many works that leverage an implicit function [19,42,43,77] or NeRF-based [44] volume rendering [41,56,80] have also been recently explored. The NeRF-based method such as HumanNeRF [80] produces a convincing novel view synthesis but still shows blurry artifacts around highly articulated parts and cannot be easily exported to other applications.\nWe argue that democratizing hand avatar creation for AR/VR users requires a method that is (1) accurate: so that personalized hand appearance and geometry can be faithfully reconstructed; (2) scalable: allowing hand avatars to be obtained using a commodity camera; (3) robust: capable of handling out-of-distribution appearance and selfshadows between fingers and palm; and (4) efficient: with real-time rendering capability.\nTo this end, we propose HARP, a personalized hand reconstruction method that can create a faithful hand avatar from a short RGB video captured by a hand-held mobile phone. HARP leverages a parametric hand model, an explicit appearance, and a differentiable rasterizer and shader to reconstruct a hand avatar and environment lighting in an analysis-by-synthesis manner, without any neural network component. Our observation is that human hands are highly articulated. The appearance changes of observed hands in a captured sequence can be dramatic and largely attributed to articulations and light interaction. Learning neural representations, such as implicit texture fields [12] or volume-based representations like NeRF [73], is vulnerable to the over-fitting to a short monocular training sequence and can hardly generalize well to sophisticated and dexterous hand movements. By properly disentangling geometry, appearance, and self-shadow with explicit representations, HARP can significantly improve the reconstruction quality and generate life-like renderings on novel views and novel animations performing highly articulated motions. Furthermore, the nature of the explicit representation allows the results from HARP to be conveniently exported to standard graphics applications.\nIn summary, the key advantages of HARP are: (1) HARP is a simple personalized hand avatar creation method\nthat reconstructs high-fidelity appearance and geometry using only a short monocular video. HARP demonstrates that an explicit representation with a differentiable rasterizer and shader is enough to obtain life-like hand avatars. (2) The hand avatar from HARP is controllable and compatible with standard rasterization graphics pipelines allowing for photo-realistic rendering in AR/VR applications. (3) Moreover, HARP can be used to improve 3D hand pose estimation in challenging viewpoints. We perform extensive experiments on the tasks of appearance reconstruction, novel-view-and-pose synthesis, and 3D hand poses refinement. Compared to existing approaches, HARP is more accurate, robust, and generalizable with superior scalability."
        },
        {
            "heading": "2. Related Work",
            "text": "Hand Models. Hand models are crucial for compactly representing a hand surface and using it in downstream applications. Many models rely on an explicit mesh surface [36, 37, 47, 65, 83], while others implicitly represent the surface with neural networks [12, 25, 26]. The widely used MANO model [65] represents a hand with pose and shape vectors, generating hand meshes using a PCA model and linear blend skinning. While parametric models like MANO can handle various hand geometries, their expressivity is limited by the learned parameter spaces. An alternative approach is to store vertex locations directly to broaden geometry representation [18, 38, 45, 70], but this prevents re-animation. Conversely, a limited amount of work has been explored for hand appearance modeling. Notable works include HTML [62], a linear appearance model for inferring the UV texture on top of MANO, and NIMBLE [37], an extension of the hand skeleton model [36], which can infer the surface appearance, including an albedo, specular, and normal map. Despite their realistic texture, these model suffers from their linear nature of appearance space and limited appearance data, making them unsuitable for adapting to a novel identity. In this work, we enhance the parametric model with personalized geometry adjustment, explicit albedo, and normal maps, enabling animatable personalized geometry and stronger texture representation than PCA-based textures.\nGeometry Reconstruction. To build a hand avatar, one must first derive the hand geometry from the input images which has been a long-studied problem [1, 4, 6, 51, 52, 66, 74, 85]. To estimate the hand surface, these methods generally leverage the statistical prior in the MANO model by predicting its pose and shape parameters. One advantage of such a method is that it could prevent geometry from collapsing by encouraging the hand shape to be close to the mean shape [21, 26]. To overcome this limited expressiveness of the MANO shape space, a convolution neural network (CNN) can be used to directly estimate the vertex lo-\ncations of the mesh topology [10, 18, 31, 32, 45]. Lin et al. [38] replace the convolution operation with a Transformer model [76] and achieve state-of-the-art results on various datasets. Alternatively, the hand surface can be represented by an implicit function [12, 25, 26]. With HALO [25], the hand geometry can be estimated by any key point estimation methods [3, 14, 17, 23, 48\u201350, 72, 75, 84, 89]. Given ground truth hand poses in multi-view images, differentiable ray tracing can also be used to refine the annotations [27]. In this work, we leverage the prediction by METRO [38] as initialization to refine a personalized geometry.\nAppearance Representations. Numerous methods have been proposed to learn and estimate appearance from images or videos for bodies with clothing [22,34,54,59,61,68] and faces [2, 15, 16, 19, 29, 56], including those that use NeRF [44] to implicitly represent appearance [56, 57]. To model a human, pose information can also be used as conditions to transform the radiance field of a human avatar [9,55,73,80,82]. However, these models are not suitable for hand appearance due to the high degree of articulation. Furthermore, mesh extraction is required to make these methods compatible with traditional graphics applications. In contrast, less research has been done on hand appearance representation [67]. While LISA [12] and HandAvatar [7] both learn implicit color fields together with implicit surface representations, they require a large number of images to train a subject-specific shape and appearance. Given an image, S2hand [8] employs an MLP to estimate the vertex colors and lighting together with MANO parameters. Concurrently, Wang et al. proposed SunStage [78], an outdoor face reconstruction method that shares similarities to our method, in particular, the use of explicit mesh representation and differentiable rendering. In this work, we present an explicit hand appearance model and the optimization pipeline that can capture detailed hand texture while also taking lighting and shadowing into account."
        },
        {
            "heading": "3. Method",
            "text": "Overview. The overview of HARP is illustrated in Fig. 2. Given a short monocular RGB video of a hand consisting of N frames, we reconstruct a realistic hand avatar with personalized shape and texture. Specifically, our method outputs a triangle mesh M , containing vertices V and faces F , and its UV texture, which is decomposed into albedo and normal maps. We optimize the personalized hand mesh M , the albedo, and the normal map using an analysis-by-synthesis approach by comparing the input images to the images of our reconstruction M rendered using a differentiable rendering framework. Our approach focuses on efficiency, robustness, and exportability, while also maintaining a high-quality hand appearance. Given the focus, we employ (1) explicit represen-\ntations (mesh, normal map, and albedo) which can be easily exported to any graphics application; (2) direct optimization of the explicit appearance without relying on a learned appearance space, such that we do not require pre-training nor a large number of training images. (3) the explicit and efficient rasterizing and shading which provides a good balance between rendering quality and computation cost."
        },
        {
            "heading": "3.1. Hand Representation",
            "text": "Template Model. Our hand template model is built upon the MANO [65] model, which we extend to a higher mesh resolution and allow surface vertex deformation from the template. Concretely, we perform a linear subdivision on the MANO template to increase the number of vertices from 778 to 3093, which in turn allows our template to capture more surface details. The subdivision process is differentiable, thus enabling gradient propagation back to the MANO pose parameter \u03b3 and shape parameter \u03b2. Additionally, the MANO hand is truncated at the wrist, which does not reflect the reality where hands are attached to the arms and the foreground mask does not separate the wrist. Therefore, we built another template from SMPLX [58] by truncating the arm at the elbow to facilitate the hand-and-arm fitting depending on the available mask. Their interactions with the rest of the system remain identical.\nGeometry Refinement. To utilize the higher mesh resolution for finer geometry details, we allow each posed vertex to additionally deform based on a personalized vertex displacement D along the vertex normal. The posed hand vertex locations without arm could be obtained with:\nV = S(M(\u03b3, \u03b2)) +D, (1) S : R778\u00d73 \u2192 R3093\u00d73, (2)\nwhere M is the MANO function which takes pose \u03b3 and shape \u03b2 as inputs and returns posed mesh vertex locations, S is the mesh subdivision function. The vertex displacement vector D is subject-specific, which we obtain by optimizing it along with other parameters.\nTexture. We model the hand skin as a Lambertian surface with an albedo map a, which is defined per subject in a UV space. Additionally, to add high-frequency details without upsampling the mesh, we utilize a UV-space normal map which can be combined with the surface normal N\u0302 when computing the illumination at the 3D surface point x."
        },
        {
            "heading": "3.2. Shadow-aware Differentiable Rendering",
            "text": "There are multiple options for differentiable rendering such as a differentiable path tracer [53], differentiable rasterizer [40], neural renderer [28] or the recently developed NeRF-based volume rendering [44]. The path tracer is known for its realistic rendering and explicit factorization of\nmaterial properties. However, this comes at a high computation cost and has not been shown to work with highly articulated objects [20,53]. NeRF-based methods [73,80,87] and implicit texture-based methods [12] are often computationally expensive, and as they leverage MLPs to approximate the light interaction with the articulated parts; the result can be blurry and have self-shadows baked into the texture representation.\nIn this work, we demonstrate that, to create a realistic hand avatar from a short video sequence, it is not necessary to rely on neural implicit representations, such as neural volume rendering. Standard explicit geometry and appearance representations together with a carefully implemented differentiable rendering scheme are able to provide the right balance between quality, speed, and compatibility with other graphics applications.\nTo render the hand in the camera view, we first utilize the differentiable rasterizer [63] to determine the visible surfaces from the camera views. We then use the Phong model [60] without the specular component to compute the illumination, i.e. color, at the surface point x:\nIx = kaia + \u2211\nm\u2208lights kdim,d(L\u0302m \u00b7 N\u0302) (3)\nwhere ka is an ambient reflection constant, kd is a diffuse reflection constant, ia and im,d are intensities of the light sources for diffuse surface, N\u0302 is the normal at point x, and L\u0302m is a ray from the surface point toward each light source. We observe that having just one dominant light source (|m| = 1) provides a good balance between computation cost and the rendering quality, under the assumption that the hard shadow is usually produced by the closest light source when indoors and by the sun outdoors.\nSelf-shadowing. Notably, the Phong model does not account for self-shadowing which often occurs when a finger\ncasts a shadow onto other fingers and the palm. To accommodate this scenario, we add the visibility term V (x,m) at x with respect to the light m to the diffuse component, making it kd(L\u0302m \u00b7 N\u0302)V (x,m)im,d. We compute the visibility V by performing a two-step rasterization of the mesh. We integrate the classic idea of shadow mapping in computer graphics [81] into our differentiable pipeline. The step-by-step computation is shown in Alg 1. The zbuffer Zm is a depth image when seen from the camera camm. If the light position is more than 1 m away from the hand, we project it to 1 m distance. The Sigmoid function is used to ensure smooth gradients, with a bias term b = 0.005 and scale s = 1000. To produce a softer shadow, we use percentage-closer filtering [64], which averages the visibility values of the nearby points. These visibility values then allow us to integrate the self-shadow into the pipeline. We note that our self-shadowing implementation is differentiable with respect to both geometry and appearance. Our implementation is compatible with the pyTorch3D [63] package, and will be made publicly available.\nAlgorithm 1 Visibility V (x,m) from light m 1: Place virtual camera camm at light m pointing at hand 2: Compute z-buffer Zm(\u00b7) from camm 3: Get 3D points {Xhit} seen from the actual camera 4: for x \u2208 Xhit do 5: Transform x to camm coordinate\nxm = Tm(x) 6: Get 2D pixel coordinate of xm x2d = \u03c0(xm) 7: Compute distance to the light dm\u2192x = ||x\u2212m|| 8: V (x,m) = Sigmoid(s(Zm(x\n2d)\u2212 dm\u2192x + b)) 9: end for"
        },
        {
            "heading": "3.3. Optimization",
            "text": "To find the parameters that describe the personalized hand, we optimize the parameters using short RGB videos. The optimization objective is to minimize the difference between the input and the rendered hand images using the proposed differentiable rendering pipeline. For each subject, we optimize for a joint objective consisting of a geometry alignment term Egeo and an appearance term Eapp:\nE = Egeo + Eapp, (4)\nwhere Egeo focuses on mesh configuration and Eapp encourages the same appearance as in the input images.\nGeometry Objective. For the geometry, the goal is to match the rendered silhouette with the hand mask while also satisfying 3D mesh constraints. The geometry objective is defined independently from the appearance as follows:\nEgeo = wsil \u00b7 Esil + Ereg, (5)\nwhere Esil is the silhouette difference term, Ereg is the mesh regularization term, and w are the weights.\nThe silhouette difference term is the l1-difference between the hand mask and the rendered silhouette Srender:\nEsil = |Sin \u2212 Srender|, (6)\nwhere Sin \u2208 {0, 1}H\u00d7W is an input binary hand mask that can be obtained from off-the-shelf segmentation tool [24].\nTo prevent the optimized mesh from collapsing, we employ a combination of 3D mesh regularizations defined as:\nEreg = Einit + Everts + Elap + Enorm + Earap, (7)\nwhere each term is accompanied by its weight. The term Einit penalizes key points deviation from the\ninitial pose with l1 distance. The vertex offset regularization Everts controls the deviation from the MANO mesh to be small using an l2-norm: Everts = \u2225D\u22252.\nThe mesh vertices V are regularized by the Laplacian mesh regularizer [13] Elap and the normal consistency regularizer Enorm defined on the posed mesh.\nThe term Earap is the as-rigid-as-possible term [71] that encourages the 3D mesh to be more rigid and distributes the changes in length among multiple edges. The edge length difference is defined with respect to the MANO template as:\nEarap = \u2211V\nv \u2211 u\u2208N (v) \u2225\u2225vt \u2212 ut\u2225 \u2212 \u2225v\u2217 \u2212 u\u2217\u2225\u2225 2 , (8)\nwhere N (v) are the adjacent vertices of v, vt is a vertex from frame t, and v\u2217 is a vertex from the MANO template.\nAppearance Objective. The appearance term Eapp measures the similarity between the input and the rendered image. Note that as the texture is mapped to the triangle mesh\nfor rendering, the appearance term is also affected by the geometry change. We define the appearance term Eapp as:\nEapp = wphoto \u00b7 Ephoto + wvgg \u00b7 Evgg + Eapp reg (9)\nwhere Ephoto is a per-pixel l1 color difference between the input images and the predicted images, Evgg is the VGG loss [33] that captures the perceptual difference between the two images by comparing the features extracted using the VGG model [69], and Eapp reg is a regularization term that encourages both albedo and normal map to be locally smooth [87] (details in the Appendix).\nInitialization. For optimization, we initialize the hand parameters with per-frame predictions from the hand pose estimator METRO [38]. As METRO directly predicts the mesh coordinate without using the MANO pose and shape space, we obtain the equivalent MANO pose \u03b3 and shape \u03b2 parameters by minimizing the l2-distances between corresponding vertices from the prediction and the MANO mesh.\nOptimization. In summary, we optimize: hand geometry parameters: (1) \u03b2, the global MANO shape parameter, (2) D, the per-vertex displacement, (3) \u03b3, the per-frame MANO pose parameter, including (4) the per-frame translation, global appearance parameters: (5) a, the UV-space albedo, and (6) the UV-space normal map, lighting parameters: (7) xlights, the light positions, (8) k, the global reflection constant. Fig. 2 shows the overview of our optimization process. More details are in Appendix."
        },
        {
            "heading": "4. Experiments",
            "text": "Datasets. There is a rich literature on datasets for hand pose estimation and geometry reconstruction [5,49,88], but less on hand appearance reconstruction. For our personalized hand avatar reconstruction task, there is no suitable dataset captured in the out-of-lab environment with monocular commodity equipment. Therefore, apart from evaluating on the existing InterHand2.6M [49] dataset, we create our own datasets for hand avatar creation, including a hand appearance dataset and a synthetic dataset (Fig. 3).\nHand Appearance Dataset. To simulate less-constrained capture settings similar to what the end-users of AR/VR applications typically have, all of our videos are captured by holding a phone camera pointing at a right hand in normal office lighting (Fig.1). Our hand appearance dataset consists of three parts: (1) Multi-subject single-view hand sequences. The captures contain four subjects, three male subjects and one female subject, in motions ranging from flipping the hand to more complex interactions between fingers. In total, there are 750 training and 600 testing frames for each subject. (2) Out-of-distribution hand appearance. We captured 2 additional subjects with tattoos for testing the out-of-distribution appearance, each containing\n4 sequences. Otherwise, the setting is the same as the first part. (3) Lighting and shadow variation. We selected a subject from the previous part to capture 6 sequences of simple hand motions under varying directions of a single dominant light source. The shadow is highly pronounced in this portion. For all parts, we ensure that both sides of the hand are visible. It is still possible that parts that are usually occluded, such as areas between fingers, are not visible. We obtain a binary hand mask for each frame using an online segmentation tool [24] that considers the hand and the visible part of the arm as foreground.\nInterhand2.6M Dataset. [49] The data is captured in a capturing dome with controlled lighting which is more restrictive than our primary goal of casually captured video. The foreground masks are obtained using RVM [39], which are sometimes noisy. We select a single-view sequence of length 500 frames from the test set where most of the surface is visible for appearance evaluation.\nSynthetic Dataset. As the 3D annotations of real datasets often contain fitting error (reported at 2-3mm for InterHand2.6M [49]), we opt to use a synthetic dataset with perfect ground truth to evaluate hand pose estimation task. We rendered images of two subjects, each with two sequences, using a ray tracing engine Cycles in Blender [11]. More details about the datasets can be found in the Appendix.\nBaselines. We summarize the overview of the available baselines for hand avatar creation from RGB images in Tab. 1. Our main advantages compared to the baselines are: (i) by relying on explicit UV and normal maps, our output is directly compatible with standard graphics pipelines; (ii) we can represent non-standard hand textures which are often not captured well by the models that rely on a PCA space for appearance (Sec 4.1); (iii) HARP is more efficient in term of optimization time and memory than MLP-based methods due to the lower number of parameters.\nFor the methods which are a variation of a hand model, namely S2Hand [8], HTML [62], and NIMBLE [37], the appearance is often predicted by a regressor, which can be inaccurate. Nevertheless, such models also allow test time optimization to refine the appearance according to the input images. For a fair comparison, we optimize the hand appearances in all of these baselines at test time.\nWe provide details on how we adapt each work for the hand avatar creation in the Appendix. For the methods that require pose and shape initialization\u2013all except S2Hand\u2013we fit the hand model to the vertex predictions by METRO [38]. The comparison to LISA [12] is omitted as there is no released code, model, or result that we could compare to. Nevertheless, our results on InterHand2.6M (Fig. 6) show a superior qualitative appearance than those presented in [12].\nEvaluation Metrics. For appearance evaluation, we report metrics that focus on the rendered image quality including the pixel-wise L1, the silhouette intersection-overunion (IoU), the learned perceptual image patch similarity (LPIPS) [86], and the multi-scale structural similarity metric (MS-SSIM) [79]. The rendered image with white background is compared to the masked input. Importantly, the pixel-wise L1 difference, LPIPS, and the intersection-overunion evaluation are not directly comparable between methods that produce different hand geometry due to the missing appearance of the truncated wrist in MANO and NIMBLE. Therefore they should be considered as references rather than direct comparisons. For pose evaluation, we report Procrustes-aligned vertex error (PA-MPVPE) in mm compared to the MANO ground truth."
        },
        {
            "heading": "4.1. Hand Appearance Reconstruction",
            "text": "To demonstrate the realism and robustness of HARP, we first evaluate its ability to reconstruct and re-render hand appearance from RGB sequences. We use the first part of our appearance dataset which reflects a more common environment for hand-related applications, e.g., personalized hand reconstruction and rendering for AR/VR. We show the qualitative texture and geometry in Fig. 4 and the quantitative evaluation in Tab. 2 averaged over all subjects. The results\nsuggest that HARP can faithfully reconstruct hand appearance with much higher details than the baselines. In addition, we demonstrate the robustness of HARP with appearance evaluation on the InterHand2.6M sequence in Tab. 3 and avatar reconstruction on various datasets in Fig. 6.\nOut-of-distribution Hand Appearance. To demonstrate the advantage of our method to capture out-of-distribution appearance, we compare the optimized results to those of the PCA-based HTML and NIMBLE and the MLP-based NHA on the videos of hands with tattoos, using the same optimization pipeline and objectives. The results are shown in Tab. 4. By not being constrained by the PCA space, we can reasonably capture such out-of-distribution appearance. At the same time, the tattoos are completely discarded by HTML and NIMBLE as they are not in the training set of those models. The qualitative results Fig. 5 demonstrate the robustness of our system to capture the non-standard ap-\npearance. Note that we only demonstrate with the tattoo but such appearance deviation can also be scars or nail coloring (please see the Appendix).\nDifferentiable Self-shadow Modeling. Self-shadowing between fingers and palm is almost inevitable due to highly articulated and dexterous hand movements. However, none of the baselines can properly capture and model selfshadowing. To validate the effectiveness of our shadowaware differentiable shader, we compare the appearance reconstruction quality with and without shadow modeling. The quantitative comparison is shown in Tab. 5. Without properly modeling shadow, the optimized color at each pixel averages the color when that pixel is in and out of the shadow, resulting in dark patches baked into the texture."
        },
        {
            "heading": "4.2. Hand Pose Reconstruction via Appearance",
            "text": "Modeling\nWith a realistic hand appearance obtained from a video using HARP, we demonstrate that such appearance information can improve the RGB hand pose estimation from the same identity using differentiable rendering if the appearance is known. Intuitively, with differentiable rendering, the optimization should be able to improve the initial prediction using only the hand mask to refine the pose. However, we observe that such masks are not always useful depending on the poses and viewpoints (see Appendix). In such cases, knowing the hand appearance supplements the mask in guiding the optimization toward the correct pose.\nTo the best of our knowledge, we are the first to qual-\nitatively and quantitatively demonstrate such improvement for hand pose estimation. We believe that the main component for enabling such improvement lies in the design of the rendering process such as shading and visibility check. However, such topics did not receive much attention in the hand community especially in the context of pose estimation prior to this work. We analyze the scenario in which such improvement is possible in the Appendix.\nIn Tab. 6, we compare the hand pose error between METRO [38], HARP with silhouette loss only (HARP-sil), a normal HARP (HARP-full), and HARP with a known appearance that is frozen during optimization (HARP-known). The known appearance is obtained from running HARP on a simple video of the same identity. The results indicate that HARP can leverage the differentiable rendering of appearance to improve poses. When the appearance is known in advance (HARP-known), the optimization can perform better as it avoids the shadow and lighting effect that will be baked into the texture during the optimization."
        },
        {
            "heading": "4.3. Novel View and Pose Synthesis",
            "text": "The explicit geometry obtained using HARP also enables consistent 3D geometry and appearance rendering across different poses and viewpoints. We evaluate this feature by rendering the known appearance onto the hands in novel views and novel poses, using the test sequences of our dataset. To match the pose in the test sequences, we optimize only the pose parameter \u03b3 (see Sec. 3.1) with respect to the hand masks while freezing all other components. The quantitative results shown in Tab. 7 suggest that HARP can produce consistent appearances in novel views and poses."
        },
        {
            "heading": "5. Conclusion and Limitation",
            "text": "In conclusion, we present HARP, a method for reconstructing personalized hand geometry and appearance from monocular RGB sequences. Starting from a parametric hand model as a geometry backbone, HARP refines the surface to a personalized hand shape. The texture is factorized into an albedo and a normal map. The resulting hand model is robust when rendered in novel views and novel poses, outperforming existing baselines both qualitatively and quantitatively. Furthermore, HARP is efficient, scalable, and compatible with traditional rendering pipelines. It provides a foundation for the realistic experience of personalized hands in AR/VR applications.\nLimitation. As our system assumes only a single light source and ambient light, with no specular effect, its ability to replicate the appearance under other lighting assumptions might still be limited. Incorporating an environment map, modeling bounced light, as well as increasing the resolution of the rendered texture are all interesting steps for future works toward a more photorealistic rendering.\nAcknowledgement. We sincerely acknowledge Shaofei Wang and Marko Mihajlovic for the discussions, and Malte Prinzler for the help with the Neural Head Avatar baseline. This work was supported by the SNF grant 200021 204840 and an ETH Zu\u0308rich Postdoctoral Fellowship."
        },
        {
            "heading": "A. Datasets",
            "text": "In this section, we describe the details of each dataset used in the experiments. We reiterate that our goal is to propose a scalable and robust system that can create faithful hand avatars given a short video sequence that is captured by commodity hardwares, such as a smartphone. Such setup facilitates the utility of our method in downstream applications, e.g., personalized hand avatar creation for end-users of AR/VR devices. Unfortunately, there is no existing dataset designed and captured for this scenario. Therefore we capture the Hand Appearance Dataset with a smartphone in a room with common lighting conditions (e.g., light bulbs on the ceiling).\nTo demonstrate that HARP is robust to different capture setups, we additionally test HARP on sequences from the InterHand2.6M [49] and HanCo [88] datasets. To supplement the lack of accurate ground truth to evaluate the pose refinement results, we create our Synthetic Dataset using a ray tracing engine. The details of each dataset are as follows:\nHand Appearance Dataset. The dataset is partitioned into three parts as described in the main paper. All of the sequences are captured with a hand-held smartphone camera in different conditions. The foreground masks, which include both the hand and arm, are obtained using an off-the-shelf segmentation tool Unscreen [24]. The images are resized to 448x448 pixels, which we use as a default size in all of our experiments unless indicated otherwise.\nInterHand2.6M [49]. We demonstrate HARP\u2019s ability to create an avatar from existing datasets on sequences from the Interhand2.6M dataset. The sequences in the dataset are captured in a capture dome with a multi-view camera rig and uniform lighting. As the dataset does not include segmentation masks, we obtain foreground-background masks using RVM [39]. We notice that flares from light bulbs in the capture dome often interfere with the segmentation and are sometimes categorized as foreground. We note that such artifact is difficult to remove and could degrade the optimized appearance. To avoid such artifacts, we use 500 frames from cam400266 of Capture0/ROM03 RT No Occlusion from the 30-FPS test set (frame 500th to 999th ). The images are cropped to 334x334 pixels with the mean of hand vertices at the center.\nSynthetic Dataset. In order to evaluate the pose refinement results, we create the synthetic dataset with perfect ground truth pose annotations. The images are rendered using the ray tracing engine Cycles in Blender [11]. We leverage the NIMBLE model [37] to obtain the hand meshes and appearances. The appearances are manually selected to ensure diversity in size and skin color from the appearances sampled from NIMBLE. Due to the dependency on NIMBLE, the generated hands are truncated at the wrist and the arm is not visible in the images. For rendering, we use the same Blender settings as the one provided in the demo of NIMBLE. The hand motions are the same in all of the generated sequences. The main differences between each sequence are the viewpoint, the hand shape, and the hand appearance. For each identity, we generate two motions, one is a hand-flipping motion and another one with finger movement. The motions are 5 seconds long at 30 FPS. Fig. A.1 shows the images from our Synthetic dataset.\nHanCo [88]. We show the results from the following sequences in Fig. 6 in the main paper: 0 (cam7), 2 (cam1), 10 (cam4), and 27 (cam4). Both sides of the hand are visible in these sequences. The provided foreground-background masks are used as input for the optimization. Nevertheless, note that the dataset is not suitable for avatar creation and should be treated only as a reference due to the low resolution of the image at 224x224 pixels and the unrealistic light stage. Samples images are shown in Fig. A.2."
        },
        {
            "heading": "B. Baselines",
            "text": "To ensure a fair comparison between HARP and the baseline methods, all of the baselines are optimized at test time, with an equivalent number of epochs when possible. We use the officially released code from each baseline. As some of the baselines are not designed for hand avatar creation via optimization, we need to make adjustments and modifications, which we describe for each baseline in the followings. In the case of HTML [62] and NIMBLE [37], we use the same optimization pipeline as in HARP, replacing only the relevant parts with their models. All the methods take the same images and pose initialization as input.\nHTML [62]. We replace the UV texture and normal map of HARP with the texture produced by the HTML model. The HTML texture vector is optimized instead of the HARP texture. As the HTML texture is defined on the surface of MANO [65] hand mesh, we use the MANO template instead of our HARP template. In addition, we allow vertex displacement along normals in the same manner as in HARP. The same optimizers and loss terms are used in the optimization.\nNIMBLE [37]. As the NIMBLE model provides both shape and appearance space, we replace the HARP hand geometry and appearance with NIMBLE. The NIMBLE pose, shape, and appearance parameters are updated during the optimization. The same optimizers and loss terms are applied. For initialization, we fit NIMBLE to the METRO prediction instead of our template, using the same formulation as described in Sec. C.2.\nS2Hand [8]. We modified the original S2hand code which predicts the appearance from the input image to allow the optimization with photometric losses used in HARP. Because the S2Hand model requires ground truth camera parameters for projecting the predicted mesh onto the image frame, during evaluation we optimize the camera parameters by minimizing the photometric loss with respect to the input image. This optimization is done separately to obtain the best match for each frame. We acknowledge that the optimized texture quality might be affected by the less accurate pose estimation from S2Hand. Nevertheless, due to the fact that S2hand requires ground truth camera intrinsics for image projection but METRO estimates camera extrinsics for a fixed set of intrinsics, it is not possible to optimize S2Hand with our coarse initialization.\nNeural Head Avatar [19]. As this method is designed specifically for reconstructing a human head avatar, we make several modifications to adapt it to the hand avatar creation task. Notably, we drop the dependency on face segmentation, landmark, and predicted normals from the model. For segmentation, there are only foreground and background, which are the same as the ones used in other baselines. For landmarks, the landmark locations are replaced with hand key point locations. The predicted normal input is discarded from the model. In terms of implementation, a fixed identity is used in place of the unavailable input. For the geometry, we replace the FLAME [35] model with our hand template with arm (details in Sec. C.1). We follow the official code and instructions for training and evaluating the results.\nC. Implementation Details\nC.1. Hand Templates\nIn order to create a hand avatar, we observe that the truncation at the wrist in the MANO [65] model is problematic to the appearance-creation process and does not reflect the reality that a hand is always attached to an arm. Thus, we implement a version of the hand model with an arm, which we derive from SMPLX [58] by truncating the mesh at the elbow, moving the root joint to the right-hand wrist, and linearly subdividing the mesh once. As a result, our hand model has two modes: handonly and hand-with-arm, which can be used interchangeably depending on the available mask. The comparison between the template meshes is shown in Fig. C.1.\nC.2. Initialization\nMesh Initialization. As discussed in the main paper, before we start the optimization process, the hand pose and shape need to be initialized. Therefore, we employ the pose estimator METRO [38] which estimates the camera translations with fixed intrinsic parameters. We average the camera translations across all frames from the same sequence and fix them throughout the optimization. However, because the METRO model predicts the mesh vertex locations directly, the predicted meshes cannot be used for animation. To this end, we fit the hand template to METRO predictions by optimizing pose parameters \u03b3 and shape parameters \u03b2 using the following energy term E:\nE = V\u2211 v \u2225vt \u2212 vp\u22252\nwhere Vt = M(\u03b3, \u03b2)\nwhere V is the set of MANO vertices in the template, and vp is the predicted location from METRO. Note that this optimization is possible because METRO prediction and MANO model share the same template mesh. To avoid local minima during fitting, we re-run the optimization process if the mean distance between METRO vertices and the optimized vertices is more than 1 cm.\nC.3. Optimization\nFrom the initialization, we first optimize using only the geometry term Egeo to reconstruct the hand surface. We then jointly optimize both the geometry and the appearance with the addition of Eapp. Once the geometry is stable in the joint optimization, we then freeze the geometry optimization and continue to refine the appearance with only Eapp. Concretely, to obtain the personalized hand pose, shape, and appearance, we employ a multi-stage optimization scheme as follows: (1) geometry optimization, (2) both geometry and appearance optimization, and (3) only appearance optimization. We use the Adam [30] for both Egeo and Eapp optimization. In total, the optimization takes an average of 80 minutes on a single Nvidia 3090 GPU.\nGeometry Optimization. Given the masked images, we first optimize the pose \u03b3, shape \u03b2, vertex displacements D, translations, and rotations, with respect to the geometry objective Egeo. In this stage, only the geometry energy term Egeo is used. loss is used. We optimize using a learning rate of 1e\u22123 for 100 epochs.\nJoint Optimization. After the coarse geometry alignment, we begin the appearance optimization with respect to the appearance objective Eapp. In this stage, both the geometry objective Egeo and appearance objective Eapp are optimized together for 50 epochs to correct geometry misalignment using appearance information on the input images. We use the learning rate of 1e\u22122 for the appearance optimizer.\nAppearance Optimization. Lastly, we refine the appearance with only the appearance objective Eapp for another 50 epochs. This step focuses on retrieving fine texture details which are difficult to optimize while the geometry is still changing.\nAppearance Regularization Terms. To regularize the reconstruction of the UV texture and normal map, we define the appearance regularization term in the UV space. Let T be an albedo map and G be a normal map, and I is a pixel in the UV space:\nEapp reg = Et reg + En reg, (10) Et reg = \u2211 I 1 3 \u2225T (I)\u2212 T (I + \u03f51)\u22251 , (11)\nEn reg = \u2211 I 1 3 (\u2225G(I)\u2212 G(I + \u03f52)\u22251 + \u2225G(I)\u2212 uz\u2225 2 2), (12)\nwhere \u03f51, \u03f52 are random pixel-space displacements sampled from a Gaussian with a standard deviation of 2, uz is a unit vector pointing along z direction. Both terms ensure a smooth transition in the UV space, while the En reg encourages the normal to be close to the surface normal.\nLosses. The weights for each energy term are as defined in the table C.1:\nC.4. Lighting Contribution\nAs we assume that the hand surface is largely non-reflective, we ignore the specular contribution in our lighting formulation. In our method, the color at each surface point is only affected by the ambient contribution, which dictates how bright each point is regardless of its position, and the diffuse contribution, which determines the brightness based on the angle between the point normal and the light direction. Diffuse lighting is also affected by the visibility term V that determines direct occlusion with respect to the light source. A higher ambient contribution will make the shadow less visible and the brightness more uniform. Figure C.2 shows the decomposition of each component in our pipeline. We show the differences between the final albedo after optimizing with and without considering the visibility term V in Fig. C.3.\nIn our experiments, we empirically disable the self-shadowing term of HARP when we compare it with the baselines on the first part of our hand appearance dataset, where there is no hard shadow and dominant light source. In other experiments, the self-shadowing term is enabled and the ratio between the ambient and diffuse contributions is optimized together with other parameters as described in the main paper."
        },
        {
            "heading": "D. Ablation",
            "text": "In this section, we discuss the importance and effect of each energy term in our method.\nAppearance. We observe that, without the perceptual term EV GG, the resulting texture looks overly smooth as the colors are averaged over the pixels that map to a slightly different point on the hand surface. However, without the photometric L1 term Ephoto, the result might contain noisy artifacts. The qualitative comparison is shown in Table D.1.\nGeometry. Figure D.1 shows the qualitative comparison between the results from optimizing without a specific shape regularization term. The as-rigid-as-possible term Earap prevents sharp edges when the mesh is deformed to fit the silhouette.\nThe vertex displacement term Everts ensures that the deviation from the shape space of the underlining parametric model is minimal, such that the blendshape from the parametric model is still useful when the mesh is reposed. The other terms including the normal consistency regularization Enorm and the Laplacian regularization Elap encourage the surface to be more smooth and less bumpy. We note that the effect of each term is less noticeable in the rendered image evaluation as the optimization can counteract the geometry change with a texture change. However, the regularization terms are necessary to ensure mesh integrity for any downstream application.\nFigure E.1. Qualitative results of out-of-distribution hands with different lighting conditions. Our method can accurately capture diverse appearances. The first two rows are captured in different lighting conditions with a different number of ceiling lights. The last three rows are synthetic data rendered with three light sources and ambient light. Please zoom in for details."
        },
        {
            "heading": "E. Discussion",
            "text": "E.1. Additional Results\nWe show additional results in Fig. E.1 that demonstrate our method\u2019s ability to capture diverse patterns such as tattoos, nail colors, and scars faithfully on diverse skin colors.\nE.2. Pose-dependent surface deformation\nAs our model is built on top of MANO, it has the pose-dependent surface deformation modeled by MANO pose blend shapes. Our displacement map is designed to capture detailed, personalized hand shapes that cannot be represented by MANO. We found that conditioning the displacement map on poses results in two different sets of parameters governing the same surface deformation which could be difficult to optimize.\nE.3. Failure Cases\nIn this section, we discuss the noticeable failure cases of our system. The examples are shown in Fig. E.2. First, HARP mainly uses the silhouette from a monocular view to guide the personalized geometry. As a consequence, it is crucial that the images and the masks provide sufficient information about the shape. When the data is not sufficient, the hand mesh can deform in an unexpected way to satisfy the mask. We show this failure case in Fig. E.2(a) where some vertices extend perpendicular to the palm as there are not enough side view images. Note that the optimization can always compensate for the bumpy geometry with a change in the texture in order to replicate the input images. A potential solution to this problem\nFigure E.2. Failure cases and limitations. (a) With limited pixel information, the geometry could deform in an undesirable way as it uses mainly the silhouette for supervision. (b) HARP pose optimization is sensitive to initialization and could stuck in local minima when the initialization and foreground mask do not align. In this case, a large immediate increase in silhouette loss will prevent the optimizer from leaving the minima.\nis to vary the geometry regularization terms based on the characteristic of the hand in the video. Second, as our method relies on the initialization from a hand pose estimator and the foreground mask, the final pose and the appearance quality are influenced by the performance of the pose estimator and the segmentation tool. In some cases, the pose might be stuck in a local minimum due to the initialization (Fig. E.2(b)).\nE.4. Pose Refinement via Appearance Optimization\nIn the main paper, we show that by optimizing the hand pose parameters with HARP, we can refine the estimated hand pose to better fit the image, which can lead to a slight improvement in the Procrustes-aligned hand pose error. Our intuition is that if the hand\u2019s appearance is known in advance, it should be possible to leverage pixel color optimization to obtain more accurate poses. We compare the results between (1) the initial estimation, (2) HARP with only geometry term Egeo (HARP-sil), (3) normal HARP (HARP-full), and (4) HARP with known appearance (HARP-known). Case (2) is a known task that is often associated with a differentiable renderer [40,63] where the silhouette is used to optimize for an object pose. However, we observe that for a highly articulated object such as a hand, using silhouette alone might not be enough to obtain the correct pose. We visualize such scenarios in Fig. E.3. Case (3) leverages only the appearance consistency within the optimized video. As both the poses and the appearance are optimized together, it is possible to obtain the colors that are associated with wrong poses. Case (4) leverages the appearance that is obtained from possibly easier hand motion. All of the hand parameters, except for hand poses, are given as initialization. Those parameters, including the appearance, are obtained from running HARP on another sequence. The given parameters are frozen during the optimization and only the hand poses are updated. All loss terms are the same as normal HARP.\nWe demonstrate that such pose refinement is possible if the appearance consistency is leveraged in the optimization (both case 3 and case 4). We acknowledge that our synthetic dataset is small relative to the recent hand pose dataset such as InterHand2.6M [49] However, due to the lack of ground truth with accurate 3D annotations, we could only perform the experiment on our synthetic dataset where we have perfectly accurate ground truth. The InterHand2.6M dataset [49], which offers the hand motions that are the closest to our target use case, reported the MANO ground truth fitting error at around 5 mm [46]. On the other hand, the Procrustes-aligned MANO vertex error of the METRO [38] prediction on our selected sequence is at 6 mm. Any quantitative improvement below 1 mm would be statistically meaningless as it is an order of magnitude smaller than the supposed ground truth error. Therefore, we do not report the pose refinement on this dataset and other existing datasets due to similar reasons.\nFuture work. We foresee that the ideal scenario for this use case would be when a user starts using AR/VR equipment, they do a hand-flipping motion to provide a hand appearance. And with that appearance, the pose estimation can be improved. Practically, however, the optimization speed would still prevent real-time pose refinement. As such improving the speed and pose estimation error which would be interesting to explore in future work.\nFigure E.3. Example cases where the hand mask is not informative enough for determining the hand pose."
        }
    ],
    "title": "HARP: Personalized Hand Reconstruction from a Monocular RGB Video",
    "year": 2023
}