{
    "abstractText": "Crop-type mapping is the foundation of grain security and digital agricultural management. Accuracy, efficiency and large-scale scene consistency are required to perform crop classification from remote sensing images. Many current remote-sensing crop extraction methods based on deep learning cannot account for adaptation effects in large-scale, complex scenes. Therefore, this study proposes a novel adaptive feature-fusion network for crop classification using single-temporal Sentinel-2 images. The selective patch module implemented in the network can adaptively integrate the features of different patch sizes to assess complex scenes better. TabNet was used simultaneously to extract spectral information from the center pixels of the patches. Multitask learning was used to supervise the extraction process to improve the weight of the spectral characteristics while mitigating the negative impact of a small sample size. In the network, superpixel optimization was applied to post-process the classification results to improve the crop edges. By conducting the crop classification of peanut, rice, and corn based on Sentinel-2 images in 2022 in Henan Province, China, the novel method proposed in this paper was more accurate, indicated by an F1 score of 96.53%, than other mainstream methods. This indicates our model\u2019s potential for application in crop classification in",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangyu Tian"
        },
        {
            "affiliations": [],
            "name": "Yongqing Bai"
        },
        {
            "affiliations": [],
            "name": "Guoqing Li"
        },
        {
            "affiliations": [],
            "name": "Xuan Yang"
        },
        {
            "affiliations": [],
            "name": "Jianxi Huang"
        },
        {
            "affiliations": [],
            "name": "Zhengchao Chen"
        }
    ],
    "id": "SP:d1ad53e900c0f5b317958f4042110e8eb614d4b2",
    "references": [
        {
            "authors": [
                "X. Chen",
                "Z. Cui",
                "M. Fan",
                "P. Vitousek",
                "M. Zhao",
                "W. Ma",
                "Z. Wang",
                "W. Zhang",
                "X. Yan",
                "J Yang"
            ],
            "title": "Producing more grain with lower environmental costs",
            "venue": "Nature",
            "year": 2014
        },
        {
            "authors": [
                "B. Kuzman",
                "B. Petkovi\u0107",
                "N. Deni\u0107",
                "D. Petkovi\u0107",
                "B. \u0106irkovi\u0107",
                "J. Stojanovi\u0107",
                "M. Mili\u0107"
            ],
            "title": "Estimation of optimal fertilizers for optimal crop yield by adaptive neuro fuzzy logic",
            "venue": "Rhizosphere 2021,",
            "year": 1990
        },
        {
            "authors": [
                "J.M. Jez",
                "C.N. Topp",
                "G. Silva",
                "J. Tomlinson",
                "N. Onkokesung",
                "S. Sommer",
                "L. Mrisho",
                "J. Legg",
                "I.P. Adams",
                "Y Gutierrez-Vazquez"
            ],
            "title": "Plant pest surveillance: From satellites to molecules",
            "venue": "Emerg. Top. Life Sci. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Rasti",
                "C.J. Bleakley",
                "N. Holden",
                "R. Whetton",
                "D. Langton",
                "G. O\u2019Hare"
            ],
            "title": "A survey of high resolution image processing techniques for cereal crop growth monitoring",
            "venue": "Inf. Process. Agric",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wen",
                "X. Li",
                "H. Mu",
                "L. Zhong",
                "H. Chen",
                "Y. Zeng",
                "S. Miao",
                "W. Su",
                "P. Gong",
                "B Li"
            ],
            "title": "Mapping corn dynamics using limited but representative samples with adaptive strategies",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2022
        },
        {
            "authors": [
                "J. Gallego",
                "E. Carfagna",
                "B. Baruth"
            ],
            "title": "Accuracy, objectivity and efficiency of remote sensing for agricultural statistics",
            "venue": "In Agricultural Survey Methods; John Wiley & Sons, Inc.: Hoboken, NJ,",
            "year": 2010
        },
        {
            "authors": [
                "N. Kussul",
                "M. Lavreniuk",
                "S. Skakun",
                "A. Shelestov"
            ],
            "title": "Deep learning classification of land cover and crop types using remote sensing data",
            "venue": "IEEE Geosci. Remote Sens. Lett. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "D. Bargiel"
            ],
            "title": "A new method for crop classification combining time series of radar images and crop phenology information",
            "venue": "Remote Sens. Environ",
            "year": 2017
        },
        {
            "authors": [
                "D. Haboudane",
                "J.R. Miller",
                "N. Tremblay",
                "P.J. Zarco-Tejada",
                "L. Dextraze"
            ],
            "title": "Integrated narrow-band vegetation indices for prediction of crop chlorophyll content for application to precision agriculture",
            "venue": "Remote Sens. Environ",
            "year": 2002
        },
        {
            "authors": [
                "L. Zhang",
                "L. Gao",
                "C. Huang",
                "N. Wang",
                "S. Wang",
                "M. Peng",
                "X. Zhang",
                "Q. Tong"
            ],
            "title": "Crop classification based on the spectrotemporal signature derived from vegetation indices and accumulated temperature",
            "venue": "Int. J. Digit. Earth",
            "year": 2022
        },
        {
            "authors": [
                "X. Huang",
                "J. Huang",
                "X. Li",
                "Q. Shen",
                "Z. Chen"
            ],
            "title": "Early mapping of winter wheat in Henan province of China using time series of Sentinel-2 data",
            "venue": "GIScience Remote Sens",
            "year": 2022
        },
        {
            "authors": [
                "N. Yang",
                "D. Liu",
                "Q. Feng",
                "Q. Xiong",
                "L. Zhang",
                "T. Ren",
                "Y. Zhao",
                "D. Zhu",
                "J. Huang"
            ],
            "title": "Large-scale crop mapping based on machine learning and parallel computation with grids",
            "year": 2019
        },
        {
            "authors": [
                "Z. Mingwei",
                "Z. Qingbo",
                "C. Zhongxin",
                "L. Jia",
                "Z. Yong",
                "C. Chongfa"
            ],
            "title": "Crop discrimination in Northern China with double cropping systems using Fourier analysis of time-series MODIS data",
            "venue": "Int. J. Appl. Earth Obs. Geoinf",
            "year": 2008
        },
        {
            "authors": [
                "X. Xiao",
                "S. Boles",
                "S. Frolking",
                "C. Li",
                "J.Y. Babu",
                "W. Salas",
                "Moore",
                "III"
            ],
            "title": "Mapping paddy rice agriculture in South and Southeast Asia using multi-temporal MODIS images",
            "venue": "Remote Sens. Environ",
            "year": 2006
        },
        {
            "authors": [
                "M.A. Hearst",
                "S.T. Dumais",
                "E. Osuna",
                "J. Platt",
                "B. Scholkopf"
            ],
            "title": "Support vector machines",
            "venue": "IEEE Intell. Syst. Their Appl",
            "year": 1998
        },
        {
            "authors": [
                "M. Ahmed",
                "R. Seraj",
                "S.M.S. Islam"
            ],
            "title": "The k-means algorithm: A comprehensive survey and performance evaluation",
            "venue": "Electronics 2020,",
            "year": 2020
        },
        {
            "authors": [
                "I. Nitze",
                "U. Schulthess",
                "H. Asche"
            ],
            "title": "Comparison of machine learning algorithms random forest, artificial neural network and support vector machine to maximum likelihood for supervised crop type classification",
            "venue": "In Proceedings of the 4th GEOBIA, Rio de Janeiro, Brazil,",
            "year": 2012
        },
        {
            "authors": [
                "T. Chen",
                "T. He",
                "M. Benesty",
                "V. Khotilovich",
                "Y. Tang",
                "H. Cho",
                "K. Xgboost Chen"
            ],
            "title": "Extreme Gradient Boosting",
            "venue": "R Package Version 0.4-2; 2015; pp. 1\u20134. Available online: http://cran.fhcrc.org/web/packages/xgboost/vignettes/xgboost.pdf",
            "year": 2023
        },
        {
            "authors": [
                "N. You",
                "J. Dong",
                "J. Li",
                "J. Huang",
                "Z. Jin"
            ],
            "title": "Rapid early-season maize mapping without crop labels",
            "venue": "Remote Sens. Environ",
            "year": 2023
        },
        {
            "authors": [
                "F. Waldner",
                "M.J. Lambert",
                "W. Li",
                "M. Weiss",
                "V. Demarez",
                "D. Morin",
                "C. Marais-Sicre",
                "O. Hagolle",
                "F. Baret",
                "P. Defourny"
            ],
            "title": "Land cover and crop type classification along the season based on biophysical variables retrieved from multi-sensor high-resolution time series",
            "year": 2015
        },
        {
            "authors": [
                "A. Bagnall",
                "J. Lines",
                "A. Bostrom",
                "J. Large",
                "E. Keogh"
            ],
            "title": "The great time series classification bake off: A review and experimental evaluation of recent algorithmic advances",
            "venue": "Data Min. Knowl. Discov",
            "year": 2017
        },
        {
            "authors": [
                "M. Belgiu",
                "O. Csillik"
            ],
            "title": "Sentinel-2 cropland mapping using pixel-based and object-based time-weighted dynamic time warping analysis",
            "venue": "Remote Sens. Environ",
            "year": 2018
        },
        {
            "authors": [
                "X. Yuan",
                "J. Shi",
                "L. Gu"
            ],
            "title": "A review of deep learning methods for semantic segmentation of remote sensing imagery",
            "venue": "Expert Syst. Appl",
            "year": 2021
        },
        {
            "authors": [
                "D. Wen",
                "X. Huang",
                "F. Bovolo",
                "J. Li",
                "X. Ke",
                "A. Zhang",
                "J.A. Benediktsson"
            ],
            "title": "Change detection from very-high-spatial-resolution optical remote sensing images: Methods, applications, and future directions",
            "venue": "IEEE Geosci. Remote Sens. Mag. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "X. Sun",
                "P. Wang",
                "C. Wang",
                "Y. Liu",
                "K. Fu"
            ],
            "title": "PBNet: Part-based convolutional neural network for complex composite object detection in remote sensing imagery",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "L. Gu",
                "F. He",
                "S. Yang"
            ],
            "title": "Crop classification based on deep learning in northeast China using sar and optical imagery",
            "venue": "In Proceedings of the 2019 SAR in Big Data Era (BIGSARDATA), Beijing, China,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Yuan",
                "H. Shen",
                "T. Li",
                "Z. Li",
                "S. Li",
                "Y. Jiang",
                "H. Xu",
                "W. Tan",
                "Q. Yang",
                "J Wang"
            ],
            "title": "Deep learning in environmental remote sensing: Achievements and challenges",
            "venue": "Remote Sens. Environ",
            "year": 2020
        },
        {
            "authors": [
                "L. Wang",
                "J. Wang",
                "Z. Liu",
                "J. Zhu",
                "F. Qin"
            ],
            "title": "Evaluation of a deep-learning model for multispectral remote sensing of land use and crop classification",
            "venue": "Crop J. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J. Xu",
                "J. Yang",
                "X. Xiong",
                "H. Li",
                "J. Huang",
                "K. Ting",
                "Y. Ying",
                "T. Lin"
            ],
            "title": "Towards interpreting multi-temporal deep learning models in crop mapping",
            "venue": "Remote Sens. Environ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Yu",
                "X. Si",
                "C. Hu",
                "J. Zhang"
            ],
            "title": "A review of recurrent neural networks: LSTM cells and network architectures",
            "venue": "Neural Comput",
            "year": 2019
        },
        {
            "authors": [
                "L. Ren",
                "X. Cheng",
                "X. Wang",
                "J. Cui",
                "L. Zhang"
            ],
            "title": "Multi-scale dense gate recurrent unit networks for bearing remaining useful life prediction",
            "venue": "Future Gener. Comput. Syst",
            "year": 2019
        },
        {
            "authors": [
                "R. Pullanagari",
                "M. Dehghan-Shoar",
                "I.J. Yule",
                "N. Bhatia"
            ],
            "title": "Field spectroscopy of canopy nitrogen concentration in temperate grasslands using a convolutional neural network",
            "venue": "Remote Sens. Environ",
            "year": 2021
        },
        {
            "authors": [
                "L. Zhong",
                "L. Hu",
                "H. Zhou"
            ],
            "title": "Deep learning based multi-temporal crop classification",
            "venue": "Remote Sens. Environ",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhao",
                "S. Duan",
                "J. Liu",
                "L. Sun",
                "L. Reymondin"
            ],
            "title": "Evaluation of five deep learning models for crop type mapping using sentinel-2 time series images with missing information",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhao",
                "Y. Zhong",
                "X. Hu",
                "L. Wei",
                "L. Zhang"
            ],
            "title": "A robust spectral-spatial approach to identifying heterogeneous crops using remote sensing imagery with high spectral and spatial resolutions",
            "venue": "Remote Sens. Environ",
            "year": 2020
        },
        {
            "authors": [
                "B. Xie",
                "H.K. Zhang",
                "J. Xue"
            ],
            "title": "Deep convolutional neural network for mapping smallholder agriculture using high spatial resolution satellite image",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "S.T. Seydi",
                "M. Amani",
                "A. Ghorbanian"
            ],
            "title": "A Dual Attention Convolutional Neural Network for Crop Classification Using Time-Series Sentinel-2 Imagery",
            "venue": "Remote Sens. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Yang",
                "L. Gu",
                "X. Li",
                "T. Jiang",
                "R. Ren"
            ],
            "title": "Crop classification method based on optimal feature selection and hybrid CNN-RF networks for multi-temporal remote sensing imagery",
            "venue": "Remote Sens. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "N. Kussul",
                "G. Lemoine",
                "F.J. Gallego",
                "S.V. Skakun",
                "M. Lavreniuk",
                "A.Y. Shelestov"
            ],
            "title": "Parcel-based crop classification in Ukraine using Landsat-8 data and Sentinel-1A data",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "D. Wang",
                "W. Cao",
                "F. Zhang",
                "Z. Li",
                "S. Xu",
                "X. Wu"
            ],
            "title": "A review of deep learning in multiscale agricultural sensing",
            "venue": "Remote Sens. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Orynbaikyzy",
                "U. Gessner",
                "C. Conrad"
            ],
            "title": "Crop type classification using a combination of optical and radar remote sensing data: A review",
            "venue": "Int. J. Remote Sens",
            "year": 2019
        },
        {
            "authors": [
                "S.\u00d6. Arik",
                "T. Pfister"
            ],
            "title": "Tabnet: Attentive interpretable tabular learning",
            "venue": "Proc. Aaai Conf. Artif. Intell",
            "year": 2021
        },
        {
            "authors": [
                "Y. Sun",
                "Q. Qin",
                "H. Ren",
                "T. Zhang",
                "S. Chen"
            ],
            "title": "Red-edge band vegetation indices for leaf area index estimation from sentinel-2/msi imagery",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "W. Wang",
                "X. Hu",
                "J. Yang"
            ],
            "title": "Selective kernel networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "R. Li",
                "S. Zheng",
                "C. Zhang",
                "C. Duan",
                "J. Su",
                "L. Wang",
                "P.M. Atkinson"
            ],
            "title": "Multiattention network for semantic segmentation of fine-resolution remote sensing images",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "D. Wang",
                "X. Chen",
                "M. Jiang",
                "S. Du",
                "B. Xu",
                "J. Wang"
            ],
            "title": "ADS-Net: An Attention-Based deeply supervised network for remote sensing image change detection",
            "venue": "Int. J. Appl. Earth Obs. Geoinf. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "T.D. Buskirk"
            ],
            "title": "Surveying the forests and sampling the trees: An overview of classification and regression trees and random forests with applications in survey research",
            "venue": "Surv. Pract. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "K.H. Thung",
                "C.Y. Wee"
            ],
            "title": "A brief review on multi-task learning",
            "venue": "Multimed. Tools Appl",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "M.R. Sabuncu"
            ],
            "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "R. Achanta",
                "A. Shaji",
                "K. Smith",
                "A. Lucchi",
                "P. Fua",
                "S. S\u00fcsstrunk"
            ],
            "title": "SLIC superpixels compared to state-of-the-art superpixel methods",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2012
        },
        {
            "authors": [
                "Y.J.N. Kumar",
                "V. Spandana",
                "V. Vaishnavi",
                "K. Neha",
                "V. Devi"
            ],
            "title": "Supervised machine learning approach for crop yield prediction in agriculture sector",
            "venue": "In Proceedings of the 2020 5th International Conference on Communication and Electronics Systems (ICCES), Coimbatore, India,",
            "year": 2020
        },
        {
            "authors": [
                "J. Verrelst",
                "J.P. Rivera",
                "F. Veroustraete",
                "J. Mu\u00f1oz-Mar\u00ed",
                "J.G. Clevers",
                "G. Camps-Valls",
                "J. Moreno"
            ],
            "title": "Experimental Sentinel-2 LAI estimation using parametric, non-parametric and physical retrieval methods\u2014A comparison",
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "year": 2015
        },
        {
            "authors": [
                "J. Estornell",
                "J.M. Mart\u00ed-Gavil\u00e1",
                "M.T. Sebasti\u00e1",
                "J. Mengual"
            ],
            "title": "Principal component analysis applied to remote sensing",
            "venue": "Model. Sci. Educ. Learn",
            "year": 2013
        },
        {
            "authors": [
                "R. Fitzgerald",
                "B. Lees"
            ],
            "title": "Assessing the classification accuracy of multisource remote sensing data",
            "venue": "Remote Sens. Environ",
            "year": 1994
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980",
            "venue": "Remote Sens. 2023,",
            "year": 1990
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "X. Hu",
                "X. Wang",
                "Y. Zhong",
                "L. Zhang"
            ],
            "title": "S3ANet: Spectral-spatial-scale attention network for end-to-end precise crop classification based on UAV-borne H2 imagery",
            "venue": "ISPRS J. Photogramm. Remote Sens. 2022,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Citation: Tian, X.; Bai, Y.; Li, G.; Yang,\nX.; Huang, J.; Chen, Z. An Adaptive\nFeature Fusion Network with\nSuperpixel Optimization for Crop\nClassification Using Sentinel-2\nImagery. Remote Sens. 2023, 15, 1990.\nhttps://doi.org/10.3390/rs15081990\nAcademic Editor: Dino Ienco\nReceived: 12 February 2023\nRevised: 29 March 2023\nAccepted: 6 April 2023\nPublished: 10 April 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: crop mapping; deep learning; feature fusion; multitask learning; Sentinel-2"
        },
        {
            "heading": "1. Introduction",
            "text": "Despite being a basic guarantee of human life, grain security is at risk owing to global population growth and accelerated climate change in recent years [1]. Crop type information is fundamental for crop yield estimation [2], crop pest monitoring [3], and growth monitoring [4], which are critical for maintaining grain security. However, the extraction and acquisition of crop type information are difficult, largely because the manual methods required are costly in terms of labor and resources [5], often resulting in small sample sizes. Remote sensing has been widely used in extracting crop-type information due to its wide monitoring range, low cost, and high timeliness [6\u20138]. Vegetation indices can be formed by combining visible and near-infrared bands of images, which can measure the condition of surface vegetation simply and effectively [9\u201311]. Therefore, most mature application methods use a vegetation index for crop classification [12]. Supported by time series of Moderate Resolution Imaging Spectroradiometer (MODIS) data, Zhang et al. [13] used the fast Fourier transform to smooth normalized difference vegetation index (NDVI) time-series curves while related parameters such as the curve\u2019s mean, phase, and amplitude were used to extract the spatial distribution data of crops in North\nRemote Sens. 2023, 15, 1990. https://doi.org/10.3390/rs15081990 https://www.mdpi.com/journal/remotesensing\nChina. Xiao et al. [14] used the NDVI, the enhanced vegetation index (EVI), and the land surface water index (LSWI) calculated from MODIS multi-temporal images to classify rice, water, and evergreen plants. With the recent increase in the use of algorithm iterations and remote sensing data, a variety of machine learning methods such as support vector machine (SVM) [15], k-means algorithm [16], maximum likelihood method [17], extreme gradient boosting (XGBoost) [18], and random forest (RF) [12] have been successfully applied to remote sensing crop classification [19]. For example, using the RF method, Markus et al. [20] used single-temporal Sentinel-2 data to classify seven crops in Austria. They achieved an overall accuracy of 76%, establishing the foundation for applying Sentinel-2 data in crop extraction. Waldner et al. [21] used time-series images from Landsat 8 and SPOT-4 to classify wheat, corn, and sunflower using artificial neural networks, achieving the highest classification accuracy of 85%. Furthermore, Wen et al. [5] used time-series Landsat data and limited high-quality samples to achieve large-scale corn classification mapping using the RF method. As one of the more effective methods for crop classification, dynamic time warping (DTW) is widely used in crop classification [22]. Belgiu et al. [23] evaluated how a time-weighted dynamic time warping (TWDTW) method that uses Sentinel-2 time series perform when applied to pixel-based and object-based classifications of various crop types in three different study areas. However, the above classification methods require the manual design of crop features, which relies on expert knowledge and fixed scenes, resulting in the insufficient generalization ability of these methods [24]. Therefore, these methods are not dependable for large-scale intelligent crop classification with multiple scenes. After ten years of development, the concept of deep learning was proposed in 2006, and it has since made breakthroughs in many remote sensing applications, such as land cover classification [25], building change detection [26], and complex ground object detection [27]. In the field of crop classification, deep learning has become a mainstream method with large-scale applications [28]. Compared with traditional crop classification methods, deep learning methods can automatically mine deep features from remote sensing data; can make full use of time, spatial, and spectral information in images; and have better antinoise and generalization abilities, making them the mainstream method of large-scale crop classification [29\u201331]. Recurrent neural network (RNN) methods, such as long short-term memory (LSTM) [32], gated recurrent units (GRU) [33], and Conv1D-CNN [34], originated from natural language processing and are particularly dependable for sequential data. Thus, they have been applied to crop extraction methods based on time-series images. For example, Zhong et al. [35] used Landsat time series data and land use survey results from Yolo County, California, to test the classification accuracy of LSTM and Conv1D-CNN methods in a variety of summer crops. However, obtaining complete time-series data is difficult, often resulting in cloud occlusions or missing data. To address these limitations, some data-filling methods exist. For example, Zhao et al. [36] classified crops from missing Sentinel-2 time series data using the filled missing data method and GRU. However, these processes requiring multiple data sources are complex and undependable for large-scale crop extraction. Furthermore, to introduce the spatial information of an image, such as texture and planting structure, to assist crop classification [37], most studies have used a specific patch size around the point as the model input. For example, Xie et al. [38] decomposed images into patches of different sizes as inputs in a convolutional neural network (CNN) for crop classification to compare the effect of patch size on crop classification accuracy. Additionally, Seyd et al. [39] used 11 \u00d7 11 pixels patches for classification and designed a two-stream attention CNN network to extract the spatial and spectral information of crops simultaneously. These methods generally use fixed-size patches as input, but plot sizes can vary greatly in actual large-scale scenes, lowering the model\u2019s accuracy. Furthermore, crop samples are generally obtained manually from the field, which can result in a small number of samples depending on staffing and available resources. A standard solution to this issue is to use a deep network for feature extraction followed by a machine learning method as the classifier, as demon-\nstrated by Yang et al. [40], who used a combination of CNN and RF for crop classification. However, there are significant differences in training and principles between deep neural networks (DNNs) and machine learning methods. The direct combination of these methods cannot fully exploit the advantages of deep learning. Limited by the spatial and spectral resolutions of satellite images, the spatial resolution of satellite images generally used for crop classification is low. For example, the highest resolution of Sentinel-2 is 10 m. To improve the boundary accuracy of crop extraction results, Kussul et al. [41] used plot vector data to optimize crop extraction results and proposed a new voting optimization method that could significantly improve the accuracy of the results. However, obtaining high-precision plot vector data and applying them to large-scale crop extraction is challenging. Using the images themselves for optimization may be a feasible method.\nAbove all, large-scale crop extraction still has the following problems: (1) Due to inevitable conditions such as cloud cover and missing data, it is difficult to\nobtain complete time-series images, especially for large-scale crop classification. Moreover, there are few studies on crop extraction from single-temporal remote sensing images [42,43]. (2) Using patches as model inputs may introduce noise while introducing spatial information. As shown in Figure 1, in mixed planting or terrace scenarios, the crops are small, long, and narrow in the area. Therefore, the number of pixels different from the central pixels category in a large patch exceeds half of the total redundant information may lead to classification errors. (3) Insufficient crop samples are usually obtained, and since deep learning methods with more parameters are prone to overfitting, there is a risk of the model having insufficient generalization ability and low accuracy in large-scale classification. (4) Limited by the spatial resolution of multispectral images, the phenomenon of mixed pixels is consequential, resulting in inaccurate crop boundaries.\nIn response to these problems, we propose a deep neural network for large-scale crop classification using single-temporal images named selective patches TabNet (SPTNet). The contributions of this study are as follows. (1) A selective patch module that can adaptively fuse the features of patches of different sizes is designed to improve the network\u2019s ability to extract small crop plots in complex scenes. (2) TabNet [44] and multitask learning were introduced to capture the spectral and spatial information of the central pixel to improve the weight of the central pixel during the classification process and enhance the network\u2019s generalization ability, which effectively reduced the negative impact of insufficient sample numbers. (3) Superpixel segmentation was implemented in the post-processing of classification results to increase the boundaries of crop plots.\n(4) High classification accuracy was achieved using the above modules and insufficient crop phenology information. A large-scale crop mapping of three major crops in 2022 in Henan Province, China, was produced to meet the government\u2019s demands on crop yield estimation and agricultural insurance."
        },
        {
            "heading": "2. Materials and Methods",
            "text": ""
        },
        {
            "heading": "2.1. Study Area",
            "text": "Crop classification and extraction experiments were carried out in Henan Province, China. As shown in Figure 2, Henan Province is located in central China, between 32\u00b023\u2032N\u2013 36\u00b022\u2032N and 110\u00b021\u2032E\u2013116\u00b038\u2032E. With a cultivated land area of 81,500 km2. Accounting for 6.05% of the total cultivated land area in China, Henan is an essential breadbasket in the country and has continuously produced the most grain output per province in China for many years. Located in the continental monsoon climate region of the transition from the northern subtropical to the warm temperate zone, Henan Province has two harvest seasons each year, in summer and autumn. Summer crops include mostly wheat, while autumn crops mainly include corn, rice, and peanut. This study focused on the classification and extraction of autumn crops in Henan Province in 2022."
        },
        {
            "heading": "2.2. Data and Processing",
            "text": "2.2.1. Remote Sensing Data\nThe remote sensing data used in this study were the bottom-of-atmosphere corrected reflectance products (L2A) of Sentinel-2 images, which can be downloaded from the European Space Agency (ESA) (https://scihub.copernicus.eu/, accessed on 16 March 2023). As shown in Table 1, there are 12 bands with resolutions of 10\u201360 m, in which the 4 vegetation red edge bands and shortwave infrared bands are sensitive to plant characteristics and are thus more dependable for crop classification [45]. Because the resolution of Sentinel-2 images varies between bands, the bands of 20 m and 60 m were pan-sharpened to 10 m, and we reprojected all images to the WGS-84 coordinate system. Considering image overlap and cloud occlusion, we processed the mosaic and standard map divisions after the cloud mask for all images. The entire Henan Province was completely covered by 38 Sentinel-2 images. The phenology of corn, peanut, and rice is shown in Figure 3. Considering both crop phenology and image cloud cover, we selected all 38 images with the minor cloud cover between 31 July and 15 August 2022 for the experiments.\n2.2.2. Reference Samples\nFigure 4 shows the distribution of the sample points collected through multiple field surveys across the study area. All field surveys were completed from August to September 2022. Over 4000 sample points were recorded in the field using a handheld global positioning system device (GARMIN Etrex221x; the positioning error is less than 3 m), mainly covering three crop types: corn, peanut, and rice. Additionally, a small number of other crops, such as soybean and pepper, were also recorded simultaneously and placed in a fourth category, titled others. In the field surveys, we abided by the following sampling rules to ensure the representativeness of the samples:\n(1) Record the coordinates of sample points when the positioning signal is strong; (2) Select the sampling position in the center of the crop plot, away from forests, green\nbelts, rivers, and lakes, which easily affect the characteristics of the crops; (3) Collect samples as evenly as possible in the experimental area to ensure samples are in various planting scenarios; (4) Concentrate on collecting samples in areas with complex planting structures to increase the number of samples. We then manually interpreted the images around the field sampling points to enrich the number of samples and supplement other background categories, such as residential areas, water, woodlands, and bare land, which were not covered in the field survey. As shown in Figure 5, all pixels in the extended area were used as samples. We then randomly divide the samples into training, validation, and test sets at a ratio of 10:1:9 to carry out the crop classification experiments. Finally, we obtained more than 300,000 sample points of the crop and non-crop types in Henan Province, of which peanut samples were the most prominent and corn samples were the least (Table 2)."
        },
        {
            "heading": "2.3. Methods",
            "text": "The general workflow of crop type classification based on the proposed method is illustrated in Figure 6. The proposed classification framework was implemented in three main steps: (1) data and sample preparation, (2) model training and accuracy evaluation, and (3) prediction and crop mapping. The details of each step are discussed in the following subsections.\nWe proposed a new DNN architecture named SPTNet, depicted in Figure 7, that is characterized by a selective patch module (SPM) that adaptively acquires multi-size patch features, a TabNet branch that models the spectral information of the center point separately, and multiple loss functions. Through the SPM, the input patches are fused to select the appropriate patch size adaptively. This process balances the relationship between the obtained spatial information and noise. To increase the weight of the spectral information of the central pixels, we also used multitask learning to model the spectral\ninformation of the pixel and introduce the corresponding loss to supervise the process directly. Finally, we introduced a superpixel segmentation method for post-processing to improve the boundaries of crop plots and reduce the negative impact of mixed pixels on the model\u2019s function.\n2.3.1. Selective Patch Module\nBecause crop plots vary in size in different scenarios, using oversized patches introduces too many pixels that are not in the same category as the central pixel, which may lead to some pixels being classified as noise. Conversely, using a too-small patch will lead to insufficient available spatial information, making the classification inaccurate. The above problem is similar to the receptive field problem in semantic segmentation, which requires selection according to the target size. The difference lies in selecting an appropriate receptive field or patch size. SK-Conv [46] automatically selects the convolutional receptive field. Several studies have used SK-Conv, which calculates the weight of feature maps obtained by convolution kernels of different sizes and fuses the feature maps according to the weights to realize the adaptive selection of the convolution kernel receptive field [47,48]. Inspired by the SK-Conv method, we constructed an SPM that adaptively fuses features of different patch sizes to solve scaling problems and obtain more accurate spatial information. Figure 7a shows that SPM consists of three stages: split, fusion, and selection. The division into three stages optimizes the ability to generate different patch sizes, aggregate different patch information to obtain a global representation of the selection weight, and obtain feature maps of different patch sizes based on the selection weight. First, in the split phase, the input patch is divided into new patches P\u0302, P\u0304, and P\u0303 according to size. In the study of crop classification in Henan Province, we divided the patches into the sizes of 3 \u00d7 3, 5 \u00d7 5, and 7 \u00d7 7 pixels due to the area of plots in Henan mainly being around 1 hectare. These patches are proportionate to the land scale in Henan Province. In the fusion phase, the patches were then convoluted twice to obtain the same size feature maps U\u0302 , U\u0304, and U\u0303. Next, information of different branches was fused in the fusion stage U = U\u0302 + U\u0304 + U\u0303. Then, global average pooling Fqp is used to embed global information to generate channel statistics for s \u2208 RC. To reduce the dimensions of the channel statistics and obtain the final channel weight z \u2208 RD, the full connection layer Ff c is used. This process can be expressed by Formula (1):\nz = Ff c(Fqp(U\u0302 + U\u0304 + U\u0303)) (1)\nFinally, in the selection phase, three weight matrices, Wa, Wb, and Wc, where a , b, and c are related to different patch sizes, were generated using the final channel weight z. To\nensure consistency, Wa, Wb, and Wc were generated by the softmax function such that the sum of the elements at the same position in the weight matrix is 1. Then, the weight matrix was used to weight the feature maps P\u0302, P\u0304, and P\u0303, and an adaptively selected spatial feature V was obtained by adding these weighted feature maps. This process can be expressed by Formulas (2) and (3):\nWa, Wb, Wc = Fso f tmax(z) (2)\nV = Wa \u00b7 U\u0302 + Wb \u00b7 U\u0304 + Wc \u00b7 U\u0303 (3)\n2.3.2. Branch of Central Pixel Spectral Feature Extraction\nLimited by the method used to obtain sample points, the number of samples used in crop classification is often insufficient, which forces the network to resist overfitting and generalization. However, even if patches are used as the model input, the central pixels need to be classified. Moreover, when introducing spatial information, the central pixel and other pixels in the patch are weighted equally in the classification, which may lead to classification errors. Therefore, increasing the weight of the central pixel features during the classification process is necessary. As shown in Figure 7b, we used a separate branch to extract the spectral information for the 12 bands of the central pixel. TabNet [44], characterized by its use of a convolutional network structure to simulate the RF operation process, was implemented for feature extraction. Compared with deep learning methods, traditional machine learning methods such as RF are more dependable when training small samples and have a better anti-overfitting ability [49]. Therefore, we used TabNet to simulate an RF for spectral feature extraction. Multitask learning involves designing multiple related tasks of a neural network using prior knowledge and then accelerating the training and convergence of the network by optimizing multiple tasks simultaneously. Multitask learning obtains more comprehensive and controllable information from data, which can improve the ability of the network to extract certain features [50]. Therefore, to improve the extraction of spectral features and increase the weight of the central pixel in the classification process, we use multitask learning alongside an auxiliary loss function to supervise the spectral information extraction process of the central pixel. Finally, the extracted spectral features of the central pixel and the spatial features obtained by SPM were multiplied to make full use of the spectral information of the central pixel, thereby improving classification accuracy.\n2.3.3. Loss Function\nThe loss function comprises two parts of the network. In this study, cross entropy loss (CE loss) [51] was used as the loss function of the network. CE loss is the most used loss function for multiple classification tasks because its value is only related to the probability of the correct class and its derivation process is convenient. The formula for CE loss is:\nLossCE = \u2212 1 N\nN\n\u2211 i=1 (yilogy\u0302i + (1\u2212 yi)(1\u2212 logy\u0302i)) (4)\nIn Formula (4), yi is the probability that the ground truth is true, y\u0302i is the probability that the forward propagation result is true, and N is the number of classes. The total loss function of the network consists of two branch loss functions, which can be expressed as:\nLoss = \u03bb1 \u00d7 Loss1 + \u03bb2 \u00d7 Loss2 (5)\nIn Formula (5), \u03bb1 + \u03bb2 = 1, Loss1, and Loss2 are both CE losses. The weight of the spectral information of the central pixels during the classification process can be adjusted by adjusting the values of \u03bb1 and \u03bb2.\n2.3.4. Superpixel Optimization\nDue to the low spatial resolution of Sentinel-2 images, mixed pixel phenomena are frequently observed, especially in densely distributed crops with a high variation in crop type over short distances. For example, the characteristics of pixels at the junction of corn and peanut are different from those of peanut and corn, which brings great difficulties in crop classification. Superpixel segmentation is an over-segmentation technique that obtains image objects with accurate boundaries by clustering the features of image pixels. Simple linear iterative clustering (SLIC) [52] is a classical superpixel segmentation method that converts the colors of an image from the RGB color space to the CIELab color space and clusters according to the distance of pixels in the color space to obtain an accurate boundary of image objects. To improve the boundaries of the crop plots and reduce the influence of mixed pixels, we used SLIC to optimize our classification results. The SLIC algorithm converts RGB images into CIELab color space and clusters pixels based on the distance between pixels in the color space. In general, the greater the color difference between pixels, the more accurate the crop plots edges that can be extracted by SLIC. Sentinel-2 L2A products have 12 bands with multiple band combinations. To select the most suitable band combination as the input of the SLIC algorithm, we first counted the reflectance of the 12 bands in Sentinel-2 images of our sample. The results are shown in the box diagram [53] in Figure 8, where the reflectance of the three crops is mainly different in the four red edge bands of Band 5, Band 6, Band 7, and Band 8A and in the near-infrared band of Band 8. In some other bands such as B4, B5, and B11, the reflectance of the three crops is also partially different and to reduce the impact of mixed pixels, we prefer to use the highest resolution bands, namely, Band 2, Band 3, Band 4, and Band 8, to carry out band combination. Using our samples, we combined the above factors to calculate the average distance of different crops in the CIELab space under different band combinations. The band combinations used include color syntheses (Band 4, Band 3, Band 2), standard false color synthesis (Band 8, Band 6, Band 4), and commonly used Sentinel-2 agricultural bands (Band 8, Band 11, Band 2) [54]. We also performed a principal component analysis (PCA) [55] to map the 12 bands of the Sentinel-2 image to three principal component bands to calculate the distance of different crops in the CIELab space. Finally, the experimental results are shown in Table 3. In the experimental results, the average distances between peanut and corn and between peanut and rice in the color space of the band combinations of Band 8, Band 11, and Band 4 were the largest at 26.5757 and 20.7087, respectively. These results have great advantages over the second-largest distance. Under this band combination, although the average distance between corn and rice in the CIELab color space is not the largest, the result of 6.9893 has no obvious disadvantage compared with the maximum of 7.8800. Therefore, when using the SLIC algorithm for post-processing, we used an RGB image composed of Band 8, Band 11, and Band 4 as input.\nAs shown in Figure 9, we used the RGB false color image formed by by Band 8, Band 11, and Band 4 of the Sentinel-2 image after performing image enhancement operations, such as image stretching, as input to the SLIC to obtain superpixel segmentation blocks of the image. Then, each superpixel segmentation block was traversed to calculate the area proportion of each class of pixels. If the proportion of a certain class of pixels exceeds threshold \u03b8, all pixels in the superpixel block were modified to this category. Otherwise, the category of each pixel remained unchanged.\n2.3.5. Evaluation Metrics\nTo evaluate the classification results, we used a confusion matrix. The confusion matrix is a standard format for evaluating crop classification accuracy. In the confusion matrix, the number of rows, N, represents the number of categories to be evaluated. The elements Pi,j of i rows and j columns represent the number of pixels that are actually class i but are predicted to be class j. Through the confusion matrix, we mainly used four types of accuracy evaluation metrics. First, we used producer accuracy (PA) metrics,\nwhich is the proportion of the number of pixels correctly classified into the class to the total number of pixels in the class. Second, we used user accuracy (UA) metrics, which refers to the proportion of pixels correctly classified into the class and the total number of pixels classified into the class. Using PA and UA, we can analyze the classification ability of each type of crop and explore the reasons for the change in accuracy. The overall accuracy (OA) refers to the proportion of all correctly classified pixels in the total number of pixels. Finally, the kappa coefficient (KC) is an indicator of consistency and can also be used to measure the effect of classification. The kappa coefficient is shown to be a more discerning statistical tool for assessing the classification accuracy of different classifiers and has the added advantage of being statistically testable against the standard normal distribution [56]. In the classification problem, consistency refers to whether the model prediction results are consistent with the actual classification results.At the same time, we We also used the F1 score as an evaluation metric because the classification process focuses on more than just the recall or accuracy rates. The F1 score, which considers the accuracy and recall rate, is a commonly used evaluation index to evaluate classification accuracy better. The F1 score can be regarded as the harmonic mean of the model\u2019s accuracy and recall. The relevant formulas are as follows:\nPrecision = TP\nTP + FP (6)\nRecall = TP\nTP + FN (7)\nF1 = 2\u00d7 Precision\u00d7 Recall Precision + Recall\n(8)\nIn Formulas (6)\u2013(8), true positive (TP) represents pixels correctly classified as positive pixels. False positives (FP) are pixels incorrectly classified as a positive pixel. False negatives (FN) are pixels incorrectly classified as negative."
        },
        {
            "heading": "2.4. Train Details",
            "text": "We used the PyTorch deep learning framework to build all deep learning models used in this experiment. All training and validation samples were used in the training phase. We trained our network using one NVIDIA TITAN XP GPU (12 GB of memory). In terms of the the training parameters, the batch size was set to 256, Adam [57] was used as the optimizer, the initial learning rate was set to 0.001, and cosine annealing [58] was used to reduce the learning rate gradually. In addition, a drop rate of 0.4 was added. For the hyperparameter setting of the loss function, we set the value of \u03bb1 to 0.7 and \u03bb2 to 0.3. In the post-processing process to balance superpixel segmentation and crop extraction, we set the hyperparameter \u03b8 in the superpixel optimization to 0.6."
        },
        {
            "heading": "3. Results",
            "text": ""
        },
        {
            "heading": "3.1. Comparing Methods",
            "text": "We conducted quantitative and qualitative comparisons of the Henan crop dataset obtained using other mainstream crop classification methods, such as RF [12], XGBoost [18], CNN [38], CNN-RF [40], and S3ANet [59]. RF and XGBoost are commonly used machine learning classification methods in crop classification mapping. However, they only use the spectral information of pixels for classification. In our experiments, the input of these two methods was a 1 \u00d7 12 vector of 12 band values for a single pixel. The CNN is the most basic deep learning classification network. The combined CNN-RF first extracts feature through a CNN and then uses RF for classification, which can improve the model\u2019s generalization ability by leveraging the advantages of the two methods. For fairness of comparison, the CNN network depths of these two methods are the same as those in our method. S3ANet uses various attention methods to weight spatial, scale, and spectral information to improve crop classification accuracy. All deep learning methods in the\nexperiment used a 7 \u00d7 7 \u00d7 12 vector as the input, which is a patch with a central pixel size of 7. The other parameter settings of all deep learning methods are the same as those described in Section 2.4. Machine learning methods were constructed in the Scikit-learn python library.\n3.1.1. Quantitative Comparisons\nOur comparison results are shown in Tables 4 and 5. As shown in Table 4, SPTNet has achieved competitive overall and single-crop accuracy results. Our method also has advantages in terms of the parameters and inference time. Regarding the non-deep learning algorithms, the performance of RF alone was insufficient, with an F1 score of 0.8501. In contrast, XGBoost obtained a slightly higher F1 score of 0.8741, which is close to the accuracy of the deep learning method CNN-RF. Additionally, using the deep learning method, CNN achieves an F1 score of 0.8217, which is the lowest accuracy among all of the methods and can be attributed to the many false detections of the background. It may also be due to the lack of attention to spectral information in the CNN, so some minor crop plots or backgrounds may have been missed. The accuracy of CNN-RF is significantly improved compared with that of CNN, which may be because the overfitting of CNN is reduced after classification using RF. However, it also leads to a significant increase in the number of parameters and the inference time. S3ANet, which uses a variety of attentions for information extraction, has achieved sub-optimal accuracy in multiple metrics. Our method provides the highest classification accuracy, with an F1 score and KC of 0.9653 and 0.9531, respectively.\n3.1.2. Visualization Results\nA comparison of the visualization results is shown in Figure 10. Specifically, many small objects are in the first and second rows of Figure 10. The difference is that the first row is a typical mixed planting area of corn and peanut, where the crop plots are small and broken. In the second row, although the distribution of crops was more concentrated, many roads shuttled through the field. The features of these roads are not as obvious as those of large-area crops. RF and XGBoost use only the spectral information of pixels and cannot accurately distinguish between different crops, leading to confusion around the classification of peanut and corn in their results. Other deep learning methods directly use 7 \u00d7 7 pixels patches, which are too large compared to the crop plots in these scenarios. For this reason, they have poor extraction effects on small crop plots and other small surface features such as roads and greenhouses, resulting in categorical confusion when classifying crop plots and background erosion. Our method uses SPM to adaptively fuse the features of different patch sizes for specific scenarios, which can improve the extraction ability of small surface features. Therefore, our model could accurately classify roads in fields and small crop plots. There were small and alternately planted corn and peanut where the first row was marked. Our method was able to accurately distinguish between them accurately, whereas the extraction results of other methods have obvious misclassifications. In the area marked in the second row, small roads were misclassified by most other methods, but our method accurately distinguished these roads from crops because of the better extraction of small features. Moreover, after using SLIC for post-processing, the crop plots are more regular. Other methods could not distinguish the mixed pixels, which is manifested in the images where the connected parts of different crops are classified as the background, and the extraction results of some main roads are too wide. These conditions were reduced in our results because of the use of the SLIC. In the lower-left marker of the third row, there is a river in the lower left part of the image, which is covered by aquatic plants such as cyanobacterial blooms. The image characteristics of these cyanobacterial blooms are similar to rice. Since this scenario is not common, such negative samples do not exist in our dataset. Without such negative samples, most methods misclassify them as rice. Our method is more sensitive to the spectral information of pixels because it uses TabNet and multitask learning to model and supervise the spectral information of pixels separately. The unique structure of TabNet can enhance the generalization ability of the network. Therefore, our method had the lowest number of false classifications in this scenario. In the last row, rice is scattered owing to the influence of terrain. Rice is difficult to classify because of the presence of similar grasslands and woodlands on the hills. However, compared with other methods, our method distinguished rice better from the surrounding grassland or woodland with the lowest number of false detections, and it extracted slender paddy fields between hills. In summary, SPTNet can effectively reduce the number of missed or incorrect detections in the crop extraction results, whether in the plain where crops are mixed or in small hilly lands. Overall, RF and XGBoost based on single-pixel results exhibited obvious salt and pepper noise and could not distinguish different crop categories well, indicating that deep learning methods are not necessarily superior to traditional methods. Many details of the image appear to be embodied in small crop plots, and some road classification errors are observed because CNN uses convolution operations and lacks restrictions. The other three deep learning methods performed better than the first three methods due to crop classification improvements."
        },
        {
            "heading": "3.2. Ablation Study",
            "text": "Ablation experiments were conducted on the Henan crop dataset to verify the contribution of the proposed module to crop classification. We used a CNN network with the same depth as that in our model as the baseline. We then changed or added parts of the structure to our proposed modules. We divided them into the following experiments according to the different modules added: Experiment 1: Test baseline. Experiment 2: Replace the first\nfour layers of the Baseline with SPM. Experiment 3: Add a TabNet branch to extract the spectral features of the central pixels. Experiment 4: Use a multitask learning strategy to supervise the TabNet branch. Experiment 5: Add the SLIC algorithm for post-processing.\n3.2.1. Quantitative Comparisons\nThe ablation results are summarized in Tables 6 and 7. The CNN\u2019s lack of attention to spectral information and small crop plots resulted in many false positives and a fairly low F1 score of 0.8217. After adding the SPM module, the experimental accuracy is significantly improved to 0.8989 because SPM can adaptively fuse the features of patches of different sizes for specific scenes. Therefore, a better extraction ability for small objects was obtained, and the extraction accuracy of small crop plots or roads was higher. From the quantitative results, most PA and UA values of categories increased after applying SPM. Only the peanut\u2019s PA and the background\u2019s UA decreased slightly (0.9667 \u2212 0.9552, 0.9484 \u2212 0.9115). This result shows that using the SPM module can effectively reduce errors and missed detection in the crop classification process. After adding the TabNet branch to extract the spectral features of the central pixel, we obtained an F1 score of 0.9456. With the addition of the TabNet network, the PA of crops decreased while the UA of crops increased compared with the results of the baseline CNN and the results after adding the SPM. The network is more sensitive to the spectral information of the center point, and the UA of various crops increases, thereby enhancing the direct extraction of spectral information. Despite this, more constraints are consequently added to the crop classification process, resulting in a decrease in the PA of various crops. Furthermore, we used multitask learning to supervise the TabNet, which was implemented to optimize the central pixel spectral feature extraction process in Experiment 3. After optimizing the multitask learning strategy, the UA and PA of various crops could be balanced to improve the classification accuracy of the network. The classification accuracy of Experiment 3 reached an F1 score of 0.9523. In the last experiment, we added the SLIC algorithm for post-processing. The results in Tables 6 and 7 show that the superpixel segmentation optimization strategy improves the UA and PA of various crops. The final network accuracy has an F1 score of 0.9653 because of superpixel segmentation\u2019s more accurate boundary information.\n3.2.2. Visualization Results\nTo qualitatively compare the contribution of the proposed modules, the visualization results of some ablation experiments are shown in Figure 11, which shows that the extraction results using all proposed modules are optimal and the proposed modules achieve the anticipated visualization results. The first row of Figure 11 shows waterweeds that are easily confused with rice since many tiny roads and small crop plots travel between fields. The baseline CNN incorrectly classified some waterweeds as rice because the small crop plots could not be accurately distinguished and the roads were not successfully extracted. The SPM structure can improve the model\u2019s ability to extract small objects; therefore, some roads and other crops covered by large areas of crops can be classified. However, the model still made false detections, and more waterweeds were classified as rice, indicating that using only the patch\u2019s spatial information is insufficient to accurately classify crops. After adding TabNet to model the spectral information of the central pixels and applying multitask learning for supervision, false detection was greatly reduced. In the results, water plants were correctly classified as the background, and more roads were extracted. Finally, SLIC was used for post-processing because the superpixel segmentation algorithm can obtain more accurate edges. In the final extraction results, the shape of the crop plots was more regular, and the road results were more accurate. The difference in the classification of roads in each experiment is more obvious in the second row wherein the crops are densely distributed. There are several obvious roads in the upper right part of the image, but they are spread over large areas of peanut. In this case, the main class of pixels in the road patch is peanut in this case. Hence, the baseline CNN completely ignored them and misclassified them as peanut. After adding SPM, however, the network\u2019s ability to extract small objects was improved. Despite this, owing to the mixed pixels in the image, the road in the peanut image shows different characteristics from those of the pure road pixels. Although the network could distinguish them after SPM was added, they were still misclassified as corn. After using TabNet to extract the spectral features of the central pixels separately, the classification ability of the ground objects was improved, and some roads were extracted. The weight of the central pixels\u2019 spectral information increased after using multitask\nlearning for supervision. Although more roads were extracted, some were still classified as corn. However, SLIC could extract the road into a superpixel block. Therefore, after SLIC post-processing, the more complete roads are distinguished. In the last row, rice is scattered owing to the influence of terrain. With the gradual addition of the proposed module, the surrounding grassland or woodland misclassified as rice was obviously reduced. Finally, our method accurately extracted the rice between hills, and the edges of the crop plots were more accurate."
        },
        {
            "heading": "3.3. Crop Mapping in Henan Province",
            "text": "We used the proposed network structure to complete the mapping of 10 m plots of principal crops, including corn, peanut, and rice, in Henan Province, China, in 2022, and the results are shown in Figure 12."
        },
        {
            "heading": "4. Discussion",
            "text": ""
        },
        {
            "heading": "4.1. The Split Strategy Selection of SPM",
            "text": "In the split stage of the SK-Conv structure, the feature maps were processed using convolution kernels of different sizes to obtain the features of different receptive fields. However, our method differs from that of SK-Conv. Using our method, in the split stage, patches of different sizes were used for splitting rather than the sizes of the convolution kernels. Using the size of the convolution kernel in the split stage can obtain multi-scale receptive fields, whereas using patches can balance spatial information and noise. We conducted experiments to verify the advantages and disadvantages of these two methods. To ensure the fairness of the comparison, other structures and parameter settings of the network were consistently used across methods. The results are presented in Table 8. The results show that the F1 score using convolutional kernel sizes in the split stage is 0.9480, which is less than that using patch sizes in the split stage. The scale of the input vector is too small to provide a multi-scale receptive field because the input of the model is a patch of 7 \u00d7 7 pixels. This eliminates the advantage of using convolutional kernels for splitting. Furthermore, using patches of the same size as the input in each branch of SPM introduces more noise, which may also be responsible for the reduced accuracy. The method of using patch sizes in the split stage introduces prior information, which can better adapt to different sizes of crop plots in complex scenes. Therefore, we used different patch sizes for the split stage in the structure."
        },
        {
            "heading": "4.2. The Contribution of Different Sizes of Patches to Crop Extraction",
            "text": "In this section, we describe the experiments conducted to explore the contribution of different patch sizes to crop extraction. Specifically, in Figure 7a, we removed the structure of one patch and retained only the other two patches. To ensure fairness in the experiment, the other parameter settings remained consistent across methods. The qualitative results of the experiments are presented in Table 9. Regardless of which patch is removed, the classification accuracy decreases. It shows that these three different patch sizes contribute to improving classification accuracy. In the case of removing the 3 \u00d7 3 pixel patches, the classification accuracy decreased the most, whereas the classification accuracy decreased the least when removing the 5 \u00d7 5 pixel patches. This shows that the 3 \u00d7 3 pixel patches have the smallest contribution. The 5 \u00d7 5 pixel patches have the largest contribution in the classification process, possibly because the overall plot size in Henan was more proportional to 5 \u00d7 5 pixel patches. Of note, the classification accuracy of rice decreased significantly in the case of removing 3 \u00d7 3 pixel patches, which may be because rice is mostly planted in smaller hilly areas."
        },
        {
            "heading": "4.3. Advantages and Limitations",
            "text": "This study proposes a dependable network for large-scale crop extraction and achieves satisfactory classification accuracy using only single-temporal Sentinel-2 images. First, the proposed structure can adaptively select and fuse the spatial information of the image according to the size of the plot in the actual scene, thereby improving its ability to extract small features. Second, the proposed structure uses TabNet and a multitask learning strategy to extract the spectral features and improve the stability of the network while modeling the spectral information of the central pixels separately. TabNet, which has a similar strcuture to a decision tree, can enhance the network\u2019s generalization ability and alleviate the overfitting problem caused by small sample sizes. Using multitask learning strategies can enhance the weight of the spectral features of pixels and further improve classification accuracy. Finally, the application of an SLIC algorithm to post-process the extraction results could improve the accuracy of the classification result boundary. The SPTNet obtained the highest extraction accuracy than other methods, regardless of the crop. In the qualitative comparison, the SPTNet greatly improved the extraction ability of small crop plots, and the edge of crops has also been ameliorated. These comparisons prove the advantages of our method. Although our method has obvious advantages for Henan crop extraction, there are still some problems. To avoid the problem of missing time-series images, we used singletemporal images as data sources, for which the extraction accuracy of staple crops, such as corn, peanut, and rice, is generally high. However, the classification of certain characteristic economic crops in the absence of crop phenology information remains a limiting factor. The spectral characteristics of some economic crops, such as soybeans and grapeseed, are relatively similar. Thus, using only single-temporal images to distinguish them is a challenge. Furthermore, it is difficult to overcome image resolution limitations, evidenced by the fact that the classification accuracy did not significantly improve after applying the SLIC algorithm, a superpixel segmentation method based on pixel spectral features. In future research, we will attempt to use time-series remote sensing images for crop classification. To ensure the integrity of the data and make full use of the high-precision, single-temporal extraction method proposed in this paper, we will attempt to extract the key time points of crop phenology and classify crops under the premise of using the shortest time series data. We will also attempt to combine Sentinel-2 images with high-resolution data for crop extraction. Sentinel-2 images with high spectral resolution were used for crop extraction, and high-resolution images with high spatial resolution were used for plot boundary extraction. We hope to optimize the boundaries of the crop extraction results through high-spatial-resolution plot extraction results, which can improve the overall accuracy of crop classification. Finally, we will try to add SAR data for crop classification. SAR satellites can image in any weather condition, which can make up for the lack of optical data. Furthermore, it is difficult to overcome image resolution limitations, evidenced by the fact that the classification accuracy did not significantly improve after applying the SLIC algorithm, a superpixel segmentation method based on pixel spectral features."
        },
        {
            "heading": "5. Conclusions",
            "text": "Large-scale crop extraction is essential for grain security and sustainable agriculture. This paper proposes a crop classification method using single-temporal sentinel images to better assess large-scale and high-precision crop extraction, which has been successfully applied to three main autumn crops in Henan Province: corn, peanut, and rice. To improve the extraction ability of small crop plots in complex scenes and mitigate the negative impact of insufficient samples, this paper proposes SPM, which can adaptively fuse the features of different patch sizes. In addition, to improving the classification ability of the central pixels, we used the TabNet network to extract spectral features and improve the stability of the network. Furthermore, we use multitask learning to introduce auxiliary loss and increase the weight of the central pixels\u2019 spectral information in the classification process. Finally, we introduce the SLIC superpixel segmentation method for post-processing to\nreduce the impact of mixed pixels and ameliorate the boundaries of crop plots. Qualitative and quantitative analysis shows that compared with different mainstream classification methods, SPTNet achieves state-of-the-art performance with F1 scores of 96.53% on our Henan crop classification dataset, which indicates that our proposed structure effectively improves the large-scale crop classification process. Large-scale crop classification mapping of corn, peanut, and rice in Henan Province also proves the effectiveness of our method in practical application. In future work, we will attempt to use high-resolution and Sentinel-2 images for crop classification and combine the advantages of these two images to obtain finer crop extraction results. We will also attempt to incorporate phenological information at a minimum cost to enhance the extraction of more crop varieties.\nAuthor Contributions: Conceptualization, J.H. and Z.C.; methodology, J.H. and Y.B.; validation, X.Y. and Y.B.; formal analysis, G.L.; resources, Z.C.; data curation, Y.B.; writing\u2014original draft preparation, G.L. and X.T.; writing\u2014review and editing, X.T.; visualization, X.T.; supervision, X.Y.; project administration, Z.C.; funding acquisition, Z.C. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was supported by the National Natural Science Foundation of China (Grant No. 42071407, 42071321).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not publicly available.\nAcknowledgments: The authors are grateful to the editors and anonymous reviewers for their informative suggestions.\nConflicts of Interest: The authors declare no conflicts of interest.\nAbbreviations The following abbreviations are used in this manuscript:\nCE cross entropy CNN convolutional neural network DNN deep neural network EVI enhanced vegetation index FN false negative FP false positive GRU gated recurrent unit KC kappa coefficient LSTM long short-term memory LSWI land surface water index NDVI normalized difference vegetation index PA producer accuracy PCA principal component analysis RF random forest RNN recurrent neural network SLIC simple linear iterative clustering SPM selective patch module TP true positive UA user accuracy"
        }
    ],
    "title": "An Adaptive Feature Fusion Network with Superpixel Optimization for Crop Classification Using Sentinel-2 Imagery",
    "year": 2023
}