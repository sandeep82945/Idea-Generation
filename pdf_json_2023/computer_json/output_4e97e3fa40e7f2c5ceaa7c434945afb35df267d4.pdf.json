{
    "abstractText": "Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks. To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (BIOT). The proposed BIOT model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-length segments containing local signal features, flattening them to form consistent \"sentences\". Channel embeddings and relative position embeddings are added to preserve spatio-temporal features. The BIOT model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that BIOT outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Use CHB-MIT seizure detection task as an example, our vanilla BIOT model shows 3% improvement over baselines in balanced accuracy, and the pre-trained BIOT models (optimized from other data sources) can further bring up to 4% improvements.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chaoqi Yang"
        },
        {
            "affiliations": [],
            "name": "M. Brandon Westover"
        },
        {
            "affiliations": [],
            "name": "Jimeng Sun"
        }
    ],
    "id": "SP:a128d095f486f31994b907ae440fe10eb086d4ae",
    "references": [
        {
            "authors": [
                "E.A.P. Alday",
                "A. Gu",
                "A.J. Shah",
                "C. Robichaux",
                "Wong",
                "A.-K. I",
                "C. Liu",
                "F. Liu",
                "A.B. Rad",
                "A. Elola",
                "S Seyedi"
            ],
            "title": "Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020",
            "venue": "Physiological measurement,",
            "year": 2020
        },
        {
            "authors": [
                "H. Almutairi",
                "G.M. Hassan",
                "A. Datta"
            ],
            "title": "Detection of obstructive sleep apnoea by ecg signals using deep learning architectures",
            "venue": "2020 28th European signal processing conference (EUSIPCO), pages 1382\u20131386. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "D. Anguita",
                "A. Ghio",
                "L. Oneto",
                "X. Parra",
                "Reyes-Ortiz",
                "J. L"
            ],
            "title": "A public domain dataset for human activity recognition using smartphones",
            "venue": "In Esann,",
            "year": 2013
        },
        {
            "authors": [
                "J.L. Ba",
                "J.R. Kiros",
                "G.E. Hinton"
            ],
            "title": "Layer normalization",
            "venue": "arXiv preprint arXiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "N. Bahador",
                "J. Jokelainen",
                "S. Mustola",
                "J. Kortelainen"
            ],
            "title": "Reconstruction of missing channel in electroencephalogram using spatiotemporal correlation-based averaging",
            "venue": "Journal of Neural Engineering, 18(5):056045.",
            "year": 2021
        },
        {
            "authors": [
                "S. Biswal",
                "H. Sun",
                "B. Goparaju",
                "M.B. Westover",
                "J. Sun",
                "M.T. Bianchi"
            ],
            "title": "Expert-level sleep scoring with deep neural networks",
            "venue": "Journal of the American Medical Informatics Association, 25(12):1643\u20131650.",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u2013 1607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Clevert",
                "D.-A.",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "arXiv preprint arXiv:1511.07289.",
            "year": 2015
        },
        {
            "authors": [
                "H. Cui",
                "A. Liu",
                "X. Zhang",
                "X. Chen",
                "K. Wang",
                "X. Chen"
            ],
            "title": "Eeg-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
            "venue": "Knowledge-Based Systems, 205:106243.",
            "year": 2020
        },
        {
            "authors": [
                "M.N. Dar",
                "M.U. Akram",
                "S.G. Khawaja",
                "A.N. Pujari"
            ],
            "title": "Cnn and lstm-based emotion charting using physiological signals",
            "venue": "Sensors, 20(16):4551.",
            "year": 2020
        },
        {
            "authors": [
                "J. Devlin",
                "Chang",
                "M.-W.",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2020
        },
        {
            "authors": [
                "Y. Du",
                "Y. Xu",
                "X. Wang",
                "L. Liu",
                "P. Ma"
            ],
            "title": "Eeg temporal\u2013spatial transformer for person identification",
            "venue": "Scientific Reports, 12(1):14378.",
            "year": 2022
        },
        {
            "authors": [
                "W. Ge",
                "J. Jing",
                "S. An",
                "A. Herlopian",
                "M. Ng",
                "A.F. Struck",
                "B. Appavu",
                "E.L. Johnson",
                "G. Osman",
                "Haider",
                "H. A"
            ],
            "title": "Deep active learning for interictal ictal injury continuum eeg patterns",
            "venue": "Journal of neuroscience methods,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Gong",
                "Chung",
                "Y.-A.",
                "J. Glass"
            ],
            "title": "Ast: Audio spectrogram transformer",
            "venue": "arXiv preprint arXiv:2104.01778.",
            "year": 2021
        },
        {
            "authors": [
                "A. Greco",
                "G. Valenza",
                "J. L\u00e1zaro",
                "J.M. Garz\u00f3n-Rey",
                "J. Aguil\u00f3",
                "C. De-la Camara",
                "R. Bail\u00f3n",
                "E.P. Scilingo"
            ],
            "title": "Acute stress state classification based on electrodermal activity modeling",
            "venue": "IEEE Transactions on Affective Computing.",
            "year": 2021
        },
        {
            "authors": [
                "Grill",
                "J.-B",
                "F. Strub",
                "F. Altch\u00e9",
                "C. Tallec",
                "P. Richemond",
                "E. Buchatskaya",
                "C. Doersch",
                "B. Avila Pires",
                "Z. Guo",
                "M Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "A. Harati",
                "M. Golmohammadi",
                "S. Lopez",
                "I. Obeid",
                "J. Picone"
            ],
            "title": "Improved eeg event classification using differential energy",
            "venue": "In 2015 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "A. Isin",
                "S. Ozdalili"
            ],
            "title": "Cardiac arrhythmia detection using deep learning",
            "venue": "Procedia computer science, 120:268\u2013275.",
            "year": 2017
        },
        {
            "authors": [
                "J. Jing",
                "E. d\u2019Angremont",
                "S. Zafar",
                "E.S. Rosenthal",
                "M. Tabaeizadeh",
                "S. Ebrahim",
                "J. Dauwels",
                "M.B. Westover"
            ],
            "title": "Rapid annotation of seizures and interictal-ictal continuum eeg patterns",
            "venue": "40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),",
            "year": 2018
        },
        {
            "authors": [
                "J. Jing",
                "W. Ge",
                "S. Hong",
                "M.B. Fernandes",
                "Z. Lin",
                "C. Yang",
                "S. An",
                "A.F. Struck",
                "A. Herlopian",
                "I Karakis"
            ],
            "title": "Development of expert-level classification of seizures and rhythmic and periodic patterns during eeg interpretation. Neurology",
            "year": 2023
        },
        {
            "authors": [
                "J. Jing",
                "H. Sun",
                "J.A. Kim",
                "A. Herlopian",
                "I. Karakis",
                "M. Ng",
                "J.J. Halford",
                "D. Maus",
                "F. Chan",
                "M Dolatshahi"
            ],
            "title": "Development of expert-level automated detection of epileptiform discharges during electroencephalogram interpretation",
            "venue": "JAMA neurology,",
            "year": 2020
        },
        {
            "authors": [
                "A. Katharopoulos",
                "A. Vyas",
                "N. Pappas",
                "F. Fleuret"
            ],
            "title": "Transformers are rnns: Fast autoregressive transformers with linear attention",
            "venue": "Proceedings of the International Conference on Machine Learning (ICML).",
            "year": 2020
        },
        {
            "authors": [
                "Kim",
                "M.-G.",
                "H. Ko",
                "S.B. Pan"
            ],
            "title": "A study on user recognition using 2d ecg based on ensemble of deep convolutional neural networks",
            "venue": "Journal of Ambient Intelligence and Humanized Computing, 11:1859\u20131867.",
            "year": 2020
        },
        {
            "authors": [
                "G.H. Klem",
                "H. L\u00fcders",
                "H.H. Jasper",
                "C.E. Elger"
            ],
            "title": "The ten-twenty electrode system of the international federation",
            "venue": "the international federation of clinical neurophysiology. Electroencephalography and clinical neurophysiology. Supplement, 52:3\u20136.",
            "year": 1999
        },
        {
            "authors": [
                "D. Kostas",
                "S. Aroca-Ouellette",
                "F. Rudzicz"
            ],
            "title": "Bendr: using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data",
            "venue": "Frontiers in Human Neuroscience, 15:653659.",
            "year": 2021
        },
        {
            "authors": [
                "V.J. Lawhern",
                "A.J. Solon",
                "N.R. Waytowich",
                "S.M. Gordon",
                "C.P. Hung",
                "B.J. Lance"
            ],
            "title": "Eegnet: a compact convolutional neural network for eeg-based brain\u2013computer interfaces",
            "venue": "Journal of neural engineering, 15(5):056013.",
            "year": 2018
        },
        {
            "authors": [
                "H. Li",
                "M. Ding",
                "R. Zhang",
                "C. Xiu"
            ],
            "title": "Motor imagery eeg classification algorithm based on cnn-lstm feature fusion network",
            "venue": "Biomedical signal processing and control, 72:103342.",
            "year": 2022
        },
        {
            "authors": [
                "Lin",
                "T.-Y.",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "J. Liu",
                "L. Zhang",
                "H. Wu",
                "H. Zhao"
            ],
            "title": "Transformers for eeg emotion recognition",
            "venue": "arXiv preprint arXiv:2110.06553.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "S. Lopez",
                "G. Suarez",
                "D. Jungreis",
                "I. Obeid",
                "J. Picone"
            ],
            "title": "Automated identification of abnormal adult eegs",
            "venue": "2015 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), pages 1\u20135. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "P. Nagabushanam",
                "S.T. George",
                "P. Davu",
                "P. Bincy",
                "M. Naidu",
                "S. Radha"
            ],
            "title": "Artifact removal using elliptic filter and classification using 1d-cnn for eeg signals",
            "venue": "In 2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS),",
            "year": 2020
        },
        {
            "authors": [
                "H. Nyquist"
            ],
            "title": "Certain topics in telegraph transmission theory",
            "venue": "Transactions of the American Institute of Electrical Engineers, 47(2):617\u2013644.",
            "year": 1928
        },
        {
            "authors": [
                "S. Parvaneh",
                "J. Rubin",
                "S. Babaeizadeh",
                "M. Xu-Wilson"
            ],
            "title": "Cardiac arrhythmia detection using deep learning: A review",
            "venue": "Journal of electrocardiology, 57:S70\u2013S74.",
            "year": 2019
        },
        {
            "authors": [
                "W.Y. Peh",
                "Y. Yao",
                "J. Dauwels"
            ],
            "title": "Transformer convolutional neural networks for automated artifact detection in scalp eeg",
            "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pages 3599\u20133602. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "H. Phan",
                "K. Mikkelsen"
            ],
            "title": "Automatic sleep staging of eeg signals: recent development, challenges, and future directions",
            "venue": "Physiological Measurement.",
            "year": 2022
        },
        {
            "authors": [
                "S.F. Quan",
                "B.V. Howard",
                "C. Iber",
                "J.P. Kiley",
                "F.J. Nieto",
                "G.T. O\u2019Connor",
                "D.M. Rapoport",
                "S. Redline",
                "J. Robbins",
                "Samet",
                "J. M"
            ],
            "title": "The sleep heart health study: design, rationale, and methods. Sleep, 20(12):1077\u20131085",
            "year": 1997
        },
        {
            "authors": [
                "S. Sakhavi",
                "C. Guan",
                "S. Yan"
            ],
            "title": "Learning temporal information for brain-computer interface using convolutional neural networks",
            "venue": "IEEE transactions on neural networks and learning systems, 29(11):5619\u20135629.",
            "year": 2018
        },
        {
            "authors": [
                "R.T. Schirrmeister",
                "J.T. Springenberg",
                "L.D.J. Fiederer",
                "M. Glasstetter",
                "K. Eggensperger",
                "M. Tangermann",
                "F. Hutter",
                "W. Burgard",
                "T. Ball"
            ],
            "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
            "venue": "Human brain mapping, 38(11):5391\u20135420.",
            "year": 2017
        },
        {
            "authors": [
                "C.E. Shannon"
            ],
            "title": "Communication in the presence of noise",
            "venue": "Proceedings of the IRE, 37(1):10\u201321.",
            "year": 1949
        },
        {
            "authors": [
                "A.H. Shoeb"
            ],
            "title": "Application of machine learning to epileptic seizure onset detection and treatment",
            "venue": "PhD thesis, Massachusetts Institute of Technology.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Song",
                "X. Jia",
                "L. Yang",
                "L. Xie"
            ],
            "title": "Transformer-based spatial-temporal feature learning for eeg decoding",
            "venue": "arXiv preprint arXiv:2106.11170.",
            "year": 2021
        },
        {
            "authors": [
                "N. Srivastava",
                "G. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "The journal of machine learning research, 15(1):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "N.S. Suhaimi",
                "J. Mountstephens",
                "J Teo"
            ],
            "title": "Eeg-based emotion recognition: A state-ofthe-art review of current trends and opportunities",
            "venue": "Computational intelligence and neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "K. Venkatachalam",
                "A. Devipriya",
                "J. Maniraj",
                "M. Sivaram",
                "A. Ambikapathy",
                "S.A. Iraj"
            ],
            "title": "A novel method of motor imagery classification using eeg signal",
            "venue": "Artificial intelligence in medicine, 103:101787.",
            "year": 2020
        },
        {
            "authors": [
                "P. Wagner",
                "N. Strodthoff",
                "Bousseljot",
                "R.-D.",
                "D. Kreiseler",
                "F.I. Lunze",
                "W. Samek",
                "T. Schaeffter"
            ],
            "title": "Ptb-xl, a large publicly available electrocardiography dataset",
            "venue": "Scientific data, 7(1):154.",
            "year": 2020
        },
        {
            "authors": [
                "S. Wang",
                "B.Z. Li",
                "M. Khabsa",
                "H. Fang",
                "H. Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768.",
            "year": 2020
        },
        {
            "authors": [
                "C. Yang",
                "C. Qian",
                "N. Singh",
                "C.D. Xiao",
                "M. Westover",
                "E. Solomonik",
                "J. Sun"
            ],
            "title": "Atd: Augmenting cp tensor decomposition by self supervision",
            "venue": "Advances in Neural Information Processing Systems, 35:32039\u201332052.",
            "year": 2022
        },
        {
            "authors": [
                "C. Yang",
                "M.B. Westover",
                "J. Sun"
            ],
            "title": "Manydg: Many-domain generalization for healthcare applications",
            "venue": "In The Eleventh International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "C. Yang",
                "Z. Wu",
                "P. Jiang",
                "Z. Lin",
                "J. Sun"
            ],
            "title": "Pyhealth: A deep learning toolkit for healthcare predictive modeling, 09 2022",
            "venue": "URL https://github. com/sunlabuiuc/PyHealth.",
            "year": 2022
        },
        {
            "authors": [
                "C. Yang",
                "D. Xiao",
                "M.B. Westover",
                "J. Sun"
            ],
            "title": "Self-supervised eeg representation learning for automatic sleep staging",
            "venue": "arXiv preprint arXiv:2110.15278.",
            "year": 2021
        },
        {
            "authors": [
                "Zhang",
                "G.-Q.",
                "L. Cui",
                "R. Mueller",
                "S. Tao",
                "M. Kim",
                "M. Rueschman",
                "S. Mariani",
                "D. Mobley",
                "S. Redline"
            ],
            "title": "The national sleep research resource: towards a sleep data commons",
            "venue": "Journal of the American Medical Informatics Association, 25(10):1351\u20131358.",
            "year": 2018
        },
        {
            "authors": [
                "R. Zhang",
                "Q. Zong",
                "L. Dou",
                "X. Zhao"
            ],
            "title": "A novel hybrid deep learning scheme for four-class motor imagery classification",
            "venue": "Journal of neural engineering, 16(6):066004.",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "Z. Zhao",
                "T. Tsiligkaridis",
                "M. Zitnik"
            ],
            "title": "Self-supervised contrastive pre-training for time series via time-frequency consistency",
            "venue": "arXiv preprint arXiv:2206.08496.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "J. Chen",
                "J.H. Tan",
                "Y. Chen",
                "Y. Chen",
                "D. Li",
                "L. Yang",
                "J. Su",
                "X. Huang",
                "W. Che"
            ],
            "title": "An investigation of deep learning models for eeg-based emotion recognition",
            "venue": "Frontiers in Neuroscience, 14:622759.",
            "year": 2020
        },
        {
            "authors": [
                "Ge"
            ],
            "title": "Seizure is requested from Jing et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Biosignals, such as EEG and ECG, are multi-channel time series recorded at high sampling rates (e.g., 256Hz) in various healthcare domains, including sleep medicine, neurological and cardiovascular disease detection, and activity monitoring. Deep learning (DL) models have demonstrated impressive success in automating biosignal diagnosis across diverse applications (Yang et al., 2021), encompassing sleep stage classification (Biswal et al., 2018; Yang et al., 2021; Phan and Mikkelsen, 2022), emotion analysis via EEG (Zhang et al., 2020; Suhaimi et al., 2020), action and motor imagery recognition (Venkatachalam et al., 2020), acute stress detection through electrodermal activity (Greco et al., 2021), EEG-based seizure epilepsy classification (Yang et al., 2023; Jing et al., 2023), and ECG-driven cardiac arrhythmia detection (Isin and Ozdalili, 2017; Parvaneh et al., 2019).\nVarious deep learning methods have been applied to biosignal analysis. Some works use 1D convolutional neural networks (CNN) on raw signals (Jing et al., 2023; Nagabushanam et al., 2020; Dar et al., 2020), while others preprocess the data with short-time Fourier transform (STFT) and employ 2D CNN models on the resulting spectrogram (Yang et al., 2022a; Kim et al., 2020; Cui et al., 2020). Researchers also segment the signal and use a CNN segment encoder with a downstream sequence\nPreprint. Under review.\nar X\niv :2\n30 5.\n10 35\n1v 1\n[ ee\nss .S\nP] 1\n0 M\nay 2\nmodel (Zhang et al., 2019; Biswal et al., 2018; Jing et al., 2020; Almutairi et al., 2021), such as Transformer or recurrent neural networks (RNN), to capture temporal dynamics. Other approaches involve ensemble learning, feature fusion from multiple encoders (Li et al., 2022), and multi-level transformers to encode spatial and temporal features across and within channels (Lawhern et al., 2018; Song et al., 2021; Liu et al., 2021).\nThese models (Jing et al., 2023; Yang et al., 2021; Biswal et al., 2018; Kostas et al., 2021; Du et al., 2022; Zhang et al., 2022) predominantly focus on biosignal samples with fixed formats for specific tasks, while real-world data may exhibit mismatched channels, variable lengths, and missing values. In this paper, our objective is to devise a flexible training strategy that can handle diverse biosignal datasets with varying channels, lengths, and levels of missingness. For example, is it possible to transfer knowledge from abnormal EEG detection (a binary classification task with 64 channels and a 5-second duration, recorded at 256Hz) to improve another EEG task, such as seizure type classification (a multi-class task with 16 channels and a 10-second duration at 200Hz)? In reality, such data mismatches often arise from varying devices, system errors, and recording limitations. Additionally, it is also important to explore the potential of utilizing different unlabeled data.\nTo apply existing deep learning models to such settings of different biosignals, significant data processing is required to align the formats across multiple datasets. This may involve truncating or padding signals for consistent lengths (Zhang et al., 2022), and imputing missing channels or segments (Bahador et al., 2021). Such practices, however, may introduce unnecessary noise and shift data distributions, leading to poor generalization performance. Developing a flexible and unified model that accommodates biosignals with diverse formats can be advantageous.\nIn our paper, we develop the biosignal transformer (BIOT) model (summarized in Figure 1), which, to the best of our knowledge, is the first biosignal encoding model that can handle biosignals of various formats. Our motivation stems from the vision transformer (ViT) (Dosovitskiy et al., 2020) and the audio spectrogram transformer (AST) (Gong et al., 2021). The ViT model splits the image into a \"sentence\" of patches for image representation. The AST model splits the audio spectrogram into \"sentence\" for 1D audio representation. These \"sentence\" structures combined with Transformer (Vaswani et al., 2017) can handle variable-sized inputs.\nCompared to images (RGB or gray), audios, or natural languages, biosignals are more complicated primarily as it has multiple channels. It is non-trivial to transform diverse biosignals of various formats into unified \"sentence\" structures. This paper proposes BIOT to solve the challenge by a novel biosignal tokenization module that segments each channel separately into tokens and then flattens the tokens to form consistent biosignal \"sentences\" (illustrated in Figure 2). With the design, our BIOT can enable the knowledge transfer cross different data in the wild and allow joint (pre-)training on multiple biosignal data sources. Our contributions are listed below.\n\u2022 Biosignal transformer (BIOT). This paper proposes a biosignal encoding model BIOT by tokenizing biosignals of various formats into unified \u201csentences.\u201d\n\u2022 Knowledge transfer across different data. Our BIOT can enable joint (pre-)training and knowledge transfer across different biosignal datasets in the wild, which inspires the research of large foundation models for biosignals.\n\u2022 Strong empirical performance. We evaluate our BIOT on several unsupervised and supervised EEG, ECG, and human activity sensory datasets. Results show that BIOT outperforms baseline models and can utilize the models pre-trained on other data to benefit the current task.\n2 BIOT: Biosignal Transformer\nAs shown in Figure 1, our BIOT encoder cascades two modules: (i) the biosignal tokenization module that tokenizes an arbitrary biosignal (variable lengths, different channels, and missing values) into a \"sentence\" structure. This design can potentially enable previous language modeling techniques (Devlin et al., 2018; Liu et al., 2019; OpenAI, 2023) to empower the current biosignal models; (ii) a linear transformer module that captures complex token interactions within the \"sentence\" while maintaining linear complexity. After that, we also discuss the application of BIOT in different real-world settings."
        },
        {
            "heading": "2.1 Module 1: Biosignal Tokenization",
            "text": "Motivation. The goal of this paper is to model heterogeneous biosignals (e.g., EEG samples with different channels for different tasks) with a unified encoding model. For example, common EEG samples (Lopez et al., 2015), such as those for seizure detection, are recorded at 256Hz in the international 10-20 system 1 for 10-second long (Klem et al., 1999). With standard 16 montage editing, the sample is essentially a multi-channel time-series, represented as a matrix of size (16, 2560). However, format mismatch may prevent the applications on other similar data, such as different sampling rate (e.g., 200Hz vs 256Hz) (Jing et al., 2023), mismatched channels (i.e., different datasets have their own novel channels), variable recording duration (i.e., 30s per sample vs. 10s) (Zhang et al., 2022), missing segments (i.e., part of the recording is damaged due to device error). Thus, existing models may fail to utilize the mismatched data from different datasets.\nOur BIOT solves the above challenges by the following steps. Illustrations are shown in Figure 2. Assume the multi-channel biosignal as S \u2208 RI\u00d7J (use a complete sample for the ease of notation). \u2022 Resampling. We first resample all data to the same rate (denoted by r \u2208 R+, such as 200Hz)\nby linear interpolation. The consistent rate could be selected following clinical knowledge of a certain biosignal. For example, the highest frequency of interest in both EEG and ECG signals is commonly around 100 Hz, and thus 200 Hz or 250 Hz can be suitable for typical EEG or ECG applications, according to Nyquist-Shannon sampling theorem (Nyquist, 1928; Shannon, 1949).\n\u2022 Normalization: To alleviate the unit difference and amplitude mismatch across different channels and datasets, we use the 95-percentile of the absolute amplitude to normalize each channel. Formally, each channel S[i] is normalized by S[i]percentile(|S[i]|, 95%) .\n1https://en.wikipedia.org/wiki/10-20_system_(EEG)\n\u2022 Tokenization: For handling length variation, we tokenize the recording of each channel into t-second tokens, and neighboring tokens can have overlaps of p seconds (p, t \u2208 R+ and p < t, e.g., t = 1 and p = 0.5). Thus, the k-th token (k = 1, 2, 3, ...) in the i-th channel can be represented by the slicing notation S[i, (t\u2212 p)(k \u2212 1) : (t\u2212 p)(k \u2212 1) + t]. The number of tokens per channel is limited by the inequality: (t\u2212 p)(k \u2212 1) + t \u2264 J . Here, the overlap p is essential to maintain the temporal information for shorter signals. For example, if the length of signal is J = 3, a configuration of t = 1, p = 0 will only generate 3 tokens for each channel, while a configuration of t = 1, p = 0.5 gives 5 tokens per channel. In cases of missing values, we drop the corresponding tokens directly (as shown in Sample 2 Figure 2). Note that, our tokenization applies to each channel, separately, which is different from previous works (Biswal et al., 2018; Almutairi et al., 2021; Du et al., 2022) that split all channels together (which cannot work on Sample 2).\n\u2022 Flattening: We finally flatten tokens from all channels into a consistent \"sentence\".\nThe above steps are non-parametric. To study the effect of sampling rate r, token length t, and overlap p, we provide ablation studies and insights in Appendix B.3. In the following, we design the token embedding for the biosignal \"sentence\", which combines information from three aspects.\n\u2022 Segment embedding. We learn the segment embedding from a spectral perspective by first extracting an energy vector for each token S[i, (t\u2212 p)(k \u2212 1) : (t\u2212 p)(k \u2212 1) + t] based on all frequency bands. This step is enabled by fast fourier transform (FFT). A fully connected network (FCN) is then applied on the energy vector to obtain the segment embedding.\n\u2022 Channel embedding (spatial). We learn an embedding table for all different channels and add the corresponding channel embedding to the token. Each color represents one channel in Figure 2.\n\u2022 Positional embedding (temporal). In biosignals, the segment order within the channel captures temporal information. We thus add relative positional embedding to the final toke embedding by using the sinusoidal and cosine functions, which does not need learnable parameters.\nWe denote the final tokenzied biosignal \"sentence\" as X \u2208 RN\u00d7l where N is the number of tokens and l is the dimension of token embedding. In Figure 2, the marked orange area indicates the spatialor temporal-relevant tokens w.r.t. the current token (i.e., same time step or channel). Our token embeddings can effectively capture the segment features as well as the spatio-temporal features."
        },
        {
            "heading": "2.2 Module 2: Linear transformer",
            "text": "Transformer with linear complexity for long biosignal \"sentence\". Next, we want to leverage the Transformer model (Vaswani et al., 2017) for learning the \"sentence\" embedding. However, biosignals usually have many channels, which may lead to long \"sentences\". For example, the \"sentence\" of a\n64-channel EEG signal for 20 seconds (without overlaps p = 0) can have 64\u00d7 20 = 1280 tokens, and longer with the overlaps. Given that the original Transformer model is known to have quadratic complexity in both time and space, we adopt the linear attention mechanism (Wang et al., 2020; Katharopoulos et al., 2020) for biosignal learning applications.\nFormally, let us assume WK ,WV ,WQ \u2208 Rl\u00d7k be the key, value, and query matrices. Our selfattention module uses a rank-d approximation for the softmax attention (N \u00d7N ) by reduced-rank parameter matrices E> \u2208 RN\u00d7d,F \u2208 Rd\u00d7N (where d N ). The output H \u2208 RN\u00d7k is,\nH = Attention(XWQ,EXWK ,FXWV ) (1)\n= softmax ( (XWQ)(EXWK)>\u221a\nk ) \ufe38 \ufe37\ufe37 \ufe38\nN\u00d7d\n\u00b7FXWV\ufe38 \ufe37\ufe37 \ufe38 d\u00d7l . (2)\nThe main components in our linear transformer module are one linear self-attention layer and one fully connected network. To enable stable training, we add layer normalization (Ba et al., 2016), residual connection (He et al., 2016), and dropout (Srivastava et al., 2014) right before each component (see Figure 1), which greatly accelerates the convergence and improves the final performance.\nBIOT Encoder. An illustration of our proposed BIOT encoder is shown in Figure 1 (upper), which comprises the biosignal tokenization module and multiple blocks of linear transformer modules. We obtain the final biosignal \"sentence\" embedding by a mean pooling step over all tokens. Note that appending a classification [CLS] token at the beginning of the \"sentence\" (after Module 1) is also a common option. However, we find it yields a slightly worse performance in our application, and thus we use mean pooling in the experiments."
        },
        {
            "heading": "2.3 Biosignal Learning in the Wild",
            "text": "Our proposed BIOT encoder can be applied in various real-world biosignal applications illustrated in the lower part of Figure 1). These applications include (1) standard supervised learning, (2) learning with missing channels or segments, (3) & (4) pre-training on one or more datasets, and fine-tuning on other similar datasets with different input formats.\n(1) Supervised Learning is the most common setting in the previous literature (Jing et al., 2023; Biswal et al., 2018). With the BIOT encoder, we finally apply an exponential linear unit (ELU) activation (Clevert et al., 2015) and a linear layer for classification tasks. (2) Supervised Learning (with missing). Many real biosignal data have mismatched channels, missing segments, and variable lengths, which prevents the applications of existing models (Jing et al., 2023; Song et al., 2021). Flexible as our model is, BIOT can be applied in this setting with the same model structure as in (1). (3) Unsupervised Pre-training. We can jointly pre-train a general-purpose BIOT encoder on multiple large unlabeled datasets. In the experiments, we pre-train an unsupervised encoder using 5 million resting EEG samples (16 channels, 10s, 200Hz) and 5 million sleep EEG samples (2 channels, 30s, 125Hz), which is later utilized to improve various downstream tasks. For the unsupervised pre-training, we take the following steps (a diagram is shown in Figure 1).\n\u2022 Assume S is the original biosignal. We first randomly dropout part of its channels and dropout part of the tokens from the remaining channels, resulting in a perturbed signal S\u0303.\n\u2022 We then obtain the embeddings of S and S\u0303 by the same BIOT encoder. To form the objective, we want to predict the embedding of the original signal by the perturbed signal. Thus, an additioanl predictor (i.e., two-layer neural network) is appended for the perturbed signal following (Grill et al., 2020). We use Z and Z\u0303 to denote the real embedding of S and predicted embedding from S\u0303.\nZ = BIOT(S), Z\u0303 = predictor(BIOT(S\u0303)). (3)\n\u2022 Finally, contrastive loss (He et al., 2020; Chen et al., 2020) is used on S and S\u0303 to form the objective. L = CrossEntropyLoss ( softmax ( \u3008Z, Z\u0303>\u3009/T ) , I ) . (4)\nHere, T represents the temperature (T = 0.2 throughout the paper) and I is an identity matrix. In the implementation, we also apply sample-wise L2-normalization on both Z and Z\u0303 before softmax.\n(4) Supervised Pre-training aims to pre-train a model by supervised learning on one task and then generalize and fine-tune the encoder on a new task. The goal is to transfer knowledge among different datasets and gain improvements on the new task compared to training from scratch. Our BIOT model allows the new datasets to have mismatched channels and different lengths."
        },
        {
            "heading": "3 Experiments",
            "text": "This section shows the strong performance of BIOT on several EEG, ECG and sensory datasets. Section 3.2, 3.3 compare BIOT with baselines on supervised learning and learning with missing settings. Section 3.4, 3.5, 3.6 show the that BIOT can be flexibly pre-trained on other datasets (supervised or unsupervised) to improve the current task with different sample formats. We released the codebase and the pre-trained models in GitHub 2."
        },
        {
            "heading": "3.1 Experimental Setups",
            "text": "Biosignal Datasets. We consider the following datasets in the evaluation: (i) SHHS (Zhang et al., 2018; Quan et al., 1997) is a large sleep EEG corpus from patients aged 40 years and older. (ii) PREST is a large unlabeled proprietary resting EEG dataset; (iii) Cardiology (Alday et al., 2020) is a collection of five ECG datasets (initially contains six, but we exclude the PTB-XL introduced below). (iv) The CHB-MIT database (Shoeb, 2009) is collected from pediatric patients for epilepsy seizure detection. (v) IIIC Seizure dataset is from Ge et al. (2021); Jing et al. (2023) for detecting one of the six ictal-interictal-injury-continuum (IIIC) seizure patterns (OTH, ESZ, LPD, GPD, LRDA, GRDA); (vi) TUH Abnormal EEG Corpus (TUAB) (Lopez et al., 2015) is an EEG dataset that has been annotated as normal or abnormal; (vii) TUH EEG Events (TUEV) (Harati et al., 2015) is a corpus of EEG that contains annotations of EEG segments as one of six event types: spike and sharp wave (SPSW), generalized periodic epileptiform discharges (GPED), periodic lateralized epileptiform discharges (PLED), eye movement (EYEM), artifact (ARTF) and background (BCKG); (viii) PTBXL (Wagner et al., 2020) is an ECG dataset with 12-lead recordings for diagnosis prediction, and we used it for arrhythmias phenotyping in this paper; (ix) HAR (Anguita et al., 2013) is a human action recognition dataset using smartphone accelerometer and gyroscope data."
        },
        {
            "heading": "Datasets Type (subtype) # Recordings Rate Channels Duration # Sample Tasks",
            "text": "Dataset Processing. The first three datasets are used entirely for unsupervised pre-training. The next four datasets are used for supervised learning, and we used the common 16 bipolar montage channels in the international 10-20 system. For CHB-MIT (containing 23 patients), we first use patient 1 to 19 for training, 20,21 for validation, and 22,23 for test. Then, we flip the validation and test sets and conduct the experiments again. We report the average performance on these two settings. For IIIC seizure, we divide patient groups into training/validation/test sets by 60%:20%:20%. For TUAB and TUEV, the training and test separation is provided by the dataset. We further divide the training patients into training and validation groups by 80%:20%. For PTB-XL, we divide patient groups into training/validation/test sets by 80%:10%:10%. The train and test set of HAR is provided, and we further divide the test patients into validation/text by 50%:50%. For all the datasets, after assigning the patients to either training, validation, or test groups, we will further split the patient\u2019s recording to samples, and the sample duration accords to the annotation files. The dataset statistics can be found in Table 1, and we provides more descriptions and processing details in Appendix A.1.\nBaseline. We consider the following representative models: (i) SPaRCNet (Jing et al., 2023) is a 1D-CNN based model with dense residual connections, more advanced than the popular ConvNet (Schirrmeister et al., 2017), CSCM (Sakhavi et al., 2018); (ii) ContraWR\u2019s (Yang et al., 2021)\n2https://github.com/ycq091044/BIOT\nencoder model first transforms the biosignals into multi-channel spectrogram and then uses 2D-CNN based ResNet (He et al., 2016); (iii) CNN-Transformer (Peh et al., 2022) is superior to CNN-LSTM models (Zhang et al., 2019); (iv) FFCL (Li et al., 2022) combines embeddings from CNN and LSTM encoders for feature fusion; (v) ST-Transformer Song et al. (2021) proposes an multi-level EEG transformer for learning spatial (S) and temporal (T) features simultaneously, empirically better than EEGNet Lawhern et al. (2018). Our BIOT model trained from scratch is denoted by (vanilla).\nEnvironments and Settings. The experiments are implemented by Python 3.9.12, Torch 1.13.1+cu117, Pytorch-lightning 1.6.4 on a Linux server with 512 GB memory, 128-core CPUs and eight RTX A6000 GPUs. All the models are optimized on training set and evaluated on the test set. The best model and hyperparameter combinations are selected based on the validation set. For Table 2 and Table 3, we obtain five sets of results with different random seeds and report the mean and standard deviation values. For Figure 3 and Figure 4, we report the results under three random seeds. More experimental and implementation details can refer to Appendix A.2."
        },
        {
            "heading": "3.2 Setting (1) - standard supervised learning",
            "text": "This section shows that BIOT is comparable or better than baselines in the supervised learning settings.\n\u2022 Four EEG Tasks. Both CHB-MIT and TUAB are designed to predict binary output, and we use binary cross entropy (BCE) for TUAB and the focal loss (Lin et al., 2017) for CHB-MIT due to its imbalances (around 0.6% positive ratio in training set). We use balanced accuracy (Balanced Acc.), area under precision-recall curve (AUC-PR) and AUROC as the metrics. Both IIIC Seizure and TUEV are multi-class classification tasks with cross entropy loss. We employ Balanced Acc., Cohen\u2019s Kappa, and Weighted F1 as the multi-class evaluation. To save space, we only show the performance on CHB-MIT and IIIC Seizure in Table 2 and move the other two to Appendix B.1.\n\u2022 ECG and Sensory Tasks. PTB-XL is formulated as a binary classification on detecting arrhythmias phenotypes. We use the BCE loss and binary evaluation metrics. HAR (classifying actions) uses the cross entropy loss and is evaluated by multi-class metrics. Results are reported in Table 3.\nTable 2 and 3 show that our model has superior performance over baselines in most tasks, especially on CHB-MIT, IIIC Seizure, and HAR. The reason might be that the frequency features are more useful in these three datasets as our BIOT extracts the main features from spectral perspective. SPaRCNet is a strong model among all the baselines except on the CHB-MIT task. The model might be vulnerable in the imbalanced classification setting even with the focal loss. The pre-training models at the end of the tables will be introduced and explained in Section 3.4, 3.6."
        },
        {
            "heading": "3.3 Setting (2) - learning with missing channels and segments",
            "text": "The section simulates the TUEV dataset to mimic the setting of supervised learning with missing channels and segments and show the strong performance of BIOT. We consider three missing cases:\n\u2022 Missing segments: Randomly mask out a segments (each segment spans for 0.5 seconds), a = 0, 1, 2, 3, 4, 5 with equal probability. The segment masking is applied separately for each channel.\n\u2022 Missing channels: Randomly mask out b channels, b = 0, 1, 2, 3, 4 with equal probability. We assume that the masking will not alter the underlying labels (the same assumption for other cases).\n\u2022 Missing both channels and segments: Combining Case 2 & 3 simultaneously.\nTo enable the baseline models compatible with the setting, we use all zeros to impute the masked regions. The comparison is plotted in Figure 3, which shows that (i) all models decrease the performance with more missings while BIOT and the pre-trained BIOT are less impacted (especially on Kappa and Weighted F1); (ii) \"Missing channels\" affects the performance more than \"Missing segments\", which makes sense as segment masking still preserves information from all channels."
        },
        {
            "heading": "3.4 Setting (3) - unsupervised pre-training",
            "text": "In this section, we show that BIOT enables unsupervised pre-training on existing with various formats.\n\u2022 Pre-trained (PREST): This model is pre-trained on 5 million resting EEG samples (PREST) with 2,048 as the batch size. We save the pre-trained model at the 100-th epoch.\n\u2022 Pre-trained (PRESET+SHHS): This model is jointly pre-trained on 5M PREST and 5M SHHS EEG samples. Though two datasets have different sample formats, our model is able to encode them regardless. Also, we use 2048 as the batch size and save model at the 100-th epoch.\n\u2022 Pre-trained (Cardiology-12) is jointly pre-trained on raw data of five datasets in Cardiology corpus (details in Appendix A.1). We use 1024 as batch size and save model at the 100-th epoch.\n\u2022 Pre-trained (Cardiology-6) is pre-trained similarly as Pre-trained (Cardiology-12), while we only utilize the first 6 ECG leads. By contast, Pre-trained (Cardiology-12) uses full 12 leads.\nWe fine-tune the first two pre-trained EEG models on four EEG tasks and append the results to Table 2 (also in Appendix B.1). We fine-tune the last two pre-trained ECG models on PTB-XL datasets in Table 3. The results show that the pre-trained models greatly improves the final performance on the downstream tasks in Table 2 and Table 3."
        },
        {
            "heading": "3.5 Setting (4) - supervised pre-training on other tasks",
            "text": "This section shows that BIOT allows knowledge transfer from one task to another similar task with different sample formats. We pre-train on the training set of CHB-MIT, IIIC Seizure, TUAB and fine-tunes on TUEV (which has 16 channels and 5s duration). All datasets use 200Hz sampling rate. We design three sets of configurations for the pre-trained datasets: Format (i) uses the first 8 channels and 10s duration; Format (ii) uses the full 16 channels but only the first 5s recording; Format (iii) uses full 16 channels and full 10s recording. During fine-tuning, we then remove the prediction layers from these pre-trained model and add a new prediction layer to fit the TUEV dataset.\nThe results are shown in Figure 4 where we also add the vanilla BIOT (trained from scratch) for references. We find that (i) the model pre-trained on IIIC Seizure and TUAB are generally beneficial for the event classification task on TUEV. The reason might be that TUAB and TUEV are both recorded from Temple University and share some common information, while IIIC seizure and TUEV are both related to seizure detection and may share some latent patterns. (ii) More pre-training data will be beneficial in the downstream task. Though the pre-training configuration (16 channels, 5 seconds) aligns better with the TUEV data formats, the results show that configuration of (16 channels, 10 seconds) encodes longer duration and works consistently better. (iii) Compared to the TUEV results in Appendix B.1, we also find that oftentimes the supervised pre-training (e.g., on IIIC seizure or TUAB) can be more effective than unsupervised pre-training (e.g., on SHHS and PREST)."
        },
        {
            "heading": "3.6 Pre-trained on all EEG datasets",
            "text": "In this section, we show that BIOT can leverage all six EEG resources considered in the paper. We obtain a Pre-trained (6 EEG datasets) model by loading the Pre-trained (PREST+SHHS) model and further train it on the training sets of CHB-MIT, IIIC Seizure, TUAB, and TUEV. We add separate classification layers for four tasks. Essentially, this model is pre-trained on all six EEG datasets. To use the model, we still fine-tune it on the training set of downstream tasks and append the results to Table 2 and Appendix B.1. Apparently, Pre-trained (six EEG datasets) outperforms the vanilla BIOT and is generally better than the unsupervised and the supervised pre-trained BIOT."
        },
        {
            "heading": "4 Conclusion",
            "text": "This paper proposes a new biosignal transformer model (BIOT) that learns embeddings for biosignals with variable lengths, channels and missing values. BIOT can enable effective knowledge transfer across different data and allow joint training on multiple sources. We conduct extensive evaluations on two large EEG corpus (5M each) for unsupervised pre-training, and several EEG, ECG, human action sensory datasets for supervised learning. The results show that our BIOT outperforms strong baselines in standard supervised learning and can effectively handle the learning settings with missing values. The pre-trained BIOT models also show significant improvements on various downstream classification tasks. In the end, we hope our work can inspire more follow-up researches of large foundational models for biosignals."
        },
        {
            "heading": "A Details of Datasets and Experimental Settings",
            "text": ""
        },
        {
            "heading": "A.1 More for Datasets and Processings",
            "text": "We provide more descriptions on each dataset in this section.\nFor EEG datasets. First, the 16 montages (in 10-20 international system) are \"FP1-F7\", \"F7-T7\", \"T7-P7\", \"P7-O1\", \"FP2-F8\", \"F8-T8\", \"T8-P8\", \"P8-O2\", \"FP1-F3\", \"F3-C3\", \"C3-P3\", \"P3-O1\", \"FP2-F4\", \"F4-C4\", \"C4-P4\", \"P4-O2\".\n\u2022 Sleep Heart Health Study (SHHS) (Zhang et al., 2018; Quan et al., 1997) is a multi-center cohort study from the National Heart Lung & Blood Institute assembled to study sleep-disordered breathing, which contains 5,445 recordings. The data is accessible upon request in their website 3. Each recording has 14 Polysomnography (PSG) channels, and the recording frequency is 125.0 Hz. We use the C3/A2 and C4/A1 EEG channels. The dataset is released with sleep annotations. We use the existing codes 4 and split each recordings into 30-second samples. In this study, we use SHHS samples for unsupervised pre-training without it original labels.\n\u2022 PREST is a private dataset recorded in hospital sleep lab, primarily for seizure and abnormal EEG detection purpose (such as spikes). The local IRB waived the requirement for informed consent for this retrospective analysis of EEG data. We follow the clinician\u2019s instructions and split each recordings into 10 seconds without labels. In the experiment, we use it for EEG model pre-training.\n\u2022 The CHB-MIT database 5 (Shoeb, 2009) is publicly available, which is collected at the Children\u2019s Hospital Boston, consists of EEG recordings from pediatric subjects with intractable seizures. The dataset is under Open Data Commons Attribution License v1.0 6 and is used to predict whether the EEG recordings contain seizure signals. Each recording initially contains 23 bipolar channels and we select the 16 standard montages in the experiments. We utilize the existing preprocessing 7 and follow the typical practices to further split each recordings into 10-second non-overlapping samples by default. Since the dataset is highly imbalanced, we use 5 seconds as overlaps to split the seizure regions (which could potentially double the positive samples). After processing, the positive ratio in the training set is around 0.6%.\n3https://sleepdata.org/datasets/shhs 4https://github.com/ycq091044/ContraWR/tree/main/preprocess 5https://physionet.org/content/chbmit/1.0.0/ 6https://physionet.org/content/chbmit/view-license/1.0.0/ 7https://github.com/bernia/chb-mit-scalp\n\u2022 IIIC Seizure is requested from Jing et al. (2018); Ge et al. (2021); Jing et al. (2023), and we follow the license and usage statements in Jing et al. (2023). The samples follow 16 montages and span 10-second signals at 200Hz. This dataset is used for predicting one of the six classes: lateralized periodic discharges (LPD), generalized periodic discharges (GPD), lateralized rhythmic delta activity (LRDA), generalized rhythmic delta activity (GRDA), Seizure types, and Other.\n\u2022 TUH Abnormal EEG Corpus (TUAB) (Lopez et al., 2015) and TUH EEG Events (TUEV) (Harati et al., 2015) is accessible upon request at Temple University Electroencephalography (EEG) Resources 8. We process both datasets to follow the 16 EEG montages.\nFor ECG datasets. We use the Cardiology collection to pre-train the ECG models and apply it on downstream supervisd PTB-XL task.\n\u2022 The Cardiology collection (Alday et al., 2020) is publicly available at physionet 9, which was used in the PhysioNet/Computing in Cardiology Challenge 2020. This collection is under Creative Commons Attribution 4.0 International Public License 10. In this study, we use five sets from the training portion of the collection (It has in total six sets. Another one overlaps with the PTB-XL dataset, and thus we drop it in the pre-training), which contains recordings from CPSC2018 (6,877 recordings), CPSC2018Extra (China 12-Lead ECG Challenge Database \u2013 unused CPSC 2018 data, 3,453 recordings), St Petersburg Incart (12-lead Arrhythmia Database, 74 recordings), ptb (Diagnostic ECG Database, 516 recordings), Georgia (12-Lead ECG Challenge Database, 10,344 recordings). For preprocessing, we extract 10-second samples from each recording with 0.5s as the overlapping window. All the samples are merged together as an unsupervised pre-training ECG corpus of nearly 0.5 million samples. We pre-train a Pre-trained BIOT (Cardiology-12) on all the channels and a Pre-trained BIOT (Cardiology-6) on the first 6-channels of all samples. The sample sizes are different from the below PTB-XL dataset.\n\u2022 Physikalisch-Technische Bundesanstalt (PTB-XL) 11 (Wagner et al., 2020) is a publicly available large dataset of 12-lead ECGs from 18885 patients. It is under the Creative Commons Attribution 4.0 International Public License 12. The raw waveform data was annotated by up to two cardiologists, who assigned potentially multiple ECG statements to each record up to 27 diagnoses: 1:1st degree AV block, 2:Atrial fibrillation, 3:Atrial flutter, 4:Bradycardia, 5:Complete right bundle branch block, 6:Incomplete right bundle branch block, 7:Left anterior fascicular block, 8:Left axis deviation, 9:Left bundle branch block, 10:Low QRS voltages, 11:Nonspecific intraventricular conduction disorder, 12:Pacing rhythm, 13:Premature atrial contraction, 14:Premature ventricular contractions, 15:Prolonged PR interval, 16:Prolonged QT interval, 17:Q wave abnormal, 18:Right axis deviation, 19:Right bundle branch block, 20:Sinus arrhythmia, 21:Sinus bradycardia, 22:Sinus rhythm, 23:Sinus tachycardia, 24:Supraventricular premature beats, 25:T wave abnormal, 26:T wave inversion, 27:Ventricular premature beats. We following clinical knowledges and further groups them into six broader categories: Arrhythmias, Bundle branch blocks and fascicular blocks, Axis deviations, Conduction delays, Wave abnormalities, Miscellaneous. Each recordings can be associated to multiple categories. In this paper, we conduct the \"Arrhythmias\" phenotyping prediction task. If the recordings have at least one diagnosis belonging to the Arrhythmias group, then we label them as positive, otherwise as negative.\nFor human activity sensory data. Human activity recognition (HAR) dataset 13 (Anguita et al., 2013) is publicly available at UCI machine learning repository. The data is collected from smartphone accelerometer and gyroscope data with 3D coordinates to detect six actions: walking, walking upstairs, walking downstairs, sitting, standing, laying. The samples are already splitted and provided in the original datasets.\n8https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml 9https://physionet.org/content/challenge-2020/1.0.2/\n10https://physionet.org/content/challenge-2020/view-license/1.0.2/ 11https://physionet.org/content/ptb-xl/1.0.1/ 12https://physionet.org/content/ptb-xl/view-license/1.0.1/ 13https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
        },
        {
            "heading": "A.2 More for Experimental Settings",
            "text": "For model implementation, the SPaRCNet code is requested from the authors (Jing et al., 2023), the ContraWR code is downloaded and modified upon the github 14, CNN-Transformer is easily implemented following the Fig. 3 of the original paper (Peh et al., 2022), FFCL (Li et al., 2022) combines a CNN model and a LSTM model for learning separete representations and then merges them before the final prediction layer, the implementation of ST-Transformer refer to this repo 15. The linear-complexity attention module is referred to this repo 16 in our BIOT implementation.\nFor all EEG tasks, we resample the datasets into 200Hz. The ECG tasks use 500Hz, and the HAR tasks use 50Hz by default. For each specific tasks, we have to adjust the baseline model architectures (e.g, number of layers, input channel sizes, etc) accordingly since the input data have various formats. While for our BIOT, we only adjust the fft size based on their sampling rate (200 points for EEG, 1000 points for ECG, 100 points for HAR) and use 100 points, 200 points, and 10 points as the hop length (i.e., overlaps) in three signal types. These configurations are chosen by testing several other combinations based on the validation performance. For our BIOT model, we use 8 as the number of head, 4 as the number of transformer layers, and T = 2 as the temperature in unsupervised pre-training by default. We use the Adam optimizer with learning rate 1 \u00d7 10\u22123 and 1 \u00d7 10\u22125 as the coefficient for L2 regularization by default. We use the pytorch lightning framework (with 100 as the max epoch) to handle the training, validation, and test pipeline by setting AUROC as the monitoring metirc for binary classification and Coken\u2019s Kappa as the monitoring metric for multi-class classification. More details can refer to our Supplementary codes. Below, we provide the definition of each metric used in the paper, and we use pyhealth.metrics 17 Yang et al. (2022b) module for the implementation.\nBalanced Accuracy is defined as the average of recall obtained on each class. It is used for both binary classification and multi-class classification.\nAUC-PR is the area under the precision recall (PR) curve for binary classification task.\nAUROC is the area under the ROC curve, summarizing the ROC curve into an single number that describes the performance of a model for multiple thresholds at the same time. It is used for binary classification.\nCoken\u2019s Kappa is a statistic that measures inter-annotator agreement, which is usually used for imbalanced multi-class classification task. The calculation can refer to sklearn metrics 18.\nWeighted F1 is used for multi-class classification in this paper, which is a weighted average of individual F1-scores from each class, with each score weighted by the number of samples in the corresponding class."
        },
        {
            "heading": "B Additional Results",
            "text": "This section provides additional experimental results to support claims in the main paper."
        },
        {
            "heading": "B.1 Additional Experiments on TUEV and TUAB",
            "text": "We have provided the supervised learning results on EEG dataset IIIC Seizure and CHB-MIT in the main text. For completeness, we provide similar comparison results on TUAB and TUEV below in Table 4 5, which show a similar trend that our BIOT shows better performance against baseline models, and the pre-trained BIOT models can bring significant improvements on two downstream tasks, especially on TUEV. For TUEV, we also append the results of all different pre-trained models (e.g., train from scratch, supervised training, unsupervised training, etc) in the end in Table 5.\n14https://github.com/ycq091044/ContraWR 15https://github.com/eeyhsong/EEG-Transformer 16https://github.com/lucidrains/linear-attention-transformer 17https://pyhealth.readthedocs.io/en/latest/api/metrics.html 18https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html"
        },
        {
            "heading": "B.2 Additional Experiments on CHB-MIT",
            "text": "This section performs a similar experiment on CHB-MIT, similar to Section 3.2. We pre-train on the training set of IIIC Seizure (which has 16 channels and 10s duration), TUAB (which has 16 channels and 10s duration), TUEV (which has 16 channels and 5s duration) and fine-tunes on CHB-MIT (which has 16 channels and 10s duration). All datasets use 200Hz sampling rate. We design five sets of configurations for the pre-trained datasets: Format (i) uses the first 8 channels and 10s duration; Format (ii) uses the full 16 channels but only the first 5s recording; Format (iii) uses full 16 channels and full 10s recording; Format (iv) uses 8 channels and 5s recording, and Format (v) uses full 16 channels and 2.5s recording. The last two are only for the TUEV dataset. During fine-tuning, we then remove the prediction layers from these pre-trained model and add a new prediction layer to fit the CHB-MIT dataset.\nThe results are reported in Figure 5, which shows that the supervised pre-training on both IIIC seizure and TUEV can help improve the downstream performance on CHB-MIT task compared to training from scratch. The reason is that IIIC Seizure is on multiple seizure type classification while CHB-MIT is on binary seizure or not classification, and the context of both tasks are fairly related. Although TUEV is not entirely on seizure related classification, some classes in TUEV are seizure subtypes (such as GPED, PLED), and thus its supervisd pre-trained models can also bring benefits for the CHB-MIT task."
        },
        {
            "heading": "B.3 Ablation Studies on Hyperparameters",
            "text": "This section provides ablation studies on three hyperparameters in data processing: target sampling rate, token duration, and the overlap size between two neighboring tokens. We use two EEG datasets as example: IIIC Seizure and TUAB. The default configuration in the main paper is (1) sampling: 200Hz, (2) token length: 1s, (3) overlaps: 0.5s as reference."
        },
        {
            "heading": "B.3.1 Ablation Study on Target Sampling Rate r",
            "text": "In this experiments, we fix (2)(3) and conduct ablation study on the target sampling rate. The original IIIC Seizure data is at 200Hz and the TUAB data is at 256Hz. For IIIC Seizure, we vary the sampling rate to 26Hz, 50Hz, 100Hz, 150Hz, and 200Hz. For TUAB, we vary the sampling rate to 50Hz, 100Hz, 150Hz, 200Hz, 250Hz, and 300Hz. The evaluations are conducted under three different random seeds and the mean and standard deviation values are reported.\nFor IIIC Seizure, we can observe that a higher sampling rate could give slightly better performance, especially on balanced acc. and coken\u2019s kappa. The reason is that higher sampling rate can preserve more detailed (high-frequency) biosignal information. The results on TUAB shows that the performances are similar on all sampling rates. We conjecture that different tasks might have diverse sensitivity to the the frequency bands. For example, the task on IIIC seizure is to classify different seizure types, which may need to capture minor clues from high-frequency waves (such as Gamma waves (30-100Hz)), while the TUAB dataset is for abnormal detection, and using brain waves under 50Hz might be enough for the task. In sum, the target sampling rate should be selected based on the predicting targets."
        },
        {
            "heading": "B.3.2 Ablation Study on Token Lengths t",
            "text": "In this experiments, we fix (1)(3) and conduct ablation study on the token length. Both datasets have 10s as the entire sample length and 0.5s as the overlap lengths. For both of them, we vary the token lengths to 0.75s, 1s, 1.5s, 2s, 2.5s, 5s. The evaluations are conducted under three different random seeds and the mean and standard deviation values are reported.\nFor each configuration, we also vary the fft size to match the token length, which means that 5s token length can extract more frequency information. However, we find that by increasing the token lengths, the model performance starts to decrease. Performances on IIIC Seizure starts to decrease after 1s while the performance on TUAB decreases after 2s. The reason could be that given the increaseing token lengths t, the total biosignal \"sentence\" length, which is J \u2212tt\u2212p + 1 = 10\u2212t t\u22120.5 + 1, will decrease (here, J is the channel biosignal duration, t is the token length, p is the overlapping length). For example, with x = 5 as the token lengths, the final \"sentence\" length becomes 11 while it is 19 in the\ndefault configuration with x = 1s. The performance drops is due to transformer models will be less beneficial in shorter \"sentence\"s."
        },
        {
            "heading": "B.3.3 Ablation Study on Overlapping Lengths p",
            "text": "In this experiments, we fix (1)(2) and conduct ablation study on the overlap lengths. Both datasets have 10s as the entire sample length and 1s as the token lengths. For both of them, we vary the overlap lengths to 0.875s, 0.75s, 0.5s, 0.25s, 0s. The evaluations are conducted under three different random seeds and the mean and standard deviation values are reported.\nBased on the \"sentence\" length formula J\u2212tt\u2212p +1, smaller overlap lengths will decrease the \"sentence\" length. On both datasets, we find that larger overlaps can brings slightly better results due to that the biosignal \"sentence\" becomes longer. Another reason is that with larger overlaps, neighboring tokens can capture more transitioning information and help the transformer model to better capture the temporal information."
        }
    ],
    "title": "BIOT: Cross-data Biosignal Learning in the Wild",
    "year": 2023
}