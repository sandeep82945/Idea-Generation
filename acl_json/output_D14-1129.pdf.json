{
    "abstractText": "Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for crosslingual information processing. In this paper, we propose a link-based approach to distinguish parallel web pages from bilingual web sites. Compared with the existing methods, which only employ the internal translation similarity (such as content-based similarity and page structural similarity), we hypothesize that the external translation similarity is an effective feature to identify parallel web pages. Within a bilingual web site, web pages are interconnected by hyperlinks. The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages, which can be adopted as an important source of external similarity. Thus, the translation similarity of page pairs will influence each other. An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity. Both internal and external similarity measures are combined in the iterative algorithm. Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement (6.2% F-Score) over the baseline which only utilizes internal translation similarity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Le Liu"
        },
        {
            "affiliations": [],
            "name": "Yu Hong"
        },
        {
            "affiliations": [],
            "name": "Jun Lu"
        },
        {
            "affiliations": [],
            "name": "Jun Lang"
        },
        {
            "affiliations": [],
            "name": "Heng Ji"
        },
        {
            "affiliations": [],
            "name": "Jianmin Yao"
        }
    ],
    "id": "SP:d7f1c5d468bd71abd959d9426b24c4f2890811db",
    "references": [
        {
            "authors": [
                "Chen",
                "Jiang",
                "Jianyun Nie."
            ],
            "title": "Automatic construction of parallel English-Chinese corpus for cross-language information retrieval",
            "venue": "Proceedings of the sixth conference on Applied Natural Language Processing, 21\u201328.",
            "year": 2000
        },
        {
            "authors": [
                "Resnik",
                "Philip",
                "Noah A. Smith."
            ],
            "title": "The Web as a Parallel Corpus",
            "venue": "Meeting of the Association for Computational Linguistics 29(3). 349\u2013380.",
            "year": 2003
        },
        {
            "authors": [
                "Kit",
                "Chunyu",
                "Jessica Yee Ha Ng."
            ],
            "title": "An Intelligent Web Agent to Mine Bilingual Parallel Pages via Automatic Discovery of URL Pairing Patterns",
            "venue": "Web Intelligence and Intelligent Agent Technology Workshops, 526\u2013529.",
            "year": 2007
        },
        {
            "authors": [
                "Zhang",
                "Ying",
                "Ke Wu",
                "Jianfeng Gao",
                "Phil Vines."
            ],
            "title": "Automatic Acquisition of Chinese-English Parallel Corpus from the Web",
            "venue": "Joint Proceedings of the Association for Computational Linguistics and the International Conference on Computational",
            "year": 2006
        },
        {
            "authors": [
                "Nie",
                "Jianyun",
                "Michel Simard",
                "Pierre Isabelle",
                "Richard Durand."
            ],
            "title": "Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web",
            "venue": "Proceedings of the 22nd annual international ACM SIGIR confer-",
            "year": 1999
        },
        {
            "authors": [
                "Ma",
                "Xiaoyi",
                "Mark Y. Liberman."
            ],
            "title": "BITS: A Method for Bilingual Text Search over the Web",
            "venue": "Machine Translation Summit VII.",
            "year": 1999
        },
        {
            "authors": [
                "Chen",
                "Jisong",
                "Rowena Chau",
                "Chung-Hsing Yeh."
            ],
            "title": "Discovering Parallel Text from the World Wide Web",
            "venue": "The Australasian Workshop on Data Mining and Web Intelligence, vol. 32, 157\u2013161. Dunedin, New Zealand.",
            "year": 2004
        },
        {
            "authors": [
                "Jiang",
                "Long",
                "Shiquan Yang",
                "Ming Zhou",
                "Xiaohua Liu",
                "Qingsheng Zhu."
            ],
            "title": "Mining Bilingual Data from the Web with Adaptively Learnt Patterns",
            "venue": "Proceedings of the Joint Conference of the 47th",
            "year": 2009
        },
        {
            "authors": [
                "Yanhui Feng",
                "Yu Hong",
                "Zhenxiang Yan",
                "Jianmin Yao",
                "Qiaoming Zhu."
            ],
            "title": "A novel method for bilingual web page acquisition from search engine web records",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters,",
            "year": 2010
        },
        {
            "authors": [
                "Zhao",
                "Bing",
                "Stephan Vogel."
            ],
            "title": "Adaptive Parallel Sentences Mining from Web Bilingual News Collection",
            "venue": "IEEE International Conference on Data Mining, 745\u2013748.",
            "year": 2002
        },
        {
            "authors": [
                "Smith",
                "Jason R.",
                "Chris Quirk",
                "Kristina Toutanova."
            ],
            "title": "Extracting parallel sentences from comparable corpora using document level alignment",
            "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the",
            "year": 2010
        },
        {
            "authors": [
                "Bharadwaj",
                "Rohit G.",
                "Vasudeva Varma."
            ],
            "title": "Language independent identification of parallel sentences using wikipedia",
            "venue": "Proceedings of the 20th International Conference Companion on World Wide Web, 11\u201312. Hyderabad, India.",
            "year": 2011
        },
        {
            "authors": [
                "Gusfield",
                "Dan."
            ],
            "title": "Algorithms on Strings, Trees and Sequences: Computerss Science and Computational Biology",
            "venue": "Cambridge University Press",
            "year": 1997
        },
        {
            "authors": [
                "Shi",
                "Lei",
                "Cheng Niu",
                "Ming Zhou",
                "Jianfeng Gao."
            ],
            "title": "A DOM Tree Alignment Model for Mining Parallel Data from the Web",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the As-",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216\u20131224, October 25-29, 2014, Doha, Qatar. c\u00a92014 Association for Computational Linguistics\nlingual web sites is a crucial step of bilingual resource construction for crosslingual information processing. In this paper, we propose a link-based approach to distinguish parallel web pages from bilingual web sites. Compared with the existing methods, which only employ the internal translation similarity (such as content-based similarity and page structural similarity), we hypothesize that the external translation similarity is an effective feature to identify parallel web pages. Within a bilingual web site, web pages are interconnected by hyperlinks. The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages, which can be adopted as an important source of external similarity. Thus, the translation similarity of page pairs will influence each other. An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity. Both internal and external similarity measures are combined in the iterative algorithm. Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement (6.2% F-Score) over the baseline which only utilizes internal translation similarity."
        },
        {
            "heading": "1 Introduction",
            "text": "Parallel corpora have played an important role in multilingual Natural Language Processing, especially in Machine Translation (MT) and Crosslingual Information Retrieval(CLIR). However, it\u2019s time-consuming to build parallel corpora\nsubject to subscription or license fee and thus not freely available, while others are domain-specific. Therefore, a lot of previous research has focused on automatically mining parallel corpora from the web.\nIn the past decade, there have been extensive studies on parallel resource extraction from the web (e.g., Chen and Nie, 2000; Resnik 2003; Jiang et al., 2009) and many effective Web mining systems have been developed such as STRAND, PTMiner, BITS and WPDE. For most of these mining systems, there is a typical parallel resource mining strategy which involves three steps: (1) locate the bilingual websites (2) identify parallel web pages from these bilingual websites and (3) extract bilingual resources from the parallel web pages.\nIn this paper, we focus on the step (2) which is regarded as the core of the mining system (Chunyu, 2007). Estimating the translation similarity of two pages is the most basic and key problem in this step. Previous approaches have tried to tackle this problem by using the information within the pages. For example, in the STRAND and PTMiner system, a structural filtering process that relies on the analysis of the underlying HTML structure of pages is used to determine a set of pair-specific structural values, and then the values are used to decide whether the pages are translations of one another. The BITS system filters out bad pairs by using a large bilingual dictionary to compute a content-based similarity score and comparing the score with a threshold. The WPDE system combines URL similarity, structure similarity with content-based similarity to discover and verify candidate parallel page pairs. Some other features or rules such as page size ratio, predefined hypertexts which link to different language versions of a web page are also used in most of these systems. Here, all of the mining systems are simply using the information within the page in the process of find-\n1216\ning parallel web pages. In this paper, we attempt to explore other information to identify parallel web pages.\nOn the Internet, most web pages are linked by hyperlinks. We argue that the translation similarity of two pages depends on not only their internal information but also their neighbors. The neighbors of a web page are a set of pages, which link to the page. We find that the similarity of neighbors can provide more reliable evidence in estimating the translation similarity of two pages.\nThe main issues are discussed in this paper as\nfollows:\n Can the neighbors of candidate page pairs\nreally contribute to estimating the translation similarity?\n How to estimate the translation similarity of\ncandidate page pairs by using their neighbors?\nOur method has the following advantages:\nHigh performance\nThe external and internal information is combined to verify parallel page pairs in our method, while in previous mining systems, only internal information was used. Experimental results show that compared with existing parallel page pair identification technologies, our method obtains both higher precision and recall (6.2% and 6.3% improvement than the baseline, respectively). In addition, the external information used in our method is a more effective feature than internal features alone such as structural similarity and content-based similarity.\nLanguage independent\nIn principle, our method is language independent and can be easily ported to new language pairs, except for the language-specific bilingual lexicons. Our method takes full advantage of the link information that is languageindependent. For the bilingual lexicons in our experiments, compared to previous methods, our method does not need a big bilingual lexicon, which is good news to less-resource language pairs.\nUnsupervised and fewer parameters\nIn previous work, some parameters need to be optimized. Due to the diversity of web page styles, it is not trivial to obtain the best parameters. Some previous researches(Resnik, 2003; Zhang et al., 2006) attempt to optimize parameters by employing machine learning method. In contrast, in our method, only two parameters\nneed to be estimated. One parameter remains stable for different style websites. Another parameter can be easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters."
        },
        {
            "heading": "2 Related Work",
            "text": "A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows:\nMining from bilingual websites\nMost existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by analyzing the HTML pages. PTI crawls the web to fetch (potentially parallel) candidate multilingual web documents by using a web spider. To determine the parallelism between potential document pairs, a filename comparison module is used to check filename resemblance, and a content analysis module is used to measure the semantic similarity. BITS was the first to obtain bilingual websites by employing a language identification module, and then for each bilingual website, it extracts parallel pages based on their content.\nMining from bilingual web pages\nParallel/bilingual resources may exist not only in two parallel monolingual web pages, but also in single bilingual web pages. Jiang et al. (2009) used an adaptive pattern-based method to mine interesting bilingual data based on the observation that bilingual data usually appears collectively following similar patterns. They found that bilingual web pages are a promising source of up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method\nto automatically acquire bilingual web pages from the result pages of a search engine.\nMining from comparable corpus\nSeveral attempts have been made to extract parallel resources from comparable corpora. Zhao et al. (2002) proposed a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection. In their method, sentence length models and lexiconbased models were combined under a maximum likelihood criterion. Smith et al. (2010) found that Wikipedia contains a lot of comparable documents, and adopted a ranking model to select parallel sentence pairs from comparable documents. Bharadwaj et al. (2011) used a SVM classifier with some new features to identify parallel sentences from Wikipedia."
        },
        {
            "heading": "3 Iterative Link-based Parallel Web Pages Mining",
            "text": "As mentioned, the basic idea of our method is that the similarity of two pages can be inferred from their neighbors. This idea is illustrated in Figure 1.\nIn Figure 1, A, B, C, D and E are some pages in the same language; while A\u2019, B\u2019, C\u2019, D\u2019 and E\u2019 are some pages in another language. The solid black arrows indicate the links between these pages. For example, page A points to C, page B\u2019 points to C\u2019 and so on. Then the page set {A, B, D, E} is called the neighbors of page C. Similarly, the page set {A\u2019, B\u2019, D\u2019, E\u2019} contains the neighbors of page C\u2019. If the page pairs : <A, A\u2019>, <B, B\u2019>, <D, D\u2019> and <E, E\u2019> have high translation similarities, then it can be inferred that page C and C\u2019 have a high probability to be a pair of parallel pages. Every page has its own neighbors. For each web page, our method views link-in and link-out hyperlinks as the same. Thus, the linked pages will influence each other in estimating the translation similarity. For example, the similarities of two pairs <A, A\u2019> and <C, C\u2019> will influence each other. It is an iterative process. We\nwill elaborate the process in the following sections.\nSince our goal is to find parallel pages in a specific website, the key task is to evaluate the translation similarity of two pages (which are in different languages) as accurately as possible. The final similarity of two pages should depend both on their internal similarity and external similarity. The internal similarity means the similarity estimated by using the information in the page itself, such as the structure similarity and the content-based similarity of the two pages. On the other hand, the external similarity of two pages is the similarity depending on their neighbors. The final translation similarity is called the Enhanced Translation Similarity (ETS). The ETS of two pages can be calculated as follows:\n( ) ( ) ( ) ( ) [ ] (1)\nWhere, ( ) is the internal translation similarity of two pages: e and c; ( ) represents the external translation similarity of pages e and c. ( ) indicates the final similarity of two pages, which combines the internal with external translation similarity.\nIn this paper, we conduct the experiments on English-Chinese parallel page pair mining. However, our method is language-independent. Thus, it can be applied to other language pairs by only replacing a bilingual lexicon. The symbol e and c always indicate an English page and a Chinese page respectively in this paper. In the following sections, we will describe how to calculate the\n( ) and ( ) step by step."
        },
        {
            "heading": "3.1 Preprocessing",
            "text": "The input of our method is a bilingual website. This paper aims to find English/Chinese parallel pages. So a 3-gram language model is used to identify (or classify) the language of a certain document. The performance of the language identification module achieves 99.5% accuracy through in-house testing. As a result, a set of English pages and a set of Chinese pages are obtained. In order to get the neighbors of a page, for each bilingual website, two networks are constructed based on the hyperlinks, one for English pages and another for Chinese pages."
        },
        {
            "heading": "3.2 The Internal Translation Similarity",
            "text": "Following Resnik and Smith (2003), three features are used to evaluate the internal translation similarity of two pages:\nThe size ratio of two pages\nThe length ratio of two documents is the simplest criterion for determining whether two documents are parallel or not. Parallel documents tend to be similar in length. And it is reasonable to assume that for text E in one language and text F in another language, length(E) \u2248 C\u2022length(F), where C is a constant that depends on the language pair. Here, the content length of a web page is regarded as its length.\nThe structure similarity of two pages\nThe HTML tags describe and control a web page\u2019s structure. Therefore, the structure similarity of two pages can be calculated by their HTML tags. Here, the HTML tags of each page are extracted (except the visual tags such as \u201cB\u201d, \u201cFONT\u201d.) as a linear sequence. Then the structure similarity of two pages is computed by comparing their linearized sequences. In this paper, the LCS algorithm (Dan, 1997) is adopted to find the longest common sequences of the two HTML tag sequences. The ratio of LCS length and the average length of two HTML tag sequences are used as the structure similarity of the two pages.\nThe content-based translation similarity of\ntwo pages\nThe basic idea is that if two documents are parallel, they will contain word pairs that are mutual translations (Ma, 1999). So the percentage of translation word pairs in the two pages can be considered as the content-based similarity. The translation words of two documents can be extracted by using a bilingual lexicon. Here, for each word in English document, we will try to find a corresponding word in Chinese document.\nFinally, the internal translation similarity of\ntwo pages is calculated as follows:\n( ) ( ) ( ) ( ) [ ] (2)\nWhere, ( ) and ( ) are the content-based and structural similarity of page and respectively. In addition, the size ratio of two pages is used to filter invalid page pairs."
        },
        {
            "heading": "3.3 The External and Enhanced Translation Similarity",
            "text": "As described above, the external translation similarity of two pages depends on their neighbors:\n( ) ( ( ) ( )) (3)\nWhere, PG(x), a set of pages, is the neighbors of page x. Obviously, the similarity of two sets relies on the similarity of the elements in the two sets. Here, the elements are namely web pages. So, ( ) equals to ( ( ) ( )), and\n( ( ) ( )) depends on ( )\n( belongs to ( ) ( ) , respectively)\nand ( ) . According to Equation (1), ( ) depends on ( ) and ( ) . Therefore, it is a process of iteration. ( ) will converge after a certain number of iterations. Thus, ( ) is defined as the enhanced similarity of page and after the i-th iteration,\nand the same is for ( ) and ( ( )\n( )) . ( ( ) ( )) is computed by the following algorithm:\nAlgorithm 1: Estimating the external translation similarity\nInput: ( ) ( ) Output: ( )\nProcedure:\n 0  ( )  ( ) While and are both not empty:\n ( ( ))\n + ( ) Remove from Remove from\n( ) ( ( ) ( ))\n( ( ) ( ) )\nAlgorithm 2 Estimating the enhanced translation similarity\nInput: , (the English and Chinese page set) Output: ( ) Initialization: Set ETS(e, c) random value or small value\nProcedure:\nLOOP:\nFor each in : For each in :\n( ) ( )\n( ) ( ) Parameters normalization\nUNTIL ( ) is stable\nAlgorithm 1 tries to find the real parallel pairs from ( ) and ( ). The similarity of ( ) and ( ) is calculated based on the similarity\nvalues of these pairs. Finally, ( ) is calculated by the following algorithm 2.\nIn Algorithm 2, the input and are English and Chinese page sets in a certain bilingual website. We use algorithm 2 to estimate the enhanced translation similarity."
        },
        {
            "heading": "3.4 Find the Parallel Page Pairs",
            "text": "At last, the enhanced translation similarity of every pair is obtained, and the parallel page pairs can be extracted in terms of these similarities:\nAlgorithm 3 Finding parallel page pairs\nInput: ( ) (or ) Output: Parallel Page Pairs List :\nProcedure:\nLOOP:\n( ( ))\nAdd to Remove from Remove from\nUNTIL size of > (or ( ) < )\nThis algorithm is similar to Algorithm 1 in each bilingual website. The input is an integer threshold which means that only top page pairs will be extracted in a certain website. It needs to be noted that is always less than and . While the input is another kind of threshold that is used for extracting page pairs with high translation similarity."
        },
        {
            "heading": "4 Experiments and Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Experimental setup",
            "text": "Our experiments focus on six bilingual websites. Most of them are selected from HK government websites. All the web pages were retrieved by using a web site download tool: HTTrack 1 . We notice that a small amount of pages doesn\u2019t always contain valuable contents. So, we put a threshold (100 bytes in our experiment) on the web pages' content to filter meaningless pages. In order to evaluate our method, the bilingual page pairs of each website are annotated by a human annotator. Finally, we got 23109 pages and 11684 bilingual page pairs in total for testing.\n1 http://www.httrack.com/\nThe basic information of these websites is listed in Table 1.\nIt\u2019s time-consuming to annotate whether two pages is parallel or not. Note that if a website contains N English pages and M Chinese pages, an annotator has to label N*M page pairs. To the best of our knowledge, there is no large scale and public parallel page pair dataset with human annotation. So we try to build a reliable and largescale dataset.\nIn our experiments, URL similarity is used to reduce the workload for annotation. For a certain website, firstly, we obtain its URL pattern between English and Chinese pages manually. For example, in the website \u201cwww.gov.hk\u201d, the URL pairs like:\nhttp://www.gov.hk/en/about/govdirectory/ (English)\nhttp://www.gov.hk/sc/about/govdirectory/ (Chinese)\nThe URL pairs always point to a pair of parallel pages. So <\u201d/en/\u201d,\u201d/sc/\u201d> is considered as a URL pattern that was used to find parallel pages. For the other URLs that can\u2019t match the pattern, we have to label them by hand. The column \u201cNo pattern pairs\u201d in Table 1 shows that the number of parallel page pairs which mismatch any patterns.\nEach website listed in Table 1 has a URL pattern for most parallel web pages. Some previous researches used the URL similarity or patterns to find parallel page pairs. However, due to the diversity of web page styles and website maintenance mechanisms, bilingual websites adopt varied naming schemes for parallel documents (Shi, et al, 2006). The effect of URL pattern-based mining always depends on the style of website. In order to build a large dataset, the URL pattern is not used in our method. Our method is able to handle bilingual websites without URL pattern rules.\nIn addition, an English-Chinese dictionary with 64K words pairs is used in our experiments. Algorithm 3 needs a threshold or\n. It is very hard to tune the because it varies a lot in different websites and language pairs. However, Table 1 shows that the number of parallel pages is smaller than that of English and Chinese pages. Here, for each website, the is set to the number of Chinese pages (which is always smaller than that of English pages). In this way, the precision will never reach 100%, but it is more practical in a real application. As a result, in some experiments, we only report the F-score, and the precision and recall can be calculated as follows:\n( )\n(4)\n( )\n(5)\nWhere, for each website is listed in the \u201cTotal pairs\u201d column of Table 1."
        },
        {
            "heading": "4.2 Results and Analysis",
            "text": "Performance of the Baseline\nLet\u2019s start by presenting the performance of a baseline method as follows. The baseline only employs the internal translation similarity for parallel web pages mining. Algorithm 3 is also used to get the page pairs in baseline system. Here, the input ( ) is replaced by ( ) . The parameter in Equation 2 is a discount factor. For different values, the performance of baseline system on six websites is shown in Figure 2. In the Figure 2, it shows that when is set to 0.6, the baseline system achieves the best performance. The precision, recall and F-score are 85.84%, 87.55% and 86.69% respectively. So in the following experiments, we al-\nways set  to 0.6.\nPerformance of Our Method\nAs described in Section 3, our method combines the internal with external translation similarity in estimating the final translation similarity (i.e., ETS) of two pages. So, the discount factor\nin Equation (1) is important in our method. Besides, as shown in Algorithm 2, the iterative algorithm is used to calculate the similarity. Then, one question is that how many iterations are required in our algorithm. Figure 3 shows the performance of our method on each website. Its horizontal axis represents the number of iterations and the vertical axis represents the F-score. And for each website, the F-scores with different (range from 0.2 to 0.8) are also reported in this figure. From Figure 3, it is very easy to find that the best iteration number is 3. For almost all the websites, the performance of our method achieves the maximal values and converges after the third iteration. In addition, Figure 3 also indicates that our method is robust for different websites. In the following experiments, the iteration number is set to 3.\nNext, let\u2019s turn to the discount factor . Figure 4 reports the experimental results on the whole dataset. Here, the horizontal axis represents the discount factor and the vertical axis represents the F-score. means that only the internal similarity is used in the algorithm, so the F-score equals to that in Figure 2 when . On the contrary, means that only the external similarity is used in the method, and the F-score is 80.20%. The performance is lower than the baseline system when only the external link information is used, but it is much better than the performance of the content-based method and structure-based method whose F-scores are 64.82% and 64.0% respectively. Besides, it is shown from Figure 4, the performance is improved significantly when the internal and external similarity measures are combined together. Furthermore, it is somewhat surprising that the discount factor\nis not important as we previously expected. In fact, if we discard the cases that equals to 0 or 1, the difference between the maximum and minimum F-score will be 0.76% which is very small. This finding indicates that the internal and external similarity can easily be combined and we don\u2019t need to make many efforts to tune this parameter when our method is applied to other websites. The reason of this phenomenon is that, no matter how much weight (i.e., 1- ) was assigned to the internal similarity, the internal similarity always provides a relatively good initial\niterative direction. In the following experiments,\nthe parameter  is set to 0.6.\nThe weight of pages\nThe weight of the neighbor pages should also\nbe considered. For example, in the most websites, it is very common that most of the web pages contain a hyperlink which points to the homepage of the website. While in most of the English/Chinese websites, almost every English page will link to the English homepage and each Chinese page will point to Chinese homepage. The English and Chinese homepages are probably parallel, but they will be helpless to find parallel web pages, because they are neighbors of almost every page in the site. On the contrary, sometimes the parallel homepages have negative effects on finding parallel pages They will increase the translation similarity of two pages which are\nnot indeed mutual translations. So it is necessary to amend the Algorithm 1.\nThe weight of each page is calculated accord-\ning to its popularity:\n( )\n( ) (6)\nwhere ( ) indicates the weight of page , is the number of all pages, ( ) is the number of pages pointing to page and is a constant for smoothing.\nIn this paper, the weights of pages are used in\ntwo ways:\nWeight 1: The 9th line of Algorithm 1 is\namended by the page weight as follows:\n ( ) ( ( ) ( )) Weight 2: The pages with low weight are re-\nmoved from the input of Algorithm 1.\nThe experiment results are shown in Table 2.\nSurprisingly, no big differences are found after the introduction of the page weight. The side effect of popular pages is not so large in our method. In the neighbor pages of a certain page, the popular pages are the minority. Besides, the iterative process makes our method more stable and robust.\nThe impact of the size of bilingual lexicon\nThe baseline system mainly combines the content-based similarity with structure similarity.\nAnd two kinds of similarity measures are also used in our method. As Ma and Liberman (1999) pointed out, not all translators create translated pages that look like the original page which means that the structure similarity does not always work well. Compared to the structure similarity, the content-based is more reliable and has wider applicability. Furthermore, the bilingual lexicon is the only information that relates to the language pairs, and other features (such as structure and link information) are all language independent. So, it\u2019s important to investigate the effect of lexicon size in our method. We test the performance of our method with different size of the bilingual dictionary. The experiment results are shown in Figure 5. In this figure, the horizontal axis represents the bilingual lexicon size and the vertical axis represents the F-score. With the decline of the lexicon size, the performances of both the baseline method and our method are decreased. However, we can find that the descent rate of our method is smaller than that of the baseline. It indicates that our method does not need a big bilingual lexicon which is good news for the low-resource language pairs.\nError analysis\nErrors occur when the two pages are similar in terms of structure, content and their neighbors. For example, Figure 6 illustrates a typical web page structure. There are 5 parts in the web page:\n, , , and . Part always contains the main content of this page. While part , , and always contain some hyperlinks such as \u201chome\u201d in part and \u201cAbout us\u201d in part . Links in and sometimes relate to the content of the page. For such a kind of non-parallel page pairs, let\u2019s assume that the two pages have the same structure (as shown in Figure 6). In addition, their content part is very short and contains the\nsame or related topics. As a result, the links in other 4 parts are likely to be similar. In this case, our method is likely to regard the two pages as parallel.\nThere are about 920 errors when our system obtains its best performance. By carefully investigating the error page pairs, we find that more than 90% errors fall into the category discussed above. The websites used in our experiments mainly come from Hong Kong government websites. Some government departments regularly publish quarterly or monthly work reports on one issue through their websites. These reports look very similar except the publish date and some data in them. The other 10% errors happen because of the particularity of the web pages, e.g. very short pages, broken pages and so on."
        },
        {
            "heading": "5 Conclusions and Future Work",
            "text": "Parallel corpora are valuable resources for a lot of NLP research problems and applications, such as MT and CLIR. This paper introduces an efficient and effective solution to bilingual language processing. We first explore how to extract parallel page pairs in bilingual websites with link information between web pages. Firstly, we hypothesize that the translation similarity of pages should be based on both internal and external translation similarity. Secondly, a novel iterative method is proposed to verify parallel page pairs. Experimental results show that our method is much more effective than the baseline system with 6.2% improvement on F-Score. Furthermore, our method has some significant contributions. For example, compared to previous work, our method does not depend on bilingual lexicons, and the parameters in our method have little effect on the final performance. These features improve the applicability of our method.\nIn the future work, we will study some method on extracting parallel resource from existing parallel page pairs, which are challenging tasks due to the diversity of page structures and styles. Besides, we will evaluate the effectiveness of our mined data on MT or other applications."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research work has been sponsored by National Natural Science Foundation of China (Grants No.61373097 and No.61272259), one National Natural Science Foundation of Jiangsu Province (Grants No.BK2011282), one Major Project of College Natural Science Foundation of Jiangsu Province (Grants No.11KJA520003) and one National Science Foundation of Suzhou City (Grants No.SH201212).\nThe corresponding author of this paper, according to the meaning given to this role by School of computer science and technology at Soochow University, is Yu Hong\nReference\nChen, Jiang and Jianyun Nie. 2000. Automatic con-\nstruction of parallel English-Chinese corpus for cross-language information retrieval. Proceedings of the sixth conference on Applied Natural Language Processing, 21\u201328.\nResnik, Philip and Noah A. Smith. 2003. The Web as\na Parallel Corpus. Meeting of the Association for Computational Linguistics 29(3). 349\u2013380.\nKit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelli-\ngent Web Agent to Mine Bilingual Parallel Pages via Automatic Discovery of URL Pairing Patterns. Web Intelligence and Intelligent Agent Technology Workshops, 526\u2013529.\nZhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines.\n2006. Automatic Acquisition of Chinese-English Parallel Corpus from the Web. Joint Proceedings of the Association for Computational Linguistics and the International Conference on Computational Linguistics, 420\u2013431.\nNie, Jianyun, Michel Simard, Pierre Isabelle and\nRichard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 74\u201381.\nMa, Xiaoyi and Mark Y. Liberman. 1999. BITS: A\nMethod for Bilingual Text Search over the Web. Machine Translation Summit VII.\nChen, Jisong, Rowena Chau and Chung-Hsing Yeh.\n2004. Discovering Parallel Text from the World Wide Web. The Australasian Workshop on Data Mining and Web Intelligence, vol. 32, 157\u2013161. Dunedin, New Zealand.\nJiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu\nand Qingsheng Zhu. 2009. Mining Bilingual Data from the Web with Adaptively Learnt Patterns. Proceedings of the Joint Conference of the 47th\nAnnual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, vol. 2, 870\u2013878.\nYanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin\nYao and Qiaoming Zhu. 2010. A novel method for bilingual web page acquisition from search engine web records. Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 294\u2013302.\nZhao, Bing and Stephan Vogel. 2002. Adaptive Paral-\nlel Sentences Mining from Web Bilingual News Collection. IEEE International Conference on Data Mining, 745\u2013748.\nSmith, Jason R., Chris Quirk and Kristina Toutanova.\n2010. Extracting parallel sentences from comparable corpora using document level alignment. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 403\u2013 411.\nBharadwaj, Rohit G. and Vasudeva Varma. 2011.\nLanguage independent identification of parallel sentences using wikipedia. Proceedings of the 20th International Conference Companion on World Wide Web, 11\u201312. Hyderabad, India.\nGusfield, Dan. 1997. Algorithms on Strings, Trees\nand Sequences: Computerss Science and Computational Biology. Cambridge University Press\nShi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao.\n2006. A DOM Tree Alignment Model for Mining Parallel Data from the Web. Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, 489\u2013496."
        }
    ],
    "title": "An Iterative Link-based Method for Parallel Web Page Mining",
    "year": 2014
}