{
    "abstractText": "Information-theoretic quantities reveal dependencies among variables in the structure of joint, marginal, and conditional entropies while leaving certain fundamentally different systems indistinguishable. Furthermore, there is no consensus on the correct higher-order generalisation of mutual information (MI). In this manuscript, we show that a recently proposed model-free definition of higher-order interactions among binary variables (MFIs), like mutual information, is a M\u00f6bius inversion on a Boolean algebra, except of surprisal instead of entropy. This provides an information-theoretic interpretation to the MFIs, and by extension to Ising interactions. We study the objects dual to mutual information and the MFIs on the order-reversed lattices. We find that dual MI is related to the previously studied differential mutual information, while dual interactions are interactions with respect to a different background state. Unlike (dual) mutual information, interactions and their duals uniquely identify all six 2-input logic gates, the dyand triadic distributions, and different causal dynamics that are identical in terms of their Shannon information content.",
    "authors": [
        {
            "affiliations": [],
            "name": "Abel Jansma"
        }
    ],
    "id": "SP:7ba0b2b79e60daf6945fbe8e4f1fd42f51c9e995",
    "references": [
        {
            "authors": [
                "S. Ghazanfar",
                "Y. Lin",
                "X. Su",
                "D.M. Lin",
                "E. Patrick",
                "Z.G. Han",
                "J.C. Marioni",
                "J.Y.H. Yang"
            ],
            "title": "Investigating higher-order interactions in single-cell data with scHOT",
            "venue": "Nat. Methods",
            "year": 2020
        },
        {
            "authors": [
                "T.R. Lezon",
                "J.R. Banavar",
                "M. Cieplak",
                "A. Maritan",
                "N.V. Fedoroff"
            ],
            "title": "Using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2006
        },
        {
            "authors": [
                "J. Watkinson",
                "K.C. Liang",
                "X. Wang",
                "T. Zheng",
                "D. Anastassiou"
            ],
            "title": "Inference of regulatory gene interactions from expression data using three-way mutual information",
            "venue": "Ann. N. Y. Acad. Sci",
            "year": 2009
        },
        {
            "authors": [
                "E. Kuzmin",
                "B. VanderSluis",
                "W. Wang",
                "G. Tan",
                "R. Deshpande",
                "Y. Chen",
                "M. Usaj",
                "A. Balint",
                "M.M. Usaj",
                "J Van Leeuwen"
            ],
            "title": "Systematic analysis of complex genetic interactions",
            "venue": "Science",
            "year": 2018
        },
        {
            "authors": [
                "D.M. Weinreich",
                "Y. Lan",
                "C.S. Wylie",
                "R.B. Heckendorn"
            ],
            "title": "Should evolutionary geneticists worry about higher-order epistasis",
            "venue": "Curr. Opin. Genet. Dev. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "D. Panas",
                "A. Maccione",
                "L. Berdondini",
                "M.H. Hennig"
            ],
            "title": "Homeostasis in large networks of neurons through the Ising model\u2014do higher order interactions matter",
            "venue": "BMC Neurosci",
            "year": 2013
        },
        {
            "authors": [
                "G. Tka\u010dik",
                "O. Marre",
                "D. Amodei",
                "E. Schneidman",
                "W. Bialek",
                "M.J. Berry"
            ],
            "title": "Searching for Collective Behavior in a Large Network of Sensory Neurons",
            "venue": "PLoS Comput. Biol",
            "year": 2014
        },
        {
            "authors": [
                "E. Ganmor",
                "R. Segev",
                "E. Schneidman"
            ],
            "title": "Sparse low-order interaction network underlies a highly correlated and learnable neural population code",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2011
        },
        {
            "authors": [
                "S. Yu",
                "H. Yang",
                "H. Nakahara",
                "G.S. Santos",
                "D. Nikoli\u0107",
                "D. Plenz"
            ],
            "title": "Higher-order interactions characterized in cortical activity",
            "venue": "J. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "M. Gatica",
                "R. Cofr\u00e9",
                "P.A. Mediano",
                "F.E. Rosas",
                "P. Orio",
                "I. Diez",
                "S.P. Swinnen",
                "J.M. Cortes"
            ],
            "title": "High-order interdependencies in the aging brain",
            "venue": "Brain Connect. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "A. Sanchez"
            ],
            "title": "Defining Higher-Order Interactions in Synthetic Ecology: Lessons from Physics and Quantitative Genetics",
            "venue": "Cell Syst. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Grilli",
                "G. Barab\u00e1s",
                "M.J. Michalska-Smith",
                "S. Allesina"
            ],
            "title": "Higher-order interactions stabilize dynamics in competitive network models",
            "venue": "Nature 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Li",
                "M.M. Mayfield",
                "B. Wang",
                "J. Xiao",
                "K. Kral",
                "D. Janik",
                "J. Holik",
                "C. Chu"
            ],
            "title": "Beyond direct neighbourhood effects: Higher-order interactions improve modelling and predicting tree survival and growth",
            "venue": "Natl. Sci. Rev. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "E. Tekin",
                "C. White",
                "T.M. Kang",
                "N. Singh",
                "M. Cruz-Loya",
                "R. Damoiseaux",
                "V.M. Savage",
                "P.J. Yeh"
            ],
            "title": "Prevalence and patterns of higher-order drug interactions in Escherichia coli",
            "venue": "NPJ Syst. Biol. Appl. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "U. Alvarez-Rodriguez",
                "F. Battiston",
                "G.F. de Arruda",
                "Y. Moreno",
                "M. Perc",
                "V. Latora"
            ],
            "title": "Evolutionary dynamics of higher-order interactions in social networks",
            "venue": "Nat. Hum. Behav. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "G. Cencetti",
                "F. Battiston",
                "B. Lepri",
                "M. Karsai"
            ],
            "title": "Temporal properties of higher-order interactions in social networks",
            "venue": "Sci. Rep. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M. Grabisch",
                "M. Roubens"
            ],
            "title": "An axiomatic approach to the concept of interaction among players in cooperative games",
            "venue": "Int. J. Game Theory 1999,",
            "year": 1999
        },
        {
            "authors": [
                "H. Matsuda"
            ],
            "title": "Physical nature of higher-order mutual information: Intrinsic correlations and frustration",
            "venue": "Phys. Rev. E 2000,",
            "year": 2000
        },
        {
            "authors": [
                "N.J. Cerf",
                "C. Adami"
            ],
            "title": "Entropic bell inequalities",
            "venue": "Phys. Rev. A 1997,",
            "year": 1997
        },
        {
            "authors": [
                "F. Battiston",
                "E. Amico",
                "A. Barrat",
                "G. Bianconi",
                "G. Ferraz de Arruda",
                "B. Franceschiello",
                "I. Iacopini",
                "S. K\u00e9fi",
                "V. Latora",
                "Y Moreno"
            ],
            "title": "The physics of higher-order interactions in complex systems",
            "venue": "Nat. Phys. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "P.S. Skardal",
                "A. Arenas"
            ],
            "title": "Higher order interactions in complex networks of phase oscillators promote abrupt synchronization switching",
            "venue": "Commun. Phys. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "L. Merchan",
                "I. Nemenman"
            ],
            "title": "On the Sufficiency of Pairwise Interactions in Maximum Entropy Models of Networks",
            "venue": "J. Stat. Phys. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "G. Tkacik",
                "E. Schneidman",
                "Berry",
                "M.J",
                "II",
                "W. Bialek"
            ],
            "title": "Ising models for networks of real neurons",
            "year": 2006
        },
        {
            "authors": [
                "A.A. Margolin",
                "I. Nemenman",
                "K. Basso",
                "C. Wiggins",
                "G. Stolovitzky",
                "R.D. Favera",
                "A. Califano"
            ],
            "title": "ARACNE: An algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context",
            "venue": "BMC Bioinform",
            "year": 2006
        },
        {
            "authors": [
                "I. Nemenman"
            ],
            "title": "Information theory, multivariate dependence, and genetic network inference",
            "venue": "arXiv 2004,",
            "year": 2004
        },
        {
            "authors": [
                "S. Watanabe"
            ],
            "title": "Information theoretical analysis of multivariate correlation",
            "venue": "IBM J. Res. Dev. 1960,",
            "year": 1960
        },
        {
            "authors": [
                "F.E. Rosas",
                "P.A. Mediano",
                "A.I. Luppi",
                "T.F. Varley",
                "J.T. Lizier",
                "S. Stramaglia",
                "H.J. Jensen",
                "D. Marinazzo"
            ],
            "title": "Disentangling high-order mechanisms and high-order behaviours in complex systems",
            "venue": "Nat. Phys. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "P.L. Williams",
                "R.D. Beer"
            ],
            "title": "Nonnegative decomposition of multivariate information",
            "venue": "arXiv 2010,",
            "year": 2010
        },
        {
            "authors": [
                "M. Wibral",
                "V. Priesemann",
                "J.W. Kay",
                "J.T. Lizier",
                "W.A. Phillips"
            ],
            "title": "Partial information decomposition as a unified approach to the specification of neural goal functions",
            "venue": "Brain Cogn. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "E.T. Jaynes"
            ],
            "title": "Information theory and statistical mechanics",
            "venue": "Phys. Rev",
            "year": 1957
        },
        {
            "authors": [
                "H.C. Nguyen",
                "R. Zecchina",
                "J. Berg"
            ],
            "title": "Inverse statistical problems: From the inverse Ising problem to data science",
            "venue": "Adv. Phys. 2017,",
            "year": 1873
        },
        {
            "authors": [
                "S.V. Beentjes",
                "A. Khamseh"
            ],
            "title": "Higher-order interactions in statistical physics and machine learning: A model-independent solution to the inverse problem at equilibrium",
            "venue": "Phys. Rev. E",
            "year": 2020
        },
        {
            "authors": [
                "G.F. Glonek",
                "P. McCullagh"
            ],
            "title": "Multivariate logistic models",
            "venue": "J. R. Stat. Soc. Ser. B (Methodol.) 1995,",
            "year": 1995
        },
        {
            "authors": [
                "F. Bartolucci",
                "R. Colombi",
                "A. Forcina"
            ],
            "title": "An extended class of marginal link functions for modelling contingency tables by equality and inequality constraints",
            "venue": "Stat. Sin",
            "year": 2007
        },
        {
            "authors": [
                "G. Bateson"
            ],
            "title": "Steps to an Ecology of Mind",
            "venue": "Chandler Publishing Company: San Francisco,",
            "year": 1972
        },
        {
            "authors": [
                "R.P. Stanley"
            ],
            "title": "Enumerative Combinatorics Volume 1 Second Edition; Cambridge Studies in Advanced Mathematics",
            "year": 2011
        },
        {
            "authors": [
                "G.C. Rota"
            ],
            "title": "On the foundations of combinatorial theory I",
            "venue": "Theory of Mo\u0308bius functions. Z. Fu\u0308r Wahrscheinlichkeitstheorie Verwandte Geb. 1964,",
            "year": 1964
        },
        {
            "authors": [
                "A.J. Bell"
            ],
            "title": "The co-information lattice",
            "venue": "In Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation,",
            "year": 2003
        },
        {
            "authors": [
                "D.J. Galas",
                "N.A. Sakhanenko"
            ],
            "title": "Symmetries among multivariate information measures explored using M\u00f6bius operators",
            "venue": "Entropy 2019,",
            "year": 2019
        },
        {
            "authors": [
                "D.J. Galas",
                "N.A. Sakhanenko",
                "A. Skupin",
                "T. Ignac"
            ],
            "title": "Describing the complexity of systems: Multivariable \u201cset complexity\u201d and the information basis of systems biology",
            "venue": "J. Comput. Biol",
            "year": 2014
        },
        {
            "authors": [
                "D.J. Galas",
                "J. Kunert-Graf",
                "L. Uechi",
                "N.A. Sakhanenko"
            ],
            "title": "Towards an information theory of quantitative genetics",
            "venue": "J. Comput. Biol. 2019,",
            "year": 1195
        },
        {
            "authors": [
                "Y. Freund",
                "D. Haussler"
            ],
            "title": "Unsupervised learning of distributions on binary vectors using two layer networks. Adv",
            "venue": "Neural Inf. Process. Syst. 1991,",
            "year": 1991
        },
        {
            "authors": [
                "N. Le Roux",
                "Y. Bengio"
            ],
            "title": "Representational power of restricted Boltzmann machines and deep belief networks",
            "venue": "Neural Comput. 2008,",
            "year": 2008
        },
        {
            "authors": [
                "G. Montufar",
                "N. Ay"
            ],
            "title": "Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines",
            "venue": "Neural Comput. 2011,",
            "year": 2011
        },
        {
            "authors": [
                "G. Cossu",
                "L. Del Debbio",
                "T. Giani",
                "A. Khamseh",
                "M. Wilson"
            ],
            "title": "Machine learning determination of dynamical parameters: The Ising model case",
            "venue": "Phys. Rev. B 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Krumsiek",
                "K. Suhre",
                "T. Illig",
                "J. Adamski",
                "F.J. Theis"
            ],
            "title": "Gaussian graphical modeling reconstructs pathway reactions from highthroughput metabolomics data",
            "venue": "BMC Syst. Biol",
            "year": 2011
        },
        {
            "authors": [
                "R.G. James",
                "J.P. Crutchfield"
            ],
            "title": "Multivariate dependence beyond Shannon information",
            "venue": "Entropy 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A. Jansma"
            ],
            "title": "Higher-Order Interactions in Single-Cell Gene Expression",
            "venue": "Ph.D. Thesis,",
            "year": 2023
        },
        {
            "authors": [
                "J. Pearl"
            ],
            "title": "Models, Reasoning and Inference",
            "year": 2000
        },
        {
            "authors": [
                "G.W. Imbens",
                "D.B. Rubin"
            ],
            "title": "Causal Inference in Statistics, Social, and Biomedical Sciences",
            "year": 2015
        },
        {
            "authors": [
                "T. Leinster"
            ],
            "title": "Notions of M\u00f6bius inversion",
            "venue": "Bull. Belg. Math. Soc.-Simon Stevin 2012,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Citation: Jansma, A. Higher-Order\nInteractions and their Duals Reveal\nSynergy and Logical Dependence\nBeyond Shannon-Information.\nEntropy 2023, 25, 648.\nhttps://doi.org/10.3390/e25040648\nAcademic Editor: Alessandro\nGiuliani\nReceived: 23 February 2023\nRevised: 6 April 2023\nAccepted: 7 April 2023\nPublished: 12 April 2023\nCopyright: \u00a9 2023 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: higher-order; information; entropy; synergy; triadic; M\u00f6bius inversions; Ising model; lattices\n1. Introduction 1.1. Higher-Order Interactions\nAll non-trivial structures in data or probability distributions correspond to dependencies among the different features, or variables. These dependencies can be present among pairs of variables, i.e., pairwise, or can be higher-order. A dependency, or interaction, is called higherorder if it is inherently a property of more than two variables and if it cannot be decomposed into pairwise quantities. The term has been used more generally to refer simply to complex interactions, as for example in [1] to refer to changes in gene co-expression over time; in this article, however, it is used only in the stricter sense defined in Section 2. The reason such higher-order structures are interesting is twofold. First, higher-order dependence corresponds to a fundamentally different kind of communication and interaction among the components of a system. If a system contains higher-order interactions, then its dependency structure cannot be represented by a graph and requires a hypergraph, where a single \u2018hyperedge\u2019 can connect more than two nodes. It is desirable to be able to detect and describe such systems accurately, which requires a good understanding of higher-order interactions. Second, higher-order interactions might play an important role in nature, and have been identified in various interaction networks, including genetic [2\u20135], neuronal [6\u201310], ecological [11\u201313], drug interaction [14], social [15\u201317], and physical [18,19] networks. Furthermore, there is evidence that higher-order interactions are responsible for the rich dynamics [20] or\nEntropy 2023, 25, 648. https://doi.org/10.3390/e25010648 https://www.mdpi.com/journal/entropy\nar X\niv :2\n20 5.\n04 44\n0v 2\n[ cs\n.I T\n] 8\nM ay\nEntropy 2023, 25, 648 2 of 30\nbistability [21] in biological networks; for example, synthetic lethality experiments have shown that the trigenic interactions in yeast form a larger network than the pairwise interactions [4]. Despite this, purely pairwise descriptions of nature have been remarkably successful, which the authors of [22,23] attribute to the fact that there are regimes in terms of the strength and density of coupling among the variables within which pairwise descriptions are sufficient. Alternatively, it may be attributed to the fact that higher-order interactions have been understudied and their effects underestimated. Currently, perhaps the most promising method of quantifying higher-order interactions is information theory. The two most commonly used quantities are mutual information and its higher-order generalisation (used in, e.g., [24,25]) and the total correlation (introduced in [26] and recently used in [27]). However, one particular problem of interest that total correlation and mutual information do not address is that of synergy and redundancy. Given a set of variables with an nth-order dependency, what part of that is exclusively nth-order (called the synergistic part), and what part can be found in a subset of m < n variables as well (the redundant part)? Quantifying the exact extent to which shared information is synergistic is an open problem, and is most commonly addressed using partial information decomposition [28], which has been applied mainly in the context of theoretical neuroscience [29]. In this article, a different more statistical approach to identifying synergy is taken, which is ultimately shown to be intimately related to information theory while offering significant advantages beyond classical entropy-based quantities.\n1.2. Model-Free Interactions and the Inverse Ising Problem\nIn 1957, E.T. Jaynes famously showed that statistical equilibrium mechanics can be seen as a maximum entropy solution to the inverse problem of constructing a probability distribution that best reproduces a sample distribution [30]. More precisely, the equilibrium dynamics of the (inhomogeneous, or glass-like) generalised Ising model with interactions up to the nth order arise naturally as the maximum entropy distribution compatible with a dataset after observing the first n moments among binary variables. This means that in order to reproduce the moments in the data in a maximally non-committal way, it is necessary to introduce higher-order interactions, i.e., terms that involve more than two variables, in the description of the system. Fitting such a generalised Ising model to data is nontrivial; while the log-likelihood of the Ising model is concave in the the coupling parameters, the cost of evaluating it is exponential in the total number of variables N, which is often intractable in practice [31]. In [32], the authors introduced an estimator of model-free interactions (MFIs) that exactly coincides with the solution to the inverse generalised Ising problem. Moreover, the cost of estimating all nth-order model-free interactions among N variables from M observations scales as O ( M \u00b7 (Nn ) ) = O(MNn) (i.e., polynomially) in the total system size N. However, this is true only when sufficient data is available. With limited data, certain interactions might require inferring the conditional dependencies from the data, which in the worst case scales exponentially in N again. The definition of MFIs offered in [32] seems to be a general one; in addition to offering a solution to the inverse generalised Ising problem, MFIs are expressible in terms of average treatment effects (ATEs) or regression coefficients. Throughout this article, the general term \u2018MFI\u2019 is used, and may be read simply as referring to the maximum entropy or Ising interaction.\n1.3. Outline\nIn Section 2.1, the definition of the MFIs is stated along with a number of their properties. To explicitly link the MFIs to information theory, a redefinition of mutual information in terms of M\u00f6bius inversions is provided in Section 2.2, which is then linked to a similar redefinition of the MFIs in Sections 3.1 and 3.2. A definition in terms of M\u00f6bius inversions naturally leads to dual definitions of all objects, which are subsequently explored in Section 3.3. Then, in Section 4,\nEntropy 2023, 25, 648 3 of 30\nsimple fundamental examples are used to demonstrate that MFIs can differentiate distributions that entropy-based quantities cannot. Finally, the results are summarised and reflected upon in Section 5.\n2. Background 2.1. Model-Free Interactions\nWe start by re-defining the interactions introduced in [32]. We define the isolated effect (or 1-point interaction) I(Y)i of a variable Xi \u2208 X on an observable Y as\nI(Y)i = \u2202Y \u2202Xi \u2223\u2223\u2223 X=0 , X = X \\ {Xi} (1)\nwhere the effect of Xi on Y is isolated by conditioning on all other variables being zero. This expression is well-defined, as the restriction of a derivative is the derivative of the restriction. A pair of variables Xi and Xj has a 2-point interaction I (Y) ij when the value of Xj changes the 1-point interaction of Xi on Y:\nI(Y)ij = \u2202I(Y)i \u2202Xj \u2223\u2223\u2223 X=0 = \u22022Y \u2202Xj\u2202Xi \u2223\u2223\u2223 X=0 , X = X \\ {Xi, Xj} (2)\nA third variable Xk can modulate this 2-point interaction through what we call a 3-point interaction, I(Y)ijk :\nI(Y)ijk = \u2202I(Y)ij \u2202Xk \u2223\u2223\u2223 X=0 = \u22023Y \u2202Xk\u2202Xj\u2202Xi \u2223\u2223\u2223 X=0 , X = X \\ {Xi, Xj, Xk} (3)\nThis process of taking derivatives with respect to an increasing number of variables can be repeated to define n-point interactions.\nDefinition 1 (n-point interaction with respect to outcome Y). Let p be a probability distribution over a set X of random variables Xi and let Y be a function Y : X \u2192 R. Then, the n-point interaction IX1 ...Xn between variables {X1, . . . , Xn} \u2286 X is provided by\nI(Y)X1 ...Xn = \u2202nY(X)\n\u2202X1 . . . \u2202Xn \u2223\u2223\u2223 X=0\n(4)\nwhere X = X \\ {X1, . . . Xn}.\nThis definition of interaction makes explicit the fact that interactions are defined with respect to some outcome. The authors of [32] refer to the interactions from Definition 1 as additive, which they distinguish from multiplicative interactions. However, when the outcome is chosen to be the log of the joint distribution p(X) over all variables X, then the additive and multiplicative interactions are equivalent and simply related through a logarithm [32]. Setting the outcome to be log p(X) has other nice properties as well. First, while probabilities are restricted to the non-negative reals, a log-transformation removes this restriction and makes the outcome and subsequent interactions take both positive and negative values, which can have different interpretations. Second, it is this outcome that makes the interactions interpretable as maximum entropy interactions, as they exactly coincide with Ising interactions. Finally, this can be considered the most general outcome possible, as all marginal and conditional probabilities\nEntropy 2023, 25, 648 4 of 30\nare encoded in this joint distribution. This leads to the following definition of a model-free interaction.\nDefinition 2 (model-free n-point interaction between binary variables). A model-free n-point interaction (MFI) is an n-point interaction between binary random variables with respect to the logarithm of their joint probability\nIX1 ...Xn := I (log p(X)) X1 ...Xn = \u2202n log p(X) \u2202X1 . . . \u2202Xn \u2223\u2223\u2223 X=0\n(5)\nwhere X = X \\ {X1, . . . Xn}.\nIf the variables Xi \u2208 X are binary, then a definition for a derivative with respect to a binary variable is needed.\nDefinition 3 (derivative of a function with respect to a binary variable). Let f : Bn \u2192 R be a real-valued function of a set X of n binary variables, labelled as Xi, 1 \u2264 i \u2264 n. Then, the derivative operator with respect to Xi acts on f (X) as follows:\n\u2202\n\u2202Xi f (X) = f (Xi = 1, X \\ Xi)\u2212 f (Xi = 0, X \\ Xi) (6)\nThe linearity of the derivative operator then immediately and uniquely defines the higher-order derivatives.\nUsing this definition, the n-point interactions become model-free in the sense that they are ratios of probabilities that do not involve the functional form of the joint probability distribution. For example, writing Xijk = (a, b, c) for (Xi = a, Xj = b, Xk = c), the first three orders can be written out as follows (recall that the notation \u2202\u2202Xi here refers to the derivative operator from Definition 3):\nIi = \u2202 log p(X)\n\u2202Xi\n\u2223\u2223\u2223 X=0\n= log p ( Xi = 1 | X = 0 )\np ( Xi = 0 | X = 0 ) (7)\nIij = \u22022 log p(X)\n\u2202Xj\u2202Xi \u2223\u2223\u2223 X=0\n= log p ( Xij = (1, 1) | X = 0 )\np ( Xij = (0, 1) | X = 0 )\np ( Xij = (0, 0) | X = 0 ) p ( Xij = (1, 0) | X = 0 ) (8)\nIijk = \u22023 log p(X) \u2202Xk\u2202Xj\u2202Xi \u2223\u2223\u2223 X=0 = log p ( Xijk = (1, 1, 1) | X = 0 )\np ( Xijk = (0, 0, 0) | X = 0 )\np ( Xijk = (1, 0, 0) | X = 0 ) p ( Xijk = (0, 1, 1) | X = 0 )\n\u00d7 p ( Xijk = (0, 1, 0) | X = 0 )\np ( Xijk = (1, 0, 1) | X = 0 )\np ( Xijk = (0, 0, 1) | X = 0 ) p ( Xijk = (1, 1, 0) | X = 0 ) (9)\nwhere Bayes\u2019 rule is used to replace joint probabilities with conditional probabilities. This definition of interaction has the following properties:\nEntropy 2023, 25, 648 5 of 30\n\u2022 It is symmetric in terms of the variables, as IS = I\u03c0(S) for any set of variables S and any permutation \u03c0. \u2022 Conditionally independent variables do not interact: Xi\u22a5\u22a5 Xj | X =\u21d2 Iij = 0. \u2022 If X = \u2205, the definition coincides with that of a log-odds ratio, which has already been considered as a measure of interaction in, e.g., [33,34]. \u2022 The interactions are model-free; no knowledge of the functional form of p(X) is required, and the probabilities can be directly estimated from i.i.d. samples. \u2022 The MFIs are exactly the Ising interactions in the maximum entropy model after observing\nmoments of the data. This can be readily verified by setting\np(s) = Z\u22121 exp(\u2211 n \u2211 i1,...,in Ji1 ...in si1 . . . sin)\nand using Definition 2.\nFurthermore, in Appendix A.2 the following two useful properties are introduced and proved:\n\u2022 An n-point interaction can only be non-zero if all n variables are in each other\u2019s minimal Markov blanket. \u2022 If X does not include the full complement of the interacting variables, the bias this induces in the estimate of the interaction is proportional to the pointwise mutual information of states where the omitted variables are 0.\n2.2. Mutual Information as a M\u00f6bius Inversion\nThe definition of an n-point interaction as a derivative of a derivative is reminiscent of Gregory Bateson\u2019s view of information as a difference which makes a difference [35]; however, the relationship between information theory and model-free interactions rests on more than a linguistic coincidence. It turns out that interactions and information are generalised derivatives of similar functions on Boolean algebras. To see this, consider the definition of pairwise mutual information and its third-order generalisation:\nMI(X, Y) = H(X)\u2212 H(X | Y) (10) = H(X) + H(Y)\u2212 H(X, Y) (11)\nMI(X, Y, Z) = MI(X, Y)\u2212MI(X, Y | Z) (12) = H(X) + H(Y) + H(Z)\n\u2212 H(X, Y)\u2212 H(X, Z)\u2212 H(Y, Z) + H(X, Y, Z)\nNote that all MI-based quantities can be written thusly as sums of marginal entropies of subsets of the set of variables. Given a finite set of variables S, its powerset P(S) can be assigned a partial ordering as follows:\na \u2264 b \u21d0\u21d2 a \u2286 b \u2200 a, b \u2208 P(S) (13)\nThis poset P = (P(S),\u2286) is called a Boolean algebra, and because each pair of sets has a unique supremum (their union) and infimum (their intersection), it is a lattice. This lattice structure is visualised for two and three variables in Figure 1. In general, the lattice of an n-variable Boolean algebra forms an n-cube. Furthermore, for any finite n, the n-variable Boolean algebra forms a bounded lattice, which means that it has a greatest element, denoted as 1\u0302, and a least element, denoted as 0\u0302.\nOn a poset P, we define the M\u00f6bius function \u00b5P : P\u00d7 P\u2192 R as\n\u00b5P(x, y) =    1 if x = y \u2212 \u2211 z:x\u2264z<y \u00b5P(x, z) if x < y\n0 otherwise\n(14)\nThis function type makes \u00b5P an element of the incidence algebra of P. In fact, \u00b5 is the inverse of the zeta function \u03b6 : \u03b6(x, y) = 1 iff x \u2264 y, and 0 otherwise. On a Boolean algebra, such as a powerset ordered by inclusion, the M\u00f6bius function takes the simple form \u00b5(x, y) = (\u22121)|x|\u2212|y| [36,37]. This definition allows the mutual information among a set of variables \u03c4 to be written as follows [38,39]:\nMI(\u03c4) = (\u22121)|\u03c4|\u22121 \u2211 \u03b7\u2264\u03c4 \u00b5P(\u03b7, \u03c4)H(\u03b7) (15)\n= \u2211 \u03b7\u2264\u03c4\n(\u22121)|\u03b7|+1H(\u03b7) (16)\nwhere P is the Boolean algebra with \u03c4 = 1\u0302 and H(\u03b7) is the marginal entropy of the set of variables \u03b7. Indeed, this coincides with Equation (11) for \u03c4 = {X, Y} and with Equation (13) for \u03c4 = {X, Y, Z}. Equation (15) is a convolution known as a M\u00f6bius inversion.\nDefinition 4 (M\u00f6bius inversion over a poset, Rota (1964) [37]). Let P be a poset (S,\u2264), let \u00b5 : P\u00d7 P \u2192 R be the M\u00f6bius function from Equation (14), and let g : P \u2192 R be a function on P. Then, the function\nf (y) = \u2211 x\u2264y \u00b5P(x, y)g(x) (17)\nis called the M\u00f6bius inversion of g on P. Furthermore, this equation can be inverted to yield\nf (y) = \u2211 x\u2264y \u00b5P(x, y)g(x) \u21d0\u21d2 g(y) = \u2211 x\u2264y f (x) (18)\nEntropy 2023, 25, 648 7 of 30\nThe M\u00f6bius inversion is a generalisation of the derivative to posets. If P = (N,\u2264), Equation (18) is just a discrete version of the fundamental theorem of calculus [36]. Equation (18) additionally implies that we can express joint entropy as a sum over mutual information:\nH(\u03c4) = (\u22121)|\u03c4|\u22121 \u2211 \u03b7\u2264\u03c4 MI(\u03b7) (19)\nFor example, in the case of three variables,\nH(X, Y, Z) = MI(X, Y, Z) + MI(X, Y) + MI(X, Z) + MI(Y, Z) + H(X) + H(Y) + H(Z) (20)\nInstead of starting with entropy, we could start with a quantity known as surprisal, or self-information, defined as the negative log probability of a certain state or realisation:\nS(X = x) =\u2212 log p(X = x) (21)\nSurprisal plays an important role in information theory; indeed, the expected surprisal across all possible realisations X = x is the entropy of the variable X:\nEX [S(X = x)] = H(X) (22)\nAs we are often interested in the marginal surprisal of a realisation X = x summed over Y, we can write this explicitly as\nlog p(x; Y) := \u2211 y log p(x, y) (23)\nWith this, consider the M\u00f6bius inversion of the marginal surprisal over the lattice P:\npmi(T = \u03c4) := (\u22121)|\u03c4| \u2211 \u03b7\u2264\u03c4 \u00b5P(\u03b7, \u03c4) log p(\u03b7; \u03c4 \\ \u03b7) (24)\nThis is a generalised version of the pointwise mutual information, which is usually defined on just two variables:\npmi(X = x, Y = y) = log(x, y; \u2205)\u2212 log(x; Y)\u2212 log(y; X) + log(\u2205; X, Y) (25)\n= log p(x, y)\np(x)p(y) (26)\nSummary\n\u2022 Mutual information is the M\u00f6bius inversion of marginal entropy. \u2022 Pointwise mutual information is the M\u00f6bius inversion of marginal surprisal.\n3. Interactions and Their Duals 3.1. MFIs as M\u00f6bius Inversions\nWith mutual information defined in terms of M\u00f6bius inversions, the same can be done for the model-free interactions. Again, we start with (negative) surprisal. However, on Boolean variables a state is just a partition of the variables into two sets: one in which the variables are set to 1, and another in which they are set to 0. That means that the surprisal of observing a\nEntropy 2023, 25, 648 8 of 30\nparticular state is completely specified by which variables X \u2286 Z are set to 1 while keeping all other variables Z \\ X at 0, which can be written as\nSX;Z := log p(X = 1, Z \\ X = 0) (27)\nDefinition 5 (interactions as M\u00f6bius inversions). Let p be a probability distribution over a set T of random variables and let P = (P(\u03c4),\u2286), the powerset of a set \u03c4 \u2286 T ordered by inclusion. Then, the interaction I(\u03c4; T) among variables \u03c4 is provided by\nI(\u03c4; T) := \u2211 \u03b7\u2264\u03c4 \u00b5P(\u03b7, \u03c4)S\u03b7;T (28)\n= \u2211 \u03b7\u2264\u03c4\n(\u22121)|\u03b7|\u2212|\u03c4| log p(\u03b7 = 1, T \\ \u03b7 = 0) (29)\nFor example, when \u03c4 contains a single variable X \u2286 T, then\nI({X}; T) = \u00b5P({X}, {X})S{X};T + \u00b5P(\u2205, {X})S\u2205;T (30)\n= log p(X = 1, T \\ X = 0) p(X = 0, T \\ X = 0) (31)\nwhich coincides with the 1-point interaction in Equation (7). Similarly, when \u03c4 contains two variables \u03c4 = {X, Y} \u2286 T, then\nI({X, Y}; T) = \u00b5P({X, Y}, {X, Y})S{X,Y};T + \u00b5P({X}, {X, Y})S{X};T (32) +\u00b5P({Y}, {X, Y})S{Y};T + \u00b5P(\u2205, {X, Y})S\u2205;T = log p(X = 1, Y = 1, T \\ {X, Y} = 0)p(X = 0, Y = 0, T \\ {X, Y} = 0) p(X = 1, Y = 0, T \\ {X, Y} = 0)p(X = 0, Y = 1, T \\ {X, Y} = 0) (33)\nwhich coincides with the 2-point interaction in Equation (8). In fact, this pattern holds in general.\nTheorem 1 (equivalence of interactions). The interaction I(\u03c4, T) from Definition 5 is the same as the model-free interaction I\u03c4 from Definition 2, that is, for any set of variables \u03c4 \u2286 T it is the case that\nI(\u03c4, T) = I\u03c4 (34)\nProof. We have to show that\n\u2211 \u03b7\u2264\u03c4\n(\u22121)|\u03b7|\u2212|\u03c4| log p(\u03b7 = 1, T \\ \u03b7 = 0) = \u2202 n log p(T)\n\u2202\u03c41 . . . \u2202\u03c4n \u2223\u2223\u2223 T=0\n(35)\nBoth sides of this equation are sums of \u00b1 log p(s), where s is some binary string; thus, we have to show that the same strings appear with the same sign. First, note that the Boolean algebra of sets ordered by inclusion (as in Figure 1) is equivalent to the poset of binary strings where for any two strings a and b, a \u2264 b \u21d0\u21d2 a \u2227 b = a. The equivalence follows immediately upon setting each element a \u2208 P(S) to the string where a = 1 and S \\ a = 0. This map is one-to-one and monotonic with respect to the partial order, as A \u2286 B \u21d0\u21d2 A \u2229 B = A. This means that Definition 5 can be rewritten as a M\u00f6bius inversion\nEntropy 2023, 25, 648 9 of 30\non the lattice of Boolean strings S = (B|\u03c4|,\u2264) (shown for the three-variable case on the left side of Figure 2):\nI(\u03c4; T) = \u2211 s\u22641\u0302S \u00b5S(s, 1\u0302S) log p(\u03c4 = s, T \\ \u03c4 = 0) (36)\nNote that for any pair (\u03b1, \u03c4) where \u03b1 \u2286 \u03c4 with respective string representations (s, t) \u2208 B|\u03c4| \u00d7B|\u03c4|, we have the following:\n|\u03c4| \u2212 |\u03b1| =\u2211 i (t \u2227 \u00acs)i (37)\nThus, we can write\nI(\u03c4; T) = \u2211 s\u22641\u0302S\n(\u22121)\u2211\u00acs log p(\u03c4 = s, T \\ \u03c4 = 0) (38)\nTo see that this exactly coincides with Definition 2, we can define a map\ne(n)i,s : FBn \u2192 FBn\u22121 (39)\nwhere FBn is the set of functions from n Boolean variables to R. This map is defined as\ne(n)i,s : f (X1, . . . Xi, . . . Xn) 7\u2192 f (X1, . . . Xi = s, . . . Xn) (40)\nWith this map, the Boolean derivative of a function f (X1, . . . , Xn) (see Definition 3) can be written as\n\u2202\n\u2202Xi f (X) = (e(n)i,1 \u2212 e (n) i,0 ) f (X) (41)\n= f (X1, . . . , Xi = 1, . . . , Xn)\u2212 f (X1, . . . , Xi = 0, . . . , Xn) (42)\nIn this way, the derivative with respect to a set S of m variables becomes function composition: (\nm\n\u220f i=0\n\u2202\n\u2202XSi\n) f (X) = ( \u00a9mi=0(e (n\u2212i) Si ,1 \u2212 e(n\u2212i)Si ,0 ) ) f (X) (43)\nFrom this, it is clear that a term f (s) appears with a minus sign iff e(n)i,0 has been applied an odd number of times. Therefore, terms for which s contains an odd number of 0s receive a minus sign. This can be summarised as\n( m\n\u220f i=0\n\u2202\n\u2202XSi\n) f (X) = \u2211\ns\u2208Bn (\u22121)\u2211\u00acs f (XS = s, X \\ XS) (44)\nTherefore, we can write\nI\u03c4 = \u2211 s\u2208Bn\n(\u22121)\u2211\u00acs log(\u03c4 = s, T \\ \u03c4 = 0) (45)\nThe sums \u2211s\u22641\u0302S and \u2211s\u2208Bn contain exactly the same terms, meaning that Equations (38) and (45) are equal. This completes the proof.\nEntropy 2023, 25, 648 10 of 30\nNote that the structure of the lattice reveals structure in the interactions, as previously noted in [32]. On the right-hand side of Figure 2, two faces of the three-variable lattice are shaded. The green region corresponds to the 2-point interaction between the first two variables. The red region contains a similar interaction between the first two variables, except this time in the context of the third variable fixed to 1 instead of 0. This illustrates the interpretation of a 3-point interaction as the difference in two 2-point interactions (IXYZ = IXY|Z=1 \u2212 IXY|Z=0; note that IXY|Z=0 is usually written as just IXY). The symmetry of the cube reveals the three different (though equivalent) choices as to which variable to set to 1. Treating the Boolean algebra as a die, where the sides facing up are , , and , we have\nIXYZ = \u2212 = \u2212 = \u2212 (46)\nAs before, we can invert Definition 5 and express the surprise of observing a state with all ones in terms of interactions, as follows:\nlog p(\u03c4 = 1, T \\ \u03c4 = 0) = \u2211 \u03b7\u2264\u03c4 I(\u03b7, T) (47)\nFor example, in the case where T = {X, Y, Z} and \u03c4 = {X, Y}\nS(1, 1, 0) = \u2212 log p(1, 1, 0) = \u2212IXY \u2212 IX \u2212 IY \u2212 I\u2205 (48)\nwhich illustrates that when X and Y tend to be off (IX < 0 and IY < 0) and X and Y tend to be different (IXY < 0), observing the state (1, 1, 0) is very surprising.\n3.2. Categorical Interactions\nTaking seriously the definition of interactions as the M\u00f6bius inversion of surprisal, one might ask what happens when surprisal is inverted over a different lattice instead of using a Boolean algebra. One example is shown in Figure 3; it corresponds to variables that can take three values\u20140, 1, or 2\u2014where states are ordered by a \u2264 b \u21d0\u21d2 \u2200i : ai \u2264 bi. To calculate interactions on this lattice, we need to know the value of M\u00f6bius functions of type \u00b5(s, 22). It can be readily verified that most M\u00f6bius functions of this type are zero, with the exceptions of \u00b5(22, 22) = \u00b5(11, 22) = 1 and \u00b5(21, 22) = \u00b5(12, 22) = \u22121, which provide the exact terms in the interactions between two categorical variables changing from 1 \u2192 2 (as defined in [32]). Calculating interactions on different sublattices with 1\u0302 = (21), (12) or (11) provides us with the other categorical interactions. The transitivity property of the interactions, i.e., I(X : 0\u2192 2, Y : 0\u2192 1) = I(X : 0\u2192 1, Y : 0\u2192 1) + I(X : 1\u2192 2, Y : 0\u2192 1), follows immediately from the structure of the lattice in Figure 3 and the alternating signs of the M\u00f6bius functions on a Boolean algebra.\nEntropy 2023, 25, 648 11 of 30\n3.3. Information and Interactions on Dual Lattices\nLattices have the property that a set with the reverse order remains a lattice; that is, if L = (S,\u2264) is a lattice, then Lop = (S, ) (where \u2200a, b \u2208 S : a b \u21d0\u21d2 a \u2265 b) is a lattice. This raises the question of what corresponds to mutual information and interaction on such dual lattices. Recognising that a poset L = (S,\u2264L) is a category C with objects S and a morphism f : A\u2192 B iff B \u2264L A, these become definitions in the opposite category Cop, meaning that they define dual objects. Let us start with mutual information. We can calculate the dual mutual information, denoted MI\u2217, by first noting that the dual to a Boolean algebra is another Boolean algebra,\nEntropy 2023, 25, 648 12 of 30\nmeaning that we have \u00b5(x, y) = (\u22121)|x|\u2212|y|. Simply replacing P with Pop in Equation (15) yields\nMI\u2217(\u03c4) = \u2211 \u03b7 \u03c4 (\u22121)|\u03b7|+1H(\u03b7) (49)\nThe dual mutual information of \u03c4 = 1\u0302Pop is simply MI\u2217(\u2205) = MI(1\u0302P), that is, the mutual information among all variables. However, the dual mutual information of a singleton set X is\nMI\u2217(X) = MI(1\u0302P)\u2212MI(1\u0302P \\ X) (50) = \u2206(X; 1\u0302P \\ X) (51)\nwhere \u2206 is known as the differential mutual information and describes the change in mutual information when leaving out X [40], i.e., when marginalising over the variable X. Note that a similar construction was already anticipated in [41] and that the differential mutual information has previously been used to describe information structures in genetics [39]. On the Boolean algebra of three variables {X, Y, Z}, the dual mutual information of X can be written out as follows:\nMI\u2217(X) = \u00b5({X}, {X})H(X) + \u00b5({X, Y}, {X})H(X, Y)+ \u00b5({X, Z}, {X})H(X, Z) + \u00b5({X, Y, Z}, {X})H(X, Y, Z) (52)\n= H(X)\u2212 H(X, Y)\u2212 H(X, Z) + H(X, Y, Z) (53)\nBecause \u2206 is the dual of mutual information, it should arguably be called the mutual coinformation; however, the term co-information is unfortunately already in use to refer to normal higher-order mutual information. To find the dual to the interactions, we start from Equation (36) and construct Sop = (B|\u03c4|, ), the dual to the lattice of binary strings S = (B|\u03c4|,\u2264). A dual interaction of variables \u03c4 \u2286 T is denoted as I\u2217(\u03c4; T), and is defined as follows:\nI\u2217(\u03c4; T) := \u2211 s 1\u0302Sop \u00b5Sop(s, 1\u0302Sop) log p(\u03c4 = s, T \\ \u03c4 = 0) (54)\nAgain, when \u03c4 = 1\u0302Sop = 0\u0302S = \u2205, this is simply (\u22121)|\u03c4| I(1\u0302S), while the dual interaction of a singleton set X is\nI\u2217(X; T) = (\u22121)|1\u0302S |\u22121 ( I(1\u0302S; T) + I(1\u0302S \\ X; T) )\n(55)\nFor example, on the three variable lattice in Figure 2, the dual interaction of X is\nI\u2217(X; T) = I(X, Y, Z; T) + I(Y, Z; T) (56)\nWriting pijk for p(X = i, Y = j, Z = k | T \\ {X, Y, Z} = 0), it can be seen that this is equal to\nI\u2217(X; T) = log p111 p100 p101 p110\n(57)\nwhich is similar to the 2-point interaction IYZ defined in Equation (8), now conditioned on X = 1 instead of 0. Note the difference between dual mutual information and dual interactions here; the dual mutual information of X describes the effect on the mutual information from marginalising over X, whereas the dual interaction of X describes the effect on an interaction\nEntropy 2023, 25, 648 13 of 30\nwhen fixing X = 1. This reflects a fundamental difference between mutual information and the interactions, in that the former is an averaged quantity and the latter a pointwise quantity. Dual interactions should probably be called co-interactions; however, to avoid confusion with the term co-information, we instead refer to them simply as dual interactions. Dual interactions are interactions that are conditioned on certain variables being 1 instead of 0. This makes them no longer equal to the Ising interactions between Boolean variables; however, there are situations in which an interaction is more interesting in the context of Z = 1 instead of Z = 0, for example, if Z is always 1 in the data under consideration. Summary\n\u2022 Mutual information is the M\u00f6bius inversion of marginal entropy on the lattice of subsets ordered by inclusion. \u2022 Differential (or conditional) mutual information is the M\u00f6bius inversion of marginal entropy on the dual lattice. \u2022 Model-free interactions are the M\u00f6bius inversion of surprisal on the lattice of subsets ordered by inclusion. \u2022 Model-free dual interactions are the M\u00f6bius inversion of surprisal on the dual lattice. \u2022 Dual interactions of a variable X are interactions between the other variables where X is set to 1\ninstead of 0.\nTo summarise these relationships diagrammatically, note that surprisals form a vector space as follows. Let P(T) be the powerset of a set of variables T and let |P(T)| = n. This forms a lattice P = (P(T),\u2286) ordered by inclusion, meaning that P(T) can be assigned a topological ordering indexed by i as P(T) = \u222ani=0ti. Let S be the set of linear combinations of surprisals of subsets of T:\nS = { n\n\u2211 i=0\nai log p(ti) | ai \u2208 R }\n(58)\nThis set is assigned a vector space structure over R by the usual scalar multiplication and addition. Note that the set\nB = {log p(t) | t \u2208 P(T)} (59)\nforms a basis for this vector space, because \u2211i \u03b1i log p(ti) = 0 has no non-trivial solutions and a span(B) = S . Only when two variables a and b are independent do we have linear dependencies in B, as it is then the case that log p(a, b) = log p(a) + log p(b). To define a map from S \u2192 R, we only need to specify its action on B and extend the definition linearly. This means that we can fully define the map evalT : S \u2192 R by specifying\nevalT : log p(R = r) 7\u2192 log p(R = 1, T \\ R = 0) (60)\nSimilarly, we can define the expectation map E : S \u2192 R as\nE : log p(R = r) 7\u2192\u2211 r p(R = r) log p(R = r) (61)\nwhich outputs the expected surprise over all realisations R = r. Finally, note that the M\u00f6bius inversion over a poset P is an endomorphism of the set FP of functions over P, defined as\nMP : FP \u2192 FP (62) MP : f (y) 7\u2192 \u2211\nx\u2264y \u00b5(x, y) f (x) (63)\nEntropy 2023, 25, 648 14 of 30\nTogether, these three maps ensure that the following diagram commutes:\nMI\u2217(R) = \u2206(R; 1\u0302P) H(R) MI(R)\npmi\u2217(R = r) S(R = r; T) pmi(R = r)\nI\u2217(R; T) SR;T I(R; T)\nMP\nE E\nMP\nMP\nevalT evalT\nMPop\nMPop\nMPop\nE\nevalT\nFor the case where T = {X, Y, Z} and R = {X, Y}, this explicitly amounts to\n\u2211(x,y,z)\u2208X\u00d7Y\u00d7Z p(x,y,z) log p(x,y,z) \u2212\u2211(x,y)\u2208X\u00d7Y p(x,y) \u2211(x,y)\u2208X\u00d7Y p(x, y) log p(x, y)\n\u2211(x,y)\u2208X\u00d7Y p(x,y) log p(x,y) \u2212\u2211x\u2208X p(x) log p(x) \u2212\u2211y\u2208Y p(y) log p(y)\nlog p(x,y,z)p(x,y) log p(x, y) log p(x,y)p(\u2205) p(x)p(y)\nlog p(1,1,1)p(1,1,0) log p(1, 1, 0) log p(1,1,0)p(0,0,0) p(1,0,0)p(0,1,0)\nMP\nE E\nMP\nMP\nevalT evalT\nMPop\nMPop\nMPop\nE\nevalT\n4. Results and Examples\nWhile mutual information and model-free interactions are related, there are several important differences in terms of how they capture dependencies. Note, for example, that higherorder information quantities are not independent of the lower-order quantities. The mutual information of three variables is bounded by the pairwise quantities as follows:\n\u2212min{MI(X, Y | Z), MI(Y, Z | X), MI(X, Z | Y)} \u2264 MI(X, Y, Z) \u2264 min{MI(X, Y), MI(Y, Z), MI(X, Z)} (64)\nThis means that there are no systems with zero pairwise mutual information and positive higher-order information. This is not true for the interactions. For example, a distribution with 3-point interactions and no pairwise interactions can trivially be constructed as p(X) = Z\u22121 exp ( \u2211ijk JijkXiXjXk ) . While this distribution has 3-point interactions with strength Jijk for triplets {Xi, Xj, Xk}, all pairwise interactions among {Xi, Xj} vanish when conditioning on Xk = 0. In fact, any positive discrete distribution can be written as a Boltzmann distribution with an energy function that is unique up to a constant, and as such is uniquely defined by its interactions; in other words, each interaction, at any order, can be freely varied to define a unique and valid probability distribution, namely, the Boltzmann distribution of the corresponding generalised Ising model. Note that this is closely related to the fact that a class of neural networks known as restricted Boltzmann machines are universal approximators [42\u201344] and exactly (though not uniquely) encode the Boltzmann distribution of a generalised Ising model in one of their layers [31,45]. Therefore, each distribution is uniquely determined by\nEntropy 2023, 25, 648 15 of 30\nits set of interactions, and should be distinguishable by them. This is famously not true for entropy-based information quantities, as illustrated below through several examples.\n4.1. Interactions and Their Duals Quantify and Distinguish Synergy in Logic Gates\nUnder the assumption of a causal collider structure A \u2192 C \u2190 B, nonzero 3-point interactions IABC can be interpreted as logic gates. A positive 3-point interaction means that the numerator in Equation (9) is larger than the denominator. Under the sufficient (though not necessary) assumption that each term in the numerator is larger than each term in the denominator, we obtain the following truth table as IABC \u2192 +\u221e:\nA B C 0 0 1 0 1 0 1 0 0 1 1 1\nwhich describes an XNOR gate. Let pG be the probability of each of the four states in the truth table for a gate G, and let eG be the probability of all other states. Then, the 3-point interaction of an XNOR gate can be written as\nIXNORABC = log p4XNOR e4XNOR\n(65)\nSimilarly, the truth tables of AND and OR gates imply that\nIANDABC = log eAND p3AND e3AND pAND\n(66)\nIORABC = log e3OR pOR eOR p3OR\n(67)\nIf we consider equally noisy gates such that pG = p and eG = e, the gates can be directly compared. Note that when a gate has a 3-point interaction I, its logical negation will have a 3-point interaction \u2212I. This determines the 3-point interactions of all six non-trivial logic gates on two inputs, as summarised in Table 1. The two gates with the strongest absolute interactions, XNOR and XOR, are the only two gates that are purely synergistic, i.e., knowing only one of the two inputs provides no information about the output. This relationship to synergy holds for three-input gates as well. The three-input gate with the strongest 4-point interaction has the following truth table:\nA B C D 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1\nThis is a three-input XOR gate, i.e., D = (A + B + C) mod 2, and is again maximally synergistic, as observing only two of the three inputs provides zero bits of information on the output.\nEntropy 2023, 25, 648 16 of 30\nSetting this maximum 4-point interaction to I, the three-input OR and AND gates receive a 4-point interaction I/4; thus, the hierarchies of interaction and synergy continue to match.\nThe 3-point interactions are able to separate most two-input logic gates by sign or value, leaving only AND\u223cNOR and OR\u223cNAND. Mutual information has less resolving power. Assuming a uniform distribution over all four allowed states from a gate\u2019s truth table, a brief calculation yields\nMIOR(A, B, C) = MIAND(A, B, C) = MINOR(A, B, C) = MINAND(A, B, C) (68)\n= \u2212 log ( 33/4\n4\n) \u2212 1 \u2248 \u22120.189\nMIXOR(A, B, C) = MIXNOR(A, B, C) = \u22121 (69)\nThat is, higher-order mutual information resolves strictly fewer logical gates by value and none by sign. In fact, the higher-order mutual information of a logic gate can never be positive, because it is bounded from above by the minimum of the pairwise mutual information, which is always zero for the pair of inputs. Because all entropy-based quantities inherit the degeneracy summarised in Table 2, neither the mutual information nor its dual can increase the resolving power (see Table 3).\nThe logic gate interactions and their duals are summarised in Table 3, where it can be seen that neither I\u2217GC = I G ABC + I G AB nor I \u2217G A improve the resolution beyond that of the 3- point interaction. However, the 3-point interaction requires 23 = 8 probabilities to achieve this resolving power, whereas I\u2217GC = p111 p001 p101 p011\nachieves the same resolving power with just four probabilities. However, note that because of a difference in sign convention dual mutual information is a difference between two mutual information quantities, while dual interactions are a sum of two\nEntropy 2023, 25, 648 17 of 30\ninteractions. Based on this, we can consider the difference of two interactions and define a new quantity J\u2217GA = I G ABC \u2212 IGBC. We refer to this as a J-interaction. When the MFIs are interpreted in the context of an energy-based model, such as an Ising model or a restricted Boltzmann machine, then the interactions have dimensions of energy, meaning that the J-interactions correspond to the difference in the energy contribution between a triplet and a pair. These J-interactions of the input nodes A and B assign a different value to each logic gate G, and the symmetric J-interaction J\u2217G = J\u2217GA J \u2217G B J \u2217G C , analogous to the symmetric deltas from [40], inherits the perfect resolution from J\u2217GA . Note that while J\u2217GA = J \u2217G B both have perfect resolution, J \u2217G C = I G ABC \u2212 IGAB does not improve the resolution beyond that of the 3-point interaction. This results from the fact that in logic gates we have IGABC = \u22122IGAB, meaning that IGABC and IGAB contain the same information. To see this, note that\nIGABC + 2I G AB = log p111 p001 p110 p000 p101 p011 p010 p100\n(70)\nBecause the logic gates are symmetric in their inputs, i.e., \u2200i, j pijk = pjik, this can be rewritten as\nIGABC + 2I G AB = log (p111 p110)(p001 p000) (p101 p100)(p101 p100)\n(71)\nEach of these terms in brackets has the form (pij1 pij0). Because these are two contradicting states, this product reduces to ep regardless of the truth table of G:\n= log e2 p2\ne2 p2 = 0 (72)\nNote that this pattern could already be observed in Table 3, though it was not yet explained. Thus, the J-interactions of the input nodes uniquely assign a value to each gate proportional to the synergy of its logic. The hierarchy is J\u2217XNORA > J \u2217NOR A > J \u2217AND A , which is mirrored for the respective logical complements. XNOR is indeed the most synergistic, while NOR is more synergistic than AND with respect to observing a 0 in one of the inputs; in a NOR gate, a 0 in the input provides no information on the output, while it completely fixes the output of an AND gate. Because the interactions are defined in a context of 0s, they order the synergy accordingly.\nTo illustrate how different association metrics reflect the underlying causal dynamics, consider data generated from a selection of three-node causal DAGs as follows. On a given\nEntropy 2023, 25, 648 18 of 30\nDAG G, first denote the set of nodes without parents, the orphan nodes, by S0. Each orphan node in S0 receives a random value drawn from a Bernoulli distribution, i.e., P(X = 1) = p and P(X = 0) = 1\u2212 p. Next, denote the set of children of orphan nodes as S1. Each node in S1 is then set to either the product of its parent nodes (for multiplicative dynamics) or the mean of its parent nodes (for additive dynamics), plus some zero-mean Gaussian noise with variance \u03c32. Note that for the fork and the chain this simply amounts to a noisy copying operation. All nodes are then rounded to a 0 or 1. A set S2 is then defined as the set of all children of nodes in S1, and these receive values using the same dynamics as before. As long as the causal structure is acyclic, this algorithm terminates on a set of nodes Si that has no children. For example, the chain graph A\u2192 B\u2192 C has S0 = {A}, S1 = {B}, S2 = {C}, and S3 = \u2205, at which point the updating terminates. Figure 4 shows the results for four different DAGs with multiplicative and additive dynamics (though these are the same for forks and chains). The six different dynamics are represented in four different DAGs, two different (Pearson) correlations, four different partial correlations, and two different mutual information structures, which means that each of these descriptions is degenerate in some of the dynamics. The shown pairwise partial correlations are the correlations among the residuals after a linear regression against the third variable. Because this is similar to conditioning on the third variable, it is somewhat analogous to the MFIs; in fact, when the variables are multivariate normal the partial correlations are encoded in the inverse covariance matrix and are equivalent to pairwise Ising interactions [31]. Indeed, it can be seen that the partial correlations are somewhat able to disentangle direct effects from indirect effects, although they fail to distinguish additive from multiplicative dynamics. Note that only the sign of the association and its significance are represented, as the precise value depends on the noise level \u03c32. The rightmost column shows that the MFIs assign a unique association structure to each of the dynamics, distinguish between direct and indirect effects, and reveal multiplicative dynamics as a 3-point interaction while identifying additive dynamics as a purely pairwise process. Finally, note that both the partial correlation and the MFIs assign a negative association to the parent nodes in a collider structure. This reflects that two nodes become dependent when conditioned on a common effect (cf. Berkson\u2019s paradox), a phenomenon already found in partial correlations of metabolomic data in [46]. The mutual information is affected by Berkson\u2019s paradox as well, revealed through the negative three-point mutual information. This negative three-point is a direct effect from conditioning on the common effect C, as on colliders MI(A, B, C) = MI(A, B)\u2212MI(A, B | C) = \u2212MI(A, B | C), because the mutual information among the independent inputs A and B vanishes by definition.\n4.3. Higher-Order Categorical Interactions Distinguish Dyadic and Triadic Distributions\nThat the interactions have such resolving power over distributions of binary variables is perhaps not very surprising in light of the universality of RBMs with respect to this class of distributions. More surprisingly, their resolving power extends to the case of categorical variables. In [47], the authors introduced two distributions, the dyadic and triadic distributions, which are indistinguishable by almost all commonly used information measures (i.e., Shannon, Renyi(2), residual, and Tsallis entropy, co-information, total correlation, CAEKL mutual information, interaction information, Wyner, exact, functional, and MSS common information, perplexity, disequilibrium, and LMRP and TSE complexities). The two distributions are defined on three variables, each taking a value in a four-letter alphabet {0, 1, 2, 3}. The joint probabilities are summarised in Table 4. To construct the distributions, each category is represented as a binary string ({0, 1, 2, 3} \u2192 {00, 01, 10, 11}), leading to new variables {X0, X1, Y0, Y1, Z0, Z1}. The dyadic distribution is constructed by linking these new variables with pairwise rules X0 = Y1, Y0 = Z1, Z0 = X1, while the triadic\nEntropy 2023, 25, 648 19 of 30\ndistribution is constructed with triplet rules X0 + Y0 + Z0 = 0 mod 2 and X1 = Y1 = Z1. The resulting binary strings are then reinterpreted as categorical variables to produce Table 4. The authors of [47] found that no Shannon-like measure can distinguish between the two distributions, and argued that the partial information decomposition, which is different for the two distributions, is not a natural information measure, as it has to single out one of the variables as an output. To calculate model-free categorical interactions between the variables, we can set the probabilities of the states in Table 4 uniformly to p = (1\u2212 (64\u2212 8)e)/8 and those of the other states to e (i.e., a normalised uniform distribution over legal states). There are a total of 63 = 216 interactions such that x1 > x0, y1 > y0, z1 > z0. Each of these can be written as\nIXYZ(x0 \u2192 x1; y0 \u2192 y1; z0 \u2192 z1) =\nlog p ( X = x1, Y = y1, Z = z1 | X = 0 )\np ( X = x0, Y = y0, , Z = z0 | X = 0 )\np ( X = x1, Y = y0, Z = z0 | X = 0 )\np ( X = x0, Y = y1, , Z = z1 | X = 0 )\n\u00d7 p ( X = x0, Y = y1, Z = z0 | X = 0 )\np ( X = x1, Y = y0, , Z = z1 | X = 0 )\np ( X = x0, Y = y0, Z = z1 | X = 0 )\np ( X = x1, Y = y1, , Z = z0 | X = 0 ) (73)\nEntropy 2023, 25, 648 20 of 30Version April 6, 2023 submitted to Entropy 18 of 27\nDynamics\nChain\nCausal graph\nA B\nC\nCorrelation\nA B\nC\nPartial corr.\nA B\nC\nMutual information\nA B\nC\nMFI\nA B\nC\nFork A B\nC\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nDynamics\nAdd. collider C = 12(A + B)\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nDynamics\nMult. collider C = A\u00d7 B\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nAdd. collider & chain C = 12(A + B)\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nMult. collider & chain C = A\u00d7 B\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nA B\nC\nFigure 4. Different causal dynamics lead to different association metrics. Green edges denote positive values, red edges denote negative values, circles denote a 3-point quantity, and dashed lines show edges that show marginal significance, depending on \u03c32. Correlations and mutual information cannot distinguish between most dynamics, and while partial correlation can, for certain noise levels, identify the correct pairwise relationships, it falls short of distinguishing additive from multiplicative dynamics. Only MFIs distinguish between all 6 scenarios, and reveal the combinatorial effect of the multiplicative dynamics as a 3-point interaction. See appendix A.3 for the simulation parameters and raw numbers. This figure is reproduced with permission from the author of [47].\n4.3. Higher-order categorical interactions distinguish dy- and triadic interactions 323\nThat the interactions have such resolving power over distributions of binary variables 324 is perhaps not so surprising in light of the universality of RBMs with respect to this class of 325 distributions. More surprisingly, their resolving power extends to the case of categorical 326 variables. In [48], the authors introduce two distributions, the dy- and triadic distribu- 327 tions, that are indistinguishable by almost all commonly used information measures (i.e. 328 Shannon-, Renyi(2)-, residual-, and Tsallis entropy, co-information, total correlation, CAEKL 329 mutual information, interaction information, Wyner-, exact-, functional-, and MSS common 330 information, perplexity, disequilibrium, and the LMRP- and TSE complexity). 331\n332\nThe two distributions are defined on 3 variables, each taking a value in a 4-letter 333 alphabet {0, 1, 2, 3}. The joint probabilities are summarised in Table 4. To construct the 334 distributions, each category is represented as a binary string ({0, 1, 2, 3} \u2192 {00, 01, 10, 11}), 335 leading to new variables {X0, X1, Y0, Y1, Z0, Z1}. The dyadic distribution is constructed 336 by linking these new variables with pairwise rules: X0 = Y1, Y0 = Z1, Z0 = X1, while the 337 triadic distribution is constructed with rules involving triplets: X0 + Y0 + Z0 = 0 mod 2, 338 and X1 = Y1 = Z1. The resulting binary strings are then reinterpreted as categorical 339 variables to produce Table 4. 340\nFigure 4. Different causal dynamics lead to different association metrics. Green edges denote positive values, red edges denote negative values, circles denote a three-point quantity, and dashed lines show edges with marginal significance (depending on \u03c32). Correlations and mutual information cannot distinguish between most dynamics, and while partial correlation can identify the correct pairwise relationships for certain noise levels, it falls short of distinguishing additive from multiplicative dynamics. Only MFIs can distinguish between all six scenarios and reveal the combinatorial effect of the multiplicative dynamics as a 3-point interaction. See Appendix A.3 for the simulation parameters and raw numbers. This figure is reproduced with permission from the author of [48].\nOf particular interest h re are th two quan ties IXYZ(0\u2192 3; 0\u2192 3; 0\u2192 3) and IXYZ = \u2211x0,x1,y0,y1,z0,z1 IXYZ(x0 \u2192 x1; y0 \u2192 y1; z0 \u2192 z1), where the sum is over all values such that x1 > x0, y1 > y0, z1 > z0, as all possible pairs necessarily sum to zero because IXYZ(x0 \u2192 x1; y0 \u2192 y1; z0 \u2192 z1) = \u2212IXYZ(x1 \u2192 x0; y0 \u2192 y1; z0 \u2192 z1). For the dyadic distribution, we have\nIDyXYZ(0\u2192 3; 0\u2192 3; 0\u2192 3) = log pe3\npe3 = 0, (74)\nwhile for the tri dic dist ibution we have\nITriXYZ(0\u2192 3; 0\u2192 3; 0\u2192 3) = log e4\npe3 = log\ne p (75)\nEntropy 2023, 25, 648 21 of 30\nThus, this particular 3-point interaction is zero for the dyadic distribution and negative for the triadic distribution. The sum over all three points (see Appendix A.4 for details) is provided by\nIDyXYZ = log 1 = 0 (76) ITriXYZ = 64 log e\np (77)\nThat is, the additively symmetrised 3-point interaction is zero for the dyadic distribution and strongly negative for the triadic distribution. These two distributions, which are indistinguishable in terms of their information structure, are distinguishable by their model-free interactions, which accurately reflect the higher-order nature of the triadic distribution.\nIn this paper, we have related the model-free interactions introduced in [32] to information theory by defining them as M\u00f6bius inversions of surprisal on the same lattice that relates mutual information to entropy. We then invert the order of the lattice and compute the orderdual to the mutual information, which turns out to be a generalisation of differential mutual information. Similarly, the order-dual of interaction turns out to be interaction in a different context. Both the interactions and the dual interactions are able to distinguish all six logic gates by value and sign. Moreover, their absolute strength reflects the synergy within the logic gate. In simulations, the interactions were able to perfectly distinguish six kinds of causal dynamics that are partially indistinguishable to Pearson/partial correlations, causal graphs, and mutual information. Finally, we considered dyadic and triadic distributions constructed using pairwise and higher-order rules, respectively. While these two distributions are indistinguishable in terms of their Shannon information, they have different categorical MFIs that reflect the order of the construction rules.\nEntropy 2023, 25, 648 22 of 30\nOne might wonder why the interactions enjoy this advantage over entropy-based quantities. The most obvious difference is that the interactions are defined in a pointwise way, i.e., in terms of the surprisal of particular states, whereas entropy is the expected surprisal across an ensemble of states. Furthermore, the MFIs can be interpreted as interactions in an Ising model and as effective couplings in a restricted Boltzmann machine. As both these models are known to be universal approximators with respect to positive discrete probability distributions, the MFIs should be able to characterise all such distributions. What is not immediately obvious is that the kinds of interactions that characterise a distribution should reflect properties of that distribution, such as the difference between direct and indirect effects and the presence of higher-order structure. However, in the various examples covered in this manuscript the interactions turn out to intuitively align with properties of the process used to generate the data. While the stringent conditioning on variables not considered in the interaction might make it tempting to interpret an MFI as a causal or interventional quantity, it is important to be very careful when doing this. Assigning a causal interpretation to statistical inferences, whether in Pearl\u2019s graphical do-calculus [49] or in Rubin\u2019s potential outcomes framework [50], requires further (often untestable) assumptions and analysis of the system in order to determine whether a causal effect is identifiable and which variables to control for. In contrast, an MFI is simply defined by conditioning on all observed variables, makes no reference to interventions or counterfactuals, and does not specify a direction of the effect. While in a controlled and simple setting the MFIs can be expressed in terms of causal average treatment effects [32], a causal interpretation is not justifiable in general. Moreover, the stringency in the conditioning might worry the attentive reader. Estimating log p(X = 1, Y = 1, T = 0) directly from data means counting states such as (X, Y, T1, T2, . . . , TN) = (1, 1, 0, 0, . . . 0), which for sufficiently large N are rare in most datasets. Appendix A.1 shows how to use the causal graph to construct Markov blankets, making such estimation tractable when full conditioning is too stringent. In an upcoming paper, we address this issue by estimating the graph of conditional dependencies, allowing for successful calculation of MFIs up to the fifth order in gene expression data. One major limitation of MFIs is that they are only defined on binary or categorical variables, whereas many other association metrics are defined for ordinal and continuous variables as well. As states of continuous variables no longer form a lattice, it is hard to see how the definition of MFIs could be extended to include these cases. Finally, it is worth noting that the structure of different lattices has guided much of this research. That Boolean algebras are important in defining higher-order structure is not surprising, as they are the stage on which the inclusion\u2013exclusion principle can be generalised [36]. However, it is not only their order-reversed duals that lead to meaningful definitions; completely unrelated lattices do as well. For example, the M\u00f6bius inversion on the lattice of ordinal variables from Figure 3 and the redundancy lattices in the partial information decomposition [28] both lead to new and sensible definitions of information-theoretic quantities. Furthermore, the notion of M\u00f6bius inversion has been generalised to a more general class of categories [51], of which posets are a special case. A systematic investigation of informationtheoretic quantities in this richer context would be most interesting.\nFunding: This research was funded by Medical Research Council grant number MR/N013166/1.\nData Availability Statement: No new data were created or analyzed in this study. Data sharing is not applicable to this article.\nAcknowledgments: The author is grateful for the insightful discussions of model-free interactions, causality, and RBMs with Ava Khamseh, Sjoerd Beentjes, Chris Ponting, and Luigi Del Debbio. The author also thanks John Baez for a helpful conversation on the role of surprisal in information theory. The author further thanks Sjoerd Beentjes for reading and commenting on an early version of this work,\nEntropy 2023, 25, 648 23 of 30\nand the reviewers for their helpful suggestions. A.J. is supported by an MRC Precision Medicine Grant (MR/N013166/1).\nConflicts of Interest: The funders had no role in the design of the study, in the collection, analysis, or interpretation of data, in the writing of the manuscript, or in the decision to publish the results.\nAbbreviations The following abbreviations are used in this manuscript:\nMI Mutual Information MFI Model-Free Interaction DAG Directed Acyclic Graph MB Markov Blanket PID Partial Information d=Decomposition i.i.d. independent and identically distributed\nAppendix A\nAppendix A.1. Markov Blankets\nEstimating the interaction in Definition 2 from data involves estimating the probabilities of certain states occurring. While we do not have access to the true probabilities, we can rewrite the interactions in terms of expectation values. Note that all interactions involve factors of the type\np(X = 1, Y = y | Z = 0) p(X = 0, Y = y | Z = 0) = p(X = 1 | Y = y, Z = 0) p(X = 0 | Y = y, Z = 0) (A1)\n= p(X = 1 | Y = y, Z = 0)\n1\u2212 p(X = 1 | Y = y, Z = 0) (A2)\n= E[X | Y = y, Z = 0]\n1\u2212E[X | Y = y, Z = 0] (A3)\nbecause\nE[X | Z = z] = \u2211 x\u2208{0,1} p(X = x | Z = z) x = p(X = 1 | Z = z) (A4)\nThis allows us to write the 2-point interaction, e.g., as follows:\nIij = log E ( Xi|Xj = 1, X = 0 )\nE ( Xi|Xj = 0, X = 0\n) ( 1\u2212E ( Xi|Xj = 0, X = 0 )) ( 1\u2212E ( Xi|Xj = 1, X = 0 )) (A5)\nAlthough expectation values are theoretical quantities, not empirical ones, sample means can be used as unbiased estimators to estimate each term in (A5). The stringent conditioning in this estimator can make the number of samples that satisfy the conditioning very small, which results in the estimates having large variance on different finite samples. Note that if we can find a subset of variables MBXi such that Xi\u22a5\u22a5 Xk | MBXi \u2200Xk /\u2208 MBXi and i 6= k (in causal language, a set of variables MBXi that d-separates Xi from the rest), then we only have to condition on MBXi in (A5), reducing the variance of our estimator. Such a set MBXi is called a Markov Blanket of the node Xi. There has recently been a certain degree of confusion around the notion of Markov blankets in biology, specifically with respect to their use in the free energy principle in neuroscience contexts. Here, a Markov blanket refers to the notion of a Pearl blanket in the language of [52]. Because conditioning on fewer variables should reduce the variance of the estimate by increasing the number of samples that can be used for the estimation, we are\nEntropy 2023, 25, 648 24 of 30\ngenerally interested in finding the smallest Markov blanket. This minimal Markov blanket is called the Markov boundary. Finding such minimal Markov blankets is hard; in fact, because it requires testing each possible conditional dependency between the variables, we claim here (without proof) that it is causal discovery-hard, i.e., if such a graph exists it is at least as computationally complex as constructing a causal DAG consistent with the joint probability distribution.\nAppendix A.2. Proofs\nMarkov blankets are not only a computational trick; in theory, only variables that are in each other\u2019s Markov blanket can share a nonzero interaction. To illustrate this, first note that the property of being in a variable\u2019s Markov blanket is symmetric:\nProposition A1 (symmetry of Markov blankets). Let X be a set of variables with joint distribution p(X) and let A \u2208 X and B \u2208 X such that A 6= B. We denote the minimal Markov blanket of X by MBX . Then, A \u2208 MBB \u21d0\u21d2 B \u2208 MBA, and we can say that A and B are Markov-connected.\nProof. Let Y = X \\ {A, B}. Then,\nA 6\u2208 MBB =\u21d2 p(B | A, Y) = p(B | Y) (A6)\nConsider that\np(A | B, Y) = p(A, B | Y) p(B | Y) (A7)\n= p(B | A, Y)p(A, | Y)\np(B | Y) (A8)\n= p(A | Y) (A9)\nwhich means that B 6\u2208 MBA. Because A 6\u2208 MBB \u21d0\u21d2 B 6\u2208 MBA holds, its negation holds as well, which completes the proof.\nThis definition of Markov connectedness allows us to state the following.\nTheorem A1 (only Markov-connected variables can interact). A model-free n-point interaction I1...n can only be nonzero when all variables S = {X1, . . . , Xn} are mutually Markov-connected.\nProof. Let X be a set of variables with joint distribution p(X), let S = {X1, . . . , Xn}, and let X = X \\ S. Consider the definition of an n-point interaction among S:\nI1...n = n\n\u220f i=1\n\u2202\n\u2202Xi log p(X1, . . . , Xn | X = 0) (A10)\n= ( n\u22121 \u220f i=1 \u2202 \u2202Xi ) \u2202 \u2202Xn log p(X1, . . . , Xn | X = 0) (A11)\n= ( n\u22121 \u220f i=1 \u2202 \u2202Xi ) log p(Xn = 1 | X1, . . . , Xn\u22121, X = 0) p(Xn = 0 | X1, . . . , Xn\u22121, X = 0)\n(A12)\n= ( n\u22121 \u220f i=1 \u2202 \u2202Xi ) log p(Xn = 1 | S \\ Xn, X = 0) p(Xn = 0 | S \\ Xn, X = 0)\n(A13)\nEntropy 2023, 25, 648 25 of 30\nNow, if \u2203Xj \u2208 S such that Xj 6\u2208 MBXn , we do not need to condition on Xj and can write this as\nI1...n = ( n\u22121 \u220f i=1 \u2202 \u2202Xi ) log p(Xn = 1 | S \\ {Xj, Xn}, X = 0) p(Xn = 0 | S \\ {Xj, Xn}, X = 0)\n(A14)\n=  \nn\u22121 \u220f i=1 i 6=j \u2202 \u2202Xi\n  ( \u2202\n\u2202Xj log p(Xn = 1 | S \\ {Xj, Xn}, X = 0) p(Xn = 0 | S \\ {Xj, Xn}, X = 0)\n) (A15)\n= 0 (A16)\nas the probabilities no longer involve Xj. Because Xj was chosen arbitrarily, this must hold for all variables in S, which means that if any variable in S is not in the Markov blanket of Xn then the interaction IS vanishes:\nS \\ Xn 6\u2282 MBXn =\u21d2 IS = 0 (A17)\nFurthermore, as the indexing we chose for our variables was arbitrary, this must hold for any re-indexing, which means that\n\u2200Xi \u2208 S : S \\ Xi 6\u2282 MBXi =\u21d2 IS = 0 (A18)\nThis in turn means that all variables in S must be Markov-connected in order for the interaction IS to be nonzero.\nThus, knowledge of the causal graph aids estimation in two ways: it shrinks the variance of the estimates by relaxing the conditioning, and it identifies the interactions that could be nonzero.\nWhen knowledge of the causal graph is imperfect, it is possible to accidentally exclude a variable from a Markov blanket and thereby undercondition the relevant probabilities. The resulting error can be expressed in terms of the mutual information between the variables, as follows.\nProposition A2 (underconditioning bias). Let S be a set of random variables with probability distribution p(S), let X, Y, and let Z be three disjoint subsets of S. Then, omitting Y from the conditioning set results in a bias determined by (and linear in) the pointwise mutual information that Y = 0 provides about the states of X:\nIX|YZ \u2212 IX|Z = ( |X|\n\u220f i=1\n\u2202\n\u2202xi\n) pmi(X = x, Y = 0 | Z = 0) (A19)\nProof. The pointwise mutual information (pmi) is defined as\npmi(X = x, Y = y) = log p(X = x, Y = y)\np(X = x)p(Y = y) (A20)\nNote that\np(X = x1 | Y = y, Z = z) = p(X = x1, Y = y | Z = z)\np(Y = y | Z = z) (A21)\nEntropy 2023, 25, 648 26 of 30\nmeaning that we can write\np(X = x1 | Y = y, Z = z) = epmi(X=x1,Y=y|Z=z)p(X = x1 | Z = z) (A22)\nThat is, not conditioning on Y = y results in an error in the estimate of p(X = x1 | Y = y, Z = z) that is exponential in the Z-conditional pmi of X and Y. However, consider the interaction among X,\nIX = IX|YZ = ( |X| \u220f i=1 \u2202 \u2202xi ) log p(X = x | Y = 0, Z = 0) (A23)\n= ( |X| \u220f i=1 \u2202 \u2202xi ) (log p(X = x | Z = 0) + pmi(X = x, Y = 0 | Z = 0)) (A24)\n= IX|Z + ( |X| \u220f i=1 \u2202 \u2202xi ) pmi(X = x, Y = 0 | Z = 0) (A25)\nThat is, the error in the interaction as a result of not conditioning on the right variables is linear in terms of the difference between the pmi values of different states.\nAppendix A.3. Numerics of Causal Structures\nTables A1\u2013A6 are taken from [48] with permission from the author, and list the precise values leading to Figure 4. From each graph, 100k samples were generated using p = 0.5 and \u03c3 = 0.4. To quantify the significance value of the interactions, the data were bootstrap resampled 1k times, resulting in the definition of F as the fraction of resampled interactions having a different sign from the original interaction. The smaller F is, the more significant the interaction.\nTable A1. Chain.\nVersion April 6, 2023 submitted to Entropy 24 of 27 Proposition 2. (Underconditioning bias) Let S be a set of random variables with probability distribution p(S). Let X, Y, and Z be three disjoint subsets of S. Then omitting Y from the conditioning set results in a bias determined by, and linear in, the pointwise mutual information that Y = 0 gives about states of X: IX|YZ \u2212 IX|Z = ( |X| \u220f i=1 \u2202 \u2202xi ) pmi(X = x, Y = 0 | Z = 0) (A19) Proof. The pointwise mutual information (pmi) is defined as\npmi(X = x, Y = y) = log p(X = x, Y = y)\np(X = x)p(Y = y) (A20)\nNote that\np(X = x1 | Y = y, Z = z) = p(X = x1, Y = y | Z = z)\np(Y = y | Z = z) (A21)\nso that we can write\np(X = x1 | Y = y, Z = z) = e mi(X= 1,Y=y|Z=z p(X = x1 | Z = z) (A22)\nThat is, not conditioning on Y = y results in an error in the estimate of p(X = x1 | Y = y, Z = z) that is exponential in the Z-conditional pmi of X and Y. However, consider the interaction among X:\nIX = IX|YZ = ( |X| \u220f i=1 \u2202 \u2202xi ) log p(X = x | Y = 0, Z = 0) (A23)\n= ( |X| \u220f i=1 \u2202 \u2202xi ) (log p(X = x | Z = 0) + pmi(X = x, Y = 0 | Z = 0)) (A24)\n= IX|Z + ( |X| \u220f i=1 \u2202 \u2202xi ) pmi(X = x, Y = 0 | Z = 0) (A25)\nThat is, th error in the interaction as a result of not conditioning on the right variables 457 is linear in the difference between the pmi\u2019s of different states. 458\nAppendix A.3. Numerics of causal structures 459\nTables A1 to A6 are taken from [47] ith permission from the author, and list the 460 precise values that led to Figure 4. From each graph, 100k samples were generated using 461 p = 0.5 and \u03c3 = 0.4. To quantify the significance value of the interactions, the data was 462 bootstrap resampled 1k times, which defined F: the fraction of resampled interactions that 463 had a different sign from the original interaction. The smaller F is, the more significant the 464 interaction is. 465 Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.281 0.000 0.790 0.0 0.635 0.000e+00 0.515 1 [0, 2] 0.056 0.117 0.622 0.0 0.031 2.261e-23 0.301 2 [1, 2] 4.249 0.000 0.786 0.0 0.628 0.000e+00 0.510 3 [0, 1, 2] -0.052 0.217 NaN NaN NaN NaN 0.300\nTable A1. ChainGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI 0 [0, 1] 4.281 0.000 0.790 0.0 0.635 0.000 \u00d7 10+0 0.515 1 [0, 2] 0.056 0.117 0.622 0.0 0.031 2.261 \u00d7 10\u221223 0.301 2 [1, 2] 4.249 0.000 0.786 0.0 0.628 0.000 \u00d7 10+0 0.510 3 [0, 1, 2] \u22120.052 0.217 NaN NaN NaN NaN 0.300\nTable A2. Fork.\nVersion April 6, 2023 submitted to Entropy 25 of 27Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000e+00 0.514 1 [0, 2] 4.257 0.000 0.788 0.0 0.632 0.000e+00 0.512 2 [1, 2] -0.014 0.376 0.622 0.0 0.028 6.518e-19 0.300 3 [0, 1, 2] 0.020 0.376 NaN NaN NaN NaN 0.300\nTable A2. ForkCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 1 [0, 2] -0.989 0.000 -0.002 0.593 -0.070 5.172e-109 2.059e-06 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN -2.678e-02\nTable A3. Additive colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000e+00 1.403e-01 1 [0, 2] -2.156 0.000 -0.005 0.145 -0.087 1.463e-166 1.529e-05 2 [1, 2] 0.036 0.109 0.429 0.000 0.480 0.000e+00 1.415e-01 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN NaN -1.150e-01\nTable A4. Multiplicative colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 0.0 0.599 0.0 0.515 2 [1, 2] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A5. Additive collider + chainCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] -0.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [0, 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] -0.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nTable A6. Multiplicative collider + chain\nAppendix A.4. Python code to calculate categorical dy- and triadic interactions 466\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , 467 [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ] 468 469 t r i a d i c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , 470 [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ] 471 472 s t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 } 473 474 def catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : 475\nprob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse \u2019 e \u2019 476 477 num = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) 478 denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) 479 return (num, denom) 480\n481 numDy = \u2019 \u2019 482 denomDy = \u2019 \u2019 483 numTri = \u2019 \u2019 484 denomTri = \u2019 \u2019 485 486 for x0 in range ( 4 ) : 487\nfor x1 in range ( x0 +1 , 4 ) : 488 for y0 in range ( 4 ) : 489\nfor y1 in range ( y0 +1 , 4 ) : 490 for z0 in range ( 4 ) : 491\nfor z1 in range ( z0 +1 , 4 ) : 492 493\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) 494 numDy += nDy 495 denomDy += dDy 496 497 nTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) 498 numTri += nTri 499\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000 \u00d7 10+0 0.514 1 [0, 2] 4.257 0.000 0.788 0.0 0.632 0.000 \u00d7 10+0 0.512 2 [1, 2] \u22120.014 0.376 0.622 0.0 0.028 6.518 \u00d7 10\u221219 0.300 3 [0, 1, 2] 0.020 0.376 NaN NaN NaN NaN 0.300\nEntropy 2023, 25, 648 27 of 30\nTable A3. Additive collider.\nVersion April 6, 2023 submitted to Entropy 25 of 27Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000e+00 0.514 1 [0, 2] 4.257 0.000 0.788 0.0 0.632 0.000e+00 0.512 2 [1, 2] -0.014 0.376 0.622 0.0 0.028 6.518e-19 0.300 3 [0, 1, 2] 0.020 0.376 NaN NaN NaN NaN 0.300\nTable A2. ForkCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 1 [0, 2] -0.989 0.000 -0.002 0.593 -0.070 5.172e-109 2.059e-06 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN -2.678e-02\nTable A3. Additive colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000e+00 1.403e-01 1 [0, 2] -2.156 0.000 -0.005 0.145 -0.087 1.463e-166 1.529e-05 2 [1, 2] 0.036 0.109 0.429 0.000 0.480 0.000e+00 1.415e-01 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN NaN -1.150e-01\nTable A4. Multiplicative colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 0.0 0.599 0.0 0.515 2 [1, 2] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A5. Additive collider + chainCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] -0.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [0, 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] -0.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nTable A6. Multiplicative collider + chain\nAppendix A.4. Python code to calculate categorical dy- and triadic interactions 466\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , 467 [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ] 468 469 t r i a d i c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , 470 [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ] 471 472 s t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 } 473 474 def catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : 475\nprob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse \u2019 e \u2019 476 477 num = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) 478 denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) 479 return (num, denom) 480\n481 numDy = \u2019 \u2019 482 denomDy = \u2019 \u2019 483 numTri = \u2019 \u2019 484 denomTri = \u2019 \u2019 485 486 for x0 in range ( 4 ) : 487\nfor x1 in range ( x0 +1 , 4 ) : 488 for y0 in range ( 4 ) : 489\nfor y1 in range ( y0 +1 , 4 ) : 490 for z0 in range ( 4 ) : 491\nfor z1 in range ( z0 +1 , 4 ) : 492 493\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) 494 numDy += nDy 495 denomDy += dDy 496 497 nTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) 498 numTri += nTri 499\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000 \u00d7 10+0 1.154 \u00d7 10\u22121 1 [0, 2] \u22120.989 0.000 \u22120.002 0.593 \u22120.070 5.172 \u00d7 10\u2212109 2.059 \u00d7 10\u22126 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000 \u00d7 10+0 1.154 \u00d7 10\u22121 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN \u22122.678 \u00d7 10\u22122\nTable A4. Multiplicative collider.\nVersion April 6, 2023 submitted to Entropy 25 of 27Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000e+00 0.514 1 [0, 2] 4.257 0.000 0.788 0.0 0.632 0.000e+00 0.512 2 [1, 2] -0.014 0.376 0.622 0.0 0.028 6.518e-19 0.300 3 [0, 1, 2] 0.020 0.376 NaN NaN NaN NaN 0.300\nTable A2. ForkCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 1 [0, 2] -0.989 0.000 -0. 02 0.593 -0. 70 5.172e-109 2.059e-06 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN -2.678e-02\nTable A3. Additive colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000e+00 1.403e-01 1 [0, 2] -2.156 0.000 -0.005 0.145 -0.087 1.463e-166 1.529e-05 2 [1, 2] 0.036 0.109 0.429 0.000 0.480 0.000e+00 1.415e-01 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN NaN -1.150e-01\nTable A4. Multiplicative colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 0.0 0.599 0.0 0.515 2 [1, 2] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A5. Additive collider + chainCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] -0.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [0, 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] -0.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nTable A6. Multiplicative collider + chain\nAppendix A.4. Python code to calculate categorical dy- and triadic interactions 466\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , 467 [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ] 468 469 t r i a d i c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , 470 [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ] 471 472 s t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 } 473 474 def catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : 475\nprob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse \u2019 e \u2019 476 477 num = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) 478 denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) 479 return (num, denom) 480\n481 numDy = \u2019 \u2019 482 denomDy = \u2019 \u2019 483 numTri = \u2019 \u2019 484 denomTri = \u2019 \u2019 485 486 for x0 in range ( 4 ) : 487\nfor x1 in range ( x0 +1 , 4 ) : 488 for y0 in range ( 4 ) : 489\nfor y1 in range ( y0 +1 , 4 ) : 490 for z0 i range ( 4 ) : 491\nfor z1 in range ( z0 +1 , 4 ) : 492 493\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) 494 numDy += nDy 495 denomDy += dDy 496 497 nTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) 498 numTri += nTri 499\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000 \u00d7 10+0 1.403 \u00d7 10\u22121 1 [0, 2] \u22122.156 0.000 \u22120.005 0.145 \u221240.087 1.463 \u00d7 10\u2212166 1.529 \u00d7 10\u22125 2 [1, 2] 0.036 0.109 0.429 0.000 0.480 0.000 \u00d7 10+0 1.415 \u00d7 10\u22121 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN aN \u22121.150 \u00d7 10\u22121\nTable A5. Additive collider + chain.\nVersion April 6, 2023 submitted to Entropy 25 of 27Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000e+00 0.514 1 , 2 4.257 . .788 .0 0.632 0.000e+00 0.512 2 [1, ] - .014 .376 .622 0.0 0.028 6.518e-19 0.300 3 [0, 1, 2] 0.020 .376 NaN NaN NaN NaN 0.300\nTable A2. ForkCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 1 [0, 2] -0.989 0.000 -0.002 0.593 -0.070 5.172e-109 2.059e-06 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN -2.678e-02\nTable A3. Additive colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000e+00 1.403e-01 1 [0, 2] -2.156 0.000 -0.005 0.145 -0.087 1.463e-166 1.529e-05 2 [1, 2] .036 0.109 0.429 0.000 0.480 0.000e+00 1.415e-01 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN NaN -1.150e-01\nTable A4. Multiplicative colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 0.0 0.599 0.0 0.515 2 [1, 2] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A5. Additive collider + chainCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] -0.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [0, 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] -0.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nTable A6. Multiplicative collider + chain\nAppendix A.4. Python code to calculate categorical dy- and triadic interactions 466\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , 467 [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ] 468 469 t r i a d i c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , 470 [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ] 471 472 s t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 } 473 474 def catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : 475\nprob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse \u2019 e \u2019 476 477 num = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) 478 denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) 479 return (num, denom) 480\n481 numDy = \u2019 \u2019 482 denomDy = \u2019 \u2019 483 numTri = \u2019 \u2019 484 denomTri = \u2019 \u2019 485 486 for x0 in range ( 4 ) : 487\nfor x1 in range ( x0 +1 , 4 ) : 488 for y0 in range ( 4 ) : 489\nfor y1 in range ( y0 +1 , 4 ) : 490 for z0 in range ( 4 ) : 491\nfor z1 in range ( z0 +1 , 4 ) : 492 493\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) 494 numDy += nDy 495 denomDy += dDy 496 497 nTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) 498 numTri += nTri 499\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 0.0 0.599 0.0 0.515 2 [1, ] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A6. Multiplicative collider + chain.\nVersion April 6, 2023 submitted to Entropy 25 of 27Causal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 4.268 0.000 0.789 0.0 0.634 0.000e+00 0.514 1 [0, 2] 4.257 0.000 0.788 0.0 0.632 0.000e+00 0.512 2 [1, 2] -0.014 0.376 0.622 0.0 0.028 6.518e-19 0.300 3 [0, 1, 2] 0.020 0.376 NaN NaN NaN NaN 0.300\nTable A2. ForkCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 1 [0, 2] -0.989 0.000 -0.002 0.593 -0.070 5.172e-109 2.059e-06 2 [1, 2] 2.144 0.000 0.395 0.000 0.505 0.000e+00 1.154e-01 3 [0, 1, 2] 0.003 0.438 NaN NaN NaN NaN -2.678e-02\nTable A3. Additive lliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 0.032 0.140 0.427 0.000 0.478 0.000e+00 1.403e-01 1 [0, 2] -2.156 0.000 -0.005 0.145 -0.087 1.463e-166 1.529e-05 2 [1, 2] 0.036 0.109 0.429 0.000 0.480 0.000e+00 1.415e-01 3 [0, 1, 2] 4.237 0.000 NaN NaN NaN NaN -1.150e-01\nTable A4. Multiplicative colliderCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] 2.103 0.000 0.705 0.0 0.362 0.0 0.396 1 [0, 2] 3.288 0.000 0.790 . 0.599 0.0 0.515 2 [1, 2] 2.113 0.000 0.706 0.0 0.364 0.0 0.397 3 [0, 1, 2] 0.050 0.162 NaN NaN NaN NaN 0.335\nTable A5. Additive llider + chainCausal graph\n0 1\n2\nGenes Interaction F Pearson cor. Pearson cor. p Partial cor. Partial cor. p MI\n0 [0, 1] -0.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [ , 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] -0.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nTable A6. Multiplicative collider + chain\nAppendix A.4. Python code to calculate categorical dy- and triadic interactions 466\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , 467 [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ] 468 469 t r i a d c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , 470 [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ] 471 472 s t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 } 73 474 def catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : 475\nprob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse \u2019 e \u2019 476 477 num = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) 478 denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) 479 return (num, denom) 480\n481 numDy = \u2019 \u2019 482 denomDy = \u2019 \u2019 483 numTri = \u2019 \u2019 484 denomTri = \u2019 \u2019 485 486 for x0 in range ( 4 ) : 487\nfor x1 in range ( x0 +1 , 4 ) : 488 for y0 in range ( 4 ) : 489\nfor y1 in range ( y0 +1 , 4 ) : 490 for z0 in range ( 4 ) : 491\nfor z1 in range ( z0 +1 , 4 ) : 492 493\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) 494 numDy += nDy 495 denomDy += dDy 496 497 nTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) 498 numTri += nTri 499\nGenes Interaction F Pearson cor. Pearson co . p Partial cor. Partial cor. p MI\n0 [0, 1] \u22120.017 0.342 0.709 0.0 0.365 0.0 0.403 1 [0, 2] 2.094 0.000 0.786 0.0 0.596 0.0 0.510 2 [1, 2] \u22120.057 0.092 0.707 0.0 0.361 0.0 0.401 3 [0, 1, 2] 4.359 0.000 NaN NaN NaN NaN 0.293\nAppendix A.4. Python Code for Calculating Categorical Dyadic and Triadic Interactions\ndyadicS ta tes = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , \u2019 c \u2019 , \u2019d \u2019 ] , [ \u2019 c \u2019 , \u2019 b \u2019 , \u2019 a \u2019 ] , [ \u2019 c \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019 c \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019d \u2019 ] ]\nt r i a d i c S t a t e s = [ [ \u2019 a \u2019 , \u2019 a \u2019 , \u2019 a \u2019 ] , [ \u2019 a \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ] , [ \u2019 b \u2019 , b \u2019 , \u2019 b \u2019 ] , [ \u2019 b \u2019 , \u2019d \u2019 , \u2019d \u2019 ] , [ \u2019 c \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ] , [ \u2019 c \u2019 , \u2019 c \u2019 , \u2019 a \u2019 ] , [ \u2019d \u2019 , \u2019 b \u2019 , \u2019d \u2019 ] , [ \u2019d \u2019 , \u2019d \u2019 , \u2019 b \u2019 ] ]\nEntropy 2023, 25, 648 28 of 30\ns t a t e D i c t = { 0 : \u2019 a \u2019 , 1 : \u2019 b \u2019 , 2 : \u2019 c \u2019 , 3 : \u2019d \u2019 }\ndef catIntSymb ( x0 , x1 , y0 , y1 , z0 , z1 , s t a t e s ) : prob = lambda x , y , z : \u2019p \u2019 i f [ x , y , z ] in s t a t e s e lse~ \u2019 e \u2019\nnum = prob ( x1 , y1 , z1 ) + prob ( x1 , y0 , z0 ) + prob ( x0 , y1 , z0 ) + prob ( x0 , y0 , z1 ) denom = prob ( x1 , y1 , z0 ) + prob ( x1 , y0 , z1 ) + prob ( x0 , y1 , z1 ) + prob ( x0 , y0 , z0 ) return (num, denom)\nnumDy = \u2019 \u2019 denomDy = \u2019 \u2019 numTri = \u2019 \u2019 denomTri = \u2019 \u2019\nfor x0 in range ( 4 ) : for x1 in range ( x0 +1 , 4 ) :\nfor y0 in range ( 4 ) : for y1 in range ( y0 +1 , 4 ) :\nfor z0 in range ( 4 ) : for z1 in range ( z0 +1 , 4 ) :\nnDy , dDy = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , dyadicS ta tes ) numDy += nDy denomDy += dDy\nnTri , dTri = catIntSymb ( * [ s t a t e D i c t [ x ] for x in [ x0 , x1 , y0 , y1 , z0 , z1 ] ] , t r i a d i c S t a t e s ) numTri += nTri denomTri += dTri\nprint ( f \u2019 Dyadic i n t e r a c t i o n : log ( p^{numDy. count ( \" p \" ) \u2212 denomDy . count ( \" p \" ) } e ^{numDy. count ( \" e \" ) \u2212 denomDy . count ( \" e \" ) } ) \u2019 ) print ( f \u2019 T r i a d i c i n t e r a c t i o n : log ( p^{numTri . count ( \" p \" ) \u2212 denomTri . count ( \" p \" ) } e ^{numTri . count ( \" e \" ) \u2212 denomTri . count ( \" e \" ) } ) \u2019 )\n// Output :\n>> Dyadic i n t e r a c t i o n : log ( p^0 e ^0) >> T r i a d i c i n t e r a c t i o n : log ( p^\u221264 e ^64)\nReferences 1. Ghazanfar, S.; Lin, Y.; Su, X.; Lin, D.M.; Patrick, E.; Han, Z.G.; Marioni, J.C.; Yang, J.Y.H. Investigating higher-order interactions in single-cell data with scHOT. Nat. Methods 2020, 17, 799\u2013806. https://doi.org/10.1038/s41592-020-0885-x. 2. Lezon, T.R.; Banavar, J.R.; Cieplak, M.; Maritan, A.; Fedoroff, N.V. Using the principle of entropy maximization to infer genetic\ninteraction networks from gene expression patterns. Proc. Natl. Acad. Sci. USA 2006, 103, 19033\u201319038. https://doi.org/10.1073/ pnas.0609152103.\n3. Watkinson, J.; Liang, K.C.; Wang, X.; Zheng, T.; Anastassiou, D. Inference of regulatory gene interactions from expression data using three-way mutual information. Ann. N. Y. Acad. Sci. 2009, 1158, 302\u2013313. https://doi.org/10.1111/j.749-6632.2008.03757.x. 4. Kuzmin, E.; VanderSluis, B.; Wang, W.; Tan, G.; Deshpande, R.; Chen, Y.; Usaj, M.; Balint, A.; Usaj, M.M.; Van Leeuwen, J.; et al. Systematic analysis of complex genetic interactions. Science 2018, 360, eaao1729. https://doi.org/10.1126/science.aao1729. 5. Weinreich, D.M.; Lan, Y.; Wylie, C.S.; Heckendorn, R.B. Should evolutionary geneticists worry about higher-order epistasis? Curr. Opin. Genet. Dev. 2013, 23, 700\u2013707. 6. Panas, D.; Maccione, A.; Berdondini, L.; Hennig, M.H. Homeostasis in large networks of neurons through the Ising model\u2014do higher order interactions matter? BMC Neurosci. 2013, 14, 1\u20132. https://doi.org/10.1186/1471-2202-14-s1-p166. 7. Tkac\u030cik, G.; Marre, O.; Amodei, D.; Schneidman, E.; Bialek, W.; Berry, M.J. Searching for Collective Behavior in a Large Network of Sensory Neurons. PLoS Comput. Biol. 2014, 10, e1003408. https://doi.org/10.1371/journal.pcbi.1003408. 8. Ganmor, E.; Segev, R.; Schneidman, E. Sparse low-order interaction network underlies a highly correlated and learnable neural population code. Proc. Natl. Acad. Sci. USA 2011, 108, 9679\u20139684. https://doi.org/10.1073/pnas.1019641108. 9. Yu, S.; Yang, H.; Nakahara, H.; Santos, G.S.; Nikolic\u0301, D.; Plenz, D. Higher-order interactions characterized in cortical activity. J. Neurosci. 2011, 31, 17514\u201317526. 10. Gatica, M.; Cofr\u00e9, R.; Mediano, P.A.; Rosas, F.E.; Orio, P.; Diez, I.; Swinnen, S.P.; Cortes, J.M. High-order interdependencies in the aging brain. Brain Connect. 2021, 11, 734\u2013744. 11. Sanchez, A. Defining Higher-Order Interactions in Synthetic Ecology: Lessons from Physics and Quantitative Genetics. Cell Syst. 2019, 9, 519\u2013520. https://doi.org/10.1016/j.cels.2019.11.009.\nEntropy 2023, 25, 648 29 of 30\n12. Grilli, J.; Barab\u00e1s, G.; Michalska-Smith, M.J.; Allesina, S. Higher-order interactions stabilize dynamics in competitive network models. Nature 2017, 548, 210\u2013213. 13. Li, Y.; Mayfield, M.M.; Wang, B.; Xiao, J.; Kral, K.; Janik, D.; Holik, J.; Chu, C. Beyond direct neighbourhood effects: Higher-order interactions improve modelling and predicting tree survival and growth. Natl. Sci. Rev. 2021, 8, nwaa244. 14. Tekin, E.; White, C.; Kang, T.M.; Singh, N.; Cruz-Loya, M.; Damoiseaux, R.; Savage, V.M.; Yeh, P.J. Prevalence and patterns of higher-order drug interactions in Escherichia coli. NPJ Syst. Biol. Appl. 2018, 4, 1\u201310. 15. Alvarez-Rodriguez, U.; Battiston, F.; de Arruda, G.F.; Moreno, Y.; Perc, M.; Latora, V. Evolutionary dynamics of higher-order interactions in social networks. Nat. Hum. Behav. 2021, 5, 586\u2013595. 16. Cencetti, G.; Battiston, F.; Lepri, B.; Karsai, M. Temporal properties of higher-order interactions in social networks. Sci. Rep. 2021, 11, 1\u201310. 17. Grabisch, M.; Roubens, M. An axiomatic approach to the concept of interaction among players in cooperative games. Int. J. Game Theory 1999, 28, 547\u2013565. 18. Matsuda, H. Physical nature of higher-order mutual information: Intrinsic correlations and frustration. Phys. Rev. E 2000, 62, 3096. 19. Cerf, N.J.; Adami, C. Entropic bell inequalities. Phys. Rev. A 1997, 55, 3371. 20. Battiston, F.; Amico, E.; Barrat, A.; Bianconi, G.; Ferraz de Arruda, G.; Franceschiello, B.; Iacopini, I.; K\u00e9fi, S.; Latora, V.; Moreno, Y.; et al. The physics of higher-order interactions in complex systems. Nat. Phys. 2021, 17, 1093\u20131098. 21. Skardal, P.S.; Arenas, A. Higher order interactions in complex networks of phase oscillators promote abrupt synchronization switching. Commun. Phys. 2020, 3, 1\u20136. 22. Merchan, L.; Nemenman, I. On the Sufficiency of Pairwise Interactions in Maximum Entropy Models of Networks. J. Stat. Phys. 2016, 162, 1294\u20131308. https://doi.org/10.1007/s10955-016-1456-5. 23. Tkacik, G.; Schneidman, E.; Berry, M.J., II; Bialek, W. Ising models for networks of real neurons. arXiv 2006, arXiv:q-bio/0611072. 24. Margolin, A.A.; Nemenman, I.; Basso, K.; Wiggins, C.; Stolovitzky, G.; Favera, R.D.; Califano, A. ARACNE: An algorithm for the\nreconstruction of gene regulatory networks in a mammalian cellular context. BMC Bioinform. 2006, 7, S7. https://doi.org/10.1186/14 71-2105-7-S1-S7.\n25. Nemenman, I. Information theory, multivariate dependence, and genetic network inference. arXiv 2004, arXiv:q-bio/0406015. 26. Watanabe, S. Information theoretical analysis of multivariate correlation. IBM J. Res. Dev. 1960, 4, 66\u201382. 27. Rosas, F.E.; Mediano, P.A.; Luppi, A.I.; Varley, T.F.; Lizier, J.T.; Stramaglia, S.; Jensen, H.J.; Marinazzo, D. Disentangling high-order mechanisms and high-order behaviours in complex systems. Nat. Phys. 2022, 18, 476\u2013477. 28. Williams, P.L.; Beer, R.D. Nonnegative decomposition of multivariate information. arXiv 2010, arXiv:1004.2515. 29. Wibral, M.; Priesemann, V.; Kay, J.W.; Lizier, J.T.; Phillips, W.A. Partial information decomposition as a unified approach to the specification of neural goal functions. Brain Cogn. 2017, 112, 25\u201338. 30. Jaynes, E.T. Information theory and statistical mechanics. Phys. Rev. 1957, 106, 620. https://doi.org/10.1103/PhysRev.106.620. 31. Nguyen, H.C.; Zecchina, R.; Berg, J. Inverse statistical problems: From the inverse Ising problem to data science. Adv. Phys. 2017, 66, 197\u2013261. https://doi.org/10.1080/00018732.2017.1341604. 32. Beentjes, S.V.; Khamseh, A. Higher-order interactions in statistical physics and machine learning: A model-independent solution to the inverse problem at equilibrium. Phys. Rev. E 2020, 102, 053314. https://doi.org/10.1103/PhysRevE.102.053314. 33. Glonek, G.F.; McCullagh, P. Multivariate logistic models. J. R. Stat. Soc. Ser. B (Methodol.) 1995, 57, 533\u2013546. 34. Bartolucci, F.; Colombi, R.; Forcina, A. An extended class of marginal link functions for modelling contingency tables by equality and inequality constraints. Stat. Sin. 2007, 17, 691\u2013711. 35. Bateson, G. Steps to an Ecology of Mind; Chandler Publishing Company: San Francisco, USA, 1972. 36. Stanley, R.P. Enumerative Combinatorics Volume 1 Second Edition; Cambridge Studies in Advanced Mathematics; Cambridge University Press: Cambridge, UK, 2011. 37. Rota, G.C. On the foundations of combinatorial theory I. Theory of M\u00f6bius functions. Z. F\u00fcr Wahrscheinlichkeitstheorie Verwandte Geb. 1964, 2, 340\u2013368. 38. Bell, A.J. The co-information lattice. In Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation, ICA, Nara, Japan, 1\u20134 April 2003; Volume 2003. 39. Galas, D.J.; Sakhanenko, N.A. Symmetries among multivariate information measures explored using M\u00f6bius operators. Entropy 2019, 21, 88. 40. Galas, D.J.; Sakhanenko, N.A.; Skupin, A.; Ignac, T. Describing the complexity of systems: Multivariable \u201cset complexity\u201d and the information basis of systems biology. J. Comput. Biol. 2014, 21, 118\u2013140. 41. Galas, D.J.; Kunert-Graf, J.; Uechi, L.; Sakhanenko, N.A. Towards an information theory of quantitative genetics. J. Comput. Biol. 2019, 28, 527\u2013559. https://doi.org/10.1101/811950. 42. Freund, Y.; Haussler, D. Unsupervised learning of distributions on binary vectors using two layer networks. Adv. Neural Inf. Process.\nSyst. 1991, 4, pp. 912-919.\nEntropy 2023, 25, 648 30 of 30\n43. Le Roux, N.; Bengio, Y. Representational power of restricted Boltzmann machines and deep belief networks. Neural Comput. 2008, 20, 1631\u20131649. 44. Montufar, G.; Ay, N. Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines. Neural Comput. 2011, 23, 1306\u20131319. 45. Cossu, G.; Del Debbio, L.; Giani, T.; Khamseh, A.; Wilson, M. Machine learning determination of dynamical parameters: The Ising model case. Phys. Rev. B 2019, 100, 064304. 46. Krumsiek, J.; Suhre, K.; Illig, T.; Adamski, J.; Theis, F.J. Gaussian graphical modeling reconstructs pathway reactions from highthroughput metabolomics data. BMC Syst. Biol. 2011, 5, 21. 47. James, R.G.; Crutchfield, J.P. Multivariate dependence beyond Shannon information. Entropy 2017, 19, 531. 48. Jansma, A. Higher-Order Interactions in Single-Cell Gene Expression. Ph.D. Thesis, University of Edinburgh, Edinburgh, UK, 2023. 49. Pearl, J. Models, Reasoning and Inference; Cambridge University Press: Cambridge, UK, 2000; Volume 19. 50. Imbens, G.W.; Rubin, D.B. Causal Inference in Statistics, Social, and Biomedical Sciences; Cambridge University Press: Cambridge, UK, 2015. 51. Leinster, T. Notions of M\u00f6bius inversion. Bull. Belg. Math. Soc.-Simon Stevin 2012, 19, 909\u2013933. 52. Bruineberg, J.; Do\u0142e\u0328ga, K.; Dewhurst, J.; Baltieri, M. The emperor\u2019s new Markov blankets. Behav. Brain Sci. 2022, 45, e183.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Higher-Order Interactions and their Duals Reveal Synergy and Logical Dependence Beyond Shannon-Information",
    "year": 2023
}