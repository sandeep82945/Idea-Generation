{
    "abstractText": "Object detection based on deep learning is a popular trend, and it includes object recognition and positioning. This paper proposes a method that can accurately obtain object type and accurate threedimensional position. The method is divided into three parts: object recognition and coarse positioning based on deep learning, precise positioning based on deep learning combined with B-spline level set in color images, and precise three-dimensional positioning with depth information of RGB-D camera. The precise positioning of the object provides accurate end pose information for the autonomous grasping of the robotic arm, and it has great significance to the gripping of the robotic arm. Performance metrics include mAP (mean average precision) and IOU (intersection of a union). Experimental results show that the mAP value of Yolo-v3 in this paper can reach 87.62%, the average IOU of Yolo-v3 in this paper can reach 66.74%, the average IOU of Yolo-v3 and B-spline level set can reach 100%, and can get accurate 3D location in the real scene. In addition, the experiments comparisons between VOC dataset and our own dataset validate that our dataset can take higher mAP and average IOU values. INDEX TERMS Object detection, deep learning, level set, depth information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lin Zhang"
        },
        {
            "affiliations": [],
            "name": "Xinyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Ning An"
        },
        {
            "affiliations": [],
            "name": "Rui Gao"
        },
        {
            "affiliations": [],
            "name": "Yingjie Zhang"
        }
    ],
    "id": "SP:73524969dfa1fe0c565d2e19b1e3a22bf1e94d7b",
    "references": [
        {
            "authors": [
                "A. Ilioudi",
                "A. Dabiri",
                "B.J. Wolf"
            ],
            "title": "Deep Learning for Object Detection and Segmentation in Videos: Toward an Integration With Domain Knowledge",
            "venue": "IEEE Access, vol. 10, pp. 34562-34576, Apr. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Jiao",
                "F. Zhang",
                "F. Liu"
            ],
            "title": "A survey of deep learning-based object detection",
            "venue": "IEEE Access, vol. 7, pp. 128837-128868, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Kalake",
                "W Wan",
                "L. Hou"
            ],
            "title": "Analysis based on recent deep learning approaches applied in real-time multi-object tracking: a review",
            "venue": "IEEE Access, vol. 9, pp. 32650-32671, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Sun",
                "W. Zhang",
                "R. Yu"
            ],
            "title": "Motion planning for mobile Robots\u2013focusing on deep reinforcement learning: A systematic Review",
            "venue": "IEEE Access, vol. 9, pp. 69061-69081, May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Mujeeb",
                "W. Dai",
                "M. Erdt"
            ],
            "title": "One class based feature learning approach for defect detection using deep autoencoders",
            "venue": "Adv. Eng. Inform., vol. 42, pp. 100933, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Viola",
                "M.J. Jones"
            ],
            "title": "Robust real-time object detection",
            "venue": "Cambridge Research Laboratory, Feb. 2001.",
            "year": 2001
        },
        {
            "authors": [
                "D.J. Pasadas",
                "P. Baskaran",
                "H.G. Ramos"
            ],
            "title": "Detection and classification of defects using ECT and multi-level SVM model",
            "venue": "IEEE Sensors J., vol. 20, no. 5, pp. 2329-2338, Mar. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wu",
                "H. Yue",
                "J. Wang"
            ],
            "title": "Object detection based on RGC mask R-CNN",
            "venue": "IET Image Process., vol. 14, no. 8, pp. 1502-1508, May 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast R-CNN",
            "venue": "IEEE Conf. Vis. Pattern Recognit., Santiago, Chile, 2015, pp. 1440-1448.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick"
            ],
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "venue": "IEEE Pattern. Anal., vol. 37, no. 9, pp. 1137-1139, Jun. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Lu",
                "J. Ji",
                "Z. Xing"
            ],
            "title": "Attention and Feature Fusion SSD for Remote Sensing Object Detection",
            "venue": "IEEE Trans. Instrumen Meas., vol. 70, pp. 1-9, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Lee",
                "K. Hwang"
            ],
            "title": "YOLO with adaptive frame control for realtime object detection applications",
            "venue": "Multimed Tools Appl., pp. 1-22, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Commun. ACM., vol. 60, no. 6, pp. 84-90, Jun. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Xiang",
                "Z. Cheng",
                "W. Yuan",
                "Z. Liang",
                "C. Yi"
            ],
            "title": "Compressive sensing ghost imaging object detection using generative adversarial networks",
            "venue": "Opt Eng, vol. 58, no. 1, pp. 1, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conf. Vis. Pattern Recognit., Las Vegas, Nv, USA, 2016, pp. 770-778.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Zhao",
                "P. Zheng",
                "S. Xu"
            ],
            "title": "Object detection with deep learning: A review",
            "venue": "IEEE Trans. Neur Net Lear., vol. 30, no. 11, pp. 3212- 3232, Nov. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Ottonelli",
                "P. Spagnolo",
                "P.L. Mazzeo"
            ],
            "title": "Improved video segmentation with color and depth using a stereo camera",
            "venue": "2013 IEEE International Conference on Industrial Technology (ICIT), 2013, pp. 1134-1139.",
            "year": 2013
        },
        {
            "authors": [
                "N. Wang",
                "X. Gong"
            ],
            "title": "Adaptive fusion for RGB-D salient object detection",
            "venue": "IEEE Access, vol. 7, pp. 55277-55284, May 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Lin",
                "P. Goyal",
                "R. Girshick"
            ],
            "title": "Focal Loss for Dense Object Detection",
            "venue": "IEEE Int. Conf. Comput. Vis., Venice, Italy, 2017, pp. 2999-3007.",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLOv3: An Incremental Improvement",
            "venue": "arXiv e-prints, 2017, DOI. 10.48550/arXiv.1804.02767.",
            "year": 2017
        },
        {
            "authors": [
                "L. Zhang"
            ],
            "title": "Automatic liver tumor segmentation based on improved factorization and B-spline level set",
            "venue": "M.S. thesis, Colleg. Comput. Scien. ,Chongqing Univ, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nand positioning. This paper proposes a method that can accurately obtain object type and accurate threedimensional position. The method is divided into three parts: object recognition and coarse positioning based on deep learning, precise positioning based on deep learning combined with B-spline level set in color images, and precise three-dimensional positioning with depth information of RGB-D camera. The precise positioning of the object provides accurate end pose information for the autonomous grasping of the robotic arm, and it has great significance to the gripping of the robotic arm. Performance metrics include mAP (mean average precision) and IOU (intersection of a union). Experimental results show that the mAP value of Yolo-v3 in this paper can reach 87.62%, the average IOU of Yolo-v3 in this paper can reach 66.74%, the average IOU of Yolo-v3 and B-spline level set can reach 100%, and can get accurate 3D location in the real scene. In addition, the experiments comparisons between VOC dataset and our own dataset validate that our dataset can take higher mAP and average IOU values.\nINDEX TERMS Object detection, deep learning, level set, depth information.\nI. INTRODUCTION Object detection is one of the basic tasks in the field of computer vision, and has a research history of more than 20 years in academia. Traditional object detection algorithms must first select regions (sliding window) on a given image, then perform feature extraction on these regions, and finally use a trained classifier for classification. This type of method uses hand-designed features, which has poor robustness and a complicated process.\nWith the development of convolutional neural networks, deep learning has been widely used in object detection. Compared with traditional object detection methods, deep learning in detection has many advantages. For example, traditional methods require researchers to manually extract features using relevant knowledge and experience. Deeplearning-based methods can learn features of corresponding data differences through a large amount of data, and the resulting features are more representative. The deep learning model directly extracts features from the original image by simulating the visual perception system of the human brain\nand transfers them layer by layer to obtain high-dimensional information about the image [1-2].\nObject detection is developed from the task of image classification. The difference is that it no longer only classifies a single type of object in an image, but also completes the classification and positioning of multiple objects that may exist in an image at the same time. Classification refers to assigning category labels to the object, and positioning refers to determining the coordinates of the vertices of the outer rectangular frame of the object. Therefore, object detection tasks are more challenging and have broader application prospects, such as autonomous driving, face recognition, pedestrian detection, medical detection, and so on. At the same time, object detection can also be used as the research basis for more complex computer vision tasks such as image segmentation, image description, object tracking, and action recognition [3].\nIn recent years, with the continuous development of deep learning technology, computer vision has been widely used in autonomous driving, intelligent robot, and other fields [4].\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\nObject recognition is one of the important basic tasks in computer vision. The accurate position of the object of interest in the image improves the perception and navigation capabilities.\nTraditional object recognition algorithms often use\nartificial features such as HOG features [5], SIFT features [6], and Haar features [7] for image feature extraction, and linear classifiers such as Support Vector Machine (SVM) [8] for classification. This type of method performs feature extraction and classification on multiple regions of the image in a specific way, so the recognition speed is relatively slow and the generation performance based on deep learning uses convolutional neural network as an adaptive feature extraction module, which can achieve better recognition performance. The current mainstream object recognition algorithms are mainly divided into two-stage object recognition and single-stage object recognition. The twostage object recognition algorithm first generates candidate regions and then predicts the detection frame based on this. Representative algorithms include R-CNN [9], Fast R-CNN [10] and Faster R-CNN [11]. However, because this type of algorithm divides the object recognition task into two stages, the overall detection speed is slower. In response to this problem, the single-stage object recognition algorithm directly generates and regresses the detection frame on the original image, which effectively improves the real-time performance of the object recognition algorithm. Representative algorithms include SSD [12] and Yolo [13].\nNowadays, deep learning methods have played a great role in the field of computer vision, and there have been many excellent convolutional neural network [14-16]. Compared with traditional object features, it greatly improves the robustness and accuracy of object recognition [17]. Because the convolution feature can better reflect the essence of images, deep learning has achieved significant results in image-based object detection and classification, while precise position and posture information of the object in the image cannot be obtained by only deep learning.\nObject positioning is responsible for calculating the position and range of objects of a specified category through regression or other methods. How to design algorithms with higher reliability and real-time performance at a lower cost has become a key issue in the development of object recognition and positioning technology. Object positioning more depends on various sensors, and this paper carries out object recognition and positioning based on Kinect2. There are some literatures [18-19] combining RGB information and depth information.\nIn order to solve the problem of improving the positioning accuracy of deep learning in object positioning, this paper combines the level set method to obtain the precise position information of the object, while combining the depth information of RGB-D camera to obtain the precise threedimensional position of the object. The contributions of this paper are that (1) the traditional level set algorithm is\nimproved to be suitable for color images, (2) the object position obtained by the level set is used as Yolo-v3\u2019s feedback on the object position, (3) the depth information of the RGB-D camera is combined to get the precise 3D position of the object, which provides position information for subsequent robotic arm grasping.\nThe rest of this paper is described as follows. The method is presented in Sec. 2, which includes object identification and positioning based on Yolo-v3, precise positioning of Bspline level set for color images and 3D object positioning with RGB-D. Experimental results and analysis are presented in Sec. 3. In Sec. 4, conclusions and future research directions are discussed. II. RELATED WORKS Figure 1 is a flowchart of object recognition and precise positioning algorithm based on Yolo-v3 and level set. Firstly, for the RGB image of Kinect2, Yolo-v3 algorithm is used to identify and roughly locate the object. Secondly, for the Depth image of Kinect2, pre-processing the coordinates of identification frame where the object is obtained by Yolo-v3 as the initial contour of the next level set algorithm, and using level set to accurately extract the object contour, obtain the smallest rectangle surrounding the object, and locate the precise location of the object. Finally, combine the corresponding depth information to obtain the precise threedimensional position of the object.\nA. Object Identification and Positioning based on Yolov3 With the continuous development of computers and deep learning, object detection is currently an important research direction in computer vision field. Object recognition and positioning research has greatly improved accuracy and efficiency with the efforts of domestic and foreign researchers. These rich research results provide theoretical basis and technical support for the development of face recognition, autonomous driving, smart robots, smart homes, etc. Object recognition is one of the important basic tasks in computer vision. Through object recognition algorithm, the accurate position of object of interest in the image can be obtained, so as to lay the foundation for the next information fusion decision and improve the overall perception and navigation ability.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\nThe combination of visual SLAM and deep learning can improve the disadvantages of cameras in environments with large changes in indoor lighting. The training of deep learning models improves the robustness of the system in an environment where objects move fast. This section uses Yolo-v3 to train a deep learning network based on the images acquired by Kinect2, classifies the object, recognizes and locates the object in the indoor scene. The advantages and disadvantages of the Yolo series algorithms are described in\nTable 1.\nTABLE I ADVANTAGES AND DISADVANTAGES OF YOLO\u2019S SERIES ALGORITHMS.\nMethods Advantages Disadvantages\nYolo Simple network\nstructure, fast, and real-time.\nPoor positioning accuracy, poor detection performance in small but many object scenes.\nYolo-v2 Allow users to adjust\nbetween accuracy and speed.\nUse pre-training, difficult to migrate.\nYolo-v3 Low background false\ndetection, fast, realtime, high robustness.\nAccuracy needs to be further improved.\nYolo-v3 can detect and locate objects in real time and maintain a high recognition rate. It performs well in terms of time, accuracy, and robustness, so it is one of the preferred algorithms in object detection. The network structure model of Yolo-v3 is shown in Figure 2. In this network model, the size of the input image is 416\u00d7416\u00d73, and the size of the output images 1y , 2y and 3y are 13\u00d713\u00d7255, 26\u00d726\u00d7255 and 52\u00d752\u00d7255, respectively, and output object position and category. Among them, 1y contains the largest feature information and is suitable for detecting large objects, 2y is suitable for detecting objects of general size, and 3y contains the mostly small details and is suitable for detecting small objects.\nFigure 3 describes the result of comparing the performance of Yolo-v3 with other algorithms. Compared with SSD, Yolo-v3 is faster and much better.\nYolo-v3 first divides the input image into S S cells (as shown in Figure 4), and inputs the image into the network for training. If the center coordinates of the input object fall in a certain grid, the grid detects this object. Yolo-v3 uses anchors to predict bounding boxes. For each predicted bounding box, the coordinates ( ), , ,x y w ht t t t output by the network are the learning parameters. ( ),x yc c is the offset of the upper left corner of the grid, the width and height of the boundary identification box are respectively wp and hp . The center coordinates of the corresponding prediction box are ( ),x yb b , and the width and height are wb and hb , respectively.  is the logistic regression function.\nYolo-v3 divides the image into S S grids, if the center coordinates of the target ground truth fall into a certain grid in S S , the grid will detect the target. Each grid predicts B bounding boxes, confidence levels and the probability of belonging to a certain target. Here, B is 3 in Yolo-v3 algorithm. The initial sizes of the three bounding boxes are different, and the bounding box with the largest IOU of ground truth predicts the object target. Then the prediction is\n( )\n( ) tw\nth\nx x x\ny y y\ne\nw w\ne\nh h\nb t c\nb t c\nb p\nb p\n\n\n= +  = +  =  =\n(1)\nFor each obtained predicted bounding box, Yolo-v3 uses logistic regression to score each predicted bounding box. When the prediction box and the real box coincide, the score is 1. For each image obtained, there are multiple prediction\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\nframes, and an appropriate confidence level is set to obtain the final prediction output."
        },
        {
            "heading": "B. Precise Positioning of B-spline Level Set for Color Images",
            "text": "Recently, most target recognition and target positioning are carried out separately. This Section introduces Yolo-v3based target recognition and positioning. The positioning results of this method need to be further improved, and the precise position of the target cannot be obtained by purely using deep learning methods. When the robot finishes grabbing the specified object, it must first locate the object. With the development of computer vision, the application of robot positioning has become more and more extensive.\nIn the image acquired by Kinect2, the target position information is obtained according to the rough position of the target recognized by Yolo-v3, the target in this area is taken as ROI (region of interest) and taken as the input image of the level set algorithm. Then the level set algorithm is used to extract the contour, and the smallest rectangular frame surrounding the target is obtained, which is the precise position of the target, and this precise position is fed back to the target positioning of Yolo-v3.\nThe existing level set algorithm only considers gray information, and the segmentation accuracy of color images has not reached the level of application. Therefore, this paper proposes a B-spline level set algorithm for color images. Current level set algorithms mostly use manual methods to obtain initial contours, which is tedious and time-consuming. The initial contour of level set algorithm used in this paper is provided by the rough positioning result obtained by Yolo-v3, and the rectangular frame is reduced by 10 pixels. The Bspline level set of color images is improved based on Bspline level set [22] proposed in 2017, and is extended from gray-scale CT images to color images.\nThe traditional CV is based on the Mumford-Shah model. The main idea of Mumford-Shah segmentation is to use the image boundary C in the image domain to divide the image into several regions that do not overlap each other and are approximately homogeneous. The energy expression of the traditional CV is\n( ) ( )( ) ( )( )\n( )( ) ( )( )( )\n( ) ( )( )\n2\n1 2 1\n2\n2\n, ,\n1\nJ f x H x dxdy\nf x H x dxdy\nv x x dxdy\n    \n \n  \n\n\n\n= \u2212\n+ \u2212 \u2212\n+ \n\n\n\n(2)\nImage segmentation is to minimize (2). The CV model is to find the best approximation of the vector 1 and 2 to minimize the energy function. The B-spline level set is a linear combination of multiple parameter B-spline functions, and the level set function is transformed into the B-spline space, thereby converting the discrete form to the continuous form to express the level set function (x) .\n  k (x) k ( ) d n xc k h    = \u2212 (3)\nwhere n is the B-spline basis function of the d-dimensional space,  kc is the coefficient of B-spline, and its set is used to present the level set.\nThe three channels of the color image are used as threedimensional vectors. The three channels are combined using Euclidean distance instead of gray value, and the edge information of each channel is effectively used. The distance at this time is closer to the physical meaning of the multidimensional space, and the problems caused by simple weighting can be avoided. The B-spline level set function is based on the Chan-Vese model. This piecewise constant divides the image into foreground and background regions. The energy function is\n( ) ( ) ( )( ) ( )( )\n( ) ( )( )( ) ( )( )\n( ) ( )( )\n3 2 2\n1, 2, 1, 1, 1\n1\n3 2 2\n2, 2, 1\n1\n1\n1 , ,\n3\n1 1 3\ni i i i d\ni\ni i d\ni\nd\nJ H x f x dx dx\nH x f x dx dx\nv x x dx dx\n     \n  \n  \n =\n =\n\n= \u2212\n+ \u2212 \u2212\n+ \n\n\n\n(4)\nwhere the first two items represent area items, and the third item is an outline item, and v is used to balance the two parameters which represent different items. H and  are Heaviside function and Dirac function respectively.  1, 1,1 1,2 1,3, ,i   = and  2, 2,1 2,2 2,3, ,i   = have three channels,  1, 1,1 1,2 1,3, ,i   = and  2, 2,1 2,2 2,3, ,i   = are the parameter weights corresponding to the three channels 1,i and 2,i .\nTake the partial derivative of coefficient 0[k ]c to\nminimize the energy function.\n0 1\n0\n(x) ( k ) [k ]\nn\nd\nJ x w dx dx\nc h  \n = \u2212\n  (5)\nwhere (x)w represents the characteristics of the object to be segmented.\n3 3 2 2\n1, 2,\n1 1\n1 1 (x) ( ( (x) ) ( (x) )\n3 3\n(x) ( )) ( (x))\n(x)\ni i\ni i\nw f f\nvdiv\n \n   \n= =\n= \u2212 \u2212 \u2212\n \u2212\n\n \n(6)\nThe local minimum value is solved by the gradient descent\nmethod.\n1 ( )i i icc c J c + = \u2212  (7)\nwhere c represents the energy gradient related to the Bspline coefficient,  indicates the number of iteration steps, and the expression of the energy gradient is\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\n  ( ) 1...\nn\nc d\nJ x J w x k dx dx\nc k h  \n    = = \u2212 \n    (8)\nThe minimization of the level set energy function is\nobtained by changing the B-spline coefficients v and h .\nC. 3D Positioning with RGB-D Camera With the above accurate two-dimensional target positioning, the depth range within the two-dimensional target range is acquired by Kinect2, and a 3D rectangular block can be obtained, which is the 3D area where the target is located.\nAs shown in Figure 5, it is a schematic diagram of 3D positioning based on the level set method of the RGB-D camera. In the real environment, the image obtained by Kinect2, the smallest rectangular frame surrounded by the edge obtained by the level set is expressed as a rectangle with the coordinates ( ),x y of the lower left corner, width w , and height h , and the depth range obtained by the RGB-D camera is  min max,d d . Finally, we can represent the 3D area where the target is located as  min max: , : , :x x w y y h d d+ + .\nKinect\u2019s level set.\n0\n0\n0 0\n0 0 0 1\n1 0 0 0 1\nx W\nc y W\nW\nf u Xu R T\nZ v f v Y\nZ\n           =                \n(9)\n0\n0\n0 0\n0 0\n0 0 0 1\nx\ny\nf u\nK f v\n    =      \n(10)\nwhere cZ is the depth value of RGB-D camera, ( ),u v is the value of the pixel coordinate system, xf and yf are the focal length, ( )0 0,u v is the origin of the pixel coordinate system, K is the internal parameter matrix, R and T form the external parameter matrix.\nThrough the above equations, the coordinate ( ), ,W W WX Y Z of the target point in the real-world coordinate system can be calculated."
        },
        {
            "heading": "III. Experiment and Discussion",
            "text": ""
        },
        {
            "heading": "A. EVALUATION INDICATOR",
            "text": "Target recognition and positioning metrics include image detection frame rate (Frame Per Second, FPS), mAP and IOU, and the evaluation is made from two perspectives:\nclassification and detection. mAP is a measure of algorithm performance, and the calculation formula is as follows.\n1\nN ii AP\nmAP N\n== \n(11)\nwhere N represents the number of categories, and AP represents the average accuracy of each category.\nThe calculation formula of the average precision AP is as follows:\n( ) ( ) 1 1 max n n n r r AP r r p r + +  = \u2212 (12)\nThe main factors affecting mAP are the quality and quantity of training data, the accuracy of the labeling frame and the variability of the data. But sometimes the change of mAP is not obvious when increasing the amount of training data. The higher the mAP value, the stronger the target detection ability. It can be seen from Figure 3 that Yolo-v3 consumes the least time when the mAP is at the same value.\n( )\n( )\nArea A B IOU\nArea A B\n =\n (13)\nThe intersection ratio IOU represents ratio of the area of the intersection of rectangular boxes A and B to area of the union of the rectangular boxes A and B, as shown in Figure 6.\nIn general, the target detection is successful when the IOU\nis greater than 0.5."
        },
        {
            "heading": "B. SIMULATION EXPERIMENTAL RESULTS AND ANALYSIS OF YOLO-V3",
            "text": "The experiments in this section are carried out on the Dell all-in-one computer with Intel Core i5-9500 and 8G RAM as the computer CPU and NVIDIA GTX 1050 as the graphics card, and vs2015 and opencv3.4.16\uff0ccuda10.2, Cudnn are configured.\nFor VOC data sets, the sequence of training before testing and final verification is adopted. The training set includes 9963 pictures of VOC2007 and 17125 pictures of VOC2012. The number of recognition types is 20, the number of iterations is 20000, and the number of batches is 64. After nearly 76 hours of training, the loss curve obtained is shown in Figure 7. It can be seen average loss value is 0.4104, and the curve shows a convergence trend.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\nAs shown in Table 2, the AP value of each method based on VOC dataset is compared. Here, the names of 20 categories of targets are aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike,\nperson, potted plant, sheep, sofa, train, tv monitor. For category names with more than four letters, only the first four letters are taken for abbreviation in Table 2. The data of each method comes from the sum of VOC2007 and VOC2012. The mAPs of methods FAST R-CNN [10], FASTER R-CNN [11], Yolo [13], SSD512 [23], ResNet [16] and Yolo-v3 are 68.4%, 70.4%, 57.9%, 74.9%, 73.8% and 73.3% respectively. Through comparison, it can be seen that mAP value of Yolov3 is the third largest, second only to SSD512 and ResNet, which is only 0.5% worse than ResNet, and 8 of the 20 categories of AP are the largest. The AP of bottle and chair is less than 60%, because the hardware configuration of the computer needs to be improved and the number of iterations is limited. Yolo-v3 will perform better if the graphics card replacement performance is better and the number of iterations is set more.\nIn the process of obtaining mAP, recall value is 0.68, mAP is 73.3%, and the average IOU value is 59.01%. After training, select Figure 8 in the test set for testing, and we can see that the result is correct.\nTABLE \u2161 AVERAGE PRECISION COMPARISONS WITH DIFFERENT METHODS BASED ON VOC DATASET\nMethods aero bicy bird boat bott bus car cat chai cow din dog hors moto pers pott shee sofa trai tv\nFast R-CNN 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 Faster R-CNN 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nYOLO 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 SSD512 87.4 82.3 75.8 59.0 52.6 81.7 81.5 90.0 55.4 79.0 59.8 88.4 84.3 84.7 83.3 50.2 78.0 66.3 86.3 72.0 ResNet 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 Yolo-v3 79.1 79.3 72.2 62.1 54.9 84.2 85.0 83.8 57.6 71.1 69.0 81.2 83.2 82.0 76.8 45.7 71.5 72.0 81.8 73.4\nFIGURE 8. VOC2007 test result."
        },
        {
            "heading": "C. EXPERIMENTAL VERIFICATION RESULTS AND",
            "text": "ANALYSIS IN REAL-WORLD Figure 9 shows the process of combining Yolo-v3 and level set. First, input the image to be detected, and then use Yolov3 to identify and coarsely locate the target in the input image. During positioning, there will be two situations where\nthe prediction frame box is smaller or larger than the real area of the target. In both cases, the prediction box is reduced by 10 pixels as the initial contour of the level set. After the level set algorithm obtains the edge of the target, the smallest rectangular frame corresponding to the level set algorithm is obtained, and finally the rectangular frame is mapped to the location of Yolo-v3.\nFIGURE 9. Processing flow of Yolo-v3 combined with level set.\nWhen conducting verification experiments in real scenes, we need to convert images into VOC format. In this paper, 15 targets in the real environment are used for training. Each target has 50 pictures, a total of 750 pictures. The 15 species are dog, person, table tennis, table tennis, rat, badminton, badminton racket, volleyball, penguin, basketball, fire hydrant, safe exit, football, switch, electric car and garbage\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\ncan. The loss curve after 20000 training iterations is shown in Figure 10. The training process takes 78.5 hours, the average loss value converges to 0.046, the recall value is 0.8, and the average IOU is 66.74%, which is better than the effect of training VOC data set in the previous section.\nThe AP and mAP values of these 15 types of targets are\nshown in Figure 11.\nFor the quantitative comparison of real scenes, we use the same type of dog and person in our own training dataset and VOC dataset. The performance comparisons between VOC and our dataset based on Yolo-v3 is described in Table 3, and it indicates that the AP, mAP and average IOU value are all higher than that of VOC. TABLE \u2162\nPERFORMANCE COMPARISONS BETWEEN VOC AND OUR DATASET\nDataset AP of dog AP of person mAP Average IOU\nVOC 81.2% 76.8% 73.3% 59.01%\nOur dataset 100% 83.6% 87.62% 66.74%\nWe shot the fire hydrant and safe exit from the real environment. The corresponding test results only using Yolo-\nv3 are shown in Figure 12, and the test results of B-spline level set and Yolo-v3 combined with B-spline level set are shown in Figure 13. The RGB-D camera kinect2 takes pictures with size of 1920\u00d71080. To facilitate detection, we reduced the image size to 512\u00d7288. In Figure 13, the target detection results are fire hydrant 100% [104,52164239], safe exit 100% [235,10147,67], and the detection process takes 425.733ms and 1246.365ms respectively. In Figure 13, the target location results combined with B-spline level set correction are fire hydrant [117,67,136,221], safe exit [250,14118,61].\n(a) B-spline level set in Fire hydrant\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2017\nYolo-v3 combined with B-spline level set method, the target positioning result is the rectangular frame of the real\ntarget. Therefore, the average IOU value of this method is 100%. As shown in Figure 14, it is a scene where the depth camera measures the depth. Combined with the depth information of the depth camera, the 3D position range of the target is further obtained. The ranges of fire hydrant and emergency exit are [78.8cm, 114.9cm] and [75.9cm, 83.4cm] respectively.\nFor clear comparison, Table 4 summarizes the specific\ndata of the above experiments. TABLE \u2163\nOBJECT DETECTION RESULT DATA STATISTICS.\nImage Fire hydrant Safe exit\nsize 512\u00d7288 512\u00d7288\nYolo-v3 Fire hydrant 100%\n[104,52,164,239]\nSafe exit 100% [235,10,147,67]\nLevel set [117,67,136,221] [250,14,118,61]\nDepth range/cm [78.8,114,9] [75.9,83.4]"
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "The method based on deep learning has high enough accuracy for target recognition, but the accuracy of target positioning needs to be improved. Therefore, this paper proposes a three-dimensional target recognition and precise positioning method. The method consists of three parts: Firstly, it recognizes and coarsely locates two-dimensional targets based on Yolo-v3. Secondly, The B-spline level set algorithm is improved to be used for color images and to correct the results of Yolo-v3 positioning. Finally, for the two-dimensional positioning corrected by level set algorithm, combine the depth information obtained by RGB-D camera to perform three-dimensional positioning. Simulation experiments show that the accuracy of target recognition based on Yolo-v3 can reach 87.62%, the mAP value tested in VOC data is only 73.3%, and the average IOU value based on Yolo-v3 and B-spline level set method can be improved from 59.01% to 66.74%, and finally to 100%. The real-world experimental results show that the method proposed in this\npaper can carry out the accurate three-dimensional target position based on the depth information of RGB-D camera."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This research is supported by Natural Science Basic Research Plan in Shaanxi Province of China (Grant No. 2017ZDJC-21)."
        }
    ],
    "title": "Object detection based on deep learning and B-spline level set in color images",
    "year": 2022
}