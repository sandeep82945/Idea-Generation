{
    "abstractText": "This work introduces a method for fitting to the degree distributions of complex network datasets, such that the most appropriate distribution from a set of candidate distributions is chosen while maximizing the portion of the distribution to which the model is fit. Current methods for fitting to degree distributions in the literature are inconsistent and often assume a priori what distribution the data are drawn from. Much focus is given to fitting to the tail of the distribution, while a large portion of the distribution below the tail is ignored. It is important to account for these low degree nodes, as they play crucial roles in processes such as percolation. Here, we address these issues, using maximum likelihood estimators to fit to the entire dataset or close to it. This methodology is applicable to any network dataset (or discrete empirical dataset), and we test it on over 25 network datasets from a wide range of sources, achieving good fits in all but a few cases. We also demonstrate that numerical maximization of the likelihood performs better than commonly used analytical approximations. In addition, we have made available a Python package which can be used to apply this methodology.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shane Mannion"
        }
    ],
    "id": "SP:009bd7d3e4f23c6190830f692c1e4a17d64dc1aa",
    "references": [
        {
            "authors": [
                "Barab\u00e1si",
                "A.-L",
                "R. Albert"
            ],
            "title": "Emergence of scaling in random networks",
            "year": 1999
        },
        {
            "authors": [
                "A.D. Broido",
                "A. Clauset"
            ],
            "title": "Scale-free networks are rare",
            "venue": "Nat. Commun.,",
            "year": 2019
        },
        {
            "authors": [
                "H. Ito",
                "K. Tamura",
                "T. Wada",
                "T. Yamamoto",
                "S. Morita"
            ],
            "title": "Is the network of heterosexual contact in Japan scale free",
            "venue": "PLoS ONE,",
            "year": 2019
        },
        {
            "authors": [
                "P. Holme"
            ],
            "title": "Rare and everywhere: perspectives on scale-free networks",
            "venue": "Nat. Commun.,",
            "year": 2019
        },
        {
            "authors": [
                "I. Voitalov",
                "P. van der Hoorn",
                "R. van der Hofstad",
                "D. Krioukov"
            ],
            "title": "Scale-free networks well done",
            "venue": "Phys. Rev. Res.,",
            "year": 2019
        },
        {
            "authors": [
                "M.L. Goldstein",
                "S.A. Morris",
                "G.G. Yen"
            ],
            "title": "Problems with fitting to the power-law distribution",
            "venue": "Eur. Phys. J. B Condens. Matter Complex Syst.,",
            "year": 2004
        },
        {
            "authors": [
                "A. Clauset",
                "C.R. Shalizi",
                "M.E. Newman"
            ],
            "title": "Power-law distributions in empirical data",
            "venue": "SIAM Rev.,",
            "year": 2009
        },
        {
            "authors": [
                "M.E. Newman",
                "S.H. Strogatz",
                "D.J. Watts"
            ],
            "title": "Random graphs with arbitrary degree distributions and their applications",
            "venue": "Phys. Rev. E,",
            "year": 2001
        },
        {
            "authors": [
                "N. Mehrabi",
                "F. Morstatter",
                "N. Peng",
                "A. Galstyan"
            ],
            "title": "Debiasing community detection: the importance of lowly connected nodes",
            "venue": "IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)",
            "year": 2019
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "The structure and function of complex networks",
            "venue": "SIAM Rev.,",
            "year": 2003
        },
        {
            "authors": [
                "H. Jeong",
                "S.P. Mason",
                "Barab\u00e1si",
                "A.-L",
                "Z.N. Oltvai"
            ],
            "title": "Lethality and centrality in protein",
            "venue": "networks. Nature,",
            "year": 2001
        },
        {
            "authors": [
                "R. Albert",
                "Barab\u00e1si",
                "A.-L"
            ],
            "title": "Statistical mechanics of complex networks",
            "venue": "Rev. Mod. Phys.,",
            "year": 2002
        },
        {
            "authors": [
                "S.H. Strogatz"
            ],
            "title": "Exploring complex networks. Nature, 410, 268\u2013276. D ow naded rom http/academ ic.p.com om net/articlecnad023/7227325 by Idian",
            "venue": "Intitute of Tenology Ptna user on 18 Jauary 2024 DEGREE DISTRIBUTIONS OF COMPLEX NETWORKS",
            "year": 2001
        },
        {
            "authors": [
                "A.M. Edwards",
                "R.A. Phillips",
                "N.W. Watkins",
                "M.P. Freeman",
                "E.J. Murphy",
                "V. Afanasyev",
                "S.V. Buldyrev",
                "M.G. Da Luz",
                "E.P. Raposo",
                "H.E. Stanley"
            ],
            "title": "Revisiting L\u00e9vy flight search patterns of wandering albatrosses, bumblebees and deer",
            "year": 2007
        },
        {
            "authors": [
                "M. Gerlach",
                "E.G. Altmann"
            ],
            "title": "Testing statistical laws in complex systems",
            "venue": "Phys. Rev. Lett.,",
            "year": 2019
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "Power laws, Pareto distributions and Zipf\u2019s law",
            "venue": "Contemp. Phys.,",
            "year": 2005
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "The structure of scientific collaboration networks",
            "venue": "Proc. Natl. Acad. Sci. USA.,",
            "year": 2001
        },
        {
            "authors": [
                "P.M. Gleiser",
                "L. Danon"
            ],
            "title": "Community structure in jazz",
            "venue": "Adv. Complex Syst.,",
            "year": 2003
        },
        {
            "authors": [
                "A. Lyon"
            ],
            "title": "Why are normal distributions normal",
            "venue": "Br. J. Philos. Sci.,",
            "year": 2014
        },
        {
            "authors": [
                "H. Akaike"
            ],
            "title": "A new look at the statistical model identification",
            "venue": "IEEE Trans. Automat. Contr.,",
            "year": 1974
        },
        {
            "authors": [
                "G. Schwarz"
            ],
            "title": "Estimating the dimension of a model",
            "venue": "Ann. Stat.,",
            "year": 1978
        },
        {
            "authors": [
                "F.J. Massey Jr."
            ],
            "title": "The Kolmogorov\u2013Smirnov test for goodness of fit",
            "venue": "J. Am. Stat. Assoc.,",
            "year": 1951
        },
        {
            "authors": [
                "M. Kratz",
                "S.I. Resnick"
            ],
            "title": "The QQ-estimator and heavy tails",
            "venue": "Stoch. Models,",
            "year": 1996
        },
        {
            "authors": [
                "M. H\u00f6glund",
                "A. Frigyesi",
                "F. Mitelman"
            ],
            "title": "A gene fusion network in human neoplasia",
            "year": 2006
        },
        {
            "authors": [
                "P. Mac Carron",
                "R. Kenna"
            ],
            "title": "Network analysis of the \u00cdslendinga s\u00f6gur\u2014the Sagas of Icelanders",
            "venue": "Eur. Phys. J. B,",
            "year": 2013
        },
        {
            "authors": [
                "L. Isella",
                "J. Stehl\u00e9",
                "A. Barrat",
                "C. Cattuto",
                "Pinton",
                "J.-F",
                "W. van Den Broeck"
            ],
            "title": "What\u2019s in a crowd? Analysis of face-to-face behavioral networks",
            "venue": "J. Theor. Biol.,",
            "year": 2011
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "Finding community structure in networks using the eigenvectors of matrices",
            "venue": "Phys. Rev. E,",
            "year": 2006
        },
        {
            "authors": [
                "T. Gessey-Jones",
                "C. Connaughton",
                "R. Dunbar",
                "R. Kenna",
                "P. MacCarron",
                "C. Oconchobhair",
                "J. Yose"
            ],
            "title": "Narrative structure of A Song of Ice and Fire creates a fictional world with realistic measures of social complexity",
            "venue": "Proc. Natl. Acad. Sci. USA.,",
            "year": 2020
        },
        {
            "authors": [
                "J. Moody"
            ],
            "title": "Peer influence groups: identifying dense clusters in large networks",
            "venue": "Soc. Netw.,",
            "year": 2001
        },
        {
            "authors": [
                "Rual",
                "J.-F",
                "K. Venkatesan",
                "T. Hao",
                "T. Hirozane-Kishikawa",
                "A. Dricot",
                "N. LI",
                "G.F. Berriz",
                "F.D. Gibbons",
                "M. Dreze",
                "N. Ayivi-Guedehoussou",
                "N. Klitgord"
            ],
            "title": "Towards a proteome-scale map of the human protein\u2013protein interaction",
            "venue": "network. Nature,",
            "year": 2005
        },
        {
            "authors": [
                "D.J. Watts",
                "S.H. Strogatz"
            ],
            "title": "Collective dynamics of \u2018small-world",
            "venue": "networks. Nature,",
            "year": 1998
        },
        {
            "authors": [
                "L. \u0160ubelj",
                "M. Bajec"
            ],
            "title": "Software systems through complex networks science: review, analysis and applications",
            "venue": "In Proceedings of the First International Workshop on Software Mining,",
            "year": 2012
        },
        {
            "authors": [
                "G. Joshi-Tope",
                "M. Gillespie",
                "I. Vastrik",
                "P. D\u2019eustachio",
                "E. Schmidt",
                "B. De Bono",
                "B. Jassal",
                "G. Gopinath",
                "G. Wu",
                "L. Matthews"
            ],
            "title": "Reactome: a knowledgebase of biological pathways",
            "venue": "Nucleic Acids Res.,",
            "year": 2005
        },
        {
            "authors": [
                "M. Bogun\u00e1",
                "R. Pastor-Satorras",
                "A. D\u00edaz-Guilera",
                "A. Arenas"
            ],
            "title": "Models of social networks based on social distance attachment",
            "venue": "Phys. Rev. E,",
            "year": 2004
        },
        {
            "authors": [
                "R. Dempsey",
                "A. Parnell",
                "P. McCarron",
                "G. McCarthy"
            ],
            "title": "Excess mortality in Dublin during the COVID-19 pandemic: Using RIP.ie as a geographical source",
            "venue": "Irish Geogr.,",
            "year": 2020
        },
        {
            "authors": [
                "J. Leskovec",
                "J. Kleinberg",
                "C. Faloutsos"
            ],
            "title": "Graph evolution: densification and shrinking diameters",
            "venue": "ACM Trans. Knowl. Data,",
            "year": 2007
        },
        {
            "authors": [
                "J. Leskovec",
                "J. McAuley"
            ],
            "title": "Learning to discover social circles in ego networks. Adv",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2012
        },
        {
            "authors": [
                "M. De Choudhury",
                "H. Sundaram",
                "A. John",
                "D.D. Seligmann"
            ],
            "title": "Social synchrony: Predicting mimicry of user actions in online social media",
            "venue": "In 2009 International Conference on Computational Science and Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "B. Zhang",
                "R. Liu",
                "D. Massey",
                "L. Zhang"
            ],
            "title": "Collecting the Internet AS-level topology",
            "venue": "ACM SIGCOMM Comput. Commun. Rev.,",
            "year": 2005
        },
        {
            "authors": [
                "B. Viswanath",
                "A. Mislove",
                "M. Cha",
                "K.P. Gummadi"
            ],
            "title": "On the evolution of user interaction in facebook",
            "venue": "In Proceedings of the 2nd ACM Workshop on Online Social Networks,",
            "year": 2009
        },
        {
            "authors": [
                "V. G\u00f3mez",
                "A. Kaltenbrunner",
                "V. L\u00f3pez"
            ],
            "title": "Statistical analysis of the social network and discussion threads in slashdot",
            "venue": "In Proceedings of the 17th International Conference on World Wide Web,",
            "year": 2008
        },
        {
            "authors": [
                "B. Klimt",
                "Y. Yang"
            ],
            "title": "The enron corpus: a new dataset for email classification research",
            "venue": "In European Conference on Machine Learning",
            "year": 2004
        },
        {
            "authors": [
                "J. Kunegis"
            ],
            "title": "2013d) Konect: the koblenz network collection",
            "venue": "In Proceedings of the 22nd International Conference on World Wide Web,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Journal of Complex Networks (2023) 4, Advance Access Publication on 18 July 2023 https://doi.org/10.1093/comnet/cnad023"
        },
        {
            "heading": "A robust method for fitting degree distributions of complex networks",
            "text": "Shane Mannion\u2020 and P\u00e1draig MacCarron\nMACSI, Department of Mathematics and Statistics, University of Limerick, Limerick V94 T9PX, Ireland \u2020Corresponding author. Email: shane.mannion@ul.ie\n[Received on 8 December 2022; editorial decision on 9 June 2023; accepted on 14 June 2023]\nThis work introduces a method for fitting to the degree distributions of complex network datasets, such that the most appropriate distribution from a set of candidate distributions is chosen while maximizing the portion of the distribution to which the model is fit. Current methods for fitting to degree distributions in the literature are inconsistent and often assume a priori what distribution the data are drawn from. Much focus is given to fitting to the tail of the distribution, while a large portion of the distribution below the tail is ignored. It is important to account for these low degree nodes, as they play crucial roles in processes such as percolation. Here, we address these issues, using maximum likelihood estimators to fit to the entire dataset or close to it. This methodology is applicable to any network dataset (or discrete empirical dataset), and we test it on over 25 network datasets from a wide range of sources, achieving good fits in all but a few cases. We also demonstrate that numerical maximization of the likelihood performs better than commonly used analytical approximations. In addition, we have made available a Python package which can be used to apply this methodology.\nKeywords: degree distribution; maximum likelihood estimators; power law; graph; network; complex networks."
        },
        {
            "heading": "1. Introduction",
            "text": "Although complex networks have been studied for around a quarter of a century, there is still a lack of consistency in determining one of their core properties\u2014namely, the degree distribution. One of the early observations about complex networks was that their degree distributions differed from those of the random graph models which had been studied for many years before. Specifically, it was observed that complex networks had right-skewed distributions and Barab\u00e1si and Albert [1] claim that these right tails often followed power-law distributions. Their work appears to show that different types of networks have different power-law exponents and these exponents are often in the range 2 \u2264 \u03b3 \u2264 3.\nHowever, Amaral et al. [2] argue that power-law distributions are not so common and suggest that for social networks other distributions are sometimes more appropriate. In addition they define three classes of small-world networks: power-law or \u2018scale-free\u2019 networks, \u2018broad-scale\u2019 networks (powerlaw distributions with sharp cut-offs) and \u2018single-scale\u2019 networks, which have distributions that decay quickly, such as exponential or Gaussian distributions. The distribution of the network determines which of these classes it belongs to.\nMore recently, Broido and Clauset [3] argue that scale-free networks are rare. They introduce a strict procedure for identifying power laws as well as several weaker regimes. This method, however, tends to be more focused on the tail of the distribution, as even the strongest criteria they outline only requires a power-law to be a good fit to the 50 highest-degree nodes and the method requires comparing to many simulated graphs which are generated under strict conditions. It is more concerned with whether or not evidence can be found to support or rule out a power-law distribution than being able to robustly identify the most appropriate distribution.\n\u00a9 The Author(s) 2023. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse,\ndistribution, and reproduction in any medium, provided the original work is properly cited.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nKnowing the distribution of a graph is important as it allows us to model and estimate other properties of the network structure. Similarly, it allows us to build general models for the formation of complex networks such as the preferential-attachment model [1]. The preferential-attachment model is still one of the most popular complex network formation models, though it requires the degree distribution to follow a power law or at least with slight deviations from it.\nThe question of what distribution a given network\u2019s degree distribution follows remains a heavily researched topic (see [3, 4]). In addition, some controversy exists around the prevalence of power-law distributions in empirical networks [3, 5, 6].\nDespite this, current methods for fitting degree distributions are inconsistent. Often still, the method of least squares is used to determine the parameter of a distribution, despite work highlighting the issues with this approach [7]. The method of least squares has several drawbacks; for example, in the case of a power-law distribution, one would plot the distribution on a log\u2013log scale and use least squares to determine the slope of the relatively straight line through the data. This assumes a priori that the data are a power-law however, and data appearing as a relatively straight line on a log\u2013log scale are not a sufficient condition to rule out other possibilities. Additionally, it is not applicable to data that are less easy to linearize.\nA better method is described in the work of Clauset et al. [8]. This uses the method of maximum likelihood to estimate the parameters of the distribution. This method is intended for general empirical datasets. A problem with using it to specifically fit degree distributions is that it focuses on fitting to the tail of the distribution, and in doing so can ignore huge amounts of the data available, as we will discuss in Section 3. Nodes with a low degree are important for percolation processes [9] and community identity in social networks [10], choosing a high degree cut-off will miss these.\nIn this article, we build on the method of Clauset et al. [8] in order to identify the most appropriate distribution for the degree distribution of a network from a set of candidate distributions and apply this methodology to a variety of network datasets. We do this while taking minimal approximations (e.g. using a minimum value of kmin), which are discussed in more detail throughout this article. The methods are applicable to any network dataset and do not rely on choosing things such as kmin or the distribution by eye.\nThe rest of this article proceeds as follows. In the next section, we cover important background information concerning some basics of network science. In Section 3, we cover power-law distributions, methods of estimating power-law exponents and maximum likelihood estimators (MLEs). Section 4 compares the estimates for the power-law exponent covered in the previous section. In Section 5, we introduce the distributions which we use in our methodology before discussing model selection and goodness of fit in Section 6. Section 7 describes in detail exactly how our methodology works. An in-depth analysis of the results we obtain and discussion around them are found in Sections 8 and 9.\nA Python package for the implementation of these methods is included in Appendix A."
        },
        {
            "heading": "2. Background",
            "text": "A network is defined as an ordered set of N nodes and M edges between them. In this work, we are concerned with unweighted and undirected networks, however the methods described can be easily applied to weighted degree distributions and directed networks. Nodes and edges can represent many things in different networks. For example, nodes may represent people in a social network and the edges represent friendship ties between them.\nThe degree of a node is the number of edges connected to it. The distribution of the degrees in a network gives some important characteristics related to its structure. The degree distribution pk is the\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nFig. 1. Degree distribution for yeast protein interactions on a log\u2013log scale (a) and the complementary cumulative degree distribution, again on a log\u2013log scale.\nfraction of vertices in a network with degree k,\npk = 1 N N\u2211 i=1 \u03b4kik, (2.1)\nwhere ki is the degree of node i and \u03b4ki ,k is the Kronecker delta function which is one if ki = k and zero otherwise. We are assuming there are no nodes of degree zero in the network. We use pk to denote probability mass functions (for discrete distributions) and p(k) to denote probability density functions (for continuous distributions).\nFor complex networks, degree distributions are often found to have a positive or right skew [11], which is also referred to as being heavy-tailed. These heavy tails are frequently due to a small number of vertices having very large degrees. Such highly connected nodes are often referred to as hubs. In contrast, Erdo\u030bs-R\u00e9nyi graphs tend to lack hubs due to having a small standard deviation in their degree when compared with complex networks.\nIn Fig. 1, we display the degree distribution for protein interactions in yeast, data taken from Ref. [12]. The distribution has a noisy tail due to many instances of only one vertex with a specific (large) degree. To reduce the noise in the tail, we often consider instead the complementary cumulative distribution function (CCDF) for the dataset [11]. This is the probability that a node has a degree greater than or equal to k and is denoted Pk:\nPk = \u221e\u2211\nj=0 pk+j = \u221e\u2211 q=k pq. (2.2)"
        },
        {
            "heading": "3. Power-law distributions",
            "text": "One of the most common and thoroughly studied distributions observed in complex networks is the power-law distribution [13, 14]. A power-law distribution has the form\npk \u223c k\u2212\u03b3 , (3.1)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nwhere the exponent of the power law, \u03b3 \u2265 1. This is the most basic definition of a power law, though the criteria for what is classified as a power law is a much debated topic in network science (see [3] for a summary of different definitions). As discussed in Section 1, this behaviour is usually found only in the tail of the distribution beginning at some value kmin. If we take logarithm of Equation (3.1),\nlog pk \u223c log k\u2212\u03b3 \u223c \u2212\u03b3 log k, (3.2)\nwe can see that the relationship between the log of pk and the log of k is linear. Hence, when a power law is displayed on a log\u2013log scale, it will appear linear.\nTherefore, one way in which we can obtain an estimate for the exponent \u03b3 is by performing a least squares regression on Equation (3.2). In this section, we look at this and other methods of estimating the power-law exponents. First, we must normalize Equation (3.1). We start with\nkmin\u22121\u2211 k=1 \u03c1k + kmax\u2211 k=kmin pk = 1, (3.3)\nwhere \u03c1k is the distribution below kmin. Our equation therefore becomes\npk = (1 \u2212 ) \u03b6(\u03b3 , kmin) k\u2212\u03b3 , (3.4)\nwhere\n\u2261 kmin\u22121\u2211\nk=0 \u03c1k and \u03b6(\u03b3 , kmin) \u2261 \u221e\u2211 n=0 (kmin + n)\u2212\u03b3 .\nHere, \u03b6(\u03b3 , kmin) is the Hurwitz-zeta function. In the case where the entire distribution follows a power law and kmin = 1, the denominator is just the Riemann zeta function \u03b6(\u03b3 ). The CCDF for the full degree distribution is given by\nPk = \u221e\u2211\nq=k pq = (1 \u2212 ) \u03b6(\u03b3 , k) \u03b6(\u03b3 , kmin) . (3.5)\n3.1 Continuous Approach\nOften when fitting, the approximation is made that the distribution is continuous instead of made up of discrete integers. We now consider the continuous regime to introduce an approximation for the exponent of a power law before returning to the discrete case. In the continuous case, Equation (3.1) can be normalized by\n\u222b kmin 0 \u03c1(k) dk + \u222b \u221e\nkmin\np(k) dk = 1,\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nwhere \u03c1(k) is some unknown distribution below kmin. The normalized distribution is then\np(k) = (1 \u2212 \u03b4)\u03b3 \u2212 1 kmin\n( k\nkmin\n)\u2212\u03b3 , (3.6)\nwhere \u03b4 \u2261 \u222b kmin0 \u03c1(k) dk is treated as constant. We can then integrate Equation (3.6) to determine the cumulative distribution function,\nP(k) = \u222b \u221e\nkmin\np(q) dq = (1 \u2212 \u03b4) ( k\nkmin\n)\u2212\u03b3+1 .\nWe can see from this that the cumulative distribution function also follows a power law but with an exponent differing by one to the original. As with Equation (3.2), a least squares fit to the cumulative distribution can be used to determine an estimate for the exponent. This estimate will usually provide a better fit than a fit to the probability density function as the cumulative distribution is less noisy in the tail [11].\n3.2 Moments of the distribution\nIf we make the assumption that the distribution can be used for the entire data (i.e. set kmin = 1) and thus \u03b4 \u2192 0, the probability density function Equation (3.6) is\np(k) = (\u03b3 \u2212 1)k\u2212\u03b3 . (3.7)\nSince the probability that k = 1 is bounded by unity, we must have \u03b3 < 2. The first moment of Equation (3.7) is\n\u3008k\u3009 = (\u03b3 \u2212 1) 1 2 \u2212 \u03b3\n[ k2\u2212\u03b3 ]\u221e 1 . (3.8)\nIf \u03b3 \u2264 2, then \u3008k\u3009 will diverge when we evaluate this integral (for the discrete case, it will be a large finite value). As a result, we should use kmin > 1 to remain consistent with k = 1 implying \u03b3 < 2. Empirically in complex networks we often observe that the mean degree is often low compared with the size of the network. Hence, choosing a high value of kmin could lead to the distribution only being fitted to a small number of nodes. Indeed, in the case of 1,000 simulated true power-law distributions with \u03b3 = 2.5, 918 of these have an average degree less than 3.\nIt may also be worth considering using the modal degree as the value for kmin as the distribution likely decays beyond this point. Clauset et al. [8] provide various methods for estimating kmin, however, they assume large values of N, and so these methods may not be applicable to smaller datasets. We aim to provide a methodology to fit to datasets of any size. Methods for choosing kmin will be discussed in Section 6. For now we continue in the continuous regime. If \u03b3 > 2 in Equation (3.8) then we can obtain an estimate for \u03b3 from the mean,\n\u3008k\u3009 = \u03b3 \u2212 1 \u03b3 \u2212 2 =\u21d2 \u03b3 = 2\u3008k\u3009 \u2212 1 \u3008k\u3009 \u2212 1 . (3.9)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nWe will use this as one of the estimates for the exponent. The same procedure can be repeated for the second moment and we obtain\n\u3008k2\u3009 = \u222b N\u22121\nKmin\nk2p(k) dk \u223c 1 3 \u2212 \u03b3\n[ k3\u2212\u03b3 ]N kmin .\nAs N goes to infinity, one observes that the second moment diverges when \u03b3 \u2264 3. Therefore, if the degree distributions follow a power law with \u03b3 \u2264 3, they would be expected to have a large value of \u3008k2\u3009.\n3.3 Maximum likelihood estimators\nFrom the continuous regime, we have three estimates for the exponent: (1) a least squares fit to the log\u2013 log probability mass function, (2) a least squares fit to the log\u2013log CCDF (exponent will be different by one) and (3) an estimate from the first moment of the degree distribution. We now turn our attention to the main focus of this article which is using MLEs to obtain estimates for distribution parameters.\nIn general, the likelihood L is the probability that an independent and identically distributed dataset of N observations were drawn from a model pk with parameter \u03b8 ,\nL (\u03b8 |k) = N\u220f i p\u03b8 (ki). (3.10)\nHere, ki is the degree of node i and N is the number of nodes in the network. As mentioned above, the complementary cumulative distribution in Equation (2.2) is often used to reduce noise in the tail of the probability distribution. Fits are then often made to the cumulative distribution Pk rather than the original distribution pk. It has been suggested that applying the method of least squares to cumulative distributions is unsuitable for empirical data [8, 15]. This is in part due to the data being discrete and therefore the continuous approach to the complementary cumulative distribution may not be appropriate (particularly for small datasets). Network data will always contain dependencies between the observations, violating one of the assumptions underlying maximum likelihood estimation [16]. Despite this, MLE still shows good results as we see below and shown in Ref. [3].\nFurthermore, the estimates for the parameters obtained using a least squares fit can be inaccurate and the error in the parameter value can be difficult to estimate [8]. Instead, MLEs are said to provide better measurements for the parameters as no estimator has lower asymptotic error in the large sample size as the MLE [8].\nSticking with the continuous regime for now, the likelihood corresponding to the power law from Equation (3.6) is\nL (\u03b3 |k) = N\u220f\ni=1 (1 \u2212 )\u03b3 \u2212 1 kmin\n( ki\nkmin\n)\u2212\u03b3 . (3.11)\nTaking the logarithm we obtain\nln L = N ln(1 \u2212 ) + N ln(\u03b3 \u2212 1) \u2212 N ln kmin \u2212 \u03b3 N\u2211\ni=1 ln ki kmin .\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nWe can obtain a value for from the data. This does not affect the maximization, however, it is important to have an estimate for when displaying the fit to the distribution. Typically, or \u2211kmin\u22121 k=1 \u03c1k as in Equation (3.3) is ignored in works such as Ref. [8] or any other methodology that focuses on the tail of the distribution. Here, we aim to account for it as fully as possible, though it does not affect the maximization.\nWe set \u2202(ln L /\u2202\u03b3 ) = 0 and solve analytically for an estimate for the power-law exponent. We obtain\n\u03b3\u0302 = 1 + N [\nN\u2211 i=1 ln ki kmin\n]\u22121 , (3.12)\nwhere \u03b3\u0302 denotes an estimate from the data rather than the true value. If a power law is a good model for the data then \u03b3\u0302 \u2248 \u03b3 . Our tests show however, that this estimate performs relatively poorly and this will be discussed in the next section (see Tables 1, A.1, and A.2).\nIn addition, complex network data, even when relatively large, are rarely a complete set of integers and there may be large gaps between consecutive degrees. It is therefore much more accurate to look at the discrete case from Equation (3.4). Taking the log likelihood of this, we get\nln L = N ln(1 \u2212 ) \u2212 N ln \u03b6(\u03b3 , kmin) \u2212 \u03b3 N\u2211\ni=1 ln ki. (3.13)\nThis can be numerically maximized to obtain an estimate for \u03b3 . Additionally, Clauset et al. [8] find an approximation for the log likelihood in Equation (3.13) as\n\u03b3\u0302 = 1 + N [\nN\u2211 i=1 ln ki kmin \u2212 12\n]\u22121 . (3.14)\nHowever, the convenience of this estimate comes at the cost of accuracy for smaller networks, particularly with small kmin as we show later. We compare all of these estimates in Section 4."
        },
        {
            "heading": "4. Comparison of estimates",
            "text": "In the previous section we outlined four ways of obtaining estimates for the power-law estimate \u03b3 . In this section, we will compare these four estimates along with estimates obtained by performing least squares on the log-transformed PDF and CCDF of the data.\nWe denote as \u03b3c the estimate obtained from the continuous version of the distribution as in Equation (3.12), \u03b3mle as the maximization of the discrete discrete log likelihood in Equation (3.13) and \u03b3d as the discrete approximation from Equation (3.14).\nAdditionally we define \u03b3pdf to be the value obtained by performing least squares regression on log\u2013 log plot of the probability density function and \u03b3ccdf to be the value obtained by performing least squares regression on the log\u2013log plot of the complementary cumulative distribution. Lastly, \u03b3\u3008k\u3009 is the estimate calculated from the first moment of the distribution as described by Equation (3.9).\nFor this we generated synthetic power-law distributions of varying sizes, all with a parameter \u03b3 = 2.5. We generate these using the NumPy package\u2019s built-in random number generators. For each value of N, we generated 1,000 distributions and calculated each estimate. The average estimates as well as their\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nTable 1 Values for each power-law exponent estimate discussed in Section 4 obtained on 1,000 datasets each of size N = 1, 000 with true parameter value \u03b3 = 2.5. Values shown are means with standard deviations in brackets. \u03b3c is the continuous estimator for the exponent from Equation (3.12), \u03b3pdf and \u03b3ccdf are the values obtained by least squares regression of the pdf and ccdf, respectively. \u03b3d is the value obtained by the discrete approximation Equation (3.14), \u03b3\u3008k\u3009 is the value obtained by the method of moments and \u03b3mle is the value obtained by maximum likelihood estimation. Best estimates for each value of kmin are highlighted in bold\nkmin \u03b3c \u03b3pdf \u03b3ccdf \u03b3d \u03b3\u3008k\u3009 \u03b3mle 1 4.47 (0.23) 2.57 (0.28) 2.53 (0.23) 2.02 (0.02) 3.15 (0.23) 2.50 (0.05) 2 3.28 (0.21) 2.34 (0.29) 2.45 (0.26) 2.38 (0.07) 2.29 (0.06) 2.50 (0.10) 3 3.00 (0.24) 2.18 (0.30) 2.43 (0.30) 2.46 (0.13) 2.17 (0.04) 2.52 (0.14) 4 2.88 (0.29) 2.04 (0.32) 2.43 (0.34) 2.50 (0.18) 2.12 (0.03) 2.53 (0.19) 5 2.81 (0.32) 1.89 (0.30) 2.41 (0.36) 2.51 (0.22) 2.09 (0.02) 2.53 (0.23) 6 2.77 (0.35) 1.74 (0.28) 2.37 (0.38) 2.53 (0.26) 2.07 (0.02) 2.54 (0.26) 7 2.75 (0.38) 1.62 (0.25) 2.37 (0.39) 2.54 (0.29) 2.06 (0.02) 2.56 (0.29) 8 2.74 (0.42) 1.52 (0.25) 2.37 (0.43) 2.55 (0.33) 2.06 (0.02) 2.56 (0.34) 9 2.73 (0.48) 1.42 (0.27) 2.35 (0.48) 2.57 (0.38) 2.05 (0.02) 2.57 (0.37) 10 2.76 (0.54) 1.37 (0.27) 2.37 (0.50) 2.60 (0.44) 2.05 (0.01) 2.60 (0.42)\nstandard distributions for N = 1, 000 are shown below in Table 1. Additional tables for graphs of size N = 100 and N = 10, 000 can be found in Appendix B. We see that in this case, with small kmin, the numerical maximization \u03b3mle performs best when compared with other estimates.\nIn Ref. [8], the authors say that the continuous estimate, \u03b3c is less accurate than the discrete estimate and no easier to calculate and so it should not be used. Our results here agree with this statement as we can see the continuous estimate is inaccurate even at large values of kmin. Instead, they suggest using the discrete approximation, however, we observe here that the numerically maximized discrete MLE performs better than the approximation.\nAs we can see here the least squares fit to the CCDF, \u03b3ccdf , is highly consistent under variation of kmin, however, Clauset et al. [8] also point out flaws with this method, such as the underlying assumptions being invalid. The fit to the PDF, \u03b3pdf , is always less accurate than that to the CCDF, on account of the PDF having a noisier tail [17]. In addition, as mentioned in Sections 1 and 3.3, there are drawbacks and limitations to applying the method of least squares to cumulative distributions.\nThe discrete estimate, \u03b3d, that Clauset et al. [8] recommend using is accurate at kmin = 3 and above, however, at this point already over 90% of the points will be missed. In Ref. [8], it is stated that this method works well only for kmin 6. With this we would run into a more extreme version of this problem and our tests show that it begins to become less accurate at this point.\nThe method calculated using the first moment of the distribution, \u03b3\u3008k\u3009 performs poorly for all values of kmin and we recommend that it should not be used.\nWe can see from Table 1 that in the case of small kmin, the numerical maximization of the log likelihood \u03b3d performs best of all the estimates compared here. This is not surprising, given that all other methods are approximations, however, we believe the increased accuracy of MLEs justifies the relative increase in difficulty of their implementation. There are several other advantages that MLEs have over the other estimates as well, as mentioned in Section 3.3.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nFurthermore, none of the other estimators here are applicable to any distribution in the way that MLEs are, as the estimates \u03b3c and \u03b3d are unique to power laws, and least squares regression is only applicable if the data can be easily linearized. The method of moments can be applied to some other distributions (e.g. exponential), however, its performance is still poor. Finally, all of these other estimates assume the distribution a priori, whereas MLEs allow for easy comparison of multiple distributions using information criteria. For these reasons, we argue that MLEs should be the standard approach for fitting distributions to network data.\nIndeed, this is a clarification needed when discussing the work of Clauset et al. [8]; the methodology they discuss is applied to empirical data, but not necessarily network distributional data, which are often small and incomplete, especially in the case of poor sampling. It should be noted as well that the above paper is more concerned with determining if a power-law distribution is an appropriate fit for data than it is with testing many distributions to find the most appropriate fit. In the succeeding sections, we will show all of the common distributions that one should consider when fitting a distribution to network data as well as equations for the log likelihood to be maximized, and some of the pitfalls that should be avoided in plotting these distributions against empirical data to ensure correct fitting."
        },
        {
            "heading": "5. Other distributions",
            "text": "Additionally we looked at the following distributions: (a) truncated power-law distributions, (b) exponential distributions, (c) stretched exponential distributions, (d) Weibull distributions, (e) Poisson distributions, (f) normal (Gaussian) distributions and (g) lognormal distributions. Most of these distributions are for continuous variables, but here we discretize and normalize them in order to obtain their corresponding log likelihoods. We choose these as they are common in formation processes and the complex network literature (e.g. [2, 8, 18]).\n5.1 Truncated power-law distributions\nThe second class of network described by Amaral et al. [2] contains a power-law regime followed by a sharp cut-off. Albert and Barab\u00e1si [13] provide a list of exponents and cut-offs for a range of complex networks. A truncated power law or a power law with exponential cut-off has the form\npk \u223c k\u2212\u03b3 e\u2212k/\u03ba . (5.1) A truncated power law is shown for the collaboration network of the condensed matter arXiv from 1995 to 1999 in Ref. [19]. The log likelihood then is\nln L = N ln(1 \u2212 ) + Nkmin \u03ba\n\u2212 N ln Z(kmin) \u2212 N\u2211\ni=1\n( \u03b3 ln ki + ki\n\u03ba\n) , (5.2)\nwhere Z(kmin) \u2261 \u2211\u221em=0(x + m)\u2212\u03b3 e\u2212m/\u03ba . We then maximize this numerically. Details of obtaining Equation (5.2) from Equation (5.1) are shown in Appendix A.\n5.2 Exponential and Weibull distributions\nThe final class of network described by Amaral et al. is so-called single-scale networks characterized by fast-decaying tails. Although they are rarely used, these classifications provide a useful ordering by complexity of the different types of distributions.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nWe first look at the exponential distribution which is given by\npk \u223c e\u2212k/\u03ba . (5.3)\nIts log likelihood is\nln L = N ln(1 \u2212 ) + N ln (1 \u2212 e\u22121/\u03ba) \u2212 1 \u03ba N\u2211 i=1 (ki \u2212 kmin). (5.4)\nThe exponential distribution can be generalized to a stretched exponential distribution which has been found to be a good fit for some empirical datasets, such as the jazz band network in Ref. [20], however, the stretched exponential and Weibull distributions are related with the stretched exponential being the CCDF of the Weibull distribution. Hence, we only include the Weibull here.\nThe Weibull distribution is given by\npk \u223c ( k\n\u03ba\n)\u03b2\u22121 e(k/\u03ba) \u03b2 . (5.5)\nAs we can see when \u03b2 = 1, the Weibull simplifies to the exponential distribution. When \u03b2 = 2, it is closely related to the normal distribution (below). Its log likelihood is\nln L = N ln(1 \u2212 ) \u2212 N ln \u239b \u239d \u221e\u2211\nm=kmin\n( k\n\u03ba\n)\u03b2\u22121 e(\u2212k/\u03ba) \u03b2 \u239e \u23a0 + N ln ((k/\u03ba)\u03b2\u22121) + N ln (e(k/\u03ba)\u03b2 ) . (5.6)\nDetails of this calculation can once again be found in Appendix A.\n5.3 Normal and lognormal distributions\nThe normal distribution (or Gaussian) is given by\npk \u223c e\u2212 (k\u2212\u03bc)2 2\u03c32 , (5.7)\nwhere \u03bc is the mean and \u03c3 2 is the variance. This distribution is included here as it is found to fit some social networks [2], however, given the skewed nature of network degree sequences it is unlikely to be chosen in most cases. The log likelihood is given by\nln L = N ln(1 \u2212 ) \u2212 N ln \u239b \u239d \u221e\u2211\nm=kmin e\n\u2212 (m\u2212\u03bc)2 2\u03c32 \u239e \u23a0 \u2212 N\u2211\ni=1\n(ki \u2212 \u03bc)2 2\u03c3 2 . (5.8)\nThe lognormal distribution then is given by\npk \u223c 1 k e \u2212 (ln k\u2212\u03bc)2 2\u03c32 . (5.9)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nThe log likelihood is given by\nln L = N ln(1 \u2212 ) \u2212 N ln \u239b \u239d \u221e\u2211\nm=kmin\n1 m e \u2212 (ln(m)\u2212\u03bc)2 2\u03c32\n\u239e \u23a0 \u2212 N\u2211\nk=kmin ln(k) \u2212 N\u2211 k=kmin (ln(k) \u2212 \u03bc)2 2\u03c3 2 . (5.10)\n5.4 Poisson distribution\nThe last distribution we test for these network datasets is the Poisson distribution. It is given by\npk = \u03bb k\nk! e \u2212\u03bb. (5.11)\nThe log likelihood then is\nln L = N ln(1 \u2212 ) \u2212 ln ( 1 \u2212 e\u2212\u03bb kmin\u22121\u2211\nm=0\n\u03bbm m!\n) \u2212 N\u03bb + ln \u03bb\nN\u2211 i=1 ki \u2212 N\u2211 i=1 ln(ki!). (5.12)\nAgain, details of obtaining this equation as well as those for the normal and lognormal distributions can be found in Appendix A. As with the normal distribution, a Poisson is an unlikely candidate for most network datasets given their skewed nature, however, both are included here for completeness given their ubiquity in other empirical datasets and scientific settings [21, 22].\n6. Model selection, choosing kmin and goodness of fit\nAs discussed in Section 4, there are several advantages to using maximum likelihood estimation. Most important is the ability to compare the likelihoods of different distributions in order to choose the best distribution for a given dataset. In order to compare different models we use the Akaike information criterion with correction for finite sample sizes (AICc) [23, 24] and the Bayesian information criterion (BIC) [25]. The AICc is given by\nAICc = \u22122 ln L (\u03b8 |ki) + 2n\u03b8 + 2n\u03b8 (n\u03b8 + 1) N \u2212 n\u03b8 \u2212 1 (6.1)\nand the BIC is given by\nBIC = \u22122 ln L (\u03b8 |ki) + n\u03b8 ln N. (6.2)\nIn each equation, n\u03b8 is the number of parameters in the model and N is the sample size. This AICc gives greater penalty for the number of parameters and the penalty is diminished with a larger sample size. Anderson and Burnham [24] recommend always using the AICc instead of the regular AIC (which is the same as Equation (6.1) but without the last term) as in the case of large N the correction tends to zero.\nWe also make use of AIC weights. For a given set of m models for a given dataset, the AIC weights are given by\nWi = e \u2212 i/2\u2211m\n1 e \u2212 i/2 , (6.3)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nwhere i = AICci \u2212 min(AICc). These values represent the relative likelihood of the models and allow us to determine if there is support for more than one model. The values of the weights will sum to 1 and the closer a value is to one the more support for that model there is. Usually the weights are accompanied by some cut-off, below which we say a model is not supported.\nSince the AICc and the BIC both depend on N, we can only compare two distributions that start at the same value of kmin. For this reason, the model and kmin must be chosen in tandem. It is important to remember that to capture the true distribution of a dataset, it should be a priority that kmin be minimized. How exactly we achieve this is discussed in Sections 7 and 8.\nGoodness-of-fit tests for this type of analysis leave much to be desired. Two common means of assessing goodness-of-fit are (i) Kolmogorov-Smirnov tests (KS tests) and (ii) Q\u2013Q plots, both of which have their drawbacks. Descriptions of how these tests work can be found in Refs. [26] and [27], respectively.\nFirst, with network datasets, we are often dealing with sample sizes of the order of 10,000 nodes or more. These sample sizes are in contrast to the work done by Goldstein et al. [7], who find success using the KS test on sample sizes of the order of 1,000 nodes. The critical value for a KS test at a 5% level of significance in the limit of large n is given by 1.36/ \u221a n, where n is the sample size or in our case the number of nodes. Our tests find that the KS test is simply too restrictive and will tell the user that even the best of our fits are poor.\nQuantile\u2013quantile or Q\u2013Q plots (shown in Fig. 4) are useful in many cases but since they require visual interpretation, they are not suitable in a situation where automation is desired. Furthermore, they are sensitive to outliers or extreme values, which are common across all of our datasets."
        },
        {
            "heading": "7. Fitting",
            "text": "Here, we outline the methodology used to choose distributions for datasets based on the methods discussed already. The datasets to which this methodology was applied can be found in Table A.3. For a given degree sequence, the methodology works as follows:\n(i) Beginning with kmin = 1, we find the parameter values that maximize the log likelihood functions of each of the distributions discussed above.\n(ii) The AICc and AIC weights for each distribution are calculated. The user may choose to use the AICc or the BIC to choose the best distribution or both.\n(iii) We increase kmin by one and repeat steps (i) and (ii). Once the same distribution is chosen for three consecutive kmin values we choose this distribution for the dataset. In the case of small datasets (here chosen to be fewer than 2,500 nodes), the same distribution need only be chosen for two consecutive kmin values. The value 2,500 was chosen based on our datasets; below this value, requiring the same distribution to be chosen three times simply resulted in cutting off too much of the data below kmin.\n(iv) We use the determined distribution and parameters to fit a curve to the CCDF of the degree sequence for visual purposes. At this point, both the CCDF and the PDF should be inspected, as we find that looking at just the CCDF is insufficient to determine if we have obtained a good fit or not. Other goodness-of-fit metrics such as a Q\u2013Q plots can be used. If one determines that the fit obtained is poor, then kmin should be increased and the algorithm repeated. We find that we can best determine the degree distribution when using all available information. We will explore this further in the \u2018Results\u2019 section.\n(v) We then bootstrap the dataset 1,000 times and fit the same distribution to our bootstrapped samples in order to obtain means, standard deviations and other desired summary statistics for the parameters of our fitted distributions. These summary statistics are shown in Table A.5.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nWhen fitting we must account for data below kmin. This is done using our empirical CCDF. If, for example, we have kmin = 3, then we multiply the values in our fitted CCDF by \u22113k=0 p(k) \u2261 P(3). This ensures that the fitted curve begins at the correct point.\nThere are some limitations for what can be done given a network dataset. Many networks are poorly sampled or not a complete sample. Consider the Enron email dataset [49], here emails between employees were recorded but not emails to external addresses, i.e. contacts outside of the organization would not add to an individual\u2019s degree. Therefore, we do not have the actual degrees of nodes and so conclusions from the dataset about the degree distribution cannot be extended to the network as a whole. In this case, we may just want to fit to the tail and not the full distribution so a high value of kmin may be appropriate.\nIn contrast to this is the power-grid network studied in Ref. [36]. Here, the degree is the number of physical connections to a generator, a transformer or a substation and so the degree of a node is correct to the underlying network. In this case, a high kmin may result in a different distribution being chosen for the tail and not be representative of the full network.\nKeeping these limitations in mind, this methodology can be applied to any distribution of discrete integers. In the following section, we will see how it performs on a wide variety of network types."
        },
        {
            "heading": "8. Results",
            "text": "We now look at some of the results obtained by using the above described methodology on a collection of network datasets, some of our own, one sexual contact network taken from Ref. [4] and many taken from the Konect database. Konect stands for the Koblenz Network Collection [50]; an open-access online database of network datasets from various areas of science. A full list of the datasets we applied the methodology to as well as a description of each dataset can be seen in Table A.3 in Appendix B. A summary of some basic properties of these networks can be found in Table A.4 in Appendix B.\nFor most networks that we looked at, we obtain good estimates for the parameters of the chosen distributions, as can be seen from the closely fitting curves to the datapoints. Note as well that in the majority of these cases we begin at kmin values of 1 and 2, in line with our priority of minimizing kmin.\nThe results shown in Fig. 2 are representative of the results as a whole (results for all other datasets can be seen in Figs A.1 and A.2). In fact, based on its CCDF, the Google+ (Fig. 2(h)) dataset appears to be the worst fit of any dataset displayed. One possible reason for this relatively poor fit is the very noisy tail of the distribution, which can be seen in the PDF of this distribution in Fig. 6. This noisy tail is caused by many hubs of different degrees, i.e. there are many points with different degrees but equal probabilities, hence the long horizontal line of points in Fig. 6(b). This is in contrast to the more \u2018wellbehaved\u2019 PDF of the astrophysics dataset in Fig. 6(a). Even in this case, looking at the PDF, we can see that there is not that much room for improvement in the fit.\nAccording to the CCDF in Fig. 5(b) for the Linux dataset, we appear to obtain a bad fit here too. However, looking at the PDF (Fig. 5(a)), we see that we actually have quite a good fit. In this case, the sharp drop off in CCDF values may have been caused by finite size effects, cumulative errors in individual probability values or something else. The lesson here is that we cannot judge fit based solely on the CCDF.\nA counter example to this is the Twitter dataset whose CCDF and PDF can be seen in Figs A.1(v) and A.2(v), respectively. Here, we see a poor fit to both the PDF and CCDF, highlighting the need to look at both before determining if a distribution is a good fit or not.\nIn Fig. 2(g), we have the Gene Fusion dataset. Overall, this fits the data well, however the dataset only contains approximately 250 nodes, hence we have large distances between the predicted and observed probabilities. As mentioned in step (iii) of the algorithm, in the case of such small datasets,\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nFig. 2. Results for a selection of the networks whose degree distributions we fitted. As we can see in most cases, we manage to obtain excellent fits with low kmin values in all cases. Notable exceptions are Gene Fusion and Google+, which are discussed in turn in Section 8.\nthe methodology is altered slightly so that only two votes for the same distribution are required for the distribution to be chosen, as we found that a larger value results in too large a portion of the data being excluded.\nWe can see that in cases such as the Facebook dataset in Fig. 2(c), we obtain a good fit for the entire distribution. In the work of Clauset et al. [8], it would be reasonable to choose a kmin value even as high as kmin \u2248 110. In doing so all of the data below this point would be ignored, which equates to over 99% of the values, and with our methodology we have fit to the entire dataset. Other methodologies would similarly ignore the bulk of the data and fit a power law to the tail, using one of the estimators shown in Table 1 based on it appearing as a straight line when plotted on a log\u2013log scale.\nSome fits may be poor while prioritizing a low value of kmin, but in this situation we would increase kmin to see if we can obtain a better fit. Examples of this can be seen in the Internet (CAIDA), Internet AS 2 and Internet Topology datasets, for which we get a very poor fit for low values of kmin but see improvement when we increase kmin. This is in contrast to the Twitter dataset, for which we find that even larger values of kmin struggle to obtain a good fit. In this case, perhaps some less common distribution which we have not covered here would provide a better fit.\nIn the case of the RIP Ireland dataset, we see that the chosen distribution is a Weibull with parameters \u03ba = 183.14, \u03b2 = 0.99. Since \u03b2 is so close to 1 this is essentially an exponential distribution. However,\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nFig. 3. Weibull distribution and a lognormal distribution fit to the Petster dataset at different kmin values as described in Section 8.\nFig. 4. Q\u2013Q plots of a Weibull distribution (left) and a lognormal distribution (right) fit to the Petster dataset as described in Section 8.\nsince an exponential distribution is a special case of a Weibull distribution, and one can always achieve a higher likelihood by adding more parameters [8], the likelihood of an exponential will always be less than that of a Weibull. In this case, we rely on the information criteria to sufficiently penalize the distribution with more parameters. If using the Bayesian information criteria with kmin = 1, we obtain an exponential fit for this distribution, though not for kmin = 2 or greater. With the advantage of a lower kmin and a simpler distribution, an exponential distribution is likely the better choice here. Results for this dataset shown in Table A.5 are for a Weibull distribution to illustrate our point.\nIn all other cases, we find that choosing BIC instead of AIC has no effect on what distribution is chosen.\nLastly, we focus on the Petstser dataset, which is a more ambiguous example. Initially, we fit using the steps outlined in the previous section, which tells us that the data follow a Weibull distribution for k \u2265 3. This fit is shown in Fig. 3.\nSuppose we feel that based on the CCDF we could improve the fit to the tail of the distribution. We increase kmin to 4 and repeat the algorithm, which gives the lognormal again shown in Fig. 3.\nThis distribution arguably fits better in the tail of the distribution, so how do we choose between the two options? Looking at the PDF for both distributions we see there is very little appreciable difference, aside from the different kmin values. We then look at the Q\u2013Q plots for each distribution, shown in Fig. 4.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nFig. 5. PDF and CCDF of fits to the Linux dataset. As we can see, both the PDF and CCDF should be accounted for when assessing the fit.\nFig. 6. Probability density functions for the Astrophysics dataset (left) and Google+ dataset (right). As we can see the Google+ data are far noisier in the tail, making the distribution more difficult to fit to.\nAs we can see here the Weibull appears to be a better fit for the distribution, as the data follow the diagonal more closely (both begin to deviate at the tail which is to be expected when dealing with extreme values).\nWhen the choice between two distributions is so close, we must look at the AIC weights. At kmin = 3, the weights for the Weibull and lognormal distributions were 0.99 and 0.01, respectively. At kmin = 5, these values were 0.3 and 0.7. Because of this, at kmin = 3, we can rule out a lognormal distribution. However, at kmin = 5, there is still some support for a Weibull distribution.\nCombining all of this information, along with our priority of minimizing kmin we conclude that the Weibull distribution is the better fit for the data. In the case of more ambiguous fits, or wanting to improve the fit to a distribution, we recommend this approach of looking at all available information.\nAll in all, we can see that the methodology described in this article performs well in a variety of scenarios, and only does poorly in a few cases. This may be improved upon by testing other distributions, as here we have only tested a handful of commonly found distributions.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024"
        },
        {
            "heading": "9. Discussion",
            "text": "For the majority of the time that the field of complex networks has been studied, questions around the degree distributions of networks have persisted to be left without satisfying answers. Here, we have built upon the work of Clauset et al. [8] to provide a methodology that introduces more rigour to the process of determining the degree distribution of a network dataset. We have three key differences to their method.\nFirstly, we suggest taking a low value of kmin to consider as many of the nodes as possible. Depending on the network\u2019s distribution, even a value of 3 or larger could end up fitting to less than 10% of the network. This is only desirable when a network is poorly sampled and we are just interested in the tail. If we suspect the network is well sampled, then a high value of kmin will miss the majority of the network. These often-ignored low degree nodes are important in many contexts [51], playing crucial roles in processes such as percolation [9], as well community detection in social networks [10].\nSecondly, we show that the parameter estimate obtained through numerical maximization of Equation (3.13) is superior to the estimate given by Equation (3.14), especially for low values of kmin. Taking the approximation for smaller networks with low values of kmin will give poor results and possibly rule out a power law even if that is the true distribution.\nThirdly, we use information criteria to choose the model rather than constructing a p-value from the Kolmogorov\u2013Smirnov tests. These methods do not tell us about the goodness of fit but allow us to choose the most appropriate of the candidate models for this dataset.\nAs we can see from the results in Section 8, using maximum likelihood estimation to determine the most appropriate fit from a set of candidate models performs well on a variety of network datasets with complex distributions. This work improves the standard of distribution fitting, with most if not all of the data being fit to in all cases, through minimization of kmin. This process is not entirely automated and we believe this should be the case. In some cases, Q\u2013Q plots may be necessary to interpret the results and one may need to check both the CCDF and PDF. These are noisy empirical datasets so caution must be taken and an automated process could easily miss these subtleties.\nAfter checking many different distributions, one may wonder why we are so interested in the specific distribution a complex network has. Networks have different underlying formation processes. For example, if we know the degree distribution follows a Poisson or binomial distribution, then we know this network if formed by purely a random process. If we know nodes prefer to connect to high degree nodes, we know it\u2019s a preferential-attachment process. Hence, if we know the underlying distribution, we can come up with a theory that will give rise to that distribution.\nThis is one of the reasons why the power law is so popular, not only is it instrumental in many statistical physics theories, and required for the epidemic threshold in disease transmission models, but it is the distribution underlying the preferential-attachment model. It is also a single parameter distribution making it quite desirable for fitting with low statistical complexity. In our results, we find support for power laws in many internet-based networks. However, this does not seem like a good mechanism for many of the social networks we analyse.\nPower laws suggest that some nodes will have an extremely high degree, often approaching the system size, something not realistic for social systems. In the social networks here for example, they are better fitted by Weibull or log-normal distributions. In fact the RIP Ireland data are well fitted by a Weibull with \u03b2 \u2248 1\u2014i.e. an exponential. With the exception of two outliers (people known in the media) no one has a lifetime degree even close to 2,000 (for a system size > 4 million). Therefore, preferential attachment seems like a poor mechanism for social networks in particular with their fast decaying tails.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nMany other processes for social networks have been suggested even in the early years of complex networks, for example, Ebel et al. [18] propose a mechanism which will give rise to a stretched exponential. This is closer to the results that we observe. We aim to test this further going forward to identify an appropriate mechanism for social networks.\nThe foundation presented here performs well and can be extended to improve its performance further. In the future, we aim to extend the analysis both to more datasets and to test more parametric distributions. Another avenue for the extension of this work that we aim to explore is allowing for fitting of more than one distribution function to distributions in which we feel the tail behaves differently to the body of the distribution, i.e. splitting the distribution at some value k\u2217 and fitting two different distributions to the data above and below this value.\nWe believe that this widely applicable and user-friendly approach provides much needed rigour in answering a fundamental question about the properties of a network."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank David O\u2019Sullivan, Samuel Unicomb and James Gleeson for their suggestions and comments on the manuscript.\nFunding\nThis publication has emanated from research conducted with the financial support of Science Foundation Ireland under Grant number 18/CRT/6049.\nReferences\n1. Barab\u00e1si, A.-L. & Albert, R. (1999) Emergence of scaling in random networks. Science, 286, 509\u2013512. 2. Amaral, L. A. N., Scala, A., Barthelemy, M. & STANLEY, H. E. (2000) Classes of small-world networks.\nProc. Natl. Acad. Sci. USA, 97, 11149\u201311152. 3. Broido, A. D. & Clauset, A. (2019) Scale-free networks are rare. Nat. Commun., 10, 1\u201310. 4. Ito, H., Tamura, K., Wada, T., Yamamoto, T. & Morita, S. (2019) Is the network of heterosexual contact in\nJapan scale free? PLoS ONE, 14, e0221520. 5. Holme, P. (2019) Rare and everywhere: perspectives on scale-free networks. Nat. Commun., 10, 1\u20133. 6. Voitalov, I., van der Hoorn, P., van der Hofstad, R. & Krioukov, D. (2019) Scale-free networks well done.\nPhys. Rev. Res., 1, 033034. 7. Goldstein, M. L., Morris, S. A. & Yen, G. G. (2004) Problems with fitting to the power-law distribution. Eur.\nPhys. J. B Condens. Matter Complex Syst., 41, 255\u2013258. 8. Clauset, A., Shalizi, C. R. & Newman, M. E. (2009) Power-law distributions in empirical data. SIAM Rev.,\n51, 661\u2013703. 9. Newman, M. E., Strogatz, S. H. & Watts, D. J. (2001) Random graphs with arbitrary degree distributions\nand their applications. Phys. Rev. E, 64, 026118. 10. Mehrabi, N., Morstatter, F., Peng, N. & Galstyan, A. (2019) Debiasing community detection: the impor-\ntance of lowly connected nodes. In 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, pp. 509\u2013512. 11. Newman, M. E. (2003) The structure and function of complex networks. SIAM Rev., 45, 167\u2013256. 12. Jeong, H., Mason, S. P., Barab\u00e1si, A.-L. & Oltvai, Z. N. (2001) Lethality and centrality in protein networks.\nNature, 411, 41\u201342. 13. Albert, R. & Barab\u00e1si, A.-L. (2002) Statistical mechanics of complex networks. Rev. Mod. Phys., 74, 47. 14. Strogatz, S. H. (2001) Exploring complex networks. Nature, 410, 268\u2013276.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\n15. Edwards, A. M., Phillips, R. A., Watkins, N. W., Freeman, M. P., Murphy, E. J., Afanasyev, V., Buldyrev, S. V., Da Luz, M. G., Raposo, E. P., Stanley, H. E. & others (2007) Revisiting L\u00e9vy flight search patterns of wandering albatrosses, bumblebees and deer. Nature, 449, 1044\u20131048. 16. Gerlach, M. & Altmann, E. G. (2019) Testing statistical laws in complex systems. Phys. Rev. Lett., 122, 168301. 17. Newman, M. E. (2005) Power laws, Pareto distributions and Zipf\u2019s law. Contemp. Phys., 46, 323\u2013351. 18. Ebel, H., Davidsen, J. & Bornholdt, S. (2002) Dynamics of social networks. Complexity, 8, 24\u201327. 19. Newman, M. E. (2001) The structure of scientific collaboration networks. Proc. Natl. Acad. Sci. USA., 98,\n404\u2013409. 20. Gleiser, P. M. & Danon, L. (2003) Community structure in jazz. Adv. Complex Syst., 6, 565\u2013573. 21. Johnson, N. L., Kotz, S. & Kemp, A. W. (2005) Univariate Discrete Distributions, vol. 444, John Wiley &\nSons. 22. Lyon, A. (2014) Why are normal distributions normal?. Br. J. Philos. Sci., 65, 621\u2013649. 23. Akaike, H. (1974) A new look at the statistical model identification. IEEE Trans. Automat. Contr., 19, 716\u2013723. 24. Anderson, D. & Burnham, K. (2004) Model Selection and Multi-model Inference, vol. 63, 2nd end. New York:\nSpringer-Verlag, p. 10. 25. Schwarz, G. (1978) Estimating the dimension of a model. Ann. Stat., 6, 461\u2013464. 26. Massey Jr, F. J. (1951) The Kolmogorov\u2013Smirnov test for goodness of fit. J. Am. Stat. Assoc., 46, 68\u201378. 27. Kratz, M. & Resnick, S. I. (1996) The QQ-estimator and heavy tails. Stoch. Models, 12, 699\u2013724. 28. H\u00f6glund, M., Frigyesi, A. & Mitelman, F. (2006) A gene fusion network in human neoplasia. Oncogene, 25,\n2674\u20132678. 29. Mac Carron, P. & Kenna, R. (2013) Network analysis of the \u00cdslendinga s\u00f6gur\u2014the Sagas of Icelanders. Eur.\nPhys. J. B, 86, 1\u20139. 30. Isella, L., Stehl\u00e9, J., Barrat, A., Cattuto, C., Pinton, J.-F. & van Den Broeck, W. (2011) What\u2019s in a\ncrowd? Analysis of face-to-face behavioral networks. J. Theor. Biol., 271, 166\u2013180. 31. Newman, M. E. (2006) Finding community structure in networks using the eigenvectors of matrices. Phys. Rev.\nE, 74, 036104. 32. Gessey-Jones, T., Connaughton, C., Dunbar, R., Kenna, R., MacCarron, P., Oconchobhair, C. & Yose,\nJ. (2020) Narrative structure of A Song of Ice and Fire creates a fictional world with realistic measures of social complexity. Proc. Natl. Acad. Sci. USA., 117, 28582\u201328588. 33. Kunegis, J. (2013a) Konect. http://konect.cc/networks/petster-hamster/. Accessed 6 July 2023. 34. Moody, J. (2001) Peer influence groups: identifying dense clusters in large networks. Soc. Netw., 23, 261\u2013283. 35. Rual, J.-F., Venkatesan, K., Hao, T., Hirozane-Kishikawa, T., Dricot, A., LI, N., Berriz, G. F., Gibbons,\nF. D., Dreze, M., Ayivi-Guedehoussou, N. & Klitgord, N. (2005) Towards a proteome-scale map of the human protein\u2013protein interaction network. Nature, 437, 1173\u20131178. 36. Watts, D. J. & Strogatz, S. H. (1998) Collective dynamics of \u2018small-world\u2019 networks. Nature, 393, 440\u2013442. 37. \u0160ubelj, L. & Bajec, M. (2012) Software systems through complex networks science: review, analysis and\napplications. In Proceedings of the First International Workshop on Software Mining, pp. 9\u201316. 38. Joshi-Tope, G., Gillespie, M., Vastrik, I., D\u2019eustachio, P., Schmidt, E., De Bono, B., Jassal, B., Gopinath,\nG., Wu, G., Matthews, L. & Others (2005) Reactome: a knowledgebase of biological pathways. Nucleic Acids Res., 33(Suppl_1), D428\u2013D432. 39. Kunegis, J. (2013b) Konect. http://konect.cc/networks/dimacs10-as-22july06/. Accessed 6 July 2023. 40. Bogun\u00e1, M., Pastor-Satorras, R., D\u00edaz-Guilera, A. & Arenas, A. (2004) Models of social networks based\non social distance attachment. Phys. Rev. E, 70, 056122. 41. Dempsey, R., Parnell, A., McCarron, P. & McCarthy, G. (2020) Excess mortality in Dublin during the\nCOVID-19 pandemic: Using RIP.ie as a geographical source. Irish Geogr., 53, 163\u2013172. 42. Leskovec, J., Kleinberg, J. & Faloutsos, C. (2007) Graph evolution: densification and shrinking diameters.\nACM Trans. Knowl. Data, 1, 2\u2013es. 43. Leskovec, J. & McAuley, J. (2012) Learning to discover social circles in ego networks. Adv. Neural Inform.\nProcess. Syst., 25.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\n44. De Choudhury, M., Sundaram, H., John, A. & Seligmann, D. D. (2009) Social synchrony: Predicting mimicry of user actions in online social media. In 2009 International Conference on Computational Science and Engineering, vol. 4. IEEE, pp. 151\u2013158. 45. Kunegis, J. (2013c) Konect. http://konect.cc/networks/linux/. Accessed 6 July 2023. 46. Zhang, B., Liu, R., Massey, D. & Zhang, L. (2005) Collecting the Internet AS-level topology. ACM SIGCOMM\nComput. Commun. Rev., 35, 53\u201361. 47. Viswanath, B., Mislove, A., Cha, M. & Gummadi, K. P. (2009) On the evolution of user interaction in\nfacebook. In Proceedings of the 2nd ACM Workshop on Online Social Networks, pp. 37\u201342. 48. G\u00f3mez, V., Kaltenbrunner, A. & L\u00f3pez, V. (2008) Statistical analysis of the social network and discussion\nthreads in slashdot. In Proceedings of the 17th International Conference on World Wide Web, pp. 645\u2013654. 49. Klimt, B. & Yang, Y. (2004) The enron corpus: a new dataset for email classification research. In European\nConference on Machine Learning. Springer, pp. 217\u2013226. 50. Kunegis, J. (2013d) Konect: the koblenz network collection. In Proceedings of the 22nd International Confer-\nence on World Wide Web, pp. 1343\u20131350. http://konect.cc/. Accessed 6 July 2023. 51. Tanaka, G., Morino, K. & Aihara, K. (2012) Dynamical robustness in complex networks: the crucial role of\nlow-degree nodes. Sci. Rep., 2, 1\u20136."
        },
        {
            "heading": "A. Code and other distributions",
            "text": "A.1 Code\nA documented package with which the results in this article were produced is available at https://github.com/Shaneul/MLE.\nA.2 Truncated power-law distribution\nNormalizing the truncated power law from Equation (5.1) yields\npk = (1 \u2212 ) e kmin/\u03ba\nZ(kmin) k\u2212\u03b3 e\u2212k/\u03ba , (A.1)\nwhere Z(x) = \u221em=0(x + m)\u2212\u03b3 e\u2212m/\u03ba . The log likelihood then is straightforward to obtain from this and shown in Equation (5.2).\nA.3 Exponential distribution\nStarting with Equation (5.3), we expand like so\n1 \u2212 = C kmax\u2211 k=kmin e\u2212k/\u03ba = C (e\u2212kmin/\u03ba + e\u2212kmin+1/\u03ba + ...) .\nTo normalize the distribution, we multiply the summation here above and below by 1 \u2212 e\u22121/\u03ba like so\nC 1 1 \u2212 e\u22121/\u03ba ( 1 \u2212 e\u22121/\u03ba) (e\u2212kmin/\u03ba + e\u2212kmin+1/\u03ba + ...)\n= C 1 \u2212 e\u22121/\u03ba\n( e\u2212kmin/\u03ba + e\u2212kmin+1/\u03ba + ... \u2212 e\u2212(kmin+1)/\u03ba \u2212 e\u2212(kmax+1)/\u03ba).\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nAll but the first and last terms here cancel, leaving us with\nC 1 \u2212 e\u22121/\u03ba ( e\u2212kmin/\u03ba \u2212 e\u2212(kmax+1)/\u03ba) ,\nand so\nC = (1 \u2212 ) 1 \u2212 e \u22121/\u03ba\ne\u2212kmin/\u03ba \u2212 e\u2212(kmax+1)/\u03ba .\nIf kmax \u2192 \u221e then we get\nC = (1 \u2212 )1 \u2212 e \u22121/\u03ba\ne\u2212kmin/\u03ba ,\nand this leads to the normalized exponential distribution\np(k) = (1 \u2212 ) (\n1 \u2212 e\u22121/\u03ba e\u2212kmin/\u03ba\n) e\u2212k/\u03ba , (A.2)\nfrom which the log likelihood Equation (5.4) is obtained.\nA.4 Weibull distribution\nStarting with Equation (5.5), we first define the constant to normalize the distribution.\n(1 \u2212 ) = C \u221e\u2211\ni=kmin (k/\u03ba)\u03b2\u22121e\u2212(k/\u03ba) \u03b2 .\nThis gives\nC = (1 \u2212 )\u2211\u221e i=kmin(k/\u03ba) \u03b2\u22121e\u2212(k/\u03ba)\u03b2 ,\nand with C defined as above, we have\np(k) = C(k/\u03ba)\u03b2\u22121e\u2212(k/\u03ba)\u03b2 .\nThe likelihood then is given by\nL = N\u220f (1 \u2212 )\u2211\u221e\nm=kmin(m/\u03ba) \u03b2\u22121e\u2212(m/\u03ba)\u03b2\n(k/\u03ba)\u03b2\u22121e\u2212(k/\u03ba) \u03b2 ,\nfrom which the log likelihood Equation (5.6) is obtained\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nA.5 Poisson distribution\nStarting with Equation (5.11), we normalize like so\n1 \u2212 = C \u221e\u2211\nk=kmin\n\u03bbk k! e \u2212\u03bb.\nAdding and subtracting the same term from the above equation leads to\n1 \u2212 = C \u23a1 \u23a3 \u221e\u2211\nk=kmin\n\u03bbk k! e \u2212\u03bb + kmin\u22121\u2211 k=0 \u03bbk k! e \u2212\u03bb \u2212 kmin\u22121\u2211 k=0 \u03bbk k! e \u2212\u03bb\n\u23a4 \u23a6\n= Ce\u2212\u03bb ( \u221e\u2211\nk=0\n\u03bbk k! \u2212 kmin\u22121\u2211\nk=0\n\u03bbk k!\n)\n= Ce\u2212\u03bb ( e\u03bb \u2212 kmin\u22121\u2211\nk=0\n\u03bbk k!\n)\n= C ( 1 \u2212 e\u2212\u03bb kmin\u22121\u2211\nk=0\n\u03bbk k!\n) .\nThis gives\nC = (1 \u2212 ) ( 1 \u2212 e\u2212\u03bb kmin\u22121\u2211\nk=0\n\u03bbk k!\n)\u22121 .\nEquation (5.11) then becomes\np(k) = (1 \u2212 ) ( 1 \u2212 e\u2212\u03bb kmin\u22121\u2211\nk=0\n\u03bbk k! e \u2212\u03bb\n)\u22121 \u03bbk\nk! e \u2212\u03bb,\nand our log likelihood is as shown in Equation (5.12).\nA.6 Lognormal distribution\nStarting with Equation (5.9), we normalize in the usual way\n1 \u2212 = C \u221e\u2211\nk=kmin\n1 ki e\n\u2212 (ln ki\u2212\u03bc) 2\n2\u03c32 .\nThis gives\nC = (1 \u2212 ) \u239b \u239d \u221e\u2211\nk=kmin\n1 k e \u2212 (ln k\u2212\u03bc)2 2\u03c32\n\u239e \u23a0 \u22121\n.\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nEquation (5.9) then becomes\np(k) = (1 \u2212 ) \u239b \u239d \u221e\u2211\nk=kmin\n1 k e \u2212 (ln k\u2212\u03bc)2 2\u03c32\n\u239e \u23a0 \u22121 1\nk e\n\u2212 (ln k\u2212\u03bc)2 2\u03c32 ,\nand the log likelihood Equation (5.10) is obtained from this."
        },
        {
            "heading": "B. Additional tables and figures",
            "text": "Below are a collection of supporting tables and figures referenced throughout the article.\nTable A.1 Values for each power-law exponent estimate discussed in Section 4 obtained on 1,000 datasets each of size N = 100 with true parameter value \u03b3 = 2.5. Values shown are means with standard deviations in brackets. \u03b3c is the continuous estimator for the exponent from Equation (3.12), \u03b3pdf and \u03b3ccdf are the values obtained by least squares regression of the pdf and ccdf, respectively. \u03b3d is the value obtained by the discrete approximation Equation (3.14), \u03b3\u3008k\u3009 is the value obtained by the method of moments and \u03b3mle is the value obtained by maximum likelihood estimation. Best estimates for each value of kmin are highlighted in bold\nkmin \u03b3c \u03b3pdf \u03b3ccdf \u03b3d \u03b3\u3008k\u3009 \u03b3mle 1 4.49 (0.19) 2.76 (0.23) 2.72 (0.21) 2.02 (0.02) 3.31 (0.13) 2.51 (0.04) 2 3.23 (0.21) 2.24 (0.28) 2.37 (0.24) 2.35 (0.08) 2.27 (0.06) 2.48 (0.10) 3 2.98 (0.20) 2.20 (0.24) 2.46 (0.22) 2.45 (0.11) 2.17 (0.02) 2.51 (0.12) 4 2.90 (0.27) 2.09 (0.37) 2.48 (0.36) 2.51 (0.17) 2.12 (0.02) 2.55 (0.18) 5 2.58 (0.20) 1.71 (0.17) 2.20 (0.21) 2.36 (0.14) 2.08 (0.02) 2.37 (0.15) 6 2.77 (0.25) 1.75 (0.26) 2.40 (0.33) 2.53 (0.19) 2.08 (0.02) 2.54 (0.19) 7 2.75 (0.43) 1.59 (0.29) 2.29 (0.44) 2.54 (0.34) 2.06 (0.02) 2.55 (0.34) 8 2.63 (0.43) 1.45 (0.22) 2.27 (0.32) 2.46 (0.35) 2.05 (0.02) 2.47 (0.36) 9 2.91 (0.58) 1.44 (0.24) 2.47 (0.34) 2.71 (0.46) 2.06 (0.01) 2.72 (0.47) 10 2.59 (0.49) 1.27 (0.18) 2.33 (0.61) 2.46 (0.41) 2.04 (0.02) 2.46 (0.42)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nTable A.2 Values for each power-law exponent estimate discussed in Section 4 obtained on 1,000 datasets each of size N = 10, 000 with true parameter value \u03b3 = 2.5. Values shown are means with standard deviations in brackets. \u03b3c is the continuous estimator for the exponent from Equation (3.12), \u03b3pdf and \u03b3ccdf are the values obtained by least squares regression of the pdf and ccdf, respectively. \u03b3d is the value obtained by the discrete approximation Equation (3.14), \u03b3\u3008k\u3009 is the value obtained by the method of moments and \u03b3mle is the value obtained by maximum likelihood estimation. Best estimates for each value of kmin are highlighted in bold\nkmin \u03b3c \u03b3pdf \u03b3ccdf \u03b3d \u03b3\u3008k\u3009 \u03b3mle 1 4.46 (0.07) 2.60 (0.19) 2.51 (0.14) 2.02 (0.01) 3.08 (0.13) 2.50 (0.02) 2 3.27 (0.06) 2.48 (0.20) 2.48 (0.16) 2.37 (0.02) 2.27 (0.03) 2.50 (0.03) 3 2.96 (0.07) 2.39 (0.19) 2.47 (0.16) 2.45 (0.04) 2.16 (0.02) 2.50 (0.04) 4 2.83 (0.08) 2.31 (0.20) 2.46 (0.17) 2.47 (0.05) 2.11 (0.01) 2.50 (0.05) 5 2.76 (0.09) 2.22 (0.20) 2.44 (0.18) 2.49 (0.07) 2.08 (0.01) 2.50 (0.07) 6 2.72 (0.10) 2.17 (0.20) 2.45 (0.19) 2.49 (0.07) 2.07 (0.01) 2.50 (0.08) 7 2.68 (0.11) 2.09 (0.20) 2.43 (0.20) 2.50 (0.09) 2.06 (0.01) 2.50 (0.09) 8 2.66 (0.12) 2.02 (0.19) 2.43 (0.20) 2.50 (0.10) 2.05 (0.01) 2.51 (0.10) 9 2.65 (0.13) 1.95 (0.19) 2.42 (0.22) 2.50 (0.11) 2.04 (0.01) 2.51 (0.11) 10 2.63 (0.14) 1.91 (0.19) 2.43 (0.22) 2.51 (0.12) 2.04 (0.01) 2.51 (0.12)\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nTable A.4 Quantities for each of our studied networks. N is the number of nodes, L is the number of edges, \u3008k\u3009 is the mean degree and kmax is the maximum degree\nName N L \u3008k\u3009 kmax Gene Fusion 291 279 1.92 34 Laxdaela 338 894 5.29 45 Infectious Disease 410 2,765 13.49 50 Network Science 1,461 2,742 3.75 34 ASoIaF 1,786 14,478 16.21 215 Yeast 1,870 2,277 2.44 56 Petster 2,426 16,631 13.71 273 Moreno Health 2,539 10,455 8.24 27 Human Proteins 3,133 6,726 4.29 129 US Powergrid 4,941 6,594 2.67 19 Sexual Contact 4,999 23,979 9.59 985 Java Classes 6,120 50,290 16.43 5,655 Reactome 6,327 147,547 46.64 855 Internet AS 1 6,474 13,895 4.29 1,460 PGP 10,680 24,316 4.55 205 RIP Ireland 15,310 1,410,272 184.23 4,235 Astrophysics 18,771 198,050 21.10 504 Internet AS 2 22,963 48,436 4.22 2,390 Twitter 23,370 32,831 2.81 238 Gplus 23,628 39,194 3.32 2,761 Internet (CAIDA) 26,475 53,381 4.03 2,628 Munmun Digg 30,398 86,312 5.68 285 Linux 30,837 213,747 13.86 9,340 Internet Topology 34,761 107,720 6.20 2,760 Facebook Wall 46,952 193,494 8.24 223 Facebook 63,731 817,035 25.64 1,098 Slashdot 79,116 467,731 11.82 2,534 Enron 87,273 299,220 6.86 1,728\nD ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nDEGREE DISTRIBUTIONS OF COMPLEX NETWORKS 27 Ta bl e A .5 N et w or k da ta se ts an d th ei r co rr es po nd in g di st ri bu ti on s, al lf ig ur es ro un de d to tw o de ci m al pl ac es .x 1 is th e fir st pa ra m et er fo r a di st ri bu ti on ,x\u0304 1 is th e m ea n pa ra m et er ob ta in ed fr om fit ti ng th is di st ri bu ti on to bo ot st ra pp ed sa m pl es .\u03c3 x 1 is th e st an da rd de vi at io n fo r th is pa ra m et er .P 2. 5 (x 1 ) an d P 97 .5 (x 1 ) ar e th e 2. 5t h an d 97 .5 th pe rc en ti le s of th e pa ra m et er s, re sp ec ti ve ly .T he se va lu es ar e th en re pe at ed fo r x 2 in th e ca se of tw opa ra m et er di st ri bu ti on s N et w or k D is tr ib ut io n k m in x 1 x\u0304 1 \u03c3 x 1 P 2. 5 (x 1 ) P 97 .5 (x 1 ) x 2 x\u0304 2 \u03c3 x 2 P 2. 5 (x 2 ) P 97 .5 (x 2 ) G en e Fu si on Po w er la w 1 2. 50 2. 51 0. 11 2. 32 2. 72 L ax da el a L og no rm al 1 1. 27 1. 28 0. 05 1. 17 1. 37 0. 90 0. 92 0. 05 0. 83 1. 02 In fe ct io us D is ea se W ei bu ll 1 14 .9 9 15 .0 4 0. 50 14 .0 8 16 .0 7 1. 62 1. 60 0. 08 1. 45 1. 76 N et w or k Sc ie nc e L og no rm al 1 1. 02 1. 02 0. 02 0. 98 1. 06 0. 77 0. 78 0. 02 0. 74 0. 82 A So Ia F W ei bu ll 1 12 .6 8 12 .6 9 0. 49 11 .7 3 13 .6 2 0. 75 0. 75 0. 02 0. 71 0. 80 Y ea st Pr ot ei ns L og no rm al 2 0. 01 0. 24 0. 85 0. 00 4. 17 1. 11 1. 06 0. 21 0. 01 1. 17 Pe ts te r W ei bu ll 3 7. 40 7. 31 0. 65 6. 06 8. 69 0. 62 0. 62 0. 03 0. 57 0. 67 M or en o H ea lth L og no rm al 2 0. 00 82 0. 26 0. 92 0. 00 4. 31 1. 11 1. 05 0. 23 0. 01 1. 17 H um an Pr ot ei ns L og no rm al 1 0. 64 0. 66 0. 52 0. 21 0. 72 1. 16 1. 17 0. 12 1. 11 1. 41 U S Po w er gr id L og no rm al 1 0. 82 0. 82 0. 01 0. 80 0. 84 0. 59 0. 59 0. 01 0. 58 0. 60 Se xu al C on ta ct L og no rm al 1 1. 31 1. 31 0. 28 1. 24 1. 36 1. 30 1. 30 0. 07 1. 25 1. 37 Ja va C la ss es L og no rm al 1 2. 05 2. 05 0. 01 2. 04 2. 07 0. 84 0. 84 0. 01 0. 81 0. 87 R ea ct om e W ei bu ll 1 25 .6 0 25 .4 0 1. 79 23 .8 8 27 .1 4 0. 56 0. 55 0. 11 0. 52 0. 56 In te rn et A S 1 Po w er la w 2 2. 30 2. 30 0. 02 2. 26 2. 34 PG P W ei bu ll 2 0. 17 0. 14 0. 05 0. 07 0. 25 0. 33 0. 32 0. 07 0. 29 0. 36 R IP Ir el an d W ei bu ll 1 18 3. 14 18 3. 20 1. 60 18 0. 16 18 6. 45 0. 99 0. 99 0. 01 0. 97 1. 00 A st ro ph ys ic s W ei bu ll 2 10 .8 7 10 .8 2 0. 26 10 .3 0 11 .3 1 0. 59 0. 59 0. 01 0. 57 0. 60 In te rn et A S 2 Po w er la w 5 2. 09 2. 09 0. 02 2. 04 2. 13 Tw itt er Po w er la w 1 2. 47 2. 47 0. 01 2. 44 2. 49 G pl us Po w er la w 1 2. 38 2. 38 0. 01 2. 36 2. 40 In te rn et (C A ID A ) Po w er la w 5 2. 11 2. 11 0. 02 2. 07 2. 15 M un m un D ig g W ei bu ll 1 0. 48 0. 47 0. 03 0. 41 0. 54 0. 37 0. 37 0. 01 0. 36 0. 38 L in ux L og no rm al 1 1. 85 1. 85 0. 01 1. 84 1. 86 1. 02 1. 02 0. 01 1. 01 1. 03 In te rn et to po lo gy Po w er la w 5 1. 92 1. 92 0. 01 1. 90 1. 95 Fa ce bo ok w al l W ei bu ll 1 4. 29 4. 29 0. 06 4. 18 4. 40 0. 61 0. 61 0. 00 0. 61 0. 62 Fa ce bo ok W ei bu ll 1 14 .0 0 14 .0 1 0. 15 13 .7 2 14 .3 0 0. 58 0. 58 0. 00 0. 57 0. 58 Sl as hd ot W ei bu ll 1 0. 05 2. 34 4. 67 0. 05 12 .0 2 0. 23 1. 54 1. 66 0. 18 4. 00 E nr on W ei bu ll 3 0. 05 0. 05 0. 00 0. 05 0. 05 0. 22 0. 26 0. 39 0. 22 0. 22 D ow nloaded from https://academ ic.oup.com /com net/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nnet/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024\nnet/article/11/4/cnad023/7227325 by Indian Institute of Technology Patna user on 18 January 2024"
        }
    ],
    "title": "A robust method for fitting degree distributions of complex networks",
    "year": 2023
}