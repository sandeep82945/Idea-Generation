{
    "abstractText": "Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chao Feng"
        },
        {
            "affiliations": [],
            "name": "Ziyang Chen"
        },
        {
            "affiliations": [],
            "name": "Andrew Owens"
        }
    ],
    "id": "SP:006f725454b51ef259be01f761d86b23dc6bcd38",
    "references": [
        {
            "authors": [
                "Triantafyllos Afouras",
                "Yuki M. Asano",
                "Francois Fagan",
                "Andrea Vedaldi",
                "Florian Metze"
            ],
            "title": "Self-supervised object detection from audio-visual correspondence, 2021",
            "year": 2021
        },
        {
            "authors": [
                "T. Afouras",
                "J.S. Chung",
                "A. Senior",
                "O. Vinyals",
                "A. Zisserman"
            ],
            "title": "Deep audio-visual speech recognition",
            "venue": "In arXiv:1809.02108,",
            "year": 2018
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Lrs3-ted: a large-scale dataset for visual speech recognition",
            "venue": "arXiv preprint arXiv:1809.00496,",
            "year": 2018
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Asr is all you need: Cross-modal distillation for lip reading",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Andrew Owens",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Self-supervised learning of audio-visual objects from video",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Shruti Agarwal",
                "Hany Farid",
                "Ohad Fried",
                "Maneesh Agrawala"
            ],
            "title": "Detecting deep-fake videos from phonemeviseme mismatches",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Juan Le\u00f3n Alc\u00e1zar",
                "Fabian Caba",
                "Ali K Thabet",
                "Bernard Ghanem"
            ],
            "title": "Maas: Multi-modal assignation for active speaker detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Relja Arandjelovic",
                "Andrew Zisserman"
            ],
            "title": "Objects that sound",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yuki Asano",
                "Mandela Patrick",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Labelling unlabelled videos from scratch with multi-modal self-supervision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "R\u00e9jean Ducharme",
                "Pascal Vincent"
            ],
            "title": "A neural probabilistic language model",
            "venue": "Advances in neural information processing systems,",
            "year": 2000
        },
        {
            "authors": [
                "Tiziano Bianchi",
                "Alessandro Piva"
            ],
            "title": "Image forgery localization via block-grained analysis of jpeg artifacts",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2012
        },
        {
            "authors": [
                "Luca Bondi",
                "Silvia Lameri",
                "David Guera",
                "Paolo Bestagini",
                "Edward J Delp",
                "Stefano Tubaro"
            ],
            "title": "Tampering detection and localization through clustering of camera-based cnn features",
            "venue": "In CVPR Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Adrian Bulat",
                "Georgios Tzimiropoulos"
            ],
            "title": "How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)",
            "venue": "In International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Lucy Chai",
                "David Bau",
                "Ser-Nam Lim",
                "Phillip Isola"
            ],
            "title": "What makes fake images detectable? understanding properties that generalize",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ken Chatfield",
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Return of the devil in the details: Delving deep into convolutional nets",
            "venue": "arXiv preprint arXiv:1405.3531,",
            "year": 2014
        },
        {
            "authors": [
                "H Chen",
                "W Xie",
                "T Afouras",
                "A Nagrani",
                "A Vedaldi",
                "A Zisserman"
            ],
            "title": "Audio-visual synchronisation in the wild",
            "venue": "In Proceedings of the 32nd British Machine Vision Conference. British Machine Vision Association,",
            "year": 2021
        },
        {
            "authors": [
                "Ziyang Chen",
                "David F Fouhey",
                "Andrew Owens"
            ],
            "title": "Sound localization by self-supervised time delay estimation",
            "venue": "In Computer Vision\u2013ECCV 2022:",
            "year": 2022
        },
        {
            "authors": [
                "Komal Chugh",
                "Parul Gupta",
                "Abhinav Dhall",
                "Ramanathan Subramanian"
            ],
            "title": "Not made for each other-audiovisual dissonance-based deepfake detection and localization",
            "venue": "In Proceedings of the 28th ACM international conference on multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Joon Son Chung",
                "Arsha Nagrani",
                "Andrew Zisserman"
            ],
            "title": "Voxceleb2: Deep speaker recognition",
            "venue": "Proc. Interspeech",
            "year": 2018
        },
        {
            "authors": [
                "J.S. Chung",
                "A. Zisserman"
            ],
            "title": "Lip reading in the wild",
            "venue": "In Asian Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Out of time: automated lip sync in the wild",
            "venue": "In Asian conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Soo-Whan Chung",
                "Joon Son Chung",
                "Hong-Goo Kang"
            ],
            "title": "Perfect match: Improved cross-modal embeddings for audiovisual synchronisation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Davide Cozzolino",
                "Matthias Nie\u00dfner",
                "Luisa Verdoliva"
            ],
            "title": "Audio-visual person-of-interest deepfake detection",
            "venue": "arXiv preprint arXiv:2204.03083,",
            "year": 2022
        },
        {
            "authors": [
                "Davide Cozzolino",
                "Luisa Verdoliva"
            ],
            "title": "Single-image splicing localization through autoencoder-based anomaly detec- 9 tion",
            "venue": "IEEE international workshop on information forensics and security (WIFS),",
            "year": 2016
        },
        {
            "authors": [
                "Davide Cozzolino",
                "Luisa Verdoliva"
            ],
            "title": "Noiseprint: a cnnbased camera model fingerprint",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2019
        },
        {
            "authors": [
                "Dario D\u2019Avino",
                "Davide Cozzolino",
                "Giovanni Poggi",
                "Luisa Verdoliva"
            ],
            "title": "Autoencoder with recurrent neural networks for video forgery detection",
            "venue": "arXiv preprint arXiv:1708.08754,",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Akshay Raj Dhamija",
                "Manuel G\u00fcnther",
                "Terrance Boult"
            ],
            "title": "Reducing network agnostophobia",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ricard Durall",
                "Margret Keuper",
                "Janis Keuper"
            ],
            "title": "Watch your up-convolution: Cnn based generative deep neural networks are failing to reproduce spectral distributions",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jianwei Fei",
                "Yunshu Dai",
                "Peipeng Yu",
                "Tianrun Shen",
                "Zhihua Xia",
                "Jian Weng"
            ],
            "title": "Learning second order local anomaly for general face forgery detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Gereon Fox",
                "Wentao Liu",
                "Hyeongwoo Kim",
                "Hans-Peter Seidel",
                "Mohamed Elgharib",
                "Christian Theobalt"
            ],
            "title": "Videoforensicshq: Detecting high-quality manipulated face videos",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2021
        },
        {
            "authors": [
                "Joel Frank",
                "Thorsten Eisenhofer",
                "Lea Sch\u00f6nherr",
                "Asja Fischer",
                "Dorothea Kolossa",
                "Thorsten Holz"
            ],
            "title": "Leveraging frequency analysis for deep fake image recognition",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ruohan Gao",
                "Kristen Grauman"
            ],
            "title": "Visualvoice: Audiovisual speech separation with cross-modal consistency",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hui Guo",
                "Shu Hu",
                "Xin Wang",
                "Ming-Ching Chang",
                "Siwei Lyu"
            ],
            "title": "Eyes tell all: Irregular pupil shapes reveal gangenerated faces",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Alexandros Haliassos",
                "Rodrigo Mira",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "title": "Leveraging real talking faces via selfsupervision for robust forgery detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandros Haliassos",
                "Konstantinos Vougioukas",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "title": "Lips don\u2019t lie: A generalisable and robust approach to face forgery detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kensho Hara",
                "Hirokatsu Kataoka",
                "Yutaka Satoh"
            ],
            "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Thomas Dietterich"
            ],
            "title": "Deep anomaly detection with outlier exposure",
            "venue": "arXiv preprint arXiv:1812.04606,",
            "year": 2018
        },
        {
            "authors": [
                "Xixi Hu",
                "Ziyang Chen",
                "Andrew Owens"
            ],
            "title": "Mix and localize: Localizing sound sources in mixtures",
            "venue": "Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Ziheng Hu",
                "Hongtao Xie",
                "Yuxin Wang",
                "Jiahong Li",
                "Zhongyuan Wang",
                "Yongdong Zhang"
            ],
            "title": "Dynamic inconsistency-aware deepfake video detection",
            "venue": "In IJCAI,",
            "year": 2021
        },
        {
            "authors": [
                "Minyoung Huh",
                "Andrew Liu",
                "Andrew Owens",
                "Alexei A Efros"
            ],
            "title": "Fighting fake news: Image splice detection via learned self-consistency",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ye Jia",
                "Yu Zhang",
                "Ron Weiss",
                "Quan Wang",
                "Jonathan Shen",
                "Fei Ren",
                "Patrick Nguyen",
                "Ruoming Pang",
                "Ignacio Lopez Moreno",
                "Yonghui Wu"
            ],
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Liming Jiang",
                "Ren Li",
                "Wayne Wu",
                "Chen Qian",
                "Chen Change Loy"
            ],
            "title": "Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Sri Kalyan Yarlagadda",
                "David G\u00fcera",
                "Paolo Bestagini",
                "Fengqing Maggie Zhu",
                "Stefano Tubaro",
                "Edward J Delp"
            ],
            "title": "Satellite image forgery detection and localization using gan and one-class classifier",
            "venue": "arXiv e-prints, pages arXiv\u20131802,",
            "year": 2018
        },
        {
            "authors": [
                "Will Kay",
                "Joao Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "10 Tim Green",
                "Trevor Back",
                "Paul Natsev"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950,",
            "year": 2017
        },
        {
            "authors": [
                "Hasam Khalid",
                "Shahroz Tariq",
                "Minha Kim",
                "Simon Woo"
            ],
            "title": "Fakeavceleb: A novel audio-video multimodal deepfake dataset",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Hasam Khalid",
                "Simon S Woo"
            ],
            "title": "Oc-fakedect: Classifying deepfakes using one-class variational autoencoder",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Charles Knapp",
                "Glifford Carter"
            ],
            "title": "The generalized correlation method for estimation of time delay",
            "venue": "IEEE transactions on acoustics, speech, and signal processing,",
            "year": 1976
        },
        {
            "authors": [
                "Shu Kong",
                "Deva Ramanan"
            ],
            "title": "Opengan: Open-set recognition via open data generation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Okan K\u00f6p\u00fckl\u00fc",
                "Maja Taseska",
                "Gerhard Rigoll"
            ],
            "title": "How to design a three-stage architecture for audio-visual active speaker detection in the wild",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Bruno Korbar",
                "Du Tran",
                "Lorenzo Torresani"
            ],
            "title": "Cooperative learning of audio and video models from self-supervised synchronization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Iryna Korshunova",
                "Wenzhe Shi",
                "Joni Dambre",
                "Lucas Theis"
            ],
            "title": "Fast face-swap using convolutional neural networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Prajwal KR",
                "Rudrabha Mukhopadhyay",
                "Jerin Philip",
                "Abhishek Jha",
                "Vinay Namboodiri",
                "CV Jawahar"
            ],
            "title": "Towards automatic face-to-face translation",
            "venue": "In Proceedings of the 27th ACM international conference on multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Kwon",
                "Jaeseong You",
                "Gyuhyeon Nam",
                "Sungwoo Park",
                "Gyeongsu Chae"
            ],
            "title": "Kodf: A large-scale korean deepfake detection dataset",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Lingzhi Li",
                "Jianmin Bao",
                "Ting Zhang",
                "Hao Yang",
                "Dong Chen",
                "Fang Wen",
                "Baining Guo"
            ],
            "title": "Face x-ray for more general face forgery detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yuezun Li",
                "Siwei Lyu"
            ],
            "title": "Exposing deepfake videos by detecting face warping artifacts",
            "venue": "arXiv preprint arXiv:1811.00656,",
            "year": 2018
        },
        {
            "authors": [
                "Shiyu Liang",
                "Yixuan Li",
                "Rayadurgam Srikant"
            ],
            "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
            "venue": "arXiv preprint arXiv:1706.02690,",
            "year": 2017
        },
        {
            "authors": [
                "Peter J Liu",
                "Mohammad Saleh",
                "Etienne Pot",
                "Ben Goodrich",
                "Ryan Sepassi",
                "Lukasz Kaiser",
                "Noam Shazeer"
            ],
            "title": "Generating wikipedia by summarizing long sequences",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yezheng Liu",
                "Zhe Li",
                "Chong Zhou",
                "Yuanchun Jiang",
                "Jianshan Sun",
                "Meng Wang",
                "Xiangnan He"
            ],
            "title": "Generative adversarial active learning for unsupervised outlier detection",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Zhengzhe Liu",
                "Xiaojuan Qi",
                "Philip HS Torr"
            ],
            "title": "Global texture enhancement for fake face detection in the wild",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Pingchuan Ma",
                "Brais Martinez",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "title": "Towards practical lipreading with distilled and efficient models",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Sagnik Majumder",
                "Ziad Al-Halah",
                "Kristen Grauman"
            ],
            "title": "Move2hear: Active audio-visual source separation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Hafiz Malik",
                "Hany Farid"
            ],
            "title": "Audio forensics from acoustic reverberation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2010
        },
        {
            "authors": [
                "Brais Martinez",
                "Pingchuan Ma",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "title": "Lipreading using temporal convolutional networks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Trisha Mittal",
                "Uttaran Bhattacharya",
                "Rohan Chandra",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "title": "Emotions don\u2019t lie: An audiovisual deepfake detection method using affective cues",
            "venue": "In Proceedings of the 28th ACM international conference on multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Shentong Mo",
                "Pedro Morgado"
            ],
            "title": "Localizing visual sounds the easy way",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Pedro Morgado",
                "Nuno Vasconcelos",
                "Ishan Misra"
            ],
            "title": "Audiovisual instance discrimination with cross-modal agreement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yuval Nirkin",
                "Yosi Keller",
                "Tal Hassner"
            ],
            "title": "Fsgan: Subject agnostic face swapping and reenactment",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Owens",
                "Alexei A Efros"
            ],
            "title": "Audio-visual scene analysis with self-supervised multisensory features",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Daniel P\u00e9rez-Cabo",
                "David Jim\u00e9nez-Cabello",
                "Artur Costa- Pazo",
                "Roberto J L\u00f3pez-Sastre"
            ],
            "title": "Deep anomaly detection for generalized face anti-spoofing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Ivan Perov",
                "Daiheng Gao",
                "Nikolay Chervoniy",
                "Kunlin Liu",
                "Sugasa Marangonda",
                "Chris Um\u00e9",
                "Mr Dpfks",
                "Carl Shift Facenheim",
                "RP Luis",
                "Jian Jiang"
            ],
            "title": "Deepfacelab: A simple, flexible and extensible face swapping",
            "year": 2020
        },
        {
            "authors": [
                "Stanislav Pidhorskyi",
                "Ranya Almohsen",
                "Gianfranco Doretto"
            ],
            "title": "Generative probabilistic novelty detection with adversarial autoencoders",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "KR Prajwal",
                "Rudrabha Mukhopadhyay",
                "Vinay P Namboodiri",
                "CV Jawahar"
            ],
            "title": "A lip sync expert is all you need for speech to lip generation in the wild",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Yuyang Qian",
                "Guojun Yin",
                "Lu Sheng",
                "Zixuan Chen",
                "Jing Shao"
            ],
            "title": "Thinking in frequency: Face forgery detection by mining frequency-aware clues",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Ali Razavi",
                "Aaron Van den Oord",
                "Oriol Vinyals"
            ],
            "title": "Generating diverse high-fidelity images with vq-vae-2",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Rossler",
                "Davide Cozzolino",
                "Luisa Verdoliva",
                "Christian Riess",
                "Justus Thies",
                "Matthias Nie\u00dfner"
            ],
            "title": "Faceforensics++: Learning to detect manipulated facial images",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Lukas Ruff",
                "Robert A Vandermeulen",
                "Nico G\u00f6rnitz",
                "Alexander Binder",
                "Emmanuel M\u00fcller",
                "Klaus-Robert M\u00fcller",
                "Marius Kloft"
            ],
            "title": "Deep semi-supervised anomaly detection",
            "venue": "arXiv preprint arXiv:1906.02694,",
            "year": 1906
        },
        {
            "authors": [
                "Mohammad Sabokrou",
                "Mohammad Khalooei",
                "Mahmood Fathy",
                "Ehsan Adeli"
            ],
            "title": "Adversarially learned one-class classifier for novelty detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tim Salimans",
                "Andrej Karpathy",
                "Xi Chen",
                "Diederik P Kingma"
            ],
            "title": "Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications",
            "venue": "arXiv preprint arXiv:1701.05517,",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Schlegl",
                "Philipp Seeb\u00f6ck",
                "Sebastian M Waldstein",
                "Ursula Schmidt-Erfurth",
                "Georg Langs"
            ],
            "title": "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery",
            "venue": "In International conference on information processing in medical imaging,",
            "year": 2017
        },
        {
            "authors": [
                "Aliaksandr Siarohin",
                "St\u00e9phane Lathuili\u00e8re",
                "Sergey Tulyakov",
                "Elisa Ricci",
                "Nicu Sebe"
            ],
            "title": "First order motion model for image animation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Taesup Kim",
                "Sebastian Nowozin",
                "Stefano Ermon",
                "Nate Kushman"
            ],
            "title": "Pixeldefend: Leveraging generative models to understand and defend against adversarial examples",
            "venue": "arXiv preprint arXiv:1710.10766,",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "The journal of machine learning research,",
            "year": 2014
        },
        {
            "authors": [
                "Ruijie Tao",
                "Zexu Pan",
                "Rohan Kumar Das",
                "Xinyuan Qian",
                "Mike Zheng Shou",
                "Haizhou Li"
            ],
            "title": "Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Van den Oord",
                "Nal Kalchbrenner",
                "Lasse Espeholt",
                "Oriol Vinyals",
                "Alex Graves"
            ],
            "title": "Conditional image generation with pixelcnn decoders",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Aaron Van Oord",
                "Nal Kalchbrenner",
                "Koray Kavukcuoglu"
            ],
            "title": "Pixel recurrent neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sheng-Yu Wang",
                "Oliver Wang",
                "Richard Zhang",
                "Andrew Owens",
                "Alexei A Efros"
            ],
            "title": "Cnn-generated images are surprisingly easy to spot.",
            "venue": "now. Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Yue Wu",
                "Wael AbdAlmageed",
                "Premkumar Natarajan"
            ],
            "title": "Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ran Yi",
                "Zipeng Ye",
                "Juyong Zhang",
                "Hujun Bao",
                "Yong- Jin Liu"
            ],
            "title": "Audio-driven talking face video generation with learning-based personalized head pose",
            "venue": "arXiv preprint arXiv:2002.10137,",
            "year": 2002
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Houssam Zenati",
                "Manon Romain",
                "Chuan-Sheng Foo",
                "Bruno Lecouat",
                "Vijay Chandrasekhar"
            ],
            "title": "Adversarially learned anomaly detection",
            "venue": "IEEE International conference on data mining (ICDM),",
            "year": 2018
        },
        {
            "authors": [
                "Zhaoyang Zeng",
                "Daniel McDuff",
                "Yale Song"
            ],
            "title": "Contrastive learning of global and local video representa- 12 tions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hongjie Zhang",
                "Ang Li",
                "Jie Guo",
                "Yanwen Guo"
            ],
            "title": "Hybrid models for open set recognition",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Shifeng Zhang",
                "Xiangyu Zhu",
                "Zhen Lei",
                "Hailin Shi",
                "Xiaobo Wang",
                "Stan Z Li"
            ],
            "title": "S3fd: Single shot scale-invariant face detector",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Tianchen Zhao",
                "Xiang Xu",
                "Mingze Xu",
                "Hui Ding",
                "Yuanjun Xiong",
                "Wei Xia"
            ],
            "title": "Learning self-consistency for deepfake detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yinglin Zheng",
                "Jianmin Bao",
                "Dong Chen",
                "Ming Zeng",
                "Fang Wen"
            ],
            "title": "Exploring temporal coherence for more general video face forgery detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Hang Zhou",
                "Xudong Xu",
                "Dahua Lin",
                "Xiaogang Wang",
                "Ziwei Liu"
            ],
            "title": "Sep-stereo: Visually guided stereophonic audio generation by associating source separation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Tianfei Zhou",
                "Wenguan Wang",
                "Zhiyuan Liang",
                "Jianbing Shen"
            ],
            "title": "Face forensics in the wild",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yipin Zhou",
                "Ser-Nam Lim"
            ],
            "title": "Joint audio-visual deepfake detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Supervised learning underlies today\u2019s most successful methods for image and video forensics. However, the difficulty of collecting large, labeled datasets that fully capture all of the possible manipulations that one might encounter in the wild places significant limitations on this approach. A longstanding goal of the forensics community has been to design methods that, instead, learn to detect manipulations using cues discovered by analyzing large amounts of real data through self-supervision [27, 47].\nWe propose a method that identifies manipulated video through anomaly detection. Our model learns how audio and visual data temporally co-occur by training on large amounts of real, unlabeled video. At test time, we can then flag videos that our model assigns low probability, such as those whose video and audio streams are inconsistent.\nOne might expect that this problem could be posed as simply detecting out-of-sync examples, such as by finding cases in which a speaker\u2019s mouth does not open precisely at the onset of a spoken word. Unfortunately, videos in the wild are often \u201cnaturally\u201d misaligned due to errors in encoding or recording, such as by having a single, consistent shift by a few frames [2, 23].\nInstead, we pose the problem as detecting anomalies in what we call synchronization features: audio-visual features\nthat are designed to convey the temporal alignment between vision and sound. We evaluate several feature sets, each extracted from a model that has been trained to temporally align audio and visual streams of a video [18, 23, 78]. In Figure 1, we show one such feature set: the amount of time that each video frame appears to be temporally offset from its corresponding sound. To detect anomalies, we fit an autoregressive generative model [84, 99] to sequences of synchronization features extracted from real videos, and identify low probability examples.\nA key advantage of our formulation is that it does not require any manipulated examples for training. It also does not require the speakers in the test set to already be present in the training set. This is in contrast to previous audiovisual forensics approaches, which either require finetuning on datasets of manipulated video [40], or which are based on verifying that the speaker\u2019s voice matches previously observed examples [25].\nar X\niv :2\n30 1.\n01 76\n7v 2\n[ cs\n.C V\n] 2\n7 M\nar 2\n02 3\nWe evaluate our model on videos that have manipulated a person\u2019s speech and face, using datasets of lip-synced and audio-driven face reenactment videos, some of which are also manipulated by faceswap techniques. Our model obtains strong performance on the FakeAVCeleb [52] and KoDF [61] datasets, despite the fact that it is trained entirely on real examples obtained from other video datasets. Our model generalizes to other spoken languages without retraining and obtains robustness to a variety of postprocessing operations, such as compression and blurring. We show through our experiments that: \u2022 Video forensics can be posed as an audio-visual anomaly\ndetection problem. \u2022 Synchronization features convey information about video\nmanipulations. \u2022 Our model can successfully detect fake videos, while train-\ning solely on real videos. \u2022 Our model generalizes to many types of image postpro-\ncessing operations and to speech videos from spoken languages not observed during training."
        },
        {
            "heading": "2. Related Work",
            "text": "Audio-visual forensics. In early work, Malik and Farid [71] detected audio manipulations by finding inconsistencies in reverberation. Recent work has focused on detecting manipulated speech videos using audio-visual inconsistencies. Several approaches have directly trained audiovisual networks through supervised learning, using labels indicating whether a video is manipulated [20,73]. A variety of methods have recently used audio-visual self-supervision for pretraining supervised models, which are finetuned with \u201creal or fake\u201d labels. Zeng et al. [105] used local and global contrastive learning methods to learn video features. Haliassos et al. [40] jointly solved a negative-less contrastive learning problem [38] and a forensics task. Other work [41] pretrains using lip-reading data. Zhou and Lim [112] used audio-visual synchronization signal implicitly, and proposed a dataset for audio-visual deepfake detection1. In contrast to these methods, our approach is trained entirely using real data and does not require any labels or examples of fake videos. Other work has used speaker verification [25] and phoneme-viseme mismatches [7] to detect fake videos and it also detects face swap manipulations, which preserve the synchronization between modalities. In contrast, our approach detects misaligned images and sounds and does not require that examples from the speaker be present in the training set.\nAudio-visual representation learning. A variety of methods have been proposed to learn audio-visual representation from videos via self-supervision. Researchers have leveraged the natural semantic correspondence in the videos be-\n1Their dataset is not publicly available.\ntween frames and audio tracks [10, 75, 105] to learn multimodal features and applied them to downstream tasks such as sound localization [9, 45, 74]. Other work studies temporal synchronization between audio and visual signals to learn audio-visual features [23, 58, 78], which can be used for active speaker detection [8, 57, 95], source separation [36, 70, 110], lip reading [5, 69, 72] and so on. Our method uses the off-the-shelf audio-visual synchronization model to perform anomaly detection.\nVisual face forensics. A major focus of the forensics field has been on the problem of detecting manipulated videos of human faces. In recent years, a variety of visual face manipulation datasets are proposed, such as FaceForensics++ [87], VideoForensicsHQ [34] and FFIW10K [111]. Meanwhile, many methods are proposed to detect synthetic contents to fight against their potential threats. Some work [12, 39, 63] has proposed to use hand-crafted features to capture inconsistent visual or JPEG artifacts. Other work has proposed to use deep learning to inspect specific artifacts, such as blending [62], frequency domain [31, 35, 83], or texture [67]. A variety of methods have studied the generalization between detection classifiers [16, 100].\nAnomaly detection. A variety of methods have learned a distribution, then flagged unusual examples. These examples are often considered anomalies [66, 91, 104, 113] or outliers [81, 89], and are used as part of open-set recognition [56, 106]. We formulate video forensics as the task of detecting anomalies, using a feature set that conveys information that would be hard for a forger to create. There have been a variety of methods proposed for learning this distribution, such as GAN discriminators [37,56,66,81,89,91,104,113], flow-based models [106], and autoregressive models [93]. Similarly, our model is based on an autoregressive generative model [11, 84], since they have achieved strong performance at modeling complex distributions. Other work addresses goals similar to anomaly detection by creating methods that model uncertainty [64] or that leverage outlier exposure [30, 44, 88]. Some work [13, 26, 28, 47, 50, 53, 79] has used specialpurpose anomaly detection methods for image/video forensics. Other work [33, 46, 101, 108] uses supervised learning to find anomalous patterns. In contrast, our method builds the likelihood function entirely on real videos and views low-probability examples as fake."
        },
        {
            "heading": "3. Method",
            "text": "We formulate the problem of detecting manipulated videos as an anomaly detection problem. We model the distribution of audio-visual examples, then flag examples that have low probability. If we were to fit a model on the raw data, then this would be a very challenging learning task. Instead, we learn the distribution over a feature set that\nconveys subtle properties that are unlikely to be accurately captured in manipulated video."
        },
        {
            "heading": "3.1. Estimating audio-visual synchronization",
            "text": "We obtain our feautres from a network that performs audio-visual synchronization [18, 23, 24, 78]. We use the model of Chen et al. [18]. We learn a function \u03c6(Vi, Aj) that indicates how likely video clip Vi temporally co-occurs with audio clip Aj . We estimate the synchronization score S(i, j) of all audio-visual pairs in a temporal window:\nS(i, j) = exp (\u03c6(Vi, Aj))\u2211i+\u03c4\nk=i\u2212\u03c4 exp (\u03c6(Vi, Ak)) , (1)\nwhere \u03c4 is maximum time difference between two streams, and \u03c6(Vi, Aj) = h (gv(Vi), ga(Aj)) is calculated using late fusion by a visual encoder gv, audio encoder ga and the fusion module h. We also interpret S(i, j) as synchronization probability. We maximize the synchronization of true audio-visual pairs (Vi, Ai) using the InfoNCE loss [77]:\nLsync = \u2212 1\nT T\u2211 i=1 logS(i, i), (2)\nfor a video of length T . We provide details about the architecture and training procedure in Appendix A.5.\nAfter training, we can use the learned model to obtain a feature set for anomaly detection. For example, we can use the rows of S, which provide a probability distribution over possible alignments between video clips and audio clips."
        },
        {
            "heading": "3.2. Audio-visual anomaly detection",
            "text": "We use our learned model to obtain a feature set for anomaly detection. We learn the distribution of these features\non a training set of real videos. Then at test time, videos with low probability will be flagged as potential fakes. We now explore two key design decisions that go into such a system: what feature set to use, and how the distribution is learned.\nGiven features for each frame, we learn a distribution p\u03b8(x1,x2, . . . ,xN ). We generally use autoregressive models to learn this distribution, given their success in modeling complex distributions [14,103]. These models take the form:\np\u03b8(x1,x2, \u00b7 \u00b7 \u00b7 ,xN ) = N\u22121\u220f i=0 p\u03b8(xi+1|x1, \u00b7 \u00b7 \u00b7 ,xi). (3)\nWe train a model x\u0302i+1 = f\u03b8(x1,x2, . . . ,xi) that estimates the features of the next frame, given all of the features from the previous frames. Maximizing the log probability can be posed as minimizing a per-frame loss, L:\nL = N\u2211 i=1 L(x\u0302i,xi). (4)\nWe now describe different formulations of the loss function L, the feature representation xi. In each case, we implement f\u03b8 as a Transformer [99].\nDiscrete time delays. We first consider a simple model that uses discrete time delay as our features, following the success of autoregressive models for fitting discrete data [32, 86, 97]. Inspired by work on time delay estimation [19, 55], for every video frame, we estimate how far ahead (or behind) it appears to be from the audio signal. For each frame, we set xi to be the time delay with the highest probability, i.e., xi = argmaxj(S(i, j)). We then set L to be the cross\nentropy loss between the ground truth and predicted time delay. This amounts to solving a categorization problem with 2\u03c4 + 1 possible labels for each frame.\nDistributions over delays. While discrete time delays are straightforward to represent in the model, they discard important information, such as when there is ambiguity in the delay. We, therefore, propose a model that directly predicts the entries of the time delay distribution. We set the features xi to be the rows of S, i.e. the probability of each possible delay, and use cross entropy loss:\nL(x\u0302i,xi) = \u2212 2\u03c4+1\u2211 j xi,j log(x\u0302i,j). (5)\nWe constrain the predictions made by our model f\u03b8 to sum to 1 by applying a softmax.\nAudio-visual network activations. The feature activations within the audio-visual synchronization network convey information about the time delay. We, therefore, ask whether these activations can be directly used as features for anomaly detection. We concatenate the representations of the visual and audio subnetworks, gv and ga. To provide a straightforward comparison with the time delay distribution model, we reduce the dimensionality of the features by projecting them onto the top 2\u03c4 + 1 principal components, following other work in autoregressive models of features [85]. We use squared distance as our loss: L (x\u0302i,xi) = \u2016xi \u2212 x\u0302i\u20162."
        },
        {
            "heading": "4. Results",
            "text": "We evaluate the different variations of our model on a variety of video forensics tasks."
        },
        {
            "heading": "4.1. Implementation details",
            "text": "Synchronization model. Following Chen et al. [18], we use ResNet-18 2D+3D [42, 43] as the visual encoder, using 5 frames (25 fps) as input. The audio encoder uses VGGM [17] and extracts features from 0.2s audio clips (16kHz). We fuse audio and visual data using a Transformer that has 3 standard Transformer encoder blocks [99], 4 attention heads, and 512 channels. We train using the cropped faces provided by each dataset. Please see Appendix A.5 for details.\nAnomaly detection model. We use a decoder-only autoregressive Transformer [32,65,84] to learn the distribution over synchronization features. We use 2 decoder blocks [99], each with 16 attention heads and 256 channels. For models that use time delay or continuous distribution, we set the maximum delay to be \u03c4 = 15 frames, resulting in the distribution Si \u2208 R31 for each video frame. We use sequences of length N = 50 from 2.0s video.\nHyperparameters. We resample videos to 25 fps and audios to 16kHz. We represent audio segments as mel spectrograms of size 21\u00d780 by short-time Fourier transform (STFT) with 80 mel filter banks, a hop length of 160, and a window size of 320. Please see more details in Appendix A.5."
        },
        {
            "heading": "4.2. Dataset",
            "text": "We train our model on real, unlabeled speech video, and evaluate it on forensics datasets.\nTraining datasets. We train our models on Lip Reading Sentences 2 (LRS2, 97k videos) [3] and Lip Reading Sentences 3 (LRS3, 120k videos) [4]. The videos in each contain tightly cropped face tracks. We divide each dataset into 3 splits and train the audio-visual synchronization model and the autoregressive model on different splits.\nEvaluation datasets. We evaluate on two video forensics datasets, spanning several different types of manipulations that change the speech and face of a human speaker. FakeAVCeleb [52], which is derived from VoxCeleb2 [21]. This dataset contains 500 real videos and 19,500 fake videos manipulated by Faceswap [59], FSGAN [76], and Wav2Lip [82], and fake sounds that are generated by SV2TTS [48]. The examples in the dataset contain different combinations of these manipulations. We use the dataset\u2019s provided face crops. We sample 2400 videos (400 real videos and 2000 fake videos) as train/val splits and 600 videos (100 real videos and 500 fake videos) as test split. We note that our method does not use any videos from train/val splits, since it is trained from another dataset (LRS2 or LRS3). Second, we evaluate on KoDF [61], a large-scale Korean-language deepfake detection dataset. It contains 62,166 real videos and 175,776 fake videos, where fake videos are generated by 6 synthesized methods: FaceSwap [1], FSGAN [76], DeepFaceLab [80], FOMM [92], ATFHP [102] and Wav2Lip [82]. We extract faces by using face detection [107] and alignment [15]."
        },
        {
            "heading": "4.3. Evaluation methods",
            "text": "Following common practice [16, 40, 41, 52, 61, 62, 83, 87, 100, 109], we evaluate using average precision (AP) and AUC. These evaluation metrics are widely used for crossdataset generalization and unsupervised models since they avoid the need to threshold the predictions. We compare our approach to both supervised and self-supervised methods at the video level. Unless otherwise stated, we use time delay distributions as our feature set (Sec. 3.2).\nSupervised methods. For supervised methods, we retrain several state-of-the-art detectors on FakeAVCeleb [52]: 1) Xception [87]: a popular baseline for forensics detection; 2) LipForensics [41]: a detector is built on high-level semantic embeddings of mouth and targets irregularities in\nmouth movements; 3) AD DFD [112]: a multimodal detector with audio and video branches, utilizes audio-visual synchronization signal implicitly for detection; 4) FTCN [109]: a video forensics detector leverages temporal incoherence to boost generalization capability; 5) RealForensics [40]: it first pretrains the network by audio-visual BYOL [38] framework and then finetunes the pretrained model on forensics datasets by multi-task learning to obtain robust and general face forgery detection.\nSelf-supervised methods. Since we are not aware of any existing methods that consider self-supervised speech video forensics, we adapt two existing methods to the task. First, we consider an audio-visual contrastive learning model, which we call AVBYOL, that learns to determine whether the visual and audio streams of a video do (or do not) match, an approach that has been used as a part of other audio-visual forensics models [25, 40]. We adapt the model of Haliassos et al. [40], which uses BYOL [38] to learn a joint audiovisual embedding for pretraining. Instead of pretraining, we directly use the model\u2019s audio-visual similarity score to flag fake examples. Second, we use a generative model VQGAN [32], trained on LRS2 [3], for anomaly detection. VQGAN converts an image into a sequence of discrete codes, then uses an autoregressive Transformer to learn the distribution of codes. We use the code\u2019s log likelihood, averaged over each video frame, for anomaly detection."
        },
        {
            "heading": "4.4. Evaluation",
            "text": "In real-world scenarios, the deployed detectors are expected to recognize fake videos manipulated by unseen techniques. Thus, following the standard procedure used in [40, 41, 109, 112], we conduct the experiment to evaluate the cross-manipulation generalization ability of our model on the FakeAVCeleb dataset [52] which videos are\nmanipulated in various ways. Since our approach and other self-supervised baselines learn from real, unmanipulated videos and perform zero-shot fake video detection, all the fake videos during evaluation are considered as manipulated by unseen methods.\nWe split FakeAVCeleb dataset [52] into five categories based on the manipulation methods and manipulated modalities: 1) RVFA: real video with fake audio by SV2TTS [48]; 2) FVRA-WL: real audio with fake video by Wav2Lip [82]; 3) FVFA-WL: fake video by Wav2Lip [82], and fake audio by SV2TTS [48]; 4) FVFA-FS: fake video by Faceswap [59] and Wav2Lip [82], and fake audio by SV2TTS [48]; 5) FVFA-GAN: fake video by FaceswapGAN [76] and Wav2Lip [82], and fake audio by SV2TTS [48]. For supervised methods, we hold out the evaluated category and train the models on the remaining categories. Note that some approaches are only able to detect the manipulation on a certain modality, we do not report their performance on the categories with the manipulation only on the other modalities (since they can not differentiate real and fake videos).\nWe show our results in Tab. 1. Our method substantially outperforms both self-supervised methods AVBYOL [38,40] and VQGAN [32] on each category by a large margin. More importantly, our method works on par with or outperforms some supervised methods on certain categories, especially FVFA-GAN, even though our method does not use any labeled supervision or fake examples. Moreover, our method has quite consistent performances and it can achieve AP over 90% in the most of categories. While Xception [87], LipForensics [41], AD DFD [112] and FTCN [109] work well on 75% of the settings, there are settings where performance collapses to near-chance (e.g., AD DFD [112] on FVFAGAN). Interestingly, two self-supervised baselines struggle to detect fake videos, perhaps because both models do not necessarily capture the subtle information that would be\nneeded to detect manipulations. In addition, VQGAN [32] compresses the visual signal using a codebook, which might drop the artifact clues and harm the detection performance. Moreover, our model trained on LRS2 [3] works on par with the one trained on LRS3 [4], indicating that our method\u2019s performance is not tied to a single training set.\nCross-dataset generalization. We also evaluate the generalization capability of our model by evaluating it on the KoDF dataset [61], following [40, 41, 109, 112]. We focus on the audio-driven synthesis examples in the dataset, where videos are manipulated by ATFHP [102] or Wav2Lip [82], and randomly selected 100 real and 100 fake videos. We train the supervised models on FakeAVCeleb [52] to evaluate their generalization ability. Many of these training videos share the same technique used in KoDF for synthesis [82]. As the results are shown in Tab. 2, our approach obtains a comparable performance to many supervised methods. Although our system is trained on the English speech datasets, it still generalizes to KoDF [61] dataset of Korean speech, perhaps because it learns low-level lip motion cues that are broadly useful. We provide more results in Appendix A.2.\nQualitative results. We visualize the ground truth and predicted time delay distributions generated by our autoregressive continuous time delay model (Sec. 3.2) in Fig. 3. We use the four main categories from FakeAVCeleb dataset [52]. For each one, we display a heat map indicating the predicted time delay distribution, using a model that obtains the ground truth distributions of the previous frames as input. We also plot the cumulative prediction loss (Eq. 4) over time. From Fig. 3, we can see that our autoregressive model accurately predicts the ground truth for real video, which results in a lower score. For fake videos, we can find clear differences between ground truth and predicted time delay distribution, leading to higher prediction loss.\nSynchronization probabilities are in a range from 0 1. We show the predictions of the autoregressive model when feeding it ground truth observations of the previous timesteps. We show cumulative prediction error (indicating the probability of being fake) for each sample over time steps in the last row."
        },
        {
            "heading": "4.5. Robustness to unseen perturbations",
            "text": "When the fake video is redistributed, it may undergo many types of postprocessing that result in corruption, making detection more difficult. Thus, it is important for forensics models to be robust to the types of postprocessing operations they may encounter in the wild. Following [40, 41, 109], we use the set of visual perturbations proposed in [49]: 1) Color saturation change; 2) Block-wise distortion; 3) Color contrast change; 4) Gaussian blur; 5) Gaussian noise; 6) JPEG compression; 7) Video compression rate change. We set the intensity levels from 0 to 5 for each perturbation.\nWe compare our model with four supervised methods XceptionNet [87] FTCN [112], AD DFD [112] and RealForensics [40]. As shown in Fig. 4, our self-supervised model is overall more robust to unseen visual perturbations on average compared with these supervised methods, with the exception of RealForensics [40]. This is also true when we consider \u201cworst case\u201d performance, by taking the mini-\nmum performance over all types of augmentation of a given intensity level. Interestingly, we obtain this performance even though our model is trained in a very different way from other works, suggesting that the feature set continues to convey useful information to the anomaly detection model, even in the presence of significant corruption."
        },
        {
            "heading": "4.6. Feature set analysis",
            "text": "We evaluate the effectiveness of different feature sets used by our anomaly detection model. As described in Sec. 3.2, we start with discrete time delays as our feature representation and optimize the model with the cross entropy loss. Then, we use continuous time delay distributions as representations instead, where we optimize models with different objective functions: 1) Soft CE: we use the time delay distribution as the target (akin to a \u201csoft\u201d label) and use the cross entropy loss (Sec. 3.2); 2) CE: we map each distribution into one-hot encoding as the target by using argmax and employ the cross entropy loss; 3) BCE: we use the distribution as the target while treating each synchronization score S(i, j) (Sec. 3.1) within the same time step independently. We use the sigmoid function and binary cross entropy loss to train the model. We also use our network\u2019s feature activations as in Sec. 3.2: 1) audio-visual feature activations (activationAV); 2) visual-only feature activations (activation-V). Besides, we consider using a combination of different feature sets where we concatenate continuous time delay distributions and audio-visual feature activations (Act.-AV+dist.) as a new feature. Similar to audio-visual feature activations as in Sec. 3.2, we use squared distance as the loss for the concatenation of these two types of feature sets.\nWe also compare with a simple model based on Naive\nBayes and discrete time delays. This model assumes that each frame\u2019s time delay is independent, and obtains a probability for the entire sequence by multiplying the probability of each frame\u2019s time delay. This amounts to simply detecting large misalignments since in practice the Naive Bayes model will assign probability solely based on the magnitude of each delay.\nFinally, we consider a version of the model that autoregressively predicts the entire distribution of time delays, inspired by autoregressive models, such as PixelCNN [96] that generate images in a raster scan order. We autoregressively predict each element of the 2D matrix S\u0302(i, j), where S\u0302(i, j) is created by vector quantizing the entries of the synchronization probability S(i, j) using k-means (see Appendix A.4 for details).\nAnalysis. We evaluate each variant on FakeAVCeleb [52] and report results in Tab. 3. These results suggest that all formulations achieve performance significantly better than chance, indicating that these feature sets are useful for anomaly detection. As in Tab. 3, the time delay distribution model outperforms the discrete time delay model, suggesting that there is important information conveyed in the probability of unlikely delays. The autoregressive model that uses distribution as input and soft labels (soft CE) performs best since it forces the output prediction to match the distribution from the synchronization model. Interestingly, the model that uses audio-visual feature activations obtains performance close to that of the soft CE model, indicating that the networks\u2019 audio-visual features convey useful information. Finally, the multimodal activation-AV model significantly outperforms the visual-only activation-V model, suggesting that having access to both modalities is useful for our anomaly detection model."
        },
        {
            "heading": "4.7. Ablation study",
            "text": "Different training dataset. We ask how the choice of dataset affects the quality of the model. To test this, we train our synchronization and autoregressive models on different datasets to analyze the generalization abilities of each component, i.e., training the synchronization model on LRS2/LRS3 and training the autoregressive model on LRS3/LRS2 or LRS2+LRS3 with the same hyperparameters. As shown in Tab. 4, there is no significant performance change when we train these two components on different combinations of datasets, including when they are trained on the same dataset. This suggests that the distribution of time delay predictions may be stable between these speech video datasets.\nInfluence of sequence length. To explore the influence of input sequence length for the autoregressive model, we sample the same amount of training videos for sequence lengths N of 10, 20, 30, 40, 50, and 60, and keep other hyperparameters the same. We test these models on FakeAVCeleb dataset [52]. Fig. 5 shows that as the sequence length in-\ncreases, the performance increases with it.\nEffect of time delay distribution maximum offset. We also study how the length of time delay distribution would affect the performance of the autoregressive model with distribution over delays. We experiment with maximum offset \u03c4 \u2208 {5, 10, 15, 20, 25} resulting in the delay distribution length of {11, 21, 31, 41, 51}. We test these models on the FakeAVCeleb dataset [52]. Fig. 5 shows that as distribution length increases, the performance first increases, after which point results plateau or slightly decrease. This may be due to the fact that when considering larger ranges of offsets, the distribution spreads over a large number of unlikely possibilities, making important information less apparent after normalization."
        },
        {
            "heading": "5. Conclusion",
            "text": "We have proposed a method for detecting video manipulation by self-supervised anomaly detection. To do this, we create novel feature sets that convey audio-visual synchronization. We then show that fake videos can be detected by flagging examples with unlikely sequences of these features, according to a learned distribution. Our model obtains strong performance on the FakeAVCeleb and KoDF datasets, despite the fact that it was trained only on real video. It also obtains robustness to visual postprocessing operations and to videos containing other spoken languages. We see our work as opening in two directions. The first is in posing forensics as an anomaly detection problem with a self-supervised feature set. While we have proposed one such model, based on autoregressive sequence models, the field of anomaly detection offers many possible future approaches. The second direction is in developing new feature sets that are wellsuited to forensics problems, beyond the synchronization features used in this work.\nLimitations and Broader Impacts. Our work provides methods that can potentially be applied to detecting malicious video manipulations and disinformation. While we have shown that our model is capable of detecting several\ntypes of fake video, there may be other techniques that our model fails to detect. In particular, due to the design of our use of synchronization-based features, our model is not well suited to detecting manipulations that leave the synchronization between motion and sound relatively consistent, such as those that change a speaker\u2019s appearance without significantly changing the motion of their mouth.\nAcknowledgements. We thank David Fouhey, Richard Higgins, Sarah Jabbour, Yuexi Du, Mandela Patrick, Deva Ramanan, Haochen Wang, and Aayush Bansal for helpful discussions. This work was supported in part by DARPA Semafor. The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
        }
    ],
    "title": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection",
    "year": 2023
}