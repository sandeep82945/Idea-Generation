1. **Investigate the exact nature of the interaction between attention heads and the residual stream.** How do attention heads steer the direction of the fixed point search algorithm that the transformer performs? How does the residual stream provide feedback to the attention heads?


2. **Explore the relationship between the induced representation (QK) and the intertwining operator (OV) on one end and the trace (residual stream) on the other.** How does the induced representation distribute the unit of attention among the potential candidates? How does the intertwining operator combine the candidate values to produce a single output?


3. **Develop a detailed explanation of the transformer learning mechanism in terms of Laplace transform and Tannaka–Krein duality.** How does the Laplace transform arise out of the interaction between convolution and shuffle product of combinatorial Hopf algebras? How does Tannaka–Krein duality relate to the Laplace transform in this context?


4. **Investigate the relationship between transformer models and dynamic programming algorithms.** How can dynamic programming provide a simplified model of certain elements of transformer models? How does the transformer model prune its recurrences when combining a solution from subsolutions?


5. **Explore the exact mechanism by which one layer and two layer attention heads learn bigram statistics and induction heads, respectively.** How does the shuffle product allow the model to generate possible interleavings and ways of combining them? How can the chain "A . B C" be understood as a sum over all the coproducts which start with "A" and end with "B C"?