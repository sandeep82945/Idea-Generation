{
    "abstractText": "RGB-Thermal (RGB-T) semantic segmentation has shown great potential in handling low-light conditions where RGB-based segmentation is hindered by poor RGB imaging quality. The key to RGB-T semantic segmentation is to effectively leverage the complementarity nature of RGB and thermal images. Most existing algorithms fuse RGB and thermal information in feature space via concatenation, element-wise summation, or attention operations in either unidirectional enhancement or bidirectional aggregation manners. However, they usually overlook the modality gap between RGB and thermal images during feature fusion, resulting in modality-specific information from one modality contaminating the other. In this paper, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet) for RGB-T semantic segmentation, which propagates only modality-shared information across different modalities and alleviates the modality-specific information contamination issue. Our CSRPNet first performs relation-propagation in channel and spatial dimensions to capture the modality-shared features from the RGB and thermal features. CSRPNet then aggregates the modality-shared features captured from one modality with the input feature from the other modality to enhance the input feature without the contamination issue. While being fused together, the enhanced RGB and thermal features will be also fed into the subsequent RGB or thermal feature extraction layers for interactive feature fusion, respectively. We also introduce a dual-path cascaded feature refinement module that aggregates multi-layer features to produce two refined features for semantic and boundary prediction. Extensive experimental results demonstrate that CSRPNet performs favorably against state-of-the-art algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zikun Zhoua"
        },
        {
            "affiliations": [],
            "name": "Shukun Wub"
        },
        {
            "affiliations": [],
            "name": "Guoqing Zhub"
        },
        {
            "affiliations": [],
            "name": "Hongpeng Wangb"
        },
        {
            "affiliations": [],
            "name": "Zhenyu Hea"
        }
    ],
    "id": "SP:971441ff89fb4bf2c8ab3e8bd7606b1ff186b638",
    "references": [
        {
            "authors": [
                "B. Kang",
                "Y. Lee",
                "T.Q. Nguyen"
            ],
            "title": "Depth-adaptive deep neural network for semantic segmentation",
            "venue": "IEEE Transactions on Multimedia 20 (9) ",
            "year": 2018
        },
        {
            "authors": [
                "W. Zhou",
                "J. Liu",
                "J. Lei",
                "L. Yu",
                "J.-N. Hwang"
            ],
            "title": "Gmnet: Graded-feature multilabel-learning network for rgb-thermal urban scene semantic segmentation",
            "venue": "IEEE Transactions on Image Processing 30 ",
            "year": 2021
        },
        {
            "authors": [
                "X. Xiao",
                "Y. Zhao",
                "F. Zhang",
                "B. Luo",
                "L. Yu",
                "B. Chen",
                "C. Yang"
            ],
            "title": "Baseg: Boundary aware semantic segmentation for autonomous driving",
            "venue": "Neural Networks 157 ",
            "year": 2023
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "in: Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer",
            "year": 2015
        },
        {
            "authors": [
                "M. Gridach"
            ],
            "title": "Pydinet: Pyramid dilated network for medical image segmentation",
            "venue": "Neural networks 140 ",
            "year": 2021
        },
        {
            "authors": [
                "N. Ibtehaz",
                "M.S. Rahman"
            ],
            "title": "Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation",
            "venue": "Neural networks 121 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Ren",
                "R.S. Zemel"
            ],
            "title": "End-to-end instance segmentation with recurrent attention",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "J. Fu",
                "J. Liu",
                "H. Tian",
                "Y. Li",
                "Y. Bao",
                "Z. Fang",
                "H. Lu"
            ],
            "title": "Dual attention network for scene segmentation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "S. Huang",
                "Z. Lu",
                "R. Cheng",
                "C. He"
            ],
            "title": "Fapn: Feature-aligned pyramid network for dense image prediction",
            "venue": "in: Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "P. Cao",
                "J. Wang",
                "O.R. Zaiane"
            ],
            "title": "Uctransnet: Rethinking the skip connections in u-net from a channelwise perspective with transformer",
            "venue": "in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36",
            "year": 2022
        },
        {
            "authors": [
                "S.-J. Park",
                "K.-S. Hong",
                "S. Lee"
            ],
            "title": "Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation",
            "venue": "in: Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2017
        },
        {
            "authors": [
                "W. Wang",
                "U. Neumann"
            ],
            "title": "Depth-aware cnn for rgb-d segmentation",
            "venue": "in: Proceedings of the European Conference on Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "Q. Zhang",
                "S. Zhao",
                "Y. Luo",
                "D. Zhang",
                "N. Huang",
                "J. Han"
            ],
            "title": "Abmdrnet: Adaptive-weighted bi-directional modality difference reduction network for rgb-t semantic segmentation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "K.-Y. Lin",
                "J. Wang",
                "W. Wu",
                "C. Qian",
                "H. Li",
                "G. Zeng"
            ],
            "title": "Bi-directional cross-modality feature propagation with separation-and-aggregation gate for rgb-d semantic segmentation",
            "venue": "in: Proceedings of the European Conference on Computer Vision, Springer",
            "year": 2020
        },
        {
            "authors": [
                "Y. Sun",
                "W. Zuo",
                "M. Liu"
            ],
            "title": "Rtfnet: Rgb-thermal fusion network for semantic segmentation of urban scenes",
            "venue": "IEEE Robotics and Automation Letters 4 (3) ",
            "year": 2019
        },
        {
            "authors": [
                "F. Deng",
                "H. Feng",
                "M. Liang",
                "H. Wang",
                "Y. Yang",
                "Y. Gao",
                "J. Chen",
                "J. Hu",
                "X. Guo",
                "T.L. Lam"
            ],
            "title": "Feanet: Featureenhanced attention network for rgb-thermal real-time semantic segmentation",
            "venue": "in: Proceedings of the International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhou",
                "S. Dong",
                "C. Xu",
                "Y. Qian"
            ],
            "title": "Edge-aware guidance fusion network for rgb\u2013thermal scene parsing",
            "venue": "in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36",
            "year": 2022
        },
        {
            "authors": [
                "C. Hazirbas",
                "L. Ma",
                "C. Domokos",
                "D. Cremers"
            ],
            "title": "Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture",
            "venue": "in: Proceedings of the Asian Conference on Computer Vision, Springer",
            "year": 2016
        },
        {
            "authors": [
                "W. Zhou",
                "X. Lin",
                "J. Lei",
                "L. Yu",
                "J.-N. Hwang"
            ],
            "title": "Mffenet: Multiscale feature fusion and enhancement network for rgb\u2013thermal urban road scene parsing",
            "venue": "IEEE Transactions on Multimedia 24 ",
            "year": 2021
        },
        {
            "authors": [
                "X. Hu",
                "K. Yang",
                "L. Fei",
                "K. Wang"
            ],
            "title": "Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation",
            "venue": "in: Proceedings of the IEEE International Conference on Image Processing, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "A. Dutta",
                "B. Mandal",
                "S. Ghosh",
                "N. Das"
            ],
            "title": "Using thermal intensities to build conditional random fields for object segmentation at night",
            "venue": "in: Proceedings of the International Conference on Computational Intelligence and Networks, IEEE",
            "year": 2020
        },
        {
            "authors": [
                "G. Zhang",
                "J.-H. Xue",
                "P. Xie",
                "S. Yang",
                "G. Wang"
            ],
            "title": "Non-local aggregation for rgb-d semantic segmentation",
            "venue": "IEEE Signal Processing Letters 28 ",
            "year": 2021
        },
        {
            "authors": [
                "H. Liu",
                "J. Zhang",
                "K. Yang",
                "X. Hu",
                "R. Stiefelhagen"
            ],
            "title": "Cmx: Cross-modal fusion for rgb-x semantic segmentation with transformers",
            "venue": "arXiv preprint arXiv:2203.04838 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2015
        },
        {
            "authors": [
                "P. Wang",
                "P. Chen",
                "Y. Yuan",
                "D. Liu",
                "Z. Huang",
                "X. Hou",
                "G. Cottrell"
            ],
            "title": "Understanding convolution for semantic segmentation",
            "venue": "in: Proceedings of the IEEE Winter Conference on Applications of Computer Vision, IEEE",
            "year": 2018
        },
        {
            "authors": [
                "S. Arora",
                "H.K. Suman",
                "T. Mathur",
                "H.M. Pandey",
                "K. Tiwari"
            ],
            "title": "Fractional derivative based weighted skip connections for satellite image road segmentation",
            "venue": "Neural Networks 161 ",
            "year": 2023
        },
        {
            "authors": [
                "H. Noh",
                "S. Hong",
                "B. Han"
            ],
            "title": "Learning deconvolution network for semantic segmentation",
            "venue": "in: Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2015
        },
        {
            "authors": [
                "V. Badrinarayanan",
                "A. Kendall",
                "R. Cipolla"
            ],
            "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (12) ",
            "year": 2017
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu"
            ],
            "title": "L",
            "venue": "Van Der Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "S. J\u00e9gou",
                "M. Drozdzal",
                "D. Vazquez",
                "A. Romero",
                "Y. Bengio"
            ],
            "title": "The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "year": 2017
        },
        {
            "authors": [
                "Z. Huang",
                "X. Wang",
                "L. Huang",
                "C. Huang",
                "Y. Wei",
                "W. Liu"
            ],
            "title": "Ccnet: Criss-cross attention for semantic segmenta- 19 tion",
            "venue": "in: Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "D. Lin",
                "R. Zhang",
                "Y. Ji",
                "P. Li",
                "H. Huang"
            ],
            "title": "Scn: Switchable context network for semantic segmentation of rgb-d images",
            "venue": "IEEE Transactions on Cybernetics 50 (3) ",
            "year": 2018
        },
        {
            "authors": [
                "M.S. Pavel",
                "H. Schulz",
                "S. Behnke"
            ],
            "title": "Object class segmentation of rgb-d video using recurrent convolutional neural networks",
            "venue": "Neural Networks 88 ",
            "year": 2017
        },
        {
            "authors": [
                "Q. Ha",
                "K. Watanabe",
                "T. Karasawa",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes",
            "venue": "in: Proceedings of the IEEE International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2017
        },
        {
            "authors": [
                "Y. Sun",
                "W. Zuo",
                "P. Yun",
                "H. Wang",
                "M. Liu"
            ],
            "title": "Fuseseg: Semantic segmentation of urban scenes based on rgb and thermal data fusion",
            "venue": "IEEE Transactions on Automation Science and Engineering 18 (3) ",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "X. Wang",
                "R. Girshick",
                "A. Gupta",
                "K. He"
            ],
            "title": "Non-local neural networks",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, IEEE",
            "year": 2009
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30 ",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Tan",
                "Q. He",
                "Y. Xiao"
            ],
            "title": "Swinnet: Swin transformer drives edge-aware rgb-d and rgb-t salient object detection",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology 32 (7) ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Tian",
                "H. Zhao",
                "M. Shu",
                "Z. Yang",
                "R. Li",
                "J. Jia"
            ],
            "title": "Prior guided feature enrichment network for few-shot segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 44 (2) ",
            "year": 2020
        },
        {
            "authors": [
                "A. Paszke",
                "A. Chaurasia",
                "S. Kim",
                "E. Culurciello"
            ],
            "title": "Enet: A deep neural network architecture for real-time semantic segmentation",
            "venue": "arXiv preprint arXiv:1606.02147 ",
            "year": 2016
        },
        {
            "authors": [
                "S.S. Shivakumar",
                "N. Rodrigues",
                "A. Zhou",
                "I.D. Miller",
                "V. Kumar",
                "C.J. Taylor"
            ],
            "title": "Pst900: Rgb-thermal calibration",
            "venue": "dataset and segmentation network, in: 2020 IEEE international conference on robotics and automation (ICRA), IEEE",
            "year": 2020
        },
        {
            "authors": [
                "T. Pohlen",
                "A. Hermans",
                "M. Mathias",
                "B. Leibe"
            ],
            "title": "Full-resolution residual networks for semantic segmentation in street scenes",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "C. Yu",
                "J. Wang",
                "C. Peng",
                "C. Gao",
                "G. Yu",
                "N. Sang"
            ],
            "title": "Bisenet: Bilateral segmentation network for real-time semantic segmentation",
            "venue": "in: Proceedings of the European Conference on Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "C. Yu",
                "J. Wang",
                "C. Peng",
                "C. Gao",
                "G. Yu",
                "N. Sang"
            ],
            "title": "Learning a discriminative feature network for semantic segmentation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "J. Liu",
                "W. Zhou",
                "Y. Cui",
                "L. Yu",
                "T. Luo"
            ],
            "title": "Gcnet: Grid-like context-aware network for rgb-thermal semantic segmentation",
            "venue": "Neurocomputing 506 ",
            "year": 2022
        },
        {
            "authors": [
                "W. Liang",
                "Y. Yang",
                "F. Li",
                "X. Long",
                "C. Shan"
            ],
            "title": "Mask-guided modality difference reduction network for rgb-t semantic segmentation",
            "venue": "Neurocomputing 523 ",
            "year": 2023
        },
        {
            "authors": [
                "T. Gong",
                "W. Zhou",
                "X. Qian",
                "J. Lei",
                "L. Yu"
            ],
            "title": "Global contextually guided lightweight network for rgb-thermal urban scene understanding",
            "venue": "Engineering Applications of Artificial Intelligence 117 ",
            "year": 2023
        },
        {
            "authors": [
                "X. Lan",
                "X. Gu",
                "X. Gu"
            ],
            "title": "Mmnet: Multi-modal multi-stage network for rgb-t image semantic segmentation",
            "venue": "Applied Intelligence 52 (5) ",
            "year": 2022
        },
        {
            "authors": [
                "E. Shelhamer",
                "J. Long",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 39 (4) ",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords: RGB-thermal semantic segmentation, relation-propagation, modality-shared information, modality-specific information."
        },
        {
            "heading": "1. Introduction",
            "text": "Semantic segmentation, which aims to predict the category for every pixel in an image, is an important task in computer vision with a diverse range of applications, such as autonomous driving [1, 2, 3], medical diagnosis [4, 5, 6], and robot sensing [7]. Recently, many deep segmentation\n\u2217Zikun Zhou and Shukun Wu contribute equally. \u2217\u2217Zhenyu He is the corresponding author.\nEmail address: zhenyuhe@hit.edu.cn (Zhenyu He)\nar X\niv :2\n30 8.\n12 53\n4v 1\n[ cs\n.C V\n] 2\n4 A\nug 2\n02 3\nalgorithms [8, 9, 10] have made significant progress in RGB-based semantic segmentation. However, the imaging quality of visible light cameras degenerates in low-light conditions, restricting the application of the RGB segmentation methods in such extreme conditions.\nTo enhance segmentation robustness in extreme conditions, various studies [2, 11, 12, 13] utilize other modality data to complement RGB images. Commonly used additional modalities include depth [11, 12, 14] and thermal [2, 13, 15] images. Depth images can provide 3D geometric information of the scene, while thermal images measure the thermal radiation of any object whose temperature is over absolute zero. However, depth cameras utilizing structured light or timeof-flight technology remain vulnerable to strong outdoor lighting conditions, thus limiting their application in extreme outdoor situations. By comparison, thermal cameras are less susceptible to lighting conditions and work well under all weather. Hence, we focus on RGB-Thermal (RGB-T) semantic segmentation in this work. The common characteristics of RGB and thermal images are that they both reflect the shape of objects in the scene. The specific characteristics lie in that RGB images depict color distribution while thermal images capture temperature distribution.\nThe core challenge in RGB-T segmentation lies in how to effectively fuse multi-modality data to make them complementary to each other. A common pipeline for multi-modality fusion is to use a two-stream network to extract deep features of different modalities and then fuse the deep features [2, 13, 15, 18, 19, 20]. Among these methods, several algorithms [15, 16, 18] employ a unidirectional feature enhancement strategy for multi-modality feature fusion. As shown in Figure 1 (a), these methods use the thermal feature as auxiliary information to enhance the RGB feature via element-wise summation or concatenation. Following this, they perform segmentation based on the enhanced feature. However, using the thermal image as an auxiliary modality is less effective for night scenes, as the thermal image plays a more crucial role in revealing night scenes compared to the RGB image.\nBesides, numerous algorithms [2, 11, 13, 17, 20, 21] utilize the bidirectional feature aggregation strategy for multi-modality fusion. As shown in Figure 1 (b), they treat the RGB and thermal images equally in multi-modality feature fusion, usually involving sophisticated attention mecha-\nnisms to improve the feature aggregation. For example, several approaches [2, 13, 17] first aggregate the features from different modalities via element-wise summation or concatenation. Then they use the attention operation to model long-range dependencies to enhance the aggregated features. Generally, feature aggregation is independent of feature extraction [2, 11, 20, 13, 17]. In particular, several algorithms [14, 22, 23] attempt to aggregate the multi-modality features interactively. Namely, they feed the aggregated features into the feature extraction block of the next layer, as depicted by dashed arrows in Figure 1 (b). Although the above algorithms achieve advanced performance, they overlook the discrepancies between the modality-specific characteristics of RGB and thermal images (also named modality gap) during fusion. Especially for interactive aggregation methods, aggregating the modality-specific information of one modality into the other modality will contaminate the specific characteristics of the latter.\nIn this paper, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet) for RGB-T semantic segmentation. The core idea is to first capture the features containing modalityshared information (also called modality-shared features) and then utilize them to perform interactive multi-modality fusion, as shown in Figure 1 (c). By aggregating only the modality-shared features with the original features, we can alleviate the issue of modality-specific information from one modality contaminating the other during interactive fusion.\nCSRPNet captures the modality-shared features from the RGB and thermal features via relationpropagation in channel and space. The rationale behind this design is: (1) the inter-channel and inter-pixel relation matrices can be used to represent the intra-modality semantic dependencies at the feature channel and pixel levels, respectively; (2) and thus the feature channels or pixels sharing similar relations across different modalities are more likely to model modality-shared information. In this way, we can obtain the modality-shared feature channels and pixels by capturing those sharing similar relations. Such a process is named relation-propagation. Herein, we investigate two relation functions, including dot product and Gaussian function, to model the relation between two channels or two pixels for calculating the relation matrices. By integrating captured modality-shared channels and pixels from one modality with the other modality, we can adaptively and interactively fuse multi-modality features while avoiding feature contamination resulting from the modality gap. Based on the above idea, we design a channel and spatial relation-propagation module and plug it into different layers of the two-steam backbone for interactive fusion.\nTo fully exploit the multi-layer fusion features, we design a Dual-path Cascaded Feature Refinement (DCFR) module. The DCFR module aggregates the multi-layer fusion features layer-bylayer via two paths, generating two refined features for accurate semantic prediction and boundary prediction, respectively. To conclude, we make the following contributions:\n\u2022 We propose a channel and spatial relation-propagation network for RGB-T segmentation. It can fuse the multi-modality features interactively while preserving modality-specific information for each modality by capturing and integrating the modality-shared features.\n\u2022 We propose a channel and spatial relation-propagation module with two types of relation functions, which enables the effective capture of modality-shared features. We also design a dual-path cascaded feature refinement module to fully utilize the multi-layer fusion features.\n\u2022 We achieve favorable performance against state-of-the-art methods on the MFNet and PST900 datasets, demonstrating the effectiveness of the proposed algorithm."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. RGB Semantic Segmentation",
            "text": "Significant progress has been achieved in semantic segmentation with the development of deep learning-based algorithms [4, 8, 24, 25, 26]. The pioneering method FCN [24] proposes a simple yet effective framework for dense segmentation, which is capable of accepting images of any size as input and substantially improves the segmentation efficiency. Despite the great progress, the prediction mask of the FCN method tends to lose fine details. Numerous methods [4, 27, 25] have been proposed to address this issue. For example, U-Net [4] fuses the feature map in the encoder with the feature map in the corresponding layer of the decoder to obtain richer context information to improve the segmentation performance. DUC [25] proposes the dense upsampling convolution operation to replace the original bilinear upsampling and deconvolution operations. To reduce computational and memory loads, SegNet [28] proposes the max pooling indices to perform the non-linear upsampling of the feature maps in the decoder. Building on the success of DenseNet [29] on image classification, FC-DenseNet [30] adapts and extends the DenseNet architecture into a fully convolutional network for semantic segmentation.\nRecently, several methods [8, 31] exploit the attention mechanism to capture more contextual information for semantic segmentation. CCNet [31] proposes a criss-cross attention module to model contextual information from surrounding pixels on the cross path. DANet [8] introduces a position attention module to selectively aggregate the long-range contextual information according to the spatial attention map. It also proposes a channel attention module to model interdependencies between different semantic responses based on the channel attention map. Despite the excellent performance, these RGB-based semantic segmentation methods remain highly susceptible to lighting conditions."
        },
        {
            "heading": "2.2. Multi-modality Semantic Segmentation",
            "text": "In recent years, multi-modality data have been utilized for semantic segmentation to tackle the issue that the RGB segmentation methods are vulnerable to lighting conditions. And numerous RGB-Depth (RGB-D) [11, 12, 14, 18, 20, 22, 23, 32, 33] and RGB-T [2, 13, 15, 16, 17, 19, 21, 34, 35] segmentation algorithms have been proposed. Multi-modality feature fusion is the core challenge for multi-modality semantic segmentation. Existing multi-modality feature fusion methods can be coarsely divided into two categories: unidirectional feature enhancement and bidirectional feature aggregation.\nThe methods [15, 16, 18, 19, 35] employing the unidirectional feature enhancement strategy leverage the depth or thermal image as auxiliary information to enhance RGB information. Specifically, they usually feed the deep features extracted by the depth or thermal backbone into the RGB backbone to enhance the RGB features for robust semantic segmentation. RTFNet [15] and FEANet [16] use two independent ResNet [36] models as backbones to extract the deep features of the RGB and thermal images. To enhance the RGB features with the thermal features, they integrate the corresponding thermal features into the RGB backbone stream via element-wise\nfeature summation. The unidirectional feature enhancement strategy allows RTFNet and FEANet to perform semantic segmentation directly based on the RGB backbone stream features. However, such a strategy is less effective in processing night scenes, due to the thermal image playing a more important role than the RGB image.\nAlgorithms [2, 11, 13, 14, 17, 20, 21, 22, 23] exploiting the bidirectional feature aggregation strategy treat data from different modalities equally in aggregation. Several methods [13, 20] leverage attention-based fusion modules to aggregate multi-modality features. For example, ACNet [20] utilizes a channel attention-based module to extract the weighted RGB and depth features and then aggregates them via element-wise summation. ABMDRNet [13] proposes a multiscale spatial and channel context module to capture long-range dependencies along the spatial and channel dimensions to improve RGB-T segmentation accuracy. Some methods [2, 11, 17] use customized fusion modules to obtain discriminative cross-modality features. Specifically, RDFNet [11] first uses residual convolution units to refine the RGB and thermal image features separately and then merge the refined features by element-wise summation. GMNet [2] divides the multi-stage features into \u201csenior\u201d, \u201cintermediate\u201d, and \u201cjunior\u201d features, and further introduces the shallow and deep feature fusion modules at different stages to merge RGB and thermal features.\nUnlike the above methods, the other methods [14, 22, 23] utilizing the bidirectional feature aggregation strategy aggregate the features in an interactive manner. They usually feed the aggregated features back to the next feature extraction layer of the backbone, aiming to maximize cross-modality synergies. Specifically, SAG [14] proposes an SA-Gate to perform informative feature propagation between different modalities via channel-wise attention. NANet [22] employs the non-local [37] operation to interactively exchange information between the RGB and depth modalities along the spatial and channel dimensions. CMX [23] proposes a transformer-based multi-modality fusion architecture for RGB-X semantic segmentation. It introduces a cross-modal feature rectification module to conduct channel-wise and spatial-wise rectification, and a feature fusion module to merge the multi-modality features. However, this directly interactive fusion manner without considering the modality gap inevitably results in contaminating the specific information of each modality. Differently, our proposed model first mines the modality-shared features and then interactively fuse the captured modality-shared features, alleviating the modality-specific information contamination issue."
        },
        {
            "heading": "3. Method",
            "text": "The crux of RGB-T semantic segmentation is to overcome the modality gap between the RGB and thermal images and to effectively fuse their features. To surmount this crux, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet), which first captures the modalityshared features via relation-propagation in the channel and spatial dimensions and then aggregates the captured modality-shared features. Figure 2 illustrates the overall architecture of CSRPNet, which comprises an encoder and a decoder. The encoder consists of a two-stream feature extraction network and the Channel and Spatial Relation-Propagation (CSRP) module. The encoder takes as input the RGB and thermal images and extracts their deep features while fusing the multimodality features in an interactive manner. The decoder is comprised of a Dual-path Cascaded Feature Refinement (DCFR) module, a boundary classifier, and a semantic classifier; it takes as\ninput the multi-layer fused features and outputs the boundary and segmentation maps. Here, learning to predict the boundary map can enhance the ability of the model to perceive boundary details, which in turn improves the semantic segmentation precision. In the following, we present each component of CSRPNet in detail."
        },
        {
            "heading": "3.1. Two-stream feature extractor and interactive fusion",
            "text": "We use a two-stream backbone network to extract the feature of the RGB and thermal images. Similar to [15, 17, 16], we use ResNet-50 [36] pre-trained on ImageNet [38] as the backbone. Note that the RGB and thermal backbones share a similar structure, with the only difference being that the input channel of the thermal backbone is modified to 1 since the thermal image is singlechannel. Given the RGB image IR and thermal image IT , the two-stream backbone network extract the multi-layer RGB features {FRl |l=1,2,3,4} and the multi-layer thermal features {FTl |l=1,2,3,4}. To effectively leverage the complementarity between RGB and thermal images, we adopt an interactive feature fusion manner. Specifically, we first enhance the feature from each modality with the feature from the other one using the CSRP module without contaminating modality-specific\ninformation. Then we feed the enhanced RGB or thermal features back to the subsequent feature extraction layer while fusing them together. This interactive fusion manner enables the deeper feature extraction layer to accept and process information from both RGB and thermal images, promoting tight coupling between multi-modality feature extraction and fusion. In the following, we present the channel and spatial relation-propagation module for multi-modality feature fusion."
        },
        {
            "heading": "3.2. Channel and spatial relation-propagation",
            "text": "The modality gap between the RGB and thermal images presents a problem in directly fusing their features interactively: directly aggregating the RGB and thermal features and feeding them back into the two-stream backbone network could lead to the contamination of modality-specific information in each modality. To handle the problem, we develop a Channel and Spatial RelationPropagation (CSRP) module to capture and aggregate the modality-shared features from different modalities. The modality-shared features enable interactive feature fusion without contaminating the modality-specific feature. The CSRP module is composed of a Channel Relation-Propagation (CRP) block, a Spatial Relation-Propagation (SRP) block, and an aggregation block.\nThe CRP and SRP blocks aim to capture the modality-shared features for interactive multimodality feature fusion by considering the channel-wise and pixel-wise relations, respectively. For this purpose, we first compute the relation matrices to represent the semantic and spatial dependencies between different channel pairs and pixel pairs, respectively. Then we capture feature channels and pixels sharing common relations across different modalities via relation-propagation. Specifically, given the paired feature maps {FM|M=R,T} \u2208Rc\u00d7h\u00d7w, we first introduce three different linear layers to learn three new feature representations, denoted by {QM,KM,V M} \u2208 Rc\u00d7h\u00d7w. Note that we omit the subscript l denoting the layer for presentation clarity in this subsection. Based on QM, KM, and V M, the CRP and SRP blocks perform relation-propagation for capturing modality-shared features."
        },
        {
            "heading": "3.2.1. Channel relation-propagation",
            "text": "Taken as input QM, KM, and V M, the CRP block first calculates the inter-channel relation matrix W Mch \u2208 Rc\u00d7c for every modality. Denoting the vectorial representation of the m-th channel of QM and the n-th channel of KM as qMch,m and k M ch,n, respectively, the (m,n)-th weight value in W Mch is computed via a relation function f (\u00b7, \u00b7), which can be formulated as:\nwMch,(m,n) = f (q M ch,m,k M ch,n). (1)\nHerein we consider two choices for f (\u00b7, \u00b7): dot product and Gaussian function. Dot product. Similar to [37, 39], the relation function f (\u00b7, \u00b7) can be defined as the dot product:\nf (x,y) = x \u00b7 y, (2)\nin which \u00b7 denotes the dot product operation. Gaussian function. Following the non-local network [37], a natural choice of f is the Gaussian function, which can be formulated as:\nf (x,y) = exp(x \u00b7 y). (3)\nDifferent feature channels represent various semantics. Thus, the calculated inter-channel relation matrix models the long-range semantic dependencies between different feature channels. A large relation weight denotes the strong semantic dependency between the corresponding channels. With W Rch and W T ch, we propagate the semantic relation between different modalities via performing the element-wise product between W Rch and W T ch, which enables us to identify the shared semantic dependencies between the RGB and thermal features. Formally, the shared relation matrix Pch \u2208 Rc\u00d7c can be formulated as:\nPch = \u03c3n(ReLU(W Rch)\u2299ReLU(W Tch)). (4)\nHerein \u2299 refers to the element-wise product operation, and \u03c3n denotes the normalization operation used to normalize the sum of each row of the input matrix to 1. The ReLU function in Eq. 4 is used to filter out the negative relation weight, which can be omitted when using the Gaussian function as the relation function f . After that, we use the shared relation matrix Pch as the attention weights to capture the modality-shared features, which can be implemented via matrix multiplication:\nSRch = PchV R, STch = PchV T .\n(5)\nHerein V M is reshaped to c\u00d7hw before performing matrix multiplication, and SRch and STch denote the captured modality-shared features for the RGB and thermal images, respectively."
        },
        {
            "heading": "3.2.2. Spatial relation-propagation",
            "text": "Similar to the CRP block, taken as input QM, KM, and V M, the SRP block first computes the inter-pixel relation matrix W Msp \u2208Rhw\u00d7hw for every modality. Denoting the vectorial representation of the u-th pixel of QM and the v-th pixel of KM as qMsp,u and k M sp,v, respectively, the (u,v)-th weight in W Msp can be computed via the relation function defined by Eq. 2 or Eq. 3. With the calculated inter-pixel relation matrices W Rsp and W T sp, we perform element-wise product between them to obtain the shared relation matrix Psp \u2208 Rhw\u00d7hw:\nPsp = \u03c3n(ReLU(W Rsp)\u2299ReLU(W Tsp)). (6)\nAfter that, we perform matrix multiplication to capture the features containing the modality-shared spatial information SRsp and S T sp:\nSRsp =V RPsp, STsp =V T Psp.\n(7)"
        },
        {
            "heading": "3.2.3. Interactive feature fusion",
            "text": "With the captured modality-shared features SRch, S T ch, S R sp, and S T sp, we aggregate them with the input features via the element-wise summation operation to enhance the input features. To be more specific, the modality-shared feature extracted from one modality will be aggregated with the input feature of the other modality. For example, the modality-shared features extracted from the RGB\nimage SRch and S R sp will be aggregated with the input feature of the thermal modality F T . In this way, we can exploit the modality-shared features of one modality to enhance the features of the other modality without contaminating its modality-specific information. The above aggregation process can be formulated as:\nFRenhance = F R \u2295\u03bb TchSTch \u2295\u03bb TspSTsp, FTenhance = F T \u2295\u03bb RchSRch \u2295\u03bb RspSRsp.\n(8)\nHerein \u2295 denotes the element-wise summation operation. \u03bb Rch, \u03bb Rsp, \u03bb Tch, and \u03bb Tsp are learnable parameters, whose initialization values are set to 1 at beginning of the training in our experiments. While feeding FRenhance and F T enhance back to the feature extraction block of the next layer, we also fuse them together to obtain the fused feature FC."
        },
        {
            "heading": "3.3. Dual-path cascaded feature refinement",
            "text": "As above-mentioned, we introduce boundary prediction as an auxiliary task into our architecture to improve the ability of our CSRPNet to perceive the boundary between different semantic regions, which has been proven effective by [17, 40]. Unlike the semantic segmentation task, the boundary prediction task only necessitates the model to predict whether a pixel falls on a boundary between different semantic regions. Considering this difference, we propose a Dual-path Cascaded Feature Refinement (DCFR) module to generate two different refined feature maps for boundary prediction and semantic segmentation, respectively. Besides, the cascaded refinement mechanism in the DCFR module enables our method to harness the potential of the multi-layer features. As shown in Figure 3 (a), the DCFR module consists of an auxiliary path and a main path, taking as input the multi-layer fused features {FCl |l=1,2,3,4} and outputting two refined feature maps.\nThe auxiliary path is mainly composed of cascaded merging blocks. Every merging block, except the first one, accepts the fused feature FCl and the output feature of the last merging block\nFMergel+1 as inputs. In the inputs, F C l with higher resolution contains more spatial details, and F Merge l+1 from a deeper layer contains more semantic information. Merging them yields more informative feature representations. Figure 3 (b) illustrates the structure of the merging block. Similar to [41], it first uses a 1\u00d71 convolutional layer to reduce the channel numbers of FCl , yielding the channeldownsampled feature FAd jl . Meanwhile, it uses spatial up-sampling to adjust the spatial size of FMergel+1 to that of F Ad j l . Then it concatenates F Ad j l and the up-sampled F Merge l+1 and processes the concatenated feature using a 1\u00d71 convolutional layer with a skip connection. After that, we use a 3\u00d73 convolutional layer with a skip connection to generate the final merged feature. This process can be formulated as:\nFAd jl = \u03c6conv1(F C l ), FEl = \u03c6conv2(F Ad j l \u228e\u03c6up(F Merge l+1 ))\u2295F Ad j l , FMergel = ReLU(\u03c6conv3(F E l ))\u2295FEl ,\n(9)\nwhere \u03c6conv1, \u03c6conv2, and \u03c6conv3 denote the three convolutional layers in Figure 3 (b). \u03c6up denotes the spatial up-sampling operation. \u228e and \u2295 denote the concatenation and element-wise summation operations, respectively. In the auxiliary path, we only use the shallow fused features {FCl |l = 3,2,1}, since predicting boundary maps requires more spatial details than semantic information. For the first merging block that only accepts the fused feature as input, we remove the up-sample and concatenation operations.\nIn the main path, we take the fused features from all layers into account as both the semantic information and spatial details are crucial for semantic segmentation. The main path is composed of the cascaded Upception blocks [15], which are used to increase the spatial resolution of the input features. In addition, we introduce the skip connections between the auxiliary path and the main path. This design enables the main path to exploit the boundary information learned by the auxiliary path from the boundary prediction task."
        },
        {
            "heading": "3.4. Classifiers",
            "text": "On top of the output features Fbdr and Fseg, we construct a boundary classifier and a semantic classifier to predict the boundary map and the semantic map, respectively. Specifically, the boundary classifier is composed of a 1\u00d71 convolutional layer and a 3\u00d73 convolutional layer, following the design of [17]. The semantic classifier is constructed using an Upception block [15]."
        },
        {
            "heading": "3.5. Loss function",
            "text": "To train our CSRPNet, we adopt a multi-task loss function that imposes supervision on the predicted boundary map and semantic map. The multi-task loss function can be formulated as:\nLtotal = \u03bb1Lbdr +\u03bb2Lseg, (10)\nwhere Lbdr and Lseg denote the boundary loss and the semantic segmentation loss, respectively, and \u03bb1 and \u03bb2 are the balance weights. We utilize a sliding window mechanism to obtain the boundary label. We define the center pixel of the sliding window as a boundary pixel if there is more than one semantic class in the sliding window. Herein the size of the sliding window\nis set to 5\u00d7 5. Considering the imbalance between the boundary pixels and the non-boundary pixels, we use the weighted cross-entropy loss as our boundary loss, in which the weights are set following [42]. In addition, we use the cross-entropy loss as our semantic segmentation loss."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Implementation details",
            "text": "We implement the proposed CSRPNet with the PyTorch toolkit on a computer equipped with the NVIDIA GTX 2080Ti GPU. We use a two-stage learning strategy for training. In the first stage, we only optimize the learnable parameters in the encoder, the main path of the DCFR module, and the semantic classifier. In the second stage, we freeze the parameters optimized in the first stage and optimize the learnable parameters in the auxiliary path of the DCFR module and the boundary classifier. In particular, we use the Stochastic Gradient Descent (SGD) method with a momentum of 0.9 and a weight decay of 0.0005 as the optimizer in both two training stages. The initial learning rates in both two training stages are set to 0.01 and decrease in an exponential manner with a decay factor of 0.95. The balance weights \u03bb1 and \u03bb2 are tuned to be both 1. In addition, the data augmentation methods including random flipping and random cropping are used to alleviate the problem of insufficient training data."
        },
        {
            "heading": "4.2. Benchmarks and metrics",
            "text": "We evaluate CSRPNet on two popular benchmarks, including MFNet [34] and PST900 [43]. The MFNet dataset contains nine semantic classes, including Car, Person, Bike, Curve, Car stop, Guardrail, Color cone, and Bump. It consists of 1569 annotated RGB and thermal image pairs in total, in which 820 and 749 image pairs are taken in the daytime and nighttime, respectively. All images are resized to 480\u00d7 640. For a fair comparison, we use the same training and evaluation configurations as those in MFNet [34]. In particular, we follow MFNet [34] to split the data into the training, validation, and testing sets, which contain 784, 392, and 393 image pairs, respectively. The PST900 [43] dataset is composed of 894 aligned RGB and thermal image pairs and contains four visible artifacts, including Fire-Extinguisher, Backpack, Hand-Drill, and Survivor. Following\nMFNet [34], we use the widely used mean accuracy (mAcc) and mean intersection over union (mIoU) to quantitatively evaluate segmentation performance. The mean accuracy metric is also known as recall. Formally, these two metrics are defined as:\nmAcc= 1 N\nN\n\u2211 i=1 \u2211Kk=1 pkii \u2211Kk=1 pkii+\u2211 K k=1 \u2211 N j=1, j \u0338=i p k i j ,\nmIoU= 1 N\nN\n\u2211 i=1 \u2211Kk=1 pkii \u2211Kk=1 pkii+\u2211 K k=1 \u2211 N j=1, j \u0338=i(p k ji+p k i j) .\n(11)\nHerein N is the number of semantic classes. K is the number of RGB-T image pairs. pkii is the number of pixels that belong to class i and are correctly classified as class i in the k-th RGB-T image pair. pkji is the number of pixels that belong to class j but are incorrectly classified as class i in the k-th RGB-T image pair. pki j is the number of pixels that belong to class i but are incorrectly classified as class j in the k-the RGB-T image pair."
        },
        {
            "heading": "4.3. Ablation study",
            "text": "We first conduct experiments to investigate the effect of each proposed component and the effect of using features from different layers. Note that we use the dot product as the relation function in this section."
        },
        {
            "heading": "4.3.1. Effect of each component",
            "text": "To analyze the effect of each component, we evaluate six variants of our algorithm: (a) the baseline (BL) model without the proposed CSRP and DCFR modules, which directly fuses the features of the RGB and thermal images from the last layer via element-wise summation and uses the decoder in MFNet [34] to predict the semantic map; (b) the variant that exploits the SRP block for multi-modality feature fusion based on the baseline model; (c) the variant that uses the CRP block for feature fusion based on the baseline model; (d) the variant that uses the proposed CSRP\nmodule for feature fusion based on the baseline model; (e) our intact model including the proposed CSRP and DCFR modules; (f) the variant that removes the boundary prediction branch and the boundary supervision from our intact model. Table 1 reports the overall, daytime, and nighttime performance of these variants on the MFNet datasets.\nCompared with the baseline model, the variants using the SRP and CRP blocks achieve performance gains of 6.5%/8.5% and 4.7%/4.9% in overall mAcc and mIoU, respectively. These performance gains demonstrate the effectiveness of spatial or channel relation-propagation. Compared with the variants using the SRP block or the CRP block alone, exploiting them together (the variant using the CSRP module) further promotes segmentation performance, which manifests that the relation-propagation in space and channel is complementary. Furthermore, the performance gap in overall mAcc between the variant using the CSRP module and our intact model validates the effectiveness of the DCFR module. Besides, removing the boundary prediction branch and the boundary supervision leads to performance drops, demonstrating their effectiveness. The comparisons between these variants in terms of daytime and nighttime performance also demonstrate the effectiveness of each proposed component.\nWe also visualize the segmentation results of different variants to analyze the effect of the CSRP and DCFR modules, as shown in Figure 4. Compared with the baseline model, the variant using the CSRP module predicts more accurate segmentation masks. The comparison between the last two rows shows that the DCFR module enables our method to predict more refined masks, especially for small objects. Removing the boundary prediction branch and boundary supervision leads to a less precise segmentation."
        },
        {
            "heading": "4.3.2. Effect of using features from different layers",
            "text": "We also conduct experiments to analyze the effect of deploying the CSRP module in different layers of the encoder based on the above-mentioned variant (d). Table 2 reports the detailed experimental results. Considering that calculating the relation matrix of the feature map of the\nfirst layer with a high resolution is memory-consuming and the low-level features lack abstract semantic information, we omit the CSRP module in the first layer of the encoder. From the first to the fourth rows in Table 2, we can observe that deploying the proposed CSRP module in the deeper layer leads to better performance, which demonstrates that the feature from the deeper layer models more abstract semantics to extract the modality-shared information. In addition, we can observe that deploying the proposed CSRP module in layer2, layer3, and layer4 leads to the best segmentation performance."
        },
        {
            "heading": "4.4. Comparison with state-of-the-art methods",
            "text": "In this section, we evaluate our method on the MFNet [34] and PST900 [43] datasets. We detail the comparison results per dataset in the following. The MFNet dataset. We compare our proposed CSRPNet with 15 state-of-the-art methods, including FRRN [44], BiSeNet [45], DFN [46], FuseNet [18], DepthAwareCNN [12], MFNet [34],\nRTFNet [15], FuseSeg-161 [35], ABMDRNet [13], EGFNet [17], GCNet [47], MMDRNet [48], GCGLNet [49], FEANet [16], MMNet [50], on the MFNet dataset. Table 5 reports the experimental results. For a fair comparison, we test two versions of several RGB segmentation methods. One is the \u20183c\u2019 version taking as input the 3-channel RGB data for testing, and the other is the \u20184c\u2019 version taking as input the 4-channel RGB-T data for testing. Herein we display the results of our approaches with two different relation functions, including dot product and Gaussian function. The one with the Gaussian function performs marginally better than the one using the dot product. We attribute it to the Gaussian function performing relation modeling in a high-dimension space. Our methods perform comparably against state-of-the-art methods including MMDRNet, GCGLNet, and FEANet. Besides, compared with the recently proposed EGFNet method, CSRPNet (Gaussian) improves the mIoU score by 1.2%. We also find that several methods tested with the four-channel RGB-T data do not perform better than the version tested with the three-channel RGB data in some cases. We speculate the reason is that these methods ignore the modality gap between the RGB and thermal images, which will adversely affect the segmentation performance.\nFurthermore, we compare CSRPNet with the other state-of-the-art methods on the MFNet dataset for daytime and nighttime images. Table 4 reports the corresponding experimental results. CSRPNet (Dot product) and CSRPNet (Gaussian) achieve promising performance for both the daytime and nighttime testing images. Many methods including FuseNet, MFNet, RTFNet-50,\nand RTFNet-152 ignore the modality gap and thus only obtain limited performance. The PST900 dataset. Seven state-of-the-art methods are involved in the comparison, including Efficient FCN [51], CCNet [31], ACNet [20], MFNet [34], RTFNet [15], and PSTNet [43]. The proposed CSRPNet (Dot product) and CSRPNet (Gaussian) obtain mAcc scores of 81.4 and 73.9 and mIoU scores of 73.9 and 75.0, respectively. Our methods perform favorably against the other state-of-the-art algorithms, which demonstrates the effectiveness of our method."
        },
        {
            "heading": "4.5. Qualitative Study",
            "text": "To obtain more insights into our CSRPNet, we qualitatively compare it with state-of-the-art methods. Figure 5 shows the segmentation results of different methods. The left four image pairs\nare captured in the daytime, while the right four image pairs are captured in the nighttime. In the daytime scene, the objects with low temperatures are poorly visible in the thermal images, while their texture and color are clearly visible in the RGB images, such as the bump in the first column and the bike in the fourth column. CSRPNet (Dot product) successes recognizing the bump, while the other methods almost cannot. Besides, CSRPNet (Dot product) and CSRPNet (Gaussian) segment the bike more precisely than the MFNet and EGFNet methods. In the night scene, most objects are poorly visible in the RGB images due to insufficient light, while they are clearly visible in the thermal images. Compared with the other methods, CSRPNet (Dot product) and CSRPNet (Gaussian) predict more precise segmentation masks. For example, in the fifth column, the MFNet method misrecognizes several pixels belonging to the person as the car while CSRPNet (Dot product) and CSRPNet (Gaussian) segment the persons precisely. In the seventh column, the EGFNet method misses the bike on the left while CSRPNet (Dot product) and CSRPNet (Gaussian) recognize all the bikes. Besides, the MFNet, RTFNet-50, and RTFNet-152 methods do not use boundaries as auxiliary supervisions. As a result, they cannot process the details of the object boundary well, especially for small objects.\nWe also visualize the modality-shared features calculated via the CSRP module in layer4. Herein dot product is used as the relation function for visualization. Specifically, we visualize two modality-shared feature maps SR = SRch+S R sp and S T = STch+S T sp; they contain the modality-shared information captured from the features of the thermal image and the RGB image, respectively. Figure 6 illustrates the raw RGB-T image pair, ground truth mask, the modality-shared feature of the RGB image SR, and the modality-shared feature of the thermal image ST . We can observe that SR and ST share similar activation distributions and both focus on the foreground objects. Such an observation validates that the proposed CSRP module effectively captures the modality-\nshared information, allowing interactive feature fusion across modalities without corrupting the modality-specific information."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a channel and spatial relation-propagation network for RGB-T semantic segmentation. Specifically, we propose a channel and spatial relation-propagation (CSRP) module to overcome the modality gap for multi-modality feature fusion. The CSRP module first captures modality-shared features, and then performs feature aggregation based on the captured features, alleviating the issue of contaminating modality-specific information. By plugging the proposed CSRP module into different layers of the two-stream backbone network, we achieve interactive feature fusion without the adverse effect of the modality gap, enabling our model to better leverage the complementary nature between RGB and thermal images. In addition, we propose a dual-path cascaded feature refinement module to make full use of the multi-layer fusion features. The proposed method achieves favorable performance against state-of-the-art methods on the MFNet and PST900 datasets, demonstrating its effectiveness."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was supported in part by the National Natural Science Foundation of China (No. 62172126), in part by the Shenzhen Research Council (No. JCYJ20210324120202006 and No. JCYJ20210324132212030), and in part by the Major Key Project of PCL (No. PCL2021A03-1 and No. PCL2021A07)."
        }
    ],
    "title": "Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation",
    "year": 2023
}