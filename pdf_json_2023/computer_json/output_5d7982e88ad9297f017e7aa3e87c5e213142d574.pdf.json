{
    "abstractText": "Class-agnostic object counting aims to count object instances of an arbitrary class at test time. It is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs which are often unavailable for novel categories, especially for autonomous systems. Thus, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. This obviates the need for human annotators and enables automated operation. To perform ZSC, we propose finding a few object crops from the input image and use them as counting exemplars. The goal is to identify patches containing the objects of interest while also being visually representative for all instances in the image. To do this, we first construct class prototypes using large language-vision models, including CLIP and Stable Diffusion, to select the patches containing the target objects. Furthermore, we propose a ranking model that estimates the counting error of each patch to select the most suitable exemplars for counting. Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingyi Xu"
        },
        {
            "affiliations": [],
            "name": "Hieu Le"
        },
        {
            "affiliations": [],
            "name": "Dimitris Samaras"
        }
    ],
    "id": "SP:2156f271377f4669d085ce886929b674e9ae5a7b",
    "references": [
        {
            "authors": [
                "D.B. Sam",
                "A. Agarwalla",
                "J. Joseph",
                "V.A. Sindagi",
                "R.V. Babu",
                "V.M. Patel"
            ],
            "title": "Completely self-supervised crowd counting via distribution matching",
            "venue": "ECCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.N. Mundhenk",
                "G. Konjevod",
                "W.A. Sakla",
                "K. Boakye"
            ],
            "title": "A large contextual dataset for classification, detection and counting of cars with deep learning",
            "venue": "ECCV, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Arteta",
                "V.S. Lempitsky",
                "A. Zisserman"
            ],
            "title": "Counting in the wild",
            "venue": "ECCV, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "W. Xie",
                "J.A. Noble",
                "A. Zisserman"
            ],
            "title": "Microscopy cell counting and detection with fully convolutional regression networks",
            "venue": "Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, vol. 6, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "V. Ranjan",
                "U. Sharma",
                "T. Nguyen",
                "M. Hoai"
            ],
            "title": "Learning to count everything",
            "venue": "CVPR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Shi",
                "H. Lu",
                "C. Feng",
                "C. Liu",
                "Z. Cao"
            ],
            "title": "Represent, compare, and learn: A similarity-aware framework for classagnostic counting",
            "venue": "CVPR, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Lu",
                "W. Xie",
                "A. Zisserman"
            ],
            "title": "Class-agnostic counting",
            "venue": "ACCV, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "V. Ranjan",
                "M. Hoai"
            ],
            "title": "Exemplar free class agnostic counting",
            "venue": "ACCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zheng",
                "J. Wu",
                "Y. Qin",
                "F. Zhang",
                "L. Cui"
            ],
            "title": "Zero-shot instance segmentation",
            "venue": "CVPR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Bansal",
                "K. Sikka",
                "G. Sharma",
                "R. Chellappa",
                "A. Divakaran"
            ],
            "title": "Zero-shot object detection",
            "venue": "ECCV, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Xu",
                "H. Le",
                "V. Nguyen",
                "V. Ranjan",
                "D. Samaras"
            ],
            "title": "Zero-shot object counting",
            "venue": "CVPR, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark",
                "G. Krueger",
                "I. Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "ICML, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "CVPR, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Lian",
                "J. Li",
                "J. Zheng",
                "W. Luo",
                "S. Gao"
            ],
            "title": "Density map regression guided detection network for rgb-d crowd counting and localization",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Zhang",
                "L. Yue",
                "J. Shen",
                "F. Zhu",
                "X. Zhen",
                "X. Cao",
                "L. Shao"
            ],
            "title": "Attentional neural fields for crowd counting",
            "venue": "ICCV, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wang",
                "J. Gao",
                "W. Lin",
                "X. Li"
            ],
            "title": "Nwpu-crowd: A large-scale benchmark for crowd counting and localization",
            "venue": "pami, vol. 43, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "V.A. Sindagi",
                "R. Yasarla",
                "V.M. Patel"
            ],
            "title": "Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method",
            "venue": "ICCV, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Idrees",
                "M. Tayyab",
                "K. Athrey",
                "D. Zhang",
                "S.A. Al-Maadeed",
                "N.M. Rajpoot",
                "M. Shah"
            ],
            "title": "Composition loss for counting, density map estimation and localization in dense crowds",
            "venue": "ECCV, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Abousamra",
                "M. Hoai",
                "D. Samaras",
                "C. Chen"
            ],
            "title": "Localization in the crowd with topological constraints",
            "venue": "AAAI, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhang",
                "A. Chan"
            ],
            "title": "Calibration-free multi-view crowd counting",
            "venue": "ECCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Xiong",
                "A. Yao"
            ],
            "title": "Discrete-constrained regression for local counting models",
            "venue": "ECCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Liu",
                "N. Durasov",
                "P. Fua"
            ],
            "title": "Leveraging self-supervision for cross-domain crowd counting",
            "venue": "CVPR, 2022. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12",
            "year": 2022
        },
        {
            "authors": [
                "J. Wan",
                "Z. Liu",
                "A.B. Chan"
            ],
            "title": "A generalized loss function for crowd counting and localization",
            "venue": "CVPR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.-R. Hsieh",
                "Y.-L. Lin",
                "W.H. Hsu"
            ],
            "title": "Drone-based object counting by spatially regularized regional proposal network",
            "venue": "ICCV, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P. Chattopadhyay",
                "R. Vedantam",
                "R.R. Selvaraju",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Counting everyday objects in everyday scenes",
            "venue": "CVPR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "I.H. Laradji",
                "N. Rostamzadeh",
                "P.H.O. Pinheiro",
                "D. V\u00e1zquez",
                "M.W. Schmidt"
            ],
            "title": "Where are the blobs: Counting by localization with point supervision",
            "venue": "ECCV, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhang",
                "H. Li",
                "X. Wang",
                "X. Yang"
            ],
            "title": "Cross-scene crowd counting via deep convolutional neural networks",
            "venue": "CVPR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Cholakkal",
                "G. Sun",
                "F.S. Khan",
                "L. Shao"
            ],
            "title": "Object counting and instance segmentation with image-level supervision",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Cholakkal",
                "G. Sun",
                "S.H. Khan",
                "F.S. Khan",
                "L. Shao",
                "L.V. Gool"
            ],
            "title": "Towards partial supervision for generic object counting in natural scenes",
            "venue": "TPAMI, vol. 44, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Wang",
                "H. Liu",
                "D. Samaras",
                "M.H. Nguyen"
            ],
            "title": "Distribution matching for crowd counting",
            "venue": "NeurIPS, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "D. Zhou",
                "S. Chen",
                "S. Gao",
                "Y. Ma"
            ],
            "title": "Single-image crowd counting via multi-column convolutional neural network.",
            "year": 2016
        },
        {
            "authors": [
                "A.B. Chan",
                "Z.-S.J. Liang",
                "N. Vasconcelos"
            ],
            "title": "Privacy preserving crowd monitoring: Counting people without people models or tracking",
            "venue": "CVPR, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "W. Liu",
                "M. Salzmann",
                "P.V. Fua"
            ],
            "title": "Context-aware crowd counting",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Yang",
                "H.-T. Su",
                "W.H. Hsu",
                "W.-C. Chen"
            ],
            "title": "Class-agnostic few-shot object counting",
            "venue": "WACV, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Gong",
                "S. Zhang",
                "J. Yang",
                "D. Dai",
                "B. Schiele"
            ],
            "title": "Class-agnostic object counting robust to intraclass diversity",
            "venue": "ECCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Nguyen",
                "C. Pham",
                "K. Nguyen",
                "M. Hoai"
            ],
            "title": "Few-shot object counting and detection",
            "venue": "ECCV, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Liu",
                "Y. Zhong",
                "A. Zisserman",
                "W. Xie"
            ],
            "title": "Countr: Transformerbased generalised visual counting",
            "venue": "BMVC, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. You",
                "K. Yang",
                "W. Luo",
                "X. Lu",
                "L. Cui",
                "X. Le"
            ],
            "title": "Few-shot object counting with similarity-aware feature enhancement",
            "venue": "WACV, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "C. Arteta",
                "V.S. Lempitsky",
                "J.A. Noble",
                "A. Zisserman"
            ],
            "title": "Interactive object counting",
            "venue": "ECCV, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R.B. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "TPAMI, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L. Chen",
                "H. Zhang",
                "J. Xiao",
                "W. Liu",
                "S.-F. Chang"
            ],
            "title": "Zeroshot visual recognition using semantics-preserving adversarial embedding networks",
            "venue": "CVPR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Jayaraman",
                "K. Grauman"
            ],
            "title": "Zero-shot recognition with unreliable attributes",
            "venue": "NIPS, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Frome",
                "G.S. Corrado",
                "J. Shlens",
                "S. Bengio",
                "J. Dean",
                "M. Ranzato",
                "T. Mikolov"
            ],
            "title": "Devise: A deep visual-semantic embedding model",
            "venue": "NIPS, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Rezaei",
                "M. Shahidi"
            ],
            "title": "Zero-shot learning and its applications from autonomous vehicles to covid-19 diagnosis: A review",
            "venue": "Intelligence-Based Medicine, vol. 3, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Romera-Paredes",
                "P.H.S. Torr"
            ],
            "title": "An embarrassingly simple approach to zero-shot learning",
            "venue": "ICML, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Atzmon",
                "G. Chechik"
            ],
            "title": "Adaptive confidence smoothing for generalized zero-shot learning",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C.H. Lampert",
                "H. Nickisch",
                "S. Harmeling"
            ],
            "title": "Learning to detect unseen object classes by between-class attribute transfer",
            "venue": "CVPR, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Xian",
                "Z. Akata",
                "G. Sharma",
                "Q.N. Nguyen",
                "M. Hein",
                "B. Schiele"
            ],
            "title": "Latent embeddings for zero-shot classification",
            "venue": "CVPR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "L. Zhang",
                "T. Xiang",
                "S. Gong"
            ],
            "title": "Learning a deep embedding model for zero-shot learning",
            "venue": "CVPR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Xu",
                "H. Le",
                "M. Huang",
                "S. Athar",
                "D. Samaras"
            ],
            "title": "Variational feature disentangling for fine-grained few-shot classification",
            "venue": "ICCV, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Xu",
                "H. Le"
            ],
            "title": "Generating representative samples for few-shot classification",
            "venue": "CVPR, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Le",
                "T.F.Y. Vicente",
                "V. Nguyen",
                "M. Hoai",
                "D. Samaras"
            ],
            "title": "A+D Net: Training a shadow detector with adversarial shadow attenuation",
            "venue": "ECCV, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xian",
                "S. Sharma",
                "B. Schiele",
                "Z. Akata"
            ],
            "title": "F-vaegan-d2: A feature generating framework for any-shot learning",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xian",
                "C.H. Lampert",
                "B. Schiele",
                "Z. Akata"
            ],
            "title": "Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly",
            "venue": "pami, vol. 41, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Narayan",
                "A. Gupta",
                "F.S. Khan",
                "C.G.M. Snoek",
                "L. Shao"
            ],
            "title": "Latent embedding feedback and discriminative features for zeroshot classification",
            "venue": "ECCV, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Arjovsky",
                "S. Chintala",
                "L. Bottou"
            ],
            "title": "Wasserstein gan",
            "venue": "ICML, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models.",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "J. Sohl-Dickstein",
                "E. Weiss",
                "N. Maheswaranathan",
                "S. Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics.",
            "venue": "in ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Song",
                "J. Sohl-Dickstein",
                "D.P. Kingma",
                "A. Kumar",
                "S. Ermon",
                "B. Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations.",
            "venue": "ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ramesh",
                "P. Dhariwal",
                "A. Nichol",
                "C. Chu",
                "M. Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "ArXiv, vol. abs/2204.06125, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "S. Saxena",
                "L. Li",
                "J. Whang",
                "E.L. Denton",
                "K. Ghasemipour",
                "R.G. Lopes",
                "B.K. Ayan",
                "e. a"
            ],
            "title": "Tim Salimans, \u201cPhotorealistic text-to- image diffusion models with deep language understanding.",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhang",
                "X. Hu",
                "B. Li",
                "S. Huang",
                "H. Deng",
                "H. Li",
                "Y. Qiao",
                "P. Gao"
            ],
            "title": "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners.",
            "year": 2023
        },
        {
            "authors": [
                "W. Wu",
                "Y. Zhao",
                "M.Z. Shou",
                "H. Zhou",
                "C. Shen"
            ],
            "title": "Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models.",
            "year": 2023
        },
        {
            "authors": [
                "L. Karazija",
                "I. Laina",
                "A. Vedaldi",
                "C. Rupprecht"
            ],
            "title": "Diffusion models for zero-shot open-vocabulary segmentation.",
            "venue": "ArXiv, vol. abs/2306.09316,",
            "year": 2023
        },
        {
            "authors": [
                "C. Ma",
                "Y. Yang",
                "C. Ju",
                "F. Zhang",
                "J. Liu",
                "Y. Wang",
                "Y. Zhang"
            ],
            "title": "Diffusionseg: Adapting diffusion towards unsupervised object discovery.",
            "venue": "ArXiv, vol",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1\nZero-Shot Object Counting with Language-Vision Models\nJingyi Xu, Hieu Le and Dimitris Samaras\nAbstract\u2014Class-agnostic object counting aims to count object instances of an arbitrary class at test time. It is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs which are often unavailable for novel categories, especially for autonomous systems. Thus, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. This obviates the need for human annotators and enables automated operation. To perform ZSC, we propose finding a few object crops from the input image and use them as counting exemplars. The goal is to identify patches containing the objects of interest while also being visually representative for all instances in the image. To do this, we first construct class prototypes using large language-vision models, including CLIP and Stable Diffusion, to select the patches containing the target objects. Furthermore, we propose a ranking model that estimates the counting error of each patch to select the most suitable exemplars for counting. Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method.\nIndex Terms\u2014Class-agnostic object counting, variational autoencoder, diffusion models, stable diffusion, zero-shot learning\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Object counting aims to infer the number of objects in an image. Most of the existing methods focus on counting objects from specialized categories such as human crowds [1], cars [2], animals [3], and cells [4]. These methods count only a single category at a time. Recently, class-agnostic counting [5], [6], [7] has been proposed to count objects of arbitrary categories. Several human-annotated bounding boxes of objects are required to specify the objects of interest (see Figure 1a). However, having humans in the loop is not practical for many real-world applications, such as fully automated wildlife monitoring systems or visual anomaly detection systems.\nA more practical setting, exemplar-free class-agnostic counting, has been proposed recently by Ranjan et al. [8]. They introduce RepRPN, which first identifies the objects that occur most frequently in the image, and then uses them as exemplars for object counting. Even though RepRPN does not require any annotated boxes at test time, the method simply counts objects from the class with the highest number of instances. As a result, it can not be used for counting a specific class of interest. The method is only suitable for counting images with a single dominant object class, which limits its potential applicability.\nThus, our goal is to build an exemplar-free object counter where we can specify what to count. To this end, we introduce a new counting task in which the user only needs to provide the name of the class for counting rather than the exemplars (see Figure 1b). Note that the class to count during test time can be arbitrary. For cases where the test class is completely unseen to the trained model, the counter needs to adapt to the unseen class without any annotated\n\u2022 J. Xu and D. Samaras are with the Department of Computer Science, Stony Brook University, Stony Brook, NY 11794. E-mail: {jingyixu, samaras}@cs.stonybrook.edu \u2022 H. Le is with the Computer Vision Lab, EPFL, Laussane, Switzerland, E-mail: minh.le@epfl.ch\ndata. Hence, we name this setting zero-shot object counting (ZSC), inspired by previous zero-shot learning approaches [9], [10].\nTo count without any annotated exemplars, we propose finding a few patches in the input image containing the target object to use them as counting exemplars. There are two challenges: 1) how to localize patches that contain the object of interest based on the provided class name, and 2) how to select good exemplars for counting. Ideally, good object exemplars are visually representative for most instances in the image, which can benefit the object counter. In addition, we want to avoid selecting patches that contain irrelevant objects or backgrounds, which likely lead to incorrect object counts. To this end, we propose a two-step method that first localizes the class-relevant patches which contain the objects of interest based on the given class name, and then selects among these patches the optimal exemplars\nar X\niv :2\n30 9.\n13 09\n7v 1\n[ cs\n.C V\n] 2\n2 Se\np 20\n23\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2\nfor counting. We use these selected exemplars, together with a pre-trained exemplar-based counting model, to achieve exemplar-free object counting.\nThe first step of our framework involves constructing a class prototype based on the given class name. Essentially, this requires a mapping between the categorical label and its visual feature. We employ pre-trained large language-vision models to accomplish this via two approaches.\nIn the conference version of this paper [11], we learn this mapping between language queries and visual features via a conditional variational autoencoder (VAE). This VAE model is trained to generate visual features of object crops for any arbitrary class, conditioned on its semantic embedding extracted from a pre-trained language-vision model [12]. We take the average of the generated features to compute the class prototype, which can be then used to select classrelevant patches through a simple nearest-neighbour lookup scheme. In essence, our VAE-based approach creates a single prototypical feature for each category that can be applied to any images of this class.\nHowever, a single prototypical feature might not work well for categories with significant intra-class variance. Objects of the same category across different images can exhibit significant differences in colors (e.g., a green apple versus a red apple), shapes (e.g., an SUV versus a regular car), scales, or materials (a wooden versus a fabric chair). To better handle this variability, we propose to construct a class prototype specific to each image. To do so, we leverage the recent advancements in text-to-image generative models, i.e., Stable Diffusion [13], for prototype generation. Compared to classic VAE-based models, Stable Diffusion enables more realistic and diverse sample generation thanks to large-scale training data. This provides a way to deal with the variations in query objects. Specifically, given a query image, we first use Stable Diffusion to generate a variety of images containing the objects of interest. Then we select among them the object crops that most resemble the query objects and only use them for constructing the class prototype. In this way, a unique prototype is constructed specifically for each testing sample, as opposed to the VAEbased approach that uses a single universal prototype for all images. We show that using image-specific prototypes generally leads to better counting performance, compared to using a single generic categorical one.\nAfter obtaining the class-relevant patches, we want to select among them the optimal patches to be used as counting exemplars. Here we observe that the feature maps obtained using good exemplars and bad exemplars often exhibit distinguishable differences. An example of the feature maps obtained with different exemplars is shown in Figure 2. The feature map from a good exemplar typically exhibits some repetitive patterns (e.g., the dots on the feature map) that center around the object areas while the patterns from a bad exemplar are more irregular and occur randomly across the image. Based on this observation, we train a model to measure the goodness of an input patch based on its corresponding feature maps. Specifically, given an arbitrary patch and a pre-trained exemplar-based object counter, we train this model to predict the counting error of the counter when using the patch as the exemplar. Here the counting error can indicate the goodness of the exemplar.\nAfter this error predictor is trained, we use it to select those patches with the smallest predicted errors as the final exemplars for counting.\nExperiments on the FSC-147 dataset show the effectiveness of our proposed patch selection method. We also provide analyses to show that patches selected by our method can be used in other exemplar-based counting methods to achieve exemplar-free counting. In short, our main contributions are:\n\u2022 We introduce the task of zero-shot object counting that counts the number of instances of a specific class in the input image, given only the class name and without relying on any human-annotated exemplars.\n\u2022 We leverage language-vision models to construct class prototypes via two approaches: VAE-based approach and SD-based approach. We show that in both cases the class prototypes can be used to accurately select patches containing objects of interests for counting.\n\u2022 We introduce an error prediction model to further select the optimal patches that yield the smallest counting errors.\n\u2022 We verify the effectiveness of our patch selection method on the FSC-147 dataset, through extensive ablation studies and visualization results."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Class-specific Object Counting",
            "text": "Class-specific object counting focuses on counting predefined categories, such as humans [1], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], animals [3], cells [4], or cars [2], [24]. Generally, existing methods can be categorized into two groups: detection-based methods [24], [25], [26] and regression-based methods [27], [28], [29], [30], [31], [32], [33]. Detection-based methods apply an object detector on the image and count the number of objects based on the detected boxes. Regression-based methods predict a density map for each input image, and the final result is obtained by summing up the pixel values. Both types of methods require abundant training data to learn a good model. Class-specific\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3\ncounters can perform well on trained categories. However, they can not be used to count objects of arbitrary categories at test time."
        },
        {
            "heading": "2.2 Class-agnostic Object Counting",
            "text": "Class-agnostic object counting aims to count arbitrary categories given only a few exemplars [5], [6], [7], [34], [35], [36], [37], [38], [39]. GMN [7] uses a shared embedding module to extract feature maps for both query images and exemplars, which are then concatenated and fed into a matching module to regress the object count. FamNet [5] adopts a similar way to do correlation matching and further applies test-time adaptation. These methods require human-annotated exemplars as inputs. Recently, exemplarfree object counting has been proposed to eliminate the need for user inputs. Ranjan et al. have proposed RepRPN [8], which achieves exemplar-free counting by identifying exemplars from the most frequent objects via a Region Proposal Network (RPN)-based [40] model. However, the class of interest can not be explicitly specified for the RepRPN. In comparison, our proposed method can count instances of a specific class given only the class name."
        },
        {
            "heading": "2.3 Zero-shot Image Classification",
            "text": "Zero-shot classification aims to classify unseen categories for which data is not available during training [41], [42], [43], [44], [45], [46]. Semantic descriptors are mostly leveraged as a bridge to enable the knowledge transfer between seen and unseen classes. Earlier zero-shot learning (ZSL) works relate the semantic descriptors with visual features in an embedding space and recognize unseen samples by searching their nearest class-level semantic descriptor in this embedding space [45], [47], [48], [49]. Recently, generative models [50], [51], [52] have been widely employed to synthesize unseen class data to facilitate ZSL [53], [54], [55]. Xian et al. [54] use a conditional Wasserstein Generative Adversarial Network (GAN) [56] to generate unseen features which can then be used to train a discriminative classifier for ZSL. In our method, we also train a generative model conditioned on class-specific semantic embedding. Instead of using this generative model to hallucinate data, we use it to compute a prototype for each class. This class prototype is then used to select patches that contain objects of interest."
        },
        {
            "heading": "2.4 Diffusion Models",
            "text": "Diffusion models [57], [58], [59] recently have demonstrated great success in text-to-image generative systems (e.g., DALL-E [60], Imagen [61] and Stable Diffusion (SD) [13]). Exploring the potential of pre-trained diffusion models in downstream tasks has gained increasing attention. They have been used in few-shot image classification [62], semantic segmentation [63], [64] and object discovery [65]. Karazija et al. [64] leverage a diffusion-based generative model to produce a set of feature prototypes, which can then be used in a nearest-neighbour lookup scheme to segment images. In our method, we also use a text-conditioned diffusion model (i.e., Stable Diffusion) to construct visual class prototypes. We show that these prototypes can be used to find class-relevant patches for the task of class-agnostic counting."
        },
        {
            "heading": "3 METHOD",
            "text": "Figure 3 summarizes our proposed method. We first construct a class prototype for the given class name in a pretrained feature space. Then given an input query image, we generate a set of object proposals with a pre-trained RPN and crop the corresponding image patches. We extract the feature embedding for each patch and select the patches whose embeddings are the nearest neighbors of the class prototype as class-relevant patches. We further use an error predictor to select the patches with the smallest predicted errors as the final exemplars for counting. We use the selected exemplars in an exemplar-based object counter to infer the object counts. For the rest of the paper, we denote this exemplar-based counter as the \u201cbase counting model\u201d. We will first describe how we train this base counting model and then present the details of our patch selection method."
        },
        {
            "heading": "3.1 Training Base Counting Model",
            "text": "We train our base counting model using abundant training images with annotations. Similar to previous works [5], [6], the base counting model uses the input image and the exemplars to obtain a density map for object counting. The model consists of a feature extractor F and a counter C . Given a query image I and an exemplar B of an arbitrary class c, we input I and B to the feature extractor to obtain the corresponding output, denoted as F (I) and F (B) respectively. F (I) is a feature map of size d \u2217 hI \u2217 wI and F (B) is a feature map of size d\u2217hB\u2217wB . We further perform global average pooling on F (B) to form a feature vector b of d dimensions.\nAfter feature extraction, we obtain the similarity map S by correlating the exemplar feature vector b with the image feature map F (I). Specifically, if wij = Fij(I) is the channel feature at spatial position (i, j), S can be computed by:\nSij(I,B) = w T ijb. (1)\nIn the case where n exemplars are given, we use Eq. 1 to calculate n similarity maps, and the final similarity map is the average of these n similarity maps.\nWe then concatenate the image feature map F (I) with the similarity map S, and input them into the counter C to predict a density map D. The final predicted count N is obtained by summing over the predicted density map D:\nN = \u2211\ni,j\nD(i,j), (2)\nwhere D(i,j) denotes the density value for pixel (i, j). The supervision signal for training the counting model is the L2 loss between the predicted density map and the ground truth density map:\nLcount = \u2225D(I,B)\u2212D\u2217(I)\u222522, (3) where D\u2217 denotes the ground truth density map."
        },
        {
            "heading": "3.2 Zero-shot Object Counting",
            "text": "In this section, we describe how we count objects of any unseen category given only the class name without access to any exemplar. Our strategy is to select a few patches in the image that can be used as exemplars for the base counting\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4\nmodel. These patches are selected such that: 1) they contain the objects that we are counting and 2) they benefit the counting model, i.e., lead to small counting errors."
        },
        {
            "heading": "3.2.1 Selecting Class-relevant Patches",
            "text": "To select patches that contain the objects of interest, we first generate a class prototype based on the given class name. The class prototype can be considered as a class center representing the patch-level feature distribution of the corresponding class in an embedding space. Then we use the generated class prototype to select the class-relevant patches from a set of object patches cropped from the testing image.\nSpecifically, we introduce two ways of generating prototypes, i.e., generating semantics prototypes using conditional VAE and generating visual prototypes using samples from a latent text-to-image diffusion model, i.e., Stable Diffusion.\nVAE-based prototype generation. To generate class prototypes, we train a conditional VAE model to generate patch-level visual features for an arbitrary class based on the semantic embedding of the class. This strategy is inspired by previous zero-shot learning approaches [53], [54]. The semantic embedding is obtained from a pre-trained textvision model [12] given the corresponding class name. Specifically, we train a VAE model to reconstruct deep features extracted from a pre-trained ImageNet model. The VAE is composed of an Encoder E, which maps a visual feature x to a latent code z, and a decoder G which reconstructs x from z. Both E and G are conditioned on the semantic embedding a .The loss function for training this VAE for an input feature x can be defined as:\nLV (x) = KL (q(z|x, a)||p(z|a)) \u2212Eq(z|x,a)[log p(x|z, a)].\n(4)\nThe first term is the Kullback-Leibler divergence between the VAE posterior q(z|x, a) and a prior distribution p(z|a). The second term is the decoder\u2019s reconstruction\nerror. q(z|x, a) is modeled as E(x, a) and p(x|z, a) is equal to G(z, a). The prior distribution is assumed to be N (0, I) for all classes.\nWe can use the trained VAE to generate the semantics prototype for an arbitrary target class for counting. Specifically, given the target class name y, we first generate a set of features by inputting the respective semantic vector ay and a noise vector z to the decoder G:\nGy = {x\u0302|x\u0302 = G(z, y), z \u223c N (0, I)}. (5) The class prototype py is computed by taking the mean of all the features generated by VAE:\npy = 1 |Gy| \u2211 x\u0302\u2208Gy x\u0302 (6)\nSD-based prototype generation. In addition to VAEbased approach for prototype generation, we further leverage the recent advancements in text-to-image models, i.e., Stable Diffusion, to construct class prototypes from SDgenerated images. Compared to classic VAE-based models, Stable Diffusion enables more realistic and diverse sample generation, which allows handling the intra-class variation among query objects more effectively.\nSpecifically, given the target class name for counting, we first use a pre-trained Stable Diffusion model to generate a set of images with the class name as prompt. We observe that the SD-generated images often contain multiple object instances in various contexts and backgrounds. However, our goal is to obtain a few representative object crops of the target class that can be used to construct reference prototypes. In particular, given a query image, we aim to find a few diffusion-generated object crops that most resemble the target objects in the query image. To do so, we first apply a pre-trained RPN to predict object proposals on both the diffusion-generated images and the query image. Then we compute the pairwise distance between the diffusion-generated object embeddings and the query image\u2019s object embeddings. We select the topk diffusion-generated object embeddings with the nearest\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5\nmean distance over all query embeddings. We average these k embeddings to construct the visual class prototype. An example is shown in Figure 4 where the target objects are red apples. We first obtain a set of object patches containing various crops of apples and select from them a set of red apples to construct the prototype.\nClass-relevant patch selection. Using the class prototype, either generated using VAE or Stable Diffusion, we can select the class-relevant patches across the query image. Specifically, we first use a pre-trained RPN to predict object proposals across the query image and extract their corresponding ImageNet features {f1, f2, ..., fm}. To select the class-relevant patches, we calculate the L2 distance between the class prototype and the patch embedding, namely di = \u2225fi \u2212 py\u22252. Then we select the patches whose embeddings are the nearest neighbors of the class prototype as the class-relevant patches. Since the ImageNet feature space is highly discriminative, i.e., features close to each other typically belong to the same class, the selected patches are likely to contain the objects of the target class."
        },
        {
            "heading": "3.2.2 Selecting Exemplars for Counting",
            "text": "Given a set of class-relevant patches and a pre-trained exemplar-based object counter, we aim to select a few exemplars from these patches that are optimal for counting. To do so, we introduce an error prediction network that predicts the counting error of an arbitrary patch when the patch is used as the exemplar. The counting error is calculated from the pre-trained counting model. Specifically, to train this error predictor, given a query image I\u0304 and an arbitrary patch B\u0304 cropped from I\u0304 , we first use the base counting model to get the image feature map F (I\u0304), similarity map S\u0304, and the final predicted density map D\u0304. The counting error of the base counting model can be written as:\n\u03f5 = | \u2211\ni,j\nD\u0304(i,j) \u2212 N\u0304\u2217|, (7)\nwhere N\u0304\u2217 denotes the ground truth object count in image I\u0304 . \u03f5 can be used to measure the goodness of B\u0304 as an exemplar for I\u0304 , i.e., a small \u03f5 indicates that B\u0304 is a suitable exemplar for counting and vice versa.\nThe error predictor R is trained to regress the counting error produced by the base counting model. The input of R is the channel-wise concatenation of the image feature map F (I\u0304) and the similarity map S\u0304. The training objective is the minimization of the mean squared error between the output of the predictor R(F (I\u0304), S\u0304) and the actual counting error produced by the base counting model \u03f5.\nAfter the error predictor is trained, we can use it to select the optimal patches for counting. The candidates for selection here are the class-relevant patches selected by the class prototype in the previous step. For each candidate patch, we use the trained error predictor to infer the counting error when it is being used as the exemplar. The final selected patches for counting are the patches that yield the top-s smallest counting errors."
        },
        {
            "heading": "3.2.3 Using the Selected Patches as Exemplars",
            "text": "Using the error predictor, we predict the error for each candidate patch and select the patches that lead to the smallest counting errors. The selected patches can then be used as exemplars for the base counting model to get the density map and the final count. We also conduct experiments to show that these selected patches can serve as exemplars for other exemplar-based counting models to achieve exemplar-free class-agnostic counting."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Implementation Details",
            "text": "Network Architecture. For the base counting model, we use ResNet-50 as the backbone of the feature extractor, initialized with the weights of a pre-trained ImageNet model. The backbone outputs feature maps of 1024 channels. For each query image, the number of channels is reduced to 256 using an 1 \u00d7 1 convolution. For each exemplar, the feature maps are first processed with global average pooling and then linearly mapped to obtain a 256-d feature vector. The counter consists of 5 convolutional and bilinear upsampling layers to regress a density map of the same size as the query image. For the feature generation model, both the encoder and the decoder are two-layer fully-connected (FC) networks with 4096 hidden units. LeakyReLU and ReLU are the non-linear activation functions in the hidden and output layers, respectively. The dimensions of the latent space and the semantic embeddings are both set to be 512. The error predictor composes of 5 convolutional and bilinear upsampling layers, followed by a linear layer to output the counting error.\nDataset. We use the FSC-147 dataset [5] to train the base counting model and the error predictor. FSC-147 is the first large-scale dataset for class-agnostic counting. It includes 6135 images from 147 categories varying from animals, kitchen utensils, to vehicles. The categories in the training, validation, and test sets do not overlap. The feature generation model is trained using ImageNet features extracted from MS-COCO objects.\nTraining Details. Both the base counting model and the error predictor are trained using the AdamW optimizer with a fixed learning rate of 10\u22125. The base counting model is trained for 300 epochs with a batch size of 8. We resize the input query image to a fixed height of 384, and the width is adjusted accordingly to preserve the aspect ratio of the original image. Exemplars are resized to 128 \u00d7 128 before being input into the feature extractor. To select the class-relevant patches, we use the Region Proposal Network of Faster RCNN pre-trained on MS-COCO dataset to generate 100 object proposals per image. The feature generation model is trained using the Adam optimizer and\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6\nthe learning rate is set to be 10\u22124. With the VAE-based generator, the semantic embeddings are extracted from CLIP [12]. For visual prototype generation, we use stablediffusion-v1-4 [13] pre-trained on the laion dataset [66]. The generated images are in the size of 512 \u00d7 512. For each generated image, we take the top-5 RPN proposals with the highest objectness scores. We combine all the proposals from diffusion-generated images and extract their embeddings to do similarity matching with the object embeddings from the query image. We select the top-5 embeddings with nearest distances over query embeddings and compute their average to obtain the class prototype. The final selected patches are those that yield the top-3 smallest counting errors predicted by the error predictor."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We use Mean Average Error (MAE) and Root Mean Squared Error (RMSE) to measure the performance of different object counters. Besides, we follow [36] to report the Normalized Relative Error (NAE) and Squared Relative Error (SRE). In particular, MAE = 1n \u2211n i=1 |yi \u2212 y\u0302i|;\nRMSE = \u221a\n1 n \u2211n i=1(yi \u2212 y\u0302i)2; NAE = 1n \u2211n i=1 |yi\u2212y\u0302i| yi\n; SRE =\u221a 1 n \u2211n i=1 (yi\u2212y\u0302i)2 yi\nwhere n is the number of test images, and yi and y\u0302i are the ground truth and the predicted number of objects for image i respectively. Compared with the absolute errors MAE and RMSE, the relative errors NAE and SRE better reflect the practical usage of visual counting [36]."
        },
        {
            "heading": "4.3 Comparing Methods",
            "text": "We compare our method with the previous works on classagnostic counting, which can be categorized into exemplarbased counting methods and reference-less counting methods. Exemplar-based methods include FamNet (Few-shot adaptation and matching Network [5]), BMNet (Bilinear Matching Network [6]), CounTR (Counting TRansformer [37]) and SAFECount (Similarity-Aware Feature Enhancement block for object Counting [38]). These methods require a few human-annotated exemplars as inputs. Referenceless methods, i.e., RepRPN [8] and CounTR [37], do not require annotated boxes at test time. Nevertheless, the class of interest can not be specified, which makes them only suitable for counting images with a single dominant object class. Our proposed zero-shot counting, is a new setup which allows the user to specify what to count by simply providing the class name without any exemplar. We also make exemplar-based methods work in the exemplar-free manner by replacing the human-provided exemplars with the exemplars generated by a pre-trained object detector. Specifically, we use the RPN of Faster RCNN pre-trained on MS-COCO dataset and select the top-3 proposals with the highest objectness score as the exemplars."
        },
        {
            "heading": "4.4 Results",
            "text": "Quantitative results. As shown in Table 1, the performance of all exemplar-based counting methods drops significantly when replacing human-annotated exemplars with RPN generated proposals. BMNet+ [6], for example, shows an 19.90 error increase w.r.t. the test MAE and a 40.81 increase\nw.r.t. the test RMSE. In comparison, the performance gap is much smaller when using our selected patches as exemplars. Our patch selection method with VAE-generated prototype obtains 27.47 MAE on the validation set and 23.14 MAE on the test set. By using the SD-generated class prototype, the error rates can be further reduced, achieving 26.30 MAE on the validation set and 21.53 MAE on the test set. Noticeably, compared with the human-annotated exemplars, the NAE and the SRE on the test set are even reduced when using our selected patches.\nQualitative analysis. In Figure 5, we present a few input images, the image patches selected by our method, and the corresponding density maps. Our method effectively identifies the patches that are suitable for object counting. The density maps produced by our selected patches are meaningful and close to the density maps produced by human-annotated patches. The counting model with random image patches as exemplars, in comparison, fails to output meaningful density maps and infers incorrect object counts.\nIn Figure 6, we visualize some images from the FSC147 dataset and the corresponding patches selected by RPN and our method respectively. The RPN-selected patches are the top-3 proposals with the highest objectness scores. As can be seen from the figure, the patches selected by RPN may contain objects not relevant to the provided class name or contain multiple object instances. These patches are not suitable to be used as counting exemplars and will lead to inaccurate counting results. This suggests that choosing counting exemplars based on objectness score is not reliable. In comparison, our proposed method can accurately localize image patches according to the given class name. These selected patches can then be used as counting exemplars and yield meaningul density maps and reasonable counting results."
        },
        {
            "heading": "5 ANALYSIS",
            "text": ""
        },
        {
            "heading": "5.1 Ablation Studies",
            "text": "Our proposed patch selection method consists of two steps: the selection of class-relevant patches via a generated class prototype and the selection of the optimal patches via an error predictor. We analyze the contribution of each step quantitatively and qualitatively. Quantitative results are in Table 2. We first evaluate the performance of a simple RPNbased baseline, i.e. using the top-3 RPN proposals with the highest objectness scores as exemplars without any selection step. This baseline method has an error rate of 32.19 on the validation MAE and 29.25 on the test MAE. As shown in Table 2, using the class prototype generated by VAE to select class-relevant patches reduces the error rate by 2.34 and 4.87 on the validation and test set w.r.t. MAE, respectively. Using the class prototype generated by Stable Diffusion reduces the MAE by 4.43 and 7.26 on the validation and test set respectively. Applying the error predictor can improve the baseline performance by 4.63 on the validation MAE and 6.46 on the test MAE. Finally, using the Stable Diffusion prototype and error predictor together further boosts performance, achieving 26.30 on the validation MAE and 21.53 on the test MAE.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7\nWe provide further qualitative analysis by visualizing the selected patches. As shown in Figure 7, for each input query image, we show 10 class-relevant patches selected using the class prototype generated with Stable Diffusion, ranked by their predicted counting error (from low to high). All the 10 selected class-relevant patches exhibit some class specific features. However, not all these patches are suitable to be used as counting exemplars, i.e., some patches only contain parts of the object, and some patches contain some background. By further applying our proposed error predictor, we can identify the most suitable patches with the smallest predicted counting errors. Both the quantitative comparison in Table 1 and qualitative results presented in Figure 7 validate the effectiveness of our patch selection method."
        },
        {
            "heading": "5.2 Generalization to Exemplar-based Methods",
            "text": "Our proposed method can be considered as a general patch selection method that is applicable to other visual counters\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8\nto achieve zero-shot object counting. To verify that, we use our selected patches as the exemplars for four other different exemplar-based methods: FamNet+ [5], BMNet [6], BMNet+ [6] and SAFECount [38]. Table 3 shows the\nresults on the FSC-147 dataset. The baseline uses top3 RPN proposals with the highest objectness scores as exemplars for the pre-trained exemplar-based counter. Our patch selection method with VAE-generated class prototype reduces the error rates by a large margin for all exemplarbased methods. For example, the MAE for BMNet+ [6] reduces from 35.51 to 26.89 on the validation set and from 34.52 to 23.14 on the test set. By using our patch selection method with the SD-generated class prototype, the error rates are further reduced for most cases, e.g., we observe for FamNet+ [5], there is an error reduction of 12.4% w.r.t. the validation MAE and 18.0% w.r.t. the test MAE. The consistent performance improvements validate that our patch selection method generalizes well to other exemplarbased counting methods.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9\nBaseline Exemplars Val Set Test SetMAE RMSE NAE SRE MAE RMSE NAE SRE\nFamNet+ RPN 42.85 121.59 0.75 6.94 42.70 146.08 0.74 7.14\nPatch-Sel (VAE) 39.51 101.70 0.84 6.80 39.91 143.04 0.74 6.77 Patch-Sel (SD) 34.60 96.76 0.63 5.71 32.71 139.93 0.55 5.47\nBMNet RPN 37.26 108.54 0.42 5.43 37.22 143.13 0.41 5.31\nPatch-Sel (VAE) 27.71 93.98 0.36 4.55 24.45 130.42 0.33 4.09 Patch-Sel (SD) 26.53 91.55 0.32 4.36 22.27 129.67 0.28 3.85\nBMNet+ RPN 35.51 106.07 0.41 5.28 34.52 132.64 0.39 5.26\nPatch-Sel (VAE) 26.89 92.37 0.35 4.56 23.14 114.40 0.34 3.95 Patch-Sel (SD) 25.91 90.62 0.33 4.40 19.45 109.82 0.27 3.53\nSAFECount RPN 34.98 107.46 0.38 5.22 33.89 139.92 0.39 5.34\nPatch-Sel (VAE) 28.34 94.22 0.41 4.65 23.60 110.95 0.40 4.26 Patch-Sel (SD) 26.85 91.09 0.33 4.30 21.44 115.30 0.30 5.74\nTABLE 3 Using our selected patches as exemplars for other exemplar-based class-agnostic counting methods (FamNet+, BMNet, BMNet+ and SAFECount) on FSC-147 dataset. Our patch selection method generalizes well to other exemplar-based counting methods."
        },
        {
            "heading": "5.3 Multi-class Object Counting",
            "text": "Our method can count instances of a specific class given the class name, which is particularly useful when there are multiple classes in the same image. In this section, we show some visualization results in this multi-class scenario. As shown in Figure 8, our method selects patches according to the given class name and counts instances from that specific class in the input image. Correspondingly, the heatmap highlights the image regions that are most relevant to the specified class. Here the heatmaps are obtained by correlating the exemplar feature vector with the image feature map in a pre-trained ImageNet feature space. Note that we mask out the image region where the activation value in the heatmap is below a threshold when counting the objects of interests. We also show the patches selected using another exemplar-free counting method, RepRPN [8]. The class of RepRPN selected patches can not be explicitly specified. It simply selects patches from the class with the highest number of instances in the image according to the repetition score."
        },
        {
            "heading": "5.4 Qualitative Comparison between SD-generated Prototypes and VAE-generated Prototypes",
            "text": "In this section, we provide qualitative comparison between patches selected via SD-generated prototypes and VAEgenerated prototypes. As shown in Figure 9, we present a few input images and the corresponding patches selected by VAE-generated prototypes and SD-generated prototypes. Although the patches selected by VAE-generated prototypes generally contain the objects of interest, they miss parts of the objects in some cases (e.g., the second patch of grape), or contain multiple object instances within one patch (e.g., the second patch of strawberry). In comparison, the patches selected by SD-generated prototypes are generally better exemplars for counting, i.e., one patch mostly contains a single complete object instance."
        },
        {
            "heading": "5.5 Analysis on SD-generated Prototypes",
            "text": "Qualitative Visualization. To generate visual class prototypes, we first use a pre-trained Stable Diffusion to generate a set of object patches for the class of interest. Then given a query image, we select the generated patches that\nmost resemble the target objects in the query image. We compute the average feature embeddings of the selected generated patches to construct the prototype. In Figure 10, we demonstrate this process for three different categories, i.e., grapes, eggs, and apples. For each category, we show how we select different patches to construct the prototypes for the given query image. As can be seen in the first\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10\n(a) Patches selected by VAE Prototypes\n(b) Patches selected by SD Prototypes\nFig. 9. Visualization of the patches selected by VAE-generated prototypes and SD-generated prototypes. Patches selected by SD-generated prototypes are of higher quality.\ncolumn, the set of RPN proposals extracted from SDgenerated images exhibit rich variations. Among these proposals, our method selects those that are most relevant to the testing images for constructing prototypes. For example, in the last row of Figure 10 (c), only apples with mixed colors are selected to match the objects\u2019 colors in the testing image. Compared with VAE-generated prototypes which remain the same for all query images, SD-generated prototypes can better handle the variations of query images and select more accurate counting exemplars.\nNumber of Patches for Prototype Generation. In our main experiments, we select the top-5 SD-generated patches with the nearest mean distance over query patches, and compute their average features to construct prototypes. In this section, we conduct an ablation study on how the number of patches selected for prototype generation affects the counting performance. Specifically, we select top-5, top25, top-50 and all patches to construct class prototypes and use them to select exemplars. Results are summarized in Table 4. We observe that the performance drops on both the validation set and test set as the number of selected patches increases. The counting errors are highest when using all generated patches to construct prototypes. In this case, the same class prototype is applied for all images of this class, which is not optimal for counting objects with large intraclass diversity. Our method, in comparison, selects the most similar patches based on the query image, which leads to more accurate prototypes."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose a new task, zero-shot object counting, to count instances of a specific class given only the class name without access to any exemplars. To address this, we developed a two-step approach that accurately localizes the optimal patches to be used as counting exemplars. We leverage language-vision models to construct class prototypes via two approaches: VAE-based apporoach and diffusion-based approach. Through these two approaches, we present a comprehensive study of prototype construction at both category and image levels. In the context of our specific task, the diffusion-based image prototype has notably outperformed the category-level prototype constructed via VAE, thanks to its ability to customize prototypes to match object appearances in each image. More generally, our approach of employing language-vision models for zeroshot recognition is applicable in various tasks. In scenarios where data of large-scale generative models do not exist such as medical imaging, or remote sensing, VAE can be a useful choice.\nWe show that the prototypes can be used to select the patches containing objects of interests. Furthermore, we\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11\nintroduce an error prediction model to select those patches with the smallest predicted errors as the final exemplars for counting. Extensive results demonstrate the effectiveness of our method. We also conduct experiments to show that our selected patches can be incorporated into other exemplarbased counting methods to achieve exemplar-free counting."
        },
        {
            "heading": "1 DIFFERENT METHODS TO ACQUIRE PROPOSALS FOR SELECTION",
            "text": "In our main experiments, the proposals for selection are generated by a pre-trained RPN [?]. In this section, we conduct an ablation study on how to obtain the proposal patches for selection. Instead of using proposals generated via a pre-trained RPN, we use the proposals from selective search [?] and a pre-trained Background Aware RPN (BA-RPN) proposed in [?]. Results are summarized in Table 1. As can be seen from the table, our proposed patch selection method can bring consistent performance improvements for all the three set of proposal patches. with SD-generated prototypes, we observe the lowest MAE on the validation set (i.e., 26.05) using BA-RPN proposals and the lowest MAE on the test set (i.e., 21.53) using RPN proposals.\nProposals for selection\nPatch Selection Val Set Test SetMAE RMSE NAE SRE MAE RMSE NAE SRE\nSelective Search [?]\n\u2717 33.79 102.87 0.57 6.44 29.10 133.49 0.45 4.95 \u2713(VAE) 28.45 93.55 0.39 4.64 23.04 117.63 0.35 3.89 \u2713(SD) 27.70 94.77 0.36 4.46 22.68 117.96 0.32 3.92\nBA-RPN [?] \u2717 32.02 97.44 0.37 4.79 26.83 130.53 0.32 4.16\n\u2713(VAE) 26.48 87.67 0.34 4.22 22.22 115.04 0.32 3.70 \u2713(SD) 26.05 88.39 0.33 4.21 21.77 116.06 0.31 3.62\nRPN \u2717 32.19 99.21 0.38 4.80 29.25 130.65 0.35 4.35\n\u2713(VAE) 27.47 90.85 0.37 4.52 23.14 114.40 0.34 3.95 \u2713(SD) 26.30 88.80 0.34 4.27 21.53 113.28 0.31 3.61\nTABLE 1 Performance on FSC-147 dataset when using different sets of proposal patches for selection. Our proposed method brings consistent improvement in the performance."
        },
        {
            "heading": "2 QUALITATIVE ANALYSIS ON SD-GENERATED PROTOTYPES",
            "text": "In this section, we provide additional qualitative analysis on prototype generation using Stable Diffusion. As shown in Figure 1, the patches generated by Stable Diffusion exhibit rich variations in terms of colors, shapes and textures. According to the query image, our method selects the patches that most resemble the query objects for constructing class prototypes.\nar X\niv :2\n30 9.\n13 09\n7v 1\n[ cs\n.C V\n] 2\n2 Se\np 20\n23\n2 Prototype Patches SD Patches Test ImageSelected Exemplars"
        },
        {
            "heading": "3 COMPARISON WITH USING CLIP FEATURES FOR PATCH SELECTION",
            "text": "To select class-relevant patches, we first construct a class prototype based on the corresponding textual description. Then we select the class-relevant patches among the object proposals using a nearest-neighbor lookup strategy. Another straightforward way is to directly apply this nearest-neighbor selection on the CLIP [?] features of the text and the object proposals. Here we compare this baseline method with ours. Specifically, we use the language encoder of a pre-trained CLIP (ViT-B/32) to extract text embeddings from the labels of the query classes, and its image encoder to extract visual embeddings for the query object proposals. We select the patches whose CLIP visual embeddings are the nearest neighbors of the corresponding CLIP\u2019s text embedding as class-relevant patches. Results are summerized in Table 2. Using our generated prototypes for patch selection outperforms using CLIP features by a large margin."
        },
        {
            "heading": "4 MULTI-CLASS ZERO-SHOT COUNTING",
            "text": "Figure 2 provides additional visualizations of the selected patches in multi-class cases. As can be seen from the figure, our proposed method can select counting exemplars according to the given class name and count instances from that specific class in the input image. The reference-less methods i.e., RepRPN [?] and CounTR [?], can not be applied in this multi-class scenario.\n3 \u201cPotato\u201d\nPotato: 21 Tomato: 25\n\u201cTomato\u201d\nPear: 14 Apple: 16\n\u201cBlueberry\u201d\n\u201cStrawberry\u201d\n\u201cLemon\u201d\n\u201cTomato\u201d\nBlueberry: 32 Strawberry: 20\nLemon: 48 Tomato: 38\n(1) (2)\n(3) (4)\n23\n25\n33\n26\n\u201cPear\u201d\n\u201cApple\u201d\n13\n14\n66\n42\n\u201cBlueberry\u201d\n\u201cStrawberry\u201d\n\u201cCelery\u201d\n\u201cCarrot\u201d\n5\n7\nBlueberry : 130 Strawberry : 15\n92\n11\nCelery: 5 Carrot: 10\n(5) (6)\nFig. 2. Visualizations of the selected patches: There are two classes with multiple object instances in a single image. To specify what to count, we provide the class name at test time. Our proposed method selects counting exemplars according to the given class name and count instances from the specific class."
        },
        {
            "heading": "5 ABLATION STUDY ON THE NUMBER OF NEAREST NEIGHBORS",
            "text": "In the first step of our proposed patch selection method, we select the patches whose embeddings are the N -nearest neighbors of the class prototypes as class-relevant patches. In this section, we provide an ablation study on the choice of N . We experiment with 5, 10, 15 and 20. The results are summarized in Table 3. We observe that as the number of class-relevant patches (i.e., N ) increases, the performance drops slightly. Setting N too high might result in class-irrelevant patches being selected, which can decrease the performance. We observe the lowest MAE on the validation set when N = 10 and the lowest MAE on the test set when N = 5."
        },
        {
            "heading": "6 FAILURE CASES FOR SD-BASED PATCH SELECTION",
            "text": "In this section, we analyze the failure cases when employing the prototypes generated by Stable Diffusion (SD) for patch selection. A failure mode arises from the ambiguity in class labels. For example, the term \u201ckiwi\u201d could either refer to kiwi bird or kiwifruit. As shown in the last row of Figure 3, Stable Diffusion generates images of birds while the objects to count in the query images are kiwifruits. Another case where SD-generated prototypes would fail is when the class name is too generic to describe the objects accurately. For example, when provided with the class name \u201cfresh cut\u201d, Stable Diffusion generates images of different types of fruits, while the query images for FSC-147 [?] are cut fruits in plastic containers on\n4 shelves. Another case is with a category labeled as \u201cnail polish\u201d. In this case, Stable Diffusion generates images of painted fingernails, while the query images are nail polish bottles on shelves. Although Stable Diffusion is capable of generating diverse and realistic images, it might generate images that do not resemble target images when provided with inaccurate / ambiguous prompts. One way to resolve this issue is prompt engineering, i.e., designing and refining the prompts for more accurate image generation, which can be a future direction of this work.\nSD-generated Images Query Images\nNail Polish\nFinger Foods\nCarrom Board Pieces\nFresh Cut\nKiwis\nFig. 3. Visualizations of the SD-generated images and query images in some failure cases. Most cases are due to the ambiguity in the class labels which leads to mismatch between objects in the generated images and those present in the query images."
        },
        {
            "heading": "7 QUALITATIVE ANALYSIS OF ERROR PREDICTOR",
            "text": "In Figure 4, we show a few input images and the corresponding patches with the top-3 lowest and highest predicted counting errors. As can be seen from the figure, the patches with the smallest predicted errors are suitable to serve as counting exemplars and output meaningful density maps and accurate counting results. In comparison, the density maps produced by patches with the highest predicted errors fail to highlight the relevant image regions and lead to inaccurate counting results. This suggests that the predicted counting error can effectively indicate the goodness of the counting exemplars.\n5 20\n21\n9\n8\n5\n14\n12\n12\n19\n8\n7\n39\n(a)\n(b)\n(c)\n(d)\nFig. 4. Visualizations of the patches with top-3 lowest and highest predicted counting errors. Our error predictor can pick out object crops results in accurate object counting."
        }
    ],
    "title": "Zero-Shot Object Counting with Language-Vision Models",
    "year": 2023
}