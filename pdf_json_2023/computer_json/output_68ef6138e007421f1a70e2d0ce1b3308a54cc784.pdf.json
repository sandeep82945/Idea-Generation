{
    "abstractText": "Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there\u2019s a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we\u2019ve compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research. Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5. We study single-task training, multi-task training, and a \"chain-of-thought\" knowledge distillation finetuning technique to assess the model\u2019s performance across the different logical reasoning categories. By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Man Luo"
        },
        {
            "affiliations": [],
            "name": "Shrinidhi Kumbhar"
        },
        {
            "affiliations": [],
            "name": "Ming shen"
        },
        {
            "affiliations": [],
            "name": "Mihir Parmar"
        },
        {
            "affiliations": [],
            "name": "Neeraj Varshney"
        },
        {
            "affiliations": [],
            "name": "Pratyay Banerjee"
        },
        {
            "affiliations": [],
            "name": "Somak Aditya"
        },
        {
            "affiliations": [],
            "name": "Chitta Baral"
        }
    ],
    "id": "SP:8478ac693e2ca2a18296abfadac1b1650148dc0e",
    "references": [
        {
            "authors": [
                "Stephen H. Bach",
                "Matthias Broecheler",
                "Bert Huang",
                "Lise Getoor."
            ],
            "title": "Hinge-loss markov random fields and probabilistic soft logic",
            "venue": "J. Mach. Learn. Res., 18(1):3846\u20133912.",
            "year": 2017
        },
        {
            "authors": [
                "Pratyay Banerjee",
                "Chitta Baral",
                "Man Luo",
                "Arindam Mitra",
                "Kuntal Pal",
                "Tran C Son",
                "Neeraj Varshney"
            ],
            "title": "Can transformers reason about effects of actions? arXiv:2012.09938",
            "year": 2020
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Wen-tau Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Samuel Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Ruth MJ Byrne",
                "Jonathan St BT Evans",
                "Stephen E Newstead."
            ],
            "title": "Human reasoning: The psychology of deduction",
            "venue": "Psychology Press.",
            "year": 2019
        },
        {
            "authors": [
                "Chunkit Chan",
                "Xin Liu",
                "Tsz Ho Chan",
                "Jiayang Cheng",
                "Yangqiu Song",
                "Ginny Wong",
                "Simon See."
            ],
            "title": "Self-consistent narrative prompts on abductive natural language inference",
            "venue": "arXiv preprint arXiv:2309.08303.",
            "year": 2023
        },
        {
            "authors": [
                "Peter Clark",
                "Oyvind Tafjord",
                "Kyle Richardson."
            ],
            "title": "Transformers as soft reasoners over language",
            "venue": "IJCAI.",
            "year": 2020
        },
        {
            "authors": [
                "Alain Colmerauer",
                "Philippe Roussel."
            ],
            "title": "The birth of prolog",
            "venue": "History of programming languages\u2014II.",
            "year": 1996
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Ishita Dasgupta",
                "Andrew K Lampinen",
                "Stephanie CY Chan",
                "Antonia Creswell",
                "Dharshan Kumaran",
                "James L McClelland",
                "Felix Hill."
            ],
            "title": "Language models show human-like content effects on reasoning",
            "venue": "arXiv preprint arXiv:2207.07051.",
            "year": 2022
        },
        {
            "authors": [
                "Luc De Raedt",
                "Angelika Kimmig",
                "Hannu Toivonen."
            ],
            "title": "Problog: A probabilistic prolog and its application in link discovery",
            "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, page 2468\u20132473, San Francisco,",
            "year": 2007
        },
        {
            "authors": [
                "J. Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL, Minneapolis, Minnesota. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Richard Evans",
                "David Saxton",
                "David Amos"
            ],
            "title": "Can neural networks understand logical entailment? In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Xuemeng Song Liqiang Nie Fangkai Jiao",
                "Yangyang Guo."
            ],
            "title": "Merit: Meta-path guided contrastive learning for logical reasoning",
            "venue": "arXiv preprint arXiv:2203.00357.",
            "year": 2022
        },
        {
            "authors": [
                "Tejas Gokhale",
                "Swaroop Mishra",
                "Man Luo",
                "Bhavdeep Sachdeva",
                "Chitta Baral."
            ],
            "title": "Generalized but not robust? comparing the effects of data modification methods on out-of-domain generalization and adversarial robustness",
            "venue": "Findings of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Lin Guan",
                "Karthik Valmeekam",
                "Sarath Sreedharan",
                "Subbarao Kambhampati."
            ],
            "title": "Leveraging pretrained large language models to construct and utilize world models for model-based task planning",
            "venue": "arXiv preprint arXiv:2305.14909.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Hahn",
                "Frederik Schmitt",
                "Jens U Kreber",
                "Markus Norman Rabe",
                "Bernd Finkbeiner."
            ],
            "title": "Teaching temporal logics to neural networks",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Simeng Han",
                "Hailey Schoelkopf",
                "Yilun Zhao",
                "Zhenting Qi",
                "Martin Riddell",
                "Luke Benson",
                "Lucy Sun",
                "Ekaterina Zubova",
                "Yujie Qiao",
                "Matthew Burtell"
            ],
            "title": "2022. Folio: Natural language reasoning with firstorder logic. arXiv preprint arXiv:2209.00840",
            "year": 2022
        },
        {
            "authors": [
                "Simon Jerome Han",
                "Keith J Ransom",
                "Andrew Perfors",
                "Charles Kemp."
            ],
            "title": "Inductive reasoning in humans and large language models",
            "venue": "Cognitive Systems Research, page 101155.",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Fan Zhang"
            ],
            "title": "The impact of symbolic representations on in-context learning for few-shot reasoning. arXiv preprint arXiv:2212.08686",
            "year": 2022
        },
        {
            "authors": [
                "Weinan He",
                "Canming Huang",
                "Yongmei Liu",
                "Xiaodan Zhu."
            ],
            "title": "WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Evan Heit."
            ],
            "title": "What Is Induction and Why Study It?, page 1\u201324",
            "venue": "Cambridge University Press.",
            "year": 2007
        },
        {
            "authors": [
                "Chadi Helwe",
                "Chlo\u00e9 Clavel",
                "Fabian Suchanek."
            ],
            "title": "Logitorch: A pytorch-based library for logical reasoning on natural language",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.",
            "year": 2022
        },
        {
            "authors": [
                "Jerry R Hobbs",
                "Mark E Stickel",
                "Douglas E Appelt",
                "Paul Martin."
            ],
            "title": "Interpretation as abduction",
            "venue": "Artificial intelligence, 63(1-2):69\u2013142.",
            "year": 1993
        },
        {
            "authors": [
                "Lifu Huang",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
            "venue": "EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Ignatiev",
                "Nina Narodytska",
                "Joao MarquesSilva."
            ],
            "title": "Abduction-based explanations for machine learning models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1511\u20131519.",
            "year": 2019
        },
        {
            "authors": [
                "Fangkai Jiao",
                "Zhiyang Teng",
                "Shafiq Joty",
                "Bosheng Ding",
                "Aixin Sun",
                "Zhengyuan Liu",
                "Nancy F Chen."
            ],
            "title": "Logicllm: Exploring self-supervised logic-enhanced training for large language models",
            "venue": "arXiv preprint arXiv:2305.13718.",
            "year": 2023
        },
        {
            "authors": [
                "Pratik Joshi",
                "Somak Aditya",
                "Aalok Sathe",
                "Monojit Choudhury."
            ],
            "title": "TaxiNLI: Taking a ride up the NLU hill",
            "venue": "CoNLL.",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng Kaiyu Yang",
                "Danqi Chen."
            ],
            "title": "Generating natural language proofs with verifier-guided search",
            "venue": "arXiv preprint arXiv:2205.12443.",
            "year": 2022
        },
        {
            "authors": [
                "Seyed Mehran Kazemi",
                "Najoung Kim",
                "Deepti Bhatia",
                "Xin Xu",
                "Deepak Ramachandran."
            ],
            "title": "Lambada: Backward chaining for automated reasoning in natural language",
            "venue": "arXiv preprint arXiv:2212.13894.",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez."
            ],
            "title": "Measuring faithfulness in chainof-thought reasoning",
            "venue": "CoRR, abs/2307.13702.",
            "year": 2023
        },
        {
            "authors": [
                "Joohyung Lee",
                "Man Luo."
            ],
            "title": "Strong equivalence for lpmln programs",
            "venue": "35th International Conference on Logic Programming (ICLP 2019).",
            "year": 2018
        },
        {
            "authors": [
                "Joohyung Lee",
                "Yi Wang."
            ],
            "title": "Weighted rules under the stable model semantics",
            "venue": "KRR.",
            "year": 2016
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "KRR.",
            "year": 2012
        },
        {
            "authors": [
                "Vladimir Lifschitz."
            ],
            "title": "Answer set programming",
            "venue": "Springer Berlin.",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Lin",
                "Oyvind Tafjord",
                "Peter Clark",
                "Matt Gardner."
            ],
            "title": "Reasoning over paragraph effects in situations",
            "venue": "MRQA.",
            "year": 2019
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Wanli: Worker and ai collaboration for natural language inference dataset creation",
            "venue": "arXiv preprint arXiv:2201.05955.",
            "year": 2022
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Ruoxi Ning",
                "Zhiyang Teng",
                "Jian Liu",
                "Qiji Zhou",
                "Yue Zhang."
            ],
            "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "venue": "arXiv preprint arXiv:2304.03439.",
            "year": 2023
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Zhiyang Teng",
                "Leyang Cui",
                "Chaoli Zhang",
                "Qiji Zhou",
                "Yue Zhang."
            ],
            "title": "Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4",
            "venue": "arXiv preprint arXiv:2305.12147.",
            "year": 2023
        },
        {
            "authors": [
                "Jian Liu",
                "Leyang Cui",
                "Hanmeng Liu",
                "Dandan Huang",
                "Yile Wang",
                "Yue Zhang."
            ],
            "title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
            "venue": "IJCAI.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Liu",
                "Leyang Cui",
                "Hanmeng Liu",
                "Dandan Huang",
                "Yile Wang",
                "Yue Zhang."
            ],
            "title": "Logiqa: a challenge dataset for machine reading comprehension with logical reasoning",
            "venue": "Proceedings of the Twenty-Ninth International Conference on Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Lourie",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark",
            "venue": "arXiv preprint arXiv:2103.13009.",
            "year": 2021
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "KaiWei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao."
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "arXiv preprint arXiv:2304.09842.",
            "year": 2023
        },
        {
            "authors": [
                "Man Luo",
                "Kazuma Hashimoto",
                "Semih Yavuz",
                "Zhiwei Liu",
                "Chitta Baral",
                "Yingbo Zhou."
            ],
            "title": "Choose your qa model wisely: A systematic study of generative and extractive readers for question answering",
            "venue": "Spa-NLP 2022, page 7.",
            "year": 2022
        },
        {
            "authors": [
                "Man Luo",
                "Shailaja Keyur Sampat",
                "Riley Tallman",
                "Yankai Zeng",
                "Manuha Vancha",
                "Akarshan Sajja",
                "Chitta Baral"
            ],
            "title": "just because you are right, doesn\u2019t mean i am wrong\u2019: Overcoming a bottleneck in development and evaluation of open-ended vqa",
            "year": 2021
        },
        {
            "authors": [
                "Man Luo",
                "Sharad Saxena",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral."
            ],
            "title": "Biotabqa: Instruction learning for biomedical table question answering",
            "venue": "CEUR Workshop Proceedings, volume 3180, pages 291\u2013304. CEUR-WS.",
            "year": 2022
        },
        {
            "authors": [
                "Man Luo",
                "Xin Xu",
                "Zhuyun Dai",
                "Panupong Pasupat",
                "Mehran Kazemi",
                "Chitta Baral",
                "Vaiva Imbrasaite",
                "Vincent Y Zhao."
            ],
            "title": "Dr",
            "venue": "icl: Demonstration-retrieved in-context learning. arXiv preprint arXiv:2305.14128.",
            "year": 2023
        },
        {
            "authors": [
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "The natural language decathlon: Multitask learning as question answering",
            "venue": "arXiv preprint arXiv:1806.08730.",
            "year": 2018
        },
        {
            "authors": [
                "John McCarthy."
            ],
            "title": "Artificial intelligence, logic and formalizing common sense",
            "venue": "Philosophical logic and artificial intelligence. Springer.",
            "year": 1989
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Se-Young Yun Namgyu Ho",
                "Laura Schmid."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "arXiv preprint arXiv:2212.10071.",
            "year": 2022
        },
        {
            "authors": [
                "Timothy Niven",
                "Hung-Yu Kao."
            ],
            "title": "Probing neural network comprehension of natural language arguments",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Liangming Pan",
                "Alon Albalak",
                "Xinyi Wang",
                "William Yang Wang."
            ],
            "title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "venue": "arXiv preprint arXiv:2305.12295.",
            "year": 2023
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro."
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "venue": "arXiv preprint arXiv:2303.09014.",
            "year": 2023
        },
        {
            "authors": [
                "Mihir Parmar",
                "Swaroop Mishra",
                "Mirali Purohit",
                "Man Luo",
                "Murad Mohammad",
                "Chitta Baral."
            ],
            "title": "In-boxbart: Get instructions into biomedical multitask learning",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 112\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Paul."
            ],
            "title": "Approaches to abductive reasoning: an overview",
            "venue": "Artificial intelligence review, 7(2):109\u2013152.",
            "year": 1993
        },
        {
            "authors": [
                "Adam Poliak",
                "Aparajita Haldar",
                "Rachel Rudinger",
                "J. Edward Hu",
                "Ellie Pavlick",
                "Aaron Steven White",
                "Benjamin Van Durme."
            ],
            "title": "Collecting diverse natural language inference problems for sentence representation evaluation",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Kyle Richardson",
                "Ashish Sabharwal."
            ],
            "title": "Pushing the limits of rule reasoning in transformers through natural language satisfiability",
            "venue": "arXiv:2112.09054.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Richardson",
                "Pedro Domingos."
            ],
            "title": "Markov logic networks",
            "venue": "Mach. Learn., 62(1\u20132):107\u2013136.",
            "year": 2006
        },
        {
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Matthew Downey",
                "Anna Rumshisky."
            ],
            "title": "Getting closer to ai complete question answering: A set of prerequisite real tasks",
            "venue": "AAAI, volume 34.",
            "year": 2020
        },
        {
            "authors": [
                "Mohammed Saeed",
                "Naser Ahmadi",
                "Preslav Nakov",
                "Paolo Papotti."
            ],
            "title": "RuleBERT: Teaching soft rules to pre-trained lms",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Sayan Ghosh",
                "Shashank Srivastava",
                "Mohit Bansal."
            ],
            "title": "Prover: Proof generation for interpretable reasoning over rules",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Soumya Sanyal",
                "Yichong Xu",
                "Shuohang Wang",
                "Ziyi Yang",
                "Reid Pryzant",
                "Wenhao Yu",
                "Chenguang Zhu",
                "Xiang Ren."
            ],
            "title": "Apollo: A simple approach for adaptive pretraining of language models for logical reasoning",
            "venue": "arXiv preprint arXiv:2212.09282.",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Vered Shwartz",
                "Antoine Bosselut",
                "Yejin Choi",
                "Dan Roth."
            ],
            "title": "Introductory tutorial: Commonsense reasoning for natural language processing",
            "venue": "ACL 2020, page 27.",
            "year": 2020
        },
        {
            "authors": [
                "Abulhair Saparov",
                "He He."
            ],
            "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Abulhair Saparov",
                "He He."
            ],
            "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Abulhair Saparov",
                "Richard Yuanzhe Pang",
                "Vishakh Padmakumar",
                "Nitish Joshi",
                "Seyed Mehran Kazemi",
                "Najoung Kim",
                "He He."
            ],
            "title": "Testing the general deductive reasoning capacity of large language models using ood examples",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Bruno Sauce",
                "Louis D Matzel."
            ],
            "title": "Inductive reasoning",
            "venue": "Encyclopedia of animal cognition and behavior, 6:1\u20138.",
            "year": 2017
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Selsam",
                "Matthew Lamm",
                "Benedikt B\u00fcnz",
                "Percy Liang",
                "Leonardo de Moura",
                "David L Dill."
            ],
            "title": "Learning a sat solver from single-bit supervision",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Koustuv Sinha",
                "Shagun Sodhani",
                "Jin Dong"
            ],
            "title": "Clutrr: A diagnostic benchmark for inductive reasoning from text",
            "year": 2019
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabili",
            "year": 2023
        },
        {
            "authors": [
                "Theodore Sumers",
                "Shunyu Yao",
                "Karthik Narasimhan",
                "Thomas L Griffiths."
            ],
            "title": "Cognitive architectures for language agents",
            "venue": "arXiv preprint arXiv:2309.02427.",
            "year": 2023
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi",
                "Peter Clark."
            ],
            "title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "venue": "Findings-ACL-IJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Matt Gardner",
                "Kevin Lin",
                "Peter Clark."
            ],
            "title": "Quartz: An open-domain dataset of qualitative relation questions",
            "venue": "arXiv:1909.03553.",
            "year": 2019
        },
        {
            "authors": [
                "Jidong Tian",
                "Yitian Li",
                "Wenqing Chen",
                "Liqiang Xiao",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Diagnosing the firstorder logical reasoning ability through LogicNLI",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Jidong Tian",
                "Yitian Li",
                "Wenqing Chen",
                "Liqiang Xiao",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Diagnosing the first-order logical reasoning ability through logicnli",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Alberto Olmo",
                "Sarath Sreedharan",
                "Subbarao Kambhampati."
            ],
            "title": "Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change)",
            "venue": "NeurIPS 2022 Foundation Models for Decision Making Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Sarath Sreedharan",
                "Matthew Marquez",
                "Alberto Olmo",
                "Subbarao Kambhampati."
            ],
            "title": "On the planning abilities of large language models (a critical investigation with a proposed benchmark)",
            "venue": "arXiv preprint arXiv:2302.06706.",
            "year": 2023
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Mihir Parmar",
                "Nisarg Patel",
                "Divij Handa",
                "Sayantan Sarkar",
                "Man Luo",
                "Chitta Baral"
            ],
            "title": "Can nlp models correctly reason over contexts that break the common assumptions? arXiv preprint arXiv:2305.12096",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Sumit Chopra",
                "Alexander M Rush",
                "Bart Van Merri\u00ebnboer",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "venue": "arXiv preprint arXiv:1502.05698.",
            "year": 2015
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes"
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv:1502.05698",
            "year": 2015
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Zhaofeng Wu",
                "Linlu Qiu",
                "Alexis Ross",
                "Ekin Aky\u00fcrek",
                "Boyuan Chen",
                "Bailin Wang",
                "Najoung Kim",
                "Jacob Andreas",
                "Yoon Kim."
            ],
            "title": "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks",
            "venue": "CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Carbonell",
            "year": 2019
        },
        {
            "authors": [
                "Zonglin Yang",
                "Xinya Du",
                "Rui Mao",
                "Jinjie Ni",
                "Erik Cambria."
            ],
            "title": "Logical reasoning over natural language as knowledge representation: A survey",
            "venue": "arXiv preprint arXiv:2303.12023.",
            "year": 2023
        },
        {
            "authors": [
                "Fei Yu",
                "Hongbo Zhang",
                "Benyou Wang."
            ],
            "title": "Nature language reasoning, a survey",
            "venue": "arXiv preprint arXiv:2303.14725.",
            "year": 2023
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Honghua Zhang",
                "Liunian Harold Li",
                "Tao Meng",
                "KaiWei Chang",
                "Guy Van den Broeck."
            ],
            "title": "On the paradox of learning to reason from data",
            "venue": "arXiv preprint arXiv:2205.11502.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we\u2019ve compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research. Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5. We study single-task training, multi-task training, and a \"chain-of-thought\" knowledge distillation finetuning technique to assess the model\u2019s performance across the different logical reasoning categories. By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field1."
        },
        {
            "heading": "1 Introduction",
            "text": "With logical reasoning, humans can explain an answer to a question via step-wise deduction, make\n0*Equal contribution 1The dataset and models are available in Hugging-\nface:logicreasoning/logi_glue and logicreasoning/LogiT5\nrobust planning and decision, or even reason about the workings in an unseen universe. Black holes serve as a compelling example of the power of logical reasoning. Even before the first observational evidence of black holes, scientists like Stephen Hawking used the principles of relativity and quantum mechanics to predict their existence and properties. Through pure thought and mathematical calculations alone, they logically deduced the presence of these mysterious cosmic entities, which were later confirmed through real observation by MIT researchers in 2015, fifty years later after the theorem had been derived. This underscores the profound capability of human reasoning to unveil truths about the universe that lie beyond our immediate perception2.\nIn the field of Artificial Intelligence (AI), there has been significant attention directed towards the aspiration to develop machines equipped with logical reasoning capabilities (McCarthy, 1989; Colmerauer and Roussel, 1996). Early approaches in logical reasoning were primarily dedicated to the design of formal logic languages to encapsulate rules and knowledge, along with the development of automated theorem provers (Lifschitz, 2019). This paradigm, however, necessitated a deep understanding of the syntax and semantics of the formal logic for manual rule formulation \u2013 making knowledge representation and knowledge acquisition hard and expert-driven endeavor. Due to these challenges, contemporary research has progressively turned towards addressing logical reasoning tasks (Clark et al., 2020; Tian et al., 2021a; Han et al., 2022) by employing transformerbased (Vaswani et al., 2017) pre-trained language models (Devlin et al., 2019a; Brown et al., 2020).\nThe Language models (LMs) that are pretrained using objectives such as the mask language modeling (Devlin et al., 2019a) and next word predic-\n2https://news.mit.edu/2021/hawkings-black-holetheorem-confirm-0701\nar X\niv :2\n31 0.\n00 83\n6v 2\n[ cs\n.C L\n] 3\nN ov\n2 02\n3\ntion (Brown et al., 2020) enables them to acquire adequate syntax and semantics of language, alongside commonsense knowledge. These language models can excel in numerous natural language understanding tasks, owing to the unsupervised pretraining on a vast array of unstructured text data. However, it is unclear if the current pretraining objectives are sufficient enough for the models to infer logical reasoning because this involves understanding structure; coupled with inductive, deductive, and abductive reasoning skills. This question has drawn intense attention and inspired different research directions to examine if LMs can learn logical reasoning ability (Wu et al., 2023; Lanham et al., 2023; Clark et al., 2020; Joshi et al., 2020). For instance, Clark et al. (2020) shows that pre-trained language models can serve as a \u201csoft-reasoner\" based on their near-perfect performance on synthetic datasets. Creswell et al. (2022) showed that large LMs are few-shot logical reasoners. On the other hand, Liu et al. (2020); Joshi et al. (2020); Han et al. (2022) shows that logical reasoning remains challenging for language models. Furthermore, Wu et al. (2023); Lanham et al. (2023) showed that LLMs maybe retrieving or reciting previously seen facts and steps, instead of actually reasoning. Liu et al. (2023b) shows that while ChatGPT and GPT-4 generally perform well on some benchmarks, their performance noticeably dimin-\nishes when faced with new or out-of-distribution datasets.\nTo better understand the progress of logical reasoning ability in the current language model era, we first provide a concise survey of its role within current language models. Based on the insights gathered through the survey, we assembled a logical reasoning benchmark termed as LogiGLUE. Subsequently, we trained a model on this benchmark by utilizing diverse training strategies; our contributions are summarized below.\nConcise Survey. We provide a brief survey of the recent development of logical reasoning using natural language (see Figure 1). First we discuss three types of logical reasoning. Then we focus on the relevant benchmarks and the methodologies for applying LMs to logical reasoning tasks.\nLogiGLUE. One result of this survey is a benchmark for logical reasoning (LogiGLUE), with the aim to facilitate a consistent progress of logical reasoning in NLP. The importance for the LogiGLUE benchmark arises from several critical considerations. First, it encompasses diverse logical reasoning tasks and generalization evaluation, ensuring a comprehensive assessment of how a model performs across varied logical paradigms. Second, the unique format of each dataset within LogiGLUE simplifies both training and evaluation processes,\nfacilitating swift integration into research workflows. Lastly, researchers can easily compare with established baselines, and the LogiGLUE offers the flexibility to seamlessly integrate new datasets in the future, ensuring its lasting relevance in logical reasoning evaluation.\nLogiT5. Drawing inspiration from recent successes in multi-task learning and instruction-finetuned models, we trained seq2seq models, specifically Flan-T5 (Chowdhery et al., 2022), using multi-task learning on LogiGLUE\u2019s in-domain data. The resulting model, named LogiT5, demonstrated effective generalization on out-of-domain data."
        },
        {
            "heading": "2 A Concise Survey of Logical Reasoning in NLP: Types of Reasoning, Datasets and Language Models Approach",
            "text": "The advent of large language models has been transformative for the AI community; prompting many to speculate that we are on the cusp of achieving general artificial intelligence (GAI). Yet, as astounding as their capabilities are, these models grapple with numerous challenges, particularly with logical reasoning (Han et al., 2022; Valmeekam et al., 2023, 2022; Guan et al., 2023). Recognizing the significance of this, our survey aims to provide a timely comprehensive overview of advancements in logical reasoning within the context of language models, elucidating their performance, limitations, and the obstacles that remain, which casts a vision for future research directions. While other surveys have touched upon the broader theme of logical reasoning using natural language (Helwe et al., 2022; Yu et al., 2023; Yang et al., 2023), our survey has led us to propose a comprehensive bnechmark collection, and include a systematic review of techniques to adopt LLMs for logical reasoning tasks. More importantly, we categorize different ways of using LMs on logical reasoning tasks, highlighting the intricacies and challenges faced by models for such tasks."
        },
        {
            "heading": "2.1 Three Types of Logical Reasoning",
            "text": "Deductive Reasoning. In this predominant form of reasoning, we start with a set of premises which can be facts or rules, and derive a specific conclusion based on the available premises with a valid logical derivation path. In short, deductive reasoning derives specific conclusion(s) from generic observation(s) (Byrne et al., 2019). There are two\ncharacteristics related to a deductive reasoning system, validity and soundness. A conclusion is valid if and only if it is fully supported by the premises irrespective of the factuality of the premises. A conclusion is sound if and only if it is valid and the premises are true. For example in Figure 2, the conclusion is valid but it is not sound because it is not true that \u201cAll kids love animals.\u201d Most of the synthetic deductive reasoning datasets such as RuleTaker (Clark et al., 2020) has valid conclusions, but may not be sound as the rules in the premises are often synthetically generated and may be untrue in the real world. Datasets such as PrOntoQA (Saparov and He, 2023a) offer a broader view, by sourcing the premise rules from a true, a false and a fictional ontology.\nInductive Reasoning. For inductive reasoning, one starts with a set of observations, and derives a general conclusion that is merely true, but not certain (Heit, 2007; Sauce and Matzel, 2017). In contrast to deductive reasoning, inductive reasoning is a bottom-up reasoning process which starts from specific observations and derives a generic conclusion. Many Knowledge graph completion task requires inductive reasoning such as WN18RR3. To apply inductive reasoning, one usually relies on a large number of observations (both positive and negative in support or against an induced rule). Since large language models are pretrained on large amount of free-text, it learns several generic patterns or conclusions, therefore reasoning inductively (even if the rules may not be represented symbolically or a human-readable fashion) (Han et al., 2023). In general, commonsense reasoning tasks in NLP require both inductive and deductive reasoning.\nAbductive Reasoning. Abductive reasoning typically begins with an incomplete set of observations and proceeds to derive most likely explanations for the observations to be true (Paul, 1993; Hobbs et al., 1993). Similar to inductive reasoning, this also involves uncertainty, as there can be different explanations. Compared to inductive reasoning, the deductive reasoning is a process from known facts or rules to derive a new conclusion, while abductive reasoning is from an observation to \u201cguess\u201d what can be the reason to cause the observation. It is\n3Here, we exclude this task since we are more interested in natural language input. In this paper, we do not discuss about knowledge graph completion tasks since most of them are not in natural language forms.\nused more often in our daily decision-making, such as medical diagnoses based on a set of incomplete symptoms.\nIn previous paragraphs, we mentioned how both inductive and abductive reasoning inherently encompass uncertainty. In fact, deductive reasoning can also operate within the realm of uncertainty (De Raedt et al., 2007; Richardson and Domingos, 2006; Lee and Wang, 2016; Bach et al., 2017; Lee and Luo, 2018). Such reasoning paradigm uses \u201csoft rules\u201d to indicate the likelihood of a rule being true rather than its absolute truth. Consequently, conclusions derived may carry probabilistic true/false values. Reasoning under uncertainty is particularly useful because the real world is inherently unpredictable and full of unknown variables. While many datasets operate under the assumption that rules are unequivocally true, Rulebert (Saeed et al., 2021) deviates by attributing weight values to each rule."
        },
        {
            "heading": "2.2 Logical Reasoning Tasks and Datasets",
            "text": "We discuss the task and datasets in terms of format of the tasks and how they are created."
        },
        {
            "heading": "2.2.1 Four Types of Tasks",
            "text": "Multiple Choice Question Answering (MCQA). In the MCQA task, the given inputs are a paragraph which forms a context, a question, and a list of answer candidates (typically four choices). The goal is to predict which candidate is (most likely)\ncorrect. All datasets are pure-text (Yu et al., 2019; Liu et al., 2020)4.\nFree Form Question Answering. Unlike MCQA, where a set of answer choices are given, freeform QA only has a context and a question, and the answer to the question can be any format, including but not limited to a single word, a list of words, and a number (Weston et al., 2015b; Banerjee et al., 2020).\nFact Checking. In fact verification, given a context and a fact, the goal is to classify the binary truth value of the fact according to the given information (Clark et al., 2020; Saeed et al., 2021; He et al., 2021).\nNatural language inference (NLI) NLI is the task of detecting inferential relationships between a premise and a hypothesis. For most NLI datasets, there are three relationships, entailment (the hypothesis follows or can be inferred by the premise), neutral (the truth of hypothesis is undetermined by the premise), and contradiction (the hypothesis contradicts the premise or some facts in the premise) (Tian et al., 2021a).\n4ReClor and LogiQA sources the datasets from real examination questions that may involve images or charts. But they remove such questions and retain only those which are self-contained and answerable from the provided text."
        },
        {
            "heading": "2.2.2 Dataset Creation Techniques",
            "text": "Human Annotation. Crowdsourcing is one of the major approaches to create datasets, such as for NLI tasks. The advantages of this methodology include a richer linguistic grammar and potentially increased task complexity. However, it comes with drawbacks. In addition to being a costintensive process, crowdsourced datasets tend to harbor biases (as highlighted in numerous previous studies (Yu et al., 2019)). These biases can be leveraged by neural models to artificially inflate accuracy scores. Furthermore, assembling a dataset for logical reasoning tasks demands a level of expertise that poses a significant challenge.\nExtraction from Academic Challenge. It is hard for crowdsourcing workers to produce questions requiring complicated logical reasoning since such reasoning tasks require extensive training and practice. Fortunately, questions in some standardized tests are aligned with the goal of logical reasoning and can be utilized to create such datasets after some preprocessing (Yu et al., 2019; Liu et al., 2020). However, the domains of these examinations are limited and the dataset size is small.\nSynthetic Generation. Synthetic generation is more efficient to create large data than manually created ones (Luo et al., 2022b). There are two ways, simulation based (Weston et al., 2015b) and rule-based (Clark et al., 2020; Saeed et al., 2021; Banerjee et al., 2020). In rule based methods, logic programs (either written by humans or mined from knowledge graphs) are generated, and then implications are drawn by automatic theorem prover. Last, the rules and facts in the logic programs are converted into English form using natural language patterns. Synthetic generation has issues that the rules or facts do not have real-world meaning and the language could be simple."
        },
        {
            "heading": "2.3 Language Models for Logical Reasoning over Natural Language",
            "text": "Language models (LMs) have been actively studied these days for logical reasoning tasks. Dasgupta et al. (2022) demonstrates that large language mdoels (LLMs) show human-level abstract reasoning skill. Creswell et al. (2022) proposes a selection-inference pipeline that given a context and question, the model can firstly select which facts or rules given in the context are important to answer the question and decompose the ques-\ntion into step by step reasoning. Wei et al. (2022) demonstrates that language models have the capacity to engage in chain-of-thoughts (CoT) reasoning. This approach facilitates a step-by-step reasoning process that enhances the performance of the model in downstream tasks such as mathematical reasoning. In following section, we summarize the five prevalent trends in utilizing language models for logical reasoning over language."
        },
        {
            "heading": "2.3.1 Supervised Finetuning",
            "text": "Fine-tuning a language model on the downstream tasks has been a standard way to teach a model to perform a task. Such a paradigm has also been the prevalent method for logical reasoning tasks (Clark et al., 2020; Liu et al., 2021; Tian et al., 2021b; Saeed et al., 2021; Han et al., 2022; Chan et al., 2023). In general, such a method is usually applied to a moderate size of the language model such as BERT (Devlin et al., 2019b), GPT2 (Radford et al.), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). It has been shown that transformer based models perform better than the other types of neural models such as LSTM (Yu et al., 2019; Liu et al., 2020), probably because such pretrained models have a certain degree of commonsense and logical reasoning (Huang et al., 2019). This has been further proven in (Clark et al., 2020). They show that when every word in the passage is replaced by a random word resulting in no grammaticality, the performance of a transformerbased model dramatically decreases. In addition, the larger model performs better than smaller ones, indicating that the deeper a model is, the more complicated reasoning it can execute (He et al., 2021). While the IID performance of a fine-tuned model can be nearly perfect, such model has poor generalization. For example, model can not generalize from lower depth to higher depth reasoning (Clark et al., 2020), from low level language diversity to high level diversity (Richardson and Sabharwal, 2021; Tafjord et al., 2021), from one domain to another domain (Banerjee et al., 2020). Such observations indicate that models might just learn the inductive pattern in the training data rather than the underline logical reasoning skill (Zhang et al., 2022)."
        },
        {
            "heading": "2.3.2 Logical Reasoning Pretraining",
            "text": "The next word prediction or mask language modeling pretraining tasks allow the language models to learn the language syntax and semantic as well\nas the world knowledge, however, it does not guarantee a model to learn logical operations. Thus, researchers have been exploring logical-oriented pretraining tasks to teach a model of logical reasoning from large free data. APOLLO (Sanyal et al., 2022)improves the logical reasoning of a model by two pre-training tasks. The first pretraining task is selective mask language modeling (MLM). Unlike the naive MLM which randomly masks the words, s-MLM selects and masks the logical words (defined by Spacy POS tags). The second pretraining task is entailment classification which aims to classify if there is an entailment relationship within a masked sentence or not. MERIt (Fangkai Jiao, 2022) proposes a meta-path-guided pretraining task to teach a model to learn logical reasoning by selfsupervised learning. They construct the training data by converting any document into a graph with entities as the node and the relation between the entities as edges. Then, given a pair of entities, the positive candidates are the sentences that connect this pair of entities, and the negative candidates are obtained by data augmentation. Such training data allows the model trained by contrastive learning manner to identify the positive sentence from the negative sentences. MERIt+ (Jiao et al., 2023) combines MERIt with the autoregression training objective: rather than using contrastive learning, MERIt+ optimizes the probability of positive candidate sentences."
        },
        {
            "heading": "2.3.3 Proof Generation",
            "text": "Proof generation is found to be harder than answer generation (Saha et al., 2020; Tafjord et al., 2021). However, models developed for a proof generation task have better performance on out-of-domain datasets or unseen depth reasoning (e.g., train on lower depth and test on higher depth). Kaiyu Yang and Chen (2022) introduce NLProofS, a novel method for generating step-by-step logically valid and relevant proofs given a set of supporting facts and hypothesis. In their proposed method, they employ a prover which generates candidate proofs step-by-step, a verifier to measure the validity of generated proof steps to avoid the prover from hallucinating proof steps, and an algorithm for retrieving the entire proof with highest validity score by aggregating proof step scores. ProofWriter (Tafjord et al., 2021) proposed two ways to generate proofs based on T5 models. The first one is to predict the sequence of proof in one output; the second one is to iteratively generate a proof and specifically,\npredict one intermediate conclusion and combine it with the given facts and rules as a new input to predict the following conclusion, and repeat this process until no new conclusion is predicted.\nTypically, proof strategies fall into one of two categories: backward chaining, also known as bottom-up reasoning, and forward chaining, or topdown reasoning. In forward chaining, the process begins with established facts and rules, cyclically deriving new inferences and integrating them into the known facts until the target is either confirmed or rejected. Conversely, backward chaining initiates with the target in question, employing rules recursively to break it down into sub-goals. These sub-goals are then verified against the known rules and facts. Kazemi et al. (2022) found that backward chaining is more beneficial for LLM to solve deductive logical reasoning tasks."
        },
        {
            "heading": "2.3.4 CoT Knowledge Distillation",
            "text": "The previous approach relies on the proof annotations in the datasets, however, in many cases, the dataset does not come with the proof. It is shown that large language model (LLM) can generate stepby-step reasoning (similar as the proof) (Saparov and He, 2023a; Liu et al., 2023c). Namgyu Ho (2022) propose Fine-tune-CoT (i.e. chain-ofthought (Wei et al., 2022)) approach which involves three key steps. In the first step, a large teacher model is prompted to address intricate queries, generating multi-step reasoning explanations. These explanations are then filtered based on the accuracy of the final prediction. In the second step, a reasoning sample is constructed, incorporating the question, rationale, and answer, thereby forming a comprehensive prompt and multi-step solution. This collection of carefully curated reasoning samples is leveraged to fine-tune a compact student model, imbuing it with the ability to engage in reasoning tasks. Nonetheless, LLMs encounter difficulties in planning proofs, occasionally making wrong selections when presented with multiple valid choices. This challenge leads to proofs that are not fully developed and consequently produces inaccurate responses."
        },
        {
            "heading": "2.3.5 Neural Symbolic",
            "text": "Recent advancements in pre-trained language models have demonstrated impressive reasoning abilities using explanations or \"chain-of-thought\" for in-context learning. Conversely, reasoning tasks are considered more straightforward for symbolic\nprogramming. A promising way is to use LLM to translate a natural language input into a symbolic program which can be consumed by a symbolic solver. Such a paradigm has been shown to effectively avoid unfaithfulness of LLM (Pan et al., 2023). Hanlin Zhang1 (2022) employs LLMS as Logic Programmers (LMLP) to learn logic rules and examples and reason over knowledge bases (KBs) using Prolog\u2019s backward chaining algorithm. They show that LMLP outperforms CoT in deductive reasoning settings, achieving over 25% higher accuracy on length generalization benchmarks, even with fewer parameters. Pan et al. (2023) propose Logic-LM to handle deductive reasoning, first-order logic, and constraint programming tasks. They leverage GPT-3 and in-context learning (providing a few examples) to translate a natural language input to a formal language formulation that can be executed by symbolic engines. They also show that the error messages of the symbolic engines can refine the output of an LLM. Such a paradigm has been investigated for addressing other challenges, wherein LLMs act as planners, and external tools are utilized to execute the plan (Lu et al., 2023; Sumers et al., 2023; Paranjape et al., 2023; Guan et al., 2023; Schick et al., 2023)."
        },
        {
            "heading": "2.4 Survey Summary",
            "text": "The survey delineates how current datasets address three types of logical reasoning distributed across four task formats. Additionally, the curation process of a dataset can influence its inherent difficulty level. We\u2019ve also identified five approaches for utilizing LLMs in addressing these reasoning tasks. This structured insight serves as a foundation for future research, offering a roadmap to optimize model performance and curation methodologies. In the following section, we will present a logical reasoning benchmark, positioned alongside established benchmarks like SuperGlue (Wang et al., 2019), BigBench (Srivastava et al., 2023), and Unicorn (Lourie et al., 2021), all aimed at exhaustively gauging system capabilities."
        },
        {
            "heading": "3 LogiGLUE: General Logical Reasoning Benchmark",
            "text": "As mentioned in the introduction, the reasoning ability of language models as assessed by various studies seems to differ. One plausible explanation for this variance is the inconsistency in the bench-\nmarks used or differences in task formats, leading to performance disparities. To rectify this, our goal is to offer a standardized testbed. It becomes imperative to meticulously formulate our selection criteria to create a testbed that evaluates a system\u2019s logical reasoning capabilities. Guiding our dataset choice are two primary principles outlined in \u00a73.1. These endeavors have led to the formation of a diverse and comprehensive logical reasoning benchmark, which we\u2019ve named LogiGLUE (\u00a73.2)."
        },
        {
            "heading": "3.1 Principle of Collecting LogiGLUE",
            "text": "Numerous logical reasoning datasets have been accessible since 2015, such as bAbi (Weston et al., 2015b). Our selection process for including a dataset in LogiGLUE is primarily driven by principles of diversity and generalization (Gokhale et al., 2022).\nDiversity. There are two aspects to Diversity. First aspect concerns the types of reasoning in the dataset. We ensure that our coverage encompasses three main reasoning types, which collectively represent the full spectrum of logical reasoning. These three categories have been previously discussed. The second aspects concerns the level of difficulty, with datasets ranging from easy to hard. Our experimental results indicate a varied model performance across different datasets \u2013 excelling in some, delivering average results on others, and struggling significantly on a few. We discovered a strong correlation between the complexity of a dataset and the methodology employed in its creation. Datasets built using simple templates and basic rule structures tend to be easier. In contrast, those with more sophisticated rules and uncertain elements are relatively more challenging. However, the most difficult datasets are those meticulously crafted by human hands.\nGeneralization. We also consider the axis of generalization, which aims to quantify (or assess) whether a model trained on the logical reasoning tasks can genuinely acquire reasoning skills. Previous studies have found that the superior performance of a fine-tuned language model primarily stems from learning the patterns exhibited within the dataset, which unfortunately often leads to poor generalization to other datasets. Consequently, the model\u2019s performance tends to be overestimated due to the identical and independently distributed (IID) nature of the testing data. To counteract this, LogiGLUE includes an out-of-domain testing set\nthat also encompasses the three types of reasoning. The out-of-domain testing set is readily adaptable to incorporate future logical reasoning tasks.\nExcluded Datasets. Lexicographically speaking, reasoning is defined as \u201cthe process of forming conclusions, judgments, or inferences from facts or premises.\"5 Reasoning is usually associated with an entailment relation, where given a premises, the truth value of a hypothesis depends on if the latter is entailed by the premise or not. There are many datasets that require reasoning which we decided to exclude from the scope of this work. This includes some well-known NLI datasets, such as, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). These datasets use many linguistic forms, unstated background knowledge, and sometimes unsupported inference steps (Clark et al., 2020). We also exclude datasets where reasoning with external domain knowledge is required since for such tasks, retrieving the external knowledge is essential and it is hard to diag-\n5https://www.dictionary.com/browse/reasoning\nnose whether the noisy retrieved knowledge affects systems or systems lack of logical reasoning capacity. This includes QuAIL (Rogers et al., 2020), WSC (Levesque et al., 2012), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019). Commonsense reasoning datasets are not covered in this survey either since they focus on solving a task using commonsense knowledge (Sap et al., 2020) and thus it is more important to acquire the commonsense knowledge rather than to do logical reasoning. Other datasets that we exclude are ones that require logical reasoning but are not presented in the natural language form such as logical entailment (Evans et al., 2018), NeuroSAT (Selsam et al., 2019), and LTL (Hahn et al., 2020)."
        },
        {
            "heading": "3.2 Statistic of LogiGLUE",
            "text": "A suite of natural language logical reasoning benchmarks with 10 in-domain and 12 out-domain datasets that cover different types of logical reasoning. In addition, LogiGLUE includes three task formats, multiple choice question answer (MCQA), natural language inference (NLI), and fact verifica-\ntion (FV)/ fact checking (FC). Table 1 shows the statistics.\nUnique Format. There are many existing practicing for standardizing different datasets into a consistent format (Mishra et al., 2022; Lourie et al., 2021), such as transforming all tasks to question answering (McCann et al., 2018) or NLI (Poliak et al., 2018) styles. Through our model analysis, it\u2019s evident that certain models can only manage specific reasoning tasks. For instance, classification models are commonly used for NLI and MCQA tasks, where the number of classification heads matches the number of choices (like 3 for NLI and 4 for MCQA). Yet, these models struggle when confronted with free-form question answering, thus limiting their versatility. Hence, to develop a model adept at logical reasoning regardless of task structure, we convert them into a singular format. An added advantage of this standardized format is that it ensures consistency in the input, ruling out performance disparities arising from different inputs. Every dataset is then adapted to this specific format. In the case of MCQA/FV/NLI tasks, each instance encompasses a context, a question, and potential answer options. Conversely, FF tasks don\u2019t present any answer choices. The correct answer is the expected output for every instance. For FV tasks, we use the statement as the question with true/false as potential answers. In NLI tasks, the options include natural, contradictory, and entailment."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "We selected Flan-T5-large (Chowdhery et al., 2022) as our base model for training due to two pivotal reasons. Firstly, Flan-T5 stands as an instructionfine-tuned iteration of T5, exhibiting enhanced performance when compared to its peers. Secondly, Flan-T5\u2019s manageable size renders it to be trainable on a machine that is conducive to an academic setting. In the following, we present results that encompass both quantitative and qualitative aspects."
        },
        {
            "heading": "4.1 In-Domain Performance",
            "text": ""
        },
        {
            "heading": "4.1.1 Single Task Fine-tuning",
            "text": "We fine-tune the Flan-T5-large on each individual dataset and the result is presented in Table 2. We draw some interesting observations. It is apparent that the model exhibits superior performance when operating on synthetic data compared to handcrafted alternatives. In support of this, the datasets\nthat garnered the top 5 performances are predominantly synthetic. This trend holds even when considering the ANLI dataset, which, despite having a more substantial training set than its synthetic counterparts, yielded inferior results. Moreover, we ventured to explore if the model displayed a predilection for one form of reasoning over another. Preliminary insights suggest a potential preference towards abductive reasoning in comparison to deductive reasoning, as evidenced in the disparity in performance between the \u03b1NLI and ANLI datasets \u2013 both of which are similar in terms of training size and are hand-crafted. This, however, is a mild observation and warrants further exploration to derive a conclusive statement. For instance, our statistical analysis revealed that the average context length for ANLI is 105, whereas for \u03b1NLI, it is 66, potentially leading to varying degrees of difficulty."
        },
        {
            "heading": "4.1.2 Multitask Fine-tuning",
            "text": "We fine-tune the Flan-T5 on all in-domain datasets utilizing a weighted sampling technique to accommodate for the unbalanced size of the training datasets. We find that this sampling is better than random sampling and the comparison is given in the Appendix. We termed this model as LogiT5.\nOne benefit of multi-task training compared to the single task training is that the low resources data can benefit from other tasks (Parmar et al., 2022; Luo et al., 2022a). From Table 2, it is apparent that the multi-task training model holds a significant advantage when dealing with tasks with small training set. It showcases higher proficiency compared to its single-task counterpart, notably performing better by 5% and 8% on the \u03b1ARCT and FOLIO tasks, respectively. These datasets, characterized by their smaller training size (limited to 1/2 K training samples), benefited notably from the multi-task training approach. Contrastingly, the tasks with large trainig set did not reap any benefits from multi-task training, such as \u03b1NLI and ANLI datasets. A potential explanation for this could be the substantial training set already facilitates optimum learning for the model, rendering the multitask training approach redundant. This observation underlines a critical limitation in leveraging multitask training when the individual training datasets are already sufficiently large."
        },
        {
            "heading": "4.1.3 Fine-tuned LogiT5 on Single Dataset",
            "text": "Here, we further fine-tune LogiT5 on each dataset. However, upon analyzing the performance dis-\nplayed in Table 2, we did not observe any notable advantages from this additional fine-tuning even though small margin gains are achieved. This suggests that LogiT5 has likely already learned the majority of knowledge from these tasks."
        },
        {
            "heading": "4.2 Out-of-Domain Generation",
            "text": "When we study the out-of-domain generalization, we compare three models, Flan-T5, LogiT5, and LLama-2 (7B) (Touvron et al., 2023). In addition, for Llama-2, we also study the chain-of-thought prompting (Wei et al., 2022). Here, we evaluate the model\u2019s zero-shot capabilities rather than its fewshot in-context learning performance(Luo et al., 2023). Investigating the latter will be reserved for future research. More specifically, we add a prompt \"let\u2019s think step by step\" after the question. However, by our results, we do not see the advantage of CoT prompting, probably because the model already generates the reasoning even without such a prompt. Evaluating the LLama-2 answer poses a challenge since the output is usually a free form and not use the exact answer option. On the other hand Flan-T5 generate answer in a more structure way that is easier for evaluation, probably because FlanT5 is already trained on instruction fine-tuned data which are already in a structured templates. The preliminary results of LLama-2 were poor. Upon manually reviewing the predictions, we observed that LLama-2 occasionally produces synonyms of the ground truth. To address this, we employed ConceptNET (Speer et al., 2017) to identify synonyms and verify if the prediction aligns with any of them, a strategy akin to the one explored in (Luo et al., 2021). Furthermore, on the babi dataset, we have seen that sometimes the llama model ignores the input text and generates answer based on its pre-\ntrained knowledge. This is similar to the findings revealed in Varshney et al. (2023)."
        },
        {
            "heading": "4.3 CoT Distillation",
            "text": "As shown by previous work (Namgyu Ho, 2022), distill the chain-of-thoughts from a large model to a small student model can boost the performance of the student model. We apply such a CoT finetuning strategy and conduct experiments on LogiQA, identified as the most challenging task, by distilling the CoT from LLama-7B to Flan-T5. Initially, we generated a single answer for each question, retaining only the samples where the predicted answer was correct, resulting in approximately 3K valid samples. Alternatively, we created 10 answers for each question and preserved the samples with at least one correct predicted answer, which generated a unique set of 6K questions. It is worth noting that some questions offered multiple correct reasoning paths. In such cases, we either opted for a singular path or utilized all available paths, the latter approach amassing a total of 15K training samples. With the CoT fine-tuning, we observe that the finetuning takes longer time and a larger learning rate in the beginning is helpful. Thus, instead of using 1e-4 as the learning rate, we use 3e-4. we train the model with 40 epochs. We do see that the model performance increase when the number of epoch increase.\nFollowing this, we trained the Flan-T5 model utilizing datasets consisting of 3K, 6K, and 15K samples, derived from the generated CoT,], with the results delineated in Table 4. Our findings indicate that the training with 3K and 6K samples did not enhance the CoT\u2019s fine-tuning efficacy. However, an increased dataset size of 15K samples facilitated a 4% improvement in performance, suggesting that\nCoT distillation becomes more beneficial with a larger volume of data."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this study, we concentrate our efforts on a crucial area of research: logical reasoning over natural language. Initially, we offer a survey to provide a thorough comprehension of this domain, emphasizing the role of large language models in addressing this demanding task. Following this, we assemble a benchmark for logical reasoning named LogiGLUE, set to be publicly available to aid forthcoming research. Finally, we refine a language model utilizing LogiGLUE, demonstrating encouraging results across both in-domain and outof-domain datasets."
        }
    ],
    "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
    "year": 2023
}