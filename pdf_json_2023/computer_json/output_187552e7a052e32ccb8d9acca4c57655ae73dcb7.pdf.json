{
    "abstractText": "The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing referencebased metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: https: //github.com/aimagelab/pacscore.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sara Sarto"
        },
        {
            "affiliations": [],
            "name": "Manuele Barraco"
        },
        {
            "affiliations": [],
            "name": "Marcella Cornia"
        },
        {
            "affiliations": [],
            "name": "Lorenzo Baraldi"
        },
        {
            "affiliations": [],
            "name": "Rita Cucchiara"
        }
    ],
    "id": "SP:aaaa1e8fd5efacca8728a8196e1044abbb35341b",
    "references": [
        {
            "authors": [
                "Somak Aditya",
                "Yezhou Yang",
                "Chitta Baral",
                "Cornelia Fermuller",
                "Yiannis Aloimonos"
            ],
            "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge",
            "venue": "arXiv preprint arXiv:1511.03292,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould"
            ],
            "title": "SPICE: Semantic Propositional Image Caption Evaluation",
            "venue": "In ECCV, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie"
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "In ACL Workshops,",
            "year": 2005
        },
        {
            "authors": [
                "Manuele Barraco",
                "Marcella Cornia",
                "Silvia Cascianelli",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "The Unreasonable Effectiveness of CLIP Features for Image Captioning: An Experimental Analysi",
            "venue": "In CVPR Workshops,",
            "year": 2022
        },
        {
            "authors": [
                "Manuele Barraco",
                "Matteo Stefanini",
                "Marcella Cornia",
                "Silvia Cascianelli",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "CaMEL: Mean Teacher Learning for Image Captioning",
            "venue": "In ICPR,",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "UNITER: UNiversal Image-TExt Representation Learning",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Marcella Cornia",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Marcella Cornia",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability",
            "venue": "In ICRA,",
            "year": 2020
        },
        {
            "authors": [
                "Marcella Cornia",
                "Matteo Stefanini",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "Meshed-Memory Transformer for Image Captioning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yin Cui",
                "Guandao Yang",
                "Andreas Veit",
                "Xun Huang",
                "Serge Belongie"
            ],
            "title": "Learning to Evaluate Image Captioning",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In NAACL,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Make-A-Scene: Scene- Based Text-to-Image Generation with Human Priors",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Simao Herdade",
                "Armin Kappeler",
                "Kofi Boakye",
                "Joao Soares"
            ],
            "title": "Image captioning: Transforming objects into words",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
            "venue": "In EMNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Micah Hodosh",
                "Peter Young",
                "Julia Hockenmaier"
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation",
            "venue": "metrics. JAIR,",
            "year": 2013
        },
        {
            "authors": [
                "Lun Huang",
                "Wenmin Wang",
                "Jie Chen",
                "Xiao-Yong Wei"
            ],
            "title": "Attention on Attention for Image Captioning",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Ming Jiang",
                "Junjie Hu",
                "Qiuyuan Huang",
                "Lei Zhang",
                "Jana Diesner",
                "Jianfeng Gao"
            ],
            "title": "REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning",
            "venue": "In EMNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Ming Jiang",
                "Qiuyuan Huang",
                "Lei Zhang",
                "Xin Wang",
                "Pengchuan Zhang",
                "Zhe Gan",
                "Jana Diesner",
                "Jianfeng Gao"
            ],
            "title": "TIGEr: Text-to-Image Grounding for Image Caption Evaluation",
            "venue": "In EMNLP, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Apoorv Khandelwal",
                "Luca Weihs",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi"
            ],
            "title": "Simple but Effective: CLIP Embeddings for Embodied AI",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jin-Hwa Kim",
                "Yunji Kim",
                "Jiyoung Lee",
                "Kang Min Yoo",
                "Sang-Woo Lee"
            ],
            "title": "Mutual Information Divergence: A Unified Metric for Multimodal Generative Models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hwanhee Lee",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Trung Bui",
                "Kyomin Jung"
            ],
            "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning",
            "venue": "In ACL,",
            "year": 2021
        },
        {
            "authors": [
                "Hwanhee Lee",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Kyomin Jung"
            ],
            "title": "ViL- BERTScore: Evaluating Image Caption Using Vision-and- Language BERT",
            "venue": "In EMNLP Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Kuang-Huei Lee",
                "Xi Chen",
                "Gang Hua",
                "Houdong Hu",
                "Xiaodong He"
            ],
            "title": "Stacked Cross Attention for Image-Text Matching",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
            "venue": "In ICML, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In ACL Workshops,",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "In ICLR, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee"
            ],
            "title": "ViL- BERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Joanna Materzy\u0144ska",
                "Antonio Torralba",
                "David Bau"
            ],
            "title": "Disentangling Visual and Written Concepts in CLIP",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Yingwei Pan",
                "Ting Yao",
                "Yehao Li",
                "Tao Mei"
            ],
            "title": "X-Linear Attention Networks for Image Captioning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "BLEU: a method for automatic evaluation of machine translation",
            "venue": "In ACL,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Cyrus Rashtchian",
                "Peter Young",
                "Micah Hodosh",
                "Julia Hockenmaier"
            ],
            "title": "Collecting Image Annotations Using Amazon\u2019s Mechanical Turk",
            "venue": "In NAACL Workshops,",
            "year": 2010
        },
        {
            "authors": [
                "Stephen Robertson"
            ],
            "title": "Understanding inverse document frequency: on theoretical arguments for IDF",
            "venue": "Journal of Documentation,",
            "year": 2004
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "title": "Object Hallucination in Image Captioning",
            "venue": "In EMNLP,",
            "year": 2018
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes",
                "Tim Salimans",
                "Jonathan Ho",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic Text-to- Image Diffusion Models with Deep Language Understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Sara Sarto",
                "Marcella Cornia",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "Retrieval-Augmented Transformer for Image Captioning",
            "venue": "In CBMI,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman",
                "Patrick Schramowski",
                "Srivatsa Kundurthy",
                "Katherine Crowson",
                "Ludwig Schmidt",
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ravi Shekhar",
                "Sandro Pezzelle",
                "Yauhen Klimovich",
                "Aur\u00e9lie Herbelot",
                "Moin Nabi",
                "Enver Sangineto",
                "Raffaella Bernardi"
            ],
            "title": "FOIL it! Find One mismatch between Image and Language caption",
            "venue": "In ACL,",
            "year": 2017
        },
        {
            "authors": [
                "Rakshith Shetty",
                "Marcus Rohrbach",
                "Lisa Anne Hendricks",
                "Mario Fritz",
                "Bernt Schiele"
            ],
            "title": "Speaking the same language: Matching machine to human captions by adversarial training",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Yaya Shi",
                "Xu Yang",
                "Haiyang Xu",
                "Chunfeng Yuan",
                "Bing Li",
                "Weiming Hu",
                "Zheng-Jun Zha"
            ],
            "title": "EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Van Miltenburg",
                "Desmond Elliott",
                "Piek Vossen"
            ],
            "title": "Measuring the diversity of automatic image descriptions",
            "venue": "In COLING,",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "CIDEr: Consensus-based Image Description Evaluation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Qingzhong Wang",
                "Antoni B Chan"
            ],
            "title": "Describing like humans: on diversity in image captioning",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Qingzhong Wang",
                "Jia Wan",
                "Antoni B Chan"
            ],
            "title": "On Diversity in Image Captioning: Metrics and Methods",
            "venue": "IEEE Trans. PAMI,",
            "year": 2020
        },
        {
            "authors": [
                "Sijin Wang",
                "Ziwei Yao",
                "Ruiping Wang",
                "Zhongqin Wu",
                "Xilin Chen"
            ],
            "title": "FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Wang",
                "Jiawei Wu",
                "Junkun Chen",
                "Lei Li",
                "Yuan-Fang Wang",
                "William Yang Wang"
            ],
            "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Zeyu Wang",
                "Berthy Feng",
                "Karthik Narasimhan",
                "Olga Russakovsky"
            ],
            "title": "Towards unique and informative captioning of images",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Zirui Wang",
                "Jiahui Yu",
                "Adams Wei Yu",
                "Zihang Dai",
                "Yulia Tsvetkov",
                "Yuan Cao"
            ],
            "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong",
                "Ludwig Schmidt"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Xu",
                "Jimmy Ba",
                "Ryan Kiros",
                "Kyunghyun Cho",
                "Aaron Courville",
                "Ruslan Salakhutdinov",
                "Richard S Zemel",
                "Yoshua Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Xu Yang",
                "Kaihua Tang",
                "Hanwang Zhang",
                "Jianfei Cai"
            ],
            "title": "Auto-Encoding Scene Graphs for Image Captioning",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Yao",
                "Yingwei Pan",
                "Yehao Li",
                "Zhaofan Qiu",
                "Tao Mei"
            ],
            "title": "Boosting image captioning with attributes",
            "venue": "In ICCV, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Yanzhi Yi",
                "Hangyu Deng",
                "Jinglu Hu"
            ],
            "title": "Improving Image Captioning Evaluation by Considering Inter References Variance",
            "venue": "In ACL,",
            "year": 2020
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier"
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "TACL,",
            "year": 2014
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao"
            ],
            "title": "VinVL: Revisiting visual representations in vision-language models",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi"
            ],
            "title": "BERTScore: Evaluating Text Generation with BERT",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Luowei Zhou",
                "Yannis Kalantidis",
                "Xinlei Chen",
                "Jason J Corso",
                "Marcus Rohrbach"
            ],
            "title": "Grounded video description",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Wanrong Zhu",
                "Xin Eric Wang",
                "An Yan",
                "Miguel Eckstein",
                "William Yang Wang"
            ],
            "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation",
            "venue": "In EACL,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The task of image captioning, which requires an algorithm to describe visual contents with natural language sentences, has been gaining considerable attention from the research community in the past few years [22, 53, 61]. As such, the task has witnessed methodological and architectural innovations, ranging from the usage of self-attentive models [10, 16, 19, 36] to the development of better connections between visual and textual modalities with the addition of objects [3, 62, 66] and tags [29, 63] or the use of more powerful cross-modal features [5, 6, 45].\nTogether with an increase in generation quality, the automatic evaluation of captions has also witnessed a significant effort. While early evaluation scores were based on translation metrics [4, 30, 37], more effective text-based [2, 52, 67] and multimodal solutions [21,56] have been proposed in the\nlast few years. Among these, the usage of cross-modal models in which both visual and textual data can be matched has proven to be a viable strategy that can lead to highquality metrics [17, 24\u201326]. Recently, the large-scale CLIP model [38] was tested for image captioning evaluation, resulting in the CLIP-Score [17] which proved to have a significant correlation with human judgment.\nWhile these advancements demonstrate the appropriateness of using contrastive-based embedding spaces for evaluating image captions, large-scale models pre-trained on web-collected data also have limitations, due to the lack in style of captions collected from alt-tags and of the distribution of web-scale images which is not aligned with those on which captioning systems are evaluated. While cleaned data sources, on the contrary, are limited in size, recent advances in both image [14,39,43,44] and text generation [28,59,66] have made it possible to synthetically generate data in both modalities, with controlled style and quality.\nFollowing this insight, in this paper we propose a learnable metric that fuses together the advantages of both these scenarios, by leveraging the quality of the pre-training on\nar X\niv :2\n30 3.\n12 11\n2v 3\n[ cs\n.C V\n] 2\n0 Ju\nl 2 02\nweb-collected data and that of cleaned data, and also regularizing the training by considering additional positive samples hailing from visual and textual generators. Specifically, our proposed metric, PAC-S, is trained via a newly conceived positive-augmented contrastive learning approach, in which pairs of generated images and texts act as additional positives in addition to real images and human-annotated captions taken from a cleaned data source. We demonstrate that the combination of these factors, i.e. the usage of a cleaned data source and the pairing with multimodal generated data, when used to finetune a large-scale contrastive model, results in an embedding space with significantly higher alignment with the human judgment (Fig. 1). We apply the resulting metric to evaluate both images and videos, both in reference-based and reference-free settings.\nWe investigate the quality of the proposed metric by conducting extensive experiments on a variety of image and video datasets, including Flickr8k-Expert and Flickr8kCF [18], Composite [1], Pascal-50S, and Abstract-50S [52] for the image scenario and the VATEX-EVAL dataset [49] to evaluate video-caption pairs. Further, we verify its sensitivity to object hallucination on the FOIL [47] and ActivityNet-FOIL [49] datasets and compare the performance of state-of-the-art caption generators with respect to the proposed metric. Our proposal outperforms previous reference-based and reference-free metrics and showcases superior performance with respect to CLIP-Score [17] and the corresponding video-based version (i.e. EMScore [49]), which also employ a contrastive-based embedding space. Overall, our metric ranks first in terms of correlation with human judgment with respect to all existing image and video captioning metrics.\nTo sum up, the main contribution of this paper is a novel metric for image and video captioning, based on a positive-augmented training of a multimodal embedding space, which exploits both curated image-caption pairs and additional synthetically generated positives. Extensive experiments on several datasets demonstrate a higher correlation with human judgment and an increased sensitivity to object hallucination."
        },
        {
            "heading": "2. Related Work",
            "text": "Image and video captioning solutions have been traditionally evaluated using a set of standard evaluation metrics, specifically BLEU [37], METEOR [4], ROUGE [30], CIDEr [52], and SPICE [2]. Some of them have been originally introduced to evaluate NLP tasks such as machine translation and summarization, while others have been specifically designed for the captioning task.\nRecently, research efforts have been made to introduce additional metrics that can capture different aspects of generated textual sentences, like diversity [48, 50, 54, 55], robustness of object hallucination [42], uniqueness [58], and\ncoverage of ground-truth named entities [8, 9]. A new trend, instead, is to exploit the capabilities of pre-trained models to compare textual-only [64, 67] or visual-textual contents [17, 20, 21, 25, 26, 56]. Among them, the BERT score [67] and its improved version [64] use pre-trained BERT embeddings [12] to represent and compare word tokens in the generated and ground-truth sentences.\nIn addition to these text-based metrics, other solutions leverage the multimodal nature of vision-and-language models to exploit not only textual information but also the visual content of images and potentially video frames. For example, Jiang et al. [21] introduced the TIGEr metric, which considers the similarities between words and image regions computed according to a cross-modal matching model [27] trained on COCO [31]. Other approaches, instead, exploit the effectiveness of web-scale vision-andlanguage models such as VilBERT [33], UNITER [7], and CLIP [38], pre-trained on millions or even billions of image-text pairs, to obtain more robust metrics [17,24\u201326]. Among them, the recent CLIP-Score [17] is based on a modified cosine similarity between image and candidate caption representations coming from the CLIP model. Recently, Kim et al. [24] proposed using CLIP visual-textual features to compute the negative Gaussian cross-mutual information, obtaining a more effective evaluation metric.\nWhile all the aforementioned evaluation metrics have originally been introduced for image captioning, there is only one attempt to evaluate video descriptions through learnable metrics also taking into account the visual content appearing in video frames. In particular, Shi et al. [49] presented the EMScore, in its both reference-free and reference-based versions, that computes fine-grained similarities between video frames and words of the candidate caption using CLIP visual-textual embeddings.\nAnother related work is that proposed in [69] where diffusion models are used to evaluate text-only tasks. Differently from our proposal, the introduced metric exploits similarities between machine-generated images obtained by a visual generator [43] starting from reference and candidate textual items during evaluation."
        },
        {
            "heading": "3. Positive-Augmented Contrastive Learning",
            "text": "We are interested in devising an image and video captioning metric based on a shared embedding space in which both visual data and text can be projected and compared. To this aim, we start from the dual-encoder architecture popularized by CLIP [38], which comprises an image encoder [13, 15] and a text encoder [51]. In this architecture, the multimodal interaction is performed in a late fusion fashion, by projecting the output of both encoders to a common dimensionality and then on the \u21132 hypersphere via normalization. The visual and the textual inputs can then be compared via cosine similarity.\nStarting from a trained embedding space, an evaluation metric for image captioning can be defined by simply scaling, and eventually thresholding, the similarity computed inside of the embedding itself. For instance, given a visual embedding v and a textual embedding t, Hessel et al. [17] define the evaluation score as\nScore(t, v) = w \u00b7max(cos(t, v), 0), (1)\nwhere cos indicates the cosine similarity computed inside of the embedding space and w is a scaling factor to enhance numerical readability.\nLarge-scale contrastive models like CLIP [38] are trained on web-collected image-caption pairs. These provide a large-scale source of supervision for learning scalable low-level and semantic visual and textual features, as testified by their zero-shot classification performance and by their adaptability to different tasks [5, 23, 34, 39]. Nevertheless, it shall be noted that the textual annotations contained in alt-tags are far from the quality level that a captioning evaluator should look for, and that the distribution of web-scale images might not be properly aligned with those on which image captioning systems are evaluated.\nTo solve this issue, one might think of learning the metric directly on cleaned data sources. However, recent attempts of learning contrastive-based evaluation metrics on cleaned datasets like COCO [31] perform poorly when compared to traditional metrics, potentially because of the lack of training data [21]. We, therefore, advocate the usage of synthetic generators of both visual and textual data, which showcase sufficiently high quality levels when generating both images and texts, do lack in terms of style, and are controllable in terms of visual distribution.\nSpecifically, given a positive image-text pair (v, t), we augment it by generating a synthetic caption t\u2032 from v using an image captioner [28], and a synthetic image v\u2032 from t via\na diffusion-based text-to-image model [43], thus building a dataset consisting of tuples of four elements (v, t, v\u2032, t\u2032). As in Eq. 1, we represent t\u2032 and v\u2032 via their respective text and image embedding. We then train our evaluation model by jointly taking into account contrastive relationships between real and generated matching image-caption pairs (Fig. 2). To lower the computational requirements, we start with pretrained CLIP visual and textual encoders and only train the projection toward the embedding space.\nFormally, given a batch of N real images V = [v1, v2, ..., vN ] and their corresponding captions T = [t1, t2, ..., tN ], generated images V \u2032 = [v\u20321, v\u20322, ..., v\u2032N ] and generated texts T \u2032 = [t\u20321, t\u20322, ..., t\u2032N ], we define multiple N \u00d7N matrices containing pairwise cosine similarities between the different inputs. We then adopt a symmetric InfoNCE loss [35] which aims at maximizing the cosine similarity between the N matching pairs and minimize those of the N2 \u2212N non-matching pairs. The loss which compares real images V with respect to real texts T can be defined, for instance, as\nLV,T = \u2212 1\nN N\u2211 i=1 log exp(cos(vi, ti)/\u03c4)\u2211N j=1 exp(cos(vi, tj)/\u03c4) +\n\u2212 1 N N\u2211 i=1 log exp(cos(vi, ti)/\u03c4)\u2211N j=1 exp(cos(vj , ti)/\u03c4) , (2)\nwhere \u03c4 is a temperature parameter. In addition to a loss term between real images and real texts, LV,T , we also add symmetrical loss terms between cross-modal generated and real pairs, i.e. between generated images and humanannotated texts, and between original images and generated texts. In this way, generated items act as additional positive samples for the real matching pairs, thus adding a supervisory signal without paying the cost of the noisy data on which contrastive-based features extractors like CLIP are\nlearned. In summary, the final loss is a weighted combination of the three loss terms, i.e.\nL = LV,T + \u03bbvLV\u2032,T + \u03bbtLV,T \u2032 , (3)\nwhere LV\u2032,T is the loss between generated images and real texts and LV,T \u2032 its counterpart between generated texts and real images."
        },
        {
            "heading": "3.1. Captioning evaluation score for images",
            "text": "After training with positive-augmented contrastive learning, we employ two evaluation scores for evaluating images in both a reference-free and a reference-based setting. Specifically, we employ Eq. 1 with w = 21 as our referencefree score. Then, we follow the approach proposed in [17] to include reference ground-truth captions in the evaluation process. Specifically, we compute the representation of each reference caption using the textual encoder. Then, we compute the harmonic mean between the reference-free score (Eq. 1) and the maximum cosine similarity between the candidate caption and all reference captions. Formally, given a set of reference captions R = {r1, r2, ..., rm}, the score is computed as\nRef-Score(t, v, R) = H-Mean(Score(t, v), max(0,max\nr\u2208R cos(t, r))), (4)\nwhere Score(\u00b7) indicates the reference-free evaluation score as reported by our positive-augmented embedding space, and H-Mean(\u00b7) indicates the harmonic mean."
        },
        {
            "heading": "3.2. Captioning evaluation score for videos",
            "text": "To test the proposed positive-augmented strategy for evaluating video captions, we extend the above defined metric following the approach of [49]. In this case, matching scores are computed at two granularity levels, i.e. a coarsegrained level in which the global representation of the candidate caption is compared with the global representation of the video, and a fine-grained level in which the embeddings of single words are compared to those of single frames.\nSpecifically, we use the positive-augmented CLIP visual encoder to extract the embeddings of single frames and average-pool them to get the representation of the entire video. Similarly, we employ the corresponding textual encoder to get single tokens and whole caption embeddings. The fine-grained score is then computed by taking the F1score of pairwise word-frame similarities and TF-IDF [41] weighting, and the coarse-grained score is computed as the similarity between the global video and caption representations. Given a source video V and a candidate caption c, the overall score is defined as\nScore(t, V ) = Score(t, V )c + Score(t, V )f\n2 , (5)\n1To stretch the range of the score distribution in [0, 1].\nwhere Scorec represents the coarse-grained embedding matching and Scoref stands for the fine-grained similarity. Finally, to include a set of reference captions R, we follow the reference version of the aforementioned approach:\nRef-Score(t, V, r) = Score(t, V ) + maxr\u2208R Score(t, r)\n2 ,\n(6) where Score(t, r) is computed as defined in Eq. 5 by using the word-level embeddings of the reference caption."
        },
        {
            "heading": "4. Experimental Evaluation",
            "text": ""
        },
        {
            "heading": "4.1. Implementation details",
            "text": "Architecture and training details. In continuity with existing literature [17, 24, 49], we use CLIP ViT-B/32 [38] as backbone to encode images (or video frames) and textual sentences. We finetune the visual and textual final projections of the model using the approach described in Sec. 3 on the COCO dataset [31], which contains more than 120k images annotated with five captions. In particular, we employ the splits introduced by Karpathy et al. [22], where 5,000 images are used for validation, 5,000 images are used for test and the rest for training. During finetuning, we use AdamW [32] as optimizer with a learning rate equal to 0.0001 and a batch size of 256. The \u03bbv and \u03bbt values are selected with a grid search, choosing the combination that provides the best average across datasets. Specifically, we set \u03bbv to 0.05 and \u03bbt to 0.1, and stop the training stage when the validation loss stops decreasing for 1,500 iterations. Positive image-text generation. To augment the training set with new positive examples, we use Stable Diffusion2 [43] for generating new visual data and the BLIP model [28] for generating new textual descriptions. Specifically, to generate images, we employ the model pretrained on the English image-text pairs of the LAION-5B dataset [46] and finetuned at a resolution equal to 512\u00d7512 on the LAION-Aesthetics subset3, which has been filtered\n2https://github.com/CompVis/stable-diffusion 3https://laion.ai/blog/laion-aesthetics/\nwith aesthetic requirements. During generation, we employ the safety checker module to reduce the probability of explicit images and disable the invisible watermarking of the outputs to avoid easy identification of the images as machine-generated. To generate text, instead, we use the ViT-L/14 version4 of the BLIP model pre-trained on 129M image-text pairs and finetuned on the COCO dataset. After this generation phase, we get a new version of the COCO dataset in which each image is additionally associated with a machine-generated caption and each humanannotated caption is instead associated with a newly generated image. Sample image-text data employed for finetuning are shown in Fig. 3."
        },
        {
            "heading": "4.2. Correlation with human judgment",
            "text": "To evaluate the correlation of the proposed metric with human ratings, we conduct experiments on both image and video captioning datasets. Specifically, we employ the Flickr8k-Expert, Flickr8k-CF, and Composite datasets [1, 18] for the image setting and the VATEX-EVAL dataset [49] to evaluate video-caption pairs.\nImage captioning results. We first evaluate our solution on the Flickr8k-Expert and Flickr8k-CF datasets [18] which include image-caption pairs with corresponding human ratings. In particular, Flickr8k-Expert contains 17k expert annotations for visual-textual pairs, with a total of 5,664 different images. The pairs are evaluated with a score from 1 to 4, where 1 indicates that the caption does not correlate with the image and 4 indicates that the caption describes the corresponding image without errors. Flickr8k-CF, instead,\n4https://github.com/salesforce/BLIP\nis composed of 145k binary quality judgments, collected from CrowdFlower, for 48k image-caption pairs (with 1,000 unique images). Each pair is annotated with at least three binary scores, where \u201cyes\u201d indicates that the caption correlates with the image. To measure the correlation with human judgment, we compute the mean proportion of \u201cyes\u201d annotations as the score for each pair.\nFollowing previous works [17, 25, 26, 67], we compute Kendall correlation scores in both \u03c4b and \u03c4c versions. Results are reported in Table 1 comparing the proposed PAC-S metric with respect to both standard captioning evaluation scores (i.e. BLEU [37], ROUGE [30], METEOR [4], CIDEr [52], and SPICE [2]) and more recent solutions that either exploit text-only or cross-modal learned embeddings, such as BERT-S [67], BERT-S++ [64], LEIC [11], TIGEr [21], UMIC [25], VilBERTScore [26], MID [24], and CLIP-S [17]. While CLIP-S is reported in both reference-free and reference-based versions, all other metrics require reference captions. The only exception is the MID score which is positioned between a reference-free and a reference-based metric since it utilizes the mean and covariance of the correct captions.\nFrom the results, it can be seen that the proposed score achieves the best correlation with human judgment on both considered datasets, demonstrating its effectiveness compared to previously proposed metrics. In particular, when comparing our score with CLIP-S and RefCLIP-S, we can notice an improvement in terms of Kendall \u03c4b of 2.8 and 2.9 points on Flickr8k-Expert, and 1.6 and 1.2 points on Flickr8k-CF, respectively. Similar improvements can be also observed in terms of Kendall \u03c4c correlation score. It is also important to note that the reference-free version of PAC-S overcomes by a large margin the correlation scores\nachieved by traditional reference-based metrics such as CIDEr and SPICE (e.g. +10.3/10.4 points with respect to the CIDEr metric on Flickr8k-Expert).\nWe also conduct experiments on the Composite dataset [1] which contains 12k human judgments for imagecaption pairs taken from COCO [31] (2,007 images), Flickr8k [18] (997 images), and Flickr30k [65] (991 images). Each image-caption pair is evaluated with a score, given by humans, between 1 and 5 to estimate the correspondence of the caption with the associated image. Experimental results are shown in Table 2, again in terms of Kendall \u03c4b and Kendall \u03c4c correlation scores. Also in this case, our metric achieves a better correlation with human ratings than that obtained by both traditional and more recent evaluation scores, confirming its effectiveness even when compared to CLIP-S and RefCLIP-S.\nVideo captioning results. To evaluate the correlation with humans in the context of video-caption pairs, we consider the VATEX-EVAL dataset [49] which includes 3,000 videos from the VATEX [57] validation set, each of them associated with six captions of mixed quality. Each video-caption pair has been evaluated by three human annotators with a score from 1 (to denote inconsistency between the video and the caption) to 5 (to denote consistency). Overall, the dataset contains 54k human ratings for 18k video-caption pairs. Following recent literature [49], we compute Kendall \u03c4b and Spearman \u03c1 rank correlation coefficients, considering a different number of reference sentences when measuring correlation (i.e. zero, one, or nine). Correlation scores are reported in Table 3 in comparison with standard evaluation metrics, BERT-S, BERT-S++, and the only videospecific captioning metric existing in literature, i.e. EMScore. On the right, we also report the correlation scores at varying the number of reference captions. It can be seen that PAC-S achieves the best correlation scores in all settings, improving EMScore of 2.3, 3.4, and 1.6 Spearman \u03c1 points respectively with no references, one reference, and nine reference sentences. These results further confirm the appropriateness of our positive-augmented contrastive learning\nstrategy to improve captioning evaluation also when considering videos instead of static images."
        },
        {
            "heading": "4.3. Caption pairwise ranking",
            "text": "We assess the effectiveness of the proposed metric on the Pascal-50S dataset [52], which reports pairwise preference judgments between two captions. Specifically, the dataset comprises 4k sentence pairs, each of them associated with an image from the UIUC Pascal sentence dataset [40]. For each pair, 48 human judgments have been collected, in which each evaluation expresses which sentence best describes the given image. Sentence pairs are divided into four different categories: two human-written and correct captions (HC), two human-written captions where one is correct and the other is wrong (HI), two correct captions but one written by humans and the other machine-generated\n(HM), two machine-generated and correct captions (MM). In this setting, instead of computing correlation scores, we compute accuracy by considering for each pair the caption preferred by the majority of human ratings as correct (where ties are broken randomly) and measuring how often the evaluation metric assigns a higher score to the selected caption. For each evaluation, we randomly sample five reference captions (among the 48 provided by the dataset) and average the results over five different draws. Accuracy values are reported in Table 4 in comparison with previously proposed metrics. Similarly to previous works, we also include the results of a length-based baseline in which the longer caption is always considered the better one. From the results, we can observe that PAC-S and RefPAC-S respectively perform better than CLIP-S and RefCLIP-S in almost all categories, with an increase of 1.5 points in terms of averaged accuracy. Also, our results are generally higher than those of the other metrics, with the only exception of the MID score which achieves slightly better accuracy. However, our results are not directly comparable to the ones reported in previous works (as, for example, FAIEr and MID), given the random selection of ground-truth sentences used to compute reference-based metrics.\nAs a further analysis, we evaluate the results on the Abstract-50S dataset [52] which contains clip-art images from [70] associated with 48 human-annotated reference sentences. Similar to Pascal-50S, each image is associated with a pair of candidate captions and 48 human judgments, collected asking to select which candidate caption is most similar to a given reference sentence. Overall, the dataset is composed of 400 candidate caption pairs, of which 200 describe the corresponding image (i.e. both captions are correct) and 200 instead contain one correct caption and one caption of another image. Again, we compute accuracy scores by considering the most preferred caption as correct, averaging the results over five random draws of reference sentences. Table 5 shows the results of our score in comparison with CLIP-S. In both reference-free and referencebased versions, PAC-S achieves better accuracy scores than CLIP-S, demonstrating its effectiveness also in this challenging setting of non-photographic images."
        },
        {
            "heading": "4.4. Sensitivity to object hallucination",
            "text": "Correctly identifying captions with potential object hallucinations (i.e. with objects that are not present in the image or video) is fundamental for the captioning task [42]. Therefore, we extend our analysis to two datasets for detect-\ning hallucinations in textual sentences, namely FOIL [47] and ActivityNet-FOIL [49]. In particular, the FOIL dataset is composed of image-caption pairs from the COCO dataset [31]. In this case, captions are perturbed by creating modified versions that are highly similar to the original ones but contain one single error (i.e. a foil word). For a fair comparison, we take the subset of the validation set that does not overlap with the portion of COCO used to finetune our model thus obtaining 8k images, each associated with a foil-correct textual pair. The ActivityNet-FOIL dataset, instead, contains video-text pairs from the ActivityNet test set [68]. Each video comes with two annotated paragraphs, one used to construct foil-correct pair and the other used as ground-truth for reference-based metrics. To create a foil caption, a noun phrase in the original caption is replaced with a similar but incorrect visual concept. Overall, the dataset is composed of 1,900 foil-correct paragraph pairs.\nSince each image or video is associated with a foilcorrect caption pair, we measure the portion of times in which the correct caption obtains a higher score than the foil one. Table 6 shows the accuracy results on the considered datasets. As it can be seen, PAC-S achieves better results than previous solutions, increasing the accuracy score of 2.7 and 0.6 points compared to CLIP-S and EMScore,\nrespectively. Similar improvements can also be observed in the reference-based version, demonstrating the capabilities of our metric to correctly identify hallucinated objects."
        },
        {
            "heading": "4.5. System-level correlation",
            "text": "After demonstrating the benefits of using PAC-S over other evaluation metrics, we also analyze its effectiveness when evaluating existing captioning methods. To this aim, we consider different popular captioning models and compute their predictions on images coming from the COCO test set. Results are reported in Table 7 in terms of BLEU4, METEOR, CIDEr, CLIP-S, and our PAC-S, in both reference-free and reference-based versions. We also include the results of a human-based baseline, in which for each sample one human-annotated sentence (among the five provided by the COCO dataset) is randomly selected as candidate caption and compared with the remaining references5. As shown in the table, our metric well correlates with previous ones in identifying the best captioning model. Interestingly, PAC-S can also effectively evaluate humanannotated sentences, unlike for example the METEOR and CIDEr scores which rank human captions even below those generated by early captioning approaches [53, 61]."
        },
        {
            "heading": "4.6. Analyzing other cross-modal features",
            "text": "Finally, we report in Table 8 captioning evaluation results when using different cross-modal features. In particular, we employ ViT-B/16 and ViT-L/14 models of CLIP [38] and the ViT-B/32 and ViT-L/14 versions of the open source implementation (i.e. OpenCLIP [60]6) trained on the English subset of the LAION-5B dataset [46]. For all backbones, we employ the same finetuning procedure and train-\n5The BLEU-4 score of the human-based baseline is not reported due to its sensitivity to the number of references used for evaluation.\n6https://github.com/mlfoundations/open_clip\ning settings described in Sec 4.1. We conduct the analysis on the majority of the datasets considered in the previous experiments and compare the proposed PAC-S with CLIP-S and EMScore, respectively for image and video captioning datasets. Noticeably, PAC-S achieves the best results across all cross-modal backbones and all datasets, overcoming correlation and accuracy scores of other metrics by a large margin. When comparing the results when using different backbones, both ViT-L/14 models outperform other considered architectures as well as the standard CLIP ViT-B/32 model used in previous experiments, thus demonstrating the usefulness of using more powerful cross-modal models to evaluate captioning predictions."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we have proposed a positive-augmented contrastive learning approach for image and video captioning evaluation. Our proposal, PAC-S, is trained by considering cleaned data sources and leveraging synthetic images and captions as an additional source of supervision. Experimentally, we have demonstrated that PAC-S is superior to all previous metrics in terms of correlation with human judgment and sensitivity to hallucinated objects in both reference-free and reference-based settings."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank CINECA for providing computational resources. Work conducted under a research grant co-funded by Leonardo S.p.A. and supported by the projects: PNRR-M4C2 (PE00000013) \u201cFAIR - Future Artificial Intelligence Research\u201d funded by the European Commission, \u201cELSA - European Lighthouse on Secure and Safe AI\u201d funded by the EU (GA 101070617), and the PRIN \u201cCREATIVE: CRoss-modal understanding and gEnerATIon of Visual and tExtual content\u201d co-funded by the Italian Ministry of University and Research (CUP B87G22000460001)."
        },
        {
            "heading": "A. Additional Experimental Results",
            "text": "Correlation with MID score. In addition to the experiments presented in the main paper, we conducted further comparisons with the MID metric [24]. Since it exploits CLIP-based features as CLIP-S [17] and our proposal, in Table 9 we compare the results of the original MID score with a re-implemented version that uses our embeddings in place of those of CLIP. In particular, we conduct this analysis on the Flickr8k-Expert, Flickr8k-CF, and FOIL datasets and show that using our embeddings can further improve the results of the MID score in the majority of the considered settings, thus further demonstrating the appropriateness of our positive-augmented contrastive learning approach.\nReference-based results using ViT-based backbones. As a complement to Table 8, in Table 10 we report the referenced-based results using different cross-modal features. In particular, we experiment with different ViT-based backbones of CLIP [38] and OpenCLIP [60] models. From these results, we confirm the effectiveness of PAC-S also in the reference-based setting on both image and video captioning datasets. Both ViT-L/14 models outperform the others even in this case, still confirming that using more powerful features can lead to better results.\nAnalyzing ResNet-based backbones. In Table 11, we conduct the same analysis in both reference-free and referencebased settings but using visual features extracted from a ResNet backbone [15]. Specifically, we use the following CLIP-based models: ResNet-50, ResNet-101, and ResNet-50\u00d74, which employ an EfficientNet-style architecture scaling. For these experiments, we finetune the last attention pooling of the visual backbone and the final projection of the textual branch using the same settings described in the main paper. Also in this case, our metric achieves the best results in almost all datasets, with the only exception of VATEX-EVAL in which the EMScore obtains slightly better correlation scores.\nChoice of hyperparameters. The scaling factor, denoted by w in Eq. 10, is utilized to adjust the scale of the final metric to improve its numerical readability, without affecting the ranking of the results. CLIP-S also employs a comparable technique, where w is assigned the value of 2.5. To provide additional clarification, we present in Fig. 4 the impact of varying values of w. The raw PAC-S scores with w = 1 lie between 0 and 0.5 on all datasets. Therefore, we decide to use a scaling factor w equal to 2 which stretch the PAC-S scores between 0 and 1."
        },
        {
            "heading": "B. Generated Samples and Qualitatives",
            "text": "Fig. 5 shows additional image-text generated examples used for the presented positive-augmented contrastive learning strategy. As it can be seen, both image and text generated samples are realistic and plausible and can be effectively used as an additional source of supervision.\nWe report in Fig. 6 some additional qualitative comparisons between PAC-S and well-known metrics on the Pascal-50S dataset. These qualitative results show that in"
        },
        {
            "heading": "A wooden table with two cups on",
            "text": "the majority of cases PAC-S is more aligned with the human judgments than other metrics. Finally, in Fig. 7 and 8, we report sample results comparing our metric with CLIPS [17] on FOIL, Flickr8k-Expert, and Flickr8k-CF datasets. As it can be observed, PAC-S can correctly identify hallucinated objects and better correlates with human judgments, demonstrating its effectiveness compared to CLIP-S also from a qualitative point of view."
        },
        {
            "heading": "A man and young girl eat a meal on",
            "text": ""
        }
    ],
    "title": "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation",
    "year": 2023
}