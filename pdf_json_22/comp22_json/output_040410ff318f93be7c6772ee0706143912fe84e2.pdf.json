{
    "abstractText": "High-resolution remote sensing images (HRRSIs) play an important role in large area and real-time earth observation tasks. However, HRRSIs typically comprise heterogeneous objects of various sizes and complex boundary lines, which poses challenges to HRRSI segmentation. Despite the fact that deep convolutional neural networks dramatically boosted the accuracy, several limitations exist in standard models. Existing methods, mainly concatenate multi-scale information to extract the various sizes of objects. However, these methods ignore differentiating information, making it difficult to take advantage of them and completely extract small objects. In addition, there have remained some difficulties in extracting boundary information with positions of uncertainty in previous works. In this paper, we propose a novel multi-scale feature weightedaggregating and boundary enhancement network (MFBE-Net) for the segmentation of HRRSIs. ResNet-50, possessing a strong ability to extract features, is employed as the backbone. To fully utilize the information that was extracted, we propose a multiscale feature weighted-aggregating module, which aims to weightintegrate deep features, shallow features, and global information. The boundary enhancement module is designed to solve the blurry boundary information problems and locate its positions. Coordinate attention is also applied in the framework to coherently label size-varied ground objects from different categories and reduce information redundancy. Meanwhile, a mixed loss function is used to supervise the network training process. Finally, MFBE-Net was verified on two public HRRSI datasets, and the experimental results show that the proposed framework outperformed other existing mainstream deep learning methods and could further improve the accuracy of",
    "authors": [
        {
            "affiliations": [],
            "name": "Yingying Zhao"
        },
        {
            "affiliations": [],
            "name": "Guizhou Zheng"
        },
        {
            "affiliations": [],
            "name": "Zhangyan Xu"
        },
        {
            "affiliations": [],
            "name": "Zhonghang Qiu"
        },
        {
            "affiliations": [],
            "name": "Zhixing Chen"
        }
    ],
    "id": "SP:1c5d35403fc89c6a22e080fc44ea24d2769fbf66",
    "references": [
        {
            "authors": [
                "M. Wang",
                "H. Zhang",
                "W. Sun",
                "S. Li",
                "F. Wang",
                "G. Yang"
            ],
            "title": "A Coarse-to- Fine deep learning based land use change detection method for highresolution remote sensing images",
            "venue": "Remote Sens., vol. 12, no. 12, pp. 1933, Jun. 2020, doi: 10.3390/rs12121933.",
            "year": 1933
        },
        {
            "authors": [
                "S. Kaliraj",
                "N. Chandrasekar",
                "K.K. Ramachandran",
                "Y. Srinivas",
                "S. Saravanan"
            ],
            "title": "Coastal land use and land cover change and transformations of Kanyakumari coast, India using remote sensing and GIS",
            "venue": "Egypt. J. Remote Sens. Space Sci., vol. 20, no. 2, pp. 169-185, Dec. 2017, doi: 10.1016/j.ejrs.2017.04.003.",
            "year": 2017
        },
        {
            "authors": [
                "I. Ahmad",
                "A. Singh",
                "M. Fahad",
                "M.M. Waqas"
            ],
            "title": "Remote sensing-based framework to predict and assess the interannual variability of maize yields in Pakistan using Landsat imagery",
            "venue": "Comput. Electron. Agr., vol. 178, pp. 105732, Nov. 2020, doi: 10.1016/j.compag.2020.105732.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Li",
                "C. Zhou",
                "X. Yang",
                "X. Chen",
                "F. Meng",
                "C. Lu",
                "T. Pan",
                "W. Qi"
            ],
            "title": "Urban landscape extraction and analysis in the mega-city of China \u2019s coastal regions using high-resolution satellite imagery: A case of Shanghai, China",
            "venue": "Int. J. Appl. Earth Obs., vol. 72, pp. 140-150, Oct. 2018, doi: 10.1016/j.jag.2018.03.002.",
            "year": 2018
        },
        {
            "authors": [
                "K. Yuan",
                "X. Zhuang",
                "G. Schaefer",
                "J. Feng",
                "L. Guan",
                "H. Fang"
            ],
            "title": "Deep- Learning-Based Multispectral Satellite Image Segmentation for Water Body Detection",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 7422-7434, Jul. 2021, doi: 10.1109/JSTARS.2021.3098678.",
            "year": 2021
        },
        {
            "authors": [
                "G. Zheng",
                "Z. Pan",
                "Y. Meng",
                "H. Wang"
            ],
            "title": "Inversion of Sea Surface Flow Field in Southern South China Sea Based on Satellite Remote Sensing Data",
            "venue": "Earth Sci.,",
            "year": 2021
        },
        {
            "authors": [
                "M.D. Hossain",
                "D. Chen"
            ],
            "title": "Segmentation for Object-Based Image Analysis (OBIA): A review of algorithms and challenges from remote sensing perspective",
            "venue": "ISPRS J. Photogrammetry Remote Sens., vol. 150, pp. 115-134, Apr. 2019, doi: 10.1016/j.isprsjprs.2019.02.009.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang"
            ],
            "title": "Joint Deep Learning for land cover and land use classification",
            "venue": "Remote Sens. Environ., vol. 221, pp. 173-187, Feb. 2019, doi: 10.1016/j.rse.2018.11.014.",
            "year": 2019
        },
        {
            "authors": [
                "M.Z. Ali",
                "W. Qazi",
                "N. Aslam"
            ],
            "title": "A comparative study of ALOS-2 PALSAR and landsat-8 imagery for land cover classification using maximum likelihood classifier",
            "venue": "Egypt. J. Remote Sens. Space Sci., vol. 21, pp. S29-S35, Jul. 2018, doi: 10.1016/j.ejrs.2018.03.003.",
            "year": 2018
        },
        {
            "authors": [
                "R.M.D.S. Vieira"
            ],
            "title": "Land degradation mapping in the MATOPIBA region (Brazil) using remote sensing data and decision-tree analysis",
            "venue": "Sci. Total Environ., vol. 782, pp. 146900, Aug. 2021, doi: 10.1016/j.scitotenv.2021.146900.",
            "year": 2021
        },
        {
            "authors": [
                "M.S. Navin",
                "L. Agilandeeswari"
            ],
            "title": "Land use land cover change detection using k-means clustering and maximum likelihood classification method in the javadi hills, Tamil Nadu, India",
            "venue": "Int. J. Eng. Adv. Technol., Dec. 2019, doi: 10.35940/ijeat.A1011.1291S319.",
            "year": 2019
        },
        {
            "authors": [
                "S.W. Myint",
                "P. Gober",
                "A. Brazel",
                "S. Grossman-Clarke",
                "Q. Weng"
            ],
            "title": "Perpixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery",
            "venue": "Remote Sens. Environ., vol. 115, no. 5, pp. 1145-1161, May 2011, doi: 10.1016/j.rse.2010.12.017.",
            "year": 2011
        },
        {
            "authors": [
                "T. Cover",
                "P. Hart"
            ],
            "title": "Nearest neighbor pattern classification",
            "venue": "IEEE Tran. Inform. Theory, vol. 13, no. 1, pp. 21-27, Jan. 1967, doi: 10.1109/TIT.1967.1053964.",
            "year": 1967
        },
        {
            "authors": [
                "S.S. Jamsandekar",
                "R.R. Mudholkar"
            ],
            "title": "Fuzzy classification system by self generated membership function using clustering technique",
            "venue": "BVICA M's International Journal of Inf. Technol., vol. 6, no. 1, p. 697, Feb. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Q. Zhu"
            ],
            "title": "A Global Context-aware and Batch-independent Network for road extraction from VHR satellite imagery",
            "venue": "ISPRS J. Photogrammetry Remote Sens., vol. 175, pp. 353-365, May 2021, doi: 10.1016/j.isprsjprs.2021.03.016.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhu",
                "Y. Zhong",
                "L. Zhang",
                "D. Li"
            ],
            "title": "Adaptive deep sparse semantic modeling framework for high spatial resolution image scene classification",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 56, no. 10, pp. 6180-6195, Oct. 2018, doi: 10.1109/TGRS.2018.2833293.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Guo",
                "Y. Liu",
                "T. Georgiou",
                "M.S. Lew"
            ],
            "title": "A review of semantic segmentation using deep neural networks",
            "venue": "Int. J. of Multimedia Information Retrieval, vol. 7, no. 2, pp. 87-93, 2018-01-01. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Nijhawan",
                "D. Joshi",
                "N. Narang",
                "A. Mittal",
                "A. Mittal"
            ],
            "title": "A futuristic deep learning framework approach for land use-land cover classification using remote sensing imagery",
            "venue": "Adv. computing commun. technol., pp. 87- 96, Jul. 2019, doi: 10.1007/978-981-13-0680-89.",
            "year": 2019
        },
        {
            "authors": [
                "P. Liu",
                "K.R. Choo",
                "L. Wang",
                "F. Huang"
            ],
            "title": "SVM or deep learning? A comparative study on remote sensing image classification",
            "venue": "Soft Comput., vol. 21, no. 23, pp. 7053-7065, Jul. 2016, doi: 10.1007/s00500-016-2247- 2.",
            "year": 2016
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2015, pp. 3431-3440.",
            "year": 2015
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Proc. Int. Conf. on Med. image Comput. Comput.-Assist Intervention, 2015, pp. 234-241.",
            "year": 2015
        },
        {
            "authors": [
                "L.-C. Chen",
                "Y. Zhu",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "venue": "Proc. Eur. Conf. Comput. Vis., 2018, pp. 801-818.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhao",
                "J. Shi",
                "X. Qi",
                "X. Wang",
                "J. Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2017, pp. 2881-2890.",
            "year": 2017
        },
        {
            "authors": [
                "F.I. Diakogiannis",
                "F. Waldner",
                "P. Caccetta",
                "C. Wu"
            ],
            "title": "ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data",
            "venue": "ISPRS J. Photogrammetry Remote Sens., vol. 162, pp. 94-114, Apr. 2020, doi: 10.1016/j.isprsjprs.2020.01.013.",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "Z. Wang",
                "K. Cheng"
            ],
            "title": "An improved algorithm for semantic segmentation of remote sensing images based on DeepLabv3+",
            "venue": "Proc. 5th Int. Conf. Commun. Inf. Process., 2019, pp. 124-128.",
            "year": 2019
        },
        {
            "authors": [
                "P. Zhang",
                "Y. Ke",
                "Z. Zhang",
                "M. Wang",
                "P. Li",
                "S. Zhang"
            ],
            "title": "Urban land use and land cover classification using novel deep learning models based on high spatial resolution satellite imagery",
            "venue": "Sensors-Basel, vol. 18, no. 11, p. 3717. Aug. 2018, doi: 10.3390/s18113717.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liu",
                "B. Fan",
                "L. Wang",
                "J. Bai",
                "S. Xiang",
                "C. Pan"
            ],
            "title": "Semantic labeling in very high resolution images via a self-cascaded convolutional neural network",
            "venue": "ISPRS J. Photogrammetry Remote Sens., vol. 145, pp. 78-95. Nov. 2018, doi: 10.1016/j.isprsjprs.2017.12.007.",
            "year": 2018
        },
        {
            "authors": [
                "L.-C Chen",
                "G. Papandreou",
                "I. Kokkinos",
                "K. Murphy",
                "A.L. Yuille"
            ],
            "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
            "venue": "2014, arXiv:1412.7062. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Geman",
                "C. Graffigne"
            ],
            "title": "Markov random field image models and their applications to computer vision",
            "venue": "Proc. Int'l Congress of Math., pp. 1496-1517, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "P. Kr\u00e4henb\u00fchl",
                "V. Koltun"
            ],
            "title": "Efficient inference in fully connected crfs with gaussian edge potentials",
            "venue": "Proc. Advances Neural Inf. Process. Syst., pp. 109-117, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Zhen"
            ],
            "title": "Joint semantic segmentation and boundary detection using iterative pyramid contexts",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 13666-13675, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Takikawa",
                "D. Acuna",
                "V. Jampani",
                "S. Fidler"
            ],
            "title": "Gated-scnn: Gated shape cnns for semantic segmentation",
            "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 5229-5238.",
            "year": 2019
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "2014, arXiv:1409.1556.",
            "year": 2014
        },
        {
            "authors": [
                "C. Szegedy"
            ],
            "title": "Going deeper with convolutions",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2015, pp. 1-9.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770-778.",
            "year": 2016
        },
        {
            "authors": [
                "X. Yuan",
                "J. Shi",
                "L. Gu"
            ],
            "title": "A review of deep learning methods for semantic segmentation of remote sensing imagery",
            "venue": "Expert Syst. Appl., vol. 169, p. 114417, May 2021, doi: 10.1016/j.eswa.2020.114417.",
            "year": 2021
        },
        {
            "authors": [
                "P. Harrington"
            ],
            "title": "Machine Learning in Action, Greenwich, CT",
            "venue": "USA: Manning,",
            "year": 2012
        },
        {
            "authors": [
                "C. Yang"
            ],
            "title": "Application of decision tree technology for image classification using remote sensing data",
            "venue": "Agr. Syst., vol. 76, no. 3, pp. 1101-1117, Jun. 2003, doi: 10.1016/S0308-521X(02)00051-3.",
            "year": 2003
        },
        {
            "authors": [
                "J.R. Otukei",
                "T. Blaschke"
            ],
            "title": "Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms",
            "venue": "Int. J. Appl. Earth Obs., vol. 12, pp. S27-S31, Feb. 2010, doi: 10.1016/j.jag.2009.11.002.",
            "year": 2010
        },
        {
            "authors": [
                "J. Zhao"
            ],
            "title": "Multi-source collaborative enhanced for remote sensing images semantic segmentation",
            "venue": "Neurocomputing, vol. 493, pp. 76-9, Jul. 2022, doi: 10.1016/j.neucom.2022.04.045.",
            "year": 2022
        },
        {
            "authors": [
                "R. Dong",
                "X. Pan",
                "F. Li"
            ],
            "title": "DenseU-net-based semantic segmentation of small objects in urban remote sensing images",
            "venue": "IEEE Access, vol. 7, pp. 65347-65356, May 2019, doi: 10.1109/ACCESS.2019.2917952.",
            "year": 2019
        },
        {
            "authors": [
                "J. Lee",
                "C. Kim",
                "S. Sull"
            ],
            "title": "Weakly Supervised Segmentation of Small Buildings With Point Labels",
            "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 7406-7415.",
            "year": 2021
        },
        {
            "authors": [
                "L. Gao"
            ],
            "title": "STransFuse: Fusing Swin Transformer and Convolutional Neural Network for Remote Sensing Image Semantic Segmentation",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 10990-11003, Oct. 2021, doi: 10.1109/JSTARS.2021.3119654.",
            "year": 2021
        },
        {
            "authors": [
                "X. Ma",
                "X. Zhang",
                "M. -O. Pun"
            ],
            "title": "A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15, pp. 3463-3474, 2022, doi: 10.1109/JSTARS.2022.3165005.",
            "year": 2022
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7132-7141.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Hou",
                "D. Zhou",
                "J. Feng"
            ],
            "title": "Coordinate attention for efficient mobile network design",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 13713-13722.",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "K. Qiu",
                "L. Chen",
                "X. Mei",
                "L. Hong",
                "C. Tao"
            ],
            "title": "SCAttNet: Semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images",
            "venue": "IEEE Geosci. Remote Sen. Lett., vol. 18, no. 5, pp. 905-909, Apr. 2020, doi: 10.1109/LGRS.2020.2988294.",
            "year": 2020
        },
        {
            "authors": [
                "R. Li"
            ],
            "title": "Multiattention network for semantic segmentation of fineresolution remote sensing images",
            "venue": "IEEE Trans. Geosci. Remote Sens., Jul. 2021, doi: 10.1109/TGRS.2021.3093977.",
            "year": 2021
        },
        {
            "authors": [
                "X. Pan",
                "L. Gao",
                "B. Zhang",
                "F. Yang",
                "W. Liao"
            ],
            "title": "High-resolution aerial imagery semantic labeling with dense pyramid network",
            "venue": "Sensors-Basel, vol. 18, no. 11, p. 3774, Sep. 2018, doi: 10.3390/s18113774.",
            "year": 2018
        },
        {
            "authors": [
                "P. Luc",
                "C. Couprie",
                "S. Chintala",
                "J. Verbeek"
            ],
            "title": "Semantic segmentation using adversarial networks",
            "venue": "2016, arXiv:1611.08408.",
            "year": 2016
        },
        {
            "authors": [
                "J. Peng",
                "Z. Nan",
                "L. Xu",
                "J. Xin",
                "N. Zheng"
            ],
            "title": "A deep model for joint object detection and semantic segmentation in traffic scenes",
            "venue": "Proc. 2020 IJCNN, 2020, pp. 1-8.",
            "year": 2020
        },
        {
            "authors": [
                "G. Sistu",
                "I. Leang",
                "S. Yogamani"
            ],
            "title": "Real-time joint object detection and semantic segmentation network for automated driving",
            "venue": "2019, arXiv:1901.03912.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "Improving semantic segmentation via decoupled body and edge supervision",
            "venue": "Proc. Eur. Conf. Comput. Vis., 2020, pp. 435-452.",
            "year": 2020
        },
        {
            "authors": [
                "X. Ma",
                "L. Wang",
                "K. Qi",
                "G. Zheng"
            ],
            "title": "Remote Sensing Image Scene Classification Method Based on Multi-Scale Cyclic Attention Network",
            "venue": "Earth Sci., vol. 46, no. 10, pp. 3740-3752. 2021, doi: 10.3799/dqkx.2020.365.",
            "year": 2021
        },
        {
            "authors": [
                "A. Li",
                "L. Jiao",
                "H. Zhu",
                "L. Li",
                "F. Liu"
            ],
            "title": "Multitask semantic boundary awareness network for remote sensing image segmentation",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-14, Feb. 2021, doi: 10.1109/TGRS.2021.3050885.",
            "year": 2021
        },
        {
            "authors": [
                "G. Bertasius",
                "J. Shi",
                "L. Torresani"
            ],
            "title": "Semantic segmentation with boundary neural fields",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 3602-3610.",
            "year": 2016
        },
        {
            "authors": [
                "D. Marmanis",
                "K. Schindler",
                "J.D. Wegner",
                "S. Galliani",
                "M. Datcu",
                "U. Stilla"
            ],
            "title": "Classification with an edge: Improving semantic image segmentation with boundary detection",
            "venue": "ISPRS J. Photogrammetry Remote Sens., vol. 135, pp. 158-172. Jan. 2018, doi: 10.1016/j.isprsjprs.2017.11.009.",
            "year": 2018
        },
        {
            "authors": [
                "F. Milletari",
                "N. Navab",
                "S. Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "Proc. 4th Int. Conf. 3D Vis., 2016, pp. 565-571.",
            "year": 2016
        },
        {
            "authors": [
                "M. Volpi",
                "D. Tuia"
            ],
            "title": "Dense semantic labeling of subdecimeter resolution images with convolutional neural networks",
            "venue": "IEEE Trans. Geosci. Remote Sens., vol. 55, no. 2, pp. 881-893, Oct. 2016, doi: 10.1109/TGRS.2016.2616585",
            "year": 2016
        },
        {
            "authors": [
                "X. Tong",
                "G. Xia",
                "Q. Lu",
                "H. Shen",
                "S. Li",
                "S. You",
                "L. Zhang"
            ],
            "title": "Learning transferable deep models for land-use classification with high-resolution remote sensing images",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "2014, arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "O. Russakovsky"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "Int. J. Comput. Vis., vol. 115, no. 3, pp. 211-252, Apr. 2015, doi: 10.1007/s11263-015-0816-y.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Xu",
                "W. Zhang",
                "T. Zhang",
                "J. Li",
                "\"HRCNet"
            ],
            "title": "High-resolution context extraction network for semantic segmentation of remote sensing images",
            "venue": "Remote Sens., vol. 13,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "1\n> JSTARS-2022-00888 <\nMulti-scale Feature Weighted-Aggregating and\nBoundary Enhancement Network for Semantic\nSegmentation of High-Resolution Remote Sensing\nImages Yingying Zhao, Guizhou Zheng, Zhangyan Xu, Zhonghang Qiu, Student Member, IEEE, and Zhixing Chen.\nAbstract\u2014High-resolution remote sensing images (HRRSIs) play an important role in large area and real-time earth observation tasks. However, HRRSIs typically comprise heterogeneous objects of various sizes and complex boundary lines, which poses challenges to HRRSI segmentation. Despite the fact that deep convolutional neural networks dramatically boosted the accuracy, several limitations exist in standard models. Existing methods, mainly concatenate multi-scale information to extract the various sizes of objects. However, these methods ignore differentiating information, making it difficult to take advantage of them and completely extract small objects. In addition, there have remained some difficulties in extracting boundary information with positions of uncertainty in previous works. In this paper, we propose a novel multi-scale feature weightedaggregating and boundary enhancement network (MFBE-Net) for the segmentation of HRRSIs. ResNet-50, possessing a strong ability to extract features, is employed as the backbone. To fully utilize the information that was extracted, we propose a multiscale feature weighted-aggregating module, which aims to weightintegrate deep features, shallow features, and global information. The boundary enhancement module is designed to solve the blurry boundary information problems and locate its positions. Coordinate attention is also applied in the framework to coherently label size-varied ground objects from different categories and reduce information redundancy. Meanwhile, a mixed loss function is used to supervise the network training process. Finally, MFBE-Net was verified on two public HRRSI datasets, and the experimental results show that the proposed framework outperformed other existing mainstream deep learning methods and could further improve the accuracy of HRRSI segmentation.\nIndex Terms\u2014High-resolution remote sensing images, deep learning, feature weighted-aggregating, boundary enhancement, semantic segmentation.\nI. INTRODUCTION\nIGH-RESOLUTION remote sensing images (HRRSIs) are an important information source with higher image quality, wider coverage, and more\nstructured data in comparison to traditional images [1]. HRRSI\nThis study was financially supported by the National Natural Science Foundation of China (Grant Nos. 42130309 and 41972066). (Corresponding author: Guizhou Zheng.)\nY. Zhao, G. Zheng, Z. Xu, and Z. Chen are with the School of Geography and Information Engineering, China University of Geosciences, Wuhan\nsegmentation is to divide each pixel unit in HRRSIs into corresponding categories, which holds great significance in applications such as land use and land cover [2], [3], ecological system [4], [5], inversion of water depth [6],[7] and other industries. However, visual interpretation of HRRSIs is a method with low efficiency and strong dependence on knowledge and experience. In recent years, automatic segmentation of HRRSIs has become an active research topic [8].\nVarious methods for HRRSI segmentation have been proposed, which fall into three main methods: pixel-based segmentation, object-oriented segmentation, and deep learning segmentation. The pixel-based method [9] takes pixels as the basic unit with which to extract information, and classifies pixels according to the spectral information depending on prior knowledge, such as the maximum likelihood classifier [10], decision tree analysis [11], and K-means clustering [12]. The object-oriented method [13] takes objects as the basic unit, and an object is an entity composed of a group of adjacent homogeneous pixels, such as nearest-neighbor pattern classification [14] and membership function [15]. However, traditional HRRSI segmentation methods cannot capture more detailed features due to the more complex geometrical structures and richer spectral features in HRRSIs [16]. The complex classification and small objects make the traditional segmentation method difficult to complete the segmentation of HRRSIs.\nDeep learning can segment HRRSIs and obtain high accuracy through automatically learning useful features from images [17]. Deep learning methods used in HRRSI segmentation are based mostly on convolutional neural networks (CNNs) and fully convolutional networks (FCNs) [18]. Related works [19], [20] have proven that deep learning methods are more accurate than traditional machine learning in the segmentation of HRRSIs. The FCN proposed by Long et al. replaces the fully connected layers of CNNs with fully convolutional layers [21], allowing input of any size, and is the\n430074, China (e-mail: zhaoyy@cug.edu.cn; zhenggz@cug.edu.cn; Xuzhangyan0815@163.com; ChenZhixing@cug.edu.cn).\nZ. Qiu is with the School of Resource and Environmental Sciences, Wuhan\nUniversity, Wuhan 430079, China (e-mail: qiu_zh@whu.edu.cn).\nColor versions of one or more of the figures in this article are available\nonline at http://ieeexplore.ieee.org\nH\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\n> JSTARS-2022-00888 <\nfirst true work of semantic segmentation. Later, based on FCNs, semantic segmentation models such as U-Net [22], DeepLab [23] and PSPNet [24] were successively proposed. Motivated by their remarkable improvements in comparison with traditional machine learning approaches, many scholars utilize these models in HRRSI segmentation. Diakogiannis et al. combined a U-Net encoder-decoder backbone in combination with residual connections, atrous/dilated convolutions, pyramid scene parsing pooling, and multitasking inference to segment HRRSIs [25]. Based on DeepLabv3+, Liu et al. designed a decoder by adding more skip connections and convolution layers to obtain more detailed information to improve segmentation results [26].\nAlthough the methods based on CNN and FCN show dramatic performance, the incomplete identification of small objects and the difficulty in extracting boundary information have invariably been problems that need to be solved urgently in HRRSI segmentation. To alleviate the problem of incomplete identification and low accuracy of small objects, the main solution is to fuse multi-scale information. Zhang et al. applied the atrous spatial pyramid pooling (ASPP) module and the residual module to the proposed models to segment HRRSIs, showing that information fusion of different scales can restore some detailed features to a certain extent [27]. Liu et al. designed ScasNet, where multi-scale contexts are captured on the output of a CNN encoder, and then are successively aggregated in a self-cascaded manner to progressively refine the target objects [28]. To a certain extent, multi-scale information fusion can restore some detailed features and improve the rate of small object segmentation accuracy. However, previous works have mainly simply concatenated multi-scale information while ignoring the differences in multiscale information. Therefore, how to differentially fuse multiscale information in HRRSI segmentation is of great significance. Different from previous works, we consider the difference in the multi-scale information coefficient, so we use not simply concatenation, but a weighting aggregate (which is more beneficial in segmenting small targets). Post-processing is often used to solve the problem of poor localization near object boundaries, such as conditional random fields (CRF) [29], Markov random fields (MRF) [30] and DenseCRF [31], but these methods require additional parameters and low-level features, greatly affecting the running speed of a computer. In recent years, some studies have designed boundary detection modules as a branch of the model to improve the performance of boundary positions and certainty. Zhen et al. [32] combined semantic segmentation and boundary detection using the iterative pyramid context module. Takikawa et al. [33] think that color, shape and texture contain very different types of information relevant for recognition, so they proposed a twostream CNN. That is to say, the shape stream runs parallel to the classical stream as a separate branch to process information. Joint boundary detection and semantic segmentation have achieved certain results, but supervised training of boundary detection greatly increases the burden of a computer, which is secondary training. It is better if one uses one network and one\ntrain to extract body and boundary information at the same time. The multi-level structure of CNNs could solve this problem. Differently introducing an additional boundary detection network, we enhance boundary information by fusing multilevel information.\nTo address the aforementioned problems in the extraction of small objects, as well as blurry and uncertain position boundaries, a multi-scale feature weighted-aggregating and boundary enhancement network (MFBE-Net) is proposed for HRRSI segmentation in this paper. The MFBE-Net uses Resnet-50 as the backbone, in combination with the multi-scale feature weighted-aggregating module (MFW), boundary enhancement (BEM), and coordinate attention (CA). The MFBE-Net adopts the encoding and decoding structure. In the decoding, MFW weighted aggregate of different scale information, CA is added between two decoding blocks, and boundary enhancement (BEM) utilizes multi-level information to enhance the boundary feature. Finally, we construct a mixed loss function to learn local and global contextual information and supervise the feature generation. The main contributions of this paper are as follows:\n1) A multi-scale feature weighted-aggregating and boundary\nenhancement network is proposed for the segmentation of HRRSIs. The MFBE-Net can alleviate detail recovery and blurry boundary information problems to achieve a better performance of body and boundary. 2) To sufficiently leverage the differences in multi-scale\nfeatures, we design a multi-scale feature weightedaggregating module to model the relationship between multi-scale features. MFW uses shallow features to generate the weight matrix of deep features because deep features lose some details related to salient targets, which is beneficial in recovering detail features. 3) We propose BEM to retrieve lost boundary information in\nthe encoding process by adding the difference between low-level information and global information. This has never been experienced, as far as we know. The efficiency of the proposed framework was verified with two HRRSI datasets: the \u201cPotsdam Data Set\u201d and \u201cGID.\u201d By comparing the results to those of other state-of-the-art (SOTA) semantic segmentation algorithms, it was found that the MFBENet manifests superior performance in both visualization and quantitative evaluation.\nThe rest of this paper is structured as follows. Section II reviews related works\u2019 segmentation of HRRSIs. Section III introduces the proposed framework and the experiment details. The extensive experimental results are presented in Section IV. The detailed discussion is presented in Section V. Finally, conclusions are given in Section VI."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Segmentation in Computer Vision",
            "text": "Deep learning has been used to great effect in the field of computer vision. VGGNet [34], GoogleNet [35] and ResNets [36] are three deep neural networks designed for feature\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\n> JSTARS-2022-00888 <\nextraction on which many later image-processing model developments have been built [37]. It is pertinent to mention ResNets. As the network deepens, training the model gets increasingly challenging. ResNets utilize residual modules to allow the construction of very deep networks without gradient vanishing, producing results that are noticeably superior to earlier networks. The main ideas of these networks also support the development of semantic segmentation. Long et al. presented an FCN [21] which uses the fully convolutional layer to replace the fully connected layer of CNNs, allowing the network to allow input of any size. However, the FCN has the problem that the receptive field is fixed and the target details are easily lost or smoothed. Therefore, U-Net was proposed by Ronneberger et al. [22]. U-Net uses the encode\u2013decode framework. Using the concatenation operation to combine shallow features and deep features could form thicker features, which perform well in medical image segmentation. However, both downsampling and pooling layers in deep learning are accompanied by a loss of information, resulting in reduced spatial resolution, and the segmenter itself has spatial insensitivity. Hence, DeepLab is designed [23], which uses atrous/dilated convolutions to expand the range of filters to incorporate image contextual information into a larger neighborhood, thus being able to explicitly control the resolution of feature responses. Later, PSPNet proposes a pyramid pooling module to aggregate global contextual information, and uses an additional loss function to further improve the robustness and accuracy of the model [24]."
        },
        {
            "heading": "B. HRRSI Semantic Segmentation",
            "text": "HRRSI segmentation is divided into three stages: visual interpretation, traditional machine learning, and deep learning. The visual interpretation method has a strong dependence on researchers\u2019 experience and knowledge, and segmentation results are different due to personal experience. Visual interpretation efficiency is low, which is suitable for fewer data. Traditional machine learning predicts the results of target variables by setting appropriate model parameters through train datasets [38]. Traditional machine learning methods include decision tree analysis [39], the maximum likelihood classifier [10], support vector machines (SVMs) [40], and so on. However, there are some problems with traditional machine learning algorithms. Decision tree analyses are prone to overfitting, and SVMs have a higher missing and wrong probability when the number of samples is large. Prior knowledge is very important because these methods usually need feature generation and selection steps before the segmentation process. With the development of segmentation in computer vision, HRRSI segmentation based on deep learning has also been greatly developed. A considerable amount of literature has been published on HRRSI tasks. Early research focused on learning features from the local and global information of images and then designed a supervised classifier to identify the learned features to label. Compared to natural images, HRRSIs have more complex classifications and objects with various sizes. The hierarchical structure of CNNs could\nsolve the problem of various-sized-object segmentation, since the semantic information of small objects exists in the shallow layer, while the semantic information of large objects is situated in the deep layer [41]. Zhao et al. proposed an end-to-end multisource remote sensing image semantic segmentation network (MCENet) aiming at the problems of intraclass inconsistency and interclass indistinguishability in HRRSIs [42]. Dong et al. designed DenseUNet to connect convolutional neural network features through cascade operations and used its symmetrical structure to fuse the detail features in shallow layers and the abstract semantic features in deep layers [43]. Lee et al. presented a segmentation network trained with a small object mask to separate small and large objects in the loss function [44]. In recent years, the self-attentive Transformer-based model which models global semantic information through selfattention has developed rapidly. Gao et al. proposed the STransFuse model that combines the benefits of SwinTransformer with CNN to improve the segmentation quality of HRRSIs [45]. Ma et al. introduced a crossmodal multiscale fusion network (CMFNet) by exploiting both CNN and the transformer architecture to capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities [46]."
        },
        {
            "heading": "C. Optimization and Post-processing Strategy",
            "text": "In optimization strategies, attention mechanisms are generally proposed to solve data redundancy and coherently label by explicitly modeling interdependencies between spatial positions or channels. Squeeze-and-excitation (SE) [47] is one of the most popular attention mechanisms, but SE only considers channel information, ignoring the significance of location information. Therefore, Hou et al. [48] put forth coordinate attention (CA), which embeds location information into channel attention and is more suitable for vision tasks with dense predictions, such as semantic segmentation. Li et al. [49] proposed a new semantic segmentation network, i.e., Network with Spatial and Channel Attention (SCAttNet), to improve the semantic segmentation accuracy of HRRSIs. Li et al. proposed a multi-attention network (MANet) to address the underuse of information by extracting contextual dependencies through multiple efficient attention modules [50]. To improve spatial contiguity and sharpen borders in output label maps, a variety of post-processing techniques have been explored. Chen et al. [29] combined the responses in the final DCNN layer with a fully connected conditional random field (CRF) for HRRSI segmentation, considering convolution scale and spatial positioning characteristics. Pan et al. [51] used a CRF as a postprocessing method to further improve segmentation accuracy. However, the combination of CNNs and a probability graph model is time-consuming with limited precision improvement, and a CRF lacks spatial consistency, at least during the training period, which is generally used in the case of paired or highorder models with few trainable parameters [52]. In order to alleviate the problem of feature maps with large receptive fields losing high-frequency information and causing blurred boundaries, many scholars construct a comparatively simple\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\n> JSTARS-2022-00888 <\nmodel by adding boundary detection to existing multi-level architecture. Peng et al. [53] proposed an encoder-decoder convolution network that extracts a set of feature maps shared by the detection branch and the segmentation branch to jointly carry out object detection and semantic segmentation. Sistu et al. [54] presented a joint multi-task network design for learning object detection and semantic segmentation simultaneously and demonstrating the efficiency of the joint network. Li et al. [55] explicitly modeled the object\u2019s body and edge, with the body feature and the residual edge feature being further optimized under decoupled supervision."
        },
        {
            "heading": "III. METHOD",
            "text": "This section is organized as follows. First, in Section III-A, an overview of the proposed model for HRRSIs is illustrated. Thereafter, the main modules, backbone, MFW, and BEM are introduced below. For feature maps from the backbone, MFW is utilized to model the relationship of multiple scales of feature maps during decoding. For the boundary with uncertain positions and blurry information, designing a special module to extract boundary information is necessary, and BEM is proposed to enhance the boundary information. Finally, we introduce the loss function used to supervise the training of the network."
        },
        {
            "heading": "A. Pipelines of Proposed Model",
            "text": "For the HRRSI segmentation, the multi-scale and multi-level structures are beneficial to the segmentation of challenging objects and boundary extraction. Combining the advantages of multi-scale and multi-level structures, the MFBE-Net is proposed in this paper to solve the problems of low recognition accuracy of small objects and blurry boundary information. The main components of the proposed framework are: 1) The\nMFBE-Net adopts the encoder-decoder structure, with one main advantage being that the skip-layer mechanism contributes to extracting the feature maps conveying both highlevel and low-level information. 2) For the HRRSI segmentation, the multi-scale structure is essential, since it generates feature maps with relatively large receptive fields, which are beneficial to the segmentation of challenging objects, especially for objects of varied sizes [56]. Pretrained Resnet-50 is used as an encoding machine of the backbone network to extract multi-scale features, where dilated convolution is used to capture long-range information and stop excessive downsampling. The number of channels continues to increase, and the image size does not decrease after eight times of downsampling. 3) In the decoder part, MFW is used to weightaggregate high-level, low-level and global features. The feature-fusing process is gradually completed by the three MFWs. We finally obtain the resulting map to achieve end-toend segmentation. 4) Multi-level methods can recover the resolutions of feature maps, refine the relatively coarser prediction, and precisely segment the boundaries of finely structured objects [41]. Therefore, in the process of feature extraction, BEM is proposed to enhance the boundary information. 5) Meanwhile, we add a CA module after each MFA in the decoding to coherently label ground objects of varied sizes from different categories and reduce information redundancy. Moreover, we construct a mixed loss function to supervise the local and global feature generation. The overall framework of the MFBE-Net is shown in Fig. 1."
        },
        {
            "heading": "B. Backbone",
            "text": "As the network deepens, the features are more abstract and the information is richer, but the resulting gradient explosion and disappearance make the network optimization worse. Thus,\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\n> JSTARS-2022-00888 <\nwe use U-Net as the backbone, which a residual module is introduced in the U-Net encoding process to achieve consistent training in this paper. The residual module in Resnet-50 is shown in Fig. 2. Its main idea can be defined as follows:\n\ud835\udc39(\ud835\udc65) = \ud835\udc3b(\ud835\udc65) \u2212 \ud835\udc65 (1) Where \ud835\udc65 is the input map, \ud835\udc39(\ud835\udc65) is the stacked nonlinear layer fit, and \ud835\udc3b(\ud835\udc65) formally denotes the desired underlying mapping.\nAfter the residual module is introduced, the network highlights small changes, making the mapping more sensitive to changes in the output. However, too many downsampling operations in Resnet-50 cause a loss of detailed information and a reduction in resolution, which is not conducive to semantic segmentation tasks. Downsampling is done to broaden the receptive field. Therefore, it is necessary to expand the receptive field without reducing the size of an image. Atrous/dilated convolution comes into being. Based on this, this paper adjusts Resnet-50 to make it more suitable for semantic segmentation. In the adjusted Resnet-50, block1 and block2 remain unchanged. The residual module in block3 and block4 is shown in Fig. 3. We set the convolution step size to 1, and add the expansion coefficient within each residual building block so that the network can increase the receptive field without downsampling.\nFig. 3. Deformed residual module with dilated convolution"
        },
        {
            "heading": "C. Multi-Scale Feature Weighted-Aggregating Module",
            "text": "For the segmentation of objects of varied sizes, feature maps from multiple scales could be helpful, since the information from high-resolution feature maps of CNNs can provide semantic information of small or threadlike objects, and information from low-resolution feature maps contains semantic information of relatively large objects. Meanwhile, the shallow layers of the network include low-level features such as color, boundary, and spatial information, etc. [57], but they also contain a large amount of noise. The deep layer can provide rich semantic information and suppress noise, but its resolution is low, which is not conducive to localization. The global context feature can reduce the dilution of high-level features. but different spatial scales and global contextual information have great connections and differences. The conventional fusion strategy achieved by concatenation does not have different hierarchical feature maps, which might account for the challenge that exists in labeling objects of varied sizes. Based on the above, this paper proposes MFW, which is an aggregation strategy for global features, high-level features, and low-level features, as illustrated in Fig. 4.\nDifferent from previous works, we consider the difference in the multi-scale information coefficient. We design MFW which not simply concatenates multi-scale features, but a weighting aggregate. In this paper, we use shallow features to generate the weight matrix of deep features, which is more radical and more efficient. As background noise is suppressed, the multiplication process can enhance the response of important targets. Specifically, first, the low-level feature \ud835\udc53\ud835\udc59 \ud835\udc61 (\ud835\udc61 = 1,2,3) and the high-level feature \ud835\udc53\u210e \ud835\udc61 are fed into a 1\u00d71 convolution layer \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc631, which compresses the features to have the same number of channels as \ud835\udc53\u210e \ud835\udc61. Thereafter, the 3\u00d73 convolution layer \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc632 is applied to the compressed low-level feature layer \ud835\udc53\ud835\udc59 ?\u0303? to generate the weight matrix \ud835\udc4a\ud835\udc59 \ud835\udc61 of the compressed high-level feature layer \ud835\udc53\u210e ?\u0303? . Finally, we multiply \ud835\udc4a\ud835\udc59 \ud835\udc61 to upsampling \ud835\udc53\u210e ?\u0303? . Finally, the fusion features of the t stage \ud835\udc53\u210e\ud835\udc59 \ud835\udc61 are obtained through the RELU activation function.\n\ud835\udc4a\ud835\udc59 \ud835\udc61 = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc632(\ud835\udc53\ud835\udc59 ?\u0303?) (2)\n\ud835\udc53\u210e\ud835\udc59 \ud835\udc61 = \ud835\udeff(\ud835\udc4a\ud835\udc59 \ud835\udc61\u2a00\ud835\udc62\ud835\udc5d\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54(\ud835\udc53\u210e ?\u0303?)) (3)\nWhere \ud835\udc61 is the stage index, \ud835\udc53\ud835\udc59 ?\u0303? = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc631(\ud835\udc53\ud835\udc59 \ud835\udc61) represents the\ncompressed low-level features, \ud835\udc53\u210e ?\u0303? = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc631(\ud835\udc53\u210e \ud835\udc61) is the\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\n> JSTARS-2022-00888 <\ncompressed high-level features, \u2a00 denotes multiplication, \ud835\udeff denotes the RELU activation function, and \ud835\udc62\ud835\udc5d\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54 is the upsampling operation through bilinear interpolation.\nWe apply the same fusion strategy to the global feature \ud835\udc53\ud835\udc54. The same method is different from the aforementioned in that the weight matrix \ud835\udc4a\u210e \ud835\udc61 is generated by \ud835\udc53\u210e ?\u0303? through a 3\u00d73 convolution layer, and then the mask \ud835\udc4a\u210e \ud835\udc61 is multiplied to upsampling the compressed global features \ud835\udc53\ud835\udc54 ?\u0303?.\nThe final weighted-aggregating features \ud835\udc53\ud835\udc64 \ud835\udc61 , which can be characterized as the following, are created by combining these three level features and then passing them through a 1\u00d71 convolution layer \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc633:\n\ud835\udc53\ud835\udc64 \ud835\udc61 = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc633(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc53\ud835\udc59 ?\u0303? , \ud835\udc53\u210e\ud835\udc59 \ud835\udc61 , \ud835\udc53\ud835\udc54\u210e \ud835\udc61 )) (4)"
        },
        {
            "heading": "D. Boundary Enhancement Module",
            "text": "Boundary information is considerably more ambiguous than body information during the feature extraction process, and there are uncertain positions [58], [59]. Therefore, it is unreasonable not to consider boundary. Some previous works have directly fused the multi-scale and multi-level features and then upsampled them to obtain the final result map, and some works have trained the boundary as a single branch which increases the burden on a computer. We concluded that neither approach was reasonable. For the segmentation of boundaries, feature maps from multiple levels could be helpful, since features from the shallow layer of CNNs can provide spatial information threadlike objects such as the boundary, and information from the deep layer contains semantic information such as position information. Thus, this paper proposes a BEM based on multi-level layers to enhance the boundary information.\nThe BEM is mainly used to decrease lost boundary information by finding the differences between the global features extracted from the deep network and the low-level features extracted from the shallow network. BEM adds these differences to the final segmentation map. Here, we use upsampling of global feature \ud835\udc53\ud835\udc54 to obtain the body information, which gathers contextual information from within objects to create a distinct body for each object. We consider low-level features \ud835\udc53\ud835\udc59 1 to contain all of the information. There, boundary information \ud835\udc66\ud835\udc4f is obtained by subtracting \ud835\udc66\ud835\udc4f from \ud835\udc53\ud835\udc59 1 . Therefore, we can obtain lost boundary information in the encoding and decoding process and add it to the prediction map to achieve the purpose of information enhancement. The process can be written as follows:\n\ud835\udc66\ud835\udc4f = \ud835\udefe(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63(\ud835\udc53\ud835\udc59 1) \u2212 \ud835\udc62\ud835\udc5d\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63(\ud835\udc53\ud835\udc54))) (5)\nWhere \ud835\udefe is a convolution layer and upsampling operation, and conv denotes a convolution layer with BatchNorm and RELU activation function. \ud835\udc62\ud835\udc5d\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54 is the upsampling operation through bilinear interpolation.\nThe final prediction map is jointly obtained by the \ud835\udc66\ud835\udc4f and the fusion feature information \ud835\udc53\ud835\udc64 3 , which can be calculated as follows:\n?\u0302? = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc634(\ud835\udc66\ud835\udc4f + \ud835\udc53\ud835\udc64 3) (6)\nWhere \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc634 denotes a convolution layer with BatchNorm and RELU activation function."
        },
        {
            "heading": "E. Mixed Loss Function",
            "text": "This paper uses a mixed loss function, which not only supervises the final segmentation graph, but also jointly supervises \ud835\udc53\ud835\udc64 \ud835\udc61 obtained by the MFW, which allows us to efficiently train with the training samples. To train the MFBENet end to end, the total loss function of this paper can be computed as follows:\n\ud835\udc59\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \u2211 \ud835\udc59\ud835\udc53\ud835\udc64 3 \ud835\udc56=1 + \ud835\udc59y (7)\nWhere \u2211 \ud835\udc59\ud835\udc53\ud835\udc64 3 \ud835\udc56=1 denotes the weighted sum of the losses between the label map \ud835\udc66 and the segmentation map \ud835\udc53\ud835\udc64 \ud835\udc61 (\ud835\udc61 = 1,2,3) obtained via upsampling after MFW, and \ud835\udc59y is the loss between the label map \ud835\udc66 and the final segmentation map ?\u0302?. The loss functions of both parts are DiceLoss [60]. DiceLoss was proposed by Milletari et al., which does not need to give weights to samples of different classes to establish the ideal balance between foreground and background pixels. Furthermore, this paper chooses IoU and F1-score as the evaluation value indicators. Therefore, it is more appropriate to choose DiceLoss as the loss function in this paper, which can be formulated as follows:\n\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52(\ud835\udc66, ?\u0302?) = 1 \u2212 1\n\ud835\udc5a \u2211\n2 \u2211 \ud835\udc66\ud835\udc56\ud835\udc57?\u0302?\ud835\udc56\ud835\udc57 \ud835\udc41 \ud835\udc56=1\n\u2211 \ud835\udc66\ud835\udc56\ud835\udc57 2\ud835\udc41 \ud835\udc56=1 +\u2211 ?\u0302?\ud835\udc56\ud835\udc57 2\ud835\udc41 \ud835\udc56=1\n\ud835\udc5a \ud835\udc57=1 (8)\nWhere \ud835\udc5a is the number of categories, \ud835\udc41 is the number of pixels, and \ud835\udc66\ud835\udc56 is the true category of the prediction \ud835\udc66?\u0302?."
        },
        {
            "heading": "IV. EXPERIMENTS AND ANALYSES",
            "text": "To assess the effectiveness of the proposed framework, experiments were conducted on two HRRSI datasets. In this part, we first introduce the dataset, followed by the implementation details and accurate measurement, Finally, we compare our results with SOTA methods in Sections D and E."
        },
        {
            "heading": "A. Datasets",
            "text": "1) ISPRS Potsdam Dataset\nPotsdam is a typical historic city with large building blocks, narrow streets, and dense settlement structures. The Potsdam 2D semantic labeling challenge data are provided in the framework of the 2D semantic labeling contest organized by the International Society for Photogrammetry and Remote Sensing (ISPRS) Commission III.4 (http://www2.isprs.org/commissions/comm3/wg4/semanticlabeling.html) [61]. The Potsdam dataset has six categories (impervious surfaces (imp. surf.), building, low vegetation (low veg.), tree, car, and background) and 38 tiles with a size of 6,000\u00d76,000 pixels, whose spatial resolution is 5cm with uniform color and texture distributions. The Near Infrared, Red, Green, and Blue channels, the DSM, and the normalized DSM are all included in this dataset. We employ Red, Green and Blue channels as input for our networks (three dimensions). The 38 images were cropped into a patch of 521\u00d7521 pixels, and a total of 18,392 images were obtained, of which 12,874 were used for training, 3,678 were used for validation, and 1,840 were used\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\n> JSTARS-2022-00888 <\nfor testing according to the ratio of 7:2:1.\n2) GaoFen Image Dataset\nThe GaoFen Image Dataset (GID) was published by Tong et al. [62] for semantic segmentation of HRRSIs, which contains 150 HRRSIs of five categories (built-up, farmland, forest, meadow, and waters) and 15 HRRSIs of fifteen categories captured from more than 60 cities in China. Each original image is 7,200\u00d76,800, and the panchromatic band resolution is 4m. Twenty-eight images of five categories with good quality were selected as an experience dataset and cropped into a patch of 521\u00d7521 pixels, with a total of 18,900 images being obtained, of which 13,230 were used for training, 3,780 were used for validation, and 1,890 were used for testing according to the ratio of 7:2:1.\nTable I summarizes the detailed information of the datasets illustrated above, and Fig. 5 shows some examples of these datasets. As Fig. 5 shows, for GID, there are confusing objects and more high fragmentation, which pose an extra challenge for the labeling task.\nB. Implementation Details\nOur model is implemented using the PyTorch. The semantic segmentation model was optimized by Adam [63] with a beta1 of 0.9 and a beta2 of 0.999. The ResNet-50 used in the network was pretrained on ImageNet [64] to avoid overfitting. We trained the models with a total batch size of eight for 100 iterations. The initial learning rate is set at le-5 and the \u201cpoly\u201d policy in which the initial learning rate is multiplied by (1 \u2212 (\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f/max _\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f))\ud835\udc5d\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc5f with the power = 2 is employed to change the learning rate. On a server with NVIDIA GeForce RTX 3090 GPU accelerators (with 24GB GPU memory), all of the tests were conducted."
        },
        {
            "heading": "C. Accuracy Measurement",
            "text": "To assess the quantitative performance, two common and widely accepted metrics are utilized here. We adopt the accuracy evaluation algorithm of pixel-by-pixel labeling: mean intersection over union (MIoU), and the F1-score as the model performance evaluation criteria. We assume that there are K categories in HRRSIs to be segmented, where K includes the defined segmentation category and background. TP, FP, and FN represent the number of true positives, false positives, and false negatives, respectively.\nIntersection over union (IoU) is the ratio of the intersection and union of the prediction result and the real value of a certain category. MIoU is obtained by averaging based on IoU, as shown in the following formula:\n\ud835\udc3c\ud835\udc5c\ud835\udc48 = \ud835\udc47\ud835\udc43\n\ud835\udc39\ud835\udc41+\ud835\udc39\ud835\udc43+\ud835\udc47\ud835\udc43 (9)\nMIou= 1\n\ud835\udc3e \u2211 \ud835\udc3c\ud835\udc5c\ud835\udc48\ud835\udc3e\ud835\udc58=1 (10)\nThe harmonic mean of recall and precision is called the Fscore. We obtain the F1-score when setting the precision and recall to have the same weight. The F1-score has a range of 0 to 1, with a number closer to 1 indicating a superior model. The calculation formula is as follows:\n\ud835\udc391 \u2212 \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 = 2 \u00d7 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\u00d7\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 (11)\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43 (12)\nrecall = \ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc41 (13)\nTo evaluate the performance, we calculate the\naforementioned metrics in each dataset."
        },
        {
            "heading": "D. Experiments on the Potsdam Dataset",
            "text": "In this part, we conduct experiments comparing our model with the SOTA network on the Potsdam dataset. We have chosen FCN, U-Net, PSPNet, Deeplabv3+, ScasNet and HRCNet as benchmark approaches. FCN, U-Net, PSPNet and Deeplabv3+ are the popular network in natural scene imagery segmentation and are transferred learning to various fields, and\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\n> JSTARS-2022-00888 <\nScasNet and HRCNet have also achieved success in HRRSI segmentation. ScasNet and HRCNet are as follows:\n1) ScasNet: This model was proposed by Liu et al. in 2018 [28]. ScasNet was proposed to solve confusing, manmade objects\u2019 and intricate, finely structured objects\u2019 problems regarding coherent and accurate labeling, as well as improving labeling coherence with sequential global-to-local context aggregation. Technically, multi-scale contexts are captured in the output of a CNN encoder, and then are successively aggregated in a self-cascaded manner. Meanwhile, for finely structured objects, ScasNet boosts labeling accuracy with a coarse-to-fine refinement strategy.\n2) HRCNet: This model was proposed by Xu et al. in 2021 [65]. In order to solve the imbalance of category scale and uncertain boundary information, HRCNet contains highresolution network (HRNet) lightweight dual attention (LDA) and boundary awareness (BA). HRNet was adopted to retain spatial information. LDA was designed to obtain global contextual information in the feature extraction stage, and the feature enhancement feature pyramid (FEFP) structure is promoted and employed to fuse the contextual information of different scales. BA is combined with the boundary-aware loss function to achieve boundary information.\nUsing MIoU and the F1-score as the reference, Table II presents the semantic segmentation performance of our method and benchmark approaches, and it is clear that our model outperforms them all in terms of MIoU and F1-score. Our model achieves an MIoU of 0.956 and an F1-score of 0.978. Since FCN ignores multi-scale feature fusion, the extraction of small objects (such as cars) is poor. We also can find that UNet cannot benefit from semantic supervision efficiently. Convolutional kernels have fixed receptive fields, which makes it difficult for FCN and U-Net to effectively collect visual context information. PSPNet and DeepLabv3+ ignore the differences in features and lack consideration of boundary information. Although the overall segmentation effect is better than with FCNs and UNet, the improvement in small target\nsegmentation is not obvious. In general, a multi-scale structure is necessary to extract objects of varying sizes. As shown in Table II, ScasNet is beneficial to small objects, especially buildings and cars, which can achieve competitive performance on the Potsdam dataset. However, multi-level information is just as important as multi-scale information, and it is also necessary to process the boundary information separately. Our MFW considers the above, while ScasNet only uses multi-scale information within the models. Therefore, overall ScasNet is inferior to our model. Similar to HCRNet, our model contains the MFW and BEM to aggregate multi-scale features and enhance the boundary, respectively. By contrast, the performance of HRCNet is not so fine, because it only supervises the training through the boundary loss function, and its effect is not even as good as that of ScasNet.\nTo assess the qualitative performance, we also visualized the benchmark and our proposed MFBE-Net results, as shown in Fig. 6. To zoom in on a more understandable comparison with the other methods, four 1,000\u00d71,000 pixel regions are randomly chosen. For HRRSIs, U-Net does not perform as well as in the field of medical images, and there are many misclassifications, even fragmentation (first row). In the first row, the performance of each network is not good. There are discontinuities in all comparing models, and the boundaries are not extracted completely. In contrast, MFBE-Net can produce images with better continuity (first row, third row), which also shows that multi-scale feature weighted aggregating is more efficient in detail recovery and image smoothing. Although our model does\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\n> JSTARS-2022-00888 <\nnot fully recover the boundary loss, our model outperforms them in comparison. Among the six groups of control experiments, ScasNet is the most outstanding, and the details are handled well on the whole in the remaining sets of maps, but there are some ambiguities and uncertainties boundary extraction. HRCNet, which is dedicated to solving the problem of boundary ambiguity, is slightly better than other models in boundary processing, but the overall accuracy is not ideal. Our model has added a boundary enhancement module to generate sharper boundaries. For example, the building edge (second row, fourth row) is very easy to confuse and bend. However, the MFBE-Net succeeds in segmenting it. Overall, our model has better segmentation than that of other models, with clear boundaries and complete extraction of small objects. The quantitative and qualitative results both support the effectiveness of the MFBE-Net at the system level."
        },
        {
            "heading": "E. Experiments on GID",
            "text": "We also performed benchmark experiments on another significant HRRSIs GID to further assess the proposed framework. Since GID has more complex and various-sized objects, the difficulty of the segmentation task has increased. The experimental results are provided in Table III. Our model achieved the best performance with an MIoU of 0.950 and an F1-score of 0.974, exceeding the performance of other models. We can find the same evidence that U-Net still has the lowest accuracy, probably because there are more categories of HRRSIs in comparison with medical images. Compared with ScasNet, which is the best among other models, the MFBE-Net further improves MIoU by 4.28% and the F1-score by 3.29%. Experimental results confirm the effectiveness of the MFBENet again. In addition, compared with the Potsdam dataset, GID has larger coverage and greater intraclass variation, which poses a great challenge for HRRSI segmentation. For example,\nthe FCN, DeepLabv3+, PSPNet, and ScasNet are not as effective in GID as the Potsdam dataset. However, our model significantly exceeds other models and maintains robustness. Even for complex backgrounds, our model is also well segmented. Overall, our model is superior to these models as always, which also shows the robustness of our model and the importance of MFW and BEM.\nThe six meticulous visualization results are displayed in Fig. 7 to exhibit a micro-level visual performance. We also select the FCN, U-Net, DeepLabv3+, PSPNet, ScasNet and HRCNet for comparison. We can find that on the whole our model and ScasNet detail spatial information more accurately, and obtain results closer to the real truth. For narrow targets, our model achieved the best segmentation results (first row, second row). For boundary information that is difficult to extract, our model can also handle it well (third line). In the extraction of the builtup, our model and ScasNet achieved good results, which also proves the importance of multi-scale information for small object extraction. In contrast, the FCN, U-Net, and DeepLabv3+ all face the problems of blurry boundary information and small object extraction.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\n> JSTARS-2022-00888 <"
        },
        {
            "heading": "V. DISCUSSION",
            "text": ""
        },
        {
            "heading": "A. Computational Cost Analysis",
            "text": "Model accuracy should be measured, but also model size and inference speed should be taken into account. It is important to consider the optimal trade-off between accuracy, computational efficiency, and the number of operations measured by the floating point operations per second (FLOPs) and the number of parameters. To evaluate the required amount of calculation resources of the compared models, the number of parameters, inference time for a single image and Flops of our method and comparative methods are shown in Table IV.\nAs we can see from Table IV, our proposed network has low parameters, medium FLOPs and inference time, which are 170.78G, 51.32M and 54ms in Potsdam, 52ms in GID, respectively. For inference time of a single figure, the same model performs slightly less well than GID on the Potsdam dataset. The reason is the resolution of remote sensing images in the Potsdam dataset is higher than that of GID. In addition, combing Table IV with Table II and Table III, although the FCN showed much better inference speed, its accuracy is not appropriate for semantic segmentation. ScasNet showed better accuracy results, but it required more than 321.11G FLOPs. HRCNet is slow in inference speed and not appropriate for realtime semantic segmentation. It can be observed that our method can achieve better performances with other models on most objects while performing slightly worse on computational cost. In contrast, our model can better meet the higher requirements of semantic segmentation on model deployment and application."
        },
        {
            "heading": "B. Ablation Experiment",
            "text": "In this part, we conduct an ablation experiment to confirm the efficiency of each crucial element of the proposed MFBENet. The ablation study is conducted on the Potsdam dataset. As a baseline, we train U-Net, which uses Resnet-50 as the encoder. Thereafter, each module is added progressively. We conduct experiments on those architectures and report the performance via MIoU and the F1-score. As shown in Table V, we can find that MIoU increased by 11.1% and the F1-score by 11.4% after MFW replaced the sample concatenation operation. Meanwhile, as we expected, the MFW is beneficial in the extraction of small objects, such as low vegetation, trees, and cars, which show a noticeable boost in performance. In addition, the addition of the BEM increased MIoU from 0.840 to 0.956, and the F1-score by 8.91%. The accuracy improvement for each category proves the effectiveness of the BEM in the MFBE-Net\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\n> JSTARS-2022-00888 <\nfor HRRSI segmentation. Finally, the combination of the MFW and BEM has already produced the best results.\nWe visualize the result maps generated by the ablation study in Fig. 8 to better understand the effect of the MFW and BEM, where we can see that with the addition of the MFW, the recognition of small targets is easier than the baseline, such as small-scale impervious surfaces (fourth row), and the identification of false positives could be reduced with the proposed modules (third row). Moreover, the BEM makes the object boundary smoother and more accurate (first row, second row). This implies that the BEM has improved the occurrence of wrong segmentation and image connectivity problems. To sum up, our proposed modules can better extract small targets and boundaries. To obtain the best segmentation of HRRSIs, each component in the proposed model is necessary."
        },
        {
            "heading": "C. Model Analysis",
            "text": "Compared with other semantic segmentation methods, our model outperforms other methods in each category and overall segmentation performance. This is mainly attributable to the following points. Multi-scale feature weighted aggregating and boundary enhancement are both key factors that influence how well segmentation performs. These factors are taken into account by the MFBE-Net, resulting in successful semantic segmentation. Additionally, our model improves the learning process by utilizing intermediary loss functions, where intermediate loss supervises the backpropagation process, by defining how poorly the network performs at the intermediary layers of the network, rather than using a single loss function at the end of the network.\nThe attention mechanism supports the network's ability to concentrate on key discriminative regions in the images by giving such areas higher weights while suppressing redundant and unimportant regions such as backgrounds. However, the attention mechanism greatly increases the complexity of the model, which leads to extra computational costs and more training times. In the future, we will carry out further research to reduce the computational complexity of MFBE-Net for better prospects."
        },
        {
            "heading": "D. Generalizability to Uncertainties",
            "text": "The generalizability of the model when the image quality is poor is discussed in this part. HRRSI segmentation becomes more challenging when some key objects are invisible, or suppressed due to their size, shadow, occlusion from the surrounding objects, or where the background suppresses the objects of interest. The results shown in Fig. 9 come from some areas where there are some quality issues (e.g., shadow, mosaic, image distortion) either on image or ground truth. In most instances, our model can correctly predict the category of the\narea with poor imaging quality. For instance, in the first row of Fig. 9, cars are occluded by building shadows, but our model correctly discriminates shadows, buildings and low vegetation. In the second row of Fig. 9, there is a huge region covered with mosaic, and the region is not labeled as impervious surface or background but building. Although our network prediction result is consistent with the label, it should not be the building according to its true category.\nAlthough the proposed model attains competitive accuracy results, shadow detection, alignment, and correction for HRRSI segmentation remain a great area of interest requiring further attention to mitigate shadow-prone errors. Finally, blending HRRSIs multi-band information and making full use of spectral features requires further attempts to obtain optimum results."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "Most previous HRRSI segmentation works have ignored the difference in multi-scale features and have especially not considered boundary information, which makes it difficult to extract small targets and make boundary positioning accurate and information complete. In this paper, we propose a multiscale and multi-level semantic segmentation network for HRRSI segmentation, which performs excellently in small object extraction and boundary refinement. To fully utilize multi-scale features and their differences, we design an MFW to use low-level features to weigh and fuse high-level features, which improves detail recovery and small object extraction by establishing the relationship between multi-scale features. The MFW is more effective than the general concatenation. Aiming at the problem of blurred boundaries and uncertain positioning in the encoding process, a BEM is introduced in our network to enhance boundary features which utilize multi-level layer features to retrieve a loss of boundary information to generate more accurate boundaries. The detailed ablation study suggests that the MFW and BEM are of significant importance for semantic segmentation. The effectiveness and superiority of the MFBE-Net are thoroughly evaluated in comparison to two different HRRSIs, the Potsdam datasets and GID. Compared to existing algorithms, our network achieved the best results locally and globally. Future experiments are necessary to validate the proposed framework for HRRSIs segmentation in\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\n> JSTARS-2022-00888 <\nother datasets and confirm the effectiveness of the proposed MFW and BEM in different networks."
        }
    ],
    "title": "Multi-scale Feature Weighted-Aggregating and Boundary Enhancement Network for Semantic Segmentation of High-Resolution Remote Sensing Images",
    "year": 2022
}