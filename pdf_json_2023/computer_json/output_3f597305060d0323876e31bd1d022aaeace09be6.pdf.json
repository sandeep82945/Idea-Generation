{
    "abstractText": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Most existing SBR studies model the user preferences based only on the current session while neglecting the item-transition information from the other sessions, which suffer from the inability of modeling the complicated item-transition pattern. To address the limitations, we introduce global item-transition information to strength the modeling of the dynamic item-transition. For fully exploiting the global item-transition information, two ways of exploring global information for SBR are studied in this work. Specifically, we first propose a basic GNN-based framework (BGNN), which solely uses session-level item-transition information on session graph. Based on BGNN, we propose a novel approach, called Session-based Recommendation with Global Information (SRGI), which infers the user preferences via fully exploring global item-transitions over all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM), which recursively incorporates the neighbor embeddings of each node on global graph into the learning process of session-level item representation; and (ii) Constrained-based Model (SRGI-CM), which treats the global-level item-transition information as a constraint to ensure the learned item embeddings are consistent with the global item-transition. Extensive experiments conducted on three popular benchmark datasets demonstrate that both SRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziyang Wang"
        },
        {
            "affiliations": [],
            "name": "Wei Wei"
        },
        {
            "affiliations": [],
            "name": "Gao Cong"
        },
        {
            "affiliations": [],
            "name": "Xiao-Li Li"
        },
        {
            "affiliations": [],
            "name": "Xian-Ling Mao"
        },
        {
            "affiliations": [],
            "name": "Minghui Qiu"
        }
    ],
    "id": "SP:d01c25540f7ba5dd0304e1c60244c1502a7fa29d",
    "references": [
        {
            "authors": [
                "Z. Wang",
                "W. Wei",
                "G. Cong",
                "X.-L. Li",
                "X.-L. Mao",
                "M. Qiu"
            ],
            "title": "Global context enhanced graph neural networks for sessionbased recommendation",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 169\u2013178.",
            "year": 2020
        },
        {
            "authors": [
                "A. Mnih",
                "R.R. Salakhutdinov"
            ],
            "title": "Probabilistic matrix factorization",
            "venue": "Advances in neural information processing systems, 2008, pp. 1257\u20131264.",
            "year": 2008
        },
        {
            "authors": [
                "S. Kabbur",
                "X. Ning",
                "G. Karypis"
            ],
            "title": "Fism: factored item similarity models for top-n recommender systems",
            "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 659\u2013667.",
            "year": 2013
        },
        {
            "authors": [
                "C.-K. Hsieh",
                "L. Yang",
                "Y. Cui",
                "T.-Y. Lin",
                "S. Belongie",
                "D. Estrin"
            ],
            "title": "Collaborative metric learning",
            "venue": "Proceedings of the 26th international conference on world wide web, 2017, pp. 193\u2013201.",
            "year": 2017
        },
        {
            "authors": [
                "S. Rendle",
                "C. Freudenthaler",
                "L. Schmidt-Thieme"
            ],
            "title": "Factorizing personalized markov chains for next-basket recommendation",
            "venue": "Proceedings of the 19th international conference on World wide web, 2010, pp. 811\u2013820.",
            "year": 2010
        },
        {
            "authors": [
                "X. He",
                "L. Liao",
                "H. Zhang",
                "L. Nie",
                "X. Hu",
                "T.-S. Chua"
            ],
            "title": "Neural collaborative filtering",
            "venue": "Proceedings of the 26th international conference on world wide web, 2017, pp. 173\u2013182.",
            "year": 2017
        },
        {
            "authors": [
                "D. Liang",
                "R.G. Krishnan",
                "M.D. Hoffman",
                "T. Jebara"
            ],
            "title": "Variational autoencoders for collaborative filtering",
            "venue": "Proceedings of the 2018 world wide web conference, 2018, pp. 689\u2013698.",
            "year": 2018
        },
        {
            "authors": [
                "X. Li",
                "M. de Rijke",
                "Y. Liu",
                "J. Mao",
                "W. Ma",
                "M. Zhang",
                "S. Ma"
            ],
            "title": "Learning better representations for neural information retrieval with graph information",
            "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2020, pp. 795\u2013804.",
            "year": 2020
        },
        {
            "authors": [
                "B. Hidasi",
                "A. Karatzoglou",
                "L. Baltrunas",
                "D. Tikk"
            ],
            "title": "Sessionbased recommendations with recurrent neural networks",
            "venue": "ICLR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "D. Tarlow",
                "M. Brockschmidt",
                "R. Zemel"
            ],
            "title": "Gated graph sequence neural networks",
            "venue": "ICLR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Wang",
                "L. Hu",
                "Y. Wang",
                "Q.Z. Sheng",
                "M. Orgun",
                "L. Cao"
            ],
            "title": "Modeling multi-purpose sessions for next-item recommendations via mixture-channel purpose routing networks",
            "venue": "IJCAI, 2019, pp. 3771\u20133777.",
            "year": 2019
        },
        {
            "authors": [
                "J. Li",
                "P. Ren",
                "Z. Chen",
                "Z. Ren",
                "T. Lian",
                "J. Ma"
            ],
            "title": "Neural attentive session-based recommendation",
            "venue": "CIKM, 2017, pp. 1419\u20131428.",
            "year": 2017
        },
        {
            "authors": [
                "Q. Liu",
                "Y. Zeng",
                "R. Mokhosi",
                "H. Zhang"
            ],
            "title": "Stamp: short-term attention/memory priority model for session-based recommendation",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, pp. 1831\u2013 1839.",
            "year": 2018
        },
        {
            "authors": [
                "S. Wu",
                "Y. Tang",
                "Y. Zhu",
                "L. Wang",
                "X. Xie",
                "T. Tan"
            ],
            "title": "Session-based recommendation with graph neural networks",
            "venue": "AAAI, 2019, pp. 346\u2013353.",
            "year": 2019
        },
        {
            "authors": [
                "R. Qiu",
                "J. Li",
                "Z. Huang",
                "H. Yin"
            ],
            "title": "Rethinking the item order in session-based recommendation with graph neural networks",
            "venue": "CIKM, 2019, pp. 579\u2013588.",
            "year": 2019
        },
        {
            "authors": [
                "F. Yu",
                "Y. Zhu",
                "Q. Liu",
                "S. Wu",
                "L. Wang",
                "T. Tan"
            ],
            "title": "Tagnn: Target attentive graph neural networks for session-based recommendation",
            "venue": "arXiv preprint arXiv:2005.02844, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Lio",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "ICLR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Wang",
                "P. Ren",
                "L. Mei",
                "Z. Chen",
                "J. Ma",
                "M. de Rijke"
            ],
            "title": "A collaborative session-based recommendation approach with parallel memory modules",
            "venue": "SIGIR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Luo",
                "P. Zhao",
                "Y. Liu",
                "F. Zhuang",
                "D. Wang",
                "J. Xu",
                "J. Fang",
                "V.S. Sheng"
            ],
            "title": "Collaborative self-attention network for sessionbased recommendation",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI- 20. International Joint Conferences on Artificial Intelligence Organization, 7 2020, pp. 2591\u20132597. [Online]. Available: https://doi.org/10.24963/ijcai.2020/359",
            "year": 2020
        },
        {
            "authors": [
                "G. Shani",
                "D. Heckerman",
                "R.I. Brafman"
            ],
            "title": "An mdp-based recommender system",
            "venue": "2005, pp. 1265\u20131295.",
            "year": 2005
        },
        {
            "authors": [
                "P. Ren",
                "Z. Chen",
                "J. Li",
                "Z. Ren",
                "J. Ma",
                "M. de Rijke"
            ],
            "title": "Repeatnet: A repeat aware neural recommendation machine for session-based recommendation",
            "venue": "AAAI, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y.K. Tan",
                "X. Xu",
                "Y. Liu"
            ],
            "title": "Improved recurrent neural networks for session-based recommendations",
            "venue": "Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016, pp. 17\u201322.",
            "year": 2016
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NIPS, 2017, pp. 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "W.-C. Kang",
                "J. McAuley"
            ],
            "title": "Self-attentive sequential recommendation",
            "venue": "ICDM, 2018, pp. 197\u2013206.",
            "year": 2018
        },
        {
            "authors": [
                "J. Song",
                "H. Shen",
                "Z. Ou",
                "J. Zhang",
                "T. Xiao",
                "S. Liang"
            ],
            "title": "Islf: Interest shift and latent factors combination model for sessionbased recommendation",
            "venue": "IJCAI, 2019, pp. 5765\u20135771.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Pu",
                "Z. Gan",
                "R. Henao",
                "X. Yuan",
                "C. Li",
                "A. Stevens",
                "L. Carin"
            ],
            "title": "Variational autoencoder for deep learning of images, labels and captions",
            "venue": "Advances in neural information processing systems, 2016, pp. 2352\u20132360.",
            "year": 2016
        },
        {
            "authors": [
                "S. Wang",
                "L. Hu",
                "Y. Wang",
                "L. Cao",
                "Q.Z. Sheng",
                "M. Orgun"
            ],
            "title": "Sequential recommender systems: Challenges, progress and prospects",
            "venue": "IJCAI, 2019, pp. 6332\u20136338.",
            "year": 2019
        },
        {
            "authors": [
                "C. Xu",
                "P. Zhao",
                "Y. Liu",
                "V.S. Sheng",
                "J. Xu",
                "F. Zhuang",
                "J. Fang",
                "X. Zhou"
            ],
            "title": "Graph contextualized self-attention network for sessionbased recommendation",
            "venue": "IJCAI, 2019, pp. 3940\u20133946.",
            "year": 2019
        },
        {
            "authors": [
                "W. Meng",
                "D. Yang",
                "Y. Xiao"
            ],
            "title": "Incorporating user microbehaviors and item knowledge into multi-task learning for session-based recommendation",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 1091\u20131100.",
            "year": 2020
        },
        {
            "authors": [
                "B. Sarwar",
                "G. Karypis",
                "J. Konstan",
                "J. Riedl"
            ],
            "title": "Item-based collaborative filtering recommendation algorithms",
            "venue": "Proceedings of the 10th international conference on World Wide Web, 2001, pp. 285\u2013295.",
            "year": 2001
        },
        {
            "authors": [
                "D. Jannach",
                "M. Ludewig"
            ],
            "title": "When recurrent neural networks meet the neighborhood for session-based recommendation",
            "venue": "Proceedings of the Eleventh ACM Conference on Recommender Systems, 2017, pp. 306\u2013310.",
            "year": 2017
        },
        {
            "authors": [
                "B. Perozzi",
                "R. Al-Rfou",
                "S. Skiena"
            ],
            "title": "Deepwalk: Online learning of social representations",
            "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701\u2013710.",
            "year": 2014
        },
        {
            "authors": [
                "A. Grover",
                "J. Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855\u2013864.",
            "year": 2016
        },
        {
            "authors": [
                "J. Qiu",
                "Y. Dong",
                "H. Ma",
                "J. Li",
                "K. Wang",
                "J. Tang"
            ],
            "title": "Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec",
            "venue": "Proceedings of the eleventh ACM international conference on web search and data mining, 2018, pp. 459\u2013467.",
            "year": 2018
        },
        {
            "authors": [
                "P. Velickovic",
                "W. Fedus",
                "W.L. Hamilton",
                "P. Li\u00f2",
                "Y. Bengio",
                "R.D. Hjelm"
            ],
            "title": "Deep graph infomax.",
            "venue": "ICLR (Poster),",
            "year": 2019
        },
        {
            "authors": [
                "Z. Peng",
                "W. Huang",
                "M. Luo",
                "Q. Zheng",
                "Y. Rong",
                "T. Xu",
                "J. Huang"
            ],
            "title": "Graph representation learning via graphical mutual information maximization",
            "venue": "Proceedings of The Web Conference 2020, 2020, pp. 259\u2013270.",
            "year": 2020
        },
        {
            "authors": [
                "Y. You",
                "T. Chen",
                "Y. Sui",
                "T. Chen",
                "Z. Wang",
                "Y. Shen"
            ],
            "title": "Graph contrastive learning with augmentations",
            "venue": "arXiv preprint arXiv:2010.13902, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Zhu",
                "Y. Xu",
                "F. Yu",
                "Q. Liu",
                "S. Wu",
                "L. Wang"
            ],
            "title": "Graph contrastive learning with adaptive augmentation",
            "venue": "arXiv preprint arXiv:2010.14945, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "P. Gupta",
                "D. Garg",
                "P. Malhotra",
                "L. Vig",
                "G. Shroff"
            ],
            "title": "Niser: Normalized item and session representations with graph neural networks",
            "venue": "arXiv preprint arXiv:1909.04276, 2019. JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14",
            "year": 1909
        },
        {
            "authors": [
                "F. Wang",
                "X. Xiang",
                "J. Cheng",
                "A.L. Yuille"
            ],
            "title": "Normface: L2 hypersphere embedding for face verification",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1041\u2013 1049.",
            "year": 2017
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "NIPS, 2017, pp. 1024\u20131034.",
            "year": 2017
        },
        {
            "authors": [
                "E. Zangerle",
                "M. Pichl",
                "W. Gassler",
                "G. Specht"
            ],
            "title": "nowplaying music dataset: Extracting listening behavior from twitter",
            "venue": "ISMM, 2014, pp. 21\u201326.",
            "year": 2014
        },
        {
            "authors": [
                "J. Weston",
                "S. Chopra",
                "A. Bordes"
            ],
            "title": "Memory networks",
            "venue": "arXiv preprint arXiv:1410.3916, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Srivastava",
                "G. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "JMLR, pp. 1929\u20131958, 2014.",
            "year": 1929
        }
    ],
    "sections": [
        {
            "text": "Exploring Global Information for Session-based Recommendation\nZiyang Wang, Wei Wei\u2020, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu\nAbstract\u2014Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Most existing SBR studies model the user preferences based only on the current session while neglecting the item-transition information from the other sessions, which suffer from the inability of modeling the complicated item-transition pattern. To address the limitations, we introduce global item-transition information to strength the modeling of the dynamic item-transition. For fully exploiting the global item-transition information, two ways of exploring global information for SBR are studied in this work. Specifically, we first propose a basic GNN-based framework (BGNN), which solely uses session-level item-transition information on session graph. Based on BGNN, we propose a novel approach, called Session-based Recommendation with Global Information (SRGI), which infers the user preferences via fully exploring global item-transitions over all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM), which recursively incorporates the neighbor embeddings of each node on global graph into the learning process of session-level item representation; and (ii) Constrained-based Model (SRGI-CM), which treats the global-level item-transition information as a constraint to ensure the learned item embeddings are consistent with the global item-transition. Extensive experiments conducted on three popular benchmark datasets demonstrate that both SRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.\nIndex Terms\u2014Session-based Recommendation, Graph Neural Network, Graph Contrastive Learning\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "With the explosion of information on the Internet, recommendation systems play an increasingly important role as they recommend useful content to address the information overload problem. Conventional recommendation approaches (e.g., collaborative filtering [2], [3], [4]) usually make predictions based on the user profiles and longterm interaction history. However, in many recent realworld scenarios (e.g., on-line shopping platform1 and mobile stream media2), such information may not exist. In addition, users\u2019 recent behaviors indicate their current interests, which is neglected by these methods. To bridge the gaps in conventional recommendation approaches, session-based recommendation (SBR) is emerged to predict the next action (e.g., click) of users based on anonymous behavior sequences in chronological order.\nDue to its highly practical value, SBR has attracted increasing attention and many approaches have been pro-\nThis paper is an extended version of the SIGIR \u201920 conference paper [1]. Wei Wei is the corresponding author.\n\u2022 Ziyang Wang and Wei Wei was with Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology,Huazhong University of Science and Technology, Wuhan, China. E-mail: {ziyang1997,weiw}@hust.edu.cn. \u2022 Gao Cong was with School of Computer and Engineering, Nanyang Technological University, Singapore. E-mail: gaocong@ntu.edu.sg. \u2022 Xiao-Li Li was with Institute for Infocomm Research, Singapore. E-mail: xlli@i2r.a-star.edu.sg. \u2022 Xian-Ling Mao was with School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China. E-mail: maoxl@bit.edu.cn. \u2022 Minghui Qiu was with Alibaba Group, Hangzhou, China. E-mail: minghuiqiu@gmail.com.\n1. https://www.amazon.com/ 2. https://www.tiktok.com/\nposed. Most of early studies on session-based recommendation are based on Markov-chains [5], which predict the users\u2019 next interest solely based on the previous action. However, these methods rely on strong independence assumption, which may suffer from noisy data and intractable computation problem for real-world applications.\nRecent years have witnessed the rapid development of deep learning techniques [6], [7], [8] and many neural networks based approaches are proposed for SBR, which model the item-transition within the current session and can be roughly categorized into two types, i.e., RNN-based and GNN-based. The former [9], [10], [11] regard SBR as a sequence modeling task and leverage recurrent neural networks (RNN) to capture the item-transition information in chronological order, which is then extended with attention network [12] and memory network [13]. The latter [14], [15], [16] focus on capturing the dependencies between each item and its context (i.e., adjacent items) in the session, which first convert each session into a subgraph and then employ graph neural network (e.g., GGNN [10], GAT [17]) to extract the item features from the graph.\nAlthough encouraging results are achieved, these methods still suffer from the inability of modeling the complicated inherent order of item-transition for each session, due to the noisy data and limited session information. Recently, some efforts [18], [19] are devoted to incorporating collaborative information into SBR to strength the modeling of item-transition. Specifically, CSRM [18] fuses the latest m sessions to enrich the representation of the current session via memory network. CoSAN [19] injects feature embedding represented by neighborhood sessions to enrich the item representation in the current session. However, they may unfortunately encode both relevant\nar X\niv :2\n01 1.\n10 17\n3v 2\n[ cs\n.I R\n] 1\nJ un\n2 02\n1\nand irrelevant information of the other sessions into the current session representation, which may even deteriorate the performance [11]. To illustrate this, an example is shown in Figure 1. Without loss of generality, suppose the current session is \u201cSession 3\u201d, and the session-based recommendation aims to recommend the next interest of the user based on the past three clicked items. From Figure 1, we observe that: (i) Utilizing the item-transition of the other sessions might help model the user preference of the current session. For example, from \u201cSession 1\u201d and \u201cSession 2\u201d we can find relevant items from item-transition information for \u201cIphone\u201d, such as \u201cHUAWEI Mate\u201d and \u201cOnePlus\u201d. and (ii) Directly utilizing the item-transition information of the entire other session may introduce noise when part of the item-transition information encoded in such session is not relevant to the current session. For instance, CSRM [18] and CoSAN [19] may also consider to utilize \u201cSession 1\u201d and \u201cSession 2\u201d to help modeling the user preference of \u201cSession 3\u201d, which will introduce the irrelevant items (i.e., \u201cLEGO\u201d and \u201cElectric Toothbrush\u201d) when learning \u201cSession 2\u201d\u2019s embedding as it treats other session as a whole without distinguishing relevant item-transition from irrelevant itemtransition, which is challenging.\nTo this end, we propose a novel approach to exploit the item-transitions over all sessions from two perspectives for better inferring the user preference of the current session for session-based recommendation, which is named Sessionbased Recommendation with Global Information (SRGI). In SRGI, we propose to learn two levels of item feature from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item feature by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item feature by modeling pairwise item-transitions over all sessions (including the current session). SRGI first employs a basic GNN framework (B-GNN) on the session graph to learn session-level item embedding within the current session from three kinds of relations. Then based on the constructed global graph, we present two versions of SRGI that incorporates the global transition information into SBR: (i) Fusion-based Model (SRGI-FM), which directly incorporates the neighbors\u2019 features of each node on the global graph through a session-aware attention mechanism to enrich the features of current session; and (ii) Constraint-based Model (SRGI-\nCM), which preserves the global proximity to ensure the learnt item embeddings are consistent with the global itemtransition, via graph contrastive learning.\nThe main contributions of this work are summarized as follows: \u2022 We propose a basic GNN-based session recommenda-\ntion framework for SBR, which employs graph neural networks layer to explicitly extracting item features derived from three kinds of relations on session-level graph. \u2022 We propose a novel unified model (named SRGI) to fully and effectively explore the pairwise itemtransition information from two levels of graph models, i.e., session graph and global graph from two different aspects, i.e., Fusion-based Model that circularly incorporates the global neighbor information into the item embedding learning process for enhancing the sessionlevel item representations, and Constraint-based Model treats the global-level item-transition information as a constraint and employ graph contrastive learning to ensure the learnt item embeddings are consistent with the global graph structure; \u2022 We conduct extensive experiments on three real-world datasets, which demonstrate the efficacy of SRGI-FM and SRGI-CM over state-of-the-art baselines."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Session-based Recommendation",
            "text": "Many research efforts have been conducted on sessionbased recommendation, which will be reviewed in this section.\nMarkov Chain-based SBR. Several traditional methods can be employed for SBR although they are not originally designed for SBR. For example, Markov Chain-based methods map each session into a Markov chain, where the user\u2019s next action is inferred based on the previous one. Shani et al. [20] utilize Markov Decision Processes (MDP) to model the transitions of items within a session, where simplest MDP boils down to first-order Markov chains [21]. Rendle et al. [5] propose FPMC to capture both sequential patterns and long-term user preference by a hybrid method based on the combination of matrix factorization and first-order Markov chain for recommendation. It can be adapted for SBR by ignoring the user latent representation as it is not available for anonymous SBR. However, MC-based methods usually focus on modeling sequential transition of two adjacent items. In contrast, our proposed model converts the sequentially item-transitions into graph-structure data for capturing the inherent order of item-transition patterns for SBR.\nDeep-learning based SBR. In recent years, neural networkbased methods that are capable of modeling sequential data have been utilized for SBR. Hidasi et al. [9] propose the first work called GRU4REC to apply the RNN networks for SBR, which adopts a multi-layer Gated Recurrent Unit (GRU) to model item interaction sequences. Then, Tan et al. [22] extend the method [9] by introducing data augmentation. Li et al. [12] propose NARM that incorporates attention mechanism into stack GRU encoder to capture\nthe more representative item-transition information for SBR. Liu et al. [13] propose an attention-based short-term memory networks (named STAMP) to captures the user\u2019s current interest without using RNN. Both NARM and STAMP emphasize the importance of the last click by using attention mechanism. Inspired by Transformer [23], SASRec [24] stacks multiple layers to capture the relevance between items. ISLF [25] takes into account the user\u2019s interest shift, and employs variational auto-encoder (VAE) [26] and RNN to capture the user\u2019s sequential behavior characteristics for SBR. MCPRN [11] proposes to model the multi-purpose of a given session by using a mixture-channel model for SBR. However, similar to MC-based methods, RNN-based methods focus on modeling the sequential transitions of adjacent items [27] to infer user preference via the chronology of the given sequence, and thus cannot model the complex itemtransition patterns (e.g., non-adjacent item transitions).\nRecently, several proposals employ GNN-based model on graph built from the current session to learn item embeddings for SBR. Wu et al. [14] convert each session into a graph and leverage gated GNN to the explore the complex transitions among items. Following the success of SR-GNN, some variants are also proposed for SBR, such as GCSAN [28], which combine GNN layers and self attention layers to learn the dependencies within the session. Qiu et al. [15] propose FGNN to learn each item representation by aggregating its neighbors\u2019 embeddings with multi-head attention. Yu et al. [16] leverage target-aware attention to dynamically obtain the importance of each item within the session. Meng et al. [29] incorporate knowledge graph to capture the dependencies between items. However, all these approaches only model the item-transition information on the current session. In contrast, our proposed model learns the item-transition information over all sessions to enhance the modeling of item-transition and inference of user interests.\nCollaborative Filtering-based SBR. Although deep learning based methods have achieved remarkable performance, collaborative filtering (CF) based methods can still provide competitive results. Item-KNN [30] can be extended for SBR by recommending items that are most similar to the last item of the current session. KNN-RNN [31] makes use of GRU4REC [9] and the co-occurrence-based KNN model to extract the sequential patterns for SBR. CSRM [18] first utilizes NARM over item-transitions to encode each session, then enriches the representation of the current session by exploring the latest m neighborhood sessions. CoSAN [19] incorporates neighborhood sessions embedding into the items of current session and employ multi-head selfattention to capture the dependencies between each item. However, CSRM and CoSAN may suffer from noise when integrating other sessions\u2019 embeddings for the current one. In contrast, our proposed method considers the collaborative information in item-level: we use the item embeddings in other sessions to enrich the item embeddings of the current session, and then integrate them into session representation for SBR."
        },
        {
            "heading": "2.2 Graph Contrastive Learning",
            "text": "Graph contrastive learning aims to learn the discriminative node representations by contrasting positive and negative samples in graph. Early work on graph contrastive learning focus on capturing local structural patterns, which forces neighboring nodes to have similar feature representations. For example, DeepWalk [32] and node2vec [33] obtain walk sequences on the graph and consider nodes within the same window in the sequence as positive samples. However, random-walk based approaches overly emphasize the structural information [34] and their performance are heavily affected by hyperparameter choice.\nRecently, with the development of deep learning technique, some studies employ graph neural networks to obtain better item representations for graph constrastive learning. Based on the powerful graph convolutional architectures, DGI [35] maximizes the mutual information between global and local representations. GMI [36] focus on the mutual information between input and representation of both the node and edge in the graph. Via data augmentations, GraphCL [37] maximizes the agreement between two augmented views of the input graph. Considering adaptive graph augmentations, GCA [38] incorporates priors for topological and semantic aspects of the graph."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "In this section, we first present the problem statement, and then introduce two types of graph, i.e., session graph and global graph, based on different levels of pair-wise item transitions over sessions for learning item representations, in which we highlight the modeling of global-level item transition information as it is the basis of global graph construction. For clarity, frequently used notations are summarized in Table 1."
        },
        {
            "heading": "3.1 Problem Statement",
            "text": "Let V = {v1, v2, ..., vm} be all of items, and each session S be denoted by S = {vs1, vs2, ..., vsl }, consisting of a sequence\nof interactions (i.e., items clicked by a user) in chronological order, where vsi denotes an item clicked at time-step i within session S, and the length of S is l. Given a session S, the problem of session-based recommendation aims to recommend the top-N items (1 \u2264 N \u2264 |V |) from V that are most likely to be clicked by the user in the next timestamp (i.e., vsl+1)."
        },
        {
            "heading": "3.2 Graph Models: Session Graph and Global Graph",
            "text": "In this subsection, we present two different graph models to capture different levels of item transition information over all available sessions for item representation learning."
        },
        {
            "heading": "3.2.1 Session Graph Model",
            "text": "Session-based graph aims to learn the session-level item embedding by modeling sequential patterns over pair-wise adjacent items in the current session. Inspired by [14], each session sequence is converted into a session graph for learning GNN-based item embeddings of the current session. More concretely, for each session S = {vsi }li=1, the corresponding session graph is defined as a 2-tuple Gs = (Vs, Es), where Vs \u2286 V and Es = {esij} are denoted the clicked item set and the edge set in S respectively, and esij = (v s i , v s j ) indcates the adjacent edge of node v s i and v s j in S, which is called session-level item-transition pattern."
        },
        {
            "heading": "3.2.2 Global Graph Model",
            "text": "Similar to conventional recurrent neural network (e.g., RNN [12]) based approaches, session graph can efficiently capture sequential intra-relations to learn session-level item embeddings. However, it neglects the complicated inter-relations of items over sessions, e.g., the item-transition information from other sessions, which is called global-level item transition information.\nGlobal-level Item Transition Modeling. Here, we take into account global-level item transitions for global-level item representation learning, via integrating all pairwise item transitions over sessions. As such, we propose a novel global graph model for learning global-level item embeddings, which breaks down sequence independence assumption with linking all pairs of items based on pairwise transitions over all sessions (including the current one). Next, we firstly introduce a new definition (i.e., \u03b5-neighbor set) for modeling global-level item transition, and then give the definition of global graph. Definition 1. \u03b5-Neighbor Set (N\u03b5(v)). For any item vpi in\nsession Sp, the \u03b5-neighbor set of v p i indicates a set of items, each element of which is defined as follows,\nN\u03b5(vpi ) = {v q j |v p i = v q i\u2032 \u2208 Sp \u2229 Sq; vqj \u2208 Sq;\nj \u2208 [i \u2032 \u2212 \u03b5, i \u2032 + \u03b5];Sp 6= Sq},\n(1)\nwhere i \u2032 is the order of item vpi in session Sq , \u03b5 is a hyperparameter to control the scope of modeling of item-transition between vpi and the items in Sq . Note that, parameter \u03b5 favors the modeling of short-range item transitions over sessions, since it is helpless (even noise, e.g., irrelevant dependence) for capturing the global-level item transition information if beyond the scope (\u03b5).\nAccording to Definition 1, for each item vi \u2208 V , globallevel item transition is defined as {(vi, vj)|vi, vj \u2208 V ; vj \u2208 N\u03b5(vi)}. Global Graph. Global graph aims to capture the globallevel item transition information, which will be used to learn item embeddings over all sessions. Specifically, the global graph is built based on \u03b5-neighbor sets of items in all sessions. Without loss of generality, global graph is defined as follows, let Gg = (Vg, Eg) be the global graph, where Vg denotes the graph node set that contains all items in V , and Eg = {egij |(vi, vj); vi \u2208 V, vj \u2208N\u03b5(vi)} indicates the set of edges, each corresponding to two pairwise items from all the sessions. Figure 2b shows an example of building the global graph with \u03b5 = 2. To distinguish the importance of vi\u2019s neighbors (N\u03b5(vi)), we treat the co-occurrence over sessions of node vi and its neighbor node vj \u2208 N\u03b5(vi) as the weights of the corresponding edge.\nRemark. (i) The definition of the neighbors3 (i.e., N gv ) of item v on graph Gg is same as N\u03b5(v); (ii) Gg is an undirected weighted graph as \u03b5-neighbor set is undirected; and (iii) Each item in V is encoded into an unified embedding space at time-step t, i.e., hti \u2208 Rd (d indicates the dimension of item embedding), which is feed with an initialization embedding h0i \u2208 R|V |, here we use one-hot based embedding and it is transformed into d-dimensional latent vector space by using a trainable matrix W0 \u2208 Rd\u00d7|V |."
        },
        {
            "heading": "4 THE PROPOSED METHOD",
            "text": "To leverage the different levels (e.g., local-level and globallevel) information for SBR, we first propose a basic GNNbased framework (i.e., B-GNN, as shown in Fig. 3a) in Section 4.1. Based on the B-GNN framework, we also propose\n3. We do not distinguishN\u03b5(v) andN gv when the context is clear and discriminative.\na novel approach (named Session-based Recommendation with Global Information, SRGI) with two different variants in Section 4.2 and Section 4.3, which is capable of leveraging the global item-transition information from two different aspects: (a) SRGI-FM (rf. Fig. 3b), it incorporates the global item-transitions information into the learning process of session-level item representation; and (b) SRGI-CM (rf. Fig. 3c), it treats the global-level item-transition information as a constraint to ensure the learnt item embeddings are consistent with the structure of the global graph. Next, we will illustrate each part in detail."
        },
        {
            "heading": "4.1 Basic GNN-based Framework (B-GNN)",
            "text": "In this section, we first propose a basic GNN-based model (called B-GNN, as shown in Fig. 3a), which solely utilizes three different types (i.e., in, out and in-out, as shown in Fig. 2a) of session-level item transition information for sessionbased recommendation. Specifically, B-GNN consists of three sub-components, i.e., session-level item representation learning layer, session representation learning layer and prediction, and we will be detailed in the following subsections, respectively."
        },
        {
            "heading": "4.1.1 Session-level Item Representation Learning layer",
            "text": "To learn the session-level item representation, we focus on how to enrich the representation of each item with the help of its 1-hop neighbors, via exploring the pairwise itemtransitions within the current session.\nInformation Propagation: For each node vi of session S, we consider three different types of relations between vi and its neighbors N svi , i.e., in-coming neighbor, out-coming neighbor and in-out coming neighbor which are denoted by N s,invi , N s,out vi and N s,in\u2212out vi respectively. To calculate the importance of different neighbors, we treat such three kinds of neighbors differently during the propagation of information, which are first calculated based on mean pooling, namely,\nhNs,invi = MeanPooling(hvj |vj \u2208 N s,invi )\nhNs,outvi = MeanPooling(hvj |vj \u2208 N s,out vi )\nhNs,in\u2212outvi = MeanPooling(hvj |vj \u2208 N s,in\u2212outvi )\n(2)\nThen, the neighbor representation (hNsvi ) of node vi is obtained based on the concatenation of hNs,invi , hNs,outvi and\nhNs,in\u2212outvi , namely,\nhNsvi = [hNs,invi ||hNs,outvi ||hNs,in\u2212outvi ], (3)\nwhere || denotes concatenation operation. In particular, different from SR-GNN [14] and FGNN [15], here we consider three kinds of relations on session graph and use mean pooling in our propagation layer to reduces the training parameters for avoiding over-fitting.\nInformation Aggregation: The session-level item representation is generated by aggregating the embeddings of item vi and its neighbors hNsvi , which is computed via a fully connected layer,\nhsvi = tanh(W1hvi + W2hNsvi + b1), (4)\nwhere W1 \u2208 Rd\u00d7d,W2 \u2208 Rd\u00d73d and b1 \u2208 Rd are trainable parameters. The new representations hsvi of each item is aggregated by the features of item itself and its neighbors in the current session."
        },
        {
            "heading": "4.1.2 Session Representation Learning Layer",
            "text": "Based on the learnt item representations, we now present how to obtain the session representations. Note that the contribution of different items to the next prediction is not equal. Intuitively, the items clicked later in the session are more representative of the user\u2019s current preferences. Moreover, it is important to find the main purpose of the user and filter noise in current session [12]. Hence we incorporate the reversed position information and session information to make a better prediction. After feeding a session sequence into graph neural networks, we can obtain the representation of the items involved in the session, i.e., H = [ h\u2032vs1 ,h \u2032 vs2 , ...,h\u2032vsl ] 4. We also use a learnable position embedding matrix P = [p1,p2, ...,pl], where pi \u2208 Rd is a position vector for specific position i and l is the length of the current session sequence. The position information is integrated through concatenation and non-linear transformation:\nzi = tanh ( W3 [ h\u2032vsi \u2016 pl\u2212i+1 ] + b2 ) , (5)\nwhere parameters W3 \u2208 Rd\u00d72d and b2 \u2208 Rd are trainable parameters. Here reversed position l \u2212 i + 1 can be regard as the distance between the current item and the predicted item, which contains more effective information than forward position i. Next, the corresponding weight is learned through a soft-attention mechanism:\n\u03b2i = q>2 \u03c3 (W4zi + W5s \u2032 + b3) , (6)\nwhere W4,W5 \u2208 Rd\u00d7d and q2,b3 \u2208 Rd are learnable parameters and s\u2032 denotes session information, which is obtained by sum pooling (i.e., s\u2032 = \u2211l i=1 h \u2032 vsi\n). Finally, the session representation can be obtained by linearly combining the item representations:\nS = l\u2211 i=1 \u03b2ih\u2032vsi . (7)\nThe session representation S is constructed by all the items involved in the current session, where the contribution\n4. Here h\u2032vs1 = h s vi for B-GNN.\nof each item is determined not only by the information in the session graph, but also by the chronological order in the sequence."
        },
        {
            "heading": "4.1.3 Objective Function",
            "text": "Based on the obtained session representations S, the final recommendation probability for each candidate item based on their initial embeddings as well as current session representation. Here, `2-norm is employed to normalize session representation and item representations (i.e., S\u0302 = S||S||2 and h\u0302v = hv||hv||2 ) for avoiding popular bias [39]. Then, we use inner-product and apply softmax function to obtain the output y\u0302:\ny\u0302i = Softmax ( \u03b1S\u0302 > h\u0302vi ) , (8)\nwhere \u03b1 is a scale coefficient for better convergence [39], [40] and y\u0302i \u2208 y\u0302 denotes the probability of item vi appearing as the next-click in the current session.\nThe loss function is defined as the cross-entropy of the prediction results y\u0302:\nLS = \u2212 m\u2211 i=1 yi log (y\u0302i) + (1\u2212 yi) log (1\u2212 y\u0302i) , (9)\nwhere y denotes the one-hot encoding vector of the ground truth item."
        },
        {
            "heading": "4.2 Fusion-based Model",
            "text": "In this section, we propose a novel fusion-based model under the B-GNN framework, i.e., a variant of our proposed model SRGI (e.g., SRGI-FM, as shown in Fig. 3b). We will detail how to incorporate the global neighbor features into current session, which is built based on the architecture of graph convolution network [41]. Here we first describe a single layer consisting of two components: fusion-based information propagation and information aggregation, and then show how to generalize it to multiple layers.\nFusion-based Information Propagation: To obtain the firstorder (i.e., 1-hop neighbors in the global graph) neighbor\u2019s features of item v, one straightforward solution is to use mean pooling method [42]. However, not all of items in v\u2019s \u03b5-neighbor set are relevant to the user preference of the current session, and thus we consider to utilize a sessionaware attention to distinguish the importance of items in (N\u03b5(v)). Therefore, each item in N\u03b5(v) is linearly combined according to the session-aware attention score,\nhN gvi = \u2211\nvj\u2208N gvi\n\u03c0(vi, vj)hvj , (10)\nwhere \u03c0(vi, vj) estimates the importance weight of different neighbors. Intuitively, the more consistent an item is to the preference of the current session, the more important this item is to the recommendation. Therefore, we implement \u03c0(vi, vj) based on the principle of attention network [17]:\n\u03c0(vi, vj) = qT1 LeakyRelu ( W6[(s\u2032 hvj )\u2016wij ] ) . (11)\nwhere \u2016 indicates concatenation operation; wij \u2208 R1 is the weight of edge (vi, vj) in global graph; W6 \u2208 Rd+1\u00d7d+1 and q1 \u2208 Rd+1 are trainable parameters; indicates elementwise product.\nDifferent from mean pooling, in our model the propagation of information depends on the affinity between s\u2032 and vj , which means neighbors that match the preference of current session will be more favourable, and we then normalize the coefficients across all neighbors connected with vi by adopting the softmax function:\n\u03c0(vi, vj) = exp\n( \u03c0(vi, vj) )\u2211 vk\u2208N gvi exp ( \u03c0(vi, vk)\n) . (12) As such, Eq. (12) is capable of assigning high attention scores to the important global-level 1-hop neighbors of items in the current session.\nFusion-based Information Aggregation: The final step is to aggregate the item representation hv and its neighborhood representation hgNv , we implement aggregator function agg as follows,\nhgv = Relu ( W7[hv||hN gv ] ) , (13)\nwhere W7 \u2208 Rd\u00d72d is transformation weight. Through a single aggregator layer, the representation of an item is dependent on itself and its immediate neighbors. We could explore the high-order connectivity information through extending aggregator from one layer to multiple layers, which allows more global information to be incorporated into the current representation. We formulate the representation of an item in the k-th steps as:\nhg,(k)v = agg ( h(k\u22121)v ,h (k\u22121) N gv ) , (14)\nh(k\u22121)v is representation of item v which is generated from previous information propagation steps, h(0)v is set as hv at the initial propagation iteration. In this way, the korder representation of an item is a mixture of its initial representations and its neighbors up to k hops away. This enables more effective messages to be incorporated into the representation of the current session.\nAfter obtaining the representation of the hg,(k)v , the new representation of each item can be obtained as follow,\nh\u2032v = h g,(k) v + h s v. (15)\nConsequently, we obtain the new representation for each item of the current session, which integrate both sessionlevel and global-level item-transition information and thus the recommendation prediction can be implemented based on Eq. (5)-(9) via using the new item embedding h\u2032v ."
        },
        {
            "heading": "4.3 Constrained-based Model",
            "text": "Recall that the strategy of building the global graph is based on the co-occurrence of pairwise items over sessions, namely, pairwise items with high frequency co-occurrences are remained while filtering the ones with low-frequency co-occurrences. To this end, in this section we propose another variant of our proposed model SRGI (i.e., SRGICM, as shown in Fig. 3c), which is different from SRGI-FM that aims to integrate different levels of item-transitions for learning item representation, the principle of SRGI-CM is to ensure the learnt item embeddings are consistent with global-level item-transition information, which is called global proximity that can be modeled by graph contrastive learning from two different aspects: (i) Preserving the local\nstructural patterns. An item and its global neighbors often have high relevancy (e.g., milk and bread) due to high frequency item-transitions over sessions, and thus should be assigned to the proximal vectors in the learnt embedding space; and, ii) Enforcing the discrimination for different items. The representation of different nodes should be discriminating in the learned embedding space. Next, we show the process of performing contrastive learning on the global graph.\nGraph Data Augmentation. Given the global graph Gg = (Vg, Eg), we first construct two correlated graph views based on data augmentation. Graph augmentation aims to create novel and realistically rational data via certain transformations on the original graph. Different graph augmentations for graph provide different contexts for each node, which is the key component of graph contrastive learning. Following previous graph contrastive learning approaches [37], [38], three general data augmentations are used for creating novel and realistically global graphs:\n1) Node dropping. It will randomly discard a certain portion of nodes in the global graph and also remove their corresponding connections, where the dropping probability for each node follows a uniform distribution. 2) Edge dropping. Similar to node dropping, edge dropping will randomly remove certain portion of edges in the original graph. 3) Attribute masking. It masks a certain fraction of dimensions of node features with zeros.\nContrastive Learning. After employing data augmentations we can obtain two correlated graph views which are both corrupted from the original global graph. We denotes two graph views as G1g = (V1g , E1g ) and G2g = (V2g , E2g ), where V\u2217 and E\u2217 are the node sets and the edge sets for two graph views. Then a GNN-based encoder f(\u00b7, \u00b7) is applied here to extract the features for each node in the graph:\no1 = f(V1g , E1g ) o2 = f(V2g , E2g )\n(16)\nwhere o\u2217i \u2208 o\u2217 is the learnt node representation for node i in graph view G\u2217g . Contrastive learning does not apply any constraint on GNN architecture [37], and here we choose GCN [41] for its low computational cost and stability. After obtaining the node representations o1 and o2, we use a projection function (i.e., a single fully connected layer) to transform the learnt node representation to another latent space where the contrastive loss is calculated,\nz = Relu(W8o + b4). (17)\nFinally, the contrastive loss is employed to maximize the node-level agreement while enforcing the representations of different nodes discriminate in the latent space. For the node i in the graph view G1g , only the corresponding node in the graph view G2g is regarded as positive samples, while all other nodes are treated as negative samples. The contrastive\nloss for the global graph is shown as follow: LC = N\u2211 i=1 log e\u03b8(z 1 i ,z 2 i )\ne\u03b8(z 1 i ,z 2 i ) + N\u2211 k=1 I[k 6=i]e\u03b8(z 1 i ,z 2 k) + N\u2211 k=1 I[k 6=i]e\u03b8(z 1 i ,z 1 k)\n(18) where I[\u2217] is an indicator function and \u03b8(a, b) = cos(a, b) denotes the cosine similarity function. The contrastive loss LC is aim to: i) Maximizing the consistency between the corresponding node in different graph views, which will force each node\u2019s features to be proximal with its neigbhors\u2019 features and thus the learned node representations will be more robust; and ii) Minimizing the agreement between different nodes, which lead to the discrimination of different nodes\u2019 representations in the latent embedding space.\nTo learn the item transitions during the session and preserve the global proximity, the final loss function of SRGI-CM combines prediction loss and contrastive loss,\nLG = LS + \u03bbCLC , (19)\nwhere \u03bbC is a trade-off parameter to control the impact of global proximity."
        },
        {
            "heading": "4.4 Discussion",
            "text": "As we show the detail of SRGI-FM and SRGI-CM, the common features of these two models is that they are both designed based on the technology of GNN and the proposed global graph. The global item transition is important for the modeling of the item-transition within the current session. By exploring the node features and structural information in the global graph, the global context information is introduced to enrich the representation of each item. And the experiment results demonstrate that both SRGI-FM and SRGI-CM show promising improvements compared with BGNN.\nAnd the main difference between SRGI-FM and SRGICM is reflected in two aspects: (i) The fusion-based model SRGI-FM and constrained-based model SRGI-CM present two different ways to incorporate the effective global information into the current session. SRGI-FM uses attentionbased GNN to directly incorporate the neighbor features from the global graph into the current session, while SRGICM focuses on the global graph structure and leverages graph contrastive learning to constrain the representation of each item in the latent space. (ii) Comparing with SRGI-FM, SRGI-CM needs to employ data augmentation to create different graph views based on the global graphs when in the training process. However, in the inferring process, SRGICM does not need the global graph as input or compute the contrastive loss, while SRGI-FM still needs to incorporate the features from the global graph into the current session. This means SRGI-CM requires more time to process the data during the training process, while in the process of inference, the computation cost of SRGI-CM will be much smaller than SRGI-FM."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We have conducted extensive experiments to evaluate the accuracy of the proposed method by answering the following four key research questions:"
        },
        {
            "heading": "5.1 Datesets and Preprocessing",
            "text": "To fully evaluate the effectiveness of our proposed method, we employ three real-world datasets in the experiment.\nDiginetica5 is from CIKM Cup 2016, which consisting of typical transaction data for five month from e-commerce website. Following [14], we set the sessions of last week (latest data) as the test data, and the remaining historical data for training.\nTmall6 comes from IJCAI-15 competition, which contains anonymized user\u2019s shopping logs on Tmall online shopping platform. Since the amount of items of Tmall is extremely large, we use the first 120,000 of the sessions for train and test. The sessions of last 100 seconds are set as the test data, and the remaining historical data for training.\nNowplaying7 comes from music-related tweets [43], which describes the music listening behavior of users. We set the sessions of last two months as the test data, and the remaining historical data for training.\nFollowing [14], [28], we conduct preprocessing step over the three datasets. More specifically, sessions of length 1 and items appearing less than 5 times were filtered across all the three datasets. For Tmall, sessions longer than 40 were also filtered [18]. Furthermore, for a session S = [s1, s2, ..., sn], we generate sequences and corresponding labels by a sequence splitting preprocessing, i.e., ([s1] , s2), ([s1, s2] , s3), ..., ([s1, s2, ..., sn\u22121] , sn) for both training and testing across all the three datasets. The statistics of datasets, after preprocessing, are summarized in Table 2."
        },
        {
            "heading": "5.2 Evaluation Metrics",
            "text": "We adopt two widely used ranking based metrics: P@N and MRR@N by following previous work [13], [14].\nP@N (Precision) [14]: The P@N score is typically used as a measure of accuracy. It represents the proportion of correctly recommended items in top N recommended item list, which is defined as:\nP@N = nhit ntest , (20)\n5. https://competitions.codalab.org/competitions/11161 6. https://tianchi.aliyun.com/dataset/dataDetail?dataId=42 7. https://dbis-nowplaying.uibk.ac.at/#nowplaying\nwhere ntest denotes the number of test data and nhit denotes the number of the target items appearing in the of top N recommended items.\nMRR@N (Mean Reciprocal Rank) [13]: The MRR@N score is the average of reciprocal rank of the correctlyrecommended items. The reciprocal rank is set to zero if the rank exceeds N ,\nMRR@N = 1\nntest \u2211 1 Rank(vtarget) . (21)\nMRR is a normalized score in the range of [0, 1], and a larger MRR value means that correct recommendations are in the top of the ranking list.\nHere, we choose N = 20 for both P@N and MRR@N, as recommendation systems should focus on top ranked items."
        },
        {
            "heading": "5.3 Baseline Algorithms",
            "text": "We compare our method with classic methods as well as state-of-the-art models. The following baseline models are evaluated.\nPOP: It recommends top-N frequent items of the training set.\nItem-KNN [30]: It recommends items based on the similarity between items of the current session and items of other ones.\nFPMC [5]: It combines the matrix factorization and the firstorder Markov Chain for capturing both sequential effects and user preferences. By following the previous work, we also ignore the user latent representations when computing recommendation scores.\nGRU4Rec8 [9]: It is a RNN-based model that uses Gated Recurrent Unit (GRU) to model user sequences.\nNARM9 [12]: It improves over GRU4Rec [9] by incorporating attention mechanism into hierarchical RNN for SBR.\nSTAMP10 [13]: It employs attention layers to replace all RNN encoders in previous work by fully relying on the selfattention of the last item in the current session to capture the user\u2019s short-term interest.\nSR-GNN11 [14]: It converts sessions into graphs and leverages gated GNN layer to capture the dependencies between items and its context, followed by a self-attention of the last item as STAMP [13] does to obtain the session level representation.\nCSRM12 [18]: It utilizes the memory networks [44] to investigate the latest m sessions for better predicting the intent of the current session.\nCoSAN [19]: It injects the embedding of neighbor sessions to enrich the item representation and employ multi-head attention to capture the dependencies between items.\nGCE-GNN13 [1]: A state-of-the-art GNN-based model which directly aggregates global information into current\n8. https://github.com/hidasib/GRU4Rec 9. https://github.com/lijingsdu/sessionRec NARM 10. https://github.com/uestcnlp/STAMP 11. https://github.com/CRIPAC-DIG/SR-GNN 12. https://github.com/wmeirui/CSRM SIGIR2019 13. https://github.com/CCIIPLab/GCE-GNN\nsession and utilizes attention mechanism to obtain the session representation."
        },
        {
            "heading": "5.4 Parameter Setup",
            "text": "Following previous methods [12] [13] [14], the dimension of the latent vectors is fixed to 100, and the size for mini-batch is set to 100 for all models. We keep the hyper-parameters of each model consistent for a fair comparison. For CSRM, we set the memory size to 100 which is consistent with the batch size. For our model, all parameters are initialized using a Gaussian distribution with a mean of 0 and a standard deviation of 0.1. We use the Adam optimizer with the initial learning rate 0.001, which will decay by 0.1 after every 3 epoch. The L2 penalty is set to 10\u22125 and the scale coefficient \u03b1 is set to 12. Moreover, we set the maximum distance of adjacent items \u03b5 and the number of neighbors to 3 and 12, respectively. In SRGI-FM we use dropout [45] to avoid overfitting, the dropout ratio is set to 0.5 and the graph depth is searched in {1, 2}. In SRGI-CM, the graph depth is set to 2 and the trade-off parameter \u03bbC is searched in {10, 50, 100, 150, 100}. All the parameters are searched on a validation set which is a random 10% subset of the training set."
        },
        {
            "heading": "5.5 Overall Comparison (RQ1)",
            "text": "Table 3 reports the experimental results of the state-ofthe-art baselines and our proposed model on three realworld datasets, in which the best result of each column is highlighted in boldface. It can be observed that SRGI-FM and SRGI-CM achieve better performance than state-of-theart baselines across all three datasets in terms of the two metrics, which ascertains the effectiveness of our proposed method.\nAmong the traditional methods, POP\u2019s performance is the worst, as it only recommends the most popular item without learning the preference of the user. Comparing with POP, FPMC performs better on three datasets, which shows the strength of Markov Chains in modeling sequential data. Moreover, Item-KNN achieves the best results among the traditional methods on the Diginetica and Nowplaying datasets, which demonstrates the importance of collaborative information. However, it cannot capture the complex sequential transitions within the session.\nCompared with traditional methods, neural network based methods usually have better performance for sessionbased recommendation. In sprite of preforming worse than Item-KNN on Diginetica, GRU4Rec, as the first RNN based method for SBR, still demonstrates the capability of RNN in modeling sequences. Nevertheless, it relies entirely on RNN to model the complex transitions within the session, which can only captures the point-wise dependencies and may generate fake dependencies [27].\nThe subsequent methods, NARM and STAMP outperform GRU4REC significantly. NARM combines RNN and attention mechanism, which uses the last hidden state of RNN as the main preference of user, this result indicates that directly using RNN to encode the session sequence may not be sufficient for SBR as RNN only models one way item-transition between adjacent items in a session. We also observe that STAMP, a complete attention-based method,\nachieves better performance than NARM on Tmall, which incorporates a self-attention over the last item of a session to model the short-term interest, this result demonstrates the effectiveness of assigning different attention weights on different items for session encoding. Compared with RNN, attention mechanism appears to be a better option, although STAMP neglects the chronological order of items in a session.\nCSRM and CoSAN performs better than NARM and STAMP over three datasets, which shows the effectiveness of using item transitions from other sessions. However, the memory networks used by CSRM have limited slots and both of them treat other sessions as a whole one without distinguishing the relevant item-transitions from the irrelevant ones encoded in other sessions.\nBy modeling every session sequence as a subgraph and applying GNN to encode items, SR-GNN and GCE-GNN demonstrate the effectiveness of applying GNN in sessionbased recommendation. This indicates that the graph modeling would be more suitable than the sequence modeling, RNN, or a set modeling, the attention modeling for SBR. Our approach SRGI-FM and SRGI-CM outperforms SRGNN and GCE-GNN on all the three datasets. Specifically, SRGI-CM outperforms the GCE-GNN by 1.5% on Diginetica, 13.6% on Tmall and 4.4% on Nowplaying on average. Different from SR-GNN and CoSAN, our approach integrates item-level transition information from global context, i.e., other session, and local context, i.e., the current session, and also incorporates reversed position information, leading to consistent better performance."
        },
        {
            "heading": "5.6 Impact of Global Feature Encoder (RQ2)",
            "text": "In this section, we aim to study the effect of global transition information and the impact of different graph parameters (i.e., graph depth, the number of neighbors and tradeoff parameter \u03bbC ) by conducting experiments over three datasets."
        },
        {
            "heading": "5.6.1 Effect of global transition information.",
            "text": "From Table 3, we can observe that both SRGI-FM and SRGICM achieve better performance than B-GNN, which verifies\nthat global transition information can provide useful information for learning the preference of the current session. Comparing with two version of SRGI, SRGI-CM performs better than SRGI-FM in most instances. It is because SRGIFM directly incorporates the neighbors\u2019 features from the global graph, which easily introduces extra noise into the current session. Although SRGI-FM leverages session aware attention mechanism to reduce the influence of noise, the performance of the model is still affected. In comparison, SRGI-CM is less influenced by the noise information as it does not need to fuse global features directly into the current session representation. Specifically, SRGI-CM obtains the representation of each session based on the item features within the session and utilizes contrastive learning to preserve global proximity. The global proximity forces the neighboring items more proximity while different items to be discriminating in the learned embedding space, which benefits the item representation learning and the prediction of the current session. The results in Table 3 demonstrate the effectiveness of our proposed two versions of SRGI."
        },
        {
            "heading": "5.6.2 Impact of the graph depth.",
            "text": "We next conduct experiments on three datasets to evaluate the impact of the graph depth on two versions of SRGI. For SRGI-FM, the computational cost is high as it uses attention network in the graph encoder and the number of neighbors of each item increases exponentially as the graph depth increases, therefore we only evaluate the performance of SRGI-FM with 1-hop and 2-hop neighbors due to the limited graphics memory. In contrast, in SRGI-CM we employ GCN architecture, whose computational cost is relatively lower and thus we evaluate the performance of SRGI-CM with the graph depth from 1-hop to 4-hop.\nFigure 4 shows the performance of two versions of SRGI with different graph depth. It can be observed that both SRGI-CM and SRGI-FM outperform the B-GNN with graph depth over three datasets. Specifically, SRGI-FM with 2-hop performs better than SRGI-FM with 1-hop in most situation, which indicates that high-level exploring might obtain more effective information from global graph. Besides, the performance of SRGI-CM is better than SRGI-FM on Diginetica\nand Tmall, which demonstrates the effectiveness of global proximity for session-based recommendation. Further, we observe the performance of SRGI-CM drops when graph depth is set to 4 on three datasets in terms of P@20 and MRR@20, which shows that higher-level exploring might also introduce more noise during the global proximity learning. Overall, the results in Figure 4 demonstrates that the structural information in global graph contain useful global transitions information for current session."
        },
        {
            "heading": "5.6.3 Impact of the number of neighborhoods on global graph.",
            "text": "It can be observed from Figure 1 that there will be irrelevant items in a session when user clicked items. As we collect the -Neighbor set of each item to obtain the global item transitions information, the inclusion of noise was unavoidable. In the proposed method, we only keep top-N edges with the highest weights (i.e., frequency) to reduce the influence of noise. When the number of neighbors N increases, more useful global transitions information can be captured, and at the same time more noise will be introduced. Here, we conduct experiment over three datasets to evaluate the impact of N on the performance of our proposed method.\nFrom Figure 5, we can observe that with the number of neighbors increase from 4 to 12, the performance of SRGICM in terms of MRR@20 becomes better over three datasets. It is because SRGI-CM can capture more effective global transitions information from the neighbors of each item, which benefits the prediction of current session. However, the performance of SRGI-CM drops when it has more neigh-\nbors in the global graph, which is affected by the increasing noise in the neighbors."
        },
        {
            "heading": "5.6.4 Impact of the trade-off parameter \u03bbC in SRGI-CM.",
            "text": "Trade-off parameter \u03bbC is an important scalar, which controls the influence of global proximity. Higher \u03bbC means that SRGI-CM pays more attention to the global-level item transitions. Therefore, we conduct experiments to study the impact of \u03bbC on the performance of SRGI-CM over three datasets. From Table 4 we observe that with the \u03bbC increases, the performance of SRGI-CM improves in terms of P@20 on Diginetica and Nowplaying, which shows the effective of global information to the current session. However, the performance of SRGI-CM drops on Nowplaying in terms of MRR@20 when the \u03bbC increases. This is because too much attention to global information will affect the learning of the current session. We consider that it is appropriate to set \u03bbC to 50 or 100 for most of the scenarios."
        },
        {
            "heading": "5.7 Comparison with Different Session-level item representation learning methods. (RQ3)",
            "text": "In this work, we use mean pooling-based GNNs to convey information from item\u2019s neighbors to item itself in the session graph. Specifically, We consider three different kinds of relations of each item and utilize fully connected layer to obtain the new representation of each item. As the representation learning of items is significant for session based recommendation, we conduct experiments to compare our proposed B-GNN with a series of contrast models:\n\u2022 B-GGNN: B-GNN with gated GNNs replacing the mean pooling-based GNNs. \u2022 B-GAT: B-GNN with GAT replacing the mean poolingbased GNNs.\nTable 5 shows the performance of B-GNN and different contrast models. We can observe that the proposed B-GNN outperforms B-GGNN and B-GNN-GAT on Diginetica and Tmall. Gated GNNs introduce gated recurrent units into GNNs and GAT leverages attention mechanism into graph, which enhance the ability of graph neural network to extract features. However, more parameters and nonlinear layers will make the network more prone to overfitting and these models do not consider three relations in the session graph. Our GNNs layer utilize mean pooling layer to reduce the parameters and capture three kinds of relations in the session graph. The results in Table 5 demonstrate the effectiveness of our proposed GNNs layer."
        },
        {
            "heading": "5.8 Comparison with Different Session Encoder. (RQ4)",
            "text": "For efficiently learning the importance of each item in the session sequence, we propose reversed-position aware session encoder. It incorporates the reversed position information with session information, which is used to drive our model to learn the contribution of each item in the current session. To verify this and evaluate the effectiveness of using the position vector in a reverse order, which is proposed in our method, we design a series of contrast models: \u2022 B-GNN-WP: B-GNN without using position informa-\ntion when aggregating the item features. \u2022 B-GNN-FP: B-GNN with forward position vector re-\nplacing the reversed position vector. Table 6 shows the performance of different contrast models. We can observe that B-GNN-WP\u2019s performance is not satisfactory, as the model is unable to capture the chronological information in the sessions without position vector. B-GNNFP performs better than B-GNN-WP over three datasets, which demonstrates the importance of position information. However, the forward position vector can not learn the distance between each item and the target item, which makes the improvement limited. Our attention network with reversed position vector performs better than the BGNN-FP over three datasets, it is because reversed position vector contains the relative position information between the current item and the target item. The results demonstrate the effectiveness of reversed position vector and superiority of our session encoder."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper studies the problem of session-based recommendation, which is a challenging task as the user identities and historical interactions are often unavailable in realworld scenarios. It presents two different ways to leverage the high-order global information for session-based recommendation via GNN: (i) SRGI-FM, which recursively incorporates the neighbors\u2019 feature of each node on global graph via session-aware attention mechanism; and (ii) SRGICM, which treats session-based recommendation as a multitask learning problem and utilizes graph contrastive learning for preserving global proximity to learn item representations, Furthermore, it capture three kinds of relations\nwithin the session and incorporates the reversed position embedding to better learn the contribution of each item. Comprehensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines over three benchmark datasets consistently, indicating it can be effectively used to solve real-world session-based recommendation problems."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant No.61602197 and Grant No.61772076, and in part by Equipment PreResearch Fund for The 13th Five-year Plan under Grant No.41412050801."
        }
    ],
    "title": "Exploring Global Information for Session-based Recommendation",
    "year": 2021
}