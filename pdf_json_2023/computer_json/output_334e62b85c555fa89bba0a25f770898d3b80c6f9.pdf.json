{
    "abstractText": "Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs). Additionally, existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages. To address this gap, we propose Struct-XLM, a novel multilingual language model that leverages reinforcement learning (RL) to autonomously discover universal syntactic structures for improving the cross-lingual representation alignment of PLM. Struct-XLM integrates a policy network (PNet) and a translation ranking task. The PNet is designed to discover structural information and integrate it into the last layer of the PLM through the structural multi-head attention module to obtain structural representation. The translation ranking task obtains a delayed reward based on the structural representation to optimize the PNet while improving the alignment of cross-lingual representation. Experiments show the effectiveness of the proposed approach for enhancing cross-lingual transfer of multilingual PLM on the XTREME benchmark1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Linjuan Wu"
        },
        {
            "affiliations": [],
            "name": "Weiming Lu"
        }
    ],
    "id": "SP:1344de49041df36c3b34ec082e82768b603d2e86",
    "references": [
        {
            "authors": [
                "Wasi Uddin Ahmad",
                "Haoran Li",
                "Kai-Wei Chang",
                "Yashar Mehdad."
            ],
            "title": "Syntax-augmented multilingual BERT for cross-lingual transfer",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Lars Ahrenberg."
            ],
            "title": "Lines: An english-swedish parallel treebank",
            "venue": "Proceedings of the 16th Nordic Conference of Computational Linguistics, NODALIDA 2007, Tartu, Estonia, May 2007, pages 270\u2013273.",
            "year": 2007
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Ethan A. Chi",
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "Finding universal grammatical relations in multilingual BERT",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 5564\u20135577.",
            "year": 2020
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Ming Zhou."
            ],
            "title": "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Jennimaria Palomaki",
                "Vitaly Nikolaev",
                "Eunsol Choi",
                "Dan Garrette",
                "Michael Collins",
                "Tom Kwiatkowski."
            ],
            "title": "Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages",
            "venue": "Trans. Assoc. Com-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Proceedings of the 2019 Annual Conference on Neural Information Processing Systems, NeurIPS 2019, pages 7057\u2013 7067.",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
            "year": 2018
        },
        {
            "authors": [
                "Hao Fei",
                "Meishan Zhang",
                "Fei Li",
                "Donghong Ji."
            ],
            "title": "Cross-lingual semantic role labeling with model transfer",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 28:2427\u20132437.",
            "year": 2020
        },
        {
            "authors": [
                "Fangxiaoyu Feng",
                "Yinfei Yang",
                "Daniel Cer",
                "Naveen Arivazhagan",
                "Wei Wang."
            ],
            "title": "Language-agnostic BERT sentence embedding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL",
            "year": 2022
        },
        {
            "authors": [
                "Mandy Guo",
                "Qinlan Shen",
                "Yinfei Yang",
                "Heming Ge",
                "Daniel Cer",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Keith Stevens",
                "Noah Constant",
                "Yun-Hsuan Sung",
                "Brian Strope",
                "Ray Kurzweil"
            ],
            "title": "Effective parallel corpus mining using bilingual sentence embeddings",
            "year": 2018
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "A structural probe for finding syntax in word representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "MLQA: evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2020
        },
        {
            "authors": [
                "Fuli Luo",
                "Wei Wang",
                "Jiahao Liu",
                "Yijia Liu",
                "Bin Bi",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si."
            ],
            "title": "VECO: variable and flexible cross-lingual pre-training for language understanding and generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "J. Nvire",
                "M. Abrams"
            ],
            "title": "Universal dependencies 2.2",
            "year": 2020
        },
        {
            "authors": [
                "Xuan Ouyang",
                "Shuohuan Wang",
                "Chao Pang",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Boliang Zhang",
                "Jonathan May",
                "Joel Nothman",
                "Kevin Knight",
                "Heng Ji."
            ],
            "title": "Cross-lingual name tagging and linking for 282 languages",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Barun Patra",
                "Saksham Singhal",
                "Shaohan Huang",
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Vishrav Chaudhary",
                "Xia Song."
            ],
            "title": "Beyond english-centric bitexts for better multilingual language representation learning",
            "venue": "CoRR, abs/2210.14867.",
            "year": 2022
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette"
            ],
            "title": "How multilingual is multilingual bert",
            "venue": "In Proceedings of the 57th Conference of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Richard S. Sutton",
                "David A. McAllester",
                "Satinder Singh",
                "Yishay Mansour."
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado,",
            "year": 1999
        },
        {
            "authors": [
                "Xiangpeng Wei",
                "Rongxiang Weng",
                "Yue Hu",
                "Luxi Xing",
                "Heng Yu",
                "Weihua Luo."
            ],
            "title": "On learning universal representations across languages",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Linjuan Wu",
                "Shaojuan Wu",
                "Xiaowang Zhang",
                "Deyi Xiong",
                "Shizhan Chen",
                "Zhiqiang Zhuang",
                "Zhiyong Feng."
            ],
            "title": "Learning disentangled semantic representations for zero-shot cross-lingual transfer in multilingual machine reading comprehension",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Yiquan Wu",
                "Kun Kuang",
                "Yating Zhang",
                "Xiaozhong Liu",
                "Changlong Sun",
                "Jun Xiao",
                "Yueting Zhuang",
                "Luo Si",
                "Fei Wu."
            ],
            "title": "De-biased court\u2019s view generation with causality",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Yiquan Wu",
                "Yifei Liu",
                "Weiming Lu",
                "Yating Zhang",
                "Jun Feng",
                "Changlong Sun",
                "Fei Wu",
                "Kun Kuang."
            ],
            "title": "Towards interactivity and interpretability: A rationale-based legal judgment prediction framework",
            "venue": "Proceedings of the 2022 Conference on Empiri-",
            "year": 2022
        },
        {
            "authors": [
                "Yiquan Wu",
                "Weiming Lu",
                "Yating Zhang",
                "Adam Jatowt",
                "Jun Feng",
                "Changlong Sun",
                "Fei Wu",
                "Kun Kuang."
            ],
            "title": "Focus-aware response generation in inquiry conversation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12585\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Liyan Xu",
                "Xuchao Zhang",
                "Bo Zong",
                "Yanchi Liu",
                "Wei Cheng",
                "Jingchao Ni",
                "Haifeng Chen",
                "Liang Zhao",
                "Jinho D. Choi."
            ],
            "title": "Zero-shot cross-lingual machine reading comprehension via inter-sentence dependency graph",
            "venue": "Thirty-Sixth AAAI Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yinfei Yang",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Steve Yuan",
                "Mandy Guo",
                "Qinlan Shen",
                "Daniel Cer",
                "Yun-Hsuan Sung",
                "Brian Strope",
                "Ray Kurzweil"
            ],
            "title": "Improving multilingual sentence embedding using bidirectional dual encoder with additive margin",
            "year": 2019
        },
        {
            "authors": [
                "Yinfei Yang",
                "Yuan Zhang",
                "Chris Tar",
                "Jason Baldridge."
            ],
            "title": "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Zeman",
                "Joakim Nivre",
                "Mitchell Abrams"
            ],
            "title": "Universal dependencies 2.9. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL)",
            "venue": "Faculty of Mathematics and Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Qi Zhang",
                "Jie Zhou",
                "Qin Chen",
                "Qingchun Bai",
                "Jun Xiao",
                "Liang He."
            ],
            "title": "A knowledge-enhanced adversarial model for cross-lingual structured sentiment analysis",
            "venue": "International Joint Conference on Neural Networks, IJCNN 2022, Padua, Italy, July 18-23,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyang Zhang",
                "Minlie Huang",
                "Li Zhao."
            ],
            "title": "Learning structured representation for text classification via reinforcement learning",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applica-",
            "year": 2018
        },
        {
            "authors": [
                "Zhenru Zhang",
                "Chuanqi Tan",
                "Songfang Huang",
                "Fei Huang"
            ],
            "title": "VECO 2.0: Cross-lingual language model pre-training with multi-granularity contrastive learning. CoRR, abs/2304.08205",
            "year": 2023
        },
        {
            "authors": [
                "Xingran Zhu."
            ],
            "title": "Cross-lingual word sense disambiguation using mbert embeddings with syntactic dependencies",
            "venue": "CoRR, abs/2012.05300.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3405\u20133419 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Cross-lingual representation is a crucial component in the development of multilingual natural language processing (NLP) systems (Hu et al., 2020). Various multilingual Pre-trained Language Models (PLMs) like XLM-R (Conneau et al., 2020), XYLENT (Patra et al., 2022), and VECO2.0 (Zhang et al., 2023) have demonstrated their effectiveness in cross-lingual transfer learning.\nThe alignment of cross-lingual representation is vital for effective cross-lingual transfer learn-\n\u2020Corresponding authors. 1The code is available at https://github.com/\nwulinjuan/Struct-XLM\ning. Previous researches extend the pre-training tasks by introducing novel pre-training objectives at different levels of granularity, such as token-level (Luo et al., 2021; Zhang et al., 2023), word-level (Huang et al., 2019; Wei et al., 2021), and sentencelevel (Huang et al., 2019; Wei et al., 2021; Chi et al., 2021; Ouyang et al., 2021). These objectives provide explicit guidance for the model to learn aligned information across languages. Some models (Luo et al., 2021; Ouyang et al., 2021) proposed cross-lingual attention mechanisms to aligned cross-lingual representation. However, there is still limited research on aligning crosslingual representations with syntactic structurelevel information.\nThe performance of cross-lingual transfer is also influenced by structural differences among languages (Ahmad et al., 2021; Wu et al., 2022a). And the incorporation of structural information, particularly syntax, has proven beneficial. Syntaxaugmented mBERT (Ahmad et al., 2021) proposed a Graph Attention Network (GAN) to learn universal dependency tree labels and integrate them into the self-attention mechanism of mBERT for enhancing cross-lingual transfer. The utilization of syntactic structures also demonstrates improvements in various cross-lingual tasks, such as crosslingual structured sentiment analysis (Zhang et al., 2022), cross-lingual semantic role labeling (Fei et al., 2020), and cross-lingual word sense disambiguation (Zhu, 2020). However, existing methods heavily rely on dependency treebanks or syntactic tagging tools to obtain syntactic labels, which are often limited in quantity or quality, particularly for low-resource languages.\nIn this paper, we propose Struct-XLM, a multilingual language model with structure discovery based on multilingual PLM, that aims to improve the alignment of cross-lingual representations of PLM by automatically learning universal structural information, such as constituent syntactic structure.\n3405\nAs shown in Figure 1, we employ reinforcement learning (RL) to discover these syntactic structures without explicit structure annotations. Following Zhang et al. (2018), we formulate the structure discovery as a sequential decision problem optimized through policy gradient and implemented by a masked self-attention mechanism. Additionally, we introduce a structural multi-head attention mechanism, as shown in Figure 2, to effectively integrate the acquired structural information into the last layer of the multilingual pre-trained LM. Our model follows a Siamese framework and is trained using a policy network (PNet) and translation ranking task, in which the latter provides delayed rewards for RL while enhancing crosslingual alignment. During the fine-tuning phase for downstream tasks, the parameters of the PNet are kept frozen.\nTo summarize, our main contributions are as follows.\n\u2022 We propose Struct-XLM, a novel multilingual language model that autonomously discovers universal structural information with RL to enhance cross-lingual alignment without explicit structure annotations.\n\u2022 We introduce a structural multi-head attention mechanism that integrates the learned structural information into the last layer of the multilingual PLM for improving cross-lingual transfer ability.\n\u2022 Experiments on the 7 tasks of XTREME, our model shows superior average performance compared to the baseline PLM by 4.1 points and is competitive with the InfoXLM model using only 1/5000 of the amount of training data."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Cross-lingual Representation Alignment",
            "text": "Multilingual pre-trained models such as mBERT (Pires et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020) have exhibited promising abilities of cross-lingual transfer, even in the absence of explicit encouragement of learning aligned representations across languages. These models are typically pre-trained with multilingual masked language modeling (MMLM) (Pires et al., 2019) and translation masked language modeling (TLM) (Conneau and Lample, 2019).\nSubsequent works designed more pre-training objectives to guide representation alignment with parallel corpora at different granularities. For instance, Unicoder (Huang et al., 2019) integrates crosslingual word discovery (CWD) and cross-lingual paraphrase classification (CPC) to learn the underlying word alignments between two languages and provide an explicit focus on the alignment of sentences. InfoXLM (Chi et al., 2021) adds a sentencelevel contrastive learning loss for bilingual pairs to maximize the mutual information between translation sentence pairs. HICTL (Wei et al., 2021) proposed sentence-level and word-level contrastive learning to distinguish parallel sentences and related words. VECO2.0 (Zhang et al., 2023) adds token-level contrastive loss for synonym alignment with the thesaurus dictionary. In addition to using parallel corpora, ERNIE-M (Ouyang et al., 2021) integrates back-translation into the pre-training process to generate pseudo-parallel pairs for monolingual corpora. Moreover, ERNIE-M (Ouyang et al., 2021) and VECO (Luo et al., 2021) proposed a cross-attention module that builds interdependence between languages in model inner.\nExisting work has enhanced the alignment of cross-lingual representations at multiple granularities (token-level, word-level, and sentence-level). Considering that differences in syntactic structure between languages can also affect alignment, we focused on introducing syntactic structural level information into cross-lingual representations to enhance alignment. Since syntactic tags are challenging to obtain, especially for low-resource languages, we introduce reinforcement learning methods to learn multilingual structural information without explicit structure annotations and guide the model to learn cross-lingual aligned representations on machine translation ranking tasks."
        },
        {
            "heading": "2.2 Structure-augmented Cross-lingual transfer",
            "text": "The use of universal dependency parse structures has been found to be beneficial for cross-lingual transfer in cross-lingual NLP tasks (Ahmad et al., 2021; Fei et al., 2020; Zhang et al., 2022; Zhu, 2020; Xu et al., 2022). Many approaches have utilized these structures by fusing them with input sequences or incorporating them into self-attention mechanisms to enhance the learning of language syntax. Most of the work is task-oriented(Zhang et al., 2022; Wu et al., 2020; Zhu, 2020; Wu et al.,\n2022b; Xu et al., 2022; Wu et al., 2023), but Syntaxaugmented mBERT (Ahmad et al., 2021) enables structural learning without the inference phase by learning structural encoders and incorporating structural coding into self-attention mechanisms. So it can be generalized to arbitrary cross-lingual understanding tasks. However, these approaches require syntactically labeled data, which can be difficult to obtain in high quality across multiple languages.\nTo address this challenge, we follow Zhang et al. (2018) to utilize reinforcement learning to learn the universal structure for enhancing crosslingual transfer without explicit structure annotations. Zhang et al. (2018) proposed an RL method to learn monolingual sentence representation by discovering structure for text classification. We extended their method to improve the alignment of cross-lingual representation."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "In this paper, we propose a framework that integrates structural information to enhance the crosslingual transfer ability of multilingual PLM by improving the alignment of cross-lingual representations. The framework of the Struct-XLM model, shown in Figure 1, consists of three key components: the Policy Network (PNet), Structural Encoder, and Translation Ranking Task module. We\nadopt a Siamese framework to learn representations from parallel sentence pairs (X,Y ), with shared parameters across components. The structural encoder is initialized with a multilingual PLM and provides state representation S to PNet. PNet utilizes a stochastic policy to sample actions at each state, generating an action vector a = a1a2 \u00b7 \u00b7 \u00b7 aT for the sentence x = x1x2 \u00b7 \u00b7 \u00b7xT \u2208 X comprising T tokens. The action vector can represent the structure of the sentence. As depicted in Figure 2, we convert the action vector into an action matrix and feed it into the scaled dot-product self-attention module to construct a structural multi-head attention module in the last layer of PLM. The Structural Encoder then outputs a multilingual structural representation for the Translation Ranking Task module to calculate the ranking loss and provides a delayed reward to PNet. The reinforcement learning process is naturally handled using the policy gradient method (Sutton et al., 1999).\nThese three components are interleaved together: the state representation of PNet is derived from the Structural Encoder, the calculation of ranking loss relies on the final structured representation, and PNet receives rewards from the Translation Ranking Task module to guide policy learning."
        },
        {
            "heading": "3.2 Structural Encoder",
            "text": "Our structural encoder is initialized by multilingual PLM. For learning structural representation, we propose a structural multi-head attention module\nin the last layer of PLM, and one of the structural self-attention flowing is shown in Figure 2. The original attention probability matrix is denoted as:\nE = softmax( QKT\u221a\ndk ) (1)\nwhere the dot-product is scaled by 1\u221a dk , dk denotes the dimension of query matrix Q and key matrix K. In our structural multi-head attention module, the E is not only determined by Q and K but also Action Matrix A generated from the actor vector, which guides each word only pay attention to the words within the same constituent.\nA sentence consists of constituents, such as noun phrase or verb phrase. The action vector from PNet provides information about the boundaries of constituent structure in sentences (\u20190\u2019 and \u20191\u2019 denote the word is inside or is a end boundary of a constituent), where split the sentence into several constituents. In Figure 2, for example, the phrase \u2019a competitive edge\u2019 is identified as a constituent c by the sub action vector {0, 0, 1}. This action vector is then converted into a sub-action matrix:\nAc =\n \n0 1 1 1 0 1 1 1 0\n  (2)\nwhere Acij = 1 represents ith word pay attention to the jth word in this constituent. There is a special case when only one word is divided into a single constituent, its sub-action matrix is an identity matrix. The final Action Matrix A is obtained by splicing all the sub-action matrices by the diagonal and filling with 0 on the remaining positions. The detail of algorithm can be see in Appendix B\nBy combining the Action Matrix A with the softmax attention mechanism, we calculate the structural attention probability E as follows:\nE = A\u2299 softmax(QK T\n\u221a dk ) (3)\nwhere \u2299 represents element-wise multiplication. In the case of multi-head attention, n different heads share the same action matrix A and obtain the structural representation h with the dimension of dmodel = n\u00d7dk. The structural representation is then transformed into a sentence embedding using mean-pooling. Finally, the translation ranking loss is calculated based on this sentence embedding."
        },
        {
            "heading": "3.3 Translation Ranking Task",
            "text": "For enhancing cross-lingual representation, we introduce the translation ranking task with in-batch negative sampling(Yang et al., 2019a; Feng et al., 2022):\nL = \u2212 1 N\nN\u2211\ni=1\nlog e\u03d5(xi,yj)\u2212m\ne\u03d5(xi,yj)\u2212m + N\u2211\nn=1, n \u0338=j\ne\u03d5(xi,yn)\n(4) The embedding space similarity of sentence pairs x and y is given by \u03d5(x, y) = mean(hx)mean(hy)T . m is an additive margin around positive pairs, which improves the separation between translations and non-translations.\nConsidering that there are n-way parallel pairs in data, we introduce in-batch labels l \u2208 RN\u00d7N to improve the translation ranking loss. lij = 1 represents yj and xi are translation pair and lij = 0 represents yj and xi are non-translation pair. The N is batch size. The new loss can be defined as:\nL = \u2212 1 N\nN\u2211\ni=1\nlog\nN\u2211 j=1,lij=1 e\u03d5(xi,yj)\u2212m\nN\u2211 j=1, lij=1 e\u03d5(xi,yj)\u2212m + N\u2211 n=1, lin=0 e\u03d5(xi,yn)\n(5) The loss aims to rank the true translations of xi over all other alternatives in the same batch. L is asymmetric and depends on whether the softmax is applied to the source or target sentences. To achieve bidirectional symmetry, the final loss can\nbe obtained by summing the source-to-target loss, L, and the target-to-source loss, L\u2032:\nLtrans = L+ L\u2032 (6)"
        },
        {
            "heading": "3.4 Policy Network",
            "text": "The policy network includes a masked selfattention module, linear layer, and sigmoid layer. The masked self-attention limited each state st only to notice the previous state and predict the action in the sequence. We briefly introduce state, action and policy, reward, and objective function as follows: State Each state vector st with a dimension of dmodel is each token representation from the Structural Encoder without the input of the action vector. The complete state representation S = (s1, s2, . . . , sT ) \u2208 RT\u00d7dmodel is encoded into an action embedding space using a masked selfattention layer and a linear layer. Action and Policy We adopt binary actions {Inside, End} to discover constituent structure, indicating that a word is inside or at the end of a constituent. PNet adopts a stochastic policy \u03c0(at|st; \u0398), which represents the probability of selecting action at at state t. The policy is defined as follows:\n\u03c0(a|S; \u0398) = \u03c3(W \u2217 SA(S) + b) (7)\nSA(S) = softmax( QKT\u221a dmodel ) \u00b7 V, Q = WQS,K = WKS, V = W V S\n(8)\nwhere \u03c3 represents the sigmoid function, \u0398 = {(WQ,W k,W V ),W, b} denotes the parameters of PNet. WQ, WK , and WV are the weights for the query matrix Q, key matrix K, and value matrix V of masked self-attention layer SA(\u00b7), with dimension dmodel. The W and b are the weight and bias of the linear layer. During training, the action is sampled according to the probability in Eq. 7. During the test, the action with the maximal probability (i.e., a\u2217t = argmaxa\u03c0(at|st; \u0398)) is chosen to obtain the action vector prediction. Reward After sampling all the actions from PNet, the structural representation of a sentence x is determined by our Structural Encoder. This representation is then used to calculate the translation\nranking loss and reward. The reward is defined as:\nRL = log\nN\u2211 j=1,lj=1 e\u03d5(x,yj)\u2212m\nN\u2211 j=1,lj=1 e\u03d5(x,yj)\u2212m + N\u2211 n=1,ln=0 e\u03d5(x,yn)\n(9) where the lj is the label indicates the yj whether a translation of x. This reward is considered a delayed reward since it is obtained after constructing the entire representation. Objective Function Following Zhang et al. (2018), we optimize the parameters of PNet using REINFORCE algorithm (Sutton et al., 1999) and policy gradient methods, aiming to maximize the expected reward as shown below.\nJ (\u0398) = \u2211\ns1a1\u00b7\u00b7\u00b7sTaT\n\u220f\nt\n\u03c0\u0398(at|st)RL (10)\nand the gradient with likelihood ratio trick is defined as follows:\n\u25bd\u0398J (\u0398) = T\u2211\nt=1\nRL \u25bd\u0398 log\u03c0\u0398(at|st) (11)"
        },
        {
            "heading": "3.5 Training Process",
            "text": "Refer to Zhang et al. (2018), our training process consists of three steps. Firstly, we pre-train the Structural Encoder using the Translation Ranking Task. Then, we pre-train the PNet while keeping the parameters of the other two models fixed. Finally, we jointly train all three components. To alleviate the challenges of training RL from scratch and reduce variance, we also adopt a warm-start approach in the first step. Specifically, we collect a corpus of 13.7k sentence pairs from the UD 2.9 Treebank (Zeman et al., 2021), which provides syntax tree labels. These labels are only used as a heuristic signal to split sentences into constituents solely for the warm start in the first step."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Implementation Details",
            "text": "We used the XLM-Rlarge model as a base pretrained LM to initialize the Structural Encoder, and the whole Struct-XLM model has 564M parameters (559M for fine-tuning). To train our model, we collected an 8MB parallel corpus, including 13.7k sentence pairs (English is the source language), and more detail in Appendix A. In order to facilitate\nthe smooth update of the policy gradient during training, we introduced a suppression factor that is multiplied to Eq. 11. This factor is set to 0.5. During the training process, we employed the Adam optimization algorithm to update the model parameters. The initial learning rate was set to 6e-6 for the first step, which ran for 5 epochs. The second step ran for 2 epochs with a learning rate of 3e-6, and the last step utilized 5 epochs with a learning rate of 3e-6. The batch size is 5 for all steps."
        },
        {
            "heading": "4.2 Baselines",
            "text": "Except for the base model XLM-Rlarge, we compare the cross-lingual performance of our proposed model against 4 strong cross-lingual models:\n\u2022 Info-XLM focuses on maximizing mutual information between translation pairs. We compare Struct-XLM with its large size version (558M parameter), which continues training XLM-Rlarge on 42GB parallel corpora. \u2022 ERNIE-M incorporates cross-attention masked language modeling on both parallel and monolingual corpora. The parameter size of the large version model and the training data is the same as Info-XLM.\n\u2022 XY-LENT leverages novel sampling strategies with X-Y bitexts to learn the cross-lingual alignment representation. It has a twice bigger vocabulary size than XLM-Rlarge but with a smaller parameter size (477M for the base version) by training on ELECTRA-style tasks.\n\u2022 VECO2.0 bridges the representations of synonym pairs embedded in the bilingual corpus based on VECO. It has 559M parameters and is trained with 2.5TB monolingual data and 4TB parallel pairs.\nWe do not compare our model with Syntaxaugmented mBERT due to its small parameter size."
        },
        {
            "heading": "4.3 Evaluation",
            "text": "In our evaluation, we perform experiments on seven multilingual tasks from the XTREME (Hu et al., 2020) benchmark. These tasks cover a wide range of languages. For sentence-pair classification, we evaluate on the Cross-lingual Natrual Language Inference dataset (XNLI) (Conneau et al., 2018), and Cross-lingual Paraphrase Adversaries from Word Scrambling dataset (PAWS-X) (Yang et al., 2019b). For structured prediction, POS tagging from the Universal Dependencies v2.5 treebanks (Nvire et al., 2020) and NER from Wikiann (Pan et al., 2017). For cross-lingual question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), and the gold passage version of the TyDiQA dataset (TyDiQA-GoldP) (Clark et al., 2020). Here, for a cross-lingual setting, all tasks provide English training data and dev/test set in all involved languages. We consider the zero-shot cross-lingual setting, i.e. only using the English data for fine-tuning. The hyper-parameters setting for fine-tuning are shown in Appendix C."
        },
        {
            "heading": "4.4 Results",
            "text": "In Table 1, we present the results of Struct-XLM and baselines on the seven tasks of XTREME. The results demonstrate that Struct-XLM outperforms XLM-Rlarge and latest model VECO2.0 (Zhang et al., 2023) on all tasks, with an average improvement of 4.1% and 1.1% respectively. Compared to the best strong baseline InfoXLM (Chi et al., 2021), we have a competitive average performance and we only use 8MB of training data, which is\n1/5000 of parallel data used by InfoXLM. StructXLM also surpasses XY-LENT on 8/10 metrics, with an average advancement of 2.5%.\nIn sentence-pair classification tasks, Struct-XLM performs slightly better than the best model XYLENT on the PAWS-X by 0.4%. However, on the XNLI, it is inferior to Info-XLM by 0.9%. This may be because the translation ranking task in Struct-XLM focuses more on detecting meaning equivalence rather than fine-grained distinctions of meaning overlap. For structured prediction tasks, Struct-XLM outperforms all baselines and has the lowest transfer gap as shown in Table 2, which can be attributed to the structure discovery. In question answering, Struct-XLM is inferior to the best EM score by 0.5% on MLQA. However, it achieves the best performance on the XQuAD and TyDiQA datasets. It surpasses InfoXLM by 0.9% and 1.4% in EM and F1 metrics on TyDiQA, respectively.\nOverall, Struct-XLM delivers impressive performance with only an 8MB parallel corpus. The improvement can be attributed to the multilingual structured representation alignment achieved through RL and the translation ranking task. Further investigation and analysis will be conducted in the ablation study and analysis experiments."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Transfer Gap",
            "text": "Table 2 presents the transfer gap of Struct-XLM and baselines on the XTREME benchmark. The transfer gap refers to the difference in performance\nbetween the English test set and the average performance across other languages. It is observed that the transfer gap of Struct-XLM is relatively high for certain metrics, such as accuracy in XNLI and PAWS-X, and EM in TyDiQA. This is because the model\u2019s performance in English has significantly improved. However, Struct-XLM demonstrates a lower average transfer gap than baselines, indicating improved cross-lingual transferability."
        },
        {
            "heading": "5.2 Ablation study",
            "text": "To explore the impact of PNet and RL, we fine-tune the Structural Encoder Part (i.e. w/o action) or the model only trained by the first step (i.e. only warmstart). To discuss the significance of the warm-start step, we also evaluate the ablation model without it (w/o warm-start). Moreover, we train a language model that only has Structural Encoder and Translation Ranking Task module (i.e. w/o PNet). The results are shown in Table 3, and the results of Struct-XLM in 7 tasks always be the best. Specifically, the Struct-XLM only trained on the first\nstep (i.e. only warm-start) surpasses the language model without PNet (i.e. w/o PNet) with an average advancement of 0.4%, which indicates the significance of the simple structural inspire information to improve the cross-lingual transfer ability. The evaluation of the model without action (i.e. w/o action) also obtains a greater improvement than baseline XLM-Rlarge by 3.0% on the average. This suggests that RL training greatly enhances the cross-lingual transfer ability of the multilingual PLM, even without the subsequent use of the PNet module in fine-tuning. The model w/o warm-start doesn\u2019t surpass the Struct-XLM model, confirming the effectiveness of our warmstart procedure. From the results, we can observe that all components and training settings contribute to overall performance improvement."
        },
        {
            "heading": "5.3 Cross-Lingual Representations",
            "text": "To assess the alignment of cross-lingual sentence representations obtained from our Struct-XLM, we introduce the BUCC sentence retrieval task. Table 4 presents the F1 scores of four languages on BUCC, where the representations of our model and XLM-Rlarge and InfoXLM are extracted from the middle layer (13th layer) following the choice in Hu et al. (2020). The evaluation results demonstrate that Struct-XLM produces better-aligned cross-lingual sentence representations compared to XLM-Rlarge and InfoXLM, achieving an average performance improvement of 14.8% and 1.2%, respectively.\nFurthermore, we analyze the average results of all layers of the models on BUCC, as depicted in Figure 3. For XLM-Rlarge and InfoXLM, we observe a performance drop in the last few layers, which is expected as these layers are encouraged to focus on token-level embedding due to the pretrained objectives. On the other hand, Struct-XLM\nconsistently achieves high retrieval F1 scores even in the last few layers. Although the last layer of Struct-XLM without the action component (i.e., w/o action) exhibits a lower F1 score, it still benefits from enhancing the aligned knowledge of other layers to achieve excellent performance on XTREME.\nStructural probe (Hewitt and Manning, 2019; Chi et al., 2020) demonstrates that the middle layers of PLMs capture richer syntactic information. Struct-XLM obtain significant improvements in the middle layers, indicating that it has learned structural information and provides better-aligned representations compared to XLM-Rlarge."
        },
        {
            "heading": "5.4 Analyze of Discovered Structure",
            "text": "Quantitatively Analyze Struct-XLM aim to acquire structural information that facilitates crosslingual alignment, we employ retrieval tasks in Section 5.3 to quantitatively analyze whether the information obtained through RL contributes to crosslingual alignment. Furthermore, we try to conduct a quantitative analysis of the discovered structure by evaluating the constituents\u2019 boundary predicted by the action vector. The labeled test set is from the warm-start step, including English (en) and target languages (tgt) two-way. where the recall of the boundaries predicted by Struct-XLM is 66.84% and 57.58% in English and target language, respectively, which suggests that Struct-XLM is able to correctly predict the constituent boundaries in the labeled test set to some extent. Perhaps StructXLM tends to predict more constituents, so it gets low precision (P). We also report the results of the model w/o warm-start to prove the influence of the warm-start step for structural discovery. Qualitatively Analyze Table 6 presents some interesting structures discovered by Struct-XLM. Some constituents can form a sentence with complete meaning, such as \"\u4e58\u5ba2\u4e58\u5750\u7f06\u8f66\" (Passengers ride the gondola), while others may consist of just a single word. In these examples, the structures discovered by Struct-XLM exhibit more flexibility in terms of length and amount of constituents compared to predefined structures. In the case of En-\nglish, being a source language aligned with multiple languages, the number of constituents obtained tends to be smaller. However, Struct-XLM sometimes struggles to identify the correct boundaries between phrases, as seen in the example where the term \"Potomac River\" in Finnish (fi) is split into two parts. This task is extremely challenging for any model without explicit structure annotations."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, we introduced Struct-XLM, a novel approach to enhance cross-lingual transfer learning in multilingual language models. By leveraging reinforcement learning without explicit structure annotations, the Struct-XLM discovers structures and improves the alignment of cross-lingual representations. Specifically, we employ policy gradient RL and a translation ranking task to discover constituent syntactic structures and enhance crosslingual transfer. Experimental results on seven XTREME benchmark tasks demonstrated the effectiveness of Struct-XLM, outperforming baseline PLM models with an average improvement of 4.1 points and being competitive with the best strong baseline with small training data. Our analysis also revealed that Struct-XLM improves performance on the sentence retrieval task BUCC, particularly in the middle layers of the pre-trained model, indicating the acquisition of valuable structural information for better-aligned representations."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by the Fundamental Research Funds for the Central Universities (No. 226- 2023-00060), the Key Research and Development Program of Zhejiang Province (No. 2021C01013), National Key Research and Development Project of China (No. 2018AAA0101900), and MOE Engineering Research Center of Digital Library.\nLimitations\nWhile Struct-XLM presents advancements in improving cross-lingual representation alignment, it also has some limitations. We outline these limitations in this section.\nTraining Data\nThe strong baseline models we compare against may benefit from large amounts of parallel data for training, which contributes to their superior performance. Since policy gradient RL learning requires multiple sampling learning for each sample, our model takes about 30 hours to complete three stages of training on an NVIDIA RTX A6000 using 8MB of training data. In our case, due to computational constraints, we were only able to utilize a limited amount of parallel data (8M) for training. Increasing the amount of parallel data without labels in the last two steps could potentially improve the performance of our model and provide a better-aligned cross-lingual representation.\nLearning Structure from Middle Layer\nAs mentioned, the structural probe (Chi et al., 2020) has shown that the middle layer of multilingual PLMs captures rich syntactic information. However, it is worth considering that syntax and semantics are intertwined, and the primary goal of crosslingual alignment is semantic alignment. Therefore, we explore whether structures learned from the final layer, which captures more semantic information, can enhance alignment. If structures discovered from the middle layer representations can further improve alignment, it would provide additional evidence for the effectiveness of our method.\nSiamese Framework vs. Single Tower Version\nThe Siamese framework used in Struct-XLM, which employs separate encoders for parallel sen-\ntence pairs, is effective in capturing cross-lingual alignment (Guo et al., 2018; Yang et al., 2019a; Feng et al., 2022). However, its token-level alignment may not be sufficient. An alternative approach could involve concatenating the parallel sentence pairs into a single input, allowing for more comprehensive learning from the parallel corpus and potentially encouraging the policy network to capture more universal structures through attention mechanisms.\nOverall, these limitations provide opportunities for future research to further enhance the performance and capabilities of cross-lingual representation learning models like Struct-XLM."
        },
        {
            "heading": "A Statistic of Training Data",
            "text": "The training data for Struct-XLM is sourced from various treebanks in UD 2.9 (Zeman et al., 2021). In particular, we select 20 languages (including English) parallel universal dependencies treebanks from PUD2, Atis3, LinES (Ahrenberg, 2007), and ParTUT4 treebanks. English is the source language. The statistics on the number of sentences available for each language are shown in the table 7."
        },
        {
            "heading": "B Algorithm",
            "text": "The algorithm 1 is to convert the action vector to the action matrix, as mentioned in Section 3.2."
        },
        {
            "heading": "C Hyper-parameters for Fine-Tuning",
            "text": "In Table 8, we report the hyper-parameters for finetuning Struct-XLM on the XTREME seven tasks.\nAlgorithm 1 Convert the action vector to the action matrix Input: action vector a, length of sentence L Output: action matrix A 1: number of constituents p = 1 2: the list of constituents C, in which ith constituents ci\ninclude start index cstarti and end index c end i\n3: cstartp = 0 4: for j = 1 to L do 5: if a[j \u2212 1] == 1 then 6: cendp = j \u2212 1 7: p+ = 1 8: if j < L then 9: cstartp = j\n10: end if 11: end if 12: end for 13: cendp = L\u2212 1 14: for ci in C do 15: if cstarti == cendi then 16: sub-action matrix Aci = [1] 17: else 18: Aci = 1 \u2212 I, where 1, I \u2208 Rn\u00d7n,and n =\ncendi \u2212 cstarti + 1 is the length of constituent ci. 19: end ifsentence-wise sub-action matrix A\u0302ci =\n[0lAci0 r],where 0l \u2208 Rn\u00d7cstarti and 0r \u2208\nRn\u00d7(L\u2212cendi \u22121). 20: end for 21: return\nA =   A\u0302c1 A\u0302c2\n... A\u0302cp\n \n2http://universaldependencies.org/conll17/ 3https://github.com/howl-anderson/ATIS_dataset/ 4https://github.com/msang/partut-repo"
        },
        {
            "heading": "D Results for each task and language",
            "text": "We show the detailed results for all tasks and languages in Tables 9 (XNLI), 10 (PAWS-X), 11 (POS), 12 (NER), 13 (XQuAD), 14 (MLQA), 15 (TyDiQA-GoldP). \u2020 denotes the results from our re-implement.\nBased on the experimental results in Table 9-15, even languages without warm-start training data can show improvements. For instance, in Table 11, Basque(eu), Tagalog(tl), and Yoruba(yo) do not have training corpora, yet they exhibit significant improvements compared to the best baseline result, with increases of 1.2%, 1.9%, and 19.2%, respectively. Moreover, in Table 12, Persian(fa) and Malay(ms) without warm-start training corpora improve by 3.5% and 2.7% than the best baseline result."
        }
    ],
    "title": "Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning",
    "year": 2023
}