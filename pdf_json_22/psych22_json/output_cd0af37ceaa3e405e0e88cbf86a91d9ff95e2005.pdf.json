{
    "abstractText": "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. Computational Model of the Transition from Novice to Expert Interaction Techniques Gilles Bailly, Mehdi Khamassi, Beno\u00eet Girard",
    "authors": [
        {
            "affiliations": [],
            "name": "Gilles Bailly"
        },
        {
            "affiliations": [],
            "name": "Mehdi Khamassi"
        },
        {
            "affiliations": [],
            "name": "Beno\u00eet Girard"
        }
    ],
    "id": "SP:35821b4069e162a57a6342b529b727602f63cc6f",
    "references": [
        {
            "authors": [
                "Carlos Al\u00f3s-Ferrer",
                "Sabine H\u00fcgelsch\u00e4fer",
                "Jiahui Li"
            ],
            "title": "Inertia and decision making",
            "venue": "Frontiers in psychology",
            "year": 2016
        },
        {
            "authors": [
                "John R Anderson"
            ],
            "title": "Acquisition of cognitive skill",
            "venue": "Psychological review 89,",
            "year": 1982
        },
        {
            "authors": [
                "John R Anderson",
                "Daniel Bothell",
                "Michael D Byrne",
                "Scott Douglass",
                "Christian Lebiere",
                "Yulin Qin"
            ],
            "title": "An integrated theory of the mind",
            "venue": "Psychological review 111,",
            "year": 2004
        },
        {
            "authors": [
                "Caroline Appert",
                "Shumin Zhai"
            ],
            "title": "Using Strokes As Command Shortcuts: Cognitive Bene\uffffts and Toolkit Support",
            "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
            "year": 2009
        },
        {
            "authors": [
                "Peter Auer",
                "Nicolo Cesa-Bianchi",
                "Paul Fischer"
            ],
            "title": "Finite-time analysis of the multiarmed bandit problem",
            "venue": "Machine learning 47,",
            "year": 2002
        },
        {
            "authors": [
                "Gilles Bailly",
                "Emmanouil Giannisakis",
                "Marion Morel",
                "Catherine Achard"
            ],
            "title": "Characterize the Transition from Menus to Hotkeys. In Proceedings of the 30th Conference on l\u2019Interaction Homme-Machine (Brest, France) (IHM \u201918)",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Gilles Bailly",
                "Eric Lecolinet",
                "Laurence Nigay"
            ],
            "title": "Visual Menu Techniques",
            "venue": "ACM Comput. Surv",
            "year": 2016
        },
        {
            "authors": [
                "Gilles Bailly",
                "J\u00f6rg M\u00fcller",
                "Eric Lecolinet"
            ],
            "title": "Design and evaluation of \uffffnger-count interaction: Combining multitouch gestures and menus",
            "venue": "International Journal of Human-Computer Studies 70,",
            "year": 2012
        },
        {
            "authors": [
                "Gilles Bailly",
                "Antti Oulasvirta",
                "Duncan P. Brumby",
                "Andrew Howes"
            ],
            "title": "Model of Visual Search and Selection Time in Linear Menus",
            "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario,",
            "year": 2014
        },
        {
            "authors": [
                "Gilles Bailly",
                "Antti Oulasvirta",
                "Timo K\u00f6tzing",
                "Sabrina Hoppe"
            ],
            "title": "MenuOptimizer: Interactive Optimization of Menu Systems",
            "venue": "In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (St. Andrews, Scotland, United Kingdom) (UIST \u201913)",
            "year": 2013
        },
        {
            "authors": [
                "Gilles Bailly",
                "Thomas Pietrzak",
                "Jonathan Deber",
                "Daniel J. Wigdor"
            ],
            "title": "M\u00e9tamorphe: Augmenting Hotkey Usage with Actuated Keys. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris, France) (CHI \u201913)",
            "venue": "Association for Computing Machinery,",
            "year": 2013
        },
        {
            "authors": [
                "Robert W Baloh",
                "Andrew W Sills",
                "Warren E Kumley",
                "Vicente Honrubia"
            ],
            "title": "Quantitative measurement of saccade amplitude, duration, and velocity",
            "venue": "Neurology 25,",
            "year": 1975
        },
        {
            "authors": [
                "Nikola Banovic",
                "To\uffff Buzali",
                "Fanny Chevalier",
                "Jennifer Manko",
                "Anind K. Dey"
            ],
            "title": "Modeling and Understanding Human Routine Behavior. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201916)",
            "venue": "Association for Computing Machinery,",
            "year": 2016
        },
        {
            "authors": [
                "Marc G Berman",
                "John Jonides",
                "Richard L Lewis"
            ],
            "title": "In search of decay in verbal short-term memory",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition",
            "year": 2009
        },
        {
            "authors": [
                "Michael D Byrne"
            ],
            "title": "ACT-R/PM and menu selection: Applying a cognitive architecture to HCI",
            "venue": "International Journal of Human-Computer Studies 55,",
            "year": 2001
        },
        {
            "authors": [
                "Xiang Cao",
                "Shumin Zhai"
            ],
            "title": "Modeling Human Performance of Pen Stroke Gestures",
            "venue": "Association for Computing Machinery,",
            "year": 2007
        },
        {
            "authors": [
                "Stuart K. Card",
                "Allen Newell",
                "Thomas P. Moran"
            ],
            "title": "The Psychology of Human-Computer Interaction. L",
            "year": 1983
        },
        {
            "authors": [
                "John M. Carroll (Ed"
            ],
            "title": "Interfacing Thought: Cognitive Aspects of Human-Computer Interaction",
            "year": 1987
        },
        {
            "authors": [
                "Romain Caz\u00e9",
                "Mehdi Khamassi",
                "Lise Aubin",
                "Beno\u00eet Girard"
            ],
            "title": "Hippocampal replays under the scrutiny of reinforcement learning models",
            "venue": "Journal of neurophysiology 120,",
            "year": 2018
        },
        {
            "authors": [
                "Noshaba Cheema",
                "Laura A. Frey-Law",
                "Kourosh Naderi",
                "Jaakko Lehtinen",
                "Philipp Slusallek",
                "Perttu H\u00e4m\u00e4l\u00e4inen"
            ],
            "title": "Predicting Mid-Air Interaction Movements and Fatigue Using Deep Reinforcement Learning",
            "venue": "In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,",
            "year": 2020
        },
        {
            "authors": [
                "Xiuli Chen",
                "Gilles Bailly",
                "Duncan P. Brumby",
                "Antti Oulasvirta",
                "Andrew Howes"
            ],
            "title": "The Emergence of Interactive Behavior: A Model of Rational Menu Search",
            "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Republic of Korea) (CHI \u201915)",
            "year": 2015
        },
        {
            "authors": [
                "Xiuli Chen",
                "Sandra Dorothee Starke",
                "Chris Baber",
                "Andrew Howes"
            ],
            "title": "A Cognitive Model of How People Make Decisions Through Interaction with Visual Displays",
            "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado,",
            "year": 2017
        },
        {
            "authors": [
                "Fran\u00e7ois Cinotti",
                "Virginie Fresno",
                "Nassim Aklil",
                "Etienne Coutureau",
                "Beno\u00eet Girard",
                "Alain R Marchand",
                "Mehdi Khamassi"
            ],
            "title": "Dopamine blockade impairs the exploration-exploitation trade-o\uffff in rats",
            "venue": "Scienti\uffffc reports 9,",
            "year": 2019
        },
        {
            "authors": [
                "Andy Cockburn",
                "Carl Gutwin"
            ],
            "title": "A predictive model of human performance with scrolling and hierarchical lists",
            "venue": "Human\u2013Computer Interaction 24,",
            "year": 2009
        },
        {
            "authors": [
                "Andy Cockburn",
                "Carl Gutwin",
                "Saul Greenberg"
            ],
            "title": "A PredictiveModel ofMenu Performance. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201907)",
            "year": 2007
        },
        {
            "authors": [
                "Andy Cockburn",
                "Carl Gutwin",
                "Joey Scarr",
                "Sylvain Malacria"
            ],
            "title": "Supporting Novice to Expert Transitions in User Interfaces",
            "venue": "ACM Comput. Surv",
            "year": 2014
        },
        {
            "authors": [
                "Nathaniel D. Daw"
            ],
            "title": "Trial-by-trial data analysis using computational models. Oxford University Press. https://doi.org/10.1093/acprof: oso/9780199600434.003.0001 Publisher Copyright: \u00a9 The International Association for the study",
            "venue": "Attention and Performance,",
            "year": 2011
        },
        {
            "authors": [
                "Nathaniel D. Daw"
            ],
            "title": "Advanced Reinforcement Learning",
            "venue": "https://doi.org/10.1016/B978-0-12-416008-8.00016-4 Copyright: Copyright",
            "year": 2013
        },
        {
            "authors": [
                "Nathaniel D Daw",
                "Yael Niv",
                "Peter Dayan"
            ],
            "title": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
            "venue": "Nature neuroscience 8,",
            "year": 2005
        },
        {
            "authors": [
                "Peter Dayan",
                "Nathaniel D Daw"
            ],
            "title": "Decision theory, reinforcement learning, and the brain. Cognitive, A\uffffective",
            "venue": "Behavioral Neuroscience 8,",
            "year": 2008
        },
        {
            "authors": [
                "Laurent Doll\u00e9",
                "Denis Sheynikhovich",
                "Beno\u00eet Girard",
                "Ricardo Chavarriaga",
                "Agn\u00e8s Guillot"
            ],
            "title": "Path planning versus cue responding: a bio-inspired model of switching between navigation strategies",
            "venue": "Biological cybernetics 103,",
            "year": 2010
        },
        {
            "authors": [
                "Kenji Doya"
            ],
            "title": "Modulators of decision making",
            "venue": "Nature neuroscience 11,",
            "year": 2008
        },
        {
            "authors": [
                "R\u2019emi Dromnelle",
                "B. Girard",
                "Erwan Renaudo",
                "R. Chatila",
                "M. Khamassi"
            ],
            "title": "Coping with the variability in humans reward during simulated human-robot interactions through the coordination of multiple learning strategies",
            "venue": "29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
            "year": 2020
        },
        {
            "authors": [
                "R\u00e9mi Dromnelle",
                "Erwan Renaudo",
                "Guillaume Pourcel",
                "Raja Chatila",
                "Beno\u00eet Girard",
                "Mehdi Khamassi"
            ],
            "title": "How to Reduce Computation Time While Sparing Performance During Robot Navigation? A Neuro-Inspired Architecture for Autonomous Shifting Between Model-Based and Model-Free Learning",
            "venue": "In Biomimetic and Biohybrid Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Morris Fitts",
                "Michael I Posner"
            ],
            "title": "Human performance. Brooks/Cole",
            "year": 1967
        },
        {
            "authors": [
                "Wai-Tat Fu",
                "Wayne D. Gray"
            ],
            "title": "Resolving the paradox of the active user: stable suboptimal performance in interactive tasks",
            "venue": "Cognitive Science 28,",
            "year": 2004
        },
        {
            "authors": [
                "Wai-Tat Fu",
                "Peter Pirolli"
            ],
            "title": "SNIF-ACT: A Cognitive Model of User Navigation on the World",
            "venue": "Wide Web. Human\u2013Computer Interaction 22,",
            "year": 2007
        },
        {
            "authors": [
                "Christoph Gebhardt",
                "Antti Oulasvirta",
                "Otmar Hilliges"
            ],
            "title": "Hierarchical Reinforcement Learning Explains Task Interleaving Behavior",
            "venue": "Computational Brain & Behavior (2020),",
            "year": 2020
        },
        {
            "authors": [
                "Samuel J Gershman"
            ],
            "title": "Origin of perseveration in the trade-o\uffff between reward and complexity. Cognition",
            "year": 2020
        },
        {
            "authors": [
                "Emmanouil Giannisakis",
                "Gilles Bailly",
                "Sylvain Malacria",
                "Fanny Chevalier"
            ],
            "title": "IconHK: Using Toolbar Button Icons to Communicate Keyboard Shortcuts",
            "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado,",
            "year": 2017
        },
        {
            "authors": [
                "Wayne D Gray",
                "Deborah A Boehm-Davis"
            ],
            "title": "Milliseconds matter: An introduction to microstrategies and to their use in describing and predicting interactive behavior",
            "venue": "Journal of Experimental Psychology: Applied 6,",
            "year": 2000
        },
        {
            "authors": [
                "Wayne D. Gray",
                "John K. Lindstedt"
            ],
            "title": "Plateaus, Dips, and Leaps: Where to Look for Inventions and Discoveries During Skilled Performance. Cognitive Science (2016), n/a\u2013n/a",
            "year": 2016
        },
        {
            "authors": [
                "Wayne D Gray",
                "Chris R Sims",
                "Wai-Tat Fu",
                "Michael J Schoelles"
            ],
            "title": "The soft constraints hypothesis: a rational analysis approach to resource allocation for interactive behavior",
            "venue": "Psychological review 113,",
            "year": 2006
        },
        {
            "authors": [
                "Tovi Grossman",
                "Pierre Dragicevic",
                "Ravin Balakrishnan"
            ],
            "title": "Strategies for Accelerating On-line Learning of Hotkeys",
            "venue": "Computational Model of the Transition from Novice to Expert Interaction Techniques",
            "year": 2007
        },
        {
            "authors": [
                "Carl Gutwin",
                "Andy Cockburn",
                "Joey Scarr",
                "Sylvain Malacria",
                "Scott C. Olson"
            ],
            "title": "Faster Command Selection on Tablets with FastTap. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI \u201914)",
            "venue": "Association for Computing Machinery,",
            "year": 2014
        },
        {
            "authors": [
                "Masahiko Haruno",
                "Mitsuo Kawato"
            ],
            "title": "Heterarchical reinforcement-learning model for integration of multiple cortico-striatal loops: fMRI examination in stimulus-action-reward association learning",
            "venue": "Neural networks 19,",
            "year": 2006
        },
        {
            "authors": [
                "Anthony J. Hornof",
                "David E. Kieras"
            ],
            "title": "Cognitive Modeling Reveals Menu Search in Both Random and Systematic",
            "venue": "In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (Atlanta, Georgia,",
            "year": 1997
        },
        {
            "authors": [
                "R.A. Howard"
            ],
            "title": "Dynamic Programming and Markov Processes",
            "year": 1960
        },
        {
            "authors": [
                "Poika Isokoski"
            ],
            "title": "Model for Unistroke Writing Time. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Seattle, Washington, USA) (CHI \u201901)",
            "venue": "Association for Computing Machinery,",
            "year": 2001
        },
        {
            "authors": [
                "Arthur M Jacobs",
                "Jonathan Grainger"
            ],
            "title": "Models of visual word recognition: sampling the state of the art",
            "venue": "Journal of Experimental Psychology: Human perception and performance 20,",
            "year": 1994
        },
        {
            "authors": [
                "Leslie Pack Kaelbling",
                "Michael L Littman",
                "Andrew W Moore"
            ],
            "title": "Reinforcement learning: A survey",
            "venue": "Journal of arti\uffffcial intelligence research",
            "year": 1996
        },
        {
            "authors": [
                "Antti Kangasr\u00e4\u00e4si\u00f6",
                "Kumaripaba Athukorala",
                "Andrew Howes",
                "Jukka Corander",
                "Samuel Kaski",
                "Antti Oulasvirta"
            ],
            "title": "Inferring Cognitive Models from Data Using Approximate Bayesian Computation",
            "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado,",
            "year": 2017
        },
        {
            "authors": [
                "Mehdi Keramati",
                "Amir Dezfouli",
                "Payam Piray"
            ],
            "title": "Speed/accuracy trade-o\uffff between the habitual and the goal-directed processes",
            "venue": "PLoS Comput Biol 7,",
            "year": 2011
        },
        {
            "authors": [
                "Mehdi Khamassi",
                "Mark D Humphries"
            ],
            "title": "Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies. Frontiers in behavioral neuroscience",
            "year": 2012
        },
        {
            "authors": [
                "Michel C.A. Klein",
                "Nataliya Mogles",
                "Jan Treur",
                "Arlette van Wissen"
            ],
            "title": "A Computational Model of Habit Learning to Enable Ambient Support for Lifestyle Change",
            "venue": "In Modern Approaches in Applied Intelligence,",
            "year": 2011
        },
        {
            "authors": [
                "Etienne Koechlin",
                "Christopher Summer\uffffeld"
            ],
            "title": "An information theoretical approach to prefrontal executive function",
            "venue": "Trends in cognitive sciences 11,",
            "year": 2007
        },
        {
            "authors": [
                "Nils Kolling",
                "Marco K Wittmann",
                "Tim EJ Behrens",
                "Erie D Boorman",
                "Rogier B Mars",
                "Matthew FS Rushworth"
            ],
            "title": "Value, search, persistence and model updating in anterior cingulate cortex",
            "venue": "Nature neuroscience 19,",
            "year": 2016
        },
        {
            "authors": [
                "Brian Krisler",
                "Richard Alterman"
            ],
            "title": "Training Towards Mastery: Overcoming the Active User Paradox. In Proceedings of the 5th Nordic Conference on Human-computer Interaction: Building Bridges (Lund, Sweden) (NordiCHI \u201908)",
            "venue": "https://doi.org/",
            "year": 2008
        },
        {
            "authors": [
                "Gordon Paul Kurtenbach"
            ],
            "title": "The design and evaluation of marking menus",
            "venue": "Ph.D. Dissertation",
            "year": 1993
        },
        {
            "authors": [
                "David M Lane",
                "H Albert Napier",
                "S Camille Peres",
                "Aniko Sandor"
            ],
            "title": "Hidden Costs of Graphical User Interfaces: Failure to Make the Transition from Menus and Icon Toolbars to Keyboard Shortcuts",
            "venue": "International Journal of Human-Computer Interaction",
            "year": 2005
        },
        {
            "authors": [
                "Eric Lee",
                "James MacGregor"
            ],
            "title": "Minimizing user search time in menu retrieval systems",
            "venue": "Human Factors 27,",
            "year": 1985
        },
        {
            "authors": [
                "Katri Leino",
                "Antti Oulasvirta",
                "Mikko Kurimo"
            ],
            "title": "RL-KLM: Automating Keystroke-Level Modeling with Reinforcement Learning. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI \u201919)",
            "venue": "Association for Computing Machinery,",
            "year": 2019
        },
        {
            "authors": [
                "Luis A. Leiva",
                "Daniel Mart\u00edn-Albo",
                "R\u00e9jean Plamondon",
                "Radu-Daniel Vatavu"
            ],
            "title": "KeyTime: Super-Accurate Prediction of Stroke Gesture Production Times. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918)",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Esther Levin",
                "Roberto Pieraccini",
                "Wieland Eckert"
            ],
            "title": "Using Markov decision process for learning dialogue strategies",
            "venue": "In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\u201998 (Cat. No. 98CH36181),",
            "year": 1998
        },
        {
            "authors": [
                "Blaine Lewis",
                "Greg d\u2019Eon",
                "Andy Cockburn",
                "Daniel Vogel"
            ],
            "title": "KeyMap: Improving Keyboard Shortcut Vocabulary Using Norman\u2019s Mapping",
            "venue": "In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)",
            "year": 2020
        },
        {
            "authors": [
                "Richard L Lewis",
                "Andrew Howes",
                "Satinder Singh"
            ],
            "title": "Computational rationality: Linking mechanism and behavior through bounded utility maximization",
            "venue": "Topics in cognitive science 6,",
            "year": 2014
        },
        {
            "authors": [
                "Yang Li",
                "Samy Bengio",
                "Gilles Bailly"
            ],
            "title": "Predicting Human Performance in Vertical Menu Selection Using Deep Learning. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918)",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Wanyu Liu",
                "Gilles Bailly",
                "Andrew Howes"
            ],
            "title": "E\uffffects of frequency distribution on linear menu performance",
            "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
            "year": 2017
        },
        {
            "authors": [
                "S Abraham"
            ],
            "title": "Mechanization in problem solving: The e\uffffect of Einstellung",
            "venue": "Luchins",
            "year": 1942
        },
        {
            "authors": [
                "Alan Lundgard",
                "Yiwei Yang",
                "Maya L. Foster",
                "Walter S. Lasecki"
            ],
            "title": "Bolt: Instantaneous Crowdsourcing via Just-in-Time Training. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918)",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Sylvain Malacria",
                "Gilles Bailly",
                "Joel Harrison",
                "Andy Cockburn",
                "Carl Gutwin"
            ],
            "title": "Promoting Hotkey Use Through Rehearsal with ExposeHK",
            "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris,",
            "year": 2013
        },
        {
            "authors": [
                "Sylvain Malacria",
                "Joey Scarr",
                "Andy Cockburn",
                "Carl Gutwin",
                "Tovi Grossman"
            ],
            "title": "Skillometers: Re\uffffective Widgets That Motivate and Help Users to Improve Performance",
            "venue": "In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (St",
            "year": 2013
        },
        {
            "authors": [
                "Brendon Matusch",
                "Jimmy Ba",
                "Danijar Hafner"
            ],
            "title": "Evaluating Agents without Rewards",
            "year": 2021
        },
        {
            "authors": [
                "A. Newell",
                "P.S. Rosenbloom"
            ],
            "title": "Mechanisms of Skill Acquisition and the Law of Practice",
            "year": 1993
        },
        {
            "authors": [
                "Daniel L Odell",
                "Richard C Davis",
                "Andrew Smith",
                "Paul K Wright"
            ],
            "title": "Toolglasses, marking menus, and hotkeys: a comparison of one and two-handed command selection techniques",
            "venue": "Proceedings of Graphics Interface - GI",
            "year": 2004
        },
        {
            "authors": [
                "John P O\u2019Doherty",
                "Je\uffffrey Cockburn",
                "Wolfgang M Pauli"
            ],
            "title": "Learning, reward, and decision making",
            "venue": "Annual review of psychology",
            "year": 2017
        },
        {
            "authors": [
                "Richard C Omanson",
                "Craig S Miller",
                "Elizabeth Young",
                "David Schwantes"
            ],
            "title": "Comparison of Mouse and Keyboard E\uffffciency E\uffffects of Practice",
            "year": 2010
        },
        {
            "authors": [
                "Stefano Palminteri",
                "Valentin Wyart",
                "Etienne Koechlin"
            ],
            "title": "The importance of falsi\uffffcation in computational cognitive modeling",
            "venue": "Trends in cognitive sciences 21,",
            "year": 2017
        },
        {
            "authors": [
                "S. Camille Peres",
                "II Franklin P. Tamborello",
                "II Michael D. Fleetwood",
                "II Phillip Chung",
                "II Danielle L. Paige-Smith"
            ],
            "title": "Keyboard Shortcut Usage: The Roles of Social Factors and Computer Experience",
            "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting 48,",
            "year": 2004
        },
        {
            "authors": [
                "Philip Quinn",
                "Andy Cockburn"
            ],
            "title": "Loss Aversion and Preferences in Interaction",
            "venue": "Human\u2013Computer Interaction 35,",
            "year": 2020
        },
        {
            "authors": [
                "Philip Quinn",
                "Shumin Zhai"
            ],
            "title": "Modeling Gesture-Typing Movements",
            "venue": "Human\u2013Computer Interaction 33,",
            "year": 2018
        },
        {
            "authors": [
                "Adrian E Raftery"
            ],
            "title": "Bayesian model selection in social research",
            "venue": "Sociological methodology",
            "year": 1995
        },
        {
            "authors": [
                "Roger W Remington",
                "Ho Wang Holman Yuen",
                "Harold Pashler"
            ],
            "title": "2016. With practice, keyboard shortcuts become faster than menu selection: A crossover interaction",
            "venue": "Journal of Experimental Psychology: Applied 22,",
            "year": 2016
        },
        {
            "authors": [
                "Ren\u00e9 Riedl",
                "Adriane B Randolph",
                "Jan vom Brocke",
                "Pierre-Majorique L\u00e9ger",
                "Angelika Dimoka"
            ],
            "title": "The potential of neuroscience for human-computer interaction research",
            "venue": "SIGHCI 2010 Proceedings",
            "year": 2010
        },
        {
            "authors": [
                "Joey Scarr",
                "Andy Cockburn",
                "Carl Gutwin",
                "Philip Quinn"
            ],
            "title": "Dips and Ceilings: Understanding and Supporting Transitions to Expertise in User Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI \u201911)",
            "venue": "Association for Computing Machinery,",
            "year": 2011
        },
        {
            "authors": [
                "Noah A Shamosh",
                "Colin G DeYoung",
                "Adam E Green",
                "Deidre L Reis",
                "Matthew R Johnson",
                "Andrew RA Conway",
                "Randall W Engle",
                "Todd S Braver",
                "Jeremy R Gray"
            ],
            "title": "Individual di\ufffferences in delay discounting: relation to intelligence, working memory, and anterior prefrontal cortex",
            "venue": "Psychological science 19,",
            "year": 2008
        },
        {
            "authors": [
                "Catherine Sibert",
                "Wayne D Gray",
                "John K Lindstedt"
            ],
            "title": "Interrogating feature learning models to discover insights into the development of human expertise in a real-time, dynamic decision-making task",
            "venue": "Topics in cognitive science 9,",
            "year": 2017
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 1998
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 2018
        },
        {
            "authors": [
                "Susanne Tak",
                "Piet Westendorp",
                "Iris van Rooij"
            ],
            "title": "Satis\uffffcing and the Use of Keyboard Shortcuts: Being Good Enough Is Enough? Interacting with computers",
            "year": 2013
        },
        {
            "authors": [
                "Robert Tobias"
            ],
            "title": "Changing behavior by memory aids: A social psychological model of prospective memory and habit development tested with dynamic \uffffeld data",
            "venue": "Psychological review 116,",
            "year": 2009
        },
        {
            "authors": [
                "Kashyap Todi",
                "Gilles Bailly",
                "Luis A Leiva",
                "Antti Oulasvirta"
            ],
            "title": "Adapting user interfaces with model-based reinforcement learning",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Pramod Verma"
            ],
            "title": "Gracoli: A Graphical Command Line User Interface",
            "venue": "Association for Computing Machinery,",
            "year": 2013
        },
        {
            "authors": [
                "Guillaume Viejo",
                "Mehdi Khamassi",
                "Andrea Brovelli",
                "Beno\u00eet Girard"
            ],
            "title": "Modeling choice and reaction time during arbitrary visuomotor learning through the coordination of adaptive working memory and reinforcement learning",
            "venue": "Frontiers in behavioral neuroscience",
            "year": 2015
        },
        {
            "authors": [
                "Pauli Virtanen",
                "Ralf Gommers",
                "Travis E Oliphant",
                "Matt Haberland",
                "Tyler Reddy",
                "David Cournapeau",
                "Evgeni Burovski",
                "Pearu Peterson",
                "Warren Weckesser",
                "Jonathan Bright"
            ],
            "title": "SciPy 1.0: fundamental algorithms for scienti\uffffc computing in Python",
            "venue": "Nature methods 17,",
            "year": 2020
        },
        {
            "authors": [
                "Jan vom Brocke",
                "Ren\u00e9 Riedl",
                "Pierre-Majorique L\u00e9ger"
            ],
            "title": "Neuroscience in Design-Oriented Research: Exploring New Potentials",
            "venue": "In Service- Oriented Perspectives in Design Science Research,",
            "year": 2011
        },
        {
            "authors": [
                "Nefs Walker",
                "Judith Reitmun Olson"
            ],
            "title": "Designing keybindings to be easy to learn and resistant to forgetting even when the set of commands is large",
            "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems",
            "year": 1988
        },
        {
            "authors": [
                "Mark E Walton",
                "Timothy EJ Behrens",
                "Mark J Buckley",
                "Peter H Rudebeck",
                "Matthew FS Rushworth"
            ],
            "title": "Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning",
            "venue": "Neuron 65,",
            "year": 2010
        },
        {
            "authors": [
                "Robert C Wilson",
                "Anne GE Collins"
            ],
            "title": "Ten simple rules for the computational modeling of behavioral data",
            "venue": "Elife 8 (2019),",
            "year": 2019
        },
        {
            "authors": [
                "Jingjie Zheng",
                "Daniel Vogel"
            ],
            "title": "Finger-Aware Shortcuts. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201916)",
            "venue": "Association for Computing Machinery,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "HAL Id: hal-03537963 https://hal.sorbonne-universite.fr/hal-03537963\nSubmitted on 20 Jan 2022\nHAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s.\nComputational Model of the Transition from Novice to Expert Interaction Techniques\nGilles Bailly, Mehdi Khamassi, Beno\u00eet Girard\nTo cite this version: Gilles Bailly, Mehdi Khamassi, Beno\u00eet Girard. Computational Model of the Transition from Novice to Expert Interaction Techniques. ACM Transactions on Computer-Human Interaction, 2022, 10.1145/3505557. hal-03537963\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\nComputational Model of the Transition from Novice to Expert Interaction Techniques\nGILLES BAILLY, Sorbonne Universit\u00e9, CNRS, France MEHDI KHAMASSI and BENO\u00ceT GIRARD\u2217, Sorbonne Universit\u00e9, CNRS, France\nDespite the benets of expert interaction techniques, many users do not learn them and continue to use novice ones. This article aims at better understanding if, when and how users decide to learn and ultimately adopt expert interaction techniques. This dynamic learning process is a complex skill-acquisition and decision-making problem. We rst present and compare three generic benchmark models, inspired by the neuroscience literature, to explain and predict the learning process for shortcut adoption. Results show that they do not account for the complexity of users\u2019 behavior. We then introduce a dedicated model, Transition, combining ve cognitive mechanisms: implicit and explicit learning, decay, planning and perseveration. Results show that our model outperforms the three benchmark models both in terms of model tting and model simulation. Finally, a post-analysis shows that each of the ve mechanisms contribute to goodness-of-t, but the role of perseveration is unclear regarding model simulation.\nCCS Concepts: \u2022 Human-centered computing ! HCI theory, concepts and models.\nAdditional Key Words and Phrases: Computational models; Interaction Techniques; Shortcut; Menus; Computational Rationality\nACM Reference Format:"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Expert interaction techniques such as keyboard shortcuts, gesture shortcuts or command languages allow users to reach a high level of performance in comparison with novice interaction techniques such as menus, palettes or ribbons. Expert interaction techniques are generally faster and let users focus on their main task because they do not rely on visual search [7]. However, they require an initial eort to learn how to use them and memorize the mapping between the commands and the corresponding shortcuts. This learning eort might be too high and many users, even experienced users do not adopt expert interaction techniques and continue to use what might appear as \u2018suboptimal\u2019 interactions [26, 87]. It results that several commercial (e.g. ShortcutFoo, KeyRocket, CheatSheet, Application Shortcut Mapper) and academic methods (e.g.[40, 44, 58, 65, 71, 72, 99, 99, 103]) are regularly developed to address this problem.\nWhile several methods have been proposed, it remains unclear what are the human factors and cognitive mechanisms facilitating expert interaction technique adoption. Several theoretical constructs and frameworks have been proposed to explain why many users do not adopt expert interaction techniques [18, 36, 41, 43, 69]. This includes the Einstellung \u2217Both authors contributed equally to this research.\nAuthors\u2019 addresses: Gilles Bailly, Sorbonne Universit\u00e9, CNRS, France, rst.name@sorbonne-universite.fr; Mehdi Khamassi, rst.name@sorbonneuniversite.fr; Beno\u00eet Girard, Sorbonne Universit\u00e9, CNRS, France, rst.name@sorbonne-universite.fr.\n1\n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104\neect [69], the \"soft constraint hypothesis\" [43] or the paradox of the active user [18]. For instance, users tend to \"exploit\" prior experiences (or previous techniques such as menus) to achieve the task (short term productivity) rather than \"exploring\" more ecient techniques such as shortcuts for long-term eciency [18]. While these theories and frameworks present high-level explanations, they do not allow ne-grained predictions, i.e. simulating various cognitive, individual and environmental factors to test hypothetical designs and scenarios.\nPredicting why, when and how users adopt or do not adopt expert interaction techniques is challenging because several phenomena are involved. First, it is a high-level decision making problem as the two interaction techniques co-exist: for each action, the users can decide to use the novice or expert interaction technique. This is also a learning problem as users, for instance, need to learn the mapping between the commands and the shortcuts to successfully exploit the expert interaction technique. Users can also forget what they have previously learned due to memory decay introducing complex learning and decision making dynamics.\nIn this paper, we focus on scenarios where a single operation can be performed with one among two possible interaction techniques, always available in a Graphical User Interface (GUI). One is dedicated to novice users and the other provides a higher level of performance for experienced users but requires learning eorts. A typical scenario is the transition from menus to keyboard shortcuts in the aim of selecting commands more eciently. We present and compare computational models predicting the whole learning process leading (or not) to the adoption of shortcuts. In other words, the models explain and predict if and when users decide to learn shortcuts, then how they learn them and if ultimately they adopt them. Our general approach relies on computational rationality.\nComputational rationality is at the convergence of articial intelligence, cognitive science and neuroscience [39, 66]. The key idea is that humans (or animals) are rational agents with bounds: they choose actions maximising long-term expected rewards (or utility) given their limited cognitive resources (e.g. their own neural architecture), the constraints of the environment (e.g. task, technique, device) and their own experience. In practice, it consists of computing the values of actions, reecting the long-term outcomes associated with these actions under uncertainty. In our context, we assume that users make a choice among three high-level actions before executing a command (as opposed to low-level actions, such as clicking): deciding to execute a command as fast as possible (1) within the novice technique or (2) with a known expert technique or (3) deciding to learn the expert technique now to be able to successfully use it later on. The problem the users face is which action to choose now to minimize time on a nite horizon given limited cognitive resources, e.g. memorization, and the constraints of the environments, i.e. the available set of commands and their relative frequencies.\nReinforcement Learning (RL) is an appropriate formal framework for computational rationality and to study subtle interactions between learning and decision-making [27, 39, 57, 96]. For instance, it is extensively used in neuroscience [57, 96], cognitive sciences [27] and more recently in HCI [21, 22] to determine the policies that maximise long-term expected utility.\nWe rst introduce three benchmark RL models in neuroscience, which are widely used in decision-making tasks involving a learning process [101]: Rescorla-Wagner, Choice Kernel and the combination of both. They have in common to rely on an exploration - exploitation mechanism. While the Rescorla-Wagner model learns the expected value of each action based on the history of previous rewards, the Choice Kernel model captures the tendency of users to repeat previous actions regardless of their outcome. Rescorla-Wagner+Choice Kernel mixes the two models. These model-free RL models, i.e. RL models without a representation of the environment, have several advantages to model how users adopt expert interaction techniques. They are task-independent, have few free parameters, are easy to implement, are fast and have been shown to well capture learning and decision-making dynamics in dierent contexts [101]. Manuscript submitted to ACM\n105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\nWe compare these models on the data collection of Grossman et al. [44] investigating the impact of three interaction methods on keyboard shortcut adoption. One key aspect is to apply cutting-edge methods from the decision-making eld [101] to HCI: We rst compare their goodness of t to reect the capacity of these models to replicate each participant\u2019s trial-by-trial action choice, i.e. whole learning process for each participant. We then estimate the best parameters for each model and each participant. We nally simulate these models with the best parameters to test whether they do reproduce the main behavioral properties of the participants. The results suggest that these benchmark models are not sucient to capture the complexity of shortcut adoption.\nWe then present a novel computational model, called T, dedicated to explain and predict shortcut adoption. It relies on the computational rationality principles and is inspired by neuroscience. The core of this model is the combination of ve mechanisms to update the cognitive state of the users:\n(1) a planning mechanism reects the ability of users to consider several actions ahead. This mechanism is necessary to explain why users invest some time now foreseeing the benets of using shortcuts later. (2) a mixture of implicit and explicit learning mechanisms serves to consolidate at dierent learning rates the command-to-shortcut mapping in memory. (3) a decay mechanism reecting that the command-to-shortcut mappings encoded in memory fades due to the passage of time. (4) a perseveration mechanism, based on evidence in neuroscience [101], reects the fact that users are likely to repeat the previous strategy, regardless of the strategy.\nWe tested our model on the Grossman et al. data collection [44]. Results show our T model outperforms the three benchmark models both in term of likelihood and BIC score. The T model also synthesises more realistic data when simulating.\nWe then conducted a post-analysis to better understand the impact of the dierent mechanisms involved in our model. To achieve this, we compared ve variants of the model by enabling/disabling the dierent mechanisms. Results show that (1) all the proposed mechanisms play a role in shortcut adoption as they improve the goodness-of-t; but (2) the variant of the T model without the perseveration mechanism better synthesizes human behavioral data.\nFrom a methodological point of view, these results highlight the importance of combining the two evaluation methods [80] \u2014 goodness of t and model simulation \u2014 to validate models of shortcut adoption as they provide dierent perspectives on the model and its variants.\nIn summary, our primary contribution is the development, analysis and evaluation of a new computational model of expert interaction technique adoption and more precisely shortcuts adoption. This model is a rst step towards a better understanding of the complex learning dynamics involved in expert interaction technique adoption; Our long term objective is to facilitate designers workow to choose interaction techniques. Indeed, computational models can serve to analyze dierent designs and scenarios by running model simulations, reducing the cost (time, money) of experimental studies. Once these models can evaluate a design, they can be integrated in optimisation algorithms to propose high-value solutions [10] for a population of users. Finally, these models can be embedded in intelligent systems to dynamically predict the eect of an intervention at the level of an individual (instead of the population). They allow for an AI to assist individual users and promote the adoption of interaction methods best suited at their task [94].\nFinally, our contribution is also to promote valuable cross-disciplinary exchanges on questions, models and methods between neuroscience and HCI about user behavior with interactive systems involving subtle interactions between learning and decision making such as the challenging transition from novice to expert interaction techniques.\nManuscript submitted to ACM\n157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208"
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "We rst contextualize our research in the eld of command selection with a focus on the transition from novice to expert interaction techniques. We then provide background in Reinforcement Learning on which our computational models are built."
        },
        {
            "heading": "2.1 Command Selection",
            "text": "2.1.1 Novice and expert Interaction Techniques. Common interfaces make several interaction techniques available to select a command. Novice interaction techniques such as menus, toolbar, ribbons, palette etc. require little training as they rely on visual guidance (recognition). They are easy to discover, to learn and to use [7]. However, they require visual attention, and several operations to execute a command. For instance, selecting the command \"Edit > Find > Replace\" in Microsoft Word menubar requires three pointing and click operations which are time consuming.\nExpert interaction techniques such as keyboard shortcuts, gesture shortcuts and command lines generally rely on \"recall\" forcing users to make some eorts to learn how to execute commands [7]. Expert interaction techniques are intented for more experienced users. They have been shown to be faster as they require less operations [4, 17, 59, 77, 79, 85]. For instance, users can execute a command and choose the parameters with a simple gesture. Moreover, expert interaction techniques can be performed partially or totally eyes-free, i.e. without visual feedback, letting users focus on their main task [7].\nSeveral studies show that many people do not use these expert interaction techniques despite their benets [60, 81, 92]. These studies motivated the design of several methods to favor the transition from novice to expert interaction techniques. For instance, in the context of keyboard shortcuts, previous methods include the use of advanced feedback mechanisms, e.g. visual or audio feedback [11, 44, 72, 102], feedforward mechanisms [40, 71], the use of easy-to-learn mappings [11, 65, 99, 103] or temporal penalties [44, 58]. For instance, Grossman et al. [44] present and compare dierent methods: A is a method playing the keyboard shortcut orally by a voice synthesizer when a command is executed in the menu to expose the users to the shortcut; D is a method letting the user navigate through the menu, but does not allow clicking on the items to execute it. It forces users to execute keyboard shortcuts. Both methods favor keyboard shortcut use. Similar methods (feedback, feedforward, penalty, etc.) have been proposed to favor the use of gesture shortcuts, e.g. [4, 8, 45, 59] as well as command lines [87, 95].\nThese methods aim at promoting awareness of the expert techniques, motivate their use, facilitate the learning and/or improving their performance to favor adoption. We build on this literature to elaborate our model as it identies and highlights key factors (e.g. temporal cost of the method, the nature of the feedback, etc. ). However, it remains a long term challenge to predict and explain how these factors precisely interact together and their magnitude. It also remains unclear why users do not adopt expert interaction techniques.\n2.1.2 Theories and framework. Several theoretical constructs have been proposed to explain why users do not use expert interaction techniques [18, 36, 41, 43, 69, 87]. This includes the Einstellung eect [69], the \"soft constraint hypothesis\" [43] or the paradox of the active user [18]. For instance, users tend to \"exploit\" prior experiences (or previous methods such as menus) to achieve the task (short term productivity) rather than \"exploring\" more ecient methods such as shortcuts for long-term eciency [18]. Users tend to favor well-practiced methods with fast and incremental feedback rather than methods based on recall [36].\nSome frameworks [42, 87] characterize phenomena related to intramodal and intermodal expertise development. For instance, Scarr et al. [87] highlight three main reasons why users would not adopt an expert interaction technique: Manuscript submitted to ACM\n209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\nusers are not aware that a (more ecient) expert interaction technique is available; they can under-estimate the benets of the these techniques; the temporal performance dip when switching is perceived as too high. Gray and Lindstedt [42] extend this framework and study how individuals discover and invent new methods to develop their expertise. In particular they highlight three main phenomena: plateaus, dips and leaps. Users rst reach a performance \"plateau\" (or performance ceiling [87]) with a given method (i.e. novice interaction technique) after practice. Authors distinguish performance plateaus and performance asymptote, the latter being reached only with the optimal method (typically the expert interaction technique). \"Dips\", refer to the performance dip, when users explore, experiment, learn and switch modalities, typically when they make the transition from the novice to the expert interaction technique. This performance dip is essential because it can prevent users to adopt the expert interaction technique and maintain users in a local optimum. They would then not be able to experience performance \"leaps\" oered by the use of expert interaction techniques. In neuroscience, several studies show similar human (and animal) behaviors where the decision process is not necessarily based on long-term rewards, but also on short-term rewards [32], in particular when a learning eort is required [100] or even without rewards [73], e.g. habits, intrinsic motivations, etc. These studies suggest that Reinforcement Learning (model-free, model-based, or both) can play a key role to explain these behaviors [30].\nThese works present high-level explanations. However, they do not allow for ne-grained predictions. They do not permit to predict which users, when and how they make the transition from novice to expert interaction techniques. In this article, we build on this theoretical grounding and present a computational model of this transition allowing to simulate various cognitive, individual and environmental factors to test hypothetical designs and scenarios.\n2.1.3 Computational Models. Several computational models have been proposed in the eld of command selection. They generally predict selection time in linear menus depending on several factors such as menu organisation, menu length, item position or item frequency [9, 15, 21, 24, 25, 47, 61, 67]. They rely on empirical laws of pointing (Fitts\u2019 law [35]) and visual search (e.g. [12]). However, only few of them focus on the learning process by considering practice as a factor [9, 25, 67, 94]. Among them, Bailly et al. [9] combine two visual search strategies (serial and directed search) and a pointing component which are modulated by practice. The learning component relies on the Power Law of Practice (PLP) [76]. This law is appropriate at the population level, but does not capture individual learning dynamics.\nVery few computational models have been proposed to estimate the production time of expert interaction techniques and focus on gesture shortcuts [16, 49, 63, 83]. For instance, the CLC model [16] predicts the amount of time it takes for users to make a gesture shortcut based on its geometry. The model partitions the gesture into segments, where each segment is a Curve, a straight Line, or a Corner. The total time to execute this gesture shortcut is the sum of the time to produce each segment. However, the model includes neither a learning component to reect how users performance evolves with practice nor a decision making component to predict if and when users adopt gesture shortcuts. We are not aware of a computational model to explain or predict how users switch from a novice to an expert interaction technique.\nOur work is also related to computational models of habits and/or behavioral change [13, 55, 74, 93], where \u201chabits\" refers to the cognitive associations between users\u2019 behaviors and the triggering of contexts. These models aim to predict the habit strength as a function of behavior repetition. While these models can be sucient to explain the behavior of users who only use menus, they can not explain the behavior of users who switch to shortcuts without external interventions. Our models include a perseveration mechanism (as a Choice Kernel, see below) to reect habits, but also learning and planning mechanisms necessary to explain shortcuts adoption.\nManuscript submitted to ACM\n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312\n2.1.4 Summary. In summary, the transition from novice to expert interaction techniques is a well identied and long-time challenge in HCI with several technical, theoretical, and empirical contributions. However, no computational model has been proposed to predict how users adopt expert interaction techniques probably because it involves both learning and decision making phenomena. Existing computational models mainly focus on human performance with menus or well learned gesture shortcuts. In contrast, this article presents a computational model to predict and explain how users adopt shortcuts. It relies on the Reinforcement Learning framework."
        },
        {
            "heading": "2.2 Reinforcement Learning",
            "text": "2.2.1 Markov Decision Process. Markov Decision Process (or MDP) is a mathematical framework for decision making under uncertainty [48]. The MDP is a four-tuple ((, , %,') where ( is a set of states (also called state space), a set of actions (action space), % the state transition probability for going from a state B to state B 0 after performing action 0 (% (B 0 |B,0)) and ' a function (' : ( \u21e5 ! IR ) returning the immediate reward as a function of the state B and the performed action 0. The goal for an agent in a Markov Decision Process is to perform the serie of actions that maximize the expected cumulative random reward:\n\u21e2 [ C=1\u2019 C=0 W'(BC ,0C )]\nwhere W 2 [0, 1] is the discount factor determining the importance of future rewards. In several applications, the agent is a user, an animal, a robot; the actions are the behaviors of the agent (e.g., clicking a button, pressing a lever) and the state characterizes the environment (e.g. current state of the interface, a position within a maze) and the rewards are obtained from the environment (e.g. task achieved, food). Sometimes, the agent can not fully observe the environment. The problem can then be formulated as a POMDP, a partially observable MDP. This specic formulation have been used several times in HCI [21, 22], cognitive science [28] and neuroscience [30] to model human behaviour.\n2.2.2 Reinforcement Learning. Reinforcement Learning (RL) solves MDP (or variants such as POMDP) by learning state action value function & (B,0). & (B,0) is a real scalar value which represents the estimated expected value of executing action 0 in the state B as a common currency for potentially any type of long-term reward, i.e. time, food, money, etc. There are two main classes of algorithms: model-free and model-based. Model-free algorithms use neither the state transition probability function nor the reward function from the MDP to estimate the Q-values. Examples of model-free algorithms include Q-Learning, Rescolar-Wagner (RW), Choice Kernel (CK). In the following sections, we use (and detail) the RW and CK algorithms as well as their combination (RWCK) to predict human behavior and shortcut adoption.\nIn contrast, Model-based algorithms exploit the state transition probability function and the reward function from the MDP. They have the advantage to be much more ecient to nd the optimal solutions [51], but at the expense of a high computational cost [19, 34]. In the second part of this article, we present model-based algorithms to predict and explain shortcut adoption.\n2.2.3 Reinforcement Learning and HCI. Reinforcement Learning is receiving an increasing interest in many elds (cognitive science, neuroscience) and recently in HCI [13, 20\u201322, 37, 38, 52, 62, 64, 70, 89, 94]. In HCI, several perspectives are used to represent the user and the system in the MDP framework. In the \u201cmachine perspective\", the agent represents the system and the user is part of the environment providing some reward, i.e. teaching to the system how to react to users actions [33]. In the \u201cuser\u201d perspective, the agent represents the user, the environment includes the system/interface. The primary goal is then to understand users\u2019 behavior, for instance, understanding how visual search strategies Manuscript submitted to ACM\n313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364\nspontaneously emerge based on previous experiences with the interface [21]. However, these models can also be embedded in the system. For instance, Todi et al. [94] present a system simulating dierent machine and user behaviors and choose the best adaptions.\nOur approach relies on the latter (user perspective). Our models aim at predicting and explaining users\u2019 behavior when facing both novice and expert interaction techniques. It is a rst step towards the elaboration of intelligent systems that dynamically predict and trigger interventions to foster the adoption of expert interaction techniques.\n2.2.4 \u201cLearning\" as a cognitive process? \u201cLearning\" in Reinforcement Learning (RL) refers to how the algorithm incrementally updates the State-action values (Q-values) to determine the optimal policy. However, the dynamic of the Q-values does not necessarily reect the cognitive process of skill/knowledge acquisition of the agent. It depends on the objective and thus the eld of research: Machine Learning or Neuroscience.\nIn Machine Learning, the dynamic of the Q-values only reects the quality of the solver. The faster the Q-values converge during the training phase, the better is the solver to nd the optimal policy. Previous RL-based HCI models generally adopt this perspective. During the training phase, the dynamics of the Q-values do not reect how the user/agent learns. Once the optimal policy is determined, the model can be simulated using static Q-values. The model then predicts how users behave once they reach a plateau of performance, i.e. once they have already learned the task [13, 21, 22, 38]. For instance, Chen et al. [21] study visual search in previously unseen menus and acknowledge that their RL model does not aim to explain \u201chow people learn specic menus and the location of specic items\". Todi et al. [94] recently proposed a model-based RL algorithm to predict how users nd and select items in a linear menu. While they introduce a learning component, the Q-values are only used to train the model. The learning dynamic is encoded in the model, i.e. it is estimated from the history of actions by using the base-level equation (ACT-R [3]) at the end of each session.\nIn Neuroscience, the approach is generally dierent. The dynamic of the Q-values in RL models is of importance as it reects how the agent (human, animal) learns [101]: the Q-values varies during the simulation of the model and illustrates the fact that the choice of actions for a same state can evolve with practice. The dynamics of Q-values\u2019 evolution is also of paramount importance in Neuroscience since it can be used to analyze whether neural representations of action values, as recorded with brain imaging, reect the same dynamics as reinforcement learning models or not. Finally, precisely analyzing the dynamic of the Q-values as obtained with model simulations is also important in this eld so as to compare whether the learned Q-values of dierent models can account for behavioral properties observed in humans at dierent moments across task learning. Such simulations are critical to not only compare dierent parameter values of the same model and whether they are consistent with the dynamic of Q-values, but also to compare dierent models and eliminate those that cannot account for the properties of human behavior [101].\nOur approach builds on this Neuroscience perspective as we aim at predicting and explaining how the users learn the task, i.e. eciently executing commands, when repeatedly facing the same set of commands. This is a more challenging objective as our models should predict all trial-by-trial actions (instead of the last trials once participants reached a plateau of performance).\n2.2.5 Summary. Our main contribution is the elaboration and validation of the rst computational model of the transition from menus to shortcuts. Because we address complex phenomena related to not only decision making and learning dynamics, we build on the eld of neursocience, approaching the RL framework dierently.\nManuscript submitted to ACM\n365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416"
        },
        {
            "heading": "3 APPROACH",
            "text": "Command selection relates to the execution of commands (e.g. \"Open\") using several interaction techniques. In this article, we consider the Menu (M) as novice interaction technique and Shortcuts (S) as expert interaction techniques, although the following models could be extended to other methods (e.g. palettes, ribbons). We now describe the type of decisions the models focus on, the formulation of the problem and the general approach."
        },
        {
            "heading": "3.1 Type of decisions",
            "text": "When the users have to execute a command, they make several decisions:\n\u2022 Decision about the strategy. Users rst choose which strategy to use: executing the command as fast as possible (1) using the menu, (2) using a known shortcut or (3) learning the shortcut now to be able to successfully use it later. \u2022 Decision about the mapping. The second level of decision is choosing which item to click on, the gesture to perform or which keys to press given the chosen strategy. For instance, given the functionality \"Quit\" and the choice of using keyboard shortcuts, users have to decide which combination of keys to press (e.g. Ctrl+Q or Shift+Q). This can also happen when interacting with the menu as the user does not necessarily know how the desired functionality is entitled in the menu [7]. \u2022 Decision about execution. Finally, the last level of decision is a plan for movement. For instance, for executing \"Ctrl+Q\" without looking at the keyboard, users might decide which nger to use, which might depend on the keyboard layout.\nSo, choosing between behavioral strategies is hierarchically organized [46, 56]. Our main goal is to better understand when users decide to learn, then learn and then adopt shortcuts. We therefore focus on understanding how users choose strategies (the highest level of this hierarchy).We thus leave as future work several phenomena related to the interaction within a menu (e.g. the position of items) and the specicities of the shortcuts (e.g. the position of keys on the keyboard, the shape of the gesture). In particular, we do not aim at explaining the nature of errors, e.g. explaining why users press \u201cShift+Q\" or \u201cCtrl+A\" when executing the command \u201cQuit\". Instead, we focus on behavioral changes at the strategy level."
        },
        {
            "heading": "3.2 Problem formulation",
            "text": "The problem of shortcut adoption can be described as a discrete-time stochastic control of Markov Decision process.\n3.2.1 State space S:. A state B 2 ( represents the target command to execute (e.g. \u201cOpen\"). A specicity of our denition is that the next state only depends on the frequency of the commands. In the next sections, we indierently use state or command when referring to B .\n3.2.2 Action space A:. Given a command, the user chooses an action 0 2 . In our context the set of actions are the Menu strategy (0\" ), the (keyboard or gesture shortcut) Shortcut strategy (0( ) and the Learning strategy (0!). 0\" and 0( are characterized by the fact that users execute the command as fast as possible with the corresponding method (menu or shortcut). In contrast, the learning strategy (0!) consists in opening the menu, dedicating some time to explicitly learn the shortcut mapping and then executing the command (either with the menu or a shortcut). Indeed, opening a menu does not only serve to select a command but also to gather information about the shortcut by gazing at the visual cue on the right side of the menu item [7]. Manuscript submitted to ACM\n417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n3.2.3 Q-Value. The expected value of an action 0 in a state B at the time C is represented by the Q-value & (BC ,0). In our context, a Q-value represents the relative expected benet of using a specic strategy for executing a given command. During the simulation of an interaction task, the Q-values of each strategy are not static, they evolve, trial after trial, based on the history of interactions and the environment and thus constitute predictions of each participant\u2019s upcoming behavior.\nThe way the Q-values are updated, or used to choose actions is model-dependent and described in the following sections.\n3.2.4 Model\u2019s Input/output. The input of the model is a command BC and its output is the chosen strategy (or action) for this command. We remind that our model describes a whole learning process (the policy is not xed). The strategies evolve over time depending on the evolution of the Q-values."
        },
        {
            "heading": "3.3 Computational rationality",
            "text": "Our problem formulation is inline with the computational rationality view of human behavior [39, 66] where the users\u2019 strategies (policy) emerge from the user\u2019s goal (utility), their cognitive mechanisms and the task environment:\n3.3.1 Utility. The user aims at minimizing total execution time for executing commands.\n3.3.2 Cognitive mechanisms. Plethora of cognitive mechanisms are likely to be involved in the learning and decision making process of adopting shortcuts. In this article, we consider ve main mechanisms: decay, perseveration, planning, implicit learning and explicit learning and their dierent combinations. We detail these mechanisms in the following sections.\n3.3.3 Task environment. We consider two main aspects for dening the task environment. First, the sequence of commands. Liu et al. [68] show that the user\u2019s behavior is sensitive to dierent frequency distributions and the execution time of a given command depends on, not only its frequency, but also the frequency of the other commands. It is thus important to rene the denition of sequence of commands because three components might inuence shortcut adoption: 1) the size of the set of unique commands (e.g. \u201cOpen\u201d, \u201cSave\u201d), 2) the total number of command execution and 3) the relative frequency distribution (e.g. uniform distribution, Zipan distribution, etc. [68]).\nThe second aspect is related to the teaching methods available to favor shortcut usage. While several methods have been proposed, it still remains unclear how they modify users\u2019 behavior (see Section Related Work). In this article, we focus on the three teaching methods tested in [44]: Traditional, Audio and Disabled."
        },
        {
            "heading": "3.4 Outline",
            "text": "We rst introduce three benchmark RL models in neuroscience, which are widely used in decision-making tasks involving a learning process [101] (section 4). We then present the data collection of Grossman et al. [44] (section 5) and state-of-the art methods from the decision-making eld (section 6) to evaluate and compare the models. Results (section 7) show that these benchmark models are not sucient to capture the complexity of shortcut adoption. We then present our model in sections 8 and 9 and evaluate it in the sections 10 and 11.\nManuscript submitted to ACM\n469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520\nTable 1. Key notations\nNotation Description B State: Target command 0 Action: User strategy & (B,0) Q-Value 0? Previous action\nTable 2. Free (top) and task-related (boom) parameters of the model. The range of the free parameters is the one used to fit the models\nSymbol Range Description U (U', ,U\u21e0 ) [0, 1] Learning rate V (V', , V\u21e0 ) [1, 20] Softmax inverse temperature ) ( 0.9 Keyboard shortcut strategy time )\" 2 Menu strategy time )! 3.8 Learning strategy time 2? 3 temporal penalty associated to an error"
        },
        {
            "heading": "4 BENCHMARK RL MODELS",
            "text": "We are not aware of existing models of shortcut adoption. We thus chose three benchmark models in Cognitive Neuroscience which are widely used in decision-making tasks involving a learning process [101]. These models are especially appropriate for multiarmed bandit problem[5] where the agent receives a reward after each action. Indeed, in our context, users choose a strategy and immediately receive a reward (e.g. the inverse of the execution time) at each trial. The dierent notations and parameters are summarized in the Table 1 and Table 2."
        },
        {
            "heading": "4.1 Rescorla-Wagner (RW)",
            "text": "In this model, the agent learns the expected value & of each action based on the history of previous rewards:\n& (BC+1,0) = & (BC ,0) + U ( A (BC ,0) & (BC ,0) ) (1)\nwhere U is the learning rate and A (0, C) is the reward of using the action 0 at the date C . Here, the reward, is the inverse of the execution time. To simplify the problem, we assume that the time only depends on the strategy with: )( < )\" < )! Where )( , )\" , )! are respectively the execution times of the strategies Shortcut, Menu and Learning, which are empirically estimated. The RWmodel is a myopic version of standard temporal-dierence learning algorithms [90], such as Q-learning, where the discount factor W = 0.\nTo compare and choose the action given the Q-values, the model relies on a Boltzmann soft-max function. This function converts the Q-values into action probabilities % (0 |BC ):\n% (0 |BC ) = 4V & (BC ,0)\u00d5 0 4V & (BC ,0)\n(2)\nwhere the parameter V is the inverse temperature which controls the trade-o between exploitation and exploration, i.e. a small value of V reects almost random choice (exploration) while a high value of V indicates that the user always chooses the action with the highest Q-value (exploitation).\nThis model has only two parameters U (eq. 1) and V (eq. 2)"
        },
        {
            "heading": "4.2 Choice Kernel (CK)",
            "text": "This model captures the tendency of users to repeat previous actions regardless of execution time. It computes \u21e0 values: \u21e0 (BC+1,0) = \u21e0 (BC ,0) + U ( (0 == 0? ) \u21e0 (C,0) ) (3) Manuscript submitted to ACM\n521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572\nwhere U is the learning rate and 0? the previous action. The equation 2 then converts the \u21e0 values into action probabilities. The Choice Kernel model also has two parameters (U , V\u21e0 ).\nThe CK model is thus insensitive to the rewards. While this model can appear too simple, several models without rewards have been shown to well capture human behavior in dierent contexts [73]. The CK model is part of these models and focuses on the perseveration eect: It is based on evidence in neuroscience [101] that human beings are likely to repeat the previous strategy: not only repeating the Menu strategy, but potentially also the Shortcut strategy once the shortcuts have been learned."
        },
        {
            "heading": "4.3 Rescorla-Wagner + Choice Kernel (RWCK)",
            "text": "This model mixes the two previous models, following the principle that subjects both try to maximize reward and tend to show some degree of perseveration at the same time. The model estimates the action probabilities according to the equation 4:\n% (0 |BC ) = 4V', & (BC ,0)+V\u21e0 \u21e0 (BC ,0)\u00d5 0 4V', & (C,0)+V\u21e0 \u21e0 (C ,0)\n(4)\nThe behavior of the agent is thus sensitive to the reward (inverse of the execution time) and to the strategies previously used. This model has four parameters (U', ,V', , U\u21e0 ,V\u21e0 )."
        },
        {
            "heading": "4.4 Discussion",
            "text": "These three models have in common to rely on an exploration - exploitation mechanism. While the Rescorla-Wagner model learns the expected value of each action based on the history of previous rewards, the Choice Kernel model captures the tendency of users to repeat previous actions regardless of the reward. Rescorla-Wagner+Choice Kernel mixes the two models. These model-free RL models have several advantages to model how users adopt expert interaction techniques. They are task-independent, have few free parameters, are easy to implement, are fast and have been shown to well capture learning and decision-making dynamics in dierent contexts [101]."
        },
        {
            "heading": "5 DATA COLLECTION",
            "text": "We compare the three models (RW, CK, RWCK) on Grossman et al. data [44]. We summarize the experimental design and the collected data."
        },
        {
            "heading": "5.1 Experimental Design",
            "text": "The interface consists of a menu bar opening 6 drop-down menus and a button at the bottom of the screen (Figure 1). The participants move the cursor within the button and press the space bar to start the trial. The button then displays an image representing the command to be executed as a stimulus (1). The participants execute the command by selecting the corresponding item in the menu (2-3) or by executing the corresponding keyboard shortcut. When an error occurs, the participants have to wait for 3s before the command can be executed again. The trial ends when the participants hit again the space bar with the cursor within the button (4).\nThis study compares three interaction techniques: (1) theTraditionalmenu visually highlights the keyboard shortcut of the selected item; (2) the Audio menu oers audio feedback: The keyboard shortcut was played orally by a voice synthesizer once the command was executed from the menu. Finally, (3) the Disabled menu lets the user navigate through the menu, but does not allow clicking on the item to execute it. This forces users to use keyboard shortcuts.\nManuscript submitted to ACM\n573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624\nFig. 1. Experimental task. Participants move the cursor within the buon at the boom of the screen and hit the space bar to display the stimulus, an image representing the command to select (1). The participants then execute this command by selecting the corresponding item (\u201cPencil\u201d) in the menu (2-3) or by executing the corresponding keyboard shortcut (Ctrl+R). The trial finishes when the participants press again the space bar with the cursor within the buon. Reprinted from [44] with the permission of the authors.\nThe fourteen commands have dierent frequencies. The most frequent commands are executed 144 times and the least frequent commands are executed 12 times. Each of the 42 participants (12 female, 30 male, ranged in ages from 18-28) is assigned to a technique (14 participants per techniques) and executes 720 commands organized in 12 blocks of 30 executions. Figure 9-top shows the 144 executions of the most frequent command of the user 1 with the Audio technique.\nposition the cursor inside of it, and then hit the space bar"
        },
        {
            "heading": "5.2 Collected Data and derived strategies",
            "text": "At each trial, the participant\u2019s id, technique, block, trial, target command with frequency, name and keyboard shortcut are recorded. The dependent variables are time (ms), success (0/1) and the method: Menu or Keyboard shortcuts. Because they are not directly recorded in the logs, we derive the three strategies as:\n\u2022 Menu strategy when the user only uses the menu method. \u2022 Shortcut strategy when the user only uses the keyboard shortcut method. \u2022 Learning strategy when the menu is visited, but the keyboard shortcut is executed1\n1When the participant learned the keyboard shortcut, but executed the command in the menu, the Learning strategy cannot be detected. This behavior is interpreted as the Menu strategy. The number of Learning is thus under-estimated and the number of Menu is over-estimated.\nManuscript submitted to ACM\n625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676"
        },
        {
            "heading": "5.3 Task-related parameters",
            "text": "This study includes a 3s penalty (2? = 3) when an error occurs. We also analyzed the empirical data and estimated the correct execution time for each strategy: )\" = 2.0, ) ( = 0.9, )! = 3.8. We used the mode (rather than the mean or the median) because the distributions were highly skewed."
        },
        {
            "heading": "6 EVALUATION METHODS",
            "text": "The objective of this section is to present the methods to estimate the parameters of the three models as well as the methods to compare them."
        },
        {
            "heading": "6.1 Parameter estimation",
            "text": "A main challenge in elaborating and using computational cognitive models is the number of parameters as well as their variability across individuals. In some contexts, the value (or the distribution of values) of the parameters are known and can be directly adopted from the literature [21, 52]. However, in our context of shortcut adoption, we do not have specic priors on the values of these parameters (see table 2). We thus aim to estimate values of the parameters that best explain behavioral data, i.e. the parameters that minimize the tness function.\n6.1.1 Maximum-Likelihood Approach. The tness function reects the capacity of a model to replicate a participant\u2019s trial-by-trial action choice. In Bayesian terms, it is the likelihood of the data given the model, that is the maximum probability that the model chooses the same series of actions as the participant [27, 96]. It consists of analyzing the posterior prediction of the model conditioned on the past history, i.e. evaluating the likelihood of the participant\u2019s action 0C given the past data 31:C 1, where the past data includes the actions made by the participant, not the actions made by the model. Formally, we estimate :\n!!(<, ?) = \u2019 C log % (0?C |( ? 1:C 1,<, \\ ? <) (5)\nwhere< is the tested model, ? , a given participant, \\?< the set of parameters of the model< for the participant ? and (?1:C the sequence of actions performed by the participant until the date C . We use the dierential evolution algorithm as optimization method (from the Scipy.opimize python library [97]) to nd the set of parameters \\?< which maximizes !!(<, ?) for each model< and each participant ? .\n6.1.2 Fitness function properties. Our tness function has two key properties. First, it considers individual models (i.e. a dierent set of parameters for each participant) rather than a population model (the same set of parameters for all participants), which is important to address inter-individual variability in decision-making problems [52]. Indeed, dierent users can have radically dierent policies leading to dierent behaviors. Consider an extreme case with two users, one using only Menu and one using only Keyboard shortcuts. The notion of \"average\" user does not mean that she will use 50/50 Menu and Keyboard shortcuts.\nMoreover, our tness function considers trial-by-trial action choices rather than aggregate tting as it ts each action individually. While it is not common practice in HCI, this approach is now well adopted in cognitive sciences and neuroscience [27, 96]. This permits to model the temporal evolution of participant\u2019s behavioral strategy, e.g. initially using menus and then progressively switching to shortcuts, rather than modeling again an average 50/50 Menu and Keyboard shortcuts for a single participant.\nManuscript submitted to ACM\n677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728"
        },
        {
            "heading": "6.2 Model fiing",
            "text": "6.2.1 Log-Likelihood comparison. We aim to determine which of the three models best describes the behavioral data, as a way to understand which mechanisms underlie behavior. Given the best identied parameters \\?< , we compare their likelihood !!(<, ?) (equation 5). The model with the largest likelihood is likely to better explain the observed data.\n6.2.2 BIC score comparison. In the process of model comparison, it is common to include a penalty term for model complexity, i.e. for the number of parameters [101]. The Bayesian Information Criterion (BIC) is commonly used [23] and estimated as \u232b \u21e0 = 2!! + : \u21e5 ;>6(# ) where !! is the likelihood (equation 5), : , the number of parameters and # , the number of points to predict. As each participant executes 720 commands in the experiment, # = 720. It is common practice to consider that there is a \u201cstrong evidence\" in favor of the winning model when the BIC dierence is > 6 [84].\nTable 3 and 6 report both likelihood and BIC score."
        },
        {
            "heading": "6.3 Model Simulations",
            "text": "We can use the best set of parameters \\?< to simulate the dierent models. In some cases, model simulation can lead to very dierent results from model tting if the path of actions sampled by the participant is widely dierent from the paths likely to be selected by the model [80, 101]. It is thus important to also simulate the models and verify that they do reproduce the main behavioral properties of the participants [101], in our context, the evolution of the percentage of correct shortcut execution, which is commonly used to compare interaction techniques favoring shortcuts [7]. For each model, we ran 50 simulations2 per participant using individual parameters, (i.e. 50 \u21e5 42 = 2100 B8<D;0C8>=B per model). We then aggregated per technique (14 participants).\nTable 3. Comparisons of three benchmark model-free RL models and our T model in term of free parameters, total number of free parameters (N), Likelihood and BIC. Our T model minimizes both the inverse of the likelihood and the Bic score.\nModel Free parameters N - Likelihood BIC Rescorla-Wagner (', ) V', , U', 2 193.7 400.7 Choice Kernel (\u21e0 ) V\u21e0 , U\u21e0 2 174.0 361.2 ',\u21e0 V', , U', , V\u21e0 , U\u21e0 4 159.5 345.4 T U\u21e2 , U , 3 , \u2318,F , V 6 148.5 336.5"
        },
        {
            "heading": "7 RESULTS",
            "text": "We now present our tting and simulation results at dierent levels of granularity."
        },
        {
            "heading": "7.1 Fiing Results for Action Choices",
            "text": "7.1.1 Model. Table 3 indicates the likelihood and BIC score for the three benchmark RL models. As expected, there are strong evidence (BIC dierence > 6) that the combination of the Rescorla-Wagner model and Choice Kernel model, RWCK (!! = 159.5; \u232b \u21e0 = 345.4) outperforms each model individually, ie, Choice Kernel, CK (!! = 174.0; \u232b \u21e0 = 361.2) and Rescolar-Wagner, RW (!! = 193.7;\u232b \u21e0 = 400.7) even when considering the penalty associated to the BIC score for additional parameters. Interestingly, the CK is the second best model while it is myopic to rewards and tends to repeat the previous strategies. 2a compromise between testing models and a reasonable expenditure of experimenter eort Manuscript submitted to ACM\n729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780\nFig. 2. Model comparisons in term of likelihood (LL) and BIC Score for each technique. The lower the beer. Error bars show 95% bootstrap confidence intervals. T outperforms the three benchmark RL models (Rw, CK and RWCK) both in term of Likelihood and BIC score.\n7.1.2 Interaction Method level. Figure 2 compares the three models for each interaction method. Results indicate strong evidence (BIC dierence > 6) that RWCK outperforms CK and RW for both Audio (RWCK: 463.4; CK: 486.4; RW:531.4) and Disable (RWCK: 323.6; RW: 348.6; CK: 349.5). However, for Traditional, where users are less likely to transition and thus to repeat the same Menu strategy, both CK (247.8) and RWCK (249.2) outperform RW (322.0).\n7.1.3 User level. Finally, we analyze the data for each participant. Results indicate that ',\u21e0 is the best model (BIC score) for 26 participants, \u21e0 for 9 participants (7 of them using Traditional) and ', for 7 participants. These results provide a complementary picture illustrating the variety of users\u2019 behavior, i.e. there is not a single model that best ts all participants. \"Simple\" behaviors such as rarely using shortcut can be explained with a simple \u21e0 model (e.g. only implementing the perservation mechanism). However, this model fails as soon as the users really make a transition.\n7.1.4 Parameters. Figure 3 illustrates the distribution of the values of parameters for the three models."
        },
        {
            "heading": "7.2 Model simulations",
            "text": "7.2.1 Block-by-block: Evolution of shortcuts. Figure 4 shows the evolution of shortcut use (%) per block and per method for the three benchmark models. We also report the Mean Square Error (MSE) as a measure of discrepancy. Surprisingly, we observe here that RWCK is not the best model to synthesize users\u2019 behavioral data. RW (MSE=274.9) outperforms RWCK (MSE=329.9) and CK (MSE=855.1) is by far the last model. However, a closer inspection reveals that none of them is fully satisfactory. First the initial percentage of shortcut use is too high regardless of the model and the method. Second, the performance of Audio is always under-estimated, regardless of the model.\n7.2.2 Trial-by-trial: individual participant actions. We visually inspected the 588 (42 users \u21e5 14 commands) sequences of strategies for each model. Figure 9 is one example illustrating the limit of the RWCK simulations to reproduce users\u2019 behavior. Indeed, we observe several instances where the models switch back to menus (or learning) for a long period (> 7CA80;B). This can be explained by the fact that \"optimal\" V', and/or V\u21e0 are small enough to favor exploration even after having switched to shortcuts. In comparison, we did not observe this pattern in participants\u2019 data.\nManuscript submitted to ACM\n781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832\nRWCK\nRW CK\nTraditional Audio Disabled\nFig. 3. Summary of the parameters per technique and per model.\nFig. 4. Shortcut use (%) per block and method. Observed participants\u2019 data are represented with dots. Synthetised data (solid line) are produced by aggregating 50 simulations per participant with individual parameters."
        },
        {
            "heading": "7.3 Discussion",
            "text": "In summary, these model tting results indicate that the Rescorla-Wagner+Choice Kernel (RWCK) model better accounts for the empirical data, suggesting that both the adaptation to reward (from RW) and the perseveration (from CK) mechanisms play a role in explaining and predicting the transition from menus to shortcuts. Moreover, the Choice Kernel (CK) is the second best model while it is myopic to the rewards, suggesting that perseveration is an important mechanism to explain users\u2019 behavior. However, simulation results suggest that Resocla-Wagner (RW) better synthetizes users\u2019 behavioral data. So, the role of perseveration is at this point not clear: while it signicantly contributes to the Manuscript submitted to ACM\n833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884\ngoodness-of-t (model tting), the data produced by the models without perseveration better reect participants\u2019 behavior (model simulation).\nFrom a methodological point of view, our results highlight the importance of combining model tting and model simulation [80] to validate models of shortcut adoption as they show a dierent picture. Finally, a close inspection of our results suggests that this rst set of classical models are not satisfactory as they tend to overestimate initial shortcut adoption and underestimate the performance of the Audio technique. This motivates us to elaborate a dedicated model of shortcut adoption, called T.\n8 TRANSITION: MODEL OVERVIEW AND THEORETICAL ASSUMPTIONS\nIn the previous sections, we showed that the benchmark model-free RL models in neurosciences are not sucient to explain users\u2019 behaviors. In this section, we present a model-based RL model dedicated to explain and predict shortcut adoption. It also relies on the computational rationality principles but combines ve mechanisms, grounded in neuroscience and cognitive science, likely to participate in the transition from menus to shortcuts:\nThe rst mechanism, planning [29, 31, 53, 78, 88], means that the users are able to consider \u2318 actions ahead for a given command, i.e. they mentally simulate their future strategy choices in response to the \u2318 next times. A higher \u2318 means that the users are more likely to transition as they can foresee the future benets of learning shortcuts now. Moreover, humans do not necessarily value all future actions/strategies with the same weight [18, 88]. A discount factor is generally introduced to determine the importance of future rewards [91]. However, this planning mechanism is not sucient alone as it can only explain behavior when users never transition (short horizon) or transition immediately (larger horizon).\nThen, implicit and explicit learning mechanisms consolidate at dierent learning rates the command-to-shortcut mapping in memory. When repeatedly selecting an item in the menu, users unconsciously and slowly gather information about the shortcut thanks to their peripheral vision (implicit learning). When reaching a certain level of knowledge, users can then perceive the benets of intentionally learning shortcuts (explicit learning). Some evidence in neuroscience indicate that learning for action selection relies on a balance between planning and implicit/explicit learning [78].\nA decay mechanism [3, 14] reects that the command-to-shortcut mappings encoded in memory fades away due to the passage of time.\nThe fth mechanism, perseveration, reects the fact that users are likely to repeat the previous strategy. This general behavioral tendency, at the heart of the Choice Kernel Model, has been documented for a long time in decision-making, from psychological, neuroscience and modeling points of view [1, 39, 57, 101].\nIn the next section, we describe our model using the RL formalism (e.g. states, actions, Q-Values, etc.). We then evaluate it and further demonstrate that these mechanisms are necessary altogether to explain and predict the transition from menus to shortcuts. Finally, we discuss the limitations of the model and provide several directions to rene it.\n9 TRANSITION: MODEL DEFINITION\nThe dierent notations and parameters are summarized in Table 4 and Table 5."
        },
        {
            "heading": "9.1 State and Action",
            "text": "We reused the same denitions for the states (the target to execute) and the actions: the Menu strategy (0\" ), the Shortcut strategy (0( ) and the Learning strategy (0!).\nManuscript submitted to ACM\n885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936\nTable 4. Key notations\nNotation Description B State: Target command 0 Action: User strategy & (B,0) Q-Value \u21e0 (B,0) (Temporal) Cost \u21e2\u21e0 (B,0) Expected Cost \u21e2\u21e0\u21e0 (B,0) Expected Cumulative Cost \u21e0) (B,0) Successful execution cost \u21e0' (B,0) Repair cost (B,0) User knowledge 0? Previous action\nTable 5. Free (top), fixed (center) and task-related (boom) parameters of the model. The range of the free parameters is the one used to fit the model\nSymbol Range Description V [1, 20] Softmax temperature F [0, 1] tendency to repeat the previous action U\u21e2 [0, 1] Explicit learning rate U [0, 0.33] Implicit learning rate 3 [0, 0.02] Decay \u2318 [0, 7] Horizon W 0.9 Discount factor [91] ) ( 0.9 Keyboard shortcut strategy time )\" 2 Menu strategy time )! 3.8 Learning strategy time 2? 3 temporal penalty associated to an error"
        },
        {
            "heading": "9.2 Q-values",
            "text": "& (BC ,0) remains the expected value of an action 0 in a state B at the time C . To compare those Q-values and nally choose an action, the model relies on the Boltzmann soft-max function of Equation 2. The expected value & (BC ,0) is now calculated as:\n& (BC ,0) = (1 F) \u21e5 ( \u21e2\u21e0\u21e0 (BC ,0)) + (0 = 0? ) \u21e5F (6)\nwhere \u21e2\u21e0\u21e0 (BC ,0) is the expected cumulative temporal cost of using the strategy 0 to execute the command BC ; 0? is the previous action used for the command BC andF 2 [0, 1] is a weight reecting the tendency of people to repeat the previous action, i.e. the degree of perseveration. In other words, Equation 6 reects the fact that the agent faces a multi-objective optimization problem by trying to minimize the expected cumulative temporal cost of current command execution while maximizing stability in the choice of the strategy. The formulation of the action values is thus quite similar to the one of RWCK 4, which corresponds to weighting into a common currency the RW values (reward) and CK values (perseveration).\nBefore dening the expected cumulative cost \u21e2\u21e0\u21e0 , i.e. the cost associated to a sequence of actions, we rst need to dene the cost function \u21e0 (BC ,0) and the expected cost \u21e2\u21e0 .\n9.3 Cost function \u21e0 (BC ,0)\nThe temporal cost \u21e0 to execute a command is the sum of the execution time \u21e0) and the repair time \u21e0' in case of error:\n\u21e0 (BC ,0) = \u21e0) (BC ,0) + 1 \u21e5\u21e0' (BC ,0) (7)\nwhere BC is the target command at time C , 0 the chosen strategy and 1 a Boolean indicating whether users perform an error or not. To simplify the problem, we assume that the correct execution time \u21e0) only depends on the strategy with:\n)( < )\" < )! (8)\nwhere )( , )\" , )! are respectively the correct execution times of the strategies Shortcut, Menu and Learning. The repair Time \u21e0' is the sum of the time to analyse the error (or penalty) 2? and the time to correctly re-execute the command. Manuscript submitted to ACM\n937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988\nTo simplify, we consider the users reuse the Menu strategy to repair their errors:\n\u21e0' (BC ,0) = \u21e0) (BC ,0) + 2? (9)\n9.4 Expected cost \u21e2\u21e0 (BC ,0)\n\u21e2\u21e0 (BC ,0) is the expected temporal cost of using the strategy 0 to execute the command BC . It derives from Equation 7 and is the weighted sum of the correct execution time \u21e0) (BC ,0) and incorrect execution time \u21e0' (BC ,0), where the weight depends on the user knowledge (BC ,0):\n\u21e2\u21e0 (BC ,0) = (BC ,0) \u21e5\u21e0) (BC ,0) + (1 (BC ,0)) \u21e5 (\u21e0) (BC ,0) +\u21e0' (BC ,0C )) (10)"
        },
        {
            "heading": "9.5 Knowledge",
            "text": "9.5.1 Definition. (BC ,0) 2 [0, 1] is a latent variable representing the knowlegde of the user. It is the probability to successfully execute the command BC with the strategy 0. More precisely, (BC ,0\" ) = (BC ,0!) and represents how well the mapping between a command and its location in the menu is encoded in the user\u2019s memory (the user only interacts with the menu with these two strategies). Reciprocally, (BC ,0( ) represents how well the mapping Command-to-Shortcut is encoded in the user\u2019s memory. (BC ,0( ) is a key variable to explain the transition from menus to shortcuts. Indeed, it is likely that users will not try to execute shortcuts if they do not have enough prior knowledge, i.e. if the probability of success is not high enough. In contrast the knowledge of item locations (BC ,0\" ) == (BC ,0!) is likely to have an impact on menu selection time (amount of visual search), but less on accuracy (pointing task in a menu has a high accuracy) and the transition to shortcuts. For this reason, one simplication is to assume that the users have a \u201cperfect\" knowledge of the location of menu items for a given command, i.e. the probability of successfully selecting the target item in the menu is equal to 1:\n(BC ,0\" ) = (BC ,0!) = 1 (11)\nWe can then rewrite Equation 10 for the strategies Menu and Learning, assuming that users do not make errors:\n\u21e2\u21e0 (BC ,0) = (BC ,0) \u21e5\u21e0) (BC ,0), 0 2 {0\" ,0!} (12)\n9.5.2 Updating Knowledge. We propose 2+1 mechanisms to update (BC ,0( ). The two rst mechanisms are explicit and implicit learning. Explicit learning occurs when users successfully use the Learning strategy or the Shortcut strategy: The users intentionally read/learn the shortcut cue or execute the shortcut correctly. Implicit learning occurs when users repeatedly execute a command in the menu: the users unconsciously gather information in the surroundings thanks to their peripheral vision. Explicit and implicit learning depend on the strategy and are used to increase the knowledge of shortcuts:\n(BC ,0( ) = (BC ,0( ) + U\u21e2 \u21e5 ( 1 (BC ,0( ) ) (BC ,0( ) = (BC ,0( ) + U \u21e5 ( 1 (BC ,0( ) ) (13)\nWhere U\u21e2 and U 2 [0, 1] are the explicit and implicit learning rates. While explicit learning is more ecient than implicit learning to memorize the shortcut mapping ( U\u21e2 >> U ), we will demonstrate (section Results) that implicit learning is essential for explaining the transition from menus to shortcuts.\nManuscript submitted to ACM\n989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040\nThe third mechanism is decay. At each time step, the shortcut knowledge of each command BC is updated to account for memory decay:\n8B 2 (, (BC ,0( ) = (BC ,0( ) + 3 \u21e5 ( 0 (BC ,0( ) ) = (BC ,0( ) ( 1 3 ) (14)\nWhere 3 2 [0, 1] is the decay factor. The mechanisms to update memory (U\u21e2 , U , 3) are related to the ones in ACT-R [3] but this denition is more appropriate to an RL framework and does not require to store the whole user history."
        },
        {
            "heading": "9.6 Expected Cumulative Cost \u21e2\u21e0\u21e0",
            "text": "We can now dene the expected cumulative cost \u21e2\u21e0\u21e0 used in Equation 6. We formulate the problem of command selection as a planning problem with an horizon \u2318, i.e. users plan a sequence of \u2318 actions for a given command to minimize the expected cumulative cost for this command:\n\u21e2\u21e0\u21e0 (B,0,\u2318) = \u21e2\u21e0 (B,0) + W \u21e5 \u21e2\u21e0\u21e0 (B,0A6<8=0 ( \u21e2\u21e0 (B,0) ),\u2318 1) (15)\nwhere \u21e2\u21e0 (BC ,0) is the expected temporal cost (Equation 10), \u2318 is the horizon, and W 2 [0, 1] is a discount factor determining the importance of future rewards. Typically, W close to 0 indicates that users only consider the temporal cost of the current strategy, while W close to 1 indicates that the weight of each strategy in a given horizon is very similar. In the RL literature [91], it is common to choose W = 0.9. The two parameters \u2318 and W allow to control for the cognitive bias consisting in valuing more the present than the future (in line with theoretical constructs such as the paradox of active users [18]). Our hypothesis is that users with a large horizon are more likely to perceive the benets of learning shortcuts now so as to use them in the future. In practice, the users estimate the cumulative cost \u21e2\u21e0\u21e0 (BC ,0) of each of the 3\u2318 decision branches and choose the one with the minimal cost as if the commands were performed in a row. To achieve this, they simulate each decision and their eect on the internal values, i.e. the shortcut knowledge necessary to estimate the utility of each strategy."
        },
        {
            "heading": "10 VALIDATION",
            "text": ""
        },
        {
            "heading": "10.1 Methods",
            "text": "We test our model, T, on the Grossman et al. data collection [44] and compare its likelihood and simulation performance to the three benchmark models RW, CK and RWCK."
        },
        {
            "heading": "10.2 Fiing results for Action Choices",
            "text": "10.2.1 Overall. Table 3 indicates the likelihood and BIC score of the T model. The results indicate that our model outperforms the best benchmark RL model, RWCK in terms of likelihood ( T: -148.5; RWCK: -159.4). More surprisingly, despite the larger number of parameters, the results indicate strong evidence (BIC dierence >6) in favor our model (T: 336.5; RWCK: 345.4).\n10.2.2 Method level. Figure 2 compares T to the three benchmark RL models for each method. Results indicate a strong evidence (BIC score >6) that T outperforms the best benchmark models for Traditional ( T: 230.6; CK: 247.9) and for Disabled ( T: 316.3; RWCK: 323.6). Results do not show signicant dierences between T (462.7) and RWCK (463.4) regarding the Audio method.\n10.2.3 User level. Results indicate that the best model (BIC score) is T for 31 participants, RWCK for 8 participants, RW for 2 participants and CK for 1 participant. It is a strong dierence with the comparison of the three Manuscript submitted to ACM\n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092\nFig. 5. Summary of the T model parameters per methods: Traditional (blue), Audio (orange), Disabled (green).\nbenchmark models where RWCK was the best model for 21 participants. These results indicate that T better captures the variability of users\u2019 behavior, regardless of the interaction method.\n10.2.4 Parameters analysis. Figure 5 illustrates the distribution of values for each parameter and each method. To analyze these parameters we distinguish Traditional and Audio which have three actions and Disabled which has only two (The menu strategy is not available).\nRegarding Traditional and Audio, results conrm that the distribution of values for parameters related to users\u2019 prole \u2013 ability to plan \u2318, decay 3 and tendency to repeat previous actionsF \u2013 seem independent from the interaction method. Results also conrm that participants tend to learn explicitly U\u21e2 and to try V shortcuts more often with Audio than with Traditional. More surprisingly, Audio feedback seems to also inuence implicit learning U while we were expecting this to be method-independent.\nRegarding the user proles\u2019 parameters of Disabled (\u2318, 3 ,F ), the results are similar except thatF appears higher probably due to the fact that this method relies on two actions instead of 3. Similarly, users tend to learn and try shortcuts more easily than Traditional. Finally, Disabled does not have an implicit learning parameter U (as the Menu Strategy is not available) explaining probably the longer tail for U\u21e2 ."
        },
        {
            "heading": "10.3 Model simulations",
            "text": "10.3.1 Block-by-block: Evolution of shortcuts. Figure 6 illustrates the percentage of correct shortcuts per block and per method. The results indicate that T synthesizes data which better reect users\u2019 behavior (MSE=40.7) than the three benchmark RL models (RW: 274.9; RWCK:329.9; CK: 855.1). Indeed, we observed that the simulation of T better captures the learning dynamics and thus does not suer from the two limitations of the simulation of the benchmark RL models: The initial performances predicted by the model are now close to the one observed for our participants and the prediction of the relative performance between the methods (in particular Audio) well reects the one of the actual techniques. However, a closer inspection reveals that the predicted performances of the three methods are slightly over-estimated during the rst blocks (0-6) and slightly under-estimated during the last blocks (7-11) in comparison with observed participants\u2019 data."
        },
        {
            "heading": "10.4 Discussion",
            "text": "Our results indicate that our T model outperforms the three benchmark models both in terms of model tting (likelihood and BIC score) and simulation. The results are especially impressive regarding the quality of the\nManuscript submitted to ACM\n1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144\nFig. 6. Shortcut use (%) per block and method for the T model. Observed participants\u2019 data are represented with dots. Synthetised data (solid line) are produced by aggregating 50 simulations per participant with individual parameters.\nTable 6. Comparison of the T model with five variants where one mechanism has been disabled in term of free/fixed parameters, total number of free parameters (N), Likelihood and BIC. Removing one mechanism from the Tmodel decreases not only the likelihood but also the BIC score suggesting that the five mechanisms contribute to explain and predict the transition from novice to expert interaction techniques.\nModel Free parameters Fixed parameters N - Likelihood BIC T U\u21e2 , U , 3 , \u2318,F + V W = 0.9 6 148.5 336.5 T - U\u21e2 - , U , 3 , \u2318,F + V W = 0.9 + U\u21e2 = 0 5 199.2 431.4 T - U U\u21e2 , - , 3 , \u2318,F + V W = 0.9 + U = 0 5 164.8 362.5 T - 3 U\u21e2 , U , - , \u2318,F + V W = 0.9 + 3 = 0 5 162.2 357.3 T - \u2318 U\u21e2 , U , 3 , - ,F + V W = 0.9 + \u2318 = 1 5 156.1 345.1 T -F U\u21e2 , U , 3 , \u2318, - + V W = 0.9 + F = 0 5 175.9 384.7\nsynthetized data for our model in comparison with the ones synthetized by the benchmark RL models. Indeed, our T model well reects for each method the absolute and relative evolution of shortcut use over time."
        },
        {
            "heading": "11 MODEL VARIANTS",
            "text": "Our model combines ve key mechanisms, but it remains unclear whether all of them are useful to replicate the trial-by-trial evolution of strategy choices. We thus decided to compare ve variants of our models (Table 6). Each of these variants corresponds to the T model where one of the mechanism (e.g. implicit learning) has been disabled. The objective is to study the inuence of disabling each mechanism on likelihood and BIC score."
        },
        {
            "heading": "11.1 Fiing Results for Action Choices",
            "text": "11.1.1 Overall. Table 6 summarizes the ve variants of our model depending on the dierent combinations of free and xed parameters. We observe that the best model both in terms of likelihood and BIC score is the one implementing the ve mechanisms, thus the full T model (Table 6): implicit learning U , explicit learning U\u21e2 , decay 3 , planning \u2318, and perseveration (F ). The second best model is the one without planning which has a dierence of BIC score larger Manuscript submitted to ACM\n1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196\nFig. 7. Comparison of the T with its variants where a mechanism has been disabled in term of Likelihood (Le) and BIC Score (Right) per technique. The lower, the beer. Error bars show 95% bootstrap confidence intervals\nthan 6 (336.5 vs. 345.1). These results suggest that all ve mechanisms contribute to explain the transition from menus to shortcuts in these participants.\n11.1.2 Method level. Figure 7 summarizes the goodness-of-t (likelihood and BIC) of each model variant per method. We observe that the model implementing all ve mechanisms, T, outperforms (Likelihood and BIC) all variants for Traditional and Audio.\nRegarding Disabled, T and the variant without U has the same likelihood (138.4) and outperform the other variants. The fact that, that these two models have the same likelihood is not surprising as the implicit learning mechanism is not used in this interaction method: Disabled does not let users use the Menu strategy and thus can not implicitly learn shortcuts. However, T is penalized with the BIC score as U is not used. In term of BIC score, the variant without planning ) \u2318 (312.9) is similar to ) U (312.6) and outperform T (316.3). These results rene our understanding of interacting with the Disabled technique: Not only users do not implicitly learn keyboard shortcuts as the menu is disabled but they also do not need to plan as the choice of strategies is limited.\n11.1.3 User level. Results indicate that the best model (likelihood) is T for 24 participants, the one without planning for 7, the one without implicit learning for 4, the one without decay for 4, the one without explicit learning for 2 and nally the one without perseveration for 1. However, when considering the BIC score, no model really emerges: none of the models is the best model for more than 12 participants out of 41. Altogether, these results indicate that the 5 mechanisms are necessary but not with the same weight for each participant / interaction method."
        },
        {
            "heading": "11.2 Model simulations",
            "text": "We analyze the simulated data at dierent levels of granularity:\n11.2.1 Block-by-block: Evolution of keyboard shortcuts. Figure 8 illustrates the percentage of shortcuts per block and per interaction method for each model variant. The simulation of model variants provides a slightly dierent picture than model tting. Indeed, two model variants, the one without decay 3 (MSE=26.3) and the one without perseveration F (MSE=26.3) outperform T (MSE=39.9).\nThese results echo the ones obtained when comparing the three benchmark RL models. Indeed, both RWCK and T were the best models in term of goodness of t, but their variants without perseveration (i.e. RW and\nManuscript submitted to ACM\n1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248\nT - F ) better synthesize data. The good performance of the variant without decay 3 is surprising. One possible explanation is that the absence of decay articially compensates for the presence of perseveration when simulating data. For this reason, we also analysed the performance of the variant T 3 F corresponding to the T model without the decay and perseveration mechanisms. Regarding goodness-of-t, as expected T 3 F does not outperform the previous variants in terms of likelihood (206.4) and BIC score (439.2). Regarding model simulation, the results are similar, where T 3 F (MSE = 41.4) does not outperform the variant without decay T 3 (MSE=26.3) and T F (MSE=26.3), conrming our hypothesis that the absence of decay articially compensates for the presence of perseveration when simulating data. Further investigations are necessary to precisely understand the role of perseveration.\nFig. 8. Keyboard shortcut use (%) per block and interaction method for each model variant: observed participants\u2019 data are represented with dots. Synthetised data (solid line) where produced by aggregating 50 simulations per participant with individual parameters. MSE is calculated for each model variant.\n11.2.2 Trial-by-trial: individual participant actions. Figure 9 shows one of the sequences of command executions for one participant using the audio method: The rst row shows the participant data. For the denition of the transition (yellow box), we used the data of [6] where two experts annotated all the sequences of strategies from the Grossman et al. experiment [44]. The second and third rows illustrate the synthesized data from the T model and its variant (T F ) without the perseveration mechanism. From our observations, we found that the T F model better reects the participant\u2019s transition than the T model both in terms of beginning and duration as well as in terms of variability of the strategies before, during and after the transition. The example of Figure 9 is representative of many sequences."
        },
        {
            "heading": "11.3 Discussion",
            "text": "In summary, (1) the results indicate that I ,  , D and P mechanisms play a role both in explaining and predicting the transition from menus to keyboard shortcuts; (2) the role of  is less clear: while it signicantly contributes to the goodness-of-t (model tting), the data produced by the models without  better reect participants\u2019 behavior (model simulation). Altogether, the models T and ) , appear the most promising models for the transition from menus to shortcuts. Finally, (3) our results highlight the importance of studying model variants as well as combining model tting and model simulation [80] to validate models of shortcut adoption. Manuscript submitted to ACM\n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300\nPa rti cip\nan t\nTr a n si tio n Tw\nRW CK\n(s) (s)\n(s) (s)\nFig. 9. Simulation comparisons ( T (T), T F, ',\u21e0 ) for a single command and a single user using the AUDIO interactive method. Each dot represents one command execution (144 executions for this command). The color encodes the strategy (Menu: Blue; Keyboard shortcut: Green; Learning: Pink); The height encodes the execution time; Finally, large dots indicate errors. For this participant (top row), the model T F (third row) beer reflects the observed transition (indicated as a yellow box on the participant data). The green line indicates the probability of the agent to execute shortcuts. We observe that this one is much more stable for T F than the two other models because of the lack of the perseveration mechanism."
        },
        {
            "heading": "12 DISCUSSION AND FUTUREWORK",
            "text": "In this section, we summarize our main contributions regarding the design and the empirical evaluation of the T model. We then analytically evaluate this model in light of the criteria of Jacobs and Grainger [50] providing directions for future work. Finally, we discuss the opportunities of neuroscience research to model complex HCI tasks such as the transition from novice to expert interaction techniques."
        },
        {
            "heading": "12.1 Model of the transition from novice to expert interaction techniques",
            "text": "In this paper, we presented a new model, T, to predict the transition from novice to expert interaction techniques. One key aspect of our approach was to model the whole learning process of expert technique adoption, i.e to explain whether, when and how users make the transition. Another key aspect was to rely on the Reinforcement Learning framework appropriate to address learning and decision-making problems, where we considered three high-level strategies (Menu; Shortcut and Learning) as actions."
        },
        {
            "heading": "12.2 Empirical evaluation of the model",
            "text": "The T model has been empirically evaluated on the Grossman et al. database [44]. A key contribution of our work is the variety of approaches used to evaluate our model, increase transparency and avoid potential evaluation biases. First, despite the lack of dedicated models of shortcut adoption in HCI, we compared our model to three benchmark models in neuroscience (RW, CK and RWCK), which are widely used in decision-making tasks involving a\nManuscript submitted to ACM\n1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352\nlearning process [101]. Second, we analyzed our model both in terms of goodness-of-t and simulation. These two methods have been shown to be complementary as they can lead to dierent conclusions [80, 101]. Third, we analyzed our model at four levels of granularity [9]: Overall, interaction method, participant and sequence of actions. This is important to avoid the risk to over-interpret aggregated data. Fourth, we compared our model to variant models where each of the ve involved mechanisms have been alternatively disabled to ensure all mechanisms are useful.\nAltogether our results show that the T model outperforms the three benchmark RL models and the ve mechanisms contribute to shortcut adoption."
        },
        {
            "heading": "12.3 Analytical evaluation of the model",
            "text": "We now critically discuss our model in light of the criteria of Jacobs and Grainger [50]:\n12.3.1 P  E. T is well grounded in the cognitive science and neuroscience literatures, both in terms of problem formulation and model design.\nFirst, our problem formulation is in line with the computational view of human behavior [39, 66] where the users\u2019 strategies (policy) emerge from the user\u2019s goal (utility), their cognitive mechanisms and the task environment. Moreover, we acknowledge a hierarchical nature of decision-making [56], where we focus on the higher level of decision-making: users choose a strategy among Menu, Shortcut and Learning.\nOne promising direction for future work is to investigate alternative hierarchies of high-level decisions. For instance, some users might decide rst whether they use shortcut or not. If not, they then decide whether they use the Menu or Learning strategy. However, it is not clear whether this would produce signicantly dierent results than the present model. It would also be interesting to rene the action \u201cLearning\" to capture the degree of explicit learning, e.g. the time spent to learn the shortcut. Another direction is to adopt a mechanistic approach and to rene low-level decisions, i.e. how users select items in a menu or execute shortcuts. This will be important when focusing on execution time and error rate. Indeed, one limitation of our approach is that it does not cover intramodal performance improvement [26]. Our model currently assumes that execution time does not evolves over time and only depends on the used strategy. We plan to introduce some mechanisms such as visual search and pointing (Fitts\u2019 law) from existing models of menu performance [9, 21], and include speed of recall from memory [2, 96] to reect how users behave within a menu as well as the eect of practice on execution time. We also plan to understand the nature of errors when using shortcuts. Currently our model over-estimates the number of errors. One direction would be to introduce a component for risk aversion as users might value more the certainty of correctly executing a command with menus than the uncertainty of the benets of shortcuts [82].\nSecond, we designed the T model so that it combines ve mechanisms: implicit and explicit learning, decay, planning and preservation. These ve mechanisms are grounded in the cognitive science and neuroscience literatures. The comparison of T with some variants enabling/disabling each mechanism suggests that these ve mechanisms play a role in the transition observed in participants.\nAmong these mechanisms, perseveration should require further investigation as its role is less clear. It has been demonstrated that perseveration is frequent in human choice behavior [101], but we have observed here dierences between model tting and simulation data. One possible reason is that our degree of perserveration is currently static. An alternative would be to introduces the choice kernel of the CK and RWCK models. This might provide more realistic simulations. Another direction is to add a model-free component. Indeed, several approaches in neuroscience and cognitive sciences combine model-free and model-based RL models. While this approach is more complex, it might help Manuscript submitted to ACM\n1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404\nbetter describe users\u2019 behaviors [96]. Finally, it would also be interesting to study whether the use of shortcuts acquires the properties of behavioral habits after hours of practice, because computational neuroscience studies have pointed to a role of model-free RL mechanisms in the progressive acquisition of behavioral habits [29, 54, 96].\nSeveral additional mechanisms could also be considered as future work. Among them, a promising direction is the transfer of learning between commands. Our model assumes that the evolution of the knowledge for a given command is independent of the other commands. We would like to investigate whether the successful adoption of a shortcut (for a given command) has an impact on the transition for the other commands. Future work should also investigate the ability of users to estimate command frequency and to include this estimate within the planning process.\n12.3.2 Descriptive adequacy. Our model provides a good description of the observed data in comparison with the tested benchmark RL models both in terms of model tting and model simulation. In absolute terms, our tting scores might appear low. However, this is often the case when modeling decision-making problems due to the complexity of the task [101]. Moreover, we used state-of-the art model tting methods that do not favor high tting score but better reect the adequacy with human behavior [101]. Indeed, we predicted trial-by-trial actions for each participant, i.e. we predicted more than 700 decisions per participant with a high variability within and between participants.\n12.3.3 Interpretability and Complexity. Our predictive HCI model does not rely on \u201cblack box\" machine learning such as deep learning [67]. Each parameter is associated to psychological mechanisms. Moreover, our approach shares some similarities with cognitive models (e.g. ACT-R), but is less complex as it relies on a well-established RL framework and has a limited number of parameters per participant. Finally, the model is easy to implement and test, i.e it does not require running millions of simulations such as regular RL models in HCI (e.g. [13, 21, 22, 38]). Considering the complexity of the task to predict in comparison with common HCI motor control tasks (e.g. Fitts law), we argue that our model has a low complexity.\n12.3.4 Generalizability. Further work should investigate whether the model can characterize and predict the users\u2019 behavior with dierent interaction methods, modalities, populations or tasks. An analytical examination of our model already provides some hints. For instance, some interaction methods penalize menu selection time (e.g. HotKeyCoach [58]) or reduce the temporal cost of the Learning strategy (e.g. ExposeHK [71], KeyCue [92]). The equations 8, 9 and 10 inform that these strategies (i.e. increasing )\" or reducing )!) reduce the Q-Value of Menu in comparison with the two other strategies and thus favor shortcut adoption (Equation 2). However, several interaction methods are more complex to model such as those considered in this article. Indeed, their dierences can not easily be represented with quantitative values and were represented as a nominal scale. Our long-term goal is thus to demonstrate our model can rely on a unique set of parameters independent of the interaction methods, i.e. the interaction methods are represented as a small set of variables, in order to test if the model gives plausible predictions when the techniques changed. We also plan to test whether the model can characterize the transition from menus to gesture shortcuts. This would probably require to consider additional types of decision (e.g. decision about the mapping between the command and the shortcut) to reect the fact that gestures are generally easier to learn and recall than keyboard shortcuts [4]."
        },
        {
            "heading": "12.4 Command selection and computational models",
            "text": "Beyond this work, this manuscript is also a call for computational models of command selection and in particular the transition from novice to expert interaction techniques. We argue that command selection is an important proxy to study HCI [7]. One whole interface can be too dicult to model because it involves so many dierent users\u2019 behaviors.\nManuscript submitted to ACM\n1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456\nIn contrast, a simple pointing task (which is already quite complex) hides fundamental aspects such as those related to learning or decision-making. Command selection appears to have the right level of complexity and thus especially appropriate for computational modeling. The main interactive components of command selection (menus, gestures, keyboard shortcuts, etc.) are quite well dened but involve many fascinating and challenging phenomena related to pointing, visual search, skill acquisition and decision-making, in particular, when considering the transition from novice (e.g. menus) to expert interaction techniques (e.g. shortcuts). However, we were not aware of a computational model to explain or predict how users switch from menus to shortcuts. This is surprising given the number of models of menu performance, e.g. [9, 21]. We believe that one of the reasons is that this transition involves a subtle interaction between learning and decision-making which is dicult to disentangle. This work contributes the rst step in this direction and should encourage other researchers to investigate this challenging and fundamental HCI problem."
        },
        {
            "heading": "12.5 Neuroscience and Human-Computer Interaction",
            "text": "Neuroscience inuences many elds such as economics, psychology, social sciences, marketing or information systems [86]. Recently, several authors also mentioned the potential of Neuroscience for HCI [75, 86]. In particular, in terms of empirical methods and tools to study interaction design [98]. For instance, by using fMRI, PET, EEG or GSR techniques to measure the eect of artifacts on the cognitive state (e.g. cognitive eects) of individual users or to identify cognitive conicts in specic brain regions. In this article, we demonstrate the potential of importing approaches, models and evaluation methods from Neuroscience to HCI from a theoretical perspective.\nFirst, neuroscience is strongly anchored in computational rationality [39], an emerging approach in HCI [21, 66]. Both elds address problems related to learning, decision-making or emotions with concepts of utility and reward through the Reinforcement Learning (RL) framework. However, neuroscience approaches can be benecial to HCI. For instance, previous RL-based HCI models generally adopt a \"machine learning\" perspective of RL where the evolution of the Q-values does not have meanings (see section 2.2.4). In contrast, the dynamic of the Q-values is of importance in Neuroscience and reects how the human or animal learns [101].\nSecond, several models have been proposed to study human behavior in Neuroscience (relying on the computational Rationality approach). We considered three of them: Rescorla-Wagner, Choice Kernel and their combination. However, more advanced models should be considered and transposed to HCI problems. In particular, an emergent class of models combining model-free and model-based RL approaches have been proved ecient to explain complex human behaviors. We plan to investigate such models, e.g. [96] in the context of the transition from novice to expert interaction techniques.\nThird, Neuroscience has well-established methods to evaluate models of human behavior which are not common practice in HCI. For instance, it is common in HCI to consider population models (the same parameters for each participant), while we considered individual models (each participant has a dierent set of parameters) which is more appropriate when studying decision making [101] (see section 6.1.2). Moreover, our tness function considers trial-bytrial actions rather than aggregatedmeasures.While computationallymore expensive, this better reects users\u2019 behaviors. We also combined goodness-of-t and simulation and performed post-analysis enabling/disabling mechanisms in order to increase the transparency of our results, which constitute gold standard nowadays in computational neuroscience [101]. We believe that these methods and others such as Model recovering can increase the validity, robustness and transparency of HCI computational models.\nManuscript submitted to ACM\n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508"
        },
        {
            "heading": "13 OPEN SCIENCE",
            "text": "We support adoption and further research eorts by providing an open code repository, with examples and instructions, on our project page: https://hci.isir.upmc.fr/project/model-of-transition/."
        },
        {
            "heading": "14 ACKNOWLEDGMENTS",
            "text": "We would like to thank Baptiste Caramiaux and Sylvain Malacria for their feedback on the early draft of this manuscript as well as the discussions with Olivier Sigaud, Antti Oulasvirta and Andrew Howes. This work was funded by Agence Nationale de la Recherche (grant number ANR-16-CE33-0023)."
        }
    ],
    "title": "Computational Model of the Transition from Novice to Expert Interaction Techniques",
    "year": 2023
}