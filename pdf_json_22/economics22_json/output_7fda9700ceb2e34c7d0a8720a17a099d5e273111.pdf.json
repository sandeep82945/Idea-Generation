{
    "abstractText": "Many widely used models amount to an elaborate means of making up numbers\u2014but once a number has been produced, it tends to be taken seriously and its source (the model) is rarely examined carefully. Many widely used models have little connection to the real-world phenomena they purport to explain. Common steps in modeling to support policy decisions, such as putting disparate things on the same scale, may conflict with reality. Not all costs and benefits can be put on the same scale, not all uncertainties can be expressed as probabilities, and not all model parameters measure what they purport to measure. These ideas are illustrated with examples from seismology, wind-turbine bird deaths, soccer penalty cards, gender bias in academia, and climate policy.",
    "authors": [
        {
            "affiliations": [],
            "name": "PHILIP B. STARK"
        }
    ],
    "id": "SP:6c0efca7a3a497e7a020338e80115864f72e131a",
    "references": [
        {
            "authors": [
                "G.E. Backus"
            ],
            "title": "Isotropic probability measures in infinite",
            "year": 1987
        },
        {
            "authors": [
                "A. Bicchi",
                "M. Buss",
                "M.O. Ernst",
                "A. Peer"
            ],
            "title": "The sense of touch and its rendering: progress in haptics research",
            "year": 2008
        },
        {
            "authors": [
                "P.C. Cosman"
            ],
            "title": "Gender in engineering departments",
            "year": 2017
        },
        {
            "authors": [
                "G.E.P. Box"
            ],
            "title": "Science and statistics",
            "venue": "Journal of the American Statistical Association, 71, 791\u2013799.",
            "year": 1976
        },
        {
            "authors": [
                "R.A. Sperling"
            ],
            "title": "Dissociating confidence and accuracy",
            "year": 2004
        },
        {
            "authors": [
                "A. C"
            ],
            "title": "Engineering seismic risk analysis",
            "venue": "Journal of Cognitive Neuroscience,",
            "year": 1968
        },
        {
            "authors": [
                "A. Desrosi\u00e8res"
            ],
            "title": "The politics of large numbers: A history of statistical reasoning",
            "venue": "Harvard University Press.",
            "year": 1998
        },
        {
            "authors": [
                "P. Diaconis",
                "D.A. Freedman"
            ],
            "title": "On the consistency",
            "year": 1986
        },
        {
            "authors": [
                "R. Feynman"
            ],
            "title": "Cargo cult science",
            "venue": "Engineering and Science, 37(7), 10\u201313. Fisher, R. A. (1935). The design of experiments (9th ed.).",
            "year": 1974
        },
        {
            "authors": [
                "R.H. Frank"
            ],
            "title": "Why is cost-benefit analysis so controversial",
            "year": 2000
        },
        {
            "authors": [
                "D.A. Freedman"
            ],
            "title": "Issues in the foundations of statistics",
            "year": 2010
        },
        {
            "authors": [
                "D.A. Freedman"
            ],
            "title": "On types of scientific inquiry: the role",
            "year": 2010
        },
        {
            "authors": [
                "D.A. Freedman"
            ],
            "title": "The Grand leap",
            "venue": "D. Collier, J. Sekhon,",
            "year": 2010
        },
        {
            "authors": [
                "S.O. Funtowicz",
                "J.R. Ravetz"
            ],
            "title": "The worth of a songbird",
            "year": 1994
        },
        {
            "authors": [
                "Z. Gorvett"
            ],
            "title": "What we can learn from conspiracy",
            "year": 2020
        },
        {
            "authors": [
                "T. Houser",
                "S. Hsiang",
                "R. Kopp",
                "K. Larsen"
            ],
            "title": "Economic risks of climate change: An American prospectus",
            "year": 2015
        },
        {
            "authors": [
                "D. Kahneman"
            ],
            "title": "Thinking, fast and slow",
            "venue": "Farrar, Strauss, and",
            "year": 2011
        },
        {
            "authors": [
                "P. Giroux. Kennedy"
            ],
            "title": "A Guide to Econometrics, 4 edition, the MIT",
            "year": 2001
        },
        {
            "authors": [
                "V. Kleme\u0161"
            ],
            "title": "The improbable probabilities of extreme floods",
            "year": 1989
        },
        {
            "authors": [
                "K. Krug"
            ],
            "title": "The relationship between confidence and accu",
            "year": 2007
        },
        {
            "authors": [
                "L. Le Cam"
            ],
            "title": "A note on metastatistics, or \u2018an essay",
            "year": 1977
        },
        {
            "authors": [
                "B. Luen",
                "P.B. Stark"
            ],
            "title": "Testing earthquake predictions",
            "year": 2008
        },
        {
            "authors": [
                "D. Mayo"
            ],
            "title": "Statistical inference as severe testing: How to get beyond the statistics wars",
            "venue": "Cambridge University Press.",
            "year": 2018
        },
        {
            "authors": [
                "R.A. Millikan"
            ],
            "title": "On the elementary electrical charge and the",
            "year": 1913
        },
        {
            "authors": [
                "Nature."
            ],
            "title": "Rothschild\u2019s numerate arrogance",
            "venue": "Nature, 276, 429.",
            "year": 1978
        },
        {
            "authors": [
                "W. Neiswanger",
                "A. Ramdas"
            ],
            "title": "Uncertainty quantification",
            "year": 2021
        },
        {
            "authors": [
                "A. O\u2019Hagan"
            ],
            "title": "Eliciting expert beliefs in substantial practical",
            "year": 1998
        },
        {
            "authors": [
                "N. Ogasa"
            ],
            "title": "Mass shootings and gun violence in the United",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ogata"
            ],
            "title": "Statistical models for earthquake occurrences and",
            "year": 1988
        },
        {
            "authors": [
                "M. Ranson"
            ],
            "title": "Crime, weather, and climate change",
            "venue": "Journal of Environmental Economics and Management, 67, 274\u2013302.",
            "year": 2014
        },
        {
            "authors": [
                "S. Rayner"
            ],
            "title": "Uncomfortable knowledge: The social",
            "year": 2012
        },
        {
            "authors": [
                "J.C. Regier",
                "P.B. Stark"
            ],
            "title": "Uncertainty quantification for",
            "year": 2015
        },
        {
            "authors": [
                "A. Saltelli"
            ],
            "title": "Statistical versus mathematical modelling: A",
            "year": 2019
        },
        {
            "authors": [
                "H. Sarsons"
            ],
            "title": "Gender differences in recognition for group",
            "year": 2015
        },
        {
            "authors": [
                "P.G. Sassone",
                "W.A. Schaffer"
            ],
            "title": "Cost-benefit analysis: A handbook",
            "year": 1978
        },
        {
            "authors": [
                "T. Seidenfeld"
            ],
            "title": "R",
            "venue": "A. Fisher\u2019s fiducial argument and Bayes\u2019",
            "year": 1992
        },
        {
            "authors": [
                "M. Shermer"
            ],
            "title": "Patternicity: Finding meaningful patterns",
            "year": 2008
        },
        {
            "authors": [
                "P.B. Stark"
            ],
            "title": "SticiGui, Chapter 13 Probability: Philosophy",
            "year": 1997
        },
        {
            "authors": [
                "P.B. Stark"
            ],
            "title": "Reproducibility, p-values, and Type III errors",
            "year": 2022
        },
        {
            "authors": [
                "P.B. Stark"
            ],
            "title": "Constraints versus priors",
            "venue": "SIAM/ASA Journal of Uncertainty Quantification, 3, 586\u2013598. https://doi.org/10.1137/",
            "year": 2015
        },
        {
            "authors": [
                "P.B. Stark",
                "D.A. Freedman"
            ],
            "title": "What is the chance of an earthquake",
            "year": 2010
        },
        {
            "authors": [
                "P.B. Stark",
                "R. Freishtat"
            ],
            "title": "An evaluation of course evaluations",
            "venue": "ScienceOpen Research",
            "year": 2014
        },
        {
            "authors": [
                "P.B. Stark",
                "A. Saltelli"
            ],
            "title": "Cargo-cult statistics and scientific crisis. Significance",
            "year": 2018
        },
        {
            "authors": [
                "P.B. Stark",
                "L. Tenorio"
            ],
            "title": "A primer of frequentist and bayesian inference in inverse problems",
            "year": 2010
        },
        {
            "authors": [
                "S. Stein",
                "R.J. Geller",
                "M. Liu"
            ],
            "title": "Why earthquake hazard maps often fail and what to do about",
            "venue": "it. Tectonophysics,",
            "year": 2012
        },
        {
            "authors": [
                "S. Stein",
                "J. Stein"
            ],
            "title": "Shallow versus deep uncertainties in natural hazard",
            "venue": "assessments. Eos,",
            "year": 2013
        },
        {
            "authors": [
                "S.M. Stigler"
            ],
            "title": "The history of statistics: The measurement of uncertainty before 1900",
            "venue": "Harvard University Press.",
            "year": 1986
        },
        {
            "authors": [
                "N.N. Taleb"
            ],
            "title": "The Black Swan: The impact of the highly improbable",
            "venue": "Random House.",
            "year": 2007
        },
        {
            "authors": [
                "A. Tversky",
                "D. Kahneman"
            ],
            "title": "Judgment under uncertainty: Heuristics and biases",
            "year": 1975
        },
        {
            "authors": [
                "J.P. van der Sluijs"
            ],
            "title": "Numbers running wild",
            "venue": "Science on the verge. Tempe and Washington: Consortium for science,",
            "year": 2016
        },
        {
            "authors": [
                "J.W. van Prooijen",
                "K.M. Douglas"
            ],
            "title": "Belief in conspiracy theories: Basic principles of an emerging research domain",
            "venue": "European Journal of Social Psychology,",
            "year": 2018
        },
        {
            "authors": [
                "R.T. Watson",
                "P.S. Kolar",
                "M. Ferrer",
                "T. Nyg\u00e5rd",
                "N. Johnston",
                "W.G. Hunt",
                "H.A. Smit-Robinson",
                "C.J. Farmer",
                "M. Huso",
                "T.W. Katzner"
            ],
            "title": "Raptor interactions with wind energy: case studies from around the world",
            "venue": "Journal of Raptor Research,",
            "year": 2018
        },
        {
            "authors": [
                "H.O. Witteman",
                "M. Hendricks",
                "S. Straus",
                "C. Tannenbaum"
            ],
            "title": "Are gender gaps due to evaluations of the applicant or the science? A natural experiment at a national funding agency",
            "venue": "The Lancet,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "means of making up numbers\u2014but once a number has been produced, it tends to be taken seriously and its source (the model) is rarely examined carefully. Many widely used models have little connection to the real-world phenomena they purport to explain. Common steps in modeling to support policy decisions, such as putting disparate things on the same scale, may conflict with reality. Not all costs and benefits can be put on the same scale, not all uncertainties can be expressed as probabilities, and not all model parameters measure what they purport to measure. These ideas are illustrated with examples from seismology, wind-turbine bird deaths, soccer penalty cards, gender bias in academia, and climate policy.\nKeywords: Probability models, cost\u2013benefit analysis, utility, probabilistic seismic hazard assessment, the cost of climate change."
        },
        {
            "heading": "1. Introduction",
            "text": "reality \u2026 what a concept. \u2014Robin Williams\nThere are reliable, empirically tested models for some phenomena. They are not the subject of this paper. There are also many models with little or no scientific connection to what they purport to explain.1 Such \u2018ungrounded\u2019 models may be used because they are convenient,2 customary, or familiar\u2014even\nin situations where the phenomena obviously violate the assumptions of the models.\nGeorge Box famously said, \u2018\u2018[a]ll models are wrong, but some are useful.\u2019\u2019 (Box, 1976). This raises a number of questions, for instance: \u2018\u2018useful for what?\u2019\u2019 \u2018\u2018how can we tell whether a particular model is useful?\u2019\u2019.\nVirtually any model is useful for getting a paper published or for rhetorical purposes\u2014to persuade those who aren\u2019t paying close attention to technical details. This paper concerns utility for inference and policy.\nA far better quotation from the same paper by Box is, \u2018\u2018[s]ince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.\u2019\u2019 (Box, 1976). This paper is about general issues that make models importantly wrong.\nFor the purpose of prediction, the proof of the pudding is in the eating: whether a model resembles reality matters less than whether the model\u2019s predictions are sufficiently accurate for their intended use. For instance, if a model predicts the fraction of customers who will be induced to buy by a marketing change accurately enough to decide whether to adopt that change, it doesn\u2019t matter whether the model is realistic in any sense. But for the purposes of explanation and causal inference\u2014including formulating public policy\u2014it is generally important for the map (the model) to resemble the real-world territory, even if it does not match it perfectly.\nThe title of this article alludes to the 1939 film The Wizard of Oz. When Dorothy and her entourage meet The Wizard, he manifests as a booming voice and a disembodied head amid towering flames, steam, smoke, and colored lights. During this audience, Dorothy\u2019s dog Toto tugs back a curtain, revealing The Wizard to be a small, ordinary man at the 1 Department of Statistics, University of California, Berkeley, CA, USA. E-mail: stark@stat.berkeley.edu\n1 I am ignoring trivial similarities, such as involving data of the same \u2018type\u2019 (e.g., real numbers, nonnegative numbers, integers, or categories) and I am ignoring heuristics (e.g., cyclical behavior or clustering). For example, see the discussion of the ETAS model below. 2 One reason a model may be convenient is that software to fit or run the model is available. See Stark and Saltelli (2018).\ncontrols of a machine designed to impress, distract, intimidate, and command authority. The Wizard\u2019s artificially amplified, booming voice instructs Dorothy and her entourage, \u2018\u2018[p]ay no attention to the man behind the curtain!\u2019\u2019.\nThe man behind the curtain is an apt metaphor for ungrounded models. Impressive computer results and quantitative statements about probability, risk, health consequences, economic consequences, etc., are often driven by a model behind the curtain\u2014a model that we are discouraged from paying attention to. What claims and appears to be \u2018science\u2019 may be a mechanical amplification of the opinions and ad hoc choices built into the model, which lacks any tested, empirical basis. The choices in building the model\u2014 parametrizations, transformations, assumptions about the generative mechanism for the data, nominal data uncertainties, estimation algorithms, statistical tests, prior probabilities, and other \u2018\u2018researcher degrees of freedom\u2019\u2019\u2014are like the levers in the Wizard\u2019s machine. And like the Wizard\u2019s machine, the models may have the power to persuade, intimidate, and impress, but not the power to predict or control.3\nThis paper tries to pull back the curtain to show how little connection there is between many models and the phenomena they purport to represent. It examines some of the consequences, including the fact that uncertainty estimates based on ungrounded models are typically misleading. Four ideas recur:\n\u2022 Quantifauxcation. Quantifauxcation is a neologism for the common practice of assigning a meaning-\nless number, then concluding that because the result is quantitative, it must mean something (and if the number has six digits of precision, they all matter). Quantifauxcation usually involves some combination of data, models, inappropriate use of statistics, and logical lacunae. \u2022 Type III errors: A Type I error is a false positive, i.e., to reject a true hypothesis. A Type II error is a\nfalse negative, i.e., to fail to reject a false hypothesis. A Type III error is to answer the wrong question,4 e.g., to test a statistical hypothesis\nthat has little or no connection to the scientific hypothesis (an example of the package deal fallacy, whereby things that are traditionally grouped together are presumed to have an essential connection) (Stark, 2022). Another example is to conflate endpoints in clinical trials (e.g., to conclude that a treatment decreases mortality when there is only evidence that it decreases blood pressure). \u2022 Models and Conspiracy Theories: Models and conspiracy theories are technologies for providing\na hard-to-falsify \u2018\u2018explanation\u2019\u2019 for just about anything. Both purport to reveal the underlying causes of complicated phenomena. Both are supported by similar fallacies, including equivocation, changing the subject (red-herring arguments, ignoratio elenchi), appeals to ignorance (argumentum ad ignorantiam), appeals to inappropriate authority (argumentum ad verecundium), ignoring qualifications (secundum quid), hasty generalization, package deal, cherry-picking facts, fallacy of the single cause, faulty analogies, and confirmation bias. \u2018\u2018Science is Real,\u2019\u2019 used as an unreflective slogan that considers \u2018\u2018scientific\u2019\u2019 results to be trustworthy and credible\u2014regardless of the quality of the underlying data and the methodology, conflicts of interest, and agenda\u2014is emblematic. \u2022 Freedman\u2019s Rabbit Theorem: There are two rabbit axioms:\n(1) The number of rabbits in any system is\nnonnegative.\n(2) For the number of rabbits in a closed system to\nincrease, the system must include at least one rabbit (e.g., a pregnant female).\nFrom these, we conclude: Proposition: You cannot borrow a rabbit from an empty hat, even with a binding promise to return the rabbit later. Theorem (Freedman5): You cannot pull a rabbit from a hat unless at least one rabbit has previously been placed in the hat.\n3 For a manifesto regarding responsible modeling, see Saltelli et al. (2020). 4 Another definition of Type III error is \u2018\u2018to draw the right conclusion for the wrong reason.\u2019\u2019\n5 Freedman, personal communication, circa 2001. He did not derive the theorem from axioms, but on more than one occasion told me, \u2018\u2018to pull a rabbit from a hat, a rabbit must first be placed in the hat.\u2019\u2019\nKeeping the rabbit theorem in mind can help us tell whether results must depend heavily on assumptions. For instance, if model inputs are rates but model outputs are probabilities (the rabbit in this example), the model must assume that something is random, thereby putting the rabbit in the hat.\nWe now discuss some general and specific examples."
        },
        {
            "heading": "2. Procrustes\u2019 Quantifauxcation: Forcing Incommensurable Things to the Same Scale",
            "text": "Procrustes of Greek mythology forced travelers to fit his bed, stretching them if they were shorter and cutting their limbs if they were taller. Such a bruteforce reduction of disparate things to make them comparable is a common ingredient in quantifauxcation.\nOne common example is to form an \u2018\u2018index\u2019\u2019 that combines a variety of things into a single number, for example, by adding or averaging \u2018\u2018points\u2019\u2019 on Likert scales (Stark & Freishtat, 2014).\nFor the last 70 years or so, it has been fashionable to assign numbers to things to make them \u2018\u2018scientific.\u2019\u2019 Qualitative arguments are seen as weak, and many humanities and \u2018\u2018soft\u2019\u2019 sciences such as history, sociology, and economics have adopted and exalted computation as scientific and objective.6\nA 1978 opinion piece in Nature (Nature, 1978)\nput the issue well:\nIt is objective and rational to take account of imponderable factors. It is subjective, irrational, and dangerous not to take account of them. As that champion of rationality, the philosopher Bertrand Russell, would have argued, rationality involves the whole and balanced use of human faculty, not a rejection of that fraction of it that cannot be made numerical.\nQuantitative arguments and quantitative models require quantitative inputs. In many disciplines, the quantitative inputs are problematic. For instance,\ncategories (for categorical observations) may be assigned numbers, then the magnitudes of the numbers are treated as meaningful: averages are computed and compared, etc.7 There is also a temptation to do arithmetic with numbers that represent disparate things, pretending that they are on the same scale; this happens frequently in making composite scores from multivariate measurements, for instance, in university rankings.8 The resulting outputs are somewhere between \u2018\u2018difficult to interpret,\u2019\u2019 \u2018\u2018arbitrary,\u2019\u2019 and \u2018\u2018meaningless.\u2019\u2019 This section gives some general and some specific examples related to cost\u2013 benefit analysis and the quantification of uncertainty, including using \u2018\u2018probability\u2019\u2019 as a catch-all, blurring important differences."
        },
        {
            "heading": "2.1. \u2018Utility\u2019 and cost\u2013Benefit Analyses",
            "text": "It is often claimed that the only rational basis for policy is a quantitative cost\u2013benefit analysis,9 an approach tied to reductionism as described by Scoones and Stirling (2020). But if there is no rational basis for its quantitative inputs, how can relying on cost\u2013benefit analysis be rational?\nNot only are costs and consequences hard to anticipate, enumerate, or estimate in real-world problems, but behind every cost\u2013benefit analysis is the assumption that all costs and all benefits can be put on a common, one-dimensional scale, typically money or abstract \u2018utility.\u2019 As a matter of mathematics, multidimensional spaces are not in general totally ordered: a binary relation such as \u2018\u2018has more utility than\u2019\u2019 does not necessarily hold for every pair of points in the space. The idea that you can rank all aspects of a set of outcomes on the same onedimensional scale is an assumption.\nLuce and Tukey (1964) give conditions under which an individual\u2019s preferences among items with multiple attributes can be ranked on the same scale. The conditions (axioms) are nontrivial, as we shall see. Consider a collection of items, each of which has\n6 For a defense of qualitative reasoning in science, see\nFreedman (2010b).\n7 See, e.g., Stark and Freishtat (2014). 8 See also Desrosie\u0300res (1998), chapter 3, \u2018\u2018Averages and the Realism of Aggregates.\u2019\u2019 9 For arguments for and against cost-benefit analysis, see Frank (2000). For a general treatment in the context of policy, see Sassone and Schaffer (1978).\ntwo attributes (e.g., a cost and a benefit). Each attribute has two or more possible values.\nSandwiches, for example:\nAttribute Possible attribute values\nFilling Peanut butter Turkey Ham Condiment Mustard Grape jelly Cranberry sauce\nOne of Luce and Tukey\u2019s conditions, the (double)\ncancellation axiom, requires:\nIf you prefer peanut butter and jelly to turkey and cranberry sauce, and you prefer turkey and mustard to ham and jelly, then you must prefer peanut butter and mustard to ham and cranberry sauce.\nFor sandwiches, the cancellation axiom does not hold for everyone. It fails for me: sandwich type and condiment cannot be put on a single scale of \u2018utility\u2019 for me. Similar problems arise in more realistic cost\u2013 benefit analyses, not merely sandwiches, as a result of the Procrustean insistence on putting disparate qualitative dimensions on the same scale of \u2018utility\u2019 or money.\nLuce and Tukey point out that the cancellation axiom does not always hold, and that when it fails, multidimensional attributes cannot be collapsed to a univariate scale. For instance, they write \u2018\u2018one could test the cancellation axiom by examining a sufficiently voluminous body of ordinal data \u2026 and, thereby, test the primary ingredient in additive independence.\u2019\u2019 (Luce & Tukey, 1964, p.6).\nFailure of the double cancellation axiom is not the only issue with cost\u2013benefit analysis. To quantify costs in a cost\u2013benefit analysis, in effect you must assign a dollar value to human life, including future generations; to environmental degradation; to human culture; to endangered species; and so on. You must believe that scales like \u2018\u2018quality adjusted life-years\u2019\u2019 or \u2018\u2018utility\u2019\u2019 are meaningful, reasonable, and a sound basis for decisions. Some scientists and philosophers are reluctant to be so draconian (Funtowicz & Ravetz, 1994).\nSimilarly, there is a slogan that risk equals probability times consequences. But what if the\nconcept of probability doesn\u2019t apply to the phenomenon in question? (Some reasons that uncertainty may not be described well by probability are discussed below.) What if the consequences resist enumeration and quantification, or are incommensurable?\nThere is evidence that human preference orderings are not based on probability times consequences, i.e., on expected returns or expected losses. For instance, there is a preference for \u2018\u2018sure things\u2019\u2019 over bets. Many people would prefer to receive $1 million for sure than to have a 10% chance of receiving $20 million, even though the expected return of the latter is double (Desrosie\u0301res, 1998, p.49). And many people are loss-averse; for instance, they would prefer a 10% chance of winning $1 million over a 50% chance of winning $2 million with a 50% chance of losing $100 thousand, even though the expected return of the latter is 9.5 times larger. In a repeated game, basing choices on expectations might make sense, but in a single play, other considerations may dominate. Desrosie\u0301res also quotes Leibnitz on the topic:\nAs the size of the consequence and that of the consequent are two heterogeneous considerations (or considerations that cannot be compared together), moralists who have tried tocompare them have become rather confused, as is apparent in the case of those who have dealt with probability. The truth is that in this matter\u2014as in other disparate and heterogeneous estimates involving, as it were, more than one dimension\u2014the dimension in question is the composite product of one or the other estimate, as in a rectangle, in which there are two considerations, that of length and that of breadth; and as for the dimension of the consequence and the degrees of probability, we are still lacking this part of logic which must bring about their estimate.10\nInsisting on quantifying risk and on quantitative cost\u2013 benefit analyses requires doing things that may not make sense technically or morally. Moreover, I have yet to see a compelling example of incorporating\n10 Desrosie\u0301res, 1998, pp. 50\u201351, citing Coumet, 1970, in turn\nciting Leibniz, New Essays on Human Understanding.\nuncertainty in the estimate of the probability (when the notion of \u2018probability\u2019 applies to the problem at all) and uncertainty in the consequences\u2014much less the \u2018value\u2019 of those consequences. Such problems arise in probabilistic seismic hazard assessment (PSHA), discussed below."
        },
        {
            "heading": "2.2. Uncertainty",
            "text": "Just as costs and benefits cannot necessarily be put on the same scale, uncertainties cannot necessarily be put on the same scale. Nonetheless, many practitioners insist on using \u2018probability\u2019 as a catch-all to quantify all uncertainties."
        },
        {
            "heading": "2.2.1 What is Probability?",
            "text": "Probability has an axiomatic aspect and a philosophical aspect. Kolmogorov\u2019s axioms, the mathematical basis of modern probability, are just that: mathematics. Theories of probability provide the philosophical glue to connect the mathematics to the empirical world, allowing us to interpret probability statements.11\nThe oldest interpretation of probability, equally likely outcomes, arose in the 16th and 17th centuries in studying games of chance, in particular, dice games (Stigler, 1986). This theory says that if a system is symmetric, there is no reason Nature should prefer one outcome to another, so all outcomes are \u2018\u2018equally likely.\u2019\u2019 For instance, if a vigorously tossed die is symmetric and balanced, there is no reason for it to land with any particular side on top. Therefore, all six possible outcomes are deemed \u2018\u2018equally likely,\u2019\u2019 from which many consequences follow from Kolmogorov\u2019s axioms as a matter of mathematics.\nThis interpretation of probability has trouble with situations that do not have the intrinsic symmetries of coins, dice, and roulette wheels. For example, suppose that instead of rolling a die, you toss a thumbtack. What is the probability that it lands point up versus point down? There is no obvious symmetry\nto exploit. Should you simply declare those two outcomes to be equally likely? And how might you use symmetry to make sense of \u2018\u2018the probability of an act of nuclear terrorism in the year 2099?\u2019\u2019 That is even weirder. There are many events for which defining \u2018probability\u2019 using equally likely outcomes is unpersuasive or perverse.\nThe second approach to defining probability is the frequency theory, which defines probability in terms of limiting relative frequencies in repeated trials. According to the frequency theory, what it means to say \u2018the chance that a coin lands heads\u2019 is that if one were to toss the coin again and again, the fraction of tosses that resulted in heads would converge (in a suitable sense) to a limit; that limit is defined to be the probability that the coin lands heads. This theory makes particular sense in the context of repeated games of chance, because the long-run frequency that a bet pays off is identified to be its probability, tying the definition of probability to a gambler\u2019s long-run fortune.\nThere are many phenomena for which the frequency theory makes sense (e.g., games of chance where the mechanism of randomization is known and understood) and many for which it does not. What is the probability that global average temperature will increase by three degrees in the next 50 years? What is the chance there will be an earthquake with magnitude 8 or greater in the San Francisco Bay area in the next 50 years?12 Can we repeat the next 50 years over and over to see what fraction of the time that happens, even in principle?\nThe subjective theory or (neo-)Bayesian theory may help in such situations (but see below). The subjective theory defines probability in terms of degree of belief. According to the subjective theory, what it means to say \u2018\u2018the probability that a coin lands heads is 50%\u2019\u2019 is that the speaker believes with equal strength that it will land heads as he or she believes that it will land tails. Probability thus measures the state of mind of the person making the probability statement. Compared with both the theory of equally likely outcomes and the frequency theory, the\nsubjective theory changes the subject (as it were). 11 For a more technical discussion, see Freedman (2010a) and\nStark and Freedman (2010); LeCam (1977). For an elementary discussion, see Stark (1997). For historical discussions, see Desrosie\u0301res (1998) and Diaconis and Skyrms (2018).\n12 See Stark and Freedman (2010) for a discussion of the\ndifficulty of defining \u2018\u2018the chance of an earthquake.\u2019\u2019.\nThe theory of equally likely outcomes is about the symmetry of the coin. The frequency theory is about what the coin will do in repeated tosses. The subjective theory is about what I believe. It changes the subject from geometry or physics to psychology. The situation is complicated further by the fact that people are not very good judges of what is going to happen, as discussed below. For making personal decisions\u2014for instance, deciding what to bet one\u2019s own money on\u2014the subjective theory may be a workable choice.\nThat said, LeCam (1977, pp. 134\u2013135) offers the\nfollowing observations:\n(1) The neo-Bayesian theory makes no difference\nbetween \u2018experiences\u2019 and \u2018experiments\u2019.\n(2) It confuses \u2018theories\u2019 about nature with \u2018facts\u2019,\nand makes no provision for the construction of models. (3) It applies brutally to propositions about theories\nor models of physical phenomena with the same simplified logic which every one of us ordinarily uses for \u2018events\u2019. (4) It does not provide a mathematical formalism in\nwhich one person can communicate to another the reasons for his opinions or decisions. Neither does it provide an adequate vehicle for transmission of information. (This, of course, is irrelevant to a purely \u2018personalistic\u2019 approach.) (5) The theory blends in the same barrel all forms of\nuncertainty and treats them alike.\nThe above shortcomings belong to the neo-Bayesian theory. The neo-Bayesian movement has additional unattractive facets, the most important of which is its normative interpretation of the role of Statistics. Presumably a statistician who does not abide by its regulations is either irrational, inconsistent, or incoherent. \u2026 In summary, the Bayesian theory is wonderfully attractive but grossly oversimplified. It should be used with the same respect and the same precautions as the kinetic theory of perfect monoatomic gases.\nAnother approach to making sense of probability models, different from these three classical interpretations, is to treat probability models as empirical\ncommitments (Freedman & Berk, 2010). For instance, coin tosses are not truly random: if you knew exactly the mass distribution of the coin, its initial angular velocity, and its initial linear velocity, you could predict with certainty how a tossed coin will land. But you might prefer to model the toss as random, at least for some purposes. Modelling it as random entails predictions that can be checked against future data\u2014 empirical commitments. In the usual model of coin tosses as fair and independent, all 2n possible sequences of heads and tails in n tosses of a fair coin are equally likely, which induces probability distributions for the lengths of runs, the number of heads, etc. It has been found that when the number of tosses is sufficiently large, the model does not fit adequately: data give evidence that the model is incorrect. In particular, there tends to be serial correlation among the tosses. And the coin is more likely to land with the same side up that started up.13\nA final interpretation of probability is probability as metaphor. This seems to be how probability enters most policy applications and many scientific applications. It does not assert that the world truly behaves in a random way. Rather, it says that a phenomenon occurs \u2018as if\u2019 it were a casino game. Taleb (2007, pp. 127\u2013129) discusses the ludic fallacy of treating all uncertainty as if it arose from casino games:\n\u2018\u2018The casino is the only human venture I know where the probabilities are known, Gaussian (i.e., bell-curve), and almost computable.\u2019\u2019 \u2026 [W]e automatically, spontaneously associate chance with these Platonified games. \u2026 Those who spend too much time with their noses glued to maps will tend to mistake the map for the territory. \u2026 Probability is a liberal art; it is a child of skepticism, not a tool for people with calculators on their belts to satisfy their desire to produce fancy calculations and certainties. Before Western thinking drowned in its \u2018\u2018scientific\u2019\u2019 mentality, \u2026 people prompted their brain to think\u2014not compute.\n13 The chance the coin will land with the same side on top that started on top is estimated to be about 51% by Diaconis et al. (2007). See also https://www.stat.berkeley.edu/*aldous/RealWorld/coin_tosses.html, which reports an experiment involving 40,000 tosses (last visited 16 June 2022).\nProbability as metaphor is closely tied to probability as an empirical commitment, although the key step of checking whether the model agrees adequately with the data is often omitted. We will see more on this below."
        },
        {
            "heading": "2.2.2 Uncertainty and Probability",
            "text": "Many scientists use the word \u2018probability\u2019 to describe anything uncertain. A common taxonomy classifies uncertainties as aleatory or epistemic. Aleatory uncertainty results from the play of chance mechanisms\u2014the luck of the draw. Epistemic uncertainty results from ignorance. Epistemic uncertainty is \u2018stuff we don\u2019t know\u2019 but in principle could learn.\nCanonical examples of aleatory uncertainty include coin tosses, die rolls, lotteries, radioactive decay, some kinds of measurement error, and the like. Under some circumstances, such things do behave (approximately) as if random\u2014but generally not perfectly so, as mentioned above. Canonical examples of epistemic uncertainty include ignorance of the physical laws that govern a system or ignorance of the values of parameters in a system.\nImagine a biased coin that has an unknown chance p of landing heads. Ignorance of the chance of heads is epistemic uncertainty. But even if we knew the chance of heads, we would not know the outcome of the next toss: it would still have aleatory uncertainty.\nThe standard way to combine aleatory and epistemic uncertainties involves using subjective (aka Bayesian) prior probability to represent epistemic uncertainty. In effect, this puts individual beliefs on a par with unbiased physical measurements that have known uncertainties.\nAttempting to combine aleatory and epistemic uncertainties by considering both to be \u2018probabilities\u2019 that satisfy Kolmogorov\u2019s axioms amounts to claiming that there are two equivalent ways to tell how much something weighs: I could weigh it on an actual physical scale or I could think hard about how much it weighs. The two are on a par. It claims that thinking hard about the question produces an unbiased measurement. Moreover, it claims that I know the accuracy of my internal \u2018measurement\u2019 from careful introspection. Hence, I can combine the two sources\nof uncertainty as if they were independent measurements of the same thing, both made by unbiased instruments.14\nThis doesn\u2019t work. Combining uncertainties of entirely different sources and types by simply giving them the same name (probability) is another form of Procrustean quantifauxcation.\nSir Francis Bacon\u2019s triumph over Aristotle should have put to rest the idea that it is generally possible to make sound inferences about the physical world by pure ratiocination (some Gedankenexperiments notwithstanding15). Psychology, psychophysics, and psychometrics have shown empirically that people are bad at making even rough qualitative estimates, and that quantitative estimates are usually biased.\nMoreover, the bias can be manipulated through processes such as anchoring and priming, as described in the seminal work of Tversky and Kahneman (1975). Anchoring, the tendency to stick close to an initial estimate, no matter how that estimate was derived, doesn\u2019t just affect individuals\u2014it affects entire disciplines. The Millikan oil drop experiment to measure the charge of an electron (Millikan, 1913) is an example: Millikan\u2019s value was too low, supposedly because he used an incorrect value for the viscosity of air. It took about 60 years for new estimates to climb to the currently accepted value, which is about 0.8% higher (a small difference, but considerably larger than the error bars). Other examples include measurements of the speed of light\n14 When practitioners analyse complex systems such as climate, the economy, earthquakes, and such, the same observations they use as data in the problem are also the basis of their beliefs as reflected in the prior. But the analysis generally treats the data and prior as if they provided \u2018\u2018independent\u2019\u2019 measurements\u2014another fishy aspect of this approach. 15 A favorite example is Galileo\u2019s \u2018demonstration\u2019 that Aristotle was wrong that bodies of different masses fall at different rates: evidently, Galileo refuted Aristotle using a thought experiment. https://en.wikipedia.org/wiki/Galileo%27s_Leaning_Tower_ of_Pisa_experiment (last accessed 1 June 2022).\nand the amount of iron in spinach.16 In these examples and others, somebody erred and it took a very long time for a discipline to correct the error because subsequent work did not stray too far from the previous estimate\u2014perhaps because the first estimate made them doubt the accuracy of results that were far from it.\nTversky and Kahneman also showed that people are poor judges of probability, subject to strong biases from anchoring, representativeness and availability, which in turn depends on the retrievability of instances, that is, on the vagaries of human memory. Their work also shows that probability judgments are insensitive to prior probabilities and to predictability, and that people ignore the regression to the mean effect\u2014even people who have had formal training in probability. (Regression to the mean is the mathematical phenomenon that in a sequence of independent realizations of a random variable, particularly extreme values are likely to be followed by values that are closer to the mean.)\nPeople cannot even accurately judge how much an object weighs with the object in their hands. The direct physical tactile measurement is biased by the density and shape of the object\u2014and even its color.17 The notion that one could just think hard about how global temperature will change in the next 50 years and thereby come up with a meaningful estimate and uncertainty for that estimate is preposterous. Wrapping the estimate with computer simulations that are barely grounded in physics distracts, rather than illuminates (Saltelli et al., 2015).\nHumans are also bad at judging and creating randomness: we have apophenia and pareidolia, a tendency to see patterns in randomness.18 And when we deliberately try to create randomness, what we make has fewer patterns than genuinely random\nprocesses generate. For instance, we produce too few runs and repeats (e.g., Schulz et al., 2012; Shermer, 2008). We are over-confident about our estimates and predictions (e.g., Kahnemann, 2011; Taleb, 2007). And our confidence is unrelated to our actual accuracy (e.g., Chua et al., 2004; Krug, 2007).19\nIf I don\u2019t trust your internal scale or your assessment of its accuracy, why should your subjective (Bayesian) analysis carry any weight for me?20\nIn discussing the \u2018\u2018neo-Bayesian\u2019\u2019 theory, LeCam (1977, pp. 155\u2013156) gives these examples of uncertainty:\nIt is clear that we can be uncertain for many reasons. For instance, we may be uncertain because (1) we lack definite information, (2) the events involved will occur according to the results of the spin of a roulette wheel, (3) we could find out by pure logic but it is too hard. The first type of uncertainty occurs in practically every question. The second assumes a well-defined mechanism. However, the neo-Bayesian theory seems to make no real distinction between probabilities attached to the three types. It answers in the same manner the following questions.\n(1) What is the probability that Eudoxus had bigger\nfeet than Euclid?\n(2) What is the probability that a toss of a \u2018fair\u2019 coin\nwill result in tails?\n(3) What is the probability that the 10137 ? 1 digit of\np is a 7?\nEven Savage and de Finetti admit that, especially in cases involving the third kind of uncertainty, our personal probabilities are fleeting, more or less rapidly in that the very act of cogitating to evaluate precisely the probabilities is enough or can be enough to modify or totally overcome the uncertainty situation which one wanted to express. Thus, presumably, when neo-Bayesians state that a certain event A has probability one-half, this may mean either that he did not bother to think about it, 16 It is widely believed that spinach has substantially more iron than other green vegetables. This is evidently the result of a transcription error in the 1870s that shifted the decimal, multiplying the measured value by 10 (see, e.g., http://www.dailymail.co. uk/sciencetech/article-2354580/Popeyes-legendary-love-spinachactually-misplaced-decimal-point.html0). The fact that the original value was far too high was well known before the Popeye character became popular in the 1930s. 17 E.g., Bicchi et al., (2008, Sect. 4.4.3). 18 https://en.wikipedia.org/wiki/Apophenia, https://en. wikipedia.org/wiki/Pareidolia (last accessed 16 November 2016). 19 E.g., https://en.wikipedia.org/wiki/Overconfidence_effect (last accessed 16 November 2016). 20 See Stark and Tenorio (2010).\nor that he has no information on the subject, or that whether A occurs or not will be decided by the toss of a fair coin. The number itself does not contain any information about the process by which it was obtained, fleetingly or not. As a final comment, it seem[]s necessary to mention that in certain respects the theory of personal probability is very similar to a theory of personal mass, which exhibits the same shortcomings. Suppose that a store owner is asked to assign weights to the items in his store. For this purpose he can group items in sets and compare them by hand. If a set A appears to him lighter than a set B we shall say that (A, B) [ R.21 It is fairly easy to see, in the spirit of Theorem 3, that if the relation R is not compatible with an assignment of individual masses to the items and with the additivity of masses, the system is not very coherent. It is also possible to show that if there are enough items which could be indefinitely divided into \u2018equally weighty parts\u2019 the assignment of masses will be unique up to a multiplicative constant. Nobody would be particularly surprised however if it turned out that ten thousand peas which were judged all alike when compared pairwise turn out to be quite different when parted into two sets of 5000. \u2026 In spite of the theoretical possibility of assigning masses by hand comparison in this manner, nobody seems to claim that this is just what should be done in stores. Nobody even claims that since masses are masses there is no point in specifying whether they were obtained by hand comparison, or by using a spring scale or by using a balance. \u2026 If the process of measuring something as definite as masses by hand comparison seems rather unreliable, can one really expect a similar theory of measurement of ethereal opinions to inspire much confidence? If an indication of the process of measurement is helpful in the masses problem, it also appears necessary in the opinion problem.\nFinally, an assignment of masses may conceivably be checked by experimenting with a scale, but the neo-Bayesian theory does not even pretend to make statements which could be checked by an impartial observer.\nAn editorial in Nature (1978) also pushes back on the idea that all risks can be quantified, much less quantified on the same scale:\nLORD ROTHSCHILD, speaking on British television last week, argued that we should develop a table of risks so we could compare, say, the risk of our dying in an automobile accident with the risk of Baader-Meinhoff guerillas taking over the nuclear reactor next door. Then we would know how seriously to take our risks, be they nuclear power, damage to the environment or whatever. \u2026 It is fine for Rothschild to demonstrate his agility with arithmetic, converting probabilities from one form to another (and implying that the viewers could not do it) but this is only the kindergarten of risk. \u2026 More than this, Rothschild confused two fundamental distinct kinds of risk in his table: known risks-such as car accidents-where the risk is simply calculated from past events; and unknown risks\u2014 such as the terrorists taking over a fast breeder\u2014 which are matters of estimating the future. The latter risks inevitably depend on theory. Whether the theory is a social theory of terrorism or a risktree analysis of fast breeder failure, it will be open to conjecture. And it ought to be remembered that the history of engineering is largely a history of unforeseen accidents. Risk estimates can be proved only by events. Thus it is easy for groups, consciously or unconsciously, to bend their calculations to suit their own objectives or prejudices. With unknown risks it is as important to take these into account as to come up with a number.\nIn short, insisting on quantifying all kinds of uncertainty on the same scale\u2014probability\u2014is neither\nhelpful nor sensible. 21 This is rigorous way of writing a binary relation R (lighter\nthan): the relation is a set of ordered pairs, the pairs of items that satisfy the binary relation."
        },
        {
            "heading": "2.2.3 Rates Versus Probabilities",
            "text": "It is common to conflate empirical rates with probabilities. I have seen many examples in the literature of physics, geophysics (see below), medicine, and other fields. I am not saying that historical rates have no information about a process, but that historical rates are just that: historical rates. They are not probabilities, nor are they in general estimates of probabilities. Turning a rate into (an estimate of) a probability is like pulling a rabbit from a hat. Unless the \u2018\u2018physics\u2019\u2019 of the problem put probability into the hat, Freedman\u2019s Rabbit Theorem implies that probability cannot emerge from the hat.\nKlemes\u030c (1989) wrote eloquently about this false\nequivalence in hydrology:\nThe automatic identification of past frequencies with present probabilities is the greatest plague of contemporary statistical and stochastic hydrology. It has become so deeply engrained that it prevents hydrologists from seeing the fundamental difference between the two concepts. It is often difficult to put across the fact that whereas a histogram of frequencies for given quantities [] can be constructed for any function whether it has been generated by deterministic or random mechanism, it can be interpreted as a probability distribution only in the latter case. [] Ergo, automatically to interpret past frequencies as present probabilities means a priori to deny the possibility of any signal in the geophysical history; this certainly is not science but sterile scholasticism. The point then arises, why are these unreasonable assumptions made if it is obvious that probabilistic statements based on them may be grossly misleading, especially when they relate to physically extreme conditions where errors can have catastrophic consequences? The answer seems to be that they provide the only conceptual framework that makes it possible to make probabilistic statements, i.e. they must be used if the objective is to make such probabilistic statements.\nMy experience in other branches of physical science and engineering is the same: equating historical rates with probabilities is so deeply ingrained that it can be\nimpossible to get some practitioners to see that there is a profound difference between the two concepts.\nAny finite series of dichotomous trials has an empirical rate of success. But the outcomes of a series of trials cannot tell you whether the trials were random in the first place. Suppose there is a series of Bernoulli trials,22 that each trial has the same probability p of success, and that the trials are independent\u2014like the standard model of coin tossing, treating \u2018heads\u2019 as \u2018success.\u2019 Then the Law of Large Numbers guarantees that the rate of successes converges (in probability) to the probability of success.\nIf a sequence of trials is random and the chance of success is the same in each trial, then the empirical rate of success is an unbiased estimate of the underlying chance of success. If the trials are random and they have the same chance of success and you know the dependence structure of the trials (for example, if the trials are independent), then you can quantify the uncertainty of that estimate of the underlying chance of success. But the mere fact that something has a rate does not mean that it is the result of a random process.\nFor example, suppose a sequence of heads and tails results from a series of random, independent tosses of an ideal fair coin. Then the rate of heads will converge (in probability) to one half. But suppose I give you the sequence \u2018heads, tails, heads, tails, heads, tails, heads, tails, heads, tails, \u2026\u2019 ad infinitum. The limiting rate of heads is . While that sequence could be the result of a sequence of fair random tosses, it is implausible, and it certainly need not be. Sequences of outcomes are not necessarily the result of anything random, and rates are not necessarily (estimates of) probabilities.\nHere are two thought experiments:\n1. You are in a group of 100 people. You are told\nthat one person in the group will die next year. What is the chance it is you? 2. You are in a group of 100 people. You are told\nthat one of them is named Philip. What is the chance it is you?\n22 A Bernoulli trial is a random dichotomous trial that can result in failure or success, generally represented numerically as 0 or 1, respectively.\nThere is not much difference between these scenarios: both involve a rate of 1%. But in the first one you are invited to say, \u2018the chance is 1%,\u2019 while in the second you are invited to say, \u2018that\u2019s a silly question.\u2019 The point is that a rate is not necessarily a probability, and that probability does not capture every kind of uncertainty.\nIn question 1, if the mechanism for deciding who will die in the next year is to shoot the tallest person, there is nothing random. There is then no probability that you will be the person who dies\u2014you either are or are not the tallest person, just as you either are or are not named \u2018Philip.\u2019 If the mechanism for deciding who will die is to draw lots and shoot whoever gets the short straw, that might be reasonably modeled as random, in which case the probability that you are the person who dies is indeed 1%. The existence of a probability is in the method of selection, not in the existence of a rate.\nEveryday language does not distinguish between \u2018random,\u2019 \u2018haphazard,\u2019 and \u2018unpredictable,\u2019 but the distinction is crucial for scientific inference. \u2018Random\u2019 is a very precise statistical term of art. (The next section discusses how randomness might enter a scientific problem.)\nHere is an analogy: to know whether a soup is too salty, a very good approach is to stir the soup thoroughly, dip in a tablespoon, and taste the contents of the tablespoon. That amounts to tasting a random sample of soup. If instead of stirring the soup I just dip the spoon in without looking, that would be a haphazard sample, a very different process. The second is unpredictable, but it is not a random sample of soup, and it is not possible to quantify usefully the uncertainty in estimating the saltiness of the soup from such a sample.\nNotions such as probability, P-values, confidence intervals, etc., apply only if the data have a random component, for instance, if they are a random sample, if they result from random assignment of subjects to different treatment conditions, or if they have random measurement error. They do not apply to samples of convenience; they do not apply to haphazard samples; and they do not apply to populations. The mean and standard deviation of the results of a group of studies or models that are not a random sample from\nanything does not yield actual P-values, confidence intervals, standard errors, etc. They are just numbers.\nRates and probabilities are not the same, and ignorance and randomness are not the same. Not all uncertainties can be put on the same scale."
        },
        {
            "heading": "2.2.4 Probability Models in Science",
            "text": "How does probability enter a scientific problem? It could be that the underlying physical phenomenon is random, as radioactive decay and other quantum processes are, according to quantum mechanics. Or it could be that the scientist deliberately introduces randomness, e.g., by conducting a randomized experiment or by drawing a random sample.\nProbability can enter as a subjective prior probability. Suppose we want to estimate the probability p that a particular coin lands heads (on the assumption that coin tosses are random). Surely p is between zero and one. A common way to capture that constraint involves positing a prior probability distribution for p, for instance, assuming that p was selected at random from the interval [0, 1], according to a uniform probability distribution. Unfortunately, positing any particular probability distribution for p adds an infinite amount of information about p, information not contained in the constraint that p is in the interval [0,1]. There are infinitely many probability distributions on the interval [0, 1], all of which satisfy the constraint: the constraint is not equivalent to a prior probability distribution, a function selected from an infinite-dimensional set of possibilities (see, e.g., Stark, 2015; Stark & Tenorio, 2010).\nThere are theorems that say that under some conditions, the prior does not matter asymptotically: the data eventually \u2018swamp\u2019 the prior as more and more observations are made. Those conditions need not hold in practice. In particular, the conclusion is not true in general for infinite-dimensional unknowns or for improper priors (see, e.g., Freedman, 1999). Nor is it necessarily true if the dimensionality of the problem grows as the number of data grows (Diaconis & Freedman, 1986). Nor is it true that nominally \u2018uninformative\u2019 (i.e., uniform) priors are actually uninformative, especially in high-dimensional spaces (Backus, 1987; Stark, 2015). For instance, consider a probability distribution that is\nuniform on the d-dimensional unit ball in Euclidean space. As d grows, more and more of the mass is in an infinitesimal shell between radius 1 - e and 1. Beyond the technical difficulties, there are practical issues in eliciting prior distributions, even in one-dimensional problems.23 In fact, priors are rarely elicited. More often, priors are chosen for mathematical convenience or from habit. There are arguments in favor of the Bayesian approach from Dutch book: if you are forced to bet on every possible event and you do not bet according to a Bayesian prior, there are collections of bets where you are guaranteed to lose money, no matter what happens. According to the argument, you are therefore not rational if you don\u2019t bet in a Bayesian way. But of course, you do not have to place bets on every possible outcome (Freedman, 2010a). See Sect. 2.2, above.\nA fourth way probability can enter a scientific problem is through the invention of a probability model that is supposed to describe a phenomenon, e.g., a regression model, a Gaussian process model, or a stochastic PDE. But in what sense, to what level of accuracy, and for what purpose?24 Describing data tersely and approximately (in effect, fitting a model as a means of data compression), predicting what a system will do next, and predicting what a system will do in response to a particular intervention are very different goals. The last involves causal inference, which is far more difficult than the first two (Freedman, 2010c).\nFitting a model that has a parameter called \u2018probability\u2019 to data does not mean that the estimated value of that parameter estimates the probability of anything in the real world. Just as the map is not the territory, the model is not the phenomenon, and calling something \u2018probability\u2019 does not make it a probability, any more than drawing a mountain on a map creates a real mountain.\nFinally, probability can enter a scientific problem as metaphor: a claim that the phenomenon in question behaves \u2018as if\u2019 it is random. What \u2018as if\u2019 means is rarely made precise, but this approach is common, for instance, in stochastic models for seismicity (see below).25\nCreating randomness by taking a random sample or assigning subjects at random to experimental conditions is quite different from inventing a probability model or proposing a metaphor. The first may allow inferences if the analysis properly accounts for the randomization, if there are adequate controls, and if the study population adequately matches the population for which inferences are sought. But when the probability exists only within an invented model or as a metaphor, the inferences have little foundation. The inferences are no better than the assumptions. The assumptions and the sensitivity of the conclusions to violations of the assumptions have to be checked in each application and for each set of data: one cannot \u2018\u2018borrow strength\u2019\u2019 from the fact that a model worked in one context to conclude that it will work in another context.\nIn summary, the word \u2018probability\u2019 is often used with little thought about why, if at all, the term applies, and many common uses of the word are rather removed from anything in the real world that can be reasonably described or modeled as random."
        },
        {
            "heading": "2.2.5 Simulation and Probability",
            "text": "In some fields\u2014physics, geophysics, climate science, sensitivity analysis, and uncertainty quantification in particular\u2014there is a popular impression that probabilities can be estimated in a \u2018neutral\u2019 or \u2018automatic\u2019 way by doing Monte Carlo simulations: just let the computer reveal the distribution.\nFor instance, an investigator might posit a numerical model for some phenomenon. The values of some parameters in the model are unknown. In one approach to uncertainty quantification, values of\n23 See, e.g., O\u2019Hagan (1998). Typically, some functional form is posited for the distribution, and only some parameters of that distribution, such as the mean and variance or a few percentiles, are elicited. 24 These problems may be worse in numerical modeling than in statistical modeling, yet they have received less systematic attention (Saltelli, 2019).\n25 See, e.g., Stein and Stein (2013), who claim that the occurrence of earthquakes and other natural hazards is like drawing balls from an urn, and make distinctions according to whether and how the number of balls of each type changes between draws. But why is the occurrence of natural hazards like drawing balls from an urn? This has no basis in physics.\nthose parameters are drawn pseudo-randomly from an assumed joint distribution (generally treating the parameters as independent). The distribution of outputs is interpreted as the probability of various outcomes in the real world.\nSetting aside other issues in numerical modeling, Monte Carlo simulation is a way to substitute computing for hand calculation. It is not a way to discover the probability distribution of anything; it is a way to estimate the numerical values that result from an assumed distribution. It is a substitute for doing an integral, not a way to uncover laws of Nature.\nMonte Carlo doesn\u2019t tell you anything that wasn\u2019t already baked into the simulation. The distribution of the output comes from assumptions in the input (modulo bugs): a probability model for the parameters in the simulation. It comes from what you program the computer to do. Monte Carlo reveals the consequences of your assumptions about randomness. The rabbit goes into the hat when you build the probability model and write the software. The rabbit does not come out of the hat without having gone into the hat first.\nSimilarly, Gaussian process (GP) models (Kennedy & O\u2019Hagan, 2001) are common in uncertainty quantification. I have yet to see a situation outside physics where there was reason to think that the unknown is a realization of a Gaussian process. The computed uncertainties based on GP models do not mean much if the phenomenon is not a realization of a GP.26\nUncertainty quantification and sensitivity analysis should acknowledge that the map is not the territory, that a histogram of model output is not generally an estimate of a probability distribution in the real world, and that the sensitivity of model output to a parameter in the model does not translate automatically into sensitivity of the modelled system to that parameter."
        },
        {
            "heading": "2.2.6 Cargo-Cult Confidence27",
            "text": "Suppose you have a collection of numbers, for example, a multi-model ensemble of climate predictions for global warming,28 or a list of species extinction rates for some cluster of previous studies. Find the mean (m) and the standard deviation (sd) of this list of numbers. Report the mean as an estimate of something. Calculate the interval [m - 1.96sd, m ? 1.96sd]. Claim that this is a 95% confidence interval or that there is a 95% chance that this interval contains \u2018the truth.\u2019\nThat is not a confidence interval for anything\u2014 and the probability statement is a further mangling of the interpretation of a confidence interval. If the collection of numbers were a random sample from some population and if that population had a Gaussian distribution (or if the sample size were large enough that you could invoke the central limit theorem), then the interval would be an approximate confidence interval for something. (The probability statement would still be meaningless.)\nBut if the list is not a random sample or a collection of measurements with random errors, there is nothing stochastic in the data, and hence there can be no genuine confidence interval. Applying confidence interval calculations to a sample that is not random is quantifauxcation. (See, e.g., van der Sluijs, 2016.)\nA 95% confidence interval for a parameter is a number calculated from a random sample using a method that\u2014before the sample has been drawn\u2014 has at least a 95% probability producing an interval that includes the true value of the parameter. Once the sample has been drawn, everything is determined: even if the sample was random, the computed interval either does or does not include the true value of the parameter.\nOften, including in calculations I have seen in IPCC reports, people treat uncertainty in an estimate\n26 However, see Neiswanger and Ramdas (2021) for a method of obtaining frequentist confidence sets from GP models when the prior is incorrect but the data are genuinely random.\n27 See Stark and Saltelli (2018). The title of this section alludes to Richard Feynman\u2019s discussion of cargo-cult science (Feynman, 1974). Cargo-cult confidence intervals involve calculations that look like confidence interval calculations, but they are missing a crucial element: the data are not a random sample. They involve implausible distributional assumptions as well. 28 This is essentially what IPCC does.\nas if the true parameter value is random and follows a probability distribution centered at the estimate. This is something like Fisher\u2019s fiducial inference (Seidenfeld, 1992), which was virtually abandoned by statisticians many years ago. The treatment is backwards: if the estimate is unbiased, the estimate is random with a probability distribution centered at the true parameter value, not vice versa. Moreover, the truth is a fixed quantity, and once the estimate has been made, the estimate is also a fixed quantity."
        },
        {
            "heading": "3. Example: Probabilistic Seismic Hazard",
            "text": "Assessment (PSHA)\nProbabilistic seismic hazard analysis (PSHA) is the basis of seismic building codes in many countries. It is also used to help decide where to build nuclear power plants and nuclear waste disposal sites. PSHA claims to estimate the probability of a given level of ground shaking (acceleration) in a given time period, for instance, the chance of acceleration high enough to damage the containment structure in the next 50 years. It involves modelling earthquakes as occurring at random in space and time, with random magnitude. Then it models ground motion as being random, conditional on the occurrence of an earthquake of a given magnitude in a given place.\nFrom this, PSHA claims to estimate \u2018exceedance probability,\u2019 the chance that the acceleration in some particular place exceeds some tolerable level in some number of years. In the U.S., building codes generally require structures to withstand accelerations that will occur with probability of 2% or greater in 50 years.\nPSHA arose from probabilistic risk assessment, which originated in aerospace and nuclear power, primarily. A big difference between PSHA and these other applications is that even before humans had launched a manned spaceflight, a spacecraft is an engineered system. Its properties are relatively well known (or predictable), as are those of the environment it is operating in, and so on. Even before a nuclear reactor was built, people knew something about nuclear physics and thermodynamics. They knew something about the physical properties of concrete and steel. They knew something about pressure vessels.\nWe know very little about earthquakes, other than their phenomenology. We don\u2019t really understand the physical generating processes (Geller et al., 2015). We don\u2019t know in detail how they occur. There is a big difference between an engineered system whose components can be tested and a natural system that is inaccessible to experimentation.\nPSHA models earthquakes as a marked stochastic process with known parameters. The fundamental relationship in PSHA is that the probability of a given level of ground movement in a given place is the integral over space and magnitude of the conditional probability of that level of movement, given that there is an event of a particular magnitude in a particular place, times the probability that there is an event of a particular magnitude.\nThis is just the law of total probability and the multiplication rule for conditional probabilities, but where is it coming from? That earthquakes occur at random is an assumption, not a matter of physics. Seismicity is complicated and unpredictable: haphazard, but not necessarily random. The standard argument to calibrate the PSHA fundamental relationship requires conflating rates with probabilities. For instance, suppose a magnitude eight event has been observed to occur about once a century in a given region. PSHA would assume that, therefore, the chance of a magnitude 8 event is 1% per year.\nThat is wrong, for a variety of reasons. First, there is an epistemic leap from a rate to the existence of an underlying, stationary random process that generated the rate, as discussed above (see the quotation from Klemes\u030c in particular). Second, it assumes that seismicity is uniform, which contradicts the observed clustering of seismicity in space and time. Third, it ignores the fact that the data at best give an estimate of a probability (if there is such a probability), not the exact value.\nAmong other infelicities, PSHA conflates frequencies with probabilities in treating relationships such as the Gutenberg-Richter (G-R) law, the historical spatial distribution of seismicity, and ground acceleration given the distance and magnitude of an earthquake as probability distributions. For instance, the G-R law says that the log of the number of earthquakes of a given magnitude in a given region is approximately proportional to the magnitude.\nMagnitude is a logarithmic scale, so the G-R law says that the relationship between \u2018\u2018size\u2019\u2019 and frequency is approximately linear on a log\u2013log plot (at least over some range of magnitude). While the G-R law is empirically approximately true, PSHA involves the additional assumptions that the magnitudes of future earthquakes are drawn randomly from the G-R law.\nPSHA relies the metaphor that earthquakes occur as if in a casino game. According to the metaphor, it\u2019s as if there is a special deck of cards, the earthquake deck. The game involves dealing one card per time period. If the card is blank, there is no earthquake. If the card is an eight, there is a magnitude eight earthquake. If the card is a six, there is a magnitude 6 earthquake, and so forth.\nThere are tens of thousands of journal pages that, in effect, argue about how many cards of each kind are in the deck, how well the deck is shuffled, whether after each draw you replace the card in the deck and shuffle again before dealing the next card, whether you add high-numbered cards to the deck if no high card has been drawn in a while, and so on. The literature, and the amount of money spent on this kind of work, are enormous\u2014especially given that it has been unsuccessful scientifically. Three recent destructive earthquakes were in regions that seismic hazard maps said were relatively safe (Stein et al., 2012; see also Panza et al., 2014; Kossobokov et al., 2015). This should not be surprising, because PSHA is based on a metaphor, not on physics.\nHere is a different metaphor: earthquakes occur like terrorist bombings. We don\u2019t know when or where they\u2019re going to happen. We know that they could be be large enough to hurt people when they do happen, but not how large. We know that some places are easier targets than others (e.g., places near active faults), and that some are more vulnerable than others (e.g., places subject to soil liquefaction and structures made of unreinforced masonry). But there is no probability per se. We might choose to invent a probability model to try to improve law enforcement or prevention, but that is different from a generative model according to which terrorists decide when and where to strike by rolling dice. In principle, the predictions of such a model could be tested\u2014but fortunately, such events are sufficiently rare that no meaningful test is possible.\nWhat would justify using the casino metaphor for earthquakes? It might be apt if the physics of earthquakes were stochastic\u2014but it isn\u2019t. It might be apt if stochastic models provided a compact, accurate representation of earthquake phenomenology, but they don\u2019t: the data show that the models are no good (see, e.g., Luen & Stark, 2012; Luen, 2010). The metaphor might be apt if the models led to useful predictions of future seismicity, but they don\u2019t (Luen, 2010).\nA rule of the form, \u2018if there is an earthquake of a magnitude X or greater, predict that there is going to be another one within Y kilometers within Z days\u2019 predicts earthquakes quite well, without relying on stochastic mumbo jumbo pulled from a hat (Luen, 2010; Luen & Stark, 2008).\nPSHA suffers from two of the issues we have been discussing, viz. forcing all uncertainties to be on the same scale and conflating rates with probabilities. Cornell (1968), the foundational PSHA paper, writes:\nIn this paper a method is developed to produce [various characteristics of ground motion] and their average return period for his site. The minimum data needed are only the seismologist\u2019s best estimates of the average activity levels of the various potential sources of earthquakes \u2026 The technique to be developed provides the method for integrating the individual influences of potential earthquake sources, near and far, more active or less, into the probability distribution of maximum annual intensity (or peak-ground acceleration, etc.). The average return period follows directly. \u2026 In general the size and location of a future earthquake are uncertain. They shall be treated therefore as random variables.\nWhatever intuitive appeal and formulaic simplicity PSHA might have, what justifies treating everything that is uncertain as if it were random, with distributions that are known but for the values of a few parameters? Nothing. Moreover, the method does not work in practice (Mulargia et al., 2017). Note how many knobs and levers Cornell points out. Their settings matter.\nI think we are better at ranking risks than quantifying them, and we are better at doing engineering calculations on human-built structures than we are at\ntectonic and seismological calculations. Thus, we might be better off asking questions by starting with a financial budget rather than a risk budget. Instead of asking, \u2018\u2018how can we make this structure have a 90 percent chance of lasting 100 years?,\u2019\u2019 we might be better off asking, \u2018\u2018if we were willing to spend $10 million to harden this structure, how should we spend it?\u2019\u2019.\nIn summary, assigning numbers to things cannot always be done in a coherent or grounded way. To simply assume that the numbers must be meaningful because they are numbers is to commit quantifauxcation. In some situations, resisting the urge to assign numbers might be the wiser, safer, and more honest course."
        },
        {
            "heading": "4. Example: Avian-Turbine Interactions",
            "text": "Wind turbine generators occasionally kill birds, in particular, raptors (Watson et al., 2018). This leads to a variety of questions. How many birds, and of what species? What design and siting features of the turbines matter? Can you design turbines or wind farms in such a way as to reduce avian mortality? What design changes would help?\nI was peripherally involved in this issue for the Altamont Pass wind farm in the San Francisco Bay Area. Raptors are rare; raptor collisions with wind turbines are rarer. To measure avian mortality from turbines, people look for pieces of birds underneath the turbines. The data aren\u2019t perfect. There is a background mortality rate unrelated to wind turbines. Generally, you don\u2019t find whole birds: you find bird fragments. Is this two pieces of one bird or pieces of two birds? Carcasses decompose. Scavengers consume pieces. There are problems of attribution: birds may be flung a distance from the turbine they hit, or they can be wounded and land some distance away. How do you figure out which turbine is the culprit? Is it possible to make an unbiased estimate of the raptor mortality from the turbines? Is it possible to relate that mortality to individual turbines and wind farms, in a way that is reliable?\nA standard stochastic model for the data is \u2018zeroinflated Poisson regression,\u2019 which is a mixture of a point mass at zero and a Poisson distribution on the\nnonnegative integers. The extra mass at zero is intended to account for censoring: the data collection is likely to miss some of the deaths. The Poisson rate is related parametrically to selected covariates. There are many other models one might use instead. For instance, one might model the observations as the true count with errors that are dependent, not necessarily identically distributed, and not necessarily zero mean.\nThe management of the Altamont Pass wind farm hired a consultant, who modeled bird collisions with turbines as random, using a zero-inflated Poisson distribution with parameters that depend on properties of each turbine. The probability distribution of collisions is the same for all birds. Collisions are independent across birds, and the expected collision rates follow a hierarchical Bayesian model that relates the rate parametrically to properties of the location and design of the turbine. The consultant introduced additional smoothing to make the parameters identifiable. In the end, he estimated the coefficients of the variables that the Poisson rates depend on\u2014according to the model.\nWhat does the model say about how collisions occur? According to the model, when a bird approaches a turbine, in effect it tosses a biased coin. If the coin lands heads, the bird throws itself on the blades of the turbine. If the coin lands tails, the bird avoids the turbine. The chance the coin lands heads depends some aspects of the location and design of the turbine, according to a pre-specified formula that involves some unknown parameters. Those parameters are assumed to be realizations of random variables with a known joint probability distribution. For each turbine location and design, every bird uses a coin with the same chance of heads, and the birds all toss the coin independently.\nWhoever chose this model ignored the fact that some birds\u2014including raptors\u2014fly in groups: collisions are dependent. Why are avian-turbine interactions random? Why should they follow a Poisson distribution? Why do all birds use the same coin for the same turbine, regardless of their species, the weather, and other factors? Why doesn\u2019t the chance of detecting a bird on the ground depend on how big the bird is, how tall the grass is, or how long it\u2019s been since the last survey? I don\u2019t know.\nPerhaps the most troubling thing is that the formulation changes the subject from \u2018how many birds does this turbine kill?\u2019 to \u2018what are the numerical values of some coefficients in this zero-inflated Poisson regression model I made up?\u2019\u2019 This is no longer about birds and wind turbines. This is about a cartoon model\u2014the model behind the curtain. The analysis is a red herring, changing the subject from bird deaths to a coefficient in the cockamamie model. It is also an example of a Type III error: testing a statistical model with little connection to the scientific question. Rayner (2012, p. 120) refers to this as displacement:\nDisplacement is the term that I use to describe the process by which an object or activity, such as a computer model, designed to inform management of a real-world phenomenon actually becomes the object of management. Displacement is more subtle than diversion in that it does not merely distract attention away from an area that might otherwise generate uncomfortable knowledge by pointing in another direction, which is the mechanism of distraction, but substitutes a more manageable surrogate. The inspiration for recognizing displacement can be traced to A. N. Whitehead\u2019s fallacy of misplaced concreteness, \u2018the accidental error of mistaking the abstract for the concrete.\u2019"
        },
        {
            "heading": "5. Example: Clinical trials",
            "text": "Here is another example of a Type III error. Consider the statistical analysis of clinical trials. In the simplest setting, N enrolled subjects meeting the trial criteria are assigned at random to treatment or control, for instance by selecting n at random to receive the active treatment, and the remaining N\u2013n to receive a placebo. Then outcomes are observed. The scientific null hypothesis is that treatment does not affect the outcome, either subject-by-subject (the strong null) or on average (the weak null). A common way to test whether the treatment has an effect is the 2-sample Student t-test. The statistical null hypothesis for that test is that the responses of the control group and of the treatment group are realizations of\nindependent, identically distributed Gaussian random variables; equivalently, the two groups of responses are independent random samples from the same \u2018\u2018superpopulation\u2019\u2019 of normally distributed values.\nIn the scientific experiment, the treatment and control groups are dependent because they arise from randomly partitioning a single group, so if a subject is in the treatment group, it is not in the control group, and vice versa. In the statistical null, the groups are independent. In the experiment, the only source of randomness is the random allocation of the N subjects to treatment or control. In the statistical model, subjects\u2019 responses are random, as if the subjects were a random sample from a superpopulation. In the scientific experiment, there is no assumption about the distribution of responses for treatment or control. In the statistical model, responses (in the superpopulation) are Gaussian. What does testing the statistical null hypothesis tell us about the scientific null hypothesis?\nFisher (1935, pp. 50ff) himself notes:\n\u2018\u2018Student\u2019s\u2019\u2019 t test, in conformity with the classical theory of errors, is appropriate to the null hypothesis that the two groups of measurements are samples drawn from the same normally distributed population. \u2026 [I]t seems to have escaped recognition that the physical act of randomization, which, as has been shown, is necessary for the validity of any test of significance, affords the means, in respect of any particular body of data, of examining the wider hypothesis in which no normality of distribution is implied.\nHe goes on to describe how randomization by itself justifies permutation tests, which make no distributional assumptions about the data values per se, instead relying solely on the design of the experiment. While P-values computed for Student\u2019s t test in this situation may be close to the P-values for a permutation test (under mild conditions, the former converges asymptotically to the latter), the hypotheses are conceptually very different. One is about the actual experiment and the randomization that was performed. The other is about imaginary superpopulations, bell curves, and independence. Using Student\u2019s t test in this situation is to commit a Type III error. It also is a red herring argument, changing\nthe subject from whether treatment has an effect to whether the data are plausibly IID Gaussian."
        },
        {
            "heading": "6. Example: Interruptions of Academic Job Talks",
            "text": "Women are disadvantaged in many ways in academia, including grant applications (Kaatz et al., 2014; Witteman et al., 2019), letters of reference (Madera et al., 2009; Schmader et al., 2007), job applications (Moss-Racusin et al., 2012; Reuben et al., 2014), and credit for joint work (Sarsons, 2015). Do academic audiences interrupt female speakers more often than they interrupt male speakers?29 Blair-Loy et al. (2017) addressed this question by annotating 119 job talks from two engineering schools to note the number, nature, and duration of questions of various types, then fitting a zero-inflated negative binomial regression model with coefficients for gender, speaker\u2019s years since PhD, the proportion of faculty in the department who are female, and a dummy variable for university, and a dummy variable for department (CS, EE, or ME). The statistical hypothesis might be that the coefficient of gender in the \u2018\u2018positive\u2019\u2019 model is zero (the negative binomial portion).\nBlair-Loy et al. (2017) write:\nThe standard choices for modeling count data are a Poisson model, negative binomial model, or a zero-inflated version of either of these models [55]. We prefer a zero-inflated, negative binomial (ZINB) model for this analysis \u2026 We now estimate ZINB models to address our first research question: do women get more questions than men during the job talk?\nAccording to the ZINB model, questions occur as follows: in each talk, a biased coin is tossed. If it lands heads, there are no questions. If it lands tails, a (possibly) different biased coin is tossed repeatedly, independently, until that coin lands heads for the kth time. The number of questions asked is the number of tosses it takes to get the kth head. The probabilities\nthat each coin lands heads and the value of k are related parametrically to the covariates listed above.\nPerhaps I lack imagination, but I can\u2019t see how that is connected to the number of questions in an academic job talk in the real world. It does produce a nonnegative integer, and the number of questions in a talk is a nonnegative integer. And it allows an arbitrarily large chance that the number of questions is zero, so it is more flexible than negative binomial regression without the extra mass at zero.\nBlair-Loy et al. then test the hypothesis that the coefficient of gender in the \u2018\u2018positive\u2019\u2019 part of the ZINB model is zero. That is, they ask whether it would be surprising for the estimated coefficient of gender to be as large as it was if interruptions of job talks followed the ZINB model but the true coefficient of gender in that model were zero. Setting aside the fact that the positive part alone does not reveal whether women or men get more questions on average,30 what does that statistical hypothesis have to do with the original scientific question? The analysis changes the question from \u2018\u2018are women interrupted more than men?\u2019\u2019 to \u2018\u2018is this coefficient in a statistical model with no discernable scientific connection to the world zero?\u2019\u2019 This is an example of a Type III error, a red-herring statistical argument."
        },
        {
            "heading": "7. Example: Many Analysts, One Data Set",
            "text": "Silberzahn et al. (2018) involved 29 teams comprising 61 \u2018\u2018analysts\u2019\u2019 attempting to answer the same question from the same data: are soccer referees more likely to give penalties (\u2018\u2018red cards\u2019\u2019) to dark-skintoned players than to light-skin-toned players. The 29 teams used a wide variety of models and came to different conclusions: 20 found a \u2018\u2018statistically significant positive effect\u2019\u2019 and the other 9 did not.\nThis study was a great example of reproducible research, in that the data, models, and algorithms were made available to the public. Unfortunately, it was also a great example (29 great examples) of how\n29 A separate question is, \u2018\u2018and if so, does that make them less\nlikely to be offered a job?\u2019\u2019.\n30 For instance, on average women might get more questions than men when they get questions at all (the \u2018\u2018positive part\u2019\u2019 of the model), but they might not get any questions at all more often than men (the \u2018\u2018zero model\u2019\u2019).\nnot to model data and how to misinterpret statistical tests.\nThe teams used models and tests including:\n\u2022 Least-squares regression, with or without robust standard errors or clustered standard errors, with or\nwithout weights\n\u2022 Multiple linear regression \u2022 Generalized linear models \u2022 General linear mixed-effects models, with or\nwithout a logit link\n\u2022 Negative binomial regression, with or without a logit link \u2022 Multilevel regression \u2022 Hierarchical log-linear models \u2022 Linear probability models \u2022 Logistic regression \u2022 Bayesian logistic regression \u2022 Mixed-model logistic regression \u2022 Multilevel logistic regression \u2022 Multilevel Bayesian logistic regression \u2022 Multilevel logistic binomial regression \u2022 Clustered robust binomial logistic regression \u2022 Dirichlet-process Bayesian clustering \u2022 Poisson regression \u2022 Hierarchical Poisson regression \u2022 Zero-inflated Poisson regression \u2022 Poisson multilevel modelling \u2022 Cross-classified multilevel negative binomial\nregression\n\u2022 Hierarchical generalized linear modeling with Poisson sampling \u2022 Tobit regression \u2022 Spearman correlation\nThe teams chose 21 distinct subsets of the 14 available covariates. The project included a phase of \u2018\u2018round-robin\u2019\u2019 peer feedback on each team\u2019s model, after which the analytic approaches and models were generally revised. Then there was a second period of discussion and revision of the analyses. These \u2018\u2018model-selection\u2019\u2019 steps alone would make the nominal significance levels of the tests wrong, even if all other aspects of the analyses were sound.31\nWith perhaps one exception, all the approaches involve positing parametric probability models for the issuance of red cards. Why is giving a red card random? Why does the issuance of a red card depend on just the predictors in the model? Why does it depend on those predictors in the way the model assumes?\nThe scientific hypothesis that referees give darker skin tone players more red cards has been turned into the statistical null hypothesis that red cards are issued according to a parametric probability model, and the coefficient of skin tone in that model is zero. The model changes the subject from what happens in the world to whether the value of a coefficient in an absurd cartoon model is zero. In particular, the Pvalue for the statistical hypothesis answers the question, if red cards were issued according to the model with those covariates, and the \u2018\u2018true\u2019\u2019 coefficient of skin tone is 0, what is the chance that the coefficient of skin tone would be estimated to be at least as large as it was estimated to be?\nIf that is not how red cards are issued, the regression\u2014and estimates of and hypothesis tests about coefficients in that regression\u2014say little if anything about the world. This analysis is a Type III error.\nIt is no wonder that the teams came to differing conclusions, since everything is simply invented from whole cloth, with no scientific basis for any of the models.\n31 To illustrate how tortuous the relationship between these models and the data is, consider one of the simpler models, logistic regression, which was used by two of the teams. Both teams used 6\nFootnote 31 continued covariates in their model, 4 of which were in both models. One of those teams used player\u2019s position (13 possible values, presumably represented as 12 dummy variables), height, weight, goals scored, victories, and the referee\u2019s country as the covariates, in addition to skin tone (which had 5 levels: 0, .25, .5, .75, and 1): 18 predictors of red cards.\nAccording to the logistic regression model, there is a single\nvector of coefficients \u00df common to all (player, referee) pairs. Each (player, referee) pair i has its own logit-distributed random variable Ui; those logit variables are independent across pairs. Let Xi denote the 18-vector of predictors for pair i. There is a single vector of coefficients \u00df common to all pairs. The referee gives the player a red card iff Xi \u00df ? Ui[ 0. See Freedman (2009, ch. 7). Why is the issuance of a red cards independent across (player, referee) pairs, even though there are many players and typically several referees in each game? Why does the issuance of a red card depend on just those 18 predictors? Why does the threshold value of Ui for issuing a red card depend linearly on those predictors, and with the same coefficients for all players?"
        },
        {
            "heading": "8. Example: Climate Models",
            "text": "The IPCC treats all uncertainties as random:\n\u2026quantified measures of uncertainty in a finding expressed probabilistically (based on statistical analysis of observations or model results or expert judgment). \u2026 Depending on the nature of the evidence evaluated, teams have the option to quantify the uncertainty in the finding probabilistically. In most cases, level of confidence\u2026 \u2026 Because risk is a function of probability and consequence, information on the tails of the distribution of outcomes can be especially important\u2026 Author teams are therefore encouraged to provide information on the tails of distributions of key variables\u202632\nIn these few sentences, the IPCC is asking or requiring people to do things that don\u2019t make sense and that don\u2019t work. As discussed above, subjective probability assessments (even by experts) are generally untethered to reality, and subjective confidence is unrelated to accuracy. Mixing measurement errors with subjective probabilities doesn\u2019t work. And climate variables have unknown values, not probability distributions.\nCargo-cult confidence confusion seems to be common in IPCC reports. For instance, the \u2018multimodel ensemble approach\u2019 the IPCC uses involves taking a group of models, computing the mean and the standard deviation of their predictions, then treating the mean as if it were the expected value of the outcome (which it isn\u2019t) and the standard deviation as if it were the standard error of the natural process that is generating climate (which it isn\u2019t).33\nThe resulting numbers say little if anything about climate."
        },
        {
            "heading": "9. Example: The Rhodium Group American Climate Prospectus",
            "text": "The Bloomberg Philanthropies, the Office of Hank Paulson, the Rockefeller Family Fund, the Skoll Global Threats Fund, and the TomKat Charitable Trust funded a study34 that purports to predict various impacts of climate change.\nThe report starts somewhat circumspect:\nWhile our understanding of climate change has improved dramatically in recent years, predicting the severity and timing of future impacts is a challenge. Uncertainty around the level of greenhouse gas emissions going forward and the sensitivity of the climate system to those emissions makes it difficult to know exactly how much warming will occur and when. Tipping points, beyond which abrupt and irreversible changes to the climate occur, could exist. Due to the complexity of the Earth\u2019s climate system, we don\u2019t know exactly how changes in global average temperatures will manifest at a regional level. There is considerable uncertainty\u2026\nBut then the report makes rather bold claims:\nIn this climate prospectus, we aim to provide decision-makers in business and government with the facts about the economic risks and opportunities climate change poses in the United States.\nYep, the \u2018facts.\u2019 They proceed to estimate the effect that climate change will have on mortality, crop yields, energy use, the labor force, and crime, at the level of individual counties in the United States through the year 2099. They claim to be using an \u2018evidence-based approach.\u201935 32 Mastrandrea et al. (2010) at p.2. The authors of the paper have expertise in biology, climatology, physics, economics, ecology, and epidemiology. To my surprise (given how garbled the statistics is), one author is a statistician. 33 The IPCC also talks about simulation errors being \u2018independent,\u2019 which presupposes that such errors are random. https:// www.ipcc.ch/publications_and_data/ar4/wg1/en/ch10s10-5-4-1. html (last accessed 16 November 2016) But modeling errors are not random\u2014they are a textbook example of systematic error. And even if they were random and independent, averaging would tend to reduce the variance of the result, but not necessarily improve accuracy, since accuracy depends on bias as well. 34 Houser et al. (2015). See also http://riskybusiness.org/site/ assets/uploads/2015/09/RiskyBusiness_Report_WEB_09_08_14. pdf (last accessed 16 November 2016). 35 To my eye, their approach is \u2018evidence-based\u2019 in the same sense that alien abduction movies are \u2018based on a true story.\u2019.\nAmong other things, the prospectus predicts that violent crime will increase just about everywhere, with different increases in different counties. How do they know? In some places, on hot days there is on average more crime than on cool days.36 Fit a regression model to the increase.37 Assume that the fitted regression model is a response schedule, i.e., it is how Nature generates crime rates from temperature. Input the average temperature change predicted by the climate model in each county; out comes the average increase in crime rate.\nThink about this for a heartbeat. Even if you knew exactly what the temperature and humidity would be in every cubic centimeter of the atmosphere every millisecond of every day, you would have no idea how that would affect the crime rate in the U.S. next year, much less in 2099, much less at the level of individual counties. And that is before you factor in the uncertainty in climate models, which is enormous, even for globally averaged quantities, and far larger for variables at the level of individual counties (Regier & Stark, 2015). And it is also before you consider that society is not a constant: changes in technology, wealth, and culture over the next 100 years surely matter, as evidenced by the recent rise in mass shootings (Ogasa, 2022).\nGlobal circulation models (climate models) are theorists\u2019 tools, not policy tools: they might help us understand climate processes, but they are not a suitable basis for planning, economic decisions, and so on (see, e.g., Saltelli et al., 2015). The fact that intelligent, wealthy people spent a lot of money to conduct this study and that the study received highprofile coverage in The New York Times and other\nvisible periodicals speaks to the effectiveness of quantifauxcation as a rhetorical device."
        },
        {
            "heading": "10. Discussion",
            "text": "The reliance on models, especially for policy recommendations, has much in common with belief in conspiracy theories. Proponents of models and conspiracy theories generally treat agreement with a selected set of facts as affirmative evidence, even if there are countless competing explanations that fit the evidence just as well and evidence that calls their position into question. John von Neumann is reported (by Enrico Fermi) to have said, \u2018\u2018With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\u2019\u2019 Fit to past data says little about whether a model is \u2018correct\u2019 in any useful sense.\nProponents of models and conspiracy theories generally look for confirmation rather than for alternative explanations or critical experiments or observations that could falsify their position. Faced with inconvenient facts, modelers and conspiracy theorists tend to complicate their theories\u2014e.g., by adding parameters to a model or agents to a conspiracy theory\u2014rather than reconsider their approach. Proponents of models and conspiracy theories often borrow authority from other theories in a misuse of inductive reasoning.\nFor example, PredPol , software for predictive policing, claims their model is reliable because it is used in seismology. It is the ETAS model (Ogata, 1988), which is rather problematic within seismology (e.g., Baranov et al., 2019; Grimm et al., 2022; Luen, 2010). Similarly, conspiracy theorists point to other conspiracy theories that have either been among the few that are true, or among the many that cannot be disproved, as support for their theory.\nModels and conspiracy theories generally ride on unstated assumptions. For models, those assumptions often involve a version of the equivocation fallacy based on giving terms in the model the names of realworld phenomena. Labelling a term in the model \u2018\u2018the coefficient of X\u2019\u2019 does not mean that its magnitude measures the effect of X in the real world.\n36 Ranson (2014) claims, \u2018\u2018[b]etween 2010 and 2099, climate change will cause an additional 22,000 murders, 180,000 cases of rape, 1.2 million aggravated assaults, 2.3 million simple assaults, 260,000 robberies, 1.3 million burglaries, 2.2 million cases of larceny, and 580,000 cases of vehicle theft in the United States.\u2019\u2019. 37 I used Ranson (2014) as the basis of a course project in 2018. The University of California, Berkeley, is in Alameda County, which has a population of about 1.68 million and an area of 739 square miles. The students split Alameda county into two pieces, east Alameda and west Alameda. They repeated Ranson\u2019s analysis in those two pieces and compared the results. The weather in east and west differed by an amount that is material and statistically significant. The models for crime as a function of temperature differed by an amount that was statistically significant.\nAnd models often rely on authority or tribalism for support, rather than on empirical testing. For instance, there are entire disciplines built around a particular model, as econometrics is largely built on linear regression, seismic engineering is largely built on PSHA, and statistical earthquake prediction is largely built on ETAS.\nLike conspiracy theories, models purport to reveal the underlying causes of complicated phenomena. Both offer an authoritative explanation of what we see, even when there are countless conflicting explanations that fit the data equally well. Both are well suited to reinforcing pre-existing beliefs through confirmation bias. Neither generally gets much \u2018\u2018stress testing\u2019\u2019 (Mayo, 2018).\nConspiracy theories that contain a germ of truth are generally more successful: if some fact woven into the theory can be checked, that is seen as validating the theory, even if that fact is consistent with contradictory explanations. Similarly, models make some contact with data, lending the models credibility\u2014even though it is typically possible to fit many contradictory models to the same data. There are strong social pressures to believe in models, despite gaps in logic, alternative explanations, and generally weak evidence, just like conspiracy theories. And like conspiracy theories, models are often created to support political positions or policies, and to question the models is discouraged by proponents of the policies, as is evident in the American Climate Prospectus (Rhodium Group, 2014).\nvan Prooijen and Douglas (2018) write:\n[C]onspiracy theories are consequential as they have a real impact on people\u2019s health, relationships, and safety; they are universal in that belief in them is widespread across times, cultures, and social settings; they are emotional given that [] emotions and not rational deliberations cause conspiracy beliefs; and they are social as conspiracy beliefs are closely associated with psychological motivations underlying intergroup conflict.\nIf the word \u2018\u2018models\u2019\u2019 is substituted for \u2018\u2018conspiracy beliefs\u2019\u2019 and \u2018\u2018conspiracy theories,\u2019\u2019 the paragraph remains true. Models are used to justify policy decisions with enormous societal impact; a broad\nspectrum of disciplines rely on them; they are used reflexively without adequate attention to whether they are appropriate or even tied to reality; they are used to justify political positions and there are strong social pressures\u2014within science and more broadly in society\u2014to believe model outputs. Models that purport to measure the cost or societal impact of climate change are a notable example (Saltelli et al., 2015).\nComplexity is often a selling point of a model or conspiracy theory. For instance, if a computer model requires heroic supercomputing, that adds \u2018\u2018weight\u2019\u2019 to the results. The American Climate Prospectus (Rhodium Group, 2014) proudly states:\n[This work] is only possible thanks to the advent of scalable cloud computing. All told, producing this report required over 200,000 CPU-hours processing over 20 terabytes of data, a task that would have taken months, or even years, to complete not long ago.\nSimilarly, if a conspiracy theory involves more colluders and evil actors, it may be more appealing.\nTribalism is often a factor. People connected by religious and political affiliations tend to believe the same conspiracy theories. The same is true for models\u2014both within particular disciplines,38 and under the general rubric of \u2018\u2018science is real,\u2019\u2019 suggesting that because models are \u2018\u2018scientific,\u2019\u2019 they are credible and trustworthy.\nLike conspiracy theories, models are often promulgated by people who do not even understand the theory: the fact that it is \u2018\u2018science\u2019\u2019 is enough.\nAnother similarity is that, like conspiracy theories, models can be nearly impossible to disprove, because so many models are consistent with the observations. By definition, underdetermined problems admit many solutions that can differ substantially from each other, even though all fit the data equally well.\nGorvett (2020) summarizes some characteristics of successful conspiracy theories: convincing culprits, collective anxieties, tribalism, providing certainty amidst uncertainty, and exploiting\n38 Almost the entire field of econometrics is regression models, even though the assumptions of regression are rarely met in economic applications. See, e.g., Kennedy (2001).\nknowledge gaps. The parallels with successful\u2014but scientifically bogus\u2014models are striking.\nIn closing, I quote the five principles of Saltelli et al. (2020) to help ensure that models serve society:\n\u2022 Mind the assumptions: assess uncertainty and sensitivity. \u2022 Mind the hubris: complexity can be the enemy of relevance. \u2022 Mind the framing: match purpose and context. \u2022 Mind the consequences: quantification may\nbackfire.\n\u2022 Mind the unknowns: acknowledge ignorance."
        },
        {
            "heading": "Acknowledgments",
            "text": "I am grateful to Gil Anidjar, Robert Geller, Christian Hennig, Jim Rossi, Aaditya Ramdas, Andrea Saltelli, and Jacob Spertus for helpful suggestions.\nAuthor contributions Stark is solely responsible for the contents.\nFunding\nThe authors have not disclosed any funding.\nDeclarations\nConflict of interest The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nPublisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
        }
    ],
    "title": "Pay No Attention to the Model Behind the Curtain",
    "year": 2023
}