{
    "abstractText": "The primary objective of this paper is to investigate distributed ordinary differential equation (ODE) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce novel distributed ODEs, inspired by the averaging consensus method in the continuous-time domain. The convergence of the ODEs is assessed through control theory perspectives. 2) Building upon the aforementioned ODEs, we devise new distributed TD-learning algorithms. A standout feature of one of our proposed distributed ODEs is its incorporation of two independent dynamic systems, each with a distinct role. This characteristic sets the stage for a novel distributed TDlearning strategy, the convergence of which can potentially be established using Borkar-Meyn theorem.",
    "authors": [
        {
            "affiliations": [],
            "name": "Donghwan Lee"
        },
        {
            "affiliations": [],
            "name": "Han-Dong Lim"
        },
        {
            "affiliations": [],
            "name": "Don Wan Kim"
        }
    ],
    "id": "SP:d209a85d7cc7412e9e5dc8e1422126464eaacff5",
    "references": [
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 1998
        },
        {
            "authors": [
                "M.L. Puterman"
            ],
            "title": "Markov decision processes",
            "venue": "Handbooks in operations research and management science, vol. 2, pp. 331\u2013434, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Yang",
                "T. Ba\u015far"
            ],
            "title": "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
            "venue": "Handbook of Reinforcement Learning and Control, pp. 321\u2013384, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Lee",
                "N. He",
                "P. Kamalaruban",
                "V. Cevher"
            ],
            "title": "Optimization for reinforcement learning: From a single agent to cooperative agents",
            "venue": "IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 123\u2013135, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Jadbabaie",
                "J. Lin",
                "A. Morse"
            ],
            "title": "Coordination of groups of mobile autonomous agents using nearest neighbor rules",
            "venue": "IEEE Transactions on Automatic Control, vol. 48, no. 6, pp. 988\u20131001, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "A. Nedi\u0107",
                "A. Ozdaglar"
            ],
            "title": "Subgradient methods for saddle-point problems",
            "venue": "Journal of optimization theory and applications, vol. 142, no. 1, pp. 205\u2013228, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Nedic",
                "A. Ozdaglar",
                "P.A. Parrilo"
            ],
            "title": "Constrained consensus and optimization in multi-agent networks",
            "venue": "IEEE Transactions on Automatic Control, vol. 55, no. 4, pp. 922\u2013938, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "W. Shi",
                "Q. Ling",
                "G. Wu",
                "W. Yin"
            ],
            "title": "Extra: An exact first-order algorithm for decentralized consensus optimization",
            "venue": "SIAM Journal on Optimization, vol. 25, no. 2, pp. 944\u2013966, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Wang",
                "N. Elia"
            ],
            "title": "Control approach to distributed optimization",
            "venue": "48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2010, pp. 557\u2013561.",
            "year": 2010
        },
        {
            "authors": [
                "H.K. Khalil"
            ],
            "title": "Nonlinear systems third edition",
            "venue": "Patience Hall, vol. 115, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "V.S. Borkar",
                "S.P. Meyn"
            ],
            "title": "The ODE method for convergence of stochastic approximation and reinforcement learning",
            "venue": "SIAM Journal on Control and Optimization, vol. 38, no. 2, pp. 447\u2013469, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "S.V. Macua",
                "J. Chen",
                "S. Zazo",
                "A.H. Sayed"
            ],
            "title": "Distributed policy evaluation under multiple behavior strategies",
            "venue": "IEEE Transactions on Automatic Control, vol. 60, no. 5, pp. 1260\u20131274, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M.S. Stankovi\u0107",
                "S.S. Stankovi\u0107"
            ],
            "title": "Multi-agent temporal-difference learning with linear function approximation: weak convergence under time-varying network topologies",
            "venue": "American Control Conference (ACC), 2016, pp. 167\u2013172.",
            "year": 2016
        },
        {
            "authors": [
                "X. Sha",
                "J. Zhang",
                "K. Zhang",
                "K. You",
                "T. Basar"
            ],
            "title": "Asynchronous policy evaluation in distributed reinforcement learning over networks",
            "venue": "arXiv preprint arXiv:2003.00433, 2020.",
            "year": 2003
        },
        {
            "authors": [
                "T. Doan",
                "S. Maguluri",
                "J. Romberg"
            ],
            "title": "Finite-time analysis of distributed TD(0) with linear function approximation on multi-agent reinforcement learning",
            "venue": "International Conference on Machine Learning, 2019, pp. 1626\u20131635.",
            "year": 2019
        },
        {
            "authors": [
                "H.-T. Wai",
                "Z. Yang",
                "Z. Wang",
                "M. Hong"
            ],
            "title": "Multi-agent reinforcement learning via double averaging primal-dual optimization",
            "venue": "Advances in Neural Information Processing Systems, 2018, pp. 9649\u2013 9660.",
            "year": 2018
        },
        {
            "authors": [
                "L. Cassano",
                "K. Yuan",
                "A.H. Sayed"
            ],
            "title": "Multiagent fully decentralized value function learning with linear convergence rates",
            "venue": "IEEE Transactions on Automatic Control, vol. 66, no. 4, pp. 1497\u20131512, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Ding",
                "X. Wei",
                "Z. Yang",
                "Z. Wang",
                "M.R. Jovanovi\u0107"
            ],
            "title": "Fast multiagent temporal-difference learning via homotopy stochastic primaldual optimization",
            "venue": "arXiv preprint arXiv:1908.02805, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "P. Heredia",
                "S. Mou"
            ],
            "title": "Finite-sample analysis of multi-agent policy evaluation with kernelized gradient temporal difference",
            "venue": "2020 59th IEEE Conference on Decision and Control (CDC), 2020, pp. 5647\u2013 5652.",
            "year": 2020
        },
        {
            "authors": [
                "D. Lee",
                "J. Hu"
            ],
            "title": "Distributed off-policy temporal difference learning using primal-dual method",
            "venue": "IEEE Access, vol. 10, pp. 107 077\u2013 107 094, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Bhatnagar",
                "H. Prasad",
                "L. Prashanth"
            ],
            "title": "Stochastic recursive algorithms for optimization: simultaneous perturbation methods",
            "year": 2012
        },
        {
            "authors": [
                "H. Kushner",
                "G.G. Yin"
            ],
            "title": "Stochastic approximation and recursive algorithms and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2003
        },
        {
            "authors": [
                "R.S. Sutton",
                "H.R. Maei",
                "D. Precup",
                "S. Bhatnagar",
                "D. Silver",
                "C. Szepesv\u00e1ri",
                "E. Wiewiora"
            ],
            "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
            "venue": "Proceedings of the 26th annual international conference on machine learning, 2009, pp. 993\u20131000.",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n16 70\n6v 4\n[ ee\nss .S\nY ]\n1 7\nA ug\n2 02\n3\nI. INTRODUCTION\nA Markov decision problem (MDP) [1], [2] is a sequential\ndecision-making problem that aims to find an optimal policy\nin dynamic environments. Multi-agent MDPs (MAMDPs)\n[3], [4] extend the MDP framework to include multiple\nagents interacting with one another. These agents can either\ncooperate toward a shared goal or compete for individual ob-\njectives. In this study, we focus primarily on the cooperative\nscenario.\nIn MAMDPs, full information about the environment, such\nas the global state, action, and reward, is often unavailable\nto each agent. This lack of complete information can arise\nfor a variety of reasons, including sensor or infrastructure\nlimitations, privacy and security concerns, and computational\nconstraints, among others. As a result, various information\nstructures are adopted based on the specific application. One\nnotable instance is the centralized MAMDP, where every\nagent has access to complete information. In contrast, in\ndistributed MAMDPs, agents may only have access to local\ndata about the global state, action, and rewards. Sometimes,\nagents can share information with each other via commu-\nnication networks, a configuration termed as the networked\nMAMDP.\nD. Lee and H. Lim are with the Department of Electrical and Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea donghwan@kaist.ac.kr, limaries30@kaist.ac.kr.\nD. Kim is with the Department of Electrical Engineering, Hanbat National University, Daejeon 34158, South Korea dowankim@hanbat.ac.kr.\nIn this paper, we investigate distributed temporal differ-\nence (TD) learning [1] algorithms [13]\u2013[21] for a networked\nMAMDP. In this setting, agents can share their local param-\neters, \u03b8i, with their neighbors over a communication network described by a graph, G. The proposed algorithms are distributed in the sense that only local rewards, ri, are given to each agent. Meanwhile, the global reward is a sum or an average of the local rewards, i.e., r = r1+r2+\u00b7 \u00b7 \u00b7+rN , where N is the total number of agents. To address the MAMDP in a distributed manner, we employ averaging consensus\ntechniques [5]\u2013[8]. These techniques enable multiple agents\nto calculate a common solution through parameter mixing\n(or averaging) steps with their neighbors."
        },
        {
            "heading": "A. Contributions",
            "text": "The main contributions of this paper can be summarized\nas follows:\n1) We introduce novel distributed ODEs [9], inspired by\nthe averaging consensus method in the continuous-time\ndomain [10]. The convergence of the ODEs is assessed\nfrom control theory perspectives [11].\n2) Building upon the aforementioned ODEs, we devise\nnew distributed TD-learning algorithms. A standout fea-\nture of our proposed distributed ODE is its incorporation\nof two decoupled dynamic systems, each with a distinct\nrole: the first ODE focuses on estimating the local value\nfunctions, while the second ODE focuses on mixing\nthe parameters using the averaging consensus. This\nstrategic decoupling of roles within the ODEs provides\nenhanced benefits when interfaced with RL algorithms\nand potentially paves the way for the direct application\nof the Borkar-Meyn theorem [12] to its RL equivalent."
        },
        {
            "heading": "II. PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A. Notation and terminology",
            "text": "The following notation is adopted: I denotes the identity matrix with appropriate dimension; \u2016x\u2016D := \u221a xTDx for any positive-definite D; |S| denotes the cardinality of the set for any finite set S; 1n \u2208 Rn denotes an n-dimensional vector with all entries equal to one."
        },
        {
            "heading": "B. Graph theory",
            "text": "An undirected graph with the node set V and the edge set E \u2286 V\u00d7V is denoted by G = (E ,V). We define the neighbor set of node i as Ni := {j \u2208 V : (i, j) \u2208 E}. The adjacency matrix of G is defined as a matrix W with [W ]ij = 1, if and only if (i, j) \u2208 E . If G is undirected, then W = WT . A graph is connected, if there is a path between any pair of vertices.\nThe graph Laplacian is L = H \u2212W , where H is a diagonal matrix with [H ]ii = |Ni|. If the graph is undirected, then L is symmetric positive semi-definite. It holds that L1|V| = 0. If G is connected, 0 is a simple eigenvalue of L, i.e., 1|V| is the unique eigenvector corresponding to 0, and the span of 1|V| is the null space of L."
        },
        {
            "heading": "III. MULTI-AGENT MARKOV DECISION PROCESS",
            "text": "In this section, we introduce the notion of the mutli-\nagent Markov decision process (MAMDP). Consider N agents labeled by i \u2208 {1, . . . , N} =: V . An MAMDP is characterized by (S, {Ai}i\u2208V , P, {ri}i\u2208V , \u03b3), where \u03b3 \u2208 (0, 1) is the discount factor, S is a finite state space, Ai is a finite action space of agent i, a := (a1, . . . , aN ) is the joint action, A := \u220fNi=1 Ai is the corresponding joint action space, ri : S \u00d7 A \u00d7 S \u2192 R is a reward function of agent i, and P (s, a, s\u2032) represents the transition model of the state s with the joint action a and the corresponding joint action space A. The stochastic policy of agent i is a mapping \u03c0i : S \u00d7Ai \u2192 [0, 1] representing the probability \u03c0i(ai|s) of selecting action ai at the state s, and the corresponding joint policy is \u03c0(a|s) :=\u220fNi=1 \u03c0i(ai|s). P \u03c0 denotes the transition matrix. d : S \u2192 R denotes the stationary state distribution under the joint policy \u03c0. In particular, if the joint action a is selected with the current state s, then the state transits to s\u2032 with probability P (s, a, s\u2032), and each agent i observes a reward ri(s, a, s\n\u2032). We also define R\u03c0i (s) as the expected reward of agent i given the policy \u03c0 and the current state s. Given pre-selected basis (or feature) functions \u03c61, . . . , \u03c6q : S \u2192 R, \u03a6 \u2208 R|S|\u00d7q is defined as a full column rank matrix whose s-th row vector is \u03c6(s) := [ \u03c61(s) \u00b7 \u00b7 \u00b7 \u03c6q(s) ] .\nThe infinite-horizon discounted value function with policy\n\u03c0 is\nV \u03c0(s) := E\n[\n\u221e \u2211\nk=0\n\u03b3kr(sk, ak, sk+1)\n\u2223 \u2223 \u2223 \u2223 \u2223 s0 = s ] ,\nwhere E stands for the expectation taken with respect to the state-action trajectories following the state transition P \u03c0\nand r(s, a, s\u2032) := 1 N\nN \u2211\ni=1\nri(s, a, r \u2032) is the central reward\nfunction. In this paper, we assume that each agent does\nnot have access to the rewards of the other agents. For\ninstance, there is no centralized coordinator; thus, each agent\nis unaware of the rewards of other agents. On the other hand,\nwe suppose that each agent knows only the parameters of\nadjacent agents over the network graph, assuming that the\nagents can communicate with each other.\nProblem 1 (Distributed value evaluation problem): The\ngoal of each agent i is to find the weight vector \u03b8i such that V\u03b8 := \u03a6\u03b8 approximates the true value function V \u03c0 of the centralized reward r = (r1 + \u00b7 \u00b7 \u00b7 + rN )/N , where only the local reward ri is given to each agent, and parameters can be shared with its neighbors over communication network represented by the graph G. This is typically done by minimizing the mean-square\nBellman error loss function [24]\nmin \u03b8\u2208Rq MSBE(\u03b8) := min \u03b8\u2208Rq\n1 2 \u2016R\u03c0 + \u03b3P \u03c0\u03a6\u03b8 \u2212 \u03a6\u03b8\u20162D, (1)\nwhere D is a diagonal matrix with positive diagonal elements d(s), s \u2208 S, and R\u03c0 \u2208 R|S| is a vector enumerating all R\u03c0(s), s \u2208 S. The solutions of (1) is known to be equivalent to those of the so-called projected Bellman equation\n\u03a6\u03b8 = \u03a0(R\u03c0 + \u03b3P \u03c0\u03a6\u03b8), (2)\nwhere \u03a0 is the projection onto the range space of \u03a6, denoted by R(\u03a6): \u03a0(x) := argminx \u2016x \u2212 x\u2032\u20162D, x\u2032 \u2208 R(\u03a6). Thus, solving the equation (1) becomes to be equivalent to\nminimize the so-called mean-square projected Bellman error\n(MSPBE) [24]\nmin \u03b8\u2208Rq MSBE(\u03b8) := min \u03b8\u2208Rq\n1 2 \u2016\u03a0(R\u03c0 + \u03b3P \u03c0\u03a6\u03b8)\u2212 \u03a6\u03b8\u20162D (3)\nThe projection can be performed by the matrix multiplication: we write \u03a0(x) := \u03a0x, where \u03a0 := \u03a6(\u03a6TD\u03a6)\u22121\u03a6TD. Moreover, the projected Bellman equation admits a unique\nsolution given by\n\u03b8\u2217 = \u2212(\u03a6TD(\u03b3P \u03c0 \u2212 I)\u03a6)\u22121\u03a6TDR\u03c0 (4)"
        },
        {
            "heading": "IV. ODE ANALAYSIS",
            "text": "For the sake of notational simplicity in representing a\nmulti-agent environment, we first introduce the stacked vec-\ntor and matrix notations:\n\u03b8\u0304 :=\n\n  \u03b81 ...\n\u03b8N\n\n  , R\u0304\u03c0 :=\n\n  R\u03c01 ...\nR\u03c0N\n\n  ,\nP\u0304 \u03c0 := IN \u2297 P \u03c0, L\u0304 := L\u2297 Iq, D\u0304 := IN \u2297D, \u03a6\u0304 := IN \u2297 \u03a6\nBefore delving into the distributed ODE, it is beneficial\nto examine the centralized ODE version. This provides a\nfoundation that can be extended to the distributed version.\nThe centralized variant can be naturally derived from the\nsolution of the MSPBE for a single agent, as shown in (4)."
        },
        {
            "heading": "A. Centralized ODE",
            "text": "In the centralized multi-agent case, the same reward R\u03c0c = (R\u03c01 + \u00b7 \u00b7 \u00b7 + R\u03c0N )/N for every agent is given. Then, it can be simply considered as the single agent case with stacked\nvector and matrix notation. According to the single-agent\nMDP results in (4), the optimal solution is given as\n\u03b8\u0304\u2217 = \u2212(\u03a6\u0304T D\u0304(\u03b3P\u0304 \u03c0 \u2212 I)\u03a6\u0304)\u22121\u03a6\u0304T D\u0304R\u0304\u03c0, (5) which minimizes the MSPBE (3). Using algebraic manipulations, we can easily prove that \u03b8\u0304\u2217 can be represented by \u03b8\u0304\u2217 = 1\u2297 \u03b8c\u221e, where\n\u03b8c\u221e = \u2212(\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6)\u22121\u03a6TDR\u03c0c . (6) The solution can be found using a standard dynamic\nprogramming approach [9]. In this paper, we will consider\nthe linear ODE\nd dt \u03b8\u0304t = \u03a6\u0304 T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304t + \u03a6\u0304T D\u0304(1\u2297R\u03c0c ). (7)\nWe can easily prove that \u03b8\u0304\u2217 is an asymptotically stable equilibrium point.\nProposition 1: \u03b8\u0304\u2217 is a unique asymptotically stable equilibrium point of the linear system in (7), i.e., \u03b8\u0304t \u2192 \u03b8\u0304\u2217 as t \u2192 \u221e.\nProof: Using \u03a6\u0304T D\u0304(\u03b3P\u0304 \u03c0 \u2212 I)\u03a6\u0304\u03b8\u0304\u2217 + \u03a6\u0304T D\u0304R\u0304\u03c0 = 0, (7) can be represented by the linear system\nd dt (\u03b8\u0304t \u2212 \u03b8\u0304\u2217) = \u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304(\u03b8\u0304t \u2212 \u03b8\u0304\u2217).\nWe can easily prove that \u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304 is negative definite, and hence, Hurwitz [22, pp. 209]. Therefore, \u03b8\u0304t \u2212 \u03b8\u0304\u2217 \u2192 0 as t \u2192 \u221e, which completes the proof.\nTo solve (7), we must assume that the central reward R\u03c0c is accessible to all agents. In subsequent sections, we will explore distributed versions where only the local reward R\u03c0i is provided to each agent i. We present two versions: the first is inspired by [10], while the second is a novel approach that\noffers more desirable properties compared to the first when\nintegrated into RL frameworks."
        },
        {
            "heading": "V. DISTRIBUTED TD-LEARNING VERSION 1",
            "text": "In the networked multi-agent setting, each agent receives\neach of their local rewards, and parameters from neighbors\nover a communication graph. Based on the ideas in [10], we\ncan convert the continuous-time ODE in (7) into\nd dt \u03b8\u0304t = \u03a6\u0304\nT D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304t + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304\u03b8\u0304t \u2212 L\u0304w\u0304t, d dt w\u0304t = L\u0304\u03b8\u0304t. (8)\nCompared to (7), the above ODE consists of an auxiliary\nvector w\u0304t and the graph Laplacian matrix L\u0304. Here, the Laplacian helps the consensus of each agent, and the auxiliary\nvector potentially allows agents make better use of their local\ninformation. Note that each agent only uses local information\nby multiplying the Laplacian in both equations. From a local\nview, the ODE in (8) can be written as follows:\nd dt \u03b8it = \u03a6 TD(\u2212I + \u03b3P \u03c0)\u03a6\u03b8it +\u03a6TDR\u03c0i\n\u2212\n\n|Ni|\u03b8it \u2212 \u2211\nj\u2208Ni\n\u03b8jt\n\n\u2212\n\n|Ni|wit \u2212 \u2211\nj\u2208Ni\nwjt\n\n\nd dt wit = |Ni|\u03b8it \u2212 \u2211\nj\u2208Ni\n\u03b8jt\nwhere Ni is the neighborhood of node i on the graph G. As can be seen from the above ODE, each agent i updates its local parameter \u03b8it using its own reward R \u03c0 i and parameters of its neighbors \u03b8jt , j \u2208 Ni. Nevertheless, we can prove that each agent i can find the global solution \u03b8c\u221e given in (6). To this end, we first provide stationary points of this system\nin the next result, and then prove that both weight vector \u03b8\u0304t and auxiliary vector w\u0304t reach the stationary point.\nProposition 2 (Equilibrium points): The unique equilib-\nrium point, \u03b8\u0304\u221e, of the linear system in (8) corresponding to the vector \u03b8\u0304t is given by \u03b8\u0304 \u2217 = 1N \u2297 \u03b8c\u221e, where \u03b8c\u221e is\nAlgorithm 1 Distributed TD-learning version 1\n1: Initialize \u03b8i0, w i 0, i \u2208 {1, 2, . . . , N}. 2: for k \u2208 {0, 1, 2, . . .} do 3: Sample sk \u223c d\u03c0 , ak \u223c \u03c0(\u00b7|sk), s\u2032k \u223c P (\u00b7|sk, ak). 4: for agent i \u2208 {1, . . . , N} do 5: Update\n\u03b8ik+1 =\u0393Si\n\n\u03b8ik + \u03b1k\n\n\n\n\u03b4ik\u03d5k \u2212\n\n|Ni|\u03b8ik \u2212 \u2211\nj\u2208Ni\n\u03b8ik\n\n\n\u2212\n\n|Ni|wit \u2212 \u2211\nj\u2208Ni\nwjt\n\n\n\n\n\n\n\nwik+1 =\u0393Si\n\nwik + \u03b1k\n\n\n\n|Ni|\u03b8ik \u2212 \u2211\nj\u2208Ni\n\u03b8jk\n\n\n\n\n\nwhere Ni is the neighborhood of node i on the graph G and \u03b4ik = r i k + \u03b3(\u03c6 \u2032 k)\nT \u03b8ik \u2212 \u03c6Tk \u03b8ik, \u03c6k = \u03c6(sk). 6: end for\n7: end for\ndefined in (6). Moreover, for the auxiliary vectors w\u0304t, the corresponding equilibrium points are in the affine space\nH :=\n\n        \n         \nw\u0304 \u2208 RqN : L\u0304w\u0304 =\n\n         \n\u03a6TD\n(\nR\u03c01 \u2212 1N N \u2211\ni=1\nR\u03c0i\n)\n\u03a6TD\n(\nR\u03c02 \u2212 1N N \u2211\ni=1\nR\u03c0i\n)\n...\n\u03a6TD\n(\nR\u03c0N \u2212 1N N \u2211\ni=1\nR\u03c0i\n)\n\n         \n\n        \n         \n.\n(9) The proof of Proposition 2 is given in Appendix VIII-A. Proposition 2 implies that the local parameter \u03b8it reaches a consensus, i.e.,\nlim t\u2192\u221e \u03b81t = lim t\u2192\u221e \u03b82t = \u00b7 \u00b7 \u00b7 = lim t\u2192\u221e \u03b8Nt = \u03b8 c \u221e.\nOn the other hand, w\u0304\u221e lies in the affine space H in (9), which is infinite. Next, we prove global asymptotic sta-\nbility of the equilibrium points, whose proof is given in\nAppendix VIII-B.\nProposition 3 (Global asymptotic stability): The equilibrium points \u03b8\u0304\u221e \u00d7 H of (8) is globally asymptotically stable, i.e., \u03b8\u0304t \u2192 \u03b8\u0304\u221e and w\u0304t \u2192 H as t \u2192 \u221e. Proposition 3 establishes that the first ODE version converges to the solution \u03b8\u0304\u2217. As a potential application, it is easy to envision the development of a distributed TD-learning\nby Euler discretization and by replacing certain terms with\nsample transitions of the underlying MDP.\nThe corresponding algorithm is given in Algorithm 1, where Si := {v \u2208 Rq : \u2016v\u2016\u221e \u2264 bi} is a hyper cube, and \u0393Si is the projection onto Si with respect to the infinity norm, i.e., \u0393Si [x] := argmin{v \u2208 Si : \u2016v \u2212 x\u2016\u221e}. Let us assume that Si is chosen sufficiently large such that S1 \u00d7 S2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN =: S \u2229 H 6= \u2205 and \u03b8c\u221e \u2208 Si\nfor all i \u2208 {1, 2, . . . , N}. The projections are applied to guarantee the boundedness of the iterates. In such a scenario,\nthe asymptotic stability of the ODE could be leveraged to\ndemonstrate the convergence of the RL, using the well-\nestablished Borkar-Meyn theorem [12]. A primary challenge\nin applying the Borkar-Meyn theorem is the non-uniqueness\nof the equilibrium point of the ODE in (8). For the Borkar-\nMeyn theorem\u2019s application, the existence of a unique equi-\nlibrium point is a prerequisite. However, Robbins-Monro\ntheorem or Kushner-Clark theorem for projected stochastic\napproximation [22], [23] can be applied with the projection\noperations. The detailed proof is omitted here for brevity.\nProposition 4 (Convergence): Suppose that S is chosen sufficiently large such that S1\u00d7S2\u00d7\u00b7 \u00b7 \u00b7\u00d7SN =: S\u2229H 6= \u2205 and \u03b8c\u221e \u2208 Si for all i \u2208 {1, 2, . . . , N}. Under the RobbinsMonro step-size rule\n\u03b1k > 0, \u2200k \u2265 0, \u221e \u2211\nk=0\n\u03b1k = \u221e, \u221e \u2211\nk=0\n\u03b12k < \u221e, (10)\nin Algorithm 1, \u03b8\u0304k \u2192 \u03b8\u0304\u221e and w\u0304k \u2192 H\u2229S as k \u2192 \u221e with probability one."
        },
        {
            "heading": "VI. DISTRIBUTED TD-LEARNING VERSION 2",
            "text": "In this section, we propose the following ODE:\nd dt \u03b8\u0304t = \u03a6\u0304\nT D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304t + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304\u03b8\u0304t d dt w\u0304t = \u03b8\u0304t \u2212 w\u0304t \u2212 L\u0304w\u0304t \u2212 L\u0304v\u0304t\nd dt v\u0304t = L\u0304w\u0304t (11)\nWhen viewed locally, the ODE can be written as\nd dt \u03b8it = \u03a6 TD(\u2212I + \u03b3P \u03c0)\u03a6\u03b8it +\u03a6TDR\u03c0i\n\u2212\n\n|Ni|\u03b8it \u2212 \u2211\nj\u2208Ni\n\u03b8jt\n\n\nd dt wit = \u03b8 i t \u2212 wit\n\u2212\n\n|Ni|wit \u2212 \u2211\nj\u2208Ni\nwjt\n\n\u2212\n\n|Ni|vit \u2212 \u2211\nj\u2208Ni\nvjt\n\n\nd dt vit = |Ni|wit \u2212 \u2211\nj\u2208Ni\nwjt\nwhere Ni is the neighborhood of node i on the graph G. We can demonstrate that w\u0304t \u2192 \u03b8\u0304\u2217 as t \u2192 \u221e, where \u03b8\u0304\u2217 is defined in (5). Thus, this ODE can serve as an alternative to the ODE\nin (8). A key distinction between the current ODE and its\npredecessor is the decoupling of the ODE corresponding to \u03b8\u0304t from the components linked to (w\u0304t, v\u0304t) The ODE for \u03b8\u0304t can be seen as the local value function estimation, while the ODE\nfor (w\u0304t, v\u0304t) represents the parameter mixing component. As detailed in the subsequent section, this unique char-\nacteristic of the new distributed ODE renders it more apt\nfor RL applications. This utility forms our core rationale\nfor presenting the new ODE in (11) as a supplement to the\ninitial ODE Prior to delving into the TD-learning equivalent,\nwe will first establish the equilibrium points of (11) and their\nasymptotic stability.\nProposition 5 (Equilibrium points): The unique equilib-\nrium point, w\u0304\u221e, of the linear system in (11) corresponding to the vector w\u0304t is given by w\u0304\u221e = 1N \u2297 \u03b8c\u221e, where \u03b8c\u221e is defined in (6). Moreover, for the vector \u03b8\u0304t, the corresponding equilibrium points, \u03b8\u0304\u221e, are all solutions of the following linear equation:\n1\nN\nN \u2211\ni=1\n\u03b8i\u221e = \u2212(\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6)\u22121\u03a6TD ( 1\nN\nN \u2211\ni=1\nR\u03c0i\n)\n(12)\nFor another vector v\u0304t, the corresponding equilibrium points, v\u0304\u221e, are all solutions of the following linear equation:\nL\u0304v\u0304\u221e = \u03b8\u0304\u221e \u2212 w\u0304\u221e. (13) Note that the set of solutions satisfying (13) is the affine\nspace\nH := {v \u2208 RqN : L\u0304v = \u03b8\u0304\u221e \u2212 w\u0304\u221e}. The proof of Proposition 5 is given in Appendix VIII-C.\nNext, we establish the global asymptotic stability of (11),\nwhose proof is given in Appendix VIII-D.\nProposition 6 (Global asymptotic stability): The equilib-\nrium points (\u03b8\u0304\u221e, w\u0304\u221e, v\u0304\u221e) of (11) is globally asymptotically stable, i.e., \u03b8\u0304t \u2192 \u03b8\u0304\u221e, w\u0304t \u2192 w\u0304\u221e, and v\u0304t \u2192 H := {v \u2208 RqN : L\u0304v = \u03b8\u0304\u221e \u2212 w\u0304\u221e} as t \u2192 \u221e.\nDrawing from the new distributed ODE detailed in (11),\nin this section, we derive a novel distributed TD-learning algorithm presented in Algorithm 2, where Si := {v \u2208 Rq : \u2016v\u2016\u221e \u2264 bi} is a hyper cube, and \u0393Si is the projection onto Si with respect to the infinity norm, i.e., \u0393Si [x] := argmin{v \u2208 Si : \u2016v \u2212 x\u2016\u221e}. Let us assume that Si is chosen sufficiently large such that S1 \u00d7 S2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN =: S \u2229 H := {v \u2208 RqN : L\u0304v = \u03b8\u0304\u221e \u2212 w\u0304\u221e} 6= \u2205.\nThis algorithm aligns with the ODE described in (11). It\nis evident that only the system associated with \u03b8\u0304t in (11) depends on the model parameters. Consequently, when de-\nveloping an RL counterpart, it is solely the system associated\nwith \u03b8\u0304t that requires substitution with its sampled iteration. Furthermore, the system associated with \u03b8\u0304t in (11) possesses a unique asymptotically stable equilibrium point as\nindicated in Proposition 5, i.e., Hurwitz. Thus, Borkar-Meyn\ntheorem [12] needs to be applied exclusively to the first\nequation in (11), owing to the decoupling of \u03b8\u0304t and (w\u0304t, v\u0304t). Upon convergence of \u03b8\u0304k to a specific equilibrium point, it is feasible to confirm the convergence of the additional\nvariables w\u0304k, v\u0304k using approaches such as the RobbinsMonro theorem [22], [23].\nProposition 7 (Convergence): Suppose that S is chosen sufficiently large such that S\u2229H := {v \u2208 RqN : L\u0304v = \u03b8\u0304\u221e\u2212 w\u0304\u221e} 6= \u2205. Under the Robbins-Monro step-size rule (10), in Algorithm 2, w\u0304k \u2192 w\u0304\u221e, \u03b8\u0304k \u2192 \u03b8\u0304\u221e, and v\u0304k \u2192 H \u2229 S as k \u2192 \u221e with probability one.\nProof: First of all, the first ODE in (11) can be seen as a linear system x\u0307t = Axt with A = \u03a6\u0304 T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304 \u2212\nAlgorithm 2 Distributed TD-learning\n1: Initialize \u03b8i0, w i 0, v i 0, i \u2208 {1, 2, . . . , N}. 2: for k \u2208 {0, 1, 2, . . .} do 3: Sample sk \u223c d\u03c0 , ak \u223c \u03c0(\u00b7|sk), s\u2032k \u223c P (\u00b7|sk, ak). 4: for agent i \u2208 {1, . . . , N} do 5: Update\n\u03b8ik+1 =\u03b8 i k + \u03b1k\n\n\n\n\u03b4ik\u03c6k \u2212\n\n|Ni|\u03b8ik \u2212 \u2211\nj\u2208Ni\n\u03b8ik\n\n\n\n\n\nwik+1 =w i k + \u03b1k\n\n\n\n\u03b8ik \u2212 wik \u2212\n\n|Ni|wik \u2212 \u2211\nj\u2208Ni\nwik\n\n\n\u2212\n\n|Ni|vik \u2212 \u2211\nj\u2208Ni\nvik\n\n\n\n\n\nvik+1 =\u0393S\n\nvik + \u03b1k\n\n\n\n|Ni|wik \u2212 \u2211\nj\u2208Ni\nwik\n\n\n\n\n\nwhere Ni is the neighborhood of node i on the graph G and \u03b4ik = r i k + \u03b3(\u03c6 \u2032 k)\nT \u03b8ik \u2212 \u03c6Tk \u03b8ik, \u03c6k = \u03c6(sk). 6: end for\n7: end for\nL\u0304, b = \u03a6\u0304T D\u0304R\u0304\u03c0. Moreover, it is clear from Proposition 6 that A is Hurwitz. Therefore, by Borkar-Meyn theorem [12], it can be easily proven that in the RL counterpart in Algorithm 2, \u03b8\u0304k = [ \u03b81k \u03b8 2 k \u00b7 \u00b7 \u00b7 \u03b8Nk\n]T \u2192 \u03b8\u0304\u221e with probability one. Next, the convergence of wik and v i k cannot be proved using Borkar-Meyn theorem because it does not\nadmit a unique equilibrium point. However, we can apply\nRobbins-Monro theorem [22], [23]. First of all, we need\nthe boundedness of the updates. Due to the projection, v\u0304k is bounded. By Borkar-Meyn theorem, \u03b8\u0304k is bounded with probability one. The update equation for w\u0304k is a linear system, where the system matrix is Schur, and \u03b8\u0304k and v\u0304k can be regarded as external noises, which are bounded with\nprobability one. Therefore, it can be proved that w\u0304k is also bounded with probability one. Next, the system for w\u0304k and v\u0304k can be seen as a system where \u03b8\u0304k plays the role of the stochastic noise. Therefore, Robbins-Monro theorem can be\napplied. This completes the proof."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this paper, we presented novel distributed ODEs along\nwith their corresponding TD-learning approachs for net-\nworked MAMDPs. In particular, one of the distributed\nODEs comprises two decoupled subsystems: the first system\nfocuses on estimating the local value functions, while the\nsecond facilitates averaging consensus through its ODE. This\nseparation of roles within distinct ODE potentially enables\nthe direct application of the Borkar-Meyn theorem to the\nrelated RL counterpart."
        },
        {
            "heading": "VIII. APPENDIX",
            "text": ""
        },
        {
            "heading": "A. Proof of Proposition 2",
            "text": "Let (\u03b8\u0304\u221e, w\u0304\u221e) be an equilibrium point corresponding to (\u03b8\u0304t, w\u0304t). Then, it should satisfy the following equation:\n\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304\u221e + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304\u03b8\u0304\u221e \u2212 L\u0304w\u0304\u221e = 0 L\u0304\u03b8\u0304\u221e = 0.\nThe second equation implies \u03b8\u0304\u221e = 1N \u2297 v for some v \u2208 R q . Plugging this relation into the first equation, we have\n\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304\u221e + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304w\u0304\u221e = 0, (14) where we used L\u0304\u03b8\u0304\u221e = 0. Multiplying (1N \u2297 I)T from the left yields\n(1N \u2297 I)T [\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304\u221e + \u03a6\u0304T D\u0304R\u0304\u03c0] = 0, where we used (1N \u2297 I)T L\u0304w\u0304\u221e = 0. The equation can be equivalently written as\nN\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6v = \u2212 N \u2211\ni=1\n\u03a6TDR\u03c0i\nSince \u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6 is nonsingular from [22, pp. 209], it follows that\nv = \u2212(\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6)\u22121\u03a6TD ( 1\nN\nN \u2211\ni=1\nR\u03c0i\n)\n,\nwhich is identical to (6), i.e., v = \u03b8c\u221e. Therefore, \u03b8\u0304 c \u221e = 1N \u2297 v = 1N \u2297 \u03b8c\u221e = \u03b8\u0304\u2217. Plugging it back to (14) leads to (9). This completes the proof."
        },
        {
            "heading": "B. Proof of Proposition 3",
            "text": "With \u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304\u221e + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304w\u0304\u221e = 0 and L\u0304\u03b8\u0304\u221e = 0, the ODEs in (8) become\nd dt (\u03b8\u0304t \u2212 \u03b8\u0304\u221e) = \u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\n\u2212 L\u0304(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\u2212 L\u0304(w\u0304t \u2212 w\u0304\u221e) (15)\nd dt (w\u0304t \u2212 w\u0304\u221e) = L\u0304(\u03b8\u0304t \u2212 \u03b8\u0304\u221e).\nConsider the function\nV\n([\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n])\n=\n[\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n]T [\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n]\n.\nIts time-derivative along the trajectory is\nV\u0307\n([\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n])\n=\n[\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n]T [\n2\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u2212 2L\u0304 0 0 0\n]\n\u00d7 [ \u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e ]\n= (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T [2\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u2212 2L\u0304](\u03b8\u0304t \u2212 \u03b8\u0304\u221e). Using the following well-known inequality [22, pp. 209]:\n\u03a6\u0304T D\u0304(\u03b3P\u0304 \u03c0 \u2212 I)\u03a6\u0304 + \u03a6\u0304T (\u03b3P\u0304 \u03c0 \u2212 I)T D\u0304\u03a6\u0304 2(\u03b3 \u2212 1)D\u0304, (16)\none gets \u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304 + \u03a6\u0304T (\u03b3P\u0304 \u03c0 \u2212 I)T D\u0304\u03a6\u0304\u2212 2L\u0304 2(\u03b3 \u2212 1)D\u0304\u2212 2L\u0304 \u227a 0, where the second inequality is due to D\u0304 \u227b 0, L\u0304 0, and \u03b3 \u2212 1 < 0. Equivalently, we have\nV\u0307\n([\n\u03b8\u0304t \u2212 \u03b8\u0304\u221e w\u0304t \u2212 w\u0304\u221e\n])\n\u2264(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T ((\u03b3 \u2212 1)D\u0304 \u2212 L\u0304)(\u03b8\u0304t \u2212 \u03b8\u0304\u221e) <0,\nfor any \u03b8\u0304t \u2212 \u03b8\u0304\u221e 6= 0. Taking the integral on both sides and rearranging terms lead to\nV\n([\n\u03b8\u0304T \u2212 \u03b8\u0304\u221e w\u0304T \u2212 w\u0304\u221e\n]) \u2212 V ([ \u03b8\u03040 \u2212 \u03b8\u0304\u221e w\u03040 \u2212 w\u0304\u221e ])\n\u2264 \u222b T\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T ((\u03b3 \u2212 1)D\u0304 \u2212 L\u0304)(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\nRearranging terms lead to\n\u03bbmin((1 \u2212 \u03b3)D\u0304 + L\u0304) \u222b T\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\n\u2264 \u222b T\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T ((1\u2212 \u03b3)D\u0304 + L\u0304)(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\n\u2264V ([ \u03b8\u03040 \u2212 \u03b8\u0304\u221e w\u03040 \u2212 w\u0304\u221e ]) \u2212 V ([ \u03b8\u0304T \u2212 \u03b8\u0304\u221e w\u0304T \u2212 w\u0304\u221e ]) \u2264V ([\n\u03b8\u03040 \u2212 \u03b8\u0304\u221e w\u03040 \u2212 w\u0304\u221e\n])\n.\nTaking the limit T \u2192 \u221e yields \u222b \u221e\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\n\u2264 1 \u03bbmin((\u03b3 \u2212 1)D\u0304 \u2212 L\u0304) V\n([\n\u03b8\u03040 \u2212 \u03b8\u0304\u221e w\u03040 \u2212 w\u0304\u221e\n])\nwhich implies from Barbalat\u2019s lemma, that \u03b8\u0304t \u2192 \u03b8\u0304\u221e as t \u2192 \u221e. Now, taking the limit t \u2192 \u221e on both sides of (15) yields L\u0304w\u0304\u221e = limt\u2192\u221e L\u0304w\u0304t. Combining the above equation with (9) leads to limt\u2192\u221e L\u0304w\u0304t = 0, which is the desired conclusion."
        },
        {
            "heading": "C. Proof of Proposition 5",
            "text": "First of all, note that the stationary points should satisfy\n\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u03b8\u0304\u221e + \u03a6\u0304T D\u0304R\u0304\u03c0 \u2212 L\u0304\u03b8\u0304\u221e = 0 \u03b8\u0304\u221e \u2212 w\u0304\u221e \u2212 L\u0304w\u0304\u221e \u2212 L\u0304v\u0304\u221e = 0\nL\u0304w\u0304\u221e = 0 (17)\nMultiplying (1N \u2297 I)T from the left, the first equation becomes\n\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6 N \u2211\ni=1\n\u03b8i\u221e +\u03a6 TD\nN \u2211\ni=1\nR\u03c0i = 0.\nRearranging terms, we can prove that \u03b8i\u221e should satisfy (12). On the other hand, the third equation implies\nw1\u221e = w 2 \u221e = \u00b7 \u00b7 \u00b7 = wN\u221e =: w\u221e. (18)\nPlugging this relation into the second equation and multiplying (1N \u2297 I)T from the left, we have (1N \u2297 I)T \u03b8\u0304\u221e \u2212 (1N \u2297 I)T w\u0304\u221e = 0.\nCombining the above equation with (18) leads to\nw\u221e = 1\nN\nN \u2211\ni=1\n\u03b8i\u221e\n= \u2212(\u03a6TD(\u2212I + \u03b3P \u03c0)\u03a6)\u22121\u03a6TD ( 1\nN\nN \u2211\ni=1\nR\u03c0i\n)\n,\nwhere the second equality comes from (12). Finally, the\nsecond equation with L\u0304w\u0304\u221e results in (13). This completes the proof."
        },
        {
            "heading": "D. Proof of Proposition 6",
            "text": "Noting that the equilibrium points satisfy (17), the ODEs\nin (11) can be written by\nd dt (\u03b8\u0304t \u2212 \u03b8\u0304\u221e) = [\u03a6\u0304T D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304\u2212 L\u0304](\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\nd dt (w\u0304t \u2212 w\u0304\u221e) = (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\u2212 (I + L\u0304)(w\u0304t \u2212 w\u0304\u221e)\n\u2212 L\u0304(v\u0304t \u2212 v\u0304\u221e) d dt (v\u0304t \u2212 v\u0304\u221e) = L\u0304(w\u0304t \u2212 w\u0304\u221e) (19)\nLet us consider the Lyapunov function candidate\nV (\u03b8\u0304t \u2212 \u03b8\u0304\u221e) = (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e),\nwhose time-derivative along the trajectory is\nd dt V (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\n= (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304 + (D\u0304(\u2212I + \u03b3P\u0304 \u03c0)\u03a6\u0304)T \u2212 2L\u0304)(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)\n< 0\nfor all \u03b8\u0304t \u2212 \u03b8\u0304\u221e 6= 0, where the last inequality is due to (16) and L\u0304 0. By the Lyapunov theorem [11], \u03b8\u0304t \u2192 \u03b8\u0304\u221e as t \u2192 \u221e. Moreover, since the system is a linear system, the convergence is exponential. For the convergence of w\u0304t, consider the function\nV (w\u0304t \u2212 w\u0304\u221e, v\u0304t \u2212 v\u0304\u221e) = (w\u0304t \u2212 w\u0304\u221e)T (w\u0304t \u2212 w\u0304\u221e) + (v\u0304t \u2212 v\u0304\u221e)T (v\u0304t \u2212 v\u0304\u221e),\nwhose time-derivative along the trajectories is\nd dt V (w\u0304t \u2212 w\u0304\u221e, v\u0304t \u2212 v\u0304\u221e)\n=\u2212 (w\u0304t \u2212 w\u0304\u221e)T (2I + 2L\u0304)(w\u0304t \u2212 w\u0304\u221e) + 2(w\u0304t \u2212 w\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e).\nIntegrating both sides from t = 0 to T yields\nV (w\u0304T \u2212 w\u0304\u221e, v\u0304T \u2212 v\u0304\u221e)\u2212 V (w\u03040 \u2212 w\u0304\u221e, v\u03040 \u2212 v\u0304\u221e)\n= \u2212 \u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (2I + 2L\u0304)(w\u0304t \u2212 w\u0304\u221e)dt\n+ 2\n\u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\nRearranging terms lead to\n2\n\u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (I + L\u0304)(w\u0304t \u2212 w\u0304\u221e)dt\n= \u2212V (w\u0304T \u2212 w\u0304\u221e, v\u0304T \u2212 v\u0304\u221e) + V (w\u03040 \u2212 w\u0304\u221e, v\u03040 \u2212 v\u0304\u221e)\n+ 2\n\u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\n\u2264 V (w\u03040 \u2212 w\u0304\u221e, v\u03040 \u2212 v\u0304\u221e) + 2 \u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\n\u2264 V (w\u03040 \u2212 w\u0304\u221e, v\u03040 \u2212 v\u0304\u221e) + \u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (w\u0304t \u2212 w\u0304\u221e)dt\n+\n\u222b T\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\nwhere the second inequality comes from the Young\u2019s inequal-\nity. Rearranging some terms again, we have \u222b T\n0\n(w\u0304t \u2212 w\u0304\u221e)T (I + 2L\u0304)(w\u0304t \u2212 w\u0304\u221e)dt\n\u2264 V (w\u03040 \u2212 w\u0304\u221e, v\u03040 \u2212 v\u0304\u221e) + \u222b T\n0\n(\u03b8\u0304t \u2212 \u03b8\u0304\u221e)T (\u03b8\u0304t \u2212 \u03b8\u0304\u221e)dt\nThe integral on the right-hand side is bounded because \u03b8\u0304t \u2212 \u03b8\u0304\u221e converges to zero exponentially. Moreover, since I +2L\u0304 is positive definite, the above inequality implies that w\u0304t \u2192 w\u0304\u221e = \u03b8\u0304\n\u2217 = 1N \u2297 \u03b8\u0304\u221e as t \u2192 \u221e from the Barbalat\u2019s lemma. Now, taking the limit t \u2192 \u221e on both sides of the third equation in (19) leads to\nlim t\u2192\u221e\nd dt (v\u0304t \u2212 v\u0304\u221e) = 0\nimplying that v\u0304t converges to some constant v\u0304\u221e, where we used the fact that w\u0304t \u2192 w\u0304\u221e as t \u2192 \u221e. Finally, it remains to prove the convergence of \u03b8\u0304t. To this end, taking the limit t \u2192 \u221e on both sides of the second equation in (11) leads to 0 = \u03b8\u0304\u221e\u2212 w\u0304\u221e\u2212 L\u0304w\u0304\u221e\u2212 limt\u2192\u221e L\u0304v\u0304t, which is equivalent to limt\u2192\u221e L\u0304v\u0304t = \u03b8\u0304\u221e \u2212 w\u0304\u221e using L\u0304w\u0304\u221e = 0. This completes the proof."
        }
    ],
    "title": "An ODE Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes",
    "year": 2023
}