{
    "abstractText": "Ground-penetrating radar (GPR) is an important nondestructive testing (NDT) tool for the underground exploration of urban roads. However, due to the large amount of GPR data, traditional manual interpretation is time-consuming and laborious. To address this problem, an efficient underground target detection method for urban roads based on neural networks is proposed in this paper. First, robust principal component analysis (RPCA) is used to suppress the clutter in the B-scan image. Then, three time-domain statistics of each A-scan signal are calculated as its features, and one backpropagation (BP) neural network is adopted to recognize A-scan signals to obtain the horizontal regions of targets. Next, the fusion and deletion (FAD) algorithm is used to further optimize the horizontal regions of targets. Finally, three time-domain statistics of each segmented A-scan signal in the horizontal regions of targets are extracted as the features, and another BP neural network is employed to recognize the segmented A-scan signals to obtain the vertical regions of targets. The proposed method is verified with both simulation and real GPR data. The experimental results show that the proposed method can effectively locate the horizontal ranges and vertical depths of underground targets for urban roads and has higher recognition accuracy and less processing time than the traditional segmentation recognition methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pier Matteo Barone"
        },
        {
            "affiliations": [],
            "name": "Alastair Ruffell"
        },
        {
            "affiliations": [],
            "name": "Wei Xue"
        },
        {
            "affiliations": [],
            "name": "Kehui Chen"
        },
        {
            "affiliations": [],
            "name": "Ting Li"
        },
        {
            "affiliations": [],
            "name": "Li Liu"
        },
        {
            "affiliations": [],
            "name": "Jian Zhang"
        }
    ],
    "id": "SP:10a3e6fe9887e6cc8f152a5a53cf3f3ca458307a",
    "references": [
        {
            "authors": [
                "T. Saarenketo",
                "T. Scullion"
            ],
            "title": "Road evaluation with ground penetrating radar",
            "venue": "J. Appl. Geophys",
            "year": 2000
        },
        {
            "authors": [
                "Z. Dong",
                "S. Ye",
                "Y. Gao",
                "G. Fang",
                "X. Zhang",
                "Z. Xue",
                "T. Zhang"
            ],
            "title": "Rapid Detection Methods for Asphalt Pavement Thicknesses and Defects by a Vehicle-Mounted Ground Penetrating Radar (GPR) System",
            "venue": "Sensors 2016,",
            "year": 2067
        },
        {
            "authors": [
                "W.W. Lai",
                "X. D\u00e9robert",
                "P. Annan"
            ],
            "title": "A review of ground penetrating radar application in civil engineering: A 30-year journey from locating and testing to imaging and diagnosis",
            "venue": "NDT E Int. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "B. Park",
                "J. Kim",
                "J. Lee",
                "M.S. Kang",
                "Y.K. An"
            ],
            "title": "Underground Object Classification for Urban Roads Using Instantaneous Phase Analysis of Ground-Penetrating Radar (GPR) Data",
            "venue": "Remote Sens. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "X. Yang",
                "W. Li",
                "S. Zhang",
                "Y. Jia"
            ],
            "title": "Automatic detection of moisture damages in asphalt pavements from GPR data with deep CNN and IRS method",
            "venue": "Autom. Constr",
            "year": 2020
        },
        {
            "authors": [
                "M. Rasol",
                "J.C. Pais",
                "V. P\u00e9rez-Gracia",
                "M. Solla",
                "F.M. Fernandes",
                "S. Fontul",
                "D. Ayala-Cabrera",
                "F. Schmidt",
                "H. Assadollahi"
            ],
            "title": "GPR monitoring for road transport infrastructure: A systematic review and machine learning insights",
            "venue": "Constr. Build. Mater",
            "year": 2022
        },
        {
            "authors": [
                "Y. Gao",
                "L. Pei",
                "S. Wang",
                "W. Li"
            ],
            "title": "Intelligent Detection of Urban Road Underground Targets by Using Ground Penetrating Radar based on Deep Learning",
            "venue": "J. Phys. Conf. Ser",
            "year": 2021
        },
        {
            "authors": [
                "W. Shao",
                "A. Bouzerdoum",
                "S.L. Phung",
                "L. Su",
                "B. Indraratna",
                "C. Rujikiatkamjorn"
            ],
            "title": "Automatic Classification of Ground-PenetratingRadar Signals for Railway-Ballast Assessment",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2011
        },
        {
            "authors": [
                "W. Al-Nuaimy",
                "Y. Huang",
                "M. Nakhkash",
                "M.T.C. Fang",
                "V.T. Nguyen",
                "A. Eriksen"
            ],
            "title": "Automatic detection of buried utilities and solid objects with GPR using neural networks and pattern recognition",
            "venue": "J. Appl. Geophys",
            "year": 2000
        },
        {
            "authors": [
                "J. Wu",
                "T. Mao",
                "H. Zhou"
            ],
            "title": "Feature Extraction and Recognition Based on SVM",
            "venue": "In Proceedings of the 2008 4th International Conference on Wireless Communications, Networking and Mobile Computing (WiCOM), Dalian, China,",
            "year": 2008
        },
        {
            "authors": [
                "H. Frigui",
                "P. Gader"
            ],
            "title": "Detection and discrimination of land mines in ground-penetrating radar based on edge histogram descriptors and a possibilistic K-nearest neighbor classifier",
            "venue": "IEEE Trans. Fuzzy Syst",
            "year": 2009
        },
        {
            "authors": [
                "P. Torrione",
                "K.D. Morton",
                "R. Sakaguchi",
                "L. Collins"
            ],
            "title": "Histograms of oriented gradients for landmine detection in groundpenetrating radar data",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2014
        },
        {
            "authors": [
                "X. Xie",
                "H. Qin",
                "C. Yu",
                "L. Liu"
            ],
            "title": "An automatic recognition algorithm for GPR images of RC structure voids",
            "venue": "J. Appl. Geophys",
            "year": 2013
        },
        {
            "authors": [
                "X. N\u00fa\u00f1ez-Nieto",
                "M. Solla",
                "P. G\u00f3mez-P\u00e9rez",
                "H. Lorenzo"
            ],
            "title": "GPR Signal Characterization for Automated Landmine and UXO Detection Based on Machine Learning Techniques",
            "venue": "Remote Sens. 2014,",
            "year": 2014
        },
        {
            "authors": [
                "H. Harkat",
                "A.E. Ruano",
                "M.G. Ruano",
                "S.D. Bennani"
            ],
            "title": "GPR target detection using a neural network classifier designed by a multi-objective genetic algorithm",
            "venue": "Appl. Soft Comput",
            "year": 2019
        },
        {
            "authors": [
                "Z. Tong",
                "J. Gao",
                "H. Zhang"
            ],
            "title": "Innovative method for recognizing subgrade defects based on a convolutional neural network",
            "venue": "Constr. Build. Mater",
            "year": 2018
        },
        {
            "authors": [
                "W. Xue",
                "X. Dai",
                "L. Liu"
            ],
            "title": "Remote Sensing Scene Classification Based on Multi-Structure Deep Features Fusion",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "W. Huang",
                "Z. Zhao",
                "L. Sun",
                "M. Ju"
            ],
            "title": "Dual-Branch Attention-Assisted CNN for Hyperspectral Image Classification",
            "venue": "Remote Sens. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Tong",
                "J. Gao",
                "H. Zhang"
            ],
            "title": "Recognition, location, measurement, and 3D reconstruction of concealed cracks using convolutional neural networks",
            "venue": "Constr. Build. Mater",
            "year": 2017
        },
        {
            "authors": [
                "K. Dinh",
                "N. Gucunski",
                "T.H. Duong"
            ],
            "title": "An algorithm for automatic localization and detection of rebars from GPR data of concrete bridge decks",
            "venue": "Autom. Constr",
            "year": 2018
        },
        {
            "authors": [
                "H. Liu",
                "C. Lin",
                "J. Cui",
                "L. Fan",
                "X. Xie",
                "B.F. Spencer"
            ],
            "title": "Detection and localization of rebar in concrete by deep learning using ground penetrating radar",
            "venue": "Autom. Constr",
            "year": 2020
        },
        {
            "authors": [
                "U. Ozkaya",
                "F. Melgani",
                "M.B. Bejiga",
                "L. Seyfi",
                "M. Donelli"
            ],
            "title": "GPR B Scan Image Analysis with Deep Learning Methods",
            "venue": "Measurement",
            "year": 2020
        },
        {
            "authors": [
                "L. Qiao",
                "Y. Qin",
                "X. Ren",
                "Q. Wang"
            ],
            "title": "Identification of buried objects in GPR using amplitude modulated signals extracted from multiresolution monogenic signal analysis",
            "venue": "Sensors",
            "year": 2015
        },
        {
            "authors": [
                "F.H.C. Tivive",
                "A. Bouzerdoum",
                "C. Abeynayake"
            ],
            "title": "GPR Target Detection by Joint Sparse and Low-Rank Matrix Decomposition",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2019
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "X. Li",
                "Y. Ma",
                "J. Wright"
            ],
            "title": "Robust Principal Component Analysis",
            "venue": "J. ACM",
            "year": 2011
        },
        {
            "authors": [
                "Z. He",
                "P. Peng",
                "L. Wang",
                "Y. Jiang"
            ],
            "title": "PickCapsNet: Capsule Network for Automatic P-Wave Arrival Picking",
            "venue": "IEEE Geosci. Remote Sens. Lett. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "P. Gamba",
                "S. Lossani"
            ],
            "title": "Neural Detection of Pipe Signatures in Ground Penetrating Radar Images",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2000
        },
        {
            "authors": [
                "D. Gao",
                "S. Wu"
            ],
            "title": "An optimization method for the topological structures of feed-forward multi-layer neural networks",
            "venue": "Pattern Recognit",
            "year": 1998
        },
        {
            "authors": [
                "H. Liu",
                "J. Liu",
                "Y. Wang",
                "Y. Xia",
                "Z. Guo"
            ],
            "title": "Identification of grouting compactness in bridge bellows based on the BP neural network",
            "venue": "Structures",
            "year": 2021
        },
        {
            "authors": [
                "D. Rumelhart",
                "G.E. Hinton",
                "R.J. Williams"
            ],
            "title": "Learning Representations by Back-Propagating Errors",
            "venue": "Nature",
            "year": 1986
        },
        {
            "authors": [
                "I. Giannakis",
                "A. Giannopoulos",
                "C. Warren"
            ],
            "title": "A Realistic FDTD Numerical Modeling Framework of Ground Penetrating Radar for Landmine Detection",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2016,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Citation: Xue, W.; Chen, K.; Li, T.;\nLiu, L.; Zhang, J. Efficient\nUnderground Target Detection of\nUrban Roads in Ground-Penetrating\nRadar Images Based on Neural\nNetworks. Remote Sens. 2023, 15, 1346.\nhttps://doi.org/10.3390/rs15051346\nAcademic Editors: Pier Matteo Barone,\nAlastair Ruffell and Carlotta Ferrara\nReceived: 11 January 2023\nRevised: 25 February 2023\nAccepted: 26 February 2023\nPublished: 28 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: ground-penetrating radar; underground target detection; urban road; neural network; robust principal component analysis; fusion and deletion algorithm\n1. Introduction\nUnderground target detection plays an important role in urban road exploration and can be used to conduct road maintenance and management. Underground targets of urban roads mainly include voids, pipes, and cables. Due to its advantages of having a nondestructive nature, high scanning efficiency, and penetration, ground-penetrating radar (GPR) has been widely used in urban road exploration [1\u20135]. Because of the complex structures of underground targets, the interpretation of GPR data still mainly relies on skilled operators. In general, the time needed for analyzing GPR data is considerably longer than the time taken for data acquisition. When long-distance urban roads need to be surveyed, the amount of GPR data is very large and manual handling is apparently powerless. Therefore, it is necessary to develop automatic target detection methods, which can not only increase the interpretation efficiency, but also avoid the influence of subjective factors. At present, automatic target detection methods in GPR data can be divided into two categories: machine learning methods and deep learning methods [6,7]. Automatic detection methods based on machine learning generally include three stages: preprocessing, feature extraction, and signal classification [8]. Preprocessing mainly performs clutter and\nRemote Sens. 2023, 15, 1346. https://doi.org/10.3390/rs15051346 https://www.mdpi.com/journal/remotesensing\nRemote Sens. 2023, 15, 1346 2 of 22\nnoise suppression. Feature extraction reduces the preprocessed data to form a set of measures that represent the data. Signal classification uses classifiers to recognize the object according to its features. Al-Nuaimy et al. [9] proposed a method based on a neural network to recognize buried objects in GPR B-scan images. The method employs ensemble mean subtraction to remove the background clutter and uses the Welch power spectra of A-scan signal segments as the features. However, the ensemble mean subtraction fails to remove the non-horizontal clutter, which causes some incorrect identification for sections of the targets. Wu et al. [10] presented a method based on a support vector machine (SVM) to detect holes under a railway with GPR signals. The method uses dyadic wavelet transform to extract the energy features of A-scan signals and reaches a high recognition rate, but it only recognizes the whole A-scan signal and cannot provide the depth information of the holes. Frigui et al. [11] proposed an algorithm for landmine detection based on K-nearest neighbor (KNN) using the GPR data. The method uses edge histogram descriptors (EHD) for feature extraction and obtains better performance than the hidden Markov model (HMM) because EHD applies fuzzy techniques to distinguish true detection from false alarms. Torrione et al. [12] developed a method based on the histogram of oriented gradients (HOG) features and the random forest method to detect landmines in the GPR data. The results indicate that HOG features provide better target classification performance than EHD and HMM. Xie et al. [13] used SVM to recognize RC structure voids in GPR images. The predictive deconvolution process is used to suppress clutter and three time-domain statistical features are extracted for each segmented A-scan signal. The method achieves high accuracy in depth and lateral range locations, but the accuracy is strongly affected by the noise level. N\u00faez-Nieto et al. [14] presented a method based on neural networks to detect the landmines and unexploded ordnance (UXO) in GPR images and demonstrated that the neural network was superior to logistic regression. However, the method adopts A-scan signals within a window as the features. It also only recognizes the lateral ranges of targets and lacks the depth location of targets. Harkat et al. [15] presented a binary radial basis function (RBF) neural network based on the multiobjective genetic algorithm (MOGA) to detect targets in GPR images. The method uses high-order statistical cumulants of a segmented image with 41*41 pixels as the features and obtains better classification performance than convolutional neural network (CNN) and SVM. However, the MOGA framework is time-consuming due to the large size of the features. Recently, image target detection methods based on depth learning have gradually become more popular in the field of image recognition [16\u201318]. Deep learning methods learn the feature representations directly from the original image instead of traditional manual feature extraction in machine learning methods, which have high robustness in the detection task. As the most popular deep learning network, CNN uses multiple convolutional layers and pooling layers to extract features and uses the fully connected layer and the softmax layer to perform the classification. CNN has been proposed for the interpretation of GPR images such as concealed crack detection in asphalt pavement [19], rebar detection and localization in concrete [20,21], material type and shape classification and soil type classification [22], and internal defect detection in roads [23]. Though deep learning methods have gained increasing attention in GPR target detection, they still have two main limitations. One limitation is that they need a large amount of labeled data to train the deep learning models. Another limitation is that deep learning models have complex network structures, and the training and testing of deep learning models require high-performance graphical processing units (GPUs). Therefore, this research still focuses on machine learning methods. To obtain the locations of underground targets for urban roads, traditional machine learning methods need to recognize each segmented A-scan signal in the B-scan image. However, it is not necessary to identify all segmented signals for classifiers due to the sparse distribution of underground targets in B-scan images. Therefore, reducing the number of segmented signals to be recognized is a potential approach to improve detection efficiency. According to the above analysis, an efficient method based on neural networks is proposed for underground target detection of urban roads in GPR images. The proposed\nRemote Sens. 2023, 15, 1346 3 of 22\nmethod first uses robust principal component analysis (RPCA) to eliminate the clutter in the B-scan image and then divides the target detection into whole recognition and segmentation recognition of A-scan signals in the B-scan image. The whole recognition of A-scan signals is used to obtain the horizontal regions of targets, while the segmentation recognition of A-scan signals is used to obtain the vertical regions of targets, which can decrease the recognition for segmented A-scan signals not from targets. In addition, the fusion and deletion (FAD) algorithm is used to optimize the horizontal regions of targets. The experimental results with simulation and real GPR data demonstrate the effectiveness of the proposed method in underground target detection of urban roads. The remainder of this paper is organized as follows. Section 2 describes the proposed method in detail. Section 3 presents the experimental results and discussion. Section 4 lists the conclusion.\n2. Theory and Method 2.1. Detection Model of GPR\nGPR data may be composed in three different forms: A-scans, B-scans, and C-scans. The A-scan signal is presented in the form of a time-series signal, the B-scan image is constructed by stacking multiple A-scan signals, and the 3D C-scan data cube is formed by stacking multiple B-scan images. This paper mainly researches target detection in B-scan images. A simple detection model of a buried target for GPR [24] is shown in Figure 1. When GPR antennas scan the target along the horizontal axis, the antenna position xn and the time delay tn corresponding to the target approximately satisfy the hyperbolic equation:\nt2n t20 \u2212 4(xn \u2212 x0)\n2\n(vt0) 2 = 1 (1)\nwhere x0 is the horizontal position of the target, t0 is the time delay at the position x0, and v is the wave velocity in the underground medium. Therefore, the target detection in the B-scan image mainly refers to the recognition of hyperbolic patterns.\nRemote Sens. 2023, 14, x FOR PEER REVIEW 3 of 22 ground targets in B-scan images. Therefore, reducing the number of segmented signals to be recognized is a potential approach to improve detection efficiency.\nAccording to the above analysis, an efficient method based on neural networks is proposed for underground target detection of urban roads in GPR images. The proposed method first uses robust principal component analysis (RPCA) to eliminate the clutter in the B-scan image and then divides the target detection into whole recognition and segmentation recognition of A-scan signals in the B-scan image. The whole recognition of A-scan signals is used to obtain the horizontal regions of targets, while the segmentation recognition of A-scan signals is used to obtain the vertical regions of targets, which can decrease the recognition for segmented A-scan signals not from targets. In addition, the fusion and deletion (FAD) algorithm is used to optimize the horizontal regions of targets. The experimental results with simulation and real GPR data demonstrate the effectiveness of the proposed method in underground target detection of urban roads.\nThe remainder of this paper is organized as follows. Section 2 describes the proposed method in detail. Section 3 presents the experimental results and discussion. Section 4 lists th conclusion.\n2. Theory and Method 2.1. Detection Model of GPR\nGPR data may be composed in hree different forms: A-scans, B-scans, and C-scans. The A-scan signal is presented in the form of a time-series signal, the B-scan image is constructed by stacking multiple A-scan signals, and the 3D C-scan data cube is formed by stacking multiple B-scan images. This paper mainly researches target detection in B-scan images.\nA simple detection model of a buried target for GPR [24] is shown in Figure 1. W en GPR a tennas scan the target long th horizontal axis, the antenna position nx and the time delay nt corresponding to the target approximately satisfy the hyperbolic equation:\n2 2 0 2 2 0 0 4( ) 1 ( ) \u2212 \u2212 =n n t x x t vt\n(1)\nwhere 0x is the horizontal position of the target, 0t is the time delay at the position 0x , and v is the wave velocity in the underground medium. Therefore, the target detec-\ntion in the B-scan image mainly refers to the recognition of hyperbolic patterns.\nFigure 1. Detection model of a buried target for GPR.\n2.2. Detection Method A flowchart of the proposed target detection method for GPR images based on\nneural networks is shown in Figure 2. First, the preprocessing stage employs RPCA to suppress clutter in the original B-scan image. Second, three time-domain statistics of each A-scan signal in the image are selected to form the feature vector, and one\n2.2. Detection Method\nA flowchart of the proposed target detection method for GPR images based on neural networks is shown in Figure 2. First, the preprocessing stage employs RPCA to suppress clutter in the original B-scan image. Second, three time-domain statistics of each A-scan signal in the image are selected to form the feature vector, and one back-propagation (BP) neural network is used to recognize the horizontal regions of targets. Third, the recognized horizontal regions are optimized by the FAD algorithm. Finally, the same three statistics of each segmented A-scan signal in horizontal regions are used to for the feature vector, and another BP neural network is used to recognize the vertical regions of targets, and then\nRemote Sens. 2023, 15, 1346 4 of 22\nthe final target regions can be obtained from the horizontal and vertical regions. The main stages of the proposed method are described in detail in the following sections.\nRemote Sens. 2023, 14, x FOR PEER REVIEW 4 of 22\nback-propagation (BP) neural network is used to recognize the horizontal regions of targets. Third, the recognized horizontal regions are optimized by the FAD algorithm. Finally, the same three statistics of each segmented A-scan signal in horizontal regions are used to form the feature vector, and another BP neural network is used to recognize the vertical regions of targets, and then the final target regions can be obtained from the horizontal and vertical regions. The main s ages of the proposed method are described in d tail in the following sections.\n2.2.1. Preprocessing The original GPR B-scan image contains background clutter, target response, and noise. The background clutter can be caused by direct waves from the ground, reflections from the underground layer and non-targets, and multiple reflections from the target, which has a great impact on the target detection in the B-scan image. Therefore, the preprocessing stage mainly involves suppression of background clutter. The two-dimensional GPR B-scan image can be denoted by [ ]1 2, , , \u00d7\u2208 M NNX= x x x R , where M is the number of sampling points in each trace (A-scan) and N is the number of traces. The ith A-scan signal 1\u00d7\u2208 Mix R ( 1,2, , )= i N is composed of the background clutter ia , the target response is , and noise ie . Therefore, the B-scan image can be expressed as [25]\n= + +X A S E (2)\nwhere [ ]1 2, , , NA= a a a is the low-rank clutter matrix, [ ]1 2, , , NS= s s s is the sparse target response matrix, and [ ]1 2, , , NE= e e e is the full-rank noise matrix. The low-rank and sparse property of the B-scan image can be utilized to discriminate the clutter A and the target response S by RPCA decomposition. RPCA is proposed to overcome the drawback of classical principal component analysis (PCA), which is that it is sensitive to outliers. RPCA aims to find a low-rank structure in high dimensional data by solving a convex optimization problem [26]:\n* 1, + A S min A \u03bb S s.t. X=A+S (3)\nwhere * is the nuclear norm of the matrix argument (the sum of its singular values),\n1  is the l1-norm of the matrix (the sum of the absolute values of matrix entries), \u03bb is a positive regularization parameter, and s.t. is the abbreviation of \u201csubject to\u201d. Instead of directly solving the optimization problem in (3), an augmented Lagrangian function is constructed as follows:\n2\n* 1 ( , , , ) , 2 = + + \u2212 \u2212 + \u2212 \u2212 F\n\u03b1L A S Y \u03b1 A \u03bb S Y X A S X A S (4)\nwhere Y is the Lagrange multiplier matrix,  is the inner product, and 0>\u03b1 is the penalty factor. The optimization problem can be solved by minimizing the function\n( , , , )L A S Y \u03b1 with the augmented Lagrange multipliers (ALM) algorithm.\ni r . l rt f t r t .\n2.2.1. Preprocessing\nThe original GPR B-scan image contains background clutter, target response, and noise. The background clutter can be caused by direct waves from the ground, reflections from the underground layer and non-targets, and multiple reflections from the target, which has a great impact on the target detection in the B-scan image. Therefore, the preprocessing stage mainly involves suppression of background clutter. The two-dimensional GPR B-scan image can be denoted by X =[x1, x2, \u00b7 \u00b7 \u00b7 , xN ] \u2208 RM\u00d7N, where M is the number of sampling points in each trace (A-scan) and N is the number of traces. The ith A-scan signal xi \u2208 RM\u00d71(i = 1, 2, \u00b7 \u00b7 \u00b7 , N) is composed of the background clutter ai, the target response si, and noise ei. Therefore, the B-scan image can be expressed as [25]\nX = A + S + E (2)\nwhere A =[a1, a2, \u00b7 \u00b7 \u00b7 , aN ] is the low-rank clutter matrix, S =[s1, s2, \u00b7 \u00b7 \u00b7 , sN ] is the sparse target response matrix, and E =[e1, e2, \u00b7 \u00b7 \u00b7 , eN ] is the full-rank noise matrix. The low-rank and sparse property of the B-scan image can be utilized to discriminate the clutter A and the target response S by RPCA decomposition. RPCA is proposed to overcome the drawback of classical principal component analysis (PCA), which is that it is sensitive to outliers. RPCA aims to find a low-rank structure in high dimensional data by solving a convex optimization problem [26]:\nmin A,S \u2016A\u2016\u2217 + \u03bb\u2016S\u20161 s.t. X = A + S (3)\nwhere \u2016 \u2022 \u2016\u2217 is the nuclear norm of the matrix argument (the sum of its singular values), \u2016 \u2022 \u20161 is the l1-norm of the matrix (the sum of the absolute values of matrix entries), \u03bb is a positive regularization parameter, and s.t. is the abbreviation of \u201csubject to\u201d. Instead of directly solving the optimization problem in (3), an augment d Lagrangian function is constructed as follows:\nL(A, S, Y, \u03b1) = \u2016A\u2016\u2217 + \u03bb\u2016S\u20161 + \u3008Y, X\u2212 A\u2212 S\u3009+ \u03b1 2 \u2016X\u2212 A\u2212 S\u20162F (4)\nwhere Y is the Lagrange multiplier matrix, \u3008\u2022\u3009 is the inner product, and \u03b1 > 0 is the penalty factor. The optimization problem can be solved by minimizing the function L(A, S, Y, \u03b1) with the augmented Lagrange multipliers (ALM) algorithm. After RPCA decomposition, the sparse matrix S is used as the target image, and the low-rank mat ix A is removed as the clutter.\n2.2.2. Feature Extraction of A-Scan Signals\nFeature extraction is a crucial aspect of target recognition. Here, the features extracted from the A-scan signal are used to identify the horizontal regions of targets. Features based on the A-scan signal include frequency-domain spectral features, wavelet-domain features, and time-domain features. Compared with the other types of features, time-domain feature extraction from the A-scan signal requires less computation. Time-domain features can be\nRemote Sens. 2023, 15, 1346 5 of 22\nextracted from the original signal, as well as from various transformations of the original signal, such as absolute value, envelope of Hilbert transform, and the short-term average to long-term average (STA/LTA) ratio [27]. The three transformations of the original signal can strengthen the description for the variation in signal energy but weaken the description for the variation in signal oscillation. For GPR, target reflections and non-target reflections can be better distinguished from the variation in signal oscillation. Therefore, considering the feature representation capability of signal oscillation and the number of features, three time domain statistics of the original A-scan signal are selected as the features, which are expressed as follows:\n(1) Mean absolute deviation\nMADi = 1 M\nM\n\u2211 m=1 |xi(m)\u2212 xi| (5)\n(2) Standard deviation\nSTDi = \u221a\u221a\u221a\u221a 1 M M\n\u2211 m=1\n(xi(m)\u2212 xi)2 (6)\n(3) Fourth root of the fourth moment\nFRFMi = 4 \u221a E(xi \u2212 xi)4 (7)\nwhere xi(m) is the mth element in the ith A-scan signal xi, xi is the mean value of xi, and E(\u2022) is the expected value function.\n2.2.3. Target Horizontal Region Recognition\nTarget horizontal region recognition aims to obtain lateral ranges of targets by classifying A-scan signals. Here, the neural network is chosen as the classifier because it has high robustness to noise and is available for recognizing target reflections in A-scan signals [28]. The neural network is an information processing technology similar to the human nervous system, which can solve complex problems through nonlinear mapping [29]. The neural network consists of many basic computing neurons that are mesh-connected to each other for learning. A training algorithm is used to update the weights and biases until the actual output of the multi-layer perceptron (MLP) overlaps the desired output. The neural network used here is a standard three-layer feedforward network trained with the backpropagation (BP) algorithm. The architecture of the BP neutral network is shown in Figure 3. The input layer consists of three neurons (one neuron per feature), and the output layer contains two neurons (target reflection or non-target reflection). The number of neurons in the hidden layer is basically of the same order of magnitude as the mean term of the proportion between the number of neurons in the input and output layers [30]. By means of the least-squares method, the number of neurons in the hidden layer is determined approximately by the empirical formula:\nl < \u221a (p + q) + a (8)\nwhere p is the number of neurons in the input layer, q is the number of neurons in the output layer, and a is constant with 0 < a < 10. Here, the number of neurons in the hidden layer is set to 10 according to several simulations, which can provide high accuracy and low computational complexity.\nRemote Sens. 2023, 15, 1346 6 of 22\nRemote Sens. 2023, 14, x FOR PEER REVIEW 6 of 22 ( )< + +l p q a (8) where p is the number of neurons in the input layer, q is the number of neurons in the output layer, and a is constant with 0 < a < 10. Here, the number of neurons in the hidden layer is set to 10 according to several simulations, which can provide high accuracy and low computational complexity. In the BP neural network, the output of the jth neuron in the kth layer is given by 1 , 1, , 1 \u2212 \u2212 =   = +     kL k j ij k i k j i y f w y b (9) where  kj=1,2, ,L and Lk is the number of neurons in the kth layer, ( )f is the activation function, ijw is the weight, and ,k jb is the bias.\nThe activation functions mainly include the logsig function, tansig function, and purelin function. The first two are nonlinear functions, and the latter is a linear function. Here, the hidden layer adopts the logsig function and the output layer adopts the purelin function.\nFigure 3. BP neural network architecture.\nIn the BP algorithm, the gradient of the error function with respect to each weight is computed, and the weights are adjusted along the downhill direction of the gradient in order to reduce the error. Generally, such a learning scheme is slow, so a momentum term is introduced to increase the convergence rate [31]. The weight adjustment with a momentum term can be expressed as follows:\n( )( ) ( 1) ( ) \u2202\u0394 = \u2212 + \u0394 \u2212 \u2202ij ijij E ww n \u03b7 \u03b1 w n w n\n(10)\nwhere n is the index of iterations, \u03b7 is the learning rate, ( ) ( ) \u2202 \u2202 ij E w w n is the gradient of the\nerror function with respect to the weight, and\u03b1 is the momentum factor. Based on the BP neural network designed above, the steps of target horizontal re-\ngions recognition are as follows: 1. Training set construction. Km1 A-scan signals with target reflections and kn1\nA-scan signals without target reflections are selected from the B-scan images for training, and three features of each selected A-scan signal are extracted. Then, the features are normalized to construct the training set TR1, including km1 positive samples and kn1 negative samples. The output of the positive sample is set to [1 0], and the output of the negative sample is set to [0 1].\n2. Network training. The training set TR1 is used to train the designed BP neural network, and the network model NET1 is obtained.\nIn the BP neural network, the output of the jth neuron in the kth layer is given by\nyk, j = f\n( Lk\u22121\n\u2211 i=1 wijyk\u22121, i + bk, j\n) (9)\nwhe e j = 1, 2, \u00b7 \u00b7 \u00b7 , Lk and Lk is the number of neurons in t e kth layer, f (\u2022) is the activation function, wij is the weight, and bk, j is the bias. The activation functions mainly include the logsig function, tansig function, and pureli f nction. The first two are nonlinear functions, and the latter is a linear function. Here, the hidden layer adopts the logsig function and the output layer adopts the purelin function. In the BP algorithm, the gradient of the error function with respect to each weight is computed, and the weights are adjusted along the downhill direction of the gradient in order to reduce the error. Generally, such a learning scheme is slow, so a momentum term is introduced to increase the convergence rate [31]. The weight adjustment with a momentum term ca be expressed as follows:\n\u2206wij(n) = \u2212\u03b7 \u2202E(w) \u2202wij(n) + \u03b1\u2206wij(n\u2212 1) (10)\nwhere n is the index of iterations, \u03b7 is the learning rate, \u2202E(w) \u2202wij(n) is the gradient of the error function with respect to the weight, and \u03b1 is the momentum factor. Based on the BP neural network designed above, the steps of target horizontal regions recognition are as follows:\n1. Training set construction. Km1 A-scan signals with target reflections and kn1 A-scan signals without target reflections are selected from the B-scan images for training, and three features of each selected A-scan signal are extracted. Then, the features are normalized to construct the training set TR1, including km1 positive samples and kn1 negative samples. The output of the positive sample is set to [1 0], and the output of the negative sample is set to [0 1]. 2. Network training. The training set TR1 is used to train the designed BP neural network, and the network model NET1 is obtained. 3. Horizontal region recognition. The three features of all A-scan signals in the test B-scan image are extracted and normalized to construct the test set TE1. Then, the model NET1 is used to classify the samples in TE1. Assuming that K1 samples are identified as positive samples, the corresponding A-scan signals can be written as xi (i = i1, i2, \u00b7 \u00b7 \u00b7 , iK1). Then, the target horizontal regions can be denoted as H1 = {ik\u2206d, 1 \u2264 k \u2264 K1}, where \u2206d is the trace interval.\nRemote Sens. 2023, 15, 1346 7 of 22\n2.2.4. Optimization of Target Horizontal Regions\nDue to the influence of residual clutter and target reflection amplitude fluctuation, false detection and missing detection are inevitable in target horizontal region recognition. In general, the projection of the target reflection hyperbola in the horizontal direction should be continuous and have a certain width. Based on the characteristics of target reflections, a fusion and deletion (FAD) algorithm is proposed to optimize the target horizontal regions. The illustration of target horizontal region recognition is shown in Figure 4. The recognized target A-scan signals are marked in black, and the recognized non-target A-scan signals are marked in white. For the target horizontal regions H1 = {ik\u2206d, 1 \u2264 k \u2264 K1}, the steps of the FAD algorithm are described as follows:\n1. Fusion processing. Fusion processing refers to further judgment for non-target A-scan signals between two adjacent target regions, which aims to reduce the false negative rate (missing detection). The judgment can be expressed as{\nxi is the target signal, 1 < ik+1 \u2212 ik \u2264 dth/\u2206d xi is the non\u2212 target signal, else\n(11)\nwhere xi (i k+1 \u2264 i \u2264 ik+1\u22121) is the non-target A-scan signal between two discontinuous target A-scan signals xik and xik+1 in H1 and dth is the horizontal interval threshold. Through the processing, the two adjacent target regions with intervals less than dth will be fused together. Then, the horizontal region after fusion processing can be described as H2 = {ik\u2206d, 1 \u2264 k \u2264 K2}, where K2 \u2265 K1.\n2. Deletion processing. Deletion processing further judges the target horizontal regions after fusion processing, which aims to decrease the false positive rate (false detection). The judgment can be represented as{\nxi is the non\u2212 target signal, ik \u2212 i1 = k\u2212 1 \u2264 wth/\u2206d xi is the target signal, else\n(12)\nwhere xi (i 1 \u2264 i \u2264 ik) is the continuous target A-scan signal in one target region in H2 and wth is the horizontal width threshold. Through the processing, the target region with width less than wth in H2 will be deleted. If ik= i1, xik is an isolated target signal, and the location point ik will also be deleted. Then, the final optimized horizontal regions after fusion and deletion processing can be denoted as H3 = {ik\u2206d, 1 \u2264 k \u2264 K3}, where K3 \u2264 K2. Remote Sens. 2023, 14, x FOR PEER REVIEW 8 of 22\nFigure 4. Illustration of target horizontal region recognition.\n2.2.5. Feature Extraction of Segmented A-Scan Signals After the target horizontal regions are obtained from the recognition of A-scan sig-\nnals, the vertical regions of targets need to be further determined from the recognition of segmented A-scan signals in the horizontal regions H3. The segmentation of one A-scan signal is shown in Figure 5. Assuming that the segment length is ml and the time-sampling interval is \u0394t , the corresponding number of segments in one A-scan signal can be written as:\n =     MKL floor ml\n(13)\nwhere ( )floor is the round down function. The basic selection principle of segment length ml is that the length should be as short as possible and the segment can contain enough information about the target reflection. Generally, the range of ml is\n,    \u0394 \u0394 c c 0.5 1.5 f t f t , where cf is the antenna central frequency.\nThen, the segmented A-scan signal can be expressed as\n( ), ( ) ( 1)i r ixs j x r ml j= \u2212 + (14) where 1 2 3, , ,=  Ki i i i , 1,2, ,= r KL , and 1, 2, ,= j ml . Similar to feature extraction of the A-scan signal, the same three time domain statistics of the segmented A-scan signal are chosen as the features, which are expressed as:\n(1) Mean absolute deviation\n, , , 1 1 ( ) = = \u2212 ml i r i r i r j MAD xs j xs ml\n(15)\n(2) Standard deviation\n( )2, , , 1 1 ( ) = \u2212 ml i r i r i r j STD = xs j xs ml\n(16)\n(3) Fourth root of the fourth moment\n44 , , ,( )= \u2212i r i r i rFRFM E xs xs (17)\nwhere ,i rxs is the mean value of ,i rxs .\nRemote Sens. 2023, 15, 1346 8 of 22\n2.2.5. Feature Extraction of Segmented A-Scan Signals\nAfter the target horizontal regions are obtained from the recognition of A-scan signals, the vertical regions of targets need to be further determined from the recognition of segmented A-scan signals in the horizontal regions H3. The segmentation of one Ascan signal is shown in Figure 5. Assuming that the segment length is ml and the timesampling interval is \u2206t, the corresponding number of segments in one A-scan signal can be written as:\nKL = f loor (\nM ml\n) (13)\nwhere f loor(\u2022) is the round down function. The basic selection principle of segment length ml is that the length should be as short as possible and the segment can contain enough information about the target reflection. Generally, the range of ml is [ 0.5 fc\u2206t , 1.5fc\u2206t ] , where fc is the antenna central frequency. Remote Sens. 2023, 14, x FOR PEER REVIEW 9 of 22\n2.2.6. Target Vertical Region Recognition After feature extraction from all segmented A-scan signals, another BP neural net-\nwork is used as the classifier to recognize each segment to obtain the vertical regions of targets. The BP neural network also consists of three layers. The input layer contains three neurons, the hidden layer contains ten neurons, and the output layer contains two neurons.\nThe procedure of target vertical region recognition based on the BP neural network is summarized as follows: 1. Training set construction. Km2 segments with target reflections and kn2 segments\nwithout target reflections are selected from the segmented A-scan signals for training, and three features of each selected segment are extracted. Then, the features are normalized to construct the training set TR2, including km2 positive samples and kn2 negative samples. The output of the positive sample is set to [1 0], and the output of the negative sample is set to [0 1].\n2. Network training. The training set TR2 is used to train the BP neural network, and the network model NET2 is obtained.\n3. Vertical region recognition. The three features of all segments in the optimized horizontal regions H3 of the test B-scan image are extracted and normalized to form the test set TE2. Then, the model NET2 is used to classify the samples in TE2. Assuming that iJ samples are identified as positive samples in the A-scan signal\n( )1 2 3, , ,= i Kx i i i i , the corresponding segments can be written as ( ), 1 2, , ,=  ii r Jxs r r r r . Then, the target vertical regions in the A-scan signal ix can be\ndenoted as ( ){ }1 ( 1) 1 1i p p iV r ml t, r ml t , p J = \u2212 + \u0394 \u0394 \u2264 \u2264  . 4. The recognized segments are arranged in the two-dimensional image, and then the\nfinal target regions can be obtained.\n3. Results To evaluate the proposed target detection method, both numerical simulations and\nfield experiments are carried out. The simulation data are generated using the \u201cgprMax\u201d simulator based on the finite-difference time-domain (FDTD) method [32], and the real field data are obtained from a road evaluation using a commercial impulse GPR system with a central frequency of 400 MHz. All the programs are executed on a 3.60-GHz CPU and 16 GB-memory computer.\nxsi ,r(j) = xi((r\u2212 1) \u2022ml + j) (14)\nwhere i = i1, i2, \u00b7 \u00b7 \u00b7 , iK3, r = 1, 2, \u00b7 \u00b7 \u00b7 , KL, and j = 1, 2, \u00b7 \u00b7 \u00b7 , ml. Similar to feature extraction of the A-scan signal, the same three time domain statistics of the segmented A-scan signal are chosen as the features, which are expressed as:\n(1) Mean absolute deviation\nMADi,r = 1\nml\nml \u2211 j=1 |xsi,r(j)\u2212 _ xsi,r| (15)\n(2) Standard deviation\nSTDi,r = \u221a\u221a\u221a\u221a 1 ml ml\n\u2211 j=1\n(xsi,r(j)\u2212 _ xsi,r) 2 (16)\nRemote Sens. 2023, 15, 1346 9 of 22\n(3) Fourth root of the fourth moment\nFRFMi,r = 4 \u221a E(xsi,r \u2212 _ xsi,r) 4 (17)\nwhere _\nxsi,r is the mean value of xsi,r.\n2.2.6. Target Vertical Region Recognition\nAfter feature extraction from all segmented A-scan signals, another BP neural network is used as the classifier to recognize each segment to obtain the vertical regions of targets. The BP neural network also consists of three layers. The input layer contains three neurons, the hidden layer contains ten neurons, and the output layer contains two neurons. The procedure of target vertical region recognition based on the BP neural network is summarized as follows:\n1. Training set construction. Km2 segments with target reflections and kn2 segments without target reflections are selected from the segmented A-scan signals for training, and three features of each selected segment are extracted. Then, the features are normalized to construct the training set TR2, including km2 positive samples and kn2 negative samples. The output of the positive sample is set to [1 0], and the output of the negative sample is set to [0 1]. 2. Network training. The training set TR2 is used to train the BP neural network, and the network model NET2 is obtained. 3. Vertical region recognition. The three features of all segments in the optimized horizontal regions H3 of the test B-scan image are extracted and normalized to form the test set TE2. Then, the model NET2 is used to classify the samples in TE2. Assuming that Ji samples are identified as positive samples in the A-scan signal xi (i = i1, i2, \u00b7 \u00b7 \u00b7 , iK3), the corresponding segments can be written as xsi ,r ( r = r1, r2, \u00b7 \u00b7 \u00b7 , rJi ) . Then, the tar-\nget vertical regions in the A-scan signal xi can be denoted as V1i = {[( (rp \u2212 1)ml + 1 ) \u2206t, rpml\u2206t ] , 1 \u2264 p \u2264 Ji } .\n4. The recognized segments are arranged in the two-dimensional image, and then the final target regions can be obtained.\n3. Results\nTo evaluate the proposed target detection method, both numerical simulations and field experiments are carried out. The simulation data are generated using the \u201cgprMax\u201d simulator based on the finite-difference time-domain (FDTD) method [32], and the real field data are obtained from a road evaluation using a commercial impulse GPR system with a central frequency of 400 MHz. All the programs are executed on a 3.60-GHz CPU and 16 GB-memory computer.\n3.1. Numerical Simulations\nFigure 6 shows the geometry of the simulation model. The model has a depth of 1.6 m and a width of 12.0 m. The model consists of three layers: 0.1 m thick air (\u03b5r = 1, \u03c3 = 0), 0.2 m thick concrete (\u03b5r = 6, \u03c3 = 0.01), and 1.3 m thick soil (\u03b5r = 8, \u03c3 = 0.003). The model contains three targets: one circular void, one metal pipe (\u03b5r = 300, \u03c3 = 108), and one PVC pipe (\u03b5r = 3, \u03c3 = 0.01). The radius of the void is 0.1 m, the outer radius of the two pipes is 0.1 m, and the inner radius of the two pipes is 0.05 m. The inside of two pipes is filled with water (\u03b5r = 81, \u03c3 = 0.001). The three targets are buried at the same depth of 0.4 m. The lateral distances of the three targets are 3.0 m, 6.0 m, and 9.0 m, respectively.\nRemote Sens. 2023, 15, 1346 10 of 22\nRemote Sens. 2023, 14, x FOR PEER REVIEW 10 of 22 3.1. Numerical Simulations Figure 6 shows the geometry of the simulation model. The model has a depth of 1.6 m and a width of 12.0 m. The model consists of three layers: 0.1 m thick air ( 1=r\u03b5 , 0\u03c3 = ), 0.2 m thick concrete ( 6r\u03b5 = , 0.01\u03c3 = ), and 1.3 m thick soil ( 8r\u03b5 = , 0.003\u03c3 = ). The model contains three targets: one circular void, one metal pipe ( 300=r\u03b5 ,\n810=\u03c3 ), and one PVC pipe ( 3=r\u03b5 , 0.01=\u03c3 ). The radius of the void is 0.1 m, the outer radius of the two pipes is 0.1 m, and the inner radius of the two pipes is 0.05 m. The inside of two pipes is filled with water ( 81=r\u03b5 , 0.001=\u03c3 ). The three targets are buried at the same depth of 0.4 m. The lateral distances of the three targets are 3.0 m, 6.0 m, and 9.0 m, respectively.\nDistance (m)\nD ep\nth (m\n)\n2 4 6 8 10 12\n0.5\n1\n1.5\nVoid Metal pipe PVC pipe\nConcrete Air\nSoil\nFigure 6. Geometry of the simulation model.\nThe parameters of FDTD simulation are listed in Table 1. To verify the performance of RPCA, noise and clutter are added to the original GPR image. First, Gaussian white noise is added to the original GPR image. Then, the low-frequency components of three A-scan signals with target reflections are added to all A-scan signals as horizontal clutter. Finally, five small regions containing target reflections extracted from the image are added to different positions in the image as the point clutter. Figure 7 shows the simu-\nlated GPR image.\nTable 1. Parameters of FDTD simulation.\nParameter Value Antenna central frequency 400 MHz\nExcitation waveform Ricker wavelet Time window 20 ns\nNumber of time samples 848 Trace interval 0.02 m\nNumber of traces 590\nThe signal-to-clutter ratio (SCR) [33] is used to measure the quality of the GPR image, which is defined as\n( )\n( )\n2\n2 \u2208\n\u2208\n= \n t\nc\nc p R\nt p R\nN I p SCR\nN I p (18)\nwhere ( )I p is the p-th pixel in the image, tR is the target region, cR is the clutter re-\ngion, and cN and tN are the number of pixels in the clutter and target regions, respec-\nFigure 6. Geometry of the si ulation model.\nThe parameters of FDTD simulation are listed in Table 1. To verify the performance of RPCA, noise and clutter are added to the original GPR image. First, Gaussian white noise is added to the original GPR image. Then, the low-frequency component of three A-scan signals with target reflections are added to all A-scan signals as horizontal clutter. Finally, five small regions containing target reflections extracted from the image are added to different positions in the image as the point clutter. Figure 7 shows the simulated GPR image.\nTable 1. Parameters of FDTD simulation.\nParameter Value\nAntenna central frequency 400 MHz Excitation waveform Ricker wavelet\nTime window 20 ns Number of time samples 848\nTrace interval 0.02 m Number of traces 590\nRemote Sens. 2023, 14, x FOR PEER REVIEW 11 of 22\ntively. The target region is indicated by a box containing the target, and the clutter re-\ngion is defined as the entire i age excluding the target region.\nAs shown in Figure 7, the strong direct wave almost masks the effective reflection signals of targets, and th SCR values of the original image and the image with clutter and noise are \u221210.3 dB and \u221212.5 dB, respectively. Because the ratio of the direct wave energy to total im ge energy is very large, the difference in SCR between the two images is not obvious.\nTi m\ne (n\ns)\n(a) (b)\nFigure 7. Simulated GPR images: (a) original image; (b) image with clutters and noise.\nFirst, RPCA is applied to the simulated image with clutters and noise, and the experimental results of RPCA are compared with those of PCA. Figure 8 shows the clutter suppression results of the two methods. As shown in Figure 8a, PCA eliminates the direct wave, but it retains the point clutter, some horizontal clutter, and some noise. As shown in Figure 8b, RPCA can remove the direct wave and horizontal clutter completely, but it also retains the point clutter and some noise. The SCR of PCA and RPCA is 4.4 dB and 6.6 dB, respectively. The results show that RPCA has better clutter suppression performance than PCA, especially for horizontal clutter. Similar to target reflection signals, the point clutters in the image are sparsely distributed, so they cannot be removed by the two methods. The noise still exists in the sparse matrix of RPCA and the target component of PCA after the decomposition, so the two methods are not available for noise suppression. In the simulation model, the main parameters of the underground target include depth, lateral distance, and radius. Nine simulation models are established by setting different target parameters, and nine simulated GPR images are obtained from the nine models. Table 2 lists the parameters of the three targets in the nine models. Then, 400 A-scan signals with target reflections and 400 A-scan signals without target reflections are selected from the nine simulated images after clutter suppression. The three features of the 800 A-scan signals are extracted to construct the training set TR1, in which each sample is a vector with three features. Similarly, the three features of all A-scan signals in the test image in Figure 8b are extracted to construct the test set TE1.\nFigure 7. Simulated GPR images: (a) original image; (b) image with clutters and noise.\nThe signal-to-clutter ratio (SCR) [33] is used to measure the qu lity of the GPR image, which is defined as\nSCR =\nNc \u2211 p\u2208Rt |I(p)|2 Nt \u2211 p\u2208Rc |I(p)|2\n(18)\nRemote Sens. 2023, 15, 1346 11 of 22\nwhere I(p) is the p-th pixel in the image, Rt is the target region, Rc is the clutter region, and Nc and Nt are the number of pixels in the clutter and target regions, respectively. The target region is indicated by a box containing the target, and the clutter region is defined as the entire image excluding the target region. As shown in Figure 7, the strong direct wave almost masks the effective reflection signals of targets, and the SCR values of the original image and the image with clutter and noise are \u221210.3 dB and \u221212.5 dB, respectively. Because the ratio of the direct wave energy to total image energy is very large, the difference in SCR between the two images is not obvious. First, RPCA is applied to the simulated image with clutters and noise, and the experimental results of RPCA are compared with those of PCA. Figure 8 shows the clutter suppression results of the two methods. As shown in Figure 8a, PCA eliminates the direct wave, but it retains the point clutter, some horizontal clutter, and some noise. As shown in Figure 8b, RPCA can remove the direct wave and horizontal clutter completely, but it also retains the point clutter and some noise. The SCR of PCA and RPCA is 4.4 dB and 6.6 dB, respectively. The results show that RPCA has better clutter suppression performance than PCA, especially for horizontal clutter. Similar to target reflection signals, the point clutters in the image are sparsely distributed, so they cannot be removed by the two methods. The noise still exists in the sparse matrix of RPCA and the target component of PCA after the decomposition, so the two methods are not available for noise suppression. Remote Sens. 2023, 14, x FOR PEER REVIEW 12 of 22\nTable 2. Parameters of the three targets for different simulation models.\nModel Number\nVoid Metal Pipe PVC Pipe\nDepth Lateral Distance Radius Depth Lateral Distance Radius (Outer/Inner) Depth Lateral Distance Radius\n(Outer/Inner) 1 0.30 m 3 m 0.10 m 0.30 m 6 m 0.10 m/0.05 m 0.30 m 9 m 0.10 m/0.05 m 2 0.50 m 3 m 0.10 m 0.50 m 6 m 0.10 m/0.05 m 0.50 m 9 m 0.10 m/0.05 m 3 0.70 m 3 m 0.10 m 0.70 m 6 m 0.10 m/0.05 m 0.70 m 9 m 0.10 m/0.05 m 4 0.35 m 3 m 0.15 m 0.35 m 6 m 0.15 m/0.10 m 0.35 m 9 m 0.15 m/0.10 m 5 0.55 m 3 m 0.15 m 0.55 m 6 m 0.15 m/0.10 m 0.55 m 9 m 0.15 m/0.10 m 6 0.75 m 3 m 0.15 m 0.75 m 6 m 0.15 m/0.10 m 0.75 m 9 m 0.15 m/0.10 m 7 0.40 m 3 m 0.20 m 0.40 m 6 m 0.20 m/0.15 m 0.40 m 9 m 0.20 m/0.15 m 8 0.60 m 3 m 0.20 m 0.60 m 6 m 0.20 m/0.15 m 0.60 m 9 m 0.20 m/0.15 m 9 0.80 m 3 m 0.20 m 0.80 m 6 m 0.20 m/0.15 m 0.80 m 9 m 0.20 m/0.15 m\nFigure 9 shows the three features of all A-scan signals in the test image, which are the samples in test set TE1. As shown in Figure 9, the amplitude of the three features increases significantly in the horizontal regions of three targets, as well as in the horizontal regions of the five-point clutter regions. The results show that the three features can distinguish target reflections and non-target reflections but fail to distinguish target reflections and point clutter reflections.\nIn the simulation model, he main parameters of the underground target include\ndepth, lateral distance, and radius. Nine simulation models are established by setting different target parameters, and nine simulated GPR images are obtained from the nine models. Table 2 lists the p rameters of the three targets in the nine models. Then, 400 A-scan signals with target reflections and 400 A-scan signals without target reflections are selected from the nine simulated images after clutter suppression. The three features of the 800 A-scan signals are extracted to construct the training set TR1, in which each sample is a vector with three features. Similarly, the three features of all A-scan signals in the test image in Figure 8b are extracted to construct the test set TE1.\nRemote Sens. 2023, 15, 1346 12 of 22\nTable 2. Parameters of the three targets for different simulation models.\nModel Number\nVoid Metal Pipe PVC Pipe\nDepth LateralDistance Radius Depth Lateral Distance Radius (Outer/Inner) Depth Lateral Distance Radius (Outer/Inner)\n1 0.30 m 3 m 0.10 m 0.30 m 6 m 0.10 m/0.05 m 0.30 m 9 m 0.10 m/0.05 m 2 0.50 m 3 m 0.10 m 0.50 m 6 m 0.10 m/0.05 m 0.50 m 9 m 0.10 m/0.05 m 3 0.70 m 3 m 0.10 m 0.70 m 6 m 0.10 m/0.05 m 0.70 m 9 m 0.10 m/0.05 m 4 0.35 m 3 m 0.15 m 0.35 m 6 m 0.15 m/0.10 m 0.35 m 9 m 0.15 m/0.10 m 5 0.55 m 3 m 0.15 m 0.55 m 6 m 0.15 m/0.10 m 0.55 m 9 m 0.15 m/0.10 m 6 0.75 m 3 m 0.15 m 0.75 m 6 m 0.15 m/0.10 m 0.75 m 9 m 0.15 m/0.10 m 7 0.40 m 3 m 0.20 m 0.40 m 6 m 0.20 m/0.15 m 0.40 m 9 m 0.20 m/0.15 m 8 0.60 m 3 m 0.20 m 0.60 m 6 m 0.20 m/0.15 m 0.60 m 9 m 0.20 m/0.15 m 9 0.80 m 3 m 0.20 m 0.80 m 6 m 0.20 m/0.15 m 0.80 m 9 m 0.20 m/0.15 m\nFigure 9 shows the three features of all A-scan signals in the test image, which are the samples in test set TE1. As shown in Figure 9, the amplitude of the three features increases significantly in the horizontal regions of three targets, as well as in the horizontal regions of the five-point clutter regions. The results show that the three features can distinguish target reflections and non-target reflections but fail to distinguish target reflections and point clutter reflections.\nRemote\u00a0Sens.\u00a02023,\u00a014,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 12\u00a0 of\u00a0 22\u00a0 \u00a0\n\u00a0\nFigure\u00a08.\u00a0Clutter\u00a0suppression\u00a0results\u00a0of\u00a0two\u00a0methods\u00a0for\u00a0the\u00a0simulated\u00a0GPR\u00a0image:\u00a0(a)\u00a0PCA\u00a0(SCR\u00a0=\u00a0 4.4\u00a0dB);\u00a0(b)\u00a0RPCA\u00a0(SCR\u00a0=\u00a06.6\u00a0dB).\u00a0\nTable\u00a02.\u00a0Parameters\u00a0of\u00a0the\u00a0three\u00a0targets\u00a0for\u00a0different\u00a0simulation\u00a0models.\u00a0\nModel\u00a0 Number\u00a0\nVoid\u00a0 Metal\u00a0Pipe\u00a0 PVC\u00a0Pipe\u00a0\nDepth\u00a0 Lateral\u00a0Distance\u00a0 Radius\u00a0 Depth\u00a0\nLat ral\u00a0\nDistance\u00a0\nRadius\u00a0\n(Outer/Inner)\u00a0 Depth\u00a0\nLateral\u00a0\nDistance\u00a0\nRadius\u00a0 (Outer/Inner)\u00a0\n1\u00a0 0.30\u00a0m\u00a0 3\u00a0m\u00a0 0.10\u00a0m\u00a0 0.30\u00a0m\u00a0 6\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 0.30\u00a0m\u00a0 9\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 2\u00a0 0.50\u00a0m\u00a0 3\u00a0m\u00a0 0.10\u00a0m\u00a0 0.50\u00a0m\u00a0 6\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 0.50\u00a0m\u00a0 9\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 3\u00a0 0.70\u00a0m\u00a0 3\u00a0m\u00a0 0.10\u00a0m\u00a0 0.70\u00a0m\u00a0 6\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 0.70\u00a0m\u00a0 9\u00a0m\u00a0 0.10\u00a0m/0.05\u00a0m\u00a0 4\u00a0 0.35\u00a0 \u00a0 \u00a0 \u00a0 5\u00a0m\u00a0 0.35\u00a0m\u00a0 6\u00a0m\u00a0 0.15\u00a0m/ .10\u00a0m\u00a0 .35\u00a0m\u00a0 9\u00a0 \u00a0 0. 5\u00a0 /0.10\u00a0m\u00a0 5\u00a0 0.55\u00a0m\u00a0 3\u00a0m\u00a0 0.15\u00a0m\u00a0 0.55\u00a0m\u00a0 6\u00a0m\u00a0 0.15\u00a0m/0.10\u00a0m\u00a0 0.55\u00a0m\u00a0 9\u00a0m\u00a0 0.15\u00a0m/0.10\u00a0m\u00a0 6\u00a0 0.75\u00a0 \u00a0 3\u00a0 \u00a0 0.15\u00a0m\u00a0 0.75\u00a0m\u00a0 6\u00a0m\u00a0 0.15\u00a0m/0.10\u00a0m\u00a0 0.75\u00a0m\u00a0 9\u00a0 \u00a0 0.15\u00a0 /0.10\u00a0m\u00a0 7\u00a0 0.40\u00a0 \u00a0 3\u00a0 \u00a0 0.20\u00a0m\u00a0 0.40\u00a0m\u00a0 6\u00a0m\u00a0 0.20\u00a0m/0.15\u00a0m\u00a0 0.40\u00a0m\u00a0 9\u00a0 \u00a0 0.20\u00a0 /0.15\u00a0m\u00a0 8\u00a0 0.60\u00a0m\u00a0 3\u00a0m\u00a0 0.20\u00a0m\u00a0 0.60\u00a0m\u00a0 6\u00a0m\u00a0 0.20\u00a0m/0.15\u00a0m\u00a0 0.60\u00a0m\u00a0 9\u00a0m\u00a0 0.20\u00a0m/0.15\u00a0m\u00a0 \u00a0 0. \u00a0 \u00a0 \u00a0 \u00a0 0\u00a0m\u00a0 0.80\u00a0m\u00a0 6\u00a0m\u00a0 0.20\u00a0m/0.15\u00a0m\u00a0 .80\u00a0m\u00a0 9\u00a0 \u00a0 0. \u00a0 /0.15\u00a0m\u00a0\nFigure\u00a09\u00a0shows\u00a0the\u00a0three\u00a0features\u00a0of\u00a0all\u00a0A\u2010scan\u00a0signals\u00a0in\u00a0the\u00a0test\u00a0image,\u00a0which\u00a0are\u00a0 the\u00a0samples\u00a0in\u00a0test\u00a0set\u00a0TE1.\u00a0As\u00a0shown\u00a0in\u00a0Figure\u00a09,\u00a0the\u00a0amplitude\u00a0of\u00a0the\u00a0three\u00a0features\u00a0in\u2010 creases\u00a0significantly\u00a0in\u00a0the\u00a0horizontal\u00a0regions\u00a0of\u00a0three\u00a0targets,\u00a0as\u00a0well\u00a0as\u00a0in\u00a0the\u00a0horizontal\u00a0 regions\u00a0of\u00a0the\u00a0five\u2010point\u00a0clutter\u00a0regions.\u00a0The\u00a0results\u00a0show\u00a0that\u00a0the\u00a0three\u00a0features\u00a0can\u00a0dis\u2010 tinguish\u00a0target\u00a0reflections\u00a0and\u00a0non\u2010target\u00a0reflections\u00a0but\u00a0fail\u00a0to\u00a0distinguish\u00a0target\u00a0reflec\u2010 tions\u00a0and\u00a0point\u00a0clutter\u00a0reflections.\u00a0 \u00a0\n0 2 4 6 8 10 1210\n12\n14\n16\n18\n20\n22\nDistance (m)\nA m\npl itu\nde\n\u00a0 0 2 4 6 8 10 1210\n15\n20\n25\n30\n35\n40\nDistance (m)\nA m\npl itu\nde\n\u00a0 0 2 4 6 8 10 120\n20\n40\n60\n80\nDistance (m)\nA m\npl itu\nde\n\u00a0 (a)\u00a0 (b)\u00a0 (c)\u00a0\nFigure\u00a09.\u00a0Three\u00a0 features\u00a0of\u00a0all\u00a0A\u2010scan\u00a0signals\u00a0 in\u00a0 the\u00a0 test\u00a0 image:\u00a0 (a)\u00a0mean\u00a0 absolute\u00a0deviation;\u00a0 (b)\u00a0 standard\u00a0deviation;\u00a0(c)\u00a0fourth\u00a0root\u00a0of\u00a0the\u00a0fourth\u00a0moment.\u00a0\n\u00a0 2 4 6 8 10 Distance (m)\n0 5 10 15\n(a)\u00a0 (b)\u00a0\nFigure 9. Three features of all A-scan signals in the test image: (a) mean absolute deviation; (b) standard deviation; (c) fourth root of the fourth moment.\nThe training set TR1 is used to train one BP neural network with the structure of 3-10-2 (number of neurons in the input-layer\u2013hidden-layer\u2013output-layer) to obtain a network model NET1, which is used to recognize the samples in the test set TE1 to obtain the target horizontal regions. Then, the FAD algorithm is used to further optimize the recognized horizontal regions. The horizontal interval threshold dth is determined according to the target region width in the features of A-scan signals in the nine training images, which is generally 1/10 of the average width of the target region. The horizontal width threshold wth is determined according to the clutter region width in the features of A-scan signals in the training images, which is generally twice the maximum width of the clutter region. Here, dth and wth are set to 0.1 m and 0.3 m, respectively. Figure 10 shows the recognized original target horizontal regions and optimized target horizontal regions. As shown in Figure 10a, the original target horizontal regions contain multiple regions. Five-point clutter regions have a width of less than 0.3 m, which represents false detection. In addition, there are several intervals with widths less than 0.1 m on the sides of the three target regions, which represent the missing detection. As shown in Figure 10b, the FAD algorithm can effectively overcome false detection and missing detection and obtain more accurate horizontal regions for the three targets.\nRemote Sens. 2023, 15, 1346 13 of 22\nRemote Sens. 2023, 14, x FOR PEER REVIEW 13 of 22 The training set TR1 is used to train one BP neural network with the structure of 3-10-2 (number of neurons in the input-layer\u2013hidden-layer\u2013output-layer) to obtain a network model NET1, which is used to recognize the samples in the test set TE1 to obtain the target horizontal regions. Then, the FAD algorithm is used to further optimize the recognized horizontal regions. The horizontal interval threshold dth is determined according to the target region width in the features of A-scan signals in the nine training images, which is generally 1/10 of the average width of the target region. The horizontal width threshold wth is determined according to the clutter region width in the features of A-scan signals in the training images, which is generally twice the maximum width of the clutter region. Here, dth and wth are set to 0.1 m and 0.3 m, respectively.\nFigure 10 shows the recognized original target horizontal regions and optimized target horizontal regions. As shown in Figure 10a, the original target horizontal regions contain multiple regions. Five-point clutter regions have a width of less than 0.3 m, which represents false detection. In addition, there are several intervals with widths less than 0.1 m on the sides of the three target regions, which represent the missing detection. As shown in Figure 10b, the FAD algorithm can effectively overcome false detection and missing detection and obtain more accurate horizontal regions for the three targets.\n(a) (b)\nFigure 10. Target horizontal region recognition results for the simulated GPR image: (a) original horizontal regions; (b) optimized horizontal regions.\nHere, three metrics, accuracy, false positive rate (FPR), and false negative rate (FNR) are used to quantitatively measure the horizontal region recognition performance of the proposed method. The three metrics are given by\n+= + + + TP TNAccuracy TP TN FP FN\n(19)\n= + FPFPR FP TN (20)\n= + FNFNR TP FN\n(21)\nwhere TP is the number of targets that are correctly classified, TN is the number of non-targets that are correctly classified, FN is the number of targets that are incorrectly classified, and FP is the number of non-targets that are incorrectly classified. The accuracy indicates the proportion of correctly classified samples to the total samples, which reflects the overall recognition performance. FPR indicates the propor-\nFigure 10. Target horizontal region recognition results for the simulated GPR image: (a) original horizontal regions; (b) optimized horizontal regions.\nHere, three metrics, accuracy, false positive rate (FPR), and false negative rate (FNR) are used to quantitatively measure the horizontal region recognition performance of the proposed method. The three metrics are given by\nAccuracy = TP + TN\nTP + TN + FP + FN (19)\nFPR = FP\nFP + TN (20)\nFNR = FN\nTP + FN (21)\nwhere TP is the number of targets that are correctly classified, TN is the number of nontargets that are correctly classified, FN is the number of targets that are incorrectly classified, and FP is the number of non-targets that are incorrectly classified. The accuracy indicates the proportion of correctly classified samples to the total samples, which reflects the overall recognition performance. FPR indicates the proportion of incorrectly classified non-targets to the total non-targets, which reflects the false-detection performance. FNR indicates the proportion of incorrectly classified targets to the total targets, which reflects the missing detection performance. Figure 11 shows the confusion matrices of target horizontal region recognition before and after optimization with the FAD algorithm. Table 3 shows the three metrics before and after optimization with the FAD algorithm. It can be seen that after optimization, the accuracy is improved by 9.5%, FPR is reduced by 10.8%, and FNR is reduced by 6.1%. The results show that the FAD algorithm can effectively improve the recognition performance of target horizontal regions.\nRemote Sens. 2023, 15, 1346 14 of 22\nRemote Sens. 2023, 14, x FOR PEER REVIEW 14 of 22 tion of incorrectly classified non-targets to the total non-targets, which reflects the false-detection performance. FNR indicates the proportion of incorrectly classified targets to the total targets, which reflects the missing detection performance.\nFigure 11 shows the confusion matrices of target horizontal region recognition before and after optimization with the FAD algorithm. Table 3 shows the three metrics before and after optimization with the FAD algorithm. It can be seen that after optimization, the accuracy is improved by 9.5%, FPR is reduced by 10.8%, and FNR is reduced by 6.1%. The results show that the FAD algorithm can effectively improve the recognition performance of target horizontal regions.\nTable 3. Recognition performance of target horizontal regions for the simulated GPR image.\nMethod Accuracy FPR FNR BP neural network 87.6% 13.8% 8.6%\nBP neural network + FAD 97.1% 3.0% 2.5%\nFinally, 800 previously selected A-scan signals are segmented. Considering the size of the target hyperbola in the vertical direction, the segment length is set to 64. Thus, each A-scan signal is divided into 13 segments, and a total of 10,400 segments are obtained. The three features of the 10,400 segments are extracted to construct the training set TR2. Similarly, all 163 A-scan signals in the optimized horizontal regions are also segmented, and the total number of segments is 2119. The three features of the 2119 segments are extracted to construct the test set TE2.\nFigure 12 shows the three features of segments in one A-scan signal in the horizontal region of the PVC pipe, which are also the samples in the test set TE2. As shown in Figure 12, the amplitude of the three features of the segments with target reflections is higher than that of the segment without target reflections, which shows that the three features are also applicable to the segmented A-scan signals.\nThe training set TR2 is used to train one BP neural network with the structure of 3-10-2 to obtain another network model NET2, which is used to recognize the samples in the test set TE2 to obtain the target vertical regions. The target regions are obtained from\nthe recognized target segments in the vertical regions.\nFor comparison, the traditional segmentation recognition methods based on the BP neural network [9] and SVM [13] are also adopted to process the image in Figure 8b. The method based on the BP neural network uses twelve spectral values of segments as features, and the method based on SVM uses three time-domain statistics of segments as features.\nthe target hyperbola in the vertical direction, the segment length is set to 64. Thus, each\nA-scan signal is divided into 13 segments, and a total of 10,400 segments are obtained. The three features of the 10,400 segments are extracted to construct the training set TR2. Similarly, all 163 A-scan signals in the optimized horizontal regions are also segmented, and the total number of segments is 2119. The three features of the 2119 segments are extracted to construct the test set TE2. Figure 12 shows the three features of segments in one A-scan signal in the horizontal region of the PVC pipe, which are also the samples in the test set TE2. As shown in Figure 12, the amplitude of the three features of the segments with target reflections is higher than that of the segment without target reflections, which s ows that the three features are also applicable to the segmented A-scan signals. Remote\u00a0Sens.\u00a02023,\u00a014,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 15\u00a0 of\u00a0 22\u00a0 \u00a0\n\u00a0\n-100 0 1000\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n\u00a0\n0 20 40 600\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n\u00a0\n0 20 40 600\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n\u00a0\n0 50 1000\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n\u00a0 (a)\u00a0 (b)\u00a0 (c)\u00a0 (d)\u00a0\nFigure\u00a012.\u00a0Three\u00a0features\u00a0of\u00a0segments\u00a0 in\u00a0one\u00a0A\u2212scan\u00a0signal:\u00a0(a)\u00a0A\u2212scan\u00a0signal;\u00a0 (b)\u00a0mean\u00a0absolute\u00a0 deviation;\u00a0(c)\u00a0standard\u00a0deviation;\u00a0(d)\u00a0fourth\u00a0root\u00a0of\u00a0the\u00a0fourth\u00a0moment.\u00a0\nFigure\u00a013\u00a0shows\u00a0 the\u00a0 target\u00a0 recognition\u00a0 results\u00a0of\u00a0 the\u00a0 three\u00a0methods.\u00a0As\u00a0shown\u00a0 in\u00a0 Figure\u00a013a,b,\u00a0some\u00a0point\u00a0clutter\u00a0regions\u00a0are\u00a0 identified\u00a0as\u00a0target\u00a0reflections,\u00a0and\u00a0several\u00a0 clutter\u00a0regions\u00a0near\u00a0the\u00a0hyperbolas\u00a0of\u00a0targets\u00a0are\u00a0also\u00a0identified\u00a0as\u00a0target\u00a0reflections.\u00a0As\u00a0 shown\u00a0in\u00a0Figure\u00a011c,\u00a0the\u00a0hyperbolic\u00a0reflections\u00a0of\u00a0targets\u00a0are\u00a0identified\u00a0clearly,\u00a0and\u00a0only\u00a0 a\u00a0few\u00a0clutter\u00a0regions\u00a0are\u00a0recognized\u00a0as\u00a0target\u00a0reflections.\u00a0The\u00a0results\u00a0show\u00a0that\u00a0the\u00a0pro\u2010 posed\u00a0method\u00a0has\u00a0better\u00a0 target\u00a0recognition\u00a0performance\u00a0 than\u00a0 traditional\u00a0segmentation\u00a0 recognition\u00a0methods.\u00a0\n\u00a0 \u00a0 2 4 6 8 10 Distance (m)\n0\n5\n10\n15\n\u00a0 (a)\u00a0 (b)\u00a0 (c)\u00a0\nFigure\u00a0 13.\u00a0Target\u00a0 recognition\u00a0 results\u00a0 for\u00a0 the\u00a0 simulated\u00a0GPR\u00a0 image:\u00a0 (a)\u00a0 traditional\u00a0 segmentation\u00a0 recognition\u00a0method\u00a0based\u00a0on\u00a0BP\u00a0neural\u00a0network;\u00a0(b)\u00a0traditional\u00a0segmentation\u00a0recognition\u00a0method\u00a0 based\u00a0on\u00a0SVM;\u00a0(c)\u00a0proposed\u00a0method.\u00a0\nFigure\u00a014\u00a0shows\u00a0the\u00a0confusion\u00a0matrices\u00a0of\u00a0the\u00a0three\u00a0methods.\u00a0Table\u00a04\u00a0lists\u00a0the\u00a0three\u00a0 metrics\u00a0of\u00a0the\u00a0three\u00a0methods.\u00a0The\u00a0FPR\u00a0of\u00a0the\u00a0proposed\u00a0method\u00a0is\u00a0much\u00a0lower\u00a0than\u00a0that\u00a0of\u00a0 the\u00a0other\u00a0two\u00a0methods,\u00a0which\u00a0shows\u00a0that\u00a0the\u00a0proposed\u00a0method\u00a0can\u00a0greatly\u00a0reduce\u00a0false\u00a0 detection.\u00a0However,\u00a0the\u00a0FNR\u00a0of\u00a0the\u00a0proposed\u00a0method\u00a0reaches\u00a018.9%,\u00a0which\u00a0indicates\u00a0that\u00a0 the\u00a0proposed\u00a0method\u00a0still\u00a0has\u00a0a\u00a0small\u00a0amount\u00a0of\u00a0missing\u00a0detection\u00a0in\u00a0the\u00a0recognition\u00a0of\u00a0 target\u00a0vertical\u00a0regions.\u00a0Because\u00a0 the\u00a0number\u00a0of\u00a0 target\u00a0segments\u00a0 is\u00a0much\u00a0 lower\u00a0 than\u00a0 the\u00a0 number\u00a0of\u00a0non\u2010target\u00a0segments\u00a0in\u00a0the\u00a0GPR\u00a0image,\u00a0the\u00a0influence\u00a0of\u00a0FNR\u00a0on\u00a0the\u00a0recogni\u2010 tion\u00a0 accuracy\u00a0 is\u00a0much\u00a0 less\u00a0 than\u00a0 that\u00a0 of\u00a0 FPR,\u00a0which\u00a0 also\u00a0 explains\u00a0why\u00a0 the\u00a0 proposed\u00a0 method\u00a0can\u00a0achieve\u00a0higher\u00a0accuracy\u00a0than\u00a0the\u00a0other\u00a0two\u00a0traditional\u00a0methods.\u00a0As\u00a0shown\u00a0in\u00a0 Figure\u00a011c,\u00a0the\u00a0small\u00a0amount\u00a0of\u00a0missing\u00a0target\u00a0reflections\u00a0will\u00a0not\u00a0affect\u00a0the\u00a0final\u00a0detec\u2010 tion\u00a0of\u00a0target\u00a0regions.\u00a0\nFigure 12. Three features f segments in one A\u2212scan signal: (a) A\u2212scan signal; (b) mea absolute deviation; (c) standard deviation; (d) fourth root of the fourth moment.\nThe training set TR2 is used to train one BP neural network with the structure of 3-10-2 to obtain another network model NET2, which is used to recognize the samples in the test set TE2 to obtain the target vertical regions. The target regions are obtained from the r cognized target segments in the vertical regions.\nRemote Sens. 2023, 15, 1346 15 of 22\nFor comparison, the traditional segmentation recognition methods based on the BP\nneural network [9] and SVM [13] are also adopted to process the image in Figure 8b. The\nmethod based on the BP neural network uses twelve spectral values of segments as features,\nand the method based on SVM uses three time-domain statistics of segments as features. Figure 13 shows the target recognition results of the three methods. As shown in Figure 13a,b, some point clutter regions are identified as target reflections, and several clutter regions near the hyperbolas of targets are also identified as target reflections. As shown in Figure 11c, the hyperbolic reflections of targets are identified clearly, and only a few clutter regions are recognized as target reflections. The results show that the proposed method has better target recognition performance than traditional segmentation recognition methods.\nRemote Sens. 2023, 14, x FOR PEER REVIEW 15 of 22 -100 0 1000\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n0 20 40 600\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n0 20 40 600\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n0 50 1000\n5\n10\n15\n20\nAmplitude\nTi m\ne (n\ns)\n(a) (b) (c) (d)\nFigure 12. Th ee features of segments in one A\u2212scan signal: (a) A\u2212scan signal; (b) mean absolute deviation; (c) standard deviation; (d) fou th root of the fourth moment.\nFigure 13 shows the target recognitio res lts of t e t ree ethods. As shown in Figure 13a,b, some point clutter regions are identified as target reflections, and several clutter regions near the hyperbolas of targets are also identified as target reflections. As shown in Figure 11c, the hyperbolic reflections of targets are identified clearly, and only a few clutter regions are recognized as target reflections. The results show that the proposed method has better target recognition performance than traditional segmentation recognition methods.\n2 4 6 8 10\nDistance (m)\n0\n5\n10\n15\n(a) (b) (c)\nFigure 13. Target recognition results for the simulated GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nFigure 14 shows the confusion matrices of the three methods. Table 4 lists the three metrics of the three methods. The FPR of the proposed method is much lower than that of the other two methods, which shows that the proposed method can greatly reduce false detection. However, the FNR of the proposed method reaches 18.9%, which indicates that the proposed method still has a small amount of missing detection in the recognition of target vertical regions. Because the number of target segments is much lower than the number of non-target segments in the GPR image, the influence of FNR on the recognition accuracy is much less than that of FPR, which also explains why the proposed method can achieve higher accuracy than the other two traditional methods. As shown in Figure 11c, the small amount of missing target reflections will not affect the final detection of target regions.\nFigure 13. Target recognition results for the simulated GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nFigure 14 shows the confusion matrices of the three methods. Table 4 lists the three metrics of the three methods. The FPR of the proposed method is much lower than that of the other two methods, which shows that the proposed method can greatly reduce false detection. However, the FNR of the proposed method reaches 18.9%, which indicates that the proposed method still has a small amount of missing detection in the recognition of target vertical regions. Because the number of target segments is much lower than the number of non-target segments in the GPR image, the influence of FNR on the recognition accuracy is much less than that of FPR, which also explains why the proposed method can achieve higher accuracy than the other two traditional methods. As shown in Figure 11c, the small amount of missing target reflections will not affect the final detection of target regions. Remote Sens. 2023, 14, x FOR PEER REVIEW 16 of 22\n300\n319\n55\n6996\nPredicted class\nTr ue\nc la\nss\nTarget Non-target\nTarget\nNon-target\n282\n187\n73\n7128\nPredicted class\nTr ue\nc la\nss\nTarget Non-target\nTarget\nNon-target\n288\n30\n67\n7285\nPredicted class\nTr ue\nc la\nss\nTarget Non-target\nTarget\nNon-target\n(a) (b) (c)\nFigure 14. Confusion matrices of the three methods for the simulated GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nTable 5 lists the processing times of the three methods. The processing time refers to the time consumption of processing the test image, which includes a clutter suppression stage and a target-recognition stage. The clutter suppression time of the three methods is the same. The target recognition time of the traditional two methods consists of the construction and classification time of the test set. The target recognition time of the proposed method consists of the construction and classification time of the test set TE1 (horizontal region recognition time), the horizontal region optimization time, and the construction and classification time of the test set TE2 (vertical region recognition time). Compared with the two traditional methods, the target recognition time and total processing time of the proposed method are reduced by about 30% and 20%, respectively. The results show that the proposed method achieves higher detection efficiency than the two traditional methods.\n3.2. Field Experiments A field experiment is conducted with the GPR system on a road in Wuhan (China). The space sampling step length (trace interval) is 0.05 m. The time window is 90.74 ns, and the number of time samples is 512. Figure 15a shows one original B-scan image\nFigure 14. Confusion matrices of the three methods for the simulated GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nRemote Sens. 2023, 15, 1346 16 of 22\nTable 5 lists the processing times of the three methods. The processing time refers to the time consumption of processing the test image, which includes a clutter suppression stage and a target-recognition stage. The clutter suppression time of the three methods is the same. The target recognition time of the traditional two methods consists of the construction and classification time of the test set. The target recognition time of the proposed method consists of the construction and classification time of the test set TE1 (horizontal region recognition time), the horizontal region optimization time, and the construction and classification time of the test set TE2 (vertical region recognition time). Compared with the two traditional methods, the target recognition time and total processing time of the proposed method are reduced by about 30% and 20%, respectively. The results show that the proposed method achieves higher detection efficiency than the two traditional methods.\nA field experiment is conducted with the GPR system on a road in Wuhan (China). The space sampling step length (trace interval) is 0.05 m. The time window is 90.74 ns, and the number of time samples is 512. Figure 15a shows one original B-scan image containing 551 A-scan signals. The image contains the reflections of two targets, the direct wave, horizontal clutter, and a lot of irregular clutter. First, RPCA is used to suppress the clutter in the original image, and the result is shown in Figure 15b. As shown in Figure 15b, the direct wave is completely removed, most horizontal clutter and irregular clutter is also eliminated, and the reflections of the two targets are well preserved. After clutter suppression, the SCR is improved by 16.9 dB. The result also demonstrates the excellent clutter-suppression performance of RPCA for the real GPR image.\nRemote Sens. 2023, 15, 1346 17 of 22\nRemote Sens. 2023, 14, x FOR PEER REVIEW 17 of 22 containing 551 A-scan signals. The image contains the reflections of two targets, the direct wave, horizontal clutter, and a lot of irregular clutter.\nFirst, RPCA is used to suppress the clutter in the original image, and the result is shown in Figure 15b. As shown in Figure 15b, the direct wave is completely removed, most horizontal clutter and irregular clutter is also eliminated, and the reflections of the two targets are well preserved. After clutter suppression, the SCR is improved by 16.9 dB. The result also demonstrates the excellent clutter-suppression performance of RPCA for the real GPR image.\nThen, 200 A-scan signals with target reflections and 200 A-scan signals without target reflections are selected from 25 real B-can images after clutter suppression. The three features of the 400 A-scan signals are extracted to construct the training set TR1, and the three features of all the A-scan signals in the test image in Figure 15b are extracted to construct the test set TE1. The network model NET1 is obtained by training a neural network with the structure of 3-10-2 using the training set TR1, which is used to recognize the samples in TE1 to obtain the target horizontal regions. Subsequently, the recognized horizontal regions are optimized by the FAD algorithm, and the two parameters dth and wth are set to 0.1 m and 0.4 m, respectively. Figure 16 shows the target horizontal region recognition results. As shown in Figure 16a, the original target horizontal regions contain two target regions with larger width and several clutter regions with smaller width. As shown in Figure 16b, the FAD algorithm eliminates the clutter regions and preserves the two target regions well. Figure 17 shows the confusion matrices of target horizontal region recognition before and after optimization with the FAD algorithm. Table 6 lists the recognition performance of target horizontal regions before and after optimization with the FAD algorithm. It can be seen that after optimization, the accuracy is improved by 7.3% and FPR is reduced by 10%, but FNR remains unchanged. The results again show that the FAD algorithm can effectively improve the accuracy and decrease FPR for recognition of the target horizontal region.\nThen, 200 A-scan signals with target reflections and 200 A-scan signals without target reflections are selected from 25 real B-can images after clutter suppression. The three features of the 400 A-scan signals are extracted to construct the training set TR1, and the three features of all the A-scan signals in the test image in Figure 15b are extracted to construct the test set TE1. The network model NET1 is obtained by training a neural network with the structure of 3-10-2 using the training set TR1, which is used to recognize the samples in TE1 to obtain the target horizontal regions. Subsequently, the recognized horizont l regions are op imized by the FAD alg ithm, nd the two parameters dth and wth are set to 0.1 m and 0.4 m, respectiv ly. Figure 16 shows the target horizontal region recognition results. As shown in Figure 16a, the original target horizontal regions contain two target regions with larger width and several clutter regions with smaller width. As shown in Figure 16b, the FAD algorithm eliminates the clutter regions and preserves the two target regions well. Remote Sens. 2023, 14, x FOR PEER REVIEW 18 of 22\n(a) (b)\nFinally, 400 previously selected A-scan signals are segmented, and the segment length is 16. Thus, each A-scan signal is divided into 32 segments, and a total of 12,800 segments are obtained. The three features of the 12,800 segments are extracted to construct the training set TR2. The A-scan signals in the optimized horizontal regions are segmented, and the three features of the segments are extracted to construct the test set TE2. The network model NET2 is obtained by training another neural network with a structure of 3-10-2 using TR2. The target vertical regions are obtained by using NET2 to recognize the samples in TE2, and the recognized target segments in the vertical regions represent the target regions. Here, the experimental results of the proposed method are also compared with those of traditional segmentation recognition methods. Figure 18 shows the target-recognition results of the three methods. As shown in Figure 18a,b, the two traditional methods generate several wrong misjudgments due to the influence of residual clutter, and the two targets can hardly be identified. As shown in Figure 18c, the proposed method can clearly identify the two targets with few misjudgments. The results also show that the proposed method is superior to traditional segmentation recognition methods.\nRemote Sens. 2023, 15, 1346 18 of 22\nFigure 17 shows the confusion matrices of target horizontal region recognition before and after optimization with the FAD algorithm. Table 6 lists the recognition performance of target horizontal regions before and after optimization with the FAD algorithm. It can be\nseen that after optimization, the accuracy is improved by 7.3% and FPR is reduced by 10%,\nbut FNR remains unchanged. The results again show that the FAD algorithm can effectively\nimprove the accuracy and decrease FPR for recognition of the target horizontal region.\nRemote Sens. 2023, 14, x FOR PEER REVIEW 18 of 22\n(a) (b)\nFigure 16. Target horizontal region recognition results for the real GPR image: (a) original horizontal regions; (b) optimized horizontal regions.\nTable 6. Recognition performance of target horizontal regions for the real GPR image.\nMethod Accuracy FPR FNR BP neural network 87.1% 10.8% 18.4%\nBP neural network + FAD 94.4% 0.8% 18.4%\nFinally, 400 previously selected A-scan signals are segmented, and the segment length is 16. Thus, each A-scan signal is divided into 32 segments, and a total of 12,800 segments are obtained. The three features of the 12,800 segments are extracted to construct the training set TR2. The A-scan signals in the optimized horizontal regions are segmented, and the three features of the segments are extracted to construct the test set TE2. The network model NET2 is obtained by training another neural network with a structure of 3-10-2 using TR2. The target vertical regions are obtained by using NET2 to recognize the samples in TE2, and the recognized target segments in the vertical regions represent the target regions. Here, the experimental results of the proposed method are also compared with those of traditional segmentation recognition methods.\nFigure 18 shows the target-recognition results of the three methods. As shown in Figure 18a,b, the two traditional methods generate several wrong misjudgments due to the influence of residual clutter, and the two targets can hardly be identified. As shown in Figure 18c, the proposed method can clearly identify the two targets with few misjudgments. The results also show that the proposed method is superior to traditional segmentation recognition methods.\nTable 6. Recognition performance of target horizontal regions for the real GPR image.\nMethod Accuracy FPR FNR\nBP neural network 87.1% 10.8% 18.4% BP neural network + FAD 94.4% 0.8% 18.4%\nFinally, 400 previously selected A-scan signals are segmented, and the segment length is 16. Thus, each A-scan signal is divided into 32 segments, and a total of 12,800 segments are obtained. The three features of the 12,800 segments are extracted to construct the training set TR2. The A-scan signals in the optimized horizontal regions are segmented, and the three featur s of the segments are extracted to construct the test set TE2. The network model NET2 is obtained by training another neural network with a structure of 3-10-2 using TR2. The target vertical regions are obtained by using NET2 to recognize the s mples in E2, and th recognized target segments in the vertical region repr sent the target regions. Here, the experimental results of the proposed method are also compared with those of traditional segmentation recognition methods. Figure 18 shows the target-recognition results of the three methods. As shown in Figure 18a,b, the two traditional methods generate several wrong misjudgments due to the influence of residual clutter, and the two targets can hardly be identified. As shown in Figure 18c, the proposed method can clearly identify the two targets with few misjudgments. The results also show that the proposed method is superior to traditional segmentation recognition methods.\nRemote Sens. 2023, 15, 1346 19 of 22 Remote Sens. 2023, 14, x FOR PEER REVIEW 19 of 22\nTi m\ne (n\ns)\n(a) (b) (c)\nFigure 18. Target recognition results for the real GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nFigure 19 shows the confusion matrices of the three methods. Table 7 lists the three metrics of the three methods. The FPR of the proposed method is also much lower than that of the two traditional methods, but the FNR of the proposed method is slightly higher than that of the two traditional methods. The results show that the proposed method greatly reduces the false detection but slightly increases the missing detection, which is also consistent with the recognition results in Figure 19. As FPR is greatly reduced, the proposed method still achieves higher accuracy than the two traditional methods.\n289\n485\n88\n16,770\nPredicted class\nTr ue\nc la\nss\nTarget Non-target\nTarget\nNon-target\n276\n303\n101\n16,952\nPredicted class\nTr ue\nc la\nss\nTarget Non-target\nTarget\nNon-target\n270\n57\n107\n17,198\nPredicted class Tr ue c la ss Target Non-target Target\nNon-target\n(a) (b) (c)\nFigure 19. Confusion matrices of the three methods for the real GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nTable 7. Target recognition performance of the three methods for the real GPR image.\nMethod Accuracy FPR FNR Traditional segmentation recognition method based on BP neural network 96.8% 2.8% 23.3%\nTraditional segmentation recognition method based on SVM 97.7% 1.8% 26.8%\nProposed method 99.1% 0.3% 28.4%\nFigure 18. Target recognition results for the real GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nFigure 19 shows the confusion matrices of the three methods. Table 7 lists the three metrics of the three methods. The FPR of the proposed method is also much lower than that of the two traditional methods, but the FNR of the proposed method is slightly higher than that of the two traditional methods. The results show that the proposed method greatly reduces the false detection but slightly increases the missing detection, which is also consistent with the recognition results in Figure 19. As FPR is greatly reduced, the proposed method still achieves higher accuracy than the two traditional methods.\nRemote Sens. 2023, 14, x FOR PEER REVIEW 19 of 22\nTi m\ne (n\ns)\n(a) (b) (c)\nFigure 18. Target recognition results for the real GPR image: (a) traditional segmentation recognition method bas d o BP neural network; (b) traditional segmentation r cognition m thod based on SVM; (c) proposed method.\nFigure 19 shows the confusion matrices of the three methods. Table 7 lists the three metr cs of the three m th ds. The FPR of the propos d is also much lower than that of the two tradi ional methods, but the FNR of the proposed ethod is slightly higher than th t of the two traditional meth ds. The results show that the proposed method greatly reduces he false detection but lightly increases the missing detection, which is also consistent with the recognition result in Figure 19. As FPR is greatly reduced, th proposed method still achieves high r accuracy than the two tra itional metho s.\n289\n485\n88\n16,770\nPredicted class\nTr ue\nc la\nss\nTarg t Non-target\nTarget\nNon-target\n276\n303\n101\n16,952\nPredicted class\nTr ue\nc la\nss\nTarg t Non-target\nTarget\nNon-target\n270\n57\n107\n17,198\nPredicted class Tr ue c la ss Targ t Non-target Target\nNon-target\n(a) (b) (c)\nFigure 19. Confusion matrices of the three methods for the real GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nTable 7. Target recognition performance of the three methods for the real GPR image.\nMethod Accuracy FPR FNR Traditional segmentation recognition method based on BP neural network 96.8% 2.8% 23.3%\nTraditional segmentation recognition method based on SVM 97.7% 1.8% 26.8%\nProposed method 99.1% 0.3% 28.4%\nFigure 19. Confusion matrices of the three methods for the real GPR image: (a) traditional segmentation recognition method based on BP neural network; (b) traditional segmentation recognition method based on SVM; (c) proposed method.\nTable 7. Target recognition performance of the three methods for the real GPR image.\nMethod Accuracy FPR FNR\nTraditional segmentation\nrecognition method based on BP neural\nnetwork\n96.8% 2.8% 23.3%\nTraditional segmentation\nrecognition method based on SVM\n97.7% 1.8% 26.8%\nProposed method 99.1% 0.3% 28.4%\nTable 8 lists the processing time of the three methods. Compared with the two traditional methods, the target-recognition time and total processing time of the proposed\nRemote Sens. 2023, 15, 1346 20 of 22\nmethod are reduced by about 20% and 14%, respectively. The results also show that the proposed method is more efficient than the two traditional methods.\nThis paper proposes an efficient GPR target-detection method based on horizontal and vertical region recognition using a BP neural network. Preprocessing is executed using RPCA, horizontal region recognition is based on the recognition of A-scan signals, and vertical region recognition is based on the recognition of segments in A-scan signals. The proposed two-stage recognition structure can reduce the recognition of segments of A-scan signals in non-target regions in traditional methods. In order to better describe the reflection characteristics of the target, three time-domain statistics are selected as features. In addition, a simple FAD algorithm is proposed to optimize the horizontal region. A series of simulated and real GPR data was used to verify the proposed method. The results show that PRCA can effectively suppress non-sparse clutter, but it is not suitable for sparse clutter and noise suppression. In the recognition of the simulated image in the horizontal region, the three features can distinguish target regions and nontarget regions but cannot distinguish target regions and point clutter regions. The FAD algorithm can effectively reduce the influence of point clutter and improve the recognition accuracy of horizontal regions. The proposed method is also compared with two traditional segmentation recognition methods. The comparison results show that the proposed method can significantly reduce FPR and improve accuracy but cannot reduce FNR. In addition, the results also show that the proposed method can effectively reduce the processing time.\n5. Conclusions\nIn this paper, an efficient recognition method based on neural networks is proposed to improve the detection performance of underground road targets in GPR images. RPCA is first used to suppress the clutter in the image. Then, one BP neural network is adopted to obtain the horizontal regions of targets by recognizing A-scan signals in the image, and another BP neural network is used to obtain the vertical regions of targets by recognizing segmented A-scan signals in the horizontal regions of targets, which provides a solution to improve the recognition efficiency. Moreover, the FAD algorithm is presented to optimize the horizontal regions of targets. The effectiveness of the proposed method is verified by both simulated and real GPR images. The experimental results show that the proposed method is superior to the two traditional segmentation recognition methods in recognition accuracy and processing time. Future work will study more efficient target recognition solutions to improve GPR detection performance.\nRemote Sens. 2023, 15, 1346 21 of 22\nAuthor Contributions: Conceptualization, W.X. and K.C.; methodology, W.X.; software, K.C.; validation, K.C. and T.L.; formal analysis, W.X. and J.Z.; data curation, K.C.; writing\u2014original draft preparation, K.C.; writing\u2014review and editing, W.X. and J.Z.; funding acquisition, L.L. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was partially supported by the National Natural Science Foundation of China (62175220).\nData Availability Statement: The data presented in the study are available on request from the first author.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Saarenketo, T.; Scullion, T. Road evaluation with ground penetrating radar. J. Appl. Geophys. 2000, 43, 119\u2013138. [CrossRef] 2. Dong, Z.; Ye, S.; Gao, Y.; Fang, G.; Zhang, X.; Xue, Z.; Zhang, T. Rapid Detection Methods for Asphalt Pavement Thicknesses and Defects by a Vehicle-Mounted Ground Penetrating Radar (GPR) System. Sensors 2016, 16, 2067. [CrossRef] [PubMed] 3. Lai, W.W.; D\u00e9robert, X.; Annan, P. A review of ground penetrating radar application in civil engineering: A 30-year journey from locating and testing to imaging and diagnosis. NDT E Int. 2018, 96, 58\u201378. 4. Park, B.; Kim, J.; Lee, J.; Kang, M.S.; An, Y.K. Underground Object Classification for Urban Roads Using Instantaneous Phase Analysis of Ground-Penetrating Radar (GPR) Data. Remote Sens. 2018, 10, 1417. [CrossRef] 5. Zhang, J.; Yang, X.; Li, W.; Zhang, S.; Jia, Y. Automatic detection of moisture damages in asphalt pavements from GPR data with deep CNN and IRS method. Autom. Constr. 2020, 113, 103119. [CrossRef] 6. Rasol, M.; Pais, J.C.; P\u00e9rez-Gracia, V.; Solla, M.; Fernandes, F.M.; Fontul, S.; Ayala-Cabrera, D.; Schmidt, F.; Assadollahi, H. GPR\nmonitoring for road transport infrastructure: A systematic review and machine learning insights. Constr. Build. Mater. 2022, 324, 126686. [CrossRef]\n7. Gao, Y.; Pei, L.; Wang, S.; Li, W. Intelligent Detection of Urban Road Underground Targets by Using Ground Penetrating Radar based on Deep Learning. J. Phys. Conf. Ser. 2021, 1757, 012081. [CrossRef] 8. Shao, W.; Bouzerdoum, A.; Phung, S.L.; Su, L.; Indraratna, B.; Rujikiatkamjorn, C. Automatic Classification of Ground-PenetratingRadar Signals for Railway-Ballast Assessment. IEEE Trans. Geosci. Remote Sens. 2011, 49, 3961\u20133972. [CrossRef] 9. Al-Nuaimy, W.; Huang, Y.; Nakhkash, M.; Fang, M.T.C.; Nguyen, V.T.; Eriksen, A. Automatic detection of buried utilities and solid objects with GPR using neural networks and pattern recognition. J. Appl. Geophys. 2000, 43, 157\u2013165. [CrossRef] 10. Wu, J.; Mao, T.; Zhou, H. Feature Extraction and Recognition Based on SVM. In Proceedings of the 2008 4th International Conference on Wireless Communications, Networking and Mobile Computing (WiCOM), Dalian, China, 12\u201314 October 2008. 11. Frigui, H.; Gader, P. Detection and discrimination of land mines in ground-penetrating radar based on edge histogram descriptors and a possibilistic K-nearest neighbor classifier. IEEE Trans. Fuzzy Syst. 2009, 17, 185\u2013199. [CrossRef] 12. Torrione, P.; Morton, K.D.; Sakaguchi, R.; Collins, L. Histograms of oriented gradients for landmine detection in groundpenetrating radar data. IEEE Trans. Geosci. Remote Sens. 2014, 52, 1539\u20131550. [CrossRef] 13. Xie, X.; Qin, H.; Yu, C.; Liu, L. An automatic recognition algorithm for GPR images of RC structure voids. J. Appl. Geophys. 2013, 99, 125\u2013134. [CrossRef] 14. N\u00fa\u00f1ez-Nieto, X.; Solla, M.; G\u00f3mez-P\u00e9rez, P.; Lorenzo, H. GPR Signal Characterization for Automated Landmine and UXO Detection Based on Machine Learning Techniques. Remote Sens. 2014, 6, 9729\u20139748. [CrossRef] 15. Harkat, H.; Ruano, A.E.; Ruano, M.G.; Bennani, S.D. GPR target detection using a neural network classifier designed by a multi-objective genetic algorithm. Appl. Soft Comput. 2019, 79, 310\u2013325. [CrossRef] 16. Tong, Z.; Gao, J.; Zhang, H. Innovative method for recognizing subgrade defects based on a convolutional neural network. Constr. Build. Mater. 2018, 169, 69\u201382. [CrossRef] 17. Xue, W.; Dai, X.; Liu, L. Remote Sensing Scene Classification Based on Multi-Structure Deep Features Fusion. IEEE Access 2020, 8, 28746\u201328755. [CrossRef] 18. Huang, W.; Zhao, Z.; Sun, L.; Ju, M. Dual-Branch Attention-Assisted CNN for Hyperspectral Image Classification. Remote Sens. 2022, 14, 6158. [CrossRef] 19. Tong, Z.; Gao, J.; Zhang, H. Recognition, location, measurement, and 3D reconstruction of concealed cracks using convolutional neural networks. Constr. Build. Mater. 2017, 146, 775\u2013787. [CrossRef] 20. Dinh, K.; Gucunski, N.; Duong, T.H. An algorithm for automatic localization and detection of rebars from GPR data of concrete bridge decks. Autom. Constr. 2018, 89, 292\u2013298. [CrossRef] 21. Liu, H.; Lin, C.; Cui, J.; Fan, L.; Xie, X.; Spencer, B.F. Detection and localization of rebar in concrete by deep learning using ground penetrating radar. Autom. Constr. 2020, 118, 103279. [CrossRef] 22. Ozkaya, U.; Melgani, F.; Bejiga, M.B.; Seyfi, L.; Donelli, M. GPR B Scan Image Analysis with Deep Learning Methods. Measurement 2020, 165, 107770. [CrossRef] 23. Liu, Z.; Wu, W.; Gu, X.; Li, S. Application of Combining YOLO Models and 3D GPR Images in Road Detection and Maintenance. Remote Sens. 2021, 13, 1081. [CrossRef]\nRemote Sens. 2023, 15, 1346 22 of 22\n24. Qiao, L.; Qin, Y.; Ren, X.; Wang, Q. Identification of buried objects in GPR using amplitude modulated signals extracted from multiresolution monogenic signal analysis. Sensors 2015, 15, 30340\u201330350. [CrossRef] [PubMed] 25. Tivive, F.H.C.; Bouzerdoum, A.; Abeynayake, C. GPR Target Detection by Joint Sparse and Low-Rank Matrix Decomposition. IEEE Trans. Geosci. Remote Sens. 2019, 57, 2583\u20132595. [CrossRef] 26. Cand\u00e8s, E.J.; Li, X.; Ma, Y.; Wright, J. Robust Principal Component Analysis? J. ACM 2011, 58, 11. [CrossRef] 27. He, Z.; Peng, P.; Wang, L.; Jiang, Y. PickCapsNet: Capsule Network for Automatic P-Wave Arrival Picking. IEEE Geosci. Remote Sens. Lett. 2021, 18, 617\u2013621. [CrossRef] 28. Gamba, P.; Lossani, S. Neural Detection of Pipe Signatures in Ground Penetrating Radar Images. IEEE Trans. Geosci. Remote Sens. 2000, 38, 790\u2013797. [CrossRef] 29. Gao, D.; Wu, S. An optimization method for the topological structures of feed-forward multi-layer neural networks. Pattern Recognit. 1998, 31, 1337\u20131342. 30. Liu, H.; Liu, J.; Wang, Y.; Xia, Y.; Guo, Z. Identification of grouting compactness in bridge bellows based on the BP neural network. Structures 2021, 32, 817\u2013826. [CrossRef] 31. Rumelhart, D.; Hinton, G.E.; Williams, R.J. Learning Representations by Back-Propagating Errors. Nature 1986, 323, 533\u2013536. [CrossRef] 32. Giannakis, I.; Giannopoulos, A.; Warren, C. A Realistic FDTD Numerical Modeling Framework of Ground Penetrating Radar for Landmine Detection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2016, 9, 37\u201351. [CrossRef] 33. Song, X.; Liu, T.; Xiang, D.; Su, Y. GPR Antipersonnel Mine Detection Based on Tensor Robust Principal Analysis. Remote Sens.\n2019, 11, 984. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Efficient Underground Target Detection of Urban Roads in Ground-Penetrating Radar Images Based on Neural Networks",
    "year": 2023
}