{
    "abstractText": "Let us consider the deconvolution problem, that is, to recover a latent source x(\u00b7) from the observations y= [y1, . . . , yN ] of a convolution process y= x ? h+ \u03b7, where \u03b7 is an additive noise, the observations in y might have missing parts with respect to y, and the filter h could be unknown. We propose a novel strategy to address this task when x is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source x, which allows for closedform Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter h can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods conceptually, via illustrative examples, and using realworld datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Felipe Tobar"
        },
        {
            "affiliations": [],
            "name": "Jorge F. Silva"
        }
    ],
    "id": "SP:e5baa58cc6119dbe4b0acc171b251271280d06fa",
    "references": [
        {
            "authors": [
                "H Wang",
                "S Sreejith",
                "Y Lin",
                "N Ramachandra",
                "A Slosar",
                "S. Yoo"
            ],
            "title": "Neural Network Based Point Spread Function Deconvolution",
            "venue": "For Astronomical Applications. arXiv preprint arXiv:2210.01666",
            "year": 2022
        },
        {
            "authors": [
                "T Clapp",
                "S. Godsill"
            ],
            "title": "Bayesian blind deconvolution for mobile communications",
            "venue": "In IEE Colloquium Adaptive Signal Processing for Mobile Communication Systems (Ref. No",
            "year": 1997
        },
        {
            "authors": [
                "ML Willardson",
                "BE Anderson",
                "SM Young",
                "MH Denison",
                "BD. Patchett"
            ],
            "title": "Time reversal focusing of high amplitude sound in a reverberation chamber",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 2018
        },
        {
            "authors": [
                "VK Arya",
                "H. Holden"
            ],
            "title": "Deconvolution of seismic data-an overview",
            "venue": "IEEE Transactions on Geoscience Electronics",
            "year": 1978
        },
        {
            "authors": [
                "H Hong",
                "Y. Shi"
            ],
            "title": "Fast deconvolution for motion blur along the blurring paths",
            "venue": "Canadian Journal of Electrical and Computer Engineering",
            "year": 2017
        },
        {
            "authors": [
                "A. Tarantola"
            ],
            "title": "Inverse Problem Theory and Methods for Model Parameter Estimation",
            "year": 2005
        },
        {
            "authors": [
                "Stuart AM"
            ],
            "title": "Inverse problems: A Bayesian perspective",
            "venue": "Acta Numerica",
            "year": 2010
        },
        {
            "authors": [
                "RC Aster",
                "B Borchers",
                "CH. Thurber"
            ],
            "title": "Parameter Estimation and Inverse Problems",
            "year": 2018
        },
        {
            "authors": [
                "N. Wiener"
            ],
            "title": "Extrapolation, Interpolation, and Smoothing of Stationary Time Series",
            "year": 1964
        },
        {
            "authors": [
                "Kalman RE"
            ],
            "title": "A new approach to linear filtering and prediction problems",
            "venue": "Journal of Basic Engineering",
            "year": 1960
        },
        {
            "authors": [
                "S. Riad"
            ],
            "title": "The deconvolution problem: An overview",
            "venue": "Proceedings of the IEEE",
            "year": 1986
        },
        {
            "authors": [
                "C Rasmussen",
                "C. Williams"
            ],
            "title": "Gaussian Processes for Machine Learning",
            "venue": "The MIT Press. 15 rspa.royatypublishing.org P oc R S oc A",
            "year": 2006
        },
        {
            "authors": [
                "Bretthorst GL"
            ],
            "title": "Bayesian interpolation and deconvolution",
            "venue": "Technical report Washington Univ St Louis Mo Dept Of Chemistry",
            "year": 1992
        },
        {
            "authors": [
                "MacKay DJ"
            ],
            "title": "Information theory, inference and learning algorithms",
            "year": 2003
        },
        {
            "authors": [
                "Adami KZ"
            ],
            "title": "Variational methods in Bayesian deconvolution",
            "venue": "Statistical Problems in Particle Physics, Astrophysics and Cosmology",
            "year": 2003
        },
        {
            "authors": [
                "SD Babacan",
                "R Molina",
                "AK. Katsaggelos"
            ],
            "title": "Variational Bayesian blind deconvolution using a total variation prior",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2008
        },
        {
            "authors": [
                "D Jeulin",
                "D. Renard"
            ],
            "title": "Practical limits of the deconvolution of images by kriging",
            "venue": "Microscopy Microanalysis Microstructures",
            "year": 1992
        },
        {
            "authors": [
                "P. Goovaerts"
            ],
            "title": "Kriging and semivariogram deconvolution in the presence of irregular geographical units",
            "venue": "Mathematical geosciences",
            "year": 2008
        },
        {
            "authors": [
                "D Ren",
                "K Zhang",
                "Q Wang",
                "Q Hu",
                "W. Zuo"
            ],
            "title": "Neural blind deconvolution using deep priors",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition pp",
            "year": 2020
        },
        {
            "authors": [
                "J Dong",
                "S Roth",
                "B. Schiele"
            ],
            "title": "Deep Wiener deconvolution: Wiener meets deep learning for image deblurring",
            "venue": "Advances in Neural Information Processing Systems vol",
            "year": 2020
        },
        {
            "authors": [
                "P Boyle",
                "M. Frean"
            ],
            "title": "Dependent Gaussian processes",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2005
        },
        {
            "authors": [
                "\u00c1lvarez M",
                "Lawrence ND"
            ],
            "title": "Sparse convolved Gaussian processes for multi-output regression",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2009
        },
        {
            "authors": [
                "G Parra",
                "F. Tobar"
            ],
            "title": "Spectral mixture kernels for multi-output Gaussian processes",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "F Tobar",
                "T Bui",
                "R. Turner"
            ],
            "title": "Learning stationary time series using Gaussian processes with nonparametric kernels",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "W. Bruinsma"
            ],
            "title": "The generalised Gaussian convolution process model",
            "venue": "Master\u2019s thesis Department of Engineering,",
            "year": 2016
        },
        {
            "authors": [
                "AG Wilson",
                "Z Hu",
                "R Salakhutdinov",
                "EP. Xing"
            ],
            "title": "Deep kernel learning",
            "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics vol. 51Proceedings of Machine Learning Research",
            "year": 2016
        },
        {
            "authors": [
                "M van der Wilk",
                "CE Rasmussen",
                "J. Hensman"
            ],
            "title": "Convolutional Gaussian processes",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "I Walker",
                "B. Glocker"
            ],
            "title": "Graph convolutional Gaussian processes",
            "venue": "Proceedings of the 36th International Conference on Machine Learning vol",
            "year": 2019
        },
        {
            "authors": [
                "K Blomqvist",
                "S Kaski",
                "M. Heinonen"
            ],
            "title": "Deep convolutional Gaussian processes",
            "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
            "year": 2019
        },
        {
            "authors": [
                "A Asensio Ramos",
                "P. Petit"
            ],
            "title": "Bayesian least squares deconvolution",
            "venue": "Astronomy & Astrophysics",
            "year": 2015
        },
        {
            "authors": [
                "F Tobar",
                "G Rios",
                "T Valdivia",
                "P. Guerrero"
            ],
            "title": "Recovering latent signals from a mixture of measurements using a Gaussian process prior",
            "venue": "IEEE Signal Processing Letters",
            "year": 2017
        },
        {
            "authors": [
                "A Arjas",
                "L Roininen",
                "MJ Sillanp\u00e4\u00e4",
                "A. Hauptmann"
            ],
            "title": "Blind hierarchical deconvolution",
            "venue": "In IEEE International Workshop on Machine Learning for Signal Processing",
            "year": 2020
        },
        {
            "authors": [
                "Paciorek CJ",
                "Schervish MJ"
            ],
            "title": "Spatial modelling using a new class of nonstationary covariance functions",
            "venue": "Environmetrics: The official journal of the International Environmetrics Society",
            "year": 2006
        },
        {
            "authors": [
                "S Ross",
                "A Arjas",
                "II Virtanen",
                "MJ Sillanp\u00e4\u00e4",
                "L Roininen",
                "A. Hauptmann"
            ],
            "title": "Hierarchical deconvolution for incoherent scatter radar data",
            "venue": "Atmospheric Measurement Techniques",
            "year": 2022
        },
        {
            "authors": [
                "R Boloix-Tortosa",
                "JJ Murillo-Fuentes",
                "FJ Pay\u00e1n-Somet",
                "F. P\u00e9rez-Cruz"
            ],
            "title": "Complex Gaussian processes for regression",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2018
        },
        {
            "authors": [
                "L Ambrogioni",
                "E. Maris"
            ],
            "title": "Complex-valued Gaussian process regression for time series analysis",
            "venue": "Signal Processing 160,",
            "year": 2019
        },
        {
            "authors": [
                "F Tobar",
                "R. Turner"
            ],
            "title": "Modelling of complex signals using Gaussian processes",
            "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2015
        },
        {
            "authors": [
                "Shannon CE"
            ],
            "title": "Communication in the Presence of Noise",
            "venue": "Proceedings of the Institute of Radio Engineers",
            "year": 1949
        },
        {
            "authors": [
                "F. Tobar"
            ],
            "title": "Band-limited Gaussian processes: The sinc kernel",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Naylor PA",
                "Gaubitch ND"
            ],
            "title": "Speech dereverberation",
            "year": 2010
        },
        {
            "authors": [
                "JL Starck",
                "E Pantin",
                "F. Murtagh"
            ],
            "title": "Deconvolution in astronomy: A review",
            "venue": "Publications of the Astronomical Society of the Pacific",
            "year": 2002
        },
        {
            "authors": [
                "F Tobar",
                "L Araya-Hern\u00e1ndez",
                "P Huijse",
                "PM. Djuri\u0107"
            ],
            "title": "Bayesian reconstruction of Fourier pairs",
            "venue": "IEEE Transactions on Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "E Cazelles",
                "A Robert",
                "F. Tobar"
            ],
            "title": "The Wasserstein-Fourier distance for stationary time series",
            "venue": "IEEE Transactions on Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report University of Toronto",
            "year": 2009
        },
        {
            "authors": [
                "F. Tobar"
            ],
            "title": "Bayesian nonparametric spectral estimation",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "rspa.royalsocietypublishing.org\nResearch\nArticle submitted to journal\nSubject Areas:\nmachine learning, signal processing\nKeywords:\nGaussian processes, deconvolution,\nBayesian inference\nAuthor for correspondence:\nFelipe Tobar\ne-mail: ftobar@uchile.cl\nGaussian Process Deconvolution Felipe Tobar1, Arnaud Robert2 and Jorge\nF. Silva3\n1Initiative for Data & Artificial Intelligence, Universidad\nde Chile 2Department of Computing, Imperial College London 3Department of Electrical and Electronic Engineering,\nUniversidad de Chile\nLet us consider the deconvolution problem, that is, to recover a latent source x(\u00b7) from the observations y= [y1, . . . , yN ] of a convolution process y= x ? h+ \u03b7, where \u03b7 is an additive noise, the observations in y might have missing parts with respect to y, and the filter h could be unknown. We propose a novel strategy to address this task when x is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source x, which allows for closedform Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter h can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods conceptually, via illustrative examples, and using realworld datasets.\nThis is the author generated postprint of the accepted manuscript (i.e., the accepted version not typeset by the journal) produced to be shared in personal or public repositories.\n\u00a9 The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/ by/4.0/, which permits unrestricted use, provided the original author and source are credited.\nar X\niv :2\n30 5.\n04 87\n1v 2\n[ st\nat .M\nL ]\n9 M\nay 2\n02 3\n2 rspa.royalsocietypublishing.org P roc R S oc A 0000000 .........................................................."
        },
        {
            "heading": "1. Introduction",
            "text": "In signal processing, the convolution between a (continuous-time) source x and a filter h, denoted by1 f(t) = x ? h= \u222b R x(\u03c4)h(t\u2212 \u03c4)d\u03c4 , (1.1) can be understood as a generalised (noiseless) observation of x through an acquisition device with impulse response h. Here, h reflects the quality or precision of the observation device, since the \"closer\" h is to a Dirac delta, the \"closer\" the convolved quantity f is to the source x. This convolution model, and the need to recover the source signal x from a set of noisy observations of f , arise in a number of scenarios including: astronomy [1], channel equalisation in telecommunications [2], de-reverberation [3], seismic wave reconstruction [4], and image restoration [5] to name a few. In these scenarios, practitioners need to remove the unwanted artefacts introduced by the non-ideal filter h, in other words, they require to perform a deconvolution to recover x from f . In practice, the deconvolution operates over observations that are noise-corrupted (due to sensing procedure) realisations of f , denoted y in our setting. Furthermore, we will de facto assume that there are missing observations, since our formulation establishes the source x and the convolution f as continuous-time objects while the observations y are always finite meaning that there are \"missing parts\" in the observations. Fig. 1 illustrates this procedure for the image of a bird using the proposed method.\nAs a particular instance of inverse problems, the deconvolution of corrupted signals has been largely addressed from a statistical perspective [6\u20138]. This approach is intimately related to the linear filter theory, where the foundations laid by the likes of [9,10] are still at the core of modern-day implementations of deconvolution. A common finding across the vast deconvolution literature is that, under the presence of noisy or missing data, an adequate model for reconstructing f (from y) before the deconvolution is fundamental. From a Bayesian perspective, this boils down to an appropriate choice of the prior distribution of the source x; we proceed by imposing a Gaussian process prior on the source.\n(a) Contribution and organisation Despite the ubiquity of the deconvolution problem and the attention it has received [11], we claim that the recovery of continuous-time signals from a finite number of corrupted (convolved) samples from a Bayesian standpoint, which can provide error bars for the deconvolution, has been largely underexplored. With this challenge in mind, we study Bayesian nonparametric 1For a lighter notation, we use the compact expressions x, h and f to represent the functions (x(t))t\u2208R, (h(t))t\u2208R, and (f(t))t\u2208R respectively.\n3 rspa.royalsocietypublishing.org P roc R S oc A 0000000 ..........................................................\nFigure 2: Graphical model for the hierarchical GP convolution. Recall that observed and latent random variables are shown in grey and white circles respectively, while black squares denote fixed parameters.\ndeconvolution using a Gaussian process (GP) prior over the latent source x, our method is thus termed Gaussian process deconvolution (GPDC). The main contributions of our work include: i) the conditions for the proposed model to be well defined and how to generate samples from it; ii) the closed-form solution for the posterior deconvolution and when this deconvolution is possible; iii) its application to the blind deconvolution and the required approximations; and iv) experimental validation of our GPDC method on 1D and 2D real-world data.\nThe article is organised as follows. Sec. 2 presents the convolution as a GP hierarchical model and the related literature. Sec. 3 studies the direct model (i.e., x generates f and y) and defines the requirements of h(t) and x for f(t) to be well defined point-wise. Then, Sec. 4 focuses on the inverse problem (i.e., x is estimated from y) and studies the recovery from the Fourier representation perspective. Sec. 5 addresses the blind deconvolution (i.e., h is unknown), while Secs. 6 and 7 present the experimental validation and conclusions respectively."
        },
        {
            "heading": "2. Deconvolution using GPs",
            "text": "(a) Proposed generative model Let us consider the following hierarchical model (see Fig. 2 for the graphical representation):\nsource process: x(t)\u223cGP(mx(t),Kx(t)), (2.1) convolution: f(t) = \u222b R x(\u03c4)h(\u03c4 \u2212 t)d\u03c4 , (2.2)\nobservations: yi \u223cN (f(ti), \u03c32n), i= 1, . . . , N, (2.3)\nwhere t= {ti}Ni=1 \u2208R N indicate the observation times. First, eq. (2.1) places a stationary GP prior on the source with covarianceKx(t); we will assumemx(t) = 0 for all t. This selection follows the rationale that the prior distribution is key for implementing deconvolution under missing and noisy data, and the well-known interpolation properties of GPs [12]. Second, eq. (2.2) defines the continuous-time convolution process f through a linear and time-invariant filter h. Third, eq. (2.3) defines a Gaussian likelihood, where the noisy observations of f at times t are denoted by y= [y1, . . . , yN ]\u2208RN . Notice that the observations y only see \"parts\" of f(\u00b7) and thus a Bayesian approach is desired to quantify the uncertainty related to the conditional process x|y.\n(b) Relationship to classical methods and prior work Perhaps the simplest approaches to deconvolution are the Inverse FT method, which performs deconvolution as a point-wise division between Fourier transforms F\u22121 {F {y} /F {h}} \u2014 where h is a discrete version of the filter h\u2014 and the Wiener method, which finds the optimal estimate of x in the mean-square-error sense. Both methods assume a known filter and perform a division in the Fourier domain which can be unstable in practice. The first Bayesian take on the deconvolution problem can be attributed to Bretthorst in 1992, who proposed to use\n4 rspa.royalsocietypublishing.org P roc R S oc A 0000000 .......................................................... a multivariate normal (MVN) prior for the discrete-time case [13]. A year later, Rhode and Whittenburg addressed the Bayesian deconvolution of exponentially-decaying sinewaves [14], and then Sir David Mackay\u2019s book presented the Bayesian derivation of the deconvolution problem assuming an MVN prior and connects it to other methods with emphasis on image restoration [15, Ch. 46]. In the Statistics community, deconvolution refers to the case when x and h are probability densities rather than time series; this case is beyond our scope. The use of different priors for Bayesian deconvolution follows the need to cater for specific properties of the latent time series x. For instance, [16] considered a mixture of Laplace priors which required variational approximations, while [17] implemented a total variation prior with the aim of developing reliable reconstruction of images. However, little attention has been paid, so far, to the case when the source x is a continuous time signal. Most existing approaches assume priors only for discrete-time finite sequences (or vectors) and, even though they can still be applied to continuous data by quantising time, their computational complexity explodes for unevenly sampled data or dense temporal grids. In the same manner, the kriging literature, which is usually compared to that of GPs, has addressed the deconvolution problem (see, e.g., [18,19]) but has not yet addressed the continuous-time setting from a probabilistic perspective. Additionally, recent advances of deconvolution in the machine learning and computer vision communities mainly focus on blending neural networks models with classic concepts such as prior design [20] and the Wiener filter [21], while still considering discrete objects (not continuous time) and not allowing in general for missing data.\nConvolution models have been adopted by the GP community mainly to parametrise covariance functions for the multioutput [22\u201324] and non-parametric cases [25,26]. With the advent of deep learning, researchers have replicated the convolutional structure of convolutional NNs (CNNs) on GPs. For instance, Deep Kernel Learning (DKL) [27] concatenates a CNN and a GP so that the kernel of the resulting structure\u2014also a GP\u2014sports shared weights and biases, which are useful for detecting common features in different regions of an image and provides robustness to translation of objects. Another example are convolutional GPs [28], which extend DKL by concatenating the kernel with a patch-response function that equips the kernel with convolutional structure without introducing additional hyperparameters. This concept has been extended to graphs and deep GPs\u2014see [29,30] respectively.\nThough convolutions have largely aided the design of kernels for GPs, contributions in the \"opposite direction\", that is, to use the GP toolbox as a means to address the general deconvolution problem, are scarce. To the best of our knowledge, the only attempts to perform Bayesian deconvolution using a GP prior are works that either: focus specifically in detecting magnetic signals from spectropolarimetric observations [31]; only consider discrete-time impulse responses [32]; or, more recently, use an MVN prior for the particular case of a non-stationary Mat\u00e9rn covariance [33], a parametrisation proposed by [34] which has, in particular, been used for scatter radar data [35].\nBuilding on the experimental findings of these works implementing deconvolution using GPs, our work focuses on the analysis and study of kernels and filters, in terms of their ability to recover x(t) from y. The proposed strategy, termed GPDC, is expected to have superior modelling capabilities as compared to previous approaches, while having a closed form posterior deconvolution density which is straightforward to compute. However, there are aspects to be addressed before implementing GPDC, these are: i) when the integral in eq. (2.2) is finite in terms of the law of x, ii) when x can be recovered from y, iii) how the blind scenario can be approached. Addressing these questions are the focus of the following sections."
        },
        {
            "heading": "3. Analysis of the direct model",
            "text": "(a) Assumptions on x and h, and their impact on f We assume that x is a stationary GP and its covariance kernel Kx is integrable (i.e., Kx \u2208L1); this is needed for x to have a well-defined Fourier power spectral density (PSD). Additionally,\n5 rspa.royalsocietypublishing.org P roc R S oc A 0000000 .......................................................... although the sample paths x are in general not integrable, they are locally integrable due to Kx \u2208 L1, therefore, we assume that the filter h decays fast enough such that the integral in eq. (2.2) is finite. This is always obtained when either h has compact support or when it is dominated by a Laplacian or a Square Exponential function. If these properties (integrability and stationarity) are met for x, they translate to f via the following results. Remark 1. If f(t) in eq. (2.2) is finite point-wise for any t\u2208R, then f(t)\u223cGP(0,Kf (t)) with stationary kernel Kf (t) = \u222b R2 h(\u03c4 \u2032)h(\u03c4)Kx(\u03c4 \u2212 (\u03c4 \u2032 \u2212 t))d\u03c4d\u03c4 \u2032. (3.1) Lemma 1. If the convolution filter h and the covariance Kx are both integrable, then Kf in eq. (3.1) is integrable. Proof. The integrability of Kf (t) in eq. (3.1) follows directly from applying Fubini Theorem and the triangle inequality (twice) to \u222b R |Kf (t)|dt, to obtain \u222b R |Kf (t)|dt <\u221e (full proof in the Appendix).\nRemark 2. Lemma 2 provides a sufficient condition for the integrability of Kf , thus theoretically justifying i) a Fourier-based analysis of f , and ii) relying upon the GP machinery to address deconvolution from the lens of Bayesian inference. However, the conditions in Lemma 2 are not necessary; for instance, if the filter h is the (non-integrable) Sinc function, the generative model in eqs. (2.1)-(2.3) is still well defined (see Example 2).\n(b) Sampling from the convolution model We devise two ways of sampling from f in eq. (2.2): we could either i) draw a path from x\u223c GP(0,Kx(t)) and convolve it against h, or ii) sample directly from f \u223cGP(0,Kf (t)). However, both alternatives have serious limitations. The first one cannot be implemented numerically since the convolution structure implies that every element in f depends on infinite values of x (for a general h). The second alternative bypasses this difficulty by directly sampling a finite version of f , however, by doing so x is integrated out, meaning that we do not know \"to which\" sample trajectory of x the (finite) samples of f correspond.\nThe key to jointly sample finite versions (aka marginalisations) of the source and convolution processes lies on the fact that, although the relationship between x and f established by eq. (2.2) is deterministic, the relationship between their finite versions becomes stochastic. Let us denote2 finite versions of x and f by x= x(tx) and f = f(tf ) respectively, where tx \u2208RNx and tf \u2208RNf . Therefore, we can hierarchically sample according to p(x, f) = p(f |x)p(x) in two stages: we first sample x\u223cMVN (0,Kx(tx)) and then f |x\u223c p(f |x) given by3\np(f |x) = MVN ( \u00b5f |x, \u03c3 2 f |x ) (3.2)\n\u00b5f |x =Kfx(tf , tx)K \u22121 x (tx)x\n\u03c32f |x =Kf (tf )\u2212Kfx(tf , tx)K \u22121 x (tx)Kxf (tx, tf ),\nwhere Kfx(t1, t2) =Kfx(t1 \u2212 t2) is the stationary covariance between f and x, given elementwise by\nKfx(t1, t2) =Kfx(t1 \u2212 t2) = \u222b R h(t\u2212 (t1 \u2212 t2))Kx(t)dt. (3.3)\nThe integrability of Kfx in eq. (3.3) is obtained similarly to that of Kf in Lemma 2, under the assumption that h,Kx \u2208L1. 2Here, we use the compact notation x(t) = [x(t1), . . . , x(tn)], where t= [t1, . . . , tn]. 3We use the reversed-argument notationKxf (tx, tf ) =K>fx(tf , tx) to avoid the use of transposes.\n6 rspa.royalsocietypublishing.org P roc R S oc A 0000000 ..........................................................\n0 2 4 6 8 10 time 2 0\n2 Sampling f using 40 samples for x source x convolved f 95% CI for f|x\n0 2 4 6 8 10 time\nSampling f using 500 samples for x\nsource x convolved f 95% CI for f|x\nFigure 3: Hierarchical sampling x and f considering 40 (left) and 500 (right) samples for x in the interval [0,10]. Both Kx and h (and consequently Kxf and Kf ) are square-exponential kernels with lengthscale l= \u221a 0.05 (rate \u03b3 = 10) .\nFurthermore, let us recall that the conditional mean of f(t)|x is given by\nE [f(t)|x] = \u222b R h(t+ \u03c4)E [x(\u03c4)|x]d\u03c4 . (3.4)\nThis expression reveals that the expected value of f(t) given x can be computed by first calculating the average interpolation of x given x, denoted E [x(\u03c4)|x], and then applying the convolution to this interpolation as per eq. (2.2).\nAdditionally, let us also recall that the conditional variance of f(t)|x is given by\nV [f(t)|x] = \u222b R2 h(t+ \u03c4)V [ x(\u03c4), x(\u03c4 \u2032)|x ] h(t+ \u03c4 \u2032)d\u03c4d\u03c4 \u2032.\nObserve that since the posterior variance of a GP decreases with the amount of observations, V [ x(\u03c4), x(\u03c4 \u2032)|x ] approaches zero whenever tx become more dense, and consequently so does V [f(t)|x]. Therefore, the more elements in x, the smaller the variance V [f(t)|x] and thus the trajectories of f |x become concentrated around the posterior mean in eq. (3.4). This resembles a connection with the discrete convolution under missing source data, where one \"interpolates and convolves\", emphasising the importance of the interpolation (i.e., the prior) of x. Definition 1 presents the Square Exponential (SE) kernel, and then Example 1 illustrates the sampling procedure and the concentration of f |x around its mean.\nDefinition 1. The stationary kernel defined as\nKSE(t) = \u03c3 2 exp ( \u2212 1 2l2 t2 )\n(3.5)\nis referred to as Square Exponential (SE) and its parameters are magnitude \u03c3 and lengthscale l. Alternatively, the SE kernel\u2019s lenghtscale can be defined in terms of its inverse lengthscale (or rate) \u03b3 = 1\n2l2 .\nThe Fourier transform of the SE kernel is also an SE kernel, therefore, the support of the PSD of a GP with the SE kernel is the entire real line.\nExample 1. Let us considerKx and h to be SE kernels with lengthscale l= \u221a 0.05 (rate \u03b3 = 10), thus,Kf and Kxf are SE as well. Fig. 3 shows x and f sampled over the interval t\u2208 [0, 10]: at the left (resp. right) plot, we sampled a 40-dimensional (resp. 500-dimensional) vector x shown in red to produce a 1000- dimensional vector f shown in blue alongside the 95% error bars for p(f(t)|x) in both plots. Notice how the larger dimensionality of x over the fixed interval resulted in a tighter conditional density p(f(t)|x).\nRemark 3. The GP representation of the convolution process allows us to generate finite samples x, f (over arbitrary inputs) of the infinite-dimensional processes x and f respectively. This represents a continuous and probabilistic counterpart to the classic discrete-time convolution that i) can be implemented computationally and, ii) is suitable for real-world scenarios where data may be non-uniformly sampled.\n7 rspa.royalsocietypublishing.org P roc R S oc A 0000000 .........................................................."
        },
        {
            "heading": "4. The inverse problem: Bayesian nonparametric deconvolution",
            "text": "We now turn to the inverse problem of recovering x from the finite vector y= [y1, . . . , yN ] observed at times t= [t1, . . . , tN ]. In our setting defined in eqs. (2.1)-(2.3), the deconvolution x|y is a GP given by x|y\u223cGP(\u00b5, \u03c32) (4.1) \u00b5(t) =Kxy(t, t)K \u22121 y y, (4.2) \u03c32(t1, t2) =Kx(t1, t2)\u2212Kxy(t1, t)K\u22121y Kyx(t, t2), (4.3) whereKy =Kf (t, t) + IN\u03c3 2 n is the marginal covariance matrix of the observations vector y, IN is the N -size identity matrix, and Kxy(t1, t2) denotes the cross-covariance between x(t1) and y(t2), which is in turn equal to Kxf (t1, t2) =Kxf (t1 \u2212 t2). (a) When is deconvolution possible? We aim to identify the circumstances under which y provides enough information to determine x with as little uncertainty as possible. To this end, we consider a windowed (Fourier) spectral representation of x|y\u2014the GP in eq.(4.1)\u2014given by x\u0302w(\u03be) =F {w(t)x(t)} (\u03be) = \u222b R w(t)x(t)e\u2212j2\u03c0\u03betdt, (4.4) where F {\u00b7} denotes the continuous-time Fourier transform (FT) operator, \u03be \u2208R is the frequency variable, and the window w :R\u2192R has compact support (such that the integral above is finite). This windowed spectral representation is chosen since the GP x|y is nonstationary (its power spectral density is not defined), and the sample trajectories of x|y following the distribution GP(\u00b5, \u03c32) are not Lebesgue integrable in general (their FT cannot be computed). Based on the representation introduced in eq. (4.4), we define the concept of successful recovery and link it to the spectral representation of Kx and h.\nDefinition 2. We say that x(\u00b7) can be successfully recovered from observations y= [y1, . . . , yN ] if, \u2200\u03be \u2208R, the spectral representation x\u0302w(\u03be) in eq. (4.4) can be recovered at an arbitrary precision by increasing the amount of observations N , for an arbitrary compact-support window w(\u00b7).\nTheorem 1. For the setting defined in eqs. (2.1)-(2.3), a necessary condition to successfully recover x from y is that the spectral support of h contains that of Kx, that is, suppF {Kx} \u2286 suppF {h}.\nProof. From eqs. (2.1)-(2.3) and linearity of the FT, the windowed spectrum x\u0302w(\u03be) is given by a complex-valued4 GP, with mean and marginal variance (calculations in the Appendix):\n\u00b5x\u0302w (\u03be) = ( m\u0302x(\u03be) + K\u0302x(\u03be)h\u0302(\u03be)e \u2212j2\u03c0t\u03beK\u22121y y ) ? w\u0302(\u03be), (4.5)\n\u03c32x\u0302w (\u03be) = K\u0302x(\u03be) ? |w\u0302(\u03be)| 2\n\u2212 \u2016w\u0302(\u03be) ? K\u0302x(\u03be)h\u0302(\u03be)e\u2212j2\u03c0\u03bet \u2016K\u22121y , (4.6)\nwhere \u2016v\u2016A = v>Av denotes the Mahalanobis norm of v w.r.t. matrixA, and h\u0302=F {h}was used as a compact notation for the FT of h. Following Definition 2, successful recovery is achieved when the posterior variance of x\u0302w(\u03be) vanishes, i.e., when the two terms at the right hand side of eq. (4.6) cancel one another. For this to occur, it is necessary that these terms have the same support, however, as this should happen for an arbitrary window w\u0302(\u03be) we require that supp K\u0302x(\u03be) =\u222aNi=1 supp K\u0302x(\u03be)h\u0302(\u03be)e \u2212j2\u03c0\u03beti . Since \u222aNi=1 supp e \u2212j2\u03c0\u03beti =R, what is truly needed is that supp K\u0302x(\u03be) = supp K\u0302x(\u03be)h\u0302(\u03be) which is obtained when supp K\u0302x(\u03be)\u2286 supp h\u0302(\u03be). 4For complex-valued GPs, see [36\u201338].\n8 rspa.royalsocietypublishing.org P roc R S oc A 0000000 ..........................................................\nob se rv at io ns :5 0 filter lengthscale: 0.224 filter lengthscale: 0.183 filter lengthscale: 0.158 ob se rv at io ns :1 00 ob se rv at io ns :5 00\nSquare-exponential case (lengthscale = 0.183)\nob se\nrv at\nio ns\n:3 0\nfilter width: 2.0 filter width: 3.5 filter width: 5.0\nob se\nrv at\nio ns\n:6 0\nob se\nrv at\nio ns\n:9 0\nSinc case (width = 5)\nFigure 4: Implementation of GPDC when both h and Kx are SE (left) or Sinc (right). In each case, we considered increasing number of observations y (top to bottom) and different parameters for h (left to right)\u2014 the values for all the width and lenghtscale parameters are presented in the figure titles. The plots show the ground truth source x (black), the posterior GPDC deconvolution mean (blue) and GPDC\u2019s 95% error bars (light blue). In line with Theorem 1 and Remark 4, notice how in the SE case (left) more observations improve the recovery, whereas in the Sinc case (right) recovery is not possible even for increasing amount of observations when the filter is too narrow (first and second columns).\nRemark 4. Theorem 1 gives a necessary condition to recover x from y in terms of how much of the spectral content of x, represented by K\u0302x, is not suppressed during the convolution against h and thus can be extracted from y. A stronger sufficient condition for successful recovery certainly depends on the locations where y is measured. Intuitively, recovery depends on having enough observations, and on the vector e\u2212j2\u03c0\u03bet being aligned with the eigenvectors of K\u22121y (t, t) defining the norm in eq. (4.6); thus resembling the sampling theorem in [39,40].\nDefinition 3 introduces the Sinc kernel [41]. Then, Example 2 illustrates the claims in Theorem 1 and Remark 4, where GPDC recovery is assessed in terms of spectral supports and amount of observations.\nDefinition 3. The stationary kernel defined as\nKSinc(t) = \u03c32 sin(\u2206\u03c0t)\n\u2206\u03c0t (4.7)\nis referred to as the (centred) Sinc kernel and its parameters are magnitude \u03c3 and width\u2206. Though the Sinc function is not integrable (see Lemma 2), it admits a Fourier transform which takes the form of a rectangle of width \u2206 centred in zero. Therefore, a GP with a Sinc kernel does not generate paths with frequencies greater than \u2206/2.\nExample 2. Let us consider two scenarios for GPDC: In the first one, both Kx and h are Square Exponential (SE) kernels and therefore suppF {Kx}= suppF {h}=R. In the second case, both Kx and h are Sinc kernels [41], and the overlap of their PSDs depends on their parameters. We implemented GPDC in both cases for different amount of observations and parameters, in particular, we considered the scenario where supp K\u0302x 6\u2282 supp h\u0302 for the Sinc case. Fig. 4 (left) shows how in the SE case an increasing number of observations always improves the recovery regardless of the parameters of h andKx. On the contrary, notice that for the Sinc case (Fig. 4, right), even with a large number of observations, the latent source x cannot be recovered. This is due to certain spectral components of x being removed by the narrowbandness of the convolution filter h. All magnitude parameters were set one, see Fig. 4 for the width (Sinc) and lenghtscale parameters (SE).\n5. The blind deconvolution case\n9 rspa.royalsocietypublishing.org P roc R S oc A 0000000 .......................................................... (a) Training and observability Implementing GPDC requires choosing the hyperparameters, i.e., the parameters of Kx and h, and the noise variance \u03c32. Depending on the application the filter might be known beforehand, for instance, in de-reverberation [42] h is given by the geometry of the room, whereas in astronomy h is given by the atmosphere and/or the telescope [43,44]. When h is unknown, a case referred to as blind deconvolution, its parameters have to be learnt from data alongside the other hyperparameters. We fit the proposed model\u2014see eqs. (2.1)-(2.3)\u2014by maximising the log-likelihood l(Kx, \u03c3 2, h) =\u2212n 2 log 2\u03c0 \u2212 1 2 log detKy \u2212 1 2 y>K\u22121y y, (5.1) where Ky =Kf (t, t) + IN , with IN the identity matrix of size N , and the relationship between Kf and Kx follows from eq. (3.1). Therefore, Kx, h and \u03c3 2 appear in l in eq. (5.1) through Ky. Maximum likelihood, however, might not recover all hyperparameters in a unique manner, since h and Kx are entangled in Ky and thus can be unidentifiable. For instance, if both h and Kx are SEs (as in Example 1) with lengthscales lh and lx respectively, then Ky is also an SE with lenghtscale ly = lx + lh; therefore, the original lengthscales cannot be recovered from the learnt ly . More intuitively, the unobservability of the convolution can be understood as follows: a given signal f could have been generated either i) by a fast source x and a wide filter h, or ii) by a slow source and a narrow filter. Additionally, it is impossible to identify the temporal location of h only from y, since the likelihood is insensitive to time shifts of x and h. Due to the symmetries in the deconvolution problem learning the hyperparameters should be aided with as much information as possible about h and Kx, in particular in the blind scenario.\n(b) A tractable approximation of h Closed-form implementation of GPDC only depends on successful computation of the integrals in eqs. (3.1) and (3.3). Those integrals can be calculated analytically for some choices of Kx and h, such as the SE or Sinc kernels in Definitions 1 and 3 respectively, but are in general intractable. Though it can be argued that computing these integrals might hinder the general applicability of GPDC, observe that when the filter h is given by a sum of Dirac deltas {wi}Mi=1 at locations {li}Mi=1, i.e.,\nh(t) = M\u2211 i=1 wi\u03b4li(t), (5.2)\nthe covariance Kf and the cross-covariance Kfx turn into summations of kernel evaluations:\nKf (t) = M\u2211 i=1 M\u2211 j=1 wiwjKx(li \u2212 (lj \u2212 t)) (5.3)\nKxf (t1, t2) =Kxf (t1 \u2212 t2) = M\u2211 i=1 wiKx(li \u2212 (t1 \u2212 t2)). (5.4)\nThe discrete-time filter h is of interest in itself, mainly in the digital signal processing community, but it is also instrumental in approximating GPDC for general applicability. This is because when the integral forms in Kf ,Kxf , in eqs. (3.1) and (3.3), cannot be calculated in closed-form, they can be approximated using different integration techniques such as quadrature methods, Monte Carlo or even importance sampling (see detailed approximations in the Appendix).\nIn terms of training (required for blind deconvolution) another advantage of the discretetime filter is that its hyperparameters (weights {wi}Mi=1) do not become entangled into Ky, but rather each of them appear bilinearly in it\u2014see eq. (5.3). This does not imply that parameters are identifiable, yet it simplifies the optimisation due to the bilinear structure. Lastly, notice that since the sample approximations of Ky (e.g., using quadrature or Monte Carlo) are of low\n2.5 0.0 2.5\nx GPDC observations\n0.000\n0.025 continuous filter\n2.5 0.0 2.5\n0.1 0.2 0.3 observed filter\n0 1 2 3 4 5 2.5 0.0 2.5 0.25 0.00 0.25 0.0 0.5 learned filter\norder compared with the data, they do not contribute with critical computational overhead, since evaluating eq. (5.1) is still dominated by the usual GP cost of O(N3). Definitions 4 and 5 present the Spectral Mixture kernel and the triangular filter respectively, then Example 3 illustrates the discretisation procedure for training GPDC in the blind and non-blind cases as described above.\nDefinition 4. The stationary kernel defined as\nKSM(t) = \u03c3 2 exp ( \u2212 1 2l2 t2 ) cos (2\u03c0\u03bdt) (5.5)\nis referred to as the (single component) Spectral Mixture (SM) and its parameters are magnitude \u03c3, lengthscale l and frequency \u03bd. Akin to the SE kernel in Def. 1, the SM kernel\u2019s lenghtscale can be defined via its rate \u03b3 = 1\n2l2 . The Fourier transform of the SM kernel is an SE kernel centred in frequency \u03bd.\nDefinition 5. The function defined as\nhTri(t) = \u03c3 2 max\n( 1\u2212 2|t|\n\u2206 , 0\n) (5.6)\nis referred to as the triangular filter, also known as the Bartlett window, and its parameters are magnitude \u03c3 and width \u2206.\nExample 3. Let us consider the case where Kx is an SE kernel (see Def. 1) and h is a triangle (see Def. 5). We avoid computing the integrals for Kf and Kxf , and consider two scenarios: i) learn discrete approximations both for h and Kx via maximum likelihood as in Sec. 5(a), ii) fix a discrete filter according to the true h, and only trainKx as in Sec. 5(b). Fig. 5 shows these implementations, notice how the discrete filter (only 5 points) allows for a reasonable recovery of the latent source x from y. Furthermore, according to the unobservability of h, there is an unidentifiable lag between the true source and the deconvolution for the blind case.\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 time [seconds] 2 0 2\n4\nSignals and filter (time domain) Speech Convolution Filter\n0 100 200 300 400 500 600 frequecy [1/secs]\n10 8 10 6 10 4 10 2 100 102\nWiener\nConvolutionInverse FT\nWindow\nGPDC True speech\nTrue source and estimates (periodogram)"
        },
        {
            "heading": "6. Experiments",
            "text": "We tested GPCD in two experimental settings. The first one simulated an acoustic dereverberation setting and its objective was to validate the point estimates of GPDC in the noiseless, no-missing-data, case (i.e., \u03c32n = 0) against standard deconvolution methods. The second experiment dealt with the recovery of latent images from low-resolution, noisy and partial observations, both when the filter h is known or unknown (a.k.a. blind superresolution). The code used for the following experiments and the examples in the paper can be found in https://github.com/GAMES-UChile/Gaussian-Process-Deconvolution.\n(a) Acoustic deconvolution. We considered a 350 [ms] speech signal (from http://www.mcsquared.com/reverb.htm), re-sampled at 5512.5 hertz (i.e., 2000 samples), standardised it and then convolved it against an SE filter of lenghtscale l= 2.2 [ms]. Fig. 6 (top) shows the speech signal, the filter and the convolution. Since speech is known to be piece-wise stationary, we implemented GPDC using an SE kernel, then learnt the hyperparameters via maximum likelihood as explained in Section 5(a). The learnt hyperparameters were \u03c3= 0.95, l= 5 \u00b7 10\u22124, \u03c3noise = 0.12. Notice that these values are consistent with the data pre-processing (the variances sum approximately one since the signal was standardised) and from what is observed in Fig. 6 (top) regarding the lengthscale.\nThe proposed GPDC convolution (assuming a known filter h given by an SE kernel presented above) was compared against the Wiener deconvolution and the Inverse FT methods mentioned in Sec. 2. Fig. 6 (bottom) shows the periodogram of the data alongside Wiener, Inverse FT and GPDC, and the filter h. Observe how GPCD faithfully followed the true latent source across most the spectral range considered, whereas Wiener diverged and the Inverse FT decayed rapidly perhaps due to their numerical instability. It is worth noting that the GPDC estimate ceases to follow the true source precisely at the frequency when the filter decays, this is in line with Thm. 1. Table 1 shows the performance of the methods considered in time and frequency, in particular via the discrepancy between the true power spectral density (PSD) and its estimate using the MSE, the KL\ndivergence and Wasserstein distance [45] (the lower the better). Notice that GPDC outperformed Wiener and inverse FT under all metrics. The estimates in the temporal domain are shown in additional figures in the Appendix.\n(b) Blind image super-resolution. For a 32\u00d7 32 image x, we created a convolved image f using a 5\u00d7 5 filter h, and a noise-corrupted missing-data (60% of pixels retained) observation y. We implemented GPDC to recover x from y both when h is known and when it is unknown, therefore, for each image, we considered i) the case where the discrete filter h is known and used by GPDC, and ii) the case where h is learnt from\n13\nrspa.royalsocietypublishing.org P roc R S oc A 0000000 ..........................................................\nmetric GPDC Wiener inv-FT MSE (time) 19.0 35.0 41.9 MSE (PSD) 0.015 0.058 0.153 Kullback\u2013Leibler (PSD) 0.05 0.20 0.45 Wasserstein (PSD) 2124.3 3643.8 4662.6\nTable 1: Quantitative evaluation of the audio de-reverberation experiment in time and frequency\ndata via maximum likelihood. We present results using three different filters: constant, unimodal and diagonal for the main body of the manuscript (Fig. 7), and other additional shapes in the Appendix. In all cases, we assumed an SE kernel for the source (ground truth image) and for each case we learnt the lenghtscale l, the magnitude \u03c3 and the noise parameter \u03c3n. For the blind cases the discrete filter h was also learnt.\nGPDC was compared against the Wiener filter (from scikit-image), applied to the complete, noiseless, image f . The reason to compare the proposed GPDC to the standard Wiener filter applied over the true image f instead of the generated observations y is twofold: i) it serves as a benchmark that uses all the image information, and ii) the Wiener is unable to deal with missing data (as in y) in its standard implementation. Fig. 7 shows the results for combinations of 3 different images from CIFAR-10 [46] and up to three different filters for each image. Notice how both non-blind and blind GPCD were able to provide reconstructions that are much more representative of the true image (x) than those provided by the Wiener filter. The superior performance of GPDC, against the Wiener benchmark, is measured through the mean squared error with respect to the true image x as a function of the amount of seen data (rightmost plot in Fig. 7). The Appendix includes additional examples with different images and filters."
        },
        {
            "heading": "7. Discussion and Conclusions",
            "text": "We have proposed Gaussian process deconvolution (GPDC), a methodology for Bayesian nonparametric deconvolution of continuous-time signals which builds on a GP prior over the latent source x and thus allows us to place error bars on the deconvolution estimate. We studied the direct generative model (the conditions under which it is well defined and how to sample from it), the inverse problem (we provided necessary conditions for the successful recovery of the source), the blind deconvoltion case, and we also showed illustrative examples and experimental validation on dereverberation and super-resolution settings. A key point of our method in connection with the classical theory can be identified by analysing eq. (4.2) in the Fourier domain, where we can interpret GPDC as a nonparametric, missing-data-able, extension of the Wiener deconvolution. In this sense, the proposed method offers a GP interpretation to classic deconvolution, thus complementing the deconvolution toolbox with the abundant resources for GPs such as off-the-shelf covariance functions, sparse approximations, and a large body of dedicated software.\nWhen constrained to the discrete and non-missing data scenario, our method reduces to the standard Wiener deconvolution, known in the literature [9]; thus, we have focused on comparing GPDC to the Wiener deconvolution method. However, despite the interpretation of GPDC from the viewpoint of classic deconvolution, we have considered a setting that is entirely different to that of digital communications. We have focused on estimating a continuous-time object (the source x) using a set of finite observations y, which are to be understood as noise-corrupted and missing-part realisations of the convolution process f = x ? h, and to quantify the uncertainty of this estimate. Since we have limited information about the source (x can only be known through y as per eq. (2.3)), we take a Bayesian approach and impose a prior over x, this prior is a Gaussian process. To the best of our knowledge, there is no prior work addressing this setting in conceptual\n14\nrspa.royalsocietypublishing.org P roc R S oc A 0000000 ..........................................................\nterms from a theoretical perspective, with the exception of a few works that have applied it to specific problems (see [31\u201335]).\nAs far as novel deconvolution methods are concerned, the trend in the literature is to merge neural networks with classical concepts such prior design [20] and the Wiener filter [21], and are mainly applied to images, perhaps following the strong and steady advances in deep learning for computer vision. However, despite some recent applications of GPs to the deconvolution problem in particular settings, there are, to the best of our knowledge, no method addressing the recovery of a continuous-time (or continuous-space in the case of images) object from a finite number of noisy observations for both the blind and non-blind cases with the two main properties of our proposal: i) taking a Bayesian standpoint that allows for the determination of error bars, and ii) providing the Fourier-inspired guarantees in Sec. 4(a). In this sense, we claim that ours is the first application-agnostic study of the method which established the conditions for the proper definition of the hierarchical model, the capacity of the deconvolution procedure, and a principled quantification of uncertainty.\nWe hope that our findings pave the way for applications in different sciences and also motivates the GP community to consider extensions of our study. In particular, we envision further research efforts to be dedicated in the following directions:\n\u2022 to develop sufficient (rather than only necessary) conditions for deconvolution to complement Theorem 1 and thus connecting with the interface between spectral estimation and GPs [47]. \u2022 to construct sparse GPs by controlling temporal correlation through trainable, multiresolution, convolutions. This would provide computationally efficient sparse GPs with clear reconstructions guarantees. \u2022 to develop a sparse GP version of the presented methodology in order to apply GPDC for deconvolution of large datasets\nData Accessibility. All data used in this article is either synthetic or public. Experiment 1 uses audio data from http://www.mcsquared.com/reverb.htm, while Experiment 2 uses CIFAR-10 [46].\nFunding. We acknowledge financial support from Google and the following ANID-Chile grants: FondecytRegular 1210606, the Advanced Center for Electrical and Electronic Engineering (Basal FB0008) and the Center for Mathematical Modeling (Basal FB210005)."
        }
    ],
    "title": "Gaussian Process Deconvolution",
    "year": 2023
}