{
    "abstractText": "Compared to other severe weather image restoration tasks, single image desnowing is a more challenging task. This is mainly due to the diversity and irregularity of snow shape, which makes it extremely difficult to restore images in snowy scenes. Moreover, snow particles also have a veiling effect similar to haze or mist. Although current works can effectively remove snow particles with various shapes, they also bring distortion to the restored image. To address these issues, we propose a novel single image desnowing network called Star-Net. First, we design a Star type Skip Connection (SSC) to establish information channels for all different scale features, which can deal with the complex shape of snow particles. Second, we present a Multi-Stage Interactive Transformer (MIT) as the base module of StarNet, which is designed to better understand snow particle shapes and to address image distortion by explicitly modeling a variety of important image recovery features. Finally, we propose a Degenerate Filter Module (DFM) to filter the snow particle and snow fog residual in the SSC on the spatial and channel domains. Extensive experiments show that our Star-Net achieves state-of-the-art snow removal performances on three standard snow removal datasets and retains the original sharpness of the images.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiawei Mao"
        },
        {
            "affiliations": [],
            "name": "Yuanqi Chang"
        },
        {
            "affiliations": [],
            "name": "Xuesong Yin"
        },
        {
            "affiliations": [],
            "name": "Binling Nie"
        }
    ],
    "id": "SP:861c1ac54fd6c3e923d014395e033a10be5d87a6",
    "references": [
        {
            "authors": [
                "Peter Barnum",
                "Takeo Kanade",
                "Srinivasa G Narasimhan"
            ],
            "title": "Spatio-temporal frequency analysis for removing rain and snow from videos",
            "venue": "In Proceedings of the First International Workshop on Photometric Analysis For Computer Vision- PACV 2007, pages 8\u2013p. INRIA,",
            "year": 2007
        },
        {
            "authors": [
                "Eric Bauer",
                "Ron Kohavi"
            ],
            "title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants",
            "venue": "Machine learning,",
            "year": 1999
        },
        {
            "authors": [
                "Peter F Brown",
                "Vincent J Della Pietra",
                "Peter V Desouza",
                "Jennifer C Lai",
                "Robert L Mercer"
            ],
            "title": "Class-based n-gram models of natural language",
            "venue": "Computational linguistics,",
            "year": 1992
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Chun-Fu Richard Chen",
                "Quanfu Fan",
                "Rameswar Panda"
            ],
            "title": "Crossvit: Cross-attention multi-scale vision transformer for image classification",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Tianyu Guo",
                "Chang Xu",
                "Yiping Deng",
                "Zhenhua Liu",
                "Siwei Ma",
                "Chunjing Xu",
                "Chao Xu",
                "Wen Gao"
            ],
            "title": "Pre-trained image processing transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jieneng Chen",
                "Yongyi Lu",
                "Qihang Yu",
                "Xiangde Luo",
                "Ehsan Adeli",
                "Yan Wang",
                "Le Lu",
                "Alan L Yuille",
                "Yuyin Zhou"
            ],
            "title": "Transunet: Transformers make strong encoders for medical image segmentation",
            "venue": "arXiv preprint arXiv:2102.04306,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Saurabh Saxena",
                "Lala Li",
                "David J Fleet",
                "Geoffrey Hinton"
            ],
            "title": "Pix2seq: A language modeling framework for object detection",
            "venue": "arXiv preprint arXiv:2109.10852,",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Ting Chen",
                "Hao-Yu Fang",
                "Jian-Jiun Ding",
                "Cheng-Che Tsai",
                "Sy-Yen Kuo"
            ],
            "title": "Jstasr: Joint size and transparencyaware snow removal algorithm based on modified partial convolution and veiling effect removal",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Ting Chen",
                "Hao-Yu Fang",
                "Cheng-Lin Hsieh",
                "Cheng-Che Tsai",
                "I Chen",
                "Jian-Jiun Ding",
                "Sy-Yen Kuo"
            ],
            "title": "All snow removed: Single image desnowing algorithm using hierarchical dual-tree complex wavelet representation and contradict channel loss",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Ting Chen",
                "Zhi-Kai Huang",
                "Cheng-Che Tsai",
                "Hao- Hsiang Yang",
                "Jian-Jiun Ding",
                "Sy-Yen Kuo"
            ],
            "title": "Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Djork-Arn\u00e9 Clevert",
                "Thomas Unterthiner",
                "Sepp Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "arXiv preprint arXiv:1511.07289,",
            "year": 2015
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In 2009 IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Ross Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Jun Han",
                "Claudio Moraga"
            ],
            "title": "The influence of the sigmoid function parameters on the speed of backpropagation learning",
            "venue": "In From Natural to Artificial Neural Computation: International Workshop on Artificial Neural Networks Malaga- Torremolinos,",
            "year": 1995
        },
        {
            "authors": [
                "Xin He",
                "Yong Zhou",
                "Jiaqi Zhao",
                "Di Zhang",
                "Rui Yao",
                "Yong Xue"
            ],
            "title": "Swin transformer embedding unet for remote sensing image semantic segmentation",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
            "venue": "CoRR, abs/1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Lukas Hoyer",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Zilong Huang",
                "Xinggang Wang",
                "Lichao Huang",
                "Chang Huang",
                "Yunchao Wei",
                "Wenyu Liu"
            ],
            "title": "Ccnet: Criss-cross attention for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Jack Lanchantin",
                "Tianlu Wang",
                "Vicente Ordonez",
                "Yanjun Qi"
            ],
            "title": "General multi-label image classification with transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Boyi Li",
                "Wenqi Ren",
                "Dengpan Fu",
                "Dacheng Tao",
                "Dan Feng",
                "Wenjun Zeng",
                "Zhangyang Wang"
            ],
            "title": "Benchmarking singleimage dehazing and beyond",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Pengyue Li",
                "Mengshen Yun",
                "Jiandong Tian",
                "Yandong Tang",
                "Guolin Wang",
                "Chengdong Wu"
            ],
            "title": "Stacked dense networks for single-image snow removal",
            "year": 2019
        },
        {
            "authors": [
                "Ruoteng Li",
                "Robby T Tan",
                "Loong-Fah Cheong"
            ],
            "title": "All in one bad weather removal using architectural search",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhi Li",
                "Juan Zhang",
                "Zhijun Fang",
                "Bo Huang",
                "Xiaoyan Jiang",
                "Yongbin Gao",
                "Jenq-Neng Hwang"
            ],
            "title": "Single image snow removal via composition generative adversarial networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yun-Fu Liu",
                "Da-Wei Jaw",
                "Shih-Chia Huang",
                "Jenq- Neng Hwang"
            ],
            "title": "Desnownet: Context-aware deep network for snow removal",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Soo-Chang Pei",
                "Yu-Tai Tsai",
                "Chen-Yu Lee"
            ],
            "title": "Removing rain and snow in a single image using saturation and visibility features",
            "venue": "In 2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW),",
            "year": 2014
        },
        {
            "authors": [
                "Sucheng Ren",
                "Daquan Zhou",
                "Shengfeng He",
                "Jiashi Feng",
                "Xinchao Wang"
            ],
            "title": "Shunted self-attention via multi-scale token aggregation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Byungseok Roh",
                "JaeWoong Shin",
                "Wuhyun Shin",
                "Saehoon Kim"
            ],
            "title": "Sparse detr: Efficient end-to-end object detection with learnable sparsity",
            "venue": "arXiv preprint arXiv:2111.14330,",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Robin Strudel",
                "Ricardo Garcia",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Segmenter: Transformer for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxim: Multi-axis mlp for image processing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jeya Maria Jose Valanarasu",
                "Rajeev Yasarla",
                "Vishal M Patel"
            ],
            "title": "Transweather: Transformer-based restoration of images degraded by adverse weather conditions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Wang",
                "Li Yuan",
                "Yunpeng Chen",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Pnp-detr: Towards efficient visual analysis with transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yinglong Wang",
                "Shuaicheng Liu",
                "Chen Chen",
                "Bing Zeng"
            ],
            "title": "A hierarchical approach for rain or snow removing in a single color image",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Yingming Wang",
                "Xiangyu Zhang",
                "Tong Yang",
                "Jian Sun"
            ],
            "title": "Anchor detr: Query design for transformer-based detector",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Jongchan Park",
                "Joon-Young Lee",
                "In So Kweon"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jing Xu",
                "Wei Zhao",
                "Peng Liu",
                "Xianglong Tang"
            ],
            "title": "An improved guidance image based method to remove rain and snow in a single image",
            "venue": "Computer and Information Science,",
            "year": 2012
        },
        {
            "authors": [
                "Tao Yan",
                "Yuyang Ding",
                "Fan Zhang",
                "Ningyu Xie",
                "Wenxi Liu",
                "Zhengtian Wu",
                "Yuan Liu"
            ],
            "title": "Snow removal from light field images",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Shujian Yu",
                "Yixiao Zhao",
                "Yi Mou",
                "Jinghui Wu",
                "Lu Han",
                "Xiaopeng Yang",
                "Baojun Zhao"
            ],
            "title": "Content-adaptive rain and snow removal algorithms for single image",
            "venue": "In Advances in Neural Networks\u2013ISNN 2014: 11th International Symposium on Neural Networks, ISNN 2014, Hong Kong and Macao, China, November 28-December",
            "year": 2014
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang"
            ],
            "title": "Restormer: Efficient transformer for high-resolution image restoration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Haokui Zhang",
                "Wenze Hu",
                "Xiaoyu Wang"
            ],
            "title": "Parc-net: Position aware circular convolution with merits from convnets and transformer",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Kaihao Zhang",
                "Rongqing Li",
                "Yanjiang Yu",
                "Wenhan Luo",
                "Changsheng Li"
            ],
            "title": "Deep dense multi-scale network for snow removal using semantic and depth priors",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Sixiao Zheng",
                "Jiachen Lu",
                "Hengshuang Zhao",
                "Xiatian Zhu",
                "Zekun Luo",
                "Yabiao Wang",
                "Yanwei Fu",
                "Jianfeng Feng",
                "Tao Xiang",
                "Philip HS Torr"
            ],
            "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xianhui Zheng",
                "Yinghao Liao",
                "Wei Guo",
                "Xueyang Fu",
                "Xinghao Ding"
            ],
            "title": "Single-image-based rain and snow removal using multi-guided filter",
            "venue": "In Neural Information Processing: 20th International Conference,",
            "year": 2013
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycleconsistent adversarial networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "arXiv preprint arXiv:2010.04159,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The main purpose of single image snow removal is to remove snow particles in the foreground of the image and snow fog in the background. Snow particles can obscure the foreground of the image, which affects the machine\u2019s observation of the foreground object. Snow fog adds a foggy or hazy effect to the image which makes it blurry. Therefore, the performance of the algorithm is reduced when performing high-level vision tasks such as image classification, object detection and semantic segmentation in snowy scenes. This appears to be particularly important for single image desnowing tasks.\nSingle image snow removal is more difficult than other\n*Corresponding author.\u2020Equal contribution.\nsevere weather image recovery tasks. As can be seen in Figure 10, the shape of the snow particles shows more irregularity and diversity compared to the rain streak. The combination of snow fog and snow particles causes the image more difficult to recognize compared to haze scene. Many methods have been proposed to meet the challenge posed by the single image desnowing task.\nOverall, these methods can be broadly divided into three categories: manual feature-based snow removal methods [1, 31, 40, 45\u201347, 53], convolutional hierarchy-based snow removal methods [9, 10, 25\u201327, 29] and attention-based snow removal methods [11, 38]. The manual feature-based snow removal methods fully utilize a prior knowledge to strip snow particles from the image background. However,\nar X\niv :2\n30 3.\n09 98\n8v 1\n[ cs\n.C V\n] 1\n7 M\nar 2\n02 3\ndue to the complex and variable shape of snow particles, these methods are poor in generalization to other snow scenarios. To solve the above issue, Liu et al. [29] proposed the first convolution-based snow removal algorithm. Chen et al. [9] proposed a convolutional snow removal algorithm based on snow particle shape and transparency, which performs the snow removal task by image recovery. And they also used the properties of dual-tree wavelet transform in HDCW-Net [10] to better learn the complex shape of snow particles. To learn the global structure of severe weather images, Li et al. [26] designed an attention-based unified severe weather recovery transformer network. And this network performs well in a single image desnowing task. In addition, Tu et al. [37] designed a general-purpose vision backbone based on a multi-axis multi-layer perceptron (MLP) architecture that achieved promising results on a variety of severe weather image recovery tasks. Despite the impressive performance achieved by these methods, there is still room for improvement in these solutions.\nOverall, there are two issues that need to be addressed for the single image desnowing task nowadays (see Figure 11).\n(i) Diversity and irregularity of snow particles shape: Snow particles exist in varying sizes and irregular shapes. This poses a huge challenge for single image desnowing work. As can be seen in Figure 11, most single image desnowing methods only consider snow particle at a single scale and are not able to do both removal for large snow particles and small snow particles. This can be solved with the help of multi scale features.\n(ii) Distortion after single image desnowing: The sharpness of the snow removal image is also an important indicator to evaluate the quality of the single image desnowing model. In Figure 11, although HDCW-Net is better at removing snow particles of different scales, there is a serious image distortion problem in the image after desnowing. Enhanced local modeling helps image after desnowing to maintain the sharpness of the original image.\nTherefore, in this paper, we propose a novel single image desnowing transformer network to address the above issues called Star-Net. Firstly, we design a Star type Skip Connection (SSC) for Star-Net. To enable effective inter-\naction of features at different scales, SSC establishes information channels for different levels of encoder and decoder modules as well as the Degenerate Filter Module (DFM) we designed. This introduce a multi scale characteristic for Star-Net to learn the complex and variable shapes of snow particles. Secondly, we design a Multi-Stage Interactive Transformer (MIT) as the basic unit of Star-Net, which explicitly models several important recovery features through multiple attention interactions and multi scale deep convolution. This further makes Star-Net focus on multi scale feature modeling capabilities and introduce rich local details for solving image distortion issue. In addition, we find snow particles and snow fog residuals exist in the SSC (see Figure 9). Thus, we propose a DFM for SSC by combing spatial attention module and channel gating module.\nNumerous experiments show that Star-Net achieves state-of-the-art (SOTA) performance on several standard snow removal datasets. Specifically, Star-Net achieves a PSNR of 44.04dB and SSIM of 0.99 on CSD [10]. In addition, Star-Net obtains PSNR gains of 0.14dB and 0.12dB on the SRRS [9] and Snow100K [29] datasets, respectively."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Single image snow removal",
            "text": "Liu et al. [29] proposed the first deep learning-based snow removal network and the first large benchmark dataset for snow removal. This method removes the image snow information step-by-step by labeling the snowflakes with attributes and a multi-stage network. Li et al. [25] proposed a snow removal algorithm that combines the physical imaging model of snowflakes and DenseNet [20]. Li et al. [27] designed an image desnowing algorithm based on generative adversarial networks. Chen et al. [9] achieved image desnowing with different snow concentrations by adding transparency perception to the snow scene images, allowing the model to obtain better results in both nontransparent and transparent snowy scenes. Additionally, they also added veiling effects to snow images to synthesize the SRRS desnowing dataset. Li et al. [26] designed a general network for severe weather such as rain, snow, and\nfog based on adversarial learning strategy. By combining dual-tree wavelet transform and convolutional neural network, Chen [10] designed a hierarchical progressive structure of snow removal network. These single image desnowing methods based on convolutional neural networks exhibit good generalization capabilities."
        },
        {
            "heading": "2.2. Vision Transformer in low-level visual tasks",
            "text": "Recently, ViT [14] has made great progress in computer high-level vision tasks such as, image classification [5, 23, 30, 50], object detection [4, 8, 33, 39, 41, 55] and semantic segmentation [7, 17, 19, 36, 44, 52]. Compared to CNN, the transformer has powerful long-range modeling capability, which greatly helps the model to learn the global information. Meanwhile, more and more attention has been paid to the application of the transformer in low-level visual tasks. Chen et al. [6] first introduced transformer to low-level vision by pre-training on ImageNet [13] and finetuning it in specific datasets to achieve tasks such as image denoising, deraining, and super-resolution. Through combining transformer block and U-Net, Wang et al. [42] used deep separable convolution as feedforword network to accomplish modeling of contextual information for image denoising, rain removal, and deblurring. Liang et al. [28] developed an image restoration network based on Swin Transformer [30]. Nevertheless, the above work has a very large computational overhead due to the long range modeling. To solve this problem, Zamir et al. [48] designed a transformer for the channel domain to replace original computation for the spatial domain, which performs remarkable in tasks such as deraining, denoising and deblurring. Valanarasu et al. [38] proposed an end-to-end network which enabled the recovery of multiple severe weather images together. All these transformer-based methods show SOTA performance in low-level vision tasks."
        },
        {
            "heading": "3. Method",
            "text": "As shown in Figure 12, Star-Net is mainly composed of three key designs: Star type Skip Connection, MultiStage Interactive Transformer and Degenerate Filter Module. SSC introduces multi scale characteristics to Star-Net. MIT implements explicit modeling of several important recovery features. DFM helps filter snow particles and snow fog degradation residuals in SSC."
        },
        {
            "heading": "3.1. Star type Skip Connection",
            "text": "The Star-Net backbone (Figure 12) adopts U-Net\u2019s encoder-decoder design guidelines. We argue that the multi scale structural design facilitates the model to learn the complex and variable shapes of snow particles compared to single scale skip connection in U-Net [34]. Therefore, we create connections for each unit of Star-Net in each layer to\nunits that are not on the same layer. We call this star type skip connection method SSC.\nWe mainly analyze SSC in three information flow directions: from encoder module to DFM, from DFM to decoder module, and from DFM to DFM.\nFirst, we define three operations of SSC: upsampling connection, downsampling connection, and connection. Given the inputs F0, ...,Fn (n denotes the number of network layers), we define the downsampling connection for ith layer as:\ndownsamplingi(F) = i\u22121\u2211 j=0 conv1(conv3(Fj)), (1)\nwhere conv3(\u00b7) denotes a downsampling operation with 3\u00d7 3 convolution kernels used to adjust different features to fit the shape of ith layer feature shape. conv1(\u00b7) represents the alignment of the channel dimension to the ith layer feature with a 1\u00d7 1 convolution kernel.\nThe upsampling connection for ith layer is computed as:\nupsamplingi(F) = n\u2211\nj=i+1\nconv1(deconv3(Fj)), (2)\nwhere deconv3(\u00b7) denotes a upsampling operation with 3\u00d73 convolution kernels for adjusting different features to fit the shape of ith layer feature shape.\nThe ith layer connection can be expressed as:\nconnecti(F) = conv1(Fi), (3)\nFrom Encoder Module to DFM. Formally, given the encoder outputs for each layer ei, i \u2208 [0, n], for the DFM at each layer, it receives the multi scale information streams from each layer encoder module via SSC. This can be indi-\ncated as:\nSSCi(e) = connecti(e) + upsamplingi(e) + downsamplingi(e), i \u2208 [0, n], (4)\nFrom DFM to Decoder Module. Specially, given the DFM outputs for each layer di, for the decoder module at each layer, it receives the multi scale information streams from each layer DFM via SSC. This can be shown as:\nSSCi(d) = connecti(d) + upsamplingi(d) + downsamplingi(d), i \u2208 [0, n], (5)\nFrom DFM to DFM. Suppose the output of DFM at each layer are m0, ...,mn. For each layer of DFM, it receives the output from the previous layer of DFM via SSC.\nSSCi(m) = conv(mi-1), i \u2208 [1, n], (6)\nFor different layer of DFM output m, we adopt different size convolution kernel conv(\u00b7) for scaling (For example, we use a convolutional kernel of size 7 for m0 and a convolutional kernel of size 5 for m1). The motivation for this branch design is to further enhance the multi scale characteristic of Star-Net and to integrate all multi scale information fused by DFM into the bottleneck of Star-Net."
        },
        {
            "heading": "3.2. Multi-Stage Interactive Transformer",
            "text": "To address the image distortion issue that occurs in the single image desnowing in Figure 11 and to enhance the learning ability of Star-Net for snow particles, we carefully thought about the design of the Star-Net basic unit.\nFor the image distortion issue, we solve it by enhancing the local modeling capability in parallel from both the window attention (WA) mechanism [30] and convolution directions. To further strengthen Star-Net\u2019s ability to learn\nthe variable shape of snow particles, we also introduce the multi scale attention (MSA) mechanism [32] in the basic unit. Considering the problem of large scale snow particle removal, we use global features to assist the solution. This leads us to introduce the criss cross attention (CCA) mechanism [21] that is computationally low and has global modeling capabilities. Considering that feature channels are rich in image recovery information [48] and the channel attention (CA) mechanism has the advantage of small computational cost, we introduce it into the basic unit of Star-Net. In addition, we replace the vanilla feedforward network with a convolutional gating network to further enhance the snow particle removal capability. We call the unit combined with the above design as Multi-Stage Interactive Transformer (Figure 13). Next, we explain in detail several key designs of MIT. MIT is composed of multi stage attention mechanisms, multi scale deep convolution, and convolutional gating network.\nMulti Stage Attention Mechanisms. Given an input feature G \u2208 RH\u00d7W\u00d7C , we first perform a channel shuffle operation on it and divide it into Gi \u2208 RH\u00d7W\u00d7 C 4 , i \u2208 [1, ..., 4] according to the channel dimension. The channel shuffle operation ensures the disorder of each channel feature and avoids the single attention modeling always executed for the same channel feature. Overall, multi stage attention can be modeled as:\nG1 = WA(G1), G2 = MSA(G2), G3 = CCA(G3), G4 = CA(G4), G = [G1,G2,G3,G4], T = CBAM(CA(G)),\n(7)\nwhere [\u00b7] denotes the concat operation and CBAM(\u00b7) is the convolutional block attention module [43]. The reason we introduce lightweight CBAM for MIT and CA for G is to\nintegrate image recovery features modeled by different attention mechanisms. Multi Scale Deep Convolution. For the input feature G, we perform multi scale modeling and local modeling from a convolutional perspective. The multi scale deep convolution can be computed as:\nH1 = conv3(conv3(conv3(G))), H2 = conv5(conv5(conv5(G))), H3 = conv7(conv7(conv7(G))), H = H1 + H2 + H3,\n(8)\nwhere conv5(\u00b7) and conv7(\u00b7) represent convolutions with convolution kernels of 5 and 7, respectively. Convolutional Gating Network. We use a convolutional gating network as a feedforward network for MIT to further remove snow particle residuals, which can be expressed as:\nO = H + T, [O1,O2] = \u03d5(conv3(O)), O1 = sigmoid(O1),O2 = ELU(O2), O = O1 \u2297 O2,\n(9)\nwhere \u03d5(\u00b7) denotes the split operation on the channel dimension, sigmoid(\u00b7) and ELU(\u00b7) denotes the sigmoid activation function [16] and ELU activation function [12], respectively. And \u2297 denotes the Hadamard product of matrices."
        },
        {
            "heading": "3.3. Degenerate Filter Module",
            "text": "To prevent the SSC from transmitting snow particles and snow fog residuals to Star-Net\u2019s decoder module, we design the degradation filter module (Figure 14) in the SSC. In addition to the possible presence of snow particles and snow fog residuals in the spatial domain, they may also be present on the channel domain. Thus, our DFM takes into account both spatial domain and channel domain snow particles and snow fog residuals.\nSpecifically, DFM consists of a spatial self-attention module and channel gating module. The spatial selfattention module mainly deals with snow residuals in the\nspatial domain. The channel gating module mainly deals with snow residuals in the channel domain.\nSpatial Self-Attention Module. Given embedding patches O, spatial self-attention module strips snow particles and snow fog residuals from the image by interacting with each patch on the space. The spatial self-attention module is modeled as:\nQ = WQO,K = WKO,V = WVO,\nO = softmax( QKT\u221a\nd )V,\n(10)\nwhere WQ,WK,WV represent the mapping matrices, respectively. softmax(\u00b7) is the softmax activation function [3]. d indicates the embedding dimension.\nChannel Gating Module. For the spatial self-attention module output, we adopt gating mechanism to control the information flow in each channel. This allows DFM to remove as much snow residuals as possible trapped in the channel. The channel gating module can be expressed as follows:\nO1 = conv1(dwconv3(O)), O2 = conv1(dwconv3(O)), O = conv1(GMP(GELU(O1)) O2),\n(11)\nwhere dwconv3(\u00b7) denotes a deep convolution with a convolution kernel size of 3. GELU(\u00b7) represents the GELU activation function [18] and means the dot product of elements. GMP(\u00b7) indicates that the global max pooling operation is used to refine the channel information."
        },
        {
            "heading": "3.4. Loss Function",
            "text": "The loss function of Star-Net is divided into two parts, the smooth L1-loss [15] between the predicted and ground truth and the perceptual loss of the features extracted by the VGG16 [35] network. First, we use the smooth L1-loss based on the correspondence between the predicted (P) and ground truth (G), which can be expressed as:\nLSmoothL1 = { 0.5\u03b82, if |\u03b8| < 1 |\u03b8| \u2212 0.5, otherwise (12)\nwhere \u03b8 = P - G. Secondly, the perceptual loss is used by Star-Net to measure the difference between predicted and ground truth features. We extract the features in P and G separately by pretraining VGG16 network on ImageNet [13], defined as follows:\nLperceptual = MSE(VGG16(P),VGG16(G)) (13) where MSE(\u00b7) is mean square error [2]. In total, our total loss can be expressed as:\nLtotal = LSmoothL1 + aLperceptual (14) where a = 0.04 is the hyperparameter of Star-Net used to control the percentage of smooth L1-loss and Lperceptual in the total loss."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Dataset and Implementation Detail",
            "text": "Datasets. We conduct extensive experiments on three snow removal benchmark datasets, Snow100k [29], SRRS [9], and CSD [10] to show the effectiveness of Star-Net in a single image desnowing task. The specific distribution of the three datasets is shown in Table 5. Details of the datasets and the division of the dataset can be found in Appendix A. Settings. During training, we use the Adam optimizer [22] and set the base learning rate to 2e-5. The total training epochs are set to 210 and we halve the learning rate every 40 epochs to achieve dynamic adjustment for the learning rate. The input image size is uniformly cropped to 224x224. The default training batch size is 2. Our experiments are implemented on a single Nvidia RTX 3090. The specific experimental setup is shown in Appendix B. Comparison Methodology and Evaluation Metrics. We select single image desnowing algorithms [9,10,29,51,54],\nadverse severe weather image recovery algorithms [11, 26, 38], and image recovery algorithms [37, 48, 49] for comparison. For quantitative evaluation, we use two metrics, peak signal to noise ratio (PSNR) and structural similarity (SSIM), to evaluate the experimental results."
        },
        {
            "heading": "4.2. Quantitative Evaluations",
            "text": "The results of the quantitative evaluation of Star-Net on the CSD dataset are shown in Table 6. As shown in Table 6, Star-Net exhibits SOTA performance in the single image desnowing task and scores an amazing 44.04dB PSNR and 0.99 SSIM. Specifically, Star-Net gains 14.98dB in PSNR and 0.08 in SSIM compared to HDCW-Net [10]. Compared to TKL [11], Star-Net has a gain of 10.15dB in PSNR and 0.03 in SSIM. In addition, Star-Net improves PSNR by 5.06dB and SSIM by 0.01 compared to Maxim [37].\nIn Table 6, our Star-Net exhibits SOTA snow removal performance in the quantitative evaluation of the SRRS dataset. Formally, compared with DDMSNet [51], our StarNet achieves a PSNR gain of 5.38dB and has an SSIM improvements of 0.06. Star-Net achieves a PSNR gain of 4.12dB and an SSIM gain of 0.05 compared to TransWeather [38]. In the SRRS dataset, Restormer [48] shows a PSNR of 32.24dB and SSIM of 0.96. Star-Net shows a PSNR of 32.41dB and SSIM of 0.97.\nAs shown in Table 6, Star-Net performs best on the quantitative evaluation of the Snow100K dataset. Specially, we score a PSNR of 34.79dB and a SSIM of 0.96. This is demonstrated by the fact that we achieve a PSNR gain of 7.98dB and a SSIM improvement of 0.07 compared to Cycle-GAN [54]. Compared to All in One [26], Star-Net achieves a PSNR improvement of 8.72dB and SSIM gain of 0.08. Furthermore, Star-Net achieves a PSNR gain of 0.28dB and an SSIM improvement of 0.01 compared to MPRNet [49]."
        },
        {
            "heading": "4.3. Qualitative Evaluations",
            "text": "Figure 15 shows the results of the qualitative comparison of Star-Net with several algorithms on CSD dataset. Although these methods achieve considerable snow removal on the CSD dataset, they still have some shortcomings. DesnowNet [29] and JSTASR [9] suffer from snow particle residual issue, and HDCW-Net [10] suffers from image distortion. TransWeather [38] for severe weather also suffers from snow particle residuals and blurred images after snow removal. Maxim [37] and our method exhibit better desnowing performance and the recovered image is closer to the ground truth. But there is a slight color difference in Maxim.\nFigure 16 visualizes the snow removal results of StarNet with several SOTA methods on the SRRS dataset. The\ncomparison algorithm in Figure 16 shows excellent single image desnowing ability on the SRRS dataset. However, the rectangular border areas of DesnowNet [29], JSTASR [9], HDCW-Net [10], and TransWeather [38] desnowed images are still somewhat different from the ground truth. For Maxim [37], artifact issue appear on the SRRS dataset. Our Star-Net successfully removes all sizes of snow particles while reatining the closet clarity to the ground truth.\nFigure 17 shows the snow removal effect of different algorithms on Snow100K. Though HDCW-Net [10], JSTASR [9] and TransWeather [38] remove most of the snow particles, there are still some snow particles with small scales remain. Maxim [37] and DesnowNet [29] successfully removes snow particles from images. However, DesnowNet fails to recover the rectangular area in Figure 17 very well. And there are color biases in Maxim desnowing image compared to the ground truth. Compared with these SOTA methods, the clear images recovered by our Star-Net are much closer to the ground truth. See Appendix D for more visualization results."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "To verify the effectiveness of the proposed algorithm, we perform ablation experiments on the three key designs of Star-Net. The settings in the ablation experiments are kept with the comparison experiments, and all the ablation ex-\nperiments are performed on the CSD dataset. Effectiveness of SSC. To demonstrate the enhancement of SSC for Star-Net single image desnowing ability, we replaced SSC with U-Net\u2019s [34] same layer skip connection for ablation studies. As shown in Table 7, the PSNR has a 4.63 dB increase with the introduction of SSC, and the SSIM also has a 0.01 increase. This is because the SSC establishes information channels for different scale feature maps. These information channels can help Star-Net distinguish different sizes of snow particles and thus contribute to their removal. In addition, we show the qualitative and quantitative ablation results of SSC on Snow100K in Appendix C.\nEffectiveness of MIT. We verify the effectiveness of the proposed MIT and the MIT internal multi stage attention mechanisms, multi scale deep convolution and convolutional gating network. For the ablation design of MIT, we replace MIT with the basic module of Restormer [48]. As shown in Table 7, after removing MIT, the PSNR metric of Star-Net decreases by 4.31dB and the SSIM metric decreases by 0.01. For the three key modules within MIT, we also design ablation experiments for them. For the multi stage attention mechanisms, in addition to the ablation studies that removed it, we also design ablation experiments that replace it with the vanilla attention (VA) mechanism. For the multi scale deep convolution and convolutional gating network ablation design, we remove them separately. As can be seen from the experimental results in Table 8, the removal of any key design within MIT leads to a decline in the evaluation metrics of Star-Net. Effectiveness of DFM. To prove the validity of the DFM, we remove it from Star-Net. The experimental results are shown in Table 7. DFM helps our Star-Net to improve\n2.12dB in PSNR and 0.01 in SSIM. This is because the DFM helps SSC filter out snow particle residuals contained in the spatial and channel level, resulting in more effective single-image desnowing. The feature visualization results in Figure 9 prove our hypothesis."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a novel end to end network for single image desnowing tasks called Star-Net. Specifically, Star-Net adopts a star type connection design SSC to introduce multi scale characteristics to remove snow particles with variable shapes. We design MIT that can explicitly model a variety of important image recovery features to address the image distortion issue related to the recovered image. In addition, by adding DFM to SSC for residual snow particle filtering on the spatial and channel domains, we further enhance the desnowing ability of Star-Net. Extensive experiments demonstrate that Star-Net achieves SOTA performance on the single image desnowing task.\nAcknowledge This work was supported by Public-welfare Technology Application Research of Zhejiang Province in China under Grant LGG22F020032, and Key Research and Development Project of Zhejiang Province in China under Grant 2021C03137.\nAppendix"
        },
        {
            "heading": "A. Dataset Details",
            "text": "Snow\nGround Truth\nSnow\nGround Truth\nSnow\nGround Truth\nSnow100K CSDSRRS\nFigure 10: Samples of three benchmark datasets Snow100K, SRRS, and CSD which are used in the experiments.\nIn the paper, we list the datasets used for the experiment. Next, we describe the details of each dataset and the division. Samples of the Snow100k [29], SRRS [9], and CSD [10] datasets are shown in Figure 10. Snow100K. Snow100k dataset is synthesized from 100,000 snow images by setting different sizes, densities, shapes, transparency and motion trajectories of snow particles. Furthermore, the synthesized snow images are classified into three categories: Snow100K-S, Snow100K-M, and Snow100K-L for different snowfall amounts. Each image contains three parts: ground truth, synthetic snow images, and snow masks. In addition, the dataset retains the original aspect ratio of each image and normalizes the maximum boundary to 640 pixels. Snow Removal in Realistic Scenario (SRRS). SRRS dataset contains 15,000 synthetic snow images and another 1,000 snow images downloaded from the Internet for realistic scenes. At the same time, the haze dataset RESIDE [24] is used to synthesize images with masking effect, which makes the synthesized snow images closer to the real scenes. Comprehensive Snow Dataset (CSD). CSD consists of 10,000 synthetic snow images. During the image synthesis, the haze dataset RESIDE is applied to this dataset while following the synthesis method in the SRRS dataset to achieve the veiling effect. The dataset synthesizes different snowflakes and snow streaks for properties such as transparency, size and, position. A Gaussian blur is also applied to the snow particles to achieve a better simulation of real-world snow scenes.\nIn the single image desnowing experiment of Snow100k, we randomly select 10,000 pairs of snow images from Snow100K-S, Snow100K-M, and Snow100K-L. Then we divide 8000 pairs of snow images out of 10000 pairs as the training set and 2000 pairs of snow images as the test set. In the SRRS dataset, we also randomly selected 8000 pairs from 15000 pairs of snow images as the training set and 2000 pairs as the test set. For the CSD dataset, we follow the official dataset settings for training and testing."
        },
        {
            "heading": "B. Experimental Details",
            "text": "Architecture details. Table 5 shows the detailed architecture configurations of Multi Stage Attention Mechanisms in each layer of MIT. In Table 5, D denotes the embedding dimension, and P indicates the patch size. H denotes the number of attention heads, and C indicates the feature map channel dimension. BIAS denotes whether bias is used in the convolution. In Multi Scale Attention mechanism, SR denotes the ratio of down-sampling. Qc, Kc, and Vc in Criss Cross Attention mechanism denote the number of channels of query, key, and value. K denotes the kernel size used in this convolutional layer. WS in Window Attention mechanism represents the window size of this layer. LN represents layer normalization. In the CBAM, R denotes the compression ratio of the spatial dimension and K denotes the size of the convolution kernel used for spatial attention in CBAM. We show the specific details of Multi Scale Deep Convolution in Table 6. In Channel indicates the input dimension of Multi Scale Deep Convolution in this layer. Out Channel represents the final output dimension of Multi Scale Deep Convolution. Low, Middle, and High in Kernel Size denote the size of convolution kernel in different scale convolution operations. Stride denotes the step size of convolution. Low, Middle, and High in Padding indicate the number of padding pixels in different scale convolution operations.\nDepth means the depth of Multi Scale Deep Convolution. Table 7 describes the specific configuration of the Convolutional Gating Network. In Channel and Out Channel represent the input and output dimensions of the gated convolution. Kernel Size and Stride represent the size and step size of the convolution kernel used in the gated convolution, correspondingly.\nTable 8 illustrates the specific configuration of each hyperparameter in DFM. Patch Size denotes the size of the patch in the Spatial Self-Attention Module. Channel indicates the number of channels in the DFM feature map for each layer. Embed Dim denotes the embedding dimension of the DFM Spatial Self-Attentive Module. Num Heads denotes the number of attention heads of the Spatial SelfAttention Module. Expansion Factor denotes the multiplier\nof MLP hidden layer dimension expansion. Kc represent the size of the convolution kernel, and Kn denotes the size of the convolutional kernel used for the DFM-to-DFM branch of the different layers of SSC. Groups represent the number of channel groups in DFM Channel Gating Module. More details about implementation. We scale the image to 224 \u00d7 224 pixels. We set the exponential decay rate \u03b21 and \u03b22 of the Adam optimizer [22] to 0.9 and 0.999, and the weight decay to 1e-8. In the perceptual loss, we calculate it with the features extracted from layers 3, 8, and 15 of the VGG16 network [35]. In smooth L1-loss [15], we set the \u03b2 to 1. Figure 11 shows the dynamic evolution of the learning rate during Star-Net training on the CSD dataset. In addition, we demonstrate the fluctuations of PSNR and SSIM in Figure 12 and Figure 13."
        },
        {
            "heading": "C. Experimental Supplement",
            "text": "In this section, we complement the SSC ablation experiments in Section 4.5 of the main text. Specifically, we ablate the SSC in Snow100K following the same settings as described in the main text. And the ablation results of SSC on Snow100K are shown in Table 9 and Figure 14, respec-\ntively. As shown in Table 9, the SSC ablation experiments on Snow100K exhibited the same results on CSD. As shown in Figure 14, when Star-Net uses the same-layer skip connection of U-Net [34], it removes most snow particles from the input image by MIT and DFM. However, there are still minor small-sized snow particles remaining in the rectangular area being plotted. After the introduction of SSC, all sizes of snow particles in the input image are completely removed. The above results illustrate that the structural design of the multi scale skip connection makes the model more effective in snow removal in scenarios with different sizes of snow particles compared to the single-scale skip connection."
        },
        {
            "heading": "D. Additional Visual Comparisons",
            "text": "We present more visualization comparison results of Star-Net with other algorithms on CSD, SRRS, and\nSnow100k datasets in Figure 15, Figure 16, and Figure 17. Compared with other algorithms, our Star-Net is more effective in removing snow particles in different snow scenes. And our recovered images are closer to the ground truth."
        }
    ],
    "title": "Star-Net: Improving Single Image Desnowing Model With More Efficient Connection and Diverse Feature Interaction",
    "year": 2023
}