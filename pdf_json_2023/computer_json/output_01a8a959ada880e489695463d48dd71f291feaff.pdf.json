{
    "abstractText": "The International Fingerprint Liveness Detection Competition (LivDet) is a biennial event that invites academic and industry participants to prove their advancements in Fingerprint Presentation Attack Detection (PAD). This edition, LivDet2023, proposed two challenges, \u201cLiveness Detection in Action\u201d and \u201cFingerprint Representation\u201d, to evaluate the efficacy of PAD embedded in verification systems and the effectiveness and compactness of feature sets. A third, \u201chidden\u201d challenge is the inclusion of two subsets in the training set whose sensor information is unknown, testing participants\u2019 ability to generalize their models. Only bona fide fingerprint samples were provided to participants, and the competition reports and assesses the performance of their algorithms suffering from this limitation in data availability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Marco Micheletto"
        },
        {
            "affiliations": [],
            "name": "Roberto Casula"
        },
        {
            "affiliations": [],
            "name": "Giulia Orr\u00f9"
        },
        {
            "affiliations": [],
            "name": "Simone Carta"
        },
        {
            "affiliations": [],
            "name": "Sara Concas"
        },
        {
            "affiliations": [],
            "name": "Simone Maurizio La Cava"
        },
        {
            "affiliations": [],
            "name": "Julian Fierrez"
        },
        {
            "affiliations": [],
            "name": "Gian Luca Marcialis"
        }
    ],
    "id": "SP:a53f93f02754cd148496327e6d4712f9396a44e0",
    "references": [
        {
            "authors": [
                "F. Alonso-Fernandez",
                "J. Fierrez"
            ],
            "title": "Fingerprint Databases and Evaluation",
            "venue": "pages 599\u2013606. Springer",
            "year": 2015
        },
        {
            "authors": [
                "R. Casula",
                "M. Micheletto",
                "G. Orr\u00fa",
                "G.L. Marcialis",
                "F. Roli"
            ],
            "title": "Towards realistic fingerprint presentation attacks: The screenspoof method",
            "venue": "Pattern Recognition Letters",
            "year": 2022
        },
        {
            "authors": [
                "I. Chingovska",
                "A. Anjos",
                "S. Marcel"
            ],
            "title": "Anti-spoofing in action: Joint operation with a verification system",
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 98\u2013104",
            "year": 2013
        },
        {
            "authors": [
                "T. Chugh",
                "A.K. Jain"
            ],
            "title": "Fingerprint presentation attack detection: Generalization and efficiency",
            "venue": "2019 International Conference on Biometrics (ICB), pages 1\u20138. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "R.C. Contreras",
                "L.G. Nonato",
                "M. Boaventura",
                "I.A.G. Boaventura",
                "F.L.D. Santos",
                "R.B. Zanin",
                "M.S. Viana"
            ],
            "title": "A new multi-filter framework for texture image representation improvement using set of pattern descriptors to fingerprint liveness detection",
            "venue": "IEEE Access, 10:117681\u2013117706",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ding",
                "A. Ross"
            ],
            "title": "An ensemble of one-class svms for fingerprint spoof detection across different fabrication materials",
            "venue": "2016 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1\u20136. IEEE",
            "year": 2016
        },
        {
            "authors": [
                "J.J. Engelsma",
                "A.K. Jain"
            ],
            "title": "Generalizing fingerprint spoof detector: Learning a one-class classifier",
            "venue": "2019 International Conference on Biometrics (ICB), pages 1\u20138. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "J. Galbally",
                "R. Cappelli"
            ],
            "title": "An evaluation of direct attacks using fake fingers generated from iso templates",
            "venue": "Pattern Recognition Letters,",
            "year": 2010
        },
        {
            "authors": [
                "J. Galbally",
                "J. Fierrez",
                "R. Cappelli",
                "G.L. Marcialis"
            ],
            "title": "Introduction to Presentation Attack Detection in Fingerprint Biometrics",
            "venue": "Springer",
            "year": 2023
        },
        {
            "authors": [
                "J. Galbally",
                "S. Marcel",
                "J. Fierrez"
            ],
            "title": "Image quality assessment for fake biometric detection: Application to iris, fingerprint and face recognition",
            "venue": "IEEE Trans. on Image Processing,",
            "year": 2014
        },
        {
            "authors": [
                "A. Hadid",
                "N. Evans",
                "S. Marcel",
                "J. Fierrez"
            ],
            "title": "Biometrics systems under spoofing attack: an evaluation methodology and lessons learned",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2015
        },
        {
            "authors": [
                "D. Maltoni",
                "D. Maio",
                "A.K. Jain",
                "J. Feng"
            ],
            "title": "Fingerprint matching",
            "venue": "Handbook of Fingerprint Recognition, pages 217\u2013297. Springer",
            "year": 2022
        },
        {
            "authors": [
                "E. Marasco",
                "C. Sansone"
            ],
            "title": "On the robustness of fingerprint liveness detection algorithms against new materials used for spoofing",
            "venue": "Multivariable Processing for Biometric Systems, volume 2, pages 553\u2013558. SCITEPRESS",
            "year": 2011
        },
        {
            "authors": [
                "M. Micheletto",
                "G.L. Marcialis",
                "G. Orr\u00f9",
                "F. Roli"
            ],
            "title": "Fingerprint recognition with embedded presentation attacks detection: are we ready? IEEE Transactions on Information Forensics and Security",
            "venue": "16:5338\u20135351",
            "year": 2021
        },
        {
            "authors": [
                "M. Micheletto",
                "G. Orr\u00f9",
                "R. Casula",
                "G.L. Marcialis"
            ],
            "title": "Mitigating sensor and acquisition method-dependence of fingerprint presentation attack detection systems by exploiting data from multiple devices",
            "venue": "Applied Sciences, 12(19):9941",
            "year": 2022
        },
        {
            "authors": [
                "M. Micheletto",
                "G. Orr\u00f9",
                "R. Casula",
                "D. Yambay",
                "G.L. Marcialis",
                "S. Schuckers"
            ],
            "title": "Review of the fingerprint liveness detection (livdet) competition series: from 2009 to 2021",
            "venue": "Handbook of Biometric Anti-Spoofing: Presentation Attack Detection and Vulnerability Assessment, pages 57\u201376",
            "year": 2023
        },
        {
            "authors": [
                "G. Orr\u00f9",
                "P. Tuveri",
                "L. Ghiani",
                "G.L. Marcialis"
            ],
            "title": "Analysis of \u201cuser-specific effect\u201d and impact of operator skills on fingerprint pad systems",
            "venue": "ICIAP 2019, pages 48\u201356, Cham",
            "year": 2019
        },
        {
            "authors": [
                "C. Sousedik",
                "C. Busch"
            ],
            "title": "Presentation attack detection methods for fingerprint recognition systems: a survey",
            "venue": "Iet Biometrics, 3(4):219\u2013233",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 9.\n15 57\n8v 1\n[ cs\n.C V\n\u00a92023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI:"
        },
        {
            "heading": "1 Introduction",
            "text": "Due to their convenience and security, fingerprint-based authentication systems have garnered significant attention in numerous applications, ranging from financial transactions to healthcare management [14]. However, these systems are vulnerable to various attacks, including spoofing or presentation attacks[13], where attackers use artificial replicas of live fingers to deceive the sensors [10]. Such attacks can lead to severe consequences, such as unauthorized access, identity theft, and financial fraud. To address this vulnerability, automated presentation attack detection (PAD) systems that utilize either hardware or software have been developed over the last few decades [20, 11]. Software-based methods, in particular, have seen significant advancements [12] thanks to pattern recognition research innovations and larger datasets\u2019 availability [3]. Among other initiatives, the International Fingerprint Liveness Detection Competition (LivDet)* [18], now in its eighth edition, has significantly promoted research and development in this area since its inception in 2009 and has become a well-known benchmark for assessing the effectiveness of PAD techniques.\nLivDet2023 presents two familiar challenges from the previous edition, \u201cLiveness Detection in Action\u201d and \u201cFingerprint Representation\u201d [18], while introducing new evaluation criteria and datasets. Challenge 2 now evaluates system speed to encourage efficient and practical PAD systems that can operate in time-critical real-world scenarios. The retention of previous challenges ensures continuity and comparability with past results.\nFurthermore, LivDet2023 has set out to tackle a new challenge often overlooked in previous competition editions: the issue of generalization. Generalization [6] refers to the ability of systems to detect the authenticity of fingerprints in a wide variety of sensor technologies, Presentation Attack Instruments (PAIs) and attack scenarios rather than being limited to specific, predefined conditions. Despite the improvements in the field, developing generalized PADs remains challenging for several reasons. Firstly, creating different PAIs for training is a non-trivial, expensive, and timeconsuming task requiring skilled operators. Secondly, even with a comprehensive training dataset, existing methods\n*https://sites.unica.it/livdet/\n- often struggle to detect PAs captured using new acquisition methods or materials [15, 17]. Previous editions of LivDet have provided strong evidence of this phenomenon [18].\nRecent works have focused on addressing the lack of interoperability across fabrication materials by using one-class classification [8, 9]. Unlike the traditional multi-class paradigm, one-class classification utilizes data from a single class, typically the bona fide class, for training the classifier. The ultimate objective of this approach is to establish a decision boundary around bona fide class samples that can accommodate as many samples as possible from that class while rejecting samples from other classes. Driven by the potential of this approach, we have added a \u201chidden\u201d challenge next to the previous ones. In particular, we included two subsets in the LivDet 2023 training set whose sensor information is unknown and contains only bona fide fingerprint samples. This challenge presents a valuable opportunity for participants to assess the effectiveness of their algorithms on unknown data, an essential aspect of realworld deployment. We hope this will increase LivDet2023\u2019s rigour and relevance while promoting the advancement of research and development in PADs."
        },
        {
            "heading": "2 LivDet2023",
            "text": "As in the previous edition, two distinct challenges characterize the LivDet2023 competition:\n\u2022 Challenge 1, Liveness Detection in Action [5]: Competitors were asked to submit a complete algorithm capable of producing both the \u201c score\u201d, that is, the probability of being a bona fide sample and the \u201cintegrated score\u201d, which combines the previous score with the probability of belonging to the claimed user. Participants in this challenge can choose whether to use the related \u201cuser-specific\u201d information [19].\n\u2022 Challenge 2, Fingerprint representation: Compactness and discriminability of feature vectors are critical in modern authentication systems to ensure high performance in terms of accuracy and speed. With the aim of evaluating speed and compactness, we asked competitors to submit PADs that return the feature vector corresponding to the input image in addition to the score.\nFurthermore, to evaluate the participating PADs\u2019 ability to generalize, an additional challenge, called Unknown sensors, is introduced: in the training set, two sensors were unknown, the name and brand of the sensor were not declared, and only bona fide fingerprint samples were provided."
        },
        {
            "heading": "2.1 Datasets and participants",
            "text": "Although the number of competitors was lower than in the previous edition, the competition showcased a diverse range of algorithms from each participant. It is worth noting that many competitors submitted multiple algorithms, which highlights their dedication to finding innovative solutions. Table 3 provides further details on each competitor, including the name of their presented algorithm, the type of solution adopted, and the challenge(s) in which they participated. In addition, we considered the quantity of data utilized by each participant in the training phase. In fact, some competitors have generated for each test set a model trained on data from the specific sensor (single, in Table 3); others have generated a single model trained with data from multiple sensors suitable for multiple test sets (multiple, in Table 3). Moreover, although we strongly advised utilizing only the LivDet 2023 dataset to maintain consistency in the results, some participants have employed additional data, which could have given them an advantage. Conversely, some competitors opted to use fewer data, typically omitting unknown sensors during the training phase. These instances will be designated by a plus (+) or minus (-) sign, respectively.\nThe LivDet 2023 training set and test set comprise four sub-sets containing fingerprint images from four different capture devices: GreenBit DactyScan 84C, Dermalog LF10, Jenetric LiveTouch Quattro and Integrated Biometrics Watson Mini.\nThe sensors can be grouped into two categories: known and unknown sensors. GreenBit and Dermalog are known sensors; therefore, we provided competitors with these devices\u2019 names, brands, and technical details. The training set for these sensors included 25 users, for a total of 2750 images, subdivided into 1250 bona fide and 1500 PAs collected with the classic consensual method. On the other hand, Jenetric and Integrated Biometrics were unknown sensors: we did not declare any information about these devices to the competitors. The training set included only bona fide fingerprint samples, totalling 1250 samples. To ensure the accuracy and dependability of the algorithms, our test sets were carefully designed to facilitate cross-material and cross-method experiments. In order to introduce more significant variability, we included synthetic fingerprints fabricated using materials different from those used in the training set. Furthermore, we incorporated presentation attacks generated with our semi-consensual ScreenSpoof technique [4], which is known for its ability to produce highly realistic forgeries. The test set is four times larger than the training set, comprising 2500 bona fide and 6000 attack presentations (including both consensual and ScreenSpoof-\n-\ngenerated PAIs) for known sensors. However, we deliberately included only 3000 ScreenSpoof-generated PAs for unknown sensors to create a more challenging scenario for the algorithms to detect and classify presentation attacks."
        },
        {
            "heading": "2.2 Algorithms submission",
            "text": "Algorithms for Challenge 1 must be submitted as console programs with the following parameters: [nameOfAlgorithm] [ndataset] [templateimagesfile] [probeimagesfile] [livenessoutputfile] [IMSoutputfile]. The parameter ndataset is an identification number for the dataset used. The file templateimagesfile contains a list of absolute paths to every template image stored in the system, while the file probeimagesfile contains a list of absolute paths to each probe image that the algorithm will test. The algorithm outputs are saved to the paths specified by the last two parameters. The file livenessoutputfile contains the degree of \u201cliveness\u201d for each processed image, normalized between 0 and 100, where 100 indicates the highest degree of liveness, and 0 denotes a fake image. Fingerprint images with scores [0,50) are classified as \u201cpresentation attack\u201d, while those with scores [50,100] are classified as \u201cbona fide\u201d. The file IMSoutputfile lists, for each probe image, the normalized probability of a fingerprint belonging to the declared identity and being authentic. Scores [0,50) classify the probe as \u201cpresentation attack\u201d or the probetemplate comparison as no-mated comparison, while scores [50,100] classify the comparison as bona-fide and mated. The evaluation threshold is set to 50. If the algorithm is unable to process an image, the corresponding value in both outputs is set to -1000.\nThe submission process for Challenge 2 in LivDet 2023 is the same as in LivDet 2021. In addition to the parameters nameOfAlgorithm, ndataset, probeimagesfile, and livenessoutputfile, Challenge 2 applications require an additional parameter called embeddingsfile, representing the file of feature vectors for each processed image."
        },
        {
            "heading": "2.3 Performance Evaluation",
            "text": "In both challenges, the performance of the PADs will be evaluated using the standard PAD ISO metrics [2, 1]:\n\u2022 PAD Accuracy: percentages of fingerprint images correctly classified by the PAD.\n\u2022 BPCER (Bona fide Presentation Classification Error Rate): Rate of misclassified bona fide images.\n\u2022 APCER (Attack Presentation Classification Error Rate): Rate of misclassified fake images.\nIn Challenge 1, to evaluate the performance of the integrated system, we employed the following metrics:\n\u2022 FNMR (False Non-Match Rate): Rate of mated comparisons that result in rejection.\n\u2022 FMR (False Match Rate): Rate of non-mated comparisons that result in acceptance.\n\u2022 IAPAR (Impostor Attack Presentation Accept Rate): rate of presentation attacks that result in acceptance.\n\u2022 Integrated Matching (IM) Accuracy: percentages of samples correctly classified by the integrated system.\nTo simulate real-world scenarios, we conducted comparisons using templates derived from bona fide fingerprints. The testing involved matching a fingerprint template to a fingerprint image from the same finger and user (mated), a fake fingerprint image from the same finger and user (presentation attack), or a bona fide fingerprint image from a different user (no-mated). Overall, we performed 5000 mated, 10000 no-mated and 10000 presentation attack comparisons.\n-\nChallenge 2 aimed to evaluate the compactness and the discriminability of feature vectors generated by various algorithms. We considered both the speed and size of the feature vectors to be essential parameters for this edition. To ensure fairness in the evaluation, we specified two machines where the algorithms were tested: a Desktop-PC Linux 18.04.1 Ubuntu or Windows 10 Pro system with an Intel\u00ae Core\u2122 i9 9900K @ 3.60GHz processor, 64 GB DDR4 2.933 MHz RAM, and dual NVIDIA\u00ae GeForce\u00ae RTX 2080 Ti (11GB each) graphics cards. The final ranking was determined based on the speed of the algorithms in generating and comparing the feature vectors, their size, and the accuracy achieved on the specific dataset. The final score was obtained by combining the contributions of speed, compactness and PAD accuracy, normalized and averaged."
        },
        {
            "heading": "3 Results",
            "text": "This section examines the results of the algorithms submitted to LivDet2023. The global results of the two challenges are shown in Tables 4 and 5.\nSeventeen algorithms were submitted to Challenge 1, which evaluates the integration between PAD and comparator; the results are shown in Table 6 for the known, that is GreenBit and Dermalog, consensual test datasets, in Table 7 for the known ScreenSpoof test datasets and in Table 8 for the two unknown sensors, Jenetric and Int. Biometrics.\nAnalyzing the results as a whole, it is evident that the test sets acquired with unknown sensors reported higher average errors. In particular, while on average, competitors achieved about 88% IMS on data acquired on known sensors, this value dropped to 84% for data from unknown sensors. Although, on average, this drop does not seem particularly significant, if we analyze the APCERs of the single methods, we can deduce that the rate of erroneously classified unknown presentation attacks is very high (14.91% for known sensors vs. 39.58% for unknown sensors).This aspect shows that the interoperability problem in fingerprint presentation attack detection is still open.\nIn this challenge, the CIS_W/Wens model emerges as the undisputed winner in terms of PAD/IM accuracy. This model, which is sensor-interoperable, has been designed to combine metric learning with the spoof detection task, aiming to encode more PAD-related information while minimizing sensor-related interference. Interestingly, despite the model\u2019s training being conducted with a smaller volume of data than what was fully available - specifically on the Dermalog and Greenbit sub-datasets - it does not appear that this necessarily led to superior performance. This suggests that the relationship between the volume of training data and the model\u2019s performance may not be directly proportional [17].\nNevertheless, a notable observation from the data is the high FNMR, much more significant than typical verification systems without PAD [14]. This distinctive characteristic applies across all participating algorithms and confirms what has been reported in [16], namely, the integration of a PAD algorithm has a substantial impact on the performance of the recognition system.\n-\nCompared to the previous year, there has been a shift in trend concerning the detection of consensual and ScreenSpoof attacks for the Greenbit dataset. In fact, the IAPAR is higher for this sensor in consensual scenarios. The reasons behind this phenomenon will be the subject of future research.\nIt is important to highlight that the handcrafted algorithms, Contr1 and Contr2, report a low IM accuracy due to the very high FMR. However, this behaviour is strictly linked to the choice of the comparator since PAD performances are in line with the other detectors.\nAs we shift focus to deep learning methods, the underperformer is hnu_aim. Despite its singular distinction as the quickest method in this edition (20 ms for probe/template comparison), it fails to demonstrate competitive potential in real-world applications due to its low accuracy.\nThe only algorithm that used additional data jiiov_all has not demonstrated substantial effectiveness, particularly concerning PAD performance. It exhibited an unacceptably high error margin in the APCER metric, specifically when detecting ScreenSpoof-fabricated PAs.\nFor Challenge 2, six algorithms were submitted. The goal of this challenge is to encourage the development of algorithms that strike a balance between accuracy, speed, and compactness. These are crucial factors for ensuring high-performance fingerprint recognition.\nTables 9 and 10 present the results for known and unknown sensors. The overall evaluation considering processing time and feature vector size is shown in Table 5. While the handcrafted methods exhibit the highest accuracy, they are also characterized by larger size and longer computational time. For example, the Contr2 method exceeds an average processing time of 4 seconds per image, which is impractical for real-world scenarios.\nConsidering these aspects, the algorithm that offers the best compromise is jiiov. By leveraging the learning capabilities of a CNN, this algorithm effectively identifies patterns and distinctive characteristics in fingerprints, enabling fast image processing. In terms of compactness, the top-ranking algorithm is unina, which employs an autoencoder-based approach to achieve a condensed representation of relevant information.\n-\nHowever, it is important to note that the algorithms generally demonstrate a limited ability to handle the hidden challenge effectively. Despite reporting an acceptable BPCER, an average APCER close to 50% is observed, implying that the PAs classification is akin to a coin toss. This result emphasizes the critical need to develop more sophisticated algorithms that can accurately identify and differentiate bona fide samples from presentation attacks, even without PA examples during training."
        },
        {
            "heading": "4 Discussion and conclusions",
            "text": "The eighth edition of the Fingerprint Liveness Detection Competition allowed for the evaluation of the degree of interoperability of current PADs, in addition to the impact of integrating a PAD system with an AFIS and the level of compactness, speed, and representativeness. To simulate a worst-case scenario for an AFIS designer, we only provided competitors with bona-fide samples for two of the sensors used. The competitors have faced the challenge in the most different ways: someone has trained with only the data of the other sensors to carry out a sort of transfer domain; others have used data from multiple sensors to increase the PAD\u2019s ability to generalize; others have used information from the training data to generate unknown PAs synthetically. No one reported using a one-class classifier. Although the proposed solutions were very different, the APCER on the unknown sensors is still high, especially on one of the two sensors. This shows that the interoperability problem is still open but that solutions to solve it are under development and have potential. A comparison with past LivDet editions reveals a pause in the rise of accuracy typical of earlier editions, with some fluctuations due to the diverse challenges and materials involved."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Dirk Morgeneier and his company Jenetric for sponsoring the LivDet 2023 competition. J.F. is supported by project BBforTAI (PID2021-127641OB-I00MICINN/FEDER).\n-"
        }
    ],
    "title": "LIVDET2023 - FINGERPRINT LIVENESS DETECTION COMPETITION: ADVANCING GENERALIZATION",
    "year": 2023
}