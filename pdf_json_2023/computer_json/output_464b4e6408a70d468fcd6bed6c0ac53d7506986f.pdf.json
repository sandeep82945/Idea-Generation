{
    "abstractText": "Personal Digital Assistants (PDAs) \u2014 such as Siri, Alexa and Google Assistant, to name a few \u2014 play an increasingly important role to access information and complete tasks spanning multiple domains, and by diverse groups of users. A textto-speech (TTS) module allows PDAs to interact in a natural, human-like manner, and play a vital role when the interaction involves people with visual impairments or other disabilities. To cater to the needs of a diverse set of users, inclusive TTS is important to recognize and pronounce correctly text in different languages and dialects. Despite great progress in speech synthesis, the pronunciation accuracy of named entities in a multi-lingual setting still has a large room for improvement. Existing approaches to correct named entity (NE) mispronunciations, like retraining Grapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciation dictionary, require expensive annotation of the ground truth pronunciation, which is also time consuming. In this work, we present a highly-precise, PDA-compatible pronunciation learning framework for the task of TTS mispronunciation detection and correction. In addition, we also propose a novel mispronunciation detection model called DTW-SiameseNet, which employs metric learning with a Siamese architecture for Dynamic Time Warping (DTW) with triplet loss. We demonstrate that a locale-agnostic, privacypreserving solution to the problem of TTS mispronunciation detection is feasible. We evaluate our approach on a real-world dataset, and a corpus of NE pronunciations of an anonymized audio dataset of person names recorded by participants from 10 different locales. Human evaluation shows our proposed approach improves pronunciation accuracy on average by \u2248 6% compared to strong phoneme-based and audio-based baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Raviteja Anantha"
        },
        {
            "affiliations": [],
            "name": "Kriti Bhasin"
        },
        {
            "affiliations": [],
            "name": "Daniela de la Parra Aguilar"
        },
        {
            "affiliations": [],
            "name": "Prabal Vashisht"
        },
        {
            "affiliations": [],
            "name": "Becci Williamson"
        },
        {
            "affiliations": [],
            "name": "Srinivas Chappidi"
        }
    ],
    "id": "SP:241c17ad75f6a9ea1a08bdb6026cfc1a7d59a883",
    "references": [
        {
            "authors": [
                "A. Van Den Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "WaveNet: A Generative Model for Raw Audio",
            "venue": "Proc. 9th ISCA Workshop on Speech Synthesis Workshop (SSW 9), 2016, p. 125.",
            "year": 2016
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang",
                "R.J. Weiss",
                "M. Schuster",
                "N. Jaitly",
                "Z. Yang",
                "Z. Chen",
                "Y. Zhang",
                "Y. Wang",
                "R. Skerry-Ryan",
                "R.A. Saurous",
                "Y. Agiomyrgiannakis",
                "Y. Wu"
            ],
            "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, p. 4779\u20134783.",
            "year": 2018
        },
        {
            "authors": [
                "S. Achanta",
                "A. Antony",
                "L. Golipour",
                "J. Li",
                "T. Raitio",
                "R. Rasipuram",
                "F. Rossi",
                "J. Shi",
                "J. Upadhyay",
                "D. Winarsky",
                "H. Zhang"
            ],
            "title": "On-device neural speech synthesis",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Lobanov",
                "L. Tsirulnik",
                "D. Zhadinets",
                "H. Karnevskaya"
            ],
            "title": "Language and speaker specific implementation of intonation contours in multilingual tts synthesis",
            "venue": "ISCA Archive, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "K. Azizah",
                "M. Adriani",
                "W. Jatmiko"
            ],
            "title": "Hierarchical transfer learning for multilingual, multi-speaker, and style transfer dnnbased tts on low-resource languages",
            "venue": "IEEE Access, vol. 8, pp. 179 798\u2013179 812, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.P. Silfverberg",
                "L. Mao",
                "M. Hulden"
            ],
            "title": "Sound analogies with phoneme embeddings",
            "venue": "Proceedings of the Society for Computation in Linguistics, pp. 136\u2013144, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Toshniwal",
                "K. Livescu"
            ],
            "title": "Jointly learning to align and convert graphemes to phonemes with neural attention models",
            "venue": "Proceedings of the IEEE Spoken Language Technology Workshop, pp. 76\u201382, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Chen",
                "M. Jain",
                "Y. Wang",
                "M.L. Seltzer",
                "C. Fuegen"
            ],
            "title": "Joint grapheme and phoneme embeddings for contextual end-to-end asr",
            "venue": "Proc. Interspeech 2019, pp. 3490\u20133494, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Ye",
                "S. Mao",
                "F. Soong",
                "W. Wu",
                "Y. Xia",
                "J. Tien",
                "Z. Wu"
            ],
            "title": "An approach to mispronunciation detection and diagnosis with acoustic, phonetic and linguistic (apl) embeddings",
            "venue": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6827\u20136831.",
            "year": 2022
        },
        {
            "authors": [
                "B. Lin",
                "L. Wang"
            ],
            "title": "Phoneme mispronunciation detection by jointly learning to align",
            "venue": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6822\u20136826.",
            "year": 2022
        },
        {
            "authors": [
                "W. Hu",
                "Y. Qian",
                "F.K. Soong",
                "Y. Wang"
            ],
            "title": "Improved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers",
            "venue": "Speech Communication, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W.-K. Leung",
                "X. Liu",
                "H. Meng"
            ],
            "title": "Cnn-rnn-ctc based end-toend mispronunciation detection and diagnosis",
            "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 8132\u20138136.",
            "year": 2019
        },
        {
            "authors": [
                "S. Witt",
                "S. Young"
            ],
            "title": "Phone-level pronunciation scoring and assessment for interactive language learning",
            "venue": "Speech Communication, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "V. Arora",
                "A. Lahiri",
                "H. Reetz"
            ],
            "title": "Phonological feature based mispronunciation detection and diagnosis using multi-task dnns and active learning",
            "venue": "Proc. Interspeech 2017, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Lee",
                "J. Glass"
            ],
            "title": "A comparison-based approach to mispronunciation detection",
            "venue": "2012 IEEE Spoken Language Technology Workshop (SLT), 2012, pp. 382\u2013387.",
            "year": 2012
        },
        {
            "authors": [
                "J.V. Davis",
                "B. Kulis",
                "P. Jain",
                "S. Sra",
                "I.S. Dhillon"
            ],
            "title": "Information-theoretic metric learning",
            "venue": "ICML \u201907: Proceedings of the 24th international conference on Machine learning, 2007. [Online]. Available: https://dl.acm.org/doi/10.1145/1273496. 1273523",
            "year": 2007
        },
        {
            "authors": [
                "J. Mei",
                "M. Liu",
                "Y.-F. Wang",
                "H. Gao"
            ],
            "title": "Learning a mahalanobis distance based dynamic time warping measure for multivariate time series classification",
            "venue": "vol. 46, 2015. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/7104107",
            "year": 2015
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141ukasz Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "31st Conference on Neural Information Processing Systems (NIPS 2017), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.H. Friedman"
            ],
            "title": "Greedy function approximation: A gradient boosting machine",
            "venue": "Annals of Statistics, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "KDD \u201916: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Sun",
                "H. Yu",
                "X. Song",
                "R. Liu",
                "Y. Yang",
                "D. Zhou"
            ],
            "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Salvador",
                "P. Chan"
            ],
            "title": "Fastdtw: Toward accurate dynamic time warping in linear time and space",
            "venue": "Intelligent Data Analysis 11.5 (2007): 561-580, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Bromley",
                "J.W. Bentz",
                "L. Bottou",
                "I. Guyon",
                "Y. LeCun",
                "C. Moore",
                "E. Sackinger",
                "R. Shah"
            ],
            "title": "Signature verification using a siamese time delay neural network",
            "venue": "International Journal of Pattern Recognition and Artificial Intelligence, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "B. Kulis",
                "M. Sustik",
                "I. Dhillon"
            ],
            "title": "Learning low-rank kernel matrices",
            "venue": "ICML \u201906: Proceedings of the 23rd international conference on Machine learning, 2006.",
            "year": 2006
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "TTS is an important component in Personal Digital Assistants (PDAs). With the rapid adoption of smart eco-systems and an increase in voice-based applications, PDAs are becoming more common helping users complete tasks. The role of TTS is critical when the interactions involve people with visual impairment or other disabilities. With recent advances in speech synthesis [1, 2, 3], current TTS systems can produce expressive and natural sounding voice close to human speech. However, there is significant room for improvement on the multilingual, inclusiveness and personalization aspects. In the digital ecosystems, where it is common to have diverse group of users, multilingual\n\u2217 Equal contribution.\nTTS is critical to make users feel acknowledged; and the named entity (NE) pronunciations are particularly important.\nIn this work we address TTS entity mispronunciations, which can occur because of either:\n\u2022 The NE being a homograph, e.g David, which can be pronounced /\u2019deI.vid/ for English NEs, or /da.\u2019bid/ for Spanish NEs.\n\u2022 The NE has a pronunciation that is difficult to derive from the orthography, and it must still be learned by the TTS system, e.g the Italian name Palatucci which is pronounced /pa.la.\u2019tu.tSi/, but can easily be mispronounced by TTS as /pa.la.\u2019tuk.si/ if, e.g using a G2P model predominantly trained on Spanish data.\nTTS personalization can address the former problem, whereas global TTS pronunciation correction is preferable to address the latter. Prior works, which address multilingual and user-specific intonation aspects [4, 5], require locale-specific models and incur high maintenance cost, especially when working with multiple locales.\nWe present a locale-agnostic, PDA-compatible, two-stage framework for TTS mispronunciation detection and correction. In the first stage, TTS mispronunciations are detected using a two-step process. First the pronunciation dissimilarity between the user\u2019s pronunciation and the TTS pronunciation is computed; second, the dissimilarity score is checked against a threshold to determine if a mispronunciation occurred. The threshold is derived from human labeling to meet the desired precision and recall metrics. In the second stage, the mispronunciation will be qualified for correction (personalization or global learning) using user-engagement signals, such as task completion to ensure precise entity selection, in a privacypreserving manner. Although we address the problem of TTS mispronunciation, it should be trivial to employ the same framework for correcting ASR NE misrecognitions.\nOur contributions can be summarized as:\n\u2022 We propose a highly-precise, locale-agnostic framework for TTS mispronunciation detection and correction by using the correlation between a TTS mispronunciation and the pronunciation dissimilarity of user and TTS pronunciations.\n\u2022 We present an empirical comparison of phoneme-based algorithms and models along with acoustic models using both intrinsic and extrinsic metrics.\n\u2022 And finally, we introduce a novel mispronunciation detection model called DTW-SiameseNet, which is trained using a metric learning paradigm and learns the distance function via triplet loss to perform Dynamic Time Warping (DTW).\nar X\niv :2\n30 3.\n00 17\n1v 1\n[ cs\n.L G\n] 1\nM ar\n2 02\n3"
        },
        {
            "heading": "2. Related Work",
            "text": "Our work is an intersection of three areas: phoneme representation, pronunciation learning, and metric learning."
        },
        {
            "heading": "2.1. Phoneme Representation",
            "text": "The task of learning phoneme representations to capture pronunciation similarities is well studied for various downstream applications. A few works have explored the use of phoneme embeddings to perform phonological analogies [6], while others have investigated using embeddings for grapheme-to-phoneme conversion [7]. Improvements in contextual end-to-end Automatic Speech Recognition (ASR) were also realized by using phoneme representations [8].\nIn this work we apply phoneme embeddings for the task of mispronunciation detection. Recent works show using ASR phoneme embeddings improves mispronunciation detection accuracy [9, 10]. However, these works use a single phoneme representation (e.g. IPA \u2014 International Phonetic Alphabet), whereas in practice PDAs may use component/task-specific phoneme notation. In our setting, we use two separate phonetic representations, one for ASR and one for TTS. Our goal is to learn dense phoneme representations which capture phonetic similarity within the same phoneme space as well as the relationship between the two different phoneme spaces."
        },
        {
            "heading": "2.2. Pronunciation Learning",
            "text": "To learn a correct pronunciation, the first step is to detect a mispronunciation. Prior works [11, 12] on mispronunciation detection require a canonical transcription and employ Goodness of Pronunciation (GOP) [13], or classifier based methods.\nA phonological feature-based active-learning method for mispronunciation detection, which estimates phoneme state probabilities using hidden markov models (HMMs) was shown to outperform GOP based methods [14], but this still requires locale-specific training and is not feasible for a multilingual setting. A comparison-based approach [15] for mispronunciation detection was explored, where two speaker Dynamic Time Warping (DTW) is carried out between student (non-native speaker) and teacher (native speaker) utterances. Unlike this approach where a database of teacher utterances are required and a static distance measure (DTW) is employed, we use a metricbased learning framework, where user and TTS pronunciations are compared using a learned distance function."
        },
        {
            "heading": "2.3. Metric Learning",
            "text": "Metric Learning aims to establish similarity (or dissimilarity) between samples while using an optimal distance metric for learning tasks. Most of the existing metric learning methods rely on learning a Mahalanobis distance [16]. The use of a learned distance function in DTW to compare multivariate time series is shown to improve both precision and robustness [17]. In this work, we adopt a similar strategy to learn a Mahalanobis distance function for audio comparison using DTW. Metric learning uses a linear projection, which limits its ability to learn non-linear characterisitics of the data, so we first apply a non-linear projection using a Siamese architecture and then apply metric learning. To the best of our knowledge, we are the first to use metric learning for mispronunciation detection and correction."
        },
        {
            "heading": "3. Methods",
            "text": "We introduce a new framework for the task of TTS mispronunciation detection and correction. We propose using the correlation between TTS mispronouncing a NE, and the pronunciation dissimilarity of the user and TTS pronunciations for the same NE exceeding a set threshold. This framework requires us to define a distance function that computes the pronunciation dissimilarity. Once a distance function is obtained, the threshold that correlates with mispronunciation detection with desired precision and recall can be empirically chosen through human labeling. Once a mispronunciation is detected, the TTS entity (e.g. contact name) pronunciation is updated for that specific user, not all users, using the user\u2019s pronunciation. An overview of the proposed mispronunciation detection and correction framework is shown in Figure 1.\nOur experiments for mispronunciation detection can be broadly classified as phoneme-based and audio-based approaches. Pronunciation correction is carried out post mispronunciation detection by using user engagement signals in a privacy-preserving manner. We describe our mispronunciation detection and correction methods below."
        },
        {
            "heading": "3.1. Phoneme-based Mispronunciation Detection",
            "text": "In this section, we elaborate on various methods we evaluated on the TTS mispronunciation detection task where phonemes are used as input."
        },
        {
            "heading": "3.1.1. Proposed Baseline: P2P Comparison Algorithm",
            "text": "We present a simple, yet strong baseline called the P2P (Phoneme-to-Phoneme) Comparison algorithm. In this algorithm, we use: \u2022 The user interactions on the device to extract the ASR\nphonemes. \u2022 The text of the NE as an input to the TTS model to generate\nthe default TTS phonemes. \u2022 The edit distance between the ASR phonemes and TTS\nphonemes using the Levenshtein distance metric. \u2022 Human-labeled data to empirically determine the edit dis-\ntance threshold based on the desired precision and recall. If the edit distance is greater than the threshold, the algorithm determines there is a TTS mispronunciation. Once a mispronunciation is detected, we use engagement signals to determine if the pronunciation can be updated with high confidence."
        },
        {
            "heading": "3.1.2. Phoneme Embeddings",
            "text": "In our setting, ASR and TTS use separate phonesets. As a result, it is not possible to directly compare an ASR phoneme sequence (representation of user\u2019s pronunciation) with a TTS phoneme sequence (representation of TTS pronunciation). In addition, these phonesets are locale-specific thereby increasing the number of phonesets.\nOne simple approach is to use one-hot or multi-hot embeddings, but the resulting representations would be sparse as they do not capture phonetic similarity. Our goals for phoneme embeddings are: (1) obtain dense representations; (2) embeddings should capture phonetic similarity within the same phoneme space, and; (3) capture the relationship between the two different phoneme spaces.\nTo accomplish these goals, we train a multi-phoneme sequence-to-sequence (seq2seq) model with multi-head attention [18] applied to both the encoder and the decoder. A uni-\ndirectional LSTM cell with an output dimension of 100 is used with both the encoder and the decoder. The encoder/decoder attention establishes the corresponding inter-relationship between the input phonemes and the target phonemes, whereas selfattention pays more attention to the intra-relationship of the phoneme pairs in a phoneme sequence."
        },
        {
            "heading": "3.1.3. GBDT",
            "text": "We train a Gradient Boosted Decision Tree (GBDT) [19] classifier using phoneme embeddings as input. For the given user and TTS pronunciations, the phoneme embedding sequences are concatenated and used as input to train a GBDT model using XGBoost [20] with logistic loss. The annotations are binary labels, where 0 represents both pronunciations are the same, and 1 otherwise."
        },
        {
            "heading": "3.1.4. MobileBERT",
            "text": "We evaluate the MobileBERT [21] architecture, a compressed and optimal version of BERT for resource-limited settings, such as running on mobile devices, with phoneme embeddings as input. MobileBERT is a bidirectional Transformer based on the BERT model. We use the HuggingFace pretrained MobileBERT1 and conduct knowledge transfer using the multi-head attention from the multi-phoneme seq2seq model described in Section 3.1.2."
        },
        {
            "heading": "3.2. Audio-based Mispronunciation Detection",
            "text": ""
        },
        {
            "heading": "3.2.1. Dynamic Time Warping",
            "text": "Dynamic Time Warping (DTW) is an algorithm which can measure the divergence between two time series, in our case audio waveforms, with different phases and lengths. The idea is to compute an optimal warp path between two given waveforms. We use a specific implementation of DTW called FastDTW [22] as a baseline for audio input."
        },
        {
            "heading": "3.2.2. Siamese Network using Mel Spectrograms",
            "text": "Mel-frequency spectrogram is a low-level acoustic representation, which is easily computed from time-domain waveforms. Mel spectrograms are also smoother than raw audio waveforms, which makes them easier to use as features to train a model using variety of loss functions. The sub-waveforms corresponding to entity pronunciation are first extracted using ASR time-spans. We obtain Mel spectrograms of both user and TTS entity pronunciations by applying short-time Fourier transform (STFT) followed by a nonlinear transform to the frequency axis of the\n1https://huggingface.co/docs/transformers/ model_doc/mobilebert\nSTFT. This representation using Mel frequency scale emphasizes details in lower frequencies, which are critical to speech intelligibility.\nWe use a Siamese neural network [23], which consists of twin networks and the parameters between them are tied. We use convolutional layers in twin networks which accept two Mel spectrograms as inputs and determines whether they are similar. We use 3 channels with filters of varying size and fixed stride length of 1. We use ReLU for activation function and max pooling. The outputs of convolutional layers are flattened and concatenated before passing on to a sigmoid activation function. We use the Adam optimizer and cross-entropy loss to learn binary classification."
        },
        {
            "heading": "3.2.3. Proposed Method: DTW-SiameseNet",
            "text": "We propose a novel mispronunciation detection model that employs metric learning with a Siamese architecture for DTW with a triplet loss. We use Mahalanobis distance as our metric; nonMahalanobis based metric learning was also proposed but this suffered from non-convexity or computational complexity [16]. Given two d-dimensional vectors x and y, the square Mahalanobis distance parametrized by a symmetric Positive Definite (PD) matrix A between the two vectors is defined as:\nDA(x, y) = (x\u2212 y)TA(x\u2212 y). (1)\nThe Positive Definiteness of the matrix A guarantees that the distance function will return a positive distance. The Mahalanobis matrix A can be decomposed as:\nA = GTG. (2)\nThis can be interpreted as G being distributed to (x - y) terms, i.e., linear transformation applied to the input. Our goal is to learn the PD matrix A based on some constraints over the distance function. We apply two constraints: \u2022 If two vectors are similar then the distance metric D(.) is\nsmaller than an upper bound ubound, and; \u2022 If two vectors are dissimilar then the distance metric D(.) is\ngreater than a lower bound lbound. We combine the two constraints into a triplet constraint. Given three d-dimensional vectors x, y and z; where x, y are similar and x, z are dissimilar we express the constraint as:\nDA(x, y)\u2212DA(x, z) < \u2212\u03c1, (3) where 0 < \u03c1 < lbound \u2212 ubound.\nWe apply non-linear projection using the Siamese architecture on the inputs before linear projection through A. We use a unidirectional LSTM with an attention mechanism, fW (.), for the twin networks, where the parameters W are tied. For given\ninputs x, y with a randomly drawn z, the objective function is defined as\nl(A,W ) = \u03c1+DA(fW (x), fW (y))\u2212DA(fW (x), fW (z)). (4)\nThe overall loss is given as:\nL(A,W ) = \u2211 t l(A,W ). (5)\nWe use SGD to update the parameters W and learn the Mahalanobis matrix A, which together constitute the distance function. Since we need the updates to A to be gradual and stable, we add a regularization term. LogDet divergence [24] is shown to be the most optimal for regularizing the metric learning process and is invariant to linear group transformations. The LogDet divergence for A and At (A at time-step t) is given as:\nDld(A,At) = tr(AA \u22121 t )\u2212 log(det(AA\u22121t ))\u2212 d. (6)\nApplying the LogDet divergence the metric learning model for updating A will be\nAt+1 = argmin A 0 Dld(A,At) + \u03b7tl(A,W ), (7)\nwhere \u03b7t > 0 is a regularization parameter that balances LogDet regularization function Dld(A,At).\nOnce the distance function is learned, we use it to compute the distance between the inputs using the traditional DTW algorithm as shown below, where we use a moving window of dimension d to choose input sub-sequences:\nDA(i, j) = DA(x i, yi) +min  DA(i\u2212 1, j \u2212 1) DA(i\u2212 1, j) DA(i, j \u2212 1).\n(8)\nThe main difference between the traditional DTW algorithm and DTW-SiameseNet lies in the fact that we learn the distance function D(.) parameterized on A and W for the inputs x and y."
        },
        {
            "heading": "3.3. Pronunciation Correction",
            "text": "Once the pronunciation dissimilarity score is computed, and if it meets the chosen threshold, we deem the TTS pronunciation as a mispronunciation. We employ user engagement signals, such as task completion, to avoid incorrectly updating the pronunciation of an entity. For example, if the task was to call a person, prior to updating the contact pronunciation, we check if the call was successful and the call duration was greater than a predetermined number of seconds."
        },
        {
            "heading": "4. Training Data",
            "text": "We use two datasets: one real-world (phoneme-based) and one human-generated (audio) NE pronunciations dataset; comprised of data from 10 locales to train and evaluate phonemebased and audio-based methods."
        },
        {
            "heading": "4.1. Phoneme-based Dataset",
            "text": "We curated a real-world dataset comprised of 50K randomized and anonymized user requests from 10 different locales, where each request contain a reference to an entity. This dataset is\nused to train and test phoneme-based approaches described in Section 3.1. Each locale has 5K data points with ASR and TTS phoneme representations for the entity pronunciation, but no user audio. On average, 30% of entity names in each locale are non-native names and >20% are homographs. This dataset has mispronunciations in the range of 15% to 28%."
        },
        {
            "heading": "4.2. Audio Dataset",
            "text": "We created an anonymized audio dataset comprised of 30K audio requests using human annotators. Each locale has 1K unique entities with person, location and business names. Human participants are provided with prompts, such as \u201cDirections to X\u201d or \u201cCall X\u201d, which are used to record the audio. Each entity gets audio from 3 different participants to capture variance from different genders and age groups. Since we did not use locale-specific participants, this dataset contains 40% to 50% of human mispronunciations. On average, 22% of the names are homographs with 17% being non-native names."
        },
        {
            "heading": "5. Results",
            "text": "Below we present both intrinsic and extrinsic metrics, unless specified otherwise metrics for methods described in Section 3.1 and 3.2 are computed using data described in Section 4.1 and 4.2 respectively.\nWe compute pronunciation accuracy using both percentage and a 3-point Likert scale, where in the latter 1 represents correct entity and TTS pronunciations are different, 2 represents a partial similarity, and 3 represents full similarity. We use a TTS system with an average pronunciation accuracy of 88%."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we presented a locale-agnostic framework for TTS mispronunciation detection and correction, which is compatible with PDAs. In addition, we described a novel metric learning model for audio comparison called DTW-SiameseNet. We investigated and presented empirical comparison of various phoneme and audio based methods."
        },
        {
            "heading": "7. References",
            "text": "[1] A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A Generative Model for Raw Audio,\u201d in Proc. 9th ISCA Workshop on Speech Synthesis Workshop (SSW 9), 2016, p. 125.\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \u201cNatural tts synthesis by conditioning wavenet on mel spectrogram predictions,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, p. 4779\u20134783.\n[3] S. Achanta, A. Antony, L. Golipour, J. Li, T. Raitio, R. Rasipuram, F. Rossi, J. Shi, J. Upadhyay, D. Winarsky, and H. Zhang, \u201cOn-device neural speech synthesis,\u201d in IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021.\n[4] B. Lobanov, L. Tsirulnik, D. Zhadinets, and H. Karnevskaya, \u201cLanguage and speaker specific implementation of intonation contours in multilingual tts synthesis,\u201d ISCA Archive, 2006.\n[5] K. Azizah, M. Adriani, and W. Jatmiko, \u201cHierarchical transfer learning for multilingual, multi-speaker, and style transfer dnnbased tts on low-resource languages,\u201d IEEE Access, vol. 8, pp. 179 798\u2013179 812, 2020.\n[6] M. P. Silfverberg, L. Mao, and M. Hulden, \u201cSound analogies with phoneme embeddings,\u201d Proceedings of the Society for Computation in Linguistics, pp. 136\u2013144, 2018.\n[7] S. Toshniwal and K. Livescu, \u201cJointly learning to align and convert graphemes to phonemes with neural attention models,\u201d Proceedings of the IEEE Spoken Language Technology Workshop, pp. 76\u201382, 2016.\n[8] Z. Chen, M. Jain, Y. Wang, M. L. Seltzer, and C. Fuegen, \u201cJoint grapheme and phoneme embeddings for contextual end-to-end asr,\u201d Proc. Interspeech 2019, pp. 3490\u20133494, 2019.\n[9] W. Ye, S. Mao, F. Soong, W. Wu, Y. Xia, J. Tien, and Z. Wu, \u201cAn approach to mispronunciation detection and diagnosis with acoustic, phonetic and linguistic (apl) embeddings,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6827\u20136831.\n[10] B. Lin and L. Wang, \u201cPhoneme mispronunciation detection by jointly learning to align,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6822\u20136826.\n[11] W. Hu, Y. Qian, F. K. Soong, and Y. Wang, \u201cImproved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers,\u201d in Speech Communication, 2014.\n[12] W.-K. Leung, X. Liu, and H. Meng, \u201cCnn-rnn-ctc based end-toend mispronunciation detection and diagnosis,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 8132\u20138136.\n[13] S. Witt and S. Young, \u201cPhone-level pronunciation scoring and assessment for interactive language learning,\u201d in Speech Communication, 1998.\n[14] V. Arora, A. Lahiri, and H. Reetz, \u201cPhonological feature based mispronunciation detection and diagnosis using multi-task dnns and active learning,\u201d in Proc. Interspeech 2017, 2017.\n[15] A. Lee and J. Glass, \u201cA comparison-based approach to mispronunciation detection,\u201d in 2012 IEEE Spoken Language Technology Workshop (SLT), 2012, pp. 382\u2013387.\n[16] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, \u201cInformation-theoretic metric learning,\u201d ICML \u201907: Proceedings of the 24th international conference on Machine learning, 2007. [Online]. Available: https://dl.acm.org/doi/10.1145/1273496. 1273523\n[17] J. Mei, M. Liu, Y.-F. Wang, and H. Gao, \u201cLearning a mahalanobis distance based dynamic time warping measure for multivariate time series classification,\u201d vol. 46, 2015. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/7104107\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141ukasz Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in 31st Conference on Neural Information Processing Systems (NIPS 2017), 2017.\n[19] J. H. Friedman, \u201cGreedy function approximation: A gradient boosting machine,\u201d in Annals of Statistics, 2001.\n[20] T. Chen and C. Guestrin, \u201cXgboost: A scalable tree boosting system,\u201d in KDD \u201916: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\n[21] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, \u201cMobilebert: a compact task-agnostic bert for resource-limited devices,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.\n[22] S. Salvador and P. Chan, \u201cFastdtw: Toward accurate dynamic time warping in linear time and space,\u201d in Intelligent Data Analysis 11.5 (2007): 561-580, 2007.\n[23] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sackinger, and R. Shah, \u201cSignature verification using a siamese time delay neural network,\u201d in International Journal of Pattern Recognition and Artificial Intelligence, 1993.\n[24] B. Kulis, M. Sustik, and I. Dhillon, \u201cLearning low-rank kernel matrices,\u201d in ICML \u201906: Proceedings of the 23rd international conference on Machine learning, 2006."
        }
    ],
    "title": "DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction",
    "year": 2023
}