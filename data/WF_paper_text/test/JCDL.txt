Peer review is the widely accepted mechanism to determine the quality of scientific work. Even though peer-reviewing has been an integral part of academia since the 1600s, it frequently receives criticism for the lack of transparency and consistency. Even for humans, predicting the peer review outcome is a challenging task as there are many dimensions and human factors involved. However, Artificial Intelligence (AI) techniques can assist the editor/chair anticipate the final decision based on the reviews from the human reviewers. Peer review texts reflect the reviewers’ opinions/ sentiments on various aspects (e.g., novelty, substance, soundness, etc.) of the paper concerning the research in the paper, which may be valuable to predict a manuscript’s acceptance or rejection. The exact types and number of aspects could vary from one to the other venue (i.e., the conferences or journals). Peer review texts, however, which often contain rich sentiment information about the reviewers and, therefore, their overall opinion of the paper’s research, can be useful in predicting a manuscript’s acceptance or rejection. Here in this work, we study how we could take advantage of aspects and their corresponding sentiment to build a generic controllable system to assist the editor/chair in determining the outcome based on the reviews of a paper to make better editorial decisions. Our proposed deep neural architecture considers three information channels, including reviews, review aspect category, and its sentiment, to predict the final decision. Experimental results show that our model can achieve up to 76.67% accuracy on the ASAP-Review dataset (Aspect-enhanced Peer Review) consisting of ICLR and NIPS reviews considering the sentiment of the reviews. Empirical results also show an improvement of around 3.3 points while aspect information is added to the sentiment information1. ∗Both authors contributed equally to this research. 1We make our code publicly available at https://github.com/sandeep82945/-PEERREVIEW-DECISION-Public.git 1 introduction The peer-review process is considered to be the primary means of ensuring the quality of scientific publications. The editors of journals and conferences assign submitted papers to reviewers who critique them. It is well known that peer reviews are controversial due to their quality, randomness, bias, and inconsistencies [2]. Also, there has been concerns about alleged reviewer bias in "single-blind" peer reviews [27] and arbitrariness between different reviewer groups [12]. The rise in submissions and shortage of expert reviewers are major reasons why editors or program chairs may sometimes consider assigning papers to novice, out-of-domain reviewers which results in more inconsistencies and poor quality reviews. An organizer of the NIPS 2014 conference studied the arbitrariness inherent in the peer review process by assigning 10% of submissions to two different committees and found that they disagreed for more than a quarter of papers [12]. Researchers at the ICML 2020 conference explored several methods to improve the quality of reviews in large conferences by addressing the scarcity of qualified reviewers. Again, it is pretty common for a paper rejected by one venue to be accepted by another with little or no improvement in quality. Kravitz et al. [9] studied the peer review process at peer-review at the Journal of General Internal Medicine (JGIM). They explained the need to improve the reliability of the peer-review process and for editors to understand the limitations
of reviewers’ recommendations. They observed that the JGIM editor’s decisions appeared to be significantly influenced by reviewer recommendations. A manuscript’s fate was essentially sealed by consensus between all the reviewers that it should be rejected. The consensus of reviewers that a manuscript merited additional consideration (revision and resubmission or conditional acceptance) decreased the likelihood of rejection from approximately half to one in five (for all manuscripts sent for peer review). They challenge biomedical journal editors to reconsider standard practice: asking reviewers to recommend whether manuscripts should be accepted, revised, or rejected. Some people believe that the peer review system is fragile since it is based on the opinions of a few people and to them, this is not an ideal process for validation [22]. Despite all its inherent flaws [23], criticisms [24] and bias [21], peer review is the most widely-accepted admission process for scientific knowledge at this moment. A study into the internal process of the peer review system is quite complex due to concerns over publisher privacy and copyright. However, with OpenReview2, we can see how peer review is evolving in some areas such as the response periods/rebuttals by authors, and an effort to increase communication between authors and reviewers. Additionally, we can observe the introduction of peer review workshops, objective questionnaires and other measures. The rapid increase in research article submissions across different venues3, has caused the peer review system to undergo a huge amount of stress [8]. This problem presents a significant administrative challenge to journal editors and conference program chairs, who are usually burdened with many tasks, like assigning reviewers, ensuring timely receipt of reviews, filling in for nonresponding reviewers, informing decisions, communicating with authors, etc. The major challenge is to decide whether to accept or reject a manuscript based on the reviews provided by the reviewers, simultaneously ensuring their validity. With our ongoing efforts to build an artificial intelligence (AI) peer review system, we are curious to know what might happen if an additional AI reviewer could predict decisions by analyzing the reviewer’s comments on its own? Editors/program chairs usually follow most reviewer recommendations. However, they still have to go over all the review texts for all the submitted papers to address the inconsistencies between reviewer recommendations and comments. The purpose of this study is not to replace human reviewers; instead, we are interested in exploring how AI can provide a supplemental opinion of reviews with respect to various aspects to assist the overall peer-review decision-making process. Such a system might effectively tell the authors which aspects they should focus on, alongside giving the program chair the liberty to configure the decision prediction system according to the needs and interests of that particular conference. ASAP-Review dataset [30] is a recent dataset of peer-reviews. It is an excellent resource for research and study on this significant and impactful problem, which not only contains the reviewer’s sentiment but also the aspect category associated with it. In this work, we include the following aspect-categories in our investigation: Motivation/Impact (MOT), Originality (ORI), Soundness/Correctness
2https://openreview.net/ 3AAAI 2021 received 9034 submissions, NeurIPS 2021 received 12115 abstracts!! (SOU), Substance (SUB), Replicability (REP), Meaningful Comparison (CMP), and Clarity (CLA) and the corresponding sentiment. We suggest that a decision can be inferred by considering the sentiments of the reviewers expressed with respect to the underlying aspects. Hence the aspects and sentiments play an important role in determining the quality of a paper. We propose a novel deep neural architecture to make use of an aspect infused embedding. We also show that aspect, along with the sentiment, helps in peer review decision prediction. Our model is generic, and can be easily adapted to different aspect categories that may have to be considered depending upon the types of conferences and/or journals. To the best of our knowledge, such a controllable aspect-based sentiment investigation for decision prediction from peer-reviews does not exist. 2 relatedwork  2.1 peer reviews The peer-review process is complex and takes place in a complex broader research system. Artificial intelligence in peer review is an essential but understudied topic. However, the topic is gaining community attention due to recent advances in artificial intelligence research. The first public dataset of scientific peer reviews was presented in [7] available for research purposes, providing an opportunity to study this important artifact. The authors also examined correlations between the overall recommendation score and the individual aspect scores (e.g., clarity, impact, and originality). They quantified the difference between reviews that recommend an oral presentation and those that recommend a poster presentation. Review texts are predictive of scientific impact. For determining citation impact from peer reviews, the CiteTracked dataset [16] has been created, which has peer reviews and citation information spanning six years and scientific articles from the machine learning community. The investigation found that a model’s performance based on review texts is higher than that using only abstracts or titles, indicating the potential of learning from peer-review texts. With the help of a variety of computational support tools, peer review comments’ quality, tone, and quantity were analyzed [18]. The work in [25] provides a comprehensive overview of criteria tools that the community can utilize to judge the quality of peer review reports in biomedical research. In addition, a tool [29] was developed to assess the transparency of the peer-review process, which they propose can be considered an indicator of the quality of the peer-review process. 2.2 role of aspects and sentiment in text or reviews Aspect level sentiment analysis is based on the idea that an opinion consists of sentiment (positive or negative) and a target (of aspect). An opinion without its target being identified is of limited use. The importance of opinion targets also helps us to understand sentiment analysis better. However, it is critical to determine the scope of each sentiment expression, i.e., whether it covers the interest in the sentence [13]. MATE [1], a neural framework for the opinion summarization model, is used to reconstruct the input segment’s embedding to produce an aspect matrix. The framework created an aspect matrix by reducing each seed matrix to a single aspect
embedding with the help of seed weight vectors. Seeds can be considered as query terms that someone would use to search for segments discussing an aspect. A recurrent convolutional network model incorporating modularity and attention mechanisms was used by Qiao et al. [19] to predict the aspects of academic papers. For aspect and sentiment analysis in peer reviews, authors in [28] propose a multiple instance learning network with a novel abstract-based memory mechanism (MILAM) to perform sentiment analysis in the domain of peer reviews for scholarly papers. Multilevel aspects of peer reviews for academic articles [15] aimed to extract the multi-level aspects in peer reviews comprehensively. ASAP-Review, a dataset [30] was created with an ambitious goal of automating scientific peer review, which we use in our current work. Based on sentiment encoded in peer review texts peer review praise and criticism was evaluated automatically [26]. Recently, a novel multitasking system [11] was proposed which leverages inter-dependency by sharing representations between two related tasks: aspect categorization and sentiment classification. 2.3 acceptance or rejection prediction for scientific papers DeepSentiPeer [4], a deep neural architecture was proposed for classifying research papers (accept/reject) and predicting recommendation scores. It examined the impact of sentiment within the review tomake their decisions. The Role of aspect scores in boosting the decision prediction task was also investigated [3]. The distribution of aspect-based sentiment differed substantially between accepted and rejected papers. Authors in [10, 17] used a deep neural network model to predict the review outcome. Our work is different from the previous works as we investigated the role of the aspect category for this task. However, the aspect score needs to be manually annotated and would not be available during evaluation. Our model does not rely on aspect scores for predicting the decision score. The significance of an aspect varies according to the venue. Not only the sentiment but also the aspect information with sentiment plays a vital role in the acceptance or rejection of a paper. For example, in a conference (say A), reviewers are explicitly advised to judge the quality of a paper and not its length. So, reviewers may focus more on aspect motivation or novelty than the aspect substance. However, for a conference (say B) the substantial quality of the paper is more important. If there are negative comments on it, then the paper might get rejected. Our model is generic and can solve this problem to some extent by adjusting the aspect weight score based on the venue or editor or chair’s focus. 3 methodology Figure 1 shows the overall architectural diagram of our model. We first introduce the aspect embedding representation layer, which creates aspect-aware matrix representations. Secondly, we introduce the sentiment polarity infused aspect embeddingmatrix, which explains how the polarity of a sentence is infused along with aspect and sentiment category knowledge. The sentiment and aspect attention layer is followed to create an aspect and sentiment aware
embedding. Finally, the output of the sentiment and aspect attention layer is passed through the prediction layer to obtain the final review outcome. 3.1 aspect embedding representation layer This layer is further described into three parts. In the input layer, reviews are grouped as per its aspect and sentiment. This is followed by the representation layer which creates embedding for each group of the sentences. Finally, the CNN(convolutional neural network) creates a final aspect aware embedding from the prior embedding representations. 3.1.1 Input layer. Initially, we have a collection of reviews 𝐷 = {𝑅𝑒1, 𝑅𝑒2, ..., 𝑅𝑒𝑛}, corresponding to a single paper. We combine all the reviews of a paper to create a single combined review 𝑃 . Each 𝑃 = {𝑥1, 𝑥2, ..., 𝑥𝑚}, is a collection of sentences, where 𝑥𝑖 ∈ 𝑃 denotes a single sentence. For our experiments from the dataset, we segregated the review sentences into separate groups. First, based on the sentiment of the comments received from reviewers positively or negatively, i.e. 𝐺𝑝𝑜𝑠 and 𝐺𝑛𝑒𝑔 . Here, 𝑝𝑜𝑠 is the group containing positive review sentences and 𝑛𝑒𝑔 is the group containing negative review sentences. Secondly, each group is divided further based on the aspect of the comments received from reviewers, i.e. 𝐺𝑎,𝑠 , where 𝑎 is the number of aspect categories(8) and 𝑠 is the number of sentiment categories(2). We use the gold annotated aspect sentiment labels from the dataset. 3.1.2 Representation layer. Each group𝐺𝑎,𝑠 of sentences, obtained from the input layer is passed through an encoder for generating the contextualized representations for those sentences. 𝐻𝑎,𝑠 = 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝐺𝑎,𝑠 ) (1) where 𝐻𝑎,𝑠 is the contextual representation of each sentence of subgroup 𝐺𝑎,𝑠 . For this purpose, we utilize Sentence Transformers s-BERT [20] encoder model fine-tuned on STSB (semantic textual similarity) task as a sentence encoder. 3.1.3 CNN layers. In this layer, we make use of a CNN [5] to extract features from the contextualized representations, 𝐻𝑎𝑠 . In recent years, CNN has always shown great success in solving the NLP problems. The convolution operation works by sliding a filter 𝑊𝑓 𝑘 ∈ 𝑅𝑙𝑑 to a window of length l, the output of such ℎ𝑡ℎ window is given as :
𝑓 𝑘 ℎ = 𝑔(𝑊𝑓 𝑘 ·𝑋ℎ−𝑙+1:ℎ + 𝑏𝑘 ) (2) 𝑋ℎ𝑙+1:ℎ means the l sentences within the ℎ𝑡ℎ window in a group of contextual embedding 𝐻𝑎𝑠 , 𝑏𝑘 is the bias for the 𝑘𝑡ℎ filter, g() is the non-linear function. The feature map 𝑓𝑘 for the kth filter is then obtained by applying this filter to each possible window of contextual embedding in a group of contextual embedding 𝐻𝑎𝑠 is given as,
𝑓 𝑘 = [𝑓 𝑘1 , 𝑓 𝑘 2 , ., 𝑓 𝑘 ℎ , .., 𝑓 𝑘 𝑛1𝑙+1], 𝑓 𝑘 ∈ 𝑅𝑛1𝑙+1 . (3)
For a contextualized sentence representations 𝐻𝑎,𝑠 , the final output of this convolution filter operation is then given as
𝑟 = [𝑓 1, 𝑓 2, 𝑓 3 ...𝑓 𝐹 ], 𝑝 ∈ 𝑅𝐹 . (4)
. where F is the total number of filters used in the operation. For the 𝑖𝑡ℎ review and 𝑗𝑡ℎ aspect of the 𝑘𝑡ℎ sentiment
𝐴𝑘𝑗 = 𝐶𝑁𝑁 (𝐻 𝑗,𝑘 ) (5) We obtain an embedding for each group of contextualized sentence representations by passing through the CNN layer, which is the embedding vector for each aspect sentiment and each category. The two metrices for each sentiment 𝐴𝑘 ∈ 𝑅𝑛𝑥𝑑 , contain one row per aspect category and holds its contextualized embedding representation, as illustrated in Figure 1. The main reason for using CNN is its ability to extract the essential features from the group representations𝐻𝑎,𝑗 , reflecting that particular aspect and sentiment pair. 3.2 sentiment polarity infused aspect embedding matrix For the jth aspect and kth sentiment, we employ a Multi-Layer Perceptron (MLP) as shown below:-
𝐶𝐴𝑘𝑗 = 𝑀𝐿𝑃 (𝐴 𝑘 𝑗 ) (6)
We use VADER [6] (Valence Aware Dictionary for Sentiment Reasoning) which returns a compound sentiment score on a continuous scale of -1 (extremely negative) to +1 (extremely positive) for each sentence. We use this positive or negative sentiment intensity as a polarity feature in our model. We create a polarity vector 𝑣𝑘
𝑗 to
add the polarity of the sentences in the reviews for its sentiment label (padded where necessary), which is already known. For the 𝑗𝑡ℎ aspect and 𝑘𝑡ℎ sentiment, the polarity vector 𝑉𝑘 𝑗 is
denoted by:-
𝑉𝑘𝑗 = [𝑣1; 𝑣2; ...; 𝑣𝑛] (7) where 𝑣𝑖 represents the polarity score of the sentence corresponding to group 𝐺𝑘,𝑗 . The vector formed is then concatenated with the aspect embedding matrix to create a sentiment polarity infused aspect embedding matrix as shown in the equation below:-
𝐻𝐶𝑘𝑗 = [𝐶𝐴 𝑘 𝑗 ;𝑉 𝑘 𝑗 ] (8)
where 𝐴 is the aspect aware embedding and 𝑉 is the VADER embedding matrix. 3.3 sentiment attention layer This layer takes the final aspect embedding representation as input, and produce a sentiment aware embedding for each aspect with respect to each sentiment category as shown in the equation below:-
For the 𝑖𝑡ℎ review 𝑗𝑡ℎ aspect
𝑠𝑎𝑖𝑗 = 2∑︁
𝑘=1 (𝐻𝐶𝑘𝑗 )
𝑇 𝑊𝑘 (9)
where𝑊𝑘 ∈ 𝑅1×1, is a learnable parameter(a.k.a sentiment importance score) and 𝑠𝑎𝑖
𝑗 ∈ 𝑅𝑑×1 is the sentiment aware embedding. 3.4 aspect attention layer This layer takes the final aspect embedding representation as input and produces a sentiment aware embedding for each aspect concerning each sentiment category as shown in the equation below:-
For the 𝑖𝑡ℎ review and jth aspect
𝑟 𝑖𝑎 = 8∑︁ 𝑗=1 𝑠𝑎𝑖𝑗𝑊𝑎 (10)
where,𝑊𝑎 ∈ 𝑅1×1 is a learnable parameter (a.k.a aspect importance score) and 𝑟 𝑖𝑎 ∈ 𝑅𝑑×1 is the aspect-aware embedding. 3.5 prediction layer We pass the embedding vector r from the previous layer to two fully connected layers with ReLU activation to obtain the vector𝑋 𝑖 . This is followed by a fully connected layer with sigmoid activation, as shown in the equation below :-
𝑋 𝑖 = 𝑀𝐿𝑃 (𝑟 𝑖𝑎) (11)
𝑐 = 𝑆𝑖𝑔𝑚𝑜𝑖𝑑 (𝑊𝑐 ∗ 𝑋 𝑖 + 𝑏𝑐 ) (12) where c is the output classification distribution across accept or reject classes and𝑊𝑐 and 𝑏𝑐 are the trainable parameters. We minimize the binary Cross-Entropy Loss between the predicted c and actual decisions. 4 datasets and experiments  4.1 data We use the recently introduced ASAP-Review dataset[30]. ASAPReview stands for Aspect-enhanced Peer Review, and is composed of review texts, their annotated review-level annotations, and review meta-data. More than 20,000 annotated reviews in this dataset are structured in JSON tags with the corresponding annotation labels. It contains ICLR (International Conference on Learning Representations, 2017-2020) and NeurIPS (Neural Information Processing Systems, 2016-2019) reviews from the open-access platform, OpenReview. We further note that the dataset is imbalanced and show the percentage of each aspect in the entire dataset in Figure 2. The topolgy and meaning of each aspect category is given in Table 1 4. We show the detailed dataset statistics in Table 2 and refer the reader to the original paper for more details. 4https://github.com/neulab/ReviewAdvisor/blob/main/materials
This dataset has eight aspects, including a review summary (SUM). In general, the summary does not have any particular sentiment associated with it; hence we omitted it in our experiments. We also added one extra category as no-aspect that contain the review sentences which do not belong to any of the aspect categories mentioned above. 4.2 experimental settings For creating review embedding, we adopt Sentence Transformers [20] with stsb-RoBERTa-base as the encoder. It is formulated via a CNN for aspect feature extraction, which is structured with three parallel convolutional layers with kernel sizes of 1,16,1, respectively. In all the settings, we apply a dropout of 0.5 and optimize binary cross-entropy loss using the AdamW optimizer [14] and trained for 5 to 12 epochs using a batch size of 16,32 and a learning rate of 0.001, 5e-6, 1e-5, 2e-5, or 5e-5. We pick the best learning rate and batch size on the test results. We found the setting that works best is a learning rate of 0.001 with a weight decay of 1e-4 and batch size of 16. 4.3 comparing system • DeepSentiPeer [4]: They extract full-text sentences from each research article review and represent each sentence using the Transformer variant of the Universal Sentence. Encoder (USE), of d, is the dimension of the semantic sentence vector, 512. They use a CNN to extract features from both the paper and review representations. The sentiment encoding of the review is done using VADER Sentiment Analyzer. A feature-level fusion was generated based on the concatenation of those features and fed into the multilayer perceptron. They reported on both settings, i.e., using only the reviews and reviews with their associated sentiment polarity, and concluded that sentiment plays a vital role in this task. • MRG [10]: This work predicted the decision using the reviews from three transformer based encoder followed by the linear layers for decision prediction. They reported their result for two settings. In the first setting, the encoded last hidden state representation from the encoder is passed to MLP for calculating the result while in other, instead of the encoder, the decoder’s last hidden state is used. These layer 4.4 result and discussion We report the evaluation results of our proposed system and the other existing methods in Table 3. Our proposed approach outperforms DeepSentiPeer model with sentiment by around 8 points and without sentiment by around 10 points with respect to the accuracy. Our proposed also outperforms the approach by Pradhan et al. by around 8.7 points w.r.t. accuracy. In case of MRG, our model outperforms the approach by around 18 points w.r.t accuracy for both the settings. The comparing systems have used PeerRead or a different dataset for their experiments. Our approach requires prior aspect knowledge of the reviews, so we have used the ASPeer dataset for our experiments. We executed their model on the recent dataset and found our approach to outperform it. The reason behind this is that the best-comparing system DeepSentiPeer [4] uses review text and sentiment within the review with sentence embedding (using Universal Sentence Encoder) to predict the acceptance decisions. Here, we use better contextual embedding (BERT). Also, we have
created aspect-aware embedding and aspect-aware sentiment polarity embeddings along with aspect and sentiment attention, which enhances our model’s performance. Our model predicted 78.9% accuracy on NIPS conferences and 81.3% on ICLR conferences. We analyze the attention visualization to understand the learning behavior of our proposed model. Figure 3 shows the trained softmax aspect weight score learned by our model. The darker color reveals higher attention scores, while the lighter has little importance. This weighted score indicates the importance to a particular aspect given by our model in determining if a paper should be accepted or rejected. We note that the contextual embedding and sentiments bound to Motivation, Meaningful Comparison, Originality and Soundness are the most effective ones in predicting the intended recommendation in a review. We believe that this result is also significant because it acts as a guideline for the authors as to what particular aspects they should focus on while reviewing their papers before submission. The importance of an aspect could vary depending upon the venue of the conference or journal to which it is submitted. A paper may get rejected in a conference but can get accepted in another conference. This motivates towards getting a generalized model that can address this issue. A user can easily control the aspect weight score based on venue or the requirement to predict their set values. See Section 4.6.1 for more details and examples. 4.5 ablation study To validate the effectiveness of our proposed framework, we analyze the outputs of our framework in different settings as shown in Table 6. • 𝑆1 : We ran the model without aspect or sentiment information, and using only the embedding of the review sentences. • 𝑆2: We ran the model without aspect information using the review sentence embedding along with its sentiment information. Table 3: Experimental results of our model. Training is done on the entire collection of papers excluding 20% ICLR and NIPS papers for testing. The comparison is done on the same training and test data splits. The accuracy values for DeepASPeer are statistically significant over DeepSentiPeer performance (two-tailed t-test, p<0.05). MRG Decision LAST and MRG Decision ENCODED are two settings of MRG model as described in Section 4.3
. Model F1 score Accuracy
Rejected Accepted Overall MRG Decision LAST [10] 27.0 74.3 62.07
MRG Decision ENCODED [10] 28.0 75.7 62.23 DeepSentiPeer(Review) [4] 59.2 71.4 69.79
Pradhan et al. [17] 60.2 74.3 71.31 DeepSentiPeer(Review+Sentiment) [4] 62.8 73.4 72.02 Our Proposed Model (DeepASPeer) 70.02 84.23 80.05
• 𝑆3 : We ran the model with both aspect and sentiment information, using the review sentence embedding along with its aspect and its sentiment information. • 𝑆4 : It is the same as 𝑆3; additionally, we add the review sentences that do not belong to any particular aspect (as a category) for our experiment. Considering the sentences in setting 𝑆4, we obtain an accuracy of 80.05%, while in setting 𝑆3 we observe a slight drop of 0.37%, in accuracy i.e., 79.68%. This shows that the review sentences belonging to the particular aspects plays a major role in determining the decision for a paper. To further investigate the importance of aspect information, we ran our model only with the information of sentiment polarities and removing the aspect wise information. In particular, we created sentence embedding infused with sentiment polarity (Vader sentiment) followed by sentiment attention. We divide the reviews based on the aspect categories and apply aspect attention on top of it. We obtain an accuracy of 76.67% as shown in 𝑆2 setting. This shows the role of aspect information in boosting the performance of the decision process. We consider the 𝑆2 setting without sentiment information in setting 𝑆1. In particular, we ran our model without aspect category or sentiment knowledge, by creating its BERT embedding and passing it through MLP and finally through softmax. We obtain an accuracy of 64.33 which is 12.34% less compared to when we consider sentiment information, which shows that sentiment importance plays an important role to determine the acceptance or rejection of a paper. . 4.6 case study and observations We conducted several human elicitation studies to further analyze the final decision produced. 4.6.1 Aspect control: Here, we try to answer the following interesting question: Can the decision be controlled according to aspect importance? For this purpose, we picked reviews for a few papers, and classified into aspect and sentiment groups in accordance with the strategy mentioned in 3.1.1. Let us take an example of review comments shown in Table 4. Despite so many negative comments in the review, the paper was still accepted; the probable reason might be the impactful motivation. The model with ablation setting 𝑆2, i.e. the model without aspect and only with sentiment clearly rejects it by a decision probability of 72%. However, DeepASPeer considers the aspect importance while making its decision but still weakly rejects it by a decision probability of 55%. The problem described above is a representative of the real world scenario of a decision prediction system, where every meta chair or reviewer has its own mindset of giving priority to certain aspects over others. To solve this issue, we introduce an additional functionality in our model, which asks human to prioritize the aspects by himself or herself, and predicts the decision accordingly. This functionality helps a particular conference to have a decision prediction model which assists them according to their own priorities. Now let us take a scenario when this paper is sent to a conference where the meta chair sets the aspect priorities as shown in Figure 4. As a result of these priorities, the model changed its decision to ACCEPT (with a decision probability of 54%), which can also be considered as weak acceptance. Similarly, for reviews, as shown in Table 5, again there are many negative comments in the review. However, the paper is accepted probably because of its originality. The DeepASPeer - Aspect model rejects it by a decision probability of 68%, and DeepASPeer models the sentiment distribution concerning aspects and weakly rejects it by a decision probability of 56%. Figure 5 shows a scenario where the paper is related to a conference where the meta chair sets the aspect priorities, giving the highest priority to originality. After incorporating these priorities,
the model changed its decision to ACCEPT (with a decision probability of 60%). This little human intervention will help them study the relationship of an aspect with the decision process more closely, thus assisting them in making a decision. By statistical analysis over more papers from a conference, more concrete aspect scores can
be calculated, which could assist the decision system in adapting itself for any specific conference. 4.6.2 Effect of adding rating information: We also investigated the effect of adding the rating information already provided
Figure 4: Example 1; Attention visualization of manually controlled aspect importance. Figure 5: Example 2; Attention visualization of manually controlled aspect importance. by the reviewers. In particular, we infused the rating score or recommendation score of the reviewers by concatenating the review scores at the last hidden representation before the final fully connected layer in our model. Adding this information boosted our accuracy to 91.89% by 11.84%. However, this is not the ideal situation, as the editor or meta chair should make the decision based on the reviewer’s textual comments, as decisions appeared to be
significantly influenced by the reviewers’ recommendations which is discussed in Section 1. 4.7 Error analysis • Contradictory reviews: Our model sometimes gets confused by the contradictory contextual statements. The reason can be contradiction between the different reviewers or the same reviewer(we have combined the reviews). Table 7 shows a few of such examples. The first example for (Paper id: ICLR_2020_1011), the review comment, indicates that the motivation is clear. In contrast, another comment for the same paper indicates that the approach is not motivated while contradicting each other. Similarly, in another example, for (Paper id: NIPS_2016_354), the review says that the paper is clear and well written. On the other hand, another review for the same paper negatively expresses the paper’s clarity. • Complex reviews having multiple aspects: Few review comments contain multiple aspects in a single review sentence. For example: "The paper is not clear and not motivating“. In our investigation, we have considered that each review sentence has only one aspect category. This often confuses the model resulting in mis-classification. • Reviewer and Editor’s diverse opinions: After reading the entire paper, there can be different opinions between reviewers and the Editor or Chair, as our model relies on its decision by considering the reviewer’s perspective. It fails in these cases. For example, in the paper (Paper id: NIPS_2017_167) majority of the reviewer comments are negative on various aspects, so our model predicts as rejected. However, the paper is accepted by the editor or chair. 5 conclusion In this work, we have investigated how AI systems could assist the editor or chair in anticipating the final decision within a limited time. Each venue has a distinct focus on a different aspect of a paper. Therefore, a generic model needs to be developed to solve this problem. We developed a deep neural architecture that uses three sources of information to tackle the complex task of peer review decision making: the peer review texts, aspects, and sentiment associated with the reviews. Our model is generic and can be easily adapted to different aspect categories that may have to be considered depending upon the types of conferences and/or journals. Our result shows that our model achieves competitive performance on the recent dataset. We found that aspects, along with their corresponding sentiment, can help build a generic controllable system to assist the editor/chair in determining the outcome based on the reviews of a paper to make better editorial decisions.