{
    "abstractText": "With the development of artificial intelligence technology, an increasing number of human action recognition (HAR) methods are being applied to tennis training action analysis. The HAR methods based on skeletal points have been extensively researched and applied due to their superior action expression capabilities. In order to enhance the HAR ability of tennis players and effectively capture the detailed features in training actions, this paper proposes a tennis training action analysis model based on graph convolutional neural networks. Firstly, this paper establishes the limb vectors of humans in threedimensional spatial coordinates and extracts the features of tennis error techniques based on the distances between the skeletal joints of five parts of the human body. Secondly, the data's time frames are segmented to extract attention and improve the model's ability to capture detailed features. Additionally, the attention mechanism is introduced to embed the position information into the attention map, enhancing the model's generalization ability. Experiments conducted on several action datasets demonstrate that the proposed model in this paper achieves higher HAR accuracy and better recognition results compared to most current methods. INDEX TERMS Human action recognition; tennis; attention mechanism; graph convolutional neural network",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Jihua Chen"
        }
    ],
    "id": "SP:b59f2c35203f2eb04b6a5bd3de29ddf1a396666e",
    "references": [
        {
            "authors": [
                "C Ding",
                "K Liu",
                "F Cheng"
            ],
            "title": "Spatio-temporal attention on manifold space for 3D human action recognition[J",
            "venue": "Applied Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "H Yang",
                "J Zhang",
                "S Li"
            ],
            "title": "Attend it again: Recurrent attention convolutional neural network for action recognition[J",
            "venue": "Applied Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "E Martin P",
                "J Benois-Pineau",
                "R P\u00e9teri"
            ],
            "title": "3D attention mechanism for fine-grained classification of table tennis strokes using a Twin Spatio-Temporal Convolutional Neural Networks[C]//2020",
            "venue": "25th International Conference on Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "H Ge",
                "Z Yan",
                "W Yu"
            ],
            "title": "An attention mechanism based convolutional LSTM network for video action recognition[J",
            "venue": "Multimedia Tools and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Z Zhang",
                "Z Lv",
                "C Gan"
            ],
            "title": "Human action recognition using convolutional LSTM and fully-connected LSTM with different attentions[J",
            "year": 2020
        },
        {
            "authors": [
                "K Zhu",
                "R Wang",
                "Q Zhao"
            ],
            "title": "A cuboid CNN model with an attention mechanism for skeleton-based action recognition[J",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "J Yu",
                "H Gao",
                "W Yang"
            ],
            "title": "A discriminative deep model with feature fusion and temporal attention for human action recognition[J",
            "venue": "IEEE Access,",
            "year": 2020
        },
        {
            "authors": [
                "W Du",
                "Y Wang",
                "Y. Qiao"
            ],
            "title": "Rpan: An end-to-end recurrent pose-attention network for action recognition",
            "venue": "in videos[C]//Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2017
        },
        {
            "authors": [
                "J Zhang",
                "H Hu",
                "X. Lu"
            ],
            "title": "Moving foreground-aware visual attention and key volume mining for human action recognition[J",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Z Chen",
                "L Zhang",
                "C Jiang"
            ],
            "title": "WiFi CSI based passive human activity recognition using attention based BLSTM[J",
            "venue": "IEEE Transactions on Mobile Computing,",
            "year": 2018
        },
        {
            "authors": [
                "C Dhiman",
                "K Vishwakarma D",
                "P. Agarwal"
            ],
            "title": "Part-wise spatio-temporal attention driven CNN-based 3D human action recognition[J",
            "venue": "ACM Transactions on Multimidia Computing Communications and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Y Quan",
                "Y Chen",
                "R Xu"
            ],
            "title": "Attention with structure regularization for action recognition[J",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2019
        },
        {
            "authors": [
                "K Chen",
                "D Zhang",
                "L Yao"
            ],
            "title": "Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities[J",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "L Wang",
                "Y Xu",
                "J Cheng"
            ],
            "title": "Human action recognition by learning spatio-temporal features with deep neural networks[J",
            "venue": "IEEE Access, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Z Gharaee",
                "P G\u00e4rdenfors",
                "M. Johnsson"
            ],
            "title": "First and second order dynamics in a hierarchical SOM system for action recognition[J",
            "venue": "Applied Soft Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Y Zhang",
                "M Po L",
                "M Liu"
            ],
            "title": "Data-level information enhancement: Motion-patch-based Siamese Convolutional Neural Networks for human activity recognition in videos[J",
            "venue": "Expert Systems with Applications,",
            "year": 2020
        },
        {
            "authors": [
                "W Peng",
                "J Shi",
                "T Varanka"
            ],
            "title": "Rethinking the ST-GCNs for 3D skeleton-based human action recognition[J",
            "year": 2021
        },
        {
            "authors": [
                "F Tsai M",
                "H. Chen C"
            ],
            "title": "Spatial Temporal Variation Graph Convolutional Networks (STV-GCN) for skeleton-based emotional action recognition[J",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "W Peng",
                "J Shi",
                "G. Zhao"
            ],
            "title": "Spatial temporal graph deconvolutional network for skeleton-based human action recognition[J",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2021
        },
        {
            "authors": [
                "N Kilis",
                "C Papaioannidis",
                "I Mademlis"
            ],
            "title": "An Efficient Framework for Human Action Recognition Based on Graph Convolutional Networks[C]//2022",
            "venue": "IEEE International Conference on Image Processing",
            "year": 2022
        },
        {
            "authors": [
                "C Dai",
                "Y Wei",
                "Z Xu"
            ],
            "title": "An Investigation of GCN-based Human Action Recognition",
            "venue": "Using Skeletal Features[C]//2022 27th International Conference on Automation and Computing",
            "year": 2022
        },
        {
            "authors": [
                "F Setiawan",
                "N Yahya B",
                "J Chun S"
            ],
            "title": "Sequential inter-hop graph convolution neural network (SIhGCN) for skeleton-based human action recognition[J",
            "venue": "Expert Systems with Applications,",
            "year": 2022
        },
        {
            "authors": [
                "N Heidari",
                "A. Iosifidis"
            ],
            "title": "Temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition[C]//2020 25th international conference on pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "S Alsawadi M",
                "M. Rio"
            ],
            "title": "Human Action Recognition using BlazePose Skeleton on Spatial Temporal Graph Convolutional Neural Networks[C]//2022",
            "venue": "9th International Conference on Information Technology,",
            "year": 2022
        },
        {
            "authors": [
                "H Park",
                "J Wang Z",
                "N Das"
            ],
            "title": "SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition",
            "venue": "Models[C]//Proceedings of the AAAI Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Y Tang",
                "L Zhang",
                "F Min"
            ],
            "title": "Multiscale deep feature learning for human activity recognition using wearable sensors[J",
            "venue": "IEEE Transactions on Industrial Electronics,",
            "year": 2022
        },
        {
            "authors": [
                "W Huang",
                "L Zhang",
                "H Wu"
            ],
            "title": "Channel-Equalization-HAR: a lightweight convolutional neural network for wearable sensor based human activity recognition[J",
            "venue": "IEEE Transactions on Mobile Computing,",
            "year": 2022
        },
        {
            "authors": [
                "W Huang",
                "L Zhang",
                "S Wang"
            ],
            "title": "Deep ensemble learning for human activity recognition using wearable sensors via filter activation[J",
            "venue": "ACM Transactions on Embedded Computing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "C Han",
                "L Zhang",
                "Y Tang"
            ],
            "title": "Human activity recognition using wearable sensors by heterogeneous convolutional neural networks[J",
            "venue": "Expert Systems with Applications,",
            "year": 2022
        },
        {
            "authors": [
                "K Wang",
                "J He",
                "L. Zhang"
            ],
            "title": "Sequential weakly labeled multiactivity localization and recognition on wearable sensors using recurrent attention networks[J",
            "venue": "IEEE Transactions on Human-Machine Systems,",
            "year": 2021
        },
        {
            "authors": [
                "S Liu",
                "X Bai",
                "M Fang"
            ],
            "title": "Mixed graph convolution and residual transformation network for skeleton-based action recognition[J",
            "venue": "Applied Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "J Zhang",
                "G Ye",
                "Z Tu"
            ],
            "title": "A spatial attentive and temporal dilated (SATD) GCN for skeleton \u2010 based action recognition[J",
            "venue": "CAAI Transactions on Intelligence Technology,",
            "year": 2022
        },
        {
            "authors": [
                "Q Xu",
                "W Zheng",
                "Y Song"
            ],
            "title": "Scene image and human skeletonbased dual-stream human action recognition[J",
            "venue": "Pattern Recognition Letters,",
            "year": 2021
        },
        {
            "authors": [
                "C Zhang",
                "J Liang",
                "X Li"
            ],
            "title": "Human action recognition based on enhanced data guidance and key node spatial temporal graph convolution[J",
            "venue": "Multimedia Tools and Applications,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nINDEX TERMS Human action recognition; tennis; attention mechanism; graph convolutional neural network\nI. INTRODUCTION Combining sports and computer technology can help athletes learn and master sports skills more efficiently and effectively, thereby improving their performance. This has been made possible due to the rapid development of computers and artificial intelligence in recent years [1]. During tennis training, it is crucial to efficiently identify and correct incorrect movements in a timely manner. Evaluation and analysis of human action are central to false HAR [2-3]. The majority of earlier HAR methods relied on kinetic models. While these methods achieved remarkable results in simple periodic movements, their prediction accuracy for more complex movements was not as precise. The accurate prediction of future motion postures for humans is challenging due to the non-periodic and stochastic nature of human motion. Some successful research has been conducted on false HAR, proposing the establishment of a limb node coordinate system to enhance the generality and efficiency of user HAR. This has been achieved through the use of a finite state method\nof pose sequences, which has shown potential in improving user recognition [4]. However, the recognition accuracy of this method is relatively low. Other researchers have implemented clustering algorithms in video sequences of human motion to enhance motion precision [5]. These methods reduce the computational complexity of the computer while improving the practicability and precision of recognition. However, they are not suitable for precise real-time evaluation of actions and are primarily employed for HAR [6-9]. Estimating a person's pose involves locating the joints of their body to provide an accurate description of their current position and motion. The depth camera is commonly used to measure distances, construct a 3D model, and determine precise joint locations. In 2010, Microsoft released Kinect, a motion-sensing gaming device [10-11]. Due to its depth camera, Kinect has been frequently utilized as a research tool for HAR in vision algorithms. Some researchers have developed a real-time Kinect-based retrieval and evaluation system for human motion [12]. More research is being conducted on motion\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\nrecognition rather than human motion assessment in the context of medical rehabilitation training [3]. Skeletal data lacks complex background information and dynamic environment, but it possesses greater expressive capacity and adaptability to surroundings [14]. Consequently, there has been an increase in research on HAR for skeletal point data. To employ CNN for action categorization, researchers have encoded the characteristics of skeleton groups in both time and space. By utilizing the joint positions of the 3D skeleton and a particle swarm-optimized support vector machine, researchers have successfully identified human actions [15-17]. Additionally, some researchers have proposed a method that combines deep motion maps and convolutional neural networks (CNN) to extract superior features using the robust feature extraction capabilities of deep learning. In recent years, the graph convolutional neural network (GCN) has emerged, which can process graph-structured data as input and possesses inherent advantages over CNN in modeling and analyzing skeletal point data [18-21]. In 2018, researchers proposed the application of GCN to skeletal point HAR and introduced the space-time graph convolutional network (ST-GCN), which extended GCN to the space-time domain while considering the adjacency relationship between skeletal points in both space and time. Subsequently, a two-stream adaptive GCN (2s-AGCG) based on ST-GCN was proposed, incorporating two learnable matrices [22-25]. However, this approach involves significant computational requirements and overlooks frame differences. To address these limitations, researchers proposed the Action Structure GCN (AS-GCN), which included an inference module for capturing potential dependencies among actions and achieving greater expressiveness [26]. Other scholars have also applied techniques such as wearable sensors, lightweight convolutional neural networks, and ensemble learning to HAR in order to improve recognition accuracy [27-31]. Table 1 summarizes the experimental results of the aforementioned methods based on skeletal point data.\nNevertheless, while the aforementioned methods pay attention to the spatial connection of bone point data, they\ndisregard the detailed information in the action process, the recognition accuracy is low, and they are not well suited for the recognition of complex tennis movements [32-36]. To improve the accuracy of HAR and the performance of the model in terms of complex tennis actions, this paper proposes a tennis training action analysis model. The application of the GCN-based method in tennis training contributes to the advancement of HAR techniques in the context of tennis, leading to improved accuracy, effective feature extraction, and enhanced recognition capabilities. The contributions of this paper is as follows:  This paper introduces and applies the GCN method\nfor the first time in the context of tennis training. By utilizing GCN, the model can effectively capture and analyze the complex relationships and dependencies among different skeletal joints, leading to improved accuracy in recognizing and analyzing tennis actions.\n The proposed GCN-based method significantly improves the accuracy of Human Action Recognition HAR specifically for tennis moves. By leveraging the structural information of the human body and incorporating limb vectors and attention mechanisms, the model achieves higher HAR accuracy compared to existing approaches.  The model successfully extracts technical action characteristics of tennis errors by considering the distances between the skeletal joints. This approach enables the identification and analysis of specific errors in tennis techniques, providing valuable insights for players to improve their performance.  The incorporation of attention mechanisms in the proposed method enhances the model's capability to extract relevant attributes from the temporal frames of the data. This attention-based approach improves the model's ability to focus on important aspects of the tennis actions, leading to more accurate recognition and analysis results. II. BACKGROUND Before evaluating the tennis training action, it is necessary to preprocess the video image and analyze the characteristics of the human body action. Video image preprocessing is the key and foundation of HAR feature extraction, and the effect of feature extraction affects the final action evaluation and recognition results.\nA. IMAGE PREPROCESSION When the video image is collected by the camera, if the external light is too sufficient, the brightness of the image will be too high, resulting in overexposure. Conversely, if the light is too dark, the image brightness will be too low, which means underexposure. In order to extract more effective features, it is necessary to adjust the brightness of the improperly exposed image as much as possible [7].\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\nThe most intuitive way is to convert RGB color space to HSL or HSV color space. Convert from RGB to HSL, the calculation formula of lightness l is     0.5 max , , min , ,l R G B RG B  where  , , 0,1R G B . And we have  0,1l . From RGB to HSV, the calculation formula of value v is\n max , ,v R G B Further, linearly adjust the RGB three channels of the\nimage at the same time    , ,g x y f x y  \nwhere  ,f x y is the pixel to be processed,  ,g x y is the output pixel,  is the gain parameter, which is used to control the contrast degree, and  is the bias parameter to control the lightness. During the acquisition of video from a camera, digitization, transmission through a network or other media, storage to equipment, and related processing, external hardware environment or internal coding algorithm will generate noise. The image quality is negatively impacted by the image noise generated by the video image, which may interfere with the image content and influence the subsequent processing algorithm. The image is denoised using the mean filtering technique in this paper. Mean filtering [4, 13, 19] consists of establishing a template for a specific pixel point  ,p x y on an image, this template containing the adjacent points  ,f x y , and obtaining the average value of all pixels in the template, that is, all pixels in the template. The sum of pixels is then divided by the template's total number of pixels, m. Then, this average value is assigned to the previously specified pixel point\n    1\n1, , m\ni i p x y f x y m   \nB. HUMAN POSTURE ESTIMATION Ordinarily, depth cameras or binocular cameras are used to capture data during the acquisition of human posture [1, 2, 11]. The data captured by depth cameras contains more distance information compared to data captured by standard cameras. Microsoft Kinect employs a Light Coding structured light solution, where a laser strikes the surface of an object, creating a spot known as a laser scattering spot. The laser scatter varies based on the object's surface and distance. Kinect emits near-infrared light and evenly projects it into space, equipped with an infrared camera to capture the scattered light and decode it, resulting in a depth image. Figure 1 depicts the perspective of the information acquired by the Kinect device. The human skeleton joint points acquired by the Kinect device are represented as the spatial coordinate system Z axis facing directly in front of the camera, while the camera records a picture in a plane X, Y, perpendicular to the axis of rotation Z.\nIn human action recognition, skeletal joints are the key points of the human body that are used to represent and classify different human actions. These joints are typically located at the major points of the body, including the head, neck, shoulders, elbows, wrists, hips, knees, and ankles. The skeletal joints are often captured using depth sensors such as Microsoft Kinect or RGB-D cameras, and the data is then processed to extract the key features required for action recognition. The captured data can be represented as a time series of 3D joint positions or as a skeleton graph with joints as nodes and the connections between joints as edges. The skeleton joint diagram of the Kinect detection channel is shown in Figure 2."
        },
        {
            "heading": "III. METHOD",
            "text": "In this section, we propose a GCN-based model for tennis training action analysis. Firstly, we establish the limb vector and extract the technical action features related to tennis training. Secondly, we segment the time frames of the data to extract attention. Additionally, we introduce an attention mechanism to incorporate location information into the attention map. As depicted in Figure 3, we divide the human body into four parts: the torso, the left arm, the right arm, and the left leg. A tennis player's overall action can be categorized into primary action and secondary action. The primary action reflects the global state of the movement pattern, while the secondary action reflects the local state of the movement pattern. To accurately represent the action, it is essential to combine the characteristics of both primary and secondary\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\nactions. For each of the five major human body parts, the respective limb vectors are as follows:\n         1 , 2 , 3 , 4 , 5P t P t P t P t P t And the angular velocity is      1t t t     The distance from each part of the tennis player to the joint point is\n       2 2 21 2 1 2 1 2, ,id x y z x x y y z z     \n    1 , , , ,\nn\ni i\ni\nd x y z d x y z n   \nFurther, we establishe ST-GCN. Input the skeleton sequence diagram, divide according to the distance relationship between the joint point and the center point, and divide it into the current node, the point adjacent to the current node and close to the center of gravity, and the point adjacent to the current node and far away from the center of gravity, a total of three sub-points Set, and set three weight vectors. The specific expression formula is:\n  0,if 1,if\n0,if\ni j\nti ti i j\ni j\nr r\nw k r r\nr r\n      \nwhere tik represents the characteristics of different nodes, t is the node of different frames, and i is the joint point of the different human body. The average distance between the node and the center of gravity is denoted by r. This method of division improves the difference information of the joint points, which can represent the centripetal and centrifugal motions of the joint points, which is more consistent with the actual situation of human activities, and has enhanced action modeling capabilities. The structure of the space-time graph is shown in Figure 4.\nThe input data is  /h T h N  , where h is the number of segments in the time frame, N is the size of the attention matrix, and T is the dimension of the frame. And the average operation in each time period eliminates the influence of channel C. The rules for constructing the spatio-temporal diagram are as follows: 1) Within a single frame, follow the natural skeleton connection relationship of the human body to construct the spatial graph. 2) Connecting the same nodes in two adjacent frames to form a temporal edge.\n3) The nodes in all frames form a node-set. The skeletal data contains valuable information within the connections between joint points. In this paper, we represent all bones as vectors, considering the central joint point in the human skeleton data as the source joint point. The length of each bone segment is reflected by the magnitude of its corresponding vector. Furthermore, the direction of the bone vector indicates the orientation of the bone, originating from the closest node to the source joint point and extending towards the farthest node. In our approach, both the skeletal node data and the skeleton data are processed through the same network to yield distinct outcomes. The final score is determined by employing a two-stream fusion method that combines the results obtained from the two data streams.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\nThe method in this paper is based on GCN as a whole, which integrates the attention mechanism and ST-GCN, and divides the human skeleton into five parts. The flow of our algorithm is as follows:\nThe flow of the method in this paper\nInput: Tennis training data set\nOutput: Action type\n1. Built a human limb vector in 3D spatial coordinates;\n2. Calculate the distance between the skeletal joints of the five parts of the human body; 3. Extract the technical movement characteristics of tennis errors;\n4. Segment the time frames of the data to extract attention;\n5. Embed the position information into the attention map;\n6. Input the extracted feature information into ST-GCN;\n7. Output of the identified tennis action type through the full connection layer. The data is first regularized, further, fed into a network containing a double-layer AM and ST-GCN, and finally passed through a fully connected layer. The overall structure is shown in Figure 5."
        },
        {
            "heading": "IV. RESULTS",
            "text": "In this section, we evaluate the proposed approach for tennis training action detection. All of the experiments in this paper use two Nvidia RTX3070 GPUs, an Intel Core l9-10850K CPU, and 64 GB memory.\nA. DATASETS This article utilizes the NTU-RGB+D 60 and Kinetics datasets [23, 26]. The NTU-RGB+D 60 dataset is specifically designed for HAR and includes joint annotations. It comprises 56,880 action samples, each consisting of videos, sequences of depth maps, 3D skeleton data, and infrared data. The dataset was captured using three Microsoft Kinect v2 cameras, with the 3D skeleton data providing the positions of 25 body joints per frame. Each sample consists of 300 frames, and for samples with\nfewer frames, the sequence is repeated to reach a total of 300 frames. The dataset includes X-Sub and X-View segmentation methods. On the other hand, the Kinetics dataset is a large-scale dataset for HAR extracted from YouTube. It encompasses 600 categories of human actions, with each category containing at least 600 videos. In total, there are 300,000 video fragments in this dataset. Unlike the NTU-RGBD dataset, Kinetics only provides RGB videos. Therefore, we utilize the OpenPose toolbox to obtain skeleton data for specific pixels. Each skeleton in this dataset consists of 18 body nodes, and each sample consists of 300 frames. The NTU-RGB+D 120 dataset is an extension of the NTURGB+D 60 dataset, containing a more extensive collection of human action recognition skeleton data. It consists of 11,480 sequences performed by 106 subjects, covering 120 action categories. The NTU-RGB+D 120 dataset also includes X-Sub and X-View segmentation methods. In data preprocessing, we follow the approaches used in previous works such as references 7, 13, and 15.\nB. PARAMETER DETERMINATION To determine the optimal number of time frame segments, a series of experiments were conducted with varying segment counts. Using the same method of segmentation on the same dataset. In this paper, we seek the optimal number of segments using only skeletal point single-stream data without dual-stream fusion on NTU-RGB+D 60, as shown in Table 2.\nTable 2 Comparison of HAR accuracy of different segmented AM\nParameter X-View X-Sub Kinectics ST-GCN+AM-0 92.17 88.64 35.41 ST-GCN+AM-2 93.14 88.99 36.71 ST-GCN+AM-4 94.15 89.13 36.89 ST-GCN+AM-6 94.26 89.28 37.21 ST-GCN+AM-8 94.23 88.75 37.03 ST-GCN+AM-10 93.89 88.12 36.55 The inflection point occurs when there are four segments. When there are fewer than six segments, there is a noticeable correlation between the number of segments and an increase in the accuracy of the model. However, when there are more than six segments, the correlation between the model's accuracy and the increase in the number of segments tends to diminish. Analyzing the results, we observe that segmenting an entire action frame allows us to extract attention within each segment. Generally, the attention position at the beginning and end of a complete action differs. Therefore, increasing the number of segments can significantly enhance the model's final score. The reason why the model score doesn't show significant improvement beyond six segments is that a complete action\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\ncan be adequately represented by dividing it into three to six segments. Adding more segments only escalates the computational effort of the model without substantially enhancing its score.\nC. RESULTS AND ANYALYSIS In this paper, ST-GCN is selected as the baseline, and 2sAGCN, T-Conv, ST-LSTM, and ST-RNN are selected as the comparison algorithms to verify the effectiveness of the proposed method. Without modifying the number of network layers and the same hyperparameters as ST-GCN, we conduct multiple experiments, taking the accuracy of Top-1 and Top-5 as the test standard, as shown in Table 3, Table 4, and Figure 6.\nTable 3. Comparison of HAR accuracy of different methods on X-View\nMethods Top-1 Top-5 ST-GCN [20] 84.37 - 2s-AGCN [23] 84.49 - T-Conv [24] 77.63 - ST-LSTM [11] 79.37 - ST-RNN [13] 67.45 -\nOUR 85.58 95.69\nTable 4. Comparison of HAR accuracy of different methods on X-Sub\nMethods Top-1 Top-5 ST-GCN 83.54 - 2s-AGCN 84.89 - T-Conv 75.37 - ST-LSTM 76.49 - ST-RNN 73.52 - OUR 85.06 96.59\nFigure 6. Comparison of HAR accuracy of different methods on Kinectics\nIn comparison to the unimproved ST-GCN, the addition of segmented AM and joint block modules resulted in improved recognition accuracy for the model. This improvement was consistent across large datasets as well. The network model with the two added modules exhibited an improvement rate of approximately 1.52% on X-Sub Top-1 and approximately 1.21% on X-View Top-1, compared to ST-GCN. On the Kinetics dataset, there was\nan improvement rate of about 1.07% on Top-1 and about 1.45% on Top-5. Furthermore, we compared the performance of different methods on the larger dataset NTU-RGB+D 120, we also increased the comparison algorithms, as shown in Table 5. It can be observed that due to the increase in data volume and the complexity of actions, the HAR accuracy of different methods has slightly decreased. However, the performance of our proposed method remains significantly better than other algorithms.\nTable 5. Comparison of Top-1 of different methods\nMethods NTU-RGB+D 120X-View X-Sub ST-GCN 79.53 78.87 2s-AGCN 80.33 81.27 T-Conv 76.34 75.37 ST-LSTM 78.33 77.65\nAS-GCN [26] 80.21 81.37 STV-GCN [28] 82.37 81.55 ST-GRU [13] 78.97 79.11 ST-RNN 75.59 74.12 OUR 82.33 83.17\nD. Segment time comparison The proposed method solely includes the segmental time AM module, in contrast to the 2s-AGCN and other recent methods, which introduce dual-stream fusion. As a result, the proposed method has a lower time complexity. The paper compared the running time of different methods to demonstrate the efficiency of the proposed method, and the results are depicted in Figure 7.\nIt can be observed that the algorithm of the proposed method takes less time and has a lower time complexity than 2s-AGCN, T-Conv, ST-LSTM, and ST-RNN.\nE. Ablation experiments Finally, we conducted ablation experiments to validate the effectiveness of the GAN-based tennis training action analysis model proposed in this paper. The core modules of our algorithm are tennis technique feature extraction (TFE), data temporal frame segmentation (TFS), and AM. We\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2017 7\nvalidated the effectiveness of each module on the X-View of the NTU-RGB+D 60 dataset, with ST-LSTM as the baseline algorithm. The experimental results are shown in\nTable 7.\nTable 6. Comparison of HAR accuracy of different modules\nMethods Top-1 ST-LSTM 76.49 ST-LSTM+TFE 79.63 ST-LSTM+TFS 77.56 ST-LSTM+AM 82.13\nST-LSTM+TFE+TFS+AM 85.06\nIt can be observed that all three modules can improve the performance of ST-LSTM to some extent, with the data temporal frame segmentation module providing the smallest improvement and the AM module providing the largest improvement. Effective extraction of tennis technique features contributes to enhancing HAR (Human Action Recognition) accuracy."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, a tennis training action analysis model based on GCN is proposed to enhance the HAR accuracy of tennis moves and assist players in effective training. The paper first establishes limb vectors in three-dimensional spatial coordinates and extracts the technical action characteristics of tennis errors based on the distance between the five skeletal joints of the human body. Secondly, the temporal frames of the data are segmented to extract attention, thereby improving the model's ability to capture specific attributes. Additionally, an attention mechanism is developed to incorporate position information into the attention graph, enhancing the model's generalization capability. Experimental results on multiple action datasets demonstrate that the proposed model achieves higher HAR accuracy and better recognition results compared to existing approaches. One limitation of this paper is the absence of a coordinated attention structure to capture cross-channel information. In future research, the model will be further enriched to better extract feature information across different time periods, enhancing the model's generalization ability and recognition accuracy. Furthermore, based on the method proposed in this article, a training behavior detection system for athletes will be developed, enabling real-time detection of specific training actions and allowing for specific adjustments according to different needs."
        }
    ],
    "title": "A Tennis Training Action Analysis Model Based on Graph Convolutional Neural Network",
    "year": 2023
}