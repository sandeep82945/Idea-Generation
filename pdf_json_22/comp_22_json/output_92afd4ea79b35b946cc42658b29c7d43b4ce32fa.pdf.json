{
    "abstractText": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org GECCO \u201922 Companion, July 9\u201413, 2022, Boston, MA, USA \u00a92022 Association for Computing Machinery. ACM ISBN 978-1-4503-9268-6/22/07. . . $15.00 https://doi.org/10.1145/3520304.3533645",
    "authors": [
        {
            "affiliations": [],
            "name": "Dirk Thierens"
        },
        {
            "affiliations": [],
            "name": "Peter A.N. Bosman"
        }
    ],
    "id": "SP:06d49d530792bf05edc938196cf20fb32683d72e",
    "references": [],
    "sections": [
        {
            "text": "GECCO 2022 Tutorial\nModel-Based Evolutionary Algorithms\nDirk Thierens Utrecht University Department of Information and Computing Sciences Utrecht, The Netherlands\nPeter A.N. Bosman Centrum Wiskunde & Informatica \u2013 CWI (Centre for Mathematics and Computer Science) Amsterdam, The Netherlands\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org GECCO \u201922 Companion, July 9\u201413, 2022, Boston, MA, USA \u00a92022 Association for Computing Machinery. ACM ISBN 978-1-4503-9268-6/22/07. . . $15.00 https://doi.org/10.1145/3520304.3533645\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 01/120\nOutline\nModel-Based Evolutionary Algorithms (MBEA)\n\u25ee Introduction \u25ee Part I: Discrete Representation \u25ee Part II: Real-Valued, Permutation, and Program Representations\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 02/120\nWhat ?\nEvolutionary Algorithms\n\u25ee Population-based, stochastic search algorithms \u25ee Exploitation: selection \u25ee Exploration: mutation & crossover\nModel-Based Evolutionary Algorithms\n\u25ee Population-based, stochastic search algorithms \u25ee Exploitation: selection \u25ee Exploration:\n1. Learn a model from selected solutions 2. Generate new solutions from the model (& population)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 03/120\nWhat ?\nModel-Based Evolutionary Algorithms (MBEA)\n\u25ee a.k.a. Estimation of Distribution Algorithms (EDAs) \u25ee a.k.a. Probabilistic Model-Building Genetic Algorithms \u25ee a.k.a. Iterated Density Estimation Evolutionary Algorithms\nMBEA = Evolutionary Computing + Machine Learning\nNote: model not necessarily probabilistic\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 04/120\nWhy ?\nGoal: Black Box Optimization\n\u25ee Little known about the structure of the problem \u25ee Clean separation optimizer from problem definition \u25ee Easy and generally applicable\nApproach\n* Classical EAs: need suitable representation & variation operators * Model-Based EAs: learn structure from good solutions\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 05/120\nDiscrete Representation\n\u25ee Typically binary representation \u25ee Higher order cardinality: similar approach\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 06/120\nProbabilistic Model-Building Genetic Algorithm\nType of Models\n\u25ee Univariate: no statistical interaction between variables considered. \u25ee Bivariate: pairwise dependencies learned. \u25ee Multivariate: higher-order interactions modeled.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 07/120\nUnivariate PMBGA\nModel\n* Model: probability vector [p1, ... , p\u2113] (\u2113: string length) * pi : probability of value 1 at string position i * p(X ) = \u220f \u2113\ni=1 p(xi ) (p(xi ): univariate marginal distribution)\n\u25ee Learn model: count proportions of 1 in selected population \u25ee Sample model: generate new solutions with specified probabilities\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 08/120\nUnivariate PMBGA\nDifferent Variants \u25ee PBIL (Baluja; 1995)\n\u25ee Prob. vector incrementally updated over successive generations\n\u25ee UMDA (Mu\u0308hlenbein, Paass; 1996) \u25ee No incremental updates: example above \u25ee Compact GA (Harik, Lobo, Goldberg; 1998) \u25ee Models steady-state GA with tournament selection \u25ee DEUM (Shakya, McCall, Brown; 2004) \u25ee Uses Markov Random Field modeling\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 09/120\nA hard problem for the univariate FOS\nData\n000000 111111 010101 101010 000010 111000 010111 111000 000111 111111\nMarginal Product (MP) FOS\nP\u0302(X0X1X2) P\u0302(X3X4X5)\n000 0.3 0.3 001 0.0 0.0 010 0.2 0.2 011 0.0 0.0 100 0.0 0.0 101 0.1 0.1 110 0.0 0.0 111 0.4 0.4\nUnivariate FOS\nP\u0302(X0) P\u0302(X1) P\u0302(X2) P\u0302(X3) P\u0302(X4) P\u0302(X5)\n0 0.5 0.4 0.5 0.5 0.4 0.5 1 0.5 0.6 0.5 0.5 0.6 0.5\n\u25ee What is the probability of generating 111111? \u25ee Univariate FOS: 0.5 \u00b7 0.6 \u00b7 0.5 \u00b7 0.5 \u00b7 0.6 \u00b7 0.5 = 0.0225 \u25ee MP FOS: 0.4 \u00b7 0.4 = 0.16 (7 times larger!)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 10/120\nLearning problem structure on the fly\n\u25ee Without a \u201cgood\u201d decomposition of the problem, important partial solutions (building blocks) are likely to get disrupted in variation. \u25ee Disruption leads to inefficiency. \u25ee Can we automatically configure the model structure favorably? \u25ee Selection increases proportion of good building blocks and thus \u201ccorrelations\u201d between variables of these building blocks. \u25ee So, learn which variables are \u201ccorrelated\u201d. \u25ee See the population (or selection) as a data set. \u25ee Apply statistics / probability theory / probabilistic modeling.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 11/120\nBivariate PMBGA\nModel\n\u25ee Need more than just probabilities of bit values \u25ee Model pairwise interactions: conditional probabilities\n\u25ee MIMIC (de Bonet, Isbell, Viola; 1996) \u25ee Dependency Chain \u25ee COMIT (Baluja, Davies; 1997) \u25ee Dependency Tree \u25ee BMDA (Pelikan , Mu\u0308hlenbein; 1998) \u25ee Independent trees (forest)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 12/120\nBivariate PMBGA\nMIMIC\n\u25ee Model: chain of pairwise dependencies. \u25ee p(X ) = \u220f\n\u2113\u22121 i=1 p(xi+1|xi )p(x1).\n\u25ee MIMIC greedily searches for the optimal permutation of variables that minimizes Kullack-Leibler divergence.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 13/120\nBivariate PMBGA\nCOMIT\n\u25ee Optimal dependency tree instead of linear chain. \u25ee Compute fully connected weighted graph between problem variables. \u25ee Weights are the mutual information I (X ,Y ) between the variables. \u25ee I (X ,Y ) = \u2211\ny\u2208Y\n\u2211 x\u2208X p(x , y) log p(x ,y) p(x)p(y) .\n\u25ee COMIT computes the maximum spanning tree of the weighted graph.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 14/120\nBivariate PMBGA\nBMDA\n\u25ee BMDA also builds tree model. \u25ee Model not necessarily fully connected: set of trees or forrest. \u25ee Pairwise interactions measured by Pearson\u2019s chi-square statistics.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 15/120\nBivariate PMBGA\nDSMGA\n\u25ee Dependency Structure Matrix Genetic Algorithm (Yu, Goldberg, Sastry, Lima, Pelikan; 2009) \u25ee Dependency Structure Matrix (DSM) contains the information of pairwise interactions. \u25ee DSMGA constructs the DSM by using mutual information metric. \u25ee DSM clustering aims to transfer the pair-wise interaction information into higher-order interaction information. \u25ee DSM Clustering Metric based on the minimum description length principle (MDL).\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 16/120\nBivariate PMBGA\nDSMGA-II\n\u25ee Extended version \u21d2 DSMGA-II (Hsu, Yu; 2015). \u25ee DSMGA-II consists of four major components:\n1. pair-wise linkage detection 2. model building 3. restricted mixing 4. back mixing\n\u25ee Clustering the DSM leads to the Incremental Linkage Set: starting from one gene, incrementally add the next most dependent gene one-by-one. \u25ee Restricted mixing: focus on building-block supply. \u25ee Back mixing: when no improvement occurs, switch to the equal-acceptance criterion to reduce unnecessary evaluations on plateaus.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 17/120\nMultivariate PMBGA\nMarginal Product Model\n\u25ee Extended Compact GA (ECGA) (Harik; 1999) was first EDA going beyond pairwise dependencies. \u25ee Greedily searches for the Marginal Product Model that minimizes the minimum description length (MDL). \u25ee p(X ) = \u220fG\ng=1 p(Xg )\n\u25ee Choose the probability distribution with the lowest MDL score. \u25ee Start from simplest model: the univariate factorization. \u25ee Join two groups that result in the largest improvement in the used scoring measure. \u25ee Stop when no joining of two groups improves the score further.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 18/120\nMultivariate PMBGA\nMinimum Description Length (MDL)\n\u25ee MDL(M,D) = DModel + DData \u25ee Best factorization = the one with the lowest MDL score. \u25ee MDL is a measure of complexity.\n1. Compressed population complexity: how well the population is compressed by the model (measure of goodness of the probability distribution estimation). 2. Model complexity: the number of bits required to store all parameters of the model.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 19/120\nMultivariate PMBGA\nLearning MP model\n1. Start from univariate FOS: {{0}, {1}, {2}, ... , {l \u2212 2}, {l \u2212 1}} 2. All possible pairs of partitions are temporarily merged: {{0, 1}, {2}, ... , {l \u2212 2}, {l \u2212 1}} {{0, 2}, {1}, ... , {l \u2212 2}, {l \u2212 1}}\n... {{0}, {1, 2}, ... , {l \u2212 2}, {l \u2212 1}} ... {{0}, {1}, {2}, ... , {l \u2212 2, l \u2212 1}}\n3. Compute MDL score of each factorization. 4. Choose the best scoring factorization if better than current. 5. Repeat until no better scoring factorization is found.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 20/120\nMultivariate PMBGA\nBayesian Network\n\u25ee Probability vector, dependency tree, and marginal product model are limited probability models. \u25ee Bayesian network much more powerful model. \u25ee Acyclic directed graph. \u25ee Nodes are problem variables. \u25ee Edges represent conditional dependencies.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 21/120\nMultivariate PMBGA\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 22/120\nMultivariate PMBGA\nBayesian network learning\n\u25ee Similar to ECGA: scoring metric + greedy search \u25ee Scoring metric: MDL or Bayesian measure \u25ee Greedy search: \u25ee Initially, no variables are connected. \u25ee Greedily either add, remove, or reverse an edge between two\nvariables. \u25ee Until local optimum is reached.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 23/120\nMultivariate PMBGA\nBayesian Network PMBGAs variants\n\u25ee Bayesian Optimization Algorithm (BOA) (Pelikan, Goldberg, Cantu\u0301-Paz; 1998) \u25ee Estimation of Distribution Networks Algorithm (EBNA) (Etxeberria, Larran\u0303aga; 1999) \u25ee Learning Factorized Distribution Algorithm (LFDA) (Mu\u0308hlenbein, Mahnig, Rodriguez; 1999)\n\u25ee Similarities: All use Bayesian Network as probability model. \u25ee Dissimilarities: All use different method to learn BN.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 24/120\nHierarchical BOA\n\u25ee hBOA (Pelikan, Goldberg; 2001) \u25ee Decomposition on multiple levels. \u25ee Bayesian network learning by BOA \u25ee Compact representation. \u25ee Local Structures to represent conditional probabilities. \u25ee Preservation of alternative solutions. \u25ee Niching with Restricted Tournament Replacement\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 25/120\nMultivariate PMBGA\nMarkov Network\n\u25ee Markov Netwok EDA (MN-EDA: Santana, 2005) (DEUM: Shakya & McCall, 2007). \u25ee Probability model is undirected graph. \u25ee Factorise the joint probability distribution in cliques of the undirected graph and sample it. \u25ee Most recent version: Markovian Optimisation Algorithm (MOA) (Shakya & Santana, 2008). \u25ee MOA does not explicitly factorise the distribution but uses the local Markov property and Gibbs sampling to generate new solutions.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 26/120\nFamily Of Subsets (FOS) model\nFOS F \u25ee PMBGAs learn a probabilistic model of good solutions to\nmatch the structure of the optimization problem\n\u25ee Key idea is to identify groups of problem variables that together make an important contribution to the quality of solutions. \u25ee Dependency structure generally called a Family Of Subsets (FOS). \u25ee Let there be \u2113 problem variables x0, x1, ... , x\u2113\u22121. \u25ee Let S be a set of all variable indices {0, 1, ... , \u2113\u2212 1}. \u25ee A FOS F is a set of subsets of the set S. \u25ee FOS F is a subset of the powerset of S (F \u2286 P(S)).\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 27/120\nFamily Of Subsets (FOS) model\n\u25ee FOS can be written more specifically as:\nF = {F0,F1, ... ,F|F|\u22121}\nwhere Fi \u2286 {0, 1, ... , l \u2212 1}, i \u2208 {0, 1, ... , |F| \u2212 1}\n\u25ee Every variable is in at least one subset in the FOS, i.e.:\n\u2200i \u2208 {0, 1, ... , l \u2212 1} : ( \u2203j \u2208 {0, 1, ... , |F| \u2212 1} : i \u2208 Fj )\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 28/120\nThe Univariate Structure\n\u25ee The univariate FOS is defined by:\nFi = {i}, i \u2208 {0, 1, ... , l \u2212 1}\n\u25ee For l = 10 the univariate FOS is:\nF = {{0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}}\n\u25ee Every variable is modeled to be independent of other varibables.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 29/120\nThe Marginal Product Structure\n\u25ee The marginal product (MP) FOS is a FOS such that:\nFi \u2229 Fj = \u2205, i , j \u2208 {0, 1, ... , l \u2212 1}.\n\u25ee Univariate FOS is a MP FOS. \u25ee For l = 10 a possible MP FOS is:\nF = {{0, 1, 2}, {3}, {4, 5}, {6, 7, 8, 9}}\n\u25ee Every group of variables is modeled to be independent of other variables.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 30/120\nThe Linkage Tree Structure\n\u25ee The linkage tree (LT) FOS is a hierarchical structure. \u25ee Group of all variables is in there. \u25ee For any subset Fi with more than one variable, there are subsets Fj and Fk such that:\nFj \u2229 Fk = \u2205, |Fj | < |Fi |, |Fk | < |Fi | and Fj \u222a Fk = Fi\n\u25ee For l = 10 a possible LT FOS is\nF = {{7, 5, 8, 6, 9, 0, 3, 2, 4, 1},\n{7, 5, 8, 6, 9}, {0, 3, 2, 4, 1}, {7}, {5, 8, 6, 9},\n{0, 3, 2, 4}, {1}, {5, 8, 6}, {9}, {0, 3}, {2, 4},\n{5, 8}, {6}, {0}, {3}, {2}, {4}, {5}, {8}}\n\u25ee Variables sometimes independent, sometimes dependent. \u25ee \u2248 Path through dependency space, from univariate to joint.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 31/120\nLinkage Tree\n\u25ee Linkage Tree structure: subsets of FOS F form a hierarchical clustering. \u25ee F = {{0,1,2,3,4,5,6,7,8,9}, {0,1,2,3,4,5}, {6,7,8,9}, {0,1,2}, {3,4,5}, {7,8,9}, {0,1}, {4,5}, {8,9}, {0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}} \u25ee Each subset (of length > 1) is split in two mutually exclusive subsets. \u25ee Problem variables in subset are considered to be dependent on each other but become independent in a child subset. \u25ee For a problem of length \u2113 the linkage tree has \u2113 leaf nodes (the clusters having a single problem variable) and \u2113\u2212 1 internal nodes.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 32/120\nLinkage Tree Learning\n\u25ee Start from univariate structure. \u25ee Build linkage tree using bottom-up hierarchical clustering algorithm. \u25ee Similarity measure:\n1. Between individual variables X and Y : mutual information I (X ,Y ). 2. Between cluster groups XF i and XF j : average pairwise linkage clustering (= unweighted pair group method with a arithmetic mean: UPGMA).\nIUPGMA(XF i ,XF j ) = 1\n|XF i ||XF j |\n\u2211\nX\u2208X F i\n\u2211\nY\u2208X F j\nI (X ,Y ).\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 33/120\nLinkage Tree Learning\n\u25ee This agglomerative hierarchical clustering algorithm is computationally efficient. \u25ee Only the mutual information between pairs of variables needs to be computed once, which is a O(\u21132) operation. \u25ee The bottom-up hierarchical clustering can also be done in O(\u21132) computation by using the reciprocal nearest neighbor chain algorithm.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 34/120\nOptimal Mixing Evolutionary Algorithm (OMEA)\n\u25ee OMEA is a Model-Building EA that uses a FOS as its linkage model (Thierens & Bosman, 2011). \u25ee Characteristic of Optimal Mixing Evolutionary Algorithm (OMEA) is the use of intermediate function evaluations (inside variation) \u25ee Can be regarded as greedy improvement of existing solutions \u25ee Coined \u201cOptimal\u201d Mixing because better instances for substructures are immediately accepted and not dependent on \u201cnoise\u201d coming from other parts of the solution\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 35/120\nGene-pool Optimal Mixing EA (GOMEA)\n\u25ee FOS linkage models specify the linked variables. \u25ee A subset of the FOS is used as mixing mask \u25ee Mixing is greedy: accept only improvements (or equal). \u25ee A new FOS model is built from the population every generation. \u25ee FOS model may also be pre-specified, of course. \u25ee Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) \u25ee For each solution in the population\n\u25ee all subsets of the FOS are tried with a donor solution randomly picked from the population\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 36/120\nGene-pool Optimal Mixing EA\nGOMEA()\nPop \u2190 InitPopulation() while NotTerminated(Pop)\nFOS \u2190 BuildFOS(Pop) forall Sol \u2208 Pop\nforall SubSet \u2208 FOS Donor \u2190 Random(Pop) Sol \u2190 OptimalMixing(Sol,Donor,Subset,Pop)\nreturn Sol\nOptimalMixing(Sol,Donor,SubSet,Pop)\nNewSol \u2190 ReplaceSubSetValues(Sol,SubSet,Donor) if ImprovementOrEqual(NewSol,Sol) then Sol \u2190 NewSol return Sol\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 37/120\nLinkage Tree Genetic Algorithm\n\u25ee The LTGA is an instance of GOMEA that uses a Linkage Tree as FOS model (Thierens & Bosman, 2010, 2011). \u25ee AKA LT-GOMEA. \u25ee Each generation a new hierarchical cluster tree is built. \u25ee For each solution in population, traverse tree (random order). \u25ee Nodes (= clusters) in the linkage tree form FOS.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 38/120\nBenchmark problems\n\u25ee Onemax (counting ones)\nfOnemax(x) = \u2113\u22121 \u2211\ni=0\nxi\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 39/120\nDeceptive Trap Function\nInteracting, non-overlapping, deceptive groups of variables.\nfDT(x) = l\u2212k \u2211\ni=0\nf subDT ( x(i ,...,i+k\u22121) )\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 40/120\nNearest-neighbor NK-landscape\n\u25ee Overlapping, neighboring random subfunctions\nfNK-S1(x) = l\u2212k \u2211\ni=0\nf subNK ( x(i ,...,i+k\u22121) ) with f subNK ( x(i ,...,i+k\u22121) ) \u2208 [0..1]\n\u25ee eg. 16 subsfcts, length k = 5, overlap o = 4 \u21d2 stringlength \u2113 = 20\n0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n\u25ee Global optimum computed by dynamic programming \u25ee Benchmark function: structural information is not known ! \u25ee \u21d2 Randomly shuffled variable indices.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 41/120\nBenchmark problems\n\u25ee Hierarchical If-and-only-iFF (HIFF) (Watson, Hornby and Pollack, 1998) \u25ee Computed over multiple layers \u25ee Nodes are combined as a perfectly balanced binary tree \u25ee Problem lengths are powers of two (i.e., \u2113 = 2, 4, 8, 16, 32, ...) \u25ee Each variable is considered to be a leaf \u25ee Leaf contributes 1 \u25ee Internal node contributes 2height if children both 0 or both 1 \u25ee Internal node is 0 if children both 0; 1 if both 1; NIL else\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 42/120\nBenchmark problems \u25ee Weighted MAX-CUT\n(Karp, 1972)\n\u25ee Given a weighted graph (V ,E ), divide nodes into two sets so that total weight of edges between sets is maximized \u25ee Identify binary variable xi with each node vi\nfWeighted MAX-CUT(x) = \u2211\n(vi ,vj )\u2208E\n{\nwij if xi 6= xj 0 otherwise\n\u25ee For now, considered as a black-box problem (no partial evaluations allowed) \u25ee 5 instance types: \u25ee Fully connected graphs (\u03b2-distributed, \u03b1 = 100, \u03b2 = 1) \u25ee 2D Square-grid graphs (\u03b2-distributed, \u03b1 = 100, \u03b2 = 1) \u25ee 3D Square-torus graphs (\u03b2-distributed, \u03b1 = 100, \u03b2 = 1) \u25ee Uniformly distributed in a box, fully connected \u25ee Uniformly distributed in a box, \u230a \u221a (\u2113)\u230b nearest neighbors\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 43/120\nExperimental setup\n\u25ee 100 independent runs \u25ee Fully black-box evaluated \u25ee Observe time required to reach optimum \u25ee On a relatively slow CPU: 2.8 GHz AMD core \u25ee Harik-Lobo Population-sizing-free scheme \u25ee Setting population size optimally is hard/impossible \u25ee Use interleaving of different runs with different population sizes \u25ee Generations in larger population sizes tick slower \u25ee Smaller populations converge first \u25ee Overhead: typically somewhere between 1-4 times slower \u25ee Can actually also be faster! \u25ee Gains outweigh the overhead!\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 44/120\nExperiments - Onemax\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n25 50 100 200 400 800 1600 3200 6400 12800 25600 51200 102400 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 45/120\nExperiments - Deceptive trap\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n25 50 100 200 400 800 1600 3200 6400 12800 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 46/120\nExperiments - Overlapping NK\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n25 50 100 200 400 800 1600 3200 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 47/120\nExperiments - HIFF\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n32 64 128 256 512 1024 2048 4096 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 48/120\nExperiments - MAX-CUT fully connected\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n6 12 25 50 100 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 49/120\nExperiments - MAX-CUT 2D square grid\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n9 16 25 49 100 196 400 784 1600 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 50/120\nExperiments - MAX-CUT 2D square torus\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n9 16 25 49 100 196 400 784 1600 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 51/120\nExperiments - MAX-CUT box, fully connected\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n6 12 25 50 100 200 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 52/120\nExperiments - MAX-CUT box, \u230a \u221a (\u2113)\u230b nearest neighbors\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n10000\n6 12 25 50 100 200 GA-UX\nEDA-UF GOMEA-U GA-MPMX EDA-MPM GOMEA-MPM\nGOMEA-LT hBOA\nT im\ne to\nop ti m u m\n(s ec on\nd s)\nNumber of variables Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 53/120\nDiscussion\n\u25ee Use of univariate structure leads to exponential scale-up on non-trivial problems \u25ee GOMEA mixing is then the worst \u25ee Use of learned structure leads to polynomial scale-up \u25ee GOMEA mixing is then the best \u25ee Most efficient: LT-GOMEA (a.k.a. LTGA)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 54/120\nExperiments: conclusion\n\u25ee LTGA (= GOMEA with LT FOS) very efficient on Deceptive Trap function, Nearest-Neighbor NK landscape, and Hierarchical Trap function. \u25ee Other FOS models possible: Linkage Neighborhood OM (Bosman & Thierens, 2012). \u25ee Linkage Tree seems to be good compromise between FOS model complexity and search efficiency.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 55/120\nPredetermined vs. Learned FOS\n\u25ee Problem structure unknown: learn a FOS model. \u25ee Problem structure Information available: predetermined FOS model. \u25ee What is a good predetermined FOS model ? \u25ee Direct mapping of dependency structure of problem definition to a predetermined FOS model ? \u25ee Predetermined linkage models mirroring the static structure of the problem not sufficient (Thierens & Bosman, 2012). \u25ee Dynamically learned tree model superior to mirror structured models and to static tree model. \u25ee Question: is there an optimal, predetermined linkage model that outperforms the learned (tree) model ?\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 56/120\nParameter-less Population Pyramid\n\u25ee P3 (Goldman, Punch; 2014) \u25ee Similar to Harik-Lobo scheme: eliminates population-size\nparameter\n\u25ee Each level of a pyramid-like structure is a population of solutions. \u25ee Solutions are always hill-climbed. \u25ee All solutions encountered are stored in the pyramid structure. \u25ee At each level a Linkage Tree GA is run. \u25ee Solutions climb the pyramid ladder with increasing fitness. \u25ee Whenever a solution enters a level the linkage tree is relearned.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 57/120\nConclusions\n\u25ee \u201cBlind\u201d Evolutionary Algorithms are limited in their capability to detect and mix/exploit/re-use partial solutions (building blocks). \u25ee One requires luck or analyzing and designing ways of structure exploitation directly into problem representation and search operators. \u25ee Having a configurable model can help overcome this. \u25ee Algorithm then must learn to configure the model and thereby exploit structure online during optimization (e.g. EDAs, OMEAs).\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 58/120\nModel-based optimization\n\u25ee Assumption: problems are somehow structured \u25ee Use induction to find structure \u25ee Exploit structure for increased efficiency \u25ee Preferable to enumeration or iterated random sampling \u25ee What to induce? \u25ee Use a model that defines reasonable structures \u25ee Induce instance of the model \u25ee Model capacity determines bias strength\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 59/120\nModel-based optimization\n\u25ee Model = probability distribution \u25ee Induction = learning/estimation \u25ee Variation = sampling\n\u25ee Estimation-of-Distribution Algorithm (EDA)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 60/120\nThe Estimation-of-Distribution Algorithm (EDA)\n\u25ee Use a set of n solutions for distribution estimation \u25ee Focus on better solutions by selection \u25ee Estimate from selection \u25ee EDA: Mu\u0308hlenbein and Paa\u00df(1996)\nEDA\n1 Initialize P with n random solutions 2 Repeat until termination criterion met\n2.1 Select subset S from P 2.2 Estimate distribution from S 2.3 Draw new set of solutions O from distribution 2.4 Update P with O\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 61/120\nModel-based optimization\n\u25ee Model = description of linkages/dependencies \u25ee Induction = learning/statistical testing \u25ee Variation = mixing\n\u25ee Optimal Mixing Evolutionary Algorithm (OMEA)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 62/120\nThe Optimal Mixing Evolutionary Algorithm (OMEA)\n\u25ee Use a set of n solutions for linkage detection \u25ee Focus on better solutions by selection within variation \u25ee Estimate from selection \u25ee OMEA: Thierens and Bosman (2011)\nOMEA\n1 Initialize P with n random solutions 2 Repeat until termination criterion met\n2.1 Select subset S from P 2.2 Learn linkage model from S 2.3 Apply linkage-model guided optimal mixing\nto every individual in P to generate O 2.4 Replace P by O\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 63/120\nModel-based optimization\n\u25ee General concepts \u25ee Can be applied to different types of optimization problems/domains \u25ee In second part of tutorial: focus on other domains than binary/integer\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 64/120\nReal-valued Model-Based Evolutionary Algorithms\n\u25ee Essentially similar questions to case of binary/integer variables \u25ee We don\u2019t have the optimal model. . . \u25ee Approximate the optimal model \u25ee Match inductive search bias and problem structure \u25ee How to learn and perform variation efficiently and effectively \u25ee Trade-offs: \u25ee Quality versus complexity of approximation \u25ee Efficiency in # evaluations versus time \u25ee Essential model questions: \u25ee Can key problem structure be represented? \u25ee Can key problem structure be represented efficiently? \u25ee Can the model be learned from data? \u25ee Can the model be learned (and used for variation) efficiently?\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 65/120\nNormal distribution\n\u25ee Require practically useful models. \u25ee For instance normal distribution:\n-10 -5\n0 5\n10 -10\n-5\n0\n5\n10\n0\n2e-005\n4e-005\n6e-005\n8e-005\n0.0001\n0.00012\n0.00014\n0.00016\nx0\nx1\nP\u0302(X)\n\u25ee Only O(l2) parameters (mean, covariance matrix) \u25ee maximum-likelihood (ML) estimates well known\n\u00b5\u0302 = 1\n|S|\n|S|\u22121 \u2211\nj=0\n(S j), \u03a3\u0302 = 1\n|S|\n|S|\u22121 \u2211\nj=0\n((S j)\u2212 \u00b5\u0302)((S j)\u2212 \u00b5\u0302) T\n\u25ee Can only model linear dependencies\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 66/120\nEDAs based on the Normal Distribution\n\u25ee First uses were adaptations of PBIL \u25ee Rudlof and Ko\u0308ppen (1996) \u25ee Sebag and Ducoulombier (1998) \u25ee Although initial results were interesting, quickly found that some problems were solved more efficiently if dependencies were modeled\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 67/120\nEDAs based on the Normal Distribution\n\u25ee Make decisions based on better fit and increased complexity (e.g. P\u0302(X0,X1) vs. P\u0302(X0)P\u0302(X1))\nS P\u0302(X0)P\u0302(X1) P\u0302(X0,X1) -1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\n0.3 0.25 0.2 0.15 0.1 0.05\n-1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\n0.3 0.25 0.2 0.15 0.1 0.05\n-1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\n-1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\n0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05\n-1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\n2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2\n-1\n-0.5\n0\n0.5\n1 -1 -0.5 0 0.5 1\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 68/120\nEDAs based on the Normal Distribution\n\u25ee EDAs with factorized Normal Distributions (MIMIC, COMIT, Bayesian, Copula selection, Multivariate (Markov networks)) \u25ee Bosman and Thierens (2000, 2001) \u25ee Larran\u0303aga, Etxeberria, Lozano, and Pen\u0303a (2000) \u25ee Salinas-Gutie\u0300rrez, Herna\u0300ndez-Aguirre, and Villa-Diharce (2011) \u25ee Karshenas, Santana, Bielza, and Larran\u0303aga (2012) \u25ee On selected problems, improvements were found when using higher-order dependencies \u25ee On some problems, results didn\u2019t get much better however \u25ee Initially mainly attributed to mismatch between model and search space \u25ee Clearly true to some extent\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 69/120\nEDAs based on the Normal\u2013kernels distribution\n-10 -5\n0 5\n10 -10\n-5\n0\n5\n10\n0 0.001 0.002 0.003\n0.004 0.005 0.006 0.007 0.008 0.009\ny0\ny1\nP\u0302NK (Y0,Y1)(y0, y1)\n\u25ee Bosman and Thierens (2000) \u25ee Ocenasek and Schwarz (2002) \u25ee Ocenasek, Kern, Hansen, Mu\u0308ller, and Koumoutsakos (2004)\n\u25ee Natural tendency to fit structure of data (linear or not) \u25ee But also tendency to overfit \u25ee Maximum\u2013likelihood estimate not usable \u25ee Quality of estimation depends heavily on size of kernel\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 70/120\nEDAs based on the Normal\u2013mixture distribution\n-10 -5\n0 5\n10 -10\n-5\n0\n5\n10\n0\n0.005\n0.01\n0.015\n0.02\n0.025\ny0\ny1\nP\u0302NM (Y0,Y1)(y0, y1)\n\u25ee Gallagher, Fream, and Downs (1999) \u25ee Bosman and Thierens (2001) \u25ee Cho and Zhang (2002) \u25ee Ahn, Ramakrishna, and Goldberg (2004) \u25ee Li, Goldberg, Sastry, and Yu (2007) \u25ee Maree, Alderliesten, Thierens, and Bosman (2017)\n\u25ee Trade\u2013off between normal and normal kernels. \u25ee Maximum-Likelihood Estimate is lot of effort (EM algorithm). \u25ee Alternative: cluster, then est. normal (with max. likelihood). Dirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 71/120\nEDAs based on the Histogram Distribution\ny0\ny1\nP\u0302H(Y0,Y1)(y0, y1)\n\u25ee Bosman and Thierens (2000) \u25ee Tsutsui, Pelikan, and Goldberg (2001)\n\u25ee Easy to implement and map to integers. \u25ee Require many bins to get a good estimate. \u25ee Curse of dimensionality. \u25ee Greedy incr. factorization selection hardly possible.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 72/120\nEDAs based on latent variable models\n\u25ee Build models by projecting data onto model of lower dimensionality \u25ee Helmholtz machines, mixture of factor analyzers, etc \u25ee Shin and Zhang (2001) \u25ee Cho and Zhang (2001) \u25ee Shin, Cho, and Zhang (2001) \u25ee Cho and Zhang (2002) \u25ee Cho and Zhang (2004) \u25ee Better results than standard normal EDA on some problems, but still unable to come close to the optimum of 10-dimensional Rosenbrock function\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 73/120\nDirect use of normal distribution\n\u25ee Bad results \u25ee Rosenbrock:\nF(x) = \u2211l\u22122\ni=0 100(xi+1 \u2212 x 2 i ) 2 + (1\u2212 xi ) 2\n-4\n-2\n0\n2\n4 -4\n-2\n0\n2\n4\n0.01 0.1\n1 10\n100 1000\n10000 100000\nx0 x1\n\u25ee because. . . \u25ee Rosenbrock has narrow valley leading to minimum \u25ee Quickly samples no longer centered around minimum\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 74/120\nNo attention for the gradient\n\u25ee Distribution estimation makes no assumption on source \u25ee Source is just selected points in parameter space \u25ee Gradient info is ignored in maximum-likelihood estimate \u25ee For normal distribution: Variance goes to zero too fast\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 75/120\nIllustration on the 1-D sphere function\nF(x) = x20\nProgression in first 6 generations (top-left to bottom-right)\n0\n20\n40\n60\n80\n100\n-10 -5 0 5 10 0\n20\n40\n60\n80\n100\n-10 -5 0 5 10 0\n20\n40\n60\n80\n100\n-10 -5 0 5 10\n0\n20\n40\n60\n80\n100\n-10 -5 0 5 10 0\n20\n40\n60\n80\n100\n-10 -5 0 5 10 0\n20\n40\n60\n80\n100\n-10 -5 0 5 10\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 76/120\nAnalysis of the premature-convergence problem\n\u25ee Theoretical analysis reveals indeed limits \u25ee Gonzalez, Lozano, and Larran\u0303aga (2000) \u25ee Grahl, Minner, and Rothlauf (2005) \u25ee Bosman and Grahl (2005) \u25ee Yuan and Gallagher (2006) \u25ee There is for instance a bound on how far the mean can shift\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 77/120\nAnalysis of the premature-convergence problem\n\u25ee Variance decreases (exponentially fast)\nlim t\u2192\u221e {\u03c3\u0302(t)} = lim t\u2192\u221e\n{ \u03c3\u0302(0)c(\u03c4)t } = 0\n\u25ee This limits mean shift to a fixed factor times initial spread!\nlim t\u2192\u221e\n{\u00b5\u0302(t)} = \u00b5\u0302(0) + d(\u03c4)\n1\u2212 \u221a c(\u03c4) \u03c3\u0302(0)\n\u25ee c(\u03c4) and d(\u03c4) functions of \u25ee \u03c6() (standard normal distribution) and \u25ee \u03a6() (inverse cumulative normal distribution)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n\u03c4\nc(\u03c4) d(\u03c4) d(\u03c4) 1\u2212 \u221a\nc(\u03c4) (Bosman and Grahl (2005))\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 78/120\nIllustration on the 2-D plane function F(x) = x0 + x1\nProgression in first 6 generations\n-12 -10 -8 -6 -4 -2 0 2 4 6 -12\n-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\nx0\nx1\nError ellipse 95% Population 0 Selection 0\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 79/120\nWhat is missing?\n\u25ee Structure of landscape can be very complicated \u25ee \u201cSimple\u201d normal distr. hardly matches global structure \u25ee More involved distributions possible, but \u25ee harder, or even impossible, to estimate with ML \u25ee requires lots of data \u25ee Local structure can be approximated but. . . \u25ee there is no generalization outside of the data range \u25ee Once optimum \u201clost\u201d outside data range, EDA converges\nelsewhere, possibly not even a local optimum!\n\u25ee EDA based on maximum-likelihood estimate not efficient\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 80/120\nWays to improve\n\u25ee Gradient hybridization \u25ee Explicit use of gradient information \u25ee Apply gradient-based search to certain solutions\n(e.g. conjugate gradients) \u25ee Requires gradient computation\n\u25ee not always possible \u25ee not always reliable\n\u25ee Adapt(ive) (ML) estimation \u25ee Derivative Free \u25ee Maintain EDA properties for valley case \u25ee Adapt in other cases (to explore beyond selected solutions) \u25ee How to distinguish? \u25ee Three ingredients:\n\u25ee Adaptive Variance Scaling (AVS) \u25ee Standard-Deviation Ratio (SDR) \u25ee Anticipated Mean Shift (AMS)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 81/120\nAdapted Maximum-Likelihood Gaussian Model\n\u25ee Adaptive Variance Scaling (AVS) & Standard-Deviation Ratio (SDR) \u25ee If improvements are found\na) far from the mean, b) close to the mean,\nenlarge \u03a3\u0302 do nothing\n\u25ee Close to the mean: within one standard deviation\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 82/120\nAdapted Maximum-Likelihood Gaussian Model \u25ee Anticipated Mean Shift (AMS)\n\u25ee Anticipate where the mean is shifting \u25ee Alter part of generated solutions by shifting \u25ee On a slope, predictions are better (further down slope) \u25ee Require balanced selection to re-align covariance matrix\n-14 -12 -10 -8 -6 -4 -2 0 2 -14\n-12\n-10\n-8\n-6\n-4\n-2\n0\n2\nx0\nx1\nUnaltered Altered Realigned\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 83/120\nIllustration on a 2-D slope F(x) = x0 + x1\nProgression in first 6 generations\n-48 -42 -36 -30 -24 -18 -12 -6 0 6 -48\n-42\n-36\n-30\n-24\n-18\n-12\n-6\n0\n6\nx0\nx1\nError ellipse 95% Population 0 Selection 0\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 84/120\nAMaLGaM, CMA-ES, NES, and RP\n\u25ee AMaLGaM IDEA (or AMaLGaM for short) Adapted Maximum\u2013Likelihood Gaussian Model Iterated Density-Estimation Evolutionary Algorithm \u25ee Natural question: what is the relation to CMA-ES (Hansen (2001)) and NES (Wierstra, Schaul, Peters, and Schmidhuber (2008)) \u25ee Answer: the probability distribution \u25ee All can be seen to be EDAs: every generation they estimate/update a probability distribution (which also happens to be the normal distribution in all three cases) and perform variation by generating new samples from this distribution.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 85/120\nAMaLGaM, CMA-ES, NES, and RP\n\u25ee Differences are only in how the distribution is obtained. Where AMaLGaM uses maximum-likelihood estimates from the current generation, CMA-ES and NES base estimates on differences between subsequent generations as well as many elaborate enhancements (see tutorial on CMA-ES) and RP uses ensembles of random projections to lower dimensions to estimate covariance matrices more efficiently. \u25ee On typical unimodal benchmark problems (sphere, (rotated) ellipsoid, cigar, etc) these algorithms exhibit polynomial scalability in both minimally required population size and required number of function evaluations \u25ee CMA-ES, NES scale better than AMaLGaM on such problems\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 86/120\nParameter-free Gaussian EDAs \u25ee Parameters get in the way of ease\u2013of\u2013use\n\u25ee Remove all parameters: derive and implement guidelines \u25ee Restart mechanism to increase success probability \u25ee Typical restart scheme: increase size exponentially \u25ee Works well on Griewank (left), not so much on Michalewicz (right) \u25ee Many different schemes exist therefore (also algorithm specific, e.g. BIPOP-CMA-ES and IPOP-CMA-ES)\n0\n50\n100\n150 y0\n50\n100\n150\n200\ny1\n0 1 2 3 4 5 6 7\n0\n0.5\n1\n1.5\n2\n2.5\n3\ny0\n0.5\n1\n1.5\n2\n2.5\n3\ny1\n-2\n-1.5\n-1\n-0.5\n0\n0.5\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 87/120\nNoiseless BBOB comparison with other algorithms\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 88/120\nNoiseless BBOB comparison with other algorithms\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 89/120\nNoiseless BBOB comparison with other algorithms\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 90/120\nDimensionality reduction and problem-specific models\n\u25ee Real-world problems may be high(er) dim. (at least, \u2113\u226b 40) \u25ee Handling a full covariance matrix becomes expensive \u25ee Restrict size of covariance matrix somehow\n\u25ee Random projections, tested up to \u2113 = 103\n(Kaba\u0301n, Bootkrajang, and Durrant (2013)) \u25ee Projection-based restricted CMA-ES, tested up to \u2113 = 103\n(Akimoto and Hansen (2016)) \u25ee GOMEA-based, tested up to \u2113 = 5 \u00b7 106 (with partial eval.\u2019s)\n(Bouter, Alderliesten, Witteveen, and Bosman (2017))\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 91/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee Binary/Integer representations are discrete, but also Cartesian \u25ee Other discrete search spaces exist that are non-Cartesian \u25ee Most notably: permutation-based problems \u25ee Important real-world relevance, e.g. routing and scheduling \u25ee Brings different challenges than Cartesian spaces however \u25ee Relative ordering problems \u25ee Absolute ordering problems \u25ee Neighbor ordering problems \u25ee Combinations of these \u25ee Different types of models are more suited for specific types of ordering problem\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 92/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee Building permutation models directly not straightforward \u25ee Potential aid in the form of random keys (Bean (1997)) \u25ee Random keys encode permutations in real-valued space (via sorting)\n0 1 2 3 0.61 0.51 0.62 0.31\n\u21d2 3 1 0 2\n0.31 0.51 0.61 0.62\n\u25ee Real-valued approaches can thus be used directly \u25ee Bosman and Thierens (2001) (normal EDA) \u25ee Larran\u0303aga et al (2001) (normal EDA) \u25ee Inefficient scale-up behavior on deceptive additively decomposable relative ordering problems \u25ee Highly redundant encoding that is hard to model with a normal distribution\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 93/120\nPermutation Model-Based Evolutionary Algorithms \u25ee Use crossover on the basis of a factorization of the normal\ndistribution instead \u25ee Bosman and Thierens (2001)\n\u25ee Now obtain polynomial scale-up behavior, but redundant encoding \u25ee How about a direct modelling of probabilities of permutations? \u25ee Consider a marginal product factorization (i.e. mutually exclusive subsets of variables as in ECGA) \u25ee Once an instance is sampled for a subset of variables, other variables can\u2019t use these values anymore \u25ee One way to deal with this is explicit repair of probability tables during sampling \u25ee Bengoetxea et al (2000) \u25ee Pelikan et al (2007) \u25ee Requires very large sample sizes \u25ee Sampling repair can introduce unwanted biases\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 94/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee For relative-ordering variables, a probabilistically correct factorization approach is possible \u25ee Bosman (2003) \u25ee Continuous, Binary: P(X) = P(X0,X4)P(X1)P(X3,X2). \u25ee Permutation: P(X) = 2!1!2!5! P(X0,X4)P(X1)P(X3,X2). \u25ee Random variable Xi : position of integer i in the permutation \u2192 tackle relative\u2013ordering permutation problems. \u25ee Normalization required, because there are 5! permutations. \u25ee \u201cOddities\u201d specific to permutations exist (spurious dependencies between \u201clow\u201d variables in one building block and \u201chigh\u201d variables in another) \u25ee Require specialized adaptations of standard linkage learning / factorization techniques\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 95/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee Generate instance for each subset of variables independently \u25ee Then map to the real-valued domain using random keys and then translate the entire string into a valid permutation \u25ee Preserves relative ordering of variables in subsets \u25ee Can sample directly instead of using crossover (crossover still more robust however) \u25ee Scales polynomially and much better than normal-pdf induced crossover\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 96/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee Edge-histogram based sampling \u25ee Tsutsui, Pelikan, and Goldberg (2003) \u25ee Maps well to problems with neighboring variable relations \u25ee Model is a matrix with probabilities of edges \u25ee Matrix needs to be adjusted while sampling \u25ee For problems with neighboring relations works better than random keys\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 97/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee Gaussian \u201cequivalent\u201d in permutation space: Mallows model (GM-EDA) \u25ee Ceberio, Mendiburu, and Lozano (2011) \u25ee Requires a distance measure between permutations and a central permutation \u25ee Also requires a spread parameter (not estimated from data) \u25ee Most commonly used distance: Kendall-\u03c4 , allows factorization \u25ee Finding central permutation is NP-hard however \u25ee Fast heuristics are possible (linear in l and n) \u25ee Final parameter estimation and sampling are not trivial and require dedicated algorithms \u25ee First results are promising (permutation flow shop), outperforming Tsutsui \u25ee Ceberio, Irurozki, Mendiburu, and Lozano (2014)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 98/120\nPermutation Model-Based Evolutionary Algorithms\n\u25ee GOMEA variant for permutations based on random keys \u25ee Bosman, Luong, and Thierens (2016) \u25ee Requires a distance measure between permutations. Used product of: \u25ee Relative ordering information \u25ee Adjacency information \u25ee Possibly add random rescaling and re-encoding \u25ee First results are promising (permutation flow shop), mostly outperforming GM-EDA\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 99/120\nModel-Based Genetic Programming\n\u25ee Estimation-of-Distribution Programming (EDP) \u25ee Typically grammar based, but not always \u25ee Grammar Guided Genetic Programming (GGGP) \u25ee Grammars very useful to limit search space \u25ee But how do we use it learn structural features?\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 100/120\nModel-Based Genetic Programming\n\u25ee Early works did not use grammar, e.g PIPE (Probabilistic Incremental Program Evolution) \u25ee Salustowicz and Schmidhuber (1997) \u25ee Store probabilities of options (operators/terminals) for any node in the solution tree, bound maximum size \u25ee All nodes thus independent\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 101/120\nModel-Based Genetic Programming\n\u25ee If looking at solutions node-based, and using a fixed template, essentially have Cartesian fixed-length representation \u25ee Can use existing integer-based model-based EAs on this \u25ee eCGP (ECGA for GP) does exactly this \u25ee Sastry and Goldberg (2003) \u25ee Better results for selected problems, but use of a template has its limitations\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 102/120\nModel-Based Genetic Programming\n\u25ee Extensions to Bayesian factorizations are also possible \u25ee POLE does exactly this \u25ee Hasegawa and Iba (2008) \u25ee MOSES does not use fixed template, but has incremental tree complexity (and model complexity) using special operators \u25ee Looks, Goertzel, and Pennachin (2004) \u25ee Looks (2006)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 103/120\nModel-Based Genetic Programming\n\u25ee Alternative approach: grammar-based \u25ee Start with basic production rules \u25ee Learning: assign probabilities to rules and increase complexity and specificity of rules using heuristics \u25ee Sampling: select probabilistically from appropriate production rules \u25ee Results are promising in that less function evaluations are often needed than standard GP, but time-complexity is (much) larger \u25ee Shan, McKay, Baxter, Abbass, and Essam (2003) \u25ee Bosman and de Jong (2004) \u25ee Shan, McKay, Baxter, Abbass, Essam, and Hoai (2004) \u25ee Hasegawa and Iba (2007)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 104/120\nModel-Based Genetic Programming\n\u25ee Intermediate approach: n-grams \u25ee Focus probabilities on most important relationships (local, e.g. with parents and grandparents) \u25ee Enumerate all possible relationships beforehand \u25ee Learning: estimate probabilities for the n-grams \u25ee Sampling: recursively employ the n-grams \u25ee Advantage: learning is much faster than with grammar transformations \u25ee Hemberg, Veeramachaneni, McDermott, Berzan, and O\u2019Reilly\n(2012)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 105/120\nModel-Based Genetic Programming\n\u25ee Impossible to cover everything in this tutorial, see literature \u25ee Kim, Shan, Nguyen, and McKay (2014)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 106/120\nModel-Based Genetic Programming\n\u25ee Hybrid approach: \u25ee Use GOMEA on template \u25ee Learn new candidate functions using entropy of wrong cases \u25ee Best-ever performance by non-specific EA on even-parity problem\n\u25ee Virgolin, Alderliesten, Witteveen, and Bosman (2017)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 107/120\nMulti-objective Model-Based Evolutionary Algorithms \u25ee Multiple objectives should be optimized simultaneously \u25ee Conflicting objectives, no expression of weights \u25ee Can\u2019t combine the objectives in a single scalar objective \u25ee Want to present\na set of promising alternatives to a decision maker\n\u25ee Example: Maximize the quality and minimize the production costs of a product \u25ee NOTE: This is NOT an MO tutorial\nQ u al it y\nCosts\nNon-dominated (front)\nDominated\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 108/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee Algorithm attempts to obtain improvements all along the current Pareto front \u25ee Different regions along Pareto front may be very different \u25ee E.g. what are far ends of the optimal Pareto front? Optimal solutions for individual objectives fi \u25ee Restrict variation to clusters (restricted mating) \u25ee For instance: obtain clusters along Pareto front: cluster selected solutions \u25ee Bosman and Thierens (2002) \u25ee Pelikan, Sastry, and Goldberg (2009)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 109/120\nMulti-objective Model-Based Evolutionary Algorithms\nPopulation Selection\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 110/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee In EDAs, this clustering corresponds to use of mixture probability distributions\nP(\u03c2,\u03b8)(Z) = k\u22121 \u2211\ni=0\n\u03b2iP(\u03c2 i ,\u03b8i )(Z)\n\u25ee Cluster solutions in objective space (e.g. k-means) \u25ee Estimate a simpler distribution P(\u03c2 i ,\u03b8i )(Z) in each cluster \u25ee Set all mixing coefficients to \u03b2i = 1 k \u25ee Parallel, specialized exploration along front\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 111/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee Each distribution explores own region \u25ee Learning may however by incremental (CMA-ES, iAMaLGaM, iBOA, etc) \u25ee Assign each distribution own adaptive incremental mechanisms \u25ee Cannot combine directly with clustering each generation \u25ee Need correspondence over generations \u25ee Number of clusters fixed beforehand (k)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 112/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee Implicit cluster registration \u25ee Keep clusters spatially separated during run. \u25ee Assign new solution to its nearest, non-full cluster \u25ee Can over time lead to inefficient cluster movement\n0\n0.6\n1.2\n1.8\n2.4\n3\n3.6\n4.2\n4.8\n5.4\n6\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1 2\n3\n4\n5\n1\n2\n3\n4\n51\n2\n3\n4\n5\nf0\nf 1f 1f 1\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 113/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee Explicit cluster registration \u25ee Minimize sum of cluster distance over all permutations of clusters in subsequent generations \u25ee Bosman (2010)\n0\n0.6\n1.2\n1.8\n2.4\n3\n3.6\n4.2\n4.8\n5.4\n6\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1 2\n3\n4 5\n6 7\n8 9 10\n1 2 3\n4 5 6 7 8\n9 10 123 4 5 6 7 8\n9 10\nf0\nf 1f 1f 1\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 114/120\nMulti-objective Model-Based Evolutionary Algorithms\n\u25ee Other model-based MOEAs or MOEA concepts: \u25ee MOEA/D: simultaneously evolve different directions\n(Tchebycheff) \u25ee Can be combined with model-based EAs\n\u25ee e.g. CMA-ES, see: Wang, Liaw, and Ting (2016)\n\u25ee Copula-based EDAs \u25ee Mart\u0301\u0131, de Mello Jr., Sanchez-Pi, and Vellasco (2016)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 115/120\nConclusions\n\u25ee \u201cBlind\u201d metaheuristics are limited in their capability to detect and mix/exploit/re-use structural features of an optimization problem (e.g. partial solutions, building blocks, promising search directions, etc). \u25ee One requires luck or analyzing and designing ways of structure exploitation directly into problem representation and search operators. \u25ee Having a configurable model can help \u201covercome\u201d this / help to do this automatically. \u25ee Algorithm then must learn to configure the model and thereby exploit structure online during optimization. \u25ee Having an explicitly tunable model can really help\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 116/120\nConclusions\n\u25ee We don\u2019t have the optimal model. . . \u25ee Approximate the optimal model \u25ee Match inductive search bias and problem structure \u25ee How to learn and perform variation efficiently and effectively \u25ee Trade-offs: \u25ee Quality versus complexity of approximation \u25ee Efficiency in # evaluations versus time \u25ee Essential model questions: \u25ee Can key problem structure be represented? \u25ee Can key problem structure be represented efficiently? \u25ee Can the model be learned from data? \u25ee Can the model be learned (and used for variation) efficiently?\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 117/120\nConclusions\n\u25ee Efficient model-based evolutionary algorithms (EDAs/IDEAs/PMBGAs/OMEAs) exist \u25ee Binary/Integer/Permutation/Real-valued/GP & multi-objective \u25ee Research is ongoing \u25ee Especially useful when optimizing from a black-box perspective (e.g. complex simulations) \u25ee Also useful from a white-box perspective \u25ee Can learn more about the problem through learnt models \u25ee Models configurable by hand (remove \u201cexpensive\u201d learning\noverhead)\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 118/120\nConclusions\n\u25ee Books \u25ee Larran\u0303aga and Lozano (eds) (2001). Estimation of Distribution\nAlgorithms: A New Tool for Evolutionary Computation. Kluwer.\n\u25ee Lozano, Larran\u0303aga, Inza, Bengoetxea (2006). Towards a New Evolutionary Computation: Advances on Estimation of Distribution Algorithms, Springer. \u25ee Pelikan, Sastry, Cantu\u0301-Paz (eds) (2006). Scalable Optimization via Probabilistic Modeling: From Algorithms to Applications, Springer.\nDirk Thierens & Peter A.N. Bosman. GECCO 2022 Tutorial - Model-Based Evolutionary Algorithms. 119/120\nAcknowledgements\n\u25ee Selected images were re-used from the 2012 GECCO tutorial \u201cProbabilistic Model-building Genetic Algorithms\u201d by Martin Pelikan."
        }
    ],
    "title": "Model-Based Evolutionary Algorithms",
    "year": 2010
}