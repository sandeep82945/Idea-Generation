{
    "abstractText": "We generalize the derivation of model predictive path integral control (MPPI) to allow for a single joint distribution across controls in the control sequence. This reformation allows for the implementation of adaptive importance sampling (AIS) algorithms into the original importance sampling step while still maintaining the benefits of MPPI such as working with arbitrary system dynamics and cost functions. The benefit of optimizing the proposal distribution by integrating AIS at each control step is demonstrated in simulated environments including controlling multiple cars around a track. The new algorithm is more sample efficient than MPPI, achieving better performance with fewer samples. This performance disparity grows as the dimension of the action space increases. Results from simulations suggest the new algorithm can be used as an anytime algorithm, increasing the value of control at each iteration versus relying on a large set of samples. Repository\u2014https://github.com/sisl/MPOPIS",
    "authors": [
        {
            "affiliations": [],
            "name": "Dylan M. Asmar"
        },
        {
            "affiliations": [],
            "name": "Ransalu Senanayake"
        },
        {
            "affiliations": [],
            "name": "Shawn Manuel"
        },
        {
            "affiliations": [],
            "name": "Mykel J. Kochenderfer"
        }
    ],
    "id": "SP:a19c030975ac83327102f6b784c4f4f8f118d53a",
    "references": [
        {
            "authors": [
                "J. Ji",
                "A. Khajepour",
                "W.W. Melek",
                "Y. Huang"
            ],
            "title": "Path planning and tracking for vehicle collision avoidance based on model predictive control with multiconstraints",
            "venue": "IEEE Transactions on Vehicular Technology, vol. 66, no. 2, pp. 952\u2013964, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Samadikhoshkho",
                "S. Ghorbani",
                "F. Janabi-Sharifi",
                "K. Zareinia"
            ],
            "title": "Nonlinear control of aerial manipulation systems",
            "venue": "Aerospace Science and Technology, vol. 104, p. 105 945, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Foehn",
                "D. Brescianini",
                "E. Kaufmann",
                "T. Cieslewski",
                "M. Gehrig",
                "M. Muglikar",
                "D. Scaramuzza"
            ],
            "title": "Alphapilot: Autonomous drone racing",
            "venue": "Autonomous Robots, pp. 1\u201314, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Schwenzer",
                "M. Ay",
                "T. Bergs",
                "D. Abel"
            ],
            "title": "Review on model predictive control: An engineering perspective",
            "venue": "International Journal of Advanced Manufacturing Technology, vol. 117, no. 5, pp. 1327\u2013 1349, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Kouzoupis",
                "G. Frison",
                "A. Zanelli",
                "M. Diehl"
            ],
            "title": "Recent advances in quadratic programming algorithms for nonlinear model predictive control",
            "venue": "Vietnam Journal of Mathematics, vol. 46, no. 4, pp. 863\u2013 882, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.O. Williams",
                "I.G. Kevrekidis",
                "C.W. Rowley"
            ],
            "title": "A data\u2013driven approximation of the Koopman operator: Extending dynamic mode decomposition",
            "venue": "Journal of Nonlinear Science, vol. 25, no. 6, pp. 1307\u20131346, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Korda",
                "I. Mezi\u0107"
            ],
            "title": "Linear predictors for nonlinear dynamical systems: Koopman operator meets model predictive control",
            "venue": "Automatica, vol. 93, pp. 149\u2013160, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C.A. Johnson",
                "E. Yeung"
            ],
            "title": "A class of logistic functions for approximating state-inclusive Koopman operators",
            "venue": "American Control Conference (ACC), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E.A. Theodorou",
                "J. Buchli",
                "S. Schaal"
            ],
            "title": "A generalized path integral control approach to reinforcement learning",
            "venue": "Journal of Machine Learning Research, vol. 11, no. 104, pp. 3137\u20133181, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "G. Williams",
                "N. Wagener",
                "B. Goldfain",
                "P. Drews",
                "J.M. Rehg",
                "B. Boots",
                "E.A. Theodorou"
            ],
            "title": "Information theoretic MPC for modelbased reinforcement learning",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E.A. Theodorou",
                "E. Todorov"
            ],
            "title": "Relative entropy and free energy dualities: Connections to path integral and KL control",
            "venue": "IEEE Conference on Decision and Control (CDC), 2012.",
            "year": 2012
        },
        {
            "authors": [
                "G.I. Boutselis",
                "Z. Wang",
                "E.A. Theodorou"
            ],
            "title": "Constrained sampling-based trajectory optimization using stochastic approximation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Williams",
                "P. Drews",
                "B. Goldfain",
                "J.M. Rehg",
                "E.A. Theodorou"
            ],
            "title": "Information-theoretic model predictive control: Theory and applications to autonomous driving",
            "venue": "IEEE Transactions on Robotics, vol. 34, no. 6, pp. 1603\u20131622, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Williams",
                "B. Goldfain",
                "P. Drews",
                "K. Saigol",
                "J.M. Rehg",
                "E.A. Theodorou"
            ],
            "title": "Robust sampling based model predictive control with sparse objective information",
            "venue": "Robotics: Science and Systems, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.S. Gandhi",
                "B. Vlahov",
                "J. Gibson",
                "G. Williams",
                "E.A. Theodorou"
            ],
            "title": "Robust model predictive path integral control: Analysis and performance guarantees",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 1423\u20131430, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P.-T.D. Boer",
                "D.P. Kroese",
                "S. Mannor",
                "R.Y. Rubinstein"
            ],
            "title": "A tutorial on the cross-entropy method",
            "venue": "Annals of Operations Research, vol. 134, pp. 19\u201367, 1 2005.",
            "year": 2005
        },
        {
            "authors": [
                "G. Williams"
            ],
            "title": "Model predictive path integral control: Theoretical foundations and applications to autonomous driving",
            "venue": "Ph.D. dissertation, Georgia Institute of Technology, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Kusumoto",
                "L. Palmieri",
                "M. Spies",
                "A. Csiszar",
                "K.O. Arras"
            ],
            "title": "Informed information theoretic model predictive control",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Okada",
                "T. Taniguchi"
            ],
            "title": "Acceleration of gradient-based path integral method for efficient optimal and inverse optimal control",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Lambert",
                "F. Ramos",
                "B. Boots",
                "D. Fox",
                "A. Fishman"
            ],
            "title": "Stein variational model predictive control",
            "venue": "Conference on Robot Learning, ser. Proceedings of Machine Learning Research, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Pravitra",
                "E.A. Theodorou",
                "E.N. Johnson"
            ],
            "title": "Flying complex maneuvers with model predictive path integral control",
            "venue": "AIAA SciTech Forum, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "O. So",
                "J. Gibson",
                "B. Vlahov",
                "M.S. Gandhi",
                "G.-H. Liu",
                "E.A. Theodorou"
            ],
            "title": "Variational inference MPC using Tsallis divergence",
            "venue": "Robotics: Science and Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.F. Bugallo",
                "V. Elvira",
                "L. Martino",
                "D. Luengo",
                "J. Miguez",
                "P.M. Djuric"
            ],
            "title": "Adaptive importance sampling: The past, the present, and the future",
            "venue": "IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 60\u201379, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "O. Capp\u00e9",
                "A. Guillin",
                "J.-M. Marin",
                "C.P. Robert"
            ],
            "title": "Population Monte Carlo",
            "venue": "Journal of Computational and Graphical Statistics, vol. 13, no. 4, pp. 907\u2013929, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "O. Capp\u00e9",
                "R. Douc",
                "A. Guillin",
                "J.-M. Marin",
                "C.P. Robert"
            ],
            "title": "Adaptive importance sampling in general mixture classes",
            "venue": "Stat and Comput, vol. 18, no. 4, pp. 447\u2013459, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "R.Y. Rubinstein",
                "D.P. Kroese"
            ],
            "title": "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning",
            "year": 2004
        },
        {
            "authors": [
                "Y. El-Laham",
                "V. Elvira",
                "M.F. Bugallo"
            ],
            "title": "Robust covariance adaptation in adaptive importance sampling",
            "venue": "IEEE Signal Processing Letters, vol. 25, no. 7, pp. 1049\u20131053, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A.W. Moore"
            ],
            "title": "Efficient memory-based learning for robot control",
            "venue": "Ph.D. dissertation, University of Cambridge, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "J.K. Subosits",
                "J.C. Gerdes"
            ],
            "title": "Impacts of model fidelity on trajectory optimization for autonomous vehicles in extreme maneuvers",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 6, no. 3, pp. 546\u2013558, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Brown",
                "J.C. Gerdes"
            ],
            "title": "Coordinating tire forces to avoid obstacles using nonlinear model predictive control",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 5, no. 1, pp. 21\u201331, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "MuJoCo: A physics engine for model-based control",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J. Weng",
                "M. Lin",
                "S. Huang",
                "B. Liu",
                "D. Makoviichuk",
                "V. Makoviychuk",
                "Z. Liu",
                "Y. Song",
                "T. Luo",
                "Y. Jiang",
                "Z. Xu",
                "S. Yan"
            ],
            "title": "EnvPool: A highly parallel reinforcement learning environment execution engine",
            "venue": "arXiv preprint arXiv:2206.10558, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Repository\u2014https://github.com/sisl/MPOPIS\nI. INTRODUCTION\nMany modern control applications benefit from modeling nonlinear dynamics of the system. Examples include vehicle collision avoidance [1], aerial maneuvers [2], and autonomous racing [3]. Model predictive control (MPC) has become a popular method to accommodate the nonlinearity of the dynamics [4]. Some MPC formulations use a quadratic objective function and linear constraints encoding an approximation of the dynamics. Other approaches involve solving the nonlinear program associated with the dynamics by using variants of sequential quadratic programming and interior point methods to approximate solutions [5]. Another technique that has emerged is the use of the Koopman operator [6]. Korda and Mezic\u0301 [7] show that combining this approach with a linear MPC controller reduces the computational complexity of the underlying optimization problem to one that is comparable to a linear dynamical system of similar state and action space sizes. However, efficiently developing finite-dimensional representations of the linear dynamics is still an area of active research [8].\nAn alternative MPC-based approach is to use Monte Carlo sampling as part of the optimization of the trajectory. Model predictive path integral control (MPPI) is one such approach, which has been motivated from the perspective of the Hamilton-Jacobi-Bellman equation [9], information theory [10], [11], and stochastic optimization [12]. MPPI can work with arbitrary system dynamics and cost functions, and it uses importance sampling to estimate the optimal control input. Williams et al. [13] demonstrated MPPI to work in real time with a neural network representing the system dynamics. The main idea behind MPPI is to sample a set\n1 Stanford Intelligent Systems Laboratory (SISL), Stanford University <asmar, ransalu, sman64, mykel>@stanford.edu\nof control sequences, compute the cost of each sequence, and then calculate a cost informed weighted average of the sampled controls. The first control in the sequence is executed and the remainder of the weighted average is used as the mean of the proposal distributions at the next time step.\nThe insight that the performance of MPPI is tightly coupled with sampling from a proposal distribution defined from the previous control sequence has led to research in different areas. One focus has been on increasing the robustness when a disturbance changes the state of the system. Such a disturbance often results in samples from the proposal distribution, which is based on the previous control sequence, to not accurately estimate the the optimal control. Williams et al. [14] augment MPPI with an ancillary controller which tracks the nominal trajectory forming Tube-MPPI. Gandhi et al. [15] further extend this approach and provide bounds on the free energy growth while rejecting unknown disturbances. This methodology still relies on developing a nominal control input from sampling separate distributions at each time step around the previous control sequence.\nThe formulation of MPPI structured the importance sampling to be from separate distributions at each control step. This formulation limits the form of the proposal distribution. In execution, we are limited to a finite number of samples and the quality of the estimate using importance sampling is influenced by the reference distribution [16]. This limitation often requires large sample sizes to approximate the optimal control and manifests in experiments when the dimension of the action space increases [17].\nKusumoto et al. [18] extend MPPI by adding an informed sampling process. They use conditional variational autoencoders to learn distributions that imitate samples from a training data set containing optimized controls. Using these learned distributions, they adjust the mean of the sampling distribution before executing the MPPI process. Okada and Taniguchi [19] use optimization methods originally developed for gradient descent to adjust the control input at each time step, increasing the convergence to the optimal control. Lambert et al. [20] introduce a Bayesian MPC approach that allows for multi-modal distributions and approximate the posterior distribution using Stein variational gradient descent. Pravitra et al. [21] implement an iteration step within the MPPI framework, but only adjust the mean of the sampling distributions from the weighted average of the previous MPPI iteration. Wang et al. [22] use the Tsallis divergence to form a general sampling based algorithm and use a similar iteration procedure to refine the distribution within the control update step. However, the sampling and ar X\niv :2\n20 3.\n16 63\n3v 3\n[ ee\nss .S\nY ]\n1 M\nar 2\n02 3\noptimization are over separate distributions at each control step.\nThese works either adjust the control output after the sampling, adjust only the mean of the proposal distribution, or update individual distributions corresponding to the control at each time step. Our work extends these ideas by providing a method to further refine the proposal distribution by learning a more suitable mean and covariance after obtaining feedback from an original set of samples. To accomplish this change, we generalize the derivation of MPPI to allow for a single joint distribution across controls in the control sequence. With this formulation, we add an adaptive importance sampling (AIS) procedure at each time step. The other approaches discussed can be implemented in addition with our proposed reformulation and benefit from the introduction of an AIS step within the MPPI process. Adapting the proposal distribution across control inputs results in an increase in performance with fewer samples, albeit with a reduction in parallel capability. Our work allows for the MPPI algorithm to be implemented with smaller sample sizes and as an anytime algorithm, increasing performance as computation time allows."
        },
        {
            "heading": "II. MODEL PREDICTIVE OPTIMIZED PATH INTEGRAL CONTROL",
            "text": "In this section, we generalize MPPI to treat the controls across a time horizon as a single input. We then discuss modifications outlined in other work with MPPI, present the model predictive optimized path integral (MPOPI) control algorithm, and then comment on the computational cost."
        },
        {
            "heading": "A. Generalized Problem Formulation and Derivation",
            "text": "In MPPI, the control sequence is treated as a set of distinct controls at time t with the inputs to the system as vt \u223c N (ut,\u03a3) where ut is the commanded control input at time t and \u03a3 is the covariance of the Gaussian noise associated with the input. Therefore, when sampling from the control distribution and estimating the optimal control, we are sampling from separate distributions corresponding to each time step. However, the weights derived from importance sampling are dependent on the cost of the whole control sequence. Using separate distributions at each time step presents a problem if we want to improve our estimation of the expectation by integrating techniques that use the calculated trajectory costs and the control sequence as a single input. Our solution is to treat the sequence of controls as one control input and construct a combined covariance matrix and sample from one, larger joint distribution. This formulation allows different optimization techniques to be applied between time steps to obtain a better estimate of the optimal control through adaptive importance sampling.\nMirroring the derivation of Williams et al. [10], we consider the same dynamical system, xt+1 = F(xt, vt), where xt \u2208 Rn is the state of the system at time t, vt \u2208 Rm is the input to the system, and F is the state-transition function of the system. The input, vt, is a random vector generated by a white-noise process with density function\nvt \u223c N (ut,\u03a3t) where ut \u2208 Rm is the commanded control input at time t and \u03a3t is the covariance of the Gaussian noise associated with input ut. We will define the vector U to be composed of the components of each control input U = [u0,u1, . . . ,uT\u22121] ,U \u2208 RmT and define the combined covariance matrix as\n\u03a3 =  \u03a30 0 . . . 0 0 \u03a31 . . . 0 ... ... . . .\n... 0 0 . . . \u03a3T\u22121  ,\u03a3 \u2208 RmT\u00d7mT . (1) Therefore, the input, V, is a random vector with density function V \u223c N (U,\u03a3) and we can express the analytical form of the density function q corresponding to the distribution QU,\u03a3 as\nq(V | U,\u03a3) = ((2\u03c0)mT |\u03a3|)\u22121/2\nexp ( \u22121\n2 (V\u2212 U)\u1d40\u03a3\u22121(V\u2212 U)\n) . (2)\nSimilar to Williams et al. [10], we define our cost function as one that can be decomposed into a state dependent cost c(X) and a quadratic control cost L(X,U) = c(X)+ \u03bb2 (U\n\u1d40\u03a3\u22121U+ \u03b2\u1d40\nU\u0303 U) + cU\u0303 where X = [x0,F(x0, v0),F(F(x0, v0), v1), . . . ], X \u2208 RnT , vt \u2208 Rm are the components of V \u2208 RmT , and \u03b2U\u0303 and cU\u0303 are constants. We can now write our optimal control problem as\nU\u2217 = argmin U\u2208U EQU,\u03a3 [\u03c6(X) + L(X,U)] (3)\nwhere U is the set of all valid control vectors and \u03c6(X) is a terminal cost function operating on the terminal state within X. Based on our definition of X, the state cost of a control sequence is S(V) = c(X) +\u03c6(X) and the free-energy of the control system is then\nF(S, p,X, \u03bb) = \u2212\u03bb log ( EP [ exp ( \u2212 1 \u03bb S(V) )]) (4)\n= \u2212\u03bb log ( EQU,\u03a3 [ exp ( \u2212 1 \u03bb S(V) ) p(V) q(V | U,\u03a3) ]) (5)\nwhere \u03bb is the inverse temperature and P is the distribution over a set of control vectors with probability density, p. From the concavity of the logarithm function, we can apply Jensen\u2019s inequality and then use the definition of the KL divergence\nF(S, p,X, \u03bb) \u2264 EQU,\u03a3 [S(V)] + \u03bbDKL(QU,\u03a3 || P). (6)\nThis inequality relates the free energy to the state cost of the control problem and the divergence between distribution QU,\u03a3 and P.\nSuppose the base distribution is p(V) = q(V | U\u0303,\u03a3) where U\u0303 is a nominal input applied to the system. Defining, \u03b2\u1d40\nU\u0303 = \u22122U\u0303\u1d40\u03a3\u22121 and cU\u0303 = U\u0303\u03a3 \u22121U\u0303, we have\nDKL(QU,\u03a3 ||QU\u0303,\u03a3) = EQU,\u03a3 [ log ( q(U | \u03a3) q(U\u0303 | \u03a3) )] (7)\n= EQU,\u03a3\n[ 1\n2\n( (U\u2212 U\u0303)\u1d40\u03a3\u22121(U\u2212 U\u0303) )] (8)\n= EQU,\u03a3\n[ 1\n2\n( U\u1d40\u03a3\u22121U + \u03b2\u1d40\nU\u0303 U + cU\u0303\n)] . (9)\nUsing eq. (9) and the state cost, we can rewrite eq. (6) as F(S, p,X, \u03bb)\u2264 EQU,\u03a3 [ \u03c6(X) + c(X)\n+ \u03bb\n2\n( U\u1d40\u03a3\u22121U + \u03b2\u1d40\nU\u0303 U + cU\u0303\n)] (10)\n\u2264 EQU,\u03a3 [\u03c6(X) + L(X,U)] (11)\nand we have established that the free energy provides a lower bound on our reformulated optimal control problem. Furthermore, this inequality becomes an equality if we consider the optimal control distribution Q\u2217 defined by its density function q\u2217(V) = 1\u03b7 exp ( \u2212 1\u03bbS(V) ) p(V). We can now minimize the divergence between our control distribution and the optimal distribution and achieve an optimal control trajectory instead of minimizing eq. (3) directly.\nThe processes of minimizing the KL-divergence between our control distribution and the optimal distribution also follows closely to the original derivation [10] and results in the quadratic minimization problem\nU\u2217 = argmin U\u2208U\n( EQ\u2217 [ (V\u2212 U)\u1d40\u03a3\u22121(V\u2212 U) ]) (12)\nwith the optimal solution in the unconstrained case of\nU\u2217 = EQ\u2217 [V] . (13)\nPrevious work has addressed incorporating control constraints as part of the dynamical model, therefore we only consider the unconstrained solution and discuss modifications in section II-B.\nUsing importance sampling to determine the expectation in eq. (13) also follows the original derivation closely. Let QU\u0302,\u03a3 represent a tentative sampling distribution to perform importance sampling from, then we get our control through\nU\u2217 = EQU\u0302,\u03a3 [w(V)V] (14)\nw(V) = 1 \u03b7 exp ( \u2212 1 \u03bb ( S(V) + \u03bb(U\u0302\u2212 U\u0303)\u1d40\u03a3\u22121V )) (15)\n\u03b7 = \u222b exp ( \u2212 1 \u03bb ( S(V) + \u03bb(U\u0302\u2212 U\u0303)\u1d40\u03a3\u22121V )) dV. (16)\nNo assumptions were changed and all of the theoretical results from MPPI still hold such as eq. (14) being a global optimal solution (from the information-theoretic perspective) under the assumption we can evaluate the expectation perfectly. With no other modification, this approach is equivalent to traditional MPPI. When constructing \u03a3 in eq. (1) we only allowed for control input noise in order to mirror MPPI. In the derivation, we made no assumptions on the structure of \u03a3 except that it is a valid covariance matrix and \u03a3 \u2208 RmT\u00d7mT ."
        },
        {
            "heading": "B. Other Modifications",
            "text": "Information-theoretic model predictive control does present some practical issues such as numerical stability with the trajectory costs. Previous work established methods to address such problems and we extend those concepts to our algorithm. Notice in eq. (15) that \u03bb directly affects the control cost. That is, as \u03bb increases and distributes the weight among trajectories, it also increases the contribution of the control cost. We can decouple the influence of \u03bb by\nredefining the base distribution as p\u0303(V) = p(V | \u03b1U\u0302,\u03a3) where 0 \u2264 \u03b1 \u2264 1. The new base distribution changes our weight to\nw(V) = 1 \u03b7 exp ( \u2212 1 \u03bb ( S(V) + \u03b3(U\u0302\u2212 U\u0303)\u1d40\u03a3\u22121V )) (17)\nwhere \u03b3 = \u03bb(1\u2212 \u03b1). With this change, as \u03b1 moves from 0 to 1, the control cost is less of a factor in the overall weight. Additionally, calculating the weight in eq. (17) can result in numerical instability. To avoid those issues a normalizing factor is introduced to set the lowest cost to zero.\nThe control sequence produced by the sampling methodology can produce inputs that change rapidly across time steps. There are a few ways to address this case. One option is to incorporate a smoothing process into the dynamics model. Since there are no restrictions on the model dynamics in the formulation, we can incorporate different input modifications as part of the dynamics. This idea follows the same concept on how to integrate control constraints allowing us to only consider the unconstrained solution in eq. (13). We can modify our inputs by introducing a function g that applies the appropriate smoothing and control constraints to our inputs before applying the dynamics and our system becomes xt+1 = F(xt, g(vt))."
        },
        {
            "heading": "C. Adaptive Importance Sampling",
            "text": "Using importance sampling to estimate eq. (14) is a key aspect of MPPI. However, since we are limited to a finite number of samples, the quality of the estimate is influenced by the reference distribution [16]. One approach to improve our estimate is to use AIS to adapt our proposal distribution iteratively. There are many AIS algorithms with different performance costs and benefits as surveyed by Bugallo et al. [23]. Control problems present various challenges and the choice of which AIS algorithm to implement will vary across scenarios. With the generalized formulation, we are not restricted to the choice of an AIS algorithm as U and \u03a3 can be updated to achieve a better proposal distribution including correlation terms across control steps. Separating the AIS step from the MPPI algorithm also allows different parameter settings for the control problem and the AIS algorithm (e.g. different \u03bb values to allow exploration during AIS but more selective when calculating the final control). This work focuses on the integration of different AIS algorithms into MPPI and leaves the investigation of specific trade-offs of different algorithms for future research."
        },
        {
            "heading": "D. Algorithm",
            "text": "Algorithm 1 outlines the MPOPI algorithm. This algorithm is executed at each time step until the task is complete. Bold capital letters (e.g. U,\u03a3,X) represent the composite vectors/matrices as described in section II-A and bold lowercase letters with subscripts (e.g. ut, xt) represent the components of the composite vectors. The call to PerformAIS represents a call to an AIS algorithm to execute at each iteration. One of the key differences from MPPI to note is the calculation of the control cost component on line 9 and the weighted\naverage to update U (line 16). We have to account for the appropriate change in control amount as we change our proposal distribution. If we set L = 1, then U\u2032\u2212U vanishes and MPOPI is equivalent to MPPI.\nAlgorithm 1: Model Predictive Optimized Path Integral Control\nGiven: F, g,K, T U \u2208 RmT \u03a3 \u2208 RmT\u00d7mT \u03c6, c, \u03bb, \u03b1 /* Cost function/parameters */ L /* Max number of AIS iterations */ \u03c81, \u03c82, . . . , \u03c8p /* AIS parameters */\n1 x0 \u2190 GetStateEstimate() 2 U\u2032 \u2190 U 3 \u03a3\u2032 \u2190 \u03a3 4 for `\u2190 1 to L do 5 for k \u2190 1 to K do 6 Sample Ek from N (0,\u03a3\u2032) 7 for t\u2190 1 to T do 8 xt \u2190 F(xt\u22121, g(u\u2032t\u22121 + \u03b5kt\u22121)) 9 sk \u2190 c(X) +\u03c6(X) +\u03bb(1\u2212\u03b1)U\u2032\u1d40\u03a3\u22121 (Ek + U\u2032 \u2212 U)\n10 if ` < L then 11 U\u2032,\u03a3\u2032 \u2190 PerformAIS(U\u2032,\u03a3\u2032, S, \u03c81, \u03c82, . . . , \u03c8p, `) 12 \u03c1\u2190 min(S) 13 \u03b7 \u2190 \u2211K k=1 exp ( \u2212 1 \u03bb (sk \u2212 \u03c1) ) 14 for k \u2190 1 to K do 15 wk \u2190 1\u03b7 exp ( \u2212 1 \u03bb (sk \u2212 \u03c1) ) 16 U += wk (Ek + U\u2032 \u2212 U) 17 SendToActuators(u0) 18 for t\u2190 1 to T \u2212 1 do 19 ut\u22121 \u2190 ut 20 uT\u22121 \u2190 Initialize(uT\u22121)\nFigure 1 provides a graphical depiction of how MPOPI can improve samples to better estimate the optimal control. The trajectories were generated from throttle and steering commands for controlling a car. Figure 1a depicts the trajectories of the controls sampled from the initial proposal distribution. After each iteration, the proposal distribution is updated and new samples are generated. Figure 1b and fig. 1c show how the sampled controls evolve, resulting in higher scoring trajectories after each iteration of the AIS algorithm."
        },
        {
            "heading": "E. Computational Cost",
            "text": "A key enabler for MPPI to work real time is its ability to be implemented in parallel. The computational complexity of MPPI can be summarized as the complexity of obtaining KT samples and the complexity of propagating the system dynamics T times for K samples in parallel. The addition of AIS changes this complexity. The sampling and propagation of the system dynamics must occur L times, along with the addition of the complexity of the AIS algorithm L \u2212 1 times. Modern sampling and AIS algorithms are efficient. The largest computational burden comes from propagating the dynamics in L sequential steps. Using AIS will allow the use of fewer samples and still take advantage of parallel operations during the propagation step, but at the burden of reestablishing the parallel computations L times.\nThe MPOPI algorithm is more sample efficient and the iterative nature lends itself to an anytime approach where L can increase with computation time available. However,\nthese benefits come at the cost of a reduction in parallel capability. For example, given M = KL samples, MPPI would propagate the system dynamics in parallel for all M samples. However, MPOPI requires propagating K samples in parallel in L sequential steps where the execution of the AIS algorithm must occur after each propagation of K samples. The complexity of establishing parallel computations is hardware, problem, and implementation specific. As problem complexity increases, MPOPI provides an alternative approach to adding more samples."
        },
        {
            "heading": "III. EXPERIMENTS",
            "text": "We tested the MPOPI algorithm in different virtual environments and compared it to MPPI with the same modifications discussed in section II-B. MPPI has been shown across the literature to perform equitably or outperform numerous other control algorithms and was chosen as the baseline [13], [18], [21]. We integrated five different AIS algorithms with MPOPI. The algorithms have the ability to use multiple proposal distributions, but we chose to use one proposal distribution to keep the comparison closer to MPPI. We implemented a Population Monte Carlo (PMC) algorithm [24], a mean only moment matching AIS algorithm (\u00b5-AIS), a mean and covariance moment matching AIS algorithm (\u00b5\u03a3AIS) similar to Mixture-PMC [25], a Cross-Entropy method (CE) [26], and a Covariance Matrix Adaptation Evolutionary Strategy (CMA) [27]. The \u00b5-AIS algorithm is similar to the iterative approach introduced by Pravitra et al. [21], but we allowed for different values of \u03bb for updating the mean and the final weight calculation of the samples. This separation resulted in better performance versus using a shared \u03bb. We kept parameters constant across similar algorithms to show a more direct comparison. Reference the repository for parameters used and details of the implementation.\nWe first looked at the continuous version of the MountainCar environment [28] and then performed simulations with a non-linear car model. To increase the action space size and complexity, we modified the car environment to control multiple cars cooperatively and conducted experiments on MuJoCo gym environments. In this section, the term effective samples refers to the total samples an algorithm had available. For MPPI, it is the number of samples and for MPOPI, it is the number of samples multiplied by the number of AIS iterations, KL. We also discuss rewards when describing objective functions instead of costs. The effective sample sizes were increased by changing the number of samples or AIS iterations with MPOPI or the total number of samples for MPPI. The smallest sample size in the MountainCar and car racing scenarios restricted MPOPI to one AIS iteration to show the equivalence to MPPI and provide a more direct comparison. All simulations used the same parameters for MPPI and MPOPI (e.g. \u03bb, \u03b1, T )."
        },
        {
            "heading": "A. MountainCar Environment",
            "text": "The goal of the MountainCar problem is to get an underpowered car up a hill to a goal location with the dynamics and parameters as specified by Kochenderfer et al. [29]. The\naction space consisted of a single continuous action at each time step. The reward function for this problem was modified to be R(xt, vt) = \u22121 + |vt| + 100000R\u2217 where R\u2217 is an indicator variable for reaching the goal location with vt > 0. The environment terminated when the car reached the goal location or at 200 steps. The simulations consisted of 1000 trials for effective samples ranging from 20 to 180 with MPOPI using 20 samples and varying the number of AIS iterations. The results are shown in fig. 2. The algorithms converge to a similar number of steps while also reaching a point where increasing the number of samples provided little return. MPPI was able to reach similar performance but failed to outperform versions of MPOPI except for PMC. The CE and CMA versions of MPOPI were able to reach the plateau with as few as 40 and 60 effective samples respectively. The PMC algorithm uses multinomial resampling and struggles with a small initial sample size."
        },
        {
            "heading": "B. Car Racing",
            "text": "This scenario simulated a form of car racing and was constructed to be similar to the virtual racing environment used by Williams [17]. The total length of the track was\napproximately 1.18 km with multiple curves. We implemented a nonlinear single-track model that captures transient dynamics in corning with a variant of a Fiala brush tire model [30]. The model was a representation of the electric X1, an autonomous experimental car. For details of the model dynamics and parameters of the X1 see Subosits and Gerdes [30] and Brown and Gerdes [31]. The action space for a single car consisted of two continuous inputs per time step: a steering command and a throttle/brake command. The controls were issued at 10 Hz with the dynamics operating at 100 Hz. The reward function for each car was R(xt) = 2|vt| \u2212 |d| \u2212 5000R\u2217\u03b2 \u2212 1000000R\u2217t where vt is the velocity of the car at time step t, d is the distance of the car from the middle of the track lane, R\u2217\u03b2 is an indicator variable for exceeding a certain drift amount \u03b2, and R\u2217t is an indicator variable for when the car leaves the track boundary (\u03b2 is the angle between the velocity vector and the longitudinal axis; the limit was set to 45\u00b0). All runs consisted of 25 trials and each trial concluded when the car completed 2 laps, exceeded the \u03b2 limit for more than 5 sec, or was outside the track boundary for greater than 1 sec. The number of effective samples varied from 375 to 2250 with MPOPI using iterations of 150 or 375 samples depending on the number of cars and the AIS algorithm. More details of this environment and videos can be seen in the repository.\nResults of controlling a single car are shown in fig. 3a. There were no violations of the track boundary or the \u03b2 limit. All versions of MPOPI except for \u00b5-AIS outperformed MPPI and achieved MPPI peak performance with as few as 375 samples and 3 iterations of the AIS algorithm. There was more of a disparity in performance with fewer effective samples. The single car scenario resulted in the car attaining speeds of 39 m/s with \u03b2 values exceeding 34\u00b0.\nWe modified the car racing environment to cooperatively control multiple cars. For Nc cars, the reward function was\nR(xt) = Nc\u2211 i=1 Ri \u2212 11000R\u2217c \u2212 Nc\u22121\u2211 i=1 Nc\u2211 j=i+1 |di,j | (18)\nwhere Ri is the individual reward for car i, R\u2217c is an indicator for the cars being within 4 m of each other, and di,j is the distance between car i and car j. The last term was added to increase complexity by emphasizing coordinated actions across the action space.\nResults for two and three cars are shown in fig. 3b and fig. 3c. There were no violations for the two and three car scenarios. The success rate of a trial in the 4-car scenario with no violations is shown in table I. For a 5-car scenario, MPPI was only able to complete 3 out of 25 trials with 6000 samples. The CE and CMA versions of MPOPI were able to control up to six cars successfully with as few as 1500 effective samples.\nFor a given effective sample size, the CE and CMA methods benefited from using fewer samples with more iterations as the number of cars increased. However, as the sample size decreased, the method to approximate the covariance matrix in the CE version of MPOPI became more important. Algorithms specializing in estimating large covariance matrices with few samples resulted in higher performance. Details of the different covariance matrix estimation algorithms tested are provided in the repository. Changing the sample size and iteration count on the single and two car scenarios did not have a noteworthy change in performance."
        },
        {
            "heading": "C. MuJoCo Gym Environments",
            "text": "To further test MPOPI, we conducted experiments on MuJoCo gym environments [32], [33] using EnvPool [34]. No environmental parameters were changed from the documented implementations. We ran each scenario 10 times and the experiments were conducted for 250 steps. We conducted experiments using only the CE version of MPOPI based on previous results. The average total accumulated reward is shown for two tasks in table II. Algorithm parameters were\nnot tuned for performance on the different scenarios and were kept consistent across MPPI and MPOPI. Reference the repository wiki page for more details on the parameters of the algorithms used.\nHalfCheetah-v4 and Ant-v4 have six and eight dimensional action spaces respectively. These environments were chosen to test MPOPI on different scenarios with complex control spaces. Similar to previous results, MPOPI achieved similar performance as MPPI with fewer effective samples. The MuJoCo gym and multi-car results demonstrate the significance the proposal distribution can have on the quality of the importance sampling estimate of the optimal control. MPOPI increases in performance with each iteration of the AIS algorithm and also increases in performance with a larger initial sample size. These results suggest MPOPI can be used in complex problems as an anytime algorithm where the sample size and iteration count can be adjusted as computation time allows."
        },
        {
            "heading": "250 1554\u00b1 224 2154\u00b1 223 1444\u00b1 92 1711\u00b1 161",
            "text": ""
        },
        {
            "heading": "500 1973\u00b1 162 2771\u00b1 183 1635\u00b1 147 1839\u00b1 93",
            "text": ""
        },
        {
            "heading": "1000 2071\u00b1 103 3358\u00b1 328 1814\u00b1 100 2197\u00b1 89",
            "text": ""
        },
        {
            "heading": "1500 2192\u00b1 197 3798\u00b1 140 1946\u00b1 131 2251\u00b1 77",
            "text": ""
        },
        {
            "heading": "3000 2440\u00b1 108 4737\u00b1 315 2180\u00b1 95 2417\u00b1 114",
            "text": ""
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "We increased the sample efficiency of MPPI by implementing adaptive importance sampling into the original importance sampling step of MPPI. The benefit of learning a better proposal distribution from previous samples was demonstrated in simulated environments and became further evident as the dimension of the action space increased. Simulation results suggest MPOPI can be used as an anytime algorithm, increasing the value of control at each iteration versus relying on a single large set of samples. The modification to adjust the proposal distribution to increase the efficiency and quality of the samples used is a promising change to an already high performing control algorithm."
        }
    ],
    "title": "Model Predictive Optimized Path Integral Strategies",
    "year": 2023
}