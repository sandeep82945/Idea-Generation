1. **Investigate the impact of different similarity metrics on the performance of the proposed method.** The paper uses the negative L2 distance as the similarity metric, but other metrics, such as cosine similarity or Jaccard similarity, could also be used. It would be interesting to see how the choice of similarity metric affects the performance of the method.


2. **Explore the use of external domain data for k-NN search.** The paper only explores leveraging the training data for k-NN search, but various external domain data are also suitable for k-nearest neighbor retrieval. It would be interesting to see how the use of external domain data can further improve the performance of the method.


3. **Develop more sophisticated methods for calibrating the training process using k-NN.** The paper uses a simple method for calibrating the training process using k-NN, but more sophisticated methods could be developed. For example, one could use a reinforcement learning approach to learn the optimal calibration parameters.


4. **Investigate the use of k-NN for other NLP tasks.** The paper only evaluates the proposed method on a few NLP tasks, but it could be applied to other tasks as well. It would be interesting to see how the method performs on tasks such as machine translation, summarization, and question answering.


5. **Develop a theoretical framework for understanding the benefits of integrating k-NN with PLMs.** The paper provides some empirical evidence of the benefits of integrating k-NN with PLMs, but a theoretical framework for understanding these benefits would be valuable. Such a framework could help to guide the development of new methods for integrating k-NN with PLMs.