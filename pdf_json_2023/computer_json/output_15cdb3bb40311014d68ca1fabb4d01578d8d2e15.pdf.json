{
    "abstractText": "This paper introduces the first two pixel retrieval benchmarks. Pixel retrieval is segmented instance retrieval. Like semantic segmentation extends classification to the pixel level, pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object. In addition to retrieving images for the given query, it helps users quickly identify the query object in true positive images and exclude false positive images by denoting the correlated pixels. Our user study results show pixel-level annotation can significantly improve the user experience. Compared with semantic and instance segmentation, pixel retrieval requires a fine-grained recognition capability for variable-granularity targets. To this end, we propose pixel retrieval benchmarks named PROxford and PRParis, which are based on the widely used image retrieval datasets, ROxford and RParis. Three professional annotators label 5,942 images with two rounds of doublechecking and refinement. Furthermore, we conduct extensive experiments and analysis on the SOTA methods in image search, image matching, detection, segmentation, and dense matching using our pixel retrieval benchmarks. Results show that the pixel retrieval task is challenging to these approaches and distinctive from existing problems, suggesting that further research can advance the contentbased pixel-retrieval and thus user search experience. The datasets can be downloaded from this link.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guoyuan An"
        },
        {
            "affiliations": [],
            "name": "Woo Jae Kim"
        },
        {
            "affiliations": [],
            "name": "Saelyne Yang"
        },
        {
            "affiliations": [],
            "name": "Rong Li"
        },
        {
            "affiliations": [],
            "name": "Yuchi Huo"
        },
        {
            "affiliations": [],
            "name": "Sung-Eui Yoon"
        }
    ],
    "id": "SP:a44b6374e11c3328cb87ba529598ccfdd5c72451",
    "references": [
        {
            "authors": [
                "Guoyuan An",
                "Yuchi Huo",
                "Sung-Eui Yoon"
            ],
            "title": "Hypergraph propagation and community selection for objects retrieval",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Relja Arandjelovi\u0107",
                "Andrew Zisserman"
            ],
            "title": "Three things everyone should know to improve object retrieval",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Herbert Bay",
                "Tinne Tuytelaars",
                "Luc Van Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "In European conference on computer vision,",
            "year": 2006
        },
        {
            "authors": [
                "Daniel Bolya",
                "Chong Zhou",
                "Fanyi Xiao",
                "Yong Jae Lee"
            ],
            "title": "Yolact: Real-time instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Eva Breznik",
                "Elisabeth Wetzer",
                "Joakim Lindblad",
                "Nata\u0161a Sladoje"
            ],
            "title": "Cross-modality sub-image retrieval using contrastive multimodal image representations",
            "venue": "arXiv preprint arXiv:2201.03597,",
            "year": 2022
        },
        {
            "authors": [
                "Bingyi Cao",
                "Andr\u00e9 Araujo",
                "Jack Sim"
            ],
            "title": "Unifying deep local and global features for image search",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision,",
            "year": 2010
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Herve Jegou",
                "Matthijs Douze",
                "Cordelia Schmid"
            ],
            "title": "Hamming embedding and weak geometric consistency for large scale image search",
            "venue": "In European conference on computer vision,",
            "year": 2008
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Cordelia Schmid",
                "Patrick P\u00e9rez"
            ],
            "title": "Aggregating local descriptors into a compact image representation",
            "venue": "IEEE computer society conference on computer vision and pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Yuhe Jin",
                "Dmytro Mishkin",
                "Anastasiia Mishchuk",
                "Jiri Matas",
                "Pascal Fua",
                "Kwang Moo Yi",
                "Eduard Trulls"
            ],
            "title": "Image Matching across Wide Baselines: From Paper to Practice",
            "venue": "International Journal of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Christoph H Lampert"
            ],
            "title": "Detecting objects in large image collections and videos by efficient subimage retrieval",
            "venue": "In 2009 IEEE 12th International Conference on Computer Vision,",
            "year": 2009
        },
        {
            "authors": [
                "Seongwon Lee",
                "Hongje Seong",
                "Suhyeon Lee",
                "Euntai Kim"
            ],
            "title": "Correlation verification for image retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengqi Li",
                "Noah Snavely"
            ],
            "title": "Megadepth: Learning singleview depth prediction from internet photos",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Zhe Lin",
                "Jonathan Brandt"
            ],
            "title": "A local bag-of-features model for large-scale object retrieval",
            "venue": "In European conference on Computer vision,",
            "year": 2010
        },
        {
            "authors": [
                "Shu Liu",
                "Lu Qi",
                "Haifang Qin",
                "Jianping Shi",
                "Jiaya Jia"
            ],
            "title": "Path aggregation network for instance segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Wei Liu",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Christian Szegedy",
                "Scott Reed",
                "Cheng-Yang Fu",
                "Alexander C Berg"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scaleinvariant keypoints",
            "venue": "International journal of computer vision,",
            "year": 2004
        },
        {
            "authors": [
                "Neville Mehta",
                "Alomari Raja\u2019S",
                "Vipin Chaudhary"
            ],
            "title": "Content based sub-image retrieval system for high resolution pathology images using salient interest points",
            "venue": "In 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,",
            "year": 2009
        },
        {
            "authors": [
                "Juhong Min",
                "Dahyun Kang",
                "Minsu Cho"
            ],
            "title": "Hypercorrelation squeeze for few-shot segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Matthias Minderer",
                "Alexey Gritsenko",
                "Austin Stone",
                "Maxim Neumann",
                "Dirk Weissenborn",
                "Alexey Dosovitskiy",
                "Aravindh Mahendran",
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Zhuoran Shen"
            ],
            "title": "Simple open-vocabulary object detection with vision transformers",
            "venue": "arXiv preprint arXiv:2205.06230,",
            "year": 2022
        },
        {
            "authors": [
                "David Nister",
                "Henrik Stewenius"
            ],
            "title": "Scalable recognition with a vocabulary tree",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Hyeonwoo Noh",
                "Andre Araujo",
                "Jack Sim",
                "Tobias Weyand",
                "Bohyung Han"
            ],
            "title": "Large-scale image retrieval with attentive deep local features",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017",
            "year": 2017
        },
        {
            "authors": [
                "Anton Osokin",
                "Denis Sumin",
                "Vasily Lomakin"
            ],
            "title": "OS2D: One-stage one-shot object detection by matching anchor features",
            "venue": "In proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "James Philbin",
                "Ondrej Chum",
                "Michael Isard",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "Object retrieval with large vocabularies and fast spatial matching",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2007
        },
        {
            "authors": [
                "James Philbin",
                "Ondrej Chum",
                "Michael Isard",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "Lost in quantization: Improving particular object retrieval in large scale image databases",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2008
        },
        {
            "authors": [
                "Jean Ponce",
                "Tamara L Berg",
                "Mark Everingham",
                "David A Forsyth",
                "Martial Hebert",
                "Svetlana Lazebnik",
                "Marcin Marszalek",
                "Cordelia Schmid",
                "Bryan C Russell",
                "Antonio Torralba"
            ],
            "title": "Dataset issues in object recognition",
            "venue": "Toward categorylevel object recognition,",
            "year": 2006
        },
        {
            "authors": [
                "Filip Radenovi\u0107",
                "Ahmet Iscen",
                "Giorgos Tolias",
                "Yannis Avrithis",
                "Ond\u0159ej Chum"
            ],
            "title": "Revisiting oxford and paris: Large-scale image retrieval benchmarking",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Filip Radenovi\u0107",
                "Giorgos Tolias",
                "Ond\u0159ej Chum"
            ],
            "title": "Finetuning cnn image retrieval with no human annotation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ali S Razavian",
                "Josephine Sullivan",
                "Stefan Carlsson",
                "Atsuto Maki"
            ],
            "title": "Visual instance retrieval with deep convolutional networks",
            "venue": "ITE Transactions on Media Technology and Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "arXiv preprint arXiv:1506.01497,",
            "year": 2015
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas Schops",
                "Johannes L Schonberger",
                "Silvano Galliani",
                "Torsten Sattler",
                "Konrad Schindler",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "A multi-view stereo benchmark with highresolution images and multi-camera videos",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xi Shen",
                "Fran\u00e7ois Darmon",
                "Alexei A Efros",
                "Mathieu Aubry"
            ],
            "title": "Ransac-flow: generic two-stage image alignment",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaohui Shen",
                "Zhe Lin",
                "Jonathan Brandt",
                "Ying Wu"
            ],
            "title": "Spatially-constrained similarity measurefor large-scale object retrieval",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Xiaoyong Shen",
                "Xin Tao",
                "Chao Zhou",
                "Hongyun Gao",
                "Jiaya Jia"
            ],
            "title": "Regional foremost matching for internet scene images",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2016
        },
        {
            "authors": [
                "Yujiao Shi",
                "Hongdong Li"
            ],
            "title": "Beyond cross-view image retrieval: Highly accurate vehicle localization using satellite image",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yael Shrager",
                "Jeffrey J Gold",
                "Ramona O Hopkins",
                "Larry R Squire"
            ],
            "title": "Intact visual perception in memory-impaired patients with medial temporal lobe lesions",
            "venue": "Journal of Neuroscience,",
            "year": 2006
        },
        {
            "authors": [
                "Ran Tao",
                "Efstratios Gavves",
                "Cees GM Snoek",
                "Arnold WM Smeulders"
            ],
            "title": "Locality in generic instance search from one example",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Marvin Teichmann",
                "Andre Araujo",
                "Menglong Zhu",
                "Jack Sim"
            ],
            "title": "Detect-to-retrieve: Efficient regional aggregation for image search",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Giorgos Tolias",
                "Yannis Avrithis",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "To aggregate or not to aggregate: Selective match kernels for image search",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2013
        },
        {
            "authors": [
                "Giorgos Tolias",
                "Yannis Avrithis",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Image search with selective match kernels: aggregation across single and multiple images",
            "venue": "International Journal of Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Giorgos Tolias",
                "Ronan Sicre",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Particular object retrieval with integral max-pooling of cnn activations",
            "venue": "arXiv preprint arXiv:1511.05879,",
            "year": 2015
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Luc V Gool",
                "Radu Timofte"
            ],
            "title": "Gocor: Bringing globally optimized correspondence volumes into your neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Radu Timofte"
            ],
            "title": "Glunet: Global-local universal network for dense flow and correspondences",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Pdc-net+: Enhanced probabilistic dense correspondence network",
            "venue": "arXiv preprint arXiv:2109.13912,",
            "year": 2021
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Fisher Yu",
                "Luc Van Gool"
            ],
            "title": "Warp consistency for unsupervised learning of dense correspondences",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jasper RR Uijlings",
                "Koen EA Van De Sande",
                "Theo Gevers",
                "Arnold WM Smeulders"
            ],
            "title": "Selective search for object recognition",
            "venue": "International journal of computer vision,",
            "year": 2013
        },
        {
            "authors": [
                "Tianyi Wei",
                "Dongdong Chen",
                "Wenbo Zhou",
                "Jing Liao",
                "Hanqing Zhao",
                "Weiming Zhang",
                "Nenghai Yu"
            ],
            "title": "Improved image matting via real-time user clicks and uncertainty estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Weyand",
                "Andre Araujo",
                "Bingyi Cao",
                "Jack Sim"
            ],
            "title": "Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ning Xu",
                "Brian Price",
                "Scott Cohen",
                "Thomas Huang"
            ],
            "title": "Deep image matting",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Lin Yang",
                "Xin Qi",
                "Fuyong Xing",
                "Tahsin Kurc",
                "Joel Saltz",
                "David J Foran"
            ],
            "title": "Parallel content-based sub-image retrieval using hierarchical searching",
            "venue": "Bioinformatics, 30(7):996\u20131002,",
            "year": 2014
        },
        {
            "authors": [
                "Lihe Yang",
                "Wei Zhuo",
                "Lei Qi",
                "Yinghuan Shi",
                "Yang Gao"
            ],
            "title": "Mining latent classes for few-shot segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Xiao Wang",
                "Basil Mustafa",
                "Andreas Steiner",
                "Daniel Keysers",
                "Alexander Kolesnikov",
                "Lucas Beyer"
            ],
            "title": "Lit: Zero-shot transfer with locked-image text tuning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yunke Zhang",
                "Lixue Gong",
                "Lubin Fan",
                "Peiran Ren",
                "Qixing Huang",
                "Hujun Bao",
                "Weiwei Xu"
            ],
            "title": "A late fusion cnn for digital matting",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhongyan Zhang",
                "Lei Wang",
                "Yang Wang",
                "Luping Zhou",
                "Jianjia Zhang",
                "Fang Chen"
            ],
            "title": "Dataset-driven unsupervised object discovery for region-based instance image retrieval",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Sijie Zhu",
                "Taojiannan Yang",
                "Chen Chen"
            ],
            "title": "Vigor: Crossview image geo-localization beyond one-to-one retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Image retrieval is a long-standing and fundamental computer vision task and has achieved remarkable advances. However, because the retrieved ranking list contains false positive images and the true positive images contain complex co-occurring backgrounds, users may be difficult to identify the query object from the ranking list. In this paper,\nwe execute a user study and show that providing pixel-level annotations can help users better understand the retrieved results. Therefore, this paper introduces the pixel retrieval task and its first benchmarks. Pixel retrieval is defined as searching pixels that depict the query object from the database. More specifically, it requires the machine to recognize, localize, and segment the query object in database images in run time, as shown in Figure 1.\nSimilar to semantic segmentation, which works as an extension of classification and provides pixel-level category information to the machines, pixel retrieval is an extension of image retrieval. However, pixel retrieval differs from existing semantic segmentation [11, 62, 21] in two aspects: the fine-grained particular instance recognition and\nar X\niv :2\n30 9.\n05 43\n8v 1\n[ cs\n.C V\n] 1\n1 Se\np 20\nthe variable-granularity recognition. On the one hand, pixel retrieval asks the machine to consider the fine-grained information to segment the same instance with the query, e.g., segment the particular query building in the street figures that contain many similar buildings. This is different from existing semantic segmentation [11] and instance segmentation [21, 62]. Semantic segmentation only requires the category level information, e.g., to segment all the buildings in the street figures. On top of semantic segmentation, instance segmentation additionally requires demarcating individual instances, e.g., segmenting all the buildings and giving the boundary of each building separately. However, instance segmentation does not distinguish the differences among the buildings [62, 21, 4].\nOn the other hand, pixel retrieval requires adjusting the recognition granularity as needed. The query image can be the whole building or only a part of the building. The search engine should understand the intention of the query and adjust the segmentation granularity in demand. This differs from existing segmentation benchmarks [62, 7, 19, 8, 10], where the recognition granularity is fixed in advance. Therefore, the pixel retrieval task is supplementary to semantic and instance segmentation by considering the recognition and segmentation featured with fine-grained and variable-granularity properties, which are also fundamental visual abilities of humans.\nIn order to promote the study of pixel retrieval, we create the pixel retrieval benchmarks Pixel-Revisited-Oxford (PROxford) and Pixel-Revisited-Paris (PRParis) on top of the famous image retrieval benchmarks Revisited-Oxford (ROxford) and Revisited-Paris (RParis) [30, 31, 33]. There are three reasons to use ROxford and RParis as our base benchmarks. Firstly, they are notoriously difficult and can better reflect the search engines\u2019 performance. Secondly, each query in these datasets has up to hundreds of positive images, so they are suitable for evaluating the fine-grained recognition ability. Thirdly, every positive image is guaranteed to be identifiable by people without considering any contextual visual information [33].\nWe provide the segmentation labels to a total of 5,942 images in ROxford and RParis. To ensure the label quality, three professional annotators independently label the queryindex pairs and then refine and check the labels. The annotators are aged between 26 to 32 years old and have worked full-time on annotation for over two years. We then design new metrics, mAP@50:5:95, and mAP, to evaluate the pixel retrieval performance (Section 2).\nWe provide an extensive comparison of State-Of-TheArt (SOTA) methods in related fields, including image search, detection, segmentation, and dense matching with our benchmarks. We have some interesting findings from the experiment. For example, we find the SOTA spatial verification methods [6, 28] give a high inlier number to some\ntrue query-index pairs but match the wrong regions. We find the dense and pixel-level approaches [25, 52] helpful for the pixel retrieval task. Most importantly, our results show that pixel retrieval is difficult and further research is needed for advancing the user experience on the content-based search task.\nOur contributions are as follows:\n\u2022 We introduced the pixel retrieval task and provided the first two landmark pixel retrieval benchmarks, PROxford and PRParis. Three professional annotators labeled, refined, and checked the labels.\n\u2022 We executed the user study and showed that the pixel level annotation could significantly improve user experience.\n\u2022 We performed extensive experiments with SOTA methods in image search, detection, segmentation, and dense matching. Our experiment results can be used as the baselines for future study."
        },
        {
            "heading": "2. Content-based pixel retrieval",
            "text": ""
        },
        {
            "heading": "2.1. Why Revisited Oxford and Paris?",
            "text": "We design the first content-based pixel retrieval benchmarks, PROxford and PRParis, directly on top of the famous image retrieval benchmarks Revisited-Oxford (ROxford) and Revisited-Paris (RParis) [30, 31, 33]. Oxford [30] and Paris [31] are introduced by Philbin et al. in 2007 and 2008, respectively. Their images are obtained from Flickr by searching text tags for famous landmarks in Oxford University and Paris. Radenovic et al. [33] refined the annotations and updated more difficult queries for them in 2018; the refined datasets are called ROxford and RParis.\nWe choose ROxford and RParis because they are among the most popular image retrieval benchmarks. Many wellknown image retrieval methods are evaluated on them, from the traditional methods like RootSIFT [2], VLAD [13], and ASMK [48], to the recent deep learning based methods like R-MAC [48], GeM [34], and DELF [28].\nThese datasets are the ideal data sources for our pixelretrieval, thanks to several properties. Firstly, compared to other famous datasets like image matching Phototourism [14] and dense matching Megadepth [18], the positive image pairs in ROxford and RParis have severe viewpoint changes, occlusions, and illumination changes. The new queries added by Radenovic et al. [33] have cropped regions that cause extreme zooms with the positive database images. These properties make the ROxford and RParis notoriously difficult. Secondly, each query image contains up to hundreds of positive database images, while other datasets, such as UKBench [27] and Holiday [12], only have 4 to 5 positive images for each query. A large amount of\nchallenging positive images are suitable for evaluating finegrained recognition ability.\nThe Google Landmark Dataset (GLD) [55] encompasses more landmarks than ROxford and RParis. However, ROxford and RParis outshine GLD in labeling quality. Notably, they stand as distinct benchmarks for contrasting machine and human recognition prowess.\nIt is known that people cannot easily recognize an object if it changes its pose significantly [32], but we do not know where the limit is. ROxford and RParis are the only existing datasets that can reflect the human ability to identify objects in the landmark domain to the best of our knowledge. Every positive image in ROxford and RParis is checked by five annotators independently based on the image appearance, and all the unclear cases are excluded [33]. This kind of annotation has two benefits. Firstly, although these benchmarks are difficult, the positive images are guaranteed to be identifiable by people without considering any contextual visual information [33]. This shows the possibility of enabling the machine to recognize these positive images by only analyzing the visual clue in the given query-index image pair. Secondly, these datasets can be used to compare human and machine recognition performance; human-level recognition performance should identify all the positive images. Although the classification performance (the top 5 accuracy) of machines on ImageNet has surpassed that of humans [37], the SOTA identification ability about the first-seen objects in ROxford and RParis is still far from human-level [17, 1, 6]."
        },
        {
            "heading": "2.2. From image retrieval to pixel retrieval",
            "text": "In a similar spirit that semantic segmentation works as an extension of classification and provides pixel-level category information to the machines, pixel retrieval is an extension of image retrieval. It offers information about which pixels or regions are related to the query object. This task is very helpful when only a small region of the positive image corresponds to the query. Such situations frequently happen in many image retrieval applications, such as web search [33, 16, 20], medical image analysis [24, 5, 57], geographical information systems [61, 63, 42], and so on. We discuss the related applications in Section 3. Distinguishing and segmenting the first-seen objects is also one basic function of human vision system [43]; it is meaningful to understand and automate this ability.\nSome previous works also noticed the importance of localizing the query object in the searched image. They have tried to combine image search and object localization [16, 20, 40]. However, due to the lack of a challenging pixel retrieval benchmark, they show only the qualitative result instead of the quantitative performance. Pixel-level labeling and quality assurance are arduous. In this work, 5,942 images are labeled, refined, and checked by three pro-\nfessional annotators. We hope this benchmark can boost and encourage future research on pixel-level retrieval.\nWe also compare our pixel-retrieval benchmark with segmentation, image matching, and dense matching benchmarks in the supplementary material."
        },
        {
            "heading": "2.3. Pixel-level annotation",
            "text": "Images to annotate. ROxford and RParis each contains 70 queries. The 70 queries are divided into 26 and 25 query groups in ROxford and RParis, respectively, based on the visual similarity; queries in the same query group share the same ground truth index image list. There are total 1,985 and 3,957 images to annotate for our PROxford and PRParis, respectively. Mask annotation. Figure 2 shows our labeling process. Researchers with a computer vision background first annotate the target object in each query image. Each annotator for our new benchmark observes all the queries with masks in a query group and labels the segmentation mask for the images in the ground-truth list. Annotators are asked to identify the query object in the labeling image first and then label all the pixels depicting the target object. We show the query masks and the labeling instruction details in the supplementary materials. Objectivity. To ensure the pixel retrieval task and our benchmark are objectively defined, we adopt two approaches. Firstly, we use query masks to distinctly identify the target objects and segregate them from the background (e.g., the sky), occlusions (e.g., other buildings), and the remaining part of the same building if the object is only a small part of it. These masks guide the removal of background and indicate the query boundary. Secondly, by examining the query with masks, our annotators reach a consensus on the target object and its boundary, thereby avoiding disagreement about our query intention. This consensus-based approach is a common method for reducing subjectivity in recognition tasks; it is also employed in the original ROxford and RParis benchmarks, where voting is used to determine the final ground truth for each query [33].\nWe retain small-sized occlusion objects, like windows\nand fences, during annotation. While this may involve subjective judgments regarding what qualifies as a small-sized occlusion, it is worth noting that well-known semantic segmentation datasets like VOC [8] and COCO [19] also involve subjective elements, such as identifying objects on a table as a table or the background behind the bike wheel as a bike. Such subjectivities are inevitable, given the difficulty of removing them. Nonetheless, they do not diminish the usefulness of benchmarks as reliable metrics for evaluating state-of-the-art methods. We include in the supplementary materials our mask rules, all the queries with masks, and our consensus checking. Quality assurance. To improve the annotation quality, every query-index image pair labeling is performed by three professional annotators following the three steps: 1) annotate; 2) refine + inspect; 3) refine + insp, as shown in Figure 2. The three annotators are aged between 26 to 32 years old and have worked on annotation full-time for over 2 years. Their works have been qualified in many annotation projects."
        },
        {
            "heading": "2.4. Evaluation metrics",
            "text": "Pixel retrieval from the database. Pixel retrieval aims to search all the pixels depicting the query object from the large scale database images. An ideal pixel retrieval algorithm should achieve the image ranking, reranking, localization, and segmentation simultaneously. To the best of our knowledge, there is no existing pixel retrieval metric yet. Detection and segmentation tasks usually use mIoU and mAP@50:5:95 as the standard measurement [36]. Image retrieval methods commonly use mAP as the metric [33]. We combine them to evaluate the ranking, localization, and segmentation performance in pixel retrieval. Each groundtruth image in the ranking list is treated as a true-positive (TP), only if its detection or segmentation Intersection over Union (IoU) is larger than a threshold n. The other process of calculating AP and mAP follows the traditional image search mAP. Note that the mAP calculation methods in image search and traditional segmentation [8] are different; image search focuses more on ranking. Similar to detection and segmentation fields, the threshold n is set from 0.5 to 0.95, with step 0.05. The average of scores under these thresholds are the final metric mAP@50:5:95. It is desirable to report both detection and segmentation mAP@50:5:95 for the methods that can generate pixel-level results; high segmentation performance does not necessarily lead to high localization performance, as shown in Sec 5. We follow the medium and hard protocols in ROxford and RParis [33] with and without 1 M distractors. Pixel retrieval from ground-truth query-index image pairs. We can use existing ranking/reranking methods and treat the remaining process as one-shot detection/segmentation. In this case, the detection or segmen-\ntation performance is evaluated using the mean of mIoU of all the queries, where mIoU is the mean of the IoUs for all the ground-truth index images. We do not consider the false pairs because the ranking metric mAP well reflects the influence of false pairs in the ranking list."
        },
        {
            "heading": "3. Applications of pixel retrieval",
            "text": "Pixel retrieval requires the machine to recognize, localize, and segment a particular first-seen object, which is one of the fundamental abilities of the human visual system. It is useful for many applications. In this section, we first show that it can significantly improve the user experience in web search. We then discuss how pixel retrieval can help imagelevel ranking techniques. Finally, we introduce some other applications that may also benefit from pixel retrieval. Web search user experience improvement. Modern image retrieval techniques focus on improving the image-level ranking performance of hard cases, such as images under extreme lighting conditions, novel views, or complicated occlusions. However, users may not easily perceive a hard case as a true positive, even if it is at the top of the ranking list. We claim that pixel-level annotation can significantly improve the user experience on the web search application.\nTo see how pixel-level annotation improves the user experience on image search, we ran a user study where users were asked to find images that contain a given target among candidate images in two different conditions; the one with pixel-level annotations (i.e., Pixel retrieval) and the other with no annotations (i.e., Image retrieval). We recruited 40 participants on Prolific1 and compared the time taken to complete the task between the two conditions.\nParticipants were asked to complete 16 questions in total, where eight of them were Pixel retrieval and the other eight were Image retrieval. We divided the participants into four groups and counterbalanced the type of questions (Figure 3). For each question, participants were given a query image and 12 candidate images. There were three true positives and nine false positives in the candidate images, and we randomly chose ground truth images of other queries as false positives. We shuffled the order of the candidate images and asked participants to choose three images that contain the query image (i.e., true positives) among them. Figure 1 shows one of the 16 questions. You can check our user study from this link. To start the user study, please enter any character into the \u201cunique Prolific ID\u201d blank. Anonymity is guaranteed.\nOur results show that participants completed the task faster when the pixel-level annotations were presented (mean=37.07s, std=49.76s) than when no annotations were presented (mean=53.71s, std=80.08s). The difference between two conditions is statistically significant (T-\n1prolific.co\ntest, p-value=0.00091), and participants responded that it was helpful to see annotations in completing the task (mean=6.375/7, std=0.89). Other applications. Image retrieval techniques have been applied to many applications, such as medical diagnosis and geographical information systems (GIS). The pixel-level retrieval is also desirable for these applications. For example, the size of medical and geographical images are usually huge, and the doctors and GIS experts are interested in retrieving regions of the particular structures or landmarks from the whole images in the database [57, 5, 24, 63].\nPixel retrieval can also help image matting [54, 56, 60]. Current image matting techniques rely on the user\u2019s click to confirm the target matting region [54, 56, 60]. Our pixel retrieval provides a new interaction method: deciding the target object based on the query example. This query-based interaction can significantly reduce user effort in situations where many images depict the same object [41]."
        },
        {
            "heading": "4. Experiment",
            "text": "We evaluate the performance of state-of-the-art (SOTA) methods in multiple fields on our new pixel retrieval benchmarks. Our new pixel retrieval task is a visual object recognition problem. It requires the search engine to automate the human visual system\u2019s ability to identify, localize, and segment an object under illumination and viewpoint changes. It can be seen as a combination of image retrieval, one-shot detection, and one-shot segmentation. We introduce these related tasks and their SOTA methods in this section, and we implement these SOTA methods and discuss their results in Section 5."
        },
        {
            "heading": "4.1. Localization in retrieval",
            "text": "Some pioneering works [16, 20, 40] in image retrieval emphasized the importance of localization and tried to combine the retrieval and detection methods. However, due to the lack of a standard pixel retrieval benchmark, these pioneering works only showed qualitative results instead of quantitative comparisons. In this paper, we implement and\ncompare the SOTA localization-related retrieval methods on our new benchmark dataset. They can be divided into two categories: spatial verification (SP) and detection.\nSP [40, 23, 2, 28, 6] is one of the most popular reranking approaches in image retrieval. It is also known as image matching [14]; SP and stereo task in Image Matching Challenge (IMC) [14] share the same pipeline and theory except for the final evaluation step. In this work, we selected the local features and matching hyperparameters with the best retrieval performance on ROxford and RParis, which contain more challenging cases than datasets in IMC.\nSP compares the spatial configurations of the visual words in two images. Theoretically, it can achieve verification and localization simultaneously. However, the imagelevel ranking performance cannot fully reflect the SP accuracy or localization performance. In the hard positive cases, e.g., where many repeated patterns exist in the background, even though SP generates a high inlier number and ranks an image on top of the ranking list, the matched visual words can be wrong due to the repeated patterns. Our pixel retrieval benchmark can not only evaluate the localization performance, but also better reflect the SP accuracy and be helpful for future SP studies.\nResearchers mainly focus on generating better local features to improve SP performance. The classical local features have SIFT [23], SUFT [3], and rootSIFT [2]. Recently, DELF [28] and DELG [6] local features, which are learned from the large landmarks training set [55], achieve the SOTA SP result. We evaluate the SP performance with SIFT, DELF, and DELG features on our new benchmark datasets in this paper.\nAnother localization-related image search approach is to directly apply the detection methods [16, 20, 35, 36, 53, 44, 45]. Faster-RCNN [36] and SSD detector [22] fine-tuned on a huge manually boxed landmark dataset [45] achieve the SOTA detect-related retrieval result [45]. Detect-to-retrieve (D2R) [45] uses these fine-tuned models to detect several landmark regions for a database image and uses aggregation methods like the Vector of Locally Aggregated Descriptors (VLAD) [13] and the Aggregated Selective Match Kernel (ASMK) [46] to represent each region. To better check the effect of the aggregation methods, we also implement the Mean aggregation (Mean), which simply represents each region using the mean of its local descriptors. The region with highest similarity can be seen as the target region for a given query. We evaluate the combination of different detectors and aggregation methods on our pixel retrieval benchmarks."
        },
        {
            "heading": "4.2. One-shot detection and segmentation",
            "text": "We can treat pixel retrieval as combining image retrieval and one-shot detection and segmentation. We test the performance of these approaches.\nThe Vision Transformer for Open-World Localization\n(OWL-ViT) [26] is a vision transformer model trained on the large-scale 3.6 billion images in LiT dataset [59]. It has shown the SOTA performance on several tasks including one-shot detection. The One-Stage one-shot Detector (OS2D) combines and refines the traditional descriptor matching and spatial verification pipeline in image search to do the one-shot detection. It achieves impressive detection performance in several domains, e.g., retail products, buildings, and logos. We test these two detection methods on our new benchmarks.\nThe Hypercorrelation Squeeze Network (HSNet) [25] is one of the most famous few-shot segmentation methods. It finds multi-level feature correlations for a new class. The Mining model (Mining) [58] exploits the latent novel classes during the offline training stage to better tackle the new classes in the testing time. The Self-Support Prototype model (SSP) [9] generates the query prototype in testing time and uses the self-support matching to get the final segmentation mask. The self-support matching is based on one of the classical Gestalt principles [15]: pixels of the same object tend to be more similar than those of different objects. It achieves the SOTA few-shot segmentation results on multiple datasets. We evaluate these three methods on our new pixel retrieval benchmarks."
        },
        {
            "heading": "4.3. Dense matching",
            "text": "Different from image matching (SP in this paper), which calculates the transformation between two images of the same object from different views, dense matching focuses on finding dense pixel correspondence. We check if we can use the SOTA dense matching methods to correctly find the correspondence points for pixels in the query image and achieve our pixel retrieval target.\nGLUNet [50] and RANSAC-flow [39] are popular among many famous dense matching methods. Recently, Truong et al. have shown that the warp consistency objective (WarpC) [52] and the GOCor module [49] can further improve the performance and achieve the new SOTA. Another popular method is PDC-Net [51]. It can predict the uncertainty of the matching pixels. The uncertainty can be useful for our pixel retrieval task, which is sensitive to the outliers. We test the origin GLUNet, GLUNet with WarpC (WarpC-GLUNet), GLUNet with GOCor module (GOCorGLUNet), and PDC-Net in Table 1."
        },
        {
            "heading": "4.4. Experiment detail",
            "text": "We try our best to find the best possible result for each method on our novel benchmark. The retrieval localization methods employed in this study, including image matching (SP in this paper) and D2R, were configured to achieve optimal performance on ROxford and RParis. These methods rely on precise localization to enhance image retrieval performance. Thus, we adopt the same experimental configu-\nrations in our similar pixel retrieval benchmark. Similarly, dense matching methods, which encompass geometric and semantic matching tasks, are expected to operate directly on our pixel retrieval benchmark, as per task definitions. We evaluate its geometric models with the best performance on MegaDepth [18] and ETH3D [38], datasets that feature actual building images, rendering them the ideal valid sets for our benchmark. The difference is that our dataset contains more extreme viewpoints and illumination changes. Moreover, we evaluate the performance of semantic models to see if including semantic information can enhance rigid body recognition in our benchmarks. We refrained from fine-tuning the segmentation methods as there is no segmentation training set pertaining to the building domain to the best of our knowledge. Our comprehensive experimental findings can be employed as baseline metrics for future comparisons. We include the detailed experimental configurations for each method in the supplementary materials and intend to make them, along with their codes, publicly available."
        },
        {
            "heading": "5. Results and discussion",
            "text": "We report the results of pixel retrieval from ground-truth image pairs (mean of mIoU) for all the above mentioned methods in Table 1. We choose one to two representative methods for each field and show their qualitative results in Figure 4. To evaluate the performance of pixel retrieval from database, we combine these methods with SOTA image level ranking and reranking methods: DELG and hypergraph propagation (HP) [1]. We show their final mAP@50:5:95 in Table 2.\nAlthough SP achieves impressive image-level retrieval results [6, 28], it shows suboptimal performance on pixel retrieval. We observe some true positive pairs where SP gives a high inlier number but matches the wrong regions. For example, in the first easy case in Figure 4, SP with DELG features generates 19 inliers, but none of the inliers are in the target object region. Note that 19 inlier number is high and only 4 false positive images are ahead of the this easy case in the final DELG reranking list [6]. This is not to say DELG is bad; in fact, its matching results are quite good in most cases. We choose this striking example only to show that the image-level ranking performance is not enough to reflect the SP accuracy. Our pixel retrieval benchmarks can be used to evaluate the matched features\u2019 locations of SP.\nFor SP, both deep-learning features DELF and DELG significantly outperform the SIFT features. Interestingly, although DELG shows better image retrieval performance [6] than DELF, it is slightly inferior to DELF in the pixel retrieval task. One reason might be that though DELG generates more matching inliers for the positive pairs than DELF, these inliers tend to exist in a small region and do not reflect the location or size of the target object. Improving SP per-\nformance in both image and pixel level can be a practical research topic.\nAlthough the detect-2-retrieval [45] is inferior to SP in image retrieval [6, 28, 33], it shows better performance than SP in our pixel-level retrieval benchmarks. We conjecture that the detection models tend to cover the whole building more than SP. Our benchmark is helpful in checking this conjecture and designing a better pixel retrieval model for future works. The results of the region detector and the aggregation method are similar to the trend in image search [45]. The VLAD and ASMK aggregation methods significantly improve the Mean aggregation. A fasterRCNN-based detector shows better performance than SSD.\nFor dense matching methods, GLU-Net using warp consistency or GOCor module and PDC-Net show better results than other models. This trend is similar to that in the dense\nmatching benchmark Megadepth [18]. The segmentation methods significantly outperform other methods in terms of the mean of segmentation mIoU. However, their detection mIoU results are not so impressive. They tend to predict the entire foreground, which contains the target building, as shown in the SSP line of Figure 4. Among the segmentation methods, SSP shows better segmentation than others, showing its self-support approach is helpful for finding more related pixels.\nAnother interesting finding is that better image ranking mAP does not necessarily brings better pixel retrieval mAP@50:5:95, as shown in Table 2. The reason might be that the image search techniques rank some hard cases high, but detection methods do not well localize the query object in them.\nIt is interesting to note that segmentation and dense\nmatching methods have demonstrated superior mIoU results compared to matching-based and detection-based retrieval methods, despite not being originally designed for retrieval tasks. However, to effectively tackle the pixel retrieval task, these methods must work in conjunction with image search techniques. While dense matching and segmentation meth-\nods are better suited for identifying target object areas, they may not achieve fine-grained recognition. In contrast, existing retrieval methods tend to identify certain textures or corners but lack the ability to capture the entire object\u2019s shape. Without a reliable benchmark, retrieval methods may simply associate an object and its context to improve image-\nlevel performance, leading to low localization and segmentation results, as we discussed above. We did our best to prepare our new benchmark so that it can provide a valuable evaluation for novel methods targeting pixel retrieval, which requires fine-grained and variable-granularity detection and segmentation. Moreover, we find pixel retrieval challenging. The current best mAP@50:5:95 in PROxford and PRParis at medium setting without distractors are only 37.3 and 47.0."
        },
        {
            "heading": "6. Future works",
            "text": "We present a novel task termed \u201dPixel Retrieval.\u201d This task mandates segmentation but transitions from a semantic directive to the content-based one, thus bypassing semantic vagueness. Concurrently, it demands large-scale, instance-level recognition\u2014a subject frequently explored by the retrieval community. This innovative task poses several unique challenges, some of which we outline below:"
        },
        {
            "heading": "6.1. Enhancing accuracy",
            "text": "For a superior user experience, it\u2019s vital to embrace methods, workflows, and datasets that bolster accuracy. Our findings illustrate that segmentation and dense matching methods are beneficial, especially when an image ranking list is provided using existing retrieval techniques. Beyond merely superimposing segmentation over retrieval, a compelling approach would be to rank images based on the results of the segmentation. Further insights and experimental outcomes in this regard are available on our website.\nAlthough the introduction of new datasets, even those echoing the landmarks in our benchmarks, is commendable, it\u2019s pivotal to articulate their application to discern the sources of performance enhancements. If PROxford/PRParis and ROxford/RParis are employed as benchmarks, it\u2019s crucial to ensure the consistent usage of the same training set. Given the public accessibility of our ground truth files, it\u2019s imperative to prevent any unintended data leaks during training."
        },
        {
            "heading": "6.2. Scalability and speed",
            "text": "A major challenge lies in scaling the algorithms and augmenting the retrieval speed. Techniques like segmentation and dense matching, which compute for every pair, inherently lag in speed when compared to retrieval methods such as ASMK and D2R. Therefore, swift methods that can cater to extensive scales are highly sought after."
        },
        {
            "heading": "6.3. Innate visual recognition and The significance",
            "text": "of training data\nThe prevalent trend in research is to amass expansive training or fine-tuning sets closely aligned with test instances\u2014certainly a commendable approach. However, intriguingly, humans exhibit an innate ability to discern in-\nstances in query images. Our annotators, despite being unfamiliar with European landmarks, could effortlessly segment target objects in each positive image, even when subjected to extreme lighting and perspective alterations. What fuels this innate recognition? Is it purely due to extensive prior exposure, or are there underlying mechanisms at play? How pivotal is the training dataset in replicating human-like content-based segmentation, especially when semantic influences are excluded? These questions beckon exploration."
        },
        {
            "heading": "7. Conclusion",
            "text": "We introduced the first landmark pixel retrieval benchmark datasets, i.e., PROxford and PRParis, in this paper. To create these benchmarks, three professional annotators labeled, refined, and checked the segmentation masks for a total of 5,942 image pairs. We executed the user study and found that pixel-level annotation can significantly improve the user experience on web search; pixel retrieval is a practical task. We did extensive experiments to evaluate the performance of SOTA methods in multiple fields on our pixel retrieval task, including image search, detection, segmentation, and dense matching. Our experiment results show that pixel retrieval is challenging and further research is needed."
        }
    ],
    "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
    "year": 2023
}