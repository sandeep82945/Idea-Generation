{
    "abstractText": "Peer review is the widely accepted mechanism to determine the quality of scientific work. Even though peer-reviewing has been an integral part of academia since the 1600s, it frequently receives criticism for the lack of transparency and consistency. Even for humans, predicting the peer review outcome is a challenging task as there are many dimensions and human factors involved. However, Artificial Intelligence (AI) techniques can assist the editor/chair anticipate the final decision based on the reviews from the human reviewers. Peer review texts reflect the reviewers\u2019 opinions/ sentiments on various aspects (e.g., novelty, substance, soundness, etc.) of the paper concerning the research in the paper, which may be valuable to predict a manuscript\u2019s acceptance or rejection. The exact types and number of aspects could vary from one to the other venue (i.e., the conferences or journals). Peer review texts, however, which often contain rich sentiment information about the reviewers and, therefore, their overall opinion of the paper\u2019s research, can be useful in predicting a manuscript\u2019s acceptance or rejection. Here in this work, we study how we could take advantage of aspects and their corresponding sentiment to build a generic controllable system to assist the editor/chair in determining the outcome based on the reviews of a paper to make better editorial decisions. Our proposed deep neural architecture considers three information channels, including reviews, review aspect category, and its sentiment, to predict the final decision. Experimental results show that our model can achieve up to 76.67% accuracy on the ASAP-Review dataset (Aspect-enhanced Peer Review) consisting of ICLR and NIPS reviews considering the sentiment of the reviews. Empirical results also show an improvement of around 3.3 points while aspect information is added to the sentiment information1. \u2217Both authors contributed equally to this research. 1We make our code publicly available at https://github.com/sandeep82945/-PEERREVIEW-DECISION-Public.git Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. JCDL \u201922, June 20\u201324, 2022, Cologne, Germany \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9345-4/22/06. . . $15.00 https://doi.org/10.1145/3529372.3530937 CCS CONCEPTS \u2022 Computing methodologies \u2192 Information extraction; \u2022 Information systems\u2192 Information extraction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sandeep Kumar"
        },
        {
            "affiliations": [],
            "name": "Hardik Arora"
        },
        {
            "affiliations": [],
            "name": "Tirthankar Ghosal"
        },
        {
            "affiliations": [],
            "name": "Asif Ekbal"
        }
    ],
    "id": "SP:6e5689a79afee01685e2db55707aee9692ad608e",
    "references": [
        {
            "authors": [
                "Stefanos Angelidis",
                "Mirella Lapata"
            ],
            "title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October",
            "year": 2018
        },
        {
            "authors": [
                "Lutz Bornmann",
                "Hans-Dieter Daniel"
            ],
            "title": "Reliability of reviewers\u2019 ratings when using public peer review: a case study",
            "venue": "Learn. Publ. 23,",
            "year": 2010
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Rajeev Verma",
                "Asif Ekbal",
                "Pushpak Bhattacharyya"
            ],
            "title": "A Sentiment Augmented Deep Architecture to Predict Peer Review Outcomes",
            "venue": "In 19th ACM/IEEE Joint Conference on Digital Libraries, JCDL 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Rajeev Verma",
                "Asif Ekbal",
                "Sriparna Saha",
                "Pushpak Bhattacharyya"
            ],
            "title": "Investigating Impact Features in Editorial Pre-Screening of Research Papers",
            "venue": "In Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries, JCDL 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Clayton J. Hutto",
                "Eric Gilbert"
            ],
            "title": "VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text",
            "venue": "In Proceedings of the Eighth International Conference on Weblogs and Social Media,",
            "year": 2014
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Waleed Ammar",
                "Bhavana Dalvi",
                "Madeleine van Zuylen",
                "Sebastian Kohlmeier",
                "Eduard H. Hovy",
                "Roy Schwartz"
            ],
            "title": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018",
            "year": 2018
        },
        {
            "authors": [
                "Jacalyn Kelly",
                "Tara Sadeghieh",
                "Khosrow Adeli"
            ],
            "title": "Peer Review in Scientific Publications: Benefits, Critiques",
            "venue": "A Survival Guide. EJIFCC",
            "year": 2014
        },
        {
            "authors": [
                "Richard L. Kravitz",
                "Peter Franks",
                "Mitchell D. Feldman",
                "Martha Gerrity",
                "Cindy Byrne",
                "William M. Tierney"
            ],
            "title": "Editorial peer reviewers\u2019 recommendations at a general medical journal: are they reliable and do editors care",
            "venue": "PLOS ONE 5,",
            "year": 2010
        },
        {
            "authors": [
                "Asheesh Kumar",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "title": "ADeepNeural Architecture for Decision-Aware Meta-Review Generation",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL). 222\u2013225",
            "year": 2021
        },
        {
            "authors": [
                "Sandeep Kumar",
                "Tirthankar Ghosal",
                "Prabhat Kumar Bharti",
                "Asif Ekbal"
            ],
            "title": "Sharing is Caring! Joint Multitask Learning Helps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL). 270\u2013273",
            "year": 2021
        },
        {
            "authors": [
                "John Langford",
                "Mark Guzdial"
            ],
            "title": "The arbitrariness of reviews, and advice for school administrators",
            "venue": "Commun. ACM 58,",
            "year": 2015
        },
        {
            "authors": [
                "Bing Liu"
            ],
            "title": "Sentiment Analysis and Opinion Mining",
            "year": 2012
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yuzhuo Wang Minghui Meng",
                "Chengzhi Zhang"
            ],
            "title": "Building Multi-level Aspects of Peer Reviews for Academic Articles",
            "venue": "In 18th INTERNATIONAL CON- FERENCE ON INFORMETRICS",
            "year": 2021
        },
        {
            "authors": [
                "Barbara Plank",
                "Reinard van Dalen"
            ],
            "title": "CiteTracked: A Longitudinal Dataset of Peer Reviews and Citations",
            "venue": "(CEUR Workshop Proceedings,",
            "year": 2019
        },
        {
            "authors": [
                "Tribikram Pradhan",
                "Chaitanya Bhatia",
                "Prashant Kumar",
                "Sukomal Pal"
            ],
            "title": "A deep neural architecture based meta-review generation and final decision prediction of a scholarly",
            "venue": "article. Neurocomputing",
            "year": 2021
        },
        {
            "authors": [
                "Simon Price",
                "Peter A. Flach"
            ],
            "title": "Computational support for academic peer review: a perspective from artificial intelligence",
            "venue": "Commun. ACM 60,",
            "year": 2017
        },
        {
            "authors": [
                "Feng Qiao",
                "Lizhen Xu",
                "Xiaowei Han"
            ],
            "title": "Modularized and Attention- Based Recurrent Convolutional Neural Network for Automatic Academic Paper Aspect Scoring",
            "venue": "In Web Information Systems and Applications - 15th International Conference,WISA",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Nihar B. Shah"
            ],
            "title": "Tutorial on Systematic Challenges and Computational Solutions on Bias and Unfairness in Peer Review. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM \u201921)",
            "venue": "WSDM",
            "year": 2021
        },
        {
            "authors": [
                "Richard Smith"
            ],
            "title": "Peer review: a flawed process at the heart of science and journals",
            "venue": "Journal of the Royal Society of Medicine",
            "year": 2006
        },
        {
            "authors": [
                "Ivan Stelmakh",
                "Nihar B. Shah",
                "Aarti Singh",
                "Hal Daum\u00e9 III"
            ],
            "title": "A Novice- Reviewer Experiment to Address Scarcity of Qualified Reviewers in Large Conferences",
            "venue": "In Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "M Sun"
            ],
            "title": "Peer review comes under peer review",
            "venue": "Science",
            "year": 1989
        },
        {
            "authors": [
                "Cecilia Superchi",
                "Jos\u00e9-Antonio Gonz\u00e1lez",
                "Iv\u00e1n Sol\u00e1",
                "Erik Cobo",
                "Darko Hren",
                "Isabelle Boutron"
            ],
            "title": "Tools used to assess the quality of peer review reports: a methodological systematic review",
            "venue": "BMC Medical Research Methodology",
            "year": 2019
        },
        {
            "authors": [
                "Mike Thelwall",
                "Eleanor-Rose Papas",
                "Zena Nyakoojo",
                "Liz Allen",
                "Verena Weigert"
            ],
            "title": "Automatically detecting open academic review praise and criticism",
            "venue": "Online Inf. Rev. 44,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Tomkins",
                "Min Zhang",
                "William D. Heavlin"
            ],
            "title": "Reviewer bias in single- versus double-blind peer review",
            "venue": "Proc. Natl. Acad. Sci. USA 114,",
            "year": 2017
        },
        {
            "authors": [
                "Ke Wang",
                "Xiaojun Wan"
            ],
            "title": "Sentiment Analysis of Peer Review Texts for Scholarly Papers",
            "venue": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "Jelte M. Wicherts"
            ],
            "title": "Peer Review Quality and Transparency of the Peer- Review Process in Open Access and Subscription Journals",
            "venue": "PLoS ONE",
            "year": 2016
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Pengfei Liu",
                "GrahamNeubig"
            ],
            "title": "CanWe Automate Scientific Reviewing? CoRR abs/2102.00176 (2021)",
            "venue": "https://arxiv.org/abs/",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "\u2217Both authors contributed equally to this research. 1We make our code publicly available at https://github.com/sandeep82945/-PEERREVIEW-DECISION-Public.git\nPublication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. JCDL \u201922, June 20\u201324, 2022, Cologne, Germany \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9345-4/22/06. . . $15.00 https://doi.org/10.1145/3529372.3530937\nCCS CONCEPTS \u2022 Computing methodologies \u2192 Information extraction; \u2022 Information systems\u2192 Information extraction.\nKEYWORDS Peer Reviews, Decision Prediction, Aspect-based Sentiment Analysis, Deep Neural Network\nACM Reference Format: Sandeep Kumar, Hardik Arora, Tirthankar Ghosal, and Asif Ekbal. 2022. DeepASPeer: Towards an Aspect-level Sentiment Controllable Framework for Decision Prediction from Academic Peer Reviews. In The ACM/IEEE Joint Conference on Digital Libraries in 2022 (JCDL \u201922), June 20\u201324, 2022, Cologne, Germany. ACM, New York, NY, USA, 11 pages. https://doi.org/10. 1145/3529372.3530937"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The peer-review process is considered to be the primary means of ensuring the quality of scientific publications. The editors of journals and conferences assign submitted papers to reviewers who critique them. It is well known that peer reviews are controversial due to their quality, randomness, bias, and inconsistencies [2]. Also, there has been concerns about alleged reviewer bias in \"single-blind\" peer reviews [27] and arbitrariness between different reviewer groups [12]. The rise in submissions and shortage of expert reviewers are major reasons why editors or program chairs may sometimes consider assigning papers to novice, out-of-domain reviewers which results in more inconsistencies and poor quality reviews. An organizer of the NIPS 2014 conference studied the arbitrariness inherent in the peer review process by assigning 10% of submissions to two different committees and found that they disagreed for more than a quarter of papers [12]. Researchers at the ICML 2020 conference explored several methods to improve the quality of reviews in large conferences by addressing the scarcity of qualified reviewers. Again, it is pretty common for a paper rejected by one venue to be accepted by another with little or no improvement in quality. Kravitz et al. [9] studied the peer review process at peer-review at the Journal of General Internal Medicine (JGIM). They explained the need to improve the reliability of the peer-review process and for editors to understand the limitations\nof reviewers\u2019 recommendations. They observed that the JGIM editor\u2019s decisions appeared to be significantly influenced by reviewer recommendations. A manuscript\u2019s fate was essentially sealed by consensus between all the reviewers that it should be rejected. The consensus of reviewers that a manuscript merited additional consideration (revision and resubmission or conditional acceptance) decreased the likelihood of rejection from approximately half to one in five (for all manuscripts sent for peer review). They challenge biomedical journal editors to reconsider standard practice: asking reviewers to recommend whether manuscripts should be accepted, revised, or rejected.\nSome people believe that the peer review system is fragile since it is based on the opinions of a few people and to them, this is not an ideal process for validation [22]. Despite all its inherent flaws [23], criticisms [24] and bias [21], peer review is the most widely-accepted admission process for scientific knowledge at this moment. A study into the internal process of the peer review system is quite complex due to concerns over publisher privacy and copyright. However, with OpenReview2, we can see how peer review is evolving in some areas such as the response periods/rebuttals by authors, and an effort to increase communication between authors and reviewers. Additionally, we can observe the introduction of peer review workshops, objective questionnaires and other measures.\nThe rapid increase in research article submissions across different venues3, has caused the peer review system to undergo a huge amount of stress [8]. This problem presents a significant administrative challenge to journal editors and conference program chairs, who are usually burdened with many tasks, like assigning reviewers, ensuring timely receipt of reviews, filling in for nonresponding reviewers, informing decisions, communicating with authors, etc. The major challenge is to decide whether to accept or reject a manuscript based on the reviews provided by the reviewers, simultaneously ensuring their validity. With our ongoing efforts to build an artificial intelligence (AI) peer review system, we are curious to know what might happen if an additional AI reviewer could predict decisions by analyzing the reviewer\u2019s comments on its own? Editors/program chairs usually follow most reviewer recommendations. However, they still have to go over all the review texts for all the submitted papers to address the inconsistencies between reviewer recommendations and comments. The purpose of this study is not to replace human reviewers; instead, we are interested in exploring how AI can provide a supplemental opinion of reviews with respect to various aspects to assist the overall peer-review decision-making process. Such a system might effectively tell the authors which aspects they should focus on, alongside giving the program chair the liberty to configure the decision prediction system according to the needs and interests of that particular conference.\nASAP-Review dataset [30] is a recent dataset of peer-reviews. It is an excellent resource for research and study on this significant and impactful problem, which not only contains the reviewer\u2019s sentiment but also the aspect category associated with it. In this work, we include the following aspect-categories in our investigation: Motivation/Impact (MOT), Originality (ORI), Soundness/Correctness\n2https://openreview.net/ 3AAAI 2021 received 9034 submissions, NeurIPS 2021 received 12115 abstracts!!\n(SOU), Substance (SUB), Replicability (REP), Meaningful Comparison (CMP), and Clarity (CLA) and the corresponding sentiment. We suggest that a decision can be inferred by considering the sentiments of the reviewers expressed with respect to the underlying aspects. Hence the aspects and sentiments play an important role in determining the quality of a paper. We propose a novel deep neural architecture to make use of an aspect infused embedding. We also show that aspect, along with the sentiment, helps in peer review decision prediction. Our model is generic, and can be easily adapted to different aspect categories that may have to be considered depending upon the types of conferences and/or journals. To the best of our knowledge, such a controllable aspect-based sentiment investigation for decision prediction from peer-reviews does not exist."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "2.1 Peer reviews",
            "text": "The peer-review process is complex and takes place in a complex broader research system. Artificial intelligence in peer review is an essential but understudied topic. However, the topic is gaining community attention due to recent advances in artificial intelligence research. The first public dataset of scientific peer reviews was presented in [7] available for research purposes, providing an opportunity to study this important artifact. The authors also examined correlations between the overall recommendation score and the individual aspect scores (e.g., clarity, impact, and originality). They quantified the difference between reviews that recommend an oral presentation and those that recommend a poster presentation. Review texts are predictive of scientific impact. For determining citation impact from peer reviews, the CiteTracked dataset [16] has been created, which has peer reviews and citation information spanning six years and scientific articles from the machine learning community. The investigation found that a model\u2019s performance based on review texts is higher than that using only abstracts or titles, indicating the potential of learning from peer-review texts. With the help of a variety of computational support tools, peer review comments\u2019 quality, tone, and quantity were analyzed [18]. The work in [25] provides a comprehensive overview of criteria tools that the community can utilize to judge the quality of peer review reports in biomedical research. In addition, a tool [29] was developed to assess the transparency of the peer-review process, which they propose can be considered an indicator of the quality of the peer-review process."
        },
        {
            "heading": "2.2 Role of aspects and sentiment in text or reviews",
            "text": "Aspect level sentiment analysis is based on the idea that an opinion consists of sentiment (positive or negative) and a target (of aspect). An opinion without its target being identified is of limited use. The importance of opinion targets also helps us to understand sentiment analysis better. However, it is critical to determine the scope of each sentiment expression, i.e., whether it covers the interest in the sentence [13]. MATE [1], a neural framework for the opinion summarization model, is used to reconstruct the input segment\u2019s embedding to produce an aspect matrix. The framework created an aspect matrix by reducing each seed matrix to a single aspect\nembedding with the help of seed weight vectors. Seeds can be considered as query terms that someone would use to search for segments discussing an aspect. A recurrent convolutional network model incorporating modularity and attention mechanisms was used by Qiao et al. [19] to predict the aspects of academic papers.\nFor aspect and sentiment analysis in peer reviews, authors in [28] propose a multiple instance learning network with a novel abstract-based memory mechanism (MILAM) to perform sentiment analysis in the domain of peer reviews for scholarly papers. Multilevel aspects of peer reviews for academic articles [15] aimed to extract the multi-level aspects in peer reviews comprehensively.\nASAP-Review, a dataset [30] was created with an ambitious goal of automating scientific peer review, which we use in our current work. Based on sentiment encoded in peer review texts peer review praise and criticism was evaluated automatically [26]. Recently, a novel multitasking system [11] was proposed which leverages inter-dependency by sharing representations between two related tasks: aspect categorization and sentiment classification."
        },
        {
            "heading": "2.3 Acceptance or Rejection prediction for scientific papers",
            "text": "DeepSentiPeer [4], a deep neural architecture was proposed for classifying research papers (accept/reject) and predicting recommendation scores. It examined the impact of sentiment within the review tomake their decisions. The Role of aspect scores in boosting the decision prediction task was also investigated [3]. The distribution of aspect-based sentiment differed substantially between accepted and rejected papers. Authors in [10, 17] used a deep neural network model to predict the review outcome.\nOur work is different from the previous works as we investigated the role of the aspect category for this task. However, the aspect score needs to be manually annotated and would not be available during evaluation. Our model does not rely on aspect scores for predicting the decision score. The significance of an aspect varies according to the venue. Not only the sentiment but also the aspect information with sentiment plays a vital role in the acceptance or rejection of a paper. For example, in a conference (say A), reviewers are explicitly advised to judge the quality of a paper and not its length. So, reviewers may focus more on aspect motivation or novelty than the aspect substance. However, for a conference (say B) the substantial quality of the paper is more important. If there are negative comments on it, then the paper might get rejected. Our model is generic and can solve this problem to some extent by adjusting the aspect weight score based on the venue or editor or chair\u2019s focus."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Figure 1 shows the overall architectural diagram of our model. We first introduce the aspect embedding representation layer, which creates aspect-aware matrix representations. Secondly, we introduce the sentiment polarity infused aspect embeddingmatrix, which explains how the polarity of a sentence is infused along with aspect and sentiment category knowledge. The sentiment and aspect attention layer is followed to create an aspect and sentiment aware\nembedding. Finally, the output of the sentiment and aspect attention layer is passed through the prediction layer to obtain the final review outcome."
        },
        {
            "heading": "3.1 Aspect embedding representation layer",
            "text": "This layer is further described into three parts. In the input layer, reviews are grouped as per its aspect and sentiment. This is followed by the representation layer which creates embedding for each group of the sentences. Finally, the CNN(convolutional neural network) creates a final aspect aware embedding from the prior embedding representations.\n3.1.1 Input layer. Initially, we have a collection of reviews \ud835\udc37 = {\ud835\udc45\ud835\udc521, \ud835\udc45\ud835\udc522, ..., \ud835\udc45\ud835\udc52\ud835\udc5b}, corresponding to a single paper. We combine all the reviews of a paper to create a single combined review \ud835\udc43 . Each \ud835\udc43 = {\ud835\udc651, \ud835\udc652, ..., \ud835\udc65\ud835\udc5a}, is a collection of sentences, where \ud835\udc65\ud835\udc56 \u2208 \ud835\udc43 denotes a single sentence.\nFor our experiments from the dataset, we segregated the review sentences into separate groups. First, based on the sentiment of the comments received from reviewers positively or negatively, i.e. \ud835\udc3a\ud835\udc5d\ud835\udc5c\ud835\udc60 and \ud835\udc3a\ud835\udc5b\ud835\udc52\ud835\udc54 . Here, \ud835\udc5d\ud835\udc5c\ud835\udc60 is the group containing positive review sentences and \ud835\udc5b\ud835\udc52\ud835\udc54 is the group containing negative review sentences. Secondly, each group is divided further based on the aspect of the comments received from reviewers, i.e. \ud835\udc3a\ud835\udc4e,\ud835\udc60 , where \ud835\udc4e is the number of aspect categories(8) and \ud835\udc60 is the number of sentiment categories(2). We use the gold annotated aspect sentiment labels from the dataset.\n3.1.2 Representation layer. Each group\ud835\udc3a\ud835\udc4e,\ud835\udc60 of sentences, obtained from the input layer is passed through an encoder for generating the contextualized representations for those sentences.\n\ud835\udc3b\ud835\udc4e,\ud835\udc60 = \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f (\ud835\udc3a\ud835\udc4e,\ud835\udc60 ) (1) where \ud835\udc3b\ud835\udc4e,\ud835\udc60 is the contextual representation of each sentence of subgroup \ud835\udc3a\ud835\udc4e,\ud835\udc60 . For this purpose, we utilize Sentence Transformers s-BERT [20] encoder model fine-tuned on STSB (semantic textual similarity) task as a sentence encoder.\n3.1.3 CNN layers. In this layer, we make use of a CNN [5] to extract features from the contextualized representations, \ud835\udc3b\ud835\udc4e\ud835\udc60 . In recent years, CNN has always shown great success in solving the NLP problems. The convolution operation works by sliding a filter \ud835\udc4a\ud835\udc53 \ud835\udc58 \u2208 \ud835\udc45\ud835\udc59\ud835\udc51 to a window of length l, the output of such \u210e\ud835\udc61\u210e window is given as :\n\ud835\udc53 \ud835\udc58 \u210e = \ud835\udc54(\ud835\udc4a\ud835\udc53 \ud835\udc58 \u00b7\ud835\udc4b\u210e\u2212\ud835\udc59+1:\u210e + \ud835\udc4f\ud835\udc58 ) (2) \ud835\udc4b\u210e\ud835\udc59+1:\u210e means the l sentences within the \u210e\ud835\udc61\u210e window in a group of contextual embedding \ud835\udc3b\ud835\udc4e\ud835\udc60 , \ud835\udc4f\ud835\udc58 is the bias for the \ud835\udc58\ud835\udc61\u210e filter, g() is the non-linear function. The feature map \ud835\udc53\ud835\udc58 for the kth filter is then obtained by applying this filter to each possible window of contextual embedding in a group of contextual embedding \ud835\udc3b\ud835\udc4e\ud835\udc60 is given as,\n\ud835\udc53 \ud835\udc58 = [\ud835\udc53 \ud835\udc581 , \ud835\udc53 \ud835\udc58 2 , ., \ud835\udc53 \ud835\udc58 \u210e , .., \ud835\udc53 \ud835\udc58 \ud835\udc5b1\ud835\udc59+1], \ud835\udc53 \ud835\udc58 \u2208 \ud835\udc45\ud835\udc5b1\ud835\udc59+1 . (3)\nFor a contextualized sentence representations \ud835\udc3b\ud835\udc4e,\ud835\udc60 , the final output of this convolution filter operation is then given as\n\ud835\udc5f = [\ud835\udc53 1, \ud835\udc53 2, \ud835\udc53 3 ...\ud835\udc53 \ud835\udc39 ], \ud835\udc5d \u2208 \ud835\udc45\ud835\udc39 . (4)\n.\nwhere F is the total number of filters used in the operation. For the \ud835\udc56\ud835\udc61\u210e review and \ud835\udc57\ud835\udc61\u210e aspect of the \ud835\udc58\ud835\udc61\u210e sentiment\n\ud835\udc34\ud835\udc58\ud835\udc57 = \ud835\udc36\ud835\udc41\ud835\udc41 (\ud835\udc3b \ud835\udc57,\ud835\udc58 ) (5) We obtain an embedding for each group of contextualized sentence representations by passing through the CNN layer, which is the embedding vector for each aspect sentiment and each category. The two metrices for each sentiment \ud835\udc34\ud835\udc58 \u2208 \ud835\udc45\ud835\udc5b\ud835\udc65\ud835\udc51 , contain one row per aspect category and holds its contextualized embedding representation, as illustrated in Figure 1. The main reason for using CNN is its ability to extract the essential features from the group representations\ud835\udc3b\ud835\udc4e,\ud835\udc57 , reflecting that particular aspect and sentiment pair."
        },
        {
            "heading": "3.2 Sentiment polarity infused aspect embedding matrix",
            "text": "For the jth aspect and kth sentiment, we employ a Multi-Layer Perceptron (MLP) as shown below:-\n\ud835\udc36\ud835\udc34\ud835\udc58\ud835\udc57 = \ud835\udc40\ud835\udc3f\ud835\udc43 (\ud835\udc34 \ud835\udc58 \ud835\udc57 ) (6)\nWe use VADER [6] (Valence Aware Dictionary for Sentiment Reasoning) which returns a compound sentiment score on a continuous scale of -1 (extremely negative) to +1 (extremely positive) for each sentence. We use this positive or negative sentiment intensity as a polarity feature in our model. We create a polarity vector \ud835\udc63\ud835\udc58\n\ud835\udc57 to\nadd the polarity of the sentences in the reviews for its sentiment label (padded where necessary), which is already known.\nFor the \ud835\udc57\ud835\udc61\u210e aspect and \ud835\udc58\ud835\udc61\u210e sentiment, the polarity vector \ud835\udc49\ud835\udc58 \ud835\udc57 is\ndenoted by:-\n\ud835\udc49\ud835\udc58\ud835\udc57 = [\ud835\udc631; \ud835\udc632; ...; \ud835\udc63\ud835\udc5b] (7) where \ud835\udc63\ud835\udc56 represents the polarity score of the sentence corresponding to group \ud835\udc3a\ud835\udc58,\ud835\udc57 . The vector formed is then concatenated with the aspect embedding matrix to create a sentiment polarity infused aspect embedding matrix as shown in the equation below:-\n\ud835\udc3b\ud835\udc36\ud835\udc58\ud835\udc57 = [\ud835\udc36\ud835\udc34 \ud835\udc58 \ud835\udc57 ;\ud835\udc49 \ud835\udc58 \ud835\udc57 ] (8)\nwhere \ud835\udc34 is the aspect aware embedding and \ud835\udc49 is the VADER embedding matrix."
        },
        {
            "heading": "3.3 Sentiment attention layer",
            "text": "This layer takes the final aspect embedding representation as input, and produce a sentiment aware embedding for each aspect with respect to each sentiment category as shown in the equation below:-\nFor the \ud835\udc56\ud835\udc61\u210e review \ud835\udc57\ud835\udc61\u210e aspect\n\ud835\udc60\ud835\udc4e\ud835\udc56\ud835\udc57 = 2\u2211\ufe01\n\ud835\udc58=1 (\ud835\udc3b\ud835\udc36\ud835\udc58\ud835\udc57 )\n\ud835\udc47 \ud835\udc4a\ud835\udc58 (9)\nwhere\ud835\udc4a\ud835\udc58 \u2208 \ud835\udc451\u00d71, is a learnable parameter(a.k.a sentiment importance score) and \ud835\udc60\ud835\udc4e\ud835\udc56\n\ud835\udc57 \u2208 \ud835\udc45\ud835\udc51\u00d71 is the sentiment aware embedding."
        },
        {
            "heading": "3.4 Aspect attention layer",
            "text": "This layer takes the final aspect embedding representation as input and produces a sentiment aware embedding for each aspect concerning each sentiment category as shown in the equation below:-\nFor the \ud835\udc56\ud835\udc61\u210e review and jth aspect\n\ud835\udc5f \ud835\udc56\ud835\udc4e = 8\u2211\ufe01 \ud835\udc57=1 \ud835\udc60\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc4a\ud835\udc4e (10)\nwhere,\ud835\udc4a\ud835\udc4e \u2208 \ud835\udc451\u00d71 is a learnable parameter (a.k.a aspect importance score) and \ud835\udc5f \ud835\udc56\ud835\udc4e \u2208 \ud835\udc45\ud835\udc51\u00d71 is the aspect-aware embedding."
        },
        {
            "heading": "3.5 Prediction layer",
            "text": "We pass the embedding vector r from the previous layer to two fully connected layers with ReLU activation to obtain the vector\ud835\udc4b \ud835\udc56 . This is followed by a fully connected layer with sigmoid activation, as shown in the equation below :-\n\ud835\udc4b \ud835\udc56 = \ud835\udc40\ud835\udc3f\ud835\udc43 (\ud835\udc5f \ud835\udc56\ud835\udc4e) (11)\n\ud835\udc50 = \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 (\ud835\udc4a\ud835\udc50 \u2217 \ud835\udc4b \ud835\udc56 + \ud835\udc4f\ud835\udc50 ) (12) where c is the output classification distribution across accept or reject classes and\ud835\udc4a\ud835\udc50 and \ud835\udc4f\ud835\udc50 are the trainable parameters. We minimize the binary Cross-Entropy Loss between the predicted c and actual decisions."
        },
        {
            "heading": "4 DATASETS AND EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Data",
            "text": "We use the recently introduced ASAP-Review dataset[30]. ASAPReview stands for Aspect-enhanced Peer Review, and is composed of review texts, their annotated review-level annotations, and review meta-data. More than 20,000 annotated reviews in this dataset are structured in JSON tags with the corresponding annotation labels. It contains ICLR (International Conference on Learning Representations, 2017-2020) and NeurIPS (Neural Information Processing Systems, 2016-2019) reviews from the open-access platform, OpenReview. We further note that the dataset is imbalanced and show the percentage of each aspect in the entire dataset in Figure 2. The topolgy and meaning of each aspect category is given in Table 1 4. We show the detailed dataset statistics in Table 2 and refer the reader to the original paper for more details.\n4https://github.com/neulab/ReviewAdvisor/blob/main/materials\nThis dataset has eight aspects, including a review summary (SUM). In general, the summary does not have any particular sentiment associated with it; hence we omitted it in our experiments. We also added one extra category as no-aspect that contain the review sentences which do not belong to any of the aspect categories mentioned above."
        },
        {
            "heading": "4.2 Experimental settings",
            "text": "For creating review embedding, we adopt Sentence Transformers [20] with stsb-RoBERTa-base as the encoder. It is formulated via a CNN for aspect feature extraction, which is structured with three parallel convolutional layers with kernel sizes of 1,16,1, respectively. In all the settings, we apply a dropout of 0.5 and optimize binary cross-entropy loss using the AdamW optimizer [14] and trained for 5 to 12 epochs using a batch size of 16,32 and a learning rate of 0.001, 5e-6, 1e-5, 2e-5, or 5e-5. We pick the best learning rate and batch size on the test results. We found the setting that works best is a learning rate of 0.001 with a weight decay of 1e-4 and batch size of 16."
        },
        {
            "heading": "4.3 Comparing system",
            "text": "\u2022 DeepSentiPeer [4]: They extract full-text sentences from each research article review and represent each sentence using the Transformer variant of the Universal Sentence. Encoder (USE), of d, is the dimension of the semantic sentence vector, 512. They use a CNN to extract features from both the paper and review representations. The sentiment encoding of the review is done using VADER Sentiment Analyzer. A feature-level fusion was generated based on the concatenation of those features and fed into the multilayer perceptron. They reported on both settings, i.e., using only the reviews and reviews with their associated sentiment polarity, and concluded that sentiment plays a vital role in this task.\n\u2022 MRG [10]: This work predicted the decision using the reviews from three transformer based encoder followed by the linear layers for decision prediction. They reported their result for two settings. In the first setting, the encoded last hidden state representation from the encoder is passed to MLP for calculating the result while in other, instead of the encoder, the decoder\u2019s last hidden state is used. These layer"
        },
        {
            "heading": "4.4 Result and discussion",
            "text": "We report the evaluation results of our proposed system and the other existing methods in Table 3. Our proposed approach outperforms DeepSentiPeer model with sentiment by around 8 points and without sentiment by around 10 points with respect to the accuracy. Our proposed also outperforms the approach by Pradhan et al. by around 8.7 points w.r.t. accuracy. In case of MRG, our model outperforms the approach by around 18 points w.r.t accuracy for both the settings.\nThe comparing systems have used PeerRead or a different dataset for their experiments. Our approach requires prior aspect knowledge of the reviews, so we have used the ASPeer dataset for our experiments. We executed their model on the recent dataset and found our approach to outperform it. The reason behind this is that the best-comparing system DeepSentiPeer [4] uses review text and sentiment within the review with sentence embedding (using Universal Sentence Encoder) to predict the acceptance decisions. Here, we use better contextual embedding (BERT). Also, we have\ncreated aspect-aware embedding and aspect-aware sentiment polarity embeddings along with aspect and sentiment attention, which enhances our model\u2019s performance. Our model predicted 78.9% accuracy on NIPS conferences and 81.3% on ICLR conferences.\nWe analyze the attention visualization to understand the learning behavior of our proposed model. Figure 3 shows the trained softmax aspect weight score learned by our model. The darker color reveals higher attention scores, while the lighter has little importance. This weighted score indicates the importance to a particular aspect given by our model in determining if a paper should be accepted or rejected. We note that the contextual embedding and sentiments bound to Motivation, Meaningful Comparison, Originality and Soundness are the most effective ones in predicting the intended recommendation in a review. We believe that this result is also significant because it acts as a guideline for the authors as to what particular aspects they should focus on while reviewing their papers before submission.\nThe importance of an aspect could vary depending upon the venue of the conference or journal to which it is submitted. A paper may get rejected in a conference but can get accepted in another conference. This motivates towards getting a generalized model that can address this issue. A user can easily control the aspect weight score based on venue or the requirement to predict their set values. See Section 4.6.1 for more details and examples."
        },
        {
            "heading": "4.5 Ablation study",
            "text": "To validate the effectiveness of our proposed framework, we analyze the outputs of our framework in different settings as shown in Table 6.\n\u2022 \ud835\udc461 : We ran the model without aspect or sentiment information, and using only the embedding of the review sentences. \u2022 \ud835\udc462: We ran the model without aspect information using the review sentence embedding along with its sentiment information.\nTable 3: Experimental results of our model. Training is done on the entire collection of papers excluding 20% ICLR and NIPS papers for testing. The comparison is done on the same training and test data splits. The accuracy values for DeepASPeer are statistically significant over DeepSentiPeer performance (two-tailed t-test, p<0.05). MRG Decision LAST and MRG Decision ENCODED are two settings of MRG model as described in Section 4.3\n. Model F1 score Accuracy\nRejected Accepted Overall MRG Decision LAST [10] 27.0 74.3 62.07\nMRG Decision ENCODED [10] 28.0 75.7 62.23 DeepSentiPeer(Review) [4] 59.2 71.4 69.79\nPradhan et al. [17] 60.2 74.3 71.31 DeepSentiPeer(Review+Sentiment) [4] 62.8 73.4 72.02 Our Proposed Model (DeepASPeer) 70.02 84.23 80.05\n\u2022 \ud835\udc463 : We ran the model with both aspect and sentiment information, using the review sentence embedding along with its aspect and its sentiment information. \u2022 \ud835\udc464 : It is the same as \ud835\udc463; additionally, we add the review sentences that do not belong to any particular aspect (as a category) for our experiment.\nConsidering the sentences in setting \ud835\udc464, we obtain an accuracy of 80.05%, while in setting \ud835\udc463 we observe a slight drop of 0.37%, in accuracy i.e., 79.68%. This shows that the review sentences belonging to the particular aspects plays a major role in determining the decision for a paper.\nTo further investigate the importance of aspect information, we ran our model only with the information of sentiment polarities and removing the aspect wise information. In particular, we created sentence embedding infused with sentiment polarity (Vader sentiment) followed by sentiment attention. We divide the reviews based on the aspect categories and apply aspect attention on top of it. We obtain an accuracy of 76.67% as shown in \ud835\udc462 setting. This shows the role of aspect information in boosting the performance of the decision process.\nWe consider the \ud835\udc462 setting without sentiment information in setting \ud835\udc461. In particular, we ran our model without aspect category or sentiment knowledge, by creating its BERT embedding and passing it through MLP and finally through softmax. We obtain an accuracy of 64.33 which is 12.34% less compared to when we consider sentiment information, which shows that sentiment importance plays an important role to determine the acceptance or rejection of a paper.\n."
        },
        {
            "heading": "4.6 Case study and observations",
            "text": "We conducted several human elicitation studies to further analyze the final decision produced.\n4.6.1 Aspect control: Here, we try to answer the following interesting question: Can the decision be controlled according to aspect importance? For this purpose, we picked reviews for a few papers, and classified into aspect and sentiment groups in accordance with the strategy mentioned in 3.1.1.\nLet us take an example of review comments shown in Table 4. Despite so many negative comments in the review, the paper was still accepted; the probable reason might be the impactful motivation. The model with ablation setting \ud835\udc462, i.e. the model without aspect and only with sentiment clearly rejects it by a decision probability of 72%. However, DeepASPeer considers the aspect importance while making its decision but still weakly rejects it by a decision probability of 55%.\nThe problem described above is a representative of the real world scenario of a decision prediction system, where every meta chair or reviewer has its own mindset of giving priority to certain aspects over others. To solve this issue, we introduce an additional functionality in our model, which asks human to prioritize the aspects by himself or herself, and predicts the decision accordingly. This functionality helps a particular conference to have a decision prediction model which assists them according to their own priorities.\nNow let us take a scenario when this paper is sent to a conference where the meta chair sets the aspect priorities as shown in Figure 4. As a result of these priorities, the model changed its decision to ACCEPT (with a decision probability of 54%), which can also be considered as weak acceptance.\nSimilarly, for reviews, as shown in Table 5, again there are many negative comments in the review. However, the paper is accepted probably because of its originality. The DeepASPeer - Aspect model rejects it by a decision probability of 68%, and DeepASPeer models the sentiment distribution concerning aspects and weakly rejects it by a decision probability of 56%.\nFigure 5 shows a scenario where the paper is related to a conference where the meta chair sets the aspect priorities, giving the highest priority to originality. After incorporating these priorities,\nthe model changed its decision to ACCEPT (with a decision probability of 60%).\nThis little human intervention will help them study the relationship of an aspect with the decision process more closely, thus assisting them in making a decision. By statistical analysis over more papers from a conference, more concrete aspect scores can\nbe calculated, which could assist the decision system in adapting itself for any specific conference.\n4.6.2 Effect of adding rating information: We also investigated the effect of adding the rating information already provided\nFigure 4: Example 1; Attention visualization of manually controlled aspect importance.\nFigure 5: Example 2; Attention visualization of manually controlled aspect importance.\nby the reviewers. In particular, we infused the rating score or recommendation score of the reviewers by concatenating the review scores at the last hidden representation before the final fully connected layer in our model. Adding this information boosted our accuracy to 91.89% by 11.84%. However, this is not the ideal situation, as the editor or meta chair should make the decision based on the reviewer\u2019s textual comments, as decisions appeared to be\nsignificantly influenced by the reviewers\u2019 recommendations which is discussed in Section 1.\n4.7 Error analysis \u2022 Contradictory reviews: Our model sometimes gets confused by the contradictory contextual statements. The reason can be contradiction between the different reviewers or the same reviewer(we have combined the reviews). Table 7 shows a few of such examples. The first example for (Paper id: ICLR_2020_1011), the review comment, indicates that the motivation is clear. In contrast, another comment for the same paper indicates that the approach is not motivated while contradicting each other. Similarly, in another example, for (Paper id: NIPS_2016_354), the review says that the paper is clear and well written. On the other hand, another review for the same paper negatively expresses the paper\u2019s clarity.\n\u2022 Complex reviews having multiple aspects: Few review comments contain multiple aspects in a single review sentence. For example: \"The paper is not clear and not motivating\u201c. In our investigation, we have considered that each review sentence has only one aspect category. This often confuses the model resulting in mis-classification. \u2022 Reviewer and Editor\u2019s diverse opinions: After reading the entire paper, there can be different opinions between reviewers and the Editor or Chair, as our model relies on its decision by considering the reviewer\u2019s perspective. It fails in these cases. For example, in the paper (Paper id: NIPS_2017_167) majority of the reviewer comments are negative on various aspects, so our model predicts as rejected. However, the paper is accepted by the editor or chair."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we have investigated how AI systems could assist the editor or chair in anticipating the final decision within a limited time. Each venue has a distinct focus on a different aspect of a paper. Therefore, a generic model needs to be developed to solve this problem. We developed a deep neural architecture that uses three sources of information to tackle the complex task of peer review decision making: the peer review texts, aspects, and sentiment associated with the reviews. Our model is generic and can be easily adapted to different aspect categories that may have to be considered depending upon the types of conferences and/or journals. Our result shows that our model achieves competitive performance on the recent dataset. We found that aspects, along with their corresponding sentiment, can help build a generic controllable system to assist the editor/chair in determining the outcome based on the reviews of a paper to make better editorial decisions.\nOur future work aims to write a meta-review automatically influenced by the decision of a manuscript. However, we all agree that scholarly language processing is not straightforward. In order to decide the fate of a manuscript, robust, pervasive models are required to capture the high-level interaction between the paper and the peer reviews. We also recommend a larger dataset for peer reviews than what is used for this paper (8,877 papers) to avoid overfitting during the training process."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "Sandeep Kumar acknowledges the Prime Minister Research Fellowship (PMRF) program of the Govt of India for its support. Asif Ekbal acknowledges the Young Faculty Research Fellowship (YFRF), supported by Visvesvaraya PhD scheme for Electronics and IT, Ministry of Electronics and Information Technology (MeitY), Government of India, being implemented by Digital India Corporation (formerly Media Lab Asia). Tirthankar Ghosal acknowledges Cactus Communications, India."
        }
    ],
    "title": "DeepASPeer: Towards an Aspect-level Sentiment Controllable Framework for Decision Prediction from Academic Peer Reviews",
    "year": 2024
}