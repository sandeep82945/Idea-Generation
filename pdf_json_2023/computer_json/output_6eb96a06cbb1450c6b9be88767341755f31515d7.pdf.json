{
    "abstractText": "Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotatorcentric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97% recall while substantially reducing the workload required by a fully manual annotation process.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shafiuddin Rehan Ahmed"
        },
        {
            "affiliations": [],
            "name": "Abhijnan Nath"
        },
        {
            "affiliations": [],
            "name": "Michael Regan"
        },
        {
            "affiliations": [],
            "name": "Adam Pollins"
        },
        {
            "affiliations": [],
            "name": "Nikhil Krishnaswamy"
        },
        {
            "affiliations": [],
            "name": "James H. Martin"
        },
        {
            "affiliations": [],
            "name": "Matt Smith"
        },
        {
            "affiliations": [],
            "name": "David Tennant"
        },
        {
            "affiliations": [],
            "name": "Peter Capaldi"
        }
    ],
    "id": "SP:d7e475bf5b3e0e68660bcc122ff1cdc6c92d870e",
    "references": [
        {
            "authors": [
                "Amit Bagga",
                "Breck Baldwin."
            ],
            "title": "Algorithms for scoring coreference chains",
            "venue": "In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563\u2013566.",
            "year": 1998
        },
        {
            "authors": [
                "Shany Barhom",
                "Vered Shwartz",
                "Alon Eirew",
                "Michael Bugert",
                "Nils Reimers",
                "Ido Dagan"
            ],
            "title": "Revisiting joint modeling of cross-document entity",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "year": 2020
        },
        {
            "authors": [
                "Ari Bornstein",
                "Arie Cattan",
                "Ido Dagan."
            ],
            "title": "CoRefi: A crowd sourcing suite for coreference annotation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 205\u2013215, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Avi Caciularu",
                "Arman Cohan",
                "Iz Beltagy",
                "Matthew Peters",
                "Arie Cattan",
                "Ido Dagan."
            ],
            "title": "CDLM: Cross-document language modeling",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648\u20132662, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Arie Cattan",
                "Alon Eirew",
                "Gabriel Stanovsky",
                "Mandar Joshi",
                "Ido Dagan"
            ],
            "title": "Streamlining crossdocument coreference resolution: Evaluation and modeling",
            "year": 2020
        },
        {
            "authors": [
                "Agata Cybulska",
                "Piek Vossen."
            ],
            "title": "Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May",
            "year": 2014
        },
        {
            "authors": [
                "Agata Cybulska",
                "Piek Vossen."
            ],
            "title": "Translating granularity of event slots into features for event coreference resolution",
            "venue": "Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1\u201310, Denver, Col-",
            "year": 2015
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Jan-Christoph Klie",
                "Michael Bugert",
                "Beto Boullosa",
                "Richard Eckart de Castilho",
                "Iryna Gurevych."
            ],
            "title": "The INCEpTION platform: Machine-assisted and knowledge-oriented interactive annotation",
            "venue": "Proceedings of the 27th International Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoqiang Luo."
            ],
            "title": "On coreference resolution performance metrics",
            "venue": "Proceedings of the Conference on",
            "year": 2005
        },
        {
            "authors": [
                "Xiaoqiang Luo",
                "Sameer Pradhan",
                "Marta Recasens",
                "Eduard Hovy."
            ],
            "title": "An extension of BLANC to system mentions",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 24\u201329,",
            "year": 2014
        },
        {
            "authors": [
                "Yehudit Meged",
                "Avi Caciularu",
                "Vered Shwartz",
                "Ido Dagan."
            ],
            "title": "Paraphrasing vs coreferring: Two sides of the same coin",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897\u20134907, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Emanuele Pianta",
                "Christian Girardi",
                "Roberto Zanoli."
            ],
            "title": "The TextPro tool suite",
            "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908), Marrakech, Morocco. European Language Resources Association",
            "year": 2008
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Xiaoqiang Luo",
                "Marta Recasens",
                "Eduard Hovy",
                "Vincent Ng",
                "Michael Strube."
            ],
            "title": "Scoring coreference partitions of predicted mentions: A reference implementation",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Compu-",
            "year": 2014
        },
        {
            "authors": [
                "M. Recasens",
                "Eduard Hovy."
            ],
            "title": "Blanc: Implementing the rand index for coreference evaluation",
            "venue": "Natural Language Engineering, 17:485 \u2013 510.",
            "year": 2011
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "ArXiv, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Peng Shi",
                "Jimmy Lin."
            ],
            "title": "Simple bert models for relation extraction and semantic role labeling",
            "venue": "arXiv preprint arXiv:1904.05255.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyi Song",
                "Ann Bies",
                "Justin Mott",
                "Xuansong Li",
                "Stephanie Strassel",
                "Christopher Caruso."
            ],
            "title": "Cross-document, cross-language event coreference annotation using event hoppers",
            "venue": "Proceedings of the Eleventh International Conference on Language",
            "year": 2018
        },
        {
            "authors": [
                "Marc Vilain",
                "John Burger",
                "John Aberdeen",
                "Dennis Connolly",
                "Lynette Hirschman."
            ],
            "title": "A modeltheoretic coreference scoring scheme",
            "venue": "Proceedings of the 6th Conference on Message Understanding, MUC6 \u201995, page 45\u201352, USA. Association for",
            "year": 1995
        },
        {
            "authors": [
                "Piek Vossen",
                "Filip Ilievski",
                "Marten Postma",
                "Roxane Segers."
            ],
            "title": "Don\u2019t annotate, but validate: A data-to-text method for capturing event data",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Kristin Wright-Bettner",
                "Martha Palmer",
                "Guergana Savova",
                "Piet de Groen",
                "Timothy Miller."
            ],
            "title": "Cross-document coreference: An approach to capturing coreference without context",
            "venue": "Proceedings of the Tenth International Workshop on Health Text Min-",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "ing and Information Analysis (LOUHI 2019),",
            "year": 2019
        },
        {
            "authors": [
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Richard Eckart de Castilho",
                "Iryna Gurevych."
            ],
            "title": "Automatic annotation suggestions and custom annotation layers in WebAnno",
            "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Xiaodong Yu",
                "Wenpeng Yin",
                "Dan Roth"
            ],
            "title": "Paired representation learning for event and entity coreference",
            "year": 2020
        },
        {
            "authors": [
                "Michelle Yuan",
                "Patrick Xia",
                "Chandler May",
                "Benjamin Van Durme",
                "Jordan Boyd-Graber."
            ],
            "title": "Adapting coreference resolution models through active learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Zeng",
                "Xiaolong Jin",
                "Saiping Guan",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Event coreference resolution with their paraphrases and argument-aware embeddings",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event Coreference Resolution (ECR) is the task of identifying mentions of the same event either within or across documents. Consider the following excerpts from three related documents:\ne1: 55 year old star will replacem1 Matt Smith, who announced in June that he was leaving the sci-fi show.\ne2: Matt Smith, 26, will make his debut in 2010, replacingm2 David Tennant, who leaves at the end of this year.\ne3: Peter Capaldi takes overm3 Doctor Who . . . Peter Capaldi stepped intom4 Matt Smith\u2019s soon to be vacant Doctor Who shoes.\ne1, e2, and e3 are example sentences from three documents where the event mentions are highlighted and sub-scripted by their respective identifiers (m1 through m4). The task of ECR is to automatically form the two clusters {m1,m3,m4}, and {m2}. We refer to any pair between the mentions of a cluster, e.g., (m1,m3) as an ECR link. Any pair formed across two clusters, e.g., (m1,m2) is referred to as non-ECR link.\nAnnotating ECR links can be challenging due to the large volume of mention pairs that must be compared. The annotating task becomes increasingly time-consuming as the number of events in the corpus increases. As a result, this task requires a lot of mental effort from the annotator and can lead to poor quality annotations (Song et al., 2018; Wright-Bettner et al., 2019). Indeed, an annotator has to examine multiple documents simultaneously often relying on memory to identify all the links which can be an error-prone process.\nTo reduce the cognitive burden of annotating ECR links, annotation tools can provide integrated model-in-the-loop for sampling likely coreferent mention pairs (Pianta et al., 2008; Yimam et al., 2014; Klie et al., 2018). These systems typically store a knowledge base (KB) of annotated documents and then use this KB to suggest relevant candidates. The annotator can then inspect the candidates and choose a coreferent event if present.\nThe model\u2019s querying and ranking operations are typically driven by machine learning (ML) systems that are trained either actively (Pianta et al., 2008; Klie et al., 2018; Bornstein et al., 2020; Yuan et al., 2022) or by using batches of annotations (Yimam et al., 2014). While there have been advances in suggestion-based annotations, there is little to no work in evaluating the effectiveness of these systems, particularly in the use case of ECR. Specifically, both the overall coverage, or recall, of the annotation process as well as the degree of annotator effort needed depend on the performance of the model. In order to address this shortcoming, we offer the following contributions:\n1. We introduce a method of model-in-the-loop annotations for ECR1.\n2. We compare three existing methods for ECR (differing widely in their computational costs), by adapting them as the underlying ML mod-\n1repo: github.com/ahmeshaf/model_in_coref\nar X\niv :2\n30 6.\n05 43\n4v 1\n[ cs\n.C L\n] 6\nJ un\n2 02\n3\nels governing the annotations.\n3. We introduce a novel methodology for assessing the workflow by simulating the annotations and then evaluating an annotator-centric Recall-Annotation effort tradeoff."
        },
        {
            "heading": "2 Related Work",
            "text": "Previous work for ECR is largely based on modeling the probability of coreference between mention pairs. These models are built on supervised classifiers trained using features extracted from the pairs. Most recent work uses a transformer-based language model (LM) like BERT (Devlin et al., 2018; Liu et al., 2019) to generate joint representations of mention pairs, a method known as cross-encoding. The cross-encoder is fine-tuned using a coreference scoring objective (Barhom et al., 2019; Cattan et al., 2020; Meged et al., 2020; Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021). These methods use scores generated from the scorer to then agglomeratively cluster coreferent events.\nOver the years, a number of metrics have been proposed to evaluate ECR (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Luo et al., 2014; Pradhan et al., 2014). An ECR system is evaluated using these metrics to determine how effectively it can find event clusters (recall) and how cleanly separated the clusters are (precision). From the perspective of annotation, it may only be necessary to focus on the system\u2019s recall or its effectiveness in finding ECR links. However, an annotator might still want to know how much effort is required to identify these links in a corpus to estimate their budget. In the remainder of the paper, we attempt to answer this question by first quantifying annotation effort and analyzing its relation with recall of the system.\nWe use the Event Coreference Bank Plus (ECB+; Cybulska and Vossen (2014)) and the Gun Violence Corpus (GVC; Vossen et al. (2018)) for our experiments. The ECB+ is a common choice for assessing ECR, as well as the experimental setup of Cybulska and Vossen (2015) and gold topic clustering of documents and gold mention annotations for both training and testing2. On the other hand, the GVC offers a more challenging set of exclusively event-specific coreference decisions that require resolving gun violence-related events.\n2The ECB+ test set has 1,780 event mentions with 5K ECR links among 100K pairwise mentions, while the GVC test set has 1,008 mentions with 2K ECR links in 20K pairs. Full"
        },
        {
            "heading": "3 Annotation Methodology",
            "text": "We implement an iterative model-in-the-loop methodology3 for annotating ECR links in a corpus containing annotated event triggers. This approach has two main components - (1) the storage and retrieval of annotated event clusters, which are then compared with each new target event, and (2), an ML model that ranks and prunes the sampled candidate clusters by evaluating their semantic similarity to the target mention.\nAs illustrated in Figure 1, our annotation workflow queries the Annotated Event Store for the target event (m1), retrieving three potential coreferring candidates (m2, m\u2217, and m4). The ranking module then evaluates these candidates based on their lexical and semantic similarities to m1. The annotator then compares each candidate to the target and determines if they are coreferent. Upon finding a coreferent candidate, the target is merged into the coreferent cluster, and any remaining option(s) (m\u2217) are skipped."
        },
        {
            "heading": "3.1 Ranking",
            "text": "We investigate three separate methods to drive the ranking of candidates distinguished by their computational cost. We use these methods to generate the average pair-wise coreference scores between mentions of the candidate and target events, then\nstatistics in Table 1 in Appendix A 3Utilizing the prodi.gy annotation tool. See Appendix D\nuse these scores to rank candidates. We use a single RTX 3090 (24 GB) for running our experiments.\nCross-encoder (CDLM): In this method, we use the fine-tuned cross-encoder ECR system of Caciularu et al. (2021) to generate pairwise mention scores4. Their state of the art system uses a modified Longformer (Beltagy et al., 2020) as the underlying LM to generate document-level representations of the mention pairs (detailed in \u00a7B.1). More specifically, we generate a unified representation (Eq. 1) of the mention pair (mi, mj) by concatenating the pooled output of the transformer (ECLS), the outputs of the individual event triggers (Emi , Emj ), and their element-wise product. Thereafter, pairwise-wise scores are generated for each mention-pair after passing the above representations through a MultiLayer Perceptron (mlp) (Eq. 2) that was trained using the gold-standard labels for supervision.\nLF(mi,mj) = \u2329 ECLS, Emi , Emj , Emi \u2299 Emj \u232a (1) CDLM(mi,mj) = mlp(LF(mi,mj)) (2)\nBERTScore (BERT): (Zhang et al., 2019) BERTScore (BS) is a NLP metric that measures pairwise text similarity by exploiting pretrained BERT models. It calculates cosine similarity of token embeddings with inverse document frequency weights to rate token importance and aggregates them into precision, recall, and F1 scores. This method emphasizes semantically significant tokens, resulting in a more accurate similarity score (details in \u00a7B.2).\nSbert(m) = \u27e8tm, [SEP], Sm\u27e9 (3) BERT(mi,mj) = \u03bb BS(tmi , tmj )\n+ (1\u2212 \u03bb) BS(Sbert(mi), Sbert(mj)) (4)\nTo calculate the BERTScore between the mentions, we first construct a combined sentence (Sbert(m); Shi and Lin (2019)) for a mention (m) by concatenating the mention text (tm) and its corresponding sentence (Sm), as depicted in Equation 3. Subsequently, we compute the BS for each mention pair using Sbert(m) and tm separately, then extract the F1 from each. We then take the weighted average of the two scores as shown in Equation 4 as our ranking metric. This process, carried out using the distilbert\u2212 base\u2212 uncased (Sanh\n4This method is compute-intensive since the transformer\u2019s encoding process scales quadratically with the number of mentions. Using the trained weights, running inference on the two test sets for our experiments takes approximately forty minutes to calculate the similarities of all the mention pairs. The weights are provided by Caciularu et al. (2021) here.\net al., 2019) model, requires approximately seven seconds to complete on each test set. Lemma Similarity (Lemma): The lemma5 similarity method emulates the annotation process carried out by human annotators when determining coreference based on keyword comparisons between two mentions. To estimate this similarity, we compute the token overlap (Jaccard similarity; JS) between the triggers and sentences containing the respective mentions and take a weighted average of the two similarities (like Eq 4) as shown in Eq 56.\nLemma(mi,mj) = \u03bb JS(tmi , tmj )\n+ (1\u2212 \u03bb) JS(Smi , Smj ) (5)\nNo Ranking (Random): For our baseline approach, we employ a method that directly picks the candidate-mention pairs through random sampling and without ranking, providing a reference point for evaluating the effectiveness of the above three ranking techniques."
        },
        {
            "heading": "3.2 Pruning",
            "text": "To control the comparisons between candidate and target events, we restrict our selection to the topk ranked candidates. To refine our analysis, we employ non-integer k values, allowing for the inclusion of an additional candidate with a probability equal to the decimal part of k. We vary the values of k from 2 to 20 on increments of 0.5 and then investigate its relation to recall and effort in \u00a74."
        },
        {
            "heading": "3.3 Simulation",
            "text": "To evaluate the ranking methods, we conduct annotation simulations on the events in the ECB+ and GVC development and test sets. These simulations follow the same annotation methodology of retrieving and ranking candidate events for each target but utilize ground-truth for clustering. By executing simulations on different ranking methods and analyzing their performance, we effectively isolate and assess each approach."
        },
        {
            "heading": "4 Evaluation Methodology",
            "text": "We evaluate the performance of the model-in-theloop annotation with the ranking methods through simulation on two aspects: (1) how well it finds the coreferent links, and (2) how much effort it would take to annotate the links using the ranking method.\n5We use spaCy 3.4 en_core_web_md lemmatizer 6\u03bb is a hyper-parameter to control the weightage of the trigger and sentence similarities in Equations 4 and 5, which we tune using the development set. See Appendix C."
        },
        {
            "heading": "4.1 Recall-Annotation Effort Tradeoff",
            "text": "Recall: The recall metric evaluates the percentage of ECR links that are correctly identified by the suggestion model. It is calculated as the ratio of the number of times the true coreferent candidate is among the suggested candidates. The recall error is introduced when the coreferent candidate is erroneously removed based on the top-k value7. Comparisons: A unit effort represents the comparison between a candidate and target mentions that an annotator would have to make in the annotation process. We count the sampled candidates for each target and stop counting when the coreferent candidate is found. For example, the number of comparisons for the target m1, in Figure 1, is 2 (m2 and m4). We count this number for each target event and present the sum as Comparisons."
        },
        {
            "heading": "4.2 Analysis and Discussion",
            "text": "We present an analysis of the various ranking methods employed in our study, highlighting the performance and viability of each approach. We employ the ranking methods on the test sets of ECB+ and GVC. Then, estimate the Recall and Comparisons measures for different k values, and collate them into the plots as shown in Figure 2. Performance Comparison: The performance improvement of CDLM over BERT and BERT over Lemma can be quantified by examining the graph for the ECB+ and GVC datasets. For example, when targeting a 95% recall for the ECB+ corpus, CDLM provides an almost 100 percent improvement over BERT reducing the number of\n7Note that recall is always 100% if no candidates are ever pruned.\ncomparisons to almost half of the latter. However, both CDLM and BERT outperform Lemma by a significant margin while being drastically better than the Random baseline (See Fig. 2). Interestingly, for GVC, the performance gap between CDLM and BERT is quite close, both needing at least three-fourths as many comparisons as the Lemma and crucially outperforming the Random baseline. CDLM\u2019s inconsistent performance on GVC suggests that a corpus-fine-tuned model such as itself is more effective when applied to a dataset similar to the one it was trained on. Efficiency and Generalizability of BERT: BERT offers a compelling advantage in terms of efficiency, as it can be run on low-compute settings. Moreover, BERT exhibits greater generalizability out-of-the-box when comparing its performance on both the ECB+ and GVC datasets. This makes it an attractive option for ECR annotation task especially when compute resources are limited or when working with diverse corpora."
        },
        {
            "heading": "5 Conclusion",
            "text": "We introduced a model-in-the-loop annotation method for annotating ECR links. We compared three ranking models through a novel evaluation methodology that answers key questions regarding the quality of the model in the annotation loop (namely, recall and effort). Overall, our analysis demonstrates the viability of the models, with CDLM exhibiting the best performance on the ECB+ dataset, followed by BERT and Lemma. The choice of ranking method depends on the specific use case, dataset, and resource constraints, but all three methods offer valuable solutions for different scenarios.\nLimitations\nIt is important to note that the approaches presented in this paper have several constraints. Firstly, the methods presented are restricted to English language only, as Lemma necessitates a lemmatizer and, BERT and CDLM rely on models trained exclusively on English corpora. Secondly, the utilization of the CDLM model demands at least a single GPU, posing potential accessibility issues. Thirdly, ECR annotation is susceptible to errors and severe disagreements amongst annotators, which could entail multiple iterations before achieving a goldstandard quality. Lastly, the generated corpora may be biased to the model used during the annotation process, particularly for smaller values of k.\nEthics Statement\nWe use publicly-available datasets, meaning any bias or offensive content in those datasets risks being reflected in our results. By its nature, the Gun Violence Corpus contains violent content that may be troubling for some."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to express our sincere gratitude to the anonymous reviewers whose insightful comments and constructive feedback helped to greatly improve the quality of this paper. We gratefully acknowledge the support of U.S. Defense Advanced Research Projects Agency (DARPA) FA8750-18-20016-AIDA \u2013 RAMFIS: Representations of vectors and Abstract Meanings For Information Synthesis. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the U.S. government. We would also like to thank ExplosionAI GmbH for partly funding this work. Finally, we extend our thanks to the BoulderNLP group and the SIGNAL Lab at Colorado State for their valuable input and collaboration throughout the development of this work."
        },
        {
            "heading": "A ECB+ Corpus Event Statistics",
            "text": "Table 1 contains the detailed statistics for both the ECB+ and the GVC corpora."
        },
        {
            "heading": "B Model Details",
            "text": "B.1 CDLM The CDLM model, based on the Longformer architecture, cleverly uses a combination of global and local attention for event trigger words and the rest of the document containing those events respectively. More specifically, the Longformer\u2019s increased input capacity of 4096 tokens is utilized\nto encode much longer documents at finetuning that are usually seen in coreference corpora like the ECB+. As seen in Fig. 3, apart from the documentseparator tokens like <doc-s> and <doc-s/> that help contextualize each document in a pair, it adds two special tokens (<m> and </m>) to the model\nvocabulary while pretraining to achieve a greater level of contextualization of a document pair while attending to the event triggers globally at finetuning. Apart from the event-trigger words, the finetuned CDLM model also applies the global attention mechanism on the [CLS] token resulting in a more refined embedding for that document pair while maintaining linearity in the transformer\u2019s selfattention.\nB.2 BERTScore\nBERT-Score is an easy-to-use, low-compute scoring metric that can be used to evaluate NLP tasks that require semantic-similarity matching. This task-agnostic metric uses a base language model like BERT to generate token embeddings and leverages the entire sub-word tokenized reference and candidate sentences (x and x\u0302 in Fig. 4) to calculate the pairwise cosine similarity between the sentence pair. It uses a combination of a greedy-matching subroutine to maximize the similarity scores while\nContextual Embedding\nPairwise Cosine Similarity\nImportance Weighting (Optional) Maximum Similarity\nnormalizing the generated scores based on the IDF (Inverse Document Frequency) of the sub-tokens thereby resulting in more human-readable scores. The latter weighting parameter takes care of rareword occurrences in sentence pairs that are usually more indicative of how semantically similar such pairs are. In our experiments, we use the distilbert\u2212 base\u2212 uncased model to get the pairwise coreference scores, consistent with our goal of deploying an annotation workflow suitable for resource-constrained settings. Such lighter and \u2019distilled\u2019 encoders allow us to optimize resources at inference with minimal loss in performance."
        },
        {
            "heading": "C \u03bb Hyper-parameter Tuning",
            "text": "We employ the evaluation methodology detailed in \u00a74 to determine the optimal value of \u03bb (the weight for trigger similarity and sentence similarity) for both BERT and Lemma approaches. By conducting incremental annotation simulations on the development sets of ECB+ and GVC, we assess \u03bb values ranging from 0 to 1. The recall-effort curve is plotted for each \u03bb value, as shown in Figure 5, allowing us to identify the one that consistently achieves the highest recall with the fewest comparisons. Remarkably, the optimal value for both methods is found to be 0.7, and this value remains consistent across the two datasets and approaches."
        },
        {
            "heading": "D Annotation Interface using Prodigy",
            "text": "Figure 6 illustrates the interface design of the annotation methodology on the popular modelin-the-loop annotation tool - Prodigy (prodi.gy). We use this tool for the simplicity it offers in plugging in the various ranking methods we explained. The recipe for plugging it in to the tool along with other experiment code: github.com/ahmeshaf/model_in_coref."
        }
    ],
    "title": "How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?",
    "year": 2023
}