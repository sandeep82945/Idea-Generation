{
    "abstractText": "For solving linear inverse problems, particularly of the type that appears in tomographic imaging and compressive sensing, this paper develops two new approaches. The first approach is an iterative algorithm that minimizes a regularized least squares objective function where the regularization is based on a compound Gaussian prior distribution. The compound Gaussian prior subsumes many of the commonly used priors in image reconstruction, including those of sparsity-based approaches. The developed iterative algorithm gives rise to the paper\u2019s second new approach, which is a deep neural network that corresponds to an \u201cunrolling\u201d or \u201cunfolding\u201d of the iterative algorithm. Unrolled deep neural networks have interpretable layers and outperform standard deep learning methods. This paper includes a detailed computational theory that provides insight into the construction and performance of both algorithms. The conclusion is that both algorithms outperform other state-of-the-art approaches to tomographic image formation and compressive sensing, especially in the difficult regime of low training.",
    "authors": [
        {
            "affiliations": [],
            "name": "Carter Lyons"
        },
        {
            "affiliations": [],
            "name": "Raghu G. Raj"
        },
        {
            "affiliations": [],
            "name": "Margaret Cheney"
        }
    ],
    "id": "SP:4d749ed089d87f6799dda63cef949ab0b0be74e7",
    "references": [
        {
            "authors": [
                "A. Beck",
                "M. Teboulle"
            ],
            "title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems",
            "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, Jan 2009.",
            "year": 2009
        },
        {
            "authors": [
                "S. Ji",
                "Y. Xue",
                "L. Carin"
            ],
            "title": "Bayesian Compressive Sensing",
            "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2346\u20132356, Jun 2008.",
            "year": 2008
        },
        {
            "authors": [
                "D. Needell",
                "J.A. Tropp"
            ],
            "title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples",
            "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, May 2009.",
            "year": 2009
        },
        {
            "authors": [
                "I. Daubechies",
                "M. Defrise",
                "C. De Mol"
            ],
            "title": "An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity Constraint",
            "venue": "Commun. Pure Appl. Math, vol. 57, no. 11, pp. 1413\u20131457, Nov 2004.",
            "year": 2004
        },
        {
            "authors": [
                "S.-J. Kim",
                "K. Koh",
                "M. Lustig",
                "S. Boyd",
                "D. Gorinevsky"
            ],
            "title": "An Interior- Point Method for Large-Scale l1-Regularized Least Squares",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, pp. 606\u2013 617, Dec 2007.",
            "year": 2007
        },
        {
            "authors": [
                "D. Needell",
                "R. Vershynin"
            ],
            "title": "Signal Recovery From Incomplete and Inaccurate Measurements Via Regularized Orthogonal Matching Pursuit",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 310\u2013316, 2010. 16",
            "year": 2010
        },
        {
            "authors": [
                "R.G. Raj"
            ],
            "title": "A hierarchical Bayesian-MAP approach to inverse problems in imaging",
            "venue": "Inverse Problems, vol. 32, no. 7, p. 075003, Jul 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. McKay",
                "R.G. Raj",
                "V. Monga"
            ],
            "title": "Fast stochastic hierarchical Bayesian map for tomographic imaging",
            "venue": "51st Asilomar Conference on Signals, Systems, and Computers. IEEE, Oct 2017, pp. 223\u2013227.",
            "year": 2017
        },
        {
            "authors": [
                "M.J. Wainwright",
                "E.P. Simoncelli"
            ],
            "title": "Scale Mixtures of Gaussians and the Statistics of Natural Images.",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1999
        },
        {
            "authors": [
                "M.J. Wainwright",
                "E.P. Simoncelli",
                "A.S. Willsky"
            ],
            "title": "Random Cascades on Wavelet Trees and Their Use in Analyzing and Modeling Natural Images",
            "venue": "Applied and Computational Harmonic Analysis, vol. 11, no. 1, pp. 89\u2013123, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "Z. Chance",
                "R.G. Raj",
                "D.J. Love"
            ],
            "title": "Information-theoretic structure of multistatic radar imaging",
            "venue": "IEEE RadarCon (RADAR), 2011, pp. 853\u2013858.",
            "year": 2011
        },
        {
            "authors": [
                "K. Kulkarni",
                "S. Lohit",
                "P. Turaga",
                "R. Kerviche",
                "A. Ashok"
            ],
            "title": "ReconNet: Non-Iterative Reconstruction of Images From Compressively Sensed Measurements",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 449\u2013458.",
            "year": 2016
        },
        {
            "authors": [
                "J. He",
                "Y. Wang",
                "J. Ma"
            ],
            "title": "Radon Inversion via Deep Learning",
            "venue": "IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 2076\u20132087, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K.H. Jin",
                "M.T. McCann",
                "E. Froustey",
                "M. Unser"
            ],
            "title": "Deep Convolutional Neural Network for Inverse Problems in Imaging",
            "venue": "IEEE Transactions on Image Processing, vol. 26, no. 9, pp. 4509\u20134522, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Wang",
                "J.C. Ye",
                "B. De Man"
            ],
            "title": "Deep learning for tomographic image reconstruction",
            "venue": "Nature Machine Intelligence, vol. 2, no. 12, pp. 737\u2013748, Dec 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Qin",
                "J. Schlemper",
                "J. Caballero",
                "A.N. Price",
                "J.V. Hajnal",
                "D. Rueckert"
            ],
            "title": "Convolutional Recurrent Neural Networks for Dynamic MR Image Reconstruction",
            "venue": "IEEE Transactions on Medical Imaging, vol. 38, no. 1, pp. 280\u2013290, Jan 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Bora",
                "A. Jalal",
                "E. Price",
                "A.G. Dimakis"
            ],
            "title": "Compressed Sensing using Generative Models",
            "venue": "Proceedings of the International Conference on Machine Learning, vol. 70, 2017, pp. 537\u2013546.",
            "year": 2017
        },
        {
            "authors": [
                "D. Liang",
                "J. Cheng",
                "Z. Ke",
                "L. Ying"
            ],
            "title": "Deep Magnetic Resonance Image Reconstruction: Inverse Problems Meet Neural Networks",
            "venue": "IEEE Signal Processing Magazine, vol. 37, no. 1, pp. 141\u2013151, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Lucas",
                "M. Iliadis",
                "R. Molina",
                "A.K. Katsaggelos"
            ],
            "title": "Using Deep Neural Networks for Inverse Problems in Imaging: Beyond Analytical Methods",
            "venue": "IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 20\u201336, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Gregor",
                "Y. LeCun"
            ],
            "title": "Learning fast approximations of sparse coding",
            "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 399\u2013406.",
            "year": 2010
        },
        {
            "authors": [
                "V. Monga",
                "Y. Li",
                "Y.C. Eldar"
            ],
            "title": "Algorithm Unrolling: Interpretable, efficient deep learning for signal and image processing",
            "venue": "IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 18\u201344, Mar 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Song",
                "B. Chen",
                "J. Zhang"
            ],
            "title": "Memory-Augmented Deep Unfolding Network for Compressive Sensing",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, Oct 2021, pp. 4249\u20134258.",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "B. Ghanem"
            ],
            "title": "ISTA-Net: Interpretable Optimization- Inspired Deep Network for Image Compressive Sensing",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1828\u20131837.",
            "year": 2018
        },
        {
            "authors": [
                "J. Xiang",
                "Y. Dong",
                "Y. Yang"
            ],
            "title": "FISTA-Net: Learning a Fast Iterative Shrinkage Thresholding Network for Inverse Problems in Imaging",
            "venue": "IEEE Transactions on Medical Imaging, vol. 40, no. 5, pp. 1329\u20131339, May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Meinhardt",
                "M. Moller",
                "C. Hazirbas",
                "D. Cremers"
            ],
            "title": "Learning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1781\u20131790.",
            "year": 2017
        },
        {
            "authors": [
                "S. Diamond",
                "V. Sitzmann",
                "F. Heide",
                "G. Wetzstein"
            ],
            "title": "Unrolled Optimization with Deep Priors",
            "venue": "arXiv preprint arXiv:1705.08041, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhang",
                "H. Chen",
                "W. Xia",
                "Y. Chen",
                "B. Liu",
                "Y. Liu",
                "H. Sun",
                "J. Zhou"
            ],
            "title": "LEARN++: Recurrent Dual-Domain Reconstruction Network for Compressed Sensing CT",
            "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Su",
                "Q. Lian"
            ],
            "title": "iPiano-Net: Nonconvex optimization inspired multiscale reconstruction network for compressed sensing",
            "venue": "Signal Processing: Image Communication, vol. 89, p. 115989, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Adler",
                "O. \u00d6ktem"
            ],
            "title": "Learned Primal-Dual Reconstruction",
            "venue": "IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1322\u20131332, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Lyons",
                "R.G. Raj",
                "M. Cheney"
            ],
            "title": "CG-Net: A Compound Gaussian Prior Based Unrolled Imaging Network",
            "venue": "2022 IEEE Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2022, pp. 623\u2013629.",
            "year": 2022
        },
        {
            "authors": [
                "C. Lyons",
                "R.G. Raj",
                "M. Cheney"
            ],
            "title": "A Deep Compound Gaussian Regularized Unfoled Imaging Network",
            "venue": "2022 56th Asilomar Conference on Signals, Systems, and Computers, 2022, pp. 940\u2013947.",
            "year": 2022
        },
        {
            "authors": [
                "M. Bertero",
                "C. De Mol",
                "E.R. Pike"
            ],
            "title": "Linear inverse problems with discrete data: II. Stability and regularisation",
            "venue": "Inverse Problems, vol. 4, no. 3, pp. 573\u2013594, Aug 1988.",
            "year": 1988
        },
        {
            "authors": [
                "L. Ying",
                "D. Xu",
                "Z.-P. Liang"
            ],
            "title": "On Tikhonov regularization for image reconstruction in parallel MRI",
            "venue": "Proceedings of the 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, vol. 1. IEEE, 2004, pp. 1056\u20131059.",
            "year": 2004
        },
        {
            "authors": [
                "J.M. Bioucas-Dias",
                "M.A.T. Figueiredo"
            ],
            "title": "A New TwIST: Two-Step Iterative Shrinkage/Thresholding Algorithms for Image Restoration",
            "venue": "IEEE Transactions on Image Processing, vol. 16, no. 12, pp. 2992\u2013 3004, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Dahl",
                "P.C. Hansen",
                "S.H. Jensen",
                "T.L. Jensen"
            ],
            "title": "Algorithms and software for total variation image reconstruction via first-order methods",
            "venue": "Numerical Algorithms, vol. 53, no. 1, pp. 67\u201392, Jan 2010.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Zhang",
                "C. Chen"
            ],
            "title": "Stochastic asymptotical regularization for linear inverse problems",
            "venue": "Inverse Problems, vol. 39, no. 1, p. 015007, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "A. Dogand\u017ei\u0107",
                "A. Nehorai"
            ],
            "title": "Maximum Likelihood Estimation of Compound-Gaussian Clutter and Target Parameters",
            "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 10, pp. 3884\u20133898, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Z. Idriss",
                "R.G. Raj",
                "R.M. Narayanan"
            ],
            "title": "Waveform Optimization for Multistatic Radar Imaging Using Mutual Information",
            "venue": "IEEE Transactions on Aerospace and Electronics Systems, vol. 57, no. 4, pp. 2410\u2013 2425, Aug 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Portilla",
                "V. Strela",
                "M.J. Wainwright",
                "E.P. Simoncelli"
            ],
            "title": "Image Denoising Using Scale Mixtures of Gaussians in the Wavelet Domain",
            "venue": "IEEE Transactions on Image Processing, vol. 12, no. 11, pp. 1338\u20131351, Nov 2003.",
            "year": 2003
        },
        {
            "authors": [
                "T. Huang",
                "W. Dong",
                "X. Yuan",
                "J. Wu",
                "G. Shi"
            ],
            "title": "Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 16 216\u201316 225.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhao",
                "O. Gallo",
                "I. Frosio",
                "J. Kautz"
            ],
            "title": "Loss Functions for Image Restoration With Neural Networks",
            "venue": "IEEE Transactions on Computational Imaging, vol. 3, no. 1, pp. 47\u201357, Mar 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.J. Wright"
            ],
            "title": "Coordinate descent algorithms",
            "venue": "Mathematical Programming, vol. 151, no. 1, pp. 3\u201334, Jun 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Beck",
                "L. Tetruashvili"
            ],
            "title": "On the Convergence of Block Coordinate Descent Type Methods",
            "venue": "SIAM Journal on Optimization, vol. 23, no. 4, pp. 2037\u20132060, Jan 2013.",
            "year": 2013
        },
        {
            "authors": [
                "L. Grippo",
                "M. Sciandrone"
            ],
            "title": "On the convergence of the block nonlinear Gauss\u2013Seidel method under convex constraints",
            "venue": "Operations Research Letters, vol. 26, no. 3, pp. 127\u2013136, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "Learning Multiple Layers of Features from Tiny Images",
            "venue": "University of Toronto, Tech. Rep., 2009.",
            "year": 2009
        },
        {
            "authors": [
                "L. Fei-Fei",
                "R. Fergus",
                "P. Perona"
            ],
            "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
            "venue": "Conference on Computer Vision and Pattern Recognition Workshop, 2004, pp. 178\u2013178.",
            "year": 2004
        },
        {
            "authors": [
                "M.A. Iglesias",
                "K. Lin",
                "S. Lu",
                "A.M. Stuart"
            ],
            "title": "Filter Based Methods for Statistical Linear Inverse Problems",
            "venue": "arXiv preprint arXiv:1512.01955, 2022.",
            "year": 1955
        },
        {
            "authors": [
                "D.P. Kingma",
                "J.L. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "International Conference on Learning Representations, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.G. Baydin",
                "B.A. Pearlmutter",
                "A.A. Radul",
                "J.M. Siskind"
            ],
            "title": "Automatic Differentiation in Machine Learning: a Survey",
            "venue": "Journal of Machine Learning Research, vol. 18, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Song",
                "B. Chen",
                "J. Zhang"
            ],
            "title": "Deep Memory-Augmented Proximal Unrolling Network for Compressive Sensing",
            "venue": "International Journal of Computer Vision, vol. 131, no. 6, pp. 1477\u20131496, Jun 2023.",
            "year": 2023
        },
        {
            "authors": [
                "D.C. Munson",
                "J.D. O\u2019Brien",
                "W.K. Jenkins"
            ],
            "title": "A Tomographic Formulation of Spotlight-Mode Synthetic Aperture Radar",
            "venue": "Proceedings of the IEEE, vol. 71, no. 8, pp. 917\u2013925, Aug 1983.",
            "year": 1983
        },
        {
            "authors": [
                "J.R. Silvester"
            ],
            "title": "Determinants of Block Matrices",
            "venue": "The Mathematical Gazette, vol. 84, no. 501, pp. 460\u2013467, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "W. Kulpa"
            ],
            "title": "The Poincar\u00e9-Miranda Theorem",
            "venue": "The American Mathematical Monthly, vol. 104, no. 6, pp. 545\u2013550, 1997.",
            "year": 1997
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Machine learning, neural networks, inverse problems, nonlinear programming, least squares methods\nNOTATION AND NOMENCLATURE\nR Set of real numbers. y = [yi] \u2208 Rn. Boldface characters are vectors. Y = [Yij ] \u2208 Rn\u00d7m. Uppercase characters are matrices. (\u00b7)T Transpose of vector or matrix (\u00b7). \u2299 Hadamard product. D{v} Diagonal matrix with v1, v2, . . . , vn on the diagonal. Av = AD{v} for matrix A of compatible size. f(v) = [f(vi)] for componentwise function f : R \u2192 R. Pa,b(x) = a+ ReLU(x\u2212 a)\u2212 ReLU(x\u2212 b), for a, b \u2208 R, is\na modified ReLU (mReLU) activation function.\nI. INTRODUCTION\nMOTIVATED by the success of machine learning (ML)in image recognition and advances in deep learning, ML has been extended to various inverse problems \u2013including the signal reconstruction problems of tomographic imaging and compressive sensing. Improving signal reconstruction,\ncarter.lyons@colostate.edu raghu.raj@nrl.navy.mil margaret.cheney@colostate.edu This work was sponsored by the Office of Naval Research via the NRL base program. This material is based upon research supported in part by, the U. S. Office of Naval Research under award number N00014-21-1-2145 and by the Air Force Office of Scientific Research under award number FA9550-21-1-0169.\nthe inverse problem of recovering a signal from undersampled measurements, is an active area of research as it has widespread applicability including radar, sonar, medical, and tomographic imaging. Two prevalent classes of approaches are used in practice for signal reconstruction, namely model-based and data-driven approaches.\nFirst, model-based approaches incorporate certain assumptions, such as the expected prior density or sparsity level of the coefficients, into the signal estimation method. Often, model-based methods apply an iterative algorithm designed to minimize an objective function. Examples include Iterative Shrinkage and Thresholding (ISTA), Bayesian Compressive Sensing (BCS), Basis Pursuit, and Compressive Sampling Matching Pursuit (CoSaMP) [1]\u2013[4]. Many previous works model the prior density, of the signal coefficients, as a generalized Gaussian and solve a corresponding least squares problem with \u2113p regularization [2, 4]\u2013[7]. Alternatively, some previous works consider a compound Gaussian (CG) prior [8, 9], which is a class of densities subsuming generalized Gaussian and other densities as special cases, that better captures statistical properties of the coefficients of images [10]\u2013[12].\nSecond, data-driven approaches learn the signal reconstruction mapping directly by training a standard deep neural network (DNN) on pairs of undersampled signal measurements and signal coefficients. Common DNNs in signal reconstruction are structured upon a convolutional neural network (CNN), recurrent neural network, or a generative adversarial network [13]\u2013[20].\nAlgorithm unrolling is a recent approach to signal reconstruction, stemming from the original work of Gregor and LeCun [21], which combines the model-based and data-driven methods by structuring the DNN layers based on an iterative algorithm. Unlike a standard DNN, which acts as a blackbox process, we have an understanding of the inner workings of an unrolled DNN from understanding the original iterative algorithm. Works utilizing algorithm unrolling have shown excellent performance in signal reconstruction while offering simple interpretability of the network layers [22]. Examples of iterative imaging algorithms that have been unrolled include: ISTA [21, 23]\u2013[25], proximal gradient or gradient descent [26]\u2013[28], the inertial proximal algorithm for nonconvex optimization [29], and the primal-dual algorithm [30].\nWhile many inverse problems are non-linear, we focus on linear inverse problems as this is frequently the assumed measurement model in many applications \u2013 including compressive\n\u00a92023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any\ncopyrighted component of this work in other works.\nar X\niv :2\n30 5.\n11 12\n0v 3\n[ ee\nss .S\nP] 2\n8 N\nov 2\n02 3\n2 sensing, computed tomography (CT), and magnetic resonance imaging (MRI) \u2013 and leave non-linear inverse problem applications to future work. Let x \u2208 Rn be a vectorized signal that has a representation x = \u03a6c with respect to (w.r.t) a dictionary, \u03a6 \u2208 Rn\u00d7n, and coefficients, c \u2208 Rn. An example, \u03a6 is a wavelet transform and c the wavelet coefficients. The linear measurement model is\ny = \u03a8\u03a6c + \u03bd (1)\nwhere y \u2208 Rm are the measurements produced by observing x through a sensing matrix, \u03a8 \u2208 Rm\u00d7n, with additive white noise, \u03bd \u2208 Rm. For many applications of interest m \u226a n. Signal reconstruction, or estimation, aims to recover c given y , \u03a8, and \u03a6."
        },
        {
            "heading": "A. Contributions",
            "text": "In this work, we expand on initial work from [31, 32] by: 1) Construct a novel iterative signal estimation algorithm,\nnamed compound Gaussian least squares (CG-LS), to solve (1) with general \u03a8 and \u03a6 matrices. A regularized least squares (RLS) optimization is the foundation for CG-LS where the regularization enforces a CG prior on the signal coefficients. We provide experimental results illustrating the effectiveness of CG-LS in linear tomographic imaging. 2) Develop a DNN by unrolling CG-LS. The new network, which we name CG-Net, is, to the best of our knowledge, the first DNN for general linear inverse problems to be fundamentally informed by a CG prior. The efficacy of CG-Net to reconstruct images after training is evaluated and shown to outperform other state-of-the-art deep learning methods in the low training scenario. 3) Provide fundamental characterization of the existence and location of minimizers of the CG-LS cost function. 4) Derive convergence analysis of CG-LS to stationary points of the cost function under a two-block coordinate descent with one block estimated via steepest descent.\nSpecifically, as compared to [31, 32], our work here generalizes the CG-LS and CG-Net implementations by replacing Newton\u2019s descent with a generic steepest descent option providing greater flexibility and learning capacities. Additionally, theoretical analysis and a thorough empirical validation of CGLS, absent from [31, 32], is provided here. Finally, significant additional validation of CG-Net, including the use of other datasets and evaluating on alternative reconstruction problems, is supplied here with vastly more comparisons to prior art."
        },
        {
            "heading": "B. Compound Gaussian Prior",
            "text": "A fruitful way to formulate inverse problems in imaging is by Bayesian estimation. In particular, consider the RLS estimate of c from (1) c\u0302 = argmin ||y \u2212 \u03a8\u03a6c||22 + R(c), which can, equivalently, be viewed as a maximum a posteriori (MAP) estimation when the regularization satisfies R(c) \u221d log(p(c)). Therefore, the choice of regularization, R(c), or prior density, p(c), of the coefficients, c, is a crucial\ncomponent to incorporate domain level knowledge into the signal reconstruction problem. Many previous works have employed a generalized Gaussian prior including a Gaussian prior [33, 34], corresponding to a Tikhonov regression, or a Laplacian prior [1, 2, 4]\u2013[6, 35], as is predominant in the compressive sensing (CS) framework. Additional regularizations, not derived from a specific prior density, have been implemented for signal reconstruction including total variation norm [35, 36], stochastic based regularization [37], and deep learning-based regularization [16, 26, 27].\nThrough the study of the statistics of image sparsity coefficients, it has been shown that coefficients of natural images exhibit self-similarity, heavy-tailed marginal distributions, and self-reinforcement among local coefficients [11]. Such properties are not encompassed by the generalized Gaussian prior typically assumed for the image coefficients. Instead, a class of densities known as CG densities [38], or Gaussian scale mixtures [10, 11], better capture statistical properties of natural images and signals from other modalities such as radar [12, 39]. A useful formulation of the CG prior lies in modeling the coefficients of signals as\nc = z \u2299 u (2)\nsuch that z = h(\u03c7), where h : R \u2192 R is a componentwise, positive, nonlinear function, \u03c7 follows a multi-scale Gaussian tree process [8, 11], u \u223c N (0,\u03a3u), and u and z are independent random variables. In Appendix VI-E, Proposition 7 shows the CG prior subsumes many well-known distributions including the generalized Gaussian. Additionally, Proposition 8 in Appendix VI-E provides the specific nonlinearity, h, producing a Laplace prior allowing an interpretation of the CG prior as a generalization of CS work. Previously, the CG prior has been used, with h(x) =\u221a exp(x/\u03b1) for \u03b1 \u2208 (0,\u221e), in a hierarchical Bayesian MAP estimate of wavelet and discrete cosine transformation (DCT) coefficients [8, 9]. This algorithm produces reconstructed images with superior quality, measured by the structural similarity index (SSIM), over other state-of-the-art CS techniques [8]. Furthermore, the CG prior, under a single random scale variable has been successfully used for image denoising [40] and hyperspectral image compressive sensing [41]."
        },
        {
            "heading": "C. Deep Neural Networks and Algorithm Unrolling",
            "text": "A DNN is a collection of ordered layers, denoted L0,L1, . . . ,LK for K > 1, where successive layers feed into one another from the input layer, L0, to the output layer, LK . Intermediate layers L1, . . . ,LK\u22121 are known as hidden layers. Each layer, Lk, contains dk hidden units [42], which are assigned a computed real-value when a signal is transmitted through the DNN.\nA function fk : Rdi1(k) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Rdij(k) \u2192 Rdk , that is parameterized by some \u03b8k, defines the computation, i.e. signal transmission, at layer Lk where Ik := {i1(k), . . . , ij(k)} \u2286 {0, 1, . . . , k \u2212 1} are the indices of layers that feed into Lk. That is, given an input signal y\u0303 \u2208 Rd0 assigned to L0, a DNN\n3 is a composition of parameterized vector input and vector output functions where\nLk \u2261 fk ( Li1(k), . . . ,Lij(k);\u03b8k ) \u2208 Rdk .\nFully connected networks, as an example, use Ik = {k \u2212 1} and Lk \u2261 f k(Lk\u22121;\u03b8k = [Wk, bk]) = \u03c3(WkLk\u22121 + bk) where Wk \u2208 Rdk\u22121\u00d7dk , bk \u2208 Rdk , and \u03c3 is a componentwise activation function.\nA DNN learns, or trains, its parameters, \u0398 = (\u03b81, . . . , \u03b8K), by optimizing a loss function L(\u0398) over a training dataset D = {(y\u0303i, c\u0303i) : i = 1, 2, . . . , Ns} where each (y\u0303i, c\u0303i) satisfies equation (1). Let c\u0303(y\u0303i;\u0398) denote the DNN output given the input y\u0303i. Then the loss function is defined as L(\u0398) := 1Ns \u2211Ns i=1 L (\u0303c(y\u0303i;\u0398), c\u0303i) where L (\u0303c(y\u0303i;\u0398), c\u0303i) is the loss, or error, between the network output, c\u0303(y\u0303i;\u0398), and the actual coefficients, c\u0303i. Common loss functions for image reconstruction neural networks include mean-squared error (MSE), mean-absolute error (MAE), peak signal-to-noise ratio (PSNR), or SSIM [43].\nAlgorithm unrolling creates a DNN by assigning the operations from each step k of the iterative algorithm as the function f k defining layer k. That is, layer k in the DNN should correspond to the output of k iterations of the original iterative algorithm. Then parameters, \u03b8k, on each step k of the iterative algorithm parameterize f k in the DNN [21, 22]. In training the unrolled DNN, each \u03b8k is learned, which optimizes the iterative algorithm to produce improved signal estimates.\nII. ITERATIVE ALGORITHM (CG-LS)\nLet h be the componentwise, invertible, nonlinear function in the CG prior and f = h\u22121. Defining A = \u03a8\u03a6, we consider the RLS cost function and estimate given respectively by\nF (u,z) = ||y \u2212A(z \u2299 u)||22 + \u03bb||u||22 + \u00b5||f(z)||22 (3)[ u\u2217 z\u2217 ] = argmin\n[u z]\nF (u,z). (4)\nOur CG-LS, given in Algorithm 1, is an iterative algorithm that, approximately, solves (4). The cost function (3) is a RLS where, as given by the CG prior, the coefficients are decomposed as c = z\u2299u and the regularization is taken to be R(c) = R(u,z) = \u03bb||u||22 + \u00b5||f(z)||22 to enforce normality of u and \u03c7 = f(z), a Gaussian tree process, as desired from the CG prior. We note that R(c) is not exactly proportional to log(p(c)) as specified in the MAP estimate. Instead, R(c) is an approximation capturing important statistical properties of the CG prior while also simplifying the optimization.\nDue to the explicit joint estimation in (4), we optimize by block coordinate descent [44], which on iteration k produces\nzk = argmin z\u2208Zn\n||y \u2212Auk\u22121z ||22 + \u00b5||f(z)||22 (5)\nuk = argmin u\u2208Rn\n||y \u2212Azku||22 + \u03bb||u||22 (6)\nwhere Z \u2286 [0,\u221e) is the domain of f . Convergence rates of block coordinate descent under different conditions for the cost function, such as convexity, have been proven [44]\u2013[46]. As the optimization in (5) cannot be solved analytically, for most choices of f , we implement the steepest descent method to\niteratively and approximately solve (5). Recall [47] that given a norm ||\u00b7|| on Rn and differentiable function, g(x) : Rn \u2192 R, the steepest descent vector, d : Rn \u2192 Rn, is\nd(x) = ||\u2207g(x)||\u2217 ( argmin ||v||=1 \u2207g(x)Tv ) (7)\nwhere || \u00b7 ||\u2217 is the dual norm given by ||w||\u2217 = max ||v||=1 wTv. For instance the Euclidean norm produces d(x) = \u2212\u2207g(x). Applying steepest descent to (5) we write zjk as the estimate of z on steepest descent step j of iteration k. For generality, we assume that a different norm may define each steepest descent step as is the case in Newton\u2019s descent for a convex cost function with non-constant Hessian. Let djk = d j k(z) denote the descent vector corresponding to norm || \u00b7 ||(k,j), with dual norm ||\u00b7||\u2217(k,j), for steepest descent step j of iteration k. Thus, zjk is given by\nzjk = z j\u22121 k + \u03b7 (j) k d j k(z j\u22121 k )\nwhere \u03b7(j)k is a step size determined by a backtracking line search [47]. Note, djk = d j k(z) = d j k(z ;uk\u22121, y) as d j k is parameterized by uk\u22121 and y . Let J be the maximum number of steepest descent steps; then, for notation, we take zJk = zk = z 0 k+1 for all k > 0.\nEquation (6) is the well-known Tikhonov solution\nuk = (A T zk Azk + \u03bbI) \u22121ATzky \u2261 A T zk (AzkA T zk + \u03bbI)\u22121y (8)\nwhere the second equality results from the Woodbury matrix identity. Note that we do not calculate the inverse in (8) and instead solve a system of linear equations.\nNext, define the initial estimate of z as z0 = Pa,b(ATy) where the mReLU function Pa,b is applied elementwise to ATy . We remark that Pa,b is a projection operator onto the interval [min{a, b},max{a, b}]. This eliminates negative values, as z should have positive components, and limits the maximum values in the initial z estimate for stability. The initial u estimate, denoted as u0, is given by (8).\nAlgorithm 1 Compound Gaussian Least Squares (CG-LS)\n1: z0 = Pa,b(ATy) and u0 = (ATz0Az0 + \u03bbI) \u22121ATz0y 2: for k \u2208 {1, 2, . . . ,K} do 3: z ESTIMATION: 4: z0k = zk\u22121 5: for j \u2208 {1, 2, . . . , J} do 6: if ||\u2207zF (uk\u22121, zj\u22121k )||2\u2217(k,j) < \u03b4 then 7: return zk = z j\u22121 k 8: end if 9: Compute step size \u03b7(j)k (backtracking line search)\n10: Compute descent vector djk = d j k(z j\u22121 k ;uk\u22121, y) 11: zjk = z j\u22121 k + \u03b7 (j) k d j k(z j\u22121 k ;uk\u22121, y) 12: end for 13: zk = z J k 14: u ESTIMATION: 15: uk = (A T zk Azk + \u03bbI)\n\u22121ATzky 16: end for Output: c\u2217 = zK \u2299 uK\n4 Finally, the gradient, \u2207zF (u,z), and a user-chosen parameter \u03b4 > 0 determine convergence of CG-LS. On each steepest descent step j of iteration k, we check if ||\u2207zF (uk\u22121, zj\u22121k )||\u2217(k,j) < \u03b4. When this holds, we exit the steepest descent steps taking zk = z j\u22121 k . Once ||\u2207zF (uk\u22121, z0k)||\u2217(k,1) < \u03b4 we say CG-LS has converged and return estimates uk\u22121 and zk\u22121. Otherwise, CG-LS terminates after a user-chosen maximum number of iterations K."
        },
        {
            "heading": "A. Existence and Location of Minimizers",
            "text": "Here, we discuss the existence of minimizers to (3) along with details on their location that can be ensured through a sufficient scaling on the measurements, y , or proper choices of CG-LS parameters, \u03bb and \u00b5. We remark that (3) is strongly convex in the u block, with parameter at least 2\u03bb, and thus has no local maxima.\nThroughout the remainder of this paper we will assume that f : Z \u2192 R is a C2 function defined on Z = (zmin,\u221e). While in theory f should be an invertible function, we will not require this. Additionally, we assume that f is coercive on Z , that is lim\nz\u2192zmin f(z) \u2192 \u00b1\u221e and lim z\u2192\u221e f(z) \u2192 \u00b1\u221e. Now, we\ngive a necessary and sufficient condition that any stationary point of (3) must satisfy.\nLemma 1. Define v : Zn \u2192 Rn and F : Zn \u2192 Rn by\nv(z) = AT (AzA T z + \u03bbI) \u22121y (9) F (z) = \u22122\u03bbz \u2299 v(z)\u2299 v(z) + 2\u00b5f \u2032(z)\u2299 f(z). (10)\nThen (3) has a stationary point [u\u2217, z\u2217] if and only if F (z\u2217) = 0 and u\u2217 = z\u2217 \u2299 v(z\u2217).\nProof. Solving \u2207uF (u,z) = 0 and applying the Woodbury matrix identity gives u = z \u2299 v(z). Next, applying the Woodbury matrix identity to \u2207zF (z\u2299v(z), z) produces (10). Therefore, \u2207uF (u,z) = \u2207zF (u,z) = 0 if and only if z = z\u2217 is a root of (10) and u = z\u2217 \u2299 v(z\u2217).\nWe remark that if f(z)2 is strictly decreasing on an interval I then, from Lemma 1, z\u2217 \u2208 Zn \\In for any stationary point [u\u2217, z\u2217] of (3). A particular application is when f is invertible and obtains a root, at z0, on Z then z\u2217 \u2208 [z0,\u221e)n.\nNext, we assume that the measurements, y , have been scaled by a positive constant s. That is, y = sy\u0303 for y\u0303 given by (1). Under certain conditions and choices of s, \u03bb, and \u00b5 we gain significant insight into the location of a minimizer of (3).\nProposition 2. Let f2(z) be strictly convex and obtain an interior local minimum, at z0, on [a, b] \u2286 Z . Then there exists positive s, \u03bb, and \u00b5 such that (3) has a non-degenerate local minimizer [u\u2217, z\u2217] where z\u2217 \u2208 [z0, b]n and u\u2217 = z\u2217 \u2299 v(z\u2217).\nProof of Proposition 2 is given in Appendix VI-A. We remark that Proposition 2 extends optimization insights from the single variable function f2(z) to the multivariate cost function (3) that involves z and u. In particular, Proposition 2 is useful for choosing the correct scaling constant to match the mReLU interval, which is informed by intervals of convexity and roots of f2(z), defining the initial estimate of z in Algorithm 1. Additionally, the convexity of f2(z) in conjunction with the\nscaling law revealed in Proposition 2 guarantees convexity of (5) and the existence of a unique stationary point within this convex region."
        },
        {
            "heading": "B. Convergence of CG-LS",
            "text": "For any norm, ||\u00b7||, on Rn we remark that a Euclidean lower bound exists. That is, there exists a constant 0 < \u03b3 \u2264 1 such that || \u00b7 || \u2265 \u03b3|| \u00b7 ||2. Thus, for every CG-LS steepest descent norm || \u00b7 ||(k,j) with dual norm || \u00b7 ||\u2217(k,j) we let \u03b3(k,j) and \u03b3\u2217(k,j) be the respective Euclidean bound constants.\nTo show Algorithm 1 converges, we first give a lower bound on the change in the cost function for a steepest descent step on z . To simplify notation, define the function Gk(z) = F (uk\u22121, z).\nProposition 3. For every k, j \u2208 N, there exists a positive constant, c(k,j), such that Algorithm CG-LS satisfies\nGk ( zj\u22121k ) \u2212Gk ( zjk ) \u2265 c(k,j) \u2223\u2223\u2223\u2223\u2223\u2223\u2207Gk (zj\u22121k )\u2223\u2223\u2223\u2223\u2223\u22232\u2217(k,j) . (11) A proof of Proposition 3 is given in Appendix VI-B. From Proposition 3, the sequence of cost function values, {Gk(zjk)}\u221ej=0, monotonically decreases, and when the gradient is large, we expect a large decrease in the cost function. Since Gk(z) \u2265 0, we know the sequence {Gk(zjk)}\u221ej=0 converges. That is, each steepest descent process in CG-LS will converge. To show the convergence of CG-LS to a stationary point of (3), we apply Proposition 3 and similarly bound the change in the cost function over an iteration of CG-LS.\nTheorem 4. Assume the Euclidean bound sequences {\u03b3(k,1)}\u221ek=1 and {\u03b3\u2217(k,1)}\u221ek=1 are bounded below by \u03b3 > 0. Then CG-LS converges to a stationary point of (3). Furthermore, the sequence of minimum gradient dual norms satisfies\nmin 1\u2264k\u2264K\n||\u2207F (uk, zk)||2\u2217(k,1) \u2264 O ( 1\nK\n) .\nA proof of Theorem 4 is in Appendix VI-C and shows that CG-LS generates a monotonic decreasing sequence of cost function values. Combining Theorem 4 with the fact that (3) has no local maxima, Algorithm 1 is guaranteed to converge to a local minimum or saddle point of (3). As (3) is, in general, not convex, we cannot guarantee that Algorithm 1 converges to a local nor global minimum. Although, if Proposition 2 is also satisfied, we can guarantee convergence to a global minimizer of (3) relative to [z0, b]n \u00d7 Rn.\nWe further strengthen our convergence results when Proposition 2 is satisfied. Let \u03b3 be given as in Theorem 4 and c from (26).\nTheorem 5. Let Proposition 2 be satisfied with local minimizer [u\u2217, z\u2217] corresponding to minimum value F \u2217. Then there exists a region C \u2286 Rn \u00d7 Zn such that [u\u2217, z\u2217] \u2208 C and (3) is strongly convex on C with constant \u2113. Furthermore, for any [u0, z0] \u2208 C we have for all k \u2265 0\nF (uk, zk)\u2212 F \u2217 \u2264 (1\u2212 2\u2113\u03b32c)k(F (u0, z0)\u2212 F \u2217). (12) A proof of Theorem 5 is in Appendix VI-D. Theorem 5 shows a region around the local minimizer where (3) is strongly convex exists and thus we can guarantee a linear rate of convergence of the cost function values in this region.\n5 (a) Original (b) Radon Transform (c) gCG-LS (0.952)\n(d) nCG-LS (0.953) (e) FISTA (0.896) (f) \u21131-LS (0.849)\n(g) FBP (0.844) (h) BCS (0.827) (i) CoSaMP (0.838)\nFig. 1: Image reconstructions (SSIM) using our gCG-LS, our nCG-LS, FISTA, \u21131-LS, FBP, BCS, and CoSaMP. The input to each algorithm is a vectorized (1b), which is a Radon transform of (1a) at 15 uniformly spaced angles with an SNR of 60dB. We observe that CG-LS outperforms all methods with nCG-LS slightly outperforming gCG-LS."
        },
        {
            "heading": "C. Numerical Results",
            "text": "We test the CG-LS algorithm using gradient descent, which we denote by gCG-LS, and Newton descent, which we denote by nCG-LS, as the steepest descent method for updating z . Note, the gradient and Hessian of (3) w.r.t z are, respectively:\n\u2207zF (u,z) = \u22122ATu (y \u2212Auz) + 2\u00b5f \u2032(z)\u2299 f(z) (13) HF ;z(u,z) = 2A T uAu + 2\u00b5D{hf (z)} (14)\nwhere\nhf (z) = f \u2032\u2032(z)\u2299 f(z) + f \u2032(z)\u2299 f \u2032(z). (15)\nWe use 32 \u00d7 32 images from the CIFAR10 [48] image dataset, 64 \u00d7 64 images from the CalTech101 [49] image dataset, and the 11 images from the Set11 [23] dataset resized to 128 \u00d7 128. Each image has been converted to a singlechannel grayscale image, scaled down by the maximum pixel value, and vectorized. A Radon transform, at a specified number of uniformly spaced angles, is performed on each image to which white noise is added producing noisy measurements, y , at a specified signal-to-noise ratio (SNR). Finally, a biorthogonal wavelet transformation is applied to each image to produce the coefficients, c.\nFor all simulations, we use f(z) = ln(z), \u00b5 = 2, K = 1000, J = 1, and \u03b4 = 10\u22126. For measurements at an SNR of 60dB and 40dB, we take \u03bb = 0.3 and \u03bb = 2, respectively.\nWe remark that for the 128 \u00d7 128 Set11 reconstructions we instead take K = 100 and, in the 40dB case, \u03bb = 30. Using nCG-LS requires (14) to be positive definite in the z variable, which for f(z) = ln(z), is guaranteed when z \u2208 (0, e)n. Informed by Proposition 2, we can guarantee a local minimizer lies in [1, e)n under sufficient scaling of the input data. Therefore, in each nCG-LS test using 32 \u00d7 32 images, we scale the input measurement by a factor, chosen empirically, of e\u22124. Similarly, in each nCG-LS test on larger images we scale the input measurement by e\u22126. Additionally, we use an eigendecomposition on (14) to find the closest positive semidefinite matrix that is then used in the Newton descent step. Alternatively, we remark that the mReLU function P1,e may be applied at each z update in line 11 of Algorithm 1 to ensure (14) is positive semi-definite. Finally, we choose Pa,b = P1,e for nCG-LS whereas for gCG-LS we use Pa,b = P1,e2 .\nFig. 1 contains the reconstruction of a 32 \u00d7 32 Barbara snippet from seven iterative algorithms: gCG-LS, nCGLS, Fast Iterative Shrinkage and Thresholding Algorithm (FISTA) [2], \u21131-least squares (\u21131-LS) [6], Fourier backprojection (FBP) [50], BCS [3], and CoSaMP [4]. Each algorithm takes as input (1b) vectorized, which is a 60dB SNR Radon transform at 15 uniformly spaced angles. In (1b) the angle and distance are given on the x and y axes, respectively.\nSimilarly, Fig. 2 and Fig. 3 respectively contain the reconstruction of a 64 \u00d7 64 Barbara snippet and 128 \u00d7 128 boat\n6\nimage from a 60dB SNR Radon transform at 15 uniformly spaced angles. We see by visual inspection and SSIM that both gCG-LS and nCG-LS produce superior reconstructions to the other iterative algorithms with nCG-LS slightly outperforming gCG-LS.\nThe superiority of CG-LS is further highlighted in Table I, which shows the average image reconstruction SSIM and PSNR along with 99% confidence intervals. Note, larger SSIM and PSNR values correspond to image reconstructions that are visually closer to the original image. One reason our method outperforms the comparative methods may be attributed to the relatively low sparsity level of the wavelet coefficients, c, which CG-LS can manage while \u21131-LS, BCS, and CoSaMP require a sufficiently high signal sparsity for superb performance. Although, CG-LS is slower, as shown in Table V.\nLastly, we remark that we tested a Kalman Filter approach [51] and found that it underperformed in general inverse problems. For example, in CS experiments the Kalman approach underperformed while being relatively competitive to the prior art in Radon inversion problems. A thorough examination of Kalman based approaches to general inverse problems is a subject of future work."
        },
        {
            "heading": "III. CG-NET",
            "text": ""
        },
        {
            "heading": "A. Network Structure",
            "text": "We create a DNN by applying algorithm unrolling to CG-LS in Algorithm 1. CG-Net has a structure shown in Fig. 4 and consists of an input layer, L0, an initialization layer, Z0, an initial u layer U0, K blocks given in Fig. 4b, and an output layer, O. Two main functions, f k : Rn \u00d7 Rm \u2192 Rn and\n7 (a) End-to-end network structure of CG-Net.\n(b) Block k of CG-Net (c) Mathematical descriptions of the CG-Net layers.\nFig. 4: End-to-end network structure for CG-Net, the unrolled deep neural network of Algorithm 1, is shown in (4a). A mathematical description of each layer is given in (4c). CG-Net consists of an input layer, L0, initialization layer, Z0, output layer, O, and K CG-Net blocks (4b). Each CG-Net block, k, contains J steepest descent layers, Z1k , . . . , Z J k , and a single Tikhonov layer, Uk. Every layer takes the measurements, L0 \u2261 y , as an input so these connections are omitted for clarity.\ngjk : Rn \u00d7 Rn \u00d7 Rm \u2192 Rn, define the CG-Net layers. Each f k(z,y) \u2261 f k(z,y;\u03bbk), which is parameterized by a scalar \u03bbk, corresponds to updating u as\nf k(z,y) := A T z (AzA T z + \u03bbkI) \u22121y.\nNext, each gjk(z,u,y) \u2261 g j k(z,u,y; a j k, b j k, \u03b7 (j) k , d j k), which is parameterized by a descent vector, djk, step size, \u03b7 (j) k and mReLU parameters ajk and b j k, corresponds to updating z as\ngjk(z,u,y) := Pajk,bjk ( z + \u03b7 (j) k d j k(z ;u,y) ) . (16)\nIn CG-LS, the step size, \u03b7(j)k , is found by a backtracking line search, which we cannot implement in CG-Net. Thus, the application of the mReLU activation function, Pa,b, at each steepest descent step serves to guarantee the next step is not too large and stays within Z .\nNow, we mathematically detail the CG-Net layers: L0 = y is the input measurements to the network\nZ0 = Pa0,b0(A\u0302Ty), for A\u0302 = A/||A||2, is the initial estimate of z from line 1 of Algorithm 1. U0 = f 0(Z0, y) is the initial estimate of u corresponding to line 1 of Algorithm 1.\nThe kth CG-Net block, shown in Fig. 4b, consists of layers: Zjk = g j k(Z j\u22121 k , Uk\u22121, y) is z on steepest descent step j of\niteration k, corresponding to line 11 in Algorithm 1. Uk = f k(Z J k , y) is u on iteration k, corresponding to line\n15 in Algorithm 1. O = UK \u2299 ZJK is the estimated wavelet coefficients\nproduced by CG-Net.\nNote, to simplify notation, we let Z0k = Z J k\u22121. In total, CGNet has K(J + 1) + 4 layers: K + 1 u update layers, KJ steepest descent z layers, one input, output, and initialization layer.\nBy incorporating the CG prior, CG-Net differentiates from previous unrolled DNNs in two key ways. First, the Tikhonov updates of u provide non-linear transformation layers of z . These layers impose a significant structure via data consistency\nin equating measurements and measured estimated signals. Second, instead of estimating the original signal of interest directly, CG-Net simultaneously estimates two separate signals and formulates its output as a Hadamard product, which can be viewed as a unique output activation function. These network attributes, inspired by the theory in Sections II-A and II-B, are shown to be advantageous empirically in Section III-C."
        },
        {
            "heading": "B. Network Parameters and Loss Functions",
            "text": "We further detail the parameters that will be learned by CGNet. For every k = 0, 1, . . . ,K the layer Uk is parameterized by regularization scalar, \u03bbk > 0. In Algorithm 1, every \u03bbk is taken to be the same constant, \u03bb from (3), but we increase the trainability of CG-Net by allowing different \u03bbk at each layer updating u. The initialization layer, Z0, is parameterized by two positive real numbers a0 > zmin and b0 > zmin, which are applied through the mReLU function, Pa0,b0 .\nNext, for each Zjk layer, defined by (16), we implement the steepest descent vector djk(z ;u) = \u2212B j k\u2207zF (z ;u) for a positive definite matrix Bjk that will be learned in CG-Net. Note, this descent vector is the steepest descent based upon the quadratic norm || \u00b7 ||2\n(Bjk) \u22121 . Thus, CG-Net can be understood\nas learning the quadratic norm defining every steepest descent step that optimally traverses the landscape of the cost function in (5). For matrix L, let Q and \u039b be the eigendecomposition of (L + LT )/2. That is, (L + LT )/2 = Q\u039bQT . Define, for small \u03f5 > 0, the diagonal matrix \u039b\u03f5 as [\u039b\u03f5]ii = max{\u039bii, \u03f5} and let\nP\u03f5(L) = Q\u039b\u03f5Q T . (17)\nThat is, P\u03f5(L) can be viewed as the closest symmetric, realvalued matrix with minimum eigenvalue of \u03f5 to (L + LT )/2 as measured by the Frobenius norm.\nWe enforce Bjk to be positive definite by learning a lower triangular matrix Ljk and setting B j k = P\u03f5(L j k). Therefore, CGNet layer Zjk is parameterized by a lower triangular matrix L j k defining the steepest descent vector\ndjk(z ;u) = \u2212P\u03f5(L j k)\u2207zF (u,z).\n8 Note, while the entire matrix Ljk could be learned, we set CG-Net to learn only the diagonal and sub-diagonal in Ljk constraining Bjk to be a tridiagonal matrix.\nAdditionally, as \u2207zF (u,z), given in (13), depends on the regularization scalar \u00b5, layer Zjk is parameterized by regularization scalar \u00b5jk. Furthermore, layer Z j k is parameterized by the step size \u03b7(j)k , which we take to be a diagonal matrix. That is, instead of learning a single constant to scale the steepest descent vector djk we learn a different constant to scale each component of djk separately. Finally, layer Z j k learns positive real numbers ajk > zmin and b j k > zmin, which are applied through the mReLU activation function, Pajk,bjk , in (16). Fix a small real-valued \u03f5 > 0. We remark that to ensure \u03bbk > 0 and a0, b0, a j k, b j k > zmin in implementation we use max{\u03bbk, \u03f5} in place of \u03bbk, max{a0, zmin+ \u03f5} in place of a0, and similarly for b0, a j k, and b j k.\nWhen only the diagonal and sub-diagonal in Ljk are learned then CG-Net has K(J(3n+ 2) + 1) + 3 parameters\n\u0398 = { \u03bb0, a0, b0, \u03bbk, \u00b5 j k, L j k, \u03b7 (j) k , a j k, b j k }j=1,2,...,J k=1,2,...,K\nwhere n is the image size. The CG-Net parameters are trained by minimizing a loss function involving the SSIM image quality metric [43], namely for B \u2282 D a batch of data points\nLB(\u0398) = 1 |B| \u2211\n(y\u0303i ,\u0303ci)\u2208B\n(1\u2212 SSIM(\u03a6c\u0303(y\u0303i;\u0398),\u03a6c\u0303i)) .\nWe note that the MAE loss function is an adequate alternative providing nearly identical results for CG-Net. The SSIM loss function is optimized through adaptive moment estimation (Adam) [52], which is a stochastic gradient-based optimizer. The gradient \u2207\u0398LB for Adam is calculated via backpropagation through the network, which we implement with automatic differentiation [53] using TensorFlow."
        },
        {
            "heading": "C. Numerical Results",
            "text": "Given a sensing matrix, \u03a8, dictionary, \u03a6, and noise level in SNR, we create a set of training and testing measurementcoefficient pairs, (y,c), as in Section II-C, that we use to train and evaluate a CG-Net. Measurements are formed from 32\u00d732 CIFAR10 [48] images and 64 \u00d7 64 CalTech101 [49] images. For network size, CG-Net running on 32 \u00d7 32 or 64 \u00d7 64 image measurements use (K,J) = (20, 1) or (K,J) = (5, 1), respectively. The network sizes were chosen empirically such that the time to complete one image reconstruction was reasonably quick while still producing excellent reconstructions on a validation set of test images. We let f(z) = ln(z) and initialize a0 = 1, b0 = exp(2), every \u03b7 (j) k = 1 2I , all \u00b5 j k = 2, each ajk = 8 10 , all b j k = exp(3), and every L j k = I . Finally, we initialize \u03bbk = 0.3 for 60dB SNR noise level and \u03bbk = 2 all other noise levels. Each CG-Net was trained for 30 epochs using a learning rate of 10\u22123 with early stopping.\nWe compare CG-Net against nine state-of-the-art methods: memory augmented deep unfolding network (MADUN) [23], ISTA-Net+ [24], FISTA-Net [25], iPiano-Net [29], ReconNet [13], LEARN++ [28], Learned Primal-Dual (LPD) [30],\nFBPConvNet [15], and iRadonMAP [14]. Additionally, memory augmented proximal unrolled network (MAPUN) [54] was considered, but due to the similarity in performance to MADUN only MADUN results are shown. Note, LEARN++, LPD, FBPConvNet, and iRadonMAP are CT-specific reconstruction methods relying on the structure of the CT sinogram measurements. Instead MADUN, ISTA-Net+, FISTANet, iPiano-Net, and ReconNet are linear inverse problem reconstruction methods with particular application in image compressive sensing. Furthermore, we remark that MADUN, ISTA-Net+, FISTA-Net, iPiano-Net, LEARN++, and LPD are DNNs formed by algorithm unrolling while ReconNet, iRadonMAP, and FBPConvNet are instead standard DNNs.\nFor every set of training data, each method was trained using early stopping. That is, as shown in Fig. 9, training was conducted until the model initially overfits as compared to a validation dataset. In doing so, we ensure every model is sufficiently trained while also not being over trained thereby presenting the best performance for each model for the provided set of training data.\nShown in Fig. 5 is the average SSIM quality, over 200 test image reconstructions, for CG-Net and the comparison methods when each is trained on a varying amount of training data. We consider small sets of training data specifically of size 1000, 500, 100, and 20 measurement, coefficient pairs. Note, ReconNet, iRadonMAP, and some instances of FBPConvNet perform significantly lower and are thus omitted from Fig. 5. In Fig. 5a, 5b, and 5c we see when reconstructing images from Radon transforms \u2013 at 15, 10, or 6 uniformly spaced angles, respectively \u2013 with an SNR of 60dB that CG-Net outperforms all comparative methods and does so appreciably in low training.\nThe smallest training dataset, of size 20, is further detailed in Table II, which displays the average SSIM and PSNR plus 99% confidence intervals over 200 test image reconstructions for CG-Net and each comparison method. We see in Table II that CG-Net significantly outperforms all comparative methods in this low training scenario. While the values in Table II may seem low, they are a result of training the models only on a very small training dataset. To this point, while CG-Net does not appreciably outperform CG-LS in the lowest training scenario it will do so when provided enough training data. In particular, we see in Fig. 5 and in Table III that with over 100 training samples CG-Net will outperform CG-LS. Nevertheless, as highlighted in Section IV, CG-Net provides a significant reconstruction time improvement over CG-LS while still providing comparable reconstruction quality even in low training.\nWe believe the high performance of CG-Net in low training scenarios is due to a couple of factors. First, the initialization of CG-Net corresponds precisely to CG-LS and, unlike other algorithm unrolling methods, we do not replace any part of the optimization with a CNN or other subnetwork structure. As these optimization-replacing subnetworks are learned completely from scratch, they can overfit quickly in low training. Second, CG-Net naturally incorporates the powerful statistical CG prior through the Tikhonov estimates and Hadamard product output, which provides beneficial data-\n9\nconsistency solution structure for low training. However, with greater noise and higher training, CG-Net can perform lower than the best performing comparative methods, possibly due to the enforced structure that makes CG-Net so successful in low training. Another consideration, as shown in Table VI, is that the model complexity of CG-Net may need to be increased in higher training by increasing the number of unrolled iterations.\nFor a visual comparison of the methods, Fig. 6 shows the reconstructions, plus SSIM values, of a 32\u00d732 dog image from a 60dB SNR Radon transform at 10 uniformly spaced angles.\n10\nNext, Fig. 7 shows the reconstructions of a 32\u00d732 truck image from a 60dB SNR Radon transform at 6 uniformly spaced angles. Finally, Fig. 8 shows the reconstructions of a 32\u00d7 32 and 64 \u00d7 64 Barbara snippet each from a 60dB SNR Radon transform at 15 uniformly spaced angles. All reconstructions are performed after training on a dataset of only 20 samples. In all figures, we see CG-Net producing higher quality images visually and by SSIM than the nine comparison methods.\nTo further highlight the applicability of CG-Net we consider alternative sensing matrices, \u03a8, and dictionaries, \u03a6 as summarized in Table III. As typical in CS applications, we consider \u03a8 \u2208 Rm\u00d7n as a Gaussian matrix where mn is the sampling ratio. Also, we consider a DCT as an alternative representation dictionary. The training dataset for each experiment consists of 2000 measurement, coefficient pairs. Note, for each CS problem, we initialize every \u03bbk = 102.\nTable III provides the average SSIM and PSNR, across 200 test image reconstructions, with 99% confidence intervals for various \u03a8 and \u03a6. As LEARN++, LPD, FBPConvNet, and iRadonMAP are CT-specific reconstructions the CS problem is not directly applicable and thus these comparisons are omitted from Table III. Again, CG-Net outperforms or performs comparably to the competitive methods in these alternative sensing matrices and dictionary schemes. We remark that an advantage of CG-Net is its applicability to any general linear inverse problem while many of the comparison methods are only for the specific problem of image estimation. It remains a point of future work to study CG-Net and the comparative methods for non-image estimation tasks.\nLastly, as shown in Fig. 5, Table II, and Table III the unrolled DNN methods have an advantage over standard DNN methods in low training scenarios. This is perhaps unsurprising\n11\ngiven that the unrolled methods enforce some solution structure, through data consistency layers (typically a gradient step on a data fidelity measure), providing better initialization over standard DNN methods with no such required structure. That is, with no training the reconstructions from an unrolled DNN method, generally, are closer to the actual signal of interest as compared to the reconstructions from a standard DNN method. This likely leads to unrolled methods requiring less learning for ample performance and thus succeeding in low training scenarios. To this point, the excellent performance of our CGNet method in low training, as compared to state-of-the-art DNN and standard DNN methods, can be expected as CG-Net\n12\nenforces an appreciable solution structure by incorporating the CG prior through the Tikhonov update layers and Hadamard product output. However, given enough training data, standard DNN methods could outperform unrolled methods, which may now have hindered learning capacities due to the required solution structure. As full coverage of a comparison between unrolled and standard DNN methods is beyond the scope of this paper; we leave it to a future study. As full coverage of a comparison between unrolled and standard DNN methods is beyond the scope of this paper, we leave it to a future study."
        },
        {
            "heading": "D. Ablation Study",
            "text": "We consider the effect of removing the learned steepest descent matrix Bjk from CG-Net by fixing it during training. Two possibilities are employed, a gradient CG-Net (gCGNet) where Bjk = I and a Newton CG-Net (nCG-Net) where\nBjk = ( HF ;z(uk\u22121, z j\u22121 k ) )\u22121 where HF ;z is given in (14). All other aspects of training are identical to CG-Net in Section III-C except that for nCG-Net we scale the input measurements by e\u22124 and initialize a0 = a (j) k = 1 and b0 = b (j) k = e.\nShown in Table IV is the average SSIM of 200 test image reconstructions \u2013 from Radon transforms at 15, 10, and 6 uniformly spaced angles with an SNR of 60dB or 40dB \u2013 when varying the amount of training data. With fewer than 100 training data samples both gCG-Net and nCG-Net structures perform comparably or outperform the fully general version of CG-Net. Likely, this is due to gCG-Net and nCG-Net having fewer parameters to be fit for these cases and thus avoiding overfitting. With more than 100 training data samples CG-Net outperforms both gCG-Net and nCG-Net."
        },
        {
            "heading": "IV. TIME REQUIREMENTS & PARAMETERS",
            "text": "Table V lists the average computational time per image, in milliseconds, across 200 test image reconstructions running on a 64-bit Intel(R) Xeon(R) CPU E5-2690. We see that CGLS is slower than the comparative iterative methods although it has yet to be optimized for computational efficiency and\nspeed. Fortunately, the required computational time is reduced by more than a factor of 100 for CG-Net while producing the same or slightly improved quality image reconstructions as compared to CG-LS. This reduced computational time of CGNet is one of the primary advantages over using CG-LS.\nNevertheless, CG-Net lags in computational time against the comparative DNN methods that take a fraction of the time to reconstruct an image. This is likely an outcome of the required linear solver to calculate the inverse in (8) to update u and the required eigendecomposition in (17). Instead, the comparative methods solely consist of convolutions or matrix, vector products that are appreciably faster to implement than linear solvers.\nFinally, Table VI provides the number of parameters for CG-Net and all nine comparative deep learning-based methods for linear inverse problems. We remark that CG-Net has the fewest parameters to be trained out of the compared methods, which may be a contributing factor to the success of CG-Net when small training datasets are used.\nIncreasing the number of unrolled iterations in CG-Net to produce a DNN with a model complexity matching the average of the compared methods remains a point of future work. In particular, with larger training datasets, CG-Net can be outperformed by the best-performing comparative method where we may be required to increase the number of unrolled iterations in CG-Net, thereby increasing the number of learned parameters, to more closely match the comparative methods."
        },
        {
            "heading": "V. CONCLUSION AND FUTURE WORK",
            "text": "Informed by the powerful statistical representation of signal coefficients through the compound Gaussian prior, we developed a novel iterative signal reconstruction algorithm, named CG-LS, that enforces the CG prior. CG-LS is based upon a regularized least squares estimate of the signal coefficients where the regularization, equivalent to the negative log prior from a MAP estimate, is chosen to capture the fundamental statistics of the CG prior. We conducted a rigorous theoretical characterization of CG-LS, which gave important insights\n13\ninto the implementation of the algorithm. Numerical validation of CG-LS was conducted, which showed a significant improvement over other state-of-the-art image reconstruction algorithms.\nFurthermore, we have applied algorithm unrolling to CGLS, creating a deep neural network named CG-Net. To the best of our knowledge, CG-Net is the first unrolled DNN for natural and tomographic image reconstruction to be fundamentally informed by a CG prior. Multiple datasets were used to train and test CG-Net where, after each training, CG-Net was shown to outperform other unrolled DNNs as well as standard DNNs. In particular, CG-Net significantly outperforms the comparative methods in low training for both tomographic imaging and CS applications. Finally, a comparison of the computational time to reconstruct a single image from each iterative and DNN method was discussed. CG-Net significantly improved upon the necessary time over CG-LS, but still falls short of the speed other DNN methods achieve for image reconstruction.\nImproving the speed of CG-Net serves as one direction of future work for which we suggest a technique. When training CG-Net the eigendecomposition in (17) must be implemented within each Zjk layer call as the entries of L j k are actively being updated and thus, we need to actively ensure (Ljk + (L j k)\nT )/2 stays positive definite. Using CGNet post-training, the eigendecomposition only needs to be implemented once upon instantiating the model with pretrained network parameters.\nAnalyzing the performance of CG-Net for larger image reconstructions also serves as a key point of future work. Here we presented fundamental development, theory, and results for a CG-inspired iterative reconstruction algorithm and unrolled DNN. As CG-LS continues to outperform competitive iterative methods when reconstructing larger images, we anticipate CG-Net will similarly outperform or perform comparably to competitive methods in low training scenarios when applied to larger image reconstructions. For larger images, the training time will be costly, however, so optimization of the CG-Net implementation may be required. Alternatively, a technique, common in CS applications, of splitting an image into disjoint\nblocks of small size and then measuring and reconstructing separately [23, 54] could be employed for future experimental evaluation.\nAs we focus on laying the fundamental groundwork for solving linear inverse problems while incorporating a CG prior, further future work could extend CG-LS and CG-Net to non-linear inverse problems where matrix A is replaced by a non-linear function FA. Alternatively, a linearization of a non-linear forward operator, which is an adequate approximation for many non-linear inverse problems under suitable conditions (e.g. radar imaging [55]), could be used. Therefore, the theoretical and empirical groundwork laid in this paper can serve as the basis for future applications of our CGbased inverse problem methodology to CT, radar, or other experimental data.\nAnother open question remains as to the generalizability of the CG-Net model through replacing aspects of CG-based optimization with relevant neural network structures. For instance, the CG prior depends on the choice of nonlinearity h, or the corresponding choice of its inverse f . A future implementation of CG-Net could learn f by approximating it with a sub-network embedded inside of CG-Net. Such an extension of CG-Net could expand its applicability by both no longer requiring a user-specified function f , as well as providing CG-Net with a greater learning capacity.\nFinally, further empirical comparison between standard DNN approaches and algorithm-unrolled-based DNN approaches for solving inverse problems stands as another crucial goal of future work for deep-learning-based inverse problems as a whole. Briefly discussed in Section III-C, unrolling approaches, such as CG-Net, appear to have a clear advantage when small training data sets are available. However, given enough training data, standard DNN approaches could have an advantage over unrolled approaches in a couple of ways. One is that unrolling approaches have required solution structure and restriction through data consistency layers. Additionally, unrolled approaches often share trained weights across network layers leading to a recurrent network structure, which can suffer from vanishing and exploding gradient problems.\n14"
        },
        {
            "heading": "VI. APPENDIX",
            "text": ""
        },
        {
            "heading": "A. Proposition 2 Details",
            "text": "First, a lemma on the eigenvalues of a 2\u00d7 2 block matrix.\nLemma 6. For a, b, c \u2208 Rn let A = D{a}, B = D{b}, and C = D{c}. The eigenvalues of M = [ A B B C ] are\n\u03bb\u00b1i = (ai + ci \u00b1 \u221a (ai \u2212 ci)2 + 4b2i )/2, i = 1, . . . , n.\nProof. Note B and C \u2212 rI commute. Thus, using [56]\ndet(M \u2212 rI) = det((A\u2212 rI)(C \u2212 rI)\u2212B2),\nwhich is zero for r = \u03bb\u00b1i .\nWe now prove Proposition 2.\nProof of Proposition 2. Let \u03c1 : Rn \u00d7Zn \u2192 Rn be given by\n\u03c1(u,z) = AT (AD{z}u \u2212 y). (18)\nLet i \u2208 {1, 2, . . . , n}. Define Zi(z) := {z \u2208 [z0, b]n : zi = z} for z \u2208 [z0, b]. Since f2(z) is differentiable and achieves a local minimum at z0 > 0, then f \u2032(z0)f(z0) = 0. Then\nFi(z) = \u22122\u03bbz0vi(z)2 \u2264 0 for all z \u2208 Zi(z0)\nas \u03bb > 0 and v(z) = [v1(z), . . . , vn(z)]T and F (z) = [F1(z), . . . ,Fn(z)]T are given in (9) and (10), respectively. Writing y = sy\u0303 we define v\u0303(z) = AT (AzATz + \u03bbI) \u22121y\u0303 and\nv\u0303max(b) = max 1\u2264i\u2264n max z\u2208Zi(b)\n|v\u0303i(z)|.\nLet zb \u2208 Zi(b). Note, s2 (v\u0303max(b))2 \u2265 s2v\u0303i(zb)2 = vi(zb)2. Hence, if\n1\n(v\u0303max(b)) 2\nf \u2032(b)f(b)\nb \u2265 \u03bb \u00b5 s2 (19)\nthen \u00b5f \u2032(b)f(b) \u2265 \u03bbbs2 (v\u0303max(b))2 \u2265 \u03bbbvi(zb)2. Thus,\nFi(z) = \u2212\u03bbbvi(z)2 + \u00b5f \u2032(b)f(b) \u2265 0 for all z \u2208 Zi(b).\nBy the Poincare-Miranda theorem [57], when s, \u03bb, and \u00b5 satisfy (19) there exists a z\u2217 \u2208 [z0, b]n such that F (z\u2217) = 0. Thus, by Lemma 1, [u\u2217, z\u2217] with u\u2217 = z\u2217 \u2299 v(z\u2217) is a stationary point of (3).\nNext, we assume that (19) is satisfied and show the Hessian, HF = HF (u,z), of (3) at [u\u2217, z\u2217] is positive definite. Note\nHF 2 = [ ATz ATu ] [ Az Au ] + [ \u03bbI D{\u03c1(u,z)} D{\u03c1(u,z)} \u00b5D{hf (z)} ] (20)\nfor hf and \u03c1 given in (15) and (18), respectively. Observe\n\u03c1(z \u2299 v(z), z) = AT (AzATz (AzATz + \u03bbI)\u22121 \u2212 I)y = \u2212\u03bbv(z). Thus, at x\u2217 = [u\u2217, z\u2217], using that u\u2217 = z\u2217 \u2299 v(z\u2217) we have[ \u03bbI D{\u03c1(x\u2217)}\nD{\u03c1(x\u2217)} \u00b5D{hf (z\u2217)}\n] = [ \u03bbI \u2212\u03bbD{v(z\u2217)}\n\u2212\u03bbD{v(z\u2217)} \u00b5D{hf (z\u2217)} ] which, by Lemma 6, has eigenvalues\n\u03bb\u00b1i = \u03bb(1 + ci \u00b1 \u221a (1\u2212 ci)2 + 4vi(z\u2217)2 )/2\nfor ci := \u00b5\u03bb [hf (z \u2217)]i = \u00b5 \u03bb ( f \u2032\u2032(z\u2217i )f(z \u2217 i ) + f \u2032(z\u2217i ) 2 ) . Since f2(z) is strictly convex on [a, b], then f \u2032\u2032(z\u2217i )f(z \u2217 i ) +\nf \u2032(z\u2217i ) 2 > 0 implying ci > 0 and thus \u03bb+i > 0 for all i = 1, 2, . . . , n. Now \u03bb\u2212i > 0 if and only if ci > vi(z \u2217)2. Define\nhf min = min z\u2208[z0,b]\nf \u2032\u2032(z)f(z) + f \u2032(z)2\nv\u0303max = max 1\u2264i\u2264n max z\u2208[z0,b]n\n|v\u0303i(z)|.\nNote s2v\u03032max \u2265 s2v\u0303i(z\u2217)2 = vi(z\u2217)2 and [hf (z\u2217)]i \u2265 hf min for all i = 1, 2, . . . , n. Hence, if\nhf min v\u03032max > \u03bb \u00b5 s2 (21)\nthen ci = \u00b5\u03bb [hf (z \u2217)]i \u2265 \u00b5\u03bbhf min > s 2v\u03032max \u2265 vi(z\u2217)2 and thus \u03bb\u2212i > 0. Therefore, when s, \u03bb, and \u00b5 satisfy (21) then (20) at [u\u2217, z\u2217] is the sum of a positive semi-definite and positive definite matrix, implying the Hessian is positive definite. Lastly, note f \u2032(b)f(b) > 0 and hf min > 0 as f2(z) is strictly convex on [a, b] and achieves a minimizer z0 \u2208 (a, b). Thus (19) and (21) can be satisfied for positive \u03bb, \u00b5, and s.\nComparing (19) and (21) we note v\u03032max \u2265 (v\u0303max(b)) 2. Hence, if an approximation of v\u0303max, which we denote by v\u0302max, is obtained then we can satisfy both (19) and (21) choosing min{f \u2032(b)f(b)/b, hf min}/v\u03022max > \u03bbs2/\u00b5. Note, for a given f , both f \u2032(b)f(b)/b and hf min can easily be obtained."
        },
        {
            "heading": "B. Proposition 3 Details",
            "text": "For initial estimates u0 and z0, define the sublevel set\nS(u0, z0) = {(u,z) \u2208 Rn \u00d7Zn : F (u,z) \u2264 F (u0, z0)}.\nSince F is continuous then S(u0, z0) is closed and since F is coercive in both u and z then S(u0, z0) is bounded. Thus, S(u0, z0) is compact, which implies that F has Lipschitz continuous gradient on S(u0, z0). Let L := Lz(u0, z0) be the Lipschitz constant, on S(u0, z0), for \u2207zF .\nProof of Proposition 3. Let \u03b6 = zjk, \u03be = z j\u22121 k , and \u03b7 = \u03b7 (j) k . As \u2207Gk is Lipschitz continuous with constant L, then\nGk (\u03b6 ) \u2264 Gk (\u03be) + \u03b7\u2207Gk (\u03be)T djk (\u03be) + L\n2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b7djk (\u03be)\u2223\u2223\u2223\u2223\u2223\u22232 2 . (22)\nfor sufficiently small \u03b7. Using (7) note, \u2207Gk (x)T djk(x) = \u2212||\u2207Gk(x)||2\u2217(k,j) and for the Euclidean bound constant \u03b3(k,j) of || \u00b7 ||(k,j) we have \u03b32(k,j)||d j k(x)||22 \u2264 ||\u2207Gk(x)||2\u2217(k,j). Combining with (22) gives\nGk (\u03b6 ) \u2264 Gk (\u03be)\u2212 \u03b7 ( 1\u2212 \u03b7L\n2\u03b32(k,j)\n) ||\u2207Gk (\u03be)||2\u2217(k,j) . (23)\nFrom [47] a backtracking line employs two user-chosen parameters \u03b1 \u2208 (0, 1/2] and \u03b2 \u2208 (0, 1) where the step size \u03b7 is chosen to be a multiple of \u03b2 satisfying\nGk (\u03b6 ) \u2264 Gk (\u03be)\u2212 \u03b1\u03b7 ||\u2207Gk (\u03be)||2\u2217(k,j) . (24)\nNote, (23) implies (24) when \u03b7 \u2264 \u03b32(k,j)/L and thus, from [47], the backtracking line search step size satisfies \u03b7 \u2265 min{1, \u03b2\u03b32(k,j)/L} > 0. Combining with (23) and (24) produces (11) when c(k,j) = \u03b1min{1, \u03b2\u03b32(k,j)/L}.\n15"
        },
        {
            "heading": "C. Theorem 4 Details",
            "text": "Recall Gk(z) = F (uk\u22121, z). We now prove Theorem 4.\nProof of Theorem 4. Define vk = [uk, zk], vk\u2212 12 = [uk\u22121, zk] and v j\nk\u2212 12 = [uk\u22121, z\nj k]. We let F (vk) = F (uk, zk)\nand similarly for F (vj k\u2212 12 ) and F (vk\u2212 12 ). Note, for all k \u2265 1, v0 k\u2212 12 = vk\u22121 and vJk\u2212 12 = vk\u2212 12 . Summing over j in (11)\nF (vk\u22121)\u2212 F (vk\u2212 12 ) \u2265 J\u2211\nj=1\nc(k,j) \u2223\u2223\u2223\u2223\u2223\u2223\u2207zF (vj\u22121k\u2212 12)\u2223\u2223\u2223\u2223\u2223\u22232\u2217(k,j) \u2265 c(k,1)||\u2207zF (vk\u22121)||2\u2217(k,1). (25)\nFirst, note \u2207uF (vk\u22121) = 0 since uk\u22121 is a global minimizer of (3) with z fixed at zk\u22121. Hence, we have ||\u2207zF (vk\u22121)||2\u2217(k,1) = ||\u2207F (vk\u22121)|| 2 \u2217(k,1). Second, note F (vk) \u2264 F (vk\u2212 12 ) since uk is a global minimizer of (3) with z fixed at zk. Hence, \u2212F (vk) \u2265 \u2212F (vk\u2212 12 ). Third, define\nc := \u03b1min { 1, \u03b2\u03b32/L } (26)\nand note, since \u03b3(k,1) \u2265 \u03b3, then, from Proposition 3 c(k,1) \u2265 c > 0. Combining these three results with (25) gives\nF (vk\u22121)\u2212 F (vk) \u2265 c||\u2207F (vk\u22121)||2\u2217(k,1). (27)\nThus, CG-LS generates a monotonic decreasing sequence of cost function values, which are bounded below by 0. Hence, {F (vk)}\u221ek=0 converges to a value F \u2217. Therefore, taking an infinite sum of (27) w.r.t k and using \u03b3\u2217(k,1) \u2265 \u03b3 gives\nF (v0)\u2212 F \u2217 \u2265 c \u221e\u2211 k=1 ||\u2207F (vk\u22121)||2\u2217(k,1) \u2265 \u03b3 2c \u221e\u2211 k=0 ||\u2207F (vk)||22.\nImplying that \u2211\u221e\nk=0 ||\u2207F (vk)||22 converges. Thus, lim k\u2192\u221e ||\u2207F (vk)||22 \u2192 0 and so lim k\u2192\u221e\n\u2207F (vk) \u2192 0. Finally, taking an average of (27) gives\nF (v0)\u2212 F (vK) c 1 K \u2265 min 1\u2264k\u2264K ||\u2207F (vk)||2\u2217(k,1)."
        },
        {
            "heading": "D. Theorem 5 Details",
            "text": "First, define\nB\u03b4(u\u0302, z\u0302) = {[u,z ] \u2208 Rn \u00d7Zn : ||[u,z ]\u2212 [u\u0302, z\u0302 ]||2 < \u03b4} .\nfor \u03b4 > 0, u\u0302 \u2208 Rn, and z\u0302 \u2208 Zn. We now prove Theorem 5.\nProof of Theorem 5. The non-degenerate local minimizer [u\u2217, z\u2217] in Proposition 2 satisfies, for all w \u2208 R2n, wTHF (u\n\u2217, z\u2217)w = \u21130(w) \u2265 \u03bbmin > 0 where \u03bbmin is the minimum eigenvalue of the Hessian HF (u\u2217, z\u2217). Since (3) is twice continuously differentiable then, for fixed w \u2208 R2n, Q(u,z) := wTHF (u,z)w is a continuous function. Fix \u2113 \u2208 (0, \u03bbmin) and let \u03f5 = \u03bbmin \u2212 \u2113 > 0. By continuity of Q there exists a \u03b4 > 0 such that for all [u,z ] \u2208 B\u03b4(u\u2217, z\u2217) we have |Q(u,z)\u2212Q(u\u2217, z\u2217)| < \u03f5 = \u03bbmin\u2212 \u2113, which implies Q(u,z) \u2265 \u2113. Therefore, (3) is strongly convex with constant \u2113 on C = B\u03b4(u\u2217, z\u2217).\nNext, strong convexity of (3) with constant \u2113 implies that\nF (vi)\u2212 F \u2217 \u2264 (2\u2113)\u22121||\u2207F (vi)||22 (28)\nfor all i \u2265 0. Rearranging (27), subtracting F \u2217 from both sides, and applying || \u00b7 ||\u2217(k,1) \u2265 \u03b3|| \u00b7 ||2 gives\nF (vk)\u2212 F \u2217 \u2264 F (vk\u22121)\u2212 F \u2217 \u2212 \u03b32c||\u2207F (vk\u22121)||22,\nwhich combined with (28), when i = k \u2212 1, produces\nF (vk)\u2212 F \u2217 \u2264 (1\u2212 2\u2113\u03b32c) (F (vk\u22121)\u2212 F \u2217) . (29)\nApplying (29) recursively gives (12)."
        },
        {
            "heading": "E. Compound Gaussian Representations",
            "text": "Here, we discuss the generality of the CG prior.\nProposition 7. The generalized Gaussian, student\u2019s t, \u03b1stable, and symmetrized Gamma distributions are special cases of the compound Gaussian distribution.\nThe result of Proposition 7 is found in [10, 11]. Now, we give an explicit nonlinearity, h, such that the CG prior reduces to a Laplace prior.\nProposition 8. Let \u03a5 be the cumulative distribution function (CDF) of a standard Gaussian random variable. Define\nh(x) = ( \u22122\u03bb2 ln(1\u2212\u03a5(x)) )1/2 .\nThen c = h(x)\u2299 u, for x \u223c N (0, I) and u \u223c N (0, I), has a Laplace distribution. That is, c \u223c \u03bb exp(\u2212\u03bb||c||1)/2.\nProof. A Laplace random variable X \u223c \u03bb exp(\u2212\u03bbx)/2 is decomposed as X = \u221a ZU , for U \u223c N (0, 1) and Z \u223c Exp(1/2\u03bb2). Let FZ(z) = 1 \u2212 e\u2212z/(2\u03bb 2) be the CDF of Z. Due to independence in the components of c, we only need to show that h(xi)2 \u223c Z for xi \u223c N (0, 1), which is easily observed since\nP(h(xi)2 \u2264 x) = P ( xi \u2264 \u03a5\u22121 (FZ(x)) ) = FZ(x)."
        },
        {
            "heading": "VII. ACKNOWLEDGEMENTS",
            "text": "We would like to thank all anonymous reviewers for their thoughtful comments, which helped us improve this manuscript."
        }
    ],
    "title": "A Compound Gaussian Least Squares Algorithm and Unrolled Network for Linear Inverse Problems",
    "year": 2023
}