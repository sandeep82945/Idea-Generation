{
    "abstractText": "Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pretraining, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pretrained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance. The source codes are publicly available at https://github.com/thunlp/ RecyclableTuning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yujia Qin"
        },
        {
            "affiliations": [],
            "name": "Cheng Qian"
        },
        {
            "affiliations": [],
            "name": "Xu Han"
        },
        {
            "affiliations": [],
            "name": "Yankai Lin"
        },
        {
            "affiliations": [],
            "name": "Huadong Wang"
        },
        {
            "affiliations": [],
            "name": "Ruobing Xie"
        },
        {
            "affiliations": [],
            "name": "Zhiyuan Liu"
        },
        {
            "affiliations": [],
            "name": "Maosong Sun"
        },
        {
            "affiliations": [],
            "name": "Jie Zhou"
        }
    ],
    "id": "SP:ceb476a4123366a2c5b7100b9b8d468bc535fbc5",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Anchit Gupta",
                "Akshat Shrivastava",
                "Xilun Chen",
                "Luke Zettlemoyer",
                "Sonal Gupta."
            ],
            "title": "Muppet: Massive multi-task representations with pre-finetuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "year": 2021
        },
        {
            "authors": [
                "Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Chen",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Yujia Qin",
                "Fengyu Wang",
                "Zhi Wang",
                "Xiao Chen",
                "Zhiyuan Liu",
                "Qun Liu."
            ],
            "title": "bert2BERT: Towards reusable pretrained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "Proceedings of the 11th International AAAI Conference on Web and Social Media, ICWSM \u201917, pages",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen"
            ],
            "title": "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
            "year": 2022
        },
        {
            "authors": [
                "Felix Draxler",
                "Kambis Veschgini",
                "Manfred Salmhofer",
                "Fred A. Hamprecht."
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,",
            "year": 2018
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Dipanjan Das."
            ],
            "title": "Identifying well-formed natural language questions",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 798\u2013803, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin."
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,",
            "year": 2020
        },
        {
            "authors": [
                "C. Daniel Freeman",
                "Joan Bruna."
            ],
            "title": "Topology and geometry of half-rectified network optimization",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 2426, 2017, Conference Track Proceedings. OpenRe-",
            "year": 2017
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson."
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neu-",
            "year": 2018
        },
        {
            "authors": [
                "Linyuan Gong",
                "Di He",
                "Zhuohan Li",
                "Tao Qin",
                "Liwei Wang",
                "Tieyan Liu."
            ],
            "title": "Efficient training of BERT by progressively stacking",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine",
            "year": 2019
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith"
            ],
            "title": "Don\u2019t stop pretraining",
            "year": 2020
        },
        {
            "authors": [
                "Ruining He",
                "Julian J. McAuley."
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada,",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "ArXiv preprint, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Changho Lee",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun Kim",
                "Minjoon Seo"
            ],
            "title": "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models",
            "year": 2022
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun Kim",
                "Stanley Jungkyu Choi",
                "Minjoon Seo."
            ],
            "title": "Towards continual knowledge learning of language models",
            "venue": "ArXiv preprint, abs/2110.03215.",
            "year": 2021
        },
        {
            "authors": [
                "Xisen Jin",
                "Dejiao Zhang",
                "Henghui Zhu",
                "Wei Xiao",
                "Shang-Wen Li",
                "Xiaokai Wei",
                "Andrew Arnold",
                "Xiang Ren."
            ],
            "title": "Lifelong pretraining: Continually adapting language models to emerging corpora",
            "venue": "Proceedings of BigScience Episode #5 \u2013 Workshop",
            "year": 2022
        },
        {
            "authors": [
                "David Jurgens",
                "Srijan Kumar",
                "Raine Hoover",
                "Dan McFarland",
                "Dan Jurafsky."
            ],
            "title": "Measuring the evolution of a scientific field through citation frames",
            "venue": "Transactions of the Association for Computational Linguistics, 6:391\u2013406.",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Jens Kringelum",
                "Sonny Kim Kjaerulff",
                "S\u00f8ren Brunak",
                "Ole Lund",
                "Tudor I Oprea",
                "Olivier Taboureau"
            ],
            "title": "Chemprot-3.0: a global chemical biology diseases",
            "year": 2016
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Gaurav Singh Tomar",
                "Ankur P Parikh",
                "Nicolas Papernot",
                "Mohit Iyyer."
            ],
            "title": "Thieves on sesame street! model extraction of bertbased apis",
            "venue": "arXiv preprint arXiv:1910.12366.",
            "year": 2019
        },
        {
            "authors": [
                "James Lee-Thorp",
                "Joshua Ainslie",
                "Ilya Eckstein",
                "Santiago Ontanon."
            ],
            "title": "FNet: Mixing tokens with Fourier transforms",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Ye Lin",
                "Yanyang Li",
                "Ziyang Wang",
                "Bei Li",
                "Quan Du",
                "Tong Xiao",
                "Jingbo Zhu."
            ],
            "title": "Weight distillation: Transferring the knowledge in neural network parameters",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "ArXiv preprint, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Lo",
                "Lucy Lu Wang",
                "Mark Neumann",
                "Rodney Kinney",
                "Daniel Weld."
            ],
            "title": "S2ORC: The semantic scholar open research corpus",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
            "year": 2011
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A sick cure for the evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Ninth International Conference on Lan-",
            "year": 2014
        },
        {
            "authors": [
                "Julian McAuley",
                "Jure Leskovec."
            ],
            "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
            "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172.",
            "year": 2013
        },
        {
            "authors": [
                "Seyed Iman Mirzadeh",
                "Mehrdad Farajtabar",
                "Dilan Gorur",
                "Razvan Pascanu",
                "Hassan Ghasemzadeh."
            ],
            "title": "Linear mode connectivity in multitask and continual learning",
            "venue": "arXiv preprint arXiv:2010.04495.",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013",
            "year": 2005
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterhub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Em-",
            "year": 2020
        },
        {
            "authors": [
                "Clifton Poth",
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Iryna Gurevych."
            ],
            "title": "What to pre-train on? Efficient intermediate task selection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10585\u201310605, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yujia Qin",
                "Yankai Lin",
                "Jing Yi",
                "Jiajie Zhang",
                "Xu Han",
                "Zhengyan Zhang",
                "Yusheng Su",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "Knowledge inheritance for pre-trained language models",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Qin",
                "Cheng Qian",
                "Jing Yi",
                "Weize Chen",
                "Yankai Lin",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "Exploring mode connectivity for pre-trained language models",
            "venue": "arXiv preprint arXiv:2210.14102.",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Qin",
                "Xiaozhi Wang",
                "Yusheng Su",
                "Yankai Lin",
                "Ning Ding",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Lei Hou",
                "Peng Li",
                "Maosong Sun"
            ],
            "title": "Exploring lowdimensional intrinsic task subspace via prompt tuning",
            "venue": "arXiv preprint arXiv:2110.07867",
            "year": 2021
        },
        {
            "authors": [
                "Yujia Qin",
                "Jiajie Zhang",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "ELLE: Efficient lifelong pre-training for emerging data",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2789\u20132810, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "Journal of Machine Learning Re-",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Yusheng Su",
                "Xiaozhi Wang",
                "Yujia Qin",
                "Chi-Min Chan",
                "Yankai Lin",
                "Huadong Wang",
                "Kaiyue Wen",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li",
                "Lei Hou",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "On transferability of prompt tuning",
            "year": 2022
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "Jie Zhou"
            ],
            "title": "Different tunes played with equal",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Franziska Roesner",
                "Yejin Choi."
            ],
            "title": "Defending against neural fake news",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Infor-",
            "year": 2019
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Richard S. Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "2015 IEEE Interna-",
            "year": 2015
        },
        {
            "authors": [],
            "title": "We formulate the downstream tuning as prompt learning (Schick and Sch\u00fctze, 2021)",
            "year": 2021
        },
        {
            "authors": [
                "Eval-Atheism from Barbieri"
            ],
            "title": "We partition the tasks belonging to the same category into source task",
            "year": 2020
        },
        {
            "authors": [
                "stance. D"
            ],
            "title": "Methods and Experiments For the optimizer of all the experiments in \u00a7 5, we choose AdamW (Loshchilov and Hutter, 2019)",
            "venue": "Initialization-based Recyclable Tuning",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The emergence of pre-trained language models (PLMs) has revolutionized the entire field of natural language processing (NLP) (Bommasani et al., 2021). Through downstream adaptation, PLMs effectively stimulate the knowledge acquired during pre-training and achieve remarkable success in various downstream tasks (Devlin et al., 2019; Liu\n\u2217Indicates equal contribution. \u2020Corresponding author.\net al., 2019; Raffel et al., 2020). Such adaptation can be achieved by either full-parameter fine-tuning or parameter-efficient tuning (Houlsby et al., 2019), and the latter enables learning lightweight adapted modules for downstream tasks. Currently, a de facto paradigm for handling NLP tasks has been formed, dividing practitioners into two groups: (1) upstream suppliers, who pre-train PLMs on taskagnostic data and release them on public platforms, e.g., HuggingFace (Wolf et al., 2020), and (2) downstream consumers, who download the PLM and conduct personalized adaptation using taskspecific data. The corresponding adapted weights might then be shared with third parties via platforms such as AdapterHub (Pfeiffer et al., 2020).\nIn real-world scenarios, PLMs may constantly get upgraded and released by the supplier. Correspondingly, the customer-side compatible update of adapted weights becomes necessary. Continual pre-training (Qin et al., 2022c) is a typical scenario where PLMs continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, consumers may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded. This can lead to a loss of knowledge about downstream tasks encapsulated in the outdated weights, as well as a ar X\niv :2\n30 5.\n08 70\n2v 1\n[ cs\n.C L\n] 1\n5 M\nay 2\n02 3\npotential waste of computational resources. In this paper, we bring this issue to the forefront and argue that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training, which is illustrated in Figure 1.\nDue to the parameter change during continual pre-training, one potential concern for recycling outdated adapted weights is their mismatch with the upgraded PLM. However, our pilot studies reveal that directly applying the outdated weights to the upgraded PLM yields substantial performance improvements as compared to zero-shot inference of the PLM. This shows that the upgraded PLM remains compatible with the outdated weights to some extent, indicating a close connection between continually pre-trained PLMs. Intuitively, such a connection provides a strong basis for our assertion that outdated weights are recyclable and useful.\nTo uncover hints for solving our task, we further investigate such a connection from two aspects: (1) linear mode connectivity (Qin et al., 2022b). We demonstrate that after adapting both the upgraded PLM and the original PLM to the same task, linearly interpolating the parameters of both adapted models could produce a series of checkpoints with high task performance (low loss). Such a property indicates a close parametric connection of both PLMs in the loss landscape; (2) functional similarity. After adapting both PLMs to the same task, we observe that their corresponding attention heads exhibit similar patterns given the same input. Such representational proximity implies that both PLMs own similar functionalities during text processing.\nBoth analyses above demonstrate the close connections between continually pre-trained PLMs. Based on the corresponding findings, we propose two methods for recyclable tuning:\n(1) Initialization-based method, which leverages the adapted weights of the original PLM as the initialization for the upgraded PLM. This method is motivated by their close parametric connection in the loss landscape. We demonstrate that for a target task, initializing the tunable parameters with the outdated weights from a similar source task could accelerate the convergence and improve the training efficiency, compared to using random initialization. In addition, after sufficient training, this method generally improves the final performance. We also observe that the benefits of this method in terms of convergence and performance are greater\nwhen the source and target tasks are more similar. (2) Distillation-based method, which distills the knowledge stored in outdated weights for tuning the upgraded PLM. We demonstrate that knowledge distillation can effectively facilitate knowledge transfer between continually pre-trained PLMs. Using only a small number of labeled examples, the upgraded PLM can outperform the original PLM when trained with far more examples. We also show that both initialization-based and distillation-based methods can be combined to further improve the performance. This means knowledge transfer through parameter space and model outputs are complementary to each other.\nIn a nutshell, these results highlight the practical benefits of recyclable tuning and point to an important future direction in sustainable NLP."
        },
        {
            "heading": "2 Related Work",
            "text": "Continual Pre-training. Conventionally, PLMs are trained on static data, ignoring that streaming data from various sources could continually grow. Continual pre-training requires PLMs to accumulate new knowledge in a continual manner (Gururangan et al., 2020), meanwhile alleviating the catastrophic forgetting problem. Prior works in this field focus on building benchmarks and analyses (Jang et al., 2021, 2022). Later works explored the applicability of traditional continual learning algorithms under this setting (Jin et al., 2022; Wu et al., 2021). Recent efforts were also spent on continual pre-training in a computationally efficient way (Qin et al., 2022c).\nPrevious works focus on improving the capabilities of PLMs during pre-training from the standpoint of upstream suppliers. Instead, we shift the focus to downstream adaptation from the perspective of customers. We highlight a previously overlooked issue of the incompatibility between upgraded PLMs and the existing adapted weights. For the first time, we examine the connections between continually pre-trained models and demonstrate the potential benefits of recycling outdated weights.\nKnowledge Transfer for PLMs. Transfer learning for PLMs has gained increasing attention recently. Some works study task-level transferability for an individual PLM and find that fine-tuning on certain source tasks conduces to the performance on similar target tasks (Vu et al., 2020; Poth et al., 2021; Aghajanyan et al., 2021). Differently, we also study cross-task knowledge trans-\nfer for two different PLMs under the continual pre-training scenario (\u00a7 5.1). Besides, researchers also investigate cross-model knowledge transfer. They try to recycle lightweight adapted weights of the same task between two independently pretrained PLMs, e.g., PLMs with distinct data (Su et al., 2022). As we would show later, unlike independently trained PLMs, continually pre-trained PLMs are guaranteed close connections. This distinction determines our setting is unique to previous works and may require different solutions."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "Continual Pre-training. Following Qin et al. (2022c), we simulate the scenario where new data from 4 domains is gathered sequentially, i.e., biomedical papers (BIO,D1) (Lo et al., 2020), amazon reviews (REV, D2) (He and McAuley, 2016), computer science papers (CS,D3) (Lo et al., 2020), and news articles (NS, D4) (Zellers et al., 2019). Starting from the official RoBERTaBASE (Liu et al., 2019) (denoted asM0), we continually pre-train M0 on 4 domains. For each domain, we set the pre-training steps to 12.5k and the batch size to 2048. DenoteMi as the PLM that finishes training on Di, andMi(t) as the PLM that starts from Mi\u22121 and is trained on Di for t steps. We assume the suppliers only release the PLM that finishes training on each domain, i.e., {M1, \u00b7 \u00b7 \u00b7 ,M4} are developed and released. The pre-training details are described in appendix D.1.\nDownstream Adaptation. At the same time, we have a set of downstream tasks to handle. To adapt Mi (0 \u2264 i \u2264 4) towards a task Tj , we conduct supervised training using the loss function LTj . Denote the pre-trained weights of Mi as \u03b80i , we obtain its adapted weights \u2206Tji for Tj after training. By assembling both \u03b80i and \u2206 Tj i , the resultant model \u03b8Tji = \u03b8 0 i \u2295\u2206 Tj i can be deployed to handle Tj . Throughout this paper, we consider two tuning methods: full-parameter fine-tuning and a representative parameter-efficient tuning method, adapter tuning (Houlsby et al., 2019) (see appendix A.1 for more backgrounds). For the former, we have |\u2206Tji | = |\u03b80i |; while for the latter, |\u2206 Tj i | |\u03b80i |, where | \u00b7 | denotes the number of parameters.\nRecyclable Tuning. Before the release of an upgraded PLMMi\u2032 (i<i\u2032), we have obtained adapted weights \u2206Tji of an old PLMMi for task Tj . Recyclable tuning aims at transferring the knowledge of\n\u2206 Tj i to assist tuningMi\u2032 (i.e., learning new weights \u2206 Tj i\u2032 ). We denote the above process as \u2206 Tj i \u2192\u2206 Tj i\u2032 . Intuitively, \u2206Tji encapsulates abundant knowledge about the task Tj , which should benefit learning \u2206 Tj i\u2032 if exploited properly. Such benefits may include improving training efficiency or performance. To gain insights of solving the task, we first conduct a series of empirical analyses in \u00a7 4 to understand the connections amongMi,Mi\u2032 , \u2206 Tj i , and \u2206 Tj i\u2032 ."
        },
        {
            "heading": "4 Empirical Analysis",
            "text": "We first investigate the compatibility of outdated weights and the upgraded PLM (\u00a7 4.1), then we explore the (1) parametric connections and (2) representational connections of continually pre-trained PLMs from two aspects: (1) linear mode connectivity (\u00a7 4.2) and (2) functional similarity (\u00a7 4.3). The implementation details are left in appendix D.2."
        },
        {
            "heading": "4.1 Model Compatibility Analysis",
            "text": "We explore to what extent the outdated weights are compatible with the upgraded PLM and how this compatibility changes during continual pretraining. Specifically, we directly apply outdated weights to the upgraded PLM and record the performance variation during continual pre-training.\nSettings. We first investigate the process when upgrading M0 to M1 on the BIO domain (D1). For downstream evaluation, we choose two classification tasks: CHEMPROT (Kringelum et al., 2016), which is a relevant downstream task to the BIO domain, and MNLI (Williams et al., 2018). Denote the model continually pre-trained on D1 for t steps asM1(t), its pre-trained weights as \u03b801(t), and the adapted weights ofM0 for the downstream task as \u2206T0 . We directly apply \u2206 T 0 to the upgraded PLM M1(t), i.e., \u03b801(t) \u2295 \u2206T0 , and evaluate the performance on the test set of the downstream task. In experiments, t is selected from 1.25k to 12.5k with an interval of 1.25k. We also reportM1(t)\u2019s zero-shot inference performance by testing \u03b801(t).\nResults. From the results in Figure 2 (a, b), we observe that for both adapter and fine-tuning: (1) with t increasing, the performance of \u03b801(t)\u2295\u2206T0 drops quickly at first. This means that \u2206T0 becomes outdated shortly after the backbone modelM1(t) changes. (2) After sufficient pre-training steps, the performance converges to a plateau which is still much higher than the zero-shot inference performance of M1(t). This implies that continually\npre-trained PLMs are intrinsically connected with their \u201cancestors\u201d, otherwise the ancestor\u2019s adapted weights \u2206T0 would not improve the performance of its offspringM1(t).\nExtension to Multiple Domains. Next, we extend the above experiments to 4 sequentially released PLMs as mentioned in \u00a7 3 by directly applying \u2206T0 to {M1, \u00b7 \u00b7 \u00b7 ,M4}. We derive from Figure 2 (c, d) that: (1) applying outdated weights consistently performs better than zero-shot inference even if the backbone PLM is trained over multiple domains; (2) the performance ofM4 is the best among {M1, \u00b7 \u00b7 \u00b7 ,M4} thoughM4 is trained for the longest time. This may be because the NS domain (D4) is the most similar one toM0\u2019s pretraining data (Gururangan et al., 2020), and continual pre-training on a similar domain of the original PLM mitigates the incompatibility."
        },
        {
            "heading": "4.2 Linear Mode Connectivity Analysis",
            "text": "Backgrounds. Linear mode connectivity measures whether two sets of model weights can be connected via a linear parametric path, along which the performance (loss) of the downstream task remains high (low) (Frankle et al., 2020). In other words, it tests whether linear interpolations of two model weights perform comparably to both endpoints. If this property holds, then both model weights probably lie in the same loss basin, which indicates a close connection between them in the parameter space (Qin et al., 2022b). For more de-\ntailed backgrounds, please refer to appendix A.2.\nSettings. Following most of the settings in \u00a7 4.1, we adapt both M0 and M1(t) towards the task CHEMPROT and obtain the weights \u03b8T0 and \u03b8 T 1 (t), where \u03b8T0 = \u03b8 0 0 \u2295\u2206T0 and \u03b8T1 (t) = \u03b801(t)\u2295\u2206T1 (t). Then we linearly interpolate both \u03b8T0 and \u03b8 T 1 (t) as:\n\u03b8(\u00b5) = (1\u2212 \u00b5)\u03b8T0 + \u00b5\u03b8T1 (t), (1)\nwhere \u00b5 \u2208 (0, 1). In experiments, we evaluate the performance of 25 evenly distributed interpolations and two endpoints (i.e., \u00b5 = 0 and \u00b5 = 1). If there does not exist a significant performance drop along the linear path, we deem both endpoints linearly mode connected. We choose M1(t) that is continually pre-trained for {2.5, 5.0, 7.5, 10.0, 12.5}k steps and evaluate mode connectivity for each M1(t) andM0. In addition, we pre-train a new RoBERTaBASE (dubbed asMIND) from scratch (details in appendix D.1) and test its connectivity with M0, i.e., \u03b8(\u00b5) = (1\u2212\u00b5)\u03b8T0 +\u00b5\u03b8TIND. In this way, we can compare the difference between continually pre-trained models (M0 andM1(t)) and independently pre-trained models (M0 andMIND).\nResults. We illustrate the performance of the interpolations and two endpoints in Figure 3, from which we conclude that: (1) for continually pretrained PLMs, although there exists a small performance drop in the midpoint, the interpolations generally achieve comparable performance to endpoints; (2) the connectivity does not vary much with t increasing, which means within a reasonable range, the connectivity is not sensitive to longer pre-training; (3) while for independently trained PLMs, the performance drops significantly in the middle, which means the adapted weights of these PLMs cannot be linked by a high-performance lin-\near path; (4) the above conclusions hold for both adapter and fine-tuning.\nThe above findings imply that when learning the same task, two continually pre-trained PLMs would probably be optimized into two minima lying in the same loss basin, or at least the optimal regions corresponding to both minima have a substantial intersection; otherwise, there should exist a significant performance drop in between.\nIntuitively, the existence of a high-performance (low-loss) path between two optimal regions implies that model weights can be easily optimized from one optimal region to another without incurring a loss barrier. In this regard, it is promising to use outdated adapted weights as the initialization to find the optimal solution for the upgraded PLM, which would be explored in \u00a7 5.1. In this way, we explicitly facilitate cross-model knowledge transfer through the parameter space.\nExtension to Multiple Domains. Next, we evaluate linear mode connectivity between the initial M0 andMi (1\u2264 i\u22644) using the task CHEMPROT. We derive from the results in Figure 4 that although the performance tends to drop slightly near the midpoint, the connectivity of all continually pre-trained models is still far better than independent PLMs (i.e.,MIND in Figure 3). We also observe that the performance drop betweenM0 and M2 is larger than M0 and M4, though M4 is trained for a longer time than M2. This means longer pre-training does not necessarily result in poorer connectivity; rather, the pre-training domain has a great impact."
        },
        {
            "heading": "4.3 Functional Similarity Analysis",
            "text": "The close parametric connection revealed by linear mode connectivity does not guarantee that continually pre-trained PLMs share similar functionalities\nwhen processing the text information. Following Gong et al. (2019), we explore functional similarity through the lens of attention distribution. Specifically, we investigate three continually pre-trained models (M0, M1, and M2) and fine-tune them on CHEMPROT to obtain adapted models (\u03b8T0 , \u03b8 T 1 , and \u03b8T2 ). We feed the same input sampled from CHEMPROT to the three adapted models. Then we select attention heads from the same position (i.e., the h-th head in the l-th layer) in three models, and visualize their attention distribution. Note the selected head ofMi+1 is trained from that ofMi.\nFrom Figure 5, it is found that the attention patterns of M1 and M2 are quite similar to those of their \u201cancestor\u201d M0. Such representational proximity indicates that the corresponding modules of continually pre-trained PLMs own similar functionalities. Since adapted weights play a pivotal role in stimulating PLM\u2019s abilities and functionalities (Ding et al., 2022), such functional similarity partially explains why the outdated adapted weights can be directly applied to the upgraded PLM and achieve non-trivial performance in \u00a7 4.1.\nIn a nutshell, all the analyses in this section validate the close connection between continually pre-\ntrained PLMs. Intuitively, such a connection implies that the adaptation process of these PLMs towards downstream tasks should be closely related and transferable as well, which serves as the strong basis for our recyclable tuning."
        },
        {
            "heading": "5 Methods and Experiments",
            "text": "Based on the findings in \u00a7 4, we propose two ways to explore the practical benefits of recyclable tuning: initialization-based method (\u00a7 5.1) and distillation-based method (\u00a7 5.2). The training details of this section are discussed in appendix D.3."
        },
        {
            "heading": "5.1 Initialization-based Recyclable Tuning",
            "text": "We first investigate directly using outdated weights as the initialization for tuning the upgraded PLM.\nFramework. Without loss of generality, we experiment when the initial PLMM0 is continually pre-trained on the BIO domain (D1) and upgraded toM1. Before the release of a new PLMM1, assume we have tunedM0 on N tasks {T0, \u00b7 \u00b7 \u00b7 , TN} and obtained the corresponding adapted weights {\u2206T10 , \u00b7 \u00b7 \u00b7 ,\u2206 TN 0 }. When tuning M1 on a target task Tt, instead of using the random initialization\nfor tunable weights, we initialize them usingM0\u2019s adapted weights \u2206Ts0 trained on a source task Ts.\nConsidering that in practice, it is possible that the outdated weights of exactly the same task are not available, i.e., Tt 6= Ts. Thus we explore whether initialization from the outdated weights of a different task would suffice for our goal. Specifically, we consider three types of source tasks: (1) Tsame, which is the same task as the target one; (2) Tsim, which denotes a task similar to Tt, both Tsim and Tt typically belong to the same task type; (3) Tdiff, which belongs to a different task category from Tt.\nSettings. We experiment with 6 target tasks of 3 types: (1) natural language inference: ANLI (Nie et al., 2020) and SICK (Marelli et al., 2014), (2) sentiment analysis: SST-2 (Socher et al., 2013) and Rotten Tomatoes (Pang and Lee, 2005), (3) emotion detection: Hate Speech (Davidson et al., 2017) and Tweet Eval-Offensive (Barbieri et al., 2020). The choices of Tsim and Tdiff for each target task are listed in Table 12 in the appendix.\nWe compare the proposed initialization strategies with random initialization and record (1) the test performance variation (w.r.t. training steps) during the early stage of downstream adaptation (Figure 6), and (2) the best test performance after the adaptation converges (Table 1). For adaptation, we mainly investigate adapter tuning and leave the experiments of fine-tuning in appendix C.3.\nResults. The observations and corresponding conclusions are summarized as follows:\n(1) Faster convergence: we observe from Figure 6 that compared with the random initialization baseline, our method significantly accelerates the convergence of downstream adaptation. This suggests that the outdated weights provide a more effective initialization, allowing the PLM to be more easily optimized to the desired local optima. In practice, this method could improve the training ef-\nficiency of tuning the upgraded PLM, which saves the computations needed for adaptation.\n(2) Improved task performance: we also conclude from Table 1 that after sufficient training, initialization from the outdated weights of each type of source tasks (even for Tdiff) could improve the final performance (up to +1.9 average improvement). This demonstrates that initialization serves as a valid way for cross-model knowledge transfer.\n(3) Similar source tasks benefit more: comparing the results of initialization from different source tasks, we find that the improvement in both convergence and performance can be generally ranked as Tsame>Tsim>Tdiff. This is because the knowledge required by more similar tasks has a greater overlap. Thus the knowledge transfer benefits more when the target task and source task are more similar. In practice, this finding expands the selection scope of source adapted weights, broadening the application scenarios for our initialization-based method."
        },
        {
            "heading": "5.2 Distillation-based Recyclable Tuning",
            "text": "According to Lin et al. (2021), model outputs often contain sufficient supervision that is complementary to the knowledge stored in parameters. Therefore, besides the initialization-based method, we also explore knowledge distillation (Hinton et al., 2015) to recycle the outdated weights.\nFramework. Given a task Tj , assume we have optimized an outdated PLMMi and obtained its adapted weights \u2206Tji . Our goal is to distill the knowledge stored in \u2206Tji to optimize an updated PLMMi+1. We follow Sun et al. (2019) to construct our framework. For each data point x from Tj , denote P(x, \u03b8 Tj i ) as the probability distribution the adaptedMi assigns over the label space, where \u03b8Tji =\u03b8 0 i \u2295\u2206 Tj i . We minimize the KL divergence between probabilities predicted byMi and Mi+1. In addition,Mi+1 mimicsMi\u2019s intermediate hidden representations of each layer. Specifically, given the same input x, denote hk(x, \u03b8 Tj i ) and hk(x, \u03b8 Tj i+1) as the normalized hidden states of the k-th layer ofMi andMi+1, we minimize the mean-square loss of hidden states together with the KL divergence as follows:\nLKD = KL(P(x, \u03b8 Tj i )||P(x, \u03b8 Tj i+1))+ \u03b1\u03a3k||hk(x, \u03b8 Tj i )\u2212hk(x, \u03b8 Tj i+1)|| 2, (2)\nwhere \u03b1 denotes a hyper-parameter. During optimization, only \u2206Tji+1 is tunable. Besides LKD, we\nalso introduce the original task loss LTj , which is calculated using supervised training examples from task Tj , with another hyper-parameter \u03b2:\nLfinal = \u03b2LTj + (1\u2212 \u03b2)LKD. (3)\nSettings. We consider the sequentially released PLMs {M0, \u00b7 \u00b7 \u00b7 ,M4} as mentioned in \u00a7 3. Following Gururangan et al. (2020), we choose three tasks T1: CHEMPROT, T2: IMDB (Maas et al., 2011) and T3: ACL-ARC (Jurgens et al., 2018), which are relevant to domain D1, D2 and D3, respectively. We mainly consider recyclable tuning between adjacent PLMs, i.e.,Mi andMi+1, and also evaluate non-adjacent PLMs (e.g., Mi and Mi+2) in appendix C.7. For each task Ti (i \u2208 {1, 2, 3}), we consider two settings:\n(a) First, we recycleMi\u2019s outdated weights to Mi+1, which is denoted as \u2206Tii \u2192 \u2206 Ti i+1. Here the evaluated task Ti is relevant to the pre-training domain Di of the original PLMMi. During continual pre-training on Di+1, Mi+1 suffers from catastrophic forgetting of Di. HenceMi+1 should perform worse on Ti thanMi. In experiments, both PLMs are adapted using the same 32-shot dataset.\n(b) Second, we evaluate the recyclable tuning fromMi\u22121 toMi, which is denoted as \u2206Tii\u22121\u2192 \u2206Tii . Different from setting (a), here the evaluated task Ti is relevant to the pre-training domain Di of the newly released PLMMi.Mi performs better\nthan Mi\u22121 since Mi has acquired more knowledge related to Ti when learningDi. In light of this, we explore whetherMi could achieve better performance thanMi\u22121 even when trained with fewer supervised examples. Specifically, the data size of Mi\u22121 is set to {32, 256, 32}-shot for {T1, T2, T3}, and the data size ofMi is set to {16, 32, 16}-shot, respectively. We also evaluate our method under the zero-shot setting in appendix C.4.\nWe compare our method with utilizing only the task loss (Lfinal-LKD) to validate the benefits of knowledge distillation. Further, we explore combining both distillation-based and initializationbased recyclable tuning (Lfinal+Init.). This is implemented by first using the outdated weights as the initialization then tuning with Lfinal. We also report teacher performance (Teacher) as a reference.\nResults. It can be concluded from Table 2 that: (1) compared with optimizing only the task loss (Lfinal-LKD), distilling knowledge from the outdated weights (Lfinal) significantly improves the performance, which shows that knowledge distillation is an effective way for recyclable tuning. (2) In general, Lfinal+Init. leads to better performance than Lfinal. This finding reveals that both distillation-based and initialization-based methods are complementary to each other and can be further combined to fully exploit the knowledge in outdated weights. (3) In Table 2 setting (a), Mi+1 performs worse thanMi on task Ti, which is becauseMi+1 forgets some knowledge of domain Di when learning Di+1. However, such forgetting can be mitigated by designing better continual pre-training algorithms (Qin et al., 2022c). (4) In Table 2 setting (b),Mi outperformsMi\u22121 despite being trained with fewer examples. This shows that the newly acquired knowledge on domain Di conduces toMi\u2019s performance in Di\u2019s relevant task Ti, and improves the data efficiency. We further discuss the difference between distillation-based and initialization-based methods in appendix F."
        },
        {
            "heading": "6 Discussion",
            "text": "Training-free Weight Recycling. Both methods proposed in \u00a7 5 necessitate tuning the upgraded PLM. Such a process often relies on abundant computational costs and may be infeasible practically. Given the close connections among continually pretrained PLMs, we contend that weight recycling can be realized without training. As a preliminary exploration, we show in appendix B that it\nis possible to learn a cross-task generalizable projection to directly upgrade the outdated weights and make them compatible with the new PLM. Upgrading outdated weights using such a projection requires far fewer computations (< 0.002\u2030) and still achieves satisfactory performance.\nDownstream-compatible Continual Pretraining. From another angle, recyclable tuning addresses the incompatibility between outdated adapted weights and the upgraded PLM from the customer perspective, analogous to the concept of forward compatibility in software engineering. In fact, the responsibility for maintaining compatibility can also be shifted to upstream suppliers during PLM upgrading (i.e., backward compatibility). Potential solutions include adding regularization terms during continual pre-training to maintain compatibility with existing adapted weights. In this way, we solve the incompatibility problem once and for all, which is more customer-friendly. However, modifying pre-training objectives may come at the cost of reduced model performance.\nBroader Application Scenarios. Although we primarily focus on recyclable tuning for one specific scenario (i.e., continual pre-training), PLMs may be subject to various types of evolution in practice. For instance, the expansion of model size (e.g., from T5BASE (Raffel et al., 2020) to T5LARGE), the upgrading of model architecture (Chen et al., 2022; Lee-Thorp et al., 2022), the alteration of optimization objective (e.g., from T5 to T0 (Sanh et al., 2021) and UNIFIEDQA (Khashabi et al., 2020)), etc. Once the backbone infrastructure is upgraded, massive adapted weights would become outdated and potentially wasted. Hence we believe recyclable tuning in fact has broader application scenarios and we hope our findings and solutions could inspire more future research in this area."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we formulate the task of recyclable tuning for continual pre-training. We conduct empirical analyses for this task through the lens of model compatibility, linear mode connectivity, and functional similarity. Inspired by the corresponding findings, we explore the practical benefits of recyclable tuning through parameter initialization and knowledge distillation. We also envision our setup to serve as the testbed for other topics, e.g., crossmodel knowledge transfer and continual learning."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported by the National Key R&D Program of China (No. 2020AAA0106502), Institute Guo Qiang at Tsinghua University, Beijing Academy of Artificial Intelligence (BAAI).\nYujia Qin and Cheng Qian designed the methods. Yujia Qin wrote the paper. Cheng Qian conducted the experiments. Yankai Lin, Zhiyuan Liu, Maosong Sun, and Jie Zhou advised the project. All authors participated in the discussion."
        },
        {
            "heading": "Limitations",
            "text": "We only experiment with two kinds of PLMs (RoBERTaBASE and RoBERTaLARGE (appendix C.3 and appendix C.8)), leaving more diverse kinds of PLMs unexplored. While this allows us to demonstrate the effectiveness of our approach on these specific PLMs, it is important for future work to extend our problem setup to a wider range of PLMs in order to fully understand the generalizability of our findings."
        },
        {
            "heading": "Ethical Statement",
            "text": "In this research, we consider the following ethical issues: \u2022 Privacy. Outdated adapted weights may contain\ninformation about the data and tasks they were trained on. Thus it is important to consider the potential privacy implications when recycling these weights. Efforts should be taken to ensure that personal or sensitive information is not disclosed during weight recycling.\n\u2022 Fairness. It is crucial to guarantee that the recycling of adapted weights does not introduce biases or unfairly advantage certain tasks or domains. Thorough analysis and testing are needed to make sure that recyclable tuning does not perpetuate or amplify existing inequalities.\n\u2022 Responsible AI. The responsible development and deployment of AI systems require considering the potential impacts on the environment. By improving the efficiency and sustainability of PLM adaptation, recyclable tuning contributes to the responsible development of AI systems.\n\u2022 Transparency. To facilitate the responsible and ethical use of recyclable tuning, it is vital to be transparent about the methods and assumptions underlying them. We encourage future works to clearly document the conditions under which\nrecyclable tuning is effective, as well as the potential limitations or risks."
        },
        {
            "heading": "Appendices",
            "text": ""
        },
        {
            "heading": "A Additional Backgrounds",
            "text": ""
        },
        {
            "heading": "A.1 Parameter-efficient Tuning",
            "text": "Conventional downstream adaptation of PLMs involves optimizing all parameters (i.e., fine-tuning), which may cause a heavy burden on the computational infrastructure and storage space. To efficiently utilize the knowledge contained in PLMs, parameter-efficient tuning (PET) is proposed, which optimizes only a few parameters and freezes the majority of parameters (Houlsby et al., 2019). Despite extensively reducing the tunable parameters, PET achieves comparable performance to fine-tuning. Besides, due to its lightweight nature, adapted weights produced by PET are easier to train, store, and share among consumers. Thus we deem PET as an essential component in our problem setup. Without loss of generality, we consider a representative PET algorithm, i.e., adapter (Houlsby et al., 2019) in this paper. Adapter inserts tunable modules into both the feedforward module and multi-head attention module of each Transformer (Vaswani et al., 2017) layer."
        },
        {
            "heading": "A.2 Mode Connectivity",
            "text": "Mode connectivity measures whether two minima in the parameter space can be connected by a parametric path, where the loss (performance) remains low (high) (Garipov et al., 2018; Freeman and Bruna, 2017; Draxler et al., 2018). Such a property implies that different minima can potentially form a connected manifold in the loss landscape. For two connected minima, we can interpolate them to obtain a series of high-performance solutions. These solutions can be ensembled to achieve performance (Garipov et al., 2018) that is better than the endpoints.\nPrior works in mode connectivity show that under most cases, in neural networks, there exists a non-linear low-loss path between different minima. However, only occasionally a linear low-loss path could connect different minima. Later works further contend that it is non-trivial if both minima can be connected by a linear path (Frankle et al., 2020; Mirzadeh et al., 2020). The linearity indicates that both minima may probably lie in the same loss basin (Qin et al., 2022b), which is a more favorable property and indicates a closer connection between both minima. In view of this, we focus on analyzing the linear mode connectivity in this paper.\nPrevious efforts were mainly spent on investigating mode connectivity for non-pre-trained models, until recently, Qin et al. (2022b) explore such property for PLMs. They focus on tuning one static base model with different adaptation strategies. Differently, we take the first step to explore mode connectivity for different backbone models (continually pre-trained PLMs) and reveal novel insights. Following Qin et al. (2022b), we present the results of task performance (e.g., accuracy) to evaluate the mode connectivity in the main paper and also report the results of task loss in appendix E."
        },
        {
            "heading": "B Training-free Weight Recycling",
            "text": "Although we have shown that initialization-based recyclable tuning could accelerate the convergence and improve the training efficiency, tuning the upgraded PLM still requires abundant training computations. Especially considering the massive number of tasks to handle, conducting adaptation for all of them whenever the PLM is upgraded is computationally expensive.\nIn this section, we explore whether we could alleviate the burden of supervised training, and directly upgrade the outdated weights at a small cost. A desired algorithm should consume significantly lower computations than that of training the new PLM from scratch. Meanwhile, this algorithm should achieve satisfactory task performance."
        },
        {
            "heading": "B.1 Framework",
            "text": "Inspired by Qin et al. (2021); Yi et al. (2022), we propose a training-free weight recycling method. Specifically, we learn a cross-task generalizable projection that could directly produce upgraded adapted weights for a specific task, omitting the labor of supervised training. We contend that although there exist massive downstream tasks, a large percentage of them are intrinsically similar and can be categorized into the same task type (e.g., sentiment analysis, question answering, etc.). Intuitively, the upgrading of a certain task T1 should provide a referential experience for that of a similar task T2. In view of this, we propose to make the upgrading process of T1 recyclable so that the upgrading of T2 can be achieved efficiently.\nFor two sequentially released PLMs Mi and Mi+1, assume we have the adapted weights of Mi for both T1 and T2. We aim to recycle these adapted weights for tuning Mi+1 on both tasks. As illustrated in Figure 7, our framework consists\n&!) &!&\") (\n&(() = 1 \u2212 ( &!) + ( &!&\")!\nTunable\n\u2295\n\u2206!*!\n\u2206!*%\n\u2206!&\" *!\n\u2206!&\"*%\u2217\n\u2133!&\"\n\u2133! \u2133!&\"\nFrozen\nBasic Framework\n(1) Projection Learning\nWeight Mixup\nFrozen\n(2) Projection Transferring\nCompatible\n* /+ /+ 0 0 \u2112'( \u2112#\nUnlabeled\nFigure 7: Illustration of our training-free weight recycling algorithm. We learn a cross-task generalizable projection (i.e., the projection learning stage) that could directly upgrade outdated adapted weights (i.e., the projection transferring stage).\nof two stages: (1) projection learning and (2) projection transferring. We learn an upgrading projection using task T1 in the first stage, and then apply (transfer) the learned projection to task T2 in the second stage. Note the first stage requires training while the second stage is training-free. Next, we introduce the details of the two stages.\nProjection Learning. Instead of directly optimizing the parameters in \u2206T1i+1, we learn a low-rank decomposition \u2206T1i+1 = Proj(\u2206 T1 i ) as follows:\nProj\u2217i\u2192i+1 = arg min Proj L(Proj(\u2206T1i )),\nwhere Proj=Proj\u2191\u00d7Proj\u2193. Denote d as a lowdimensional bottleneck dimension, Proj\u2193 projects the dimension of \u2206T1i to d, i.e., Proj\u2193(\u2206 T1 i ) \u2208 Rd. Then Proj\u2191 projects the dimension from d back to |\u2206T1i |, i.e., Proj\u2191(Proj\u2193(\u2206 T1 i ))\u2208R|\u2206 T1 i |. Either Proj\u2191 or Proj\u2193 is implemented by a 2- layer MLP. During training, \u2206T1i is kept frozen and only the parameters in Proj are tuned. Note the dimensions of \u2206T1i and \u2206 T1 i+1 are the same, i.e., |\u2206T1i |= |\u2206 T1 i+1|. Proj(\u2206 T1 i ) is then applied to the upgraded PLMMi+1 to compute the loss L.\nProjection Transferring. When upgrading the outdated weights of a similar task T2, we directly apply the projection Proj\u2217i\u2192i+1 learned on T1 to \u2206T2i and obtain the approximated updated weights \u2206T2\u2217i+1:\n\u2206T2\u2217i+1 = Proj \u2217 i\u2192i+1(\u2206 T2 i ).\nWe formulate the downstream tuning as prompt learning (Schick and Sch\u00fctze, 2021), instead of\nintroducing additional classification heads for different tasks. Hence the number of parameters in \u2206T1i and \u2206 T2 i is the same, i.e., |\u2206 T1 i | = |\u2206 T2 i |. Note that only applying the projection to compute the upgraded weights consumes very limited computations (see Figure 8), hence we significantly reduce the computations of learning \u2206T2i+1, compared with the conventional tuning-based method.\nBesides, since the projection Proj comprises an integral multiple (d\u00d7) of \u2206T1i \u2019s parameters, our solution is only feasible for parameter-efficient tuning. While for fine-tuning, it is computationally intractable to train the projection due to the tremendous size of parameters in \u2206T1i and Proj. Being the first attempt in this research direction, we leave corresponding explorations for fine-tuning as future work."
        },
        {
            "heading": "B.2 Experiments",
            "text": "Settings. We mainly evaluate M1 and M2 as defined in \u00a7 3. We choose a series of NLP tasks and categorize them into 3 classes: (1) natural language inference: MNLI, SICK, ANLI, QNLI (Rajpurkar et al., 2016), and WNLI (Faruqui and Das, 2018), (2) sentiment analysis: SST-2, Amazon Polarity (McAuley and Leskovec, 2013), and Rotten Tomatoes, (3) emotion detection: Hate Speech, Tweet Eval-Offensive, Tweet Eval-Hate, Tweet Eval-Abortion, Tweet Eval-Feminist, and Tweet Eval-Atheism from Barbieri et al. (2020). We partition the tasks belonging to the same category into source task T1 and target task T2 (see Table 3), and learn the projection Proj on the source task.\nWe consider the zero-shot setting for the first stage (projection learning) and use the knowledge distillation loss function LKD. Here the teacher model weights are the adapted M1, and\nthe student model weights are obtained by applying Proj(\u2206T11 ) to the pre-trained weights ofM2. For the unlabeled corpus used for distillation, we evaluate both the target task data (denoted as LTKD) and Wikipedia corpora (LwikiKD ). Note for the former, we only use the input x and discard the corresponding label y (i.e., the zero-shot setting). The former can be seen as the upper bound for the latter since the data format of the latter may not be compatible with the target task. After that, we directly utilize the learned projection to upgrade the outdated weights of similar target tasks.\nBaselines. We consider demonstration learning (Brown et al., 2020) as the baseline, which integrates a few labeled examples into the input text as additional context. The PLM directly performs inference on the test set without incurring any training. For reference, we also report the performance when M2 is adapted using the full dataset (FD) and the 32-shot dataset (FS). Instead, our method requires no labeled data.\nEfficiency Evaluation. We compare the computational costs needed for our training-free method and the conventional tuning-based method in Figure 8. For the former, we record the time needed in projection transferring (i.e., computing the upgraded weights \u2206T2\u2217i+1). For the latter, we record the training time needed until an adaptation converges. It can be derived that our method requires significantly fewer computations, which demonstrates its efficiency. In practice, such a projection can be trained once and for all. As long as we have obtained the projection, we can directly upgrade\npotentially massive outdated weights in an efficient manner, and the computations involved during projection learning can be neglected. Although currently we only support projection transferring for a similar target task that belongs to the same category of the source task, we expect future work to explore how to train a universal projection that could be applied to an arbitrary task.\nPerformance Evaluation. The results are shown in Table 3, from which we find that: (1) our method generally outperforms the demonstration baseline, and could surpass the supervised performance (FD and FS) under certain cases, despite not using any labeled data. Hence, besides being computationally efficient, our method achieves satisfactory performance in general. This also validates our intuition that for continually pre-trained PLMs, the upgrading of a specific task could provide referential experience for similar tasks; (2) using the task data (LTKD) for distillation generally performs better than using Wikipedia (LwikiKD ), showing the importance of proper data distribution used for knowledge distillation."
        },
        {
            "heading": "C Additional Experiments and Analyses",
            "text": ""
        },
        {
            "heading": "C.1 Euclidean Distance Analysis",
            "text": "We report the Euclidean distance of continually pretrained PLMs and the corresponding adapted models. We evaluate when the official RoBERTaBASE is adapted on the BIO domain for 12.5k steps following the settings in \u00a7 3. We save the checkpoint for every 2.5k steps. For each checkpoint M1(t), denote its weights as \u03b801(t), we fine-tune it on CHEMPROT to obtain its adapted weights \u2206T1 (t), where |\u2206T1 (t)| = |\u03b801(t)|. The resultant model weights are \u03b8T1 (t) = \u03b8 0 1(t)\u2295\u2206T1 (t).\nGiven two continually pre-trained modelsM1(t) andM1(t\u2032), where t\u2032= t + 2.5k, we flatten their pre-trained weights \u03b801(t) and \u03b8 0 1(t \u2032), and calculate their L-2 norm1: ||\u03b801(t\u2032) \u2212 \u03b801(t)||. In addition, we also calculate the L-2 norm of flattened adapted weights (||\u2206T1 (t)|| / ||\u2206T1 (t\u2032)||) and distance between adapted PLMs (||\u03b8T1 (t\u2032)\u2212 \u03b8T1 (t)||). We illustrate the results in Figure 10 and find that: (1) ||\u03b801(t\u2032) \u2212 \u03b801(t)|| / ||\u03b8T1 (t\u2032) \u2212 \u03b8T1 (t)|| gradually decreases with t increasing. This is mainly because the learning rates are warmed up for the first 6% steps, and the learning rate starts to decrease at the 0.75k-th step, which means the PLM gradually moves slower in the parameter space; (2) the parameter change caused by downstream adaptation (i.e., ||\u2206T1 (t)|| / ||\u2206T1 (t\u2032)||) is far\n1We use torch.dist function in PyTorch (Paszke et al., 2019) for implementation.\nsmaller than that brought by continual pre-training (||\u03b801(t\u2032) \u2212 \u03b801(t)||). This is because downstream adaptation converges shortly. After convergence, the model parameters generally stay in a specific optimal region. While continual pre-training constantly pushes the model weights away from the previous checkpoints in the parameter space. Another reason is that continual pre-training uses a large batch 2048, while downstream adaptation often uses a much smaller batch size (e.g., 16)."
        },
        {
            "heading": "C.2 More Visualization for Functional Similarity Analysis",
            "text": "In the main paper (Figure 5), we visualize three different attention heads of M0, M1, and M2. In this section, we present more visualizations to further support our claim. We also visualize the attention pattern of an independently trained PLMMIND. The results in Figure 9 again demonstrate our claim that continually pre-trained PLMs exhibit similar attention patterns, which independently trained PLMs do not have.\nC.3 Initialization-based Recyclable Tuning for Fine-tuning and RoBERTaLARGE\nIn \u00a7 5.1, we mainly evaluate initialization-based recyclable tuning using RoBERTaBASE and adapter tuning. Here we extend the experiments to either fine-tuning (Table 4) or RoBERTaLARGE (Table 5). We choose 3 tasks in Table 1 and follow most of the settings. From Table 4 and Table 5, we find that the main conclusions are generally consistent with those mentioned in the main paper. This implies that the initialization-based method can be applied to different tuning methods and PLMs."
        },
        {
            "heading": "C.4 Distillation-based Recyclable Tuning under the Zero-shot Setting",
            "text": "We extend our distillation-based recyclable tuning to the zero-shot setting where there is no labeled data for tuning the upgraded PLM. We show that it\nis able to utilize unlabeled raw corpora to distill the knowledge of outdated weights. Specifically, we remove the task loss LT in Lfinal and only retain LKD. Instead of using supervised examples, we sample unlabeled data x from Wikipedia to compute LKD. We evaluate recyclable tuning between M1 andM2 and choose 4 downstream tasks, i.e., CHEMPROT, IMDB, SST-2, and MNLI. For each task, the outdated weights ofM1 are obtained with the full dataset, and our goal is to distill their knowledge and optimizeM2\u2019s weights.\nTwo training-free baselines are considered: (1) manual prompting (Schick and Sch\u00fctze, 2021), which restructures the input into templates by inserting prompts, and (2) demonstration learning, which has been introduced in appendix B.2. For both baselines, the PLM directly performs inference on the test set without incurring any training. Moreover, we also evaluate the performance when knowledge distillation is combined with the initialization-based method.\nWe list the results in Table 6, from which it can be derived that: (1) our method surpasses manual prompting and demonstration learning by a large margin, which shows the benefits of recycling outdated adapted weights in the zero-shot setting; (2) initializing tunable weights with the outdated weights could further improve the performance of LKD, which again demonstrates that both\ninitialization-based and distillation-based methods are complementary to each other.\nC.5 Interpolation Distillation\nTraditional knowledge distillation frameworks have no assumptions about the parametric connection between the teacher and the student, and resort to pulling closer their predictions (P) or inner representations (h). As we have shown in the main paper, continually pre-trained PLMs are guaranteed with close parametric connections. Therefore, traditional knowledge distillation methods may fail to exploit the parametric knowledge contained in the teacher model\u2019s parameters. Here we explore another way for more effective distillation-based recyclable tuning under our setting.\nFramework. Inspired by MC-SGD (Mirzadeh et al., 2020), we propose an interpolation distillation technique to fully exploit the parametric knowledge contained in outdated adapted weights. Specifically, for recyclable tuning betweenMi and Mi+1, instead of optimizing the overall loss function using the only endpoint checkpoint (\u03b8Lji+1 = \u03b80i+1 \u2295 \u2206 Tj i+1) for task Tj , we linearly interpolate \u03b8 Lj i and \u03b8 Lj i+1 to obtain a series of model checkpoints: \u03b8(\u00b5) = (1\u2212\u00b5)\u03b8Lji +\u00b5\u03b8 Lj i+1. After that, we feed data into \u03b8(\u00b5) and minimize the corresponding\nloss together with L(\u03b8Lji+1):\nLITP(\u2206 Tj i+1) = L(\u03b8 Lj i+1)+\u03b3 \u2211 \u00b5\u2208{ 1\nN\u00b5 ,\u00b7\u00b7\u00b7 ,N\u00b5\u22121 N\u00b5 }\nL(\u03b8(\u00b5)),\nwhere \u03b3 is a hyper-parameter, and N\u00b5 denotes a constant integer. In practice, we found a small N\u00b5 (e.g., 2) already achieves satisfying performance. During optimization, only \u2206Tji+1 is tuned by receiving gradients from both L(\u03b8ji+1) and L(\u03b8(\u00b5)).\nExperiments. We follow most of the settings in \u00a7 5.2 and evaluate the performance of interpolation distillation. We compare it with the results of LfinalLKD, Lfinal, and Lfinal+Init.. All results are shown in Table 7, from which we observe that the interpolation distillation method (LITP) generally outperforms the vanilla distillation (Lfinal), and could surpass Lfinal+Init. in certain cases. This shows that interpolation distillation successfully exploits the parametric knowledge contained in the outdated adapted weights, and serves as an improved method for the distillation-based method."
        },
        {
            "heading": "C.6 Effects of Teacher Model Capability for Distillation-based Recyclable Tuning",
            "text": "For experiments of setting (a) in distillation-based recyclable tuning (\u00a7 5.2), the teacher model is trained with the same 32-shot dataset as the student model. Here we explore whether a teacher model with stronger capabilities would conduce to the student\u2019s performance. Specifically, keeping all the other settings the same, we change the teacher model\u2019s data to the full-data size. The new results are placed in Table 8, from which we conclude that: (1) our methods (Lfinal, LITP, and Lfinal+Init.) still outperform the baseline without knowledge distillation (Lfinal-LKD); (2) comparing the student\u2019s performance in Table 8 and Table 2 setting (a), we\nMethod Lfinal-LKD Lfinal LITP Lfinal+Init.\nFew-shot teacher\n\u2206T11 \u2192\u2206 T1 3 AP 60.5\u00b12.1 66.3\u00b11.5 67.5\u00b11.7 67.2\u00b11.5 FT 61.9\u00b11.3 64.7\u00b10.9 64.8\u00b10.8 65.4\u00b11.3\n\u2206T11 \u2192\u2206 T1 4 AP 56.6\u00b11.1 57.9\u00b11.5 65.3\u00b11.7 64.9\u00b13.6 FT 59.7\u00b12.3 62.9\u00b12.4 64.4\u00b10.5 65.1\u00b12.2\nFull-data teacher\nfind through learning from a more powerful teacher, the student\u2019s performance is improved as well."
        },
        {
            "heading": "C.7 Experiments on Non-adjacent PLMs",
            "text": "For most of the experiments, we mainly focus on recyclable tuning between adjacent PLMs. We contend that the proposed methods should also work for non-adjacent PLMs since they are still guaranteed with close connections. To demonstrate this, we take the distillation-based recyclable tuning as an example. Specifically, we evaluate the distillation-based recyclable tuning between (M1, M3) and (M1, M4) using T1, and largely follow the settings in \u00a7 5.2. We choose setting (a) in \u00a7 5.2, and the only difference is that the teacher modelM1 is trained either using the 32- shot dataset (dubbed as few-shot teacher) or the full dataset (dubbed as full-data teacher). While the student model is trained using the 32-shot dataset. In this way, we could understand the role of the teacher model in knowledge distillation.\nThe results are placed in Table 9, from which we find that: (1) introducing knowledge distillation (Lfinal) improves the performance than only using task loss (Lfinal-LKD) and (2) introducing the parametric knowledge either through interpolation distillation (LITP) or weight initialization (Lfinal+Init.) could further improve the task performance. Both conclusions are aligned with those obtained on adjacent PLMs. This demonstrates our claim that our recyclable tuning is not limited to adjacent PLMs, but also non-adjacent ones. Finally, we observe that the student performance when the teacher is trained\nusing full data is much better, which shows the benefits of learning from a more advanced teacher."
        },
        {
            "heading": "C.8 Distillation-based Recyclable Tuning",
            "text": "Experiments using RoBERTaLARGE\nPrevious experiments for distillation-based recyclable tuning are based on RoBERTaBASE, now we turn to RoBERTaLARGE to show that our proposed methods are model-agnostic. We experiment with M1 and M2 using the task CHEMPROT. Other settings are kept the same as those in appendix C.7. In Table 10, we show that the results are generally aligned with our conclusions before. These results also reflect that our proposed method is agnostic to the specific PLM chosen."
        },
        {
            "heading": "C.9 Effects of Data Size for Distillation-based Recyclable Tuning",
            "text": "Taking a step further, we study the performance of our distillation-based recyclable tuning at different data scales. Specifically, we focus on T2 (IMDB) for recycling M1\u2019s outdated weights to M2, whereM1 is adapted using the full dataset, andM2 is trained with {8, 16, 32, 64}-shot dataset,\nrespectively. By comparing the method mentioned in appendix C.5 (LITP) with only the task loss LT , we visualize the performance variation in Figure 11, from which we observe that: LITP surpasses only the task loss (LT ) in general. However, with the data scale increasing, the improvement becomes smaller. This is becauseM2 is more adept at T2 thanM1 due to the incremental knowledge acquisition of D2. When there are only a few examples to trainM2, the teacher model has the advantage of more labeled data. However, with the data size of the student gradually approaching that of the teacher, learning from the teacher gradually becomes redundant. The student model could well master the downstream knowledge on its own."
        },
        {
            "heading": "D Training Details",
            "text": "We ensure that all the artifacts used in this paper are consistent with their intended use."
        },
        {
            "heading": "D.1 Pre-training",
            "text": "We conduct pre-training using 8 NVIDIA V100 GPUs based on fairseq2 (Ott et al., 2019). We choose Adam (Kingma and Ba, 2015) as the optimizer. The hyper-parameters ( , \u03b21, \u03b22) for Adam are set to 1 \u00d7 10\u22126, 0.9, 0.98, respectively. The dropout rate and weight decay are set to 0.1 and 0.01, respectively. The total number of parameters of RoBERTaBASE and RoBERTaLARGE are 125M and 355M, respectively. We implement pretraining using the codes of Qin et al. (2022a).\nContinual Pre-training. We start with the official RoBERTa model and sequentially pre-train the\n2https://github.com/pytorch/fairseq\nPLM on 4 domains. For each domain, we set the batch size to 2048, the training steps to 12.5k, and the max sequence length to 512.\nPre-training from Scratch. For MIND that is pre-trained from scratch, we follow the model structure of RoBERTaBASE, and pre-train the model on the concatenation of Wikipedia and BookCorpus (Zhu et al., 2015), which is the same as the pre-training corpus of BERT (Devlin et al., 2019). We pre-train the model for 125k steps, using a batch size of 2048 and a sequence length of 512. The total computations involved are roughly comparable to those of BERTBASE.MIND has totally different initialization and pre-training corpus than the official RoBERTaBASE, which helps us understand the property between independently trained PLMs."
        },
        {
            "heading": "D.2 Empirical Analyses",
            "text": "Model Compatibility Analysis. We adapt the initial PLM M0 on two tasks CHEMPROT and MNLI. The training hyper-parameters conform to those listed in Table 11. All experiments are conducted 3 times with different random seeds, and we report the average results.\nLinear Mode Connectivity Analysis. All the training hyper-parameters conform to those in Table 11. The endpoints are adapted three times using different random seeds. We test the performance of 25 evenly distributed points along the linear path and two endpoints. We report the average performance over three random seeds.\nFunctional Similarity Analysis. We adapt different PLMs on task CHEMPROT using the hyperparameters listed in Table 11. We randomly sample one instance3 from CHEMPROT and feed it\n3We find empirically that the results and conclusions are very consistent across different random samples.\ninto different PLMs to obtain the scores after the self-attention computation. We draw the attention scores for the first 25 tokens of the sampled instance."
        },
        {
            "heading": "D.3 Methods and Experiments",
            "text": "For the optimizer of all the experiments in \u00a7 5, we choose AdamW (Loshchilov and Hutter, 2019).\nInitialization-based Recyclable Tuning. We adapt M0 on the source tasks using the hyperparameters listed in Table 11. The adapted weights are further used as target tasks\u2019 initialization (except the Random setting). The target tasks\u2019 training configurations also conform to Table 11. We conduct the experiments for 3 times with different random seeds and report the average performance. The choices of Tdiff and Tsim for different target tasks are shown in Table 12. The evaluation interval for each target task is also reported in Table 12.\nDistillation-based Recyclable Tuning. We set the maximum training step for CHEMPROT and ACL-ARC to 100k and the maximum training step\nfor IMDB to 50k. The learning rate and batch size are set to 1\u00d7 10\u22124 and 2, respectively. We warm up the learning rate for the first 8% percentage of total training steps. We report the average results over 3 different random seeds. As for other hyper-parameters discussed in \u00a7 5.2, we perform grid search for \u03b2 over {0.1, 0.3}, and \u03b1(1 \u2212 \u03b2) over {0, 1, 5, 10, 50, 100}. We also conduct a grid search for the temperature in knowledge distillation loss over {10, 20} when calculating KL(P(x,Mi)||P(x,Mi+1)). We select the bestperforming combination of these hyper-parameters and then report the performance. Our grid search is performed for our method and all the baseline methods for a fair comparison."
        },
        {
            "heading": "E The Visualization of Loss for Linear Mode Connectivity Analysis",
            "text": "When conducting experiments for the mode connectivity analysis in the main paper, we mainly resort to performance as the evaluation protocol for the interpolations following Qin et al. (2022b). In this section, we show the corresponding visualization of loss for Figure 3 and Figure 4, see Figure 12 and Figure 13. From these figures, we conclude that a significant loss barrier generally indicates the existence of a large performance drop."
        },
        {
            "heading": "F Comparison of Initialization-based and Distillation-based Recyclable Tuning",
            "text": "Both initialization-based and distillation-based methods serve as powerful ways for recyclable tuning under the continual pre-training scenario. Both methods have their own advantages, where the initialization-based method can bring faster convergence and performance improvement, while the distillation-based method can bring improvement in performance as well (but may be less efficient). In addition, both methods can be combined with each other to further improve performance.\nIn terms of practical application scenarios, both methods are slightly different. For one thing, the initialization-based method requires that the architectures of the new PLM and the old PLM are the same. This requirement may be infeasible for broader application scenarios, such as recyclable tuning between different PLMs as discussed in \u00a7 6. For another, the initialization-based method typically requires access to the parameters of the outdated adapted weights. This can be\na practical issue due to model privacy concerns. While some customers are willing to share their adapted weights on public platforms like AdapterHub (Pfeiffer et al., 2020), a majority of adapted weights are publicly unavailable. In contrast, the distillation-based method can be achieved without access to the model weights, but through receiving model inference from the owner (e.g., API-based online knowledge transfer (Krishna et al., 2019)). In this sense, the distillation-based method could protect the model privacy to a certain degree.\nBroader Impacts\nThis research has the potential to have a broad impact in several ways. \u2022 First, recyclable tuning could improve the ef-\nficiency of adapting PLMs to new tasks. By recycling adapted weights from previous tasks, the need for costly retraining can be reduced, potentially making it more feasible to apply PLMs in a wider range of scenarios.\n\u2022 Second, the results of this research could have implications for the sustainability of machine learning systems. Reusing adapted weights rather than discarding them can help us reduce the carbon footprint and resource consumption of PLM adaptation, making it more environmentally friendly.\n\u2022 Third, this research has the potential to benefit a wide range of stakeholders, including researchers, developers, and users of PLMs. Researchers can use the proposed task and benchmark to develop and evaluate new techniques for recyclable tuning, while developers can apply these techniques to improve the efficiency and sustainability of PLM-based systems. Finally, users of PLMs can benefit from the reduced costs and improved performance made possible by recyclable tuning. Overall, this research on recyclable tuning for continual pre-training has the potential to have a wide-ranging impact on the efficiency, sustainability, and practicality of machine learning systems."
        }
    ],
    "title": "Recyclable Tuning for Continual Pre-training",
    "year": 2023
}